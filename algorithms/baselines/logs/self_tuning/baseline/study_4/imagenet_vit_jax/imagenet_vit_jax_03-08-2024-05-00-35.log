python3 submission_runner.py --framework=jax --workload=imagenet_vit --submission_path=prize_qualification_baselines/self_tuning/jax_nadamw_full_budget.py --data_dir=/data/imagenet/jax --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=prize_qualification_self_tuning/study_4 --overwrite=true --save_checkpoints=false --rng_seed=3803206025 --max_global_steps=559998 --imagenet_v2_data_dir=/data/imagenet/jax --tuning_ruleset=self 2>&1 | tee -a /logs/imagenet_vit_jax_03-08-2024-05-00-35.log
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
I0308 05:00:55.938817 139902746892096 logger_utils.py:76] Creating experiment directory at /experiment_runs/prize_qualification_self_tuning/study_4/imagenet_vit_jax.
I0308 05:00:56.932799 139902746892096 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA Interpreter Host
I0308 05:00:56.933755 139902746892096 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0308 05:00:56.933933 139902746892096 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0308 05:00:57.768373 139902746892096 submission_runner.py:605] Creating directory at /experiment_runs/prize_qualification_self_tuning/study_4/imagenet_vit_jax/trial_1.
I0308 05:00:57.965320 139902746892096 submission_runner.py:206] Initializing dataset.
I0308 05:00:57.980430 139902746892096 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0308 05:00:57.990942 139902746892096 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0308 05:00:58.369959 139902746892096 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0308 05:01:06.608560 139902746892096 submission_runner.py:213] Initializing model.
I0308 05:01:15.343434 139902746892096 submission_runner.py:255] Initializing optimizer.
I0308 05:01:16.314308 139902746892096 submission_runner.py:262] Initializing metrics bundle.
I0308 05:01:16.314495 139902746892096 submission_runner.py:280] Initializing checkpoint and logger.
I0308 05:01:16.315275 139902746892096 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification_self_tuning/study_4/imagenet_vit_jax/trial_1 with prefix checkpoint_
I0308 05:01:16.315420 139902746892096 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification_self_tuning/study_4/imagenet_vit_jax/trial_1/meta_data_0.json.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0308 05:01:16.652510 139902746892096 logger_utils.py:220] Unable to record git information. Continuing without it.
I0308 05:01:16.959204 139902746892096 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification_self_tuning/study_4/imagenet_vit_jax/trial_1/flags_0.json.
I0308 05:01:16.971160 139902746892096 submission_runner.py:314] Starting training loop.
I0308 05:02:01.742808 139740946884352 logging_writer.py:48] [0] global_step=0, grad_norm=0.35123199224472046, loss=6.907756328582764
I0308 05:02:01.763051 139902746892096 spec.py:321] Evaluating on the training split.
I0308 05:02:01.974059 139902746892096 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0308 05:02:01.984180 139902746892096 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0308 05:02:02.078369 139902746892096 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0308 05:02:19.541634 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 05:02:19.552777 139902746892096 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0308 05:02:19.569541 139902746892096 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0308 05:02:19.642602 139902746892096 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0308 05:02:38.335506 139902746892096 spec.py:349] Evaluating on the test split.
I0308 05:02:38.342781 139902746892096 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0308 05:02:38.348623 139902746892096 dataset_builder.py:528] Reusing dataset imagenet_v2 (/data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0)
I0308 05:02:38.398636 139902746892096 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0308 05:02:43.776565 139902746892096 submission_runner.py:411] Time since start: 86.81s, 	Step: 1, 	{'train/accuracy': 0.0009960937313735485, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 44.79174327850342, 'total_duration': 86.80535459518433, 'accumulated_submission_time': 44.79174327850342, 'accumulated_eval_time': 42.01347494125366, 'accumulated_logging_time': 0}
I0308 05:02:43.794170 139708399068928 logging_writer.py:48] [1] accumulated_eval_time=42.013475, accumulated_logging_time=0, accumulated_submission_time=44.791743, global_step=1, preemption_count=0, score=44.791743, test/accuracy=0.001000, test/loss=6.907757, test/num_examples=10000, total_duration=86.805355, train/accuracy=0.000996, train/loss=6.907756, validation/accuracy=0.001000, validation/loss=6.907756, validation/num_examples=50000
I0308 05:03:46.037418 139737700456192 logging_writer.py:48] [100] global_step=100, grad_norm=0.4582446813583374, loss=6.886221885681152
I0308 05:04:28.520586 139737708848896 logging_writer.py:48] [200] global_step=200, grad_norm=0.8139545917510986, loss=6.783972263336182
I0308 05:05:13.116839 139737700456192 logging_writer.py:48] [300] global_step=300, grad_norm=1.3399606943130493, loss=6.652459621429443
I0308 05:05:57.524091 139737708848896 logging_writer.py:48] [400] global_step=400, grad_norm=1.4092906713485718, loss=6.608523845672607
I0308 05:06:41.956133 139737700456192 logging_writer.py:48] [500] global_step=500, grad_norm=1.0150960683822632, loss=6.441729545593262
I0308 05:07:26.317933 139737708848896 logging_writer.py:48] [600] global_step=600, grad_norm=1.302433729171753, loss=6.328929424285889
I0308 05:08:11.063702 139737700456192 logging_writer.py:48] [700] global_step=700, grad_norm=1.1906754970550537, loss=6.2654876708984375
I0308 05:08:55.338309 139737708848896 logging_writer.py:48] [800] global_step=800, grad_norm=1.367615818977356, loss=6.215804100036621
I0308 05:09:39.813676 139737700456192 logging_writer.py:48] [900] global_step=900, grad_norm=2.0298030376434326, loss=6.227717399597168
I0308 05:09:44.115952 139902746892096 spec.py:321] Evaluating on the training split.
I0308 05:09:55.708079 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 05:10:03.906845 139902746892096 spec.py:349] Evaluating on the test split.
I0308 05:10:05.617272 139902746892096 submission_runner.py:411] Time since start: 528.65s, 	Step: 911, 	{'train/accuracy': 0.03218749910593033, 'train/loss': 5.927576541900635, 'validation/accuracy': 0.03003999963402748, 'validation/loss': 5.954702377319336, 'validation/num_examples': 50000, 'test/accuracy': 0.024000000208616257, 'test/loss': 6.074681758880615, 'test/num_examples': 10000, 'score': 465.0541150569916, 'total_duration': 528.6460611820221, 'accumulated_submission_time': 465.0541150569916, 'accumulated_eval_time': 63.51478552818298, 'accumulated_logging_time': 0.02664351463317871}
I0308 05:10:05.634112 139708407461632 logging_writer.py:48] [911] accumulated_eval_time=63.514786, accumulated_logging_time=0.026644, accumulated_submission_time=465.054115, global_step=911, preemption_count=0, score=465.054115, test/accuracy=0.024000, test/loss=6.074682, test/num_examples=10000, total_duration=528.646061, train/accuracy=0.032187, train/loss=5.927577, validation/accuracy=0.030040, validation/loss=5.954702, validation/num_examples=50000
I0308 05:10:41.052624 139708415854336 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.2887667417526245, loss=6.038479804992676
I0308 05:11:24.719073 139708407461632 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.2446568012237549, loss=5.960165977478027
I0308 05:12:09.042211 139708415854336 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.1817066669464111, loss=6.294707298278809
I0308 05:12:53.281172 139708407461632 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.3577219247817993, loss=5.963889122009277
I0308 05:13:37.648721 139708415854336 logging_writer.py:48] [1400] global_step=1400, grad_norm=1.3026957511901855, loss=5.885857582092285
I0308 05:14:22.107021 139708407461632 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.0981281995773315, loss=6.094729900360107
I0308 05:15:06.521273 139708415854336 logging_writer.py:48] [1600] global_step=1600, grad_norm=1.3342331647872925, loss=5.779654502868652
I0308 05:15:51.110009 139708407461632 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.9762179851531982, loss=5.803779125213623
I0308 05:16:35.556736 139708415854336 logging_writer.py:48] [1800] global_step=1800, grad_norm=1.201309084892273, loss=5.8436479568481445
I0308 05:17:05.932072 139902746892096 spec.py:321] Evaluating on the training split.
I0308 05:17:17.608541 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 05:17:25.831706 139902746892096 spec.py:349] Evaluating on the test split.
I0308 05:17:27.519336 139902746892096 submission_runner.py:411] Time since start: 970.55s, 	Step: 1870, 	{'train/accuracy': 0.08306640386581421, 'train/loss': 5.143368244171143, 'validation/accuracy': 0.0786999985575676, 'validation/loss': 5.1934967041015625, 'validation/num_examples': 50000, 'test/accuracy': 0.06360000371932983, 'test/loss': 5.44067907333374, 'test/num_examples': 10000, 'score': 885.2891094684601, 'total_duration': 970.5481154918671, 'accumulated_submission_time': 885.2891094684601, 'accumulated_eval_time': 85.10202193260193, 'accumulated_logging_time': 0.05381202697753906}
I0308 05:17:27.538042 139708407461632 logging_writer.py:48] [1870] accumulated_eval_time=85.102022, accumulated_logging_time=0.053812, accumulated_submission_time=885.289109, global_step=1870, preemption_count=0, score=885.289109, test/accuracy=0.063600, test/loss=5.440679, test/num_examples=10000, total_duration=970.548115, train/accuracy=0.083066, train/loss=5.143368, validation/accuracy=0.078700, validation/loss=5.193497, validation/num_examples=50000
I0308 05:17:39.775788 139708415854336 logging_writer.py:48] [1900] global_step=1900, grad_norm=1.0409438610076904, loss=6.134364128112793
I0308 05:18:20.583013 139708407461632 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.1345226764678955, loss=5.650221347808838
I0308 05:19:04.920461 139708415854336 logging_writer.py:48] [2100] global_step=2100, grad_norm=1.0412840843200684, loss=5.551708698272705
I0308 05:19:49.392755 139708407461632 logging_writer.py:48] [2200] global_step=2200, grad_norm=1.0325231552124023, loss=5.431797504425049
I0308 05:20:33.653314 139708415854336 logging_writer.py:48] [2300] global_step=2300, grad_norm=1.3840596675872803, loss=5.604598045349121
I0308 05:21:18.084172 139708407461632 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.987842321395874, loss=5.533862590789795
I0308 05:22:02.106844 139708415854336 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.8826150298118591, loss=5.631213665008545
I0308 05:22:46.443398 139708407461632 logging_writer.py:48] [2600] global_step=2600, grad_norm=1.1280584335327148, loss=5.42926025390625
I0308 05:23:30.930284 139708415854336 logging_writer.py:48] [2700] global_step=2700, grad_norm=1.0806463956832886, loss=5.421566963195801
I0308 05:24:15.243189 139708407461632 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.7673525810241699, loss=6.4145331382751465
I0308 05:24:27.815616 139902746892096 spec.py:321] Evaluating on the training split.
I0308 05:24:39.688890 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 05:24:47.925666 139902746892096 spec.py:349] Evaluating on the test split.
I0308 05:24:49.614440 139902746892096 submission_runner.py:411] Time since start: 1412.64s, 	Step: 2830, 	{'train/accuracy': 0.1516406238079071, 'train/loss': 4.460026264190674, 'validation/accuracy': 0.13831999897956848, 'validation/loss': 4.5624847412109375, 'validation/num_examples': 50000, 'test/accuracy': 0.10940000414848328, 'test/loss': 4.899428367614746, 'test/num_examples': 10000, 'score': 1305.504187822342, 'total_duration': 1412.643222808838, 'accumulated_submission_time': 1305.504187822342, 'accumulated_eval_time': 106.90083980560303, 'accumulated_logging_time': 0.0830845832824707}
I0308 05:24:49.630918 139708415854336 logging_writer.py:48] [2830] accumulated_eval_time=106.900840, accumulated_logging_time=0.083085, accumulated_submission_time=1305.504188, global_step=2830, preemption_count=0, score=1305.504188, test/accuracy=0.109400, test/loss=4.899428, test/num_examples=10000, total_duration=1412.643223, train/accuracy=0.151641, train/loss=4.460026, validation/accuracy=0.138320, validation/loss=4.562485, validation/num_examples=50000
I0308 05:25:17.652381 139708407461632 logging_writer.py:48] [2900] global_step=2900, grad_norm=1.0160126686096191, loss=5.573594093322754
I0308 05:26:00.514312 139708415854336 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.5141512155532837, loss=5.135382175445557
I0308 05:26:44.893343 139708407461632 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.9134519696235657, loss=5.207550525665283
I0308 05:27:29.300253 139708415854336 logging_writer.py:48] [3200] global_step=3200, grad_norm=1.0291645526885986, loss=5.12495231628418
I0308 05:28:13.709106 139708407461632 logging_writer.py:48] [3300] global_step=3300, grad_norm=1.4217256307601929, loss=5.131625652313232
I0308 05:28:58.279503 139708415854336 logging_writer.py:48] [3400] global_step=3400, grad_norm=1.0347895622253418, loss=5.055929183959961
I0308 05:29:42.580336 139708407461632 logging_writer.py:48] [3500] global_step=3500, grad_norm=1.1369736194610596, loss=4.981963634490967
I0308 05:30:26.842116 139708415854336 logging_writer.py:48] [3600] global_step=3600, grad_norm=1.7900651693344116, loss=4.919894695281982
I0308 05:31:11.015218 139708407461632 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.8612973093986511, loss=5.053664684295654
I0308 05:31:49.933702 139902746892096 spec.py:321] Evaluating on the training split.
I0308 05:32:01.636104 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 05:32:11.019378 139902746892096 spec.py:349] Evaluating on the test split.
I0308 05:32:12.704230 139902746892096 submission_runner.py:411] Time since start: 1855.73s, 	Step: 3789, 	{'train/accuracy': 0.20103515684604645, 'train/loss': 4.068780422210693, 'validation/accuracy': 0.1882999986410141, 'validation/loss': 4.150966644287109, 'validation/num_examples': 50000, 'test/accuracy': 0.14630000293254852, 'test/loss': 4.5577168464660645, 'test/num_examples': 10000, 'score': 1725.7441012859344, 'total_duration': 1855.7330009937286, 'accumulated_submission_time': 1725.7441012859344, 'accumulated_eval_time': 129.67135000228882, 'accumulated_logging_time': 0.11062073707580566}
I0308 05:32:12.722784 139708415854336 logging_writer.py:48] [3789] accumulated_eval_time=129.671350, accumulated_logging_time=0.110621, accumulated_submission_time=1725.744101, global_step=3789, preemption_count=0, score=1725.744101, test/accuracy=0.146300, test/loss=4.557717, test/num_examples=10000, total_duration=1855.733001, train/accuracy=0.201035, train/loss=4.068780, validation/accuracy=0.188300, validation/loss=4.150967, validation/num_examples=50000
I0308 05:32:17.453005 139708407461632 logging_writer.py:48] [3800] global_step=3800, grad_norm=1.3576674461364746, loss=4.825761795043945
I0308 05:32:57.470477 139708415854336 logging_writer.py:48] [3900] global_step=3900, grad_norm=1.1853227615356445, loss=4.878255844116211
I0308 05:33:41.706977 139708407461632 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.9051675796508789, loss=5.912497520446777
I0308 05:34:26.126869 139708415854336 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.963656485080719, loss=4.6209797859191895
I0308 05:35:10.756753 139708407461632 logging_writer.py:48] [4200] global_step=4200, grad_norm=1.2333476543426514, loss=5.393614292144775
I0308 05:35:55.386383 139708415854336 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.6716492772102356, loss=6.02644157409668
I0308 05:36:39.964507 139708407461632 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.9180312156677246, loss=4.405512809753418
I0308 05:37:24.362193 139708415854336 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.8241650462150574, loss=5.0109124183654785
I0308 05:38:08.976071 139708407461632 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.8046782612800598, loss=6.183475017547607
I0308 05:38:53.276485 139708415854336 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.91409832239151, loss=4.831539630889893
I0308 05:39:12.860667 139902746892096 spec.py:321] Evaluating on the training split.
I0308 05:39:24.742820 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 05:39:34.359615 139902746892096 spec.py:349] Evaluating on the test split.
I0308 05:39:36.038956 139902746892096 submission_runner.py:411] Time since start: 2299.07s, 	Step: 4746, 	{'train/accuracy': 0.2476171851158142, 'train/loss': 3.7524003982543945, 'validation/accuracy': 0.2291399985551834, 'validation/loss': 3.8650801181793213, 'validation/num_examples': 50000, 'test/accuracy': 0.17740000784397125, 'test/loss': 4.310952663421631, 'test/num_examples': 10000, 'score': 2145.8203287124634, 'total_duration': 2299.0677371025085, 'accumulated_submission_time': 2145.8203287124634, 'accumulated_eval_time': 152.84962821006775, 'accumulated_logging_time': 0.1393589973449707}
I0308 05:39:36.059697 139708407461632 logging_writer.py:48] [4746] accumulated_eval_time=152.849628, accumulated_logging_time=0.139359, accumulated_submission_time=2145.820329, global_step=4746, preemption_count=0, score=2145.820329, test/accuracy=0.177400, test/loss=4.310953, test/num_examples=10000, total_duration=2299.067737, train/accuracy=0.247617, train/loss=3.752400, validation/accuracy=0.229140, validation/loss=3.865080, validation/num_examples=50000
I0308 05:39:57.793864 139708415854336 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.6941478848457336, loss=5.907039642333984
I0308 05:40:39.544524 139708407461632 logging_writer.py:48] [4900] global_step=4900, grad_norm=1.0629137754440308, loss=4.461195945739746
I0308 05:41:24.466982 139708415854336 logging_writer.py:48] [5000] global_step=5000, grad_norm=1.019897699356079, loss=4.870678901672363
I0308 05:42:08.852042 139708407461632 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.8990657925605774, loss=4.179068565368652
I0308 05:42:53.637119 139708415854336 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.949479877948761, loss=4.351895809173584
I0308 05:43:37.668772 139708407461632 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.9195323586463928, loss=4.366115570068359
I0308 05:44:21.945172 139708415854336 logging_writer.py:48] [5400] global_step=5400, grad_norm=1.1289669275283813, loss=4.400463104248047
I0308 05:45:06.409311 139708407461632 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.8307434320449829, loss=4.234140872955322
I0308 05:45:50.981378 139708415854336 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.9200742244720459, loss=4.539524555206299
I0308 05:46:35.498526 139708407461632 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.8809751272201538, loss=4.395073413848877
I0308 05:46:36.075830 139902746892096 spec.py:321] Evaluating on the training split.
I0308 05:46:47.956389 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 05:46:56.052695 139902746892096 spec.py:349] Evaluating on the test split.
I0308 05:46:57.739651 139902746892096 submission_runner.py:411] Time since start: 2740.77s, 	Step: 5703, 	{'train/accuracy': 0.3254687488079071, 'train/loss': 3.208423376083374, 'validation/accuracy': 0.2970999777317047, 'validation/loss': 3.3675014972686768, 'validation/num_examples': 50000, 'test/accuracy': 0.23010000586509705, 'test/loss': 3.9030966758728027, 'test/num_examples': 10000, 'score': 2565.7739176750183, 'total_duration': 2740.7684206962585, 'accumulated_submission_time': 2565.7739176750183, 'accumulated_eval_time': 174.51342248916626, 'accumulated_logging_time': 0.17061710357666016}
I0308 05:46:57.757233 139708415854336 logging_writer.py:48] [5703] accumulated_eval_time=174.513422, accumulated_logging_time=0.170617, accumulated_submission_time=2565.773918, global_step=5703, preemption_count=0, score=2565.773918, test/accuracy=0.230100, test/loss=3.903097, test/num_examples=10000, total_duration=2740.768421, train/accuracy=0.325469, train/loss=3.208423, validation/accuracy=0.297100, validation/loss=3.367501, validation/num_examples=50000
I0308 05:47:36.449151 139708407461632 logging_writer.py:48] [5800] global_step=5800, grad_norm=1.0887792110443115, loss=4.646831512451172
I0308 05:48:20.569567 139708415854336 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.7888306379318237, loss=4.663852214813232
I0308 05:49:05.157159 139708407461632 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.8944064378738403, loss=5.226290702819824
I0308 05:49:49.785294 139708415854336 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.932733952999115, loss=4.059269905090332
I0308 05:50:34.587870 139708407461632 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.6997201442718506, loss=6.028197288513184
I0308 05:51:19.357277 139708415854336 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.9230855107307434, loss=4.03924036026001
I0308 05:52:03.678567 139708407461632 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.8768429756164551, loss=4.180597305297852
I0308 05:52:48.490957 139708415854336 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.6834492683410645, loss=6.133124351501465
I0308 05:53:33.334585 139708407461632 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.8928042054176331, loss=4.413501739501953
I0308 05:53:57.777700 139902746892096 spec.py:321] Evaluating on the training split.
I0308 05:54:09.867904 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 05:54:18.120174 139902746892096 spec.py:349] Evaluating on the test split.
I0308 05:54:19.809979 139902746892096 submission_runner.py:411] Time since start: 3182.84s, 	Step: 6657, 	{'train/accuracy': 0.37757810950279236, 'train/loss': 2.9110069274902344, 'validation/accuracy': 0.329039990901947, 'validation/loss': 3.1772096157073975, 'validation/num_examples': 50000, 'test/accuracy': 0.2517000138759613, 'test/loss': 3.7387924194335938, 'test/num_examples': 10000, 'score': 2985.7302856445312, 'total_duration': 3182.8387632369995, 'accumulated_submission_time': 2985.7302856445312, 'accumulated_eval_time': 196.54568314552307, 'accumulated_logging_time': 0.2015841007232666}
I0308 05:54:19.831266 139708415854336 logging_writer.py:48] [6657] accumulated_eval_time=196.545683, accumulated_logging_time=0.201584, accumulated_submission_time=2985.730286, global_step=6657, preemption_count=0, score=2985.730286, test/accuracy=0.251700, test/loss=3.738792, test/num_examples=10000, total_duration=3182.838763, train/accuracy=0.377578, train/loss=2.911007, validation/accuracy=0.329040, validation/loss=3.177210, validation/num_examples=50000
I0308 05:54:37.212285 139708407461632 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.8887903094291687, loss=4.0686116218566895
I0308 05:55:18.125115 139708415854336 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.8489484190940857, loss=3.839097261428833
I0308 05:56:02.779485 139708407461632 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.9594964981079102, loss=3.873145818710327
I0308 05:56:47.271095 139708415854336 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.892325758934021, loss=4.318722248077393
I0308 05:57:31.929145 139708407461632 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.6107140183448792, loss=5.537678241729736
I0308 05:58:16.766057 139708415854336 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.7681466341018677, loss=4.236312389373779
I0308 05:59:01.442493 139708407461632 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.8823738098144531, loss=4.911396026611328
I0308 05:59:46.243448 139708415854336 logging_writer.py:48] [7400] global_step=7400, grad_norm=1.005608081817627, loss=3.927762269973755
I0308 06:00:31.095880 139708407461632 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.716719388961792, loss=5.184209823608398
I0308 06:01:15.787071 139708415854336 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.893518328666687, loss=3.9492411613464355
I0308 06:01:19.872621 139902746892096 spec.py:321] Evaluating on the training split.
I0308 06:01:32.150925 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 06:01:40.369679 139902746892096 spec.py:349] Evaluating on the test split.
I0308 06:01:42.057611 139902746892096 submission_runner.py:411] Time since start: 3625.09s, 	Step: 7611, 	{'train/accuracy': 0.38499999046325684, 'train/loss': 2.8280928134918213, 'validation/accuracy': 0.3578599989414215, 'validation/loss': 2.9794843196868896, 'validation/num_examples': 50000, 'test/accuracy': 0.27459999918937683, 'test/loss': 3.569232940673828, 'test/num_examples': 10000, 'score': 3405.709196329117, 'total_duration': 3625.086359500885, 'accumulated_submission_time': 3405.709196329117, 'accumulated_eval_time': 218.73062229156494, 'accumulated_logging_time': 0.23441076278686523}
I0308 06:01:42.083543 139708407461632 logging_writer.py:48] [7611] accumulated_eval_time=218.730622, accumulated_logging_time=0.234411, accumulated_submission_time=3405.709196, global_step=7611, preemption_count=0, score=3405.709196, test/accuracy=0.274600, test/loss=3.569233, test/num_examples=10000, total_duration=3625.086360, train/accuracy=0.385000, train/loss=2.828093, validation/accuracy=0.357860, validation/loss=2.979484, validation/num_examples=50000
I0308 06:02:19.077624 139708415854336 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.7505097985267639, loss=4.1993865966796875
I0308 06:03:03.185131 139708407461632 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.9383473992347717, loss=4.344995021820068
I0308 06:03:50.050348 139708415854336 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.9390153884887695, loss=3.7383806705474854
I0308 06:04:34.706307 139708407461632 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.8748038411140442, loss=3.695923089981079
I0308 06:05:18.981623 139708415854336 logging_writer.py:48] [8100] global_step=8100, grad_norm=1.0022157430648804, loss=3.8782074451446533
I0308 06:06:03.561304 139708407461632 logging_writer.py:48] [8200] global_step=8200, grad_norm=1.0406162738800049, loss=3.6022915840148926
I0308 06:06:48.350986 139708415854336 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.849363386631012, loss=3.7746589183807373
I0308 06:07:33.587059 139708407461632 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.8204595446586609, loss=4.545758247375488
I0308 06:08:18.812587 139708415854336 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.8529036045074463, loss=3.653083562850952
I0308 06:08:42.327791 139902746892096 spec.py:321] Evaluating on the training split.
I0308 06:08:54.579622 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 06:09:05.597769 139902746892096 spec.py:349] Evaluating on the test split.
I0308 06:09:07.280210 139902746892096 submission_runner.py:411] Time since start: 4070.31s, 	Step: 8554, 	{'train/accuracy': 0.41865232586860657, 'train/loss': 2.685232162475586, 'validation/accuracy': 0.38405999541282654, 'validation/loss': 2.860114574432373, 'validation/num_examples': 50000, 'test/accuracy': 0.2945000231266022, 'test/loss': 3.458648204803467, 'test/num_examples': 10000, 'score': 3825.8883900642395, 'total_duration': 4070.3089945316315, 'accumulated_submission_time': 3825.8883900642395, 'accumulated_eval_time': 243.68302154541016, 'accumulated_logging_time': 0.27391767501831055}
I0308 06:09:07.297971 139708407461632 logging_writer.py:48] [8554] accumulated_eval_time=243.683022, accumulated_logging_time=0.273918, accumulated_submission_time=3825.888390, global_step=8554, preemption_count=0, score=3825.888390, test/accuracy=0.294500, test/loss=3.458648, test/num_examples=10000, total_duration=4070.308995, train/accuracy=0.418652, train/loss=2.685232, validation/accuracy=0.384060, validation/loss=2.860115, validation/num_examples=50000
I0308 06:09:26.073240 139708415854336 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.8271018266677856, loss=3.7337217330932617
I0308 06:10:07.858245 139708407461632 logging_writer.py:48] [8700] global_step=8700, grad_norm=1.0454578399658203, loss=3.5917601585388184
I0308 06:10:52.512579 139708415854336 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.9205541014671326, loss=3.554917097091675
I0308 06:11:37.214880 139708407461632 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.9524821043014526, loss=3.788969039916992
I0308 06:12:21.764676 139708415854336 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.881303071975708, loss=3.6217777729034424
I0308 06:13:07.803077 139708407461632 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.8380907773971558, loss=3.695016622543335
I0308 06:13:52.410810 139708415854336 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.7871325016021729, loss=4.075154781341553
I0308 06:14:37.348386 139708407461632 logging_writer.py:48] [9300] global_step=9300, grad_norm=1.0064226388931274, loss=3.6371688842773438
I0308 06:15:22.008377 139708415854336 logging_writer.py:48] [9400] global_step=9400, grad_norm=1.089680790901184, loss=3.4886722564697266
I0308 06:16:07.215482 139708407461632 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.6234606504440308, loss=5.068737983703613
I0308 06:16:07.306191 139902746892096 spec.py:321] Evaluating on the training split.
I0308 06:16:19.645963 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 06:16:34.183218 139902746892096 spec.py:349] Evaluating on the test split.
I0308 06:16:35.861950 139902746892096 submission_runner.py:411] Time since start: 4518.89s, 	Step: 9502, 	{'train/accuracy': 0.44822263717651367, 'train/loss': 2.5037455558776855, 'validation/accuracy': 0.4045199751853943, 'validation/loss': 2.7109215259552, 'validation/num_examples': 50000, 'test/accuracy': 0.30960002541542053, 'test/loss': 3.3151302337646484, 'test/num_examples': 10000, 'score': 4245.836257696152, 'total_duration': 4518.890733718872, 'accumulated_submission_time': 4245.836257696152, 'accumulated_eval_time': 272.23877477645874, 'accumulated_logging_time': 0.3016986846923828}
I0308 06:16:35.883852 139708415854336 logging_writer.py:48] [9502] accumulated_eval_time=272.238775, accumulated_logging_time=0.301699, accumulated_submission_time=4245.836258, global_step=9502, preemption_count=0, score=4245.836258, test/accuracy=0.309600, test/loss=3.315130, test/num_examples=10000, total_duration=4518.890734, train/accuracy=0.448223, train/loss=2.503746, validation/accuracy=0.404520, validation/loss=2.710922, validation/num_examples=50000
I0308 06:17:14.940798 139708407461632 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.8121103644371033, loss=4.164614200592041
I0308 06:17:58.899339 139708415854336 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.850589394569397, loss=4.622722625732422
I0308 06:18:43.327260 139708407461632 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.9701638221740723, loss=3.508631944656372
I0308 06:19:28.014736 139708415854336 logging_writer.py:48] [9900] global_step=9900, grad_norm=1.1321650743484497, loss=3.4797351360321045
I0308 06:20:12.661254 139708407461632 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.7717058658599854, loss=5.201070308685303
I0308 06:20:57.252230 139708415854336 logging_writer.py:48] [10100] global_step=10100, grad_norm=1.125377893447876, loss=3.587555408477783
I0308 06:21:42.146122 139708407461632 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.984130322933197, loss=4.5803656578063965
I0308 06:22:27.040119 139708415854336 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.8221574425697327, loss=5.780340671539307
I0308 06:23:11.893081 139708407461632 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.7828481197357178, loss=4.517143726348877
I0308 06:23:35.910069 139902746892096 spec.py:321] Evaluating on the training split.
I0308 06:23:48.282174 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 06:24:00.382985 139902746892096 spec.py:349] Evaluating on the test split.
I0308 06:24:02.067705 139902746892096 submission_runner.py:411] Time since start: 4965.10s, 	Step: 10455, 	{'train/accuracy': 0.4553320109844208, 'train/loss': 2.4251720905303955, 'validation/accuracy': 0.43039998412132263, 'validation/loss': 2.5701441764831543, 'validation/num_examples': 50000, 'test/accuracy': 0.32850000262260437, 'test/loss': 3.205925226211548, 'test/num_examples': 10000, 'score': 4665.796049118042, 'total_duration': 4965.096488714218, 'accumulated_submission_time': 4665.796049118042, 'accumulated_eval_time': 298.39640188217163, 'accumulated_logging_time': 0.3389160633087158}
I0308 06:24:02.087069 139708415854336 logging_writer.py:48] [10455] accumulated_eval_time=298.396402, accumulated_logging_time=0.338916, accumulated_submission_time=4665.796049, global_step=10455, preemption_count=0, score=4665.796049, test/accuracy=0.328500, test/loss=3.205925, test/num_examples=10000, total_duration=4965.096489, train/accuracy=0.455332, train/loss=2.425172, validation/accuracy=0.430400, validation/loss=2.570144, validation/num_examples=50000
I0308 06:24:20.244225 139708407461632 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.9677510857582092, loss=3.351682662963867
I0308 06:25:01.764708 139708415854336 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.9503567218780518, loss=3.2927608489990234
I0308 06:25:46.262629 139708407461632 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.8752926588058472, loss=3.808448314666748
I0308 06:26:31.045441 139708415854336 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.955477237701416, loss=4.761781215667725
I0308 06:27:15.485754 139708407461632 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.8116329908370972, loss=5.815688133239746
I0308 06:28:00.340100 139708415854336 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.8622728586196899, loss=5.384802341461182
I0308 06:28:44.999500 139708407461632 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.7228991985321045, loss=5.601840496063232
I0308 06:29:29.743118 139708415854336 logging_writer.py:48] [11200] global_step=11200, grad_norm=1.0208004713058472, loss=3.370495319366455
I0308 06:30:15.021267 139708407461632 logging_writer.py:48] [11300] global_step=11300, grad_norm=1.0402767658233643, loss=3.3261232376098633
I0308 06:30:59.854797 139708415854336 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.9386584758758545, loss=3.4590184688568115
I0308 06:31:02.134522 139902746892096 spec.py:321] Evaluating on the training split.
I0308 06:31:14.541604 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 06:31:25.434065 139902746892096 spec.py:349] Evaluating on the test split.
I0308 06:31:27.122900 139902746892096 submission_runner.py:411] Time since start: 5410.15s, 	Step: 11407, 	{'train/accuracy': 0.4743359386920929, 'train/loss': 2.3213415145874023, 'validation/accuracy': 0.4418199956417084, 'validation/loss': 2.4998693466186523, 'validation/num_examples': 50000, 'test/accuracy': 0.3394000232219696, 'test/loss': 3.1411850452423096, 'test/num_examples': 10000, 'score': 5085.781652927399, 'total_duration': 5410.151679992676, 'accumulated_submission_time': 5085.781652927399, 'accumulated_eval_time': 323.38474798202515, 'accumulated_logging_time': 0.36933016777038574}
I0308 06:31:27.141407 139708407461632 logging_writer.py:48] [11407] accumulated_eval_time=323.384748, accumulated_logging_time=0.369330, accumulated_submission_time=5085.781653, global_step=11407, preemption_count=0, score=5085.781653, test/accuracy=0.339400, test/loss=3.141185, test/num_examples=10000, total_duration=5410.151680, train/accuracy=0.474336, train/loss=2.321342, validation/accuracy=0.441820, validation/loss=2.499869, validation/num_examples=50000
I0308 06:32:04.154800 139708415854336 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.9901995658874512, loss=3.2860264778137207
I0308 06:32:48.556203 139708407461632 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.8564633727073669, loss=4.388264179229736
I0308 06:33:33.201987 139708415854336 logging_writer.py:48] [11700] global_step=11700, grad_norm=1.157469391822815, loss=3.2647180557250977
I0308 06:34:17.943288 139708407461632 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.8127090334892273, loss=5.684340000152588
I0308 06:35:02.438623 139708415854336 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.8409996628761292, loss=4.18455171585083
I0308 06:35:46.826499 139708407461632 logging_writer.py:48] [12000] global_step=12000, grad_norm=1.0023678541183472, loss=3.2677230834960938
I0308 06:36:31.677929 139708415854336 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.9073351621627808, loss=5.717979431152344
I0308 06:37:16.880035 139708407461632 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.7574542760848999, loss=4.785717010498047
I0308 06:38:01.968688 139708415854336 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.954140305519104, loss=3.270841360092163
I0308 06:38:27.650545 139902746892096 spec.py:321] Evaluating on the training split.
I0308 06:38:39.866522 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 06:38:54.007977 139902746892096 spec.py:349] Evaluating on the test split.
I0308 06:38:55.692114 139902746892096 submission_runner.py:411] Time since start: 5858.72s, 	Step: 12359, 	{'train/accuracy': 0.4992382824420929, 'train/loss': 2.207763910293579, 'validation/accuracy': 0.4544000029563904, 'validation/loss': 2.4317727088928223, 'validation/num_examples': 50000, 'test/accuracy': 0.35200002789497375, 'test/loss': 3.048286199569702, 'test/num_examples': 10000, 'score': 5506.2299082279205, 'total_duration': 5858.720902204514, 'accumulated_submission_time': 5506.2299082279205, 'accumulated_eval_time': 351.4263126850128, 'accumulated_logging_time': 0.3970980644226074}
I0308 06:38:55.714606 139708407461632 logging_writer.py:48] [12359] accumulated_eval_time=351.426313, accumulated_logging_time=0.397098, accumulated_submission_time=5506.229908, global_step=12359, preemption_count=0, score=5506.229908, test/accuracy=0.352000, test/loss=3.048286, test/num_examples=10000, total_duration=5858.720902, train/accuracy=0.499238, train/loss=2.207764, validation/accuracy=0.454400, validation/loss=2.431773, validation/num_examples=50000
I0308 06:39:12.265753 139708415854336 logging_writer.py:48] [12400] global_step=12400, grad_norm=1.1811870336532593, loss=3.38702130317688
I0308 06:39:54.227609 139708407461632 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.9360304474830627, loss=5.237989902496338
I0308 06:40:38.924983 139708415854336 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.922407865524292, loss=3.5032670497894287
I0308 06:41:23.475777 139708407461632 logging_writer.py:48] [12700] global_step=12700, grad_norm=1.0948619842529297, loss=3.1642909049987793
I0308 06:42:07.964825 139708415854336 logging_writer.py:48] [12800] global_step=12800, grad_norm=1.0414716005325317, loss=3.3608319759368896
I0308 06:42:52.898603 139708407461632 logging_writer.py:48] [12900] global_step=12900, grad_norm=1.0103262662887573, loss=3.3317666053771973
I0308 06:43:38.245749 139708415854336 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.9623115062713623, loss=5.130971908569336
I0308 06:44:23.256930 139708407461632 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.9690180420875549, loss=3.517634868621826
I0308 06:45:07.966876 139708415854336 logging_writer.py:48] [13200] global_step=13200, grad_norm=1.0720717906951904, loss=3.0507748126983643
I0308 06:45:52.846123 139708407461632 logging_writer.py:48] [13300] global_step=13300, grad_norm=1.2770370244979858, loss=3.0713183879852295
I0308 06:45:55.733108 139902746892096 spec.py:321] Evaluating on the training split.
I0308 06:46:07.788338 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 06:46:22.677168 139902746892096 spec.py:349] Evaluating on the test split.
I0308 06:46:24.379147 139902746892096 submission_runner.py:411] Time since start: 6307.41s, 	Step: 13308, 	{'train/accuracy': 0.5203320384025574, 'train/loss': 2.1063053607940674, 'validation/accuracy': 0.4576999843120575, 'validation/loss': 2.4315109252929688, 'validation/num_examples': 50000, 'test/accuracy': 0.3541000187397003, 'test/loss': 3.0787501335144043, 'test/num_examples': 10000, 'score': 5926.184532165527, 'total_duration': 6307.407933473587, 'accumulated_submission_time': 5926.184532165527, 'accumulated_eval_time': 380.0723383426666, 'accumulated_logging_time': 0.4321315288543701}
I0308 06:46:24.402388 139708415854336 logging_writer.py:48] [13308] accumulated_eval_time=380.072338, accumulated_logging_time=0.432132, accumulated_submission_time=5926.184532, global_step=13308, preemption_count=0, score=5926.184532, test/accuracy=0.354100, test/loss=3.078750, test/num_examples=10000, total_duration=6307.407933, train/accuracy=0.520332, train/loss=2.106305, validation/accuracy=0.457700, validation/loss=2.431511, validation/num_examples=50000
I0308 06:47:01.116240 139708407461632 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.9527361989021301, loss=3.2455546855926514
I0308 06:47:46.100207 139708415854336 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.7650936841964722, loss=5.662360191345215
I0308 06:48:30.628498 139708407461632 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.9363771677017212, loss=3.9869747161865234
I0308 06:49:15.022750 139708415854336 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.9787187576293945, loss=3.1947460174560547
I0308 06:49:59.992784 139708407461632 logging_writer.py:48] [13800] global_step=13800, grad_norm=1.061736822128296, loss=3.1435885429382324
I0308 06:50:44.642916 139708415854336 logging_writer.py:48] [13900] global_step=13900, grad_norm=1.0460776090621948, loss=3.0475401878356934
I0308 06:51:30.059876 139708407461632 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.7832553386688232, loss=5.349855899810791
I0308 06:52:15.148524 139708415854336 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.7903652787208557, loss=4.6651291847229
I0308 06:53:00.036293 139708407461632 logging_writer.py:48] [14200] global_step=14200, grad_norm=1.0942931175231934, loss=3.3321330547332764
I0308 06:53:24.515944 139902746892096 spec.py:321] Evaluating on the training split.
I0308 06:53:36.595382 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 06:53:50.100501 139902746892096 spec.py:349] Evaluating on the test split.
I0308 06:53:51.793936 139902746892096 submission_runner.py:411] Time since start: 6754.82s, 	Step: 14256, 	{'train/accuracy': 0.512402355670929, 'train/loss': 2.137631893157959, 'validation/accuracy': 0.47807997465133667, 'validation/loss': 2.320540189743042, 'validation/num_examples': 50000, 'test/accuracy': 0.3675000071525574, 'test/loss': 2.9688971042633057, 'test/num_examples': 10000, 'score': 6346.236738204956, 'total_duration': 6754.822709321976, 'accumulated_submission_time': 6346.236738204956, 'accumulated_eval_time': 407.3503110408783, 'accumulated_logging_time': 0.4661853313446045}
I0308 06:53:51.813679 139708415854336 logging_writer.py:48] [14256] accumulated_eval_time=407.350311, accumulated_logging_time=0.466185, accumulated_submission_time=6346.236738, global_step=14256, preemption_count=0, score=6346.236738, test/accuracy=0.367500, test/loss=2.968897, test/num_examples=10000, total_duration=6754.822709, train/accuracy=0.512402, train/loss=2.137632, validation/accuracy=0.478080, validation/loss=2.320540, validation/num_examples=50000
I0308 06:54:09.543900 139708407461632 logging_writer.py:48] [14300] global_step=14300, grad_norm=1.192652702331543, loss=3.171837329864502
I0308 06:54:51.052462 139708415854336 logging_writer.py:48] [14400] global_step=14400, grad_norm=1.0587542057037354, loss=3.120791435241699
I0308 06:55:35.831917 139708407461632 logging_writer.py:48] [14500] global_step=14500, grad_norm=1.0595595836639404, loss=3.2466609477996826
I0308 06:56:20.168880 139708415854336 logging_writer.py:48] [14600] global_step=14600, grad_norm=1.0852032899856567, loss=3.1526870727539062
I0308 06:57:04.796982 139708407461632 logging_writer.py:48] [14700] global_step=14700, grad_norm=1.0408259630203247, loss=4.369129657745361
I0308 06:57:50.071561 139708415854336 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.8053280711174011, loss=5.526827812194824
I0308 06:58:35.073848 139708407461632 logging_writer.py:48] [14900] global_step=14900, grad_norm=1.008172631263733, loss=3.05633807182312
I0308 06:59:20.579240 139708415854336 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.7685831189155579, loss=5.180373191833496
I0308 07:00:05.647188 139708407461632 logging_writer.py:48] [15100] global_step=15100, grad_norm=1.1010159254074097, loss=3.032496452331543
I0308 07:00:50.978437 139708415854336 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.9630394577980042, loss=4.163135051727295
I0308 07:00:51.851044 139902746892096 spec.py:321] Evaluating on the training split.
I0308 07:01:04.017755 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 07:01:16.701685 139902746892096 spec.py:349] Evaluating on the test split.
I0308 07:01:18.385818 139902746892096 submission_runner.py:411] Time since start: 7201.41s, 	Step: 15204, 	{'train/accuracy': 0.5299218893051147, 'train/loss': 2.066849708557129, 'validation/accuracy': 0.4887799918651581, 'validation/loss': 2.277812957763672, 'validation/num_examples': 50000, 'test/accuracy': 0.3815000057220459, 'test/loss': 2.909775972366333, 'test/num_examples': 10000, 'score': 6766.212954044342, 'total_duration': 7201.414595603943, 'accumulated_submission_time': 6766.212954044342, 'accumulated_eval_time': 433.88506269454956, 'accumulated_logging_time': 0.49620556831359863}
I0308 07:01:18.410770 139708407461632 logging_writer.py:48] [15204] accumulated_eval_time=433.885063, accumulated_logging_time=0.496206, accumulated_submission_time=6766.212954, global_step=15204, preemption_count=0, score=6766.212954, test/accuracy=0.381500, test/loss=2.909776, test/num_examples=10000, total_duration=7201.414596, train/accuracy=0.529922, train/loss=2.066850, validation/accuracy=0.488780, validation/loss=2.277813, validation/num_examples=50000
I0308 07:01:56.626926 139708415854336 logging_writer.py:48] [15300] global_step=15300, grad_norm=1.0849740505218506, loss=3.038667678833008
I0308 07:02:41.835957 139708407461632 logging_writer.py:48] [15400] global_step=15400, grad_norm=1.2538843154907227, loss=3.0330915451049805
I0308 07:03:27.373974 139708415854336 logging_writer.py:48] [15500] global_step=15500, grad_norm=1.2952076196670532, loss=3.0996925830841064
I0308 07:04:12.606083 139708407461632 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.8454304337501526, loss=5.571461200714111
I0308 07:04:58.019570 139708415854336 logging_writer.py:48] [15700] global_step=15700, grad_norm=1.1316287517547607, loss=3.0174131393432617
I0308 07:05:43.758134 139708407461632 logging_writer.py:48] [15800] global_step=15800, grad_norm=1.0846943855285645, loss=3.1015572547912598
I0308 07:06:28.836380 139708415854336 logging_writer.py:48] [15900] global_step=15900, grad_norm=1.0724283456802368, loss=3.0411057472229004
I0308 07:07:13.873892 139708407461632 logging_writer.py:48] [16000] global_step=16000, grad_norm=1.001943588256836, loss=3.0180301666259766
I0308 07:07:59.222037 139708415854336 logging_writer.py:48] [16100] global_step=16100, grad_norm=1.3307108879089355, loss=3.16823148727417
I0308 07:08:18.639930 139902746892096 spec.py:321] Evaluating on the training split.
I0308 07:08:31.528502 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 07:08:45.327604 139902746892096 spec.py:349] Evaluating on the test split.
I0308 07:08:47.055748 139902746892096 submission_runner.py:411] Time since start: 7650.08s, 	Step: 16145, 	{'train/accuracy': 0.5477734208106995, 'train/loss': 1.9316043853759766, 'validation/accuracy': 0.5002399682998657, 'validation/loss': 2.178302049636841, 'validation/num_examples': 50000, 'test/accuracy': 0.3874000310897827, 'test/loss': 2.8302128314971924, 'test/num_examples': 10000, 'score': 7186.382284641266, 'total_duration': 7650.084501981735, 'accumulated_submission_time': 7186.382284641266, 'accumulated_eval_time': 462.300833940506, 'accumulated_logging_time': 0.5308792591094971}
I0308 07:08:47.092124 139708407461632 logging_writer.py:48] [16145] accumulated_eval_time=462.300834, accumulated_logging_time=0.530879, accumulated_submission_time=7186.382285, global_step=16145, preemption_count=0, score=7186.382285, test/accuracy=0.387400, test/loss=2.830213, test/num_examples=10000, total_duration=7650.084502, train/accuracy=0.547773, train/loss=1.931604, validation/accuracy=0.500240, validation/loss=2.178302, validation/num_examples=50000
I0308 07:09:09.207652 139708415854336 logging_writer.py:48] [16200] global_step=16200, grad_norm=1.224745512008667, loss=3.0345962047576904
I0308 07:09:51.399598 139708407461632 logging_writer.py:48] [16300] global_step=16300, grad_norm=1.0005415678024292, loss=5.235743522644043
I0308 07:10:36.673636 139708415854336 logging_writer.py:48] [16400] global_step=16400, grad_norm=1.2919566631317139, loss=2.91263484954834
I0308 07:11:22.056728 139708407461632 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.7848886847496033, loss=5.317059516906738
I0308 07:12:07.206897 139708415854336 logging_writer.py:48] [16600] global_step=16600, grad_norm=1.1668373346328735, loss=3.01959228515625
I0308 07:12:52.242260 139708407461632 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.9495247602462769, loss=5.637695789337158
I0308 07:13:37.470771 139708415854336 logging_writer.py:48] [16800] global_step=16800, grad_norm=1.2520833015441895, loss=2.8977339267730713
I0308 07:14:22.463403 139708407461632 logging_writer.py:48] [16900] global_step=16900, grad_norm=1.3040523529052734, loss=3.0818932056427
I0308 07:15:07.598282 139708415854336 logging_writer.py:48] [17000] global_step=17000, grad_norm=1.1730583906173706, loss=2.8582780361175537
I0308 07:15:47.462103 139902746892096 spec.py:321] Evaluating on the training split.
I0308 07:16:00.898492 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 07:16:19.624557 139902746892096 spec.py:349] Evaluating on the test split.
I0308 07:16:21.305140 139902746892096 submission_runner.py:411] Time since start: 8104.33s, 	Step: 17090, 	{'train/accuracy': 0.5459765791893005, 'train/loss': 1.9445308446884155, 'validation/accuracy': 0.5103999972343445, 'validation/loss': 2.1310389041900635, 'validation/num_examples': 50000, 'test/accuracy': 0.39240002632141113, 'test/loss': 2.787764549255371, 'test/num_examples': 10000, 'score': 7606.686836242676, 'total_duration': 8104.333922147751, 'accumulated_submission_time': 7606.686836242676, 'accumulated_eval_time': 496.14386224746704, 'accumulated_logging_time': 0.582331657409668}
I0308 07:16:21.325372 139708407461632 logging_writer.py:48] [17090] accumulated_eval_time=496.143862, accumulated_logging_time=0.582332, accumulated_submission_time=7606.686836, global_step=17090, preemption_count=0, score=7606.686836, test/accuracy=0.392400, test/loss=2.787765, test/num_examples=10000, total_duration=8104.333922, train/accuracy=0.545977, train/loss=1.944531, validation/accuracy=0.510400, validation/loss=2.131039, validation/num_examples=50000
I0308 07:16:25.684846 139708415854336 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.8767629861831665, loss=5.356686115264893
I0308 07:17:05.186976 139708407461632 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.9092040657997131, loss=5.255241394042969
I0308 07:17:49.198820 139708415854336 logging_writer.py:48] [17300] global_step=17300, grad_norm=1.0265157222747803, loss=3.714170217514038
I0308 07:18:33.808805 139708407461632 logging_writer.py:48] [17400] global_step=17400, grad_norm=1.0526942014694214, loss=3.114980697631836
I0308 07:19:19.194893 139708415854336 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.7755594849586487, loss=5.165757179260254
I0308 07:20:04.507849 139708407461632 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.9458799362182617, loss=4.784336566925049
I0308 07:20:49.415139 139708415854336 logging_writer.py:48] [17700] global_step=17700, grad_norm=1.0036612749099731, loss=2.9267308712005615
I0308 07:21:34.272968 139708407461632 logging_writer.py:48] [17800] global_step=17800, grad_norm=1.093825340270996, loss=2.8391778469085693
I0308 07:22:19.299957 139708415854336 logging_writer.py:48] [17900] global_step=17900, grad_norm=1.0572166442871094, loss=2.8963329792022705
I0308 07:23:04.143802 139708407461632 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.9278469085693359, loss=4.854690074920654
I0308 07:23:21.434141 139902746892096 spec.py:321] Evaluating on the training split.
I0308 07:23:35.762523 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 07:23:55.684051 139902746892096 spec.py:349] Evaluating on the test split.
I0308 07:23:57.368142 139902746892096 submission_runner.py:411] Time since start: 8560.40s, 	Step: 18040, 	{'train/accuracy': 0.5554491877555847, 'train/loss': 1.92086660861969, 'validation/accuracy': 0.5125399827957153, 'validation/loss': 2.1145436763763428, 'validation/num_examples': 50000, 'test/accuracy': 0.4018000066280365, 'test/loss': 2.7795822620391846, 'test/num_examples': 10000, 'score': 8026.735034227371, 'total_duration': 8560.396936416626, 'accumulated_submission_time': 8026.735034227371, 'accumulated_eval_time': 532.077849149704, 'accumulated_logging_time': 0.6126272678375244}
I0308 07:23:57.385966 139708415854336 logging_writer.py:48] [18040] accumulated_eval_time=532.077849, accumulated_logging_time=0.612627, accumulated_submission_time=8026.735034, global_step=18040, preemption_count=0, score=8026.735034, test/accuracy=0.401800, test/loss=2.779582, test/num_examples=10000, total_duration=8560.396936, train/accuracy=0.555449, train/loss=1.920867, validation/accuracy=0.512540, validation/loss=2.114544, validation/num_examples=50000
I0308 07:24:21.387867 139708407461632 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.7541659474372864, loss=5.574151992797852
I0308 07:25:03.793892 139708415854336 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.9608985781669617, loss=3.7229599952697754
I0308 07:25:48.678009 139708407461632 logging_writer.py:48] [18300] global_step=18300, grad_norm=1.3154349327087402, loss=2.9051523208618164
I0308 07:26:33.382754 139708415854336 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.8850591778755188, loss=3.77131986618042
I0308 07:27:18.465818 139708407461632 logging_writer.py:48] [18500] global_step=18500, grad_norm=1.0292493104934692, loss=2.8713555335998535
I0308 07:28:03.761312 139708415854336 logging_writer.py:48] [18600] global_step=18600, grad_norm=1.0257869958877563, loss=3.1907260417938232
I0308 07:28:48.906215 139708407461632 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.9171682000160217, loss=5.285833835601807
I0308 07:29:33.910274 139708415854336 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.839775800704956, loss=4.054333686828613
I0308 07:30:18.640594 139708407461632 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.8859443068504333, loss=4.4330034255981445
I0308 07:30:57.637435 139902746892096 spec.py:321] Evaluating on the training split.
I0308 07:31:11.960312 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 07:31:32.957228 139902746892096 spec.py:349] Evaluating on the test split.
I0308 07:31:34.644866 139902746892096 submission_runner.py:411] Time since start: 9017.67s, 	Step: 18989, 	{'train/accuracy': 0.5669531226158142, 'train/loss': 1.8463548421859741, 'validation/accuracy': 0.5207399725914001, 'validation/loss': 2.081745147705078, 'validation/num_examples': 50000, 'test/accuracy': 0.4084000289440155, 'test/loss': 2.721212148666382, 'test/num_examples': 10000, 'score': 8446.924666404724, 'total_duration': 9017.6736536026, 'accumulated_submission_time': 8446.924666404724, 'accumulated_eval_time': 569.0852816104889, 'accumulated_logging_time': 0.6402697563171387}
I0308 07:31:34.663485 139708415854336 logging_writer.py:48] [18989] accumulated_eval_time=569.085282, accumulated_logging_time=0.640270, accumulated_submission_time=8446.924666, global_step=18989, preemption_count=0, score=8446.924666, test/accuracy=0.408400, test/loss=2.721212, test/num_examples=10000, total_duration=9017.673654, train/accuracy=0.566953, train/loss=1.846355, validation/accuracy=0.520740, validation/loss=2.081745, validation/num_examples=50000
I0308 07:31:39.386228 139708407461632 logging_writer.py:48] [19000] global_step=19000, grad_norm=1.065049409866333, loss=2.8297204971313477
I0308 07:32:19.480802 139708415854336 logging_writer.py:48] [19100] global_step=19100, grad_norm=1.1277410984039307, loss=2.8869683742523193
I0308 07:33:04.514884 139708407461632 logging_writer.py:48] [19200] global_step=19200, grad_norm=1.148530125617981, loss=2.808706521987915
I0308 07:33:49.513588 139708415854336 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.8450148701667786, loss=5.347900867462158
I0308 07:34:34.487434 139708407461632 logging_writer.py:48] [19400] global_step=19400, grad_norm=1.0917932987213135, loss=2.843092441558838
I0308 07:35:19.386303 139708415854336 logging_writer.py:48] [19500] global_step=19500, grad_norm=1.0314656496047974, loss=2.8673009872436523
I0308 07:36:04.484608 139708407461632 logging_writer.py:48] [19600] global_step=19600, grad_norm=1.0196452140808105, loss=3.4318442344665527
I0308 07:36:49.176686 139708415854336 logging_writer.py:48] [19700] global_step=19700, grad_norm=1.0943323373794556, loss=3.561941385269165
I0308 07:37:34.053646 139708407461632 logging_writer.py:48] [19800] global_step=19800, grad_norm=1.0979268550872803, loss=2.9641292095184326
I0308 07:38:18.751281 139708415854336 logging_writer.py:48] [19900] global_step=19900, grad_norm=1.295931100845337, loss=2.8223226070404053
I0308 07:38:35.090543 139902746892096 spec.py:321] Evaluating on the training split.
I0308 07:38:49.280695 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 07:39:09.936741 139902746892096 spec.py:349] Evaluating on the test split.
I0308 07:39:11.611160 139902746892096 submission_runner.py:411] Time since start: 9474.64s, 	Step: 19938, 	{'train/accuracy': 0.5889257788658142, 'train/loss': 1.7574844360351562, 'validation/accuracy': 0.5252599716186523, 'validation/loss': 2.0797624588012695, 'validation/num_examples': 50000, 'test/accuracy': 0.40870001912117004, 'test/loss': 2.7397501468658447, 'test/num_examples': 10000, 'score': 8867.289863586426, 'total_duration': 9474.639935016632, 'accumulated_submission_time': 8867.289863586426, 'accumulated_eval_time': 605.6058855056763, 'accumulated_logging_time': 0.6697046756744385}
I0308 07:39:11.636833 139708407461632 logging_writer.py:48] [19938] accumulated_eval_time=605.605886, accumulated_logging_time=0.669705, accumulated_submission_time=8867.289864, global_step=19938, preemption_count=0, score=8867.289864, test/accuracy=0.408700, test/loss=2.739750, test/num_examples=10000, total_duration=9474.639935, train/accuracy=0.588926, train/loss=1.757484, validation/accuracy=0.525260, validation/loss=2.079762, validation/num_examples=50000
I0308 07:39:36.427065 139708415854336 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.9707947969436646, loss=5.392484664916992
I0308 07:40:19.647036 139708407461632 logging_writer.py:48] [20100] global_step=20100, grad_norm=1.0591521263122559, loss=4.280813694000244
I0308 07:41:04.324945 139708415854336 logging_writer.py:48] [20200] global_step=20200, grad_norm=1.1067180633544922, loss=2.7921833992004395
I0308 07:41:49.320358 139708407461632 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.869800865650177, loss=3.9980416297912598
I0308 07:42:34.232403 139708415854336 logging_writer.py:48] [20400] global_step=20400, grad_norm=1.1403263807296753, loss=2.8796403408050537
I0308 07:43:19.051208 139708407461632 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.8588523268699646, loss=4.040274620056152
I0308 07:44:03.902034 139708415854336 logging_writer.py:48] [20600] global_step=20600, grad_norm=1.0344343185424805, loss=2.889197826385498
I0308 07:44:48.841423 139708407461632 logging_writer.py:48] [20700] global_step=20700, grad_norm=1.114280343055725, loss=2.855992555618286
I0308 07:45:33.966006 139708415854336 logging_writer.py:48] [20800] global_step=20800, grad_norm=1.0065116882324219, loss=2.7773971557617188
I0308 07:46:11.818430 139902746892096 spec.py:321] Evaluating on the training split.
I0308 07:46:26.469584 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 07:46:49.421939 139902746892096 spec.py:349] Evaluating on the test split.
I0308 07:46:51.095692 139902746892096 submission_runner.py:411] Time since start: 9934.12s, 	Step: 20886, 	{'train/accuracy': 0.5757421851158142, 'train/loss': 1.8028565645217896, 'validation/accuracy': 0.5389999747276306, 'validation/loss': 1.9935595989227295, 'validation/num_examples': 50000, 'test/accuracy': 0.42340001463890076, 'test/loss': 2.6601674556732178, 'test/num_examples': 10000, 'score': 9287.409376859665, 'total_duration': 9934.12448978424, 'accumulated_submission_time': 9287.409376859665, 'accumulated_eval_time': 644.8831579685211, 'accumulated_logging_time': 0.7056596279144287}
I0308 07:46:51.118039 139708407461632 logging_writer.py:48] [20886] accumulated_eval_time=644.883158, accumulated_logging_time=0.705660, accumulated_submission_time=9287.409377, global_step=20886, preemption_count=0, score=9287.409377, test/accuracy=0.423400, test/loss=2.660167, test/num_examples=10000, total_duration=9934.124490, train/accuracy=0.575742, train/loss=1.802857, validation/accuracy=0.539000, validation/loss=1.993560, validation/num_examples=50000
I0308 07:46:57.019329 139708415854336 logging_writer.py:48] [20900] global_step=20900, grad_norm=1.176079511642456, loss=2.850979804992676
I0308 07:47:37.899115 139708407461632 logging_writer.py:48] [21000] global_step=21000, grad_norm=1.1743437051773071, loss=3.3699426651000977
I0308 07:48:22.557215 139708415854336 logging_writer.py:48] [21100] global_step=21100, grad_norm=1.06179678440094, loss=3.1109066009521484
I0308 07:49:07.514655 139708407461632 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.9812314510345459, loss=4.096296310424805
I0308 07:49:52.761208 139708415854336 logging_writer.py:48] [21300] global_step=21300, grad_norm=1.089207410812378, loss=2.728719472885132
I0308 07:50:37.910876 139708407461632 logging_writer.py:48] [21400] global_step=21400, grad_norm=1.1931978464126587, loss=2.847198486328125
I0308 07:51:23.374029 139708415854336 logging_writer.py:48] [21500] global_step=21500, grad_norm=1.1110479831695557, loss=2.8697032928466797
I0308 07:52:08.367085 139708407461632 logging_writer.py:48] [21600] global_step=21600, grad_norm=1.0190048217773438, loss=3.1304519176483154
I0308 07:52:53.435846 139708415854336 logging_writer.py:48] [21700] global_step=21700, grad_norm=1.0403032302856445, loss=5.089531898498535
I0308 07:53:38.692719 139708407461632 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.8959571719169617, loss=4.531203269958496
I0308 07:53:51.529700 139902746892096 spec.py:321] Evaluating on the training split.
I0308 07:54:06.368470 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 07:54:24.070846 139902746892096 spec.py:349] Evaluating on the test split.
I0308 07:54:25.760890 139902746892096 submission_runner.py:411] Time since start: 10388.79s, 	Step: 21830, 	{'train/accuracy': 0.5738281011581421, 'train/loss': 1.8270362615585327, 'validation/accuracy': 0.5313599705696106, 'validation/loss': 2.036630630493164, 'validation/num_examples': 50000, 'test/accuracy': 0.415800005197525, 'test/loss': 2.6884138584136963, 'test/num_examples': 10000, 'score': 9707.7606818676, 'total_duration': 10388.789666175842, 'accumulated_submission_time': 9707.7606818676, 'accumulated_eval_time': 679.1143245697021, 'accumulated_logging_time': 0.7382750511169434}
I0308 07:54:25.781989 139708415854336 logging_writer.py:48] [21830] accumulated_eval_time=679.114325, accumulated_logging_time=0.738275, accumulated_submission_time=9707.760682, global_step=21830, preemption_count=0, score=9707.760682, test/accuracy=0.415800, test/loss=2.688414, test/num_examples=10000, total_duration=10388.789666, train/accuracy=0.573828, train/loss=1.827036, validation/accuracy=0.531360, validation/loss=2.036631, validation/num_examples=50000
I0308 07:54:54.245587 139708407461632 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.9901972413063049, loss=3.684056282043457
I0308 07:55:39.542139 139708415854336 logging_writer.py:48] [22000] global_step=22000, grad_norm=1.0046294927597046, loss=5.19704008102417
I0308 07:56:24.662943 139708407461632 logging_writer.py:48] [22100] global_step=22100, grad_norm=1.0246800184249878, loss=2.732254981994629
I0308 07:57:09.757910 139708415854336 logging_writer.py:48] [22200] global_step=22200, grad_norm=1.2640013694763184, loss=2.816258430480957
I0308 07:57:55.304199 139708407461632 logging_writer.py:48] [22300] global_step=22300, grad_norm=1.0757496356964111, loss=2.7689216136932373
I0308 07:58:40.681380 139708415854336 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.9273252487182617, loss=5.031920433044434
I0308 07:59:25.976414 139708407461632 logging_writer.py:48] [22500] global_step=22500, grad_norm=1.1012579202651978, loss=3.1568286418914795
I0308 08:00:11.314542 139708415854336 logging_writer.py:48] [22600] global_step=22600, grad_norm=1.08114755153656, loss=2.8982181549072266
I0308 08:00:56.657166 139708407461632 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.9256362318992615, loss=4.818336009979248
I0308 08:01:25.871039 139902746892096 spec.py:321] Evaluating on the training split.
I0308 08:01:37.885815 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 08:01:58.693737 139902746892096 spec.py:349] Evaluating on the test split.
I0308 08:02:00.373117 139902746892096 submission_runner.py:411] Time since start: 10843.40s, 	Step: 22766, 	{'train/accuracy': 0.5947265625, 'train/loss': 1.7336976528167725, 'validation/accuracy': 0.5411800146102905, 'validation/loss': 1.9943890571594238, 'validation/num_examples': 50000, 'test/accuracy': 0.4213000237941742, 'test/loss': 2.65813946723938, 'test/num_examples': 10000, 'score': 10127.784230947495, 'total_duration': 10843.401911258698, 'accumulated_submission_time': 10127.784230947495, 'accumulated_eval_time': 713.6164371967316, 'accumulated_logging_time': 0.7727911472320557}
I0308 08:02:00.394386 139708415854336 logging_writer.py:48] [22766] accumulated_eval_time=713.616437, accumulated_logging_time=0.772791, accumulated_submission_time=10127.784231, global_step=22766, preemption_count=0, score=10127.784231, test/accuracy=0.421300, test/loss=2.658139, test/num_examples=10000, total_duration=10843.401911, train/accuracy=0.594727, train/loss=1.733698, validation/accuracy=0.541180, validation/loss=1.994389, validation/num_examples=50000
I0308 08:02:14.167329 139708407461632 logging_writer.py:48] [22800] global_step=22800, grad_norm=1.1654433012008667, loss=2.787747621536255
I0308 08:02:55.634415 139708415854336 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.8717544674873352, loss=4.041812896728516
I0308 08:03:40.732135 139708407461632 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.9926747679710388, loss=4.21909236907959
I0308 08:04:26.135713 139708415854336 logging_writer.py:48] [23100] global_step=23100, grad_norm=1.1639187335968018, loss=2.89928936958313
I0308 08:05:11.223336 139708407461632 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.9577196836471558, loss=3.6080596446990967
I0308 08:05:56.110952 139708415854336 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.8763426542282104, loss=5.368967533111572
I0308 08:06:40.867559 139708407461632 logging_writer.py:48] [23400] global_step=23400, grad_norm=1.0905370712280273, loss=2.9179534912109375
I0308 08:07:25.448474 139708415854336 logging_writer.py:48] [23500] global_step=23500, grad_norm=1.1942970752716064, loss=2.6899943351745605
I0308 08:08:10.658674 139708407461632 logging_writer.py:48] [23600] global_step=23600, grad_norm=1.048129677772522, loss=2.724851369857788
I0308 08:08:55.461689 139708415854336 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.9348759651184082, loss=3.2340004444122314
I0308 08:09:00.651324 139902746892096 spec.py:321] Evaluating on the training split.
I0308 08:09:11.822123 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 08:09:33.498304 139902746892096 spec.py:349] Evaluating on the test split.
I0308 08:09:35.165591 139902746892096 submission_runner.py:411] Time since start: 11298.19s, 	Step: 23713, 	{'train/accuracy': 0.5895312428474426, 'train/loss': 1.7517651319503784, 'validation/accuracy': 0.550059974193573, 'validation/loss': 1.9483095407485962, 'validation/num_examples': 50000, 'test/accuracy': 0.43340003490448, 'test/loss': 2.6104726791381836, 'test/num_examples': 10000, 'score': 10547.981046676636, 'total_duration': 11298.194388628006, 'accumulated_submission_time': 10547.981046676636, 'accumulated_eval_time': 748.1307110786438, 'accumulated_logging_time': 0.8031463623046875}
I0308 08:09:35.185749 139708407461632 logging_writer.py:48] [23713] accumulated_eval_time=748.130711, accumulated_logging_time=0.803146, accumulated_submission_time=10547.981047, global_step=23713, preemption_count=0, score=10547.981047, test/accuracy=0.433400, test/loss=2.610473, test/num_examples=10000, total_duration=11298.194389, train/accuracy=0.589531, train/loss=1.751765, validation/accuracy=0.550060, validation/loss=1.948310, validation/num_examples=50000
I0308 08:10:09.823149 139708415854336 logging_writer.py:48] [23800] global_step=23800, grad_norm=1.1721234321594238, loss=3.1678338050842285
I0308 08:10:54.413002 139708407461632 logging_writer.py:48] [23900] global_step=23900, grad_norm=1.1269986629486084, loss=2.7293519973754883
I0308 08:11:39.592731 139708415854336 logging_writer.py:48] [24000] global_step=24000, grad_norm=1.1317143440246582, loss=2.920487880706787
I0308 08:12:24.661875 139708407461632 logging_writer.py:48] [24100] global_step=24100, grad_norm=1.0935683250427246, loss=3.2790377140045166
I0308 08:13:09.609315 139708415854336 logging_writer.py:48] [24200] global_step=24200, grad_norm=1.1126883029937744, loss=2.700883388519287
I0308 08:13:54.685909 139708407461632 logging_writer.py:48] [24300] global_step=24300, grad_norm=1.0461335182189941, loss=3.182551145553589
I0308 08:14:39.688571 139708415854336 logging_writer.py:48] [24400] global_step=24400, grad_norm=1.305618405342102, loss=2.7859954833984375
I0308 08:15:24.555402 139708407461632 logging_writer.py:48] [24500] global_step=24500, grad_norm=1.0486788749694824, loss=2.7613418102264404
I0308 08:16:09.530727 139708415854336 logging_writer.py:48] [24600] global_step=24600, grad_norm=1.199681282043457, loss=3.124483108520508
I0308 08:16:35.329208 139902746892096 spec.py:321] Evaluating on the training split.
I0308 08:16:46.777372 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 08:17:08.433146 139902746892096 spec.py:349] Evaluating on the test split.
I0308 08:17:10.108345 139902746892096 submission_runner.py:411] Time since start: 11753.14s, 	Step: 24659, 	{'train/accuracy': 0.5965625047683716, 'train/loss': 1.7040070295333862, 'validation/accuracy': 0.5518199801445007, 'validation/loss': 1.9154651165008545, 'validation/num_examples': 50000, 'test/accuracy': 0.4394000172615051, 'test/loss': 2.570166826248169, 'test/num_examples': 10000, 'score': 10968.064725160599, 'total_duration': 11753.13712143898, 'accumulated_submission_time': 10968.064725160599, 'accumulated_eval_time': 782.9098272323608, 'accumulated_logging_time': 0.8321309089660645}
I0308 08:17:10.129605 139708407461632 logging_writer.py:48] [24659] accumulated_eval_time=782.909827, accumulated_logging_time=0.832131, accumulated_submission_time=10968.064725, global_step=24659, preemption_count=0, score=10968.064725, test/accuracy=0.439400, test/loss=2.570167, test/num_examples=10000, total_duration=11753.137121, train/accuracy=0.596563, train/loss=1.704007, validation/accuracy=0.551820, validation/loss=1.915465, validation/num_examples=50000
I0308 08:17:26.682169 139708415854336 logging_writer.py:48] [24700] global_step=24700, grad_norm=1.1241804361343384, loss=2.8349313735961914
I0308 08:18:09.085281 139708407461632 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.998401939868927, loss=3.0600483417510986
I0308 08:18:54.059588 139708415854336 logging_writer.py:48] [24900] global_step=24900, grad_norm=1.180187463760376, loss=2.739583730697632
I0308 08:19:39.264086 139708407461632 logging_writer.py:48] [25000] global_step=25000, grad_norm=1.183853030204773, loss=2.815669536590576
I0308 08:20:24.625108 139708415854336 logging_writer.py:48] [25100] global_step=25100, grad_norm=1.2109888792037964, loss=2.7216691970825195
I0308 08:21:09.448252 139708407461632 logging_writer.py:48] [25200] global_step=25200, grad_norm=1.0338484048843384, loss=2.8871521949768066
I0308 08:21:54.623074 139708415854336 logging_writer.py:48] [25300] global_step=25300, grad_norm=1.147597074508667, loss=3.857322931289673
I0308 08:22:39.463145 139708407461632 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.9300833344459534, loss=4.65479850769043
I0308 08:23:24.445366 139708415854336 logging_writer.py:48] [25500] global_step=25500, grad_norm=1.1540281772613525, loss=2.7264201641082764
I0308 08:24:09.280920 139708407461632 logging_writer.py:48] [25600] global_step=25600, grad_norm=1.0702028274536133, loss=2.824821949005127
I0308 08:24:10.318399 139902746892096 spec.py:321] Evaluating on the training split.
I0308 08:24:21.832615 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 08:24:40.145030 139902746892096 spec.py:349] Evaluating on the test split.
I0308 08:24:41.836195 139902746892096 submission_runner.py:411] Time since start: 12204.86s, 	Step: 25604, 	{'train/accuracy': 0.6005468368530273, 'train/loss': 1.726381778717041, 'validation/accuracy': 0.550819993019104, 'validation/loss': 1.9631150960922241, 'validation/num_examples': 50000, 'test/accuracy': 0.4369000196456909, 'test/loss': 2.60762095451355, 'test/num_examples': 10000, 'score': 11388.192962408066, 'total_duration': 12204.864979743958, 'accumulated_submission_time': 11388.192962408066, 'accumulated_eval_time': 814.4276103973389, 'accumulated_logging_time': 0.8629908561706543}
I0308 08:24:41.857906 139708415854336 logging_writer.py:48] [25604] accumulated_eval_time=814.427610, accumulated_logging_time=0.862991, accumulated_submission_time=11388.192962, global_step=25604, preemption_count=0, score=11388.192962, test/accuracy=0.436900, test/loss=2.607621, test/num_examples=10000, total_duration=12204.864980, train/accuracy=0.600547, train/loss=1.726382, validation/accuracy=0.550820, validation/loss=1.963115, validation/num_examples=50000
I0308 08:25:21.760131 139708407461632 logging_writer.py:48] [25700] global_step=25700, grad_norm=1.0472362041473389, loss=2.7051823139190674
I0308 08:26:08.091472 139708415854336 logging_writer.py:48] [25800] global_step=25800, grad_norm=1.1229870319366455, loss=2.9478683471679688
I0308 08:26:53.827529 139708407461632 logging_writer.py:48] [25900] global_step=25900, grad_norm=1.1698979139328003, loss=2.7771995067596436
I0308 08:27:39.405564 139708415854336 logging_writer.py:48] [26000] global_step=26000, grad_norm=1.0442599058151245, loss=2.700685739517212
I0308 08:28:24.297541 139708407461632 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.9447904825210571, loss=4.285891532897949
I0308 08:29:09.995165 139708415854336 logging_writer.py:48] [26200] global_step=26200, grad_norm=1.1820135116577148, loss=2.6908607482910156
I0308 08:29:55.745704 139708407461632 logging_writer.py:48] [26300] global_step=26300, grad_norm=1.1960574388504028, loss=2.715792179107666
I0308 08:30:41.103032 139708415854336 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.9917801022529602, loss=3.887904644012451
I0308 08:31:26.485553 139708407461632 logging_writer.py:48] [26500] global_step=26500, grad_norm=1.1316848993301392, loss=2.699134111404419
I0308 08:31:41.987216 139902746892096 spec.py:321] Evaluating on the training split.
I0308 08:31:53.589082 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 08:32:15.215576 139902746892096 spec.py:349] Evaluating on the test split.
I0308 08:32:16.888097 139902746892096 submission_runner.py:411] Time since start: 12659.92s, 	Step: 26536, 	{'train/accuracy': 0.6315820217132568, 'train/loss': 1.5329514741897583, 'validation/accuracy': 0.5600399971008301, 'validation/loss': 1.8702781200408936, 'validation/num_examples': 50000, 'test/accuracy': 0.43400001525878906, 'test/loss': 2.540038824081421, 'test/num_examples': 10000, 'score': 11807.219497203827, 'total_duration': 12659.916893959045, 'accumulated_submission_time': 11807.219497203827, 'accumulated_eval_time': 849.3285005092621, 'accumulated_logging_time': 1.937572956085205}
I0308 08:32:16.910813 139708415854336 logging_writer.py:48] [26536] accumulated_eval_time=849.328501, accumulated_logging_time=1.937573, accumulated_submission_time=11807.219497, global_step=26536, preemption_count=0, score=11807.219497, test/accuracy=0.434000, test/loss=2.540039, test/num_examples=10000, total_duration=12659.916894, train/accuracy=0.631582, train/loss=1.532951, validation/accuracy=0.560040, validation/loss=1.870278, validation/num_examples=50000
I0308 08:32:42.511455 139708407461632 logging_writer.py:48] [26600] global_step=26600, grad_norm=1.2488728761672974, loss=2.743806838989258
I0308 08:33:26.252233 139708415854336 logging_writer.py:48] [26700] global_step=26700, grad_norm=1.053576946258545, loss=3.590233087539673
I0308 08:34:11.548444 139708407461632 logging_writer.py:48] [26800] global_step=26800, grad_norm=1.06182062625885, loss=2.8873918056488037
I0308 08:34:56.778620 139708415854336 logging_writer.py:48] [26900] global_step=26900, grad_norm=1.2403039932250977, loss=4.51536226272583
I0308 08:35:41.746956 139708407461632 logging_writer.py:48] [27000] global_step=27000, grad_norm=1.2270052433013916, loss=2.5135045051574707
I0308 08:36:27.222378 139708415854336 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.9667066335678101, loss=5.262972831726074
I0308 08:37:12.308912 139708407461632 logging_writer.py:48] [27200] global_step=27200, grad_norm=1.1055047512054443, loss=3.0242528915405273
I0308 08:37:57.544553 139708415854336 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.9584817290306091, loss=5.336511611938477
I0308 08:38:42.911696 139708407461632 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.8444810509681702, loss=5.294054985046387
I0308 08:39:17.060659 139902746892096 spec.py:321] Evaluating on the training split.
I0308 08:39:28.269866 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 08:39:48.630218 139902746892096 spec.py:349] Evaluating on the test split.
I0308 08:39:50.303097 139902746892096 submission_runner.py:411] Time since start: 13113.33s, 	Step: 27477, 	{'train/accuracy': 0.6069921851158142, 'train/loss': 1.6717681884765625, 'validation/accuracy': 0.5662199854850769, 'validation/loss': 1.8602055311203003, 'validation/num_examples': 50000, 'test/accuracy': 0.4456000328063965, 'test/loss': 2.5280001163482666, 'test/num_examples': 10000, 'score': 12227.308560371399, 'total_duration': 13113.331893920898, 'accumulated_submission_time': 12227.308560371399, 'accumulated_eval_time': 882.5709488391876, 'accumulated_logging_time': 1.9710197448730469}
I0308 08:39:50.321941 139708415854336 logging_writer.py:48] [27477] accumulated_eval_time=882.570949, accumulated_logging_time=1.971020, accumulated_submission_time=12227.308560, global_step=27477, preemption_count=0, score=12227.308560, test/accuracy=0.445600, test/loss=2.528000, test/num_examples=10000, total_duration=13113.331894, train/accuracy=0.606992, train/loss=1.671768, validation/accuracy=0.566220, validation/loss=1.860206, validation/num_examples=50000
I0308 08:39:59.764945 139708407461632 logging_writer.py:48] [27500] global_step=27500, grad_norm=1.2043040990829468, loss=2.662930965423584
I0308 08:40:40.858752 139708415854336 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.9682416915893555, loss=4.952247142791748
I0308 08:41:25.967005 139708407461632 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.9698235988616943, loss=4.587061405181885
I0308 08:42:11.456393 139708415854336 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.8837422132492065, loss=4.316946983337402
I0308 08:42:56.612667 139708407461632 logging_writer.py:48] [27900] global_step=27900, grad_norm=1.2139462232589722, loss=2.6778011322021484
I0308 08:43:41.690536 139708415854336 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.8584481477737427, loss=5.165356636047363
I0308 08:44:26.959482 139708407461632 logging_writer.py:48] [28100] global_step=28100, grad_norm=1.084149718284607, loss=3.148418426513672
I0308 08:45:12.041965 139708415854336 logging_writer.py:48] [28200] global_step=28200, grad_norm=1.090657353401184, loss=3.1936659812927246
I0308 08:45:57.072965 139708407461632 logging_writer.py:48] [28300] global_step=28300, grad_norm=1.018469214439392, loss=4.961121559143066
I0308 08:46:42.502661 139708415854336 logging_writer.py:48] [28400] global_step=28400, grad_norm=1.1103322505950928, loss=2.6909115314483643
I0308 08:46:50.342671 139902746892096 spec.py:321] Evaluating on the training split.
I0308 08:47:01.721199 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 08:47:21.810723 139902746892096 spec.py:349] Evaluating on the test split.
I0308 08:47:23.493719 139902746892096 submission_runner.py:411] Time since start: 13566.52s, 	Step: 28419, 	{'train/accuracy': 0.6087499856948853, 'train/loss': 1.6305036544799805, 'validation/accuracy': 0.5629400014877319, 'validation/loss': 1.8604578971862793, 'validation/num_examples': 50000, 'test/accuracy': 0.4465000331401825, 'test/loss': 2.534619092941284, 'test/num_examples': 10000, 'score': 12647.270294189453, 'total_duration': 13566.522515296936, 'accumulated_submission_time': 12647.270294189453, 'accumulated_eval_time': 915.7219965457916, 'accumulated_logging_time': 1.998776912689209}
I0308 08:47:23.513242 139708407461632 logging_writer.py:48] [28419] accumulated_eval_time=915.721997, accumulated_logging_time=1.998777, accumulated_submission_time=12647.270294, global_step=28419, preemption_count=0, score=12647.270294, test/accuracy=0.446500, test/loss=2.534619, test/num_examples=10000, total_duration=13566.522515, train/accuracy=0.608750, train/loss=1.630504, validation/accuracy=0.562940, validation/loss=1.860458, validation/num_examples=50000
I0308 08:47:56.017080 139708415854336 logging_writer.py:48] [28500] global_step=28500, grad_norm=1.3244009017944336, loss=2.64493727684021
I0308 08:48:40.945731 139708407461632 logging_writer.py:48] [28600] global_step=28600, grad_norm=1.0808988809585571, loss=4.434353351593018
I0308 08:49:25.922711 139708415854336 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.9038788080215454, loss=4.047291278839111
I0308 08:50:11.090328 139708407461632 logging_writer.py:48] [28800] global_step=28800, grad_norm=1.199238657951355, loss=2.94854998588562
I0308 08:50:56.191575 139708415854336 logging_writer.py:48] [28900] global_step=28900, grad_norm=1.0430954694747925, loss=3.5312788486480713
I0308 08:51:41.397522 139708407461632 logging_writer.py:48] [29000] global_step=29000, grad_norm=1.1225509643554688, loss=2.7151739597320557
I0308 08:52:26.484615 139708415854336 logging_writer.py:48] [29100] global_step=29100, grad_norm=1.1368350982666016, loss=2.7301290035247803
I0308 08:53:11.527869 139708407461632 logging_writer.py:48] [29200] global_step=29200, grad_norm=1.2308093309402466, loss=2.6656739711761475
I0308 08:53:56.664206 139708415854336 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.8513176441192627, loss=4.944995403289795
I0308 08:54:23.519727 139902746892096 spec.py:321] Evaluating on the training split.
I0308 08:54:35.023713 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 08:54:56.219009 139902746892096 spec.py:349] Evaluating on the test split.
I0308 08:54:57.891475 139902746892096 submission_runner.py:411] Time since start: 14020.92s, 	Step: 29361, 	{'train/accuracy': 0.6239452958106995, 'train/loss': 1.6023894548416138, 'validation/accuracy': 0.5638799667358398, 'validation/loss': 1.8720834255218506, 'validation/num_examples': 50000, 'test/accuracy': 0.4513000249862671, 'test/loss': 2.5328025817871094, 'test/num_examples': 10000, 'score': 13067.21803355217, 'total_duration': 14020.92025589943, 'accumulated_submission_time': 13067.21803355217, 'accumulated_eval_time': 950.0937445163727, 'accumulated_logging_time': 2.026951313018799}
I0308 08:54:57.911813 139708407461632 logging_writer.py:48] [29361] accumulated_eval_time=950.093745, accumulated_logging_time=2.026951, accumulated_submission_time=13067.218034, global_step=29361, preemption_count=0, score=13067.218034, test/accuracy=0.451300, test/loss=2.532803, test/num_examples=10000, total_duration=14020.920256, train/accuracy=0.623945, train/loss=1.602389, validation/accuracy=0.563880, validation/loss=1.872083, validation/num_examples=50000
I0308 08:55:13.670179 139708415854336 logging_writer.py:48] [29400] global_step=29400, grad_norm=1.1681995391845703, loss=2.602297782897949
I0308 08:55:55.753897 139708407461632 logging_writer.py:48] [29500] global_step=29500, grad_norm=1.2104705572128296, loss=5.2776312828063965
I0308 08:56:40.869088 139708415854336 logging_writer.py:48] [29600] global_step=29600, grad_norm=1.2298907041549683, loss=2.760268449783325
I0308 08:57:26.126696 139708407461632 logging_writer.py:48] [29700] global_step=29700, grad_norm=1.0030170679092407, loss=3.896420478820801
I0308 08:58:11.221124 139708415854336 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.8768482804298401, loss=4.511618614196777
I0308 08:58:56.254368 139708407461632 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.9783350229263306, loss=4.499279499053955
I0308 08:59:41.268380 139708415854336 logging_writer.py:48] [30000] global_step=30000, grad_norm=1.1733587980270386, loss=2.538130283355713
I0308 09:00:26.608971 139708407461632 logging_writer.py:48] [30100] global_step=30100, grad_norm=1.074027180671692, loss=2.647505521774292
I0308 09:01:11.505138 139708415854336 logging_writer.py:48] [30200] global_step=30200, grad_norm=1.2011189460754395, loss=2.5473976135253906
I0308 09:01:56.787324 139708407461632 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.83977210521698, loss=4.827507019042969
I0308 09:01:58.335291 139902746892096 spec.py:321] Evaluating on the training split.
I0308 09:02:09.690700 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 09:02:31.245654 139902746892096 spec.py:349] Evaluating on the test split.
I0308 09:02:32.911405 139902746892096 submission_runner.py:411] Time since start: 14475.94s, 	Step: 30305, 	{'train/accuracy': 0.6150586009025574, 'train/loss': 1.6110780239105225, 'validation/accuracy': 0.5711399912834167, 'validation/loss': 1.8235976696014404, 'validation/num_examples': 50000, 'test/accuracy': 0.4594000279903412, 'test/loss': 2.4598233699798584, 'test/num_examples': 10000, 'score': 13487.580243349075, 'total_duration': 14475.940197706223, 'accumulated_submission_time': 13487.580243349075, 'accumulated_eval_time': 984.6698379516602, 'accumulated_logging_time': 2.058462142944336}
I0308 09:02:32.935037 139708415854336 logging_writer.py:48] [30305] accumulated_eval_time=984.669838, accumulated_logging_time=2.058462, accumulated_submission_time=13487.580243, global_step=30305, preemption_count=0, score=13487.580243, test/accuracy=0.459400, test/loss=2.459823, test/num_examples=10000, total_duration=14475.940198, train/accuracy=0.615059, train/loss=1.611078, validation/accuracy=0.571140, validation/loss=1.823598, validation/num_examples=50000
I0308 09:03:11.137918 139708407461632 logging_writer.py:48] [30400] global_step=30400, grad_norm=1.1679911613464355, loss=2.629218101501465
I0308 09:03:56.077131 139708415854336 logging_writer.py:48] [30500] global_step=30500, grad_norm=1.2078726291656494, loss=2.6043262481689453
I0308 09:04:41.236949 139708407461632 logging_writer.py:48] [30600] global_step=30600, grad_norm=1.0291317701339722, loss=5.118305683135986
I0308 09:05:26.432251 139708415854336 logging_writer.py:48] [30700] global_step=30700, grad_norm=1.206616759300232, loss=2.640424966812134
I0308 09:06:11.434233 139708407461632 logging_writer.py:48] [30800] global_step=30800, grad_norm=1.123598337173462, loss=2.599182367324829
I0308 09:06:56.545858 139708415854336 logging_writer.py:48] [30900] global_step=30900, grad_norm=1.172577142715454, loss=2.6864852905273438
I0308 09:07:41.751078 139708407461632 logging_writer.py:48] [31000] global_step=31000, grad_norm=1.1760787963867188, loss=2.5122461318969727
I0308 09:08:26.906352 139708415854336 logging_writer.py:48] [31100] global_step=31100, grad_norm=1.1115580797195435, loss=2.63413405418396
I0308 09:09:12.026682 139708407461632 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.9533489346504211, loss=4.381221771240234
I0308 09:09:33.275940 139902746892096 spec.py:321] Evaluating on the training split.
I0308 09:09:44.567981 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 09:10:05.421028 139902746892096 spec.py:349] Evaluating on the test split.
I0308 09:10:07.097353 139902746892096 submission_runner.py:411] Time since start: 14930.13s, 	Step: 31249, 	{'train/accuracy': 0.6220507621765137, 'train/loss': 1.5800942182540894, 'validation/accuracy': 0.5763999819755554, 'validation/loss': 1.7837181091308594, 'validation/num_examples': 50000, 'test/accuracy': 0.46000000834465027, 'test/loss': 2.4539012908935547, 'test/num_examples': 10000, 'score': 13907.861764431, 'total_duration': 14930.126143455505, 'accumulated_submission_time': 13907.861764431, 'accumulated_eval_time': 1018.4912509918213, 'accumulated_logging_time': 2.090975284576416}
I0308 09:10:07.116590 139708415854336 logging_writer.py:48] [31249] accumulated_eval_time=1018.491251, accumulated_logging_time=2.090975, accumulated_submission_time=13907.861764, global_step=31249, preemption_count=0, score=13907.861764, test/accuracy=0.460000, test/loss=2.453901, test/num_examples=10000, total_duration=14930.126143, train/accuracy=0.622051, train/loss=1.580094, validation/accuracy=0.576400, validation/loss=1.783718, validation/num_examples=50000
I0308 09:10:27.646141 139708407461632 logging_writer.py:48] [31300] global_step=31300, grad_norm=1.0128923654556274, loss=3.514603614807129
I0308 09:11:10.934855 139708415854336 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.9704296588897705, loss=4.544924736022949
I0308 09:11:56.143178 139708407461632 logging_writer.py:48] [31500] global_step=31500, grad_norm=1.212422251701355, loss=2.4947144985198975
I0308 09:12:41.399095 139708415854336 logging_writer.py:48] [31600] global_step=31600, grad_norm=1.1416895389556885, loss=3.338046073913574
I0308 09:13:26.493418 139708407461632 logging_writer.py:48] [31700] global_step=31700, grad_norm=1.1058484315872192, loss=2.5993125438690186
I0308 09:14:11.746355 139708415854336 logging_writer.py:48] [31800] global_step=31800, grad_norm=1.0877009630203247, loss=3.1039602756500244
I0308 09:14:56.976570 139708407461632 logging_writer.py:48] [31900] global_step=31900, grad_norm=1.1747208833694458, loss=2.6725192070007324
I0308 09:15:42.197677 139708415854336 logging_writer.py:48] [32000] global_step=32000, grad_norm=1.3010501861572266, loss=2.610684394836426
I0308 09:16:27.234323 139708407461632 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.9855267405509949, loss=4.078336715698242
I0308 09:17:07.556009 139902746892096 spec.py:321] Evaluating on the training split.
I0308 09:17:18.847415 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 09:17:40.333096 139902746892096 spec.py:349] Evaluating on the test split.
I0308 09:17:42.005119 139902746892096 submission_runner.py:411] Time since start: 15385.03s, 	Step: 32191, 	{'train/accuracy': 0.6280859112739563, 'train/loss': 1.5354851484298706, 'validation/accuracy': 0.5817399621009827, 'validation/loss': 1.7738856077194214, 'validation/num_examples': 50000, 'test/accuracy': 0.46070003509521484, 'test/loss': 2.4564712047576904, 'test/num_examples': 10000, 'score': 14328.242881059647, 'total_duration': 15385.033915281296, 'accumulated_submission_time': 14328.242881059647, 'accumulated_eval_time': 1052.9403524398804, 'accumulated_logging_time': 2.1188464164733887}
I0308 09:17:42.028548 139708415854336 logging_writer.py:48] [32191] accumulated_eval_time=1052.940352, accumulated_logging_time=2.118846, accumulated_submission_time=14328.242881, global_step=32191, preemption_count=0, score=14328.242881, test/accuracy=0.460700, test/loss=2.456471, test/num_examples=10000, total_duration=15385.033915, train/accuracy=0.628086, train/loss=1.535485, validation/accuracy=0.581740, validation/loss=1.773886, validation/num_examples=50000
I0308 09:17:45.962137 139708407461632 logging_writer.py:48] [32200] global_step=32200, grad_norm=1.3529330492019653, loss=5.0438714027404785
I0308 09:18:26.568087 139708415854336 logging_writer.py:48] [32300] global_step=32300, grad_norm=1.0001070499420166, loss=4.782096862792969
I0308 09:19:11.611239 139708407461632 logging_writer.py:48] [32400] global_step=32400, grad_norm=1.2628568410873413, loss=2.6495800018310547
I0308 09:19:56.723574 139708415854336 logging_writer.py:48] [32500] global_step=32500, grad_norm=1.2091165781021118, loss=2.5784857273101807
I0308 09:20:42.169092 139708407461632 logging_writer.py:48] [32600] global_step=32600, grad_norm=1.195003867149353, loss=2.903719186782837
I0308 09:21:27.282097 139708415854336 logging_writer.py:48] [32700] global_step=32700, grad_norm=1.1798416376113892, loss=5.335132122039795
I0308 09:22:12.385135 139708407461632 logging_writer.py:48] [32800] global_step=32800, grad_norm=1.1303131580352783, loss=5.204294681549072
I0308 09:22:57.521931 139708415854336 logging_writer.py:48] [32900] global_step=32900, grad_norm=1.2521663904190063, loss=2.6272974014282227
I0308 09:23:42.643938 139708407461632 logging_writer.py:48] [33000] global_step=33000, grad_norm=1.2193090915679932, loss=3.192775011062622
I0308 09:24:27.758932 139708415854336 logging_writer.py:48] [33100] global_step=33100, grad_norm=1.0660864114761353, loss=2.824705123901367
I0308 09:24:42.372402 139902746892096 spec.py:321] Evaluating on the training split.
I0308 09:24:53.883078 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 09:25:15.806744 139902746892096 spec.py:349] Evaluating on the test split.
I0308 09:25:17.487042 139902746892096 submission_runner.py:411] Time since start: 15840.52s, 	Step: 33134, 	{'train/accuracy': 0.6576367020606995, 'train/loss': 1.3888908624649048, 'validation/accuracy': 0.5830000042915344, 'validation/loss': 1.7361211776733398, 'validation/num_examples': 50000, 'test/accuracy': 0.46570003032684326, 'test/loss': 2.4205970764160156, 'test/num_examples': 10000, 'score': 14748.527193307877, 'total_duration': 15840.515821695328, 'accumulated_submission_time': 14748.527193307877, 'accumulated_eval_time': 1088.0549836158752, 'accumulated_logging_time': 2.1518115997314453}
I0308 09:25:17.511691 139708407461632 logging_writer.py:48] [33134] accumulated_eval_time=1088.054984, accumulated_logging_time=2.151812, accumulated_submission_time=14748.527193, global_step=33134, preemption_count=0, score=14748.527193, test/accuracy=0.465700, test/loss=2.420597, test/num_examples=10000, total_duration=15840.515822, train/accuracy=0.657637, train/loss=1.388891, validation/accuracy=0.583000, validation/loss=1.736121, validation/num_examples=50000
I0308 09:25:43.892868 139708415854336 logging_writer.py:48] [33200] global_step=33200, grad_norm=1.0985239744186401, loss=3.2916855812072754
I0308 09:26:27.402122 139708407461632 logging_writer.py:48] [33300] global_step=33300, grad_norm=1.096725583076477, loss=2.672788143157959
I0308 09:27:12.654366 139708415854336 logging_writer.py:48] [33400] global_step=33400, grad_norm=1.2306400537490845, loss=2.44577693939209
I0308 09:27:58.565775 139708407461632 logging_writer.py:48] [33500] global_step=33500, grad_norm=1.177215337753296, loss=2.9862351417541504
I0308 09:28:43.811640 139708415854336 logging_writer.py:48] [33600] global_step=33600, grad_norm=1.0775641202926636, loss=2.6428112983703613
I0308 09:29:28.909530 139708407461632 logging_writer.py:48] [33700] global_step=33700, grad_norm=1.0131466388702393, loss=2.776170253753662
I0308 09:30:14.555509 139708415854336 logging_writer.py:48] [33800] global_step=33800, grad_norm=1.0030256509780884, loss=2.955281972885132
I0308 09:30:59.632379 139708407461632 logging_writer.py:48] [33900] global_step=33900, grad_norm=1.2136539220809937, loss=2.630530595779419
I0308 09:31:44.968276 139708415854336 logging_writer.py:48] [34000] global_step=34000, grad_norm=1.287757158279419, loss=2.626648426055908
I0308 09:32:17.491590 139902746892096 spec.py:321] Evaluating on the training split.
I0308 09:32:29.011618 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 09:32:49.706817 139902746892096 spec.py:349] Evaluating on the test split.
I0308 09:32:51.376976 139902746892096 submission_runner.py:411] Time since start: 16294.41s, 	Step: 34074, 	{'train/accuracy': 0.6277148127555847, 'train/loss': 1.5349551439285278, 'validation/accuracy': 0.5827400088310242, 'validation/loss': 1.75555419921875, 'validation/num_examples': 50000, 'test/accuracy': 0.45990002155303955, 'test/loss': 2.418983221054077, 'test/num_examples': 10000, 'score': 15168.447052955627, 'total_duration': 16294.405767679214, 'accumulated_submission_time': 15168.447052955627, 'accumulated_eval_time': 1121.9403958320618, 'accumulated_logging_time': 2.186378002166748}
I0308 09:32:51.396579 139708407461632 logging_writer.py:48] [34074] accumulated_eval_time=1121.940396, accumulated_logging_time=2.186378, accumulated_submission_time=15168.447053, global_step=34074, preemption_count=0, score=15168.447053, test/accuracy=0.459900, test/loss=2.418983, test/num_examples=10000, total_duration=16294.405768, train/accuracy=0.627715, train/loss=1.534955, validation/accuracy=0.582740, validation/loss=1.755554, validation/num_examples=50000
I0308 09:33:02.038849 139708415854336 logging_writer.py:48] [34100] global_step=34100, grad_norm=1.1251646280288696, loss=2.5802533626556396
I0308 09:33:43.997484 139708407461632 logging_writer.py:48] [34200] global_step=34200, grad_norm=1.0998988151550293, loss=2.655547857284546
I0308 09:34:29.410544 139708415854336 logging_writer.py:48] [34300] global_step=34300, grad_norm=1.3454689979553223, loss=2.6637232303619385
I0308 09:35:14.795009 139708407461632 logging_writer.py:48] [34400] global_step=34400, grad_norm=1.1684539318084717, loss=2.5333456993103027
I0308 09:35:59.858668 139708415854336 logging_writer.py:48] [34500] global_step=34500, grad_norm=1.2307014465332031, loss=2.638231039047241
I0308 09:36:45.045997 139708407461632 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.8955119848251343, loss=5.118948936462402
I0308 09:37:30.314965 139708415854336 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.9839984774589539, loss=5.185317516326904
I0308 09:38:15.647468 139708407461632 logging_writer.py:48] [34800] global_step=34800, grad_norm=1.168199062347412, loss=2.6267542839050293
I0308 09:39:00.711746 139708415854336 logging_writer.py:48] [34900] global_step=34900, grad_norm=1.0335149765014648, loss=4.369020462036133
I0308 09:39:45.706242 139708407461632 logging_writer.py:48] [35000] global_step=35000, grad_norm=1.066058874130249, loss=4.4289727210998535
I0308 09:39:51.888090 139902746892096 spec.py:321] Evaluating on the training split.
I0308 09:40:03.816594 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 09:40:23.379440 139902746892096 spec.py:349] Evaluating on the test split.
I0308 09:40:25.072965 139902746892096 submission_runner.py:411] Time since start: 16748.10s, 	Step: 35015, 	{'train/accuracy': 0.6364648342132568, 'train/loss': 1.5358233451843262, 'validation/accuracy': 0.5851799845695496, 'validation/loss': 1.7673289775848389, 'validation/num_examples': 50000, 'test/accuracy': 0.46380001306533813, 'test/loss': 2.43481183052063, 'test/num_examples': 10000, 'score': 15588.879315137863, 'total_duration': 16748.10175061226, 'accumulated_submission_time': 15588.879315137863, 'accumulated_eval_time': 1155.1252624988556, 'accumulated_logging_time': 2.2159085273742676}
I0308 09:40:25.102041 139708415854336 logging_writer.py:48] [35015] accumulated_eval_time=1155.125262, accumulated_logging_time=2.215909, accumulated_submission_time=15588.879315, global_step=35015, preemption_count=0, score=15588.879315, test/accuracy=0.463800, test/loss=2.434812, test/num_examples=10000, total_duration=16748.101751, train/accuracy=0.636465, train/loss=1.535823, validation/accuracy=0.585180, validation/loss=1.767329, validation/num_examples=50000
I0308 09:40:59.145477 139708407461632 logging_writer.py:48] [35100] global_step=35100, grad_norm=1.1335147619247437, loss=2.410665273666382
I0308 09:41:44.296374 139708415854336 logging_writer.py:48] [35200] global_step=35200, grad_norm=1.1774930953979492, loss=2.564192295074463
I0308 09:42:29.370598 139708407461632 logging_writer.py:48] [35300] global_step=35300, grad_norm=1.2685539722442627, loss=2.744765043258667
I0308 09:43:14.427856 139708415854336 logging_writer.py:48] [35400] global_step=35400, grad_norm=1.1464745998382568, loss=2.976428270339966
I0308 09:43:59.707923 139708407461632 logging_writer.py:48] [35500] global_step=35500, grad_norm=1.2040321826934814, loss=2.482095956802368
I0308 09:44:44.929524 139708415854336 logging_writer.py:48] [35600] global_step=35600, grad_norm=1.1929222345352173, loss=2.6110482215881348
I0308 09:45:29.966379 139708407461632 logging_writer.py:48] [35700] global_step=35700, grad_norm=1.0055899620056152, loss=3.649994134902954
I0308 09:46:14.864326 139708415854336 logging_writer.py:48] [35800] global_step=35800, grad_norm=1.0472886562347412, loss=2.573132276535034
I0308 09:46:59.925680 139708407461632 logging_writer.py:48] [35900] global_step=35900, grad_norm=1.0694043636322021, loss=4.805495262145996
I0308 09:47:25.244833 139902746892096 spec.py:321] Evaluating on the training split.
I0308 09:47:36.975688 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 09:47:56.626893 139902746892096 spec.py:349] Evaluating on the test split.
I0308 09:47:58.313493 139902746892096 submission_runner.py:411] Time since start: 17201.34s, 	Step: 35958, 	{'train/accuracy': 0.6478710770606995, 'train/loss': 1.4692319631576538, 'validation/accuracy': 0.5889399647712708, 'validation/loss': 1.7494028806686401, 'validation/num_examples': 50000, 'test/accuracy': 0.468500018119812, 'test/loss': 2.4204137325286865, 'test/num_examples': 10000, 'score': 16008.960027456284, 'total_duration': 17201.34227848053, 'accumulated_submission_time': 16008.960027456284, 'accumulated_eval_time': 1188.193915605545, 'accumulated_logging_time': 2.2564587593078613}
I0308 09:47:58.334242 139708415854336 logging_writer.py:48] [35958] accumulated_eval_time=1188.193916, accumulated_logging_time=2.256459, accumulated_submission_time=16008.960027, global_step=35958, preemption_count=0, score=16008.960027, test/accuracy=0.468500, test/loss=2.420414, test/num_examples=10000, total_duration=17201.342278, train/accuracy=0.647871, train/loss=1.469232, validation/accuracy=0.588940, validation/loss=1.749403, validation/num_examples=50000
I0308 09:48:15.283110 139708407461632 logging_writer.py:48] [36000] global_step=36000, grad_norm=1.1059292554855347, loss=2.4100165367126465
I0308 09:48:57.849448 139708415854336 logging_writer.py:48] [36100] global_step=36100, grad_norm=1.0431554317474365, loss=5.15268611907959
I0308 09:49:42.866199 139708407461632 logging_writer.py:48] [36200] global_step=36200, grad_norm=1.085898995399475, loss=3.1235549449920654
I0308 09:50:28.480648 139708415854336 logging_writer.py:48] [36300] global_step=36300, grad_norm=1.285477876663208, loss=2.3862063884735107
I0308 09:51:13.533744 139708407461632 logging_writer.py:48] [36400] global_step=36400, grad_norm=1.0451481342315674, loss=4.17165470123291
I0308 09:51:58.513256 139708415854336 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.9836230278015137, loss=5.019137859344482
I0308 09:52:43.754415 139708407461632 logging_writer.py:48] [36600] global_step=36600, grad_norm=1.040502667427063, loss=4.224130630493164
I0308 09:53:28.831295 139708415854336 logging_writer.py:48] [36700] global_step=36700, grad_norm=1.0091650485992432, loss=4.4554829597473145
I0308 09:54:13.931716 139708407461632 logging_writer.py:48] [36800] global_step=36800, grad_norm=1.0831629037857056, loss=2.6822943687438965
I0308 09:54:59.006283 139708415854336 logging_writer.py:48] [36900] global_step=36900, grad_norm=1.238303303718567, loss=4.232943534851074
I0308 09:54:59.019968 139902746892096 spec.py:321] Evaluating on the training split.
I0308 09:55:10.430095 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 09:55:29.570641 139902746892096 spec.py:349] Evaluating on the test split.
I0308 09:55:31.248199 139902746892096 submission_runner.py:411] Time since start: 17654.28s, 	Step: 36901, 	{'train/accuracy': 0.63427734375, 'train/loss': 1.4953609704971313, 'validation/accuracy': 0.5922600030899048, 'validation/loss': 1.7099379301071167, 'validation/num_examples': 50000, 'test/accuracy': 0.4686000347137451, 'test/loss': 2.3968710899353027, 'test/num_examples': 10000, 'score': 16429.58711719513, 'total_duration': 17654.276985645294, 'accumulated_submission_time': 16429.58711719513, 'accumulated_eval_time': 1220.422137260437, 'accumulated_logging_time': 2.285893678665161}
I0308 09:55:31.272416 139708407461632 logging_writer.py:48] [36901] accumulated_eval_time=1220.422137, accumulated_logging_time=2.285894, accumulated_submission_time=16429.587117, global_step=36901, preemption_count=0, score=16429.587117, test/accuracy=0.468600, test/loss=2.396871, test/num_examples=10000, total_duration=17654.276986, train/accuracy=0.634277, train/loss=1.495361, validation/accuracy=0.592260, validation/loss=1.709938, validation/num_examples=50000
I0308 09:56:11.579360 139708415854336 logging_writer.py:48] [37000] global_step=37000, grad_norm=1.1600278615951538, loss=2.788397789001465
I0308 09:56:56.539696 139708407461632 logging_writer.py:48] [37100] global_step=37100, grad_norm=1.1633188724517822, loss=2.5985543727874756
I0308 09:57:41.714852 139708415854336 logging_writer.py:48] [37200] global_step=37200, grad_norm=1.1007558107376099, loss=2.5320956707000732
I0308 09:58:27.201803 139708407461632 logging_writer.py:48] [37300] global_step=37300, grad_norm=1.0681220293045044, loss=3.676806926727295
I0308 09:59:12.283623 139708415854336 logging_writer.py:48] [37400] global_step=37400, grad_norm=1.2704472541809082, loss=5.032806873321533
I0308 09:59:57.298784 139708407461632 logging_writer.py:48] [37500] global_step=37500, grad_norm=1.2613842487335205, loss=5.2224555015563965
I0308 10:00:42.913988 139708415854336 logging_writer.py:48] [37600] global_step=37600, grad_norm=1.1931383609771729, loss=2.497333288192749
I0308 10:01:28.029910 139708407461632 logging_writer.py:48] [37700] global_step=37700, grad_norm=1.201393485069275, loss=2.568699359893799
I0308 10:02:13.161239 139708415854336 logging_writer.py:48] [37800] global_step=37800, grad_norm=1.2286174297332764, loss=2.427422285079956
I0308 10:02:31.263426 139902746892096 spec.py:321] Evaluating on the training split.
I0308 10:02:42.566296 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 10:03:04.249956 139902746892096 spec.py:349] Evaluating on the test split.
I0308 10:03:05.923128 139902746892096 submission_runner.py:411] Time since start: 18108.95s, 	Step: 37842, 	{'train/accuracy': 0.6421288847923279, 'train/loss': 1.446354627609253, 'validation/accuracy': 0.5999400019645691, 'validation/loss': 1.6714078187942505, 'validation/num_examples': 50000, 'test/accuracy': 0.4766000211238861, 'test/loss': 2.344536066055298, 'test/num_examples': 10000, 'score': 16849.518322229385, 'total_duration': 18108.951924800873, 'accumulated_submission_time': 16849.518322229385, 'accumulated_eval_time': 1255.0818610191345, 'accumulated_logging_time': 2.3198788166046143}
I0308 10:03:05.949280 139708407461632 logging_writer.py:48] [37842] accumulated_eval_time=1255.081861, accumulated_logging_time=2.319879, accumulated_submission_time=16849.518322, global_step=37842, preemption_count=0, score=16849.518322, test/accuracy=0.476600, test/loss=2.344536, test/num_examples=10000, total_duration=18108.951925, train/accuracy=0.642129, train/loss=1.446355, validation/accuracy=0.599940, validation/loss=1.671408, validation/num_examples=50000
I0308 10:03:29.172813 139708415854336 logging_writer.py:48] [37900] global_step=37900, grad_norm=1.1706031560897827, loss=4.999278545379639
I0308 10:04:12.415576 139708407461632 logging_writer.py:48] [38000] global_step=38000, grad_norm=1.1668261289596558, loss=2.44920015335083
I0308 10:04:57.471535 139708415854336 logging_writer.py:48] [38100] global_step=38100, grad_norm=1.3416695594787598, loss=2.4993155002593994
I0308 10:05:42.674711 139708407461632 logging_writer.py:48] [38200] global_step=38200, grad_norm=1.231580138206482, loss=2.478404998779297
I0308 10:06:27.540867 139708415854336 logging_writer.py:48] [38300] global_step=38300, grad_norm=1.1808865070343018, loss=2.531582832336426
I0308 10:07:12.628520 139708407461632 logging_writer.py:48] [38400] global_step=38400, grad_norm=1.020404577255249, loss=4.870604038238525
I0308 10:07:57.821454 139708415854336 logging_writer.py:48] [38500] global_step=38500, grad_norm=1.2348378896713257, loss=2.4652905464172363
I0308 10:08:43.116520 139708407461632 logging_writer.py:48] [38600] global_step=38600, grad_norm=1.0512771606445312, loss=4.18228006362915
I0308 10:09:28.559597 139708415854336 logging_writer.py:48] [38700] global_step=38700, grad_norm=1.3433219194412231, loss=2.5483906269073486
I0308 10:10:06.016398 139902746892096 spec.py:321] Evaluating on the training split.
I0308 10:10:17.387201 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 10:10:37.136708 139902746892096 spec.py:349] Evaluating on the test split.
I0308 10:10:38.824320 139902746892096 submission_runner.py:411] Time since start: 18561.85s, 	Step: 38784, 	{'train/accuracy': 0.6454101204872131, 'train/loss': 1.46431565284729, 'validation/accuracy': 0.593239963054657, 'validation/loss': 1.7185540199279785, 'validation/num_examples': 50000, 'test/accuracy': 0.4739000201225281, 'test/loss': 2.387554883956909, 'test/num_examples': 10000, 'score': 17269.525953292847, 'total_duration': 18561.853103876114, 'accumulated_submission_time': 17269.525953292847, 'accumulated_eval_time': 1287.8897886276245, 'accumulated_logging_time': 2.355440855026245}
I0308 10:10:38.849489 139708407461632 logging_writer.py:48] [38784] accumulated_eval_time=1287.889789, accumulated_logging_time=2.355441, accumulated_submission_time=17269.525953, global_step=38784, preemption_count=0, score=17269.525953, test/accuracy=0.473900, test/loss=2.387555, test/num_examples=10000, total_duration=18561.853104, train/accuracy=0.645410, train/loss=1.464316, validation/accuracy=0.593240, validation/loss=1.718554, validation/num_examples=50000
I0308 10:10:45.566719 139708415854336 logging_writer.py:48] [38800] global_step=38800, grad_norm=1.0779627561569214, loss=4.450432300567627
I0308 10:11:26.682903 139708407461632 logging_writer.py:48] [38900] global_step=38900, grad_norm=1.1686697006225586, loss=2.4845242500305176
I0308 10:12:12.033722 139708415854336 logging_writer.py:48] [39000] global_step=39000, grad_norm=1.3685051202774048, loss=2.556090831756592
I0308 10:12:57.246834 139708407461632 logging_writer.py:48] [39100] global_step=39100, grad_norm=1.1840782165527344, loss=2.496190309524536
I0308 10:13:42.326762 139708415854336 logging_writer.py:48] [39200] global_step=39200, grad_norm=1.0014517307281494, loss=4.645769119262695
I0308 10:14:27.414677 139708407461632 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.9589700102806091, loss=3.6703224182128906
I0308 10:15:12.722818 139708415854336 logging_writer.py:48] [39400] global_step=39400, grad_norm=1.0370906591415405, loss=3.737342119216919
I0308 10:15:57.689149 139708407461632 logging_writer.py:48] [39500] global_step=39500, grad_norm=1.1896308660507202, loss=2.4524505138397217
I0308 10:16:42.869105 139708415854336 logging_writer.py:48] [39600] global_step=39600, grad_norm=1.1406941413879395, loss=2.484287977218628
I0308 10:17:28.186955 139708407461632 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.9577650427818298, loss=5.053974151611328
I0308 10:17:39.257826 139902746892096 spec.py:321] Evaluating on the training split.
I0308 10:17:50.887357 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 10:18:11.097480 139902746892096 spec.py:349] Evaluating on the test split.
I0308 10:18:12.777765 139902746892096 submission_runner.py:411] Time since start: 19015.81s, 	Step: 39726, 	{'train/accuracy': 0.6728320121765137, 'train/loss': 1.3413584232330322, 'validation/accuracy': 0.5954399704933167, 'validation/loss': 1.6972488164901733, 'validation/num_examples': 50000, 'test/accuracy': 0.4764000177383423, 'test/loss': 2.3578004837036133, 'test/num_examples': 10000, 'score': 17689.873439073563, 'total_duration': 19015.806552886963, 'accumulated_submission_time': 17689.873439073563, 'accumulated_eval_time': 1321.4097304344177, 'accumulated_logging_time': 2.3911402225494385}
I0308 10:18:12.798935 139708415854336 logging_writer.py:48] [39726] accumulated_eval_time=1321.409730, accumulated_logging_time=2.391140, accumulated_submission_time=17689.873439, global_step=39726, preemption_count=0, score=17689.873439, test/accuracy=0.476400, test/loss=2.357800, test/num_examples=10000, total_duration=19015.806553, train/accuracy=0.672832, train/loss=1.341358, validation/accuracy=0.595440, validation/loss=1.697249, validation/num_examples=50000
I0308 10:18:42.331385 139708407461632 logging_writer.py:48] [39800] global_step=39800, grad_norm=1.0477304458618164, loss=4.1669135093688965
I0308 10:19:27.320908 139708415854336 logging_writer.py:48] [39900] global_step=39900, grad_norm=1.1814830303192139, loss=4.96566104888916
I0308 10:20:12.421295 139708407461632 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.9887146353721619, loss=4.3783674240112305
I0308 10:20:57.916414 139708415854336 logging_writer.py:48] [40100] global_step=40100, grad_norm=1.1016309261322021, loss=2.5212979316711426
I0308 10:21:43.172850 139708407461632 logging_writer.py:48] [40200] global_step=40200, grad_norm=1.0252416133880615, loss=2.888705015182495
I0308 10:22:28.335285 139708415854336 logging_writer.py:48] [40300] global_step=40300, grad_norm=1.2693538665771484, loss=2.59147310256958
I0308 10:23:13.442321 139708407461632 logging_writer.py:48] [40400] global_step=40400, grad_norm=1.151517629623413, loss=2.7332499027252197
I0308 10:23:58.562073 139708415854336 logging_writer.py:48] [40500] global_step=40500, grad_norm=1.2559655904769897, loss=5.1025309562683105
I0308 10:24:43.528091 139708407461632 logging_writer.py:48] [40600] global_step=40600, grad_norm=1.2772400379180908, loss=2.410409450531006
I0308 10:25:13.040565 139902746892096 spec.py:321] Evaluating on the training split.
I0308 10:25:24.380980 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 10:25:44.720017 139902746892096 spec.py:349] Evaluating on the test split.
I0308 10:25:46.399137 139902746892096 submission_runner.py:411] Time since start: 19469.43s, 	Step: 40667, 	{'train/accuracy': 0.6483203172683716, 'train/loss': 1.4345197677612305, 'validation/accuracy': 0.6010800004005432, 'validation/loss': 1.6640561819076538, 'validation/num_examples': 50000, 'test/accuracy': 0.47780001163482666, 'test/loss': 2.346426010131836, 'test/num_examples': 10000, 'score': 18110.056242227554, 'total_duration': 19469.427932024002, 'accumulated_submission_time': 18110.056242227554, 'accumulated_eval_time': 1354.7683067321777, 'accumulated_logging_time': 2.4212679862976074}
I0308 10:25:46.420479 139708415854336 logging_writer.py:48] [40667] accumulated_eval_time=1354.768307, accumulated_logging_time=2.421268, accumulated_submission_time=18110.056242, global_step=40667, preemption_count=0, score=18110.056242, test/accuracy=0.477800, test/loss=2.346426, test/num_examples=10000, total_duration=19469.427932, train/accuracy=0.648320, train/loss=1.434520, validation/accuracy=0.601080, validation/loss=1.664056, validation/num_examples=50000
I0308 10:25:59.816399 139708407461632 logging_writer.py:48] [40700] global_step=40700, grad_norm=1.141974687576294, loss=3.009760856628418
I0308 10:26:41.576232 139708415854336 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.973213791847229, loss=3.6603682041168213
I0308 10:27:26.830521 139708407461632 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.9227912425994873, loss=3.690706729888916
I0308 10:28:12.088101 139708415854336 logging_writer.py:48] [41000] global_step=41000, grad_norm=1.309808611869812, loss=2.495793581008911
I0308 10:28:57.196372 139708407461632 logging_writer.py:48] [41100] global_step=41100, grad_norm=1.0739320516586304, loss=4.627584457397461
I0308 10:29:42.236367 139708415854336 logging_writer.py:48] [41200] global_step=41200, grad_norm=1.3146069049835205, loss=2.513009786605835
I0308 10:30:27.708535 139708407461632 logging_writer.py:48] [41300] global_step=41300, grad_norm=1.0532692670822144, loss=4.913268566131592
I0308 10:31:12.867559 139708415854336 logging_writer.py:48] [41400] global_step=41400, grad_norm=1.1441692113876343, loss=2.607940196990967
I0308 10:31:57.949653 139708407461632 logging_writer.py:48] [41500] global_step=41500, grad_norm=1.2092803716659546, loss=2.4563374519348145
I0308 10:32:43.087272 139708415854336 logging_writer.py:48] [41600] global_step=41600, grad_norm=1.0735377073287964, loss=3.0799155235290527
I0308 10:32:46.841498 139902746892096 spec.py:321] Evaluating on the training split.
I0308 10:32:58.067427 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 10:33:18.649527 139902746892096 spec.py:349] Evaluating on the test split.
I0308 10:33:20.338072 139902746892096 submission_runner.py:411] Time since start: 19923.37s, 	Step: 41610, 	{'train/accuracy': 0.6526757478713989, 'train/loss': 1.4107017517089844, 'validation/accuracy': 0.6041799783706665, 'validation/loss': 1.6475238800048828, 'validation/num_examples': 50000, 'test/accuracy': 0.4812000095844269, 'test/loss': 2.312072277069092, 'test/num_examples': 10000, 'score': 18530.417755126953, 'total_duration': 19923.366858959198, 'accumulated_submission_time': 18530.417755126953, 'accumulated_eval_time': 1388.2648763656616, 'accumulated_logging_time': 2.4519410133361816}
I0308 10:33:20.358953 139708407461632 logging_writer.py:48] [41610] accumulated_eval_time=1388.264876, accumulated_logging_time=2.451941, accumulated_submission_time=18530.417755, global_step=41610, preemption_count=0, score=18530.417755, test/accuracy=0.481200, test/loss=2.312072, test/num_examples=10000, total_duration=19923.366859, train/accuracy=0.652676, train/loss=1.410702, validation/accuracy=0.604180, validation/loss=1.647524, validation/num_examples=50000
I0308 10:33:56.464470 139708415854336 logging_writer.py:48] [41700] global_step=41700, grad_norm=1.3044459819793701, loss=2.505422592163086
I0308 10:34:41.778495 139708407461632 logging_writer.py:48] [41800] global_step=41800, grad_norm=1.1408331394195557, loss=2.8199093341827393
I0308 10:35:27.045698 139708415854336 logging_writer.py:48] [41900] global_step=41900, grad_norm=1.1028376817703247, loss=4.769790172576904
I0308 10:36:12.249236 139708407461632 logging_writer.py:48] [42000] global_step=42000, grad_norm=1.1202486753463745, loss=2.768022298812866
I0308 10:36:57.140893 139708415854336 logging_writer.py:48] [42100] global_step=42100, grad_norm=1.2567461729049683, loss=2.4869861602783203
I0308 10:37:42.474043 139708407461632 logging_writer.py:48] [42200] global_step=42200, grad_norm=1.1084057092666626, loss=2.6624295711517334
I0308 10:38:27.499891 139708415854336 logging_writer.py:48] [42300] global_step=42300, grad_norm=1.3010773658752441, loss=2.5666985511779785
I0308 10:39:12.437462 139708407461632 logging_writer.py:48] [42400] global_step=42400, grad_norm=1.359229326248169, loss=5.08439826965332
I0308 10:39:57.732034 139708415854336 logging_writer.py:48] [42500] global_step=42500, grad_norm=1.0009500980377197, loss=4.309383392333984
I0308 10:40:20.666071 139902746892096 spec.py:321] Evaluating on the training split.
I0308 10:40:32.303276 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 10:40:51.514936 139902746892096 spec.py:349] Evaluating on the test split.
I0308 10:40:53.201387 139902746892096 submission_runner.py:411] Time since start: 20376.23s, 	Step: 42552, 	{'train/accuracy': 0.6664062142372131, 'train/loss': 1.3672758340835571, 'validation/accuracy': 0.6040399670600891, 'validation/loss': 1.6572825908660889, 'validation/num_examples': 50000, 'test/accuracy': 0.48170003294944763, 'test/loss': 2.320866584777832, 'test/num_examples': 10000, 'score': 18950.665768146515, 'total_duration': 20376.23017168045, 'accumulated_submission_time': 18950.665768146515, 'accumulated_eval_time': 1420.8001787662506, 'accumulated_logging_time': 2.481510877609253}
I0308 10:40:53.229985 139708407461632 logging_writer.py:48] [42552] accumulated_eval_time=1420.800179, accumulated_logging_time=2.481511, accumulated_submission_time=18950.665768, global_step=42552, preemption_count=0, score=18950.665768, test/accuracy=0.481700, test/loss=2.320867, test/num_examples=10000, total_duration=20376.230172, train/accuracy=0.666406, train/loss=1.367276, validation/accuracy=0.604040, validation/loss=1.657283, validation/num_examples=50000
I0308 10:41:12.542273 139708415854336 logging_writer.py:48] [42600] global_step=42600, grad_norm=1.289657711982727, loss=2.4033617973327637
I0308 10:41:55.996157 139708407461632 logging_writer.py:48] [42700] global_step=42700, grad_norm=1.0195519924163818, loss=3.0396461486816406
I0308 10:42:41.557060 139708415854336 logging_writer.py:48] [42800] global_step=42800, grad_norm=1.1906909942626953, loss=2.4027533531188965
I0308 10:43:26.862126 139708407461632 logging_writer.py:48] [42900] global_step=42900, grad_norm=1.0578688383102417, loss=4.468979835510254
I0308 10:44:12.008509 139708415854336 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.9978610873222351, loss=4.835044860839844
I0308 10:44:57.123456 139708407461632 logging_writer.py:48] [43100] global_step=43100, grad_norm=1.141947627067566, loss=2.5340418815612793
I0308 10:45:42.350547 139708415854336 logging_writer.py:48] [43200] global_step=43200, grad_norm=1.2770609855651855, loss=2.42364239692688
I0308 10:46:27.549649 139708407461632 logging_writer.py:48] [43300] global_step=43300, grad_norm=1.516849160194397, loss=2.7574543952941895
I0308 10:47:12.570673 139708415854336 logging_writer.py:48] [43400] global_step=43400, grad_norm=1.2596955299377441, loss=2.6103484630584717
I0308 10:47:53.462347 139902746892096 spec.py:321] Evaluating on the training split.
I0308 10:48:04.800911 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 10:48:26.232646 139902746892096 spec.py:349] Evaluating on the test split.
I0308 10:48:27.906207 139902746892096 submission_runner.py:411] Time since start: 20830.93s, 	Step: 43492, 	{'train/accuracy': 0.6471484303474426, 'train/loss': 1.4733134508132935, 'validation/accuracy': 0.6031799912452698, 'validation/loss': 1.6737443208694458, 'validation/num_examples': 50000, 'test/accuracy': 0.482200026512146, 'test/loss': 2.3180975914001465, 'test/num_examples': 10000, 'score': 19370.83881545067, 'total_duration': 20830.934995889664, 'accumulated_submission_time': 19370.83881545067, 'accumulated_eval_time': 1455.2440390586853, 'accumulated_logging_time': 2.5197227001190186}
I0308 10:48:27.927525 139708407461632 logging_writer.py:48] [43492] accumulated_eval_time=1455.244039, accumulated_logging_time=2.519723, accumulated_submission_time=19370.838815, global_step=43492, preemption_count=0, score=19370.838815, test/accuracy=0.482200, test/loss=2.318098, test/num_examples=10000, total_duration=20830.934996, train/accuracy=0.647148, train/loss=1.473313, validation/accuracy=0.603180, validation/loss=1.673744, validation/num_examples=50000
I0308 10:48:31.474908 139708415854336 logging_writer.py:48] [43500] global_step=43500, grad_norm=1.239540696144104, loss=2.619631290435791
I0308 10:49:11.945770 139708407461632 logging_writer.py:48] [43600] global_step=43600, grad_norm=1.1094915866851807, loss=3.3555614948272705
I0308 10:49:57.186503 139708415854336 logging_writer.py:48] [43700] global_step=43700, grad_norm=1.17647385597229, loss=2.6009364128112793
I0308 10:50:43.010154 139708407461632 logging_writer.py:48] [43800] global_step=43800, grad_norm=1.220405101776123, loss=2.440469264984131
I0308 10:51:28.537005 139708415854336 logging_writer.py:48] [43900] global_step=43900, grad_norm=1.0774468183517456, loss=4.916134834289551
I0308 10:52:13.789488 139708407461632 logging_writer.py:48] [44000] global_step=44000, grad_norm=1.02753746509552, loss=4.252870082855225
I0308 10:52:58.896168 139708415854336 logging_writer.py:48] [44100] global_step=44100, grad_norm=1.029089331626892, loss=5.022511959075928
I0308 10:53:43.845323 139708407461632 logging_writer.py:48] [44200] global_step=44200, grad_norm=1.309453010559082, loss=2.456468343734741
I0308 10:54:29.075215 139708415854336 logging_writer.py:48] [44300] global_step=44300, grad_norm=1.0322588682174683, loss=3.2139766216278076
I0308 10:55:14.399973 139708407461632 logging_writer.py:48] [44400] global_step=44400, grad_norm=1.0163072347640991, loss=4.877758502960205
I0308 10:55:28.103997 139902746892096 spec.py:321] Evaluating on the training split.
I0308 10:55:39.506383 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 10:56:00.819546 139902746892096 spec.py:349] Evaluating on the test split.
I0308 10:56:02.494698 139902746892096 submission_runner.py:411] Time since start: 21285.52s, 	Step: 44432, 	{'train/accuracy': 0.6621484160423279, 'train/loss': 1.3826016187667847, 'validation/accuracy': 0.6111400127410889, 'validation/loss': 1.6152799129486084, 'validation/num_examples': 50000, 'test/accuracy': 0.4879000186920166, 'test/loss': 2.2897379398345947, 'test/num_examples': 10000, 'score': 19790.954607486725, 'total_duration': 21285.523493766785, 'accumulated_submission_time': 19790.954607486725, 'accumulated_eval_time': 1489.634738445282, 'accumulated_logging_time': 2.552811861038208}
I0308 10:56:02.516137 139708415854336 logging_writer.py:48] [44432] accumulated_eval_time=1489.634738, accumulated_logging_time=2.552812, accumulated_submission_time=19790.954607, global_step=44432, preemption_count=0, score=19790.954607, test/accuracy=0.487900, test/loss=2.289738, test/num_examples=10000, total_duration=21285.523494, train/accuracy=0.662148, train/loss=1.382602, validation/accuracy=0.611140, validation/loss=1.615280, validation/num_examples=50000
I0308 10:56:29.681799 139708407461632 logging_writer.py:48] [44500] global_step=44500, grad_norm=1.2633945941925049, loss=2.565458297729492
I0308 10:57:13.306424 139708415854336 logging_writer.py:48] [44600] global_step=44600, grad_norm=1.215968132019043, loss=2.4465889930725098
I0308 10:57:58.602497 139708407461632 logging_writer.py:48] [44700] global_step=44700, grad_norm=1.0568007230758667, loss=3.674177408218384
I0308 10:58:43.883642 139708415854336 logging_writer.py:48] [44800] global_step=44800, grad_norm=1.1776666641235352, loss=2.3075108528137207
I0308 10:59:28.889922 139708407461632 logging_writer.py:48] [44900] global_step=44900, grad_norm=1.2534745931625366, loss=2.4926984310150146
I0308 11:00:14.067503 139708415854336 logging_writer.py:48] [45000] global_step=45000, grad_norm=1.249985933303833, loss=5.076751708984375
I0308 11:00:59.445631 139708407461632 logging_writer.py:48] [45100] global_step=45100, grad_norm=1.0126314163208008, loss=2.947279453277588
I0308 11:01:44.393536 139708415854336 logging_writer.py:48] [45200] global_step=45200, grad_norm=1.2102243900299072, loss=2.6389007568359375
I0308 11:02:29.547098 139708407461632 logging_writer.py:48] [45300] global_step=45300, grad_norm=1.185034155845642, loss=2.6375584602355957
I0308 11:03:02.925336 139902746892096 spec.py:321] Evaluating on the training split.
I0308 11:03:14.430965 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 11:03:34.550342 139902746892096 spec.py:349] Evaluating on the test split.
I0308 11:03:36.244517 139902746892096 submission_runner.py:411] Time since start: 21739.27s, 	Step: 45376, 	{'train/accuracy': 0.6607617139816284, 'train/loss': 1.3963559865951538, 'validation/accuracy': 0.6053199768066406, 'validation/loss': 1.6574883460998535, 'validation/num_examples': 50000, 'test/accuracy': 0.48420003056526184, 'test/loss': 2.310580015182495, 'test/num_examples': 10000, 'score': 20211.304652929306, 'total_duration': 21739.273277044296, 'accumulated_submission_time': 20211.304652929306, 'accumulated_eval_time': 1522.9538979530334, 'accumulated_logging_time': 2.5838775634765625}
I0308 11:03:36.269439 139708415854336 logging_writer.py:48] [45376] accumulated_eval_time=1522.953898, accumulated_logging_time=2.583878, accumulated_submission_time=20211.304653, global_step=45376, preemption_count=0, score=20211.304653, test/accuracy=0.484200, test/loss=2.310580, test/num_examples=10000, total_duration=21739.273277, train/accuracy=0.660762, train/loss=1.396356, validation/accuracy=0.605320, validation/loss=1.657488, validation/num_examples=50000
I0308 11:03:46.156733 139708407461632 logging_writer.py:48] [45400] global_step=45400, grad_norm=1.2230749130249023, loss=2.4318604469299316
I0308 11:04:28.463314 139708415854336 logging_writer.py:48] [45500] global_step=45500, grad_norm=1.0734516382217407, loss=3.4590015411376953
I0308 11:05:13.625905 139708407461632 logging_writer.py:48] [45600] global_step=45600, grad_norm=1.0760533809661865, loss=3.818380117416382
I0308 11:05:58.749091 139708415854336 logging_writer.py:48] [45700] global_step=45700, grad_norm=1.1891405582427979, loss=2.5941338539123535
I0308 11:06:43.946115 139708407461632 logging_writer.py:48] [45800] global_step=45800, grad_norm=1.1443620920181274, loss=4.200315952301025
I0308 11:07:29.115016 139708415854336 logging_writer.py:48] [45900] global_step=45900, grad_norm=1.0903211832046509, loss=4.898255348205566
I0308 11:08:14.377404 139708407461632 logging_writer.py:48] [46000] global_step=46000, grad_norm=1.3509845733642578, loss=4.359635829925537
I0308 11:08:59.634263 139708415854336 logging_writer.py:48] [46100] global_step=46100, grad_norm=1.2988148927688599, loss=2.4238696098327637
I0308 11:09:44.655745 139708407461632 logging_writer.py:48] [46200] global_step=46200, grad_norm=1.3317166566848755, loss=2.781451463699341
I0308 11:10:29.820583 139708415854336 logging_writer.py:48] [46300] global_step=46300, grad_norm=1.1567907333374023, loss=2.6019351482391357
I0308 11:10:36.424611 139902746892096 spec.py:321] Evaluating on the training split.
I0308 11:10:47.527372 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 11:11:10.379770 139902746892096 spec.py:349] Evaluating on the test split.
I0308 11:11:12.055377 139902746892096 submission_runner.py:411] Time since start: 22195.08s, 	Step: 46316, 	{'train/accuracy': 0.6971093416213989, 'train/loss': 1.220287799835205, 'validation/accuracy': 0.6148999929428101, 'validation/loss': 1.5943645238876343, 'validation/num_examples': 50000, 'test/accuracy': 0.496800035238266, 'test/loss': 2.2552144527435303, 'test/num_examples': 10000, 'score': 20631.39994740486, 'total_duration': 22195.084174633026, 'accumulated_submission_time': 20631.39994740486, 'accumulated_eval_time': 1558.584661245346, 'accumulated_logging_time': 2.618739366531372}
I0308 11:11:12.080672 139708407461632 logging_writer.py:48] [46316] accumulated_eval_time=1558.584661, accumulated_logging_time=2.618739, accumulated_submission_time=20631.399947, global_step=46316, preemption_count=0, score=20631.399947, test/accuracy=0.496800, test/loss=2.255214, test/num_examples=10000, total_duration=22195.084175, train/accuracy=0.697109, train/loss=1.220288, validation/accuracy=0.614900, validation/loss=1.594365, validation/num_examples=50000
I0308 11:11:45.537614 139708415854336 logging_writer.py:48] [46400] global_step=46400, grad_norm=1.2018567323684692, loss=3.443605661392212
I0308 11:12:30.882336 139708407461632 logging_writer.py:48] [46500] global_step=46500, grad_norm=1.1673200130462646, loss=3.7226905822753906
I0308 11:13:16.423454 139708415854336 logging_writer.py:48] [46600] global_step=46600, grad_norm=1.2219407558441162, loss=2.510070562362671
I0308 11:14:01.981537 139708407461632 logging_writer.py:48] [46700] global_step=46700, grad_norm=1.2503070831298828, loss=2.2755138874053955
I0308 11:14:47.450100 139708415854336 logging_writer.py:48] [46800] global_step=46800, grad_norm=1.1464214324951172, loss=2.8527326583862305
I0308 11:15:33.053630 139708407461632 logging_writer.py:48] [46900] global_step=46900, grad_norm=1.1636444330215454, loss=2.3658344745635986
I0308 11:16:18.368284 139708415854336 logging_writer.py:48] [47000] global_step=47000, grad_norm=1.2450270652770996, loss=2.478299617767334
I0308 11:17:03.913467 139708407461632 logging_writer.py:48] [47100] global_step=47100, grad_norm=1.1652677059173584, loss=2.7140376567840576
I0308 11:17:49.317018 139708415854336 logging_writer.py:48] [47200] global_step=47200, grad_norm=1.100124716758728, loss=3.780491590499878
I0308 11:18:12.192512 139902746892096 spec.py:321] Evaluating on the training split.
I0308 11:18:23.505470 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 11:18:43.878922 139902746892096 spec.py:349] Evaluating on the test split.
I0308 11:18:45.573733 139902746892096 submission_runner.py:411] Time since start: 22648.60s, 	Step: 47252, 	{'train/accuracy': 0.6590625047683716, 'train/loss': 1.4004347324371338, 'validation/accuracy': 0.6109799742698669, 'validation/loss': 1.6322163343429565, 'validation/num_examples': 50000, 'test/accuracy': 0.4879000186920166, 'test/loss': 2.2941040992736816, 'test/num_examples': 10000, 'score': 21051.45190000534, 'total_duration': 22648.6025223732, 'accumulated_submission_time': 21051.45190000534, 'accumulated_eval_time': 1591.965892314911, 'accumulated_logging_time': 2.6543779373168945}
I0308 11:18:45.599233 139708407461632 logging_writer.py:48] [47252] accumulated_eval_time=1591.965892, accumulated_logging_time=2.654378, accumulated_submission_time=21051.451900, global_step=47252, preemption_count=0, score=21051.451900, test/accuracy=0.487900, test/loss=2.294104, test/num_examples=10000, total_duration=22648.602522, train/accuracy=0.659063, train/loss=1.400435, validation/accuracy=0.610980, validation/loss=1.632216, validation/num_examples=50000
I0308 11:19:05.012526 139708415854336 logging_writer.py:48] [47300] global_step=47300, grad_norm=1.227242112159729, loss=2.3559248447418213
I0308 11:19:47.840126 139708407461632 logging_writer.py:48] [47400] global_step=47400, grad_norm=1.151220440864563, loss=2.446293830871582
I0308 11:20:33.168882 139708415854336 logging_writer.py:48] [47500] global_step=47500, grad_norm=1.2492494583129883, loss=2.8410964012145996
I0308 11:21:18.551150 139708407461632 logging_writer.py:48] [47600] global_step=47600, grad_norm=1.1049813032150269, loss=4.724521636962891
I0308 11:22:03.591635 139708415854336 logging_writer.py:48] [47700] global_step=47700, grad_norm=1.1373323202133179, loss=2.369471788406372
I0308 11:22:49.052314 139708407461632 logging_writer.py:48] [47800] global_step=47800, grad_norm=1.1932064294815063, loss=2.610074281692505
I0308 11:23:34.380168 139708415854336 logging_writer.py:48] [47900] global_step=47900, grad_norm=1.0914843082427979, loss=3.582651138305664
I0308 11:24:19.596897 139708407461632 logging_writer.py:48] [48000] global_step=48000, grad_norm=1.2094472646713257, loss=2.5156381130218506
I0308 11:25:04.955192 139708415854336 logging_writer.py:48] [48100] global_step=48100, grad_norm=1.3508094549179077, loss=2.4672927856445312
I0308 11:25:45.746711 139902746892096 spec.py:321] Evaluating on the training split.
I0308 11:25:57.360079 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 11:26:17.950984 139902746892096 spec.py:349] Evaluating on the test split.
I0308 11:26:19.642407 139902746892096 submission_runner.py:411] Time since start: 23102.67s, 	Step: 48192, 	{'train/accuracy': 0.6631835699081421, 'train/loss': 1.3827235698699951, 'validation/accuracy': 0.6143999695777893, 'validation/loss': 1.6168955564498901, 'validation/num_examples': 50000, 'test/accuracy': 0.48500001430511475, 'test/loss': 2.307790756225586, 'test/num_examples': 10000, 'score': 21471.537014484406, 'total_duration': 23102.671187400818, 'accumulated_submission_time': 21471.537014484406, 'accumulated_eval_time': 1625.8616099357605, 'accumulated_logging_time': 2.691848039627075}
I0308 11:26:19.667560 139708407461632 logging_writer.py:48] [48192] accumulated_eval_time=1625.861610, accumulated_logging_time=2.691848, accumulated_submission_time=21471.537014, global_step=48192, preemption_count=0, score=21471.537014, test/accuracy=0.485000, test/loss=2.307791, test/num_examples=10000, total_duration=23102.671187, train/accuracy=0.663184, train/loss=1.382724, validation/accuracy=0.614400, validation/loss=1.616896, validation/num_examples=50000
I0308 11:26:23.224646 139708415854336 logging_writer.py:48] [48200] global_step=48200, grad_norm=1.0237219333648682, loss=4.312592029571533
I0308 11:27:04.346919 139708407461632 logging_writer.py:48] [48300] global_step=48300, grad_norm=1.304045557975769, loss=4.634300708770752
I0308 11:27:49.433129 139708415854336 logging_writer.py:48] [48400] global_step=48400, grad_norm=1.2038971185684204, loss=2.5811357498168945
I0308 11:28:34.582477 139708407461632 logging_writer.py:48] [48500] global_step=48500, grad_norm=1.166440725326538, loss=3.003749370574951
I0308 11:29:19.981105 139708415854336 logging_writer.py:48] [48600] global_step=48600, grad_norm=1.3758151531219482, loss=2.479336738586426
I0308 11:30:04.893352 139708407461632 logging_writer.py:48] [48700] global_step=48700, grad_norm=1.1995158195495605, loss=2.8415470123291016
I0308 11:30:50.284611 139708415854336 logging_writer.py:48] [48800] global_step=48800, grad_norm=1.016573429107666, loss=4.9678497314453125
I0308 11:31:35.418275 139708407461632 logging_writer.py:48] [48900] global_step=48900, grad_norm=1.2242562770843506, loss=2.340991497039795
I0308 11:32:20.551782 139708415854336 logging_writer.py:48] [49000] global_step=49000, grad_norm=1.0842605829238892, loss=4.740670204162598
I0308 11:33:05.844687 139708407461632 logging_writer.py:48] [49100] global_step=49100, grad_norm=1.2690573930740356, loss=2.5190587043762207
I0308 11:33:19.975081 139902746892096 spec.py:321] Evaluating on the training split.
I0308 11:33:31.477914 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 11:33:54.231158 139902746892096 spec.py:349] Evaluating on the test split.
I0308 11:33:55.906171 139902746892096 submission_runner.py:411] Time since start: 23558.93s, 	Step: 49133, 	{'train/accuracy': 0.6769726276397705, 'train/loss': 1.3200808763504028, 'validation/accuracy': 0.6147800087928772, 'validation/loss': 1.6114550828933716, 'validation/num_examples': 50000, 'test/accuracy': 0.490200012922287, 'test/loss': 2.286827802658081, 'test/num_examples': 10000, 'score': 21891.7851896286, 'total_duration': 23558.934968948364, 'accumulated_submission_time': 21891.7851896286, 'accumulated_eval_time': 1661.7927005290985, 'accumulated_logging_time': 2.726196765899658}
I0308 11:33:55.928591 139708415854336 logging_writer.py:48] [49133] accumulated_eval_time=1661.792701, accumulated_logging_time=2.726197, accumulated_submission_time=21891.785190, global_step=49133, preemption_count=0, score=21891.785190, test/accuracy=0.490200, test/loss=2.286828, test/num_examples=10000, total_duration=23558.934969, train/accuracy=0.676973, train/loss=1.320081, validation/accuracy=0.614780, validation/loss=1.611455, validation/num_examples=50000
I0308 11:34:22.698132 139708407461632 logging_writer.py:48] [49200] global_step=49200, grad_norm=1.1080663204193115, loss=4.6167097091674805
I0308 11:35:06.519268 139708415854336 logging_writer.py:48] [49300] global_step=49300, grad_norm=1.1822106838226318, loss=2.570986747741699
I0308 11:35:51.736204 139708407461632 logging_writer.py:48] [49400] global_step=49400, grad_norm=1.2200154066085815, loss=2.450174331665039
I0308 11:36:37.249325 139708415854336 logging_writer.py:48] [49500] global_step=49500, grad_norm=1.196699857711792, loss=2.615812301635742
I0308 11:37:22.178522 139708407461632 logging_writer.py:48] [49600] global_step=49600, grad_norm=1.4233031272888184, loss=2.5120928287506104
I0308 11:38:07.574288 139708415854336 logging_writer.py:48] [49700] global_step=49700, grad_norm=1.086498737335205, loss=2.6101980209350586
I0308 11:38:52.998531 139708407461632 logging_writer.py:48] [49800] global_step=49800, grad_norm=1.04911470413208, loss=3.460216999053955
I0308 11:39:38.028275 139708415854336 logging_writer.py:48] [49900] global_step=49900, grad_norm=1.1533019542694092, loss=4.085174560546875
I0308 11:40:23.197976 139708407461632 logging_writer.py:48] [50000] global_step=50000, grad_norm=1.237014651298523, loss=2.371455669403076
I0308 11:40:56.234099 139902746892096 spec.py:321] Evaluating on the training split.
I0308 11:41:07.664006 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 11:41:28.728260 139902746892096 spec.py:349] Evaluating on the test split.
I0308 11:41:30.409995 139902746892096 submission_runner.py:411] Time since start: 24013.44s, 	Step: 50074, 	{'train/accuracy': 0.6643164157867432, 'train/loss': 1.3614779710769653, 'validation/accuracy': 0.6191999912261963, 'validation/loss': 1.578934907913208, 'validation/num_examples': 50000, 'test/accuracy': 0.4976000189781189, 'test/loss': 2.2456860542297363, 'test/num_examples': 10000, 'score': 22312.02948999405, 'total_duration': 24013.438791275024, 'accumulated_submission_time': 22312.02948999405, 'accumulated_eval_time': 1695.9685862064362, 'accumulated_logging_time': 2.759733200073242}
I0308 11:41:30.431963 139708415854336 logging_writer.py:48] [50074] accumulated_eval_time=1695.968586, accumulated_logging_time=2.759733, accumulated_submission_time=22312.029490, global_step=50074, preemption_count=0, score=22312.029490, test/accuracy=0.497600, test/loss=2.245686, test/num_examples=10000, total_duration=24013.438791, train/accuracy=0.664316, train/loss=1.361478, validation/accuracy=0.619200, validation/loss=1.578935, validation/num_examples=50000
I0308 11:41:41.066597 139708407461632 logging_writer.py:48] [50100] global_step=50100, grad_norm=1.0547512769699097, loss=4.8828043937683105
I0308 11:42:23.104666 139708415854336 logging_writer.py:48] [50200] global_step=50200, grad_norm=1.065658450126648, loss=4.934847831726074
I0308 11:43:08.253179 139708407461632 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.9925435781478882, loss=3.9930715560913086
I0308 11:43:53.733917 139708415854336 logging_writer.py:48] [50400] global_step=50400, grad_norm=1.337172269821167, loss=2.4898648262023926
I0308 11:44:38.933287 139708407461632 logging_writer.py:48] [50500] global_step=50500, grad_norm=1.3080116510391235, loss=5.030027389526367
I0308 11:45:24.159371 139708415854336 logging_writer.py:48] [50600] global_step=50600, grad_norm=1.3557683229446411, loss=2.4946439266204834
I0308 11:46:09.582930 139708407461632 logging_writer.py:48] [50700] global_step=50700, grad_norm=1.1918126344680786, loss=2.660005569458008
I0308 11:46:54.685940 139708415854336 logging_writer.py:48] [50800] global_step=50800, grad_norm=1.2442615032196045, loss=2.330352306365967
I0308 11:47:40.360101 139708407461632 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.9773535132408142, loss=4.500081539154053
I0308 11:48:25.408087 139708415854336 logging_writer.py:48] [51000] global_step=51000, grad_norm=1.141625165939331, loss=4.050414562225342
I0308 11:48:30.590232 139902746892096 spec.py:321] Evaluating on the training split.
I0308 11:48:42.100849 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 11:49:01.522565 139902746892096 spec.py:349] Evaluating on the test split.
I0308 11:49:03.206506 139902746892096 submission_runner.py:411] Time since start: 24466.24s, 	Step: 51013, 	{'train/accuracy': 0.6668554544448853, 'train/loss': 1.3606523275375366, 'validation/accuracy': 0.6202799677848816, 'validation/loss': 1.5822699069976807, 'validation/num_examples': 50000, 'test/accuracy': 0.49640002846717834, 'test/loss': 2.2490456104278564, 'test/num_examples': 10000, 'score': 22732.127825021744, 'total_duration': 24466.235282182693, 'accumulated_submission_time': 22732.127825021744, 'accumulated_eval_time': 1728.5848369598389, 'accumulated_logging_time': 2.7923452854156494}
I0308 11:49:03.238190 139708407461632 logging_writer.py:48] [51013] accumulated_eval_time=1728.584837, accumulated_logging_time=2.792345, accumulated_submission_time=22732.127825, global_step=51013, preemption_count=0, score=22732.127825, test/accuracy=0.496400, test/loss=2.249046, test/num_examples=10000, total_duration=24466.235282, train/accuracy=0.666855, train/loss=1.360652, validation/accuracy=0.620280, validation/loss=1.582270, validation/num_examples=50000
I0308 11:49:38.626129 139708415854336 logging_writer.py:48] [51100] global_step=51100, grad_norm=1.0774130821228027, loss=4.9953813552856445
I0308 11:50:23.763684 139708407461632 logging_writer.py:48] [51200] global_step=51200, grad_norm=1.202219009399414, loss=2.5656118392944336
I0308 11:51:09.278474 139708415854336 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.9698231816291809, loss=4.697223663330078
I0308 11:51:54.590400 139708407461632 logging_writer.py:48] [51400] global_step=51400, grad_norm=1.115340232849121, loss=2.6481552124023438
I0308 11:52:39.763322 139708415854336 logging_writer.py:48] [51500] global_step=51500, grad_norm=1.2870092391967773, loss=2.4221017360687256
I0308 11:53:24.868996 139708407461632 logging_writer.py:48] [51600] global_step=51600, grad_norm=1.106360912322998, loss=4.709108829498291
I0308 11:54:09.970512 139708415854336 logging_writer.py:48] [51700] global_step=51700, grad_norm=1.1670832633972168, loss=2.8281350135803223
I0308 11:54:55.389309 139708407461632 logging_writer.py:48] [51800] global_step=51800, grad_norm=1.0386500358581543, loss=3.0029871463775635
I0308 11:55:40.690678 139708415854336 logging_writer.py:48] [51900] global_step=51900, grad_norm=1.091753363609314, loss=2.6834630966186523
I0308 11:56:03.237694 139902746892096 spec.py:321] Evaluating on the training split.
I0308 11:56:14.588715 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 11:56:36.278507 139902746892096 spec.py:349] Evaluating on the test split.
I0308 11:56:37.964563 139902746892096 submission_runner.py:411] Time since start: 24920.99s, 	Step: 51952, 	{'train/accuracy': 0.6714843511581421, 'train/loss': 1.339386224746704, 'validation/accuracy': 0.6193999648094177, 'validation/loss': 1.5890146493911743, 'validation/num_examples': 50000, 'test/accuracy': 0.4962000250816345, 'test/loss': 2.252509593963623, 'test/num_examples': 10000, 'score': 23152.065212011337, 'total_duration': 24920.99333834648, 'accumulated_submission_time': 23152.065212011337, 'accumulated_eval_time': 1763.3116884231567, 'accumulated_logging_time': 2.836010217666626}
I0308 11:56:37.989722 139708407461632 logging_writer.py:48] [51952] accumulated_eval_time=1763.311688, accumulated_logging_time=2.836010, accumulated_submission_time=23152.065212, global_step=51952, preemption_count=0, score=23152.065212, test/accuracy=0.496200, test/loss=2.252510, test/num_examples=10000, total_duration=24920.993338, train/accuracy=0.671484, train/loss=1.339386, validation/accuracy=0.619400, validation/loss=1.589015, validation/num_examples=50000
I0308 11:56:57.285222 139708415854336 logging_writer.py:48] [52000] global_step=52000, grad_norm=1.2520660161972046, loss=2.464590549468994
I0308 11:57:39.563787 139708407461632 logging_writer.py:48] [52100] global_step=52100, grad_norm=1.3108264207839966, loss=2.36946964263916
I0308 11:58:24.567527 139708415854336 logging_writer.py:48] [52200] global_step=52200, grad_norm=1.3194409608840942, loss=2.3260185718536377
I0308 11:59:09.848239 139708407461632 logging_writer.py:48] [52300] global_step=52300, grad_norm=1.3227465152740479, loss=2.376415491104126
I0308 11:59:54.941658 139708415854336 logging_writer.py:48] [52400] global_step=52400, grad_norm=1.258744716644287, loss=2.270505428314209
I0308 12:00:39.926666 139708407461632 logging_writer.py:48] [52500] global_step=52500, grad_norm=1.2833032608032227, loss=2.390317678451538
I0308 12:01:25.414935 139708415854336 logging_writer.py:48] [52600] global_step=52600, grad_norm=1.2525088787078857, loss=2.2398366928100586
I0308 12:02:10.559019 139708407461632 logging_writer.py:48] [52700] global_step=52700, grad_norm=1.378022313117981, loss=2.41853666305542
I0308 12:02:55.659135 139708415854336 logging_writer.py:48] [52800] global_step=52800, grad_norm=1.248206615447998, loss=2.386038064956665
I0308 12:03:38.212609 139902746892096 spec.py:321] Evaluating on the training split.
I0308 12:03:49.511861 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 12:04:09.916660 139902746892096 spec.py:349] Evaluating on the test split.
I0308 12:04:11.610402 139902746892096 submission_runner.py:411] Time since start: 25374.64s, 	Step: 52896, 	{'train/accuracy': 0.6980078220367432, 'train/loss': 1.2068614959716797, 'validation/accuracy': 0.6241999864578247, 'validation/loss': 1.5524660348892212, 'validation/num_examples': 50000, 'test/accuracy': 0.5056000351905823, 'test/loss': 2.2364532947540283, 'test/num_examples': 10000, 'score': 23572.228355884552, 'total_duration': 25374.639178276062, 'accumulated_submission_time': 23572.228355884552, 'accumulated_eval_time': 1796.709459066391, 'accumulated_logging_time': 2.8703958988189697}
I0308 12:04:11.638041 139708407461632 logging_writer.py:48] [52896] accumulated_eval_time=1796.709459, accumulated_logging_time=2.870396, accumulated_submission_time=23572.228356, global_step=52896, preemption_count=0, score=23572.228356, test/accuracy=0.505600, test/loss=2.236453, test/num_examples=10000, total_duration=25374.639178, train/accuracy=0.698008, train/loss=1.206861, validation/accuracy=0.624200, validation/loss=1.552466, validation/num_examples=50000
I0308 12:04:13.629520 139708415854336 logging_writer.py:48] [52900] global_step=52900, grad_norm=1.544138789176941, loss=2.304640054702759
I0308 12:04:54.082624 139708407461632 logging_writer.py:48] [53000] global_step=53000, grad_norm=1.2813776731491089, loss=2.3085670471191406
I0308 12:05:39.455307 139708415854336 logging_writer.py:48] [53100] global_step=53100, grad_norm=1.1757081747055054, loss=2.7356667518615723
I0308 12:06:24.960080 139708407461632 logging_writer.py:48] [53200] global_step=53200, grad_norm=1.211880087852478, loss=3.4427435398101807
I0308 12:07:10.336136 139708415854336 logging_writer.py:48] [53300] global_step=53300, grad_norm=1.2980823516845703, loss=2.3363800048828125
I0308 12:07:55.831289 139708407461632 logging_writer.py:48] [53400] global_step=53400, grad_norm=1.1720761060714722, loss=2.3330254554748535
I0308 12:08:41.322522 139708415854336 logging_writer.py:48] [53500] global_step=53500, grad_norm=1.3598549365997314, loss=2.470763921737671
I0308 12:09:26.869341 139708407461632 logging_writer.py:48] [53600] global_step=53600, grad_norm=1.3687182664871216, loss=2.32999324798584
I0308 12:10:12.011831 139708415854336 logging_writer.py:48] [53700] global_step=53700, grad_norm=1.2279531955718994, loss=2.6116185188293457
I0308 12:10:57.524832 139708407461632 logging_writer.py:48] [53800] global_step=53800, grad_norm=1.2256317138671875, loss=4.49227237701416
I0308 12:11:11.704238 139902746892096 spec.py:321] Evaluating on the training split.
I0308 12:11:23.361086 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 12:11:45.060547 139902746892096 spec.py:349] Evaluating on the test split.
I0308 12:11:46.736375 139902746892096 submission_runner.py:411] Time since start: 25829.77s, 	Step: 53833, 	{'train/accuracy': 0.6648241877555847, 'train/loss': 1.4142321348190308, 'validation/accuracy': 0.6174799799919128, 'validation/loss': 1.6313257217407227, 'validation/num_examples': 50000, 'test/accuracy': 0.4935000240802765, 'test/loss': 2.286892890930176, 'test/num_examples': 10000, 'score': 23992.235177993774, 'total_duration': 25829.765172481537, 'accumulated_submission_time': 23992.235177993774, 'accumulated_eval_time': 1831.7415869235992, 'accumulated_logging_time': 2.908019781112671}
I0308 12:11:46.763179 139708415854336 logging_writer.py:48] [53833] accumulated_eval_time=1831.741587, accumulated_logging_time=2.908020, accumulated_submission_time=23992.235178, global_step=53833, preemption_count=0, score=23992.235178, test/accuracy=0.493500, test/loss=2.286893, test/num_examples=10000, total_duration=25829.765172, train/accuracy=0.664824, train/loss=1.414232, validation/accuracy=0.617480, validation/loss=1.631326, validation/num_examples=50000
I0308 12:12:13.525623 139708407461632 logging_writer.py:48] [53900] global_step=53900, grad_norm=1.1648608446121216, loss=2.6515300273895264
I0308 12:12:57.386653 139708415854336 logging_writer.py:48] [54000] global_step=54000, grad_norm=1.206752061843872, loss=4.710539817810059
I0308 12:13:42.991563 139708407461632 logging_writer.py:48] [54100] global_step=54100, grad_norm=1.1255207061767578, loss=3.172581911087036
I0308 12:14:28.458462 139708415854336 logging_writer.py:48] [54200] global_step=54200, grad_norm=1.178368091583252, loss=2.710549831390381
I0308 12:15:13.691011 139708407461632 logging_writer.py:48] [54300] global_step=54300, grad_norm=1.2739351987838745, loss=2.2921042442321777
I0308 12:15:59.123237 139708415854336 logging_writer.py:48] [54400] global_step=54400, grad_norm=1.2703781127929688, loss=2.4791440963745117
I0308 12:16:44.604049 139708407461632 logging_writer.py:48] [54500] global_step=54500, grad_norm=1.4145548343658447, loss=2.4077234268188477
I0308 12:17:29.843501 139708415854336 logging_writer.py:48] [54600] global_step=54600, grad_norm=1.0144996643066406, loss=4.796123504638672
I0308 12:18:15.090544 139708407461632 logging_writer.py:48] [54700] global_step=54700, grad_norm=1.3309669494628906, loss=2.6749041080474854
I0308 12:18:46.881449 139902746892096 spec.py:321] Evaluating on the training split.
I0308 12:18:58.268750 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 12:19:19.189685 139902746892096 spec.py:349] Evaluating on the test split.
I0308 12:19:20.860754 139902746892096 submission_runner.py:411] Time since start: 26283.89s, 	Step: 54772, 	{'train/accuracy': 0.6703906059265137, 'train/loss': 1.3580645322799683, 'validation/accuracy': 0.6202799677848816, 'validation/loss': 1.596779227256775, 'validation/num_examples': 50000, 'test/accuracy': 0.49900001287460327, 'test/loss': 2.2520761489868164, 'test/num_examples': 10000, 'score': 24412.294757843018, 'total_duration': 26283.889540433884, 'accumulated_submission_time': 24412.294757843018, 'accumulated_eval_time': 1865.720906496048, 'accumulated_logging_time': 2.943570375442505}
I0308 12:19:20.884119 139708415854336 logging_writer.py:48] [54772] accumulated_eval_time=1865.720906, accumulated_logging_time=2.943570, accumulated_submission_time=24412.294758, global_step=54772, preemption_count=0, score=24412.294758, test/accuracy=0.499000, test/loss=2.252076, test/num_examples=10000, total_duration=26283.889540, train/accuracy=0.670391, train/loss=1.358065, validation/accuracy=0.620280, validation/loss=1.596779, validation/num_examples=50000
I0308 12:19:32.311226 139708407461632 logging_writer.py:48] [54800] global_step=54800, grad_norm=1.0918455123901367, loss=4.944497108459473
I0308 12:20:14.508296 139708415854336 logging_writer.py:48] [54900] global_step=54900, grad_norm=1.0868343114852905, loss=2.9010071754455566
I0308 12:20:59.689097 139708407461632 logging_writer.py:48] [55000] global_step=55000, grad_norm=1.3951454162597656, loss=2.348355293273926
I0308 12:21:45.474469 139708415854336 logging_writer.py:48] [55100] global_step=55100, grad_norm=1.1981513500213623, loss=2.589681625366211
I0308 12:22:30.710600 139708407461632 logging_writer.py:48] [55200] global_step=55200, grad_norm=1.2287839651107788, loss=2.3270492553710938
I0308 12:23:16.028661 139708415854336 logging_writer.py:48] [55300] global_step=55300, grad_norm=1.123283863067627, loss=4.128405570983887
I0308 12:24:01.618681 139708407461632 logging_writer.py:48] [55400] global_step=55400, grad_norm=1.2487149238586426, loss=2.3562028408050537
I0308 12:24:46.660730 139708415854336 logging_writer.py:48] [55500] global_step=55500, grad_norm=1.0533759593963623, loss=4.170585632324219
I0308 12:25:31.821842 139708407461632 logging_writer.py:48] [55600] global_step=55600, grad_norm=1.164144515991211, loss=2.8223628997802734
I0308 12:26:16.918775 139708415854336 logging_writer.py:48] [55700] global_step=55700, grad_norm=1.3386253118515015, loss=2.324432849884033
I0308 12:26:21.203362 139902746892096 spec.py:321] Evaluating on the training split.
I0308 12:26:32.800498 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 12:26:54.890467 139902746892096 spec.py:349] Evaluating on the test split.
I0308 12:26:56.561232 139902746892096 submission_runner.py:411] Time since start: 26739.59s, 	Step: 55711, 	{'train/accuracy': 0.6917187571525574, 'train/loss': 1.2428334951400757, 'validation/accuracy': 0.6261199712753296, 'validation/loss': 1.5562325716018677, 'validation/num_examples': 50000, 'test/accuracy': 0.5027000308036804, 'test/loss': 2.2339158058166504, 'test/num_examples': 10000, 'score': 24832.553787231445, 'total_duration': 26739.5900247097, 'accumulated_submission_time': 24832.553787231445, 'accumulated_eval_time': 1901.0787975788116, 'accumulated_logging_time': 2.9773149490356445}
I0308 12:26:56.584762 139708407461632 logging_writer.py:48] [55711] accumulated_eval_time=1901.078798, accumulated_logging_time=2.977315, accumulated_submission_time=24832.553787, global_step=55711, preemption_count=0, score=24832.553787, test/accuracy=0.502700, test/loss=2.233916, test/num_examples=10000, total_duration=26739.590025, train/accuracy=0.691719, train/loss=1.242833, validation/accuracy=0.626120, validation/loss=1.556233, validation/num_examples=50000
I0308 12:27:32.098555 139708415854336 logging_writer.py:48] [55800] global_step=55800, grad_norm=1.2311211824417114, loss=2.5171518325805664
I0308 12:28:17.152013 139708407461632 logging_writer.py:48] [55900] global_step=55900, grad_norm=1.1385936737060547, loss=3.2499642372131348
I0308 12:29:02.372288 139708415854336 logging_writer.py:48] [56000] global_step=56000, grad_norm=1.2081503868103027, loss=2.814004421234131
I0308 12:29:48.130211 139708407461632 logging_writer.py:48] [56100] global_step=56100, grad_norm=1.0612298250198364, loss=3.317904472351074
I0308 12:30:33.198265 139708415854336 logging_writer.py:48] [56200] global_step=56200, grad_norm=1.1639783382415771, loss=4.197887420654297
I0308 12:31:18.479457 139708407461632 logging_writer.py:48] [56300] global_step=56300, grad_norm=1.2657074928283691, loss=2.392643928527832
I0308 12:32:03.714649 139708415854336 logging_writer.py:48] [56400] global_step=56400, grad_norm=1.38680100440979, loss=2.334801197052002
I0308 12:32:48.797404 139708407461632 logging_writer.py:48] [56500] global_step=56500, grad_norm=1.2747511863708496, loss=2.2880706787109375
I0308 12:33:33.989764 139708415854336 logging_writer.py:48] [56600] global_step=56600, grad_norm=1.2035094499588013, loss=2.4762725830078125
I0308 12:33:56.809525 139902746892096 spec.py:321] Evaluating on the training split.
I0308 12:34:08.318351 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 12:34:31.343383 139902746892096 spec.py:349] Evaluating on the test split.
I0308 12:34:33.021485 139902746892096 submission_runner.py:411] Time since start: 27196.05s, 	Step: 56652, 	{'train/accuracy': 0.6736913919448853, 'train/loss': 1.337292194366455, 'validation/accuracy': 0.6258000135421753, 'validation/loss': 1.5741112232208252, 'validation/num_examples': 50000, 'test/accuracy': 0.501300036907196, 'test/loss': 2.226841449737549, 'test/num_examples': 10000, 'score': 25252.719858646393, 'total_duration': 27196.0502679348, 'accumulated_submission_time': 25252.719858646393, 'accumulated_eval_time': 1937.2907404899597, 'accumulated_logging_time': 3.00985050201416}
I0308 12:34:33.061148 139708407461632 logging_writer.py:48] [56652] accumulated_eval_time=1937.290740, accumulated_logging_time=3.009851, accumulated_submission_time=25252.719859, global_step=56652, preemption_count=0, score=25252.719859, test/accuracy=0.501300, test/loss=2.226841, test/num_examples=10000, total_duration=27196.050268, train/accuracy=0.673691, train/loss=1.337292, validation/accuracy=0.625800, validation/loss=1.574111, validation/num_examples=50000
I0308 12:34:52.368925 139708415854336 logging_writer.py:48] [56700] global_step=56700, grad_norm=1.0305548906326294, loss=4.8350396156311035
I0308 12:35:35.273954 139708407461632 logging_writer.py:48] [56800] global_step=56800, grad_norm=1.1800236701965332, loss=2.681947946548462
I0308 12:36:20.237680 139708415854336 logging_writer.py:48] [56900] global_step=56900, grad_norm=1.2667778730392456, loss=2.2767884731292725
I0308 12:37:05.667354 139708407461632 logging_writer.py:48] [57000] global_step=57000, grad_norm=1.3152215480804443, loss=2.2886929512023926
I0308 12:37:50.798197 139708415854336 logging_writer.py:48] [57100] global_step=57100, grad_norm=1.1302087306976318, loss=3.0941596031188965
I0308 12:38:35.940700 139708407461632 logging_writer.py:48] [57200] global_step=57200, grad_norm=1.2181087732315063, loss=2.2965636253356934
I0308 12:39:21.226588 139708415854336 logging_writer.py:48] [57300] global_step=57300, grad_norm=1.2657735347747803, loss=2.3102917671203613
I0308 12:40:06.363591 139708407461632 logging_writer.py:48] [57400] global_step=57400, grad_norm=1.1584744453430176, loss=3.8445043563842773
I0308 12:40:51.467569 139708415854336 logging_writer.py:48] [57500] global_step=57500, grad_norm=1.141652226448059, loss=4.059325218200684
I0308 12:41:33.093135 139902746892096 spec.py:321] Evaluating on the training split.
I0308 12:41:44.685107 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 12:42:04.269131 139902746892096 spec.py:349] Evaluating on the test split.
I0308 12:42:05.957499 139902746892096 submission_runner.py:411] Time since start: 27648.99s, 	Step: 57593, 	{'train/accuracy': 0.6827539205551147, 'train/loss': 1.2796573638916016, 'validation/accuracy': 0.629040002822876, 'validation/loss': 1.528598427772522, 'validation/num_examples': 50000, 'test/accuracy': 0.5081000328063965, 'test/loss': 2.189667224884033, 'test/num_examples': 10000, 'score': 25672.692500591278, 'total_duration': 27648.986284017563, 'accumulated_submission_time': 25672.692500591278, 'accumulated_eval_time': 1970.1550877094269, 'accumulated_logging_time': 3.059091567993164}
I0308 12:42:05.985421 139708407461632 logging_writer.py:48] [57593] accumulated_eval_time=1970.155088, accumulated_logging_time=3.059092, accumulated_submission_time=25672.692501, global_step=57593, preemption_count=0, score=25672.692501, test/accuracy=0.508100, test/loss=2.189667, test/num_examples=10000, total_duration=27648.986284, train/accuracy=0.682754, train/loss=1.279657, validation/accuracy=0.629040, validation/loss=1.528598, validation/num_examples=50000
I0308 12:42:09.140889 139708415854336 logging_writer.py:48] [57600] global_step=57600, grad_norm=1.3599380254745483, loss=2.3497729301452637
I0308 12:42:50.290513 139708407461632 logging_writer.py:48] [57700] global_step=57700, grad_norm=1.2289841175079346, loss=4.336433410644531
I0308 12:43:35.719156 139708415854336 logging_writer.py:48] [57800] global_step=57800, grad_norm=1.1579580307006836, loss=2.606102705001831
I0308 12:44:20.926602 139708407461632 logging_writer.py:48] [57900] global_step=57900, grad_norm=1.269853115081787, loss=2.854156494140625
I0308 12:45:06.544471 139708415854336 logging_writer.py:48] [58000] global_step=58000, grad_norm=1.2934575080871582, loss=2.3624062538146973
I0308 12:45:52.014873 139708407461632 logging_writer.py:48] [58100] global_step=58100, grad_norm=1.2369188070297241, loss=2.344465494155884
I0308 12:46:37.057748 139708415854336 logging_writer.py:48] [58200] global_step=58200, grad_norm=1.1472535133361816, loss=2.683590888977051
I0308 12:47:22.210534 139708407461632 logging_writer.py:48] [58300] global_step=58300, grad_norm=1.0996216535568237, loss=3.7288763523101807
I0308 12:48:07.874808 139708415854336 logging_writer.py:48] [58400] global_step=58400, grad_norm=1.2357513904571533, loss=2.4070582389831543
I0308 12:48:53.276288 139708407461632 logging_writer.py:48] [58500] global_step=58500, grad_norm=1.2603651285171509, loss=2.9830031394958496
I0308 12:49:06.172103 139902746892096 spec.py:321] Evaluating on the training split.
I0308 12:49:17.615634 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 12:49:39.008836 139902746892096 spec.py:349] Evaluating on the test split.
I0308 12:49:40.682440 139902746892096 submission_runner.py:411] Time since start: 28103.71s, 	Step: 58530, 	{'train/accuracy': 0.6944921612739563, 'train/loss': 1.2365962266921997, 'validation/accuracy': 0.6259999871253967, 'validation/loss': 1.5299229621887207, 'validation/num_examples': 50000, 'test/accuracy': 0.5099000334739685, 'test/loss': 2.191647529602051, 'test/num_examples': 10000, 'score': 26092.818296670914, 'total_duration': 28103.711234807968, 'accumulated_submission_time': 26092.818296670914, 'accumulated_eval_time': 2004.6654317378998, 'accumulated_logging_time': 3.09879994392395}
I0308 12:49:40.709367 139708415854336 logging_writer.py:48] [58530] accumulated_eval_time=2004.665432, accumulated_logging_time=3.098800, accumulated_submission_time=26092.818297, global_step=58530, preemption_count=0, score=26092.818297, test/accuracy=0.509900, test/loss=2.191648, test/num_examples=10000, total_duration=28103.711235, train/accuracy=0.694492, train/loss=1.236596, validation/accuracy=0.626000, validation/loss=1.529923, validation/num_examples=50000
I0308 12:50:08.688390 139708407461632 logging_writer.py:48] [58600] global_step=58600, grad_norm=1.2768299579620361, loss=4.698040962219238
I0308 12:50:52.827767 139708415854336 logging_writer.py:48] [58700] global_step=58700, grad_norm=1.166611909866333, loss=2.4796955585479736
I0308 12:51:38.066327 139708407461632 logging_writer.py:48] [58800] global_step=58800, grad_norm=1.1459604501724243, loss=2.3932831287384033
I0308 12:52:23.449953 139708415854336 logging_writer.py:48] [58900] global_step=58900, grad_norm=1.3046820163726807, loss=2.3903212547302246
I0308 12:53:08.530738 139708407461632 logging_writer.py:48] [59000] global_step=59000, grad_norm=1.2258535623550415, loss=3.853184461593628
I0308 12:53:53.828854 139708415854336 logging_writer.py:48] [59100] global_step=59100, grad_norm=1.0978095531463623, loss=3.425753593444824
I0308 12:54:39.182675 139708407461632 logging_writer.py:48] [59200] global_step=59200, grad_norm=1.1279428005218506, loss=3.8582863807678223
I0308 12:55:24.259807 139708415854336 logging_writer.py:48] [59300] global_step=59300, grad_norm=1.319904088973999, loss=2.2776689529418945
I0308 12:56:09.440484 139708407461632 logging_writer.py:48] [59400] global_step=59400, grad_norm=1.015825867652893, loss=3.9499614238739014
I0308 12:56:40.741776 139902746892096 spec.py:321] Evaluating on the training split.
I0308 12:56:52.013978 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 12:57:13.780549 139902746892096 spec.py:349] Evaluating on the test split.
I0308 12:57:15.457676 139902746892096 submission_runner.py:411] Time since start: 28558.49s, 	Step: 59471, 	{'train/accuracy': 0.7023242115974426, 'train/loss': 1.2232853174209595, 'validation/accuracy': 0.637499988079071, 'validation/loss': 1.5057755708694458, 'validation/num_examples': 50000, 'test/accuracy': 0.5143000483512878, 'test/loss': 2.179502487182617, 'test/num_examples': 10000, 'score': 26512.790625810623, 'total_duration': 28558.48644924164, 'accumulated_submission_time': 26512.790625810623, 'accumulated_eval_time': 2039.3813149929047, 'accumulated_logging_time': 3.1359636783599854}
I0308 12:57:15.485389 139708415854336 logging_writer.py:48] [59471] accumulated_eval_time=2039.381315, accumulated_logging_time=3.135964, accumulated_submission_time=26512.790626, global_step=59471, preemption_count=0, score=26512.790626, test/accuracy=0.514300, test/loss=2.179502, test/num_examples=10000, total_duration=28558.486449, train/accuracy=0.702324, train/loss=1.223285, validation/accuracy=0.637500, validation/loss=1.505776, validation/num_examples=50000
I0308 12:57:27.301675 139708407461632 logging_writer.py:48] [59500] global_step=59500, grad_norm=1.2450382709503174, loss=2.6752874851226807
I0308 12:58:08.602620 139708415854336 logging_writer.py:48] [59600] global_step=59600, grad_norm=1.3761438131332397, loss=2.196201801300049
I0308 12:58:53.733356 139708407461632 logging_writer.py:48] [59700] global_step=59700, grad_norm=1.2047680616378784, loss=2.390763282775879
I0308 12:59:38.897526 139708415854336 logging_writer.py:48] [59800] global_step=59800, grad_norm=1.1961499452590942, loss=2.315028667449951
I0308 13:00:24.183301 139708407461632 logging_writer.py:48] [59900] global_step=59900, grad_norm=1.307763934135437, loss=2.3094642162323
I0308 13:01:09.261935 139708415854336 logging_writer.py:48] [60000] global_step=60000, grad_norm=1.3523391485214233, loss=2.276219367980957
I0308 13:01:55.119954 139708407461632 logging_writer.py:48] [60100] global_step=60100, grad_norm=1.1729072332382202, loss=3.0480844974517822
I0308 13:02:40.503502 139708415854336 logging_writer.py:48] [60200] global_step=60200, grad_norm=1.1504958868026733, loss=3.0840280055999756
I0308 13:03:25.446328 139708407461632 logging_writer.py:48] [60300] global_step=60300, grad_norm=1.1563833951950073, loss=2.195887565612793
I0308 13:04:10.897778 139708415854336 logging_writer.py:48] [60400] global_step=60400, grad_norm=1.1288987398147583, loss=4.9893341064453125
I0308 13:04:15.481267 139902746892096 spec.py:321] Evaluating on the training split.
I0308 13:04:26.830580 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 13:04:47.244760 139902746892096 spec.py:349] Evaluating on the test split.
I0308 13:04:48.922382 139902746892096 submission_runner.py:411] Time since start: 29011.95s, 	Step: 60412, 	{'train/accuracy': 0.682324230670929, 'train/loss': 1.2863421440124512, 'validation/accuracy': 0.6322399973869324, 'validation/loss': 1.5168324708938599, 'validation/num_examples': 50000, 'test/accuracy': 0.510200023651123, 'test/loss': 2.1724071502685547, 'test/num_examples': 10000, 'score': 26932.726552009583, 'total_duration': 29011.95117688179, 'accumulated_submission_time': 26932.726552009583, 'accumulated_eval_time': 2072.8224205970764, 'accumulated_logging_time': 3.1741859912872314}
I0308 13:04:48.947567 139708407461632 logging_writer.py:48] [60412] accumulated_eval_time=2072.822421, accumulated_logging_time=3.174186, accumulated_submission_time=26932.726552, global_step=60412, preemption_count=0, score=26932.726552, test/accuracy=0.510200, test/loss=2.172407, test/num_examples=10000, total_duration=29011.951177, train/accuracy=0.682324, train/loss=1.286342, validation/accuracy=0.632240, validation/loss=1.516832, validation/num_examples=50000
I0308 13:05:24.047265 139708415854336 logging_writer.py:48] [60500] global_step=60500, grad_norm=1.2993698120117188, loss=2.110248565673828
I0308 13:06:08.967173 139708407461632 logging_writer.py:48] [60600] global_step=60600, grad_norm=1.123037576675415, loss=3.6522958278656006
I0308 13:06:54.303290 139708415854336 logging_writer.py:48] [60700] global_step=60700, grad_norm=1.1267658472061157, loss=4.929081439971924
I0308 13:07:39.877059 139708407461632 logging_writer.py:48] [60800] global_step=60800, grad_norm=1.117353916168213, loss=3.899909019470215
I0308 13:08:24.990238 139708415854336 logging_writer.py:48] [60900] global_step=60900, grad_norm=1.2893743515014648, loss=2.2939577102661133
I0308 13:09:10.053976 139708407461632 logging_writer.py:48] [61000] global_step=61000, grad_norm=1.1461282968521118, loss=3.5838308334350586
I0308 13:09:55.299909 139708415854336 logging_writer.py:48] [61100] global_step=61100, grad_norm=1.2639307975769043, loss=2.379707098007202
I0308 13:10:40.510783 139708407461632 logging_writer.py:48] [61200] global_step=61200, grad_norm=1.0882877111434937, loss=3.0186080932617188
I0308 13:11:26.135069 139708415854336 logging_writer.py:48] [61300] global_step=61300, grad_norm=1.2535090446472168, loss=3.122819662094116
I0308 13:11:49.073552 139902746892096 spec.py:321] Evaluating on the training split.
I0308 13:12:00.693365 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 13:12:23.133479 139902746892096 spec.py:349] Evaluating on the test split.
I0308 13:12:24.810169 139902746892096 submission_runner.py:411] Time since start: 29467.84s, 	Step: 61352, 	{'train/accuracy': 0.6830468773841858, 'train/loss': 1.3047661781311035, 'validation/accuracy': 0.6285399794578552, 'validation/loss': 1.5588743686676025, 'validation/num_examples': 50000, 'test/accuracy': 0.5085000395774841, 'test/loss': 2.2231738567352295, 'test/num_examples': 10000, 'score': 27352.794692516327, 'total_duration': 29467.838945388794, 'accumulated_submission_time': 27352.794692516327, 'accumulated_eval_time': 2108.5590019226074, 'accumulated_logging_time': 3.208047389984131}
I0308 13:12:24.841697 139708407461632 logging_writer.py:48] [61352] accumulated_eval_time=2108.559002, accumulated_logging_time=3.208047, accumulated_submission_time=27352.794693, global_step=61352, preemption_count=0, score=27352.794693, test/accuracy=0.508500, test/loss=2.223174, test/num_examples=10000, total_duration=29467.838945, train/accuracy=0.683047, train/loss=1.304766, validation/accuracy=0.628540, validation/loss=1.558874, validation/num_examples=50000
I0308 13:12:44.146083 139708415854336 logging_writer.py:48] [61400] global_step=61400, grad_norm=1.0904072523117065, loss=3.279330253601074
I0308 13:13:27.264133 139708407461632 logging_writer.py:48] [61500] global_step=61500, grad_norm=1.3558597564697266, loss=2.410060167312622
I0308 13:14:12.503252 139708415854336 logging_writer.py:48] [61600] global_step=61600, grad_norm=1.1656179428100586, loss=3.2571568489074707
I0308 13:14:57.734619 139708407461632 logging_writer.py:48] [61700] global_step=61700, grad_norm=1.3027069568634033, loss=2.2274301052093506
I0308 13:15:42.911692 139708415854336 logging_writer.py:48] [61800] global_step=61800, grad_norm=1.2089987993240356, loss=2.7532143592834473
I0308 13:16:28.436950 139708407461632 logging_writer.py:48] [61900] global_step=61900, grad_norm=1.3188748359680176, loss=2.6016910076141357
I0308 13:17:13.760759 139708415854336 logging_writer.py:48] [62000] global_step=62000, grad_norm=1.2309317588806152, loss=3.200087308883667
I0308 13:17:59.148165 139708407461632 logging_writer.py:48] [62100] global_step=62100, grad_norm=1.3640868663787842, loss=2.253269672393799
I0308 13:18:44.304894 139708415854336 logging_writer.py:48] [62200] global_step=62200, grad_norm=1.2780780792236328, loss=2.336768388748169
I0308 13:19:25.032128 139902746892096 spec.py:321] Evaluating on the training split.
I0308 13:19:36.478379 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 13:19:58.296283 139902746892096 spec.py:349] Evaluating on the test split.
I0308 13:19:59.972841 139902746892096 submission_runner.py:411] Time since start: 29923.00s, 	Step: 62292, 	{'train/accuracy': 0.703808605670929, 'train/loss': 1.203053593635559, 'validation/accuracy': 0.6331799626350403, 'validation/loss': 1.5297189950942993, 'validation/num_examples': 50000, 'test/accuracy': 0.5073000192642212, 'test/loss': 2.1814677715301514, 'test/num_examples': 10000, 'score': 27772.926307678223, 'total_duration': 29923.001628398895, 'accumulated_submission_time': 27772.926307678223, 'accumulated_eval_time': 2143.499714612961, 'accumulated_logging_time': 3.2494382858276367}
I0308 13:19:59.997319 139708407461632 logging_writer.py:48] [62292] accumulated_eval_time=2143.499715, accumulated_logging_time=3.249438, accumulated_submission_time=27772.926308, global_step=62292, preemption_count=0, score=27772.926308, test/accuracy=0.507300, test/loss=2.181468, test/num_examples=10000, total_duration=29923.001628, train/accuracy=0.703809, train/loss=1.203054, validation/accuracy=0.633180, validation/loss=1.529719, validation/num_examples=50000
I0308 13:20:03.545924 139708415854336 logging_writer.py:48] [62300] global_step=62300, grad_norm=1.161823034286499, loss=2.6597936153411865
I0308 13:20:44.495935 139708407461632 logging_writer.py:48] [62400] global_step=62400, grad_norm=1.2306599617004395, loss=2.1165647506713867
I0308 13:21:29.587224 139708415854336 logging_writer.py:48] [62500] global_step=62500, grad_norm=1.0609595775604248, loss=4.7726287841796875
I0308 13:22:15.265211 139708407461632 logging_writer.py:48] [62600] global_step=62600, grad_norm=1.287984013557434, loss=2.385432004928589
I0308 13:23:00.630059 139708415854336 logging_writer.py:48] [62700] global_step=62700, grad_norm=1.2451528310775757, loss=2.586594820022583
I0308 13:23:45.957714 139708407461632 logging_writer.py:48] [62800] global_step=62800, grad_norm=1.3106759786605835, loss=2.2187952995300293
I0308 13:24:31.398411 139708415854336 logging_writer.py:48] [62900] global_step=62900, grad_norm=1.3156895637512207, loss=2.146547794342041
I0308 13:25:16.741860 139708407461632 logging_writer.py:48] [63000] global_step=63000, grad_norm=1.0599865913391113, loss=4.503362655639648
I0308 13:26:02.063787 139708415854336 logging_writer.py:48] [63100] global_step=63100, grad_norm=1.119712233543396, loss=3.2773044109344482
I0308 13:26:47.610177 139708407461632 logging_writer.py:48] [63200] global_step=63200, grad_norm=1.3259727954864502, loss=2.3198659420013428
I0308 13:26:59.997874 139902746892096 spec.py:321] Evaluating on the training split.
I0308 13:27:11.639413 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 13:27:33.223780 139902746892096 spec.py:349] Evaluating on the test split.
I0308 13:27:34.893556 139902746892096 submission_runner.py:411] Time since start: 30377.92s, 	Step: 63229, 	{'train/accuracy': 0.6870507597923279, 'train/loss': 1.2610909938812256, 'validation/accuracy': 0.6409800052642822, 'validation/loss': 1.4879895448684692, 'validation/num_examples': 50000, 'test/accuracy': 0.5142000317573547, 'test/loss': 2.15094256401062, 'test/num_examples': 10000, 'score': 28192.870245218277, 'total_duration': 30377.92234969139, 'accumulated_submission_time': 28192.870245218277, 'accumulated_eval_time': 2178.395395040512, 'accumulated_logging_time': 3.282611131668091}
I0308 13:27:34.918032 139708415854336 logging_writer.py:48] [63229] accumulated_eval_time=2178.395395, accumulated_logging_time=3.282611, accumulated_submission_time=28192.870245, global_step=63229, preemption_count=0, score=28192.870245, test/accuracy=0.514200, test/loss=2.150943, test/num_examples=10000, total_duration=30377.922350, train/accuracy=0.687051, train/loss=1.261091, validation/accuracy=0.640980, validation/loss=1.487990, validation/num_examples=50000
I0308 13:28:03.262946 139708407461632 logging_writer.py:48] [63300] global_step=63300, grad_norm=1.305311918258667, loss=2.4525022506713867
I0308 13:28:47.615706 139708415854336 logging_writer.py:48] [63400] global_step=63400, grad_norm=1.223987102508545, loss=2.3623480796813965
I0308 13:29:32.794033 139708407461632 logging_writer.py:48] [63500] global_step=63500, grad_norm=1.2766449451446533, loss=2.118396043777466
I0308 13:30:18.138727 139708415854336 logging_writer.py:48] [63600] global_step=63600, grad_norm=1.3393807411193848, loss=2.388951301574707
I0308 13:31:03.354619 139708407461632 logging_writer.py:48] [63700] global_step=63700, grad_norm=1.336549162864685, loss=2.222370147705078
I0308 13:31:48.690262 139708415854336 logging_writer.py:48] [63800] global_step=63800, grad_norm=1.3141613006591797, loss=2.212240695953369
I0308 13:32:33.980746 139708407461632 logging_writer.py:48] [63900] global_step=63900, grad_norm=1.2573851346969604, loss=2.461165189743042
I0308 13:33:19.086931 139708415854336 logging_writer.py:48] [64000] global_step=64000, grad_norm=1.407300591468811, loss=2.376788854598999
I0308 13:34:04.263732 139708407461632 logging_writer.py:48] [64100] global_step=64100, grad_norm=1.4442684650421143, loss=2.197650671005249
I0308 13:34:35.268671 139902746892096 spec.py:321] Evaluating on the training split.
I0308 13:34:46.700129 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 13:35:08.824187 139902746892096 spec.py:349] Evaluating on the test split.
I0308 13:35:10.497393 139902746892096 submission_runner.py:411] Time since start: 30833.53s, 	Step: 64170, 	{'train/accuracy': 0.6971484422683716, 'train/loss': 1.215964436531067, 'validation/accuracy': 0.6412799954414368, 'validation/loss': 1.470037579536438, 'validation/num_examples': 50000, 'test/accuracy': 0.5209000110626221, 'test/loss': 2.1333417892456055, 'test/num_examples': 10000, 'score': 28613.161401748657, 'total_duration': 30833.526191473007, 'accumulated_submission_time': 28613.161401748657, 'accumulated_eval_time': 2213.624122619629, 'accumulated_logging_time': 3.317622423171997}
I0308 13:35:10.525278 139708415854336 logging_writer.py:48] [64170] accumulated_eval_time=2213.624123, accumulated_logging_time=3.317622, accumulated_submission_time=28613.161402, global_step=64170, preemption_count=0, score=28613.161402, test/accuracy=0.520900, test/loss=2.133342, test/num_examples=10000, total_duration=30833.526191, train/accuracy=0.697148, train/loss=1.215964, validation/accuracy=0.641280, validation/loss=1.470038, validation/num_examples=50000
I0308 13:35:22.748842 139708407461632 logging_writer.py:48] [64200] global_step=64200, grad_norm=1.2686128616333008, loss=2.228867292404175
I0308 13:36:04.158011 139708415854336 logging_writer.py:48] [64300] global_step=64300, grad_norm=1.0724807977676392, loss=3.980586051940918
I0308 13:36:49.488808 139708407461632 logging_writer.py:48] [64400] global_step=64400, grad_norm=1.3574403524398804, loss=2.2873377799987793
I0308 13:37:35.392148 139708415854336 logging_writer.py:48] [64500] global_step=64500, grad_norm=1.1882911920547485, loss=4.480180740356445
I0308 13:38:20.452874 139708407461632 logging_writer.py:48] [64600] global_step=64600, grad_norm=1.141127586364746, loss=2.7970969676971436
I0308 13:39:05.665218 139708415854336 logging_writer.py:48] [64700] global_step=64700, grad_norm=1.2274270057678223, loss=4.829631805419922
I0308 13:39:51.014956 139708407461632 logging_writer.py:48] [64800] global_step=64800, grad_norm=1.134443759918213, loss=2.567577362060547
I0308 13:40:36.010912 139708415854336 logging_writer.py:48] [64900] global_step=64900, grad_norm=1.383743405342102, loss=2.290977716445923
I0308 13:41:21.076299 139708407461632 logging_writer.py:48] [65000] global_step=65000, grad_norm=1.2436660528182983, loss=3.1046414375305176
I0308 13:42:06.508652 139708415854336 logging_writer.py:48] [65100] global_step=65100, grad_norm=1.320261001586914, loss=2.1189959049224854
I0308 13:42:10.674668 139902746892096 spec.py:321] Evaluating on the training split.
I0308 13:42:22.110701 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 13:42:42.769557 139902746892096 spec.py:349] Evaluating on the test split.
I0308 13:42:44.473482 139902746892096 submission_runner.py:411] Time since start: 31287.50s, 	Step: 65111, 	{'train/accuracy': 0.7034765481948853, 'train/loss': 1.2014892101287842, 'validation/accuracy': 0.6446999907493591, 'validation/loss': 1.4803105592727661, 'validation/num_examples': 50000, 'test/accuracy': 0.51910001039505, 'test/loss': 2.131485939025879, 'test/num_examples': 10000, 'score': 29033.251259088516, 'total_duration': 31287.502277612686, 'accumulated_submission_time': 29033.251259088516, 'accumulated_eval_time': 2247.422921895981, 'accumulated_logging_time': 3.3557910919189453}
I0308 13:42:44.501540 139708407461632 logging_writer.py:48] [65111] accumulated_eval_time=2247.422922, accumulated_logging_time=3.355791, accumulated_submission_time=29033.251259, global_step=65111, preemption_count=0, score=29033.251259, test/accuracy=0.519100, test/loss=2.131486, test/num_examples=10000, total_duration=31287.502278, train/accuracy=0.703477, train/loss=1.201489, validation/accuracy=0.644700, validation/loss=1.480311, validation/num_examples=50000
I0308 13:43:20.324233 139708415854336 logging_writer.py:48] [65200] global_step=65200, grad_norm=1.2463481426239014, loss=2.4220964908599854
I0308 13:44:05.238368 139708407461632 logging_writer.py:48] [65300] global_step=65300, grad_norm=1.3228932619094849, loss=2.254725217819214
I0308 13:44:50.585043 139708415854336 logging_writer.py:48] [65400] global_step=65400, grad_norm=1.2721079587936401, loss=2.1910252571105957
I0308 13:45:35.887414 139708407461632 logging_writer.py:48] [65500] global_step=65500, grad_norm=1.2006453275680542, loss=2.199139356613159
I0308 13:46:21.024143 139708415854336 logging_writer.py:48] [65600] global_step=65600, grad_norm=1.1826188564300537, loss=2.6511130332946777
I0308 13:47:06.169467 139708407461632 logging_writer.py:48] [65700] global_step=65700, grad_norm=1.3542531728744507, loss=4.889927864074707
I0308 13:47:51.496134 139708415854336 logging_writer.py:48] [65800] global_step=65800, grad_norm=1.215253472328186, loss=2.2025206089019775
I0308 13:48:36.617937 139708407461632 logging_writer.py:48] [65900] global_step=65900, grad_norm=1.2665070295333862, loss=2.5097146034240723
I0308 13:49:21.315571 139708415854336 logging_writer.py:48] [66000] global_step=66000, grad_norm=1.4332106113433838, loss=2.239564895629883
I0308 13:49:44.732703 139902746892096 spec.py:321] Evaluating on the training split.
I0308 13:49:56.212628 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 13:50:18.358299 139902746892096 spec.py:349] Evaluating on the test split.
I0308 13:50:20.031563 139902746892096 submission_runner.py:411] Time since start: 31743.06s, 	Step: 66053, 	{'train/accuracy': 0.6955664157867432, 'train/loss': 1.2484641075134277, 'validation/accuracy': 0.6377800107002258, 'validation/loss': 1.503852128982544, 'validation/num_examples': 50000, 'test/accuracy': 0.5210000276565552, 'test/loss': 2.150148630142212, 'test/num_examples': 10000, 'score': 29453.424600839615, 'total_duration': 31743.060341358185, 'accumulated_submission_time': 29453.424600839615, 'accumulated_eval_time': 2282.7217609882355, 'accumulated_logging_time': 3.39251971244812}
I0308 13:50:20.055815 139708407461632 logging_writer.py:48] [66053] accumulated_eval_time=2282.721761, accumulated_logging_time=3.392520, accumulated_submission_time=29453.424601, global_step=66053, preemption_count=0, score=29453.424601, test/accuracy=0.521000, test/loss=2.150149, test/num_examples=10000, total_duration=31743.060341, train/accuracy=0.695566, train/loss=1.248464, validation/accuracy=0.637780, validation/loss=1.503852, validation/num_examples=50000
I0308 13:50:38.963394 139708415854336 logging_writer.py:48] [66100] global_step=66100, grad_norm=1.3919081687927246, loss=2.0921380519866943
I0308 13:51:21.421451 139708407461632 logging_writer.py:48] [66200] global_step=66200, grad_norm=1.2888598442077637, loss=2.1713156700134277
I0308 13:52:06.775683 139708415854336 logging_writer.py:48] [66300] global_step=66300, grad_norm=1.2526768445968628, loss=2.319911241531372
I0308 13:52:52.252609 139708407461632 logging_writer.py:48] [66400] global_step=66400, grad_norm=1.189778447151184, loss=3.3398821353912354
I0308 13:53:37.351182 139708415854336 logging_writer.py:48] [66500] global_step=66500, grad_norm=1.2937164306640625, loss=2.2018344402313232
I0308 13:54:22.463520 139708407461632 logging_writer.py:48] [66600] global_step=66600, grad_norm=1.3071563243865967, loss=2.2040984630584717
I0308 13:55:07.731228 139708415854336 logging_writer.py:48] [66700] global_step=66700, grad_norm=1.458158016204834, loss=2.6437370777130127
I0308 13:55:52.712826 139708407461632 logging_writer.py:48] [66800] global_step=66800, grad_norm=1.298601508140564, loss=2.1831178665161133
I0308 13:56:37.781118 139708415854336 logging_writer.py:48] [66900] global_step=66900, grad_norm=1.1087206602096558, loss=2.752842426300049
I0308 13:57:20.115158 139902746892096 spec.py:321] Evaluating on the training split.
I0308 13:57:31.778611 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 13:57:51.974571 139902746892096 spec.py:349] Evaluating on the test split.
I0308 13:57:53.655429 139902746892096 submission_runner.py:411] Time since start: 32196.68s, 	Step: 66995, 	{'train/accuracy': 0.6943359375, 'train/loss': 1.2172327041625977, 'validation/accuracy': 0.6418399810791016, 'validation/loss': 1.4598710536956787, 'validation/num_examples': 50000, 'test/accuracy': 0.5213000178337097, 'test/loss': 2.1135141849517822, 'test/num_examples': 10000, 'score': 29873.4261200428, 'total_duration': 32196.684210062027, 'accumulated_submission_time': 29873.4261200428, 'accumulated_eval_time': 2316.2620203495026, 'accumulated_logging_time': 3.425584554672241}
I0308 13:57:53.690008 139708407461632 logging_writer.py:48] [66995] accumulated_eval_time=2316.262020, accumulated_logging_time=3.425585, accumulated_submission_time=29873.426120, global_step=66995, preemption_count=0, score=29873.426120, test/accuracy=0.521300, test/loss=2.113514, test/num_examples=10000, total_duration=32196.684210, train/accuracy=0.694336, train/loss=1.217233, validation/accuracy=0.641840, validation/loss=1.459871, validation/num_examples=50000
I0308 13:57:56.055117 139708415854336 logging_writer.py:48] [67000] global_step=67000, grad_norm=1.198053002357483, loss=2.2169687747955322
I0308 13:58:36.701673 139708407461632 logging_writer.py:48] [67100] global_step=67100, grad_norm=1.1458624601364136, loss=4.648969650268555
I0308 13:59:21.606263 139708415854336 logging_writer.py:48] [67200] global_step=67200, grad_norm=1.3012149333953857, loss=2.7987546920776367
I0308 14:00:06.794947 139708407461632 logging_writer.py:48] [67300] global_step=67300, grad_norm=1.3133982419967651, loss=2.2503185272216797
I0308 14:00:52.127295 139708415854336 logging_writer.py:48] [67400] global_step=67400, grad_norm=1.3215079307556152, loss=2.198235034942627
I0308 14:01:36.894359 139708407461632 logging_writer.py:48] [67500] global_step=67500, grad_norm=1.1796128749847412, loss=3.206238031387329
I0308 14:02:22.682366 139708415854336 logging_writer.py:48] [67600] global_step=67600, grad_norm=1.1555688381195068, loss=4.302012920379639
I0308 14:03:07.729329 139708407461632 logging_writer.py:48] [67700] global_step=67700, grad_norm=1.322114109992981, loss=2.1463356018066406
I0308 14:03:52.549147 139708415854336 logging_writer.py:48] [67800] global_step=67800, grad_norm=1.0731947422027588, loss=4.749666213989258
I0308 14:04:37.637853 139708407461632 logging_writer.py:48] [67900] global_step=67900, grad_norm=1.2857564687728882, loss=2.1380984783172607
I0308 14:04:53.672585 139902746892096 spec.py:321] Evaluating on the training split.
I0308 14:05:05.250426 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 14:05:27.625647 139902746892096 spec.py:349] Evaluating on the test split.
I0308 14:05:29.298764 139902746892096 submission_runner.py:411] Time since start: 32652.33s, 	Step: 67937, 	{'train/accuracy': 0.7025390267372131, 'train/loss': 1.1881595849990845, 'validation/accuracy': 0.6430400013923645, 'validation/loss': 1.46323823928833, 'validation/num_examples': 50000, 'test/accuracy': 0.5241000056266785, 'test/loss': 2.116852045059204, 'test/num_examples': 10000, 'score': 30293.34878706932, 'total_duration': 32652.327559947968, 'accumulated_submission_time': 30293.34878706932, 'accumulated_eval_time': 2351.888193130493, 'accumulated_logging_time': 3.4705111980438232}
I0308 14:05:29.326130 139708415854336 logging_writer.py:48] [67937] accumulated_eval_time=2351.888193, accumulated_logging_time=3.470511, accumulated_submission_time=30293.348787, global_step=67937, preemption_count=0, score=30293.348787, test/accuracy=0.524100, test/loss=2.116852, test/num_examples=10000, total_duration=32652.327560, train/accuracy=0.702539, train/loss=1.188160, validation/accuracy=0.643040, validation/loss=1.463238, validation/num_examples=50000
I0308 14:05:54.519990 139708407461632 logging_writer.py:48] [68000] global_step=68000, grad_norm=1.3756593465805054, loss=2.4324140548706055
I0308 14:06:38.210241 139708415854336 logging_writer.py:48] [68100] global_step=68100, grad_norm=1.0921282768249512, loss=4.742199897766113
I0308 14:07:23.558829 139708407461632 logging_writer.py:48] [68200] global_step=68200, grad_norm=1.31581711769104, loss=2.450037956237793
I0308 14:08:09.050865 139708415854336 logging_writer.py:48] [68300] global_step=68300, grad_norm=1.3859349489212036, loss=2.6054303646087646
I0308 14:08:53.965946 139708407461632 logging_writer.py:48] [68400] global_step=68400, grad_norm=1.2457268238067627, loss=3.3604278564453125
I0308 14:09:39.129277 139708415854336 logging_writer.py:48] [68500] global_step=68500, grad_norm=1.226234793663025, loss=2.661911964416504
I0308 14:10:24.062134 139708407461632 logging_writer.py:48] [68600] global_step=68600, grad_norm=1.3134841918945312, loss=2.020313262939453
I0308 14:11:09.099544 139708415854336 logging_writer.py:48] [68700] global_step=68700, grad_norm=1.1893008947372437, loss=4.778285026550293
I0308 14:11:54.550420 139708407461632 logging_writer.py:48] [68800] global_step=68800, grad_norm=1.2153724431991577, loss=3.6996328830718994
I0308 14:12:29.320556 139902746892096 spec.py:321] Evaluating on the training split.
I0308 14:12:40.781630 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 14:13:01.990949 139902746892096 spec.py:349] Evaluating on the test split.
I0308 14:13:03.667963 139902746892096 submission_runner.py:411] Time since start: 33106.70s, 	Step: 68878, 	{'train/accuracy': 0.7176562547683716, 'train/loss': 1.159710168838501, 'validation/accuracy': 0.6420599818229675, 'validation/loss': 1.4991099834442139, 'validation/num_examples': 50000, 'test/accuracy': 0.5215000510215759, 'test/loss': 2.1543941497802734, 'test/num_examples': 10000, 'score': 30713.28437423706, 'total_duration': 33106.69675326347, 'accumulated_submission_time': 30713.28437423706, 'accumulated_eval_time': 2386.2355921268463, 'accumulated_logging_time': 3.507035970687866}
I0308 14:13:03.699227 139708415854336 logging_writer.py:48] [68878] accumulated_eval_time=2386.235592, accumulated_logging_time=3.507036, accumulated_submission_time=30713.284374, global_step=68878, preemption_count=0, score=30713.284374, test/accuracy=0.521500, test/loss=2.154394, test/num_examples=10000, total_duration=33106.696753, train/accuracy=0.717656, train/loss=1.159710, validation/accuracy=0.642060, validation/loss=1.499110, validation/num_examples=50000
I0308 14:13:12.762945 139708407461632 logging_writer.py:48] [68900] global_step=68900, grad_norm=1.401807427406311, loss=2.3782858848571777
I0308 14:13:54.462356 139708415854336 logging_writer.py:48] [69000] global_step=69000, grad_norm=1.1914502382278442, loss=4.76544189453125
I0308 14:14:40.202178 139708407461632 logging_writer.py:48] [69100] global_step=69100, grad_norm=1.179331660270691, loss=2.529655933380127
I0308 14:15:25.672194 139708415854336 logging_writer.py:48] [69200] global_step=69200, grad_norm=1.3478180170059204, loss=2.1302192211151123
I0308 14:16:11.009950 139708407461632 logging_writer.py:48] [69300] global_step=69300, grad_norm=1.4076311588287354, loss=2.4403488636016846
I0308 14:16:56.186143 139708415854336 logging_writer.py:48] [69400] global_step=69400, grad_norm=1.1555577516555786, loss=2.523730754852295
I0308 14:17:42.047977 139708407461632 logging_writer.py:48] [69500] global_step=69500, grad_norm=1.129706621170044, loss=3.7206575870513916
I0308 14:18:27.307981 139708415854336 logging_writer.py:48] [69600] global_step=69600, grad_norm=1.3234984874725342, loss=2.6670494079589844
I0308 14:19:12.634636 139708407461632 logging_writer.py:48] [69700] global_step=69700, grad_norm=1.314252495765686, loss=2.178403377532959
I0308 14:19:58.067476 139708415854336 logging_writer.py:48] [69800] global_step=69800, grad_norm=1.2889196872711182, loss=4.233527660369873
I0308 14:20:04.040296 139902746892096 spec.py:321] Evaluating on the training split.
I0308 14:20:15.730255 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 14:20:38.370960 139902746892096 spec.py:349] Evaluating on the test split.
I0308 14:20:40.054895 139902746892096 submission_runner.py:411] Time since start: 33563.08s, 	Step: 69815, 	{'train/accuracy': 0.6985741853713989, 'train/loss': 1.2329505681991577, 'validation/accuracy': 0.6497399806976318, 'validation/loss': 1.4623702764511108, 'validation/num_examples': 50000, 'test/accuracy': 0.5277000069618225, 'test/loss': 2.1101725101470947, 'test/num_examples': 10000, 'score': 31133.5650537014, 'total_duration': 33563.08368849754, 'accumulated_submission_time': 31133.5650537014, 'accumulated_eval_time': 2422.250189781189, 'accumulated_logging_time': 3.54935359954834}
I0308 14:20:40.083449 139708407461632 logging_writer.py:48] [69815] accumulated_eval_time=2422.250190, accumulated_logging_time=3.549354, accumulated_submission_time=31133.565054, global_step=69815, preemption_count=0, score=31133.565054, test/accuracy=0.527700, test/loss=2.110173, test/num_examples=10000, total_duration=33563.083688, train/accuracy=0.698574, train/loss=1.232951, validation/accuracy=0.649740, validation/loss=1.462370, validation/num_examples=50000
I0308 14:21:13.948246 139708415854336 logging_writer.py:48] [69900] global_step=69900, grad_norm=1.1146495342254639, loss=2.9050540924072266
I0308 14:21:58.776601 139708407461632 logging_writer.py:48] [70000] global_step=70000, grad_norm=1.3377310037612915, loss=2.382214069366455
I0308 14:22:44.352748 139708415854336 logging_writer.py:48] [70100] global_step=70100, grad_norm=1.332628846168518, loss=2.3881053924560547
I0308 14:23:29.781689 139708407461632 logging_writer.py:48] [70200] global_step=70200, grad_norm=1.3445883989334106, loss=4.669988632202148
I0308 14:24:14.982486 139708415854336 logging_writer.py:48] [70300] global_step=70300, grad_norm=1.249725341796875, loss=3.169691801071167
I0308 14:25:00.328544 139708407461632 logging_writer.py:48] [70400] global_step=70400, grad_norm=1.3047585487365723, loss=2.1699540615081787
I0308 14:25:45.574316 139708415854336 logging_writer.py:48] [70500] global_step=70500, grad_norm=1.069166660308838, loss=4.7425007820129395
I0308 14:26:31.016809 139708407461632 logging_writer.py:48] [70600] global_step=70600, grad_norm=1.2873785495758057, loss=4.843158721923828
I0308 14:27:16.291372 139708415854336 logging_writer.py:48] [70700] global_step=70700, grad_norm=1.330029845237732, loss=2.502653121948242
I0308 14:27:40.196757 139902746892096 spec.py:321] Evaluating on the training split.
I0308 14:27:51.583839 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 14:28:13.340953 139902746892096 spec.py:349] Evaluating on the test split.
I0308 14:28:15.012344 139902746892096 submission_runner.py:411] Time since start: 34018.04s, 	Step: 70754, 	{'train/accuracy': 0.7025781273841858, 'train/loss': 1.182090163230896, 'validation/accuracy': 0.6479200124740601, 'validation/loss': 1.4353218078613281, 'validation/num_examples': 50000, 'test/accuracy': 0.5285000205039978, 'test/loss': 2.077733039855957, 'test/num_examples': 10000, 'score': 31553.619502067566, 'total_duration': 34018.04113793373, 'accumulated_submission_time': 31553.619502067566, 'accumulated_eval_time': 2457.065773963928, 'accumulated_logging_time': 3.5868256092071533}
I0308 14:28:15.037530 139708407461632 logging_writer.py:48] [70754] accumulated_eval_time=2457.065774, accumulated_logging_time=3.586826, accumulated_submission_time=31553.619502, global_step=70754, preemption_count=0, score=31553.619502, test/accuracy=0.528500, test/loss=2.077733, test/num_examples=10000, total_duration=34018.041138, train/accuracy=0.702578, train/loss=1.182090, validation/accuracy=0.647920, validation/loss=1.435322, validation/num_examples=50000
I0308 14:28:33.559468 139708415854336 logging_writer.py:48] [70800] global_step=70800, grad_norm=1.1958614587783813, loss=2.5078258514404297
I0308 14:29:16.305546 139708407461632 logging_writer.py:48] [70900] global_step=70900, grad_norm=1.3755543231964111, loss=2.338630199432373
I0308 14:30:01.405621 139708415854336 logging_writer.py:48] [71000] global_step=71000, grad_norm=1.2507498264312744, loss=3.629373073577881
I0308 14:30:46.742488 139708407461632 logging_writer.py:48] [71100] global_step=71100, grad_norm=1.1749728918075562, loss=2.463243007659912
I0308 14:31:31.816668 139708415854336 logging_writer.py:48] [71200] global_step=71200, grad_norm=1.4169230461120605, loss=2.388388156890869
I0308 14:32:17.138789 139708407461632 logging_writer.py:48] [71300] global_step=71300, grad_norm=1.1143213510513306, loss=4.586090087890625
I0308 14:33:02.863395 139708415854336 logging_writer.py:48] [71400] global_step=71400, grad_norm=1.1405442953109741, loss=3.0131235122680664
I0308 14:33:48.272608 139708407461632 logging_writer.py:48] [71500] global_step=71500, grad_norm=1.1685705184936523, loss=2.9815196990966797
I0308 14:34:33.596576 139708415854336 logging_writer.py:48] [71600] global_step=71600, grad_norm=1.3500964641571045, loss=2.316770076751709
I0308 14:35:15.039219 139902746892096 spec.py:321] Evaluating on the training split.
I0308 14:35:26.481389 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 14:35:47.723183 139902746892096 spec.py:349] Evaluating on the test split.
I0308 14:35:49.402073 139902746892096 submission_runner.py:411] Time since start: 34472.43s, 	Step: 71693, 	{'train/accuracy': 0.7128710746765137, 'train/loss': 1.1547907590866089, 'validation/accuracy': 0.6517399549484253, 'validation/loss': 1.451304316520691, 'validation/num_examples': 50000, 'test/accuracy': 0.5290000438690186, 'test/loss': 2.096698760986328, 'test/num_examples': 10000, 'score': 31973.562628507614, 'total_duration': 34472.430872917175, 'accumulated_submission_time': 31973.562628507614, 'accumulated_eval_time': 2491.428635120392, 'accumulated_logging_time': 3.6209423542022705}
I0308 14:35:49.431024 139708407461632 logging_writer.py:48] [71693] accumulated_eval_time=2491.428635, accumulated_logging_time=3.620942, accumulated_submission_time=31973.562629, global_step=71693, preemption_count=0, score=31973.562629, test/accuracy=0.529000, test/loss=2.096699, test/num_examples=10000, total_duration=34472.430873, train/accuracy=0.712871, train/loss=1.154791, validation/accuracy=0.651740, validation/loss=1.451304, validation/num_examples=50000
I0308 14:35:52.578049 139708415854336 logging_writer.py:48] [71700] global_step=71700, grad_norm=1.1302542686462402, loss=3.75
I0308 14:36:33.114639 139708407461632 logging_writer.py:48] [71800] global_step=71800, grad_norm=1.292953372001648, loss=2.1610612869262695
I0308 14:37:18.158410 139708415854336 logging_writer.py:48] [71900] global_step=71900, grad_norm=1.3004391193389893, loss=2.201582908630371
I0308 14:38:03.721952 139708407461632 logging_writer.py:48] [72000] global_step=72000, grad_norm=1.1345281600952148, loss=2.8742947578430176
I0308 14:38:48.919038 139708415854336 logging_writer.py:48] [72100] global_step=72100, grad_norm=1.3437702655792236, loss=2.160912275314331
I0308 14:39:33.939516 139708407461632 logging_writer.py:48] [72200] global_step=72200, grad_norm=1.230905532836914, loss=3.1309826374053955
I0308 14:40:19.086427 139708415854336 logging_writer.py:48] [72300] global_step=72300, grad_norm=1.3654216527938843, loss=2.2419612407684326
I0308 14:41:04.362001 139708407461632 logging_writer.py:48] [72400] global_step=72400, grad_norm=1.3678200244903564, loss=2.9657070636749268
I0308 14:41:49.366333 139708415854336 logging_writer.py:48] [72500] global_step=72500, grad_norm=1.3029265403747559, loss=2.1460156440734863
I0308 14:42:35.016046 139708407461632 logging_writer.py:48] [72600] global_step=72600, grad_norm=1.2265559434890747, loss=3.122807502746582
I0308 14:42:49.650164 139902746892096 spec.py:321] Evaluating on the training split.
I0308 14:43:00.911812 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 14:43:23.152303 139902746892096 spec.py:349] Evaluating on the test split.
I0308 14:43:24.833975 139902746892096 submission_runner.py:411] Time since start: 34927.86s, 	Step: 72634, 	{'train/accuracy': 0.7041015625, 'train/loss': 1.1773505210876465, 'validation/accuracy': 0.6512599587440491, 'validation/loss': 1.4189000129699707, 'validation/num_examples': 50000, 'test/accuracy': 0.5267000198364258, 'test/loss': 2.096280097961426, 'test/num_examples': 10000, 'score': 32393.721923351288, 'total_duration': 34927.862758398056, 'accumulated_submission_time': 32393.721923351288, 'accumulated_eval_time': 2526.6124362945557, 'accumulated_logging_time': 3.659743547439575}
I0308 14:43:24.863402 139708415854336 logging_writer.py:48] [72634] accumulated_eval_time=2526.612436, accumulated_logging_time=3.659744, accumulated_submission_time=32393.721923, global_step=72634, preemption_count=0, score=32393.721923, test/accuracy=0.526700, test/loss=2.096280, test/num_examples=10000, total_duration=34927.862758, train/accuracy=0.704102, train/loss=1.177351, validation/accuracy=0.651260, validation/loss=1.418900, validation/num_examples=50000
I0308 14:43:51.241893 139708407461632 logging_writer.py:48] [72700] global_step=72700, grad_norm=1.1611305475234985, loss=3.484055995941162
I0308 14:44:34.714988 139708415854336 logging_writer.py:48] [72800] global_step=72800, grad_norm=1.3441816568374634, loss=4.385558605194092
I0308 14:45:20.187156 139708407461632 logging_writer.py:48] [72900] global_step=72900, grad_norm=1.3560289144515991, loss=2.3208861351013184
I0308 14:46:05.624377 139708415854336 logging_writer.py:48] [73000] global_step=73000, grad_norm=1.318681240081787, loss=2.310884952545166
I0308 14:46:50.627238 139708407461632 logging_writer.py:48] [73100] global_step=73100, grad_norm=1.191784143447876, loss=2.133941173553467
I0308 14:47:36.014572 139708415854336 logging_writer.py:48] [73200] global_step=73200, grad_norm=1.2322766780853271, loss=2.881908416748047
I0308 14:48:21.363856 139708407461632 logging_writer.py:48] [73300] global_step=73300, grad_norm=1.365524172782898, loss=2.2577672004699707
I0308 14:49:06.566849 139708415854336 logging_writer.py:48] [73400] global_step=73400, grad_norm=1.201724886894226, loss=2.620121955871582
I0308 14:49:51.606332 139708407461632 logging_writer.py:48] [73500] global_step=73500, grad_norm=1.338337779045105, loss=2.101736068725586
I0308 14:50:25.064503 139902746892096 spec.py:321] Evaluating on the training split.
I0308 14:50:36.583388 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 14:50:55.590357 139902746892096 spec.py:349] Evaluating on the test split.
I0308 14:50:57.271121 139902746892096 submission_runner.py:411] Time since start: 35380.30s, 	Step: 73574, 	{'train/accuracy': 0.7027539014816284, 'train/loss': 1.1971248388290405, 'validation/accuracy': 0.6504200100898743, 'validation/loss': 1.4388371706008911, 'validation/num_examples': 50000, 'test/accuracy': 0.5236999988555908, 'test/loss': 2.107862949371338, 'test/num_examples': 10000, 'score': 32813.861067295074, 'total_duration': 35380.29990744591, 'accumulated_submission_time': 32813.861067295074, 'accumulated_eval_time': 2558.8190701007843, 'accumulated_logging_time': 3.7015137672424316}
I0308 14:50:57.305003 139708415854336 logging_writer.py:48] [73574] accumulated_eval_time=2558.819070, accumulated_logging_time=3.701514, accumulated_submission_time=32813.861067, global_step=73574, preemption_count=0, score=32813.861067, test/accuracy=0.523700, test/loss=2.107863, test/num_examples=10000, total_duration=35380.299907, train/accuracy=0.702754, train/loss=1.197125, validation/accuracy=0.650420, validation/loss=1.438837, validation/num_examples=50000
I0308 14:51:07.942780 139708407461632 logging_writer.py:48] [73600] global_step=73600, grad_norm=1.085652232170105, loss=3.78409481048584
I0308 14:51:49.848359 139708415854336 logging_writer.py:48] [73700] global_step=73700, grad_norm=1.3057104349136353, loss=2.7181780338287354
I0308 14:52:35.121917 139708407461632 logging_writer.py:48] [73800] global_step=73800, grad_norm=1.3042616844177246, loss=2.432365655899048
I0308 14:53:20.767515 139708415854336 logging_writer.py:48] [73900] global_step=73900, grad_norm=1.3137367963790894, loss=2.212491989135742
I0308 14:54:05.862044 139708407461632 logging_writer.py:48] [74000] global_step=74000, grad_norm=1.2806035280227661, loss=4.690933704376221
I0308 14:54:50.969744 139708415854336 logging_writer.py:48] [74100] global_step=74100, grad_norm=1.3072689771652222, loss=2.374868392944336
I0308 14:55:36.308742 139708407461632 logging_writer.py:48] [74200] global_step=74200, grad_norm=1.2744799852371216, loss=3.6530799865722656
I0308 14:56:21.413018 139708415854336 logging_writer.py:48] [74300] global_step=74300, grad_norm=1.3396328687667847, loss=2.132222890853882
I0308 14:57:06.548241 139708407461632 logging_writer.py:48] [74400] global_step=74400, grad_norm=1.412573218345642, loss=1.9663199186325073
I0308 14:57:51.788153 139708415854336 logging_writer.py:48] [74500] global_step=74500, grad_norm=1.1902462244033813, loss=2.849912166595459
I0308 14:57:57.323383 139902746892096 spec.py:321] Evaluating on the training split.
I0308 14:58:08.976087 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 14:58:30.808168 139902746892096 spec.py:349] Evaluating on the test split.
I0308 14:58:32.479258 139902746892096 submission_runner.py:411] Time since start: 35835.51s, 	Step: 74514, 	{'train/accuracy': 0.7067577838897705, 'train/loss': 1.1871877908706665, 'validation/accuracy': 0.6526199579238892, 'validation/loss': 1.4396685361862183, 'validation/num_examples': 50000, 'test/accuracy': 0.5236000418663025, 'test/loss': 2.103861093521118, 'test/num_examples': 10000, 'score': 33233.81961917877, 'total_duration': 35835.50805258751, 'accumulated_submission_time': 33233.81961917877, 'accumulated_eval_time': 2593.9749369621277, 'accumulated_logging_time': 3.7460293769836426}
I0308 14:58:32.505436 139708407461632 logging_writer.py:48] [74514] accumulated_eval_time=2593.974937, accumulated_logging_time=3.746029, accumulated_submission_time=33233.819619, global_step=74514, preemption_count=0, score=33233.819619, test/accuracy=0.523600, test/loss=2.103861, test/num_examples=10000, total_duration=35835.508053, train/accuracy=0.706758, train/loss=1.187188, validation/accuracy=0.652620, validation/loss=1.439669, validation/num_examples=50000
I0308 14:59:06.744369 139708415854336 logging_writer.py:48] [74600] global_step=74600, grad_norm=1.1280996799468994, loss=3.5354790687561035
I0308 14:59:51.767285 139708407461632 logging_writer.py:48] [74700] global_step=74700, grad_norm=1.3698633909225464, loss=2.195329189300537
I0308 15:00:36.798393 139708415854336 logging_writer.py:48] [74800] global_step=74800, grad_norm=1.5091103315353394, loss=2.267343759536743
I0308 15:01:22.310726 139708407461632 logging_writer.py:48] [74900] global_step=74900, grad_norm=1.299501657485962, loss=2.104151964187622
I0308 15:02:07.257510 139708415854336 logging_writer.py:48] [75000] global_step=75000, grad_norm=1.197089672088623, loss=2.9816861152648926
I0308 15:02:52.713935 139708407461632 logging_writer.py:48] [75100] global_step=75100, grad_norm=1.242943525314331, loss=2.148736000061035
I0308 15:03:37.930795 139708415854336 logging_writer.py:48] [75200] global_step=75200, grad_norm=1.1268349885940552, loss=2.883810520172119
I0308 15:04:22.947729 139708407461632 logging_writer.py:48] [75300] global_step=75300, grad_norm=1.4786330461502075, loss=2.418254852294922
I0308 15:05:08.166078 139708415854336 logging_writer.py:48] [75400] global_step=75400, grad_norm=1.2105504274368286, loss=4.540453910827637
I0308 15:05:32.550347 139902746892096 spec.py:321] Evaluating on the training split.
I0308 15:05:43.892888 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 15:06:04.695255 139902746892096 spec.py:349] Evaluating on the test split.
I0308 15:06:06.376533 139902746892096 submission_runner.py:411] Time since start: 36289.41s, 	Step: 75456, 	{'train/accuracy': 0.7224413752555847, 'train/loss': 1.1160813570022583, 'validation/accuracy': 0.648419976234436, 'validation/loss': 1.4548654556274414, 'validation/num_examples': 50000, 'test/accuracy': 0.5225000381469727, 'test/loss': 2.114449977874756, 'test/num_examples': 10000, 'score': 33653.806334257126, 'total_duration': 36289.40531396866, 'accumulated_submission_time': 33653.806334257126, 'accumulated_eval_time': 2627.8011043071747, 'accumulated_logging_time': 3.7806742191314697}
I0308 15:06:06.406931 139708407461632 logging_writer.py:48] [75456] accumulated_eval_time=2627.801104, accumulated_logging_time=3.780674, accumulated_submission_time=33653.806334, global_step=75456, preemption_count=0, score=33653.806334, test/accuracy=0.522500, test/loss=2.114450, test/num_examples=10000, total_duration=36289.405314, train/accuracy=0.722441, train/loss=1.116081, validation/accuracy=0.648420, validation/loss=1.454865, validation/num_examples=50000
I0308 15:06:24.167001 139708415854336 logging_writer.py:48] [75500] global_step=75500, grad_norm=1.1993896961212158, loss=3.4664154052734375
I0308 15:07:06.954640 139708407461632 logging_writer.py:48] [75600] global_step=75600, grad_norm=1.269634485244751, loss=2.03737735748291
I0308 15:07:52.296749 139708415854336 logging_writer.py:48] [75700] global_step=75700, grad_norm=1.3802653551101685, loss=2.317859649658203
I0308 15:08:37.683598 139708407461632 logging_writer.py:48] [75800] global_step=75800, grad_norm=1.517226219177246, loss=2.319380283355713
I0308 15:09:22.799874 139708415854336 logging_writer.py:48] [75900] global_step=75900, grad_norm=1.4469934701919556, loss=2.1307549476623535
I0308 15:10:08.016190 139708407461632 logging_writer.py:48] [76000] global_step=76000, grad_norm=1.3318989276885986, loss=3.2099435329437256
I0308 15:10:53.084699 139708415854336 logging_writer.py:48] [76100] global_step=76100, grad_norm=1.4902042150497437, loss=2.2970962524414062
I0308 15:11:38.056195 139708407461632 logging_writer.py:48] [76200] global_step=76200, grad_norm=1.2148250341415405, loss=3.8126771450042725
I0308 15:12:23.245860 139708415854336 logging_writer.py:48] [76300] global_step=76300, grad_norm=1.323323130607605, loss=2.112323045730591
I0308 15:13:06.377671 139902746892096 spec.py:321] Evaluating on the training split.
I0308 15:13:17.973687 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 15:13:37.887666 139902746892096 spec.py:349] Evaluating on the test split.
I0308 15:13:39.580317 139902746892096 submission_runner.py:411] Time since start: 36742.61s, 	Step: 76396, 	{'train/accuracy': 0.70751953125, 'train/loss': 1.1857048273086548, 'validation/accuracy': 0.6567800045013428, 'validation/loss': 1.4257631301879883, 'validation/num_examples': 50000, 'test/accuracy': 0.5279000401496887, 'test/loss': 2.0987911224365234, 'test/num_examples': 10000, 'score': 34073.717185258865, 'total_duration': 36742.60909795761, 'accumulated_submission_time': 34073.717185258865, 'accumulated_eval_time': 2661.0037302970886, 'accumulated_logging_time': 3.822371244430542}
I0308 15:13:39.618397 139708407461632 logging_writer.py:48] [76396] accumulated_eval_time=2661.003730, accumulated_logging_time=3.822371, accumulated_submission_time=34073.717185, global_step=76396, preemption_count=0, score=34073.717185, test/accuracy=0.527900, test/loss=2.098791, test/num_examples=10000, total_duration=36742.609098, train/accuracy=0.707520, train/loss=1.185705, validation/accuracy=0.656780, validation/loss=1.425763, validation/num_examples=50000
I0308 15:13:41.751823 139708415854336 logging_writer.py:48] [76400] global_step=76400, grad_norm=1.514667272567749, loss=2.283780574798584
I0308 15:14:22.387992 139708407461632 logging_writer.py:48] [76500] global_step=76500, grad_norm=1.1830686330795288, loss=4.015862464904785
I0308 15:15:07.607748 139708415854336 logging_writer.py:48] [76600] global_step=76600, grad_norm=1.4200118780136108, loss=2.1689517498016357
I0308 15:15:52.732975 139708407461632 logging_writer.py:48] [76700] global_step=76700, grad_norm=1.4621992111206055, loss=2.0490310192108154
I0308 15:16:38.109156 139708415854336 logging_writer.py:48] [76800] global_step=76800, grad_norm=1.2682985067367554, loss=2.4110469818115234
I0308 15:17:23.277089 139708407461632 logging_writer.py:48] [76900] global_step=76900, grad_norm=1.3218364715576172, loss=2.345742702484131
I0308 15:18:08.670130 139708415854336 logging_writer.py:48] [77000] global_step=77000, grad_norm=1.1693897247314453, loss=3.722935199737549
I0308 15:18:53.762269 139708407461632 logging_writer.py:48] [77100] global_step=77100, grad_norm=1.324159026145935, loss=2.19858980178833
I0308 15:19:39.111068 139708415854336 logging_writer.py:48] [77200] global_step=77200, grad_norm=1.4011024236679077, loss=2.2550125122070312
I0308 15:20:24.479833 139708407461632 logging_writer.py:48] [77300] global_step=77300, grad_norm=1.5193688869476318, loss=2.043544292449951
I0308 15:20:39.633500 139902746892096 spec.py:321] Evaluating on the training split.
I0308 15:20:51.000301 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 15:21:10.076171 139902746892096 spec.py:349] Evaluating on the test split.
I0308 15:21:11.757796 139902746892096 submission_runner.py:411] Time since start: 37194.79s, 	Step: 77335, 	{'train/accuracy': 0.71533203125, 'train/loss': 1.1208672523498535, 'validation/accuracy': 0.6605199575424194, 'validation/loss': 1.3832670450210571, 'validation/num_examples': 50000, 'test/accuracy': 0.5332000255584717, 'test/loss': 2.0563156604766846, 'test/num_examples': 10000, 'score': 34493.6725795269, 'total_duration': 37194.786583185196, 'accumulated_submission_time': 34493.6725795269, 'accumulated_eval_time': 2693.128019094467, 'accumulated_logging_time': 3.871058940887451}
I0308 15:21:11.791554 139708415854336 logging_writer.py:48] [77335] accumulated_eval_time=2693.128019, accumulated_logging_time=3.871059, accumulated_submission_time=34493.672580, global_step=77335, preemption_count=0, score=34493.672580, test/accuracy=0.533200, test/loss=2.056316, test/num_examples=10000, total_duration=37194.786583, train/accuracy=0.715332, train/loss=1.120867, validation/accuracy=0.660520, validation/loss=1.383267, validation/num_examples=50000
I0308 15:21:37.809606 139708407461632 logging_writer.py:48] [77400] global_step=77400, grad_norm=1.1645265817642212, loss=4.681710720062256
I0308 15:22:22.179320 139708415854336 logging_writer.py:48] [77500] global_step=77500, grad_norm=1.3170472383499146, loss=3.0551350116729736
I0308 15:23:08.002350 139708407461632 logging_writer.py:48] [77600] global_step=77600, grad_norm=1.3576792478561401, loss=2.142289638519287
I0308 15:23:53.413107 139708415854336 logging_writer.py:48] [77700] global_step=77700, grad_norm=1.2495862245559692, loss=2.7092745304107666
I0308 15:24:38.596476 139708407461632 logging_writer.py:48] [77800] global_step=77800, grad_norm=1.2443710565567017, loss=3.7637739181518555
I0308 15:25:24.231342 139708415854336 logging_writer.py:48] [77900] global_step=77900, grad_norm=1.2970056533813477, loss=2.134140729904175
I0308 15:26:09.605782 139708407461632 logging_writer.py:48] [78000] global_step=78000, grad_norm=1.1591466665267944, loss=4.297977447509766
I0308 15:26:54.816797 139708415854336 logging_writer.py:48] [78100] global_step=78100, grad_norm=1.5180598497390747, loss=2.178213119506836
I0308 15:27:40.330292 139708407461632 logging_writer.py:48] [78200] global_step=78200, grad_norm=1.2156230211257935, loss=4.329654693603516
I0308 15:28:12.165341 139902746892096 spec.py:321] Evaluating on the training split.
I0308 15:28:23.567611 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 15:28:44.549915 139902746892096 spec.py:349] Evaluating on the test split.
I0308 15:28:46.217214 139902746892096 submission_runner.py:411] Time since start: 37649.25s, 	Step: 78272, 	{'train/accuracy': 0.7160937190055847, 'train/loss': 1.1190756559371948, 'validation/accuracy': 0.6571199893951416, 'validation/loss': 1.4038711786270142, 'validation/num_examples': 50000, 'test/accuracy': 0.5371000170707703, 'test/loss': 2.056650161743164, 'test/num_examples': 10000, 'score': 34913.984763622284, 'total_duration': 37649.24601197243, 'accumulated_submission_time': 34913.984763622284, 'accumulated_eval_time': 2727.1798944473267, 'accumulated_logging_time': 3.917135238647461}
I0308 15:28:46.247453 139708415854336 logging_writer.py:48] [78272] accumulated_eval_time=2727.179894, accumulated_logging_time=3.917135, accumulated_submission_time=34913.984764, global_step=78272, preemption_count=0, score=34913.984764, test/accuracy=0.537100, test/loss=2.056650, test/num_examples=10000, total_duration=37649.246012, train/accuracy=0.716094, train/loss=1.119076, validation/accuracy=0.657120, validation/loss=1.403871, validation/num_examples=50000
I0308 15:28:57.665110 139708407461632 logging_writer.py:48] [78300] global_step=78300, grad_norm=1.1845206022262573, loss=3.010767698287964
I0308 15:29:38.843913 139708415854336 logging_writer.py:48] [78400] global_step=78400, grad_norm=1.3587532043457031, loss=2.0557971000671387
I0308 15:30:23.826348 139708407461632 logging_writer.py:48] [78500] global_step=78500, grad_norm=1.3309134244918823, loss=2.1635165214538574
I0308 15:31:09.202240 139708415854336 logging_writer.py:48] [78600] global_step=78600, grad_norm=1.3991318941116333, loss=2.25529146194458
I0308 15:31:54.422459 139708407461632 logging_writer.py:48] [78700] global_step=78700, grad_norm=1.157871127128601, loss=3.5344252586364746
I0308 15:32:39.649960 139708415854336 logging_writer.py:48] [78800] global_step=78800, grad_norm=1.3559378385543823, loss=2.099294900894165
I0308 15:33:25.141821 139708407461632 logging_writer.py:48] [78900] global_step=78900, grad_norm=1.4256296157836914, loss=2.4225919246673584
I0308 15:34:10.248530 139708415854336 logging_writer.py:48] [79000] global_step=79000, grad_norm=1.374393105506897, loss=2.035996675491333
I0308 15:34:55.495013 139708407461632 logging_writer.py:48] [79100] global_step=79100, grad_norm=1.2524163722991943, loss=2.6960859298706055
I0308 15:35:40.779405 139708415854336 logging_writer.py:48] [79200] global_step=79200, grad_norm=1.4670895338058472, loss=2.23268461227417
I0308 15:35:46.285327 139902746892096 spec.py:321] Evaluating on the training split.
I0308 15:35:57.599707 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 15:36:18.222430 139902746892096 spec.py:349] Evaluating on the test split.
I0308 15:36:19.913109 139902746892096 submission_runner.py:411] Time since start: 38102.94s, 	Step: 79214, 	{'train/accuracy': 0.716015636920929, 'train/loss': 1.125848412513733, 'validation/accuracy': 0.6620999574661255, 'validation/loss': 1.3742302656173706, 'validation/num_examples': 50000, 'test/accuracy': 0.5348000526428223, 'test/loss': 2.051039934158325, 'test/num_examples': 10000, 'score': 35333.96521568298, 'total_duration': 38102.94189476967, 'accumulated_submission_time': 35333.96521568298, 'accumulated_eval_time': 2760.8076634407043, 'accumulated_logging_time': 3.956195592880249}
I0308 15:36:19.944581 139708407461632 logging_writer.py:48] [79214] accumulated_eval_time=2760.807663, accumulated_logging_time=3.956196, accumulated_submission_time=35333.965216, global_step=79214, preemption_count=0, score=35333.965216, test/accuracy=0.534800, test/loss=2.051040, test/num_examples=10000, total_duration=38102.941895, train/accuracy=0.716016, train/loss=1.125848, validation/accuracy=0.662100, validation/loss=1.374230, validation/num_examples=50000
I0308 15:36:54.226303 139708415854336 logging_writer.py:48] [79300] global_step=79300, grad_norm=1.3042598962783813, loss=2.1486096382141113
I0308 15:37:39.371369 139708407461632 logging_writer.py:48] [79400] global_step=79400, grad_norm=1.3627420663833618, loss=2.1672089099884033
I0308 15:38:24.557446 139708415854336 logging_writer.py:48] [79500] global_step=79500, grad_norm=1.2250727415084839, loss=4.5855937004089355
I0308 15:39:09.821489 139708407461632 logging_writer.py:48] [79600] global_step=79600, grad_norm=1.1841777563095093, loss=3.4365456104278564
I0308 15:39:54.993802 139708415854336 logging_writer.py:48] [79700] global_step=79700, grad_norm=1.2871090173721313, loss=2.790076732635498
I0308 15:40:40.083291 139708407461632 logging_writer.py:48] [79800] global_step=79800, grad_norm=1.1705821752548218, loss=3.696012258529663
I0308 15:41:25.761566 139708415854336 logging_writer.py:48] [79900] global_step=79900, grad_norm=1.2830874919891357, loss=2.0102617740631104
I0308 15:42:10.867155 139708407461632 logging_writer.py:48] [80000] global_step=80000, grad_norm=1.496536374092102, loss=2.166990041732788
I0308 15:42:56.319010 139708415854336 logging_writer.py:48] [80100] global_step=80100, grad_norm=1.1775521039962769, loss=4.831473350524902
I0308 15:43:19.951525 139902746892096 spec.py:321] Evaluating on the training split.
I0308 15:43:31.557319 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 15:43:51.317236 139902746892096 spec.py:349] Evaluating on the test split.
I0308 15:43:53.004084 139902746892096 submission_runner.py:411] Time since start: 38556.03s, 	Step: 80154, 	{'train/accuracy': 0.7089648246765137, 'train/loss': 1.1660743951797485, 'validation/accuracy': 0.6569399833679199, 'validation/loss': 1.411213755607605, 'validation/num_examples': 50000, 'test/accuracy': 0.5333999991416931, 'test/loss': 2.053675651550293, 'test/num_examples': 10000, 'score': 35753.91080546379, 'total_duration': 38556.032870054245, 'accumulated_submission_time': 35753.91080546379, 'accumulated_eval_time': 2793.8601999282837, 'accumulated_logging_time': 3.9993815422058105}
I0308 15:43:53.036378 139708407461632 logging_writer.py:48] [80154] accumulated_eval_time=2793.860200, accumulated_logging_time=3.999382, accumulated_submission_time=35753.910805, global_step=80154, preemption_count=0, score=35753.910805, test/accuracy=0.533400, test/loss=2.053676, test/num_examples=10000, total_duration=38556.032870, train/accuracy=0.708965, train/loss=1.166074, validation/accuracy=0.656940, validation/loss=1.411214, validation/num_examples=50000
I0308 15:44:11.572270 139708415854336 logging_writer.py:48] [80200] global_step=80200, grad_norm=1.1086599826812744, loss=4.513968467712402
I0308 15:44:54.948329 139708407461632 logging_writer.py:48] [80300] global_step=80300, grad_norm=1.322644829750061, loss=4.715424537658691
I0308 15:45:40.268378 139708415854336 logging_writer.py:48] [80400] global_step=80400, grad_norm=1.2062801122665405, loss=2.60992693901062
I0308 15:46:25.817030 139708407461632 logging_writer.py:48] [80500] global_step=80500, grad_norm=1.3705642223358154, loss=2.3125476837158203
I0308 15:47:10.820620 139708415854336 logging_writer.py:48] [80600] global_step=80600, grad_norm=1.3947163820266724, loss=2.07417631149292
I0308 15:47:56.274073 139708407461632 logging_writer.py:48] [80700] global_step=80700, grad_norm=1.4809037446975708, loss=2.1548609733581543
I0308 15:48:41.667143 139708415854336 logging_writer.py:48] [80800] global_step=80800, grad_norm=1.4538193941116333, loss=2.0417017936706543
I0308 15:49:26.917312 139708407461632 logging_writer.py:48] [80900] global_step=80900, grad_norm=1.23784339427948, loss=3.3878355026245117
I0308 15:50:12.057140 139708415854336 logging_writer.py:48] [81000] global_step=81000, grad_norm=1.437213659286499, loss=3.943955183029175
I0308 15:50:53.166659 139902746892096 spec.py:321] Evaluating on the training split.
I0308 15:51:04.609359 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 15:51:24.828927 139902746892096 spec.py:349] Evaluating on the test split.
I0308 15:51:26.516651 139902746892096 submission_runner.py:411] Time since start: 39009.55s, 	Step: 81093, 	{'train/accuracy': 0.7257421612739563, 'train/loss': 1.0833772420883179, 'validation/accuracy': 0.6646599769592285, 'validation/loss': 1.3715866804122925, 'validation/num_examples': 50000, 'test/accuracy': 0.5402000546455383, 'test/loss': 2.0339879989624023, 'test/num_examples': 10000, 'score': 36173.98077607155, 'total_duration': 39009.54544401169, 'accumulated_submission_time': 36173.98077607155, 'accumulated_eval_time': 2827.2101967334747, 'accumulated_logging_time': 4.042644023895264}
I0308 15:51:26.543867 139708407461632 logging_writer.py:48] [81093] accumulated_eval_time=2827.210197, accumulated_logging_time=4.042644, accumulated_submission_time=36173.980776, global_step=81093, preemption_count=0, score=36173.980776, test/accuracy=0.540200, test/loss=2.033988, test/num_examples=10000, total_duration=39009.545444, train/accuracy=0.725742, train/loss=1.083377, validation/accuracy=0.664660, validation/loss=1.371587, validation/num_examples=50000
I0308 15:51:29.711890 139708415854336 logging_writer.py:48] [81100] global_step=81100, grad_norm=1.4605605602264404, loss=2.2217636108398438
I0308 15:52:10.316050 139708407461632 logging_writer.py:48] [81200] global_step=81200, grad_norm=1.3228697776794434, loss=2.0226728916168213
I0308 15:52:55.232760 139708415854336 logging_writer.py:48] [81300] global_step=81300, grad_norm=1.3422942161560059, loss=2.086193084716797
I0308 15:53:40.966296 139708407461632 logging_writer.py:48] [81400] global_step=81400, grad_norm=1.372678518295288, loss=2.2169878482818604
I0308 15:54:26.441013 139708415854336 logging_writer.py:48] [81500] global_step=81500, grad_norm=1.3532452583312988, loss=2.0830516815185547
I0308 15:55:11.619513 139708407461632 logging_writer.py:48] [81600] global_step=81600, grad_norm=1.285567283630371, loss=3.439253568649292
I0308 15:55:57.019011 139708415854336 logging_writer.py:48] [81700] global_step=81700, grad_norm=1.3871132135391235, loss=2.0990114212036133
I0308 15:56:42.196159 139708407461632 logging_writer.py:48] [81800] global_step=81800, grad_norm=1.1661626100540161, loss=3.76130747795105
I0308 15:57:27.453907 139708415854336 logging_writer.py:48] [81900] global_step=81900, grad_norm=1.4404646158218384, loss=2.1045289039611816
I0308 15:58:12.865175 139708407461632 logging_writer.py:48] [82000] global_step=82000, grad_norm=1.374018669128418, loss=2.350267171859741
I0308 15:58:26.629936 139902746892096 spec.py:321] Evaluating on the training split.
I0308 15:58:37.817009 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 15:58:58.442903 139902746892096 spec.py:349] Evaluating on the test split.
I0308 15:59:00.115858 139902746892096 submission_runner.py:411] Time since start: 39463.14s, 	Step: 82032, 	{'train/accuracy': 0.7426952719688416, 'train/loss': 1.036612629890442, 'validation/accuracy': 0.6611599922180176, 'validation/loss': 1.392037034034729, 'validation/num_examples': 50000, 'test/accuracy': 0.5360000133514404, 'test/loss': 2.0379531383514404, 'test/num_examples': 10000, 'score': 36594.00684118271, 'total_duration': 39463.14464759827, 'accumulated_submission_time': 36594.00684118271, 'accumulated_eval_time': 2860.696093082428, 'accumulated_logging_time': 4.080122947692871}
I0308 15:59:00.143537 139708415854336 logging_writer.py:48] [82032] accumulated_eval_time=2860.696093, accumulated_logging_time=4.080123, accumulated_submission_time=36594.006841, global_step=82032, preemption_count=0, score=36594.006841, test/accuracy=0.536000, test/loss=2.037953, test/num_examples=10000, total_duration=39463.144648, train/accuracy=0.742695, train/loss=1.036613, validation/accuracy=0.661160, validation/loss=1.392037, validation/num_examples=50000
I0308 15:59:27.314723 139708407461632 logging_writer.py:48] [82100] global_step=82100, grad_norm=1.3428666591644287, loss=2.114581346511841
I0308 16:00:11.117993 139708415854336 logging_writer.py:48] [82200] global_step=82200, grad_norm=1.446851372718811, loss=2.105384111404419
I0308 16:00:56.225313 139708407461632 logging_writer.py:48] [82300] global_step=82300, grad_norm=1.3209935426712036, loss=2.431561231613159
I0308 16:01:41.853374 139708415854336 logging_writer.py:48] [82400] global_step=82400, grad_norm=1.359682559967041, loss=2.1096179485321045
I0308 16:02:26.931781 139708407461632 logging_writer.py:48] [82500] global_step=82500, grad_norm=1.1738392114639282, loss=3.2568325996398926
I0308 16:03:12.706529 139708415854336 logging_writer.py:48] [82600] global_step=82600, grad_norm=1.2829949855804443, loss=2.441178321838379
I0308 16:03:58.159754 139708407461632 logging_writer.py:48] [82700] global_step=82700, grad_norm=1.2912962436676025, loss=2.1987228393554688
I0308 16:04:43.336639 139708415854336 logging_writer.py:48] [82800] global_step=82800, grad_norm=1.488930583000183, loss=2.136662483215332
I0308 16:05:28.757973 139708407461632 logging_writer.py:48] [82900] global_step=82900, grad_norm=1.3132280111312866, loss=2.842529773712158
I0308 16:06:00.436725 139902746892096 spec.py:321] Evaluating on the training split.
I0308 16:06:11.926611 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 16:06:32.641891 139902746892096 spec.py:349] Evaluating on the test split.
I0308 16:06:34.319541 139902746892096 submission_runner.py:411] Time since start: 39917.35s, 	Step: 82972, 	{'train/accuracy': 0.7173437476158142, 'train/loss': 1.1371502876281738, 'validation/accuracy': 0.6619399785995483, 'validation/loss': 1.3823440074920654, 'validation/num_examples': 50000, 'test/accuracy': 0.5356000065803528, 'test/loss': 2.0468437671661377, 'test/num_examples': 10000, 'score': 37014.24144101143, 'total_duration': 39917.348328113556, 'accumulated_submission_time': 37014.24144101143, 'accumulated_eval_time': 2894.578936815262, 'accumulated_logging_time': 4.116491079330444}
I0308 16:06:34.347352 139708415854336 logging_writer.py:48] [82972] accumulated_eval_time=2894.578937, accumulated_logging_time=4.116491, accumulated_submission_time=37014.241441, global_step=82972, preemption_count=0, score=37014.241441, test/accuracy=0.535600, test/loss=2.046844, test/num_examples=10000, total_duration=39917.348328, train/accuracy=0.717344, train/loss=1.137150, validation/accuracy=0.661940, validation/loss=1.382344, validation/num_examples=50000
I0308 16:06:45.766252 139708407461632 logging_writer.py:48] [83000] global_step=83000, grad_norm=1.4453136920928955, loss=2.112278938293457
I0308 16:07:27.444987 139708415854336 logging_writer.py:48] [83100] global_step=83100, grad_norm=1.1845825910568237, loss=3.1021950244903564
I0308 16:08:12.918714 139708407461632 logging_writer.py:48] [83200] global_step=83200, grad_norm=1.2614389657974243, loss=3.0876951217651367
I0308 16:08:58.445271 139708415854336 logging_writer.py:48] [83300] global_step=83300, grad_norm=1.3205702304840088, loss=2.768796682357788
I0308 16:09:43.576759 139708407461632 logging_writer.py:48] [83400] global_step=83400, grad_norm=1.1461644172668457, loss=3.4481546878814697
I0308 16:10:28.696360 139708415854336 logging_writer.py:48] [83500] global_step=83500, grad_norm=1.3209123611450195, loss=2.537492275238037
I0308 16:11:13.999237 139708407461632 logging_writer.py:48] [83600] global_step=83600, grad_norm=1.3919514417648315, loss=2.0975711345672607
I0308 16:11:59.340714 139708415854336 logging_writer.py:48] [83700] global_step=83700, grad_norm=1.3206359148025513, loss=3.87050461769104
I0308 16:12:44.344699 139708407461632 logging_writer.py:48] [83800] global_step=83800, grad_norm=1.3345931768417358, loss=4.593923568725586
I0308 16:13:29.715143 139708415854336 logging_writer.py:48] [83900] global_step=83900, grad_norm=1.160381555557251, loss=3.97918701171875
I0308 16:13:34.431489 139902746892096 spec.py:321] Evaluating on the training split.
I0308 16:13:45.801543 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 16:14:07.285448 139902746892096 spec.py:349] Evaluating on the test split.
I0308 16:14:08.954007 139902746892096 submission_runner.py:411] Time since start: 40371.98s, 	Step: 83912, 	{'train/accuracy': 0.7253515720367432, 'train/loss': 1.1027836799621582, 'validation/accuracy': 0.6668999791145325, 'validation/loss': 1.3718284368515015, 'validation/num_examples': 50000, 'test/accuracy': 0.5403000116348267, 'test/loss': 2.028369903564453, 'test/num_examples': 10000, 'score': 37434.266563653946, 'total_duration': 40371.982800245285, 'accumulated_submission_time': 37434.266563653946, 'accumulated_eval_time': 2929.101461648941, 'accumulated_logging_time': 4.154000282287598}
I0308 16:14:08.987351 139708407461632 logging_writer.py:48] [83912] accumulated_eval_time=2929.101462, accumulated_logging_time=4.154000, accumulated_submission_time=37434.266564, global_step=83912, preemption_count=0, score=37434.266564, test/accuracy=0.540300, test/loss=2.028370, test/num_examples=10000, total_duration=40371.982800, train/accuracy=0.725352, train/loss=1.102784, validation/accuracy=0.666900, validation/loss=1.371828, validation/num_examples=50000
I0308 16:14:44.175442 139708415854336 logging_writer.py:48] [84000] global_step=84000, grad_norm=1.4930434226989746, loss=2.025648593902588
I0308 16:15:28.953951 139708407461632 logging_writer.py:48] [84100] global_step=84100, grad_norm=1.284210443496704, loss=3.385056972503662
I0308 16:16:14.454549 139708415854336 logging_writer.py:48] [84200] global_step=84200, grad_norm=1.2699759006500244, loss=3.594897747039795
I0308 16:16:59.695306 139708407461632 logging_writer.py:48] [84300] global_step=84300, grad_norm=1.3071061372756958, loss=3.6044204235076904
I0308 16:17:44.986846 139708415854336 logging_writer.py:48] [84400] global_step=84400, grad_norm=1.3908032178878784, loss=2.2656235694885254
I0308 16:18:30.444156 139708407461632 logging_writer.py:48] [84500] global_step=84500, grad_norm=1.1848535537719727, loss=3.4445078372955322
I0308 16:19:15.327506 139708415854336 logging_writer.py:48] [84600] global_step=84600, grad_norm=1.372837781906128, loss=2.394649028778076
I0308 16:20:00.554967 139708407461632 logging_writer.py:48] [84700] global_step=84700, grad_norm=1.4374665021896362, loss=2.0910604000091553
I0308 16:20:45.685283 139708415854336 logging_writer.py:48] [84800] global_step=84800, grad_norm=1.3892804384231567, loss=2.6794724464416504
I0308 16:21:09.348567 139902746892096 spec.py:321] Evaluating on the training split.
I0308 16:21:20.726453 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 16:21:39.734109 139902746892096 spec.py:349] Evaluating on the test split.
I0308 16:21:41.418184 139902746892096 submission_runner.py:411] Time since start: 40824.45s, 	Step: 84854, 	{'train/accuracy': 0.7344335913658142, 'train/loss': 1.063029170036316, 'validation/accuracy': 0.6647999882698059, 'validation/loss': 1.370414137840271, 'validation/num_examples': 50000, 'test/accuracy': 0.5380000472068787, 'test/loss': 2.027428150177002, 'test/num_examples': 10000, 'score': 37854.56867027283, 'total_duration': 40824.446971178055, 'accumulated_submission_time': 37854.56867027283, 'accumulated_eval_time': 2961.1710698604584, 'accumulated_logging_time': 4.19693660736084}
I0308 16:21:41.453746 139708407461632 logging_writer.py:48] [84854] accumulated_eval_time=2961.171070, accumulated_logging_time=4.196937, accumulated_submission_time=37854.568670, global_step=84854, preemption_count=0, score=37854.568670, test/accuracy=0.538000, test/loss=2.027428, test/num_examples=10000, total_duration=40824.446971, train/accuracy=0.734434, train/loss=1.063029, validation/accuracy=0.664800, validation/loss=1.370414, validation/num_examples=50000
I0308 16:21:59.993849 139708415854336 logging_writer.py:48] [84900] global_step=84900, grad_norm=1.2917932271957397, loss=2.241027355194092
I0308 16:22:43.293947 139708407461632 logging_writer.py:48] [85000] global_step=85000, grad_norm=1.3570979833602905, loss=2.1799044609069824
I0308 16:23:28.887663 139708415854336 logging_writer.py:48] [85100] global_step=85100, grad_norm=1.2900478839874268, loss=2.988530158996582
I0308 16:24:14.508876 139708407461632 logging_writer.py:48] [85200] global_step=85200, grad_norm=1.3018990755081177, loss=3.4037413597106934
I0308 16:24:59.686955 139708415854336 logging_writer.py:48] [85300] global_step=85300, grad_norm=1.3731820583343506, loss=2.0873398780822754
I0308 16:25:44.901964 139708407461632 logging_writer.py:48] [85400] global_step=85400, grad_norm=1.3502532243728638, loss=2.1189146041870117
I0308 16:26:30.057247 139708415854336 logging_writer.py:48] [85500] global_step=85500, grad_norm=1.4478446245193481, loss=2.0765531063079834
I0308 16:27:15.362556 139708407461632 logging_writer.py:48] [85600] global_step=85600, grad_norm=1.2687898874282837, loss=2.6620032787323
I0308 16:28:00.646682 139708415854336 logging_writer.py:48] [85700] global_step=85700, grad_norm=1.246960163116455, loss=3.56668758392334
I0308 16:28:41.579694 139902746892096 spec.py:321] Evaluating on the training split.
I0308 16:28:52.933072 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 16:29:13.517360 139902746892096 spec.py:349] Evaluating on the test split.
I0308 16:29:15.194402 139902746892096 submission_runner.py:411] Time since start: 41278.22s, 	Step: 85792, 	{'train/accuracy': 0.7220702767372131, 'train/loss': 1.1111823320388794, 'validation/accuracy': 0.6682999730110168, 'validation/loss': 1.3621902465820312, 'validation/num_examples': 50000, 'test/accuracy': 0.5444000363349915, 'test/loss': 2.0217902660369873, 'test/num_examples': 10000, 'score': 38274.63127756119, 'total_duration': 41278.22320103645, 'accumulated_submission_time': 38274.63127756119, 'accumulated_eval_time': 2994.7857854366302, 'accumulated_logging_time': 4.246838569641113}
I0308 16:29:15.226503 139708407461632 logging_writer.py:48] [85792] accumulated_eval_time=2994.785785, accumulated_logging_time=4.246839, accumulated_submission_time=38274.631278, global_step=85792, preemption_count=0, score=38274.631278, test/accuracy=0.544400, test/loss=2.021790, test/num_examples=10000, total_duration=41278.223201, train/accuracy=0.722070, train/loss=1.111182, validation/accuracy=0.668300, validation/loss=1.362190, validation/num_examples=50000
I0308 16:29:18.780446 139708415854336 logging_writer.py:48] [85800] global_step=85800, grad_norm=1.3234833478927612, loss=2.2127270698547363
I0308 16:29:59.331298 139708407461632 logging_writer.py:48] [85900] global_step=85900, grad_norm=1.2807157039642334, loss=3.856674909591675
I0308 16:30:44.313343 139708415854336 logging_writer.py:48] [86000] global_step=86000, grad_norm=1.3721394538879395, loss=2.0711469650268555
I0308 16:31:29.535705 139708407461632 logging_writer.py:48] [86100] global_step=86100, grad_norm=1.3238886594772339, loss=2.60219669342041
I0308 16:32:15.086026 139708415854336 logging_writer.py:48] [86200] global_step=86200, grad_norm=1.3857911825180054, loss=2.0823817253112793
I0308 16:33:00.294175 139708407461632 logging_writer.py:48] [86300] global_step=86300, grad_norm=1.3948593139648438, loss=2.250795364379883
I0308 16:33:45.859670 139708415854336 logging_writer.py:48] [86400] global_step=86400, grad_norm=1.3274537324905396, loss=1.9730589389801025
I0308 16:34:30.825028 139708415854336 logging_writer.py:48] [86500] global_step=86500, grad_norm=1.2418248653411865, loss=4.1865057945251465
I0308 16:35:16.088346 139708407461632 logging_writer.py:48] [86600] global_step=86600, grad_norm=1.4003839492797852, loss=2.0749738216400146
I0308 16:36:01.547348 139708415854336 logging_writer.py:48] [86700] global_step=86700, grad_norm=1.509786605834961, loss=2.0045876502990723
I0308 16:36:15.206218 139902746892096 spec.py:321] Evaluating on the training split.
I0308 16:36:26.569839 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 16:36:46.985834 139902746892096 spec.py:349] Evaluating on the test split.
I0308 16:36:48.657056 139902746892096 submission_runner.py:411] Time since start: 41731.69s, 	Step: 86732, 	{'train/accuracy': 0.72572261095047, 'train/loss': 1.0874943733215332, 'validation/accuracy': 0.6696000099182129, 'validation/loss': 1.3518980741500854, 'validation/num_examples': 50000, 'test/accuracy': 0.5490000247955322, 'test/loss': 1.991302251815796, 'test/num_examples': 10000, 'score': 38694.55052232742, 'total_duration': 41731.68585777283, 'accumulated_submission_time': 38694.55052232742, 'accumulated_eval_time': 3028.2366137504578, 'accumulated_logging_time': 4.2903008460998535}
I0308 16:36:48.685275 139708407461632 logging_writer.py:48] [86732] accumulated_eval_time=3028.236614, accumulated_logging_time=4.290301, accumulated_submission_time=38694.550522, global_step=86732, preemption_count=0, score=38694.550522, test/accuracy=0.549000, test/loss=1.991302, test/num_examples=10000, total_duration=41731.685858, train/accuracy=0.725723, train/loss=1.087494, validation/accuracy=0.669600, validation/loss=1.351898, validation/num_examples=50000
I0308 16:37:15.840150 139708415854336 logging_writer.py:48] [86800] global_step=86800, grad_norm=1.3551430702209473, loss=4.295032024383545
I0308 16:37:59.744442 139708407461632 logging_writer.py:48] [86900] global_step=86900, grad_norm=1.3898372650146484, loss=2.795945405960083
I0308 16:38:44.933994 139708415854336 logging_writer.py:48] [87000] global_step=87000, grad_norm=1.267432689666748, loss=2.8372082710266113
I0308 16:39:30.343293 139708407461632 logging_writer.py:48] [87100] global_step=87100, grad_norm=1.5119503736495972, loss=4.678824424743652
I0308 16:40:15.388093 139708415854336 logging_writer.py:48] [87200] global_step=87200, grad_norm=1.384371280670166, loss=2.988640069961548
I0308 16:41:00.421353 139708407461632 logging_writer.py:48] [87300] global_step=87300, grad_norm=1.359330654144287, loss=2.01265549659729
I0308 16:41:45.635167 139708415854336 logging_writer.py:48] [87400] global_step=87400, grad_norm=1.3852999210357666, loss=2.5226354598999023
I0308 16:42:30.696694 139708407461632 logging_writer.py:48] [87500] global_step=87500, grad_norm=1.4175491333007812, loss=1.9691369533538818
I0308 16:43:16.078406 139708415854336 logging_writer.py:48] [87600] global_step=87600, grad_norm=1.358579397201538, loss=2.532102346420288
I0308 16:43:48.896131 139902746892096 spec.py:321] Evaluating on the training split.
I0308 16:44:00.400108 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 16:44:19.689935 139902746892096 spec.py:349] Evaluating on the test split.
I0308 16:44:21.373568 139902746892096 submission_runner.py:411] Time since start: 42184.40s, 	Step: 87674, 	{'train/accuracy': 0.7313281297683716, 'train/loss': 1.0686558485031128, 'validation/accuracy': 0.6709199547767639, 'validation/loss': 1.350008249282837, 'validation/num_examples': 50000, 'test/accuracy': 0.5430000424385071, 'test/loss': 2.0079195499420166, 'test/num_examples': 10000, 'score': 39114.70210146904, 'total_duration': 42184.402356147766, 'accumulated_submission_time': 39114.70210146904, 'accumulated_eval_time': 3060.714049100876, 'accumulated_logging_time': 4.327760934829712}
I0308 16:44:21.414791 139708407461632 logging_writer.py:48] [87674] accumulated_eval_time=3060.714049, accumulated_logging_time=4.327761, accumulated_submission_time=39114.702101, global_step=87674, preemption_count=0, score=39114.702101, test/accuracy=0.543000, test/loss=2.007920, test/num_examples=10000, total_duration=42184.402356, train/accuracy=0.731328, train/loss=1.068656, validation/accuracy=0.670920, validation/loss=1.350008, validation/num_examples=50000
I0308 16:44:32.057036 139708415854336 logging_writer.py:48] [87700] global_step=87700, grad_norm=1.3267446756362915, loss=3.8047614097595215
I0308 16:45:14.152638 139708407461632 logging_writer.py:48] [87800] global_step=87800, grad_norm=1.4053136110305786, loss=2.0531797409057617
I0308 16:45:59.258690 139708415854336 logging_writer.py:48] [87900] global_step=87900, grad_norm=1.3311229944229126, loss=2.245227813720703
I0308 16:46:44.515744 139708407461632 logging_writer.py:48] [88000] global_step=88000, grad_norm=1.3284728527069092, loss=4.4441142082214355
I0308 16:47:29.625645 139708415854336 logging_writer.py:48] [88100] global_step=88100, grad_norm=1.5032283067703247, loss=2.0561909675598145
I0308 16:48:14.951925 139708407461632 logging_writer.py:48] [88200] global_step=88200, grad_norm=1.4534496068954468, loss=1.9639512300491333
I0308 16:49:00.185559 139708415854336 logging_writer.py:48] [88300] global_step=88300, grad_norm=1.4868736267089844, loss=2.0049173831939697
I0308 16:49:45.191634 139708407461632 logging_writer.py:48] [88400] global_step=88400, grad_norm=1.3625788688659668, loss=3.347428560256958
I0308 16:50:30.278112 139708415854336 logging_writer.py:48] [88500] global_step=88500, grad_norm=1.507848858833313, loss=2.396317720413208
I0308 16:51:15.504493 139708407461632 logging_writer.py:48] [88600] global_step=88600, grad_norm=1.2856740951538086, loss=2.68088436126709
I0308 16:51:21.504593 139902746892096 spec.py:321] Evaluating on the training split.
I0308 16:51:32.891988 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 16:51:53.687316 139902746892096 spec.py:349] Evaluating on the test split.
I0308 16:51:55.366763 139902746892096 submission_runner.py:411] Time since start: 42638.40s, 	Step: 88615, 	{'train/accuracy': 0.7570117115974426, 'train/loss': 0.9418240785598755, 'validation/accuracy': 0.6727799773216248, 'validation/loss': 1.3221476078033447, 'validation/num_examples': 50000, 'test/accuracy': 0.5525000095367432, 'test/loss': 1.9742674827575684, 'test/num_examples': 10000, 'score': 39534.7329928875, 'total_duration': 42638.395560741425, 'accumulated_submission_time': 39534.7329928875, 'accumulated_eval_time': 3094.576204776764, 'accumulated_logging_time': 4.378791570663452}
I0308 16:51:55.397464 139708415854336 logging_writer.py:48] [88615] accumulated_eval_time=3094.576205, accumulated_logging_time=4.378792, accumulated_submission_time=39534.732993, global_step=88615, preemption_count=0, score=39534.732993, test/accuracy=0.552500, test/loss=1.974267, test/num_examples=10000, total_duration=42638.395561, train/accuracy=0.757012, train/loss=0.941824, validation/accuracy=0.672780, validation/loss=1.322148, validation/num_examples=50000
I0308 16:52:29.287163 139708407461632 logging_writer.py:48] [88700] global_step=88700, grad_norm=1.2952736616134644, loss=4.335161209106445
I0308 16:53:14.342425 139708415854336 logging_writer.py:48] [88800] global_step=88800, grad_norm=1.4695967435836792, loss=1.925597906112671
I0308 16:54:00.001287 139708407461632 logging_writer.py:48] [88900] global_step=88900, grad_norm=1.2382030487060547, loss=2.9492642879486084
I0308 16:54:45.322181 139708415854336 logging_writer.py:48] [89000] global_step=89000, grad_norm=1.4410394430160522, loss=1.9356192350387573
I0308 16:55:30.313314 139708407461632 logging_writer.py:48] [89100] global_step=89100, grad_norm=1.6413021087646484, loss=2.131218910217285
I0308 16:56:15.703367 139708415854336 logging_writer.py:48] [89200] global_step=89200, grad_norm=1.4175575971603394, loss=2.437410831451416
I0308 16:57:00.695192 139708407461632 logging_writer.py:48] [89300] global_step=89300, grad_norm=1.3143925666809082, loss=4.3956146240234375
I0308 16:57:46.048882 139708415854336 logging_writer.py:48] [89400] global_step=89400, grad_norm=1.4614089727401733, loss=1.9834272861480713
I0308 16:58:31.308188 139708407461632 logging_writer.py:48] [89500] global_step=89500, grad_norm=1.1708370447158813, loss=2.916672945022583
I0308 16:58:55.437146 139902746892096 spec.py:321] Evaluating on the training split.
I0308 16:59:06.792231 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 16:59:27.848586 139902746892096 spec.py:349] Evaluating on the test split.
I0308 16:59:29.521810 139902746892096 submission_runner.py:411] Time since start: 43092.55s, 	Step: 89555, 	{'train/accuracy': 0.7291601300239563, 'train/loss': 1.0930023193359375, 'validation/accuracy': 0.675000011920929, 'validation/loss': 1.3470532894134521, 'validation/num_examples': 50000, 'test/accuracy': 0.5479000210762024, 'test/loss': 2.000091075897217, 'test/num_examples': 10000, 'score': 39954.712794303894, 'total_duration': 43092.5505900383, 'accumulated_submission_time': 39954.712794303894, 'accumulated_eval_time': 3128.6608567237854, 'accumulated_logging_time': 4.420300722122192}
I0308 16:59:29.554888 139708415854336 logging_writer.py:48] [89555] accumulated_eval_time=3128.660857, accumulated_logging_time=4.420301, accumulated_submission_time=39954.712794, global_step=89555, preemption_count=0, score=39954.712794, test/accuracy=0.547900, test/loss=2.000091, test/num_examples=10000, total_duration=43092.550590, train/accuracy=0.729160, train/loss=1.093002, validation/accuracy=0.675000, validation/loss=1.347053, validation/num_examples=50000
I0308 16:59:47.685998 139708407461632 logging_writer.py:48] [89600] global_step=89600, grad_norm=1.4409910440444946, loss=2.039386749267578
I0308 17:00:29.923948 139708415854336 logging_writer.py:48] [89700] global_step=89700, grad_norm=1.1721066236495972, loss=3.3417305946350098
I0308 17:01:14.959353 139708407461632 logging_writer.py:48] [89800] global_step=89800, grad_norm=1.3739501237869263, loss=4.602762222290039
I0308 17:02:00.165596 139708415854336 logging_writer.py:48] [89900] global_step=89900, grad_norm=1.255645990371704, loss=4.214583396911621
I0308 17:02:45.762451 139708407461632 logging_writer.py:48] [90000] global_step=90000, grad_norm=1.6052420139312744, loss=2.0802674293518066
I0308 17:03:31.139620 139708415854336 logging_writer.py:48] [90100] global_step=90100, grad_norm=1.2245025634765625, loss=3.1688311100006104
I0308 17:04:16.563980 139708407461632 logging_writer.py:48] [90200] global_step=90200, grad_norm=1.4630398750305176, loss=2.0042505264282227
I0308 17:05:01.775670 139708415854336 logging_writer.py:48] [90300] global_step=90300, grad_norm=1.3546128273010254, loss=4.515081405639648
I0308 17:05:47.187353 139708407461632 logging_writer.py:48] [90400] global_step=90400, grad_norm=1.2808507680892944, loss=4.395262241363525
I0308 17:06:29.848903 139902746892096 spec.py:321] Evaluating on the training split.
I0308 17:06:41.201739 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 17:07:01.177460 139902746892096 spec.py:349] Evaluating on the test split.
I0308 17:07:02.856779 139902746892096 submission_runner.py:411] Time since start: 43545.89s, 	Step: 90496, 	{'train/accuracy': 0.7349218726158142, 'train/loss': 1.0604491233825684, 'validation/accuracy': 0.6757799983024597, 'validation/loss': 1.330373764038086, 'validation/num_examples': 50000, 'test/accuracy': 0.5523000359535217, 'test/loss': 1.9852027893066406, 'test/num_examples': 10000, 'score': 40374.948270082474, 'total_duration': 43545.885566711426, 'accumulated_submission_time': 40374.948270082474, 'accumulated_eval_time': 3161.6687231063843, 'accumulated_logging_time': 4.463085412979126}
I0308 17:07:02.890787 139708415854336 logging_writer.py:48] [90496] accumulated_eval_time=3161.668723, accumulated_logging_time=4.463085, accumulated_submission_time=40374.948270, global_step=90496, preemption_count=0, score=40374.948270, test/accuracy=0.552300, test/loss=1.985203, test/num_examples=10000, total_duration=43545.885567, train/accuracy=0.734922, train/loss=1.060449, validation/accuracy=0.675780, validation/loss=1.330374, validation/num_examples=50000
I0308 17:07:04.884263 139708407461632 logging_writer.py:48] [90500] global_step=90500, grad_norm=1.341810941696167, loss=2.749502182006836
I0308 17:07:45.387686 139708415854336 logging_writer.py:48] [90600] global_step=90600, grad_norm=1.6186033487319946, loss=2.2569947242736816
I0308 17:08:30.719525 139708407461632 logging_writer.py:48] [90700] global_step=90700, grad_norm=1.4481956958770752, loss=2.0643701553344727
I0308 17:09:16.131163 139708415854336 logging_writer.py:48] [90800] global_step=90800, grad_norm=1.502249836921692, loss=4.224786758422852
I0308 17:10:01.424785 139708407461632 logging_writer.py:48] [90900] global_step=90900, grad_norm=1.449042797088623, loss=2.0560030937194824
I0308 17:10:46.704381 139708415854336 logging_writer.py:48] [91000] global_step=91000, grad_norm=1.322124719619751, loss=3.4442954063415527
I0308 17:11:32.372339 139708407461632 logging_writer.py:48] [91100] global_step=91100, grad_norm=1.3563144207000732, loss=1.9664161205291748
I0308 17:12:17.783449 139708415854336 logging_writer.py:48] [91200] global_step=91200, grad_norm=1.536030888557434, loss=2.0506341457366943
I0308 17:13:03.075219 139708407461632 logging_writer.py:48] [91300] global_step=91300, grad_norm=1.3447810411453247, loss=2.0469956398010254
I0308 17:13:48.889525 139708415854336 logging_writer.py:48] [91400] global_step=91400, grad_norm=1.2541145086288452, loss=3.928353786468506
I0308 17:14:03.183255 139902746892096 spec.py:321] Evaluating on the training split.
I0308 17:14:14.712835 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 17:14:35.716020 139902746892096 spec.py:349] Evaluating on the test split.
I0308 17:14:37.394662 139902746892096 submission_runner.py:411] Time since start: 44000.42s, 	Step: 91433, 	{'train/accuracy': 0.7516992092132568, 'train/loss': 0.9681676030158997, 'validation/accuracy': 0.6752200126647949, 'validation/loss': 1.3013813495635986, 'validation/num_examples': 50000, 'test/accuracy': 0.5514000058174133, 'test/loss': 1.9640438556671143, 'test/num_examples': 10000, 'score': 40795.17732858658, 'total_duration': 44000.42345237732, 'accumulated_submission_time': 40795.17732858658, 'accumulated_eval_time': 3195.8801221847534, 'accumulated_logging_time': 4.511308193206787}
I0308 17:14:37.425447 139708407461632 logging_writer.py:48] [91433] accumulated_eval_time=3195.880122, accumulated_logging_time=4.511308, accumulated_submission_time=40795.177329, global_step=91433, preemption_count=0, score=40795.177329, test/accuracy=0.551400, test/loss=1.964044, test/num_examples=10000, total_duration=44000.423452, train/accuracy=0.751699, train/loss=0.968168, validation/accuracy=0.675220, validation/loss=1.301381, validation/num_examples=50000
I0308 17:15:04.212096 139708415854336 logging_writer.py:48] [91500] global_step=91500, grad_norm=1.3857882022857666, loss=2.2549855709075928
I0308 17:15:48.194707 139708407461632 logging_writer.py:48] [91600] global_step=91600, grad_norm=1.2800034284591675, loss=3.802818536758423
I0308 17:16:33.540000 139708415854336 logging_writer.py:48] [91700] global_step=91700, grad_norm=1.3995082378387451, loss=2.368697166442871
I0308 17:17:19.112011 139708407461632 logging_writer.py:48] [91800] global_step=91800, grad_norm=1.5341534614562988, loss=2.1047258377075195
I0308 17:18:04.052196 139708415854336 logging_writer.py:48] [91900] global_step=91900, grad_norm=1.3871641159057617, loss=1.979310393333435
I0308 17:18:49.358483 139708407461632 logging_writer.py:48] [92000] global_step=92000, grad_norm=1.4937398433685303, loss=2.031360149383545
I0308 17:19:34.355057 139708415854336 logging_writer.py:48] [92100] global_step=92100, grad_norm=1.1902883052825928, loss=3.4207544326782227
I0308 17:20:19.343788 139708407461632 logging_writer.py:48] [92200] global_step=92200, grad_norm=1.6658680438995361, loss=1.927534818649292
I0308 17:21:04.407883 139708415854336 logging_writer.py:48] [92300] global_step=92300, grad_norm=1.5980619192123413, loss=1.9672597646713257
I0308 17:21:37.452037 139902746892096 spec.py:321] Evaluating on the training split.
I0308 17:21:48.729950 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 17:22:07.552136 139902746892096 spec.py:349] Evaluating on the test split.
I0308 17:22:09.238659 139902746892096 submission_runner.py:411] Time since start: 44452.27s, 	Step: 92375, 	{'train/accuracy': 0.7336523532867432, 'train/loss': 1.0666475296020508, 'validation/accuracy': 0.6757599711418152, 'validation/loss': 1.3290544748306274, 'validation/num_examples': 50000, 'test/accuracy': 0.5496000051498413, 'test/loss': 1.9641692638397217, 'test/num_examples': 10000, 'score': 41215.14496469498, 'total_duration': 44452.26744532585, 'accumulated_submission_time': 41215.14496469498, 'accumulated_eval_time': 3227.6667318344116, 'accumulated_logging_time': 4.5512306690216064}
I0308 17:22:09.283040 139708407461632 logging_writer.py:48] [92375] accumulated_eval_time=3227.666732, accumulated_logging_time=4.551231, accumulated_submission_time=41215.144965, global_step=92375, preemption_count=0, score=41215.144965, test/accuracy=0.549600, test/loss=1.964169, test/num_examples=10000, total_duration=44452.267445, train/accuracy=0.733652, train/loss=1.066648, validation/accuracy=0.675760, validation/loss=1.329054, validation/num_examples=50000
I0308 17:22:19.550253 139708415854336 logging_writer.py:48] [92400] global_step=92400, grad_norm=1.2944636344909668, loss=2.640944242477417
I0308 17:23:01.578541 139708407461632 logging_writer.py:48] [92500] global_step=92500, grad_norm=1.5682545900344849, loss=2.1239497661590576
I0308 17:23:46.983073 139708415854336 logging_writer.py:48] [92600] global_step=92600, grad_norm=1.4154576063156128, loss=2.281773805618286
I0308 17:24:32.291071 139708407461632 logging_writer.py:48] [92700] global_step=92700, grad_norm=1.3580468893051147, loss=3.647261381149292
I0308 17:25:17.310192 139708415854336 logging_writer.py:48] [92800] global_step=92800, grad_norm=1.2677584886550903, loss=1.9317481517791748
I0308 17:26:02.578237 139708407461632 logging_writer.py:48] [92900] global_step=92900, grad_norm=1.5547475814819336, loss=2.0948843955993652
I0308 17:26:48.127377 139708415854336 logging_writer.py:48] [93000] global_step=93000, grad_norm=1.365626335144043, loss=3.3699326515197754
I0308 17:27:33.192715 139708407461632 logging_writer.py:48] [93100] global_step=93100, grad_norm=1.3946040868759155, loss=2.018296480178833
I0308 17:28:18.682131 139708415854336 logging_writer.py:48] [93200] global_step=93200, grad_norm=1.4003870487213135, loss=3.106844186782837
I0308 17:29:03.785120 139708407461632 logging_writer.py:48] [93300] global_step=93300, grad_norm=1.3596265316009521, loss=2.5847747325897217
I0308 17:29:09.409482 139902746892096 spec.py:321] Evaluating on the training split.
I0308 17:29:20.939776 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 17:29:42.356743 139902746892096 spec.py:349] Evaluating on the test split.
I0308 17:29:44.034693 139902746892096 submission_runner.py:411] Time since start: 44907.06s, 	Step: 93314, 	{'train/accuracy': 0.7378124594688416, 'train/loss': 1.0565478801727295, 'validation/accuracy': 0.677839994430542, 'validation/loss': 1.3238400220870972, 'validation/num_examples': 50000, 'test/accuracy': 0.5519000291824341, 'test/loss': 1.9784936904907227, 'test/num_examples': 10000, 'score': 41635.211717128754, 'total_duration': 44907.063487529755, 'accumulated_submission_time': 41635.211717128754, 'accumulated_eval_time': 3262.29194355011, 'accumulated_logging_time': 4.605958700180054}
I0308 17:29:44.066090 139708415854336 logging_writer.py:48] [93314] accumulated_eval_time=3262.291944, accumulated_logging_time=4.605959, accumulated_submission_time=41635.211717, global_step=93314, preemption_count=0, score=41635.211717, test/accuracy=0.551900, test/loss=1.978494, test/num_examples=10000, total_duration=44907.063488, train/accuracy=0.737812, train/loss=1.056548, validation/accuracy=0.677840, validation/loss=1.323840, validation/num_examples=50000
I0308 17:30:18.323819 139708407461632 logging_writer.py:48] [93400] global_step=93400, grad_norm=1.4367682933807373, loss=4.039377212524414
I0308 17:31:03.013666 139708415854336 logging_writer.py:48] [93500] global_step=93500, grad_norm=1.453774333000183, loss=2.010331869125366
I0308 17:31:48.435678 139708407461632 logging_writer.py:48] [93600] global_step=93600, grad_norm=1.3230699300765991, loss=3.0573110580444336
I0308 17:32:33.949571 139708415854336 logging_writer.py:48] [93700] global_step=93700, grad_norm=1.5914087295532227, loss=2.0629467964172363
I0308 17:33:18.692587 139708407461632 logging_writer.py:48] [93800] global_step=93800, grad_norm=1.5072181224822998, loss=1.9931354522705078
I0308 17:34:04.012785 139708415854336 logging_writer.py:48] [93900] global_step=93900, grad_norm=1.5054287910461426, loss=1.9646873474121094
I0308 17:34:49.192845 139708407461632 logging_writer.py:48] [94000] global_step=94000, grad_norm=1.584572196006775, loss=3.3690004348754883
I0308 17:35:34.407321 139708415854336 logging_writer.py:48] [94100] global_step=94100, grad_norm=1.4754599332809448, loss=2.148181676864624
I0308 17:36:19.470230 139708407461632 logging_writer.py:48] [94200] global_step=94200, grad_norm=1.3564590215682983, loss=1.9105279445648193
I0308 17:36:44.371448 139902746892096 spec.py:321] Evaluating on the training split.
I0308 17:36:55.492909 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 17:37:16.817154 139902746892096 spec.py:349] Evaluating on the test split.
I0308 17:37:18.489105 139902746892096 submission_runner.py:411] Time since start: 45361.52s, 	Step: 94257, 	{'train/accuracy': 0.7453515529632568, 'train/loss': 1.0164759159088135, 'validation/accuracy': 0.6785799860954285, 'validation/loss': 1.3150279521942139, 'validation/num_examples': 50000, 'test/accuracy': 0.5519000291824341, 'test/loss': 1.9706907272338867, 'test/num_examples': 10000, 'score': 42055.45607948303, 'total_duration': 45361.51790237427, 'accumulated_submission_time': 42055.45607948303, 'accumulated_eval_time': 3296.409605741501, 'accumulated_logging_time': 4.649278879165649}
I0308 17:37:18.521805 139708415854336 logging_writer.py:48] [94257] accumulated_eval_time=3296.409606, accumulated_logging_time=4.649279, accumulated_submission_time=42055.456079, global_step=94257, preemption_count=0, score=42055.456079, test/accuracy=0.551900, test/loss=1.970691, test/num_examples=10000, total_duration=45361.517902, train/accuracy=0.745352, train/loss=1.016476, validation/accuracy=0.678580, validation/loss=1.315028, validation/num_examples=50000
I0308 17:37:35.860696 139708407461632 logging_writer.py:48] [94300] global_step=94300, grad_norm=1.3037211894989014, loss=3.0898189544677734
I0308 17:38:18.051635 139708415854336 logging_writer.py:48] [94400] global_step=94400, grad_norm=1.4351857900619507, loss=3.9256205558776855
I0308 17:39:03.244473 139708407461632 logging_writer.py:48] [94500] global_step=94500, grad_norm=1.373837947845459, loss=2.32913875579834
I0308 17:39:48.359010 139708415854336 logging_writer.py:48] [94600] global_step=94600, grad_norm=1.3087177276611328, loss=3.099189281463623
I0308 17:40:33.339294 139708407461632 logging_writer.py:48] [94700] global_step=94700, grad_norm=1.55979323387146, loss=2.0248210430145264
I0308 17:41:18.559422 139708415854336 logging_writer.py:48] [94800] global_step=94800, grad_norm=1.4907459020614624, loss=1.9899793863296509
I0308 17:42:03.930711 139708407461632 logging_writer.py:48] [94900] global_step=94900, grad_norm=1.4400618076324463, loss=2.34954833984375
I0308 17:42:49.056981 139708415854336 logging_writer.py:48] [95000] global_step=95000, grad_norm=1.4313889741897583, loss=2.039985179901123
I0308 17:43:34.574041 139708407461632 logging_writer.py:48] [95100] global_step=95100, grad_norm=1.2410414218902588, loss=3.4588210582733154
I0308 17:44:18.694404 139902746892096 spec.py:321] Evaluating on the training split.
I0308 17:44:29.974474 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 17:44:52.089671 139902746892096 spec.py:349] Evaluating on the test split.
I0308 17:44:53.766153 139902746892096 submission_runner.py:411] Time since start: 45816.79s, 	Step: 95199, 	{'train/accuracy': 0.7621288895606995, 'train/loss': 0.9430046677589417, 'validation/accuracy': 0.676539957523346, 'validation/loss': 1.3137037754058838, 'validation/num_examples': 50000, 'test/accuracy': 0.5520000457763672, 'test/loss': 1.978226661682129, 'test/num_examples': 10000, 'score': 42475.57057905197, 'total_duration': 45816.79495024681, 'accumulated_submission_time': 42475.57057905197, 'accumulated_eval_time': 3331.4813554286957, 'accumulated_logging_time': 4.691254615783691}
I0308 17:44:53.795518 139708415854336 logging_writer.py:48] [95199] accumulated_eval_time=3331.481355, accumulated_logging_time=4.691255, accumulated_submission_time=42475.570579, global_step=95199, preemption_count=0, score=42475.570579, test/accuracy=0.552000, test/loss=1.978227, test/num_examples=10000, total_duration=45816.794950, train/accuracy=0.762129, train/loss=0.943005, validation/accuracy=0.676540, validation/loss=1.313704, validation/num_examples=50000
I0308 17:44:54.586681 139708407461632 logging_writer.py:48] [95200] global_step=95200, grad_norm=1.5341105461120605, loss=1.9849693775177002
I0308 17:45:34.351641 139708415854336 logging_writer.py:48] [95300] global_step=95300, grad_norm=1.4263615608215332, loss=2.079279899597168
I0308 17:46:19.491744 139708407461632 logging_writer.py:48] [95400] global_step=95400, grad_norm=1.4125292301177979, loss=2.319199562072754
I0308 17:47:05.343791 139708415854336 logging_writer.py:48] [95500] global_step=95500, grad_norm=1.2955162525177002, loss=2.5984530448913574
I0308 17:47:50.626480 139708407461632 logging_writer.py:48] [95600] global_step=95600, grad_norm=1.3295334577560425, loss=2.9849510192871094
I0308 17:48:35.873659 139708415854336 logging_writer.py:48] [95700] global_step=95700, grad_norm=1.3821638822555542, loss=2.0798163414001465
I0308 17:49:21.216904 139708407461632 logging_writer.py:48] [95800] global_step=95800, grad_norm=1.3020228147506714, loss=4.461961269378662
I0308 17:50:06.426068 139708415854336 logging_writer.py:48] [95900] global_step=95900, grad_norm=1.5018962621688843, loss=1.8291423320770264
I0308 17:50:51.314427 139708407461632 logging_writer.py:48] [96000] global_step=96000, grad_norm=1.3927240371704102, loss=4.008817672729492
I0308 17:51:36.664396 139708415854336 logging_writer.py:48] [96100] global_step=96100, grad_norm=1.4227795600891113, loss=1.937540054321289
I0308 17:51:53.975084 139902746892096 spec.py:321] Evaluating on the training split.
I0308 17:52:05.325740 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 17:52:24.274171 139902746892096 spec.py:349] Evaluating on the test split.
I0308 17:52:25.969717 139902746892096 submission_runner.py:411] Time since start: 46269.00s, 	Step: 96140, 	{'train/accuracy': 0.7389257550239563, 'train/loss': 1.0446540117263794, 'validation/accuracy': 0.680679976940155, 'validation/loss': 1.3052889108657837, 'validation/num_examples': 50000, 'test/accuracy': 0.556600034236908, 'test/loss': 1.9483904838562012, 'test/num_examples': 10000, 'score': 42895.6915576458, 'total_duration': 46268.998499155045, 'accumulated_submission_time': 42895.6915576458, 'accumulated_eval_time': 3363.4759736061096, 'accumulated_logging_time': 4.730247259140015}
I0308 17:52:26.004816 139708407461632 logging_writer.py:48] [96140] accumulated_eval_time=3363.475974, accumulated_logging_time=4.730247, accumulated_submission_time=42895.691558, global_step=96140, preemption_count=0, score=42895.691558, test/accuracy=0.556600, test/loss=1.948390, test/num_examples=10000, total_duration=46268.998499, train/accuracy=0.738926, train/loss=1.044654, validation/accuracy=0.680680, validation/loss=1.305289, validation/num_examples=50000
I0308 17:52:50.037079 139708415854336 logging_writer.py:48] [96200] global_step=96200, grad_norm=1.7294431924819946, loss=4.49875545501709
I0308 17:53:33.968687 139708407461632 logging_writer.py:48] [96300] global_step=96300, grad_norm=1.4831565618515015, loss=1.9649572372436523
I0308 17:54:19.701608 139708415854336 logging_writer.py:48] [96400] global_step=96400, grad_norm=1.3019983768463135, loss=3.7289512157440186
I0308 17:55:05.529160 139708407461632 logging_writer.py:48] [96500] global_step=96500, grad_norm=1.3277095556259155, loss=3.9799230098724365
I0308 17:55:50.616973 139708415854336 logging_writer.py:48] [96600] global_step=96600, grad_norm=1.2928719520568848, loss=2.850724220275879
I0308 17:56:35.846394 139708407461632 logging_writer.py:48] [96700] global_step=96700, grad_norm=1.3216415643692017, loss=3.737459659576416
I0308 17:57:21.162407 139708415854336 logging_writer.py:48] [96800] global_step=96800, grad_norm=1.5495145320892334, loss=2.0491690635681152
I0308 17:58:06.626616 139708407461632 logging_writer.py:48] [96900] global_step=96900, grad_norm=1.5838485956192017, loss=2.1073341369628906
I0308 17:58:51.934071 139708415854336 logging_writer.py:48] [97000] global_step=97000, grad_norm=1.173845648765564, loss=3.0574185848236084
I0308 17:59:26.213948 139902746892096 spec.py:321] Evaluating on the training split.
I0308 17:59:37.586892 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 17:59:58.726514 139902746892096 spec.py:349] Evaluating on the test split.
I0308 18:00:00.405701 139902746892096 submission_runner.py:411] Time since start: 46723.43s, 	Step: 97078, 	{'train/accuracy': 0.7458202838897705, 'train/loss': 0.9984654784202576, 'validation/accuracy': 0.6850000023841858, 'validation/loss': 1.273817777633667, 'validation/num_examples': 50000, 'test/accuracy': 0.5636000037193298, 'test/loss': 1.91632878780365, 'test/num_examples': 10000, 'score': 43315.840493917465, 'total_duration': 46723.43449640274, 'accumulated_submission_time': 43315.840493917465, 'accumulated_eval_time': 3397.667732000351, 'accumulated_logging_time': 4.7759013175964355}
I0308 18:00:00.437035 139708407461632 logging_writer.py:48] [97078] accumulated_eval_time=3397.667732, accumulated_logging_time=4.775901, accumulated_submission_time=43315.840494, global_step=97078, preemption_count=0, score=43315.840494, test/accuracy=0.563600, test/loss=1.916329, test/num_examples=10000, total_duration=46723.434496, train/accuracy=0.745820, train/loss=0.998465, validation/accuracy=0.685000, validation/loss=1.273818, validation/num_examples=50000
I0308 18:00:09.494372 139708415854336 logging_writer.py:48] [97100] global_step=97100, grad_norm=1.6543117761611938, loss=1.9690289497375488
I0308 18:00:50.860916 139708407461632 logging_writer.py:48] [97200] global_step=97200, grad_norm=1.2771022319793701, loss=2.55788516998291
I0308 18:01:35.894083 139708415854336 logging_writer.py:48] [97300] global_step=97300, grad_norm=1.570198893547058, loss=2.0094687938690186
I0308 18:02:21.418682 139708407461632 logging_writer.py:48] [97400] global_step=97400, grad_norm=1.3291044235229492, loss=3.3618383407592773
I0308 18:03:06.631420 139708415854336 logging_writer.py:48] [97500] global_step=97500, grad_norm=1.5016473531723022, loss=1.9938101768493652
I0308 18:03:51.875300 139708407461632 logging_writer.py:48] [97600] global_step=97600, grad_norm=1.423240065574646, loss=1.9582877159118652
I0308 18:04:37.338131 139708415854336 logging_writer.py:48] [97700] global_step=97700, grad_norm=1.3124897480010986, loss=4.479344844818115
I0308 18:05:22.765366 139708407461632 logging_writer.py:48] [97800] global_step=97800, grad_norm=1.5174524784088135, loss=4.578668594360352
I0308 18:06:08.037621 139708415854336 logging_writer.py:48] [97900] global_step=97900, grad_norm=1.6308083534240723, loss=1.8895883560180664
I0308 18:06:53.435959 139708407461632 logging_writer.py:48] [98000] global_step=98000, grad_norm=1.6438002586364746, loss=1.952714443206787
I0308 18:07:00.844599 139902746892096 spec.py:321] Evaluating on the training split.
I0308 18:07:12.146346 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 18:07:35.073045 139902746892096 spec.py:349] Evaluating on the test split.
I0308 18:07:36.755770 139902746892096 submission_runner.py:411] Time since start: 47179.78s, 	Step: 98018, 	{'train/accuracy': 0.7523632645606995, 'train/loss': 0.9861083626747131, 'validation/accuracy': 0.6795200109481812, 'validation/loss': 1.307973861694336, 'validation/num_examples': 50000, 'test/accuracy': 0.557200014591217, 'test/loss': 1.9540166854858398, 'test/num_examples': 10000, 'score': 43736.18922662735, 'total_duration': 47179.78457069397, 'accumulated_submission_time': 43736.18922662735, 'accumulated_eval_time': 3433.5789165496826, 'accumulated_logging_time': 4.816876649856567}
I0308 18:07:36.785243 139708415854336 logging_writer.py:48] [98018] accumulated_eval_time=3433.578917, accumulated_logging_time=4.816877, accumulated_submission_time=43736.189227, global_step=98018, preemption_count=0, score=43736.189227, test/accuracy=0.557200, test/loss=1.954017, test/num_examples=10000, total_duration=47179.784571, train/accuracy=0.752363, train/loss=0.986108, validation/accuracy=0.679520, validation/loss=1.307974, validation/num_examples=50000
I0308 18:08:09.472492 139708407461632 logging_writer.py:48] [98100] global_step=98100, grad_norm=1.4942656755447388, loss=1.994089961051941
I0308 18:08:54.028533 139708415854336 logging_writer.py:48] [98200] global_step=98200, grad_norm=1.5117888450622559, loss=1.9564510583877563
I0308 18:09:39.528335 139708407461632 logging_writer.py:48] [98300] global_step=98300, grad_norm=1.7027177810668945, loss=1.9217957258224487
I0308 18:10:24.890295 139708415854336 logging_writer.py:48] [98400] global_step=98400, grad_norm=1.5109800100326538, loss=2.0610084533691406
I0308 18:11:10.115588 139708407461632 logging_writer.py:48] [98500] global_step=98500, grad_norm=1.29300057888031, loss=2.6409144401550293
I0308 18:11:55.171419 139708415854336 logging_writer.py:48] [98600] global_step=98600, grad_norm=1.370353102684021, loss=3.6722805500030518
I0308 18:12:40.535197 139708407461632 logging_writer.py:48] [98700] global_step=98700, grad_norm=1.4084038734436035, loss=2.555302381515503
I0308 18:13:25.577885 139708415854336 logging_writer.py:48] [98800] global_step=98800, grad_norm=1.4142969846725464, loss=3.1437809467315674
I0308 18:14:11.027289 139708407461632 logging_writer.py:48] [98900] global_step=98900, grad_norm=1.3131904602050781, loss=3.5915451049804688
I0308 18:14:36.948927 139902746892096 spec.py:321] Evaluating on the training split.
I0308 18:14:48.328017 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 18:15:08.812061 139902746892096 spec.py:349] Evaluating on the test split.
I0308 18:15:10.505871 139902746892096 submission_runner.py:411] Time since start: 47633.53s, 	Step: 98959, 	{'train/accuracy': 0.7362499833106995, 'train/loss': 1.0587328672409058, 'validation/accuracy': 0.683899998664856, 'validation/loss': 1.307403326034546, 'validation/num_examples': 50000, 'test/accuracy': 0.5526000261306763, 'test/loss': 1.950338363647461, 'test/num_examples': 10000, 'score': 44156.29066824913, 'total_duration': 47633.534651994705, 'accumulated_submission_time': 44156.29066824913, 'accumulated_eval_time': 3467.1358416080475, 'accumulated_logging_time': 4.856912851333618}
I0308 18:15:10.546359 139708415854336 logging_writer.py:48] [98959] accumulated_eval_time=3467.135842, accumulated_logging_time=4.856913, accumulated_submission_time=44156.290668, global_step=98959, preemption_count=0, score=44156.290668, test/accuracy=0.552600, test/loss=1.950338, test/num_examples=10000, total_duration=47633.534652, train/accuracy=0.736250, train/loss=1.058733, validation/accuracy=0.683900, validation/loss=1.307403, validation/num_examples=50000
I0308 18:15:27.095991 139708407461632 logging_writer.py:48] [99000] global_step=99000, grad_norm=1.5394845008850098, loss=1.9633572101593018
I0308 18:16:09.940339 139708415854336 logging_writer.py:48] [99100] global_step=99100, grad_norm=1.356044888496399, loss=4.294357776641846
I0308 18:16:55.273581 139708407461632 logging_writer.py:48] [99200] global_step=99200, grad_norm=1.3507757186889648, loss=2.935356616973877
I0308 18:17:40.739937 139708415854336 logging_writer.py:48] [99300] global_step=99300, grad_norm=1.7110748291015625, loss=1.9280139207839966
I0308 18:18:25.918398 139708407461632 logging_writer.py:48] [99400] global_step=99400, grad_norm=1.4800667762756348, loss=1.8574227094650269
I0308 18:19:11.191316 139708415854336 logging_writer.py:48] [99500] global_step=99500, grad_norm=1.481798529624939, loss=3.354962110519409
I0308 18:19:56.366394 139708407461632 logging_writer.py:48] [99600] global_step=99600, grad_norm=1.4608941078186035, loss=1.903552532196045
I0308 18:20:41.500482 139708415854336 logging_writer.py:48] [99700] global_step=99700, grad_norm=1.3524037599563599, loss=2.3032164573669434
I0308 18:21:26.809380 139708407461632 logging_writer.py:48] [99800] global_step=99800, grad_norm=1.4149975776672363, loss=1.9822865724563599
I0308 18:22:10.546149 139902746892096 spec.py:321] Evaluating on the training split.
I0308 18:22:22.012807 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 18:22:43.922361 139902746892096 spec.py:349] Evaluating on the test split.
I0308 18:22:45.592934 139902746892096 submission_runner.py:411] Time since start: 48088.62s, 	Step: 99898, 	{'train/accuracy': 0.7546093463897705, 'train/loss': 0.9772475361824036, 'validation/accuracy': 0.6888599991798401, 'validation/loss': 1.2703135013580322, 'validation/num_examples': 50000, 'test/accuracy': 0.5678000450134277, 'test/loss': 1.9063079357147217, 'test/num_examples': 10000, 'score': 44576.2303814888, 'total_duration': 48088.621727228165, 'accumulated_submission_time': 44576.2303814888, 'accumulated_eval_time': 3502.182624578476, 'accumulated_logging_time': 4.907997369766235}
I0308 18:22:45.622704 139708415854336 logging_writer.py:48] [99898] accumulated_eval_time=3502.182625, accumulated_logging_time=4.907997, accumulated_submission_time=44576.230381, global_step=99898, preemption_count=0, score=44576.230381, test/accuracy=0.567800, test/loss=1.906308, test/num_examples=10000, total_duration=48088.621727, train/accuracy=0.754609, train/loss=0.977248, validation/accuracy=0.688860, validation/loss=1.270314, validation/num_examples=50000
I0308 18:22:46.811519 139708407461632 logging_writer.py:48] [99900] global_step=99900, grad_norm=1.392282247543335, loss=2.7266290187835693
I0308 18:23:26.685851 139708415854336 logging_writer.py:48] [100000] global_step=100000, grad_norm=1.5592881441116333, loss=1.9009921550750732
I0308 18:24:11.976340 139708407461632 logging_writer.py:48] [100100] global_step=100100, grad_norm=1.642370343208313, loss=2.0712993144989014
I0308 18:24:57.544387 139708415854336 logging_writer.py:48] [100200] global_step=100200, grad_norm=1.4896869659423828, loss=3.2491352558135986
I0308 18:25:42.753892 139708407461632 logging_writer.py:48] [100300] global_step=100300, grad_norm=1.4671541452407837, loss=2.104045867919922
I0308 18:26:27.856489 139708415854336 logging_writer.py:48] [100400] global_step=100400, grad_norm=1.5805740356445312, loss=1.8453640937805176
I0308 18:27:13.634204 139708407461632 logging_writer.py:48] [100500] global_step=100500, grad_norm=1.494659423828125, loss=1.9594910144805908
I0308 18:27:58.847025 139708415854336 logging_writer.py:48] [100600] global_step=100600, grad_norm=1.3885481357574463, loss=3.788006544113159
I0308 18:28:44.019613 139708407461632 logging_writer.py:48] [100700] global_step=100700, grad_norm=1.5955219268798828, loss=1.863620400428772
I0308 18:29:29.095858 139708415854336 logging_writer.py:48] [100800] global_step=100800, grad_norm=1.494506597518921, loss=1.9373512268066406
I0308 18:29:45.997278 139902746892096 spec.py:321] Evaluating on the training split.
I0308 18:29:57.661057 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 18:30:17.697233 139902746892096 spec.py:349] Evaluating on the test split.
I0308 18:30:19.387723 139902746892096 submission_runner.py:411] Time since start: 48542.42s, 	Step: 100839, 	{'train/accuracy': 0.7542577981948853, 'train/loss': 0.9589452147483826, 'validation/accuracy': 0.6887199878692627, 'validation/loss': 1.2671600580215454, 'validation/num_examples': 50000, 'test/accuracy': 0.5581000447273254, 'test/loss': 1.9453516006469727, 'test/num_examples': 10000, 'score': 44996.54544496536, 'total_duration': 48542.416513204575, 'accumulated_submission_time': 44996.54544496536, 'accumulated_eval_time': 3535.5730526447296, 'accumulated_logging_time': 4.947621583938599}
I0308 18:30:19.426634 139708407461632 logging_writer.py:48] [100839] accumulated_eval_time=3535.573053, accumulated_logging_time=4.947622, accumulated_submission_time=44996.545445, global_step=100839, preemption_count=0, score=44996.545445, test/accuracy=0.558100, test/loss=1.945352, test/num_examples=10000, total_duration=48542.416513, train/accuracy=0.754258, train/loss=0.958945, validation/accuracy=0.688720, validation/loss=1.267160, validation/num_examples=50000
I0308 18:30:43.873372 139708415854336 logging_writer.py:48] [100900] global_step=100900, grad_norm=1.6175751686096191, loss=1.71660578250885
I0308 18:31:27.404648 139708407461632 logging_writer.py:48] [101000] global_step=101000, grad_norm=1.462693214416504, loss=2.2358498573303223
I0308 18:32:12.409876 139708415854336 logging_writer.py:48] [101100] global_step=101100, grad_norm=1.517343521118164, loss=3.4096360206604004
I0308 18:32:57.810859 139708407461632 logging_writer.py:48] [101200] global_step=101200, grad_norm=1.5492031574249268, loss=1.875861644744873
I0308 18:33:42.890517 139708415854336 logging_writer.py:48] [101300] global_step=101300, grad_norm=1.4760085344314575, loss=2.1367993354797363
I0308 18:34:28.398633 139708407461632 logging_writer.py:48] [101400] global_step=101400, grad_norm=1.3358653783798218, loss=3.519519090652466
I0308 18:35:13.641785 139708415854336 logging_writer.py:48] [101500] global_step=101500, grad_norm=1.6482535600662231, loss=4.050749778747559
I0308 18:35:58.861915 139708407461632 logging_writer.py:48] [101600] global_step=101600, grad_norm=1.329665184020996, loss=2.6015937328338623
I0308 18:36:44.284116 139708415854336 logging_writer.py:48] [101700] global_step=101700, grad_norm=1.5982038974761963, loss=1.8294272422790527
I0308 18:37:19.711215 139902746892096 spec.py:321] Evaluating on the training split.
I0308 18:37:31.425870 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 18:37:54.405179 139902746892096 spec.py:349] Evaluating on the test split.
I0308 18:37:56.075823 139902746892096 submission_runner.py:411] Time since start: 48999.10s, 	Step: 101780, 	{'train/accuracy': 0.771289050579071, 'train/loss': 0.90166836977005, 'validation/accuracy': 0.6931999921798706, 'validation/loss': 1.242873191833496, 'validation/num_examples': 50000, 'test/accuracy': 0.5766000151634216, 'test/loss': 1.866647720336914, 'test/num_examples': 10000, 'score': 45416.76748251915, 'total_duration': 48999.10462117195, 'accumulated_submission_time': 45416.76748251915, 'accumulated_eval_time': 3571.9376525878906, 'accumulated_logging_time': 4.9995269775390625}
I0308 18:37:56.109639 139708407461632 logging_writer.py:48] [101780] accumulated_eval_time=3571.937653, accumulated_logging_time=4.999527, accumulated_submission_time=45416.767483, global_step=101780, preemption_count=0, score=45416.767483, test/accuracy=0.576600, test/loss=1.866648, test/num_examples=10000, total_duration=48999.104621, train/accuracy=0.771289, train/loss=0.901668, validation/accuracy=0.693200, validation/loss=1.242873, validation/num_examples=50000
I0308 18:38:04.374806 139708415854336 logging_writer.py:48] [101800] global_step=101800, grad_norm=1.2999082803726196, loss=3.4701156616210938
I0308 18:38:45.686586 139708407461632 logging_writer.py:48] [101900] global_step=101900, grad_norm=1.3308217525482178, loss=3.3139305114746094
I0308 18:39:30.966744 139708415854336 logging_writer.py:48] [102000] global_step=102000, grad_norm=1.377553105354309, loss=2.8434934616088867
I0308 18:40:16.588082 139708407461632 logging_writer.py:48] [102100] global_step=102100, grad_norm=1.3865509033203125, loss=3.711071491241455
I0308 18:41:01.723835 139708415854336 logging_writer.py:48] [102200] global_step=102200, grad_norm=1.4969712495803833, loss=2.0956623554229736
I0308 18:41:46.798257 139708407461632 logging_writer.py:48] [102300] global_step=102300, grad_norm=1.5618058443069458, loss=1.873940110206604
I0308 18:42:32.210840 139708415854336 logging_writer.py:48] [102400] global_step=102400, grad_norm=1.5350500345230103, loss=1.8746354579925537
I0308 18:43:17.312730 139708407461632 logging_writer.py:48] [102500] global_step=102500, grad_norm=1.6691257953643799, loss=2.101327896118164
I0308 18:44:02.918327 139708415854336 logging_writer.py:48] [102600] global_step=102600, grad_norm=1.3505889177322388, loss=3.3925955295562744
I0308 18:44:48.538868 139708407461632 logging_writer.py:48] [102700] global_step=102700, grad_norm=1.449042797088623, loss=3.3216283321380615
I0308 18:44:56.382366 139902746892096 spec.py:321] Evaluating on the training split.
I0308 18:45:07.917057 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 18:45:30.186254 139902746892096 spec.py:349] Evaluating on the test split.
I0308 18:45:31.856393 139902746892096 submission_runner.py:411] Time since start: 49454.89s, 	Step: 102719, 	{'train/accuracy': 0.7533984184265137, 'train/loss': 0.9679220914840698, 'validation/accuracy': 0.6896799802780151, 'validation/loss': 1.2444446086883545, 'validation/num_examples': 50000, 'test/accuracy': 0.5665000081062317, 'test/loss': 1.892961859703064, 'test/num_examples': 10000, 'score': 45836.98096561432, 'total_duration': 49454.885192871094, 'accumulated_submission_time': 45836.98096561432, 'accumulated_eval_time': 3607.4116797447205, 'accumulated_logging_time': 5.043881177902222}
I0308 18:45:31.889719 139708415854336 logging_writer.py:48] [102719] accumulated_eval_time=3607.411680, accumulated_logging_time=5.043881, accumulated_submission_time=45836.980966, global_step=102719, preemption_count=0, score=45836.980966, test/accuracy=0.566500, test/loss=1.892962, test/num_examples=10000, total_duration=49454.885193, train/accuracy=0.753398, train/loss=0.967922, validation/accuracy=0.689680, validation/loss=1.244445, validation/num_examples=50000
I0308 18:46:04.159361 139708407461632 logging_writer.py:48] [102800] global_step=102800, grad_norm=1.5639978647232056, loss=4.38518762588501
I0308 18:46:48.916992 139708415854336 logging_writer.py:48] [102900] global_step=102900, grad_norm=1.4860354661941528, loss=2.180431604385376
I0308 18:47:34.605691 139708407461632 logging_writer.py:48] [103000] global_step=103000, grad_norm=1.62364661693573, loss=2.0033817291259766
I0308 18:48:20.320604 139708415854336 logging_writer.py:48] [103100] global_step=103100, grad_norm=1.4584147930145264, loss=1.9427311420440674
I0308 18:49:05.447353 139708407461632 logging_writer.py:48] [103200] global_step=103200, grad_norm=1.5207242965698242, loss=1.842402696609497
I0308 18:49:50.761615 139708415854336 logging_writer.py:48] [103300] global_step=103300, grad_norm=1.5902460813522339, loss=1.7483677864074707
I0308 18:50:36.118738 139708407461632 logging_writer.py:48] [103400] global_step=103400, grad_norm=1.5333129167556763, loss=2.940518617630005
I0308 18:51:21.353103 139708415854336 logging_writer.py:48] [103500] global_step=103500, grad_norm=1.4168592691421509, loss=4.242907524108887
I0308 18:52:06.694646 139708407461632 logging_writer.py:48] [103600] global_step=103600, grad_norm=1.59461510181427, loss=2.0253846645355225
I0308 18:52:31.971992 139902746892096 spec.py:321] Evaluating on the training split.
I0308 18:52:43.592638 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 18:53:03.964694 139902746892096 spec.py:349] Evaluating on the test split.
I0308 18:53:05.653689 139902746892096 submission_runner.py:411] Time since start: 49908.68s, 	Step: 103657, 	{'train/accuracy': 0.7592577934265137, 'train/loss': 0.9390373229980469, 'validation/accuracy': 0.6936799883842468, 'validation/loss': 1.2363741397857666, 'validation/num_examples': 50000, 'test/accuracy': 0.5705000162124634, 'test/loss': 1.8715754747390747, 'test/num_examples': 10000, 'score': 46257.00506234169, 'total_duration': 49908.68248295784, 'accumulated_submission_time': 46257.00506234169, 'accumulated_eval_time': 3641.093369960785, 'accumulated_logging_time': 5.086168050765991}
I0308 18:53:05.684113 139708415854336 logging_writer.py:48] [103657] accumulated_eval_time=3641.093370, accumulated_logging_time=5.086168, accumulated_submission_time=46257.005062, global_step=103657, preemption_count=0, score=46257.005062, test/accuracy=0.570500, test/loss=1.871575, test/num_examples=10000, total_duration=49908.682483, train/accuracy=0.759258, train/loss=0.939037, validation/accuracy=0.693680, validation/loss=1.236374, validation/num_examples=50000
I0308 18:53:23.007869 139708407461632 logging_writer.py:48] [103700] global_step=103700, grad_norm=1.5252962112426758, loss=2.4996862411499023
I0308 18:54:05.901021 139708415854336 logging_writer.py:48] [103800] global_step=103800, grad_norm=1.486621379852295, loss=1.9007660150527954
I0308 18:54:51.174808 139708407461632 logging_writer.py:48] [103900] global_step=103900, grad_norm=1.5926696062088013, loss=1.961413025856018
I0308 18:55:36.427309 139708415854336 logging_writer.py:48] [104000] global_step=104000, grad_norm=1.547141671180725, loss=2.8735995292663574
I0308 18:56:21.590402 139708407461632 logging_writer.py:48] [104100] global_step=104100, grad_norm=1.5314583778381348, loss=4.330990791320801
I0308 18:57:06.824505 139708415854336 logging_writer.py:48] [104200] global_step=104200, grad_norm=1.4754807949066162, loss=2.2348387241363525
I0308 18:57:52.382436 139708407461632 logging_writer.py:48] [104300] global_step=104300, grad_norm=1.5338256359100342, loss=2.063408374786377
I0308 18:58:37.569612 139708415854336 logging_writer.py:48] [104400] global_step=104400, grad_norm=1.3889235258102417, loss=3.107809543609619
I0308 18:59:22.642887 139708407461632 logging_writer.py:48] [104500] global_step=104500, grad_norm=1.5176305770874023, loss=2.4319005012512207
I0308 19:00:05.674517 139902746892096 spec.py:321] Evaluating on the training split.
I0308 19:00:17.343643 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 19:00:38.144618 139902746892096 spec.py:349] Evaluating on the test split.
I0308 19:00:39.822644 139902746892096 submission_runner.py:411] Time since start: 50362.85s, 	Step: 104597, 	{'train/accuracy': 0.77699214220047, 'train/loss': 0.8573799133300781, 'validation/accuracy': 0.6948599815368652, 'validation/loss': 1.222651720046997, 'validation/num_examples': 50000, 'test/accuracy': 0.5694000124931335, 'test/loss': 1.8680320978164673, 'test/num_examples': 10000, 'score': 46676.93702673912, 'total_duration': 50362.851440668106, 'accumulated_submission_time': 46676.93702673912, 'accumulated_eval_time': 3675.2414979934692, 'accumulated_logging_time': 5.125425815582275}
I0308 19:00:39.854609 139708415854336 logging_writer.py:48] [104597] accumulated_eval_time=3675.241498, accumulated_logging_time=5.125426, accumulated_submission_time=46676.937027, global_step=104597, preemption_count=0, score=46676.937027, test/accuracy=0.569400, test/loss=1.868032, test/num_examples=10000, total_duration=50362.851441, train/accuracy=0.776992, train/loss=0.857380, validation/accuracy=0.694860, validation/loss=1.222652, validation/num_examples=50000
I0308 19:00:41.431368 139708407461632 logging_writer.py:48] [104600] global_step=104600, grad_norm=2.0493640899658203, loss=4.499935150146484
I0308 19:01:21.932933 139708415854336 logging_writer.py:48] [104700] global_step=104700, grad_norm=1.3873134851455688, loss=2.8763225078582764
I0308 19:02:06.774833 139708407461632 logging_writer.py:48] [104800] global_step=104800, grad_norm=1.5491149425506592, loss=1.9380156993865967
I0308 19:02:52.042045 139708415854336 logging_writer.py:48] [104900] global_step=104900, grad_norm=1.3958896398544312, loss=3.9589474201202393
I0308 19:03:37.527825 139708407461632 logging_writer.py:48] [105000] global_step=105000, grad_norm=1.411847472190857, loss=2.5588667392730713
I0308 19:04:22.959357 139708415854336 logging_writer.py:48] [105100] global_step=105100, grad_norm=1.5622795820236206, loss=3.7369372844696045
I0308 19:05:08.287117 139708407461632 logging_writer.py:48] [105200] global_step=105200, grad_norm=1.4454134702682495, loss=2.3999199867248535
I0308 19:05:53.657250 139708415854336 logging_writer.py:48] [105300] global_step=105300, grad_norm=1.5506064891815186, loss=1.8898420333862305
I0308 19:06:38.896473 139708407461632 logging_writer.py:48] [105400] global_step=105400, grad_norm=1.6088333129882812, loss=2.2179274559020996
I0308 19:07:24.290789 139708415854336 logging_writer.py:48] [105500] global_step=105500, grad_norm=1.5400410890579224, loss=1.9155741930007935
I0308 19:07:39.997070 139902746892096 spec.py:321] Evaluating on the training split.
I0308 19:07:51.500562 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 19:08:13.941080 139902746892096 spec.py:349] Evaluating on the test split.
I0308 19:08:15.612957 139902746892096 submission_runner.py:411] Time since start: 50818.64s, 	Step: 105536, 	{'train/accuracy': 0.7588671445846558, 'train/loss': 0.9336987137794495, 'validation/accuracy': 0.6984599828720093, 'validation/loss': 1.214532732963562, 'validation/num_examples': 50000, 'test/accuracy': 0.5735000371932983, 'test/loss': 1.8641231060028076, 'test/num_examples': 10000, 'score': 47097.02132034302, 'total_duration': 50818.641756772995, 'accumulated_submission_time': 47097.02132034302, 'accumulated_eval_time': 3710.8573791980743, 'accumulated_logging_time': 5.166118860244751}
I0308 19:08:15.645679 139708407461632 logging_writer.py:48] [105536] accumulated_eval_time=3710.857379, accumulated_logging_time=5.166119, accumulated_submission_time=47097.021320, global_step=105536, preemption_count=0, score=47097.021320, test/accuracy=0.573500, test/loss=1.864123, test/num_examples=10000, total_duration=50818.641757, train/accuracy=0.758867, train/loss=0.933699, validation/accuracy=0.698460, validation/loss=1.214533, validation/num_examples=50000
I0308 19:08:41.364734 139708415854336 logging_writer.py:48] [105600] global_step=105600, grad_norm=1.424922227859497, loss=2.392457962036133
I0308 19:09:24.916242 139708407461632 logging_writer.py:48] [105700] global_step=105700, grad_norm=1.6274348497390747, loss=4.2405242919921875
I0308 19:10:10.081640 139708415854336 logging_writer.py:48] [105800] global_step=105800, grad_norm=1.4629662036895752, loss=1.8809922933578491
I0308 19:10:55.697701 139708407461632 logging_writer.py:48] [105900] global_step=105900, grad_norm=1.547202229499817, loss=1.996920108795166
I0308 19:11:40.810670 139708415854336 logging_writer.py:48] [106000] global_step=106000, grad_norm=1.6522114276885986, loss=1.892337441444397
I0308 19:12:26.006200 139708407461632 logging_writer.py:48] [106100] global_step=106100, grad_norm=1.5668357610702515, loss=4.272282600402832
I0308 19:13:11.079496 139708415854336 logging_writer.py:48] [106200] global_step=106200, grad_norm=1.6346601247787476, loss=1.8556022644042969
I0308 19:13:56.293198 139708407461632 logging_writer.py:48] [106300] global_step=106300, grad_norm=1.8714048862457275, loss=2.028499126434326
I0308 19:14:42.285315 139708415854336 logging_writer.py:48] [106400] global_step=106400, grad_norm=1.589553952217102, loss=1.8767731189727783
I0308 19:15:15.639716 139902746892096 spec.py:321] Evaluating on the training split.
I0308 19:15:27.151349 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 19:15:47.938950 139902746892096 spec.py:349] Evaluating on the test split.
I0308 19:15:49.614595 139902746892096 submission_runner.py:411] Time since start: 51272.64s, 	Step: 106475, 	{'train/accuracy': 0.7658007740974426, 'train/loss': 0.917033851146698, 'validation/accuracy': 0.6999399662017822, 'validation/loss': 1.2070460319519043, 'validation/num_examples': 50000, 'test/accuracy': 0.5738000273704529, 'test/loss': 1.8547242879867554, 'test/num_examples': 10000, 'score': 47516.95656371117, 'total_duration': 51272.64339399338, 'accumulated_submission_time': 47516.95656371117, 'accumulated_eval_time': 3744.832273721695, 'accumulated_logging_time': 5.208755970001221}
I0308 19:15:49.648846 139708407461632 logging_writer.py:48] [106475] accumulated_eval_time=3744.832274, accumulated_logging_time=5.208756, accumulated_submission_time=47516.956564, global_step=106475, preemption_count=0, score=47516.956564, test/accuracy=0.573800, test/loss=1.854724, test/num_examples=10000, total_duration=51272.643394, train/accuracy=0.765801, train/loss=0.917034, validation/accuracy=0.699940, validation/loss=1.207046, validation/num_examples=50000
I0308 19:15:59.880323 139708415854336 logging_writer.py:48] [106500] global_step=106500, grad_norm=1.5859163999557495, loss=1.834482192993164
I0308 19:16:41.140827 139708407461632 logging_writer.py:48] [106600] global_step=106600, grad_norm=1.4622539281845093, loss=3.866736650466919
I0308 19:17:26.333086 139708415854336 logging_writer.py:48] [106700] global_step=106700, grad_norm=1.459343433380127, loss=2.4483230113983154
I0308 19:18:12.227373 139708407461632 logging_writer.py:48] [106800] global_step=106800, grad_norm=1.5055879354476929, loss=2.602752208709717
I0308 19:18:57.378279 139708415854336 logging_writer.py:48] [106900] global_step=106900, grad_norm=1.7117539644241333, loss=1.910713791847229
I0308 19:19:42.779294 139708407461632 logging_writer.py:48] [107000] global_step=107000, grad_norm=1.5016523599624634, loss=1.8337374925613403
I0308 19:20:27.995646 139708415854336 logging_writer.py:48] [107100] global_step=107100, grad_norm=1.6069152355194092, loss=4.306797027587891
I0308 19:21:13.143758 139708407461632 logging_writer.py:48] [107200] global_step=107200, grad_norm=1.4688374996185303, loss=3.372720718383789
I0308 19:21:58.438535 139708415854336 logging_writer.py:48] [107300] global_step=107300, grad_norm=1.777834177017212, loss=1.8911173343658447
I0308 19:22:43.935886 139708407461632 logging_writer.py:48] [107400] global_step=107400, grad_norm=1.4197089672088623, loss=2.562842845916748
I0308 19:22:49.876258 139902746892096 spec.py:321] Evaluating on the training split.
I0308 19:23:01.460247 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 19:23:19.900390 139902746892096 spec.py:349] Evaluating on the test split.
I0308 19:23:21.592686 139902746892096 submission_runner.py:411] Time since start: 51724.62s, 	Step: 107415, 	{'train/accuracy': 0.7672070264816284, 'train/loss': 0.9189236760139465, 'validation/accuracy': 0.6965199708938599, 'validation/loss': 1.2294237613677979, 'validation/num_examples': 50000, 'test/accuracy': 0.5769000053405762, 'test/loss': 1.8736774921417236, 'test/num_examples': 10000, 'score': 47937.12469291687, 'total_duration': 51724.62146115303, 'accumulated_submission_time': 47937.12469291687, 'accumulated_eval_time': 3776.548688173294, 'accumulated_logging_time': 5.253453969955444}
I0308 19:23:21.635819 139708415854336 logging_writer.py:48] [107415] accumulated_eval_time=3776.548688, accumulated_logging_time=5.253454, accumulated_submission_time=47937.124693, global_step=107415, preemption_count=0, score=47937.124693, test/accuracy=0.576900, test/loss=1.873677, test/num_examples=10000, total_duration=51724.621461, train/accuracy=0.767207, train/loss=0.918924, validation/accuracy=0.696520, validation/loss=1.229424, validation/num_examples=50000
I0308 19:23:56.371232 139708407461632 logging_writer.py:48] [107500] global_step=107500, grad_norm=1.6057977676391602, loss=1.8229608535766602
I0308 19:24:41.898619 139708415854336 logging_writer.py:48] [107600] global_step=107600, grad_norm=1.5448729991912842, loss=3.9526400566101074
I0308 19:25:53.334649 139708407461632 logging_writer.py:48] [107700] global_step=107700, grad_norm=1.5305640697479248, loss=1.893937349319458
I0308 19:26:40.850023 139708415854336 logging_writer.py:48] [107800] global_step=107800, grad_norm=1.3719406127929688, loss=2.7715160846710205
I0308 19:27:26.711744 139708407461632 logging_writer.py:48] [107900] global_step=107900, grad_norm=1.5885571241378784, loss=1.8841840028762817
I0308 19:28:12.485537 139708415854336 logging_writer.py:48] [108000] global_step=108000, grad_norm=1.585402011871338, loss=1.865070104598999
I0308 19:28:58.064749 139708407461632 logging_writer.py:48] [108100] global_step=108100, grad_norm=1.608647346496582, loss=2.268419027328491
I0308 19:29:43.851365 139708415854336 logging_writer.py:48] [108200] global_step=108200, grad_norm=1.385781168937683, loss=2.879686117172241
I0308 19:30:21.890068 139902746892096 spec.py:321] Evaluating on the training split.
I0308 19:30:33.476930 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 19:30:55.873581 139902746892096 spec.py:349] Evaluating on the test split.
I0308 19:30:57.555929 139902746892096 submission_runner.py:411] Time since start: 52180.58s, 	Step: 108285, 	{'train/accuracy': 0.7604101300239563, 'train/loss': 0.9380630254745483, 'validation/accuracy': 0.7011599540710449, 'validation/loss': 1.2191523313522339, 'validation/num_examples': 50000, 'test/accuracy': 0.5774000287055969, 'test/loss': 1.8574777841567993, 'test/num_examples': 10000, 'score': 48357.32349872589, 'total_duration': 52180.58470964432, 'accumulated_submission_time': 48357.32349872589, 'accumulated_eval_time': 3812.2145340442657, 'accumulated_logging_time': 5.30767297744751}
I0308 19:30:57.595035 139708407461632 logging_writer.py:48] [108285] accumulated_eval_time=3812.214534, accumulated_logging_time=5.307673, accumulated_submission_time=48357.323499, global_step=108285, preemption_count=0, score=48357.323499, test/accuracy=0.577400, test/loss=1.857478, test/num_examples=10000, total_duration=52180.584710, train/accuracy=0.760410, train/loss=0.938063, validation/accuracy=0.701160, validation/loss=1.219152, validation/num_examples=50000
I0308 19:31:03.889753 139708415854336 logging_writer.py:48] [108300] global_step=108300, grad_norm=1.536895751953125, loss=2.0988430976867676
I0308 19:31:44.517919 139708407461632 logging_writer.py:48] [108400] global_step=108400, grad_norm=1.565550446510315, loss=1.868877649307251
I0308 19:32:29.836438 139708415854336 logging_writer.py:48] [108500] global_step=108500, grad_norm=1.517176866531372, loss=4.207766056060791
I0308 19:33:15.068629 139708407461632 logging_writer.py:48] [108600] global_step=108600, grad_norm=1.691380500793457, loss=1.8261330127716064
I0308 19:34:00.795228 139708415854336 logging_writer.py:48] [108700] global_step=108700, grad_norm=1.704956293106079, loss=1.812308669090271
I0308 19:34:45.990155 139708407461632 logging_writer.py:48] [108800] global_step=108800, grad_norm=1.4208263158798218, loss=3.757791757583618
I0308 19:35:31.298025 139708415854336 logging_writer.py:48] [108900] global_step=108900, grad_norm=1.5073847770690918, loss=3.868065595626831
I0308 19:36:16.486662 139708407461632 logging_writer.py:48] [109000] global_step=109000, grad_norm=1.5730785131454468, loss=1.9143551588058472
I0308 19:37:01.712753 139708415854336 logging_writer.py:48] [109100] global_step=109100, grad_norm=1.5852092504501343, loss=2.0554182529449463
I0308 19:37:47.006653 139708407461632 logging_writer.py:48] [109200] global_step=109200, grad_norm=1.5168131589889526, loss=1.9746816158294678
I0308 19:37:57.963332 139902746892096 spec.py:321] Evaluating on the training split.
I0308 19:38:09.372093 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 19:38:30.657939 139902746892096 spec.py:349] Evaluating on the test split.
I0308 19:38:32.344665 139902746892096 submission_runner.py:411] Time since start: 52635.37s, 	Step: 109226, 	{'train/accuracy': 0.7692773342132568, 'train/loss': 0.9186345934867859, 'validation/accuracy': 0.7051799893379211, 'validation/loss': 1.20063054561615, 'validation/num_examples': 50000, 'test/accuracy': 0.5776000022888184, 'test/loss': 1.8404319286346436, 'test/num_examples': 10000, 'score': 48777.63230252266, 'total_duration': 52635.37345814705, 'accumulated_submission_time': 48777.63230252266, 'accumulated_eval_time': 3846.5958664417267, 'accumulated_logging_time': 5.356571435928345}
I0308 19:38:32.375777 139708415854336 logging_writer.py:48] [109226] accumulated_eval_time=3846.595866, accumulated_logging_time=5.356571, accumulated_submission_time=48777.632303, global_step=109226, preemption_count=0, score=48777.632303, test/accuracy=0.577600, test/loss=1.840432, test/num_examples=10000, total_duration=52635.373458, train/accuracy=0.769277, train/loss=0.918635, validation/accuracy=0.705180, validation/loss=1.200631, validation/num_examples=50000
I0308 19:39:01.919771 139708407461632 logging_writer.py:48] [109300] global_step=109300, grad_norm=1.6415200233459473, loss=4.041001319885254
I0308 19:39:45.885781 139708415854336 logging_writer.py:48] [109400] global_step=109400, grad_norm=1.5626075267791748, loss=2.036921501159668
I0308 19:40:31.005685 139708407461632 logging_writer.py:48] [109500] global_step=109500, grad_norm=1.4445104598999023, loss=2.6480400562286377
I0308 19:41:16.379130 139708415854336 logging_writer.py:48] [109600] global_step=109600, grad_norm=1.4947304725646973, loss=2.714015483856201
I0308 19:42:01.044794 139708407461632 logging_writer.py:48] [109700] global_step=109700, grad_norm=1.673668622970581, loss=4.406369686126709
I0308 19:42:46.149520 139708415854336 logging_writer.py:48] [109800] global_step=109800, grad_norm=1.535154104232788, loss=4.013937473297119
I0308 19:43:31.460494 139708407461632 logging_writer.py:48] [109900] global_step=109900, grad_norm=1.4398170709609985, loss=2.351595640182495
I0308 19:44:16.647343 139708415854336 logging_writer.py:48] [110000] global_step=110000, grad_norm=1.4949464797973633, loss=2.690521240234375
I0308 19:45:02.058630 139708407461632 logging_writer.py:48] [110100] global_step=110100, grad_norm=1.4537830352783203, loss=2.1697847843170166
I0308 19:45:32.407125 139902746892096 spec.py:321] Evaluating on the training split.
I0308 19:45:43.808985 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 19:46:05.283325 139902746892096 spec.py:349] Evaluating on the test split.
I0308 19:46:06.959605 139902746892096 submission_runner.py:411] Time since start: 53089.99s, 	Step: 110169, 	{'train/accuracy': 0.7645898461341858, 'train/loss': 0.9615533947944641, 'validation/accuracy': 0.6979799866676331, 'validation/loss': 1.2633581161499023, 'validation/num_examples': 50000, 'test/accuracy': 0.5714000463485718, 'test/loss': 1.9044029712677002, 'test/num_examples': 10000, 'score': 49197.605489730835, 'total_duration': 53089.98840522766, 'accumulated_submission_time': 49197.605489730835, 'accumulated_eval_time': 3881.148336172104, 'accumulated_logging_time': 5.3964619636535645}
I0308 19:46:06.990298 139708415854336 logging_writer.py:48] [110169] accumulated_eval_time=3881.148336, accumulated_logging_time=5.396462, accumulated_submission_time=49197.605490, global_step=110169, preemption_count=0, score=49197.605490, test/accuracy=0.571400, test/loss=1.904403, test/num_examples=10000, total_duration=53089.988405, train/accuracy=0.764590, train/loss=0.961553, validation/accuracy=0.697980, validation/loss=1.263358, validation/num_examples=50000
I0308 19:46:19.586087 139708407461632 logging_writer.py:48] [110200] global_step=110200, grad_norm=1.5489954948425293, loss=4.037827014923096
I0308 19:47:01.742932 139708415854336 logging_writer.py:48] [110300] global_step=110300, grad_norm=1.6113134622573853, loss=1.74342942237854
I0308 19:47:47.013261 139708407461632 logging_writer.py:48] [110400] global_step=110400, grad_norm=1.7022286653518677, loss=1.748053789138794
I0308 19:48:32.445533 139708415854336 logging_writer.py:48] [110500] global_step=110500, grad_norm=1.42984139919281, loss=3.4902405738830566
I0308 19:49:17.759359 139708407461632 logging_writer.py:48] [110600] global_step=110600, grad_norm=1.5876411199569702, loss=4.201353549957275
I0308 19:50:02.696280 139708415854336 logging_writer.py:48] [110700] global_step=110700, grad_norm=1.6101018190383911, loss=2.047748565673828
I0308 19:50:47.863844 139708407461632 logging_writer.py:48] [110800] global_step=110800, grad_norm=1.5424989461898804, loss=2.743940830230713
I0308 19:51:33.064894 139708415854336 logging_writer.py:48] [110900] global_step=110900, grad_norm=1.7255980968475342, loss=1.8621231317520142
I0308 19:52:18.350407 139708407461632 logging_writer.py:48] [111000] global_step=111000, grad_norm=1.9706557989120483, loss=1.8178999423980713
I0308 19:53:03.636769 139708415854336 logging_writer.py:48] [111100] global_step=111100, grad_norm=1.7447404861450195, loss=1.9869118928909302
I0308 19:53:07.026088 139902746892096 spec.py:321] Evaluating on the training split.
I0308 19:53:18.589332 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 19:53:39.748745 139902746892096 spec.py:349] Evaluating on the test split.
I0308 19:53:41.430273 139902746892096 submission_runner.py:411] Time since start: 53544.46s, 	Step: 111109, 	{'train/accuracy': 0.7912499904632568, 'train/loss': 0.8058376908302307, 'validation/accuracy': 0.7072599530220032, 'validation/loss': 1.1865533590316772, 'validation/num_examples': 50000, 'test/accuracy': 0.5804000496864319, 'test/loss': 1.8377219438552856, 'test/num_examples': 10000, 'score': 49617.58262228966, 'total_duration': 53544.459074020386, 'accumulated_submission_time': 49617.58262228966, 'accumulated_eval_time': 3915.5525193214417, 'accumulated_logging_time': 5.436882019042969}
I0308 19:53:41.464101 139708407461632 logging_writer.py:48] [111109] accumulated_eval_time=3915.552519, accumulated_logging_time=5.436882, accumulated_submission_time=49617.582622, global_step=111109, preemption_count=0, score=49617.582622, test/accuracy=0.580400, test/loss=1.837722, test/num_examples=10000, total_duration=53544.459074, train/accuracy=0.791250, train/loss=0.805838, validation/accuracy=0.707260, validation/loss=1.186553, validation/num_examples=50000
I0308 19:54:17.952174 139708415854336 logging_writer.py:48] [111200] global_step=111200, grad_norm=1.5088839530944824, loss=1.841213583946228
I0308 19:55:03.201432 139708407461632 logging_writer.py:48] [111300] global_step=111300, grad_norm=1.7482212781906128, loss=1.8770169019699097
I0308 19:55:49.077076 139708415854336 logging_writer.py:48] [111400] global_step=111400, grad_norm=1.702010989189148, loss=1.8997445106506348
I0308 19:56:34.254941 139708407461632 logging_writer.py:48] [111500] global_step=111500, grad_norm=1.901401400566101, loss=1.7918007373809814
I0308 19:57:19.367676 139708415854336 logging_writer.py:48] [111600] global_step=111600, grad_norm=1.67392897605896, loss=1.8525580167770386
I0308 19:58:04.859972 139708407461632 logging_writer.py:48] [111700] global_step=111700, grad_norm=1.5744763612747192, loss=2.3289177417755127
I0308 19:58:49.698375 139708415854336 logging_writer.py:48] [111800] global_step=111800, grad_norm=1.542446494102478, loss=1.6834359169006348
I0308 19:59:34.896680 139708407461632 logging_writer.py:48] [111900] global_step=111900, grad_norm=1.6875407695770264, loss=3.265504837036133
I0308 20:00:20.022659 139708415854336 logging_writer.py:48] [112000] global_step=112000, grad_norm=1.7329717874526978, loss=3.7056524753570557
I0308 20:00:41.911090 139902746892096 spec.py:321] Evaluating on the training split.
I0308 20:00:53.605623 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 20:01:14.431971 139902746892096 spec.py:349] Evaluating on the test split.
I0308 20:01:16.111913 139902746892096 submission_runner.py:411] Time since start: 53999.14s, 	Step: 112049, 	{'train/accuracy': 0.7679296731948853, 'train/loss': 0.9026365280151367, 'validation/accuracy': 0.7026000022888184, 'validation/loss': 1.195265769958496, 'validation/num_examples': 50000, 'test/accuracy': 0.5787000060081482, 'test/loss': 1.8381128311157227, 'test/num_examples': 10000, 'score': 50037.971556425095, 'total_duration': 53999.140706539154, 'accumulated_submission_time': 50037.971556425095, 'accumulated_eval_time': 3949.753340244293, 'accumulated_logging_time': 5.479412078857422}
I0308 20:01:16.144451 139708407461632 logging_writer.py:48] [112049] accumulated_eval_time=3949.753340, accumulated_logging_time=5.479412, accumulated_submission_time=50037.971556, global_step=112049, preemption_count=0, score=50037.971556, test/accuracy=0.578700, test/loss=1.838113, test/num_examples=10000, total_duration=53999.140707, train/accuracy=0.767930, train/loss=0.902637, validation/accuracy=0.702600, validation/loss=1.195266, validation/num_examples=50000
I0308 20:01:36.623517 139708415854336 logging_writer.py:48] [112100] global_step=112100, grad_norm=1.5250288248062134, loss=3.9719836711883545
I0308 20:02:19.437800 139708407461632 logging_writer.py:48] [112200] global_step=112200, grad_norm=1.7731024026870728, loss=1.8245270252227783
I0308 20:03:04.477109 139708415854336 logging_writer.py:48] [112300] global_step=112300, grad_norm=1.854128360748291, loss=1.8301080465316772
I0308 20:03:49.794168 139708407461632 logging_writer.py:48] [112400] global_step=112400, grad_norm=1.7359145879745483, loss=3.6289124488830566
I0308 20:04:34.887148 139708415854336 logging_writer.py:48] [112500] global_step=112500, grad_norm=1.7021472454071045, loss=1.9325017929077148
I0308 20:05:20.102177 139708407461632 logging_writer.py:48] [112600] global_step=112600, grad_norm=1.6340785026550293, loss=1.7894002199172974
I0308 20:06:06.525300 139708415854336 logging_writer.py:48] [112700] global_step=112700, grad_norm=1.7482060194015503, loss=1.844283103942871
I0308 20:06:51.620713 139708407461632 logging_writer.py:48] [112800] global_step=112800, grad_norm=1.750373125076294, loss=3.4393527507781982
I0308 20:07:36.988694 139708415854336 logging_writer.py:48] [112900] global_step=112900, grad_norm=1.6523840427398682, loss=1.7339091300964355
I0308 20:08:16.201970 139902746892096 spec.py:321] Evaluating on the training split.
I0308 20:08:27.813441 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 20:08:49.139640 139902746892096 spec.py:349] Evaluating on the test split.
I0308 20:08:50.813638 139902746892096 submission_runner.py:411] Time since start: 54453.84s, 	Step: 112988, 	{'train/accuracy': 0.774707019329071, 'train/loss': 0.8800515532493591, 'validation/accuracy': 0.707319974899292, 'validation/loss': 1.1850789785385132, 'validation/num_examples': 50000, 'test/accuracy': 0.5800000429153442, 'test/loss': 1.8336036205291748, 'test/num_examples': 10000, 'score': 50457.97093844414, 'total_duration': 54453.84243106842, 'accumulated_submission_time': 50457.97093844414, 'accumulated_eval_time': 3984.365024328232, 'accumulated_logging_time': 5.520999431610107}
I0308 20:08:50.845260 139708407461632 logging_writer.py:48] [112988] accumulated_eval_time=3984.365024, accumulated_logging_time=5.520999, accumulated_submission_time=50457.970938, global_step=112988, preemption_count=0, score=50457.970938, test/accuracy=0.580000, test/loss=1.833604, test/num_examples=10000, total_duration=54453.842431, train/accuracy=0.774707, train/loss=0.880052, validation/accuracy=0.707320, validation/loss=1.185079, validation/num_examples=50000
I0308 20:08:55.968212 139708415854336 logging_writer.py:48] [113000] global_step=113000, grad_norm=1.5782920122146606, loss=2.410292387008667
I0308 20:09:36.751040 139708407461632 logging_writer.py:48] [113100] global_step=113100, grad_norm=1.7363388538360596, loss=1.8584495782852173
I0308 20:10:21.876698 139708415854336 logging_writer.py:48] [113200] global_step=113200, grad_norm=1.5545967817306519, loss=3.3592071533203125
I0308 20:11:07.238997 139708407461632 logging_writer.py:48] [113300] global_step=113300, grad_norm=1.5717658996582031, loss=3.4825241565704346
I0308 20:11:52.304756 139708415854336 logging_writer.py:48] [113400] global_step=113400, grad_norm=1.5777175426483154, loss=1.767704725265503
I0308 20:12:37.634479 139708407461632 logging_writer.py:48] [113500] global_step=113500, grad_norm=1.5970863103866577, loss=2.7966208457946777
I0308 20:13:22.994978 139708415854336 logging_writer.py:48] [113600] global_step=113600, grad_norm=1.7420477867126465, loss=1.747809886932373
I0308 20:14:08.152784 139708407461632 logging_writer.py:48] [113700] global_step=113700, grad_norm=1.5136477947235107, loss=2.5385072231292725
I0308 20:14:53.199657 139708415854336 logging_writer.py:48] [113800] global_step=113800, grad_norm=1.7931973934173584, loss=1.8965463638305664
I0308 20:15:38.837372 139708407461632 logging_writer.py:48] [113900] global_step=113900, grad_norm=1.6181138753890991, loss=2.0719361305236816
I0308 20:15:50.885242 139902746892096 spec.py:321] Evaluating on the training split.
I0308 20:16:02.514958 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 20:16:24.468303 139902746892096 spec.py:349] Evaluating on the test split.
I0308 20:16:26.147083 139902746892096 submission_runner.py:411] Time since start: 54909.18s, 	Step: 113928, 	{'train/accuracy': 0.7822265625, 'train/loss': 0.8601695895195007, 'validation/accuracy': 0.707099974155426, 'validation/loss': 1.1927919387817383, 'validation/num_examples': 50000, 'test/accuracy': 0.5804000496864319, 'test/loss': 1.8279626369476318, 'test/num_examples': 10000, 'score': 50877.951067209244, 'total_duration': 54909.1758646965, 'accumulated_submission_time': 50877.951067209244, 'accumulated_eval_time': 4019.6268467903137, 'accumulated_logging_time': 5.563291549682617}
I0308 20:16:26.184478 139708415854336 logging_writer.py:48] [113928] accumulated_eval_time=4019.626847, accumulated_logging_time=5.563292, accumulated_submission_time=50877.951067, global_step=113928, preemption_count=0, score=50877.951067, test/accuracy=0.580400, test/loss=1.827963, test/num_examples=10000, total_duration=54909.175865, train/accuracy=0.782227, train/loss=0.860170, validation/accuracy=0.707100, validation/loss=1.192792, validation/num_examples=50000
I0308 20:16:54.937208 139708407461632 logging_writer.py:48] [114000] global_step=114000, grad_norm=1.5656664371490479, loss=3.6818253993988037
I0308 20:17:39.141390 139708415854336 logging_writer.py:48] [114100] global_step=114100, grad_norm=1.8326116800308228, loss=1.8860564231872559
I0308 20:18:24.479337 139708407461632 logging_writer.py:48] [114200] global_step=114200, grad_norm=1.4985922574996948, loss=1.9182738065719604
I0308 20:19:09.942692 139708415854336 logging_writer.py:48] [114300] global_step=114300, grad_norm=1.738061547279358, loss=4.212080001831055
I0308 20:19:55.110747 139708407461632 logging_writer.py:48] [114400] global_step=114400, grad_norm=1.7793571949005127, loss=4.273943901062012
I0308 20:20:40.611670 139708415854336 logging_writer.py:48] [114500] global_step=114500, grad_norm=1.6130160093307495, loss=1.7947036027908325
I0308 20:21:25.762451 139708407461632 logging_writer.py:48] [114600] global_step=114600, grad_norm=1.679537296295166, loss=4.011676788330078
I0308 20:22:11.081504 139708415854336 logging_writer.py:48] [114700] global_step=114700, grad_norm=1.5516383647918701, loss=3.433239459991455
I0308 20:22:56.363979 139708407461632 logging_writer.py:48] [114800] global_step=114800, grad_norm=1.5847930908203125, loss=4.229541301727295
I0308 20:23:26.482459 139902746892096 spec.py:321] Evaluating on the training split.
I0308 20:23:38.085851 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 20:23:58.330709 139902746892096 spec.py:349] Evaluating on the test split.
I0308 20:24:00.007009 139902746892096 submission_runner.py:411] Time since start: 55363.04s, 	Step: 114868, 	{'train/accuracy': 0.7724804282188416, 'train/loss': 0.8807520866394043, 'validation/accuracy': 0.7059599757194519, 'validation/loss': 1.169217824935913, 'validation/num_examples': 50000, 'test/accuracy': 0.5825000405311584, 'test/loss': 1.8185060024261475, 'test/num_examples': 10000, 'score': 51298.191059827805, 'total_duration': 55363.03579878807, 'accumulated_submission_time': 51298.191059827805, 'accumulated_eval_time': 4053.1513781547546, 'accumulated_logging_time': 5.610148906707764}
I0308 20:24:00.050191 139708415854336 logging_writer.py:48] [114868] accumulated_eval_time=4053.151378, accumulated_logging_time=5.610149, accumulated_submission_time=51298.191060, global_step=114868, preemption_count=0, score=51298.191060, test/accuracy=0.582500, test/loss=1.818506, test/num_examples=10000, total_duration=55363.035799, train/accuracy=0.772480, train/loss=0.880752, validation/accuracy=0.705960, validation/loss=1.169218, validation/num_examples=50000
I0308 20:24:13.075395 139708407461632 logging_writer.py:48] [114900] global_step=114900, grad_norm=1.7710881233215332, loss=4.104321479797363
I0308 20:24:55.046981 139708415854336 logging_writer.py:48] [115000] global_step=115000, grad_norm=1.5329668521881104, loss=3.392725944519043
I0308 20:25:40.506870 139708407461632 logging_writer.py:48] [115100] global_step=115100, grad_norm=1.572896957397461, loss=1.6956080198287964
I0308 20:26:26.301131 139708415854336 logging_writer.py:48] [115200] global_step=115200, grad_norm=1.5443575382232666, loss=2.9855504035949707
I0308 20:27:11.413688 139708407461632 logging_writer.py:48] [115300] global_step=115300, grad_norm=1.6224660873413086, loss=1.8586283922195435
I0308 20:27:56.766666 139708415854336 logging_writer.py:48] [115400] global_step=115400, grad_norm=1.5836323499679565, loss=1.8003273010253906
I0308 20:28:42.384801 139708407461632 logging_writer.py:48] [115500] global_step=115500, grad_norm=1.5946717262268066, loss=3.4981303215026855
I0308 20:29:27.717462 139708415854336 logging_writer.py:48] [115600] global_step=115600, grad_norm=1.6931182146072388, loss=4.249184608459473
I0308 20:30:13.133229 139708407461632 logging_writer.py:48] [115700] global_step=115700, grad_norm=1.7189546823501587, loss=4.029398441314697
I0308 20:30:58.505223 139708415854336 logging_writer.py:48] [115800] global_step=115800, grad_norm=1.6130106449127197, loss=3.8270153999328613
I0308 20:31:00.100294 139902746892096 spec.py:321] Evaluating on the training split.
I0308 20:31:11.633982 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 20:31:31.801925 139902746892096 spec.py:349] Evaluating on the test split.
I0308 20:31:33.485209 139902746892096 submission_runner.py:411] Time since start: 55816.51s, 	Step: 115805, 	{'train/accuracy': 0.78187495470047, 'train/loss': 0.8490674495697021, 'validation/accuracy': 0.7106599807739258, 'validation/loss': 1.1583318710327148, 'validation/num_examples': 50000, 'test/accuracy': 0.5877000093460083, 'test/loss': 1.7957428693771362, 'test/num_examples': 10000, 'score': 51718.180342674255, 'total_duration': 55816.513998031616, 'accumulated_submission_time': 51718.180342674255, 'accumulated_eval_time': 4086.5362679958344, 'accumulated_logging_time': 5.664799213409424}
I0308 20:31:33.524159 139708407461632 logging_writer.py:48] [115805] accumulated_eval_time=4086.536268, accumulated_logging_time=5.664799, accumulated_submission_time=51718.180343, global_step=115805, preemption_count=0, score=51718.180343, test/accuracy=0.587700, test/loss=1.795743, test/num_examples=10000, total_duration=55816.513998, train/accuracy=0.781875, train/loss=0.849067, validation/accuracy=0.710660, validation/loss=1.158332, validation/num_examples=50000
I0308 20:32:12.228615 139708415854336 logging_writer.py:48] [115900] global_step=115900, grad_norm=1.64175546169281, loss=1.75996732711792
I0308 20:32:57.314836 139708407461632 logging_writer.py:48] [116000] global_step=116000, grad_norm=1.4911853075027466, loss=2.164078712463379
I0308 20:33:42.449872 139708415854336 logging_writer.py:48] [116100] global_step=116100, grad_norm=1.6356394290924072, loss=3.310023546218872
I0308 20:34:27.915563 139708407461632 logging_writer.py:48] [116200] global_step=116200, grad_norm=1.714155673980713, loss=1.7267345190048218
I0308 20:35:12.944992 139708415854336 logging_writer.py:48] [116300] global_step=116300, grad_norm=1.7606701850891113, loss=1.7218317985534668
I0308 20:35:58.528626 139708407461632 logging_writer.py:48] [116400] global_step=116400, grad_norm=1.5168979167938232, loss=2.073180675506592
I0308 20:36:43.757868 139708415854336 logging_writer.py:48] [116500] global_step=116500, grad_norm=1.7822318077087402, loss=1.8566675186157227
I0308 20:37:29.112757 139708407461632 logging_writer.py:48] [116600] global_step=116600, grad_norm=1.7296687364578247, loss=1.750820517539978
I0308 20:38:14.380309 139708415854336 logging_writer.py:48] [116700] global_step=116700, grad_norm=1.5342378616333008, loss=2.877152919769287
I0308 20:38:33.597458 139902746892096 spec.py:321] Evaluating on the training split.
I0308 20:38:45.051117 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 20:39:07.778111 139902746892096 spec.py:349] Evaluating on the test split.
I0308 20:39:09.441150 139902746892096 submission_runner.py:411] Time since start: 56272.47s, 	Step: 116744, 	{'train/accuracy': 0.7876366972923279, 'train/loss': 0.8236556053161621, 'validation/accuracy': 0.7130599617958069, 'validation/loss': 1.1581529378890991, 'validation/num_examples': 50000, 'test/accuracy': 0.5914000272750854, 'test/loss': 1.7908883094787598, 'test/num_examples': 10000, 'score': 52138.19392943382, 'total_duration': 56272.46994638443, 'accumulated_submission_time': 52138.19392943382, 'accumulated_eval_time': 4122.379958868027, 'accumulated_logging_time': 5.713786840438843}
I0308 20:39:09.477023 139708407461632 logging_writer.py:48] [116744] accumulated_eval_time=4122.379959, accumulated_logging_time=5.713787, accumulated_submission_time=52138.193929, global_step=116744, preemption_count=0, score=52138.193929, test/accuracy=0.591400, test/loss=1.790888, test/num_examples=10000, total_duration=56272.469946, train/accuracy=0.787637, train/loss=0.823656, validation/accuracy=0.713060, validation/loss=1.158153, validation/num_examples=50000
I0308 20:39:31.917114 139708415854336 logging_writer.py:48] [116800] global_step=116800, grad_norm=1.671399712562561, loss=1.9678599834442139
I0308 20:40:14.575468 139708407461632 logging_writer.py:48] [116900] global_step=116900, grad_norm=1.5030046701431274, loss=2.2739202976226807
I0308 20:40:59.916148 139708415854336 logging_writer.py:48] [117000] global_step=117000, grad_norm=1.6870301961898804, loss=3.27608585357666
I0308 20:41:45.658383 139708407461632 logging_writer.py:48] [117100] global_step=117100, grad_norm=1.7475075721740723, loss=1.7055227756500244
I0308 20:42:30.724825 139708415854336 logging_writer.py:48] [117200] global_step=117200, grad_norm=1.6161305904388428, loss=1.8348008394241333
I0308 20:43:15.935960 139708407461632 logging_writer.py:48] [117300] global_step=117300, grad_norm=1.5240864753723145, loss=2.0656540393829346
I0308 20:44:01.170265 139708415854336 logging_writer.py:48] [117400] global_step=117400, grad_norm=1.6695533990859985, loss=1.9621435403823853
I0308 20:44:46.442230 139708407461632 logging_writer.py:48] [117500] global_step=117500, grad_norm=1.5361521244049072, loss=3.38069224357605
I0308 20:45:31.879436 139708415854336 logging_writer.py:48] [117600] global_step=117600, grad_norm=1.6265723705291748, loss=2.1486716270446777
I0308 20:46:09.837966 139902746892096 spec.py:321] Evaluating on the training split.
I0308 20:46:21.221948 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 20:46:42.751288 139902746892096 spec.py:349] Evaluating on the test split.
I0308 20:46:44.431066 139902746892096 submission_runner.py:411] Time since start: 56727.46s, 	Step: 117685, 	{'train/accuracy': 0.8011718392372131, 'train/loss': 0.7712819576263428, 'validation/accuracy': 0.7135599851608276, 'validation/loss': 1.1575746536254883, 'validation/num_examples': 50000, 'test/accuracy': 0.5821000337600708, 'test/loss': 1.8017897605895996, 'test/num_examples': 10000, 'score': 52558.495411634445, 'total_duration': 56727.45986151695, 'accumulated_submission_time': 52558.495411634445, 'accumulated_eval_time': 4156.973059415817, 'accumulated_logging_time': 5.760483264923096}
I0308 20:46:44.467811 139708407461632 logging_writer.py:48] [117685] accumulated_eval_time=4156.973059, accumulated_logging_time=5.760483, accumulated_submission_time=52558.495412, global_step=117685, preemption_count=0, score=52558.495412, test/accuracy=0.582100, test/loss=1.801790, test/num_examples=10000, total_duration=56727.459862, train/accuracy=0.801172, train/loss=0.771282, validation/accuracy=0.713560, validation/loss=1.157575, validation/num_examples=50000
I0308 20:46:50.769929 139708415854336 logging_writer.py:48] [117700] global_step=117700, grad_norm=1.767409324645996, loss=1.8384127616882324
I0308 20:47:31.824821 139708407461632 logging_writer.py:48] [117800] global_step=117800, grad_norm=1.5729072093963623, loss=3.958876132965088
I0308 20:48:17.057890 139708415854336 logging_writer.py:48] [117900] global_step=117900, grad_norm=1.7275569438934326, loss=1.9422521591186523
I0308 20:49:02.655117 139708407461632 logging_writer.py:48] [118000] global_step=118000, grad_norm=1.8035258054733276, loss=4.21197509765625
I0308 20:49:47.585549 139708415854336 logging_writer.py:48] [118100] global_step=118100, grad_norm=1.662732720375061, loss=1.8252097368240356
I0308 20:50:33.026638 139708407461632 logging_writer.py:48] [118200] global_step=118200, grad_norm=1.7218018770217896, loss=1.792154312133789
I0308 20:51:18.672503 139708415854336 logging_writer.py:48] [118300] global_step=118300, grad_norm=1.7977691888809204, loss=2.2839531898498535
I0308 20:52:03.853152 139708407461632 logging_writer.py:48] [118400] global_step=118400, grad_norm=1.8706001043319702, loss=1.7400939464569092
I0308 20:52:49.171235 139708415854336 logging_writer.py:48] [118500] global_step=118500, grad_norm=1.705001711845398, loss=1.7998840808868408
I0308 20:53:34.366665 139708407461632 logging_writer.py:48] [118600] global_step=118600, grad_norm=1.997984766960144, loss=4.3082804679870605
I0308 20:53:44.609016 139902746892096 spec.py:321] Evaluating on the training split.
I0308 20:53:56.011668 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 20:54:19.001138 139902746892096 spec.py:349] Evaluating on the test split.
I0308 20:54:20.666788 139902746892096 submission_runner.py:411] Time since start: 57183.70s, 	Step: 118624, 	{'train/accuracy': 0.7796484231948853, 'train/loss': 0.8606301546096802, 'validation/accuracy': 0.7136399745941162, 'validation/loss': 1.1567219495773315, 'validation/num_examples': 50000, 'test/accuracy': 0.5915000438690186, 'test/loss': 1.8010717630386353, 'test/num_examples': 10000, 'score': 52978.57805562019, 'total_duration': 57183.69558787346, 'accumulated_submission_time': 52978.57805562019, 'accumulated_eval_time': 4193.030814886093, 'accumulated_logging_time': 5.806100606918335}
I0308 20:54:20.698795 139708415854336 logging_writer.py:48] [118624] accumulated_eval_time=4193.030815, accumulated_logging_time=5.806101, accumulated_submission_time=52978.578056, global_step=118624, preemption_count=0, score=52978.578056, test/accuracy=0.591500, test/loss=1.801072, test/num_examples=10000, total_duration=57183.695588, train/accuracy=0.779648, train/loss=0.860630, validation/accuracy=0.713640, validation/loss=1.156722, validation/num_examples=50000
I0308 20:54:51.003349 139708407461632 logging_writer.py:48] [118700] global_step=118700, grad_norm=1.7776949405670166, loss=1.8836133480072021
I0308 20:55:34.978205 139708415854336 logging_writer.py:48] [118800] global_step=118800, grad_norm=1.518815279006958, loss=3.18515682220459
I0308 20:56:20.496016 139708407461632 logging_writer.py:48] [118900] global_step=118900, grad_norm=1.7760860919952393, loss=1.8378148078918457
I0308 20:57:05.855680 139708415854336 logging_writer.py:48] [119000] global_step=119000, grad_norm=1.6125664710998535, loss=4.029627323150635
I0308 20:57:51.166388 139708407461632 logging_writer.py:48] [119100] global_step=119100, grad_norm=1.5571595430374146, loss=2.522411584854126
I0308 20:58:36.455929 139708415854336 logging_writer.py:48] [119200] global_step=119200, grad_norm=1.7565304040908813, loss=3.7153477668762207
I0308 20:59:21.507579 139708407461632 logging_writer.py:48] [119300] global_step=119300, grad_norm=1.6756279468536377, loss=1.7618134021759033
I0308 21:00:06.796120 139708415854336 logging_writer.py:48] [119400] global_step=119400, grad_norm=1.6180446147918701, loss=2.740403175354004
I0308 21:00:51.932965 139708407461632 logging_writer.py:48] [119500] global_step=119500, grad_norm=1.7803844213485718, loss=1.9418259859085083
I0308 21:01:21.010192 139902746892096 spec.py:321] Evaluating on the training split.
I0308 21:01:32.414719 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 21:01:54.088969 139902746892096 spec.py:349] Evaluating on the test split.
I0308 21:01:55.774679 139902746892096 submission_runner.py:411] Time since start: 57638.80s, 	Step: 119566, 	{'train/accuracy': 0.7863476276397705, 'train/loss': 0.8422191143035889, 'validation/accuracy': 0.7168399691581726, 'validation/loss': 1.1445538997650146, 'validation/num_examples': 50000, 'test/accuracy': 0.5922000408172607, 'test/loss': 1.7661399841308594, 'test/num_examples': 10000, 'score': 53398.8308763504, 'total_duration': 57638.8034594059, 'accumulated_submission_time': 53398.8308763504, 'accumulated_eval_time': 4227.795293569565, 'accumulated_logging_time': 5.846826076507568}
I0308 21:01:55.812176 139708415854336 logging_writer.py:48] [119566] accumulated_eval_time=4227.795294, accumulated_logging_time=5.846826, accumulated_submission_time=53398.830876, global_step=119566, preemption_count=0, score=53398.830876, test/accuracy=0.592200, test/loss=1.766140, test/num_examples=10000, total_duration=57638.803459, train/accuracy=0.786348, train/loss=0.842219, validation/accuracy=0.716840, validation/loss=1.144554, validation/num_examples=50000
I0308 21:02:09.589751 139708407461632 logging_writer.py:48] [119600] global_step=119600, grad_norm=1.7680888175964355, loss=1.6353577375411987
I0308 21:02:51.879745 139708415854336 logging_writer.py:48] [119700] global_step=119700, grad_norm=1.64092218875885, loss=3.766643524169922
I0308 21:03:37.225235 139708407461632 logging_writer.py:48] [119800] global_step=119800, grad_norm=1.5702142715454102, loss=2.969072103500366
I0308 21:04:22.739500 139708415854336 logging_writer.py:48] [119900] global_step=119900, grad_norm=1.9041833877563477, loss=1.712443470954895
I0308 21:05:08.308190 139708407461632 logging_writer.py:48] [120000] global_step=120000, grad_norm=1.741086721420288, loss=1.7062323093414307
I0308 21:05:53.706448 139708415854336 logging_writer.py:48] [120100] global_step=120100, grad_norm=1.686859130859375, loss=1.7883296012878418
I0308 21:06:39.329590 139708407461632 logging_writer.py:48] [120200] global_step=120200, grad_norm=1.8976733684539795, loss=1.8767294883728027
I0308 21:07:24.684633 139708415854336 logging_writer.py:48] [120300] global_step=120300, grad_norm=1.8064210414886475, loss=1.6094093322753906
I0308 21:08:10.215233 139708407461632 logging_writer.py:48] [120400] global_step=120400, grad_norm=1.7648735046386719, loss=1.6796890497207642
I0308 21:08:55.594983 139708415854336 logging_writer.py:48] [120500] global_step=120500, grad_norm=1.8620500564575195, loss=1.7435071468353271
I0308 21:08:56.195938 139902746892096 spec.py:321] Evaluating on the training split.
I0308 21:09:07.661853 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 21:09:30.574842 139902746892096 spec.py:349] Evaluating on the test split.
I0308 21:09:32.247322 139902746892096 submission_runner.py:411] Time since start: 58095.28s, 	Step: 120503, 	{'train/accuracy': 0.7981640696525574, 'train/loss': 0.784917950630188, 'validation/accuracy': 0.7168599963188171, 'validation/loss': 1.13680100440979, 'validation/num_examples': 50000, 'test/accuracy': 0.5898000001907349, 'test/loss': 1.7828012704849243, 'test/num_examples': 10000, 'score': 53819.15520334244, 'total_duration': 58095.27610850334, 'accumulated_submission_time': 53819.15520334244, 'accumulated_eval_time': 4263.846667289734, 'accumulated_logging_time': 5.894752502441406}
I0308 21:09:32.284153 139708407461632 logging_writer.py:48] [120503] accumulated_eval_time=4263.846667, accumulated_logging_time=5.894753, accumulated_submission_time=53819.155203, global_step=120503, preemption_count=0, score=53819.155203, test/accuracy=0.589800, test/loss=1.782801, test/num_examples=10000, total_duration=58095.276109, train/accuracy=0.798164, train/loss=0.784918, validation/accuracy=0.716860, validation/loss=1.136801, validation/num_examples=50000
I0308 21:10:11.223853 139708415854336 logging_writer.py:48] [120600] global_step=120600, grad_norm=1.6503105163574219, loss=1.7393321990966797
I0308 21:10:56.437423 139708407461632 logging_writer.py:48] [120700] global_step=120700, grad_norm=1.6320551633834839, loss=3.010246753692627
I0308 21:11:42.110249 139708415854336 logging_writer.py:48] [120800] global_step=120800, grad_norm=1.6162748336791992, loss=2.397592067718506
I0308 21:12:27.357765 139708407461632 logging_writer.py:48] [120900] global_step=120900, grad_norm=1.8215733766555786, loss=1.6574344635009766
I0308 21:13:12.834964 139708415854336 logging_writer.py:48] [121000] global_step=121000, grad_norm=1.533046841621399, loss=3.048109531402588
I0308 21:13:58.121872 139708407461632 logging_writer.py:48] [121100] global_step=121100, grad_norm=1.7459856271743774, loss=1.6721301078796387
I0308 21:14:43.468906 139708415854336 logging_writer.py:48] [121200] global_step=121200, grad_norm=1.5653152465820312, loss=2.4882357120513916
I0308 21:15:28.671249 139708407461632 logging_writer.py:48] [121300] global_step=121300, grad_norm=1.7125720977783203, loss=1.7518813610076904
I0308 21:16:14.304039 139708415854336 logging_writer.py:48] [121400] global_step=121400, grad_norm=1.946219801902771, loss=1.6631288528442383
I0308 21:16:32.390974 139902746892096 spec.py:321] Evaluating on the training split.
I0308 21:16:43.765373 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 21:17:08.160709 139902746892096 spec.py:349] Evaluating on the test split.
I0308 21:17:09.829478 139902746892096 submission_runner.py:411] Time since start: 58552.86s, 	Step: 121441, 	{'train/accuracy': 0.7860156297683716, 'train/loss': 0.8371008038520813, 'validation/accuracy': 0.7204799652099609, 'validation/loss': 1.1226909160614014, 'validation/num_examples': 50000, 'test/accuracy': 0.5955000519752502, 'test/loss': 1.7644239664077759, 'test/num_examples': 10000, 'score': 54239.20383620262, 'total_duration': 58552.858276844025, 'accumulated_submission_time': 54239.20383620262, 'accumulated_eval_time': 4301.28515791893, 'accumulated_logging_time': 5.94125771522522}
I0308 21:17:09.866440 139708407461632 logging_writer.py:48] [121441] accumulated_eval_time=4301.285158, accumulated_logging_time=5.941258, accumulated_submission_time=54239.203836, global_step=121441, preemption_count=0, score=54239.203836, test/accuracy=0.595500, test/loss=1.764424, test/num_examples=10000, total_duration=58552.858277, train/accuracy=0.786016, train/loss=0.837101, validation/accuracy=0.720480, validation/loss=1.122691, validation/num_examples=50000
I0308 21:17:33.465336 139708415854336 logging_writer.py:48] [121500] global_step=121500, grad_norm=1.6140289306640625, loss=2.680501699447632
I0308 21:18:17.381563 139708407461632 logging_writer.py:48] [121600] global_step=121600, grad_norm=1.733047604560852, loss=2.2774407863616943
I0308 21:19:02.933950 139708415854336 logging_writer.py:48] [121700] global_step=121700, grad_norm=1.868924617767334, loss=3.702531337738037
I0308 21:19:48.860503 139708407461632 logging_writer.py:48] [121800] global_step=121800, grad_norm=1.7165937423706055, loss=2.9128518104553223
I0308 21:20:34.684523 139708415854336 logging_writer.py:48] [121900] global_step=121900, grad_norm=1.8561569452285767, loss=1.6837177276611328
I0308 21:21:20.370086 139708407461632 logging_writer.py:48] [122000] global_step=122000, grad_norm=1.7629327774047852, loss=4.092705249786377
I0308 21:22:05.571380 139708415854336 logging_writer.py:48] [122100] global_step=122100, grad_norm=1.7513587474822998, loss=1.6249120235443115
I0308 21:22:51.295127 139708407461632 logging_writer.py:48] [122200] global_step=122200, grad_norm=1.7995234727859497, loss=1.7573652267456055
I0308 21:23:36.880158 139708415854336 logging_writer.py:48] [122300] global_step=122300, grad_norm=1.7489112615585327, loss=3.4991400241851807
I0308 21:24:10.161751 139902746892096 spec.py:321] Evaluating on the training split.
I0308 21:24:21.684142 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 21:24:42.583606 139902746892096 spec.py:349] Evaluating on the test split.
I0308 21:24:44.289890 139902746892096 submission_runner.py:411] Time since start: 59007.32s, 	Step: 122374, 	{'train/accuracy': 0.7886718511581421, 'train/loss': 0.8289256691932678, 'validation/accuracy': 0.7191599607467651, 'validation/loss': 1.1320635080337524, 'validation/num_examples': 50000, 'test/accuracy': 0.5901000499725342, 'test/loss': 1.7791229486465454, 'test/num_examples': 10000, 'score': 54659.44230914116, 'total_duration': 59007.31868457794, 'accumulated_submission_time': 54659.44230914116, 'accumulated_eval_time': 4335.413290023804, 'accumulated_logging_time': 5.986897230148315}
I0308 21:24:44.326822 139708407461632 logging_writer.py:48] [122374] accumulated_eval_time=4335.413290, accumulated_logging_time=5.986897, accumulated_submission_time=54659.442309, global_step=122374, preemption_count=0, score=54659.442309, test/accuracy=0.590100, test/loss=1.779123, test/num_examples=10000, total_duration=59007.318685, train/accuracy=0.788672, train/loss=0.828926, validation/accuracy=0.719160, validation/loss=1.132064, validation/num_examples=50000
I0308 21:24:54.950508 139708415854336 logging_writer.py:48] [122400] global_step=122400, grad_norm=1.8361965417861938, loss=1.7518439292907715
I0308 21:25:36.989444 139708407461632 logging_writer.py:48] [122500] global_step=122500, grad_norm=1.7235480546951294, loss=1.9399670362472534
I0308 21:26:22.716323 139708415854336 logging_writer.py:48] [122600] global_step=122600, grad_norm=1.880833625793457, loss=2.214932441711426
I0308 21:27:07.997372 139708407461632 logging_writer.py:48] [122700] global_step=122700, grad_norm=1.7165781259536743, loss=1.8118540048599243
I0308 21:27:53.394026 139708415854336 logging_writer.py:48] [122800] global_step=122800, grad_norm=1.8229693174362183, loss=1.8502010107040405
I0308 21:28:38.726291 139708407461632 logging_writer.py:48] [122900] global_step=122900, grad_norm=1.8336741924285889, loss=1.7145949602127075
I0308 21:29:23.801832 139708415854336 logging_writer.py:48] [123000] global_step=123000, grad_norm=1.8071722984313965, loss=1.648735523223877
I0308 21:30:08.998800 139708407461632 logging_writer.py:48] [123100] global_step=123100, grad_norm=1.754354476928711, loss=1.6392892599105835
I0308 21:30:54.210936 139708415854336 logging_writer.py:48] [123200] global_step=123200, grad_norm=1.5482665300369263, loss=3.064539670944214
I0308 21:31:39.362615 139708407461632 logging_writer.py:48] [123300] global_step=123300, grad_norm=1.6977771520614624, loss=2.2584779262542725
I0308 21:31:44.482144 139902746892096 spec.py:321] Evaluating on the training split.
I0308 21:31:56.269336 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 21:32:17.899512 139902746892096 spec.py:349] Evaluating on the test split.
I0308 21:32:19.576995 139902746892096 submission_runner.py:411] Time since start: 59462.61s, 	Step: 123313, 	{'train/accuracy': 0.7957812547683716, 'train/loss': 0.767406165599823, 'validation/accuracy': 0.7225199937820435, 'validation/loss': 1.1092499494552612, 'validation/num_examples': 50000, 'test/accuracy': 0.5976999998092651, 'test/loss': 1.7560497522354126, 'test/num_examples': 10000, 'score': 55079.53868961334, 'total_duration': 59462.60575866699, 'accumulated_submission_time': 55079.53868961334, 'accumulated_eval_time': 4370.508100986481, 'accumulated_logging_time': 6.033722162246704}
I0308 21:32:19.614347 139708415854336 logging_writer.py:48] [123313] accumulated_eval_time=4370.508101, accumulated_logging_time=6.033722, accumulated_submission_time=55079.538690, global_step=123313, preemption_count=0, score=55079.538690, test/accuracy=0.597700, test/loss=1.756050, test/num_examples=10000, total_duration=59462.605759, train/accuracy=0.795781, train/loss=0.767406, validation/accuracy=0.722520, validation/loss=1.109250, validation/num_examples=50000
I0308 21:32:54.239549 139708407461632 logging_writer.py:48] [123400] global_step=123400, grad_norm=1.8158644437789917, loss=3.4302425384521484
I0308 21:33:39.025264 139708415854336 logging_writer.py:48] [123500] global_step=123500, grad_norm=1.8460170030593872, loss=2.348219871520996
I0308 21:34:24.132150 139708407461632 logging_writer.py:48] [123600] global_step=123600, grad_norm=1.759104609489441, loss=3.9086050987243652
I0308 21:35:09.257565 139708415854336 logging_writer.py:48] [123700] global_step=123700, grad_norm=1.8500430583953857, loss=1.669952392578125
I0308 21:35:54.466915 139708407461632 logging_writer.py:48] [123800] global_step=123800, grad_norm=1.883550763130188, loss=1.8041589260101318
I0308 21:36:40.314120 139708415854336 logging_writer.py:48] [123900] global_step=123900, grad_norm=1.8369873762130737, loss=1.6556506156921387
I0308 21:37:25.865425 139708407461632 logging_writer.py:48] [124000] global_step=124000, grad_norm=1.930160403251648, loss=1.6239022016525269
I0308 21:38:11.048324 139708415854336 logging_writer.py:48] [124100] global_step=124100, grad_norm=1.7746665477752686, loss=3.6106252670288086
I0308 21:38:56.425624 139708407461632 logging_writer.py:48] [124200] global_step=124200, grad_norm=1.7360504865646362, loss=1.8130877017974854
I0308 21:39:19.609910 139902746892096 spec.py:321] Evaluating on the training split.
I0308 21:39:30.895023 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 21:39:53.245383 139902746892096 spec.py:349] Evaluating on the test split.
I0308 21:39:54.917844 139902746892096 submission_runner.py:411] Time since start: 59917.95s, 	Step: 124253, 	{'train/accuracy': 0.8089062571525574, 'train/loss': 0.7466806769371033, 'validation/accuracy': 0.7249999642372131, 'validation/loss': 1.1076456308364868, 'validation/num_examples': 50000, 'test/accuracy': 0.598300039768219, 'test/loss': 1.7344789505004883, 'test/num_examples': 10000, 'score': 55499.473816633224, 'total_duration': 59917.94662761688, 'accumulated_submission_time': 55499.473816633224, 'accumulated_eval_time': 4405.81605887413, 'accumulated_logging_time': 6.080605506896973}
I0308 21:39:54.959306 139708415854336 logging_writer.py:48] [124253] accumulated_eval_time=4405.816059, accumulated_logging_time=6.080606, accumulated_submission_time=55499.473817, global_step=124253, preemption_count=0, score=55499.473817, test/accuracy=0.598300, test/loss=1.734479, test/num_examples=10000, total_duration=59917.946628, train/accuracy=0.808906, train/loss=0.746681, validation/accuracy=0.725000, validation/loss=1.107646, validation/num_examples=50000
I0308 21:40:13.858095 139708407461632 logging_writer.py:48] [124300] global_step=124300, grad_norm=1.811532735824585, loss=2.0696239471435547
I0308 21:40:56.642146 139708415854336 logging_writer.py:48] [124400] global_step=124400, grad_norm=1.795818567276001, loss=4.06857442855835
I0308 21:41:41.785655 139708407461632 logging_writer.py:48] [124500] global_step=124500, grad_norm=1.7658908367156982, loss=2.0141026973724365
I0308 21:42:27.249076 139708415854336 logging_writer.py:48] [124600] global_step=124600, grad_norm=1.859806776046753, loss=4.05305290222168
I0308 21:43:12.390291 139708407461632 logging_writer.py:48] [124700] global_step=124700, grad_norm=1.7600784301757812, loss=1.7553505897521973
I0308 21:43:57.583653 139708415854336 logging_writer.py:48] [124800] global_step=124800, grad_norm=1.8039242029190063, loss=1.7880265712738037
I0308 21:44:42.802211 139708407461632 logging_writer.py:48] [124900] global_step=124900, grad_norm=1.7992335557937622, loss=1.8876160383224487
I0308 21:45:27.854786 139708415854336 logging_writer.py:48] [125000] global_step=125000, grad_norm=1.5692075490951538, loss=2.577911138534546
I0308 21:46:13.204193 139708407461632 logging_writer.py:48] [125100] global_step=125100, grad_norm=1.7230840921401978, loss=3.8849093914031982
I0308 21:46:55.078379 139902746892096 spec.py:321] Evaluating on the training split.
I0308 21:47:06.551254 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 21:47:28.834191 139902746892096 spec.py:349] Evaluating on the test split.
I0308 21:47:30.509723 139902746892096 submission_runner.py:411] Time since start: 60373.54s, 	Step: 125194, 	{'train/accuracy': 0.7921679615974426, 'train/loss': 0.793973445892334, 'validation/accuracy': 0.7214199900627136, 'validation/loss': 1.105116844177246, 'validation/num_examples': 50000, 'test/accuracy': 0.6028000116348267, 'test/loss': 1.7382676601409912, 'test/num_examples': 10000, 'score': 55919.53238034248, 'total_duration': 60373.53852200508, 'accumulated_submission_time': 55919.53238034248, 'accumulated_eval_time': 4441.247399568558, 'accumulated_logging_time': 6.1333723068237305}
I0308 21:47:30.548890 139708415854336 logging_writer.py:48] [125194] accumulated_eval_time=4441.247400, accumulated_logging_time=6.133372, accumulated_submission_time=55919.532380, global_step=125194, preemption_count=0, score=55919.532380, test/accuracy=0.602800, test/loss=1.738268, test/num_examples=10000, total_duration=60373.538522, train/accuracy=0.792168, train/loss=0.793973, validation/accuracy=0.721420, validation/loss=1.105117, validation/num_examples=50000
I0308 21:47:33.301080 139708407461632 logging_writer.py:48] [125200] global_step=125200, grad_norm=1.7647833824157715, loss=3.986720561981201
I0308 21:48:13.547849 139708415854336 logging_writer.py:48] [125300] global_step=125300, grad_norm=1.6672217845916748, loss=3.268430471420288
I0308 21:48:58.738830 139708407461632 logging_writer.py:48] [125400] global_step=125400, grad_norm=1.8901480436325073, loss=1.5867493152618408
I0308 21:49:44.301784 139708415854336 logging_writer.py:48] [125500] global_step=125500, grad_norm=1.7452728748321533, loss=2.085139751434326
I0308 21:50:29.702499 139708407461632 logging_writer.py:48] [125600] global_step=125600, grad_norm=1.6127254962921143, loss=2.3173351287841797
I0308 21:51:14.847767 139708415854336 logging_writer.py:48] [125700] global_step=125700, grad_norm=1.8729267120361328, loss=1.7972328662872314
I0308 21:52:00.308615 139708407461632 logging_writer.py:48] [125800] global_step=125800, grad_norm=1.7784407138824463, loss=1.6648805141448975
I0308 21:52:45.587616 139708415854336 logging_writer.py:48] [125900] global_step=125900, grad_norm=1.8783650398254395, loss=1.7758232355117798
I0308 21:53:30.969341 139708407461632 logging_writer.py:48] [126000] global_step=126000, grad_norm=1.8029284477233887, loss=1.835328459739685
I0308 21:54:16.213130 139708415854336 logging_writer.py:48] [126100] global_step=126100, grad_norm=1.8069061040878296, loss=1.6657010316848755
I0308 21:54:30.808258 139902746892096 spec.py:321] Evaluating on the training split.
I0308 21:54:42.191850 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 21:55:01.750374 139902746892096 spec.py:349] Evaluating on the test split.
I0308 21:55:03.433090 139902746892096 submission_runner.py:411] Time since start: 60826.46s, 	Step: 126134, 	{'train/accuracy': 0.8014843463897705, 'train/loss': 0.7683876156806946, 'validation/accuracy': 0.7262399792671204, 'validation/loss': 1.1003386974334717, 'validation/num_examples': 50000, 'test/accuracy': 0.6052000522613525, 'test/loss': 1.73361337184906, 'test/num_examples': 10000, 'score': 56339.733157634735, 'total_duration': 60826.4618768692, 'accumulated_submission_time': 56339.733157634735, 'accumulated_eval_time': 4473.872217655182, 'accumulated_logging_time': 6.182005167007446}
I0308 21:55:03.474522 139708407461632 logging_writer.py:48] [126134] accumulated_eval_time=4473.872218, accumulated_logging_time=6.182005, accumulated_submission_time=56339.733158, global_step=126134, preemption_count=0, score=56339.733158, test/accuracy=0.605200, test/loss=1.733613, test/num_examples=10000, total_duration=60826.461877, train/accuracy=0.801484, train/loss=0.768388, validation/accuracy=0.726240, validation/loss=1.100339, validation/num_examples=50000
I0308 21:55:29.879879 139708415854336 logging_writer.py:48] [126200] global_step=126200, grad_norm=1.8630191087722778, loss=1.603348970413208
I0308 21:56:13.923968 139708407461632 logging_writer.py:48] [126300] global_step=126300, grad_norm=1.6552103757858276, loss=2.9155542850494385
I0308 21:56:59.463070 139708415854336 logging_writer.py:48] [126400] global_step=126400, grad_norm=1.7427384853363037, loss=1.5872882604599
I0308 21:57:45.070279 139708407461632 logging_writer.py:48] [126500] global_step=126500, grad_norm=1.8627116680145264, loss=1.710637092590332
I0308 21:58:30.312110 139708415854336 logging_writer.py:48] [126600] global_step=126600, grad_norm=1.6752197742462158, loss=2.2849056720733643
I0308 21:59:15.594866 139708407461632 logging_writer.py:48] [126700] global_step=126700, grad_norm=1.7586266994476318, loss=1.5815303325653076
I0308 22:00:00.662529 139708415854336 logging_writer.py:48] [126800] global_step=126800, grad_norm=1.9269722700119019, loss=1.9515756368637085
I0308 22:00:45.895421 139708407461632 logging_writer.py:48] [126900] global_step=126900, grad_norm=1.6673532724380493, loss=2.884209632873535
I0308 22:01:31.351178 139708415854336 logging_writer.py:48] [127000] global_step=127000, grad_norm=1.6484177112579346, loss=3.0134949684143066
I0308 22:02:03.693002 139902746892096 spec.py:321] Evaluating on the training split.
I0308 22:02:15.267516 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 22:02:36.870745 139902746892096 spec.py:349] Evaluating on the test split.
I0308 22:02:38.550193 139902746892096 submission_runner.py:411] Time since start: 61281.58s, 	Step: 127073, 	{'train/accuracy': 0.8082616925239563, 'train/loss': 0.7514926791191101, 'validation/accuracy': 0.7242000102996826, 'validation/loss': 1.1179012060165405, 'validation/num_examples': 50000, 'test/accuracy': 0.5973000526428223, 'test/loss': 1.7537530660629272, 'test/num_examples': 10000, 'score': 56759.89336299896, 'total_duration': 61281.57898569107, 'accumulated_submission_time': 56759.89336299896, 'accumulated_eval_time': 4508.729408502579, 'accumulated_logging_time': 6.23290491104126}
I0308 22:02:38.584860 139708407461632 logging_writer.py:48] [127073] accumulated_eval_time=4508.729409, accumulated_logging_time=6.232905, accumulated_submission_time=56759.893363, global_step=127073, preemption_count=0, score=56759.893363, test/accuracy=0.597300, test/loss=1.753753, test/num_examples=10000, total_duration=61281.578986, train/accuracy=0.808262, train/loss=0.751493, validation/accuracy=0.724200, validation/loss=1.117901, validation/num_examples=50000
I0308 22:02:49.621847 139708415854336 logging_writer.py:48] [127100] global_step=127100, grad_norm=1.9476354122161865, loss=1.7786738872528076
I0308 22:03:31.120450 139708407461632 logging_writer.py:48] [127200] global_step=127200, grad_norm=1.7862213850021362, loss=1.619655728340149
I0308 22:04:16.152589 139708415854336 logging_writer.py:48] [127300] global_step=127300, grad_norm=1.790343165397644, loss=3.7587993144989014
I0308 22:05:01.558115 139708407461632 logging_writer.py:48] [127400] global_step=127400, grad_norm=2.2902560234069824, loss=4.033325672149658
I0308 22:05:46.895765 139708415854336 logging_writer.py:48] [127500] global_step=127500, grad_norm=1.6839369535446167, loss=2.844630479812622
I0308 22:06:32.388899 139708407461632 logging_writer.py:48] [127600] global_step=127600, grad_norm=1.879857063293457, loss=1.6800203323364258
I0308 22:07:17.753335 139708415854336 logging_writer.py:48] [127700] global_step=127700, grad_norm=2.1363000869750977, loss=1.6708180904388428
I0308 22:08:02.920123 139708407461632 logging_writer.py:48] [127800] global_step=127800, grad_norm=1.88425874710083, loss=3.9710350036621094
I0308 22:08:48.400722 139708415854336 logging_writer.py:48] [127900] global_step=127900, grad_norm=1.6923234462738037, loss=3.0614421367645264
I0308 22:09:33.843784 139708407461632 logging_writer.py:48] [128000] global_step=128000, grad_norm=1.9288241863250732, loss=1.6704449653625488
I0308 22:09:38.942253 139902746892096 spec.py:321] Evaluating on the training split.
I0308 22:09:50.241281 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 22:10:11.876724 139902746892096 spec.py:349] Evaluating on the test split.
I0308 22:10:13.555753 139902746892096 submission_runner.py:411] Time since start: 61736.58s, 	Step: 128013, 	{'train/accuracy': 0.7968358993530273, 'train/loss': 0.7883461713790894, 'validation/accuracy': 0.7251600027084351, 'validation/loss': 1.1023790836334229, 'validation/num_examples': 50000, 'test/accuracy': 0.6034000515937805, 'test/loss': 1.7308070659637451, 'test/num_examples': 10000, 'score': 57180.19188570976, 'total_duration': 61736.58455133438, 'accumulated_submission_time': 57180.19188570976, 'accumulated_eval_time': 4543.34293794632, 'accumulated_logging_time': 6.276918172836304}
I0308 22:10:13.592665 139708415854336 logging_writer.py:48] [128013] accumulated_eval_time=4543.342938, accumulated_logging_time=6.276918, accumulated_submission_time=57180.191886, global_step=128013, preemption_count=0, score=57180.191886, test/accuracy=0.603400, test/loss=1.730807, test/num_examples=10000, total_duration=61736.584551, train/accuracy=0.796836, train/loss=0.788346, validation/accuracy=0.725160, validation/loss=1.102379, validation/num_examples=50000
I0308 22:10:48.204775 139708407461632 logging_writer.py:48] [128100] global_step=128100, grad_norm=1.77974534034729, loss=1.9204111099243164
I0308 22:11:33.359721 139708415854336 logging_writer.py:48] [128200] global_step=128200, grad_norm=1.9189175367355347, loss=1.6122987270355225
I0308 22:12:18.777310 139708407461632 logging_writer.py:48] [128300] global_step=128300, grad_norm=1.9638413190841675, loss=1.6283138990402222
I0308 22:13:04.164573 139708415854336 logging_writer.py:48] [128400] global_step=128400, grad_norm=1.8742516040802002, loss=2.038463592529297
I0308 22:13:49.450554 139708407461632 logging_writer.py:48] [128500] global_step=128500, grad_norm=1.74833083152771, loss=2.7580366134643555
I0308 22:14:34.971960 139708415854336 logging_writer.py:48] [128600] global_step=128600, grad_norm=1.6854323148727417, loss=3.1311910152435303
I0308 22:15:19.935773 139708407461632 logging_writer.py:48] [128700] global_step=128700, grad_norm=1.8543870449066162, loss=2.557006597518921
I0308 22:16:05.253203 139708415854336 logging_writer.py:48] [128800] global_step=128800, grad_norm=1.918100118637085, loss=1.6267606019973755
I0308 22:16:50.999468 139708407461632 logging_writer.py:48] [128900] global_step=128900, grad_norm=2.177427291870117, loss=4.104668140411377
I0308 22:17:13.903824 139902746892096 spec.py:321] Evaluating on the training split.
I0308 22:17:25.640682 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 22:17:46.824448 139902746892096 spec.py:349] Evaluating on the test split.
I0308 22:17:48.501763 139902746892096 submission_runner.py:411] Time since start: 62191.53s, 	Step: 128952, 	{'train/accuracy': 0.8012109398841858, 'train/loss': 0.7621845602989197, 'validation/accuracy': 0.7287200093269348, 'validation/loss': 1.0881730318069458, 'validation/num_examples': 50000, 'test/accuracy': 0.6049000024795532, 'test/loss': 1.71925950050354, 'test/num_examples': 10000, 'score': 57600.443110466, 'total_duration': 62191.53056240082, 'accumulated_submission_time': 57600.443110466, 'accumulated_eval_time': 4577.940878629684, 'accumulated_logging_time': 6.324890613555908}
I0308 22:17:48.539053 139708415854336 logging_writer.py:48] [128952] accumulated_eval_time=4577.940879, accumulated_logging_time=6.324891, accumulated_submission_time=57600.443110, global_step=128952, preemption_count=0, score=57600.443110, test/accuracy=0.604900, test/loss=1.719260, test/num_examples=10000, total_duration=62191.530562, train/accuracy=0.801211, train/loss=0.762185, validation/accuracy=0.728720, validation/loss=1.088173, validation/num_examples=50000
I0308 22:18:07.814217 139708407461632 logging_writer.py:48] [129000] global_step=129000, grad_norm=2.5586414337158203, loss=4.040249347686768
I0308 22:18:50.342081 139708415854336 logging_writer.py:48] [129100] global_step=129100, grad_norm=1.9289631843566895, loss=1.4713554382324219
I0308 22:19:36.017788 139708407461632 logging_writer.py:48] [129200] global_step=129200, grad_norm=1.8633626699447632, loss=1.809956431388855
I0308 22:20:21.769942 139708415854336 logging_writer.py:48] [129300] global_step=129300, grad_norm=1.573051929473877, loss=2.477933645248413
I0308 22:21:06.951606 139708407461632 logging_writer.py:48] [129400] global_step=129400, grad_norm=1.6590715646743774, loss=2.584660530090332
I0308 22:21:52.197800 139708415854336 logging_writer.py:48] [129500] global_step=129500, grad_norm=2.1711056232452393, loss=1.5943604707717896
I0308 22:22:37.525887 139708407461632 logging_writer.py:48] [129600] global_step=129600, grad_norm=1.9532802104949951, loss=1.6400964260101318
I0308 22:23:22.852684 139708415854336 logging_writer.py:48] [129700] global_step=129700, grad_norm=1.6782077550888062, loss=1.9066076278686523
I0308 22:24:08.249136 139708407461632 logging_writer.py:48] [129800] global_step=129800, grad_norm=2.272268772125244, loss=3.9006757736206055
I0308 22:24:48.656643 139902746892096 spec.py:321] Evaluating on the training split.
I0308 22:25:00.226913 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 22:25:21.832060 139902746892096 spec.py:349] Evaluating on the test split.
I0308 22:25:23.512339 139902746892096 submission_runner.py:411] Time since start: 62646.54s, 	Step: 129891, 	{'train/accuracy': 0.8115820288658142, 'train/loss': 0.7281399965286255, 'validation/accuracy': 0.7297999858856201, 'validation/loss': 1.0883889198303223, 'validation/num_examples': 50000, 'test/accuracy': 0.6082000136375427, 'test/loss': 1.7195762395858765, 'test/num_examples': 10000, 'score': 58020.502247571945, 'total_duration': 62646.54111742973, 'accumulated_submission_time': 58020.502247571945, 'accumulated_eval_time': 4612.796590805054, 'accumulated_logging_time': 6.371863126754761}
I0308 22:25:23.553439 139708415854336 logging_writer.py:48] [129891] accumulated_eval_time=4612.796591, accumulated_logging_time=6.371863, accumulated_submission_time=58020.502248, global_step=129891, preemption_count=0, score=58020.502248, test/accuracy=0.608200, test/loss=1.719576, test/num_examples=10000, total_duration=62646.541117, train/accuracy=0.811582, train/loss=0.728140, validation/accuracy=0.729800, validation/loss=1.088389, validation/num_examples=50000
I0308 22:25:27.489283 139708407461632 logging_writer.py:48] [129900] global_step=129900, grad_norm=1.7464401721954346, loss=1.5776022672653198
I0308 22:26:08.308611 139708415854336 logging_writer.py:48] [130000] global_step=130000, grad_norm=2.007398843765259, loss=3.7155628204345703
I0308 22:26:53.339949 139708407461632 logging_writer.py:48] [130100] global_step=130100, grad_norm=1.7031069993972778, loss=3.0034375190734863
I0308 22:27:39.475387 139708415854336 logging_writer.py:48] [130200] global_step=130200, grad_norm=1.728566288948059, loss=1.8579432964324951
I0308 22:28:25.012403 139708407461632 logging_writer.py:48] [130300] global_step=130300, grad_norm=1.9847520589828491, loss=1.657285213470459
I0308 22:29:10.491769 139708415854336 logging_writer.py:48] [130400] global_step=130400, grad_norm=1.8507739305496216, loss=1.674966812133789
I0308 22:29:56.222298 139708407461632 logging_writer.py:48] [130500] global_step=130500, grad_norm=1.9368401765823364, loss=1.6013054847717285
I0308 22:30:41.478191 139708415854336 logging_writer.py:48] [130600] global_step=130600, grad_norm=1.6821606159210205, loss=2.78877854347229
I0308 22:31:26.823128 139708407461632 logging_writer.py:48] [130700] global_step=130700, grad_norm=1.8475890159606934, loss=1.6950790882110596
I0308 22:32:12.112689 139708415854336 logging_writer.py:48] [130800] global_step=130800, grad_norm=1.9083441495895386, loss=1.6197575330734253
I0308 22:32:23.930299 139902746892096 spec.py:321] Evaluating on the training split.
I0308 22:32:35.358906 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 22:32:58.488219 139902746892096 spec.py:349] Evaluating on the test split.
I0308 22:33:00.154119 139902746892096 submission_runner.py:411] Time since start: 63103.18s, 	Step: 130828, 	{'train/accuracy': 0.8115429282188416, 'train/loss': 0.721081018447876, 'validation/accuracy': 0.7346400022506714, 'validation/loss': 1.0667911767959595, 'validation/num_examples': 50000, 'test/accuracy': 0.6073000431060791, 'test/loss': 1.7005314826965332, 'test/num_examples': 10000, 'score': 58440.81789493561, 'total_duration': 63103.182911872864, 'accumulated_submission_time': 58440.81789493561, 'accumulated_eval_time': 4649.020402431488, 'accumulated_logging_time': 6.4250733852386475}
I0308 22:33:00.192770 139708407461632 logging_writer.py:48] [130828] accumulated_eval_time=4649.020402, accumulated_logging_time=6.425073, accumulated_submission_time=58440.817895, global_step=130828, preemption_count=0, score=58440.817895, test/accuracy=0.607300, test/loss=1.700531, test/num_examples=10000, total_duration=63103.182912, train/accuracy=0.811543, train/loss=0.721081, validation/accuracy=0.734640, validation/loss=1.066791, validation/num_examples=50000
I0308 22:33:28.933134 139708415854336 logging_writer.py:48] [130900] global_step=130900, grad_norm=2.08891224861145, loss=1.6177959442138672
I0308 22:34:12.554641 139708407461632 logging_writer.py:48] [131000] global_step=131000, grad_norm=1.8403900861740112, loss=3.6082205772399902
I0308 22:34:57.867724 139708415854336 logging_writer.py:48] [131100] global_step=131100, grad_norm=1.8832645416259766, loss=2.1848111152648926
I0308 22:35:43.543813 139708407461632 logging_writer.py:48] [131200] global_step=131200, grad_norm=2.0190954208374023, loss=1.6659384965896606
I0308 22:36:28.736498 139708415854336 logging_writer.py:48] [131300] global_step=131300, grad_norm=1.6961554288864136, loss=1.9011650085449219
I0308 22:37:14.295574 139708407461632 logging_writer.py:48] [131400] global_step=131400, grad_norm=2.253873109817505, loss=1.5993931293487549
I0308 22:37:59.703738 139708415854336 logging_writer.py:48] [131500] global_step=131500, grad_norm=2.0522890090942383, loss=1.7235468626022339
I0308 22:38:44.902217 139708407461632 logging_writer.py:48] [131600] global_step=131600, grad_norm=2.022303342819214, loss=1.7370083332061768
I0308 22:39:30.224961 139708415854336 logging_writer.py:48] [131700] global_step=131700, grad_norm=1.887250304222107, loss=1.6367498636245728
I0308 22:40:00.431010 139902746892096 spec.py:321] Evaluating on the training split.
I0308 22:40:11.703874 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 22:40:35.236127 139902746892096 spec.py:349] Evaluating on the test split.
I0308 22:40:36.900835 139902746892096 submission_runner.py:411] Time since start: 63559.93s, 	Step: 131768, 	{'train/accuracy': 0.8031835556030273, 'train/loss': 0.7564061284065247, 'validation/accuracy': 0.7330399751663208, 'validation/loss': 1.073096513748169, 'validation/num_examples': 50000, 'test/accuracy': 0.6079000234603882, 'test/loss': 1.708517074584961, 'test/num_examples': 10000, 'score': 58860.99866771698, 'total_duration': 63559.92962670326, 'accumulated_submission_time': 58860.99866771698, 'accumulated_eval_time': 4685.4902358055115, 'accumulated_logging_time': 6.4724133014678955}
I0308 22:40:36.935509 139708407461632 logging_writer.py:48] [131768] accumulated_eval_time=4685.490236, accumulated_logging_time=6.472413, accumulated_submission_time=58860.998668, global_step=131768, preemption_count=0, score=58860.998668, test/accuracy=0.607900, test/loss=1.708517, test/num_examples=10000, total_duration=63559.929627, train/accuracy=0.803184, train/loss=0.756406, validation/accuracy=0.733040, validation/loss=1.073097, validation/num_examples=50000
I0308 22:40:49.933415 139708415854336 logging_writer.py:48] [131800] global_step=131800, grad_norm=1.8159974813461304, loss=2.437272548675537
I0308 22:41:31.907156 139708407461632 logging_writer.py:48] [131900] global_step=131900, grad_norm=1.805046796798706, loss=2.5283823013305664
I0308 22:42:17.094290 139708415854336 logging_writer.py:48] [132000] global_step=132000, grad_norm=1.8242319822311401, loss=2.075638771057129
I0308 22:43:02.633412 139708407461632 logging_writer.py:48] [132100] global_step=132100, grad_norm=1.8009997606277466, loss=2.7569897174835205
I0308 22:43:47.741992 139708415854336 logging_writer.py:48] [132200] global_step=132200, grad_norm=1.9131075143814087, loss=1.7466856241226196
I0308 22:44:32.821726 139708407461632 logging_writer.py:48] [132300] global_step=132300, grad_norm=2.035374641418457, loss=1.5798897743225098
I0308 22:45:18.384280 139708415854336 logging_writer.py:48] [132400] global_step=132400, grad_norm=1.8759368658065796, loss=2.047340154647827
I0308 22:46:03.734893 139708407461632 logging_writer.py:48] [132500] global_step=132500, grad_norm=1.897810697555542, loss=2.9935715198516846
I0308 22:46:48.823730 139708415854336 logging_writer.py:48] [132600] global_step=132600, grad_norm=2.0029218196868896, loss=1.6064693927764893
I0308 22:47:35.003616 139708407461632 logging_writer.py:48] [132700] global_step=132700, grad_norm=2.24657940864563, loss=1.5694382190704346
I0308 22:47:37.002809 139902746892096 spec.py:321] Evaluating on the training split.
I0308 22:47:48.539982 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 22:48:11.530384 139902746892096 spec.py:349] Evaluating on the test split.
I0308 22:48:13.203521 139902746892096 submission_runner.py:411] Time since start: 64016.23s, 	Step: 132706, 	{'train/accuracy': 0.8125976324081421, 'train/loss': 0.7448723912239075, 'validation/accuracy': 0.7306399941444397, 'validation/loss': 1.090269684791565, 'validation/num_examples': 50000, 'test/accuracy': 0.6050000190734863, 'test/loss': 1.7248249053955078, 'test/num_examples': 10000, 'score': 59281.007247686386, 'total_duration': 64016.23230576515, 'accumulated_submission_time': 59281.007247686386, 'accumulated_eval_time': 4721.690928220749, 'accumulated_logging_time': 6.516724586486816}
I0308 22:48:13.238847 139708415854336 logging_writer.py:48] [132706] accumulated_eval_time=4721.690928, accumulated_logging_time=6.516725, accumulated_submission_time=59281.007248, global_step=132706, preemption_count=0, score=59281.007248, test/accuracy=0.605000, test/loss=1.724825, test/num_examples=10000, total_duration=64016.232306, train/accuracy=0.812598, train/loss=0.744872, validation/accuracy=0.730640, validation/loss=1.090270, validation/num_examples=50000
I0308 22:48:51.168250 139708407461632 logging_writer.py:48] [132800] global_step=132800, grad_norm=2.019634962081909, loss=1.680233120918274
I0308 22:49:36.229210 139708415854336 logging_writer.py:48] [132900] global_step=132900, grad_norm=1.7534196376800537, loss=2.1454033851623535
I0308 22:50:21.894991 139708407461632 logging_writer.py:48] [133000] global_step=133000, grad_norm=1.9777592420578003, loss=1.5729248523712158
I0308 22:51:07.318417 139708415854336 logging_writer.py:48] [133100] global_step=133100, grad_norm=1.8894695043563843, loss=1.4977238178253174
I0308 22:51:52.550211 139708407461632 logging_writer.py:48] [133200] global_step=133200, grad_norm=2.043822765350342, loss=1.5923935174942017
I0308 22:52:37.929839 139708415854336 logging_writer.py:48] [133300] global_step=133300, grad_norm=1.966433048248291, loss=2.0933210849761963
I0308 22:53:22.895125 139708407461632 logging_writer.py:48] [133400] global_step=133400, grad_norm=1.6394118070602417, loss=2.444506883621216
I0308 22:54:08.576375 139708415854336 logging_writer.py:48] [133500] global_step=133500, grad_norm=2.0739548206329346, loss=1.674872875213623
I0308 22:54:53.867485 139708407461632 logging_writer.py:48] [133600] global_step=133600, grad_norm=1.9168497323989868, loss=1.5422873497009277
I0308 22:55:13.583303 139902746892096 spec.py:321] Evaluating on the training split.
I0308 22:55:24.944178 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 22:55:46.989326 139902746892096 spec.py:349] Evaluating on the test split.
I0308 22:55:48.659074 139902746892096 submission_runner.py:411] Time since start: 64471.69s, 	Step: 133645, 	{'train/accuracy': 0.8241796493530273, 'train/loss': 0.6636848449707031, 'validation/accuracy': 0.7350800037384033, 'validation/loss': 1.0521515607833862, 'validation/num_examples': 50000, 'test/accuracy': 0.6089000105857849, 'test/loss': 1.680550456047058, 'test/num_examples': 10000, 'score': 59701.293223142624, 'total_duration': 64471.68787431717, 'accumulated_submission_time': 59701.293223142624, 'accumulated_eval_time': 4756.766691684723, 'accumulated_logging_time': 6.560953378677368}
I0308 22:55:48.694552 139708415854336 logging_writer.py:48] [133645] accumulated_eval_time=4756.766692, accumulated_logging_time=6.560953, accumulated_submission_time=59701.293223, global_step=133645, preemption_count=0, score=59701.293223, test/accuracy=0.608900, test/loss=1.680550, test/num_examples=10000, total_duration=64471.687874, train/accuracy=0.824180, train/loss=0.663685, validation/accuracy=0.735080, validation/loss=1.052152, validation/num_examples=50000
I0308 22:56:10.839166 139708407461632 logging_writer.py:48] [133700] global_step=133700, grad_norm=2.0948312282562256, loss=1.6643834114074707
I0308 22:56:53.745914 139708415854336 logging_writer.py:48] [133800] global_step=133800, grad_norm=2.0370664596557617, loss=1.5663219690322876
I0308 22:57:39.410623 139708407461632 logging_writer.py:48] [133900] global_step=133900, grad_norm=2.042376756668091, loss=1.9296038150787354
I0308 22:58:25.036812 139708415854336 logging_writer.py:48] [134000] global_step=134000, grad_norm=1.743790626525879, loss=2.261641502380371
I0308 22:59:10.228703 139708407461632 logging_writer.py:48] [134100] global_step=134100, grad_norm=2.002202272415161, loss=1.643447756767273
I0308 22:59:55.864528 139708415854336 logging_writer.py:48] [134200] global_step=134200, grad_norm=1.9621330499649048, loss=1.5842089653015137
I0308 23:00:41.067859 139708407461632 logging_writer.py:48] [134300] global_step=134300, grad_norm=1.9799808263778687, loss=3.818899631500244
I0308 23:01:26.322098 139708415854336 logging_writer.py:48] [134400] global_step=134400, grad_norm=2.10476016998291, loss=1.6999714374542236
I0308 23:02:11.617259 139708407461632 logging_writer.py:48] [134500] global_step=134500, grad_norm=1.800591230392456, loss=2.033797264099121
I0308 23:02:48.849587 139902746892096 spec.py:321] Evaluating on the training split.
I0308 23:03:00.295164 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 23:03:22.809397 139902746892096 spec.py:349] Evaluating on the test split.
I0308 23:03:24.488411 139902746892096 submission_runner.py:411] Time since start: 64927.52s, 	Step: 134584, 	{'train/accuracy': 0.8122656345367432, 'train/loss': 0.7251321077346802, 'validation/accuracy': 0.7372199892997742, 'validation/loss': 1.0536880493164062, 'validation/num_examples': 50000, 'test/accuracy': 0.6104000210762024, 'test/loss': 1.690237045288086, 'test/num_examples': 10000, 'score': 60121.38954138756, 'total_duration': 64927.517173051834, 'accumulated_submission_time': 60121.38954138756, 'accumulated_eval_time': 4792.405487298965, 'accumulated_logging_time': 6.606303691864014}
I0308 23:03:24.530386 139708415854336 logging_writer.py:48] [134584] accumulated_eval_time=4792.405487, accumulated_logging_time=6.606304, accumulated_submission_time=60121.389541, global_step=134584, preemption_count=0, score=60121.389541, test/accuracy=0.610400, test/loss=1.690237, test/num_examples=10000, total_duration=64927.517173, train/accuracy=0.812266, train/loss=0.725132, validation/accuracy=0.737220, validation/loss=1.053688, validation/num_examples=50000
I0308 23:03:31.219526 139708407461632 logging_writer.py:48] [134600] global_step=134600, grad_norm=1.993796706199646, loss=1.7207070589065552
I0308 23:04:11.934992 139708415854336 logging_writer.py:48] [134700] global_step=134700, grad_norm=1.8948352336883545, loss=3.336777448654175
I0308 23:04:56.945153 139708407461632 logging_writer.py:48] [134800] global_step=134800, grad_norm=1.9818885326385498, loss=2.103668689727783
I0308 23:05:42.519016 139708415854336 logging_writer.py:48] [134900] global_step=134900, grad_norm=1.755995750427246, loss=2.356816053390503
I0308 23:06:27.833545 139708407461632 logging_writer.py:48] [135000] global_step=135000, grad_norm=1.9415357112884521, loss=1.6501502990722656
I0308 23:07:13.033057 139708415854336 logging_writer.py:48] [135100] global_step=135100, grad_norm=1.841084599494934, loss=1.8229897022247314
I0308 23:08:00.010727 139708407461632 logging_writer.py:48] [135200] global_step=135200, grad_norm=2.3509397506713867, loss=3.811782121658325
I0308 23:08:45.950670 139708415854336 logging_writer.py:48] [135300] global_step=135300, grad_norm=2.044288396835327, loss=1.5171295404434204
I0308 23:09:31.650009 139708407461632 logging_writer.py:48] [135400] global_step=135400, grad_norm=2.0644943714141846, loss=1.5261932611465454
I0308 23:10:17.695482 139708415854336 logging_writer.py:48] [135500] global_step=135500, grad_norm=1.9988008737564087, loss=1.7874835729599
I0308 23:10:24.621493 139902746892096 spec.py:321] Evaluating on the training split.
I0308 23:10:36.199777 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 23:10:59.043217 139902746892096 spec.py:349] Evaluating on the test split.
I0308 23:11:00.713617 139902746892096 submission_runner.py:411] Time since start: 65383.74s, 	Step: 135517, 	{'train/accuracy': 0.8183202743530273, 'train/loss': 0.7013428807258606, 'validation/accuracy': 0.7394199967384338, 'validation/loss': 1.0442837476730347, 'validation/num_examples': 50000, 'test/accuracy': 0.6121000051498413, 'test/loss': 1.66946280002594, 'test/num_examples': 10000, 'score': 60541.42193317413, 'total_duration': 65383.74241781235, 'accumulated_submission_time': 60541.42193317413, 'accumulated_eval_time': 4828.497610330582, 'accumulated_logging_time': 6.658292770385742}
I0308 23:11:00.748077 139708407461632 logging_writer.py:48] [135517] accumulated_eval_time=4828.497610, accumulated_logging_time=6.658293, accumulated_submission_time=60541.421933, global_step=135517, preemption_count=0, score=60541.421933, test/accuracy=0.612100, test/loss=1.669463, test/num_examples=10000, total_duration=65383.742418, train/accuracy=0.818320, train/loss=0.701343, validation/accuracy=0.739420, validation/loss=1.044284, validation/num_examples=50000
I0308 23:11:33.805084 139708415854336 logging_writer.py:48] [135600] global_step=135600, grad_norm=1.9515330791473389, loss=2.64943265914917
I0308 23:12:18.839298 139708407461632 logging_writer.py:48] [135700] global_step=135700, grad_norm=1.841575264930725, loss=1.415562629699707
I0308 23:13:04.136632 139708415854336 logging_writer.py:48] [135800] global_step=135800, grad_norm=2.0720009803771973, loss=1.4729809761047363
I0308 23:13:49.474091 139708407461632 logging_writer.py:48] [135900] global_step=135900, grad_norm=2.0542309284210205, loss=2.8924975395202637
I0308 23:14:34.852843 139708415854336 logging_writer.py:48] [136000] global_step=136000, grad_norm=2.192239284515381, loss=1.6474215984344482
I0308 23:15:20.173132 139708407461632 logging_writer.py:48] [136100] global_step=136100, grad_norm=2.0967659950256348, loss=1.4931316375732422
I0308 23:16:05.930866 139708415854336 logging_writer.py:48] [136200] global_step=136200, grad_norm=2.047558546066284, loss=1.567338466644287
I0308 23:16:51.176774 139708407461632 logging_writer.py:48] [136300] global_step=136300, grad_norm=2.2347412109375, loss=2.076416015625
I0308 23:17:36.903308 139708415854336 logging_writer.py:48] [136400] global_step=136400, grad_norm=2.0098204612731934, loss=1.5403722524642944
I0308 23:18:00.743189 139902746892096 spec.py:321] Evaluating on the training split.
I0308 23:18:12.497520 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 23:18:35.251680 139902746892096 spec.py:349] Evaluating on the test split.
I0308 23:18:36.922411 139902746892096 submission_runner.py:411] Time since start: 65839.95s, 	Step: 136454, 	{'train/accuracy': 0.8281640410423279, 'train/loss': 0.6682937741279602, 'validation/accuracy': 0.7404800057411194, 'validation/loss': 1.0448720455169678, 'validation/num_examples': 50000, 'test/accuracy': 0.6099000573158264, 'test/loss': 1.6748216152191162, 'test/num_examples': 10000, 'score': 60961.35953044891, 'total_duration': 65839.95121264458, 'accumulated_submission_time': 60961.35953044891, 'accumulated_eval_time': 4864.676825284958, 'accumulated_logging_time': 6.702035903930664}
I0308 23:18:36.961189 139708407461632 logging_writer.py:48] [136454] accumulated_eval_time=4864.676825, accumulated_logging_time=6.702036, accumulated_submission_time=60961.359530, global_step=136454, preemption_count=0, score=60961.359530, test/accuracy=0.609900, test/loss=1.674822, test/num_examples=10000, total_duration=65839.951213, train/accuracy=0.828164, train/loss=0.668294, validation/accuracy=0.740480, validation/loss=1.044872, validation/num_examples=50000
I0308 23:18:55.471545 139708415854336 logging_writer.py:48] [136500] global_step=136500, grad_norm=2.201727867126465, loss=3.9612998962402344
I0308 23:19:38.052816 139708407461632 logging_writer.py:48] [136600] global_step=136600, grad_norm=2.0202977657318115, loss=1.4585776329040527
I0308 23:20:23.260137 139708415854336 logging_writer.py:48] [136700] global_step=136700, grad_norm=1.984904408454895, loss=1.4950593709945679
I0308 23:21:08.846293 139708407461632 logging_writer.py:48] [136800] global_step=136800, grad_norm=2.134042739868164, loss=1.5274662971496582
I0308 23:21:53.851924 139708415854336 logging_writer.py:48] [136900] global_step=136900, grad_norm=2.006186008453369, loss=3.057107448577881
I0308 23:22:39.166429 139708407461632 logging_writer.py:48] [137000] global_step=137000, grad_norm=2.2682464122772217, loss=1.5829997062683105
I0308 23:23:24.668905 139708415854336 logging_writer.py:48] [137100] global_step=137100, grad_norm=1.868098497390747, loss=1.9839928150177002
I0308 23:24:09.733221 139708407461632 logging_writer.py:48] [137200] global_step=137200, grad_norm=2.1271421909332275, loss=2.706982374191284
I0308 23:24:54.932204 139708415854336 logging_writer.py:48] [137300] global_step=137300, grad_norm=2.1917436122894287, loss=1.608903169631958
I0308 23:25:36.945538 139902746892096 spec.py:321] Evaluating on the training split.
I0308 23:25:48.472261 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 23:26:11.268770 139902746892096 spec.py:349] Evaluating on the test split.
I0308 23:26:12.935744 139902746892096 submission_runner.py:411] Time since start: 66295.96s, 	Step: 137394, 	{'train/accuracy': 0.8191210627555847, 'train/loss': 0.6811794638633728, 'validation/accuracy': 0.7382599711418152, 'validation/loss': 1.0337536334991455, 'validation/num_examples': 50000, 'test/accuracy': 0.6151000261306763, 'test/loss': 1.670432448387146, 'test/num_examples': 10000, 'score': 61381.284125328064, 'total_duration': 66295.96452879906, 'accumulated_submission_time': 61381.284125328064, 'accumulated_eval_time': 4900.667014360428, 'accumulated_logging_time': 6.751345872879028}
I0308 23:26:12.976670 139708407461632 logging_writer.py:48] [137394] accumulated_eval_time=4900.667014, accumulated_logging_time=6.751346, accumulated_submission_time=61381.284125, global_step=137394, preemption_count=0, score=61381.284125, test/accuracy=0.615100, test/loss=1.670432, test/num_examples=10000, total_duration=66295.964529, train/accuracy=0.819121, train/loss=0.681179, validation/accuracy=0.738260, validation/loss=1.033754, validation/num_examples=50000
I0308 23:26:15.724218 139708415854336 logging_writer.py:48] [137400] global_step=137400, grad_norm=2.0418012142181396, loss=1.4550753831863403
I0308 23:26:56.031342 139708407461632 logging_writer.py:48] [137500] global_step=137500, grad_norm=2.284538507461548, loss=1.9369895458221436
I0308 23:27:41.175851 139708415854336 logging_writer.py:48] [137600] global_step=137600, grad_norm=2.0060200691223145, loss=1.652254581451416
I0308 23:28:26.828620 139708407461632 logging_writer.py:48] [137700] global_step=137700, grad_norm=2.1262025833129883, loss=1.6204179525375366
I0308 23:29:12.270318 139708415854336 logging_writer.py:48] [137800] global_step=137800, grad_norm=2.0095748901367188, loss=2.5236077308654785
I0308 23:29:57.649918 139708407461632 logging_writer.py:48] [137900] global_step=137900, grad_norm=2.121375799179077, loss=1.5659657716751099
I0308 23:30:43.034956 139708415854336 logging_writer.py:48] [138000] global_step=138000, grad_norm=2.118962049484253, loss=1.6382474899291992
I0308 23:31:28.436179 139708407461632 logging_writer.py:48] [138100] global_step=138100, grad_norm=2.2006571292877197, loss=3.4582316875457764
I0308 23:32:13.656651 139708415854336 logging_writer.py:48] [138200] global_step=138200, grad_norm=1.9861462116241455, loss=1.6088794469833374
I0308 23:32:59.119742 139708407461632 logging_writer.py:48] [138300] global_step=138300, grad_norm=2.279694080352783, loss=1.5439587831497192
I0308 23:33:13.390899 139902746892096 spec.py:321] Evaluating on the training split.
I0308 23:33:24.949945 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 23:33:46.709851 139902746892096 spec.py:349] Evaluating on the test split.
I0308 23:33:48.390851 139902746892096 submission_runner.py:411] Time since start: 66751.42s, 	Step: 138333, 	{'train/accuracy': 0.8222265243530273, 'train/loss': 0.6762940287590027, 'validation/accuracy': 0.7418199777603149, 'validation/loss': 1.0292267799377441, 'validation/num_examples': 50000, 'test/accuracy': 0.6124000549316406, 'test/loss': 1.660211443901062, 'test/num_examples': 10000, 'score': 61801.64013171196, 'total_duration': 66751.41964411736, 'accumulated_submission_time': 61801.64013171196, 'accumulated_eval_time': 4935.666965246201, 'accumulated_logging_time': 6.801737308502197}
I0308 23:33:48.427534 139708415854336 logging_writer.py:48] [138333] accumulated_eval_time=4935.666965, accumulated_logging_time=6.801737, accumulated_submission_time=61801.640132, global_step=138333, preemption_count=0, score=61801.640132, test/accuracy=0.612400, test/loss=1.660211, test/num_examples=10000, total_duration=66751.419644, train/accuracy=0.822227, train/loss=0.676294, validation/accuracy=0.741820, validation/loss=1.029227, validation/num_examples=50000
I0308 23:34:15.191266 139708407461632 logging_writer.py:48] [138400] global_step=138400, grad_norm=2.196821928024292, loss=1.4580267667770386
I0308 23:34:59.143682 139708415854336 logging_writer.py:48] [138500] global_step=138500, grad_norm=2.1290383338928223, loss=1.468854546546936
I0308 23:35:44.159640 139708407461632 logging_writer.py:48] [138600] global_step=138600, grad_norm=2.268017530441284, loss=1.5293982028961182
I0308 23:36:30.036317 139708415854336 logging_writer.py:48] [138700] global_step=138700, grad_norm=1.9262346029281616, loss=2.4182381629943848
I0308 23:37:15.311562 139708407461632 logging_writer.py:48] [138800] global_step=138800, grad_norm=1.9310656785964966, loss=2.3953757286071777
I0308 23:38:00.722812 139708415854336 logging_writer.py:48] [138900] global_step=138900, grad_norm=2.1844635009765625, loss=1.697277545928955
I0308 23:38:46.022839 139708407461632 logging_writer.py:48] [139000] global_step=139000, grad_norm=2.160443067550659, loss=3.3401241302490234
I0308 23:39:31.123265 139708415854336 logging_writer.py:48] [139100] global_step=139100, grad_norm=2.1103427410125732, loss=3.254608631134033
I0308 23:40:16.332572 139708407461632 logging_writer.py:48] [139200] global_step=139200, grad_norm=2.1392054557800293, loss=1.5136690139770508
I0308 23:40:48.454321 139902746892096 spec.py:321] Evaluating on the training split.
I0308 23:40:59.929526 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 23:41:22.095869 139902746892096 spec.py:349] Evaluating on the test split.
I0308 23:41:23.785208 139902746892096 submission_runner.py:411] Time since start: 67206.81s, 	Step: 139273, 	{'train/accuracy': 0.8302929401397705, 'train/loss': 0.6423133015632629, 'validation/accuracy': 0.7434799671173096, 'validation/loss': 1.0174846649169922, 'validation/num_examples': 50000, 'test/accuracy': 0.6152000427246094, 'test/loss': 1.6491221189498901, 'test/num_examples': 10000, 'score': 62221.607963085175, 'total_duration': 67206.81400728226, 'accumulated_submission_time': 62221.607963085175, 'accumulated_eval_time': 4970.997862577438, 'accumulated_logging_time': 6.8474345207214355}
I0308 23:41:23.820343 139708415854336 logging_writer.py:48] [139273] accumulated_eval_time=4970.997863, accumulated_logging_time=6.847435, accumulated_submission_time=62221.607963, global_step=139273, preemption_count=0, score=62221.607963, test/accuracy=0.615200, test/loss=1.649122, test/num_examples=10000, total_duration=67206.814007, train/accuracy=0.830293, train/loss=0.642313, validation/accuracy=0.743480, validation/loss=1.017485, validation/num_examples=50000
I0308 23:41:34.840615 139708407461632 logging_writer.py:48] [139300] global_step=139300, grad_norm=1.9679741859436035, loss=1.455761432647705
I0308 23:42:16.982768 139708415854336 logging_writer.py:48] [139400] global_step=139400, grad_norm=2.1215851306915283, loss=2.1283421516418457
I0308 23:43:02.141263 139708407461632 logging_writer.py:48] [139500] global_step=139500, grad_norm=2.050832748413086, loss=1.4217506647109985
I0308 23:43:47.518667 139708415854336 logging_writer.py:48] [139600] global_step=139600, grad_norm=2.0631210803985596, loss=1.4543116092681885
I0308 23:44:32.631729 139708407461632 logging_writer.py:48] [139700] global_step=139700, grad_norm=2.0700321197509766, loss=1.7847795486450195
I0308 23:45:17.706263 139708415854336 logging_writer.py:48] [139800] global_step=139800, grad_norm=1.9478163719177246, loss=1.4885329008102417
I0308 23:46:02.942091 139708407461632 logging_writer.py:48] [139900] global_step=139900, grad_norm=2.13791561126709, loss=1.5481951236724854
I0308 23:46:48.297348 139708415854336 logging_writer.py:48] [140000] global_step=140000, grad_norm=2.2572073936462402, loss=1.5036436319351196
I0308 23:47:33.738449 139708407461632 logging_writer.py:48] [140100] global_step=140100, grad_norm=2.745466947555542, loss=3.872509241104126
I0308 23:48:19.634006 139708415854336 logging_writer.py:48] [140200] global_step=140200, grad_norm=2.431607484817505, loss=3.7881288528442383
I0308 23:48:23.882496 139902746892096 spec.py:321] Evaluating on the training split.
I0308 23:48:35.512286 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 23:48:58.192089 139902746892096 spec.py:349] Evaluating on the test split.
I0308 23:48:59.863789 139902746892096 submission_runner.py:411] Time since start: 67662.89s, 	Step: 140211, 	{'train/accuracy': 0.8381249904632568, 'train/loss': 0.6069045662879944, 'validation/accuracy': 0.7433399558067322, 'validation/loss': 1.016687273979187, 'validation/num_examples': 50000, 'test/accuracy': 0.6163000464439392, 'test/loss': 1.6549649238586426, 'test/num_examples': 10000, 'score': 62641.6125395298, 'total_duration': 67662.89258980751, 'accumulated_submission_time': 62641.6125395298, 'accumulated_eval_time': 5006.979150533676, 'accumulated_logging_time': 6.8912672996521}
I0308 23:48:59.899240 139708407461632 logging_writer.py:48] [140211] accumulated_eval_time=5006.979151, accumulated_logging_time=6.891267, accumulated_submission_time=62641.612540, global_step=140211, preemption_count=0, score=62641.612540, test/accuracy=0.616300, test/loss=1.654965, test/num_examples=10000, total_duration=67662.892590, train/accuracy=0.838125, train/loss=0.606905, validation/accuracy=0.743340, validation/loss=1.016687, validation/num_examples=50000
I0308 23:49:35.291352 139708415854336 logging_writer.py:48] [140300] global_step=140300, grad_norm=2.1395866870880127, loss=1.4430204629898071
I0308 23:50:20.652563 139708407461632 logging_writer.py:48] [140400] global_step=140400, grad_norm=2.0737624168395996, loss=1.4546018838882446
I0308 23:51:06.341727 139708415854336 logging_writer.py:48] [140500] global_step=140500, grad_norm=2.212946653366089, loss=3.4898881912231445
I0308 23:51:51.711453 139708407461632 logging_writer.py:48] [140600] global_step=140600, grad_norm=1.9009920358657837, loss=2.7913646697998047
I0308 23:52:37.613687 139708415854336 logging_writer.py:48] [140700] global_step=140700, grad_norm=2.09385347366333, loss=2.986778974533081
I0308 23:53:23.288670 139708407461632 logging_writer.py:48] [140800] global_step=140800, grad_norm=2.007753849029541, loss=1.7221232652664185
I0308 23:54:08.349980 139708415854336 logging_writer.py:48] [140900] global_step=140900, grad_norm=2.176591157913208, loss=1.7502473592758179
I0308 23:54:53.976683 139708407461632 logging_writer.py:48] [141000] global_step=141000, grad_norm=1.9652740955352783, loss=1.8886613845825195
I0308 23:55:39.490854 139708415854336 logging_writer.py:48] [141100] global_step=141100, grad_norm=1.964785099029541, loss=2.050931692123413
I0308 23:55:59.984318 139902746892096 spec.py:321] Evaluating on the training split.
I0308 23:56:11.300557 139902746892096 spec.py:333] Evaluating on the validation split.
I0308 23:56:31.103714 139902746892096 spec.py:349] Evaluating on the test split.
I0308 23:56:32.785021 139902746892096 submission_runner.py:411] Time since start: 68115.81s, 	Step: 141147, 	{'train/accuracy': 0.82630854845047, 'train/loss': 0.6645239591598511, 'validation/accuracy': 0.7433599829673767, 'validation/loss': 1.016500473022461, 'validation/num_examples': 50000, 'test/accuracy': 0.6189000010490417, 'test/loss': 1.642565369606018, 'test/num_examples': 10000, 'score': 63061.63824224472, 'total_duration': 68115.81379771233, 'accumulated_submission_time': 63061.63824224472, 'accumulated_eval_time': 5039.779830694199, 'accumulated_logging_time': 6.936065435409546}
I0308 23:56:32.830930 139708407461632 logging_writer.py:48] [141147] accumulated_eval_time=5039.779831, accumulated_logging_time=6.936065, accumulated_submission_time=63061.638242, global_step=141147, preemption_count=0, score=63061.638242, test/accuracy=0.618900, test/loss=1.642565, test/num_examples=10000, total_duration=68115.813798, train/accuracy=0.826309, train/loss=0.664524, validation/accuracy=0.743360, validation/loss=1.016500, validation/num_examples=50000
I0308 23:56:54.116416 139708415854336 logging_writer.py:48] [141200] global_step=141200, grad_norm=2.223233699798584, loss=2.5875661373138428
I0308 23:57:37.547696 139708407461632 logging_writer.py:48] [141300] global_step=141300, grad_norm=2.1104772090911865, loss=1.4336661100387573
I0308 23:58:22.832480 139708415854336 logging_writer.py:48] [141400] global_step=141400, grad_norm=2.452305555343628, loss=3.7393860816955566
I0308 23:59:08.155855 139708407461632 logging_writer.py:48] [141500] global_step=141500, grad_norm=2.174771308898926, loss=1.7784298658370972
I0308 23:59:53.197670 139708415854336 logging_writer.py:48] [141600] global_step=141600, grad_norm=2.255012273788452, loss=1.6275712251663208
I0309 00:00:38.907310 139708407461632 logging_writer.py:48] [141700] global_step=141700, grad_norm=2.164639949798584, loss=2.084764003753662
I0309 00:01:24.256939 139708415854336 logging_writer.py:48] [141800] global_step=141800, grad_norm=2.2207956314086914, loss=1.523101806640625
I0309 00:02:09.592484 139708407461632 logging_writer.py:48] [141900] global_step=141900, grad_norm=2.3969531059265137, loss=1.6858493089675903
I0309 00:02:54.849953 139708415854336 logging_writer.py:48] [142000] global_step=142000, grad_norm=2.362682580947876, loss=3.608273506164551
I0309 00:03:33.095147 139902746892096 spec.py:321] Evaluating on the training split.
I0309 00:03:44.548089 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 00:04:05.376345 139902746892096 spec.py:349] Evaluating on the test split.
I0309 00:04:07.056686 139902746892096 submission_runner.py:411] Time since start: 68570.09s, 	Step: 142086, 	{'train/accuracy': 0.8313671946525574, 'train/loss': 0.6447664499282837, 'validation/accuracy': 0.749019980430603, 'validation/loss': 1.0095096826553345, 'validation/num_examples': 50000, 'test/accuracy': 0.6199000477790833, 'test/loss': 1.6374740600585938, 'test/num_examples': 10000, 'score': 63481.839210510254, 'total_duration': 68570.085460186, 'accumulated_submission_time': 63481.839210510254, 'accumulated_eval_time': 5073.741352796555, 'accumulated_logging_time': 6.995683908462524}
I0309 00:04:07.096974 139708407461632 logging_writer.py:48] [142086] accumulated_eval_time=5073.741353, accumulated_logging_time=6.995684, accumulated_submission_time=63481.839211, global_step=142086, preemption_count=0, score=63481.839211, test/accuracy=0.619900, test/loss=1.637474, test/num_examples=10000, total_duration=68570.085460, train/accuracy=0.831367, train/loss=0.644766, validation/accuracy=0.749020, validation/loss=1.009510, validation/num_examples=50000
I0309 00:04:12.998948 139708415854336 logging_writer.py:48] [142100] global_step=142100, grad_norm=2.029078960418701, loss=2.6395275592803955
I0309 00:04:53.863995 139708407461632 logging_writer.py:48] [142200] global_step=142200, grad_norm=2.152048110961914, loss=1.6354256868362427
I0309 00:05:38.784868 139708415854336 logging_writer.py:48] [142300] global_step=142300, grad_norm=2.2282533645629883, loss=2.8891966342926025
I0309 00:06:24.137549 139708407461632 logging_writer.py:48] [142400] global_step=142400, grad_norm=2.010451316833496, loss=2.3395862579345703
I0309 00:07:09.828773 139708415854336 logging_writer.py:48] [142500] global_step=142500, grad_norm=2.0985381603240967, loss=1.387343168258667
I0309 00:07:55.140771 139708407461632 logging_writer.py:48] [142600] global_step=142600, grad_norm=2.181049108505249, loss=3.579916477203369
I0309 00:08:41.275984 139708415854336 logging_writer.py:48] [142700] global_step=142700, grad_norm=2.7850918769836426, loss=3.672769069671631
I0309 00:09:26.968337 139708407461632 logging_writer.py:48] [142800] global_step=142800, grad_norm=2.0949041843414307, loss=2.0741052627563477
I0309 00:10:12.536002 139708415854336 logging_writer.py:48] [142900] global_step=142900, grad_norm=1.962208867073059, loss=1.8492448329925537
I0309 00:10:57.982939 139708407461632 logging_writer.py:48] [143000] global_step=143000, grad_norm=2.2498252391815186, loss=2.9513437747955322
I0309 00:11:07.476365 139902746892096 spec.py:321] Evaluating on the training split.
I0309 00:11:18.731712 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 00:11:41.373472 139902746892096 spec.py:349] Evaluating on the test split.
I0309 00:11:43.045658 139902746892096 submission_runner.py:411] Time since start: 69026.07s, 	Step: 143023, 	{'train/accuracy': 0.8359569907188416, 'train/loss': 0.6201549172401428, 'validation/accuracy': 0.748699963092804, 'validation/loss': 0.999498724937439, 'validation/num_examples': 50000, 'test/accuracy': 0.6215000152587891, 'test/loss': 1.634261131286621, 'test/num_examples': 10000, 'score': 63902.15845608711, 'total_duration': 69026.07445836067, 'accumulated_submission_time': 63902.15845608711, 'accumulated_eval_time': 5109.310643911362, 'accumulated_logging_time': 7.047288179397583}
I0309 00:11:43.082342 139708415854336 logging_writer.py:48] [143023] accumulated_eval_time=5109.310644, accumulated_logging_time=7.047288, accumulated_submission_time=63902.158456, global_step=143023, preemption_count=0, score=63902.158456, test/accuracy=0.621500, test/loss=1.634261, test/num_examples=10000, total_duration=69026.074458, train/accuracy=0.835957, train/loss=0.620155, validation/accuracy=0.748700, validation/loss=0.999499, validation/num_examples=50000
I0309 00:12:13.769785 139708407461632 logging_writer.py:48] [143100] global_step=143100, grad_norm=2.240079402923584, loss=1.4536569118499756
I0309 00:12:58.170013 139708415854336 logging_writer.py:48] [143200] global_step=143200, grad_norm=2.1064136028289795, loss=2.998783588409424
I0309 00:13:43.858365 139708407461632 logging_writer.py:48] [143300] global_step=143300, grad_norm=2.333303213119507, loss=1.5058528184890747
I0309 00:14:29.207032 139708415854336 logging_writer.py:48] [143400] global_step=143400, grad_norm=2.223592519760132, loss=1.3690805435180664
I0309 00:15:14.515476 139708407461632 logging_writer.py:48] [143500] global_step=143500, grad_norm=2.250753879547119, loss=1.4354547262191772
I0309 00:16:00.022150 139708415854336 logging_writer.py:48] [143600] global_step=143600, grad_norm=2.182882308959961, loss=1.5129554271697998
I0309 00:16:45.558037 139708407461632 logging_writer.py:48] [143700] global_step=143700, grad_norm=2.1217470169067383, loss=1.3791649341583252
I0309 00:17:30.993028 139708415854336 logging_writer.py:48] [143800] global_step=143800, grad_norm=2.369535207748413, loss=1.4959042072296143
I0309 00:18:16.848605 139708407461632 logging_writer.py:48] [143900] global_step=143900, grad_norm=2.5320889949798584, loss=3.595550060272217
I0309 00:18:43.300120 139902746892096 spec.py:321] Evaluating on the training split.
I0309 00:18:54.881026 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 00:19:16.647021 139902746892096 spec.py:349] Evaluating on the test split.
I0309 00:19:18.316882 139902746892096 submission_runner.py:411] Time since start: 69481.35s, 	Step: 143960, 	{'train/accuracy': 0.8331835865974426, 'train/loss': 0.6416350603103638, 'validation/accuracy': 0.7472400069236755, 'validation/loss': 1.003479242324829, 'validation/num_examples': 50000, 'test/accuracy': 0.6253000497817993, 'test/loss': 1.6217032670974731, 'test/num_examples': 10000, 'score': 64322.318710803986, 'total_duration': 69481.3456788063, 'accumulated_submission_time': 64322.318710803986, 'accumulated_eval_time': 5144.327417612076, 'accumulated_logging_time': 7.09255576133728}
I0309 00:19:18.355635 139708415854336 logging_writer.py:48] [143960] accumulated_eval_time=5144.327418, accumulated_logging_time=7.092556, accumulated_submission_time=64322.318711, global_step=143960, preemption_count=0, score=64322.318711, test/accuracy=0.625300, test/loss=1.621703, test/num_examples=10000, total_duration=69481.345679, train/accuracy=0.833184, train/loss=0.641635, validation/accuracy=0.747240, validation/loss=1.003479, validation/num_examples=50000
I0309 00:19:34.490474 139708407461632 logging_writer.py:48] [144000] global_step=144000, grad_norm=2.137605667114258, loss=1.4331653118133545
I0309 00:20:16.728300 139708415854336 logging_writer.py:48] [144100] global_step=144100, grad_norm=2.1905250549316406, loss=1.3369801044464111
I0309 00:21:02.204160 139708407461632 logging_writer.py:48] [144200] global_step=144200, grad_norm=2.2603402137756348, loss=1.593385100364685
I0309 00:21:47.550601 139708415854336 logging_writer.py:48] [144300] global_step=144300, grad_norm=2.276263475418091, loss=3.550372362136841
I0309 00:22:32.709970 139708407461632 logging_writer.py:48] [144400] global_step=144400, grad_norm=2.185983180999756, loss=1.70639169216156
I0309 00:23:18.173191 139708415854336 logging_writer.py:48] [144500] global_step=144500, grad_norm=2.224522113800049, loss=1.2797411680221558
I0309 00:24:03.372409 139708407461632 logging_writer.py:48] [144600] global_step=144600, grad_norm=2.147644281387329, loss=1.6352739334106445
I0309 00:24:48.522682 139708415854336 logging_writer.py:48] [144700] global_step=144700, grad_norm=2.1478846073150635, loss=2.464102029800415
I0309 00:25:34.022599 139708407461632 logging_writer.py:48] [144800] global_step=144800, grad_norm=2.123380184173584, loss=1.585687279701233
I0309 00:26:19.031457 139708415854336 logging_writer.py:48] [144900] global_step=144900, grad_norm=2.0567703247070312, loss=1.306649923324585
I0309 00:26:19.048798 139902746892096 spec.py:321] Evaluating on the training split.
I0309 00:26:30.456794 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 00:26:51.725415 139902746892096 spec.py:349] Evaluating on the test split.
I0309 00:26:53.406291 139902746892096 submission_runner.py:411] Time since start: 69936.44s, 	Step: 144901, 	{'train/accuracy': 0.8355273008346558, 'train/loss': 0.6195220947265625, 'validation/accuracy': 0.750059962272644, 'validation/loss': 0.98756343126297, 'validation/num_examples': 50000, 'test/accuracy': 0.629300057888031, 'test/loss': 1.608303427696228, 'test/num_examples': 10000, 'score': 64742.95211935043, 'total_duration': 69936.43508267403, 'accumulated_submission_time': 64742.95211935043, 'accumulated_eval_time': 5178.6849110126495, 'accumulated_logging_time': 7.1408984661102295}
I0309 00:26:53.445219 139708407461632 logging_writer.py:48] [144901] accumulated_eval_time=5178.684911, accumulated_logging_time=7.140898, accumulated_submission_time=64742.952119, global_step=144901, preemption_count=0, score=64742.952119, test/accuracy=0.629300, test/loss=1.608303, test/num_examples=10000, total_duration=69936.435083, train/accuracy=0.835527, train/loss=0.619522, validation/accuracy=0.750060, validation/loss=0.987563, validation/num_examples=50000
I0309 00:27:33.643609 139708415854336 logging_writer.py:48] [145000] global_step=145000, grad_norm=2.2359237670898438, loss=1.4308832883834839
I0309 00:28:18.983122 139708407461632 logging_writer.py:48] [145100] global_step=145100, grad_norm=2.85844087600708, loss=3.770211935043335
I0309 00:29:04.844923 139708415854336 logging_writer.py:48] [145200] global_step=145200, grad_norm=2.154026508331299, loss=2.5938940048217773
I0309 00:29:50.407507 139708407461632 logging_writer.py:48] [145300] global_step=145300, grad_norm=1.974070429801941, loss=1.6328694820404053
I0309 00:30:35.810095 139708415854336 logging_writer.py:48] [145400] global_step=145400, grad_norm=2.353877067565918, loss=1.4487279653549194
I0309 00:31:21.219593 139708407461632 logging_writer.py:48] [145500] global_step=145500, grad_norm=2.319758653640747, loss=1.480491042137146
I0309 00:32:06.650131 139708415854336 logging_writer.py:48] [145600] global_step=145600, grad_norm=2.3483896255493164, loss=1.688279390335083
I0309 00:32:52.022788 139708407461632 logging_writer.py:48] [145700] global_step=145700, grad_norm=2.1656646728515625, loss=3.2378485202789307
I0309 00:33:37.467540 139708415854336 logging_writer.py:48] [145800] global_step=145800, grad_norm=2.082871913909912, loss=1.3666898012161255
I0309 00:33:53.483990 139902746892096 spec.py:321] Evaluating on the training split.
I0309 00:34:04.778330 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 00:34:24.633553 139902746892096 spec.py:349] Evaluating on the test split.
I0309 00:34:26.317598 139902746892096 submission_runner.py:411] Time since start: 70389.35s, 	Step: 145837, 	{'train/accuracy': 0.8406445384025574, 'train/loss': 0.5987057685852051, 'validation/accuracy': 0.752240002155304, 'validation/loss': 0.9832723736763, 'validation/num_examples': 50000, 'test/accuracy': 0.6294000148773193, 'test/loss': 1.6080838441848755, 'test/num_examples': 10000, 'score': 65162.93309760094, 'total_duration': 70389.34637379646, 'accumulated_submission_time': 65162.93309760094, 'accumulated_eval_time': 5211.518479347229, 'accumulated_logging_time': 7.18864107131958}
I0309 00:34:26.363808 139708407461632 logging_writer.py:48] [145837] accumulated_eval_time=5211.518479, accumulated_logging_time=7.188641, accumulated_submission_time=65162.933098, global_step=145837, preemption_count=0, score=65162.933098, test/accuracy=0.629400, test/loss=1.608084, test/num_examples=10000, total_duration=70389.346374, train/accuracy=0.840645, train/loss=0.598706, validation/accuracy=0.752240, validation/loss=0.983272, validation/num_examples=50000
I0309 00:34:51.565746 139708415854336 logging_writer.py:48] [145900] global_step=145900, grad_norm=2.129373788833618, loss=2.026038408279419
I0309 00:35:35.454334 139708407461632 logging_writer.py:48] [146000] global_step=146000, grad_norm=2.523953676223755, loss=3.5658907890319824
I0309 00:36:20.676566 139708415854336 logging_writer.py:48] [146100] global_step=146100, grad_norm=2.1238949298858643, loss=1.3520896434783936
I0309 00:37:06.114612 139708407461632 logging_writer.py:48] [146200] global_step=146200, grad_norm=2.0412745475769043, loss=1.6402859687805176
I0309 00:37:51.438892 139708415854336 logging_writer.py:48] [146300] global_step=146300, grad_norm=2.312067985534668, loss=1.3933817148208618
I0309 00:38:37.100409 139708407461632 logging_writer.py:48] [146400] global_step=146400, grad_norm=2.2356274127960205, loss=1.4195252656936646
I0309 00:39:22.681388 139708415854336 logging_writer.py:48] [146500] global_step=146500, grad_norm=2.7984843254089355, loss=3.7290892601013184
I0309 00:40:07.776374 139708407461632 logging_writer.py:48] [146600] global_step=146600, grad_norm=2.209069013595581, loss=2.695072650909424
I0309 00:40:53.305616 139708415854336 logging_writer.py:48] [146700] global_step=146700, grad_norm=2.339979410171509, loss=1.4192315340042114
I0309 00:41:26.621455 139902746892096 spec.py:321] Evaluating on the training split.
I0309 00:41:38.051939 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 00:42:00.728920 139902746892096 spec.py:349] Evaluating on the test split.
I0309 00:42:02.392401 139902746892096 submission_runner.py:411] Time since start: 70845.42s, 	Step: 146775, 	{'train/accuracy': 0.8557812571525574, 'train/loss': 0.5449709296226501, 'validation/accuracy': 0.7529000043869019, 'validation/loss': 0.9782390594482422, 'validation/num_examples': 50000, 'test/accuracy': 0.6300000548362732, 'test/loss': 1.6063361167907715, 'test/num_examples': 10000, 'score': 65583.1298816204, 'total_duration': 70845.42119407654, 'accumulated_submission_time': 65583.1298816204, 'accumulated_eval_time': 5247.289430618286, 'accumulated_logging_time': 7.246530294418335}
I0309 00:42:02.433702 139708407461632 logging_writer.py:48] [146775] accumulated_eval_time=5247.289431, accumulated_logging_time=7.246530, accumulated_submission_time=65583.129882, global_step=146775, preemption_count=0, score=65583.129882, test/accuracy=0.630000, test/loss=1.606336, test/num_examples=10000, total_duration=70845.421194, train/accuracy=0.855781, train/loss=0.544971, validation/accuracy=0.752900, validation/loss=0.978239, validation/num_examples=50000
I0309 00:42:12.662390 139708415854336 logging_writer.py:48] [146800] global_step=146800, grad_norm=2.0663645267486572, loss=2.498684883117676
I0309 00:42:53.754015 139708407461632 logging_writer.py:48] [146900] global_step=146900, grad_norm=2.304539442062378, loss=1.6790403127670288
I0309 00:43:39.213765 139708415854336 logging_writer.py:48] [147000] global_step=147000, grad_norm=2.16078519821167, loss=1.4729113578796387
I0309 00:44:25.014932 139708407461632 logging_writer.py:48] [147100] global_step=147100, grad_norm=2.344064950942993, loss=1.4667069911956787
I0309 00:45:10.277486 139708415854336 logging_writer.py:48] [147200] global_step=147200, grad_norm=2.4751195907592773, loss=1.3972282409667969
I0309 00:45:55.535147 139708407461632 logging_writer.py:48] [147300] global_step=147300, grad_norm=2.269648551940918, loss=1.464630365371704
I0309 00:46:41.060026 139708415854336 logging_writer.py:48] [147400] global_step=147400, grad_norm=1.923649549484253, loss=2.5391345024108887
I0309 00:47:26.250371 139708407461632 logging_writer.py:48] [147500] global_step=147500, grad_norm=2.215639591217041, loss=2.1266093254089355
I0309 00:48:11.763551 139708415854336 logging_writer.py:48] [147600] global_step=147600, grad_norm=2.3439273834228516, loss=1.3592339754104614
I0309 00:48:57.222303 139708407461632 logging_writer.py:48] [147700] global_step=147700, grad_norm=2.4368762969970703, loss=2.511598587036133
I0309 00:49:02.787702 139902746892096 spec.py:321] Evaluating on the training split.
I0309 00:49:14.165073 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 00:49:36.810747 139902746892096 spec.py:349] Evaluating on the test split.
I0309 00:49:38.480969 139902746892096 submission_runner.py:411] Time since start: 71301.51s, 	Step: 147714, 	{'train/accuracy': 0.8352148532867432, 'train/loss': 0.6282013654708862, 'validation/accuracy': 0.7502999901771545, 'validation/loss': 0.9910974502563477, 'validation/num_examples': 50000, 'test/accuracy': 0.6282000541687012, 'test/loss': 1.6120684146881104, 'test/num_examples': 10000, 'score': 66003.42449641228, 'total_duration': 71301.50976467133, 'accumulated_submission_time': 66003.42449641228, 'accumulated_eval_time': 5282.982690811157, 'accumulated_logging_time': 7.298294305801392}
I0309 00:49:38.516869 139708415854336 logging_writer.py:48] [147714] accumulated_eval_time=5282.982691, accumulated_logging_time=7.298294, accumulated_submission_time=66003.424496, global_step=147714, preemption_count=0, score=66003.424496, test/accuracy=0.628200, test/loss=1.612068, test/num_examples=10000, total_duration=71301.509765, train/accuracy=0.835215, train/loss=0.628201, validation/accuracy=0.750300, validation/loss=0.991097, validation/num_examples=50000
I0309 00:50:12.750507 139708407461632 logging_writer.py:48] [147800] global_step=147800, grad_norm=2.4026005268096924, loss=3.304682731628418
I0309 00:50:57.519399 139708415854336 logging_writer.py:48] [147900] global_step=147900, grad_norm=2.2946648597717285, loss=1.6036769151687622
I0309 00:51:42.767995 139708407461632 logging_writer.py:48] [148000] global_step=148000, grad_norm=2.0934157371520996, loss=2.3299412727355957
I0309 00:52:28.077633 139708415854336 logging_writer.py:48] [148100] global_step=148100, grad_norm=2.5555856227874756, loss=3.65683913230896
I0309 00:53:13.238477 139708407461632 logging_writer.py:48] [148200] global_step=148200, grad_norm=2.3387515544891357, loss=1.4517993927001953
I0309 00:53:58.365840 139708415854336 logging_writer.py:48] [148300] global_step=148300, grad_norm=2.38703989982605, loss=2.1665072441101074
I0309 00:54:43.589039 139708407461632 logging_writer.py:48] [148400] global_step=148400, grad_norm=2.3342859745025635, loss=1.5429184436798096
I0309 00:55:29.165095 139708415854336 logging_writer.py:48] [148500] global_step=148500, grad_norm=2.086575984954834, loss=2.1810195446014404
I0309 00:56:14.344632 139708407461632 logging_writer.py:48] [148600] global_step=148600, grad_norm=2.325914144515991, loss=3.0103654861450195
I0309 00:56:38.573147 139902746892096 spec.py:321] Evaluating on the training split.
I0309 00:56:49.886920 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 00:57:12.456140 139902746892096 spec.py:349] Evaluating on the test split.
I0309 00:57:14.131729 139902746892096 submission_runner.py:411] Time since start: 71757.16s, 	Step: 148655, 	{'train/accuracy': 0.8431054353713989, 'train/loss': 0.6112679243087769, 'validation/accuracy': 0.7550599575042725, 'validation/loss': 0.9850051999092102, 'validation/num_examples': 50000, 'test/accuracy': 0.629300057888031, 'test/loss': 1.6120909452438354, 'test/num_examples': 10000, 'score': 66423.42095088959, 'total_duration': 71757.16052174568, 'accumulated_submission_time': 66423.42095088959, 'accumulated_eval_time': 5318.54127407074, 'accumulated_logging_time': 7.344376087188721}
I0309 00:57:14.168976 139708415854336 logging_writer.py:48] [148655] accumulated_eval_time=5318.541274, accumulated_logging_time=7.344376, accumulated_submission_time=66423.420951, global_step=148655, preemption_count=0, score=66423.420951, test/accuracy=0.629300, test/loss=1.612091, test/num_examples=10000, total_duration=71757.160522, train/accuracy=0.843105, train/loss=0.611268, validation/accuracy=0.755060, validation/loss=0.985005, validation/num_examples=50000
I0309 00:57:32.287800 139708407461632 logging_writer.py:48] [148700] global_step=148700, grad_norm=2.0983376502990723, loss=2.461374282836914
I0309 00:58:15.169551 139708415854336 logging_writer.py:48] [148800] global_step=148800, grad_norm=2.306380033493042, loss=1.4102245569229126
I0309 00:59:00.805631 139708407461632 logging_writer.py:48] [148900] global_step=148900, grad_norm=2.3573732376098633, loss=1.3680670261383057
I0309 00:59:47.326195 139708415854336 logging_writer.py:48] [149000] global_step=149000, grad_norm=2.543766975402832, loss=1.3825101852416992
I0309 01:00:32.708529 139708407461632 logging_writer.py:48] [149100] global_step=149100, grad_norm=2.461679697036743, loss=1.4640527963638306
I0309 01:01:18.035367 139708415854336 logging_writer.py:48] [149200] global_step=149200, grad_norm=2.286561965942383, loss=1.3483731746673584
I0309 01:02:03.641980 139708407461632 logging_writer.py:48] [149300] global_step=149300, grad_norm=2.479463815689087, loss=3.0835132598876953
I0309 01:02:48.854111 139708415854336 logging_writer.py:48] [149400] global_step=149400, grad_norm=2.363605260848999, loss=1.2890853881835938
I0309 01:03:34.130918 139708407461632 logging_writer.py:48] [149500] global_step=149500, grad_norm=2.3014297485351562, loss=1.3842397928237915
I0309 01:04:14.268192 139902746892096 spec.py:321] Evaluating on the training split.
I0309 01:04:25.736447 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 01:04:47.209467 139902746892096 spec.py:349] Evaluating on the test split.
I0309 01:04:48.888241 139902746892096 submission_runner.py:411] Time since start: 72211.92s, 	Step: 149590, 	{'train/accuracy': 0.8514257669448853, 'train/loss': 0.5645542740821838, 'validation/accuracy': 0.7563799619674683, 'validation/loss': 0.9604097604751587, 'validation/num_examples': 50000, 'test/accuracy': 0.631600022315979, 'test/loss': 1.5866049528121948, 'test/num_examples': 10000, 'score': 66843.46200037003, 'total_duration': 72211.91704010963, 'accumulated_submission_time': 66843.46200037003, 'accumulated_eval_time': 5353.161323547363, 'accumulated_logging_time': 7.391125440597534}
I0309 01:04:48.925674 139708415854336 logging_writer.py:48] [149590] accumulated_eval_time=5353.161324, accumulated_logging_time=7.391125, accumulated_submission_time=66843.462000, global_step=149590, preemption_count=0, score=66843.462000, test/accuracy=0.631600, test/loss=1.586605, test/num_examples=10000, total_duration=72211.917040, train/accuracy=0.851426, train/loss=0.564554, validation/accuracy=0.756380, validation/loss=0.960410, validation/num_examples=50000
I0309 01:04:53.263246 139708407461632 logging_writer.py:48] [149600] global_step=149600, grad_norm=2.32863712310791, loss=1.538580060005188
I0309 01:05:33.958874 139708415854336 logging_writer.py:48] [149700] global_step=149700, grad_norm=2.293607234954834, loss=1.2982264757156372
I0309 01:06:19.130116 139708407461632 logging_writer.py:48] [149800] global_step=149800, grad_norm=2.6514031887054443, loss=1.3326877355575562
I0309 01:07:04.541605 139708415854336 logging_writer.py:48] [149900] global_step=149900, grad_norm=2.25101637840271, loss=1.2964611053466797
I0309 01:07:50.148651 139708407461632 logging_writer.py:48] [150000] global_step=150000, grad_norm=2.322585344314575, loss=1.3453419208526611
I0309 01:08:35.470991 139708415854336 logging_writer.py:48] [150100] global_step=150100, grad_norm=2.278111696243286, loss=1.4344539642333984
I0309 01:09:21.356786 139708407461632 logging_writer.py:48] [150200] global_step=150200, grad_norm=2.6454288959503174, loss=1.5818793773651123
I0309 01:10:06.758819 139708415854336 logging_writer.py:48] [150300] global_step=150300, grad_norm=3.0456502437591553, loss=3.7145936489105225
I0309 01:10:52.117108 139708407461632 logging_writer.py:48] [150400] global_step=150400, grad_norm=2.452312469482422, loss=3.0118303298950195
I0309 01:11:37.620498 139708415854336 logging_writer.py:48] [150500] global_step=150500, grad_norm=2.213585615158081, loss=2.0227060317993164
I0309 01:11:49.056257 139902746892096 spec.py:321] Evaluating on the training split.
I0309 01:12:00.722629 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 01:12:24.472647 139902746892096 spec.py:349] Evaluating on the test split.
I0309 01:12:26.141632 139902746892096 submission_runner.py:411] Time since start: 72669.17s, 	Step: 150527, 	{'train/accuracy': 0.8474218845367432, 'train/loss': 0.5692348480224609, 'validation/accuracy': 0.7574599981307983, 'validation/loss': 0.9543402791023254, 'validation/num_examples': 50000, 'test/accuracy': 0.6347000598907471, 'test/loss': 1.5730087757110596, 'test/num_examples': 10000, 'score': 67263.53252577782, 'total_duration': 72669.17042684555, 'accumulated_submission_time': 67263.53252577782, 'accumulated_eval_time': 5390.246699333191, 'accumulated_logging_time': 7.440268039703369}
I0309 01:12:26.178132 139708407461632 logging_writer.py:48] [150527] accumulated_eval_time=5390.246699, accumulated_logging_time=7.440268, accumulated_submission_time=67263.532526, global_step=150527, preemption_count=0, score=67263.532526, test/accuracy=0.634700, test/loss=1.573009, test/num_examples=10000, total_duration=72669.170427, train/accuracy=0.847422, train/loss=0.569235, validation/accuracy=0.757460, validation/loss=0.954340, validation/num_examples=50000
I0309 01:12:55.287355 139708415854336 logging_writer.py:48] [150600] global_step=150600, grad_norm=2.1306822299957275, loss=2.69376540184021
I0309 01:13:39.415247 139708407461632 logging_writer.py:48] [150700] global_step=150700, grad_norm=2.2460410594940186, loss=2.4482717514038086
I0309 01:14:24.867383 139708415854336 logging_writer.py:48] [150800] global_step=150800, grad_norm=2.320783853530884, loss=1.4776992797851562
I0309 01:15:10.723295 139708407461632 logging_writer.py:48] [150900] global_step=150900, grad_norm=2.2666923999786377, loss=1.479392170906067
I0309 01:15:55.952783 139708415854336 logging_writer.py:48] [151000] global_step=151000, grad_norm=2.1715247631073, loss=1.7887821197509766
I0309 01:16:41.076013 139708407461632 logging_writer.py:48] [151100] global_step=151100, grad_norm=2.422959089279175, loss=1.3660845756530762
I0309 01:17:26.454484 139708415854336 logging_writer.py:48] [151200] global_step=151200, grad_norm=2.3488121032714844, loss=1.3710625171661377
I0309 01:18:12.009991 139708407461632 logging_writer.py:48] [151300] global_step=151300, grad_norm=2.5326647758483887, loss=2.8604636192321777
I0309 01:18:57.795531 139708415854336 logging_writer.py:48] [151400] global_step=151400, grad_norm=2.396033525466919, loss=1.2391895055770874
I0309 01:19:26.261491 139902746892096 spec.py:321] Evaluating on the training split.
I0309 01:19:37.721533 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 01:20:00.384214 139902746892096 spec.py:349] Evaluating on the test split.
I0309 01:20:02.069909 139902746892096 submission_runner.py:411] Time since start: 73125.10s, 	Step: 151464, 	{'train/accuracy': 0.8489453196525574, 'train/loss': 0.5678457021713257, 'validation/accuracy': 0.757099986076355, 'validation/loss': 0.9635516405105591, 'validation/num_examples': 50000, 'test/accuracy': 0.6391000151634216, 'test/loss': 1.577191710472107, 'test/num_examples': 10000, 'score': 67683.55815124512, 'total_duration': 73125.09868621826, 'accumulated_submission_time': 67683.55815124512, 'accumulated_eval_time': 5426.055099248886, 'accumulated_logging_time': 7.485986948013306}
I0309 01:20:02.116350 139708407461632 logging_writer.py:48] [151464] accumulated_eval_time=5426.055099, accumulated_logging_time=7.485987, accumulated_submission_time=67683.558151, global_step=151464, preemption_count=0, score=67683.558151, test/accuracy=0.639100, test/loss=1.577192, test/num_examples=10000, total_duration=73125.098686, train/accuracy=0.848945, train/loss=0.567846, validation/accuracy=0.757100, validation/loss=0.963552, validation/num_examples=50000
I0309 01:20:16.676670 139708415854336 logging_writer.py:48] [151500] global_step=151500, grad_norm=2.9542081356048584, loss=3.6838748455047607
I0309 01:20:58.836042 139708407461632 logging_writer.py:48] [151600] global_step=151600, grad_norm=2.409640073776245, loss=1.4853609800338745
I0309 01:21:44.381897 139708415854336 logging_writer.py:48] [151700] global_step=151700, grad_norm=2.79398512840271, loss=3.582671642303467
I0309 01:22:30.213085 139708407461632 logging_writer.py:48] [151800] global_step=151800, grad_norm=2.4667232036590576, loss=3.0248732566833496
I0309 01:23:15.567691 139708415854336 logging_writer.py:48] [151900] global_step=151900, grad_norm=2.4420979022979736, loss=1.6909958124160767
I0309 01:24:00.917238 139708407461632 logging_writer.py:48] [152000] global_step=152000, grad_norm=2.1775784492492676, loss=2.265436887741089
I0309 01:24:46.613216 139708415854336 logging_writer.py:48] [152100] global_step=152100, grad_norm=2.315919876098633, loss=1.3863892555236816
I0309 01:25:31.993395 139708407461632 logging_writer.py:48] [152200] global_step=152200, grad_norm=2.6102681159973145, loss=1.4446439743041992
I0309 01:26:17.326174 139708415854336 logging_writer.py:48] [152300] global_step=152300, grad_norm=2.7905046939849854, loss=3.608356475830078
I0309 01:27:02.712720 139708407461632 logging_writer.py:48] [152400] global_step=152400, grad_norm=2.4013020992279053, loss=1.8478344678878784
I0309 01:27:02.728771 139902746892096 spec.py:321] Evaluating on the training split.
I0309 01:27:14.290262 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 01:27:34.975562 139902746892096 spec.py:349] Evaluating on the test split.
I0309 01:27:36.665605 139902746892096 submission_runner.py:411] Time since start: 73579.69s, 	Step: 152401, 	{'train/accuracy': 0.8550195097923279, 'train/loss': 0.5494584441184998, 'validation/accuracy': 0.7616399526596069, 'validation/loss': 0.9430665373802185, 'validation/num_examples': 50000, 'test/accuracy': 0.641800045967102, 'test/loss': 1.5591038465499878, 'test/num_examples': 10000, 'score': 68104.11196637154, 'total_duration': 73579.6944000721, 'accumulated_submission_time': 68104.11196637154, 'accumulated_eval_time': 5459.9919238090515, 'accumulated_logging_time': 7.542108774185181}
I0309 01:27:36.708359 139708415854336 logging_writer.py:48] [152401] accumulated_eval_time=5459.991924, accumulated_logging_time=7.542109, accumulated_submission_time=68104.111966, global_step=152401, preemption_count=0, score=68104.111966, test/accuracy=0.641800, test/loss=1.559104, test/num_examples=10000, total_duration=73579.694400, train/accuracy=0.855020, train/loss=0.549458, validation/accuracy=0.761640, validation/loss=0.943067, validation/num_examples=50000
I0309 01:28:17.095350 139708407461632 logging_writer.py:48] [152500] global_step=152500, grad_norm=3.2094810009002686, loss=3.577368974685669
I0309 01:29:01.942248 139708415854336 logging_writer.py:48] [152600] global_step=152600, grad_norm=2.4163730144500732, loss=1.31251859664917
I0309 01:29:47.538799 139708407461632 logging_writer.py:48] [152700] global_step=152700, grad_norm=2.354290008544922, loss=1.295752763748169
I0309 01:30:33.094463 139708415854336 logging_writer.py:48] [152800] global_step=152800, grad_norm=2.3962156772613525, loss=1.359775185585022
I0309 01:31:18.270812 139708407461632 logging_writer.py:48] [152900] global_step=152900, grad_norm=2.411585807800293, loss=2.3048431873321533
I0309 01:32:03.635755 139708415854336 logging_writer.py:48] [153000] global_step=153000, grad_norm=2.3544905185699463, loss=1.2515448331832886
I0309 01:32:49.232761 139708407461632 logging_writer.py:48] [153100] global_step=153100, grad_norm=2.3511290550231934, loss=2.0158300399780273
I0309 01:33:34.511327 139708415854336 logging_writer.py:48] [153200] global_step=153200, grad_norm=2.1065776348114014, loss=1.986269474029541
I0309 01:34:20.041785 139708407461632 logging_writer.py:48] [153300] global_step=153300, grad_norm=2.400960922241211, loss=2.8347153663635254
I0309 01:34:36.721584 139902746892096 spec.py:321] Evaluating on the training split.
I0309 01:34:48.226825 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 01:35:10.944256 139902746892096 spec.py:349] Evaluating on the test split.
I0309 01:35:12.622806 139902746892096 submission_runner.py:411] Time since start: 74035.65s, 	Step: 153339, 	{'train/accuracy': 0.8586718440055847, 'train/loss': 0.5393452048301697, 'validation/accuracy': 0.7623199820518494, 'validation/loss': 0.9501993656158447, 'validation/num_examples': 50000, 'test/accuracy': 0.641800045967102, 'test/loss': 1.5675500631332397, 'test/num_examples': 10000, 'score': 68524.06725406647, 'total_duration': 74035.65158557892, 'accumulated_submission_time': 68524.06725406647, 'accumulated_eval_time': 5495.893121242523, 'accumulated_logging_time': 7.593789339065552}
I0309 01:35:12.669466 139708415854336 logging_writer.py:48] [153339] accumulated_eval_time=5495.893121, accumulated_logging_time=7.593789, accumulated_submission_time=68524.067254, global_step=153339, preemption_count=0, score=68524.067254, test/accuracy=0.641800, test/loss=1.567550, test/num_examples=10000, total_duration=74035.651586, train/accuracy=0.858672, train/loss=0.539345, validation/accuracy=0.762320, validation/loss=0.950199, validation/num_examples=50000
I0309 01:35:37.060963 139708407461632 logging_writer.py:48] [153400] global_step=153400, grad_norm=2.4612770080566406, loss=1.2927892208099365
I0309 01:36:20.705374 139708415854336 logging_writer.py:48] [153500] global_step=153500, grad_norm=2.666400194168091, loss=1.409084439277649
I0309 01:37:05.858937 139708407461632 logging_writer.py:48] [153600] global_step=153600, grad_norm=2.3479838371276855, loss=1.4421954154968262
I0309 01:37:51.959764 139708415854336 logging_writer.py:48] [153700] global_step=153700, grad_norm=2.323809862136841, loss=1.37485671043396
I0309 01:38:37.246489 139708407461632 logging_writer.py:48] [153800] global_step=153800, grad_norm=2.474947452545166, loss=1.352510929107666
I0309 01:39:22.710745 139708415854336 logging_writer.py:48] [153900] global_step=153900, grad_norm=2.4810843467712402, loss=1.398183822631836
I0309 01:40:08.102087 139708407461632 logging_writer.py:48] [154000] global_step=154000, grad_norm=2.3792097568511963, loss=1.434585452079773
I0309 01:40:53.409878 139708415854336 logging_writer.py:48] [154100] global_step=154100, grad_norm=2.4088761806488037, loss=2.9879252910614014
I0309 01:41:38.941465 139708407461632 logging_writer.py:48] [154200] global_step=154200, grad_norm=2.827785015106201, loss=3.3386054039001465
I0309 01:42:12.699136 139902746892096 spec.py:321] Evaluating on the training split.
I0309 01:42:24.141321 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 01:42:46.401833 139902746892096 spec.py:349] Evaluating on the test split.
I0309 01:42:48.068737 139902746892096 submission_runner.py:411] Time since start: 74491.10s, 	Step: 154276, 	{'train/accuracy': 0.8527148365974426, 'train/loss': 0.5654762387275696, 'validation/accuracy': 0.7623599767684937, 'validation/loss': 0.9489037990570068, 'validation/num_examples': 50000, 'test/accuracy': 0.6407000422477722, 'test/loss': 1.5654865503311157, 'test/num_examples': 10000, 'score': 68944.03726816177, 'total_duration': 74491.09753751755, 'accumulated_submission_time': 68944.03726816177, 'accumulated_eval_time': 5531.262712240219, 'accumulated_logging_time': 7.6498801708221436}
I0309 01:42:48.108147 139708415854336 logging_writer.py:48] [154276] accumulated_eval_time=5531.262712, accumulated_logging_time=7.649880, accumulated_submission_time=68944.037268, global_step=154276, preemption_count=0, score=68944.037268, test/accuracy=0.640700, test/loss=1.565487, test/num_examples=10000, total_duration=74491.097538, train/accuracy=0.852715, train/loss=0.565476, validation/accuracy=0.762360, validation/loss=0.948904, validation/num_examples=50000
I0309 01:42:57.948757 139708407461632 logging_writer.py:48] [154300] global_step=154300, grad_norm=2.316114664077759, loss=1.9987733364105225
I0309 01:43:39.187887 139708415854336 logging_writer.py:48] [154400] global_step=154400, grad_norm=2.3490841388702393, loss=2.3800013065338135
I0309 01:44:24.468759 139708407461632 logging_writer.py:48] [154500] global_step=154500, grad_norm=2.371316432952881, loss=1.9515972137451172
I0309 01:45:10.283918 139708415854336 logging_writer.py:48] [154600] global_step=154600, grad_norm=2.1416432857513428, loss=2.5932977199554443
I0309 01:45:55.408640 139708407461632 logging_writer.py:48] [154700] global_step=154700, grad_norm=2.314344644546509, loss=1.2389181852340698
I0309 01:46:40.621524 139708415854336 logging_writer.py:48] [154800] global_step=154800, grad_norm=2.377927303314209, loss=1.583894968032837
I0309 01:47:25.979080 139708407461632 logging_writer.py:48] [154900] global_step=154900, grad_norm=3.0350944995880127, loss=3.6658689975738525
I0309 01:48:11.332844 139708415854336 logging_writer.py:48] [155000] global_step=155000, grad_norm=2.567276954650879, loss=3.1039276123046875
I0309 01:48:56.429395 139708407461632 logging_writer.py:48] [155100] global_step=155100, grad_norm=2.461768388748169, loss=2.907457113265991
I0309 01:49:41.905059 139708415854336 logging_writer.py:48] [155200] global_step=155200, grad_norm=2.498600721359253, loss=2.1528971195220947
I0309 01:49:48.353312 139902746892096 spec.py:321] Evaluating on the training split.
I0309 01:49:59.945299 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 01:50:20.490573 139902746892096 spec.py:349] Evaluating on the test split.
I0309 01:50:22.173010 139902746892096 submission_runner.py:411] Time since start: 74945.20s, 	Step: 155216, 	{'train/accuracy': 0.8570116758346558, 'train/loss': 0.52569180727005, 'validation/accuracy': 0.7622999548912048, 'validation/loss': 0.9294662475585938, 'validation/num_examples': 50000, 'test/accuracy': 0.6420000195503235, 'test/loss': 1.5497945547103882, 'test/num_examples': 10000, 'score': 69364.2235994339, 'total_duration': 74945.20179843903, 'accumulated_submission_time': 69364.2235994339, 'accumulated_eval_time': 5565.0824184417725, 'accumulated_logging_time': 7.698629856109619}
I0309 01:50:22.218637 139708407461632 logging_writer.py:48] [155216] accumulated_eval_time=5565.082418, accumulated_logging_time=7.698630, accumulated_submission_time=69364.223599, global_step=155216, preemption_count=0, score=69364.223599, test/accuracy=0.642000, test/loss=1.549795, test/num_examples=10000, total_duration=74945.201798, train/accuracy=0.857012, train/loss=0.525692, validation/accuracy=0.762300, validation/loss=0.929466, validation/num_examples=50000
I0309 01:50:55.932614 139708415854336 logging_writer.py:48] [155300] global_step=155300, grad_norm=2.573385715484619, loss=1.809687614440918
I0309 01:51:40.959953 139708407461632 logging_writer.py:48] [155400] global_step=155400, grad_norm=2.6771304607391357, loss=2.198847532272339
I0309 01:52:26.237290 139708415854336 logging_writer.py:48] [155500] global_step=155500, grad_norm=2.9535834789276123, loss=3.3840837478637695
I0309 01:53:11.624675 139708407461632 logging_writer.py:48] [155600] global_step=155600, grad_norm=2.4007043838500977, loss=1.4650986194610596
I0309 01:53:56.730363 139708415854336 logging_writer.py:48] [155700] global_step=155700, grad_norm=2.7322075366973877, loss=2.531541347503662
I0309 01:54:41.896295 139708407461632 logging_writer.py:48] [155800] global_step=155800, grad_norm=3.029000997543335, loss=3.441810369491577
I0309 01:55:27.248909 139708415854336 logging_writer.py:48] [155900] global_step=155900, grad_norm=2.5766208171844482, loss=1.2536911964416504
I0309 01:56:12.725779 139708407461632 logging_writer.py:48] [156000] global_step=156000, grad_norm=2.429243564605713, loss=1.6890865564346313
I0309 01:56:57.789148 139708415854336 logging_writer.py:48] [156100] global_step=156100, grad_norm=2.449559450149536, loss=2.146252155303955
I0309 01:57:22.481125 139902746892096 spec.py:321] Evaluating on the training split.
I0309 01:57:34.069280 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 01:57:56.207516 139902746892096 spec.py:349] Evaluating on the test split.
I0309 01:57:57.875460 139902746892096 submission_runner.py:411] Time since start: 75400.90s, 	Step: 156156, 	{'train/accuracy': 0.8631835579872131, 'train/loss': 0.5074052214622498, 'validation/accuracy': 0.7647199630737305, 'validation/loss': 0.928766131401062, 'validation/num_examples': 50000, 'test/accuracy': 0.6428000330924988, 'test/loss': 1.5421534776687622, 'test/num_examples': 10000, 'score': 69784.42750167847, 'total_duration': 75400.90425562859, 'accumulated_submission_time': 69784.42750167847, 'accumulated_eval_time': 5600.47674703598, 'accumulated_logging_time': 7.753859281539917}
I0309 01:57:57.919010 139708407461632 logging_writer.py:48] [156156] accumulated_eval_time=5600.476747, accumulated_logging_time=7.753859, accumulated_submission_time=69784.427502, global_step=156156, preemption_count=0, score=69784.427502, test/accuracy=0.642800, test/loss=1.542153, test/num_examples=10000, total_duration=75400.904256, train/accuracy=0.863184, train/loss=0.507405, validation/accuracy=0.764720, validation/loss=0.928766, validation/num_examples=50000
I0309 01:58:15.867242 139708415854336 logging_writer.py:48] [156200] global_step=156200, grad_norm=2.2672505378723145, loss=2.115813732147217
I0309 01:58:58.226364 139708407461632 logging_writer.py:48] [156300] global_step=156300, grad_norm=2.661639928817749, loss=1.415330410003662
I0309 01:59:44.019287 139708415854336 logging_writer.py:48] [156400] global_step=156400, grad_norm=3.228455066680908, loss=3.621187925338745
I0309 02:00:29.571024 139708407461632 logging_writer.py:48] [156500] global_step=156500, grad_norm=2.6897637844085693, loss=1.6582854986190796
I0309 02:01:15.045496 139708415854336 logging_writer.py:48] [156600] global_step=156600, grad_norm=2.3281657695770264, loss=1.9173048734664917
I0309 02:02:00.225272 139708407461632 logging_writer.py:48] [156700] global_step=156700, grad_norm=2.602360486984253, loss=1.3866431713104248
I0309 02:02:45.635005 139708415854336 logging_writer.py:48] [156800] global_step=156800, grad_norm=2.4278066158294678, loss=1.5590871572494507
I0309 02:03:31.083458 139708407461632 logging_writer.py:48] [156900] global_step=156900, grad_norm=2.75286602973938, loss=1.4518001079559326
I0309 02:04:16.587571 139708415854336 logging_writer.py:48] [157000] global_step=157000, grad_norm=2.6988744735717773, loss=1.3054982423782349
I0309 02:04:57.937674 139902746892096 spec.py:321] Evaluating on the training split.
I0309 02:05:09.411607 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 02:05:30.962979 139902746892096 spec.py:349] Evaluating on the test split.
I0309 02:05:32.638942 139902746892096 submission_runner.py:411] Time since start: 75855.67s, 	Step: 157093, 	{'train/accuracy': 0.8556640148162842, 'train/loss': 0.5329411625862122, 'validation/accuracy': 0.766539990901947, 'validation/loss': 0.919713020324707, 'validation/num_examples': 50000, 'test/accuracy': 0.6445000171661377, 'test/loss': 1.5387070178985596, 'test/num_examples': 10000, 'score': 70204.14212560654, 'total_duration': 75855.6677236557, 'accumulated_submission_time': 70204.14212560654, 'accumulated_eval_time': 5635.178020000458, 'accumulated_logging_time': 8.052583456039429}
I0309 02:05:32.687328 139708407461632 logging_writer.py:48] [157093] accumulated_eval_time=5635.178020, accumulated_logging_time=8.052583, accumulated_submission_time=70204.142126, global_step=157093, preemption_count=0, score=70204.142126, test/accuracy=0.644500, test/loss=1.538707, test/num_examples=10000, total_duration=75855.667724, train/accuracy=0.855664, train/loss=0.532941, validation/accuracy=0.766540, validation/loss=0.919713, validation/num_examples=50000
I0309 02:05:35.841904 139708415854336 logging_writer.py:48] [157100] global_step=157100, grad_norm=2.933662176132202, loss=3.3797245025634766
I0309 02:06:16.390007 139708407461632 logging_writer.py:48] [157200] global_step=157200, grad_norm=2.924194097518921, loss=3.4710793495178223
I0309 02:07:01.507052 139708415854336 logging_writer.py:48] [157300] global_step=157300, grad_norm=2.397890090942383, loss=2.1376988887786865
I0309 02:07:47.171072 139708407461632 logging_writer.py:48] [157400] global_step=157400, grad_norm=2.6370046138763428, loss=1.259494423866272
I0309 02:08:32.818969 139708415854336 logging_writer.py:48] [157500] global_step=157500, grad_norm=2.59210467338562, loss=2.12931752204895
I0309 02:09:17.858105 139708407461632 logging_writer.py:48] [157600] global_step=157600, grad_norm=2.704379081726074, loss=1.3757039308547974
I0309 02:10:03.416147 139708415854336 logging_writer.py:48] [157700] global_step=157700, grad_norm=2.688204288482666, loss=1.97523832321167
I0309 02:10:48.575162 139708407461632 logging_writer.py:48] [157800] global_step=157800, grad_norm=2.77091646194458, loss=2.4318032264709473
I0309 02:11:33.914321 139708415854336 logging_writer.py:48] [157900] global_step=157900, grad_norm=2.507225275039673, loss=3.096050977706909
I0309 02:12:19.397201 139708407461632 logging_writer.py:48] [158000] global_step=158000, grad_norm=2.819089651107788, loss=1.3191779851913452
I0309 02:12:32.755697 139902746892096 spec.py:321] Evaluating on the training split.
I0309 02:12:44.353681 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 02:13:07.402602 139902746892096 spec.py:349] Evaluating on the test split.
I0309 02:13:09.067652 139902746892096 submission_runner.py:411] Time since start: 76312.10s, 	Step: 158031, 	{'train/accuracy': 0.86146479845047, 'train/loss': 0.5181689858436584, 'validation/accuracy': 0.7660399675369263, 'validation/loss': 0.9252774715423584, 'validation/num_examples': 50000, 'test/accuracy': 0.6491000056266785, 'test/loss': 1.5364491939544678, 'test/num_examples': 10000, 'score': 70624.15113449097, 'total_duration': 76312.09645199776, 'accumulated_submission_time': 70624.15113449097, 'accumulated_eval_time': 5671.48996591568, 'accumulated_logging_time': 8.11124873161316}
I0309 02:13:09.106729 139708415854336 logging_writer.py:48] [158031] accumulated_eval_time=5671.489966, accumulated_logging_time=8.111249, accumulated_submission_time=70624.151134, global_step=158031, preemption_count=0, score=70624.151134, test/accuracy=0.649100, test/loss=1.536449, test/num_examples=10000, total_duration=76312.096452, train/accuracy=0.861465, train/loss=0.518169, validation/accuracy=0.766040, validation/loss=0.925277, validation/num_examples=50000
I0309 02:13:36.613399 139708407461632 logging_writer.py:48] [158100] global_step=158100, grad_norm=2.922969102859497, loss=1.36824631690979
I0309 02:14:20.807891 139708415854336 logging_writer.py:48] [158200] global_step=158200, grad_norm=2.838996171951294, loss=2.9600374698638916
I0309 02:15:06.229809 139708407461632 logging_writer.py:48] [158300] global_step=158300, grad_norm=2.4293370246887207, loss=1.1563678979873657
I0309 02:15:51.711227 139708415854336 logging_writer.py:48] [158400] global_step=158400, grad_norm=2.6847951412200928, loss=1.2885072231292725
I0309 02:16:37.130138 139708407461632 logging_writer.py:48] [158500] global_step=158500, grad_norm=2.6027004718780518, loss=1.3905583620071411
I0309 02:17:22.522328 139708415854336 logging_writer.py:48] [158600] global_step=158600, grad_norm=2.8270795345306396, loss=1.3177391290664673
I0309 02:18:08.296016 139708407461632 logging_writer.py:48] [158700] global_step=158700, grad_norm=2.6752004623413086, loss=3.0783843994140625
I0309 02:18:53.459182 139708415854336 logging_writer.py:48] [158800] global_step=158800, grad_norm=2.6461565494537354, loss=1.3210904598236084
I0309 02:19:39.033591 139708407461632 logging_writer.py:48] [158900] global_step=158900, grad_norm=2.780379295349121, loss=1.2900009155273438
I0309 02:20:09.117289 139902746892096 spec.py:321] Evaluating on the training split.
I0309 02:20:20.679581 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 02:20:43.158947 139902746892096 spec.py:349] Evaluating on the test split.
I0309 02:20:44.832958 139902746892096 submission_runner.py:411] Time since start: 76767.86s, 	Step: 158968, 	{'train/accuracy': 0.8666796684265137, 'train/loss': 0.4957504868507385, 'validation/accuracy': 0.767799973487854, 'validation/loss': 0.9111968278884888, 'validation/num_examples': 50000, 'test/accuracy': 0.6464000344276428, 'test/loss': 1.5262484550476074, 'test/num_examples': 10000, 'score': 71044.10197019577, 'total_duration': 76767.86173796654, 'accumulated_submission_time': 71044.10197019577, 'accumulated_eval_time': 5707.205604314804, 'accumulated_logging_time': 8.161112070083618}
I0309 02:20:44.881334 139708415854336 logging_writer.py:48] [158968] accumulated_eval_time=5707.205604, accumulated_logging_time=8.161112, accumulated_submission_time=71044.101970, global_step=158968, preemption_count=0, score=71044.101970, test/accuracy=0.646400, test/loss=1.526248, test/num_examples=10000, total_duration=76767.861738, train/accuracy=0.866680, train/loss=0.495750, validation/accuracy=0.767800, validation/loss=0.911197, validation/num_examples=50000
I0309 02:20:57.865583 139708407461632 logging_writer.py:48] [159000] global_step=159000, grad_norm=2.624454975128174, loss=1.3639605045318604
I0309 02:21:39.792869 139708415854336 logging_writer.py:48] [159100] global_step=159100, grad_norm=2.871873140335083, loss=2.476287841796875
I0309 02:22:25.228846 139708407461632 logging_writer.py:48] [159200] global_step=159200, grad_norm=2.619744300842285, loss=1.2880631685256958
I0309 02:23:10.708608 139708415854336 logging_writer.py:48] [159300] global_step=159300, grad_norm=2.6618165969848633, loss=1.3433963060379028
I0309 02:23:55.922563 139708407461632 logging_writer.py:48] [159400] global_step=159400, grad_norm=2.5611796379089355, loss=1.2604408264160156
I0309 02:24:41.283828 139708415854336 logging_writer.py:48] [159500] global_step=159500, grad_norm=2.9042251110076904, loss=2.829176425933838
I0309 02:25:26.712168 139708407461632 logging_writer.py:48] [159600] global_step=159600, grad_norm=2.747424840927124, loss=1.6632397174835205
I0309 02:26:11.984477 139708415854336 logging_writer.py:48] [159700] global_step=159700, grad_norm=2.5791563987731934, loss=1.2145370244979858
I0309 02:26:57.675409 139708407461632 logging_writer.py:48] [159800] global_step=159800, grad_norm=3.1638410091400146, loss=1.4358761310577393
I0309 02:27:43.653554 139708415854336 logging_writer.py:48] [159900] global_step=159900, grad_norm=2.5231521129608154, loss=2.0449421405792236
I0309 02:27:45.118705 139902746892096 spec.py:321] Evaluating on the training split.
I0309 02:27:56.646970 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 02:28:17.390374 139902746892096 spec.py:349] Evaluating on the test split.
I0309 02:28:19.081574 139902746892096 submission_runner.py:411] Time since start: 77222.11s, 	Step: 159905, 	{'train/accuracy': 0.8684179782867432, 'train/loss': 0.49797338247299194, 'validation/accuracy': 0.7691599726676941, 'validation/loss': 0.9126954674720764, 'validation/num_examples': 50000, 'test/accuracy': 0.6496000289916992, 'test/loss': 1.5200276374816895, 'test/num_examples': 10000, 'score': 71464.28036999702, 'total_duration': 77222.11035394669, 'accumulated_submission_time': 71464.28036999702, 'accumulated_eval_time': 5741.168454885483, 'accumulated_logging_time': 8.218778133392334}
I0309 02:28:19.124210 139708407461632 logging_writer.py:48] [159905] accumulated_eval_time=5741.168455, accumulated_logging_time=8.218778, accumulated_submission_time=71464.280370, global_step=159905, preemption_count=0, score=71464.280370, test/accuracy=0.649600, test/loss=1.520028, test/num_examples=10000, total_duration=77222.110354, train/accuracy=0.868418, train/loss=0.497973, validation/accuracy=0.769160, validation/loss=0.912695, validation/num_examples=50000
I0309 02:28:57.628219 139708415854336 logging_writer.py:48] [160000] global_step=160000, grad_norm=2.8308119773864746, loss=1.5563206672668457
I0309 02:29:42.817344 139708407461632 logging_writer.py:48] [160100] global_step=160100, grad_norm=2.6765758991241455, loss=1.3006494045257568
I0309 02:30:28.815510 139708415854336 logging_writer.py:48] [160200] global_step=160200, grad_norm=2.494675636291504, loss=1.2491282224655151
I0309 02:31:14.673677 139708407461632 logging_writer.py:48] [160300] global_step=160300, grad_norm=2.576923370361328, loss=1.6605873107910156
I0309 02:32:00.096985 139708415854336 logging_writer.py:48] [160400] global_step=160400, grad_norm=2.7027578353881836, loss=1.3147131204605103
I0309 02:32:45.591635 139708407461632 logging_writer.py:48] [160500] global_step=160500, grad_norm=2.5928955078125, loss=1.5695290565490723
I0309 02:33:31.101674 139708415854336 logging_writer.py:48] [160600] global_step=160600, grad_norm=2.73091197013855, loss=1.2719637155532837
I0309 02:34:16.518253 139708407461632 logging_writer.py:48] [160700] global_step=160700, grad_norm=2.861945867538452, loss=3.0481984615325928
I0309 02:35:01.845226 139708415854336 logging_writer.py:48] [160800] global_step=160800, grad_norm=2.730592966079712, loss=1.318017601966858
I0309 02:35:19.211478 139902746892096 spec.py:321] Evaluating on the training split.
I0309 02:35:30.606103 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 02:35:51.832908 139902746892096 spec.py:349] Evaluating on the test split.
I0309 02:35:53.514671 139902746892096 submission_runner.py:411] Time since start: 77676.54s, 	Step: 160840, 	{'train/accuracy': 0.8680663704872131, 'train/loss': 0.4921901226043701, 'validation/accuracy': 0.770039975643158, 'validation/loss': 0.9016311764717102, 'validation/num_examples': 50000, 'test/accuracy': 0.6538000106811523, 'test/loss': 1.5107396841049194, 'test/num_examples': 10000, 'score': 71884.30971646309, 'total_duration': 77676.5434346199, 'accumulated_submission_time': 71884.30971646309, 'accumulated_eval_time': 5775.471606492996, 'accumulated_logging_time': 8.270018339157104}
I0309 02:35:53.556889 139708407461632 logging_writer.py:48] [160840] accumulated_eval_time=5775.471606, accumulated_logging_time=8.270018, accumulated_submission_time=71884.309716, global_step=160840, preemption_count=0, score=71884.309716, test/accuracy=0.653800, test/loss=1.510740, test/num_examples=10000, total_duration=77676.543435, train/accuracy=0.868066, train/loss=0.492190, validation/accuracy=0.770040, validation/loss=0.901631, validation/num_examples=50000
I0309 02:36:17.566681 139708415854336 logging_writer.py:48] [160900] global_step=160900, grad_norm=2.645324230194092, loss=1.1791614294052124
I0309 02:37:01.332849 139708407461632 logging_writer.py:48] [161000] global_step=161000, grad_norm=2.748333692550659, loss=1.267106056213379
I0309 02:37:46.926997 139708415854336 logging_writer.py:48] [161100] global_step=161100, grad_norm=2.744319200515747, loss=1.3419663906097412
I0309 02:38:32.646327 139708407461632 logging_writer.py:48] [161200] global_step=161200, grad_norm=2.682304620742798, loss=1.530799150466919
I0309 02:39:18.194779 139708415854336 logging_writer.py:48] [161300] global_step=161300, grad_norm=2.7238519191741943, loss=2.9120676517486572
I0309 02:40:03.847792 139708407461632 logging_writer.py:48] [161400] global_step=161400, grad_norm=2.727724552154541, loss=1.5241360664367676
I0309 02:40:49.392105 139708415854336 logging_writer.py:48] [161500] global_step=161500, grad_norm=2.6241984367370605, loss=2.764259099960327
I0309 02:41:34.817309 139708407461632 logging_writer.py:48] [161600] global_step=161600, grad_norm=2.796691417694092, loss=1.2760337591171265
I0309 02:42:20.323045 139708415854336 logging_writer.py:48] [161700] global_step=161700, grad_norm=2.955479145050049, loss=2.0655198097229004
I0309 02:42:53.612256 139902746892096 spec.py:321] Evaluating on the training split.
I0309 02:43:05.041309 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 02:43:28.337754 139902746892096 spec.py:349] Evaluating on the test split.
I0309 02:43:30.016402 139902746892096 submission_runner.py:411] Time since start: 78133.05s, 	Step: 161775, 	{'train/accuracy': 0.870898425579071, 'train/loss': 0.4825400114059448, 'validation/accuracy': 0.7693799734115601, 'validation/loss': 0.9072303175926208, 'validation/num_examples': 50000, 'test/accuracy': 0.6493000388145447, 'test/loss': 1.5251152515411377, 'test/num_examples': 10000, 'score': 72304.306691885, 'total_duration': 78133.04519224167, 'accumulated_submission_time': 72304.306691885, 'accumulated_eval_time': 5811.875743627548, 'accumulated_logging_time': 8.321635723114014}
I0309 02:43:30.057748 139708407461632 logging_writer.py:48] [161775] accumulated_eval_time=5811.875744, accumulated_logging_time=8.321636, accumulated_submission_time=72304.306692, global_step=161775, preemption_count=0, score=72304.306692, test/accuracy=0.649300, test/loss=1.525115, test/num_examples=10000, total_duration=78133.045192, train/accuracy=0.870898, train/loss=0.482540, validation/accuracy=0.769380, validation/loss=0.907230, validation/num_examples=50000
I0309 02:43:40.280398 139708415854336 logging_writer.py:48] [161800] global_step=161800, grad_norm=2.5611796379089355, loss=1.1519019603729248
I0309 02:44:21.327651 139708407461632 logging_writer.py:48] [161900] global_step=161900, grad_norm=2.879241704940796, loss=3.126009702682495
I0309 02:45:06.543367 139708415854336 logging_writer.py:48] [162000] global_step=162000, grad_norm=2.7299153804779053, loss=2.096522331237793
I0309 02:45:52.213831 139708407461632 logging_writer.py:48] [162100] global_step=162100, grad_norm=2.6158201694488525, loss=1.3360047340393066
I0309 02:46:37.582951 139708415854336 logging_writer.py:48] [162200] global_step=162200, grad_norm=2.96242618560791, loss=2.9878225326538086
I0309 02:47:22.945305 139708407461632 logging_writer.py:48] [162300] global_step=162300, grad_norm=2.7186877727508545, loss=1.5538809299468994
I0309 02:48:08.636082 139708415854336 logging_writer.py:48] [162400] global_step=162400, grad_norm=2.9920036792755127, loss=1.3879079818725586
I0309 02:48:53.815570 139708407461632 logging_writer.py:48] [162500] global_step=162500, grad_norm=2.7036705017089844, loss=1.3425716161727905
I0309 02:49:39.049880 139708415854336 logging_writer.py:48] [162600] global_step=162600, grad_norm=2.883734703063965, loss=2.4665865898132324
I0309 02:50:24.671302 139708407461632 logging_writer.py:48] [162700] global_step=162700, grad_norm=2.8930907249450684, loss=1.3813509941101074
I0309 02:50:30.226130 139902746892096 spec.py:321] Evaluating on the training split.
I0309 02:50:41.714893 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 02:51:03.352972 139902746892096 spec.py:349] Evaluating on the test split.
I0309 02:51:05.037133 139902746892096 submission_runner.py:411] Time since start: 78588.07s, 	Step: 162714, 	{'train/accuracy': 0.8746874928474426, 'train/loss': 0.4815309941768646, 'validation/accuracy': 0.7702800035476685, 'validation/loss': 0.9123331904411316, 'validation/num_examples': 50000, 'test/accuracy': 0.6517000198364258, 'test/loss': 1.525587797164917, 'test/num_examples': 10000, 'score': 72724.41666007042, 'total_duration': 78588.06591463089, 'accumulated_submission_time': 72724.41666007042, 'accumulated_eval_time': 5846.686740875244, 'accumulated_logging_time': 8.37223219871521}
I0309 02:51:05.085920 139708415854336 logging_writer.py:48] [162714] accumulated_eval_time=5846.686741, accumulated_logging_time=8.372232, accumulated_submission_time=72724.416660, global_step=162714, preemption_count=0, score=72724.416660, test/accuracy=0.651700, test/loss=1.525588, test/num_examples=10000, total_duration=78588.065915, train/accuracy=0.874687, train/loss=0.481531, validation/accuracy=0.770280, validation/loss=0.912333, validation/num_examples=50000
I0309 02:51:39.477827 139708407461632 logging_writer.py:48] [162800] global_step=162800, grad_norm=2.7155003547668457, loss=1.2826728820800781
I0309 02:52:24.668265 139708415854336 logging_writer.py:48] [162900] global_step=162900, grad_norm=2.9109039306640625, loss=1.281930923461914
I0309 02:53:10.362042 139708407461632 logging_writer.py:48] [163000] global_step=163000, grad_norm=2.686457633972168, loss=2.28299880027771
I0309 02:53:55.927139 139708415854336 logging_writer.py:48] [163100] global_step=163100, grad_norm=2.555776596069336, loss=2.0899646282196045
I0309 02:54:41.349151 139708407461632 logging_writer.py:48] [163200] global_step=163200, grad_norm=3.0319745540618896, loss=3.176988363265991
I0309 02:55:26.777506 139708415854336 logging_writer.py:48] [163300] global_step=163300, grad_norm=2.5608713626861572, loss=1.2210146188735962
I0309 02:56:12.202322 139708407461632 logging_writer.py:48] [163400] global_step=163400, grad_norm=2.641500473022461, loss=1.4474929571151733
I0309 02:56:57.500034 139708415854336 logging_writer.py:48] [163500] global_step=163500, grad_norm=2.7259111404418945, loss=1.5131937265396118
I0309 02:57:43.122608 139708407461632 logging_writer.py:48] [163600] global_step=163600, grad_norm=3.2479453086853027, loss=2.9860992431640625
I0309 02:58:05.217608 139902746892096 spec.py:321] Evaluating on the training split.
I0309 02:58:16.794025 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 02:58:37.362264 139902746892096 spec.py:349] Evaluating on the test split.
I0309 02:58:39.047063 139902746892096 submission_runner.py:411] Time since start: 79042.08s, 	Step: 163650, 	{'train/accuracy': 0.8691992163658142, 'train/loss': 0.4835098087787628, 'validation/accuracy': 0.7697199583053589, 'validation/loss': 0.8991246819496155, 'validation/num_examples': 50000, 'test/accuracy': 0.6526000499725342, 'test/loss': 1.5037193298339844, 'test/num_examples': 10000, 'score': 73144.49038362503, 'total_duration': 79042.07584261894, 'accumulated_submission_time': 73144.49038362503, 'accumulated_eval_time': 5880.516179323196, 'accumulated_logging_time': 8.430397033691406}
I0309 02:58:39.096027 139708415854336 logging_writer.py:48] [163650] accumulated_eval_time=5880.516179, accumulated_logging_time=8.430397, accumulated_submission_time=73144.490384, global_step=163650, preemption_count=0, score=73144.490384, test/accuracy=0.652600, test/loss=1.503719, test/num_examples=10000, total_duration=79042.075843, train/accuracy=0.869199, train/loss=0.483510, validation/accuracy=0.769720, validation/loss=0.899125, validation/num_examples=50000
I0309 02:58:59.164390 139708407461632 logging_writer.py:48] [163700] global_step=163700, grad_norm=2.6349642276763916, loss=2.013113260269165
I0309 02:59:42.230929 139708415854336 logging_writer.py:48] [163800] global_step=163800, grad_norm=2.7829129695892334, loss=1.6917810440063477
I0309 03:00:27.703189 139708407461632 logging_writer.py:48] [163900] global_step=163900, grad_norm=3.1732168197631836, loss=3.0871801376342773
I0309 03:01:13.524368 139708415854336 logging_writer.py:48] [164000] global_step=164000, grad_norm=2.908831834793091, loss=1.4911222457885742
I0309 03:01:58.647701 139708407461632 logging_writer.py:48] [164100] global_step=164100, grad_norm=3.414175271987915, loss=3.2436721324920654
I0309 03:02:43.958515 139708415854336 logging_writer.py:48] [164200] global_step=164200, grad_norm=2.8641862869262695, loss=2.760434150695801
I0309 03:03:29.426883 139708407461632 logging_writer.py:48] [164300] global_step=164300, grad_norm=2.7342429161071777, loss=1.6624505519866943
I0309 03:04:14.913857 139708415854336 logging_writer.py:48] [164400] global_step=164400, grad_norm=2.6979470252990723, loss=2.2733817100524902
I0309 03:05:00.342196 139708407461632 logging_writer.py:48] [164500] global_step=164500, grad_norm=2.9260830879211426, loss=2.294259548187256
I0309 03:05:39.424473 139902746892096 spec.py:321] Evaluating on the training split.
I0309 03:05:50.846905 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 03:06:12.985245 139902746892096 spec.py:349] Evaluating on the test split.
I0309 03:06:14.665328 139902746892096 submission_runner.py:411] Time since start: 79497.69s, 	Step: 164588, 	{'train/accuracy': 0.87158203125, 'train/loss': 0.47553396224975586, 'validation/accuracy': 0.7726399898529053, 'validation/loss': 0.8931786417961121, 'validation/num_examples': 50000, 'test/accuracy': 0.6550000309944153, 'test/loss': 1.5063070058822632, 'test/num_examples': 10000, 'score': 73564.75876450539, 'total_duration': 79497.69409179688, 'accumulated_submission_time': 73564.75876450539, 'accumulated_eval_time': 5915.757030487061, 'accumulated_logging_time': 8.490588188171387}
I0309 03:06:14.711036 139708415854336 logging_writer.py:48] [164588] accumulated_eval_time=5915.757030, accumulated_logging_time=8.490588, accumulated_submission_time=73564.758765, global_step=164588, preemption_count=0, score=73564.758765, test/accuracy=0.655000, test/loss=1.506307, test/num_examples=10000, total_duration=79497.694092, train/accuracy=0.871582, train/loss=0.475534, validation/accuracy=0.772640, validation/loss=0.893179, validation/num_examples=50000
I0309 03:06:19.827478 139708407461632 logging_writer.py:48] [164600] global_step=164600, grad_norm=2.6904873847961426, loss=1.748126745223999
I0309 03:07:00.258579 139708415854336 logging_writer.py:48] [164700] global_step=164700, grad_norm=2.9933788776397705, loss=1.2883777618408203
I0309 03:07:45.456785 139708407461632 logging_writer.py:48] [164800] global_step=164800, grad_norm=3.650325059890747, loss=3.380516290664673
I0309 03:08:30.639615 139708415854336 logging_writer.py:48] [164900] global_step=164900, grad_norm=2.7141902446746826, loss=1.2091326713562012
I0309 03:09:16.140379 139708407461632 logging_writer.py:48] [165000] global_step=165000, grad_norm=3.075260639190674, loss=3.170459747314453
I0309 03:10:01.715425 139708415854336 logging_writer.py:48] [165100] global_step=165100, grad_norm=2.8397889137268066, loss=1.2489794492721558
I0309 03:10:47.272279 139708407461632 logging_writer.py:48] [165200] global_step=165200, grad_norm=2.7915854454040527, loss=1.2500298023223877
I0309 03:11:32.541211 139708415854336 logging_writer.py:48] [165300] global_step=165300, grad_norm=3.606729745864868, loss=3.5028209686279297
I0309 03:12:17.935437 139708407461632 logging_writer.py:48] [165400] global_step=165400, grad_norm=3.4912807941436768, loss=3.3799362182617188
I0309 03:13:03.201159 139708415854336 logging_writer.py:48] [165500] global_step=165500, grad_norm=2.6921567916870117, loss=2.2356066703796387
I0309 03:13:15.107764 139902746892096 spec.py:321] Evaluating on the training split.
I0309 03:13:26.627291 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 03:13:47.409176 139902746892096 spec.py:349] Evaluating on the test split.
I0309 03:13:49.079169 139902746892096 submission_runner.py:411] Time since start: 79952.11s, 	Step: 165528, 	{'train/accuracy': 0.8754296898841858, 'train/loss': 0.4589492976665497, 'validation/accuracy': 0.7731800079345703, 'validation/loss': 0.8862924575805664, 'validation/num_examples': 50000, 'test/accuracy': 0.6545000076293945, 'test/loss': 1.494189739227295, 'test/num_examples': 10000, 'score': 73985.09216928482, 'total_duration': 79952.10796093941, 'accumulated_submission_time': 73985.09216928482, 'accumulated_eval_time': 5949.728416919708, 'accumulated_logging_time': 8.550045013427734}
I0309 03:13:49.118835 139708407461632 logging_writer.py:48] [165528] accumulated_eval_time=5949.728417, accumulated_logging_time=8.550045, accumulated_submission_time=73985.092169, global_step=165528, preemption_count=0, score=73985.092169, test/accuracy=0.654500, test/loss=1.494190, test/num_examples=10000, total_duration=79952.107961, train/accuracy=0.875430, train/loss=0.458949, validation/accuracy=0.773180, validation/loss=0.886292, validation/num_examples=50000
I0309 03:14:17.852349 139708415854336 logging_writer.py:48] [165600] global_step=165600, grad_norm=2.6392691135406494, loss=1.1483933925628662
I0309 03:15:01.885376 139708407461632 logging_writer.py:48] [165700] global_step=165700, grad_norm=2.9418065547943115, loss=1.1619958877563477
I0309 03:15:47.288383 139708415854336 logging_writer.py:48] [165800] global_step=165800, grad_norm=2.8947865962982178, loss=1.1994026899337769
I0309 03:16:32.837823 139708407461632 logging_writer.py:48] [165900] global_step=165900, grad_norm=3.299055337905884, loss=3.402535915374756
I0309 03:17:18.137036 139708415854336 logging_writer.py:48] [166000] global_step=166000, grad_norm=3.2220146656036377, loss=3.25495982170105
I0309 03:18:03.441310 139708407461632 logging_writer.py:48] [166100] global_step=166100, grad_norm=3.3323276042938232, loss=3.4157328605651855
I0309 03:18:48.992373 139708415854336 logging_writer.py:48] [166200] global_step=166200, grad_norm=2.8626596927642822, loss=1.8858323097229004
I0309 03:19:34.340266 139708407461632 logging_writer.py:48] [166300] global_step=166300, grad_norm=3.069378614425659, loss=3.0302720069885254
I0309 03:20:20.083901 139708415854336 logging_writer.py:48] [166400] global_step=166400, grad_norm=2.714245080947876, loss=1.8558799028396606
I0309 03:20:49.169769 139902746892096 spec.py:321] Evaluating on the training split.
I0309 03:21:00.679971 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 03:21:20.923469 139902746892096 spec.py:349] Evaluating on the test split.
I0309 03:21:22.605439 139902746892096 submission_runner.py:411] Time since start: 80405.63s, 	Step: 166466, 	{'train/accuracy': 0.8744921684265137, 'train/loss': 0.47714072465896606, 'validation/accuracy': 0.7742799520492554, 'validation/loss': 0.8912918567657471, 'validation/num_examples': 50000, 'test/accuracy': 0.6562000513076782, 'test/loss': 1.5012069940567017, 'test/num_examples': 10000, 'score': 74405.08458447456, 'total_duration': 80405.63422679901, 'accumulated_submission_time': 74405.08458447456, 'accumulated_eval_time': 5983.164102315903, 'accumulated_logging_time': 8.599197626113892}
I0309 03:21:22.657928 139708407461632 logging_writer.py:48] [166466] accumulated_eval_time=5983.164102, accumulated_logging_time=8.599198, accumulated_submission_time=74405.084584, global_step=166466, preemption_count=0, score=74405.084584, test/accuracy=0.656200, test/loss=1.501207, test/num_examples=10000, total_duration=80405.634227, train/accuracy=0.874492, train/loss=0.477141, validation/accuracy=0.774280, validation/loss=0.891292, validation/num_examples=50000
I0309 03:21:36.418495 139708415854336 logging_writer.py:48] [166500] global_step=166500, grad_norm=2.7839624881744385, loss=1.1790812015533447
I0309 03:22:19.587452 139708407461632 logging_writer.py:48] [166600] global_step=166600, grad_norm=2.767782688140869, loss=1.1718662977218628
I0309 03:23:04.744893 139708415854336 logging_writer.py:48] [166700] global_step=166700, grad_norm=2.6838810443878174, loss=1.6399672031402588
I0309 03:23:50.237733 139708407461632 logging_writer.py:48] [166800] global_step=166800, grad_norm=2.8914425373077393, loss=1.1568524837493896
I0309 03:24:35.743915 139708415854336 logging_writer.py:48] [166900] global_step=166900, grad_norm=2.961380958557129, loss=1.4048423767089844
I0309 03:25:20.981851 139708407461632 logging_writer.py:48] [167000] global_step=167000, grad_norm=2.9856326580047607, loss=1.2549309730529785
I0309 03:26:06.376396 139708415854336 logging_writer.py:48] [167100] global_step=167100, grad_norm=2.6919751167297363, loss=1.2965610027313232
I0309 03:26:51.661447 139708407461632 logging_writer.py:48] [167200] global_step=167200, grad_norm=3.1297824382781982, loss=1.22658371925354
I0309 03:27:37.226565 139708415854336 logging_writer.py:48] [167300] global_step=167300, grad_norm=2.8463172912597656, loss=1.3039259910583496
I0309 03:28:22.534884 139708407461632 logging_writer.py:48] [167400] global_step=167400, grad_norm=3.184901475906372, loss=3.2549595832824707
I0309 03:28:22.673368 139902746892096 spec.py:321] Evaluating on the training split.
I0309 03:28:34.295550 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 03:28:57.206278 139902746892096 spec.py:349] Evaluating on the test split.
I0309 03:28:58.880945 139902746892096 submission_runner.py:411] Time since start: 80861.91s, 	Step: 167402, 	{'train/accuracy': 0.8738085627555847, 'train/loss': 0.46668896079063416, 'validation/accuracy': 0.7739599943161011, 'validation/loss': 0.8845837116241455, 'validation/num_examples': 50000, 'test/accuracy': 0.6580000519752502, 'test/loss': 1.495749831199646, 'test/num_examples': 10000, 'score': 74825.04020619392, 'total_duration': 80861.90974783897, 'accumulated_submission_time': 74825.04020619392, 'accumulated_eval_time': 6019.371679782867, 'accumulated_logging_time': 8.662525653839111}
I0309 03:28:58.924258 139708415854336 logging_writer.py:48] [167402] accumulated_eval_time=6019.371680, accumulated_logging_time=8.662526, accumulated_submission_time=74825.040206, global_step=167402, preemption_count=0, score=74825.040206, test/accuracy=0.658000, test/loss=1.495750, test/num_examples=10000, total_duration=80861.909748, train/accuracy=0.873809, train/loss=0.466689, validation/accuracy=0.773960, validation/loss=0.884584, validation/num_examples=50000
I0309 03:29:38.316831 139708407461632 logging_writer.py:48] [167500] global_step=167500, grad_norm=2.8092098236083984, loss=1.1383521556854248
I0309 03:30:23.274674 139708415854336 logging_writer.py:48] [167600] global_step=167600, grad_norm=2.917011022567749, loss=1.092409610748291
I0309 03:31:09.221611 139708407461632 logging_writer.py:48] [167700] global_step=167700, grad_norm=2.9953930377960205, loss=2.3286099433898926
I0309 03:31:54.821972 139708415854336 logging_writer.py:48] [167800] global_step=167800, grad_norm=3.0509579181671143, loss=3.0851404666900635
I0309 03:32:40.286713 139708407461632 logging_writer.py:48] [167900] global_step=167900, grad_norm=2.909454345703125, loss=1.1847902536392212
I0309 03:33:25.571357 139708415854336 logging_writer.py:48] [168000] global_step=168000, grad_norm=2.9709038734436035, loss=1.8977277278900146
I0309 03:34:10.929617 139708407461632 logging_writer.py:48] [168100] global_step=168100, grad_norm=3.0316338539123535, loss=1.1433355808258057
I0309 03:34:56.478544 139708415854336 logging_writer.py:48] [168200] global_step=168200, grad_norm=2.9828779697418213, loss=1.2342593669891357
I0309 03:35:41.989348 139708407461632 logging_writer.py:48] [168300] global_step=168300, grad_norm=2.764209747314453, loss=1.9188039302825928
I0309 03:35:59.413452 139902746892096 spec.py:321] Evaluating on the training split.
I0309 03:36:10.749523 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 03:36:33.283854 139902746892096 spec.py:349] Evaluating on the test split.
I0309 03:36:34.959249 139902746892096 submission_runner.py:411] Time since start: 81317.99s, 	Step: 168340, 	{'train/accuracy': 0.8773242235183716, 'train/loss': 0.4531766176223755, 'validation/accuracy': 0.7756399512290955, 'validation/loss': 0.8850293755531311, 'validation/num_examples': 50000, 'test/accuracy': 0.6571000218391418, 'test/loss': 1.4930970668792725, 'test/num_examples': 10000, 'score': 75245.46972846985, 'total_duration': 81317.98803067207, 'accumulated_submission_time': 75245.46972846985, 'accumulated_eval_time': 6054.917459487915, 'accumulated_logging_time': 8.715810537338257}
I0309 03:36:35.004846 139708415854336 logging_writer.py:48] [168340] accumulated_eval_time=6054.917459, accumulated_logging_time=8.715811, accumulated_submission_time=75245.469728, global_step=168340, preemption_count=0, score=75245.469728, test/accuracy=0.657100, test/loss=1.493097, test/num_examples=10000, total_duration=81317.988031, train/accuracy=0.877324, train/loss=0.453177, validation/accuracy=0.775640, validation/loss=0.885029, validation/num_examples=50000
I0309 03:36:59.011265 139708407461632 logging_writer.py:48] [168400] global_step=168400, grad_norm=2.653158187866211, loss=1.4773738384246826
I0309 03:37:42.509355 139708415854336 logging_writer.py:48] [168500] global_step=168500, grad_norm=3.071812868118286, loss=2.3683993816375732
I0309 03:38:27.600463 139708407461632 logging_writer.py:48] [168600] global_step=168600, grad_norm=2.8026230335235596, loss=1.121394157409668
I0309 03:39:12.851222 139708415854336 logging_writer.py:48] [168700] global_step=168700, grad_norm=3.358290195465088, loss=3.250101089477539
I0309 03:39:58.265263 139708407461632 logging_writer.py:48] [168800] global_step=168800, grad_norm=2.785356283187866, loss=1.4526653289794922
I0309 03:40:43.932231 139708415854336 logging_writer.py:48] [168900] global_step=168900, grad_norm=2.6699161529541016, loss=1.7172858715057373
I0309 03:41:29.563929 139708407461632 logging_writer.py:48] [169000] global_step=169000, grad_norm=3.0426011085510254, loss=1.2065470218658447
I0309 03:42:15.010781 139708415854336 logging_writer.py:48] [169100] global_step=169100, grad_norm=2.9585940837860107, loss=1.3279750347137451
I0309 03:43:00.572134 139708407461632 logging_writer.py:48] [169200] global_step=169200, grad_norm=2.814321517944336, loss=1.0873377323150635
I0309 03:43:35.098562 139902746892096 spec.py:321] Evaluating on the training split.
I0309 03:43:46.602303 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 03:44:07.595340 139902746892096 spec.py:349] Evaluating on the test split.
I0309 03:44:09.280266 139902746892096 submission_runner.py:411] Time since start: 81772.31s, 	Step: 169278, 	{'train/accuracy': 0.8797265291213989, 'train/loss': 0.4421044588088989, 'validation/accuracy': 0.7756999731063843, 'validation/loss': 0.8826619386672974, 'validation/num_examples': 50000, 'test/accuracy': 0.6571000218391418, 'test/loss': 1.4991886615753174, 'test/num_examples': 10000, 'score': 75665.5037753582, 'total_duration': 81772.30905842781, 'accumulated_submission_time': 75665.5037753582, 'accumulated_eval_time': 6089.09916472435, 'accumulated_logging_time': 8.771555423736572}
I0309 03:44:09.320514 139708415854336 logging_writer.py:48] [169278] accumulated_eval_time=6089.099165, accumulated_logging_time=8.771555, accumulated_submission_time=75665.503775, global_step=169278, preemption_count=0, score=75665.503775, test/accuracy=0.657100, test/loss=1.499189, test/num_examples=10000, total_duration=81772.309058, train/accuracy=0.879727, train/loss=0.442104, validation/accuracy=0.775700, validation/loss=0.882662, validation/num_examples=50000
I0309 03:44:18.489150 139708407461632 logging_writer.py:48] [169300] global_step=169300, grad_norm=3.0676181316375732, loss=1.1325888633728027
I0309 03:45:00.194996 139708415854336 logging_writer.py:48] [169400] global_step=169400, grad_norm=3.009737730026245, loss=1.2013165950775146
I0309 03:45:45.895555 139708407461632 logging_writer.py:48] [169500] global_step=169500, grad_norm=3.000807046890259, loss=1.1085426807403564
I0309 03:46:31.373396 139708415854336 logging_writer.py:48] [169600] global_step=169600, grad_norm=3.465294599533081, loss=3.251633882522583
I0309 03:47:16.695632 139708407461632 logging_writer.py:48] [169700] global_step=169700, grad_norm=2.6969313621520996, loss=2.003340482711792
I0309 03:48:02.210929 139708415854336 logging_writer.py:48] [169800] global_step=169800, grad_norm=3.6446259021759033, loss=3.4067065715789795
I0309 03:48:47.737855 139708407461632 logging_writer.py:48] [169900] global_step=169900, grad_norm=3.0323221683502197, loss=1.5828588008880615
I0309 03:49:33.129391 139708415854336 logging_writer.py:48] [170000] global_step=170000, grad_norm=3.22525954246521, loss=1.1139838695526123
I0309 03:50:18.559174 139708407461632 logging_writer.py:48] [170100] global_step=170100, grad_norm=3.052518844604492, loss=2.3254573345184326
I0309 03:51:04.519268 139708415854336 logging_writer.py:48] [170200] global_step=170200, grad_norm=3.1381030082702637, loss=1.2178869247436523
I0309 03:51:09.639386 139902746892096 spec.py:321] Evaluating on the training split.
I0309 03:51:21.014888 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 03:51:43.850043 139902746892096 spec.py:349] Evaluating on the test split.
I0309 03:51:45.524759 139902746892096 submission_runner.py:411] Time since start: 82228.55s, 	Step: 170213, 	{'train/accuracy': 0.8807030916213989, 'train/loss': 0.44666290283203125, 'validation/accuracy': 0.7764399647712708, 'validation/loss': 0.8772845268249512, 'validation/num_examples': 50000, 'test/accuracy': 0.6593000292778015, 'test/loss': 1.4936374425888062, 'test/num_examples': 10000, 'score': 76085.7650001049, 'total_duration': 82228.5535402298, 'accumulated_submission_time': 76085.7650001049, 'accumulated_eval_time': 6124.984532356262, 'accumulated_logging_time': 8.821457386016846}
I0309 03:51:45.576377 139708407461632 logging_writer.py:48] [170213] accumulated_eval_time=6124.984532, accumulated_logging_time=8.821457, accumulated_submission_time=76085.765000, global_step=170213, preemption_count=0, score=76085.765000, test/accuracy=0.659300, test/loss=1.493637, test/num_examples=10000, total_duration=82228.553540, train/accuracy=0.880703, train/loss=0.446663, validation/accuracy=0.776440, validation/loss=0.877285, validation/num_examples=50000
I0309 03:52:20.165166 139708415854336 logging_writer.py:48] [170300] global_step=170300, grad_norm=3.5529699325561523, loss=3.2599730491638184
I0309 03:53:05.326858 139708407461632 logging_writer.py:48] [170400] global_step=170400, grad_norm=3.2599246501922607, loss=3.2007079124450684
I0309 03:53:50.798323 139708415854336 logging_writer.py:48] [170500] global_step=170500, grad_norm=3.1329116821289062, loss=1.2190500497817993
I0309 03:54:36.045100 139708407461632 logging_writer.py:48] [170600] global_step=170600, grad_norm=3.6457715034484863, loss=3.2840516567230225
I0309 03:55:21.407994 139708415854336 logging_writer.py:48] [170700] global_step=170700, grad_norm=2.9007439613342285, loss=1.1771172285079956
I0309 03:56:06.815332 139708407461632 logging_writer.py:48] [170800] global_step=170800, grad_norm=3.295714855194092, loss=2.844228982925415
I0309 03:56:52.115701 139708415854336 logging_writer.py:48] [170900] global_step=170900, grad_norm=3.3155760765075684, loss=2.3953640460968018
I0309 03:57:37.575265 139708407461632 logging_writer.py:48] [171000] global_step=171000, grad_norm=3.433919906616211, loss=3.307624578475952
I0309 03:58:22.897392 139708415854336 logging_writer.py:48] [171100] global_step=171100, grad_norm=3.053853988647461, loss=1.162047028541565
I0309 03:58:45.675490 139902746892096 spec.py:321] Evaluating on the training split.
I0309 03:58:57.068650 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 03:59:17.861451 139902746892096 spec.py:349] Evaluating on the test split.
I0309 03:59:19.550799 139902746892096 submission_runner.py:411] Time since start: 82682.58s, 	Step: 171152, 	{'train/accuracy': 0.8788085579872131, 'train/loss': 0.44848230481147766, 'validation/accuracy': 0.7775599956512451, 'validation/loss': 0.8697133660316467, 'validation/num_examples': 50000, 'test/accuracy': 0.6577000021934509, 'test/loss': 1.484135627746582, 'test/num_examples': 10000, 'score': 76505.8050429821, 'total_duration': 82682.57957959175, 'accumulated_submission_time': 76505.8050429821, 'accumulated_eval_time': 6158.859827756882, 'accumulated_logging_time': 8.883781433105469}
I0309 03:59:19.600380 139708407461632 logging_writer.py:48] [171152] accumulated_eval_time=6158.859828, accumulated_logging_time=8.883781, accumulated_submission_time=76505.805043, global_step=171152, preemption_count=0, score=76505.805043, test/accuracy=0.657700, test/loss=1.484136, test/num_examples=10000, total_duration=82682.579580, train/accuracy=0.878809, train/loss=0.448482, validation/accuracy=0.777560, validation/loss=0.869713, validation/num_examples=50000
I0309 03:59:38.871027 139708415854336 logging_writer.py:48] [171200] global_step=171200, grad_norm=3.3400750160217285, loss=1.1943273544311523
I0309 04:00:21.942892 139708407461632 logging_writer.py:48] [171300] global_step=171300, grad_norm=3.0071253776550293, loss=1.2198604345321655
I0309 04:01:07.679709 139708415854336 logging_writer.py:48] [171400] global_step=171400, grad_norm=2.748298406600952, loss=1.726747751235962
I0309 04:01:53.318094 139708407461632 logging_writer.py:48] [171500] global_step=171500, grad_norm=2.850388288497925, loss=1.1715927124023438
I0309 04:02:38.559724 139708415854336 logging_writer.py:48] [171600] global_step=171600, grad_norm=2.8688952922821045, loss=1.1797339916229248
I0309 04:03:23.994649 139708407461632 logging_writer.py:48] [171700] global_step=171700, grad_norm=3.225271224975586, loss=2.5391626358032227
I0309 04:04:09.428259 139708415854336 logging_writer.py:48] [171800] global_step=171800, grad_norm=3.302971839904785, loss=1.1685688495635986
I0309 04:04:54.664123 139708407461632 logging_writer.py:48] [171900] global_step=171900, grad_norm=2.9145352840423584, loss=1.4677270650863647
I0309 04:05:40.198495 139708415854336 logging_writer.py:48] [172000] global_step=172000, grad_norm=2.8988471031188965, loss=1.116532802581787
I0309 04:06:19.809311 139902746892096 spec.py:321] Evaluating on the training split.
I0309 04:06:31.121802 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 04:06:51.899773 139902746892096 spec.py:349] Evaluating on the test split.
I0309 04:06:53.588791 139902746892096 submission_runner.py:411] Time since start: 83136.62s, 	Step: 172089, 	{'train/accuracy': 0.8827148079872131, 'train/loss': 0.4331200420856476, 'validation/accuracy': 0.7792999744415283, 'validation/loss': 0.8637553453445435, 'validation/num_examples': 50000, 'test/accuracy': 0.6615000367164612, 'test/loss': 1.4721359014511108, 'test/num_examples': 10000, 'score': 76925.95561552048, 'total_duration': 83136.61755609512, 'accumulated_submission_time': 76925.95561552048, 'accumulated_eval_time': 6192.639292001724, 'accumulated_logging_time': 8.942861557006836}
I0309 04:06:53.632349 139708407461632 logging_writer.py:48] [172089] accumulated_eval_time=6192.639292, accumulated_logging_time=8.942862, accumulated_submission_time=76925.955616, global_step=172089, preemption_count=0, score=76925.955616, test/accuracy=0.661500, test/loss=1.472136, test/num_examples=10000, total_duration=83136.617556, train/accuracy=0.882715, train/loss=0.433120, validation/accuracy=0.779300, validation/loss=0.863755, validation/num_examples=50000
I0309 04:06:58.344677 139708415854336 logging_writer.py:48] [172100] global_step=172100, grad_norm=3.352513074874878, loss=2.3369622230529785
I0309 04:07:39.207951 139708407461632 logging_writer.py:48] [172200] global_step=172200, grad_norm=2.8133320808410645, loss=1.3802639245986938
I0309 04:08:24.601083 139708415854336 logging_writer.py:48] [172300] global_step=172300, grad_norm=2.9216129779815674, loss=1.1329352855682373
I0309 04:09:09.935770 139708407461632 logging_writer.py:48] [172400] global_step=172400, grad_norm=3.4654972553253174, loss=1.1946731805801392
I0309 04:09:55.615786 139708415854336 logging_writer.py:48] [172500] global_step=172500, grad_norm=3.2892088890075684, loss=3.057352066040039
I0309 04:10:40.850173 139708407461632 logging_writer.py:48] [172600] global_step=172600, grad_norm=3.14329195022583, loss=1.1712313890457153
I0309 04:11:26.181315 139708415854336 logging_writer.py:48] [172700] global_step=172700, grad_norm=3.166748285293579, loss=1.2460485696792603
I0309 04:12:11.414141 139708407461632 logging_writer.py:48] [172800] global_step=172800, grad_norm=2.758227825164795, loss=1.7554755210876465
I0309 04:12:56.637100 139708415854336 logging_writer.py:48] [172900] global_step=172900, grad_norm=2.9895246028900146, loss=1.1693599224090576
I0309 04:13:42.017614 139708407461632 logging_writer.py:48] [173000] global_step=173000, grad_norm=3.0431320667266846, loss=1.1853210926055908
I0309 04:13:53.644859 139902746892096 spec.py:321] Evaluating on the training split.
I0309 04:14:05.436744 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 04:14:27.427329 139902746892096 spec.py:349] Evaluating on the test split.
I0309 04:14:29.121576 139902746892096 submission_runner.py:411] Time since start: 83592.15s, 	Step: 173027, 	{'train/accuracy': 0.8812695145606995, 'train/loss': 0.44290611147880554, 'validation/accuracy': 0.7791399955749512, 'validation/loss': 0.8685945868492126, 'validation/num_examples': 50000, 'test/accuracy': 0.6621000170707703, 'test/loss': 1.4893403053283691, 'test/num_examples': 10000, 'score': 77345.90822839737, 'total_duration': 83592.15035367012, 'accumulated_submission_time': 77345.90822839737, 'accumulated_eval_time': 6228.115994215012, 'accumulated_logging_time': 8.996717929840088}
I0309 04:14:29.169787 139708415854336 logging_writer.py:48] [173027] accumulated_eval_time=6228.115994, accumulated_logging_time=8.996718, accumulated_submission_time=77345.908228, global_step=173027, preemption_count=0, score=77345.908228, test/accuracy=0.662100, test/loss=1.489340, test/num_examples=10000, total_duration=83592.150354, train/accuracy=0.881270, train/loss=0.442906, validation/accuracy=0.779140, validation/loss=0.868595, validation/num_examples=50000
I0309 04:14:58.272832 139708407461632 logging_writer.py:48] [173100] global_step=173100, grad_norm=2.8463146686553955, loss=2.0465359687805176
I0309 04:15:42.627819 139708415854336 logging_writer.py:48] [173200] global_step=173200, grad_norm=2.845449686050415, loss=1.4097683429718018
I0309 04:16:28.289265 139708407461632 logging_writer.py:48] [173300] global_step=173300, grad_norm=2.840830087661743, loss=2.3115663528442383
I0309 04:17:14.040816 139708415854336 logging_writer.py:48] [173400] global_step=173400, grad_norm=2.9800310134887695, loss=1.0615193843841553
I0309 04:17:59.469568 139708407461632 logging_writer.py:48] [173500] global_step=173500, grad_norm=2.830984592437744, loss=1.73476243019104
I0309 04:18:44.866858 139708415854336 logging_writer.py:48] [173600] global_step=173600, grad_norm=2.9238624572753906, loss=1.1313064098358154
I0309 04:19:30.483503 139708407461632 logging_writer.py:48] [173700] global_step=173700, grad_norm=3.124957799911499, loss=2.243561029434204
I0309 04:20:15.778381 139708415854336 logging_writer.py:48] [173800] global_step=173800, grad_norm=3.7851734161376953, loss=3.378286838531494
I0309 04:21:01.186725 139708407461632 logging_writer.py:48] [173900] global_step=173900, grad_norm=3.2579240798950195, loss=1.125759482383728
I0309 04:21:29.137035 139902746892096 spec.py:321] Evaluating on the training split.
I0309 04:21:40.607439 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 04:22:02.855466 139902746892096 spec.py:349] Evaluating on the test split.
I0309 04:22:04.530011 139902746892096 submission_runner.py:411] Time since start: 84047.56s, 	Step: 173963, 	{'train/accuracy': 0.8810155987739563, 'train/loss': 0.43936729431152344, 'validation/accuracy': 0.7788999676704407, 'validation/loss': 0.8695529699325562, 'validation/num_examples': 50000, 'test/accuracy': 0.6599000096321106, 'test/loss': 1.4833369255065918, 'test/num_examples': 10000, 'score': 77765.81756949425, 'total_duration': 84047.55881023407, 'accumulated_submission_time': 77765.81756949425, 'accumulated_eval_time': 6263.508965969086, 'accumulated_logging_time': 9.05425500869751}
I0309 04:22:04.573894 139708415854336 logging_writer.py:48] [173963] accumulated_eval_time=6263.508966, accumulated_logging_time=9.054255, accumulated_submission_time=77765.817569, global_step=173963, preemption_count=0, score=77765.817569, test/accuracy=0.659900, test/loss=1.483337, test/num_examples=10000, total_duration=84047.558810, train/accuracy=0.881016, train/loss=0.439367, validation/accuracy=0.778900, validation/loss=0.869553, validation/num_examples=50000
I0309 04:22:19.521349 139708407461632 logging_writer.py:48] [174000] global_step=174000, grad_norm=2.7403085231781006, loss=1.5272612571716309
I0309 04:23:01.811037 139708415854336 logging_writer.py:48] [174100] global_step=174100, grad_norm=2.908966064453125, loss=1.170494556427002
I0309 04:23:47.235206 139708407461632 logging_writer.py:48] [174200] global_step=174200, grad_norm=3.0569088459014893, loss=1.5130751132965088
I0309 04:24:32.635905 139708415854336 logging_writer.py:48] [174300] global_step=174300, grad_norm=3.087526798248291, loss=1.192326545715332
I0309 04:25:17.935657 139708407461632 logging_writer.py:48] [174400] global_step=174400, grad_norm=3.0724992752075195, loss=2.5725088119506836
I0309 04:26:03.243100 139708415854336 logging_writer.py:48] [174500] global_step=174500, grad_norm=2.8006410598754883, loss=1.1465696096420288
I0309 04:26:48.830080 139708407461632 logging_writer.py:48] [174600] global_step=174600, grad_norm=2.964181900024414, loss=2.555438756942749
I0309 04:27:34.233670 139708415854336 logging_writer.py:48] [174700] global_step=174700, grad_norm=3.0939180850982666, loss=1.1089634895324707
I0309 04:28:19.700073 139708407461632 logging_writer.py:48] [174800] global_step=174800, grad_norm=2.901425361633301, loss=1.071986436843872
I0309 04:29:04.891164 139708415854336 logging_writer.py:48] [174900] global_step=174900, grad_norm=2.889862537384033, loss=1.301817536354065
I0309 04:29:04.905846 139902746892096 spec.py:321] Evaluating on the training split.
I0309 04:29:16.288302 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 04:29:37.829944 139902746892096 spec.py:349] Evaluating on the test split.
I0309 04:29:39.509233 139902746892096 submission_runner.py:411] Time since start: 84502.54s, 	Step: 174901, 	{'train/accuracy': 0.8854491710662842, 'train/loss': 0.4333232641220093, 'validation/accuracy': 0.7795999646186829, 'validation/loss': 0.8688917756080627, 'validation/num_examples': 50000, 'test/accuracy': 0.6640000343322754, 'test/loss': 1.4762301445007324, 'test/num_examples': 10000, 'score': 78186.09090733528, 'total_duration': 84502.53801631927, 'accumulated_submission_time': 78186.09090733528, 'accumulated_eval_time': 6298.112339735031, 'accumulated_logging_time': 9.10762882232666}
I0309 04:29:39.558156 139708407461632 logging_writer.py:48] [174901] accumulated_eval_time=6298.112340, accumulated_logging_time=9.107629, accumulated_submission_time=78186.090907, global_step=174901, preemption_count=0, score=78186.090907, test/accuracy=0.664000, test/loss=1.476230, test/num_examples=10000, total_duration=84502.538016, train/accuracy=0.885449, train/loss=0.433323, validation/accuracy=0.779600, validation/loss=0.868892, validation/num_examples=50000
I0309 04:30:19.346666 139708415854336 logging_writer.py:48] [175000] global_step=175000, grad_norm=3.850891590118408, loss=3.2776708602905273
I0309 04:31:04.384816 139708407461632 logging_writer.py:48] [175100] global_step=175100, grad_norm=2.7753286361694336, loss=1.3258397579193115
I0309 04:31:49.994884 139708415854336 logging_writer.py:48] [175200] global_step=175200, grad_norm=3.000411033630371, loss=1.2064355611801147
I0309 04:32:35.404664 139708407461632 logging_writer.py:48] [175300] global_step=175300, grad_norm=3.0486576557159424, loss=1.2435266971588135
I0309 04:33:20.716605 139708415854336 logging_writer.py:48] [175400] global_step=175400, grad_norm=2.853158712387085, loss=2.3290750980377197
I0309 04:34:05.963884 139708407461632 logging_writer.py:48] [175500] global_step=175500, grad_norm=3.0267417430877686, loss=1.1390384435653687
I0309 04:34:51.043445 139708415854336 logging_writer.py:48] [175600] global_step=175600, grad_norm=3.043473720550537, loss=1.8876087665557861
I0309 04:35:36.186755 139708407461632 logging_writer.py:48] [175700] global_step=175700, grad_norm=3.160428524017334, loss=1.0428955554962158
I0309 04:36:21.434598 139708415854336 logging_writer.py:48] [175800] global_step=175800, grad_norm=3.0207414627075195, loss=1.224968671798706
I0309 04:36:39.692994 139902746892096 spec.py:321] Evaluating on the training split.
I0309 04:36:51.136346 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 04:37:11.454944 139902746892096 spec.py:349] Evaluating on the test split.
I0309 04:37:13.133207 139902746892096 submission_runner.py:411] Time since start: 84956.16s, 	Step: 175842, 	{'train/accuracy': 0.88539057970047, 'train/loss': 0.4322720766067505, 'validation/accuracy': 0.7794599533081055, 'validation/loss': 0.866097629070282, 'validation/num_examples': 50000, 'test/accuracy': 0.6651000380516052, 'test/loss': 1.4696331024169922, 'test/num_examples': 10000, 'score': 78606.16627883911, 'total_duration': 84956.16199755669, 'accumulated_submission_time': 78606.16627883911, 'accumulated_eval_time': 6331.55254650116, 'accumulated_logging_time': 9.166329622268677}
I0309 04:37:13.184120 139708407461632 logging_writer.py:48] [175842] accumulated_eval_time=6331.552547, accumulated_logging_time=9.166330, accumulated_submission_time=78606.166279, global_step=175842, preemption_count=0, score=78606.166279, test/accuracy=0.665100, test/loss=1.469633, test/num_examples=10000, total_duration=84956.161998, train/accuracy=0.885391, train/loss=0.432272, validation/accuracy=0.779460, validation/loss=0.866098, validation/num_examples=50000
I0309 04:37:36.425325 139708415854336 logging_writer.py:48] [175900] global_step=175900, grad_norm=2.8164772987365723, loss=2.1948797702789307
I0309 04:38:20.019181 139708407461632 logging_writer.py:48] [176000] global_step=176000, grad_norm=3.042097330093384, loss=2.2031733989715576
I0309 04:39:05.238832 139708415854336 logging_writer.py:48] [176100] global_step=176100, grad_norm=3.0245816707611084, loss=1.3441016674041748
I0309 04:39:50.748538 139708407461632 logging_writer.py:48] [176200] global_step=176200, grad_norm=3.0949060916900635, loss=1.1946403980255127
I0309 04:40:36.152139 139708415854336 logging_writer.py:48] [176300] global_step=176300, grad_norm=3.035987615585327, loss=1.29677414894104
I0309 04:41:21.587305 139708407461632 logging_writer.py:48] [176400] global_step=176400, grad_norm=3.207000494003296, loss=1.1521883010864258
I0309 04:42:07.078972 139708415854336 logging_writer.py:48] [176500] global_step=176500, grad_norm=3.4648959636688232, loss=2.4043753147125244
I0309 04:42:52.298783 139708407461632 logging_writer.py:48] [176600] global_step=176600, grad_norm=3.8976523876190186, loss=3.3396825790405273
I0309 04:43:37.687066 139708415854336 logging_writer.py:48] [176700] global_step=176700, grad_norm=3.17903208732605, loss=2.1546108722686768
I0309 04:44:13.202974 139902746892096 spec.py:321] Evaluating on the training split.
I0309 04:44:24.763272 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 04:44:46.929347 139902746892096 spec.py:349] Evaluating on the test split.
I0309 04:44:48.594992 139902746892096 submission_runner.py:411] Time since start: 85411.62s, 	Step: 176780, 	{'train/accuracy': 0.8858593702316284, 'train/loss': 0.424483984708786, 'validation/accuracy': 0.780019998550415, 'validation/loss': 0.8592524528503418, 'validation/num_examples': 50000, 'test/accuracy': 0.6630000472068787, 'test/loss': 1.4692275524139404, 'test/num_examples': 10000, 'score': 79026.12602353096, 'total_duration': 85411.62379169464, 'accumulated_submission_time': 79026.12602353096, 'accumulated_eval_time': 6366.944556713104, 'accumulated_logging_time': 9.227767944335938}
I0309 04:44:48.638800 139708407461632 logging_writer.py:48] [176780] accumulated_eval_time=6366.944557, accumulated_logging_time=9.227768, accumulated_submission_time=79026.126024, global_step=176780, preemption_count=0, score=79026.126024, test/accuracy=0.663000, test/loss=1.469228, test/num_examples=10000, total_duration=85411.623792, train/accuracy=0.885859, train/loss=0.424484, validation/accuracy=0.780020, validation/loss=0.859252, validation/num_examples=50000
I0309 04:44:56.899371 139708415854336 logging_writer.py:48] [176800] global_step=176800, grad_norm=2.9863860607147217, loss=2.1673765182495117
I0309 04:45:38.110794 139708407461632 logging_writer.py:48] [176900] global_step=176900, grad_norm=3.619706869125366, loss=1.1014208793640137
I0309 04:46:23.406631 139708415854336 logging_writer.py:48] [177000] global_step=177000, grad_norm=3.1732773780822754, loss=1.0994588136672974
I0309 04:47:08.915839 139708407461632 logging_writer.py:48] [177100] global_step=177100, grad_norm=2.9140830039978027, loss=1.1224079132080078
I0309 04:47:54.101717 139708415854336 logging_writer.py:48] [177200] global_step=177200, grad_norm=3.2053439617156982, loss=2.000918388366699
I0309 04:48:39.389697 139708407461632 logging_writer.py:48] [177300] global_step=177300, grad_norm=3.1301121711730957, loss=1.1786803007125854
I0309 04:49:24.738895 139708415854336 logging_writer.py:48] [177400] global_step=177400, grad_norm=3.3240103721618652, loss=3.0020041465759277
I0309 04:50:09.963888 139708407461632 logging_writer.py:48] [177500] global_step=177500, grad_norm=3.665707588195801, loss=3.1890602111816406
I0309 04:50:55.061459 139708415854336 logging_writer.py:48] [177600] global_step=177600, grad_norm=2.932499885559082, loss=1.0842446088790894
I0309 04:51:40.737077 139708407461632 logging_writer.py:48] [177700] global_step=177700, grad_norm=3.105112314224243, loss=1.059610366821289
I0309 04:51:48.610710 139902746892096 spec.py:321] Evaluating on the training split.
I0309 04:51:59.918418 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 04:52:21.679346 139902746892096 spec.py:349] Evaluating on the test split.
I0309 04:52:23.355493 139902746892096 submission_runner.py:411] Time since start: 85866.38s, 	Step: 177719, 	{'train/accuracy': 0.8868359327316284, 'train/loss': 0.4214153289794922, 'validation/accuracy': 0.7813000082969666, 'validation/loss': 0.8596096634864807, 'validation/num_examples': 50000, 'test/accuracy': 0.6668000221252441, 'test/loss': 1.4655801057815552, 'test/num_examples': 10000, 'score': 79446.03746342659, 'total_duration': 85866.38427376747, 'accumulated_submission_time': 79446.03746342659, 'accumulated_eval_time': 6401.689318180084, 'accumulated_logging_time': 9.282610654830933}
I0309 04:52:23.403638 139708415854336 logging_writer.py:48] [177719] accumulated_eval_time=6401.689318, accumulated_logging_time=9.282611, accumulated_submission_time=79446.037463, global_step=177719, preemption_count=0, score=79446.037463, test/accuracy=0.666800, test/loss=1.465580, test/num_examples=10000, total_duration=85866.384274, train/accuracy=0.886836, train/loss=0.421415, validation/accuracy=0.781300, validation/loss=0.859610, validation/num_examples=50000
I0309 04:52:55.656390 139708407461632 logging_writer.py:48] [177800] global_step=177800, grad_norm=4.205134868621826, loss=3.328674793243408
I0309 04:53:40.427800 139708415854336 logging_writer.py:48] [177900] global_step=177900, grad_norm=2.769029378890991, loss=2.0878334045410156
I0309 04:54:25.595422 139708407461632 logging_writer.py:48] [178000] global_step=178000, grad_norm=3.1020925045013428, loss=1.6817885637283325
I0309 04:55:10.741245 139708415854336 logging_writer.py:48] [178100] global_step=178100, grad_norm=3.3733205795288086, loss=3.0033740997314453
I0309 04:55:56.083035 139708407461632 logging_writer.py:48] [178200] global_step=178200, grad_norm=2.975210428237915, loss=1.2254071235656738
I0309 04:56:41.234354 139708415854336 logging_writer.py:48] [178300] global_step=178300, grad_norm=4.229214668273926, loss=3.2210652828216553
I0309 04:57:26.461133 139708407461632 logging_writer.py:48] [178400] global_step=178400, grad_norm=2.8257155418395996, loss=1.439313292503357
I0309 04:58:11.791019 139708415854336 logging_writer.py:48] [178500] global_step=178500, grad_norm=3.3493402004241943, loss=3.068641185760498
I0309 04:58:56.699548 139708407461632 logging_writer.py:48] [178600] global_step=178600, grad_norm=3.071293354034424, loss=1.0923014879226685
I0309 04:59:23.442411 139902746892096 spec.py:321] Evaluating on the training split.
I0309 04:59:34.844842 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 04:59:56.074387 139902746892096 spec.py:349] Evaluating on the test split.
I0309 04:59:57.752003 139902746892096 submission_runner.py:411] Time since start: 86320.78s, 	Step: 178661, 	{'train/accuracy': 0.88685542345047, 'train/loss': 0.419357031583786, 'validation/accuracy': 0.7822799682617188, 'validation/loss': 0.8534312844276428, 'validation/num_examples': 50000, 'test/accuracy': 0.664900004863739, 'test/loss': 1.4604169130325317, 'test/num_examples': 10000, 'score': 79866.01587152481, 'total_duration': 86320.78079080582, 'accumulated_submission_time': 79866.01587152481, 'accumulated_eval_time': 6435.998902320862, 'accumulated_logging_time': 9.340909719467163}
I0309 04:59:57.796403 139708415854336 logging_writer.py:48] [178661] accumulated_eval_time=6435.998902, accumulated_logging_time=9.340910, accumulated_submission_time=79866.015872, global_step=178661, preemption_count=0, score=79866.015872, test/accuracy=0.664900, test/loss=1.460417, test/num_examples=10000, total_duration=86320.780791, train/accuracy=0.886855, train/loss=0.419357, validation/accuracy=0.782280, validation/loss=0.853431, validation/num_examples=50000
I0309 05:00:13.538344 139708407461632 logging_writer.py:48] [178700] global_step=178700, grad_norm=3.071340560913086, loss=1.1876052618026733
I0309 05:00:55.988955 139708415854336 logging_writer.py:48] [178800] global_step=178800, grad_norm=3.367342710494995, loss=1.2615368366241455
I0309 05:01:41.582342 139708407461632 logging_writer.py:48] [178900] global_step=178900, grad_norm=3.259561538696289, loss=1.4979907274246216
I0309 05:02:27.105858 139708415854336 logging_writer.py:48] [179000] global_step=179000, grad_norm=3.3951823711395264, loss=2.881023645401001
I0309 05:03:12.239315 139708407461632 logging_writer.py:48] [179100] global_step=179100, grad_norm=3.080049753189087, loss=2.44006609916687
I0309 05:03:57.621551 139708415854336 logging_writer.py:48] [179200] global_step=179200, grad_norm=3.1813254356384277, loss=1.801443099975586
I0309 05:04:42.782738 139708407461632 logging_writer.py:48] [179300] global_step=179300, grad_norm=3.025637149810791, loss=2.6424989700317383
I0309 05:05:28.181156 139708415854336 logging_writer.py:48] [179400] global_step=179400, grad_norm=4.0890350341796875, loss=3.2039403915405273
I0309 05:06:13.711024 139708407461632 logging_writer.py:48] [179500] global_step=179500, grad_norm=3.044072389602661, loss=1.3716650009155273
I0309 05:06:57.770409 139902746892096 spec.py:321] Evaluating on the training split.
I0309 05:07:08.976114 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 05:07:30.384936 139902746892096 spec.py:349] Evaluating on the test split.
I0309 05:07:32.078359 139902746892096 submission_runner.py:411] Time since start: 86775.11s, 	Step: 179599, 	{'train/accuracy': 0.8860741853713989, 'train/loss': 0.4188991189002991, 'validation/accuracy': 0.781059980392456, 'validation/loss': 0.8571061491966248, 'validation/num_examples': 50000, 'test/accuracy': 0.6645000576972961, 'test/loss': 1.4622553586959839, 'test/num_examples': 10000, 'score': 80285.93200969696, 'total_duration': 86775.10715174675, 'accumulated_submission_time': 80285.93200969696, 'accumulated_eval_time': 6470.306844234467, 'accumulated_logging_time': 9.394191026687622}
I0309 05:07:32.121363 139708415854336 logging_writer.py:48] [179599] accumulated_eval_time=6470.306844, accumulated_logging_time=9.394191, accumulated_submission_time=80285.932010, global_step=179599, preemption_count=0, score=80285.932010, test/accuracy=0.664500, test/loss=1.462255, test/num_examples=10000, total_duration=86775.107152, train/accuracy=0.886074, train/loss=0.418899, validation/accuracy=0.781060, validation/loss=0.857106, validation/num_examples=50000
I0309 05:07:32.913196 139708407461632 logging_writer.py:48] [179600] global_step=179600, grad_norm=3.6311209201812744, loss=3.0317890644073486
I0309 05:08:13.149731 139708415854336 logging_writer.py:48] [179700] global_step=179700, grad_norm=3.0504558086395264, loss=2.688102960586548
I0309 05:08:58.259350 139708407461632 logging_writer.py:48] [179800] global_step=179800, grad_norm=3.750110387802124, loss=3.2254159450531006
I0309 05:09:43.484781 139708415854336 logging_writer.py:48] [179900] global_step=179900, grad_norm=3.146162271499634, loss=1.1180189847946167
I0309 05:10:28.859616 139708407461632 logging_writer.py:48] [180000] global_step=180000, grad_norm=3.501725435256958, loss=3.1520519256591797
I0309 05:11:14.107931 139708415854336 logging_writer.py:48] [180100] global_step=180100, grad_norm=3.122821092605591, loss=1.1539751291275024
I0309 05:11:59.726152 139708407461632 logging_writer.py:48] [180200] global_step=180200, grad_norm=3.034701108932495, loss=2.1043996810913086
I0309 05:12:45.178611 139708415854336 logging_writer.py:48] [180300] global_step=180300, grad_norm=2.957660436630249, loss=1.014631748199463
I0309 05:13:30.446990 139708407461632 logging_writer.py:48] [180400] global_step=180400, grad_norm=3.1174652576446533, loss=2.5710225105285645
I0309 05:14:16.024506 139708415854336 logging_writer.py:48] [180500] global_step=180500, grad_norm=3.044670343399048, loss=2.4173331260681152
I0309 05:14:32.079344 139902746892096 spec.py:321] Evaluating on the training split.
I0309 05:14:43.466275 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 05:15:06.566802 139902746892096 spec.py:349] Evaluating on the test split.
I0309 05:15:08.247645 139902746892096 submission_runner.py:411] Time since start: 87231.28s, 	Step: 180537, 	{'train/accuracy': 0.8853710889816284, 'train/loss': 0.42323553562164307, 'validation/accuracy': 0.7822999954223633, 'validation/loss': 0.856580913066864, 'validation/num_examples': 50000, 'test/accuracy': 0.6633000373840332, 'test/loss': 1.465488314628601, 'test/num_examples': 10000, 'score': 80705.8322134018, 'total_duration': 87231.2764441967, 'accumulated_submission_time': 80705.8322134018, 'accumulated_eval_time': 6506.475147724152, 'accumulated_logging_time': 9.446362257003784}
I0309 05:15:08.289886 139708407461632 logging_writer.py:48] [180537] accumulated_eval_time=6506.475148, accumulated_logging_time=9.446362, accumulated_submission_time=80705.832213, global_step=180537, preemption_count=0, score=80705.832213, test/accuracy=0.663300, test/loss=1.465488, test/num_examples=10000, total_duration=87231.276444, train/accuracy=0.885371, train/loss=0.423236, validation/accuracy=0.782300, validation/loss=0.856581, validation/num_examples=50000
I0309 05:15:33.477590 139708415854336 logging_writer.py:48] [180600] global_step=180600, grad_norm=2.7750377655029297, loss=1.5311877727508545
I0309 05:16:16.740562 139708407461632 logging_writer.py:48] [180700] global_step=180700, grad_norm=3.4508984088897705, loss=1.1438772678375244
I0309 05:17:02.176231 139708415854336 logging_writer.py:48] [180800] global_step=180800, grad_norm=3.0530202388763428, loss=1.1894181966781616
I0309 05:17:48.056644 139708407461632 logging_writer.py:48] [180900] global_step=180900, grad_norm=3.349942684173584, loss=2.2962610721588135
I0309 05:18:33.299689 139708415854336 logging_writer.py:48] [181000] global_step=181000, grad_norm=3.1692488193511963, loss=1.2638580799102783
I0309 05:19:18.468964 139708407461632 logging_writer.py:48] [181100] global_step=181100, grad_norm=3.8615925312042236, loss=3.130091428756714
I0309 05:20:03.945629 139708415854336 logging_writer.py:48] [181200] global_step=181200, grad_norm=3.101233720779419, loss=1.0959625244140625
I0309 05:20:49.224350 139708407461632 logging_writer.py:48] [181300] global_step=181300, grad_norm=3.316685676574707, loss=2.341378927230835
I0309 05:21:34.865108 139708415854336 logging_writer.py:48] [181400] global_step=181400, grad_norm=4.252825736999512, loss=3.0648844242095947
I0309 05:22:08.431008 139902746892096 spec.py:321] Evaluating on the training split.
I0309 05:22:19.651248 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 05:22:41.053230 139902746892096 spec.py:349] Evaluating on the test split.
I0309 05:22:42.728853 139902746892096 submission_runner.py:411] Time since start: 87685.76s, 	Step: 181476, 	{'train/accuracy': 0.8878124952316284, 'train/loss': 0.4163694381713867, 'validation/accuracy': 0.7823799848556519, 'validation/loss': 0.8540942072868347, 'validation/num_examples': 50000, 'test/accuracy': 0.664900004863739, 'test/loss': 1.4613149166107178, 'test/num_examples': 10000, 'score': 81125.91492772102, 'total_duration': 87685.75764489174, 'accumulated_submission_time': 81125.91492772102, 'accumulated_eval_time': 6540.772976398468, 'accumulated_logging_time': 9.497935771942139}
I0309 05:22:42.771024 139708407461632 logging_writer.py:48] [181476] accumulated_eval_time=6540.772976, accumulated_logging_time=9.497936, accumulated_submission_time=81125.914928, global_step=181476, preemption_count=0, score=81125.914928, test/accuracy=0.664900, test/loss=1.461315, test/num_examples=10000, total_duration=87685.757645, train/accuracy=0.887812, train/loss=0.416369, validation/accuracy=0.782380, validation/loss=0.854094, validation/num_examples=50000
I0309 05:22:52.604665 139708415854336 logging_writer.py:48] [181500] global_step=181500, grad_norm=3.348240375518799, loss=1.3414030075073242
I0309 05:23:34.208081 139708407461632 logging_writer.py:48] [181600] global_step=181600, grad_norm=4.037507057189941, loss=3.219886302947998
I0309 05:24:19.379312 139708415854336 logging_writer.py:48] [181700] global_step=181700, grad_norm=3.196317672729492, loss=1.2930340766906738
I0309 05:25:04.916283 139708407461632 logging_writer.py:48] [181800] global_step=181800, grad_norm=3.073875665664673, loss=1.1374444961547852
I0309 05:25:50.296103 139708415854336 logging_writer.py:48] [181900] global_step=181900, grad_norm=2.9501419067382812, loss=1.791198492050171
I0309 05:26:35.744604 139708407461632 logging_writer.py:48] [182000] global_step=182000, grad_norm=2.9334871768951416, loss=2.298862934112549
I0309 05:27:20.975687 139708415854336 logging_writer.py:48] [182100] global_step=182100, grad_norm=2.922919273376465, loss=1.0420373678207397
I0309 05:28:06.158696 139708407461632 logging_writer.py:48] [182200] global_step=182200, grad_norm=2.9728140830993652, loss=1.6351234912872314
I0309 05:28:51.533121 139708415854336 logging_writer.py:48] [182300] global_step=182300, grad_norm=3.0023274421691895, loss=1.7869739532470703
I0309 05:29:36.769817 139708407461632 logging_writer.py:48] [182400] global_step=182400, grad_norm=3.101832628250122, loss=1.639235019683838
I0309 05:29:42.788810 139902746892096 spec.py:321] Evaluating on the training split.
I0309 05:29:54.329306 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 05:30:16.707553 139902746892096 spec.py:349] Evaluating on the test split.
I0309 05:30:18.386306 139902746892096 submission_runner.py:411] Time since start: 88141.42s, 	Step: 182415, 	{'train/accuracy': 0.8870312571525574, 'train/loss': 0.4193125069141388, 'validation/accuracy': 0.7821399569511414, 'validation/loss': 0.8543209433555603, 'validation/num_examples': 50000, 'test/accuracy': 0.6657000184059143, 'test/loss': 1.4587408304214478, 'test/num_examples': 10000, 'score': 81545.87590956688, 'total_duration': 88141.41509056091, 'accumulated_submission_time': 81545.87590956688, 'accumulated_eval_time': 6576.370446205139, 'accumulated_logging_time': 9.548835754394531}
I0309 05:30:18.433105 139708415854336 logging_writer.py:48] [182415] accumulated_eval_time=6576.370446, accumulated_logging_time=9.548836, accumulated_submission_time=81545.875910, global_step=182415, preemption_count=0, score=81545.875910, test/accuracy=0.665700, test/loss=1.458741, test/num_examples=10000, total_duration=88141.415091, train/accuracy=0.887031, train/loss=0.419313, validation/accuracy=0.782140, validation/loss=0.854321, validation/num_examples=50000
I0309 05:30:52.264578 139708407461632 logging_writer.py:48] [182500] global_step=182500, grad_norm=3.018716335296631, loss=2.4755566120147705
I0309 05:31:36.972468 139708415854336 logging_writer.py:48] [182600] global_step=182600, grad_norm=2.989264726638794, loss=2.4613161087036133
I0309 05:32:22.637136 139708407461632 logging_writer.py:48] [182700] global_step=182700, grad_norm=3.277480363845825, loss=1.0313130617141724
I0309 05:33:08.227272 139708415854336 logging_writer.py:48] [182800] global_step=182800, grad_norm=3.650650978088379, loss=3.1220312118530273
I0309 05:33:53.444787 139708407461632 logging_writer.py:48] [182900] global_step=182900, grad_norm=3.089127540588379, loss=1.0873669385910034
I0309 05:34:38.800041 139708415854336 logging_writer.py:48] [183000] global_step=183000, grad_norm=3.063498020172119, loss=1.069314956665039
I0309 05:35:23.780826 139708407461632 logging_writer.py:48] [183100] global_step=183100, grad_norm=2.8723738193511963, loss=1.1953922510147095
I0309 05:36:09.089232 139708415854336 logging_writer.py:48] [183200] global_step=183200, grad_norm=3.2149980068206787, loss=1.081305980682373
I0309 05:36:54.776159 139708407461632 logging_writer.py:48] [183300] global_step=183300, grad_norm=2.7523980140686035, loss=1.4780864715576172
I0309 05:37:18.569666 139902746892096 spec.py:321] Evaluating on the training split.
I0309 05:37:29.909453 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 05:37:50.645633 139902746892096 spec.py:349] Evaluating on the test split.
I0309 05:37:52.324398 139902746892096 submission_runner.py:411] Time since start: 88595.35s, 	Step: 183355, 	{'train/accuracy': 0.8894140720367432, 'train/loss': 0.4123246669769287, 'validation/accuracy': 0.7827000021934509, 'validation/loss': 0.8526468276977539, 'validation/num_examples': 50000, 'test/accuracy': 0.6669000387191772, 'test/loss': 1.458864688873291, 'test/num_examples': 10000, 'score': 81965.95393848419, 'total_duration': 88595.35318779945, 'accumulated_submission_time': 81965.95393848419, 'accumulated_eval_time': 6610.12516784668, 'accumulated_logging_time': 9.60527515411377}
I0309 05:37:52.382415 139708415854336 logging_writer.py:48] [183355] accumulated_eval_time=6610.125168, accumulated_logging_time=9.605275, accumulated_submission_time=81965.953938, global_step=183355, preemption_count=0, score=81965.953938, test/accuracy=0.666900, test/loss=1.458865, test/num_examples=10000, total_duration=88595.353188, train/accuracy=0.889414, train/loss=0.412325, validation/accuracy=0.782700, validation/loss=0.852647, validation/num_examples=50000
I0309 05:38:10.479017 139708407461632 logging_writer.py:48] [183400] global_step=183400, grad_norm=3.431018590927124, loss=1.2005565166473389
I0309 05:38:53.187347 139708415854336 logging_writer.py:48] [183500] global_step=183500, grad_norm=3.3982810974121094, loss=2.6353254318237305
I0309 05:39:38.247617 139708407461632 logging_writer.py:48] [183600] global_step=183600, grad_norm=3.0230836868286133, loss=1.1553548574447632
I0309 05:40:23.679537 139708415854336 logging_writer.py:48] [183700] global_step=183700, grad_norm=2.995201349258423, loss=1.500422716140747
I0309 05:41:08.931303 139708407461632 logging_writer.py:48] [183800] global_step=183800, grad_norm=3.022801160812378, loss=1.0946279764175415
I0309 05:41:54.314908 139708415854336 logging_writer.py:48] [183900] global_step=183900, grad_norm=2.9467623233795166, loss=1.328319787979126
I0309 05:42:39.917396 139708407461632 logging_writer.py:48] [184000] global_step=184000, grad_norm=2.7302937507629395, loss=1.1962602138519287
I0309 05:43:25.267209 139708415854336 logging_writer.py:48] [184100] global_step=184100, grad_norm=3.457515001296997, loss=1.2110577821731567
I0309 05:44:10.783471 139708407461632 logging_writer.py:48] [184200] global_step=184200, grad_norm=3.1475398540496826, loss=1.102816104888916
I0309 05:44:52.711541 139902746892096 spec.py:321] Evaluating on the training split.
I0309 05:45:04.247714 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 05:45:27.608166 139902746892096 spec.py:349] Evaluating on the test split.
I0309 05:45:29.287686 139902746892096 submission_runner.py:411] Time since start: 89052.32s, 	Step: 184294, 	{'train/accuracy': 0.8896093368530273, 'train/loss': 0.412265419960022, 'validation/accuracy': 0.7827000021934509, 'validation/loss': 0.8530824184417725, 'validation/num_examples': 50000, 'test/accuracy': 0.6668000221252441, 'test/loss': 1.4599723815917969, 'test/num_examples': 10000, 'score': 82386.22452759743, 'total_duration': 89052.31648516655, 'accumulated_submission_time': 82386.22452759743, 'accumulated_eval_time': 6646.70134973526, 'accumulated_logging_time': 9.67266058921814}
I0309 05:45:29.332321 139708415854336 logging_writer.py:48] [184294] accumulated_eval_time=6646.701350, accumulated_logging_time=9.672661, accumulated_submission_time=82386.224528, global_step=184294, preemption_count=0, score=82386.224528, test/accuracy=0.666800, test/loss=1.459972, test/num_examples=10000, total_duration=89052.316485, train/accuracy=0.889609, train/loss=0.412265, validation/accuracy=0.782700, validation/loss=0.853082, validation/num_examples=50000
I0309 05:45:32.085070 139708407461632 logging_writer.py:48] [184300] global_step=184300, grad_norm=2.931650161743164, loss=1.0211052894592285
I0309 05:46:12.706462 139708415854336 logging_writer.py:48] [184400] global_step=184400, grad_norm=3.1024258136749268, loss=1.4280344247817993
I0309 05:46:57.937173 139708407461632 logging_writer.py:48] [184500] global_step=184500, grad_norm=3.0717740058898926, loss=2.219738483428955
I0309 05:47:43.555509 139708415854336 logging_writer.py:48] [184600] global_step=184600, grad_norm=3.1891233921051025, loss=0.9903405904769897
I0309 05:48:28.981556 139708407461632 logging_writer.py:48] [184700] global_step=184700, grad_norm=3.2554333209991455, loss=1.4368362426757812
I0309 05:49:14.305887 139708415854336 logging_writer.py:48] [184800] global_step=184800, grad_norm=3.020289421081543, loss=1.079466700553894
I0309 05:49:59.953306 139708407461632 logging_writer.py:48] [184900] global_step=184900, grad_norm=3.4097964763641357, loss=1.2860554456710815
I0309 05:50:45.414292 139708415854336 logging_writer.py:48] [185000] global_step=185000, grad_norm=3.12542986869812, loss=1.3384051322937012
I0309 05:51:30.616083 139708407461632 logging_writer.py:48] [185100] global_step=185100, grad_norm=3.025580644607544, loss=1.0778021812438965
I0309 05:52:16.355392 139708415854336 logging_writer.py:48] [185200] global_step=185200, grad_norm=3.2147674560546875, loss=1.1675498485565186
I0309 05:52:29.755141 139902746892096 spec.py:321] Evaluating on the training split.
I0309 05:52:41.112074 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 05:53:03.608050 139902746892096 spec.py:349] Evaluating on the test split.
I0309 05:53:05.277813 139902746892096 submission_runner.py:411] Time since start: 89508.31s, 	Step: 185231, 	{'train/accuracy': 0.8886132836341858, 'train/loss': 0.4169757664203644, 'validation/accuracy': 0.7825799584388733, 'validation/loss': 0.8518304228782654, 'validation/num_examples': 50000, 'test/accuracy': 0.6669000387191772, 'test/loss': 1.4578466415405273, 'test/num_examples': 10000, 'score': 82806.58885216713, 'total_duration': 89508.3066072464, 'accumulated_submission_time': 82806.58885216713, 'accumulated_eval_time': 6682.224026918411, 'accumulated_logging_time': 9.726844072341919}
I0309 05:53:05.323246 139708407461632 logging_writer.py:48] [185231] accumulated_eval_time=6682.224027, accumulated_logging_time=9.726844, accumulated_submission_time=82806.588852, global_step=185231, preemption_count=0, score=82806.588852, test/accuracy=0.666900, test/loss=1.457847, test/num_examples=10000, total_duration=89508.306607, train/accuracy=0.888613, train/loss=0.416976, validation/accuracy=0.782580, validation/loss=0.851830, validation/num_examples=50000
I0309 05:53:32.870871 139708415854336 logging_writer.py:48] [185300] global_step=185300, grad_norm=3.3715946674346924, loss=2.5161612033843994
I0309 05:54:16.737207 139708407461632 logging_writer.py:48] [185400] global_step=185400, grad_norm=3.9973464012145996, loss=3.190544366836548
I0309 05:55:02.332162 139708415854336 logging_writer.py:48] [185500] global_step=185500, grad_norm=3.5532355308532715, loss=1.0591751337051392
I0309 05:55:48.174005 139708407461632 logging_writer.py:48] [185600] global_step=185600, grad_norm=4.252786159515381, loss=3.212526321411133
I0309 05:56:33.397706 139708415854336 logging_writer.py:48] [185700] global_step=185700, grad_norm=3.143279790878296, loss=2.501293897628784
I0309 05:57:18.980659 139708407461632 logging_writer.py:48] [185800] global_step=185800, grad_norm=3.5175557136535645, loss=1.172000765800476
I0309 05:58:04.609163 139708415854336 logging_writer.py:48] [185900] global_step=185900, grad_norm=3.2322001457214355, loss=1.0962507724761963
I0309 05:58:49.839799 139708407461632 logging_writer.py:48] [186000] global_step=186000, grad_norm=2.8203036785125732, loss=2.1399574279785156
I0309 05:59:35.025981 139708415854336 logging_writer.py:48] [186100] global_step=186100, grad_norm=3.0184507369995117, loss=1.1165878772735596
I0309 06:00:05.602164 139902746892096 spec.py:321] Evaluating on the training split.
I0309 06:00:17.156016 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 06:00:37.840220 139902746892096 spec.py:349] Evaluating on the test split.
I0309 06:00:39.524574 139902746892096 submission_runner.py:411] Time since start: 89962.55s, 	Step: 186169, 	{'train/accuracy': 0.8907030820846558, 'train/loss': 0.4041774272918701, 'validation/accuracy': 0.7828999757766724, 'validation/loss': 0.8518894910812378, 'validation/num_examples': 50000, 'test/accuracy': 0.6664000153541565, 'test/loss': 1.4582322835922241, 'test/num_examples': 10000, 'score': 83226.80952954292, 'total_duration': 89962.55336284637, 'accumulated_submission_time': 83226.80952954292, 'accumulated_eval_time': 6716.146427869797, 'accumulated_logging_time': 9.780983686447144}
I0309 06:00:39.581398 139708407461632 logging_writer.py:48] [186169] accumulated_eval_time=6716.146428, accumulated_logging_time=9.780984, accumulated_submission_time=83226.809530, global_step=186169, preemption_count=0, score=83226.809530, test/accuracy=0.666400, test/loss=1.458232, test/num_examples=10000, total_duration=89962.553363, train/accuracy=0.890703, train/loss=0.404177, validation/accuracy=0.782900, validation/loss=0.851889, validation/num_examples=50000
I0309 06:00:52.177508 139708415854336 logging_writer.py:48] [186200] global_step=186200, grad_norm=3.1542840003967285, loss=1.206561803817749
I0309 06:01:34.379412 139708407461632 logging_writer.py:48] [186300] global_step=186300, grad_norm=2.9430952072143555, loss=1.5839025974273682
I0309 06:02:19.533359 139708415854336 logging_writer.py:48] [186400] global_step=186400, grad_norm=4.457151889801025, loss=1.1792523860931396
I0309 06:03:05.668562 139708407461632 logging_writer.py:48] [186500] global_step=186500, grad_norm=3.2229206562042236, loss=1.1937741041183472
I0309 06:03:51.200740 139708415854336 logging_writer.py:48] [186600] global_step=186600, grad_norm=3.2970545291900635, loss=2.123689651489258
I0309 06:04:36.476078 139708407461632 logging_writer.py:48] [186700] global_step=186700, grad_norm=3.1113996505737305, loss=1.1071314811706543
I0309 06:05:22.247449 139708415854336 logging_writer.py:48] [186800] global_step=186800, grad_norm=3.967393398284912, loss=3.2734994888305664
I0309 06:06:07.871101 139708407461632 logging_writer.py:48] [186900] global_step=186900, grad_norm=3.012756824493408, loss=1.2439939975738525
I0309 06:06:53.245988 139708415854336 logging_writer.py:48] [187000] global_step=187000, grad_norm=2.9990904331207275, loss=1.1210954189300537
I0309 06:07:38.862531 139708407461632 logging_writer.py:48] [187100] global_step=187100, grad_norm=3.217355251312256, loss=2.786374807357788
I0309 06:07:39.907441 139902746892096 spec.py:321] Evaluating on the training split.
I0309 06:07:51.408989 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 06:08:13.909864 139902746892096 spec.py:349] Evaluating on the test split.
I0309 06:08:15.578483 139902746892096 submission_runner.py:411] Time since start: 90418.61s, 	Step: 187104, 	{'train/accuracy': 0.8879296779632568, 'train/loss': 0.4138663411140442, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 83647.07682418823, 'total_duration': 90418.60726046562, 'accumulated_submission_time': 83647.07682418823, 'accumulated_eval_time': 6751.817445039749, 'accumulated_logging_time': 9.848082780838013}
I0309 06:08:15.631811 139708415854336 logging_writer.py:48] [187104] accumulated_eval_time=6751.817445, accumulated_logging_time=9.848083, accumulated_submission_time=83647.076824, global_step=187104, preemption_count=0, score=83647.076824, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=90418.607260, train/accuracy=0.887930, train/loss=0.413866, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 06:08:54.057294 139708407461632 logging_writer.py:48] [187200] global_step=187200, grad_norm=3.15990948677063, loss=1.0132663249969482
I0309 06:09:39.159359 139708415854336 logging_writer.py:48] [187300] global_step=187300, grad_norm=2.960394859313965, loss=1.1251347064971924
I0309 06:10:24.539087 139708407461632 logging_writer.py:48] [187400] global_step=187400, grad_norm=3.3683393001556396, loss=1.275093674659729
I0309 06:11:10.200653 139708415854336 logging_writer.py:48] [187500] global_step=187500, grad_norm=2.754213571548462, loss=1.4824336767196655
I0309 06:11:55.396660 139708407461632 logging_writer.py:48] [187600] global_step=187600, grad_norm=3.085148334503174, loss=1.1897106170654297
I0309 06:12:41.424625 139708415854336 logging_writer.py:48] [187700] global_step=187700, grad_norm=3.0394275188446045, loss=1.1633294820785522
I0309 06:13:27.452214 139708407461632 logging_writer.py:48] [187800] global_step=187800, grad_norm=2.9603111743927, loss=2.537187337875366
I0309 06:14:12.947459 139708415854336 logging_writer.py:48] [187900] global_step=187900, grad_norm=3.290478467941284, loss=1.335939884185791
I0309 06:14:58.822102 139708407461632 logging_writer.py:48] [188000] global_step=188000, grad_norm=2.913536787033081, loss=2.2973668575286865
I0309 06:15:15.858449 139902746892096 spec.py:321] Evaluating on the training split.
I0309 06:15:27.233454 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 06:15:48.574372 139902746892096 spec.py:349] Evaluating on the test split.
I0309 06:15:50.253741 139902746892096 submission_runner.py:411] Time since start: 90873.28s, 	Step: 188039, 	{'train/accuracy': 0.88783198595047, 'train/loss': 0.4124012589454651, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 84067.24569582939, 'total_duration': 90873.28254199028, 'accumulated_submission_time': 84067.24569582939, 'accumulated_eval_time': 6786.212738990784, 'accumulated_logging_time': 9.911024570465088}
I0309 06:15:50.296777 139708415854336 logging_writer.py:48] [188039] accumulated_eval_time=6786.212739, accumulated_logging_time=9.911025, accumulated_submission_time=84067.245696, global_step=188039, preemption_count=0, score=84067.245696, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=90873.282542, train/accuracy=0.887832, train/loss=0.412401, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 06:16:14.678657 139708407461632 logging_writer.py:48] [188100] global_step=188100, grad_norm=3.0623300075531006, loss=1.3457324504852295
I0309 06:16:58.249149 139708415854336 logging_writer.py:48] [188200] global_step=188200, grad_norm=3.06179141998291, loss=2.692687511444092
I0309 06:17:43.789314 139708407461632 logging_writer.py:48] [188300] global_step=188300, grad_norm=2.957564115524292, loss=1.2380321025848389
I0309 06:18:29.405179 139708415854336 logging_writer.py:48] [188400] global_step=188400, grad_norm=3.227149724960327, loss=1.1437441110610962
I0309 06:19:14.583973 139708407461632 logging_writer.py:48] [188500] global_step=188500, grad_norm=3.527507781982422, loss=1.2632936239242554
I0309 06:20:00.232107 139708415854336 logging_writer.py:48] [188600] global_step=188600, grad_norm=3.399872064590454, loss=1.7620383501052856
I0309 06:20:45.772968 139708407461632 logging_writer.py:48] [188700] global_step=188700, grad_norm=2.8290984630584717, loss=1.135305404663086
I0309 06:21:31.065450 139708415854336 logging_writer.py:48] [188800] global_step=188800, grad_norm=2.9419188499450684, loss=1.1324938535690308
I0309 06:22:16.512809 139708407461632 logging_writer.py:48] [188900] global_step=188900, grad_norm=2.8855855464935303, loss=1.3749186992645264
I0309 06:22:50.371267 139902746892096 spec.py:321] Evaluating on the training split.
I0309 06:23:01.985578 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 06:23:24.896356 139902746892096 spec.py:349] Evaluating on the test split.
I0309 06:23:26.564710 139902746892096 submission_runner.py:411] Time since start: 91329.59s, 	Step: 188976, 	{'train/accuracy': 0.8874218463897705, 'train/loss': 0.41652488708496094, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 84487.2626209259, 'total_duration': 91329.5935049057, 'accumulated_submission_time': 84487.2626209259, 'accumulated_eval_time': 6822.406176567078, 'accumulated_logging_time': 9.963230848312378}
I0309 06:23:26.612238 139708415854336 logging_writer.py:48] [188976] accumulated_eval_time=6822.406177, accumulated_logging_time=9.963231, accumulated_submission_time=84487.262621, global_step=188976, preemption_count=0, score=84487.262621, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=91329.593505, train/accuracy=0.887422, train/loss=0.416525, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 06:23:36.446471 139708407461632 logging_writer.py:48] [189000] global_step=189000, grad_norm=3.2284088134765625, loss=1.1339048147201538
I0309 06:24:17.968125 139708415854336 logging_writer.py:48] [189100] global_step=189100, grad_norm=2.8828628063201904, loss=1.5546327829360962
I0309 06:25:03.596497 139708407461632 logging_writer.py:48] [189200] global_step=189200, grad_norm=3.229024887084961, loss=1.0733827352523804
I0309 06:25:49.438378 139708415854336 logging_writer.py:48] [189300] global_step=189300, grad_norm=3.7379214763641357, loss=1.1659448146820068
I0309 06:26:34.806498 139708407461632 logging_writer.py:48] [189400] global_step=189400, grad_norm=2.9845285415649414, loss=1.895963191986084
I0309 06:27:20.253340 139708415854336 logging_writer.py:48] [189500] global_step=189500, grad_norm=3.149165153503418, loss=2.8097095489501953
I0309 06:28:05.919694 139708407461632 logging_writer.py:48] [189600] global_step=189600, grad_norm=3.259883403778076, loss=2.609767436981201
I0309 06:28:51.280481 139708415854336 logging_writer.py:48] [189700] global_step=189700, grad_norm=2.9970555305480957, loss=1.1108382940292358
I0309 06:29:36.911391 139708407461632 logging_writer.py:48] [189800] global_step=189800, grad_norm=3.0711398124694824, loss=2.3486380577087402
I0309 06:30:22.365717 139708415854336 logging_writer.py:48] [189900] global_step=189900, grad_norm=3.07436466217041, loss=1.0669872760772705
I0309 06:30:26.622719 139902746892096 spec.py:321] Evaluating on the training split.
I0309 06:30:37.991717 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 06:30:59.982586 139902746892096 spec.py:349] Evaluating on the test split.
I0309 06:31:01.657084 139902746892096 submission_runner.py:411] Time since start: 91784.69s, 	Step: 189911, 	{'train/accuracy': 0.8871288895606995, 'train/loss': 0.4155504107475281, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 84907.21473288536, 'total_duration': 91784.68587970734, 'accumulated_submission_time': 84907.21473288536, 'accumulated_eval_time': 6857.440523386002, 'accumulated_logging_time': 10.020652532577515}
I0309 06:31:01.702030 139708407461632 logging_writer.py:48] [189911] accumulated_eval_time=6857.440523, accumulated_logging_time=10.020653, accumulated_submission_time=84907.214733, global_step=189911, preemption_count=0, score=84907.214733, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=91784.685880, train/accuracy=0.887129, train/loss=0.415550, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 06:31:37.773855 139708415854336 logging_writer.py:48] [190000] global_step=190000, grad_norm=3.976893424987793, loss=3.0937740802764893
I0309 06:32:23.172986 139708407461632 logging_writer.py:48] [190100] global_step=190100, grad_norm=3.20629620552063, loss=1.1734511852264404
I0309 06:33:08.906188 139708415854336 logging_writer.py:48] [190200] global_step=190200, grad_norm=3.319758892059326, loss=3.014738082885742
I0309 06:33:54.620577 139708407461632 logging_writer.py:48] [190300] global_step=190300, grad_norm=3.0305070877075195, loss=1.5452706813812256
I0309 06:34:39.825102 139708415854336 logging_writer.py:48] [190400] global_step=190400, grad_norm=3.137239456176758, loss=1.1944561004638672
I0309 06:35:25.444183 139708407461632 logging_writer.py:48] [190500] global_step=190500, grad_norm=3.1643311977386475, loss=1.6051908731460571
I0309 06:36:10.807127 139708415854336 logging_writer.py:48] [190600] global_step=190600, grad_norm=3.1663506031036377, loss=1.4129375219345093
I0309 06:36:56.261023 139708407461632 logging_writer.py:48] [190700] global_step=190700, grad_norm=3.1102404594421387, loss=1.0962599515914917
I0309 06:37:42.063554 139708415854336 logging_writer.py:48] [190800] global_step=190800, grad_norm=3.0451040267944336, loss=1.1099493503570557
I0309 06:38:01.901331 139902746892096 spec.py:321] Evaluating on the training split.
I0309 06:38:13.383594 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 06:38:35.687165 139902746892096 spec.py:349] Evaluating on the test split.
I0309 06:38:37.368110 139902746892096 submission_runner.py:411] Time since start: 92240.40s, 	Step: 190846, 	{'train/accuracy': 0.8899218440055847, 'train/loss': 0.4114306569099426, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 85327.35544729233, 'total_duration': 92240.39691138268, 'accumulated_submission_time': 85327.35544729233, 'accumulated_eval_time': 6892.907320976257, 'accumulated_logging_time': 10.07563328742981}
I0309 06:38:37.416561 139708407461632 logging_writer.py:48] [190846] accumulated_eval_time=6892.907321, accumulated_logging_time=10.075633, accumulated_submission_time=85327.355447, global_step=190846, preemption_count=0, score=85327.355447, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=92240.396911, train/accuracy=0.889922, train/loss=0.411431, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 06:38:59.064212 139708415854336 logging_writer.py:48] [190900] global_step=190900, grad_norm=3.3694865703582764, loss=2.69024658203125
I0309 06:39:42.186252 139708407461632 logging_writer.py:48] [191000] global_step=191000, grad_norm=3.188889503479004, loss=1.0533604621887207
I0309 06:40:27.467505 139708415854336 logging_writer.py:48] [191100] global_step=191100, grad_norm=3.0263686180114746, loss=1.619089126586914
I0309 06:41:13.489706 139708407461632 logging_writer.py:48] [191200] global_step=191200, grad_norm=3.067838430404663, loss=1.42291259765625
I0309 06:41:58.684543 139708415854336 logging_writer.py:48] [191300] global_step=191300, grad_norm=3.0195696353912354, loss=2.483193874359131
I0309 06:42:43.753858 139708407461632 logging_writer.py:48] [191400] global_step=191400, grad_norm=2.886577844619751, loss=1.5848298072814941
I0309 06:43:29.308340 139708415854336 logging_writer.py:48] [191500] global_step=191500, grad_norm=3.2509450912475586, loss=1.1651477813720703
I0309 06:44:14.620618 139708407461632 logging_writer.py:48] [191600] global_step=191600, grad_norm=3.0975823402404785, loss=1.3699551820755005
I0309 06:44:59.969162 139708415854336 logging_writer.py:48] [191700] global_step=191700, grad_norm=3.14715313911438, loss=1.1964036226272583
I0309 06:45:37.740265 139902746892096 spec.py:321] Evaluating on the training split.
I0309 06:45:49.199731 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 06:46:10.102582 139902746892096 spec.py:349] Evaluating on the test split.
I0309 06:46:11.781683 139902746892096 submission_runner.py:411] Time since start: 92694.81s, 	Step: 191785, 	{'train/accuracy': 0.8871288895606995, 'train/loss': 0.4186196029186249, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 85747.62012791634, 'total_duration': 92694.81047177315, 'accumulated_submission_time': 85747.62012791634, 'accumulated_eval_time': 6926.948734521866, 'accumulated_logging_time': 10.13442349433899}
I0309 06:46:11.835719 139708407461632 logging_writer.py:48] [191785] accumulated_eval_time=6926.948735, accumulated_logging_time=10.134423, accumulated_submission_time=85747.620128, global_step=191785, preemption_count=0, score=85747.620128, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=92694.810472, train/accuracy=0.887129, train/loss=0.418620, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 06:46:18.164298 139708415854336 logging_writer.py:48] [191800] global_step=191800, grad_norm=3.3310141563415527, loss=1.1893397569656372
I0309 06:46:59.351403 139708407461632 logging_writer.py:48] [191900] global_step=191900, grad_norm=4.261106014251709, loss=3.258129358291626
I0309 06:47:44.733723 139708415854336 logging_writer.py:48] [192000] global_step=192000, grad_norm=2.9197418689727783, loss=1.5784950256347656
I0309 06:48:30.331319 139708407461632 logging_writer.py:48] [192100] global_step=192100, grad_norm=2.9109225273132324, loss=1.5196548700332642
I0309 06:49:15.469316 139708415854336 logging_writer.py:48] [192200] global_step=192200, grad_norm=3.236746311187744, loss=1.1139322519302368
I0309 06:50:01.042899 139708407461632 logging_writer.py:48] [192300] global_step=192300, grad_norm=3.177889108657837, loss=1.1167948246002197
I0309 06:50:46.388170 139708415854336 logging_writer.py:48] [192400] global_step=192400, grad_norm=3.0916736125946045, loss=1.20846426486969
I0309 06:51:31.849380 139708407461632 logging_writer.py:48] [192500] global_step=192500, grad_norm=3.3906962871551514, loss=3.0816810131073
I0309 06:52:17.352906 139708415854336 logging_writer.py:48] [192600] global_step=192600, grad_norm=3.721580982208252, loss=3.253096103668213
I0309 06:53:03.195956 139708407461632 logging_writer.py:48] [192700] global_step=192700, grad_norm=3.0160627365112305, loss=1.1247978210449219
I0309 06:53:11.986711 139902746892096 spec.py:321] Evaluating on the training split.
I0309 06:53:23.482296 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 06:53:47.121539 139902746892096 spec.py:349] Evaluating on the test split.
I0309 06:53:48.803803 139902746892096 submission_runner.py:411] Time since start: 93151.83s, 	Step: 192721, 	{'train/accuracy': 0.8883984088897705, 'train/loss': 0.4127451479434967, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 86167.71252465248, 'total_duration': 93151.8325972557, 'accumulated_submission_time': 86167.71252465248, 'accumulated_eval_time': 6963.765806436539, 'accumulated_logging_time': 10.197827577590942}
I0309 06:53:48.849567 139708415854336 logging_writer.py:48] [192721] accumulated_eval_time=6963.765806, accumulated_logging_time=10.197828, accumulated_submission_time=86167.712525, global_step=192721, preemption_count=0, score=86167.712525, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=93151.832597, train/accuracy=0.888398, train/loss=0.412745, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 06:54:20.307430 139708407461632 logging_writer.py:48] [192800] global_step=192800, grad_norm=2.8367362022399902, loss=1.426979660987854
I0309 06:55:05.075457 139708415854336 logging_writer.py:48] [192900] global_step=192900, grad_norm=2.9740498065948486, loss=1.8380248546600342
I0309 06:55:50.724989 139708407461632 logging_writer.py:48] [193000] global_step=193000, grad_norm=3.0454540252685547, loss=1.6195846796035767
I0309 06:56:36.101558 139708415854336 logging_writer.py:48] [193100] global_step=193100, grad_norm=3.726102352142334, loss=3.1483168601989746
I0309 06:57:21.409952 139708407461632 logging_writer.py:48] [193200] global_step=193200, grad_norm=3.301682233810425, loss=1.136690378189087
I0309 06:58:06.835577 139708415854336 logging_writer.py:48] [193300] global_step=193300, grad_norm=3.4630918502807617, loss=1.5337797403335571
I0309 06:58:52.058139 139708407461632 logging_writer.py:48] [193400] global_step=193400, grad_norm=2.8176188468933105, loss=1.9833855628967285
I0309 06:59:37.623396 139708415854336 logging_writer.py:48] [193500] global_step=193500, grad_norm=3.1325995922088623, loss=2.524529457092285
I0309 07:00:22.759451 139708407461632 logging_writer.py:48] [193600] global_step=193600, grad_norm=3.092655658721924, loss=1.7653582096099854
I0309 07:00:48.911931 139902746892096 spec.py:321] Evaluating on the training split.
I0309 07:01:00.242704 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 07:01:21.861397 139902746892096 spec.py:349] Evaluating on the test split.
I0309 07:01:23.533480 139902746892096 submission_runner.py:411] Time since start: 93606.56s, 	Step: 193659, 	{'train/accuracy': 0.8858984112739563, 'train/loss': 0.41859713196754456, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 86587.71695780754, 'total_duration': 93606.5622651577, 'accumulated_submission_time': 86587.71695780754, 'accumulated_eval_time': 6998.387335062027, 'accumulated_logging_time': 10.25277328491211}
I0309 07:01:23.586814 139708415854336 logging_writer.py:48] [193659] accumulated_eval_time=6998.387335, accumulated_logging_time=10.252773, accumulated_submission_time=86587.716958, global_step=193659, preemption_count=0, score=86587.716958, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=93606.562265, train/accuracy=0.885898, train/loss=0.418597, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 07:01:40.110272 139708407461632 logging_writer.py:48] [193700] global_step=193700, grad_norm=3.095367431640625, loss=2.079834461212158
I0309 07:02:22.616828 139708415854336 logging_writer.py:48] [193800] global_step=193800, grad_norm=3.2143800258636475, loss=1.1538453102111816
I0309 07:03:07.720678 139708407461632 logging_writer.py:48] [193900] global_step=193900, grad_norm=2.8251190185546875, loss=1.172461748123169
I0309 07:03:53.636759 139708415854336 logging_writer.py:48] [194000] global_step=194000, grad_norm=3.009368419647217, loss=2.314302444458008
I0309 07:04:38.921165 139708407461632 logging_writer.py:48] [194100] global_step=194100, grad_norm=2.9948582649230957, loss=1.0936872959136963
I0309 07:05:24.174740 139708415854336 logging_writer.py:48] [194200] global_step=194200, grad_norm=3.2476038932800293, loss=1.1705892086029053
I0309 07:06:09.594542 139708407461632 logging_writer.py:48] [194300] global_step=194300, grad_norm=4.018869400024414, loss=2.662785530090332
I0309 07:06:55.030758 139708415854336 logging_writer.py:48] [194400] global_step=194400, grad_norm=2.874343156814575, loss=2.2772884368896484
I0309 07:07:40.701264 139708407461632 logging_writer.py:48] [194500] global_step=194500, grad_norm=3.4470739364624023, loss=1.0907752513885498
I0309 07:08:23.630503 139902746892096 spec.py:321] Evaluating on the training split.
I0309 07:08:35.190407 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 07:08:57.137134 139902746892096 spec.py:349] Evaluating on the test split.
I0309 07:08:58.801597 139902746892096 submission_runner.py:411] Time since start: 94061.83s, 	Step: 194596, 	{'train/accuracy': 0.8871874809265137, 'train/loss': 0.416054368019104, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 87007.7027232647, 'total_duration': 94061.83039855957, 'accumulated_submission_time': 87007.7027232647, 'accumulated_eval_time': 7033.558439016342, 'accumulated_logging_time': 10.31568694114685}
I0309 07:08:58.848090 139708415854336 logging_writer.py:48] [194596] accumulated_eval_time=7033.558439, accumulated_logging_time=10.315687, accumulated_submission_time=87007.702723, global_step=194596, preemption_count=0, score=87007.702723, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=94061.830399, train/accuracy=0.887187, train/loss=0.416054, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 07:09:00.816586 139708407461632 logging_writer.py:48] [194600] global_step=194600, grad_norm=3.186342239379883, loss=1.0885335206985474
I0309 07:09:41.123133 139708415854336 logging_writer.py:48] [194700] global_step=194700, grad_norm=3.1550073623657227, loss=1.0992882251739502
I0309 07:10:26.167450 139708407461632 logging_writer.py:48] [194800] global_step=194800, grad_norm=3.007007598876953, loss=1.040116310119629
I0309 07:11:11.509230 139708415854336 logging_writer.py:48] [194900] global_step=194900, grad_norm=4.265634536743164, loss=3.135497570037842
I0309 07:11:57.073687 139708407461632 logging_writer.py:48] [195000] global_step=195000, grad_norm=2.8860175609588623, loss=1.4555402994155884
I0309 07:12:42.087707 139708415854336 logging_writer.py:48] [195100] global_step=195100, grad_norm=3.100881576538086, loss=1.3029801845550537
I0309 07:13:27.892307 139708407461632 logging_writer.py:48] [195200] global_step=195200, grad_norm=2.9587888717651367, loss=1.1936746835708618
I0309 07:14:13.630076 139708415854336 logging_writer.py:48] [195300] global_step=195300, grad_norm=3.795104503631592, loss=3.2593994140625
I0309 07:14:58.994071 139708407461632 logging_writer.py:48] [195400] global_step=195400, grad_norm=3.5132715702056885, loss=3.0227298736572266
I0309 07:15:44.698988 139708415854336 logging_writer.py:48] [195500] global_step=195500, grad_norm=3.0449182987213135, loss=1.1654276847839355
I0309 07:15:58.971168 139902746892096 spec.py:321] Evaluating on the training split.
I0309 07:16:10.369847 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 07:16:33.226078 139902746892096 spec.py:349] Evaluating on the test split.
I0309 07:16:34.906931 139902746892096 submission_runner.py:411] Time since start: 94517.94s, 	Step: 195533, 	{'train/accuracy': 0.8887109160423279, 'train/loss': 0.41404077410697937, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 87427.7664604187, 'total_duration': 94517.9357123375, 'accumulated_submission_time': 87427.7664604187, 'accumulated_eval_time': 7069.494187831879, 'accumulated_logging_time': 10.3733651638031}
I0309 07:16:34.960887 139708407461632 logging_writer.py:48] [195533] accumulated_eval_time=7069.494188, accumulated_logging_time=10.373365, accumulated_submission_time=87427.766460, global_step=195533, preemption_count=0, score=87427.766460, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=94517.935712, train/accuracy=0.888711, train/loss=0.414041, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 07:17:01.699644 139708415854336 logging_writer.py:48] [195600] global_step=195600, grad_norm=3.195456027984619, loss=2.710258960723877
I0309 07:17:45.624207 139708407461632 logging_writer.py:48] [195700] global_step=195700, grad_norm=2.9560189247131348, loss=1.0503783226013184
I0309 07:18:30.837015 139708415854336 logging_writer.py:48] [195800] global_step=195800, grad_norm=3.1137430667877197, loss=1.0999926328659058
I0309 07:19:16.507951 139708407461632 logging_writer.py:48] [195900] global_step=195900, grad_norm=3.1099789142608643, loss=1.0288968086242676
I0309 07:20:01.672601 139708415854336 logging_writer.py:48] [196000] global_step=196000, grad_norm=3.081010580062866, loss=2.3928074836730957
I0309 07:20:46.927418 139708407461632 logging_writer.py:48] [196100] global_step=196100, grad_norm=3.175325393676758, loss=1.143550992012024
I0309 07:21:32.432967 139708415854336 logging_writer.py:48] [196200] global_step=196200, grad_norm=3.078630208969116, loss=1.34443998336792
I0309 07:22:17.906815 139708407461632 logging_writer.py:48] [196300] global_step=196300, grad_norm=3.4797661304473877, loss=1.1598244905471802
I0309 07:23:03.155678 139708415854336 logging_writer.py:48] [196400] global_step=196400, grad_norm=3.069528818130493, loss=1.1335195302963257
I0309 07:23:35.199537 139902746892096 spec.py:321] Evaluating on the training split.
I0309 07:23:46.628403 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 07:24:09.084047 139902746892096 spec.py:349] Evaluating on the test split.
I0309 07:24:10.755042 139902746892096 submission_runner.py:411] Time since start: 94973.78s, 	Step: 196472, 	{'train/accuracy': 0.8884179592132568, 'train/loss': 0.41537177562713623, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 87847.94662070274, 'total_duration': 94973.78384137154, 'accumulated_submission_time': 87847.94662070274, 'accumulated_eval_time': 7105.049699783325, 'accumulated_logging_time': 10.437105655670166}
I0309 07:24:10.802196 139708407461632 logging_writer.py:48] [196472] accumulated_eval_time=7105.049700, accumulated_logging_time=10.437106, accumulated_submission_time=87847.946621, global_step=196472, preemption_count=0, score=87847.946621, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=94973.783841, train/accuracy=0.888418, train/loss=0.415372, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 07:24:22.215301 139708415854336 logging_writer.py:48] [196500] global_step=196500, grad_norm=3.3064262866973877, loss=1.1274893283843994
I0309 07:25:03.771683 139708407461632 logging_writer.py:48] [196600] global_step=196600, grad_norm=3.1653993129730225, loss=1.2093132734298706
I0309 07:25:49.021417 139708415854336 logging_writer.py:48] [196700] global_step=196700, grad_norm=2.978482961654663, loss=1.393878698348999
I0309 07:26:34.539407 139708407461632 logging_writer.py:48] [196800] global_step=196800, grad_norm=3.0536699295043945, loss=2.1543989181518555
I0309 07:27:19.932808 139708415854336 logging_writer.py:48] [196900] global_step=196900, grad_norm=2.8761136531829834, loss=2.2087321281433105
I0309 07:28:05.604747 139708407461632 logging_writer.py:48] [197000] global_step=197000, grad_norm=3.6377406120300293, loss=2.7333154678344727
I0309 07:28:51.240170 139708415854336 logging_writer.py:48] [197100] global_step=197100, grad_norm=3.377507209777832, loss=1.5181676149368286
I0309 07:29:36.375521 139708407461632 logging_writer.py:48] [197200] global_step=197200, grad_norm=2.87488055229187, loss=1.8285280466079712
I0309 07:30:21.742903 139708415854336 logging_writer.py:48] [197300] global_step=197300, grad_norm=3.1182053089141846, loss=2.3429665565490723
I0309 07:31:07.034321 139708407461632 logging_writer.py:48] [197400] global_step=197400, grad_norm=3.198838710784912, loss=1.0979427099227905
I0309 07:31:10.874998 139902746892096 spec.py:321] Evaluating on the training split.
I0309 07:31:22.341008 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 07:31:42.572955 139902746892096 spec.py:349] Evaluating on the test split.
I0309 07:31:44.260608 139902746892096 submission_runner.py:411] Time since start: 95427.29s, 	Step: 197410, 	{'train/accuracy': 0.8886327743530273, 'train/loss': 0.41803354024887085, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 88267.96205282211, 'total_duration': 95427.28940010071, 'accumulated_submission_time': 88267.96205282211, 'accumulated_eval_time': 7138.435284852982, 'accumulated_logging_time': 10.492969989776611}
I0309 07:31:44.315169 139708415854336 logging_writer.py:48] [197410] accumulated_eval_time=7138.435285, accumulated_logging_time=10.492970, accumulated_submission_time=88267.962053, global_step=197410, preemption_count=0, score=88267.962053, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=95427.289400, train/accuracy=0.888633, train/loss=0.418034, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 07:32:20.856256 139708407461632 logging_writer.py:48] [197500] global_step=197500, grad_norm=3.4929885864257812, loss=3.188786506652832
I0309 07:33:06.161878 139708415854336 logging_writer.py:48] [197600] global_step=197600, grad_norm=3.518831253051758, loss=2.9138987064361572
I0309 07:33:51.774970 139708407461632 logging_writer.py:48] [197700] global_step=197700, grad_norm=3.1575891971588135, loss=1.2518668174743652
I0309 07:34:37.228947 139708415854336 logging_writer.py:48] [197800] global_step=197800, grad_norm=2.810795545578003, loss=0.9919298887252808
I0309 07:35:22.430247 139708407461632 logging_writer.py:48] [197900] global_step=197900, grad_norm=2.8742263317108154, loss=1.7884495258331299
I0309 07:36:08.016518 139708415854336 logging_writer.py:48] [198000] global_step=198000, grad_norm=3.5739939212799072, loss=1.1437911987304688
I0309 07:36:53.661725 139708407461632 logging_writer.py:48] [198100] global_step=198100, grad_norm=3.9162564277648926, loss=2.703796625137329
I0309 07:37:39.379037 139708415854336 logging_writer.py:48] [198200] global_step=198200, grad_norm=3.0553882122039795, loss=1.0425260066986084
I0309 07:38:24.956221 139708407461632 logging_writer.py:48] [198300] global_step=198300, grad_norm=3.123744487762451, loss=1.1143741607666016
I0309 07:38:44.623588 139902746892096 spec.py:321] Evaluating on the training split.
I0309 07:38:56.225248 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 07:39:19.120272 139902746892096 spec.py:349] Evaluating on the test split.
I0309 07:39:20.790188 139902746892096 submission_runner.py:411] Time since start: 95883.82s, 	Step: 198345, 	{'train/accuracy': 0.8878905773162842, 'train/loss': 0.4148828089237213, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 88688.21113562584, 'total_duration': 95883.8189907074, 'accumulated_submission_time': 88688.21113562584, 'accumulated_eval_time': 7174.6018846035, 'accumulated_logging_time': 10.557025671005249}
I0309 07:39:20.839438 139708415854336 logging_writer.py:48] [198345] accumulated_eval_time=7174.601885, accumulated_logging_time=10.557026, accumulated_submission_time=88688.211136, global_step=198345, preemption_count=0, score=88688.211136, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=95883.818991, train/accuracy=0.887891, train/loss=0.414883, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 07:39:42.874315 139708407461632 logging_writer.py:48] [198400] global_step=198400, grad_norm=3.103318214416504, loss=1.1480062007904053
I0309 07:40:26.120174 139708415854336 logging_writer.py:48] [198500] global_step=198500, grad_norm=2.8060719966888428, loss=1.0928337574005127
I0309 07:41:11.581301 139708407461632 logging_writer.py:48] [198600] global_step=198600, grad_norm=2.9285879135131836, loss=1.0416178703308105
I0309 07:41:57.353073 139708415854336 logging_writer.py:48] [198700] global_step=198700, grad_norm=3.362091302871704, loss=1.2940139770507812
I0309 07:42:42.809780 139708407461632 logging_writer.py:48] [198800] global_step=198800, grad_norm=2.8938841819763184, loss=1.3434944152832031
I0309 07:43:27.954608 139708415854336 logging_writer.py:48] [198900] global_step=198900, grad_norm=3.3480470180511475, loss=2.5405659675598145
I0309 07:44:13.755751 139708407461632 logging_writer.py:48] [199000] global_step=199000, grad_norm=3.2835099697113037, loss=1.1619142293930054
I0309 07:44:58.953743 139708415854336 logging_writer.py:48] [199100] global_step=199100, grad_norm=3.148061513900757, loss=2.77457594871521
I0309 07:45:44.262697 139708407461632 logging_writer.py:48] [199200] global_step=199200, grad_norm=3.294206142425537, loss=1.218453288078308
I0309 07:46:21.143140 139902746892096 spec.py:321] Evaluating on the training split.
I0309 07:46:32.387196 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 07:46:53.388275 139902746892096 spec.py:349] Evaluating on the test split.
I0309 07:46:55.066605 139902746892096 submission_runner.py:411] Time since start: 96338.10s, 	Step: 199283, 	{'train/accuracy': 0.8878905773162842, 'train/loss': 0.4175741970539093, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 89108.45638513565, 'total_duration': 96338.09540224075, 'accumulated_submission_time': 89108.45638513565, 'accumulated_eval_time': 7208.525338888168, 'accumulated_logging_time': 10.615936279296875}
I0309 07:46:55.113154 139708415854336 logging_writer.py:48] [199283] accumulated_eval_time=7208.525339, accumulated_logging_time=10.615936, accumulated_submission_time=89108.456385, global_step=199283, preemption_count=0, score=89108.456385, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=96338.095402, train/accuracy=0.887891, train/loss=0.417574, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 07:47:02.199923 139708407461632 logging_writer.py:48] [199300] global_step=199300, grad_norm=3.5377044677734375, loss=1.1879215240478516
I0309 07:47:43.549207 139708415854336 logging_writer.py:48] [199400] global_step=199400, grad_norm=3.075328826904297, loss=2.3305141925811768
I0309 07:48:28.704026 139708407461632 logging_writer.py:48] [199500] global_step=199500, grad_norm=3.1877317428588867, loss=1.338138222694397
I0309 07:49:14.887600 139708415854336 logging_writer.py:48] [199600] global_step=199600, grad_norm=3.148425817489624, loss=2.5650792121887207
I0309 07:50:00.004159 139708407461632 logging_writer.py:48] [199700] global_step=199700, grad_norm=3.1476829051971436, loss=1.1752707958221436
I0309 07:50:45.198557 139708415854336 logging_writer.py:48] [199800] global_step=199800, grad_norm=3.292149305343628, loss=2.8154568672180176
I0309 07:51:30.331407 139708407461632 logging_writer.py:48] [199900] global_step=199900, grad_norm=2.9402880668640137, loss=1.3386050462722778
I0309 07:52:15.641376 139708415854336 logging_writer.py:48] [200000] global_step=200000, grad_norm=2.7070953845977783, loss=1.9173341989517212
I0309 07:53:00.894922 139708407461632 logging_writer.py:48] [200100] global_step=200100, grad_norm=3.041280508041382, loss=1.2731351852416992
I0309 07:53:46.738016 139708415854336 logging_writer.py:48] [200200] global_step=200200, grad_norm=3.175717353820801, loss=1.060294270515442
I0309 07:53:55.525686 139902746892096 spec.py:321] Evaluating on the training split.
I0309 07:54:07.148358 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 07:54:29.564951 139902746892096 spec.py:349] Evaluating on the test split.
I0309 07:54:31.228553 139902746892096 submission_runner.py:411] Time since start: 96794.26s, 	Step: 200221, 	{'train/accuracy': 0.8884570002555847, 'train/loss': 0.41540178656578064, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 89528.80938267708, 'total_duration': 96794.25735282898, 'accumulated_submission_time': 89528.80938267708, 'accumulated_eval_time': 7244.22820520401, 'accumulated_logging_time': 10.671677350997925}
I0309 07:54:31.276107 139708407461632 logging_writer.py:48] [200221] accumulated_eval_time=7244.228205, accumulated_logging_time=10.671677, accumulated_submission_time=89528.809383, global_step=200221, preemption_count=0, score=89528.809383, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=96794.257353, train/accuracy=0.888457, train/loss=0.415402, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 07:55:02.736979 139708415854336 logging_writer.py:48] [200300] global_step=200300, grad_norm=2.852667808532715, loss=1.8609814643859863
I0309 07:55:47.529649 139708407461632 logging_writer.py:48] [200400] global_step=200400, grad_norm=3.0400888919830322, loss=1.0747079849243164
I0309 07:56:33.019159 139708415854336 logging_writer.py:48] [200500] global_step=200500, grad_norm=2.745229482650757, loss=1.8775548934936523
I0309 07:57:18.481235 139708407461632 logging_writer.py:48] [200600] global_step=200600, grad_norm=2.8720643520355225, loss=1.031091570854187
I0309 07:58:03.991604 139708415854336 logging_writer.py:48] [200700] global_step=200700, grad_norm=3.172532796859741, loss=1.883178949356079
I0309 07:58:49.433789 139708407461632 logging_writer.py:48] [200800] global_step=200800, grad_norm=3.118267297744751, loss=2.5744690895080566
I0309 07:59:34.565948 139708415854336 logging_writer.py:48] [200900] global_step=200900, grad_norm=3.140216588973999, loss=1.085472822189331
I0309 08:00:19.947233 139708407461632 logging_writer.py:48] [201000] global_step=201000, grad_norm=2.9594855308532715, loss=1.1598048210144043
I0309 08:01:05.173701 139708415854336 logging_writer.py:48] [201100] global_step=201100, grad_norm=2.947939157485962, loss=1.1756439208984375
I0309 08:01:31.464552 139902746892096 spec.py:321] Evaluating on the training split.
I0309 08:01:42.864466 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 08:02:03.828035 139902746892096 spec.py:349] Evaluating on the test split.
I0309 08:02:05.506668 139902746892096 submission_runner.py:411] Time since start: 97248.54s, 	Step: 201160, 	{'train/accuracy': 0.8883593678474426, 'train/loss': 0.41660284996032715, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 89948.93990373611, 'total_duration': 97248.5354616642, 'accumulated_submission_time': 89948.93990373611, 'accumulated_eval_time': 7278.2703149318695, 'accumulated_logging_time': 10.727835655212402}
I0309 08:02:05.553120 139708407461632 logging_writer.py:48] [201160] accumulated_eval_time=7278.270315, accumulated_logging_time=10.727836, accumulated_submission_time=89948.939904, global_step=201160, preemption_count=0, score=89948.939904, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=97248.535462, train/accuracy=0.888359, train/loss=0.416603, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 08:02:21.683520 139708415854336 logging_writer.py:48] [201200] global_step=201200, grad_norm=2.9446732997894287, loss=1.0859501361846924
I0309 08:03:04.205298 139708407461632 logging_writer.py:48] [201300] global_step=201300, grad_norm=3.233123540878296, loss=1.148940086364746
I0309 08:03:49.521398 139708415854336 logging_writer.py:48] [201400] global_step=201400, grad_norm=3.7420766353607178, loss=3.164217710494995
I0309 08:04:35.001893 139708407461632 logging_writer.py:48] [201500] global_step=201500, grad_norm=3.3697919845581055, loss=1.3963712453842163
I0309 08:05:20.083425 139708415854336 logging_writer.py:48] [201600] global_step=201600, grad_norm=3.086801052093506, loss=1.1535247564315796
I0309 08:06:05.251662 139708407461632 logging_writer.py:48] [201700] global_step=201700, grad_norm=3.088534355163574, loss=1.4176729917526245
I0309 08:06:50.436912 139708415854336 logging_writer.py:48] [201800] global_step=201800, grad_norm=3.171496629714966, loss=1.9717013835906982
I0309 08:07:35.818233 139708407461632 logging_writer.py:48] [201900] global_step=201900, grad_norm=3.281383514404297, loss=1.9165819883346558
I0309 08:08:21.073526 139708415854336 logging_writer.py:48] [202000] global_step=202000, grad_norm=3.1269702911376953, loss=1.2919992208480835
I0309 08:09:05.645427 139902746892096 spec.py:321] Evaluating on the training split.
I0309 08:09:17.150249 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 08:09:36.750432 139902746892096 spec.py:349] Evaluating on the test split.
I0309 08:09:38.428052 139902746892096 submission_runner.py:411] Time since start: 97701.46s, 	Step: 202100, 	{'train/accuracy': 0.8859374523162842, 'train/loss': 0.42096877098083496, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 90368.9737598896, 'total_duration': 97701.45683455467, 'accumulated_submission_time': 90368.9737598896, 'accumulated_eval_time': 7311.052936553955, 'accumulated_logging_time': 10.78298568725586}
I0309 08:09:38.480824 139708407461632 logging_writer.py:48] [202100] accumulated_eval_time=7311.052937, accumulated_logging_time=10.782986, accumulated_submission_time=90368.973760, global_step=202100, preemption_count=0, score=90368.973760, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=97701.456835, train/accuracy=0.885937, train/loss=0.420969, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 08:09:38.900870 139708415854336 logging_writer.py:48] [202100] global_step=202100, grad_norm=2.895573139190674, loss=0.9822619557380676
I0309 08:10:19.117678 139708407461632 logging_writer.py:48] [202200] global_step=202200, grad_norm=3.2724337577819824, loss=1.1967905759811401
I0309 08:11:04.328496 139708415854336 logging_writer.py:48] [202300] global_step=202300, grad_norm=3.227661609649658, loss=1.145414113998413
I0309 08:11:49.669562 139708407461632 logging_writer.py:48] [202400] global_step=202400, grad_norm=3.10014271736145, loss=2.7232134342193604
I0309 08:12:35.470126 139708415854336 logging_writer.py:48] [202500] global_step=202500, grad_norm=2.8306002616882324, loss=2.2720794677734375
I0309 08:13:20.715442 139708407461632 logging_writer.py:48] [202600] global_step=202600, grad_norm=3.2769038677215576, loss=2.5364675521850586
I0309 08:14:06.653272 139708415854336 logging_writer.py:48] [202700] global_step=202700, grad_norm=3.0990021228790283, loss=2.758063793182373
I0309 08:14:52.141462 139708407461632 logging_writer.py:48] [202800] global_step=202800, grad_norm=2.9928805828094482, loss=1.1469335556030273
I0309 08:15:37.327428 139708415854336 logging_writer.py:48] [202900] global_step=202900, grad_norm=3.1882615089416504, loss=2.537383794784546
I0309 08:16:22.768812 139708407461632 logging_writer.py:48] [203000] global_step=203000, grad_norm=3.433638095855713, loss=2.8487229347229004
I0309 08:16:38.555503 139902746892096 spec.py:321] Evaluating on the training split.
I0309 08:16:49.953171 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 08:17:12.343199 139902746892096 spec.py:349] Evaluating on the test split.
I0309 08:17:14.014807 139902746892096 submission_runner.py:411] Time since start: 98157.04s, 	Step: 203036, 	{'train/accuracy': 0.88880854845047, 'train/loss': 0.4150761067867279, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 90788.98558115959, 'total_duration': 98157.04360675812, 'accumulated_submission_time': 90788.98558115959, 'accumulated_eval_time': 7346.512250185013, 'accumulated_logging_time': 10.849711179733276}
I0309 08:17:14.059504 139708415854336 logging_writer.py:48] [203036] accumulated_eval_time=7346.512250, accumulated_logging_time=10.849711, accumulated_submission_time=90788.985581, global_step=203036, preemption_count=0, score=90788.985581, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=98157.043607, train/accuracy=0.888809, train/loss=0.415076, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 08:17:39.637397 139708407461632 logging_writer.py:48] [203100] global_step=203100, grad_norm=3.0301263332366943, loss=1.1585168838500977
I0309 08:18:23.301933 139708415854336 logging_writer.py:48] [203200] global_step=203200, grad_norm=2.7544071674346924, loss=1.6509371995925903
I0309 08:19:09.187512 139708407461632 logging_writer.py:48] [203300] global_step=203300, grad_norm=3.0508086681365967, loss=1.602644681930542
I0309 08:19:54.757214 139708415854336 logging_writer.py:48] [203400] global_step=203400, grad_norm=5.05887508392334, loss=1.3273887634277344
I0309 08:20:39.962416 139708407461632 logging_writer.py:48] [203500] global_step=203500, grad_norm=3.202418327331543, loss=1.1528764963150024
I0309 08:21:25.467558 139708415854336 logging_writer.py:48] [203600] global_step=203600, grad_norm=3.175910234451294, loss=1.2833422422409058
I0309 08:22:10.812046 139708407461632 logging_writer.py:48] [203700] global_step=203700, grad_norm=3.002887487411499, loss=1.2709708213806152
I0309 08:22:56.273358 139708415854336 logging_writer.py:48] [203800] global_step=203800, grad_norm=3.3446593284606934, loss=1.160956621170044
I0309 08:23:41.632395 139708407461632 logging_writer.py:48] [203900] global_step=203900, grad_norm=3.4726552963256836, loss=3.1758549213409424
I0309 08:24:14.076035 139902746892096 spec.py:321] Evaluating on the training split.
I0309 08:24:25.497251 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 08:24:47.625142 139902746892096 spec.py:349] Evaluating on the test split.
I0309 08:24:49.303294 139902746892096 submission_runner.py:411] Time since start: 98612.33s, 	Step: 203972, 	{'train/accuracy': 0.8892382383346558, 'train/loss': 0.41373854875564575, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 91208.94326591492, 'total_duration': 98612.33208966255, 'accumulated_submission_time': 91208.94326591492, 'accumulated_eval_time': 7381.7394988536835, 'accumulated_logging_time': 10.903832912445068}
I0309 08:24:49.352127 139708415854336 logging_writer.py:48] [203972] accumulated_eval_time=7381.739499, accumulated_logging_time=10.903833, accumulated_submission_time=91208.943266, global_step=203972, preemption_count=0, score=91208.943266, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=98612.332090, train/accuracy=0.889238, train/loss=0.413739, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 08:25:00.767052 139708407461632 logging_writer.py:48] [204000] global_step=204000, grad_norm=3.0356125831604004, loss=1.0555070638656616
I0309 08:25:42.366267 139708415854336 logging_writer.py:48] [204100] global_step=204100, grad_norm=3.0062978267669678, loss=1.1469839811325073
I0309 08:26:27.853056 139708407461632 logging_writer.py:48] [204200] global_step=204200, grad_norm=2.7655029296875, loss=1.2415361404418945
I0309 08:27:13.813577 139708415854336 logging_writer.py:48] [204300] global_step=204300, grad_norm=2.9571592807769775, loss=1.7742375135421753
I0309 08:27:59.372280 139708407461632 logging_writer.py:48] [204400] global_step=204400, grad_norm=3.4861772060394287, loss=1.1087532043457031
I0309 08:28:44.723245 139708415854336 logging_writer.py:48] [204500] global_step=204500, grad_norm=3.961907148361206, loss=3.180018186569214
I0309 08:29:30.549647 139708407461632 logging_writer.py:48] [204600] global_step=204600, grad_norm=2.974337100982666, loss=1.0653464794158936
I0309 08:30:15.723932 139708415854336 logging_writer.py:48] [204700] global_step=204700, grad_norm=2.824796438217163, loss=1.0327680110931396
I0309 08:31:01.269727 139708407461632 logging_writer.py:48] [204800] global_step=204800, grad_norm=3.235496997833252, loss=2.370694398880005
I0309 08:31:46.578134 139708415854336 logging_writer.py:48] [204900] global_step=204900, grad_norm=2.9438669681549072, loss=1.5856852531433105
I0309 08:31:49.404508 139902746892096 spec.py:321] Evaluating on the training split.
I0309 08:32:00.881345 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 08:32:21.736921 139902746892096 spec.py:349] Evaluating on the test split.
I0309 08:32:23.429824 139902746892096 submission_runner.py:411] Time since start: 99066.46s, 	Step: 204908, 	{'train/accuracy': 0.8872265219688416, 'train/loss': 0.41070327162742615, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 91628.93712472916, 'total_duration': 99066.45861530304, 'accumulated_submission_time': 91628.93712472916, 'accumulated_eval_time': 7415.764829158783, 'accumulated_logging_time': 10.96209716796875}
I0309 08:32:23.479414 139708407461632 logging_writer.py:48] [204908] accumulated_eval_time=7415.764829, accumulated_logging_time=10.962097, accumulated_submission_time=91628.937125, global_step=204908, preemption_count=0, score=91628.937125, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=99066.458615, train/accuracy=0.887227, train/loss=0.410703, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 08:33:00.909492 139708415854336 logging_writer.py:48] [205000] global_step=205000, grad_norm=3.49761700630188, loss=3.258157730102539
I0309 08:33:46.268728 139708407461632 logging_writer.py:48] [205100] global_step=205100, grad_norm=2.9782185554504395, loss=1.254870891571045
I0309 08:34:32.055294 139708415854336 logging_writer.py:48] [205200] global_step=205200, grad_norm=3.1518759727478027, loss=2.0360641479492188
I0309 08:35:17.725574 139708407461632 logging_writer.py:48] [205300] global_step=205300, grad_norm=3.196931838989258, loss=1.0895663499832153
I0309 08:36:03.225938 139708415854336 logging_writer.py:48] [205400] global_step=205400, grad_norm=3.048605442047119, loss=1.1330369710922241
I0309 08:36:48.418260 139708407461632 logging_writer.py:48] [205500] global_step=205500, grad_norm=3.292656183242798, loss=1.2018803358078003
I0309 08:37:33.951856 139708415854336 logging_writer.py:48] [205600] global_step=205600, grad_norm=2.9131057262420654, loss=1.1918247938156128
I0309 08:38:19.414509 139708407461632 logging_writer.py:48] [205700] global_step=205700, grad_norm=3.19199538230896, loss=1.171029806137085
I0309 08:39:04.818264 139708415854336 logging_writer.py:48] [205800] global_step=205800, grad_norm=3.10306978225708, loss=1.1728360652923584
I0309 08:39:23.447093 139902746892096 spec.py:321] Evaluating on the training split.
I0309 08:39:34.943478 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 08:39:57.273842 139902746892096 spec.py:349] Evaluating on the test split.
I0309 08:39:58.940961 139902746892096 submission_runner.py:411] Time since start: 99521.97s, 	Step: 205843, 	{'train/accuracy': 0.8876953125, 'train/loss': 0.41761478781700134, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 92048.84640073776, 'total_duration': 99521.96976304054, 'accumulated_submission_time': 92048.84640073776, 'accumulated_eval_time': 7451.258689165115, 'accumulated_logging_time': 11.021414756774902}
I0309 08:39:58.990581 139708407461632 logging_writer.py:48] [205843] accumulated_eval_time=7451.258689, accumulated_logging_time=11.021415, accumulated_submission_time=92048.846401, global_step=205843, preemption_count=0, score=92048.846401, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=99521.969763, train/accuracy=0.887695, train/loss=0.417615, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 08:40:21.807507 139708415854336 logging_writer.py:48] [205900] global_step=205900, grad_norm=3.4998199939727783, loss=3.122213125228882
I0309 08:41:04.873045 139708407461632 logging_writer.py:48] [206000] global_step=206000, grad_norm=3.273761749267578, loss=1.2915127277374268
I0309 08:41:50.282722 139708415854336 logging_writer.py:48] [206100] global_step=206100, grad_norm=3.060190200805664, loss=1.6959165334701538
I0309 08:42:35.795771 139708407461632 logging_writer.py:48] [206200] global_step=206200, grad_norm=2.9230105876922607, loss=2.2202773094177246
I0309 08:43:21.173906 139708415854336 logging_writer.py:48] [206300] global_step=206300, grad_norm=2.97735595703125, loss=1.949375867843628
I0309 08:44:06.622692 139708407461632 logging_writer.py:48] [206400] global_step=206400, grad_norm=3.2227118015289307, loss=2.08988094329834
I0309 08:44:52.364701 139708415854336 logging_writer.py:48] [206500] global_step=206500, grad_norm=2.9110801219940186, loss=1.408313512802124
I0309 08:45:37.509799 139708407461632 logging_writer.py:48] [206600] global_step=206600, grad_norm=3.363617181777954, loss=3.0032267570495605
I0309 08:46:22.971559 139708415854336 logging_writer.py:48] [206700] global_step=206700, grad_norm=3.2689883708953857, loss=1.1310663223266602
I0309 08:46:58.990960 139902746892096 spec.py:321] Evaluating on the training split.
I0309 08:47:10.419455 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 08:47:32.651885 139902746892096 spec.py:349] Evaluating on the test split.
I0309 08:47:34.322429 139902746892096 submission_runner.py:411] Time since start: 99977.35s, 	Step: 206781, 	{'train/accuracy': 0.8884570002555847, 'train/loss': 0.41563305258750916, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 92468.78777194023, 'total_duration': 99977.3512263298, 'accumulated_submission_time': 92468.78777194023, 'accumulated_eval_time': 7486.59016084671, 'accumulated_logging_time': 11.081706762313843}
I0309 08:47:34.372196 139708407461632 logging_writer.py:48] [206781] accumulated_eval_time=7486.590161, accumulated_logging_time=11.081707, accumulated_submission_time=92468.787772, global_step=206781, preemption_count=0, score=92468.787772, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=99977.351226, train/accuracy=0.888457, train/loss=0.415633, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 08:47:42.242830 139708415854336 logging_writer.py:48] [206800] global_step=206800, grad_norm=3.0393965244293213, loss=1.1354094743728638
I0309 08:48:23.761116 139708407461632 logging_writer.py:48] [206900] global_step=206900, grad_norm=3.053879976272583, loss=1.3799195289611816
I0309 08:49:08.865787 139708415854336 logging_writer.py:48] [207000] global_step=207000, grad_norm=3.744955539703369, loss=3.1970949172973633
I0309 08:49:54.181617 139708407461632 logging_writer.py:48] [207100] global_step=207100, grad_norm=3.2711775302886963, loss=1.182883381843567
I0309 08:50:39.428991 139708415854336 logging_writer.py:48] [207200] global_step=207200, grad_norm=3.273669481277466, loss=2.621051788330078
I0309 08:51:24.516272 139708407461632 logging_writer.py:48] [207300] global_step=207300, grad_norm=3.17110538482666, loss=1.2796859741210938
I0309 08:52:09.774459 139708415854336 logging_writer.py:48] [207400] global_step=207400, grad_norm=3.07680082321167, loss=2.4348855018615723
I0309 08:52:55.171000 139708407461632 logging_writer.py:48] [207500] global_step=207500, grad_norm=2.854344367980957, loss=1.4347007274627686
I0309 08:53:40.297058 139708415854336 logging_writer.py:48] [207600] global_step=207600, grad_norm=4.270775318145752, loss=3.287754535675049
I0309 08:54:25.970838 139708407461632 logging_writer.py:48] [207700] global_step=207700, grad_norm=3.0194180011749268, loss=1.0403344631195068
I0309 08:54:34.485209 139902746892096 spec.py:321] Evaluating on the training split.
I0309 08:54:46.053952 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 08:55:08.596082 139902746892096 spec.py:349] Evaluating on the test split.
I0309 08:55:10.274364 139902746892096 submission_runner.py:411] Time since start: 100433.30s, 	Step: 207720, 	{'train/accuracy': 0.8912109136581421, 'train/loss': 0.40884077548980713, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 92888.84211206436, 'total_duration': 100433.30316019058, 'accumulated_submission_time': 92888.84211206436, 'accumulated_eval_time': 7522.379315137863, 'accumulated_logging_time': 11.140997648239136}
I0309 08:55:10.330985 139708415854336 logging_writer.py:48] [207720] accumulated_eval_time=7522.379315, accumulated_logging_time=11.140998, accumulated_submission_time=92888.842112, global_step=207720, preemption_count=0, score=92888.842112, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=100433.303160, train/accuracy=0.891211, train/loss=0.408841, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 08:55:42.186227 139708407461632 logging_writer.py:48] [207800] global_step=207800, grad_norm=2.955198049545288, loss=1.426071286201477
I0309 08:56:27.058571 139708415854336 logging_writer.py:48] [207900] global_step=207900, grad_norm=3.9670474529266357, loss=3.373281955718994
I0309 08:57:12.772276 139708407461632 logging_writer.py:48] [208000] global_step=208000, grad_norm=3.6572351455688477, loss=3.0849337577819824
I0309 08:57:58.738460 139708415854336 logging_writer.py:48] [208100] global_step=208100, grad_norm=3.1929304599761963, loss=1.1306045055389404
I0309 08:58:43.989098 139708407461632 logging_writer.py:48] [208200] global_step=208200, grad_norm=3.337953805923462, loss=2.159702777862549
I0309 08:59:29.233430 139708415854336 logging_writer.py:48] [208300] global_step=208300, grad_norm=3.3633317947387695, loss=1.2105216979980469
I0309 09:00:14.424705 139708407461632 logging_writer.py:48] [208400] global_step=208400, grad_norm=3.3802194595336914, loss=1.0287302732467651
I0309 09:00:59.756540 139708415854336 logging_writer.py:48] [208500] global_step=208500, grad_norm=3.021205186843872, loss=1.3389395475387573
I0309 09:01:45.528689 139708407461632 logging_writer.py:48] [208600] global_step=208600, grad_norm=3.8840222358703613, loss=3.105361223220825
I0309 09:02:10.588836 139902746892096 spec.py:321] Evaluating on the training split.
I0309 09:02:22.063594 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 09:02:43.093011 139902746892096 spec.py:349] Evaluating on the test split.
I0309 09:02:44.779281 139902746892096 submission_runner.py:411] Time since start: 100887.81s, 	Step: 208657, 	{'train/accuracy': 0.8899999856948853, 'train/loss': 0.40956464409828186, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 93309.04252171516, 'total_duration': 100887.8080637455, 'accumulated_submission_time': 93309.04252171516, 'accumulated_eval_time': 7556.569756746292, 'accumulated_logging_time': 11.206549644470215}
I0309 09:02:44.831896 139708415854336 logging_writer.py:48] [208657] accumulated_eval_time=7556.569757, accumulated_logging_time=11.206550, accumulated_submission_time=93309.042522, global_step=208657, preemption_count=0, score=93309.042522, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=100887.808064, train/accuracy=0.890000, train/loss=0.409565, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 09:03:02.145921 139708407461632 logging_writer.py:48] [208700] global_step=208700, grad_norm=3.8517074584960938, loss=3.2016842365264893
I0309 09:03:45.099342 139708415854336 logging_writer.py:48] [208800] global_step=208800, grad_norm=3.8243274688720703, loss=3.2444777488708496
I0309 09:04:30.532649 139708407461632 logging_writer.py:48] [208900] global_step=208900, grad_norm=3.0555002689361572, loss=1.1973158121109009
I0309 09:05:16.500982 139708415854336 logging_writer.py:48] [209000] global_step=209000, grad_norm=3.568010091781616, loss=3.1406326293945312
I0309 09:06:01.933172 139708407461632 logging_writer.py:48] [209100] global_step=209100, grad_norm=2.9062352180480957, loss=1.116245985031128
I0309 09:06:47.457455 139708415854336 logging_writer.py:48] [209200] global_step=209200, grad_norm=3.048039436340332, loss=1.5840204954147339
I0309 09:07:33.219258 139708407461632 logging_writer.py:48] [209300] global_step=209300, grad_norm=3.1948485374450684, loss=1.287783145904541
I0309 09:08:18.522963 139708415854336 logging_writer.py:48] [209400] global_step=209400, grad_norm=3.1483757495880127, loss=1.2516250610351562
I0309 09:09:04.066893 139708407461632 logging_writer.py:48] [209500] global_step=209500, grad_norm=3.081510305404663, loss=1.6124323606491089
I0309 09:09:45.150796 139902746892096 spec.py:321] Evaluating on the training split.
I0309 09:09:56.747274 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 09:10:17.548351 139902746892096 spec.py:349] Evaluating on the test split.
I0309 09:10:19.227625 139902746892096 submission_runner.py:411] Time since start: 101342.26s, 	Step: 209592, 	{'train/accuracy': 0.88832026720047, 'train/loss': 0.41106781363487244, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 93729.30348491669, 'total_duration': 101342.25642371178, 'accumulated_submission_time': 93729.30348491669, 'accumulated_eval_time': 7590.646590232849, 'accumulated_logging_time': 11.269054651260376}
I0309 09:10:19.277600 139708415854336 logging_writer.py:48] [209592] accumulated_eval_time=7590.646590, accumulated_logging_time=11.269055, accumulated_submission_time=93729.303485, global_step=209592, preemption_count=0, score=93729.303485, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=101342.256424, train/accuracy=0.888320, train/loss=0.411068, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 09:10:22.820505 139708407461632 logging_writer.py:48] [209600] global_step=209600, grad_norm=3.256594657897949, loss=1.0074137449264526
I0309 09:11:03.914916 139708415854336 logging_writer.py:48] [209700] global_step=209700, grad_norm=2.76542329788208, loss=1.7400445938110352
I0309 09:11:49.064593 139708407461632 logging_writer.py:48] [209800] global_step=209800, grad_norm=3.0732054710388184, loss=1.0825542211532593
I0309 09:12:34.224125 139708415854336 logging_writer.py:48] [209900] global_step=209900, grad_norm=3.4152779579162598, loss=1.106722354888916
I0309 09:13:19.718872 139708407461632 logging_writer.py:48] [210000] global_step=210000, grad_norm=4.100504398345947, loss=3.1076667308807373
I0309 09:14:04.876010 139708415854336 logging_writer.py:48] [210100] global_step=210100, grad_norm=3.429462194442749, loss=1.0823289155960083
I0309 09:14:50.498557 139708407461632 logging_writer.py:48] [210200] global_step=210200, grad_norm=3.08736252784729, loss=1.2717458009719849
I0309 09:15:36.058143 139708415854336 logging_writer.py:48] [210300] global_step=210300, grad_norm=3.3485302925109863, loss=1.0499008893966675
I0309 09:16:21.584887 139708407461632 logging_writer.py:48] [210400] global_step=210400, grad_norm=3.631531238555908, loss=3.2278647422790527
I0309 09:17:06.856134 139708415854336 logging_writer.py:48] [210500] global_step=210500, grad_norm=2.9154467582702637, loss=1.1500009298324585
I0309 09:17:19.235924 139902746892096 spec.py:321] Evaluating on the training split.
I0309 09:17:31.625922 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 09:17:53.789955 139902746892096 spec.py:349] Evaluating on the test split.
I0309 09:17:55.469671 139902746892096 submission_runner.py:411] Time since start: 101798.50s, 	Step: 210529, 	{'train/accuracy': 0.8885546922683716, 'train/loss': 0.41290420293807983, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 94149.20416498184, 'total_duration': 101798.49844503403, 'accumulated_submission_time': 94149.20416498184, 'accumulated_eval_time': 7626.880306720734, 'accumulated_logging_time': 11.327747821807861}
I0309 09:17:55.527506 139708407461632 logging_writer.py:48] [210529] accumulated_eval_time=7626.880307, accumulated_logging_time=11.327748, accumulated_submission_time=94149.204165, global_step=210529, preemption_count=0, score=94149.204165, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=101798.498445, train/accuracy=0.888555, train/loss=0.412904, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 09:18:23.845480 139708415854336 logging_writer.py:48] [210600] global_step=210600, grad_norm=3.4589974880218506, loss=3.0487375259399414
I0309 09:19:07.285238 139708407461632 logging_writer.py:48] [210700] global_step=210700, grad_norm=3.908615827560425, loss=2.0340139865875244
I0309 09:19:52.759855 139708415854336 logging_writer.py:48] [210800] global_step=210800, grad_norm=3.323218584060669, loss=2.914992570877075
I0309 09:20:37.992312 139708407461632 logging_writer.py:48] [210900] global_step=210900, grad_norm=3.674928903579712, loss=2.706979751586914
I0309 09:21:23.094215 139708415854336 logging_writer.py:48] [211000] global_step=211000, grad_norm=3.4631664752960205, loss=2.8854238986968994
I0309 09:22:08.412649 139708407461632 logging_writer.py:48] [211100] global_step=211100, grad_norm=3.01401424407959, loss=1.1352070569992065
I0309 09:22:53.316715 139708415854336 logging_writer.py:48] [211200] global_step=211200, grad_norm=3.3340444564819336, loss=2.6200146675109863
I0309 09:23:38.830868 139708407461632 logging_writer.py:48] [211300] global_step=211300, grad_norm=2.7285051345825195, loss=1.303910732269287
I0309 09:24:24.081518 139708415854336 logging_writer.py:48] [211400] global_step=211400, grad_norm=3.2128703594207764, loss=1.9972349405288696
I0309 09:24:55.890777 139902746892096 spec.py:321] Evaluating on the training split.
I0309 09:25:07.339519 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 09:25:28.494149 139902746892096 spec.py:349] Evaluating on the test split.
I0309 09:25:30.179676 139902746892096 submission_runner.py:411] Time since start: 102253.21s, 	Step: 211471, 	{'train/accuracy': 0.8863281011581421, 'train/loss': 0.4174293875694275, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 94569.50618243217, 'total_duration': 102253.2084736824, 'accumulated_submission_time': 94569.50618243217, 'accumulated_eval_time': 7661.169199705124, 'accumulated_logging_time': 11.396864175796509}
I0309 09:25:30.228725 139708407461632 logging_writer.py:48] [211471] accumulated_eval_time=7661.169200, accumulated_logging_time=11.396864, accumulated_submission_time=94569.506182, global_step=211471, preemption_count=0, score=94569.506182, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=102253.208474, train/accuracy=0.886328, train/loss=0.417429, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 09:25:42.031196 139708415854336 logging_writer.py:48] [211500] global_step=211500, grad_norm=3.1514523029327393, loss=1.135643720626831
I0309 09:26:24.092836 139708407461632 logging_writer.py:48] [211600] global_step=211600, grad_norm=3.378582715988159, loss=1.8994982242584229
I0309 09:27:09.409183 139708415854336 logging_writer.py:48] [211700] global_step=211700, grad_norm=3.2211432456970215, loss=1.1039360761642456
I0309 09:27:54.714892 139708407461632 logging_writer.py:48] [211800] global_step=211800, grad_norm=3.1008055210113525, loss=2.038721799850464
I0309 09:28:40.038310 139708415854336 logging_writer.py:48] [211900] global_step=211900, grad_norm=3.0155014991760254, loss=1.069850206375122
I0309 09:29:25.145828 139708407461632 logging_writer.py:48] [212000] global_step=212000, grad_norm=3.238945245742798, loss=1.2581332921981812
I0309 09:30:10.534495 139708415854336 logging_writer.py:48] [212100] global_step=212100, grad_norm=3.0402612686157227, loss=2.2758469581604004
I0309 09:30:55.704822 139708407461632 logging_writer.py:48] [212200] global_step=212200, grad_norm=3.272475242614746, loss=1.0888235569000244
I0309 09:31:41.195512 139708415854336 logging_writer.py:48] [212300] global_step=212300, grad_norm=3.104677438735962, loss=1.3607470989227295
I0309 09:32:26.224105 139708407461632 logging_writer.py:48] [212400] global_step=212400, grad_norm=3.0244948863983154, loss=1.13407564163208
I0309 09:32:30.408113 139902746892096 spec.py:321] Evaluating on the training split.
I0309 09:32:41.888571 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 09:33:03.892013 139902746892096 spec.py:349] Evaluating on the test split.
I0309 09:33:05.561583 139902746892096 submission_runner.py:411] Time since start: 102708.59s, 	Step: 212411, 	{'train/accuracy': 0.8870702981948853, 'train/loss': 0.41616132855415344, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 94989.62823104858, 'total_duration': 102708.59036684036, 'accumulated_submission_time': 94989.62823104858, 'accumulated_eval_time': 7696.322644948959, 'accumulated_logging_time': 11.454381227493286}
I0309 09:33:05.619509 139708415854336 logging_writer.py:48] [212411] accumulated_eval_time=7696.322645, accumulated_logging_time=11.454381, accumulated_submission_time=94989.628231, global_step=212411, preemption_count=0, score=94989.628231, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=102708.590367, train/accuracy=0.887070, train/loss=0.416161, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 09:33:40.996841 139708407461632 logging_writer.py:48] [212500] global_step=212500, grad_norm=4.404996871948242, loss=1.121544599533081
I0309 09:34:26.120152 139708415854336 logging_writer.py:48] [212600] global_step=212600, grad_norm=3.420462131500244, loss=1.5717346668243408
I0309 09:35:11.805902 139708407461632 logging_writer.py:48] [212700] global_step=212700, grad_norm=3.1896886825561523, loss=1.0087553262710571
I0309 09:35:58.117973 139708415854336 logging_writer.py:48] [212800] global_step=212800, grad_norm=3.487656354904175, loss=2.9643335342407227
I0309 09:36:43.649002 139708407461632 logging_writer.py:48] [212900] global_step=212900, grad_norm=3.0902047157287598, loss=1.169128179550171
I0309 09:37:29.621867 139708415854336 logging_writer.py:48] [213000] global_step=213000, grad_norm=3.250723123550415, loss=1.8659138679504395
I0309 09:38:15.389745 139708407461632 logging_writer.py:48] [213100] global_step=213100, grad_norm=3.189310312271118, loss=1.0929795503616333
I0309 09:39:00.806029 139708415854336 logging_writer.py:48] [213200] global_step=213200, grad_norm=3.2108821868896484, loss=1.2889026403427124
I0309 09:39:46.268218 139708407461632 logging_writer.py:48] [213300] global_step=213300, grad_norm=2.852717638015747, loss=1.4949779510498047
I0309 09:40:05.918316 139902746892096 spec.py:321] Evaluating on the training split.
I0309 09:40:17.278594 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 09:40:38.780053 139902746892096 spec.py:349] Evaluating on the test split.
I0309 09:40:40.454436 139902746892096 submission_runner.py:411] Time since start: 103163.48s, 	Step: 213345, 	{'train/accuracy': 0.8892773389816284, 'train/loss': 0.41330868005752563, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 95409.86905503273, 'total_duration': 103163.48323106766, 'accumulated_submission_time': 95409.86905503273, 'accumulated_eval_time': 7730.858766078949, 'accumulated_logging_time': 11.52181363105774}
I0309 09:40:40.501718 139708415854336 logging_writer.py:48] [213345] accumulated_eval_time=7730.858766, accumulated_logging_time=11.521814, accumulated_submission_time=95409.869055, global_step=213345, preemption_count=0, score=95409.869055, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=103163.483231, train/accuracy=0.889277, train/loss=0.413309, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 09:41:02.584592 139708407461632 logging_writer.py:48] [213400] global_step=213400, grad_norm=2.972592830657959, loss=1.309934139251709
I0309 09:41:46.073005 139708415854336 logging_writer.py:48] [213500] global_step=213500, grad_norm=3.2886452674865723, loss=1.370320200920105
I0309 09:42:31.734304 139708407461632 logging_writer.py:48] [213600] global_step=213600, grad_norm=2.973482370376587, loss=1.1405268907546997
I0309 09:43:17.313141 139708415854336 logging_writer.py:48] [213700] global_step=213700, grad_norm=3.3448219299316406, loss=2.763606309890747
I0309 09:44:02.904954 139708407461632 logging_writer.py:48] [213800] global_step=213800, grad_norm=3.2777607440948486, loss=2.6609714031219482
I0309 09:44:48.437479 139708415854336 logging_writer.py:48] [213900] global_step=213900, grad_norm=3.013178586959839, loss=1.348337173461914
I0309 09:45:34.147233 139708407461632 logging_writer.py:48] [214000] global_step=214000, grad_norm=2.9747729301452637, loss=2.0354902744293213
I0309 09:46:19.108999 139708415854336 logging_writer.py:48] [214100] global_step=214100, grad_norm=3.4447739124298096, loss=1.2145229578018188
I0309 09:47:04.258645 139708407461632 logging_writer.py:48] [214200] global_step=214200, grad_norm=3.251840114593506, loss=1.0646857023239136
I0309 09:47:40.516169 139902746892096 spec.py:321] Evaluating on the training split.
I0309 09:47:51.946658 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 09:48:14.753433 139902746892096 spec.py:349] Evaluating on the test split.
I0309 09:48:16.416959 139902746892096 submission_runner.py:411] Time since start: 103619.45s, 	Step: 214281, 	{'train/accuracy': 0.888964831829071, 'train/loss': 0.41241592168807983, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 95829.8261590004, 'total_duration': 103619.44575953484, 'accumulated_submission_time': 95829.8261590004, 'accumulated_eval_time': 7766.759558439255, 'accumulated_logging_time': 11.57755184173584}
I0309 09:48:16.464309 139708415854336 logging_writer.py:48] [214281] accumulated_eval_time=7766.759558, accumulated_logging_time=11.577552, accumulated_submission_time=95829.826159, global_step=214281, preemption_count=0, score=95829.826159, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=103619.445760, train/accuracy=0.888965, train/loss=0.412416, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 09:48:24.334598 139708407461632 logging_writer.py:48] [214300] global_step=214300, grad_norm=2.8816559314727783, loss=2.0095596313476562
I0309 09:49:05.303777 139708415854336 logging_writer.py:48] [214400] global_step=214400, grad_norm=2.86250901222229, loss=1.1383024454116821
I0309 09:49:50.593389 139708407461632 logging_writer.py:48] [214500] global_step=214500, grad_norm=3.7235121726989746, loss=3.176957368850708
I0309 09:50:36.428975 139708415854336 logging_writer.py:48] [214600] global_step=214600, grad_norm=3.245345115661621, loss=0.9975348711013794
I0309 09:51:21.644470 139708407461632 logging_writer.py:48] [214700] global_step=214700, grad_norm=3.1095564365386963, loss=1.1991186141967773
I0309 09:52:06.815901 139708415854336 logging_writer.py:48] [214800] global_step=214800, grad_norm=3.281761407852173, loss=3.000383138656616
I0309 09:52:52.203837 139708407461632 logging_writer.py:48] [214900] global_step=214900, grad_norm=3.4420576095581055, loss=2.9524412155151367
I0309 09:53:37.178901 139708415854336 logging_writer.py:48] [215000] global_step=215000, grad_norm=2.949150562286377, loss=1.2107330560684204
I0309 09:54:22.569754 139708407461632 logging_writer.py:48] [215100] global_step=215100, grad_norm=3.2304725646972656, loss=2.3719475269317627
I0309 09:55:08.020780 139708415854336 logging_writer.py:48] [215200] global_step=215200, grad_norm=3.007779836654663, loss=1.3411704301834106
I0309 09:55:16.622807 139902746892096 spec.py:321] Evaluating on the training split.
I0309 09:55:28.052923 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 09:55:48.915193 139902746892096 spec.py:349] Evaluating on the test split.
I0309 09:55:50.574503 139902746892096 submission_runner.py:411] Time since start: 104073.60s, 	Step: 215221, 	{'train/accuracy': 0.8868359327316284, 'train/loss': 0.41760995984077454, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 96249.92731976509, 'total_duration': 104073.60329914093, 'accumulated_submission_time': 96249.92731976509, 'accumulated_eval_time': 7800.7112374305725, 'accumulated_logging_time': 11.63347578048706}
I0309 09:55:50.632982 139708407461632 logging_writer.py:48] [215221] accumulated_eval_time=7800.711237, accumulated_logging_time=11.633476, accumulated_submission_time=96249.927320, global_step=215221, preemption_count=0, score=96249.927320, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=104073.603299, train/accuracy=0.886836, train/loss=0.417610, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 09:56:22.099415 139708415854336 logging_writer.py:48] [215300] global_step=215300, grad_norm=3.2174646854400635, loss=1.1580009460449219
I0309 09:57:06.515952 139708407461632 logging_writer.py:48] [215400] global_step=215400, grad_norm=3.138437032699585, loss=1.1644701957702637
I0309 09:57:51.962120 139708415854336 logging_writer.py:48] [215500] global_step=215500, grad_norm=2.9920287132263184, loss=1.1208752393722534
I0309 09:58:37.551065 139708407461632 logging_writer.py:48] [215600] global_step=215600, grad_norm=3.0214459896087646, loss=1.223987340927124
I0309 09:59:23.123821 139708415854336 logging_writer.py:48] [215700] global_step=215700, grad_norm=3.6003379821777344, loss=1.305470585823059
I0309 10:00:08.824376 139708407461632 logging_writer.py:48] [215800] global_step=215800, grad_norm=3.1390507221221924, loss=1.3649214506149292
I0309 10:00:54.048269 139708415854336 logging_writer.py:48] [215900] global_step=215900, grad_norm=3.3029894828796387, loss=1.0749276876449585
I0309 10:01:39.314850 139708407461632 logging_writer.py:48] [216000] global_step=216000, grad_norm=3.3713059425354004, loss=1.226676344871521
I0309 10:02:24.584206 139708415854336 logging_writer.py:48] [216100] global_step=216100, grad_norm=3.3527417182922363, loss=2.473529815673828
I0309 10:02:50.590174 139902746892096 spec.py:321] Evaluating on the training split.
I0309 10:03:02.085635 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 10:03:22.521071 139902746892096 spec.py:349] Evaluating on the test split.
I0309 10:03:24.199164 139902746892096 submission_runner.py:411] Time since start: 104527.23s, 	Step: 216159, 	{'train/accuracy': 0.8875195384025574, 'train/loss': 0.41307878494262695, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 96669.82828593254, 'total_duration': 104527.22795033455, 'accumulated_submission_time': 96669.82828593254, 'accumulated_eval_time': 7834.320219755173, 'accumulated_logging_time': 11.700467109680176}
I0309 10:03:24.258164 139708407461632 logging_writer.py:48] [216159] accumulated_eval_time=7834.320220, accumulated_logging_time=11.700467, accumulated_submission_time=96669.828286, global_step=216159, preemption_count=0, score=96669.828286, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=104527.227950, train/accuracy=0.887520, train/loss=0.413079, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 10:03:40.794121 139708415854336 logging_writer.py:48] [216200] global_step=216200, grad_norm=3.1723146438598633, loss=1.254129409790039
I0309 10:04:23.586369 139708407461632 logging_writer.py:48] [216300] global_step=216300, grad_norm=4.189154148101807, loss=3.183894634246826
I0309 10:05:08.996915 139708415854336 logging_writer.py:48] [216400] global_step=216400, grad_norm=2.9808051586151123, loss=1.1302802562713623
I0309 10:05:54.806108 139708407461632 logging_writer.py:48] [216500] global_step=216500, grad_norm=3.0520176887512207, loss=2.6907272338867188
I0309 10:06:40.258962 139708415854336 logging_writer.py:48] [216600] global_step=216600, grad_norm=3.2578835487365723, loss=2.8019139766693115
I0309 10:07:25.553347 139708407461632 logging_writer.py:48] [216700] global_step=216700, grad_norm=3.027195930480957, loss=1.308995008468628
I0309 10:08:11.333601 139708415854336 logging_writer.py:48] [216800] global_step=216800, grad_norm=3.1484296321868896, loss=1.4806528091430664
I0309 10:08:56.833234 139708407461632 logging_writer.py:48] [216900] global_step=216900, grad_norm=2.8475732803344727, loss=2.440570831298828
I0309 10:09:42.557975 139708415854336 logging_writer.py:48] [217000] global_step=217000, grad_norm=2.874236822128296, loss=1.7694199085235596
I0309 10:10:24.466160 139902746892096 spec.py:321] Evaluating on the training split.
I0309 10:10:35.940213 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 10:10:59.270414 139902746892096 spec.py:349] Evaluating on the test split.
I0309 10:11:00.941884 139902746892096 submission_runner.py:411] Time since start: 104983.97s, 	Step: 217094, 	{'train/accuracy': 0.8885155916213989, 'train/loss': 0.41175705194473267, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 97089.97589826584, 'total_duration': 104983.97067832947, 'accumulated_submission_time': 97089.97589826584, 'accumulated_eval_time': 7870.795943975449, 'accumulated_logging_time': 11.771633863449097}
I0309 10:11:00.991071 139708407461632 logging_writer.py:48] [217094] accumulated_eval_time=7870.795944, accumulated_logging_time=11.771634, accumulated_submission_time=97089.975898, global_step=217094, preemption_count=0, score=97089.975898, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=104983.970678, train/accuracy=0.888516, train/loss=0.411757, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 10:11:03.745667 139708415854336 logging_writer.py:48] [217100] global_step=217100, grad_norm=2.961787700653076, loss=1.310906171798706
I0309 10:11:44.101725 139708407461632 logging_writer.py:48] [217200] global_step=217200, grad_norm=2.8564794063568115, loss=1.5309616327285767
I0309 10:12:29.176563 139708415854336 logging_writer.py:48] [217300] global_step=217300, grad_norm=3.419757843017578, loss=3.123991012573242
I0309 10:13:14.476912 139708407461632 logging_writer.py:48] [217400] global_step=217400, grad_norm=2.9590494632720947, loss=1.0783582925796509
I0309 10:14:00.066499 139708415854336 logging_writer.py:48] [217500] global_step=217500, grad_norm=2.8359177112579346, loss=1.0793460607528687
I0309 10:14:45.351898 139708407461632 logging_writer.py:48] [217600] global_step=217600, grad_norm=3.0605359077453613, loss=2.698184013366699
I0309 10:15:30.958753 139708415854336 logging_writer.py:48] [217700] global_step=217700, grad_norm=3.048884630203247, loss=1.1032602787017822
I0309 10:16:16.250412 139708407461632 logging_writer.py:48] [217800] global_step=217800, grad_norm=3.1823570728302, loss=1.7865192890167236
I0309 10:17:01.614917 139708415854336 logging_writer.py:48] [217900] global_step=217900, grad_norm=3.0298662185668945, loss=1.9156506061553955
I0309 10:17:47.292205 139708407461632 logging_writer.py:48] [218000] global_step=218000, grad_norm=3.243307113647461, loss=2.7803783416748047
I0309 10:18:01.044267 139902746892096 spec.py:321] Evaluating on the training split.
I0309 10:18:12.479291 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 10:18:35.159286 139902746892096 spec.py:349] Evaluating on the test split.
I0309 10:18:36.833316 139902746892096 submission_runner.py:411] Time since start: 105439.86s, 	Step: 218032, 	{'train/accuracy': 0.8877929449081421, 'train/loss': 0.41745585203170776, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 97509.97085809708, 'total_duration': 105439.86210155487, 'accumulated_submission_time': 97509.97085809708, 'accumulated_eval_time': 7906.584993600845, 'accumulated_logging_time': 11.830165147781372}
I0309 10:18:36.887961 139708415854336 logging_writer.py:48] [218032] accumulated_eval_time=7906.584994, accumulated_logging_time=11.830165, accumulated_submission_time=97509.970858, global_step=218032, preemption_count=0, score=97509.970858, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=105439.862102, train/accuracy=0.887793, train/loss=0.417456, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 10:19:04.041296 139708407461632 logging_writer.py:48] [218100] global_step=218100, grad_norm=2.9946305751800537, loss=0.9135576486587524
I0309 10:19:47.752876 139708415854336 logging_writer.py:48] [218200] global_step=218200, grad_norm=2.9020495414733887, loss=1.1262903213500977
I0309 10:20:33.044270 139708407461632 logging_writer.py:48] [218300] global_step=218300, grad_norm=3.4531235694885254, loss=1.097839593887329
I0309 10:21:18.709834 139708415854336 logging_writer.py:48] [218400] global_step=218400, grad_norm=3.3296475410461426, loss=1.5475528240203857
I0309 10:22:03.785218 139708407461632 logging_writer.py:48] [218500] global_step=218500, grad_norm=2.94952654838562, loss=1.4271678924560547
I0309 10:22:49.163341 139708415854336 logging_writer.py:48] [218600] global_step=218600, grad_norm=3.0947351455688477, loss=1.9770947694778442
I0309 10:23:34.412825 139708407461632 logging_writer.py:48] [218700] global_step=218700, grad_norm=3.5294477939605713, loss=3.0583720207214355
I0309 10:24:19.693003 139708415854336 logging_writer.py:48] [218800] global_step=218800, grad_norm=3.1617043018341064, loss=1.7784016132354736
I0309 10:25:05.290560 139708407461632 logging_writer.py:48] [218900] global_step=218900, grad_norm=2.825315237045288, loss=1.091638445854187
I0309 10:25:36.944798 139902746892096 spec.py:321] Evaluating on the training split.
I0309 10:25:48.387904 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 10:26:09.844005 139902746892096 spec.py:349] Evaluating on the test split.
I0309 10:26:11.523077 139902746892096 submission_runner.py:411] Time since start: 105894.55s, 	Step: 218971, 	{'train/accuracy': 0.8862499594688416, 'train/loss': 0.41989243030548096, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 97929.96910381317, 'total_duration': 105894.55187416077, 'accumulated_submission_time': 97929.96910381317, 'accumulated_eval_time': 7941.163271188736, 'accumulated_logging_time': 11.894726037979126}
I0309 10:26:11.571307 139708415854336 logging_writer.py:48] [218971] accumulated_eval_time=7941.163271, accumulated_logging_time=11.894726, accumulated_submission_time=97929.969104, global_step=218971, preemption_count=0, score=97929.969104, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=105894.551874, train/accuracy=0.886250, train/loss=0.419892, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 10:26:23.482825 139708407461632 logging_writer.py:48] [219000] global_step=219000, grad_norm=3.3206124305725098, loss=3.0196118354797363
I0309 10:27:05.683041 139708415854336 logging_writer.py:48] [219100] global_step=219100, grad_norm=3.1526451110839844, loss=2.1426210403442383
I0309 10:27:50.979173 139708407461632 logging_writer.py:48] [219200] global_step=219200, grad_norm=3.131136417388916, loss=1.1804462671279907
I0309 10:28:36.451327 139708415854336 logging_writer.py:48] [219300] global_step=219300, grad_norm=2.910313129425049, loss=1.1705777645111084
I0309 10:29:21.742353 139708407461632 logging_writer.py:48] [219400] global_step=219400, grad_norm=3.0775675773620605, loss=1.8702977895736694
I0309 10:30:07.111413 139708415854336 logging_writer.py:48] [219500] global_step=219500, grad_norm=3.01676869392395, loss=1.1502851247787476
I0309 10:30:52.519673 139708407461632 logging_writer.py:48] [219600] global_step=219600, grad_norm=2.9721529483795166, loss=1.6357269287109375
I0309 10:31:37.805567 139708415854336 logging_writer.py:48] [219700] global_step=219700, grad_norm=3.47515606880188, loss=2.831636905670166
I0309 10:32:23.392501 139708407461632 logging_writer.py:48] [219800] global_step=219800, grad_norm=3.072652816772461, loss=1.8370907306671143
I0309 10:33:08.618053 139708415854336 logging_writer.py:48] [219900] global_step=219900, grad_norm=3.0425398349761963, loss=2.0805835723876953
I0309 10:33:11.891960 139902746892096 spec.py:321] Evaluating on the training split.
I0309 10:33:23.437426 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 10:33:45.842310 139902746892096 spec.py:349] Evaluating on the test split.
I0309 10:33:47.519541 139902746892096 submission_runner.py:411] Time since start: 106350.55s, 	Step: 219909, 	{'train/accuracy': 0.887499988079071, 'train/loss': 0.4200115203857422, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 98350.23102784157, 'total_duration': 106350.54834318161, 'accumulated_submission_time': 98350.23102784157, 'accumulated_eval_time': 7976.790856599808, 'accumulated_logging_time': 11.953049659729004}
I0309 10:33:47.569188 139708407461632 logging_writer.py:48] [219909] accumulated_eval_time=7976.790857, accumulated_logging_time=11.953050, accumulated_submission_time=98350.231028, global_step=219909, preemption_count=0, score=98350.231028, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=106350.548343, train/accuracy=0.887500, train/loss=0.420012, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 10:34:23.740582 139708415854336 logging_writer.py:48] [220000] global_step=220000, grad_norm=3.295159339904785, loss=2.7754130363464355
I0309 10:35:08.970032 139708407461632 logging_writer.py:48] [220100] global_step=220100, grad_norm=3.4784557819366455, loss=1.1541764736175537
I0309 10:35:58.817778 139708415854336 logging_writer.py:48] [220200] global_step=220200, grad_norm=3.1083791255950928, loss=1.0898956060409546
I0309 10:37:02.105267 139708407461632 logging_writer.py:48] [220300] global_step=220300, grad_norm=3.5000627040863037, loss=3.0216422080993652
I0309 10:37:47.540590 139708415854336 logging_writer.py:48] [220400] global_step=220400, grad_norm=2.918294906616211, loss=1.1409884691238403
I0309 10:38:33.127114 139708407461632 logging_writer.py:48] [220500] global_step=220500, grad_norm=2.9464542865753174, loss=1.3041472434997559
I0309 10:39:18.644401 139708415854336 logging_writer.py:48] [220600] global_step=220600, grad_norm=3.149892807006836, loss=2.4956188201904297
I0309 10:40:04.093798 139708407461632 logging_writer.py:48] [220700] global_step=220700, grad_norm=3.3652892112731934, loss=2.9925730228424072
I0309 10:40:47.812663 139902746892096 spec.py:321] Evaluating on the training split.
I0309 10:40:59.035068 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 10:41:21.512267 139902746892096 spec.py:349] Evaluating on the test split.
I0309 10:41:23.175962 139902746892096 submission_runner.py:411] Time since start: 106806.20s, 	Step: 220798, 	{'train/accuracy': 0.8900781273841858, 'train/loss': 0.4076668322086334, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 98770.41761732101, 'total_duration': 106806.2047586441, 'accumulated_submission_time': 98770.41761732101, 'accumulated_eval_time': 8012.15415096283, 'accumulated_logging_time': 12.012901067733765}
I0309 10:41:23.224236 139708415854336 logging_writer.py:48] [220798] accumulated_eval_time=8012.154151, accumulated_logging_time=12.012901, accumulated_submission_time=98770.417617, global_step=220798, preemption_count=0, score=98770.417617, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=106806.204759, train/accuracy=0.890078, train/loss=0.407667, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 10:41:24.415160 139708407461632 logging_writer.py:48] [220800] global_step=220800, grad_norm=3.0733323097229004, loss=1.086124062538147
I0309 10:42:04.309039 139708415854336 logging_writer.py:48] [220900] global_step=220900, grad_norm=3.2807247638702393, loss=1.1284205913543701
I0309 10:42:49.423168 139708407461632 logging_writer.py:48] [221000] global_step=221000, grad_norm=2.8487839698791504, loss=1.0922372341156006
I0309 10:43:35.138831 139708415854336 logging_writer.py:48] [221100] global_step=221100, grad_norm=3.104762077331543, loss=1.2791619300842285
I0309 10:44:20.855073 139708407461632 logging_writer.py:48] [221200] global_step=221200, grad_norm=3.5392420291900635, loss=3.0020546913146973
I0309 10:45:06.033395 139708415854336 logging_writer.py:48] [221300] global_step=221300, grad_norm=2.9144740104675293, loss=2.171384811401367
I0309 10:45:51.666782 139708407461632 logging_writer.py:48] [221400] global_step=221400, grad_norm=3.2261364459991455, loss=2.6123900413513184
I0309 10:46:37.264876 139708415854336 logging_writer.py:48] [221500] global_step=221500, grad_norm=3.0769591331481934, loss=1.978145956993103
I0309 10:47:22.410316 139708407461632 logging_writer.py:48] [221600] global_step=221600, grad_norm=3.0686943531036377, loss=1.6833571195602417
I0309 10:48:08.314275 139708415854336 logging_writer.py:48] [221700] global_step=221700, grad_norm=2.903968334197998, loss=2.2730014324188232
I0309 10:48:23.453191 139902746892096 spec.py:321] Evaluating on the training split.
I0309 10:48:34.903540 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 10:48:56.593533 139902746892096 spec.py:349] Evaluating on the test split.
I0309 10:48:58.271643 139902746892096 submission_runner.py:411] Time since start: 107261.30s, 	Step: 221735, 	{'train/accuracy': 0.8908789157867432, 'train/loss': 0.41152194142341614, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 99190.58779096603, 'total_duration': 107261.30042052269, 'accumulated_submission_time': 99190.58779096603, 'accumulated_eval_time': 8046.972583532333, 'accumulated_logging_time': 12.071365118026733}
I0309 10:48:58.319694 139708407461632 logging_writer.py:48] [221735] accumulated_eval_time=8046.972584, accumulated_logging_time=12.071365, accumulated_submission_time=99190.587791, global_step=221735, preemption_count=0, score=99190.587791, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=107261.300421, train/accuracy=0.890879, train/loss=0.411522, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 10:49:24.261071 139708415854336 logging_writer.py:48] [221800] global_step=221800, grad_norm=2.9795401096343994, loss=1.0823074579238892
I0309 10:50:08.399034 139708407461632 logging_writer.py:48] [221900] global_step=221900, grad_norm=3.031303882598877, loss=1.7239644527435303
I0309 10:50:53.761874 139708415854336 logging_writer.py:48] [222000] global_step=222000, grad_norm=3.3590102195739746, loss=2.271602153778076
I0309 10:51:39.843216 139708407461632 logging_writer.py:48] [222100] global_step=222100, grad_norm=3.157716989517212, loss=1.0928456783294678
I0309 10:52:25.182351 139708415854336 logging_writer.py:48] [222200] global_step=222200, grad_norm=3.132796049118042, loss=2.8493945598602295
I0309 10:53:10.541896 139708407461632 logging_writer.py:48] [222300] global_step=222300, grad_norm=3.0563652515411377, loss=1.6036758422851562
I0309 10:53:55.797784 139708415854336 logging_writer.py:48] [222400] global_step=222400, grad_norm=3.6360137462615967, loss=3.162986993789673
I0309 10:54:41.239235 139708407461632 logging_writer.py:48] [222500] global_step=222500, grad_norm=3.367704153060913, loss=2.490760326385498
I0309 10:55:26.536042 139708415854336 logging_writer.py:48] [222600] global_step=222600, grad_norm=3.1538238525390625, loss=1.2878451347351074
I0309 10:55:58.510790 139902746892096 spec.py:321] Evaluating on the training split.
I0309 10:56:09.978664 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 10:56:32.994397 139902746892096 spec.py:349] Evaluating on the test split.
I0309 10:56:34.665671 139902746892096 submission_runner.py:411] Time since start: 107717.69s, 	Step: 222672, 	{'train/accuracy': 0.8867382407188416, 'train/loss': 0.4187392294406891, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 99610.72120189667, 'total_duration': 107717.69447016716, 'accumulated_submission_time': 99610.72120189667, 'accumulated_eval_time': 8083.12747168541, 'accumulated_logging_time': 12.127808094024658}
I0309 10:56:34.715164 139708407461632 logging_writer.py:48] [222672] accumulated_eval_time=8083.127472, accumulated_logging_time=12.127808, accumulated_submission_time=99610.721202, global_step=222672, preemption_count=0, score=99610.721202, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=107717.694470, train/accuracy=0.886738, train/loss=0.418739, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 10:56:46.131740 139708415854336 logging_writer.py:48] [222700] global_step=222700, grad_norm=3.115846872329712, loss=1.1092904806137085
I0309 10:57:27.763862 139708407461632 logging_writer.py:48] [222800] global_step=222800, grad_norm=3.2521798610687256, loss=2.2459356784820557
I0309 10:58:13.369980 139708415854336 logging_writer.py:48] [222900] global_step=222900, grad_norm=3.294241189956665, loss=1.8908888101577759
I0309 10:58:59.084769 139708407461632 logging_writer.py:48] [223000] global_step=223000, grad_norm=2.8957769870758057, loss=2.0794966220855713
I0309 10:59:44.427755 139708415854336 logging_writer.py:48] [223100] global_step=223100, grad_norm=3.3734798431396484, loss=2.7151777744293213
I0309 11:00:29.882767 139708407461632 logging_writer.py:48] [223200] global_step=223200, grad_norm=3.9161853790283203, loss=3.233429431915283
I0309 11:01:15.555680 139708415854336 logging_writer.py:48] [223300] global_step=223300, grad_norm=3.3184094429016113, loss=1.3323951959609985
I0309 11:02:00.717349 139708407461632 logging_writer.py:48] [223400] global_step=223400, grad_norm=2.8892595767974854, loss=2.2880923748016357
I0309 11:02:46.160054 139708415854336 logging_writer.py:48] [223500] global_step=223500, grad_norm=3.5537962913513184, loss=1.1429976224899292
I0309 11:03:31.724188 139708407461632 logging_writer.py:48] [223600] global_step=223600, grad_norm=4.043113708496094, loss=3.1896939277648926
I0309 11:03:35.034974 139902746892096 spec.py:321] Evaluating on the training split.
I0309 11:03:46.444033 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 11:04:06.637744 139902746892096 spec.py:349] Evaluating on the test split.
I0309 11:04:08.327967 139902746892096 submission_runner.py:411] Time since start: 108171.36s, 	Step: 223609, 	{'train/accuracy': 0.8858398199081421, 'train/loss': 0.4228871762752533, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 100030.98379468918, 'total_duration': 108171.35675144196, 'accumulated_submission_time': 100030.98379468918, 'accumulated_eval_time': 8116.420456647873, 'accumulated_logging_time': 12.186193466186523}
I0309 11:04:08.386456 139708415854336 logging_writer.py:48] [223609] accumulated_eval_time=8116.420457, accumulated_logging_time=12.186193, accumulated_submission_time=100030.983795, global_step=223609, preemption_count=0, score=100030.983795, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=108171.356751, train/accuracy=0.885840, train/loss=0.422887, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 11:04:45.204634 139708407461632 logging_writer.py:48] [223700] global_step=223700, grad_norm=3.1333398818969727, loss=1.082788109779358
I0309 11:05:30.435057 139708415854336 logging_writer.py:48] [223800] global_step=223800, grad_norm=2.97301983833313, loss=2.354612350463867
I0309 11:06:15.711822 139708407461632 logging_writer.py:48] [223900] global_step=223900, grad_norm=3.302208423614502, loss=1.3506953716278076
I0309 11:07:01.528744 139708415854336 logging_writer.py:48] [224000] global_step=224000, grad_norm=3.2084808349609375, loss=2.414604902267456
I0309 11:07:47.077113 139708407461632 logging_writer.py:48] [224100] global_step=224100, grad_norm=2.939979314804077, loss=1.0737472772598267
I0309 11:08:32.414232 139708415854336 logging_writer.py:48] [224200] global_step=224200, grad_norm=4.193165302276611, loss=3.262697219848633
I0309 11:09:17.928167 139708407461632 logging_writer.py:48] [224300] global_step=224300, grad_norm=2.9298439025878906, loss=2.0506014823913574
I0309 11:10:03.476165 139708415854336 logging_writer.py:48] [224400] global_step=224400, grad_norm=3.1621124744415283, loss=1.3138185739517212
I0309 11:10:49.018134 139708407461632 logging_writer.py:48] [224500] global_step=224500, grad_norm=2.820863962173462, loss=1.8344868421554565
I0309 11:11:08.683362 139902746892096 spec.py:321] Evaluating on the training split.
I0309 11:11:20.211315 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 11:11:42.042212 139902746892096 spec.py:349] Evaluating on the test split.
I0309 11:11:43.709273 139902746892096 submission_runner.py:411] Time since start: 108626.74s, 	Step: 224545, 	{'train/accuracy': 0.8891015648841858, 'train/loss': 0.4168025851249695, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 100451.22117090225, 'total_duration': 108626.73807239532, 'accumulated_submission_time': 100451.22117090225, 'accumulated_eval_time': 8151.446369409561, 'accumulated_logging_time': 12.254721403121948}
I0309 11:11:43.759594 139708415854336 logging_writer.py:48] [224545] accumulated_eval_time=8151.446369, accumulated_logging_time=12.254721, accumulated_submission_time=100451.221171, global_step=224545, preemption_count=0, score=100451.221171, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=108626.738072, train/accuracy=0.889102, train/loss=0.416803, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 11:12:05.791916 139708407461632 logging_writer.py:48] [224600] global_step=224600, grad_norm=2.8695735931396484, loss=1.9757153987884521
I0309 11:12:48.964590 139708415854336 logging_writer.py:48] [224700] global_step=224700, grad_norm=3.020455837249756, loss=1.12819504737854
I0309 11:13:34.398607 139708407461632 logging_writer.py:48] [224800] global_step=224800, grad_norm=3.9896621704101562, loss=3.2102553844451904
I0309 11:14:20.201746 139708415854336 logging_writer.py:48] [224900] global_step=224900, grad_norm=3.1191515922546387, loss=1.2823894023895264
I0309 11:15:05.675171 139708407461632 logging_writer.py:48] [225000] global_step=225000, grad_norm=3.3068933486938477, loss=1.8766169548034668
I0309 11:15:51.062831 139708415854336 logging_writer.py:48] [225100] global_step=225100, grad_norm=3.128805160522461, loss=2.886594533920288
I0309 11:16:36.564889 139708407461632 logging_writer.py:48] [225200] global_step=225200, grad_norm=3.2978107929229736, loss=1.3961389064788818
I0309 11:17:22.005668 139708415854336 logging_writer.py:48] [225300] global_step=225300, grad_norm=3.1958746910095215, loss=1.0025783777236938
I0309 11:18:07.688333 139708407461632 logging_writer.py:48] [225400] global_step=225400, grad_norm=3.311432123184204, loss=1.0751068592071533
I0309 11:18:44.075402 139902746892096 spec.py:321] Evaluating on the training split.
I0309 11:18:55.462506 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 11:19:15.684632 139902746892096 spec.py:349] Evaluating on the test split.
I0309 11:19:17.368725 139902746892096 submission_runner.py:411] Time since start: 109080.40s, 	Step: 225482, 	{'train/accuracy': 0.8861327767372131, 'train/loss': 0.42009851336479187, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 100871.47759699821, 'total_duration': 109080.39751529694, 'accumulated_submission_time': 100871.47759699821, 'accumulated_eval_time': 8184.739677429199, 'accumulated_logging_time': 12.315647602081299}
I0309 11:19:17.426969 139708415854336 logging_writer.py:48] [225482] accumulated_eval_time=8184.739677, accumulated_logging_time=12.315648, accumulated_submission_time=100871.477597, global_step=225482, preemption_count=0, score=100871.477597, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=109080.397515, train/accuracy=0.886133, train/loss=0.420099, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 11:19:24.894666 139708407461632 logging_writer.py:48] [225500] global_step=225500, grad_norm=3.0018396377563477, loss=1.358566403388977
I0309 11:20:06.292129 139708415854336 logging_writer.py:48] [225600] global_step=225600, grad_norm=3.157482147216797, loss=1.1150181293487549
I0309 11:20:51.681727 139708407461632 logging_writer.py:48] [225700] global_step=225700, grad_norm=3.039874792098999, loss=1.0958737134933472
I0309 11:21:37.162864 139708415854336 logging_writer.py:48] [225800] global_step=225800, grad_norm=3.0925471782684326, loss=1.164258360862732
I0309 11:22:22.859850 139708407461632 logging_writer.py:48] [225900] global_step=225900, grad_norm=3.0343518257141113, loss=1.156915545463562
I0309 11:23:08.028688 139708415854336 logging_writer.py:48] [226000] global_step=226000, grad_norm=3.1135053634643555, loss=1.1394476890563965
I0309 11:23:53.518331 139708407461632 logging_writer.py:48] [226100] global_step=226100, grad_norm=5.679728031158447, loss=1.167810320854187
I0309 11:24:38.953340 139708415854336 logging_writer.py:48] [226200] global_step=226200, grad_norm=3.186199903488159, loss=1.1382273435592651
I0309 11:25:24.246049 139708407461632 logging_writer.py:48] [226300] global_step=226300, grad_norm=2.887985944747925, loss=1.7406225204467773
I0309 11:26:09.661279 139708415854336 logging_writer.py:48] [226400] global_step=226400, grad_norm=3.3277084827423096, loss=2.701488733291626
I0309 11:26:17.449465 139902746892096 spec.py:321] Evaluating on the training split.
I0309 11:26:28.749204 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 11:26:49.709436 139902746892096 spec.py:349] Evaluating on the test split.
I0309 11:26:51.397367 139902746892096 submission_runner.py:411] Time since start: 109534.43s, 	Step: 226419, 	{'train/accuracy': 0.8892577886581421, 'train/loss': 0.4118213951587677, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 101291.44068813324, 'total_duration': 109534.42616176605, 'accumulated_submission_time': 101291.44068813324, 'accumulated_eval_time': 8218.687573194504, 'accumulated_logging_time': 12.384529113769531}
I0309 11:26:51.447486 139708407461632 logging_writer.py:48] [226419] accumulated_eval_time=8218.687573, accumulated_logging_time=12.384529, accumulated_submission_time=101291.440688, global_step=226419, preemption_count=0, score=101291.440688, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=109534.426162, train/accuracy=0.889258, train/loss=0.411821, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 11:27:23.699608 139708415854336 logging_writer.py:48] [226500] global_step=226500, grad_norm=3.1721138954162598, loss=1.2866222858428955
I0309 11:28:08.376466 139708407461632 logging_writer.py:48] [226600] global_step=226600, grad_norm=3.1409542560577393, loss=1.1339240074157715
I0309 11:28:53.973806 139708415854336 logging_writer.py:48] [226700] global_step=226700, grad_norm=3.1860368251800537, loss=1.0485689640045166
I0309 11:29:39.194251 139708407461632 logging_writer.py:48] [226800] global_step=226800, grad_norm=3.463874101638794, loss=2.7674856185913086
I0309 11:30:24.308626 139708415854336 logging_writer.py:48] [226900] global_step=226900, grad_norm=3.202951669692993, loss=1.7756881713867188
I0309 11:31:09.765480 139708407461632 logging_writer.py:48] [227000] global_step=227000, grad_norm=3.1850955486297607, loss=1.148556113243103
I0309 11:31:55.034063 139708415854336 logging_writer.py:48] [227100] global_step=227100, grad_norm=3.132453441619873, loss=1.152627944946289
I0309 11:32:40.413320 139708407461632 logging_writer.py:48] [227200] global_step=227200, grad_norm=3.674365997314453, loss=3.181514024734497
I0309 11:33:26.102626 139708415854336 logging_writer.py:48] [227300] global_step=227300, grad_norm=2.8906290531158447, loss=1.4970993995666504
I0309 11:33:51.811581 139902746892096 spec.py:321] Evaluating on the training split.
I0309 11:34:03.432253 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 11:34:25.522793 139902746892096 spec.py:349] Evaluating on the test split.
I0309 11:34:27.200941 139902746892096 submission_runner.py:411] Time since start: 109990.23s, 	Step: 227359, 	{'train/accuracy': 0.8875781297683716, 'train/loss': 0.41566091775894165, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 101711.74678444862, 'total_duration': 109990.22974205017, 'accumulated_submission_time': 101711.74678444862, 'accumulated_eval_time': 8254.07694530487, 'accumulated_logging_time': 12.444185018539429}
I0309 11:34:27.253772 139708407461632 logging_writer.py:48] [227359] accumulated_eval_time=8254.076945, accumulated_logging_time=12.444185, accumulated_submission_time=101711.746784, global_step=227359, preemption_count=0, score=101711.746784, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=109990.229742, train/accuracy=0.887578, train/loss=0.415661, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 11:34:43.782772 139708415854336 logging_writer.py:48] [227400] global_step=227400, grad_norm=3.0175578594207764, loss=1.166304111480713
I0309 11:35:26.121750 139708407461632 logging_writer.py:48] [227500] global_step=227500, grad_norm=3.058253049850464, loss=1.141869306564331
I0309 11:36:11.691662 139708415854336 logging_writer.py:48] [227600] global_step=227600, grad_norm=2.9924330711364746, loss=1.8300716876983643
I0309 11:36:57.159939 139708407461632 logging_writer.py:48] [227700] global_step=227700, grad_norm=3.0910022258758545, loss=1.1554181575775146
I0309 11:37:42.747216 139708415854336 logging_writer.py:48] [227800] global_step=227800, grad_norm=3.4210691452026367, loss=1.1425150632858276
I0309 11:38:28.233969 139708407461632 logging_writer.py:48] [227900] global_step=227900, grad_norm=2.9489364624023438, loss=1.0697460174560547
I0309 11:39:13.391653 139708415854336 logging_writer.py:48] [228000] global_step=228000, grad_norm=3.210808753967285, loss=1.4498177766799927
I0309 11:39:58.793518 139708407461632 logging_writer.py:48] [228100] global_step=228100, grad_norm=3.6818103790283203, loss=3.100675582885742
I0309 11:40:44.178858 139708415854336 logging_writer.py:48] [228200] global_step=228200, grad_norm=3.2513253688812256, loss=2.3654158115386963
I0309 11:41:27.417540 139902746892096 spec.py:321] Evaluating on the training split.
I0309 11:41:38.824388 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 11:42:00.051726 139902746892096 spec.py:349] Evaluating on the test split.
I0309 11:42:01.734221 139902746892096 submission_runner.py:411] Time since start: 110444.76s, 	Step: 228297, 	{'train/accuracy': 0.8884570002555847, 'train/loss': 0.4122214913368225, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 102131.85267829895, 'total_duration': 110444.76302242279, 'accumulated_submission_time': 102131.85267829895, 'accumulated_eval_time': 8288.393639802933, 'accumulated_logging_time': 12.505478382110596}
I0309 11:42:01.783525 139708407461632 logging_writer.py:48] [228297] accumulated_eval_time=8288.393640, accumulated_logging_time=12.505478, accumulated_submission_time=102131.852678, global_step=228297, preemption_count=0, score=102131.852678, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=110444.763022, train/accuracy=0.888457, train/loss=0.412221, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 11:42:03.359856 139708415854336 logging_writer.py:48] [228300] global_step=228300, grad_norm=3.0758144855499268, loss=1.1601102352142334
I0309 11:42:43.874768 139708407461632 logging_writer.py:48] [228400] global_step=228400, grad_norm=3.8541204929351807, loss=3.0005831718444824
I0309 11:43:29.143732 139708415854336 logging_writer.py:48] [228500] global_step=228500, grad_norm=3.2456696033477783, loss=1.0879130363464355
I0309 11:44:14.731973 139708407461632 logging_writer.py:48] [228600] global_step=228600, grad_norm=2.9609949588775635, loss=1.344998836517334
I0309 11:45:00.182707 139708415854336 logging_writer.py:48] [228700] global_step=228700, grad_norm=3.07843279838562, loss=2.3274011611938477
I0309 11:45:45.554718 139708407461632 logging_writer.py:48] [228800] global_step=228800, grad_norm=3.214252233505249, loss=1.096505045890808
I0309 11:46:30.867013 139708415854336 logging_writer.py:48] [228900] global_step=228900, grad_norm=3.4051871299743652, loss=1.1134161949157715
I0309 11:47:16.712661 139708407461632 logging_writer.py:48] [229000] global_step=229000, grad_norm=3.3591501712799072, loss=2.4645774364471436
I0309 11:48:02.194775 139708415854336 logging_writer.py:48] [229100] global_step=229100, grad_norm=3.2879531383514404, loss=1.2321113348007202
I0309 11:48:47.443016 139708407461632 logging_writer.py:48] [229200] global_step=229200, grad_norm=3.203261613845825, loss=2.064769983291626
I0309 11:49:02.006366 139902746892096 spec.py:321] Evaluating on the training split.
I0309 11:49:13.337095 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 11:49:35.984405 139902746892096 spec.py:349] Evaluating on the test split.
I0309 11:49:37.659625 139902746892096 submission_runner.py:411] Time since start: 110900.69s, 	Step: 229234, 	{'train/accuracy': 0.8875585794448853, 'train/loss': 0.4155343770980835, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 102552.01909089088, 'total_duration': 110900.68841600418, 'accumulated_submission_time': 102552.01909089088, 'accumulated_eval_time': 8324.046884298325, 'accumulated_logging_time': 12.562983989715576}
I0309 11:49:37.710639 139708415854336 logging_writer.py:48] [229234] accumulated_eval_time=8324.046884, accumulated_logging_time=12.562984, accumulated_submission_time=102552.019091, global_step=229234, preemption_count=0, score=102552.019091, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=110900.688416, train/accuracy=0.887559, train/loss=0.415534, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 11:50:04.043250 139708407461632 logging_writer.py:48] [229300] global_step=229300, grad_norm=3.1793079376220703, loss=1.1013051271438599
I0309 11:50:47.650012 139708415854336 logging_writer.py:48] [229400] global_step=229400, grad_norm=2.904996156692505, loss=1.286442756652832
I0309 11:51:33.196220 139708407461632 logging_writer.py:48] [229500] global_step=229500, grad_norm=3.165281295776367, loss=1.2155895233154297
I0309 11:52:18.672708 139708415854336 logging_writer.py:48] [229600] global_step=229600, grad_norm=3.11734938621521, loss=1.4381120204925537
I0309 11:53:03.764373 139708407461632 logging_writer.py:48] [229700] global_step=229700, grad_norm=2.8671932220458984, loss=1.779271125793457
I0309 11:53:49.335269 139708415854336 logging_writer.py:48] [229800] global_step=229800, grad_norm=3.1290817260742188, loss=1.1639240980148315
I0309 11:54:34.242966 139708407461632 logging_writer.py:48] [229900] global_step=229900, grad_norm=3.9884531497955322, loss=2.8595690727233887
I0309 11:55:19.469654 139708415854336 logging_writer.py:48] [230000] global_step=230000, grad_norm=3.4063827991485596, loss=2.7762773036956787
I0309 11:56:04.742690 139708407461632 logging_writer.py:48] [230100] global_step=230100, grad_norm=3.3291521072387695, loss=1.036211609840393
I0309 11:56:37.973989 139902746892096 spec.py:321] Evaluating on the training split.
I0309 11:56:49.231130 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 11:57:10.422032 139902746892096 spec.py:349] Evaluating on the test split.
I0309 11:57:12.114609 139902746892096 submission_runner.py:411] Time since start: 111355.14s, 	Step: 230175, 	{'train/accuracy': 0.8898828029632568, 'train/loss': 0.4132254719734192, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 102972.22385382652, 'total_duration': 111355.14338946342, 'accumulated_submission_time': 102972.22385382652, 'accumulated_eval_time': 8358.187504053116, 'accumulated_logging_time': 12.623552083969116}
I0309 11:57:12.173429 139708415854336 logging_writer.py:48] [230175] accumulated_eval_time=8358.187504, accumulated_logging_time=12.623552, accumulated_submission_time=102972.223854, global_step=230175, preemption_count=0, score=102972.223854, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=111355.143389, train/accuracy=0.889883, train/loss=0.413225, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 11:57:22.404291 139708407461632 logging_writer.py:48] [230200] global_step=230200, grad_norm=3.1196961402893066, loss=1.158310055732727
I0309 11:58:04.234898 139708415854336 logging_writer.py:48] [230300] global_step=230300, grad_norm=2.9200801849365234, loss=1.2361623048782349
I0309 11:58:49.321696 139708407461632 logging_writer.py:48] [230400] global_step=230400, grad_norm=3.242375612258911, loss=1.0520837306976318
I0309 11:59:34.893038 139708415854336 logging_writer.py:48] [230500] global_step=230500, grad_norm=3.3858444690704346, loss=1.16217041015625
I0309 12:00:20.107418 139708407461632 logging_writer.py:48] [230600] global_step=230600, grad_norm=3.6373748779296875, loss=1.1071460247039795
I0309 12:01:05.566119 139708415854336 logging_writer.py:48] [230700] global_step=230700, grad_norm=2.9731285572052, loss=1.0572506189346313
I0309 12:01:50.740981 139708407461632 logging_writer.py:48] [230800] global_step=230800, grad_norm=2.945307731628418, loss=1.0931367874145508
I0309 12:02:35.781294 139708415854336 logging_writer.py:48] [230900] global_step=230900, grad_norm=3.229757070541382, loss=1.1958444118499756
I0309 12:03:20.992988 139708407461632 logging_writer.py:48] [231000] global_step=231000, grad_norm=3.0675246715545654, loss=1.077045202255249
I0309 12:04:06.315077 139708415854336 logging_writer.py:48] [231100] global_step=231100, grad_norm=3.003124475479126, loss=1.462742567062378
I0309 12:04:12.292900 139902746892096 spec.py:321] Evaluating on the training split.
I0309 12:04:23.691977 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 12:04:43.181912 139902746892096 spec.py:349] Evaluating on the test split.
I0309 12:04:44.868121 139902746892096 submission_runner.py:411] Time since start: 111807.90s, 	Step: 231115, 	{'train/accuracy': 0.88916015625, 'train/loss': 0.4119197726249695, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 103392.28315424919, 'total_duration': 111807.89689588547, 'accumulated_submission_time': 103392.28315424919, 'accumulated_eval_time': 8390.762695789337, 'accumulated_logging_time': 12.692643880844116}
I0309 12:04:44.937245 139708407461632 logging_writer.py:48] [231115] accumulated_eval_time=8390.762696, accumulated_logging_time=12.692644, accumulated_submission_time=103392.283154, global_step=231115, preemption_count=0, score=103392.283154, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=111807.896896, train/accuracy=0.889160, train/loss=0.411920, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 12:05:19.147937 139708415854336 logging_writer.py:48] [231200] global_step=231200, grad_norm=3.343773603439331, loss=1.2869880199432373
I0309 12:06:04.371733 139708407461632 logging_writer.py:48] [231300] global_step=231300, grad_norm=2.9601309299468994, loss=1.5903931856155396
I0309 12:06:49.625908 139708415854336 logging_writer.py:48] [231400] global_step=231400, grad_norm=3.645272731781006, loss=3.3099148273468018
I0309 12:07:35.393319 139708407461632 logging_writer.py:48] [231500] global_step=231500, grad_norm=3.136695623397827, loss=1.5133332014083862
I0309 12:08:20.553944 139708415854336 logging_writer.py:48] [231600] global_step=231600, grad_norm=2.884920835494995, loss=1.1300257444381714
I0309 12:09:05.976385 139708407461632 logging_writer.py:48] [231700] global_step=231700, grad_norm=2.9975833892822266, loss=1.1666615009307861
I0309 12:09:51.152281 139708415854336 logging_writer.py:48] [231800] global_step=231800, grad_norm=3.1209945678710938, loss=1.0974973440170288
I0309 12:10:36.459564 139708407461632 logging_writer.py:48] [231900] global_step=231900, grad_norm=3.3085215091705322, loss=1.3854440450668335
I0309 12:11:22.065119 139708415854336 logging_writer.py:48] [232000] global_step=232000, grad_norm=3.0496842861175537, loss=1.4524705410003662
I0309 12:11:45.231315 139902746892096 spec.py:321] Evaluating on the training split.
I0309 12:11:56.806840 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 12:12:18.193614 139902746892096 spec.py:349] Evaluating on the test split.
I0309 12:12:19.873957 139902746892096 submission_runner.py:411] Time since start: 112262.90s, 	Step: 232053, 	{'train/accuracy': 0.889453113079071, 'train/loss': 0.4119241535663605, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 103812.51510357857, 'total_duration': 112262.90275025368, 'accumulated_submission_time': 103812.51510357857, 'accumulated_eval_time': 8425.405321359634, 'accumulated_logging_time': 12.775279998779297}
I0309 12:12:19.923974 139708407461632 logging_writer.py:48] [232053] accumulated_eval_time=8425.405321, accumulated_logging_time=12.775280, accumulated_submission_time=103812.515104, global_step=232053, preemption_count=0, score=103812.515104, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=112262.902750, train/accuracy=0.889453, train/loss=0.411924, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 12:12:38.808794 139708415854336 logging_writer.py:48] [232100] global_step=232100, grad_norm=2.9458532333374023, loss=1.264432430267334
I0309 12:13:21.337180 139708407461632 logging_writer.py:48] [232200] global_step=232200, grad_norm=3.0074915885925293, loss=2.19598388671875
I0309 12:14:06.912691 139708415854336 logging_writer.py:48] [232300] global_step=232300, grad_norm=3.082627058029175, loss=1.6011810302734375
I0309 12:14:52.235733 139708407461632 logging_writer.py:48] [232400] global_step=232400, grad_norm=3.0947842597961426, loss=1.1191998720169067
I0309 12:15:37.511133 139708415854336 logging_writer.py:48] [232500] global_step=232500, grad_norm=3.7781972885131836, loss=3.250575542449951
I0309 12:16:22.652927 139708407461632 logging_writer.py:48] [232600] global_step=232600, grad_norm=3.238537549972534, loss=1.3957433700561523
I0309 12:17:08.401179 139708415854336 logging_writer.py:48] [232700] global_step=232700, grad_norm=3.2031917572021484, loss=1.5285413265228271
I0309 12:17:53.746956 139708407461632 logging_writer.py:48] [232800] global_step=232800, grad_norm=2.9964840412139893, loss=1.0461182594299316
I0309 12:18:38.978428 139708415854336 logging_writer.py:48] [232900] global_step=232900, grad_norm=2.782999038696289, loss=1.0280768871307373
I0309 12:19:20.254020 139902746892096 spec.py:321] Evaluating on the training split.
I0309 12:19:31.668024 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 12:19:53.608198 139902746892096 spec.py:349] Evaluating on the test split.
I0309 12:19:55.277139 139902746892096 submission_runner.py:411] Time since start: 112718.31s, 	Step: 232993, 	{'train/accuracy': 0.8890820145606995, 'train/loss': 0.4118773639202118, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 104232.78696107864, 'total_duration': 112718.30594062805, 'accumulated_submission_time': 104232.78696107864, 'accumulated_eval_time': 8460.428438425064, 'accumulated_logging_time': 12.834226846694946}
I0309 12:19:55.325953 139708407461632 logging_writer.py:48] [232993] accumulated_eval_time=8460.428438, accumulated_logging_time=12.834227, accumulated_submission_time=104232.786961, global_step=232993, preemption_count=0, score=104232.786961, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=112718.305941, train/accuracy=0.889082, train/loss=0.411877, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 12:19:58.476664 139708415854336 logging_writer.py:48] [233000] global_step=233000, grad_norm=3.0909886360168457, loss=1.2063419818878174
I0309 12:20:38.500332 139708407461632 logging_writer.py:48] [233100] global_step=233100, grad_norm=3.1860432624816895, loss=1.3390686511993408
I0309 12:21:23.658378 139708415854336 logging_writer.py:48] [233200] global_step=233200, grad_norm=3.091210126876831, loss=1.0581393241882324
I0309 12:22:09.065479 139708407461632 logging_writer.py:48] [233300] global_step=233300, grad_norm=2.817143678665161, loss=1.8039213418960571
I0309 12:22:54.359522 139708415854336 logging_writer.py:48] [233400] global_step=233400, grad_norm=3.333300828933716, loss=2.7841615676879883
I0309 12:23:39.566641 139708407461632 logging_writer.py:48] [233500] global_step=233500, grad_norm=3.235534429550171, loss=1.1610716581344604
I0309 12:24:24.858420 139708415854336 logging_writer.py:48] [233600] global_step=233600, grad_norm=3.376432180404663, loss=1.1457128524780273
I0309 12:25:10.109155 139708407461632 logging_writer.py:48] [233700] global_step=233700, grad_norm=3.0141963958740234, loss=2.098421096801758
I0309 12:25:55.507940 139708415854336 logging_writer.py:48] [233800] global_step=233800, grad_norm=3.957056760787964, loss=3.253269672393799
I0309 12:26:40.838046 139708407461632 logging_writer.py:48] [233900] global_step=233900, grad_norm=3.4181065559387207, loss=1.254453182220459
I0309 12:26:55.467343 139902746892096 spec.py:321] Evaluating on the training split.
I0309 12:27:07.498222 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 12:27:29.704258 139902746892096 spec.py:349] Evaluating on the test split.
I0309 12:27:31.377168 139902746892096 submission_runner.py:411] Time since start: 113174.41s, 	Step: 233934, 	{'train/accuracy': 0.8887304663658142, 'train/loss': 0.40977874398231506, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 104652.87011408806, 'total_duration': 113174.4059624672, 'accumulated_submission_time': 104652.87011408806, 'accumulated_eval_time': 8496.338250160217, 'accumulated_logging_time': 12.893026351928711}
I0309 12:27:31.432346 139708415854336 logging_writer.py:48] [233934] accumulated_eval_time=8496.338250, accumulated_logging_time=12.893026, accumulated_submission_time=104652.870114, global_step=233934, preemption_count=0, score=104652.870114, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=113174.405962, train/accuracy=0.888730, train/loss=0.409779, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 12:27:57.791111 139708407461632 logging_writer.py:48] [234000] global_step=234000, grad_norm=3.191178560256958, loss=2.6206133365631104
I0309 12:28:41.676673 139708415854336 logging_writer.py:48] [234100] global_step=234100, grad_norm=2.945812702178955, loss=2.037799119949341
I0309 12:29:27.181052 139708407461632 logging_writer.py:48] [234200] global_step=234200, grad_norm=3.0855350494384766, loss=1.0744423866271973
I0309 12:30:13.054596 139708415854336 logging_writer.py:48] [234300] global_step=234300, grad_norm=3.009547233581543, loss=1.1918190717697144
I0309 12:30:58.266824 139708407461632 logging_writer.py:48] [234400] global_step=234400, grad_norm=2.829819679260254, loss=1.2204477787017822
I0309 12:31:43.748259 139708415854336 logging_writer.py:48] [234500] global_step=234500, grad_norm=3.0196340084075928, loss=1.1557425260543823
I0309 12:32:28.950083 139708407461632 logging_writer.py:48] [234600] global_step=234600, grad_norm=3.9080100059509277, loss=3.2776384353637695
I0309 12:33:14.653773 139708415854336 logging_writer.py:48] [234700] global_step=234700, grad_norm=3.2768542766571045, loss=1.9562066793441772
I0309 12:34:00.099760 139708407461632 logging_writer.py:48] [234800] global_step=234800, grad_norm=2.9075241088867188, loss=1.0761699676513672
I0309 12:34:31.383541 139902746892096 spec.py:321] Evaluating on the training split.
I0309 12:34:42.812688 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 12:35:05.256968 139902746892096 spec.py:349] Evaluating on the test split.
I0309 12:35:06.925316 139902746892096 submission_runner.py:411] Time since start: 113629.95s, 	Step: 234871, 	{'train/accuracy': 0.8856640458106995, 'train/loss': 0.4191117286682129, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 105072.76435279846, 'total_duration': 113629.95410299301, 'accumulated_submission_time': 105072.76435279846, 'accumulated_eval_time': 8531.880004882812, 'accumulated_logging_time': 12.956977844238281}
I0309 12:35:06.982141 139708415854336 logging_writer.py:48] [234871] accumulated_eval_time=8531.880005, accumulated_logging_time=12.956978, accumulated_submission_time=105072.764353, global_step=234871, preemption_count=0, score=105072.764353, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=113629.954103, train/accuracy=0.885664, train/loss=0.419112, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 12:35:18.782716 139708407461632 logging_writer.py:48] [234900] global_step=234900, grad_norm=2.9104220867156982, loss=1.4848994016647339
I0309 12:36:00.465312 139708415854336 logging_writer.py:48] [235000] global_step=235000, grad_norm=3.2467315196990967, loss=1.0646599531173706
I0309 12:36:45.740964 139708407461632 logging_writer.py:48] [235100] global_step=235100, grad_norm=3.2173049449920654, loss=2.1645302772521973
I0309 12:37:31.459942 139708415854336 logging_writer.py:48] [235200] global_step=235200, grad_norm=2.8891897201538086, loss=1.0553851127624512
I0309 12:38:17.039978 139708407461632 logging_writer.py:48] [235300] global_step=235300, grad_norm=3.097578763961792, loss=2.4813833236694336
I0309 12:39:01.988810 139708415854336 logging_writer.py:48] [235400] global_step=235400, grad_norm=2.9186580181121826, loss=1.2217741012573242
I0309 12:39:47.178637 139708407461632 logging_writer.py:48] [235500] global_step=235500, grad_norm=3.0294060707092285, loss=1.0367896556854248
I0309 12:40:32.304245 139708415854336 logging_writer.py:48] [235600] global_step=235600, grad_norm=2.8776662349700928, loss=1.6940393447875977
I0309 12:41:17.559579 139708407461632 logging_writer.py:48] [235700] global_step=235700, grad_norm=3.3762319087982178, loss=2.693740129470825
I0309 12:42:02.739486 139708415854336 logging_writer.py:48] [235800] global_step=235800, grad_norm=3.2642459869384766, loss=1.0576432943344116
I0309 12:42:06.975858 139902746892096 spec.py:321] Evaluating on the training split.
I0309 12:42:18.598023 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 12:42:39.766119 139902746892096 spec.py:349] Evaluating on the test split.
I0309 12:42:41.451643 139902746892096 submission_runner.py:411] Time since start: 114084.48s, 	Step: 235811, 	{'train/accuracy': 0.8889062404632568, 'train/loss': 0.40952399373054504, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 105492.69994354248, 'total_duration': 114084.48039150238, 'accumulated_submission_time': 105492.69994354248, 'accumulated_eval_time': 8566.35573387146, 'accumulated_logging_time': 13.02313780784607}
I0309 12:42:41.510396 139708407461632 logging_writer.py:48] [235811] accumulated_eval_time=8566.355734, accumulated_logging_time=13.023138, accumulated_submission_time=105492.699944, global_step=235811, preemption_count=0, score=105492.699944, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=114084.480392, train/accuracy=0.888906, train/loss=0.409524, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 12:43:17.253860 139708415854336 logging_writer.py:48] [235900] global_step=235900, grad_norm=4.015700340270996, loss=1.1889513731002808
I0309 12:44:02.217775 139708407461632 logging_writer.py:48] [236000] global_step=236000, grad_norm=3.1783714294433594, loss=1.455336332321167
I0309 12:44:47.830496 139708415854336 logging_writer.py:48] [236100] global_step=236100, grad_norm=3.076425552368164, loss=1.7446599006652832
I0309 12:45:33.111913 139708407461632 logging_writer.py:48] [236200] global_step=236200, grad_norm=3.258114814758301, loss=1.108926773071289
I0309 12:46:18.081942 139708415854336 logging_writer.py:48] [236300] global_step=236300, grad_norm=3.102084159851074, loss=1.2140953540802002
I0309 12:47:03.378585 139708407461632 logging_writer.py:48] [236400] global_step=236400, grad_norm=3.3378798961639404, loss=1.2340326309204102
I0309 12:47:49.243953 139708415854336 logging_writer.py:48] [236500] global_step=236500, grad_norm=3.0048582553863525, loss=1.0459156036376953
I0309 12:48:34.593542 139708407461632 logging_writer.py:48] [236600] global_step=236600, grad_norm=3.1868817806243896, loss=1.126937747001648
I0309 12:49:19.888004 139708415854336 logging_writer.py:48] [236700] global_step=236700, grad_norm=2.9752120971679688, loss=2.436046600341797
I0309 12:49:41.753379 139902746892096 spec.py:321] Evaluating on the training split.
I0309 12:49:53.251800 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 12:50:16.128875 139902746892096 spec.py:349] Evaluating on the test split.
I0309 12:50:17.797152 139902746892096 submission_runner.py:411] Time since start: 114540.83s, 	Step: 236750, 	{'train/accuracy': 0.88880854845047, 'train/loss': 0.4133784770965576, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 105912.88513946533, 'total_duration': 114540.82594275475, 'accumulated_submission_time': 105912.88513946533, 'accumulated_eval_time': 8602.39948964119, 'accumulated_logging_time': 13.09098768234253}
I0309 12:50:17.852472 139708407461632 logging_writer.py:48] [236750] accumulated_eval_time=8602.399490, accumulated_logging_time=13.090988, accumulated_submission_time=105912.885139, global_step=236750, preemption_count=0, score=105912.885139, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=114540.825943, train/accuracy=0.888809, train/loss=0.413378, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 12:50:37.916599 139708415854336 logging_writer.py:48] [236800] global_step=236800, grad_norm=3.021470308303833, loss=1.1038930416107178
I0309 12:51:20.644334 139708407461632 logging_writer.py:48] [236900] global_step=236900, grad_norm=3.1150708198547363, loss=1.9434236288070679
I0309 12:52:05.762558 139708415854336 logging_writer.py:48] [237000] global_step=237000, grad_norm=3.1739861965179443, loss=1.5949134826660156
I0309 12:52:51.406705 139708407461632 logging_writer.py:48] [237100] global_step=237100, grad_norm=2.975815773010254, loss=1.0727591514587402
I0309 12:53:36.635563 139708415854336 logging_writer.py:48] [237200] global_step=237200, grad_norm=3.0547473430633545, loss=1.0881106853485107
I0309 12:54:22.105629 139708407461632 logging_writer.py:48] [237300] global_step=237300, grad_norm=3.1078896522521973, loss=1.1218312978744507
I0309 12:55:07.209675 139708415854336 logging_writer.py:48] [237400] global_step=237400, grad_norm=3.103696346282959, loss=1.218471884727478
I0309 12:55:52.491294 139708407461632 logging_writer.py:48] [237500] global_step=237500, grad_norm=3.0717411041259766, loss=1.1583564281463623
I0309 12:56:37.751306 139708415854336 logging_writer.py:48] [237600] global_step=237600, grad_norm=3.360039234161377, loss=1.182837963104248
I0309 12:57:18.213634 139902746892096 spec.py:321] Evaluating on the training split.
I0309 12:57:29.379528 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 12:57:50.541558 139902746892096 spec.py:349] Evaluating on the test split.
I0309 12:57:52.214517 139902746892096 submission_runner.py:411] Time since start: 114995.24s, 	Step: 237691, 	{'train/accuracy': 0.8860937356948853, 'train/loss': 0.4221966564655304, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 106333.18955159187, 'total_duration': 114995.24330091476, 'accumulated_submission_time': 106333.18955159187, 'accumulated_eval_time': 8636.400366544724, 'accumulated_logging_time': 13.154456853866577}
I0309 12:57:52.275792 139708407461632 logging_writer.py:48] [237691] accumulated_eval_time=8636.400367, accumulated_logging_time=13.154457, accumulated_submission_time=106333.189552, global_step=237691, preemption_count=0, score=106333.189552, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=114995.243301, train/accuracy=0.886094, train/loss=0.422197, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 12:57:56.210914 139708415854336 logging_writer.py:48] [237700] global_step=237700, grad_norm=3.2698538303375244, loss=2.862600564956665
I0309 12:58:36.601575 139708407461632 logging_writer.py:48] [237800] global_step=237800, grad_norm=3.0184619426727295, loss=1.7012255191802979
I0309 12:59:21.840389 139708415854336 logging_writer.py:48] [237900] global_step=237900, grad_norm=3.364561080932617, loss=1.100307583808899
I0309 13:00:07.257073 139708407461632 logging_writer.py:48] [238000] global_step=238000, grad_norm=4.203381061553955, loss=3.2297611236572266
I0309 13:00:52.314131 139708415854336 logging_writer.py:48] [238100] global_step=238100, grad_norm=3.2527589797973633, loss=1.2603328227996826
I0309 13:01:37.699244 139708407461632 logging_writer.py:48] [238200] global_step=238200, grad_norm=4.320690155029297, loss=3.269683599472046
I0309 13:02:23.124164 139708415854336 logging_writer.py:48] [238300] global_step=238300, grad_norm=3.0797669887542725, loss=1.3057156801223755
I0309 13:03:08.394199 139708407461632 logging_writer.py:48] [238400] global_step=238400, grad_norm=3.159263849258423, loss=2.542872428894043
I0309 13:03:53.670397 139708415854336 logging_writer.py:48] [238500] global_step=238500, grad_norm=3.5362727642059326, loss=3.098660469055176
I0309 13:04:39.043077 139708407461632 logging_writer.py:48] [238600] global_step=238600, grad_norm=2.88871169090271, loss=2.2871792316436768
I0309 13:04:52.687895 139902746892096 spec.py:321] Evaluating on the training split.
I0309 13:05:04.247812 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 13:05:26.825858 139902746892096 spec.py:349] Evaluating on the test split.
I0309 13:05:28.504087 139902746892096 submission_runner.py:411] Time since start: 115451.53s, 	Step: 238632, 	{'train/accuracy': 0.8895898461341858, 'train/loss': 0.4112511873245239, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 106753.54248261452, 'total_duration': 115451.53286147118, 'accumulated_submission_time': 106753.54248261452, 'accumulated_eval_time': 8672.216531038284, 'accumulated_logging_time': 13.225939512252808}
I0309 13:05:28.560777 139708415854336 logging_writer.py:48] [238632] accumulated_eval_time=8672.216531, accumulated_logging_time=13.225940, accumulated_submission_time=106753.542483, global_step=238632, preemption_count=0, score=106753.542483, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=115451.532861, train/accuracy=0.889590, train/loss=0.411251, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 13:05:55.699539 139708407461632 logging_writer.py:48] [238700] global_step=238700, grad_norm=3.4610445499420166, loss=1.0795588493347168
I0309 13:06:39.398363 139708415854336 logging_writer.py:48] [238800] global_step=238800, grad_norm=3.011502981185913, loss=1.9738119840621948
I0309 13:07:25.029834 139708407461632 logging_writer.py:48] [238900] global_step=238900, grad_norm=3.206653118133545, loss=2.775756359100342
I0309 13:08:10.927287 139708415854336 logging_writer.py:48] [239000] global_step=239000, grad_norm=3.113576650619507, loss=1.036710262298584
I0309 13:08:56.090911 139708407461632 logging_writer.py:48] [239100] global_step=239100, grad_norm=2.9996984004974365, loss=1.0562517642974854
I0309 13:09:41.503798 139708415854336 logging_writer.py:48] [239200] global_step=239200, grad_norm=3.1861109733581543, loss=1.0521986484527588
I0309 13:10:26.617110 139708407461632 logging_writer.py:48] [239300] global_step=239300, grad_norm=3.190631866455078, loss=1.2376651763916016
I0309 13:11:11.864357 139708415854336 logging_writer.py:48] [239400] global_step=239400, grad_norm=3.1370511054992676, loss=1.1622804403305054
I0309 13:11:57.093620 139708407461632 logging_writer.py:48] [239500] global_step=239500, grad_norm=2.908015251159668, loss=2.0588161945343018
I0309 13:12:28.901094 139902746892096 spec.py:321] Evaluating on the training split.
I0309 13:12:40.539736 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 13:13:00.021639 139902746892096 spec.py:349] Evaluating on the test split.
I0309 13:13:01.718090 139902746892096 submission_runner.py:411] Time since start: 115904.75s, 	Step: 239572, 	{'train/accuracy': 0.8858007788658142, 'train/loss': 0.42028868198394775, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 107173.82443404198, 'total_duration': 115904.74687862396, 'accumulated_submission_time': 107173.82443404198, 'accumulated_eval_time': 8705.033515691757, 'accumulated_logging_time': 13.291870594024658}
I0309 13:13:01.780624 139708415854336 logging_writer.py:48] [239572] accumulated_eval_time=8705.033516, accumulated_logging_time=13.291871, accumulated_submission_time=107173.824434, global_step=239572, preemption_count=0, score=107173.824434, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=115904.746879, train/accuracy=0.885801, train/loss=0.420289, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 13:13:13.204368 139708407461632 logging_writer.py:48] [239600] global_step=239600, grad_norm=3.1912460327148438, loss=1.9808344841003418
I0309 13:13:55.436811 139708415854336 logging_writer.py:48] [239700] global_step=239700, grad_norm=3.289214611053467, loss=1.1327787637710571
I0309 13:14:40.591854 139708407461632 logging_writer.py:48] [239800] global_step=239800, grad_norm=3.328631639480591, loss=1.2734228372573853
I0309 13:15:26.057800 139708415854336 logging_writer.py:48] [239900] global_step=239900, grad_norm=3.545590877532959, loss=2.9229159355163574
I0309 13:16:11.532124 139708407461632 logging_writer.py:48] [240000] global_step=240000, grad_norm=3.077037811279297, loss=1.3003095388412476
I0309 13:16:56.856460 139708415854336 logging_writer.py:48] [240100] global_step=240100, grad_norm=3.02105712890625, loss=1.1055748462677002
I0309 13:17:42.195595 139708407461632 logging_writer.py:48] [240200] global_step=240200, grad_norm=3.812317132949829, loss=3.2694501876831055
I0309 13:18:27.618377 139708415854336 logging_writer.py:48] [240300] global_step=240300, grad_norm=3.183011531829834, loss=1.790765404701233
I0309 13:19:12.921371 139708407461632 logging_writer.py:48] [240400] global_step=240400, grad_norm=2.9068894386291504, loss=1.6675410270690918
I0309 13:19:58.119940 139708415854336 logging_writer.py:48] [240500] global_step=240500, grad_norm=3.075418472290039, loss=1.379135012626648
I0309 13:20:01.774239 139902746892096 spec.py:321] Evaluating on the training split.
I0309 13:20:13.284344 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 13:20:36.558668 139902746892096 spec.py:349] Evaluating on the test split.
I0309 13:20:38.229957 139902746892096 submission_runner.py:411] Time since start: 116361.26s, 	Step: 240510, 	{'train/accuracy': 0.888671875, 'train/loss': 0.4127401113510132, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 107593.75956201553, 'total_duration': 116361.25875759125, 'accumulated_submission_time': 107593.75956201553, 'accumulated_eval_time': 8741.489234685898, 'accumulated_logging_time': 13.36388087272644}
I0309 13:20:38.283848 139708407461632 logging_writer.py:48] [240510] accumulated_eval_time=8741.489235, accumulated_logging_time=13.363881, accumulated_submission_time=107593.759562, global_step=240510, preemption_count=0, score=107593.759562, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=116361.258758, train/accuracy=0.888672, train/loss=0.412740, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 13:21:14.232743 139708415854336 logging_writer.py:48] [240600] global_step=240600, grad_norm=3.367777109146118, loss=2.3069093227386475
I0309 13:21:59.566309 139708407461632 logging_writer.py:48] [240700] global_step=240700, grad_norm=3.219785451889038, loss=1.0973522663116455
I0309 13:22:44.734286 139708415854336 logging_writer.py:48] [240800] global_step=240800, grad_norm=2.979980230331421, loss=1.0713770389556885
I0309 13:23:30.080113 139708407461632 logging_writer.py:48] [240900] global_step=240900, grad_norm=3.1512091159820557, loss=2.9209702014923096
I0309 13:24:15.287671 139708415854336 logging_writer.py:48] [241000] global_step=241000, grad_norm=3.8162455558776855, loss=3.047772169113159
I0309 13:25:00.630558 139708407461632 logging_writer.py:48] [241100] global_step=241100, grad_norm=3.3189468383789062, loss=1.207543134689331
I0309 13:25:45.702441 139708415854336 logging_writer.py:48] [241200] global_step=241200, grad_norm=3.892296314239502, loss=3.2782812118530273
I0309 13:26:31.052830 139708407461632 logging_writer.py:48] [241300] global_step=241300, grad_norm=3.320993423461914, loss=1.3443752527236938
I0309 13:27:16.440937 139708415854336 logging_writer.py:48] [241400] global_step=241400, grad_norm=2.8847224712371826, loss=1.7539799213409424
I0309 13:27:38.544969 139902746892096 spec.py:321] Evaluating on the training split.
I0309 13:27:49.735740 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 13:28:12.472125 139902746892096 spec.py:349] Evaluating on the test split.
I0309 13:28:14.144086 139902746892096 submission_runner.py:411] Time since start: 116817.17s, 	Step: 241450, 	{'train/accuracy': 0.8886327743530273, 'train/loss': 0.41012683510780334, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 108013.9641532898, 'total_duration': 116817.1728887558, 'accumulated_submission_time': 108013.9641532898, 'accumulated_eval_time': 8777.088340520859, 'accumulated_logging_time': 13.4263174533844}
I0309 13:28:14.193767 139708407461632 logging_writer.py:48] [241450] accumulated_eval_time=8777.088341, accumulated_logging_time=13.426317, accumulated_submission_time=108013.964153, global_step=241450, preemption_count=0, score=108013.964153, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=116817.172889, train/accuracy=0.888633, train/loss=0.410127, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 13:28:34.254465 139708415854336 logging_writer.py:48] [241500] global_step=241500, grad_norm=3.0988330841064453, loss=1.0495569705963135
I0309 13:29:16.896960 139708407461632 logging_writer.py:48] [241600] global_step=241600, grad_norm=2.967679023742676, loss=1.2295491695404053
I0309 13:30:02.407062 139708415854336 logging_writer.py:48] [241700] global_step=241700, grad_norm=3.2471468448638916, loss=1.5043905973434448
I0309 13:30:48.051983 139708407461632 logging_writer.py:48] [241800] global_step=241800, grad_norm=3.1248557567596436, loss=1.1705297231674194
I0309 13:31:33.191869 139708415854336 logging_writer.py:48] [241900] global_step=241900, grad_norm=3.296797752380371, loss=2.1349048614501953
I0309 13:32:18.757295 139708407461632 logging_writer.py:48] [242000] global_step=242000, grad_norm=3.109288454055786, loss=1.361550211906433
I0309 13:33:04.203905 139708415854336 logging_writer.py:48] [242100] global_step=242100, grad_norm=3.0572350025177, loss=1.148268699645996
I0309 13:33:49.409091 139708407461632 logging_writer.py:48] [242200] global_step=242200, grad_norm=3.5128092765808105, loss=1.2243647575378418
I0309 13:34:34.704134 139708415854336 logging_writer.py:48] [242300] global_step=242300, grad_norm=3.1820316314697266, loss=1.1179616451263428
I0309 13:35:14.535494 139902746892096 spec.py:321] Evaluating on the training split.
I0309 13:35:26.045172 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 13:35:47.218653 139902746892096 spec.py:349] Evaluating on the test split.
I0309 13:35:48.883771 139902746892096 submission_runner.py:411] Time since start: 117271.91s, 	Step: 242389, 	{'train/accuracy': 0.8875585794448853, 'train/loss': 0.4174584746360779, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 108434.24830532074, 'total_duration': 117271.91256117821, 'accumulated_submission_time': 108434.24830532074, 'accumulated_eval_time': 8811.436619997025, 'accumulated_logging_time': 13.484199047088623}
I0309 13:35:48.940571 139708407461632 logging_writer.py:48] [242389] accumulated_eval_time=8811.436620, accumulated_logging_time=13.484199, accumulated_submission_time=108434.248305, global_step=242389, preemption_count=0, score=108434.248305, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=117271.912561, train/accuracy=0.887559, train/loss=0.417458, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 13:35:53.656116 139708415854336 logging_writer.py:48] [242400] global_step=242400, grad_norm=3.162890911102295, loss=2.4648995399475098
I0309 13:36:34.185089 139708407461632 logging_writer.py:48] [242500] global_step=242500, grad_norm=3.3669300079345703, loss=2.7668557167053223
I0309 13:37:19.355069 139708415854336 logging_writer.py:48] [242600] global_step=242600, grad_norm=3.1414921283721924, loss=1.1807090044021606
I0309 13:38:04.818701 139708407461632 logging_writer.py:48] [242700] global_step=242700, grad_norm=3.0965404510498047, loss=1.153740644454956
I0309 13:38:50.178974 139708415854336 logging_writer.py:48] [242800] global_step=242800, grad_norm=3.0148229598999023, loss=1.0946787595748901
I0309 13:39:35.292758 139708407461632 logging_writer.py:48] [242900] global_step=242900, grad_norm=4.364020347595215, loss=2.6469335556030273
I0309 13:40:20.917659 139708415854336 logging_writer.py:48] [243000] global_step=243000, grad_norm=3.2521345615386963, loss=1.3966599702835083
I0309 13:41:06.110964 139708407461632 logging_writer.py:48] [243100] global_step=243100, grad_norm=3.1548452377319336, loss=1.2471896409988403
I0309 13:41:52.351198 139708415854336 logging_writer.py:48] [243200] global_step=243200, grad_norm=3.4850330352783203, loss=2.70350980758667
I0309 13:42:37.699464 139708407461632 logging_writer.py:48] [243300] global_step=243300, grad_norm=3.377039909362793, loss=2.7044668197631836
I0309 13:42:49.069004 139902746892096 spec.py:321] Evaluating on the training split.
I0309 13:43:00.961508 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 13:43:22.231886 139902746892096 spec.py:349] Evaluating on the test split.
I0309 13:43:23.909401 139902746892096 submission_runner.py:411] Time since start: 117726.94s, 	Step: 243327, 	{'train/accuracy': 0.8881250023841858, 'train/loss': 0.4179988503456116, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 108854.31929779053, 'total_duration': 117726.93820238113, 'accumulated_submission_time': 108854.31929779053, 'accumulated_eval_time': 8846.27702832222, 'accumulated_logging_time': 13.549908876419067}
I0309 13:43:23.961236 139708415854336 logging_writer.py:48] [243327] accumulated_eval_time=8846.277028, accumulated_logging_time=13.549909, accumulated_submission_time=108854.319298, global_step=243327, preemption_count=0, score=108854.319298, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=117726.938202, train/accuracy=0.888125, train/loss=0.417999, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 13:43:53.062802 139708407461632 logging_writer.py:48] [243400] global_step=243400, grad_norm=3.0306801795959473, loss=1.1170952320098877
I0309 13:44:37.661197 139708415854336 logging_writer.py:48] [243500] global_step=243500, grad_norm=2.9320790767669678, loss=1.1118522882461548
I0309 13:45:23.176728 139708407461632 logging_writer.py:48] [243600] global_step=243600, grad_norm=3.0765535831451416, loss=1.117037296295166
I0309 13:46:08.640487 139708415854336 logging_writer.py:48] [243700] global_step=243700, grad_norm=3.245403528213501, loss=2.200028896331787
I0309 13:46:54.242203 139708407461632 logging_writer.py:48] [243800] global_step=243800, grad_norm=3.0207371711730957, loss=1.4362475872039795
I0309 13:47:39.617058 139708415854336 logging_writer.py:48] [243900] global_step=243900, grad_norm=3.428463935852051, loss=2.994317054748535
I0309 13:48:25.248956 139708407461632 logging_writer.py:48] [244000] global_step=244000, grad_norm=3.0801985263824463, loss=1.8426716327667236
I0309 13:49:10.454802 139708415854336 logging_writer.py:48] [244100] global_step=244100, grad_norm=3.3034377098083496, loss=1.1616075038909912
I0309 13:49:55.801217 139708407461632 logging_writer.py:48] [244200] global_step=244200, grad_norm=2.655400276184082, loss=1.1991525888442993
I0309 13:50:24.116870 139902746892096 spec.py:321] Evaluating on the training split.
I0309 13:50:35.503190 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 13:50:58.494792 139902746892096 spec.py:349] Evaluating on the test split.
I0309 13:51:00.167286 139902746892096 submission_runner.py:411] Time since start: 118183.20s, 	Step: 244264, 	{'train/accuracy': 0.8875976204872131, 'train/loss': 0.4127923548221588, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 109274.4168112278, 'total_duration': 118183.19608783722, 'accumulated_submission_time': 109274.4168112278, 'accumulated_eval_time': 8882.327449798584, 'accumulated_logging_time': 13.610346794128418}
I0309 13:51:00.218118 139708415854336 logging_writer.py:48] [244264] accumulated_eval_time=8882.327450, accumulated_logging_time=13.610347, accumulated_submission_time=109274.416811, global_step=244264, preemption_count=0, score=109274.416811, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=118183.196088, train/accuracy=0.887598, train/loss=0.412792, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 13:51:14.778099 139708407461632 logging_writer.py:48] [244300] global_step=244300, grad_norm=3.0873820781707764, loss=2.418696403503418
I0309 13:51:56.840487 139708415854336 logging_writer.py:48] [244400] global_step=244400, grad_norm=3.2831950187683105, loss=1.1396979093551636
I0309 13:52:41.879331 139708407461632 logging_writer.py:48] [244500] global_step=244500, grad_norm=3.211887836456299, loss=2.654111623764038
I0309 13:53:27.757114 139708415854336 logging_writer.py:48] [244600] global_step=244600, grad_norm=3.1531636714935303, loss=2.6063404083251953
I0309 13:54:12.676011 139708407461632 logging_writer.py:48] [244700] global_step=244700, grad_norm=3.034590721130371, loss=1.076769471168518
I0309 13:54:57.789865 139708415854336 logging_writer.py:48] [244800] global_step=244800, grad_norm=3.073423385620117, loss=2.0640621185302734
I0309 13:55:43.240333 139708407461632 logging_writer.py:48] [244900] global_step=244900, grad_norm=3.1192433834075928, loss=1.2098270654678345
I0309 13:56:28.275329 139708415854336 logging_writer.py:48] [245000] global_step=245000, grad_norm=2.84354567527771, loss=1.1160008907318115
I0309 13:57:13.491171 139708407461632 logging_writer.py:48] [245100] global_step=245100, grad_norm=3.201173782348633, loss=2.443701982498169
I0309 13:57:58.954400 139708415854336 logging_writer.py:48] [245200] global_step=245200, grad_norm=3.2618014812469482, loss=2.8562722206115723
I0309 13:58:00.350579 139902746892096 spec.py:321] Evaluating on the training split.
I0309 13:58:11.577846 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 13:58:34.795006 139902746892096 spec.py:349] Evaluating on the test split.
I0309 13:58:36.472830 139902746892096 submission_runner.py:411] Time since start: 118639.50s, 	Step: 245205, 	{'train/accuracy': 0.8902148008346558, 'train/loss': 0.41219520568847656, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 109694.49145460129, 'total_duration': 118639.5016169548, 'accumulated_submission_time': 109694.49145460129, 'accumulated_eval_time': 8918.449665307999, 'accumulated_logging_time': 13.66965365409851}
I0309 13:58:36.524283 139708407461632 logging_writer.py:48] [245205] accumulated_eval_time=8918.449665, accumulated_logging_time=13.669654, accumulated_submission_time=109694.491455, global_step=245205, preemption_count=0, score=109694.491455, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=118639.501617, train/accuracy=0.890215, train/loss=0.412195, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 13:59:14.637935 139708415854336 logging_writer.py:48] [245300] global_step=245300, grad_norm=2.950920343399048, loss=1.0884181261062622
I0309 14:00:00.051474 139708407461632 logging_writer.py:48] [245400] global_step=245400, grad_norm=2.858649969100952, loss=1.351999282836914
I0309 14:00:45.580415 139708415854336 logging_writer.py:48] [245500] global_step=245500, grad_norm=3.108827829360962, loss=1.09563148021698
I0309 14:01:30.902673 139708407461632 logging_writer.py:48] [245600] global_step=245600, grad_norm=3.2735533714294434, loss=1.1257327795028687
I0309 14:02:16.286570 139708415854336 logging_writer.py:48] [245700] global_step=245700, grad_norm=3.1274001598358154, loss=1.2624619007110596
I0309 14:03:01.777948 139708407461632 logging_writer.py:48] [245800] global_step=245800, grad_norm=3.119211435317993, loss=1.686589002609253
I0309 14:03:46.975188 139708415854336 logging_writer.py:48] [245900] global_step=245900, grad_norm=3.254300594329834, loss=2.1024980545043945
I0309 14:04:32.451002 139708407461632 logging_writer.py:48] [246000] global_step=246000, grad_norm=2.9176669120788574, loss=1.0805994272232056
I0309 14:05:17.404220 139708415854336 logging_writer.py:48] [246100] global_step=246100, grad_norm=2.8817625045776367, loss=2.1100120544433594
I0309 14:05:36.537002 139902746892096 spec.py:321] Evaluating on the training split.
I0309 14:05:48.135553 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 14:06:10.008951 139902746892096 spec.py:349] Evaluating on the test split.
I0309 14:06:11.678201 139902746892096 submission_runner.py:411] Time since start: 119094.71s, 	Step: 246144, 	{'train/accuracy': 0.886035144329071, 'train/loss': 0.4249056279659271, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 110114.44577169418, 'total_duration': 119094.70700001717, 'accumulated_submission_time': 110114.44577169418, 'accumulated_eval_time': 8953.590856313705, 'accumulated_logging_time': 13.729873657226562}
I0309 14:06:11.732796 139708407461632 logging_writer.py:48] [246144] accumulated_eval_time=8953.590856, accumulated_logging_time=13.729874, accumulated_submission_time=110114.445772, global_step=246144, preemption_count=0, score=110114.445772, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=119094.707000, train/accuracy=0.886035, train/loss=0.424906, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 14:06:34.169973 139708415854336 logging_writer.py:48] [246200] global_step=246200, grad_norm=3.206713914871216, loss=1.8288469314575195
I0309 14:07:17.707871 139708407461632 logging_writer.py:48] [246300] global_step=246300, grad_norm=2.9980993270874023, loss=0.9625493288040161
I0309 14:08:03.067595 139708415854336 logging_writer.py:48] [246400] global_step=246400, grad_norm=3.046513319015503, loss=1.2340798377990723
I0309 14:08:48.998734 139708407461632 logging_writer.py:48] [246500] global_step=246500, grad_norm=3.0129754543304443, loss=0.9768898487091064
I0309 14:09:34.228898 139708415854336 logging_writer.py:48] [246600] global_step=246600, grad_norm=3.0994386672973633, loss=1.1614335775375366
I0309 14:10:19.481766 139708407461632 logging_writer.py:48] [246700] global_step=246700, grad_norm=3.2474327087402344, loss=1.1796883344650269
I0309 14:11:04.840655 139708415854336 logging_writer.py:48] [246800] global_step=246800, grad_norm=2.9269721508026123, loss=2.2343084812164307
I0309 14:11:50.124068 139708407461632 logging_writer.py:48] [246900] global_step=246900, grad_norm=2.7793385982513428, loss=1.5169261693954468
I0309 14:12:35.663660 139708415854336 logging_writer.py:48] [247000] global_step=247000, grad_norm=2.9516685009002686, loss=1.0947078466415405
I0309 14:13:11.866075 139902746892096 spec.py:321] Evaluating on the training split.
I0309 14:13:23.439272 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 14:13:44.118210 139902746892096 spec.py:349] Evaluating on the test split.
I0309 14:13:45.806176 139902746892096 submission_runner.py:411] Time since start: 119548.83s, 	Step: 247082, 	{'train/accuracy': 0.8880273103713989, 'train/loss': 0.41527971625328064, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 110534.5217063427, 'total_duration': 119548.83497023582, 'accumulated_submission_time': 110534.5217063427, 'accumulated_eval_time': 8987.530959367752, 'accumulated_logging_time': 13.792777299880981}
I0309 14:13:45.863856 139708407461632 logging_writer.py:48] [247082] accumulated_eval_time=8987.530959, accumulated_logging_time=13.792777, accumulated_submission_time=110534.521706, global_step=247082, preemption_count=0, score=110534.521706, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=119548.834970, train/accuracy=0.888027, train/loss=0.415280, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 14:13:53.328338 139708415854336 logging_writer.py:48] [247100] global_step=247100, grad_norm=3.2732760906219482, loss=2.1981441974639893
I0309 14:14:34.738575 139708407461632 logging_writer.py:48] [247200] global_step=247200, grad_norm=3.1822516918182373, loss=2.854125738143921
I0309 14:15:19.770116 139708415854336 logging_writer.py:48] [247300] global_step=247300, grad_norm=3.0895047187805176, loss=1.071607232093811
I0309 14:16:05.349376 139708407461632 logging_writer.py:48] [247400] global_step=247400, grad_norm=3.022730588912964, loss=1.7602882385253906
I0309 14:16:51.248670 139708415854336 logging_writer.py:48] [247500] global_step=247500, grad_norm=3.244673013687134, loss=1.1449780464172363
I0309 14:17:36.507068 139708407461632 logging_writer.py:48] [247600] global_step=247600, grad_norm=3.9823217391967773, loss=3.272374391555786
I0309 14:18:22.038920 139708415854336 logging_writer.py:48] [247700] global_step=247700, grad_norm=3.160417079925537, loss=1.1083897352218628
I0309 14:19:07.531704 139708407461632 logging_writer.py:48] [247800] global_step=247800, grad_norm=2.7558891773223877, loss=1.7990361452102661
I0309 14:19:52.789906 139708415854336 logging_writer.py:48] [247900] global_step=247900, grad_norm=3.4975967407226562, loss=1.2494537830352783
I0309 14:20:38.205724 139708407461632 logging_writer.py:48] [248000] global_step=248000, grad_norm=3.001363754272461, loss=1.6726305484771729
I0309 14:20:45.924066 139902746892096 spec.py:321] Evaluating on the training split.
I0309 14:20:57.220721 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 14:21:19.728082 139902746892096 spec.py:349] Evaluating on the test split.
I0309 14:21:21.409250 139902746892096 submission_runner.py:411] Time since start: 120004.44s, 	Step: 248019, 	{'train/accuracy': 0.8878515362739563, 'train/loss': 0.41943755745887756, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 110954.5231757164, 'total_duration': 120004.43804454803, 'accumulated_submission_time': 110954.5231757164, 'accumulated_eval_time': 9023.016151428223, 'accumulated_logging_time': 13.860776662826538}
I0309 14:21:21.460919 139708415854336 logging_writer.py:48] [248019] accumulated_eval_time=9023.016151, accumulated_logging_time=13.860777, accumulated_submission_time=110954.523176, global_step=248019, preemption_count=0, score=110954.523176, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=120004.438045, train/accuracy=0.887852, train/loss=0.419438, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 14:21:53.727265 139708407461632 logging_writer.py:48] [248100] global_step=248100, grad_norm=4.361085891723633, loss=1.0832380056381226
I0309 14:22:38.086219 139708415854336 logging_writer.py:48] [248200] global_step=248200, grad_norm=3.1450746059417725, loss=2.3667943477630615
I0309 14:23:23.394868 139708407461632 logging_writer.py:48] [248300] global_step=248300, grad_norm=2.825084924697876, loss=1.8373796939849854
I0309 14:24:08.547966 139708415854336 logging_writer.py:48] [248400] global_step=248400, grad_norm=3.744035005569458, loss=3.1106643676757812
I0309 14:24:53.763274 139708407461632 logging_writer.py:48] [248500] global_step=248500, grad_norm=3.096313238143921, loss=1.1966129541397095
I0309 14:25:39.005873 139708415854336 logging_writer.py:48] [248600] global_step=248600, grad_norm=3.0716559886932373, loss=1.8778095245361328
I0309 14:26:24.181038 139708407461632 logging_writer.py:48] [248700] global_step=248700, grad_norm=3.1526198387145996, loss=1.0834908485412598
I0309 14:27:09.230186 139708415854336 logging_writer.py:48] [248800] global_step=248800, grad_norm=3.0103580951690674, loss=1.357568621635437
I0309 14:27:54.489706 139708407461632 logging_writer.py:48] [248900] global_step=248900, grad_norm=3.2123029232025146, loss=1.118399977684021
I0309 14:28:21.448458 139902746892096 spec.py:321] Evaluating on the training split.
I0309 14:28:32.833086 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 14:28:55.250518 139902746892096 spec.py:349] Evaluating on the test split.
I0309 14:28:56.931989 139902746892096 submission_runner.py:411] Time since start: 120459.96s, 	Step: 248961, 	{'train/accuracy': 0.8873632550239563, 'train/loss': 0.41516149044036865, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 111374.45286393166, 'total_duration': 120459.96077299118, 'accumulated_submission_time': 111374.45286393166, 'accumulated_eval_time': 9058.499682426453, 'accumulated_logging_time': 13.92180871963501}
I0309 14:28:56.991490 139708415854336 logging_writer.py:48] [248961] accumulated_eval_time=9058.499682, accumulated_logging_time=13.921809, accumulated_submission_time=111374.452864, global_step=248961, preemption_count=0, score=111374.452864, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=120459.960773, train/accuracy=0.887363, train/loss=0.415161, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 14:29:12.716637 139708407461632 logging_writer.py:48] [249000] global_step=249000, grad_norm=3.0753023624420166, loss=2.1604862213134766
I0309 14:29:54.702175 139708415854336 logging_writer.py:48] [249100] global_step=249100, grad_norm=3.110568046569824, loss=1.1837124824523926
I0309 14:30:39.935762 139708407461632 logging_writer.py:48] [249200] global_step=249200, grad_norm=3.201446771621704, loss=2.251399040222168
I0309 14:31:25.413792 139708415854336 logging_writer.py:48] [249300] global_step=249300, grad_norm=3.016914129257202, loss=1.1062414646148682
I0309 14:32:10.921432 139708407461632 logging_writer.py:48] [249400] global_step=249400, grad_norm=3.0840351581573486, loss=1.058209776878357
I0309 14:32:56.245686 139708415854336 logging_writer.py:48] [249500] global_step=249500, grad_norm=3.4403553009033203, loss=1.815918207168579
I0309 14:33:41.618557 139708407461632 logging_writer.py:48] [249600] global_step=249600, grad_norm=2.9560227394104004, loss=1.4619834423065186
I0309 14:34:26.988673 139708415854336 logging_writer.py:48] [249700] global_step=249700, grad_norm=3.3030588626861572, loss=2.837005376815796
I0309 14:35:12.444169 139708407461632 logging_writer.py:48] [249800] global_step=249800, grad_norm=3.074514865875244, loss=1.0657787322998047
I0309 14:35:57.708260 139708415854336 logging_writer.py:48] [249900] global_step=249900, grad_norm=3.7333836555480957, loss=3.181894302368164
I0309 14:35:57.723466 139902746892096 spec.py:321] Evaluating on the training split.
I0309 14:36:09.149478 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 14:36:30.593806 139902746892096 spec.py:349] Evaluating on the test split.
I0309 14:36:32.270874 139902746892096 submission_runner.py:411] Time since start: 120915.30s, 	Step: 249901, 	{'train/accuracy': 0.8882030844688416, 'train/loss': 0.41577911376953125, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 111795.12572550774, 'total_duration': 120915.29966640472, 'accumulated_submission_time': 111795.12572550774, 'accumulated_eval_time': 9093.047081708908, 'accumulated_logging_time': 13.991700649261475}
I0309 14:36:32.324890 139708407461632 logging_writer.py:48] [249901] accumulated_eval_time=9093.047082, accumulated_logging_time=13.991701, accumulated_submission_time=111795.125726, global_step=249901, preemption_count=0, score=111795.125726, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=120915.299666, train/accuracy=0.888203, train/loss=0.415779, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 14:37:12.619070 139708415854336 logging_writer.py:48] [250000] global_step=250000, grad_norm=3.2508909702301025, loss=1.102074384689331
I0309 14:37:57.912251 139708407461632 logging_writer.py:48] [250100] global_step=250100, grad_norm=3.00285267829895, loss=2.33532977104187
I0309 14:38:43.275909 139708415854336 logging_writer.py:48] [250200] global_step=250200, grad_norm=2.848198175430298, loss=1.0190612077713013
I0309 14:39:28.887351 139708407461632 logging_writer.py:48] [250300] global_step=250300, grad_norm=3.425015926361084, loss=1.1275379657745361
I0309 14:40:14.172705 139708415854336 logging_writer.py:48] [250400] global_step=250400, grad_norm=3.5548319816589355, loss=3.088324546813965
I0309 14:40:59.723709 139708407461632 logging_writer.py:48] [250500] global_step=250500, grad_norm=3.2929835319519043, loss=1.61767578125
I0309 14:41:44.945543 139708415854336 logging_writer.py:48] [250600] global_step=250600, grad_norm=3.1141791343688965, loss=1.2156479358673096
I0309 14:42:30.550122 139708407461632 logging_writer.py:48] [250700] global_step=250700, grad_norm=2.8023931980133057, loss=1.1542928218841553
I0309 14:43:15.795174 139708415854336 logging_writer.py:48] [250800] global_step=250800, grad_norm=3.0412344932556152, loss=1.7280844449996948
I0309 14:43:32.634047 139902746892096 spec.py:321] Evaluating on the training split.
I0309 14:43:44.265303 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 14:44:06.738813 139902746892096 spec.py:349] Evaluating on the test split.
I0309 14:44:08.418984 139902746892096 submission_runner.py:411] Time since start: 121371.45s, 	Step: 250839, 	{'train/accuracy': 0.8891796469688416, 'train/loss': 0.41267600655555725, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 112215.37737846375, 'total_duration': 121371.44778513908, 'accumulated_submission_time': 112215.37737846375, 'accumulated_eval_time': 9128.832021713257, 'accumulated_logging_time': 14.05441927909851}
I0309 14:44:08.470512 139708407461632 logging_writer.py:48] [250839] accumulated_eval_time=9128.832022, accumulated_logging_time=14.054419, accumulated_submission_time=112215.377378, global_step=250839, preemption_count=0, score=112215.377378, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=121371.447785, train/accuracy=0.889180, train/loss=0.412676, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 14:44:32.877634 139708415854336 logging_writer.py:48] [250900] global_step=250900, grad_norm=3.0674471855163574, loss=1.561256766319275
I0309 14:45:16.266072 139708407461632 logging_writer.py:48] [251000] global_step=251000, grad_norm=3.076263427734375, loss=1.209088683128357
I0309 14:46:01.427704 139708415854336 logging_writer.py:48] [251100] global_step=251100, grad_norm=3.283625602722168, loss=1.133259892463684
I0309 14:46:46.864187 139708407461632 logging_writer.py:48] [251200] global_step=251200, grad_norm=3.074789524078369, loss=1.263573169708252
I0309 14:47:32.427920 139708415854336 logging_writer.py:48] [251300] global_step=251300, grad_norm=2.859504222869873, loss=1.0568366050720215
I0309 14:48:17.741749 139708407461632 logging_writer.py:48] [251400] global_step=251400, grad_norm=3.3298959732055664, loss=2.7651309967041016
I0309 14:49:03.634172 139708415854336 logging_writer.py:48] [251500] global_step=251500, grad_norm=2.9007644653320312, loss=1.0432549715042114
I0309 14:49:48.896681 139708407461632 logging_writer.py:48] [251600] global_step=251600, grad_norm=3.668003797531128, loss=1.1239044666290283
I0309 14:50:34.408077 139708415854336 logging_writer.py:48] [251700] global_step=251700, grad_norm=2.84492826461792, loss=1.8006120920181274
I0309 14:51:08.728871 139902746892096 spec.py:321] Evaluating on the training split.
I0309 14:51:20.230361 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 14:51:41.834558 139902746892096 spec.py:349] Evaluating on the test split.
I0309 14:51:43.503508 139902746892096 submission_runner.py:411] Time since start: 121826.53s, 	Step: 251777, 	{'train/accuracy': 0.8882421851158142, 'train/loss': 0.41136041283607483, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 112635.57611656189, 'total_duration': 121826.532310009, 'accumulated_submission_time': 112635.57611656189, 'accumulated_eval_time': 9163.606656312943, 'accumulated_logging_time': 14.1160409450531}
I0309 14:51:43.558030 139708407461632 logging_writer.py:48] [251777] accumulated_eval_time=9163.606656, accumulated_logging_time=14.116041, accumulated_submission_time=112635.576117, global_step=251777, preemption_count=0, score=112635.576117, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=121826.532310, train/accuracy=0.888242, train/loss=0.411360, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 14:51:52.989727 139708415854336 logging_writer.py:48] [251800] global_step=251800, grad_norm=3.4210143089294434, loss=3.1117265224456787
I0309 14:52:34.834172 139708407461632 logging_writer.py:48] [251900] global_step=251900, grad_norm=3.1347038745880127, loss=1.1097569465637207
I0309 14:53:20.510382 139708415854336 logging_writer.py:48] [252000] global_step=252000, grad_norm=3.3422796726226807, loss=2.689629554748535
I0309 14:54:06.315045 139708407461632 logging_writer.py:48] [252100] global_step=252100, grad_norm=3.3006491661071777, loss=1.0990080833435059
I0309 14:54:51.703014 139708415854336 logging_writer.py:48] [252200] global_step=252200, grad_norm=3.1586410999298096, loss=1.0952507257461548
I0309 14:55:37.316505 139708407461632 logging_writer.py:48] [252300] global_step=252300, grad_norm=2.9951841831207275, loss=1.010637640953064
I0309 14:56:23.301644 139708415854336 logging_writer.py:48] [252400] global_step=252400, grad_norm=3.1718738079071045, loss=2.669656991958618
I0309 14:57:08.905185 139708407461632 logging_writer.py:48] [252500] global_step=252500, grad_norm=3.122551679611206, loss=1.3166399002075195
I0309 14:57:54.551368 139708415854336 logging_writer.py:48] [252600] global_step=252600, grad_norm=3.0363433361053467, loss=2.191293239593506
I0309 14:58:40.048092 139708407461632 logging_writer.py:48] [252700] global_step=252700, grad_norm=3.418748617172241, loss=1.1477680206298828
I0309 14:58:43.838605 139902746892096 spec.py:321] Evaluating on the training split.
I0309 14:58:55.490342 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 14:59:19.067412 139902746892096 spec.py:349] Evaluating on the test split.
I0309 14:59:20.734365 139902746892096 submission_runner.py:411] Time since start: 122283.76s, 	Step: 252710, 	{'train/accuracy': 0.8890429735183716, 'train/loss': 0.4132768213748932, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 113055.79975700378, 'total_duration': 122283.76314520836, 'accumulated_submission_time': 113055.79975700378, 'accumulated_eval_time': 9200.502378463745, 'accumulated_logging_time': 14.178927898406982}
I0309 14:59:20.787284 139708415854336 logging_writer.py:48] [252710] accumulated_eval_time=9200.502378, accumulated_logging_time=14.178928, accumulated_submission_time=113055.799757, global_step=252710, preemption_count=0, score=113055.799757, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=122283.763145, train/accuracy=0.889043, train/loss=0.413277, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 14:59:56.770290 139708407461632 logging_writer.py:48] [252800] global_step=252800, grad_norm=3.191401720046997, loss=1.0862880945205688
I0309 15:00:41.842143 139708415854336 logging_writer.py:48] [252900] global_step=252900, grad_norm=2.900277853012085, loss=1.0267422199249268
I0309 15:01:27.450662 139708407461632 logging_writer.py:48] [253000] global_step=253000, grad_norm=3.088043212890625, loss=2.2074122428894043
I0309 15:02:12.848465 139708415854336 logging_writer.py:48] [253100] global_step=253100, grad_norm=3.247697353363037, loss=1.729209303855896
I0309 15:02:58.071400 139708407461632 logging_writer.py:48] [253200] global_step=253200, grad_norm=3.001845359802246, loss=1.1150262355804443
I0309 15:03:43.369966 139708415854336 logging_writer.py:48] [253300] global_step=253300, grad_norm=3.149118423461914, loss=1.1621640920639038
I0309 15:04:28.564799 139708407461632 logging_writer.py:48] [253400] global_step=253400, grad_norm=3.321620464324951, loss=1.1233383417129517
I0309 15:05:13.895533 139708415854336 logging_writer.py:48] [253500] global_step=253500, grad_norm=3.3946268558502197, loss=2.8973069190979004
I0309 15:05:59.032752 139708407461632 logging_writer.py:48] [253600] global_step=253600, grad_norm=3.175999641418457, loss=1.1437991857528687
I0309 15:06:20.867735 139902746892096 spec.py:321] Evaluating on the training split.
I0309 15:06:32.179170 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 15:06:54.988817 139902746892096 spec.py:349] Evaluating on the test split.
I0309 15:06:56.670088 139902746892096 submission_runner.py:411] Time since start: 122739.70s, 	Step: 253650, 	{'train/accuracy': 0.8868749737739563, 'train/loss': 0.4217991530895233, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 113475.82202625275, 'total_duration': 122739.69886732101, 'accumulated_submission_time': 113475.82202625275, 'accumulated_eval_time': 9236.304715394974, 'accumulated_logging_time': 14.241063117980957}
I0309 15:06:56.730091 139708415854336 logging_writer.py:48] [253650] accumulated_eval_time=9236.304715, accumulated_logging_time=14.241063, accumulated_submission_time=113475.822026, global_step=253650, preemption_count=0, score=113475.822026, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=122739.698867, train/accuracy=0.886875, train/loss=0.421799, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 15:07:16.806597 139708407461632 logging_writer.py:48] [253700] global_step=253700, grad_norm=2.946227788925171, loss=1.126280426979065
I0309 15:07:59.839228 139708415854336 logging_writer.py:48] [253800] global_step=253800, grad_norm=3.206766366958618, loss=1.0842781066894531
I0309 15:08:45.198899 139708407461632 logging_writer.py:48] [253900] global_step=253900, grad_norm=3.5502703189849854, loss=2.9155077934265137
I0309 15:09:30.890934 139708415854336 logging_writer.py:48] [254000] global_step=254000, grad_norm=3.164442300796509, loss=1.170348882675171
I0309 15:10:15.880759 139708407461632 logging_writer.py:48] [254100] global_step=254100, grad_norm=3.2199227809906006, loss=1.1545817852020264
I0309 15:11:01.076698 139708415854336 logging_writer.py:48] [254200] global_step=254200, grad_norm=2.6990485191345215, loss=2.020123243331909
I0309 15:11:46.320470 139708407461632 logging_writer.py:48] [254300] global_step=254300, grad_norm=3.033109426498413, loss=1.0542300939559937
I0309 15:12:31.602909 139708415854336 logging_writer.py:48] [254400] global_step=254400, grad_norm=3.758516550064087, loss=3.0968639850616455
I0309 15:13:17.085427 139708407461632 logging_writer.py:48] [254500] global_step=254500, grad_norm=3.361522912979126, loss=2.950331211090088
I0309 15:13:56.675872 139902746892096 spec.py:321] Evaluating on the training split.
I0309 15:14:08.071056 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 15:14:30.667608 139902746892096 spec.py:349] Evaluating on the test split.
I0309 15:14:32.344213 139902746892096 submission_runner.py:411] Time since start: 123195.37s, 	Step: 254589, 	{'train/accuracy': 0.8882030844688416, 'train/loss': 0.4138852655887604, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 113895.70823192596, 'total_duration': 123195.37301111221, 'accumulated_submission_time': 113895.70823192596, 'accumulated_eval_time': 9271.973058700562, 'accumulated_logging_time': 14.31032943725586}
I0309 15:14:32.399046 139708415854336 logging_writer.py:48] [254589] accumulated_eval_time=9271.973059, accumulated_logging_time=14.310329, accumulated_submission_time=113895.708232, global_step=254589, preemption_count=0, score=113895.708232, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=123195.373011, train/accuracy=0.888203, train/loss=0.413885, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 15:14:37.123220 139708407461632 logging_writer.py:48] [254600] global_step=254600, grad_norm=3.13198184967041, loss=1.0887432098388672
I0309 15:15:17.924860 139708415854336 logging_writer.py:48] [254700] global_step=254700, grad_norm=3.1159465312957764, loss=1.253827452659607
I0309 15:16:02.939178 139708407461632 logging_writer.py:48] [254800] global_step=254800, grad_norm=5.026256561279297, loss=2.382678747177124
I0309 15:16:48.181927 139708415854336 logging_writer.py:48] [254900] global_step=254900, grad_norm=3.2416772842407227, loss=1.1666115522384644
I0309 15:17:33.626435 139708407461632 logging_writer.py:48] [255000] global_step=255000, grad_norm=3.451854944229126, loss=1.2129954099655151
I0309 15:18:19.211378 139708415854336 logging_writer.py:48] [255100] global_step=255100, grad_norm=2.776866912841797, loss=1.1052846908569336
I0309 15:19:04.397322 139708407461632 logging_writer.py:48] [255200] global_step=255200, grad_norm=3.135160207748413, loss=1.1669796705245972
I0309 15:19:50.238257 139708415854336 logging_writer.py:48] [255300] global_step=255300, grad_norm=3.908785581588745, loss=3.2445626258850098
I0309 15:20:35.524086 139708407461632 logging_writer.py:48] [255400] global_step=255400, grad_norm=3.233189344406128, loss=1.2355338335037231
I0309 15:21:20.972415 139708415854336 logging_writer.py:48] [255500] global_step=255500, grad_norm=2.9736344814300537, loss=1.0870145559310913
I0309 15:21:32.371277 139902746892096 spec.py:321] Evaluating on the training split.
I0309 15:21:43.947405 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 15:22:08.021840 139902746892096 spec.py:349] Evaluating on the test split.
I0309 15:22:09.694526 139902746892096 submission_runner.py:411] Time since start: 123652.72s, 	Step: 255527, 	{'train/accuracy': 0.8911327719688416, 'train/loss': 0.4039798676967621, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 114315.62262010574, 'total_duration': 123652.72332525253, 'accumulated_submission_time': 114315.62262010574, 'accumulated_eval_time': 9309.296308279037, 'accumulated_logging_time': 14.374987840652466}
I0309 15:22:09.746889 139708407461632 logging_writer.py:48] [255527] accumulated_eval_time=9309.296308, accumulated_logging_time=14.374988, accumulated_submission_time=114315.622620, global_step=255527, preemption_count=0, score=114315.622620, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=123652.723325, train/accuracy=0.891133, train/loss=0.403980, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 15:22:38.847650 139708415854336 logging_writer.py:48] [255600] global_step=255600, grad_norm=3.6270854473114014, loss=1.1021877527236938
I0309 15:23:23.087496 139708407461632 logging_writer.py:48] [255700] global_step=255700, grad_norm=3.735273838043213, loss=3.164369583129883
I0309 15:24:08.623631 139708415854336 logging_writer.py:48] [255800] global_step=255800, grad_norm=3.9368817806243896, loss=3.2459001541137695
I0309 15:24:54.279189 139708407461632 logging_writer.py:48] [255900] global_step=255900, grad_norm=3.338179349899292, loss=1.1201063394546509
I0309 15:25:39.465800 139708415854336 logging_writer.py:48] [256000] global_step=256000, grad_norm=3.0932626724243164, loss=1.610487461090088
I0309 15:26:25.030230 139708407461632 logging_writer.py:48] [256100] global_step=256100, grad_norm=3.239921808242798, loss=1.280264973640442
I0309 15:27:10.171096 139708415854336 logging_writer.py:48] [256200] global_step=256200, grad_norm=3.037632465362549, loss=1.7142789363861084
I0309 15:27:55.714619 139708407461632 logging_writer.py:48] [256300] global_step=256300, grad_norm=2.769909381866455, loss=1.2132502794265747
I0309 15:28:41.125622 139708415854336 logging_writer.py:48] [256400] global_step=256400, grad_norm=3.021172285079956, loss=2.4398574829101562
I0309 15:29:09.742048 139902746892096 spec.py:321] Evaluating on the training split.
I0309 15:29:21.120890 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 15:29:43.928814 139902746892096 spec.py:349] Evaluating on the test split.
I0309 15:29:45.609171 139902746892096 submission_runner.py:411] Time since start: 124108.64s, 	Step: 256465, 	{'train/accuracy': 0.8897265195846558, 'train/loss': 0.412788063287735, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 114735.56063556671, 'total_duration': 124108.63796424866, 'accumulated_submission_time': 114735.56063556671, 'accumulated_eval_time': 9345.163439273834, 'accumulated_logging_time': 14.435834646224976}
I0309 15:29:45.663406 139708407461632 logging_writer.py:48] [256465] accumulated_eval_time=9345.163439, accumulated_logging_time=14.435835, accumulated_submission_time=114735.560636, global_step=256465, preemption_count=0, score=114735.560636, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=124108.637964, train/accuracy=0.889727, train/loss=0.412788, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 15:29:59.833055 139708415854336 logging_writer.py:48] [256500] global_step=256500, grad_norm=3.1705119609832764, loss=1.2023274898529053
I0309 15:30:41.956356 139708407461632 logging_writer.py:48] [256600] global_step=256600, grad_norm=2.9544718265533447, loss=1.6937720775604248
I0309 15:31:27.241079 139708415854336 logging_writer.py:48] [256700] global_step=256700, grad_norm=3.0794923305511475, loss=2.510324239730835
I0309 15:32:12.805429 139708407461632 logging_writer.py:48] [256800] global_step=256800, grad_norm=3.0276389122009277, loss=1.0265649557113647
I0309 15:32:58.140537 139708415854336 logging_writer.py:48] [256900] global_step=256900, grad_norm=3.0899174213409424, loss=2.043537139892578
I0309 15:33:43.841371 139708407461632 logging_writer.py:48] [257000] global_step=257000, grad_norm=3.107722759246826, loss=1.2563648223876953
I0309 15:34:29.118392 139708415854336 logging_writer.py:48] [257100] global_step=257100, grad_norm=3.19222354888916, loss=2.4896929264068604
I0309 15:35:14.348861 139708407461632 logging_writer.py:48] [257200] global_step=257200, grad_norm=3.109985589981079, loss=1.1424249410629272
I0309 15:35:59.777938 139708415854336 logging_writer.py:48] [257300] global_step=257300, grad_norm=3.282010078430176, loss=1.2247395515441895
I0309 15:36:44.970342 139708407461632 logging_writer.py:48] [257400] global_step=257400, grad_norm=4.097424030303955, loss=3.2667713165283203
I0309 15:36:45.933103 139902746892096 spec.py:321] Evaluating on the training split.
I0309 15:36:58.419816 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 15:37:19.402590 139902746892096 spec.py:349] Evaluating on the test split.
I0309 15:37:21.078184 139902746892096 submission_runner.py:411] Time since start: 124564.11s, 	Step: 257404, 	{'train/accuracy': 0.8879101276397705, 'train/loss': 0.4117997884750366, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 115155.77038359642, 'total_duration': 124564.10697579384, 'accumulated_submission_time': 115155.77038359642, 'accumulated_eval_time': 9380.308512449265, 'accumulated_logging_time': 14.500270128250122}
I0309 15:37:21.136699 139708415854336 logging_writer.py:48] [257404] accumulated_eval_time=9380.308512, accumulated_logging_time=14.500270, accumulated_submission_time=115155.770384, global_step=257404, preemption_count=0, score=115155.770384, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=124564.106976, train/accuracy=0.887910, train/loss=0.411800, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 15:38:00.011866 139708407461632 logging_writer.py:48] [257500] global_step=257500, grad_norm=2.8976848125457764, loss=1.1507554054260254
I0309 15:38:45.442502 139708415854336 logging_writer.py:48] [257600] global_step=257600, grad_norm=3.4423553943634033, loss=1.0626379251480103
I0309 15:39:30.983997 139708407461632 logging_writer.py:48] [257700] global_step=257700, grad_norm=2.8370587825775146, loss=1.476327657699585
I0309 15:40:16.783962 139708415854336 logging_writer.py:48] [257800] global_step=257800, grad_norm=3.0269181728363037, loss=1.158562183380127
I0309 15:41:02.008344 139708407461632 logging_writer.py:48] [257900] global_step=257900, grad_norm=3.2658932209014893, loss=1.1771494150161743
I0309 15:41:47.518213 139708415854336 logging_writer.py:48] [258000] global_step=258000, grad_norm=3.024656057357788, loss=1.1068578958511353
I0309 15:42:32.723330 139708407461632 logging_writer.py:48] [258100] global_step=258100, grad_norm=5.806117534637451, loss=3.266570568084717
I0309 15:43:18.002808 139708415854336 logging_writer.py:48] [258200] global_step=258200, grad_norm=3.088498592376709, loss=1.2425607442855835
I0309 15:44:03.627456 139708407461632 logging_writer.py:48] [258300] global_step=258300, grad_norm=3.2625396251678467, loss=2.5458221435546875
I0309 15:44:21.138779 139902746892096 spec.py:321] Evaluating on the training split.
I0309 15:44:32.755740 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 15:44:55.326431 139902746892096 spec.py:349] Evaluating on the test split.
I0309 15:44:57.002472 139902746892096 submission_runner.py:411] Time since start: 125020.03s, 	Step: 258340, 	{'train/accuracy': 0.8883788585662842, 'train/loss': 0.4111296832561493, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 115575.71538758278, 'total_duration': 125020.03127336502, 'accumulated_submission_time': 115575.71538758278, 'accumulated_eval_time': 9416.172207832336, 'accumulated_logging_time': 14.567237615585327}
I0309 15:44:57.055784 139708415854336 logging_writer.py:48] [258340] accumulated_eval_time=9416.172208, accumulated_logging_time=14.567238, accumulated_submission_time=115575.715388, global_step=258340, preemption_count=0, score=115575.715388, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=125020.031273, train/accuracy=0.888379, train/loss=0.411130, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 15:45:21.064289 139708407461632 logging_writer.py:48] [258400] global_step=258400, grad_norm=3.0065746307373047, loss=1.066338062286377
I0309 15:46:04.513823 139708415854336 logging_writer.py:48] [258500] global_step=258500, grad_norm=3.019657850265503, loss=1.7757765054702759
I0309 15:46:50.023272 139708407461632 logging_writer.py:48] [258600] global_step=258600, grad_norm=3.0674691200256348, loss=1.9977443218231201
I0309 15:47:35.575611 139708415854336 logging_writer.py:48] [258700] global_step=258700, grad_norm=3.0642781257629395, loss=1.0583674907684326
I0309 15:48:20.939814 139708407461632 logging_writer.py:48] [258800] global_step=258800, grad_norm=3.5401268005371094, loss=3.1103856563568115
I0309 15:49:06.453638 139708415854336 logging_writer.py:48] [258900] global_step=258900, grad_norm=3.079890489578247, loss=1.0557563304901123
I0309 15:49:51.991821 139708407461632 logging_writer.py:48] [259000] global_step=259000, grad_norm=2.860665798187256, loss=1.9062827825546265
I0309 15:50:37.002763 139708415854336 logging_writer.py:48] [259100] global_step=259100, grad_norm=3.086822509765625, loss=1.1181846857070923
I0309 15:51:22.242021 139708407461632 logging_writer.py:48] [259200] global_step=259200, grad_norm=3.0027730464935303, loss=1.5787181854248047
I0309 15:51:57.375782 139902746892096 spec.py:321] Evaluating on the training split.
I0309 15:52:08.757384 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 15:52:31.286891 139902746892096 spec.py:349] Evaluating on the test split.
I0309 15:52:32.959861 139902746892096 submission_runner.py:411] Time since start: 125475.99s, 	Step: 259279, 	{'train/accuracy': 0.8874804377555847, 'train/loss': 0.4159693717956543, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 115995.97527813911, 'total_duration': 125475.9886534214, 'accumulated_submission_time': 115995.97527813911, 'accumulated_eval_time': 9451.75629067421, 'accumulated_logging_time': 14.631416320800781}
I0309 15:52:33.013714 139708415854336 logging_writer.py:48] [259279] accumulated_eval_time=9451.756291, accumulated_logging_time=14.631416, accumulated_submission_time=115995.975278, global_step=259279, preemption_count=0, score=115995.975278, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=125475.988653, train/accuracy=0.887480, train/loss=0.415969, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 15:52:41.673408 139708407461632 logging_writer.py:48] [259300] global_step=259300, grad_norm=3.1084940433502197, loss=1.0981066226959229
I0309 15:53:23.026214 139708415854336 logging_writer.py:48] [259400] global_step=259400, grad_norm=2.966826915740967, loss=1.1372936964035034
I0309 15:54:08.192687 139708407461632 logging_writer.py:48] [259500] global_step=259500, grad_norm=3.2466137409210205, loss=2.1888046264648438
I0309 15:54:53.810355 139708415854336 logging_writer.py:48] [259600] global_step=259600, grad_norm=2.9831602573394775, loss=1.279046654701233
I0309 15:55:39.058033 139708407461632 logging_writer.py:48] [259700] global_step=259700, grad_norm=3.316861391067505, loss=1.346924066543579
I0309 15:56:24.324931 139708415854336 logging_writer.py:48] [259800] global_step=259800, grad_norm=3.2464728355407715, loss=1.1077274084091187
I0309 15:57:09.693239 139708407461632 logging_writer.py:48] [259900] global_step=259900, grad_norm=3.421994686126709, loss=3.103713274002075
I0309 15:57:55.090997 139708415854336 logging_writer.py:48] [260000] global_step=260000, grad_norm=3.0623226165771484, loss=1.0908489227294922
I0309 15:58:40.534354 139708407461632 logging_writer.py:48] [260100] global_step=260100, grad_norm=3.042588710784912, loss=1.2121244668960571
I0309 15:59:25.926075 139708415854336 logging_writer.py:48] [260200] global_step=260200, grad_norm=3.185905694961548, loss=1.1522873640060425
I0309 15:59:33.333778 139902746892096 spec.py:321] Evaluating on the training split.
I0309 15:59:44.578931 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 16:00:06.575874 139902746892096 spec.py:349] Evaluating on the test split.
I0309 16:00:08.257479 139902746892096 submission_runner.py:411] Time since start: 125931.29s, 	Step: 260218, 	{'train/accuracy': 0.888964831829071, 'train/loss': 0.41188135743141174, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 116416.23829936981, 'total_duration': 125931.2862727642, 'accumulated_submission_time': 116416.23829936981, 'accumulated_eval_time': 9486.679986476898, 'accumulated_logging_time': 14.693573951721191}
I0309 16:00:08.312856 139708407461632 logging_writer.py:48] [260218] accumulated_eval_time=9486.679986, accumulated_logging_time=14.693574, accumulated_submission_time=116416.238299, global_step=260218, preemption_count=0, score=116416.238299, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=125931.286273, train/accuracy=0.888965, train/loss=0.411881, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 16:00:41.206820 139708415854336 logging_writer.py:48] [260300] global_step=260300, grad_norm=2.974170207977295, loss=1.5323987007141113
I0309 16:01:26.276614 139708407461632 logging_writer.py:48] [260400] global_step=260400, grad_norm=3.304863929748535, loss=1.5811210870742798
I0309 16:02:11.773074 139708415854336 logging_writer.py:48] [260500] global_step=260500, grad_norm=2.8754770755767822, loss=1.540803074836731
I0309 16:02:57.364149 139708407461632 logging_writer.py:48] [260600] global_step=260600, grad_norm=3.31217360496521, loss=1.2295234203338623
I0309 16:03:42.532352 139708415854336 logging_writer.py:48] [260700] global_step=260700, grad_norm=2.956508159637451, loss=1.3147151470184326
I0309 16:04:27.994984 139708407461632 logging_writer.py:48] [260800] global_step=260800, grad_norm=3.1901626586914062, loss=1.121896743774414
I0309 16:05:13.309710 139708415854336 logging_writer.py:48] [260900] global_step=260900, grad_norm=3.418344497680664, loss=1.1529655456542969
I0309 16:05:58.560554 139708407461632 logging_writer.py:48] [261000] global_step=261000, grad_norm=3.898912191390991, loss=3.2184035778045654
I0309 16:06:43.920935 139708415854336 logging_writer.py:48] [261100] global_step=261100, grad_norm=2.8752596378326416, loss=1.598917007446289
I0309 16:07:08.641185 139902746892096 spec.py:321] Evaluating on the training split.
I0309 16:07:20.255431 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 16:07:42.568997 139902746892096 spec.py:349] Evaluating on the test split.
I0309 16:07:44.240413 139902746892096 submission_runner.py:411] Time since start: 126387.27s, 	Step: 261156, 	{'train/accuracy': 0.8884961009025574, 'train/loss': 0.41602829098701477, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 116836.50953197479, 'total_duration': 126387.26921463013, 'accumulated_submission_time': 116836.50953197479, 'accumulated_eval_time': 9522.27923154831, 'accumulated_logging_time': 14.757773399353027}
I0309 16:07:44.296017 139708407461632 logging_writer.py:48] [261156] accumulated_eval_time=9522.279232, accumulated_logging_time=14.757773, accumulated_submission_time=116836.509532, global_step=261156, preemption_count=0, score=116836.509532, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=126387.269215, train/accuracy=0.888496, train/loss=0.416028, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 16:08:02.004099 139708415854336 logging_writer.py:48] [261200] global_step=261200, grad_norm=2.931063413619995, loss=1.0985573530197144
I0309 16:08:44.529019 139708407461632 logging_writer.py:48] [261300] global_step=261300, grad_norm=3.3389689922332764, loss=1.9271584749221802
I0309 16:09:29.928544 139708415854336 logging_writer.py:48] [261400] global_step=261400, grad_norm=3.331019401550293, loss=1.2296581268310547
I0309 16:10:15.630244 139708407461632 logging_writer.py:48] [261500] global_step=261500, grad_norm=2.9950780868530273, loss=2.030667781829834
I0309 16:11:00.897805 139708415854336 logging_writer.py:48] [261600] global_step=261600, grad_norm=4.371075630187988, loss=3.148677110671997
I0309 16:11:46.135478 139708407461632 logging_writer.py:48] [261700] global_step=261700, grad_norm=3.278381109237671, loss=2.699303150177002
I0309 16:12:31.542882 139708415854336 logging_writer.py:48] [261800] global_step=261800, grad_norm=3.5117413997650146, loss=3.0330820083618164
I0309 16:13:16.907643 139708407461632 logging_writer.py:48] [261900] global_step=261900, grad_norm=3.249321937561035, loss=1.5670136213302612
I0309 16:14:02.224658 139708415854336 logging_writer.py:48] [262000] global_step=262000, grad_norm=3.0340330600738525, loss=1.0788019895553589
I0309 16:14:44.398817 139902746892096 spec.py:321] Evaluating on the training split.
I0309 16:14:55.745142 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 16:15:17.845243 139902746892096 spec.py:349] Evaluating on the test split.
I0309 16:15:19.524274 139902746892096 submission_runner.py:411] Time since start: 126842.55s, 	Step: 262095, 	{'train/accuracy': 0.8870702981948853, 'train/loss': 0.4149435758590698, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 117256.55498623848, 'total_duration': 126842.55304765701, 'accumulated_submission_time': 117256.55498623848, 'accumulated_eval_time': 9557.404682636261, 'accumulated_logging_time': 14.821744441986084}
I0309 16:15:19.578065 139708407461632 logging_writer.py:48] [262095] accumulated_eval_time=9557.404683, accumulated_logging_time=14.821744, accumulated_submission_time=117256.554986, global_step=262095, preemption_count=0, score=117256.554986, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=126842.553048, train/accuracy=0.887070, train/loss=0.414944, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 16:15:21.937846 139708415854336 logging_writer.py:48] [262100] global_step=262100, grad_norm=3.1374669075012207, loss=1.7044986486434937
I0309 16:16:02.371206 139708407461632 logging_writer.py:48] [262200] global_step=262200, grad_norm=2.944312334060669, loss=1.837906837463379
I0309 16:16:47.407861 139708415854336 logging_writer.py:48] [262300] global_step=262300, grad_norm=3.2577943801879883, loss=1.2611743211746216
I0309 16:17:33.050809 139708407461632 logging_writer.py:48] [262400] global_step=262400, grad_norm=2.951472759246826, loss=1.259450912475586
I0309 16:18:18.703644 139708415854336 logging_writer.py:48] [262500] global_step=262500, grad_norm=2.9826653003692627, loss=1.2673028707504272
I0309 16:19:03.897921 139708407461632 logging_writer.py:48] [262600] global_step=262600, grad_norm=3.3249247074127197, loss=2.615776300430298
I0309 16:19:49.099027 139708415854336 logging_writer.py:48] [262700] global_step=262700, grad_norm=3.2469840049743652, loss=1.1692601442337036
I0309 16:20:34.815898 139708407461632 logging_writer.py:48] [262800] global_step=262800, grad_norm=2.817795753479004, loss=1.5996735095977783
I0309 16:21:20.056704 139708415854336 logging_writer.py:48] [262900] global_step=262900, grad_norm=3.2254221439361572, loss=2.9104509353637695
I0309 16:22:05.424700 139708407461632 logging_writer.py:48] [263000] global_step=263000, grad_norm=3.332252264022827, loss=2.736131191253662
I0309 16:22:19.558847 139902746892096 spec.py:321] Evaluating on the training split.
I0309 16:22:31.159075 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 16:22:51.355617 139902746892096 spec.py:349] Evaluating on the test split.
I0309 16:22:53.037336 139902746892096 submission_runner.py:411] Time since start: 127296.07s, 	Step: 263033, 	{'train/accuracy': 0.8878515362739563, 'train/loss': 0.4189508855342865, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 117676.4786427021, 'total_duration': 127296.06613850594, 'accumulated_submission_time': 117676.4786427021, 'accumulated_eval_time': 9590.883177995682, 'accumulated_logging_time': 14.884145021438599}
I0309 16:22:53.094370 139708415854336 logging_writer.py:48] [263033] accumulated_eval_time=9590.883178, accumulated_logging_time=14.884145, accumulated_submission_time=117676.478643, global_step=263033, preemption_count=0, score=117676.478643, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=127296.066139, train/accuracy=0.887852, train/loss=0.418951, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 16:23:19.855248 139708407461632 logging_writer.py:48] [263100] global_step=263100, grad_norm=3.1093592643737793, loss=2.3290388584136963
I0309 16:24:03.504888 139708415854336 logging_writer.py:48] [263200] global_step=263200, grad_norm=3.031940221786499, loss=1.5532581806182861
I0309 16:24:49.058533 139708407461632 logging_writer.py:48] [263300] global_step=263300, grad_norm=2.916602611541748, loss=1.1115856170654297
I0309 16:25:34.544868 139708415854336 logging_writer.py:48] [263400] global_step=263400, grad_norm=3.0132946968078613, loss=1.0725165605545044
I0309 16:26:19.764181 139708407461632 logging_writer.py:48] [263500] global_step=263500, grad_norm=2.973402500152588, loss=1.833858847618103
I0309 16:27:05.083651 139708415854336 logging_writer.py:48] [263600] global_step=263600, grad_norm=3.075690507888794, loss=1.4752384424209595
I0309 16:27:50.559437 139708407461632 logging_writer.py:48] [263700] global_step=263700, grad_norm=3.043132781982422, loss=1.054905652999878
I0309 16:28:35.656013 139708415854336 logging_writer.py:48] [263800] global_step=263800, grad_norm=3.7260873317718506, loss=3.223327875137329
I0309 16:29:20.949411 139708407461632 logging_writer.py:48] [263900] global_step=263900, grad_norm=3.0719194412231445, loss=1.3961377143859863
I0309 16:29:53.199832 139902746892096 spec.py:321] Evaluating on the training split.
I0309 16:30:04.494253 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 16:30:26.386332 139902746892096 spec.py:349] Evaluating on the test split.
I0309 16:30:28.064823 139902746892096 submission_runner.py:411] Time since start: 127751.09s, 	Step: 263973, 	{'train/accuracy': 0.8878124952316284, 'train/loss': 0.4134690463542938, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 118096.52826428413, 'total_duration': 127751.09358382225, 'accumulated_submission_time': 118096.52826428413, 'accumulated_eval_time': 9625.748121738434, 'accumulated_logging_time': 14.949349880218506}
I0309 16:30:28.128645 139708415854336 logging_writer.py:48] [263973] accumulated_eval_time=9625.748122, accumulated_logging_time=14.949350, accumulated_submission_time=118096.528264, global_step=263973, preemption_count=0, score=118096.528264, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=127751.093584, train/accuracy=0.887812, train/loss=0.413469, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 16:30:39.141082 139708407461632 logging_writer.py:48] [264000] global_step=264000, grad_norm=3.401930093765259, loss=1.0801079273223877
I0309 16:31:20.970758 139708415854336 logging_writer.py:48] [264100] global_step=264100, grad_norm=3.638366460800171, loss=2.956981658935547
I0309 16:32:06.152955 139708407461632 logging_writer.py:48] [264200] global_step=264200, grad_norm=3.3797965049743652, loss=1.1588038206100464
I0309 16:32:51.469124 139708415854336 logging_writer.py:48] [264300] global_step=264300, grad_norm=3.1440622806549072, loss=1.1060476303100586
I0309 16:33:36.513139 139708407461632 logging_writer.py:48] [264400] global_step=264400, grad_norm=3.0934414863586426, loss=2.612020254135132
I0309 16:34:21.938078 139708415854336 logging_writer.py:48] [264500] global_step=264500, grad_norm=2.94857120513916, loss=1.0891081094741821
I0309 16:35:07.219542 139708407461632 logging_writer.py:48] [264600] global_step=264600, grad_norm=3.0415568351745605, loss=1.8646577596664429
I0309 16:35:52.496094 139708415854336 logging_writer.py:48] [264700] global_step=264700, grad_norm=3.0497772693634033, loss=1.0838112831115723
I0309 16:36:37.595493 139708407461632 logging_writer.py:48] [264800] global_step=264800, grad_norm=2.9329466819763184, loss=1.3285804986953735
I0309 16:37:22.691077 139708415854336 logging_writer.py:48] [264900] global_step=264900, grad_norm=2.9010541439056396, loss=1.1580145359039307
I0309 16:37:28.106709 139902746892096 spec.py:321] Evaluating on the training split.
I0309 16:37:40.032991 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 16:38:01.066274 139902746892096 spec.py:349] Evaluating on the test split.
I0309 16:38:02.745151 139902746892096 submission_runner.py:411] Time since start: 128205.77s, 	Step: 264914, 	{'train/accuracy': 0.8861327767372131, 'train/loss': 0.4168863594532013, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 118516.4486773014, 'total_duration': 128205.77392411232, 'accumulated_submission_time': 118516.4486773014, 'accumulated_eval_time': 9660.38653588295, 'accumulated_logging_time': 15.022777557373047}
I0309 16:38:02.809635 139708407461632 logging_writer.py:48] [264914] accumulated_eval_time=9660.386536, accumulated_logging_time=15.022778, accumulated_submission_time=118516.448677, global_step=264914, preemption_count=0, score=118516.448677, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=128205.773924, train/accuracy=0.886133, train/loss=0.416886, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 16:38:37.176880 139708415854336 logging_writer.py:48] [265000] global_step=265000, grad_norm=2.984771251678467, loss=1.5366451740264893
I0309 16:39:22.143438 139708407461632 logging_writer.py:48] [265100] global_step=265100, grad_norm=3.3367156982421875, loss=1.1910370588302612
I0309 16:40:07.525481 139708415854336 logging_writer.py:48] [265200] global_step=265200, grad_norm=3.4398720264434814, loss=1.1493951082229614
I0309 16:40:53.190992 139708407461632 logging_writer.py:48] [265300] global_step=265300, grad_norm=3.7775869369506836, loss=2.9507904052734375
I0309 16:41:38.246044 139708415854336 logging_writer.py:48] [265400] global_step=265400, grad_norm=3.3386130332946777, loss=1.202336072921753
I0309 16:42:23.481469 139708407461632 logging_writer.py:48] [265500] global_step=265500, grad_norm=2.9467930793762207, loss=2.352055072784424
I0309 16:43:08.708526 139708415854336 logging_writer.py:48] [265600] global_step=265600, grad_norm=3.034079074859619, loss=1.3543764352798462
I0309 16:43:54.023017 139708407461632 logging_writer.py:48] [265700] global_step=265700, grad_norm=3.0883350372314453, loss=1.0607296228408813
I0309 16:44:39.340545 139708415854336 logging_writer.py:48] [265800] global_step=265800, grad_norm=3.251617431640625, loss=1.0753785371780396
I0309 16:45:02.934487 139902746892096 spec.py:321] Evaluating on the training split.
I0309 16:45:14.267004 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 16:45:37.287154 139902746892096 spec.py:349] Evaluating on the test split.
I0309 16:45:38.954238 139902746892096 submission_runner.py:411] Time since start: 128661.98s, 	Step: 265854, 	{'train/accuracy': 0.8871288895606995, 'train/loss': 0.4185587167739868, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 118936.5146408081, 'total_duration': 128661.9830391407, 'accumulated_submission_time': 118936.5146408081, 'accumulated_eval_time': 9696.406280517578, 'accumulated_logging_time': 15.097080707550049}
I0309 16:45:39.009998 139708407461632 logging_writer.py:48] [265854] accumulated_eval_time=9696.406281, accumulated_logging_time=15.097081, accumulated_submission_time=118936.514641, global_step=265854, preemption_count=0, score=118936.514641, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=128661.983039, train/accuracy=0.887129, train/loss=0.418559, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 16:45:57.490874 139708415854336 logging_writer.py:48] [265900] global_step=265900, grad_norm=3.1399595737457275, loss=1.072745680809021
I0309 16:46:39.854328 139708407461632 logging_writer.py:48] [266000] global_step=266000, grad_norm=2.911426067352295, loss=2.2582814693450928
I0309 16:47:24.960124 139708415854336 logging_writer.py:48] [266100] global_step=266100, grad_norm=3.2423510551452637, loss=1.0898804664611816
I0309 16:48:10.445217 139708407461632 logging_writer.py:48] [266200] global_step=266200, grad_norm=2.930082082748413, loss=1.0628526210784912
I0309 16:48:55.775259 139708415854336 logging_writer.py:48] [266300] global_step=266300, grad_norm=2.9451894760131836, loss=1.9246971607208252
I0309 16:49:40.943421 139708407461632 logging_writer.py:48] [266400] global_step=266400, grad_norm=3.1247847080230713, loss=1.1126275062561035
I0309 16:50:26.511109 139708415854336 logging_writer.py:48] [266500] global_step=266500, grad_norm=3.94594144821167, loss=3.228673219680786
I0309 16:51:11.665486 139708407461632 logging_writer.py:48] [266600] global_step=266600, grad_norm=3.3164288997650146, loss=1.156068205833435
I0309 16:51:56.900707 139708415854336 logging_writer.py:48] [266700] global_step=266700, grad_norm=3.273470163345337, loss=1.402251958847046
I0309 16:52:39.209504 139902746892096 spec.py:321] Evaluating on the training split.
I0309 16:52:50.613033 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 16:53:13.363839 139902746892096 spec.py:349] Evaluating on the test split.
I0309 16:53:15.032842 139902746892096 submission_runner.py:411] Time since start: 129118.06s, 	Step: 266795, 	{'train/accuracy': 0.8882030844688416, 'train/loss': 0.4178065359592438, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 119356.6550552845, 'total_duration': 129118.06162571907, 'accumulated_submission_time': 119356.6550552845, 'accumulated_eval_time': 9732.229608774185, 'accumulated_logging_time': 15.162375926971436}
I0309 16:53:15.096944 139708407461632 logging_writer.py:48] [266795] accumulated_eval_time=9732.229609, accumulated_logging_time=15.162376, accumulated_submission_time=119356.655055, global_step=266795, preemption_count=0, score=119356.655055, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=129118.061626, train/accuracy=0.888203, train/loss=0.417807, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 16:53:17.457914 139708415854336 logging_writer.py:48] [266800] global_step=266800, grad_norm=3.080801486968994, loss=1.8708866834640503
I0309 16:53:57.701614 139708407461632 logging_writer.py:48] [266900] global_step=266900, grad_norm=3.216099977493286, loss=1.3246865272521973
I0309 16:54:42.749065 139708415854336 logging_writer.py:48] [267000] global_step=267000, grad_norm=3.00868558883667, loss=2.0644702911376953
I0309 16:55:28.390840 139708407461632 logging_writer.py:48] [267100] global_step=267100, grad_norm=3.0625295639038086, loss=2.6738903522491455
I0309 16:56:13.794112 139708415854336 logging_writer.py:48] [267200] global_step=267200, grad_norm=4.0560832023620605, loss=3.2736525535583496
I0309 16:56:59.263253 139708407461632 logging_writer.py:48] [267300] global_step=267300, grad_norm=3.4384982585906982, loss=2.0666818618774414
I0309 16:57:44.751151 139708415854336 logging_writer.py:48] [267400] global_step=267400, grad_norm=2.915811777114868, loss=1.0185604095458984
I0309 16:58:30.034607 139708407461632 logging_writer.py:48] [267500] global_step=267500, grad_norm=3.8008317947387695, loss=3.1641042232513428
I0309 16:59:15.217167 139708415854336 logging_writer.py:48] [267600] global_step=267600, grad_norm=2.9050402641296387, loss=1.0901082754135132
I0309 17:00:00.424681 139708407461632 logging_writer.py:48] [267700] global_step=267700, grad_norm=3.4420559406280518, loss=2.997480869293213
I0309 17:00:15.318146 139902746892096 spec.py:321] Evaluating on the training split.
I0309 17:00:26.870043 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 17:00:48.355350 139902746892096 spec.py:349] Evaluating on the test split.
I0309 17:00:50.027800 139902746892096 submission_runner.py:411] Time since start: 129573.06s, 	Step: 267734, 	{'train/accuracy': 0.887011706829071, 'train/loss': 0.41818296909332275, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 119776.81697392464, 'total_duration': 129573.05659985542, 'accumulated_submission_time': 119776.81697392464, 'accumulated_eval_time': 9766.939259529114, 'accumulated_logging_time': 15.23660135269165}
I0309 17:00:50.084700 139708415854336 logging_writer.py:48] [267734] accumulated_eval_time=9766.939260, accumulated_logging_time=15.236601, accumulated_submission_time=119776.816974, global_step=267734, preemption_count=0, score=119776.816974, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=129573.056600, train/accuracy=0.887012, train/loss=0.418183, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 17:01:16.455158 139708407461632 logging_writer.py:48] [267800] global_step=267800, grad_norm=2.9291160106658936, loss=1.1335490942001343
I0309 17:02:00.481925 139708415854336 logging_writer.py:48] [267900] global_step=267900, grad_norm=3.1063058376312256, loss=1.1079550981521606
I0309 17:02:45.634516 139708407461632 logging_writer.py:48] [268000] global_step=268000, grad_norm=3.1737821102142334, loss=2.50274658203125
I0309 17:03:31.267579 139708415854336 logging_writer.py:48] [268100] global_step=268100, grad_norm=3.1963179111480713, loss=1.7445868253707886
I0309 17:04:16.554291 139708407461632 logging_writer.py:48] [268200] global_step=268200, grad_norm=3.084134340286255, loss=1.701635718345642
I0309 17:05:01.718499 139708415854336 logging_writer.py:48] [268300] global_step=268300, grad_norm=3.0931177139282227, loss=1.0962867736816406
I0309 17:05:46.917660 139708407461632 logging_writer.py:48] [268400] global_step=268400, grad_norm=3.347353458404541, loss=2.9513120651245117
I0309 17:06:32.318866 139708415854336 logging_writer.py:48] [268500] global_step=268500, grad_norm=3.8778858184814453, loss=3.2789275646209717
I0309 17:07:17.587202 139708407461632 logging_writer.py:48] [268600] global_step=268600, grad_norm=3.03865122795105, loss=1.4609447717666626
I0309 17:07:50.181514 139902746892096 spec.py:321] Evaluating on the training split.
I0309 17:08:01.658673 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 17:08:23.260485 139902746892096 spec.py:349] Evaluating on the test split.
I0309 17:08:24.934472 139902746892096 submission_runner.py:411] Time since start: 130027.96s, 	Step: 268674, 	{'train/accuracy': 0.8915820121765137, 'train/loss': 0.4057655334472656, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 120196.85601568222, 'total_duration': 130027.96326994896, 'accumulated_submission_time': 120196.85601568222, 'accumulated_eval_time': 9801.692220449448, 'accumulated_logging_time': 15.302525758743286}
I0309 17:08:24.994779 139708415854336 logging_writer.py:48] [268674] accumulated_eval_time=9801.692220, accumulated_logging_time=15.302526, accumulated_submission_time=120196.856016, global_step=268674, preemption_count=0, score=120196.856016, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=130027.963270, train/accuracy=0.891582, train/loss=0.405766, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 17:08:35.749477 139708407461632 logging_writer.py:48] [268700] global_step=268700, grad_norm=3.328348398208618, loss=2.2781894207000732
I0309 17:09:17.558117 139708415854336 logging_writer.py:48] [268800] global_step=268800, grad_norm=3.768728017807007, loss=3.2326912879943848
I0309 17:10:02.664306 139708407461632 logging_writer.py:48] [268900] global_step=268900, grad_norm=3.797222137451172, loss=2.072246789932251
I0309 17:10:48.194902 139708415854336 logging_writer.py:48] [269000] global_step=269000, grad_norm=2.9727585315704346, loss=2.193941593170166
I0309 17:11:33.488483 139708407461632 logging_writer.py:48] [269100] global_step=269100, grad_norm=3.2396934032440186, loss=1.2309963703155518
I0309 17:12:18.904889 139708415854336 logging_writer.py:48] [269200] global_step=269200, grad_norm=3.1910817623138428, loss=1.1821523904800415
I0309 17:13:04.282699 139708407461632 logging_writer.py:48] [269300] global_step=269300, grad_norm=3.108175277709961, loss=1.1378417015075684
I0309 17:13:49.401996 139708415854336 logging_writer.py:48] [269400] global_step=269400, grad_norm=3.3099780082702637, loss=1.156769037246704
I0309 17:14:34.470449 139708407461632 logging_writer.py:48] [269500] global_step=269500, grad_norm=2.965989589691162, loss=1.1440602540969849
I0309 17:15:19.844939 139708415854336 logging_writer.py:48] [269600] global_step=269600, grad_norm=3.065277099609375, loss=2.4535977840423584
I0309 17:15:24.963193 139902746892096 spec.py:321] Evaluating on the training split.
I0309 17:15:36.457840 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 17:15:59.248187 139902746892096 spec.py:349] Evaluating on the test split.
I0309 17:16:00.920387 139902746892096 submission_runner.py:411] Time since start: 130483.95s, 	Step: 269613, 	{'train/accuracy': 0.8875976204872131, 'train/loss': 0.42039817571640015, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 120616.76523089409, 'total_duration': 130483.94918727875, 'accumulated_submission_time': 120616.76523089409, 'accumulated_eval_time': 9837.649421453476, 'accumulated_logging_time': 15.373100996017456}
I0309 17:16:00.976229 139708407461632 logging_writer.py:48] [269613] accumulated_eval_time=9837.649421, accumulated_logging_time=15.373101, accumulated_submission_time=120616.765231, global_step=269613, preemption_count=0, score=120616.765231, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=130483.949187, train/accuracy=0.887598, train/loss=0.420398, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 17:16:35.577168 139708415854336 logging_writer.py:48] [269700] global_step=269700, grad_norm=2.92828106880188, loss=2.0080795288085938
I0309 17:17:20.623237 139708407461632 logging_writer.py:48] [269800] global_step=269800, grad_norm=2.9640769958496094, loss=0.971979558467865
I0309 17:18:06.204649 139708415854336 logging_writer.py:48] [269900] global_step=269900, grad_norm=2.8054165840148926, loss=2.0783309936523438
I0309 17:18:51.752871 139708407461632 logging_writer.py:48] [270000] global_step=270000, grad_norm=3.0246129035949707, loss=1.1636306047439575
I0309 17:19:36.966493 139708415854336 logging_writer.py:48] [270100] global_step=270100, grad_norm=3.193660259246826, loss=1.678110122680664
I0309 17:20:22.278349 139708407461632 logging_writer.py:48] [270200] global_step=270200, grad_norm=3.2908358573913574, loss=1.3212637901306152
I0309 17:21:07.833923 139708415854336 logging_writer.py:48] [270300] global_step=270300, grad_norm=3.098207473754883, loss=1.2005730867385864
I0309 17:21:52.816210 139708407461632 logging_writer.py:48] [270400] global_step=270400, grad_norm=2.904775381088257, loss=1.9316455125808716
I0309 17:22:38.168889 139708415854336 logging_writer.py:48] [270500] global_step=270500, grad_norm=2.9802565574645996, loss=1.0333225727081299
I0309 17:23:01.345402 139902746892096 spec.py:321] Evaluating on the training split.
I0309 17:23:12.643749 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 17:23:34.840914 139902746892096 spec.py:349] Evaluating on the test split.
I0309 17:23:36.506549 139902746892096 submission_runner.py:411] Time since start: 130939.54s, 	Step: 270553, 	{'train/accuracy': 0.8871679306030273, 'train/loss': 0.41848626732826233, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 121037.0772664547, 'total_duration': 130939.53532242775, 'accumulated_submission_time': 121037.0772664547, 'accumulated_eval_time': 9872.810532808304, 'accumulated_logging_time': 15.437132596969604}
I0309 17:23:36.570314 139708407461632 logging_writer.py:48] [270553] accumulated_eval_time=9872.810533, accumulated_logging_time=15.437133, accumulated_submission_time=121037.077266, global_step=270553, preemption_count=0, score=121037.077266, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=130939.535322, train/accuracy=0.887168, train/loss=0.418486, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 17:23:55.470874 139708415854336 logging_writer.py:48] [270600] global_step=270600, grad_norm=3.7563912868499756, loss=3.3301854133605957
I0309 17:24:38.028654 139708407461632 logging_writer.py:48] [270700] global_step=270700, grad_norm=3.5480449199676514, loss=1.214684247970581
I0309 17:25:23.140524 139708415854336 logging_writer.py:48] [270800] global_step=270800, grad_norm=3.763497829437256, loss=3.078770160675049
I0309 17:26:08.515186 139708407461632 logging_writer.py:48] [270900] global_step=270900, grad_norm=3.029792547225952, loss=1.1719005107879639
I0309 17:26:53.494917 139708415854336 logging_writer.py:48] [271000] global_step=271000, grad_norm=3.1332435607910156, loss=1.0465476512908936
I0309 17:27:38.703160 139708407461632 logging_writer.py:48] [271100] global_step=271100, grad_norm=3.1152591705322266, loss=2.25089693069458
I0309 17:28:23.919567 139708415854336 logging_writer.py:48] [271200] global_step=271200, grad_norm=3.282723903656006, loss=1.6392452716827393
I0309 17:29:08.990262 139708407461632 logging_writer.py:48] [271300] global_step=271300, grad_norm=3.041217565536499, loss=1.235822081565857
I0309 17:29:54.195890 139708415854336 logging_writer.py:48] [271400] global_step=271400, grad_norm=3.1696479320526123, loss=1.584944486618042
I0309 17:30:36.640966 139902746892096 spec.py:321] Evaluating on the training split.
I0309 17:30:48.138327 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 17:31:11.196865 139902746892096 spec.py:349] Evaluating on the test split.
I0309 17:31:12.876456 139902746892096 submission_runner.py:411] Time since start: 131395.91s, 	Step: 271495, 	{'train/accuracy': 0.8880859017372131, 'train/loss': 0.41766998171806335, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 121457.0885810852, 'total_duration': 131395.90521931648, 'accumulated_submission_time': 121457.0885810852, 'accumulated_eval_time': 9909.046003103256, 'accumulated_logging_time': 15.510629415512085}
I0309 17:31:12.944917 139708407461632 logging_writer.py:48] [271495] accumulated_eval_time=9909.046003, accumulated_logging_time=15.510629, accumulated_submission_time=121457.088581, global_step=271495, preemption_count=0, score=121457.088581, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=131395.905219, train/accuracy=0.888086, train/loss=0.417670, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 17:31:15.308014 139708415854336 logging_writer.py:48] [271500] global_step=271500, grad_norm=3.9000191688537598, loss=3.186429023742676
I0309 17:31:55.849929 139708407461632 logging_writer.py:48] [271600] global_step=271600, grad_norm=2.9212563037872314, loss=1.1990662813186646
I0309 17:32:41.012265 139708415854336 logging_writer.py:48] [271700] global_step=271700, grad_norm=3.004040002822876, loss=1.3241382837295532
I0309 17:33:26.287456 139708407461632 logging_writer.py:48] [271800] global_step=271800, grad_norm=4.037271976470947, loss=2.96648907661438
I0309 17:34:11.687953 139708415854336 logging_writer.py:48] [271900] global_step=271900, grad_norm=3.8574378490448, loss=3.354264736175537
I0309 17:34:57.137962 139708407461632 logging_writer.py:48] [272000] global_step=272000, grad_norm=3.1204590797424316, loss=1.311881422996521
I0309 17:35:42.525418 139708415854336 logging_writer.py:48] [272100] global_step=272100, grad_norm=3.034271717071533, loss=2.0905566215515137
I0309 17:36:27.515366 139708407461632 logging_writer.py:48] [272200] global_step=272200, grad_norm=2.9018912315368652, loss=1.7049095630645752
I0309 17:37:12.791206 139708415854336 logging_writer.py:48] [272300] global_step=272300, grad_norm=2.840378522872925, loss=1.3385651111602783
I0309 17:37:58.329678 139708407461632 logging_writer.py:48] [272400] global_step=272400, grad_norm=3.0127389430999756, loss=1.1652508974075317
I0309 17:38:13.324580 139902746892096 spec.py:321] Evaluating on the training split.
I0309 17:38:24.777555 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 17:38:47.298784 139902746892096 spec.py:349] Evaluating on the test split.
I0309 17:38:48.974745 139902746892096 submission_runner.py:411] Time since start: 131852.00s, 	Step: 272435, 	{'train/accuracy': 0.8871484398841858, 'train/loss': 0.4141007661819458, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 121877.40932011604, 'total_duration': 131852.0035393238, 'accumulated_submission_time': 121877.40932011604, 'accumulated_eval_time': 9944.696164131165, 'accumulated_logging_time': 15.589151382446289}
I0309 17:38:49.034279 139708415854336 logging_writer.py:48] [272435] accumulated_eval_time=9944.696164, accumulated_logging_time=15.589151, accumulated_submission_time=121877.409320, global_step=272435, preemption_count=0, score=121877.409320, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=131852.003539, train/accuracy=0.887148, train/loss=0.414101, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 17:39:14.995984 139708407461632 logging_writer.py:48] [272500] global_step=272500, grad_norm=3.4969892501831055, loss=1.6052491664886475
I0309 17:39:58.437978 139708415854336 logging_writer.py:48] [272600] global_step=272600, grad_norm=3.232919454574585, loss=1.1528685092926025
I0309 17:40:43.734596 139708407461632 logging_writer.py:48] [272700] global_step=272700, grad_norm=3.32045316696167, loss=2.9551844596862793
I0309 17:41:29.240351 139708415854336 logging_writer.py:48] [272800] global_step=272800, grad_norm=3.1398537158966064, loss=1.0595641136169434
I0309 17:42:14.364985 139708407461632 logging_writer.py:48] [272900] global_step=272900, grad_norm=3.2718679904937744, loss=1.6120985746383667
I0309 17:42:59.667124 139708415854336 logging_writer.py:48] [273000] global_step=273000, grad_norm=3.447852611541748, loss=3.053941249847412
I0309 17:43:45.170128 139708407461632 logging_writer.py:48] [273100] global_step=273100, grad_norm=2.984724760055542, loss=1.1858469247817993
I0309 17:44:30.361752 139708415854336 logging_writer.py:48] [273200] global_step=273200, grad_norm=2.94014310836792, loss=1.119147777557373
I0309 17:45:15.612308 139708407461632 logging_writer.py:48] [273300] global_step=273300, grad_norm=2.7801592350006104, loss=1.8076057434082031
I0309 17:45:49.181282 139902746892096 spec.py:321] Evaluating on the training split.
I0309 17:46:00.657668 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 17:46:21.088945 139902746892096 spec.py:349] Evaluating on the test split.
I0309 17:46:22.768339 139902746892096 submission_runner.py:411] Time since start: 132305.80s, 	Step: 273376, 	{'train/accuracy': 0.887499988079071, 'train/loss': 0.416747510433197, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 122297.49721193314, 'total_duration': 132305.79712724686, 'accumulated_submission_time': 122297.49721193314, 'accumulated_eval_time': 9978.283198833466, 'accumulated_logging_time': 15.658845663070679}
I0309 17:46:22.837647 139708415854336 logging_writer.py:48] [273376] accumulated_eval_time=9978.283199, accumulated_logging_time=15.658846, accumulated_submission_time=122297.497212, global_step=273376, preemption_count=0, score=122297.497212, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=132305.797127, train/accuracy=0.887500, train/loss=0.416748, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 17:46:32.690410 139708407461632 logging_writer.py:48] [273400] global_step=273400, grad_norm=3.1130664348602295, loss=1.7014013528823853
I0309 17:47:14.518958 139708415854336 logging_writer.py:48] [273500] global_step=273500, grad_norm=2.9637835025787354, loss=1.4805012941360474
I0309 17:47:59.850926 139708407461632 logging_writer.py:48] [273600] global_step=273600, grad_norm=3.0896754264831543, loss=1.1256368160247803
I0309 17:48:44.835269 139708415854336 logging_writer.py:48] [273700] global_step=273700, grad_norm=2.9617228507995605, loss=1.0847177505493164
I0309 17:49:29.996906 139708407461632 logging_writer.py:48] [273800] global_step=273800, grad_norm=3.017165184020996, loss=1.9417228698730469
I0309 17:50:15.224696 139708415854336 logging_writer.py:48] [273900] global_step=273900, grad_norm=3.2436678409576416, loss=1.1207342147827148
I0309 17:51:00.759768 139708407461632 logging_writer.py:48] [274000] global_step=274000, grad_norm=2.884577751159668, loss=1.9371718168258667
I0309 17:51:45.838226 139708415854336 logging_writer.py:48] [274100] global_step=274100, grad_norm=3.082763910293579, loss=2.239797830581665
I0309 17:52:31.316846 139708407461632 logging_writer.py:48] [274200] global_step=274200, grad_norm=3.0228264331817627, loss=1.012420415878296
I0309 17:53:16.845796 139708415854336 logging_writer.py:48] [274300] global_step=274300, grad_norm=3.301938772201538, loss=1.22390878200531
I0309 17:53:22.846488 139902746892096 spec.py:321] Evaluating on the training split.
I0309 17:53:34.523613 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 17:53:57.700672 139902746892096 spec.py:349] Evaluating on the test split.
I0309 17:53:59.368474 139902746892096 submission_runner.py:411] Time since start: 132762.40s, 	Step: 274315, 	{'train/accuracy': 0.888671875, 'train/loss': 0.41597700119018555, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 122717.44657683372, 'total_duration': 132762.39725995064, 'accumulated_submission_time': 122717.44657683372, 'accumulated_eval_time': 10014.80516576767, 'accumulated_logging_time': 15.73838758468628}
I0309 17:53:59.424003 139708407461632 logging_writer.py:48] [274315] accumulated_eval_time=10014.805166, accumulated_logging_time=15.738388, accumulated_submission_time=122717.446577, global_step=274315, preemption_count=0, score=122717.446577, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=132762.397260, train/accuracy=0.888672, train/loss=0.415977, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 17:54:33.244809 139708415854336 logging_writer.py:48] [274400] global_step=274400, grad_norm=3.358567237854004, loss=3.034036159515381
I0309 17:55:18.094579 139708407461632 logging_writer.py:48] [274500] global_step=274500, grad_norm=3.1044692993164062, loss=1.0419905185699463
I0309 17:56:03.630806 139708415854336 logging_writer.py:48] [274600] global_step=274600, grad_norm=3.508500337600708, loss=1.1198009252548218
I0309 17:56:48.834906 139708407461632 logging_writer.py:48] [274700] global_step=274700, grad_norm=3.1884727478027344, loss=1.1146067380905151
I0309 17:57:34.176150 139708415854336 logging_writer.py:48] [274800] global_step=274800, grad_norm=3.40964412689209, loss=1.2814353704452515
I0309 17:58:19.526133 139708407461632 logging_writer.py:48] [274900] global_step=274900, grad_norm=2.938077688217163, loss=1.006569266319275
I0309 17:59:04.596199 139708415854336 logging_writer.py:48] [275000] global_step=275000, grad_norm=3.251685380935669, loss=1.112188458442688
I0309 17:59:49.821174 139708407461632 logging_writer.py:48] [275100] global_step=275100, grad_norm=2.9575018882751465, loss=1.1190669536590576
I0309 18:00:35.027943 139708415854336 logging_writer.py:48] [275200] global_step=275200, grad_norm=4.047340393066406, loss=3.187654972076416
I0309 18:00:59.788778 139902746892096 spec.py:321] Evaluating on the training split.
I0309 18:01:11.230488 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 18:01:33.640875 139902746892096 spec.py:349] Evaluating on the test split.
I0309 18:01:35.309361 139902746892096 submission_runner.py:411] Time since start: 133218.34s, 	Step: 275256, 	{'train/accuracy': 0.8893945217132568, 'train/loss': 0.4109589755535126, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 123137.75361943245, 'total_duration': 133218.33816456795, 'accumulated_submission_time': 123137.75361943245, 'accumulated_eval_time': 10050.325754642487, 'accumulated_logging_time': 15.802060842514038}
I0309 18:01:35.365210 139708407461632 logging_writer.py:48] [275256] accumulated_eval_time=10050.325755, accumulated_logging_time=15.802061, accumulated_submission_time=123137.753619, global_step=275256, preemption_count=0, score=123137.753619, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=133218.338165, train/accuracy=0.889395, train/loss=0.410959, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 18:01:53.072127 139708415854336 logging_writer.py:48] [275300] global_step=275300, grad_norm=2.997481107711792, loss=1.061814785003662
I0309 18:02:35.707506 139708407461632 logging_writer.py:48] [275400] global_step=275400, grad_norm=2.9470176696777344, loss=2.2160511016845703
I0309 18:03:20.986054 139708415854336 logging_writer.py:48] [275500] global_step=275500, grad_norm=3.111739158630371, loss=1.203207015991211
I0309 18:04:06.622048 139708407461632 logging_writer.py:48] [275600] global_step=275600, grad_norm=3.1574363708496094, loss=1.038367748260498
I0309 18:04:51.749956 139708415854336 logging_writer.py:48] [275700] global_step=275700, grad_norm=3.58913516998291, loss=2.254450798034668
I0309 18:05:37.317475 139708407461632 logging_writer.py:48] [275800] global_step=275800, grad_norm=3.5288872718811035, loss=2.9396419525146484
I0309 18:06:22.665804 139708415854336 logging_writer.py:48] [275900] global_step=275900, grad_norm=3.252804756164551, loss=1.3591134548187256
I0309 18:07:07.740092 139708407461632 logging_writer.py:48] [276000] global_step=276000, grad_norm=3.574836492538452, loss=3.208752155303955
I0309 18:07:53.228708 139708415854336 logging_writer.py:48] [276100] global_step=276100, grad_norm=3.385810136795044, loss=1.1565110683441162
I0309 18:08:35.528463 139902746892096 spec.py:321] Evaluating on the training split.
I0309 18:08:46.901150 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 18:09:07.994287 139902746892096 spec.py:349] Evaluating on the test split.
I0309 18:09:09.679236 139902746892096 submission_runner.py:411] Time since start: 133672.71s, 	Step: 276195, 	{'train/accuracy': 0.88636714220047, 'train/loss': 0.4172734320163727, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 123557.85822701454, 'total_duration': 133672.7080309391, 'accumulated_submission_time': 123557.85822701454, 'accumulated_eval_time': 10084.47655248642, 'accumulated_logging_time': 15.868030309677124}
I0309 18:09:09.736552 139708407461632 logging_writer.py:48] [276195] accumulated_eval_time=10084.476552, accumulated_logging_time=15.868030, accumulated_submission_time=123557.858227, global_step=276195, preemption_count=0, score=123557.858227, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=133672.708031, train/accuracy=0.886367, train/loss=0.417273, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 18:09:12.100743 139708415854336 logging_writer.py:48] [276200] global_step=276200, grad_norm=2.9659340381622314, loss=1.1781158447265625
I0309 18:09:52.793291 139708407461632 logging_writer.py:48] [276300] global_step=276300, grad_norm=3.8646087646484375, loss=3.282555341720581
I0309 18:10:37.990315 139708415854336 logging_writer.py:48] [276400] global_step=276400, grad_norm=3.077039957046509, loss=1.7414538860321045
I0309 18:11:23.645000 139708407461632 logging_writer.py:48] [276500] global_step=276500, grad_norm=3.0682010650634766, loss=1.1169731616973877
I0309 18:12:08.982931 139708415854336 logging_writer.py:48] [276600] global_step=276600, grad_norm=2.97190260887146, loss=1.13407301902771
I0309 18:12:54.192585 139708407461632 logging_writer.py:48] [276700] global_step=276700, grad_norm=2.9940524101257324, loss=1.1571248769760132
I0309 18:13:39.405413 139708415854336 logging_writer.py:48] [276800] global_step=276800, grad_norm=2.9065046310424805, loss=1.091200590133667
I0309 18:14:24.623630 139708407461632 logging_writer.py:48] [276900] global_step=276900, grad_norm=3.2314693927764893, loss=1.221842646598816
I0309 18:15:09.787953 139708415854336 logging_writer.py:48] [277000] global_step=277000, grad_norm=5.019538879394531, loss=3.195261240005493
I0309 18:15:55.068455 139708407461632 logging_writer.py:48] [277100] global_step=277100, grad_norm=3.1766421794891357, loss=1.1979515552520752
I0309 18:16:10.102530 139902746892096 spec.py:321] Evaluating on the training split.
I0309 18:16:21.593516 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 18:16:41.854261 139902746892096 spec.py:349] Evaluating on the test split.
I0309 18:16:43.537056 139902746892096 submission_runner.py:411] Time since start: 134126.57s, 	Step: 277135, 	{'train/accuracy': 0.8903319835662842, 'train/loss': 0.41360557079315186, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 123978.16338539124, 'total_duration': 134126.56583857536, 'accumulated_submission_time': 123978.16338539124, 'accumulated_eval_time': 10117.911062717438, 'accumulated_logging_time': 15.936080694198608}
I0309 18:16:43.601684 139708415854336 logging_writer.py:48] [277135] accumulated_eval_time=10117.911063, accumulated_logging_time=15.936081, accumulated_submission_time=123978.163385, global_step=277135, preemption_count=0, score=123978.163385, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=134126.565839, train/accuracy=0.890332, train/loss=0.413606, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 18:17:09.575244 139708407461632 logging_writer.py:48] [277200] global_step=277200, grad_norm=2.937236785888672, loss=1.114402174949646
I0309 18:17:53.723835 139708415854336 logging_writer.py:48] [277300] global_step=277300, grad_norm=2.9649715423583984, loss=1.0883694887161255
I0309 18:18:39.170690 139708407461632 logging_writer.py:48] [277400] global_step=277400, grad_norm=3.220477819442749, loss=1.1393728256225586
I0309 18:19:24.347344 139708415854336 logging_writer.py:48] [277500] global_step=277500, grad_norm=3.233980178833008, loss=1.2018252611160278
I0309 18:20:09.464028 139708407461632 logging_writer.py:48] [277600] global_step=277600, grad_norm=2.793107271194458, loss=1.419930100440979
I0309 18:20:54.479679 139708415854336 logging_writer.py:48] [277700] global_step=277700, grad_norm=3.15276837348938, loss=1.1689730882644653
I0309 18:21:39.925402 139708407461632 logging_writer.py:48] [277800] global_step=277800, grad_norm=2.9135282039642334, loss=1.1017789840698242
I0309 18:22:25.240687 139708415854336 logging_writer.py:48] [277900] global_step=277900, grad_norm=3.0416436195373535, loss=1.9484398365020752
I0309 18:23:10.433279 139708407461632 logging_writer.py:48] [278000] global_step=278000, grad_norm=3.2310290336608887, loss=1.4175238609313965
I0309 18:23:43.642794 139902746892096 spec.py:321] Evaluating on the training split.
I0309 18:23:55.030826 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 18:24:15.634099 139902746892096 spec.py:349] Evaluating on the test split.
I0309 18:24:17.321740 139902746892096 submission_runner.py:411] Time since start: 134580.35s, 	Step: 278075, 	{'train/accuracy': 0.8899218440055847, 'train/loss': 0.41580575704574585, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 124398.14441418648, 'total_duration': 134580.3505373001, 'accumulated_submission_time': 124398.14441418648, 'accumulated_eval_time': 10151.590016841888, 'accumulated_logging_time': 16.011402368545532}
I0309 18:24:17.379913 139708415854336 logging_writer.py:48] [278075] accumulated_eval_time=10151.590017, accumulated_logging_time=16.011402, accumulated_submission_time=124398.144414, global_step=278075, preemption_count=0, score=124398.144414, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=134580.350537, train/accuracy=0.889922, train/loss=0.415806, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 18:24:27.611810 139708407461632 logging_writer.py:48] [278100] global_step=278100, grad_norm=3.0188980102539062, loss=1.2473889589309692
I0309 18:25:09.028139 139708415854336 logging_writer.py:48] [278200] global_step=278200, grad_norm=3.224808692932129, loss=1.2178466320037842
I0309 18:25:54.233439 139708407461632 logging_writer.py:48] [278300] global_step=278300, grad_norm=3.3506999015808105, loss=1.168359398841858
I0309 18:26:40.095514 139708415854336 logging_writer.py:48] [278400] global_step=278400, grad_norm=3.9593234062194824, loss=3.2583484649658203
I0309 18:27:25.064856 139708407461632 logging_writer.py:48] [278500] global_step=278500, grad_norm=2.973737955093384, loss=1.2770339250564575
I0309 18:28:10.542216 139708415854336 logging_writer.py:48] [278600] global_step=278600, grad_norm=3.8598716259002686, loss=3.078329563140869
I0309 18:28:55.922839 139708407461632 logging_writer.py:48] [278700] global_step=278700, grad_norm=3.4022586345672607, loss=1.1292445659637451
I0309 18:29:41.049884 139708415854336 logging_writer.py:48] [278800] global_step=278800, grad_norm=3.885207414627075, loss=3.262197256088257
I0309 18:30:26.237867 139708407461632 logging_writer.py:48] [278900] global_step=278900, grad_norm=3.5454294681549072, loss=1.520198941230774
I0309 18:31:11.848953 139708415854336 logging_writer.py:48] [279000] global_step=279000, grad_norm=2.9947292804718018, loss=1.140541911125183
I0309 18:31:17.772272 139902746892096 spec.py:321] Evaluating on the training split.
I0309 18:31:29.271333 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 18:31:49.103857 139902746892096 spec.py:349] Evaluating on the test split.
I0309 18:31:50.778891 139902746892096 submission_runner.py:411] Time since start: 135033.81s, 	Step: 279015, 	{'train/accuracy': 0.88734370470047, 'train/loss': 0.40889811515808105, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 124818.47934174538, 'total_duration': 135033.80769109726, 'accumulated_submission_time': 124818.47934174538, 'accumulated_eval_time': 10184.596621990204, 'accumulated_logging_time': 16.07777214050293}
I0309 18:31:50.840731 139708407461632 logging_writer.py:48] [279015] accumulated_eval_time=10184.596622, accumulated_logging_time=16.077772, accumulated_submission_time=124818.479342, global_step=279015, preemption_count=0, score=124818.479342, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=135033.807691, train/accuracy=0.887344, train/loss=0.408898, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 18:32:24.690371 139708415854336 logging_writer.py:48] [279100] global_step=279100, grad_norm=3.1123135089874268, loss=1.139933466911316
I0309 18:33:09.407815 139708407461632 logging_writer.py:48] [279200] global_step=279200, grad_norm=2.9796481132507324, loss=2.10306453704834
I0309 18:33:54.792171 139708415854336 logging_writer.py:48] [279300] global_step=279300, grad_norm=3.06441330909729, loss=1.9928840398788452
I0309 18:34:40.093287 139708407461632 logging_writer.py:48] [279400] global_step=279400, grad_norm=3.40775990486145, loss=1.097508430480957
I0309 18:35:25.384830 139708415854336 logging_writer.py:48] [279500] global_step=279500, grad_norm=2.819690465927124, loss=1.5408543348312378
I0309 18:36:10.758114 139708407461632 logging_writer.py:48] [279600] global_step=279600, grad_norm=3.1378989219665527, loss=1.5017682313919067
I0309 18:36:55.978875 139708415854336 logging_writer.py:48] [279700] global_step=279700, grad_norm=3.3635611534118652, loss=1.10276460647583
I0309 18:37:41.275862 139708407461632 logging_writer.py:48] [279800] global_step=279800, grad_norm=3.206054210662842, loss=1.3430455923080444
I0309 18:38:26.478172 139708415854336 logging_writer.py:48] [279900] global_step=279900, grad_norm=3.568894147872925, loss=2.762629747390747
I0309 18:38:51.301759 139902746892096 spec.py:321] Evaluating on the training split.
I0309 18:39:02.694550 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 18:39:22.488490 139902746892096 spec.py:349] Evaluating on the test split.
I0309 18:39:24.170138 139902746892096 submission_runner.py:411] Time since start: 135487.20s, 	Step: 279957, 	{'train/accuracy': 0.890429675579071, 'train/loss': 0.40998199582099915, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 125238.88067746162, 'total_duration': 135487.19892835617, 'accumulated_submission_time': 125238.88067746162, 'accumulated_eval_time': 10217.464999198914, 'accumulated_logging_time': 16.149641752243042}
I0309 18:39:24.240804 139708407461632 logging_writer.py:48] [279957] accumulated_eval_time=10217.464999, accumulated_logging_time=16.149642, accumulated_submission_time=125238.880677, global_step=279957, preemption_count=0, score=125238.880677, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=135487.198928, train/accuracy=0.890430, train/loss=0.409982, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 18:39:41.576076 139708415854336 logging_writer.py:48] [280000] global_step=280000, grad_norm=3.1217703819274902, loss=1.051920771598816
I0309 18:40:24.513023 139708407461632 logging_writer.py:48] [280100] global_step=280100, grad_norm=3.001469612121582, loss=1.9156824350357056
I0309 18:41:09.359396 139708415854336 logging_writer.py:48] [280200] global_step=280200, grad_norm=3.8593227863311768, loss=3.184597969055176
I0309 18:41:54.909665 139708407461632 logging_writer.py:48] [280300] global_step=280300, grad_norm=3.0925192832946777, loss=1.2644219398498535
I0309 18:42:39.993919 139708415854336 logging_writer.py:48] [280400] global_step=280400, grad_norm=2.966813325881958, loss=1.3687243461608887
I0309 18:43:25.134731 139708407461632 logging_writer.py:48] [280500] global_step=280500, grad_norm=3.101470947265625, loss=2.7135467529296875
I0309 18:44:10.267902 139708415854336 logging_writer.py:48] [280600] global_step=280600, grad_norm=3.0923757553100586, loss=1.1744980812072754
I0309 18:44:55.491023 139708407461632 logging_writer.py:48] [280700] global_step=280700, grad_norm=2.896467447280884, loss=2.309035301208496
I0309 18:45:40.812081 139708415854336 logging_writer.py:48] [280800] global_step=280800, grad_norm=3.0862884521484375, loss=1.176326036453247
I0309 18:46:24.194695 139902746892096 spec.py:321] Evaluating on the training split.
I0309 18:46:36.279768 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 18:46:57.997913 139902746892096 spec.py:349] Evaluating on the test split.
I0309 18:46:59.674428 139902746892096 submission_runner.py:411] Time since start: 135942.70s, 	Step: 280898, 	{'train/accuracy': 0.8888476490974426, 'train/loss': 0.4090248644351959, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 125658.77555394173, 'total_duration': 135942.70322799683, 'accumulated_submission_time': 125658.77555394173, 'accumulated_eval_time': 10252.94473028183, 'accumulated_logging_time': 16.230097770690918}
I0309 18:46:59.730232 139708407461632 logging_writer.py:48] [280898] accumulated_eval_time=10252.944730, accumulated_logging_time=16.230098, accumulated_submission_time=125658.775554, global_step=280898, preemption_count=0, score=125658.775554, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=135942.703228, train/accuracy=0.888848, train/loss=0.409025, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 18:47:00.911361 139708415854336 logging_writer.py:48] [280900] global_step=280900, grad_norm=2.9960601329803467, loss=1.6539396047592163
I0309 18:47:40.718163 139708407461632 logging_writer.py:48] [281000] global_step=281000, grad_norm=3.2600767612457275, loss=2.7094030380249023
I0309 18:48:25.631427 139708415854336 logging_writer.py:48] [281100] global_step=281100, grad_norm=3.1454551219940186, loss=1.1550170183181763
I0309 18:49:10.840147 139708407461632 logging_writer.py:48] [281200] global_step=281200, grad_norm=2.8461334705352783, loss=1.2188987731933594
I0309 18:49:56.423367 139708415854336 logging_writer.py:48] [281300] global_step=281300, grad_norm=3.0544042587280273, loss=1.432354211807251
I0309 18:50:41.547295 139708407461632 logging_writer.py:48] [281400] global_step=281400, grad_norm=2.912452220916748, loss=1.3087526559829712
I0309 18:51:30.718805 139708415854336 logging_writer.py:48] [281500] global_step=281500, grad_norm=3.0488932132720947, loss=1.315711259841919
I0309 18:52:34.148853 139708407461632 logging_writer.py:48] [281600] global_step=281600, grad_norm=2.9612138271331787, loss=1.71188223361969
I0309 18:53:19.248721 139708415854336 logging_writer.py:48] [281700] global_step=281700, grad_norm=2.8801164627075195, loss=1.1532154083251953
I0309 18:53:59.749424 139902746892096 spec.py:321] Evaluating on the training split.
I0309 18:54:11.275076 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 18:54:34.993599 139902746892096 spec.py:349] Evaluating on the test split.
I0309 18:54:36.668828 139902746892096 submission_runner.py:411] Time since start: 136399.70s, 	Step: 281791, 	{'train/accuracy': 0.8855664134025574, 'train/loss': 0.41811031103134155, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 126078.73848581314, 'total_duration': 136399.6976134777, 'accumulated_submission_time': 126078.73848581314, 'accumulated_eval_time': 10289.864123106003, 'accumulated_logging_time': 16.29589319229126}
I0309 18:54:36.735597 139708407461632 logging_writer.py:48] [281791] accumulated_eval_time=10289.864123, accumulated_logging_time=16.295893, accumulated_submission_time=126078.738486, global_step=281791, preemption_count=0, score=126078.738486, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=136399.697613, train/accuracy=0.885566, train/loss=0.418110, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 18:54:40.673654 139708415854336 logging_writer.py:48] [281800] global_step=281800, grad_norm=3.6456823348999023, loss=1.0741294622421265
I0309 18:55:21.239495 139708407461632 logging_writer.py:48] [281900] global_step=281900, grad_norm=3.092311382293701, loss=1.6412384510040283
I0309 18:56:06.397074 139708415854336 logging_writer.py:48] [282000] global_step=282000, grad_norm=3.125051259994507, loss=1.9658794403076172
I0309 18:56:51.657189 139708407461632 logging_writer.py:48] [282100] global_step=282100, grad_norm=3.709198474884033, loss=3.099534273147583
I0309 18:57:37.033942 139708415854336 logging_writer.py:48] [282200] global_step=282200, grad_norm=3.1005008220672607, loss=1.1373859643936157
I0309 18:58:22.107170 139708407461632 logging_writer.py:48] [282300] global_step=282300, grad_norm=3.0347025394439697, loss=1.121110439300537
I0309 18:59:07.401109 139708415854336 logging_writer.py:48] [282400] global_step=282400, grad_norm=3.3826229572296143, loss=1.1628034114837646
I0309 18:59:52.472032 139708407461632 logging_writer.py:48] [282500] global_step=282500, grad_norm=3.194847583770752, loss=1.1776057481765747
I0309 19:00:37.706622 139708415854336 logging_writer.py:48] [282600] global_step=282600, grad_norm=2.8608968257904053, loss=1.9176894426345825
I0309 19:01:22.929793 139708407461632 logging_writer.py:48] [282700] global_step=282700, grad_norm=3.1470704078674316, loss=1.3886970281600952
I0309 19:01:37.006285 139902746892096 spec.py:321] Evaluating on the training split.
I0309 19:01:48.156742 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 19:02:10.939494 139902746892096 spec.py:349] Evaluating on the test split.
I0309 19:02:12.607160 139902746892096 submission_runner.py:411] Time since start: 136855.64s, 	Step: 282733, 	{'train/accuracy': 0.8891015648841858, 'train/loss': 0.4115650951862335, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 126498.94877243042, 'total_duration': 136855.63595891, 'accumulated_submission_time': 126498.94877243042, 'accumulated_eval_time': 10325.465045690536, 'accumulated_logging_time': 16.3734393119812}
I0309 19:02:12.664611 139708415854336 logging_writer.py:48] [282733] accumulated_eval_time=10325.465046, accumulated_logging_time=16.373439, accumulated_submission_time=126498.948772, global_step=282733, preemption_count=0, score=126498.948772, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=136855.635959, train/accuracy=0.889102, train/loss=0.411565, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 19:02:39.412485 139708407461632 logging_writer.py:48] [282800] global_step=282800, grad_norm=3.067523717880249, loss=2.315850019454956
I0309 19:03:23.239455 139708415854336 logging_writer.py:48] [282900] global_step=282900, grad_norm=3.0217294692993164, loss=2.3091249465942383
I0309 19:04:08.623812 139708407461632 logging_writer.py:48] [283000] global_step=283000, grad_norm=2.998042106628418, loss=1.3458172082901
I0309 19:04:53.999058 139708415854336 logging_writer.py:48] [283100] global_step=283100, grad_norm=2.796990156173706, loss=1.5571786165237427
I0309 19:05:39.063248 139708407461632 logging_writer.py:48] [283200] global_step=283200, grad_norm=3.975151300430298, loss=3.299940347671509
I0309 19:06:24.651498 139708415854336 logging_writer.py:48] [283300] global_step=283300, grad_norm=3.445507526397705, loss=1.1284159421920776
I0309 19:07:10.188240 139708407461632 logging_writer.py:48] [283400] global_step=283400, grad_norm=3.082784652709961, loss=1.3372567892074585
I0309 19:07:55.969737 139708415854336 logging_writer.py:48] [283500] global_step=283500, grad_norm=3.3711185455322266, loss=1.5557445287704468
I0309 19:08:41.654344 139708407461632 logging_writer.py:48] [283600] global_step=283600, grad_norm=3.6090242862701416, loss=1.1016077995300293
I0309 19:09:12.975450 139902746892096 spec.py:321] Evaluating on the training split.
I0309 19:09:24.336337 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 19:09:45.183720 139902746892096 spec.py:349] Evaluating on the test split.
I0309 19:09:46.868477 139902746892096 submission_runner.py:411] Time since start: 137309.90s, 	Step: 283671, 	{'train/accuracy': 0.8883788585662842, 'train/loss': 0.41344889998435974, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 126919.20204520226, 'total_duration': 137309.89725899696, 'accumulated_submission_time': 126919.20204520226, 'accumulated_eval_time': 10359.358072280884, 'accumulated_logging_time': 16.440247058868408}
I0309 19:09:46.925500 139708415854336 logging_writer.py:48] [283671] accumulated_eval_time=10359.358072, accumulated_logging_time=16.440247, accumulated_submission_time=126919.202045, global_step=283671, preemption_count=0, score=126919.202045, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=137309.897259, train/accuracy=0.888379, train/loss=0.413449, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 19:09:58.725725 139708407461632 logging_writer.py:48] [283700] global_step=283700, grad_norm=3.9041495323181152, loss=3.234818696975708
I0309 19:10:40.930766 139708415854336 logging_writer.py:48] [283800] global_step=283800, grad_norm=3.111131429672241, loss=1.711431622505188
I0309 19:11:26.195122 139708407461632 logging_writer.py:48] [283900] global_step=283900, grad_norm=3.191164016723633, loss=1.9917643070220947
I0309 19:12:11.613018 139708415854336 logging_writer.py:48] [284000] global_step=284000, grad_norm=3.2027838230133057, loss=1.0788655281066895
I0309 19:12:56.612802 139708407461632 logging_writer.py:48] [284100] global_step=284100, grad_norm=3.141991138458252, loss=1.1118332147598267
I0309 19:13:41.927264 139708415854336 logging_writer.py:48] [284200] global_step=284200, grad_norm=2.970853567123413, loss=1.7359907627105713
I0309 19:14:27.214990 139708407461632 logging_writer.py:48] [284300] global_step=284300, grad_norm=3.9693121910095215, loss=3.0500779151916504
I0309 19:15:12.345880 139708415854336 logging_writer.py:48] [284400] global_step=284400, grad_norm=2.83862566947937, loss=1.4637322425842285
I0309 19:15:57.602556 139708407461632 logging_writer.py:48] [284500] global_step=284500, grad_norm=3.354616165161133, loss=2.7257895469665527
I0309 19:16:42.796970 139708415854336 logging_writer.py:48] [284600] global_step=284600, grad_norm=2.8856563568115234, loss=1.000016450881958
I0309 19:16:47.065650 139902746892096 spec.py:321] Evaluating on the training split.
I0309 19:16:58.555277 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 19:17:21.639345 139902746892096 spec.py:349] Evaluating on the test split.
I0309 19:17:23.309018 139902746892096 submission_runner.py:411] Time since start: 137766.34s, 	Step: 284611, 	{'train/accuracy': 0.8886132836341858, 'train/loss': 0.41738325357437134, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 127339.28399133682, 'total_duration': 137766.33781647682, 'accumulated_submission_time': 127339.28399133682, 'accumulated_eval_time': 10395.601438760757, 'accumulated_logging_time': 16.506649494171143}
I0309 19:17:23.367100 139708407461632 logging_writer.py:48] [284611] accumulated_eval_time=10395.601439, accumulated_logging_time=16.506649, accumulated_submission_time=127339.283991, global_step=284611, preemption_count=0, score=127339.283991, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=137766.337816, train/accuracy=0.888613, train/loss=0.417383, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 19:17:58.763049 139708415854336 logging_writer.py:48] [284700] global_step=284700, grad_norm=3.113097667694092, loss=1.1059679985046387
I0309 19:18:43.589328 139708407461632 logging_writer.py:48] [284800] global_step=284800, grad_norm=2.974226951599121, loss=1.1180121898651123
I0309 19:19:28.957209 139708415854336 logging_writer.py:48] [284900] global_step=284900, grad_norm=3.0503458976745605, loss=1.1392358541488647
I0309 19:20:13.882783 139708407461632 logging_writer.py:48] [285000] global_step=285000, grad_norm=3.5176336765289307, loss=1.109902024269104
I0309 19:20:59.043969 139708415854336 logging_writer.py:48] [285100] global_step=285100, grad_norm=3.040416955947876, loss=1.130351185798645
I0309 19:21:44.333371 139708407461632 logging_writer.py:48] [285200] global_step=285200, grad_norm=2.950875759124756, loss=1.9136021137237549
I0309 19:22:29.895482 139708415854336 logging_writer.py:48] [285300] global_step=285300, grad_norm=3.3763484954833984, loss=2.2524101734161377
I0309 19:23:15.225418 139708407461632 logging_writer.py:48] [285400] global_step=285400, grad_norm=3.0731089115142822, loss=1.1618213653564453
I0309 19:24:00.688286 139708415854336 logging_writer.py:48] [285500] global_step=285500, grad_norm=2.9403510093688965, loss=1.0960519313812256
I0309 19:24:23.388207 139902746892096 spec.py:321] Evaluating on the training split.
I0309 19:24:34.671395 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 19:24:55.953016 139902746892096 spec.py:349] Evaluating on the test split.
I0309 19:24:57.638103 139902746892096 submission_runner.py:411] Time since start: 138220.67s, 	Step: 285552, 	{'train/accuracy': 0.88783198595047, 'train/loss': 0.41496291756629944, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 127759.24665808678, 'total_duration': 138220.6668958664, 'accumulated_submission_time': 127759.24665808678, 'accumulated_eval_time': 10429.8513276577, 'accumulated_logging_time': 16.57482123374939}
I0309 19:24:57.696218 139708407461632 logging_writer.py:48] [285552] accumulated_eval_time=10429.851328, accumulated_logging_time=16.574821, accumulated_submission_time=127759.246658, global_step=285552, preemption_count=0, score=127759.246658, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=138220.666896, train/accuracy=0.887832, train/loss=0.414963, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 19:25:16.974374 139708415854336 logging_writer.py:48] [285600] global_step=285600, grad_norm=3.099992036819458, loss=1.1575937271118164
I0309 19:25:59.818236 139708407461632 logging_writer.py:48] [285700] global_step=285700, grad_norm=3.0781524181365967, loss=1.0980353355407715
I0309 19:26:45.255190 139708415854336 logging_writer.py:48] [285800] global_step=285800, grad_norm=3.1023290157318115, loss=1.122723937034607
I0309 19:27:30.954607 139708407461632 logging_writer.py:48] [285900] global_step=285900, grad_norm=3.0657918453216553, loss=1.6712795495986938
I0309 19:28:16.111233 139708415854336 logging_writer.py:48] [286000] global_step=286000, grad_norm=3.2498669624328613, loss=1.1948636770248413
I0309 19:29:01.268842 139708407461632 logging_writer.py:48] [286100] global_step=286100, grad_norm=3.1490721702575684, loss=1.0988471508026123
I0309 19:29:46.482635 139708415854336 logging_writer.py:48] [286200] global_step=286200, grad_norm=3.6098668575286865, loss=3.0218260288238525
I0309 19:30:31.765916 139708407461632 logging_writer.py:48] [286300] global_step=286300, grad_norm=3.0488197803497314, loss=1.0997285842895508
I0309 19:31:16.996583 139708415854336 logging_writer.py:48] [286400] global_step=286400, grad_norm=3.1607437133789062, loss=1.4911978244781494
I0309 19:31:57.792613 139902746892096 spec.py:321] Evaluating on the training split.
I0309 19:32:09.175043 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 19:32:32.126907 139902746892096 spec.py:349] Evaluating on the test split.
I0309 19:32:33.795820 139902746892096 submission_runner.py:411] Time since start: 138676.82s, 	Step: 286492, 	{'train/accuracy': 0.8872656226158142, 'train/loss': 0.417816162109375, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 128179.28594827652, 'total_duration': 138676.82459497452, 'accumulated_submission_time': 128179.28594827652, 'accumulated_eval_time': 10465.854510307312, 'accumulated_logging_time': 16.641371726989746}
I0309 19:32:33.853832 139708407461632 logging_writer.py:48] [286492] accumulated_eval_time=10465.854510, accumulated_logging_time=16.641372, accumulated_submission_time=128179.285948, global_step=286492, preemption_count=0, score=128179.285948, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=138676.824595, train/accuracy=0.887266, train/loss=0.417816, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 19:32:37.642917 139708415854336 logging_writer.py:48] [286500] global_step=286500, grad_norm=2.9722070693969727, loss=1.0699108839035034
I0309 19:33:18.102112 139708407461632 logging_writer.py:48] [286600] global_step=286600, grad_norm=3.0714783668518066, loss=1.1514692306518555
I0309 19:34:03.067505 139708415854336 logging_writer.py:48] [286700] global_step=286700, grad_norm=3.1415393352508545, loss=2.058704376220703
I0309 19:34:48.547561 139708407461632 logging_writer.py:48] [286800] global_step=286800, grad_norm=3.191270112991333, loss=2.716526746749878
I0309 19:35:33.752748 139708415854336 logging_writer.py:48] [286900] global_step=286900, grad_norm=3.492406129837036, loss=3.074324607849121
I0309 19:36:18.910680 139708407461632 logging_writer.py:48] [287000] global_step=287000, grad_norm=3.3824620246887207, loss=1.4651901721954346
I0309 19:37:04.256933 139708415854336 logging_writer.py:48] [287100] global_step=287100, grad_norm=3.616698741912842, loss=3.1286425590515137
I0309 19:37:49.392654 139708407461632 logging_writer.py:48] [287200] global_step=287200, grad_norm=2.982231616973877, loss=1.0182794332504272
I0309 19:38:34.520772 139708415854336 logging_writer.py:48] [287300] global_step=287300, grad_norm=3.0722877979278564, loss=1.1601897478103638
I0309 19:39:19.833014 139708407461632 logging_writer.py:48] [287400] global_step=287400, grad_norm=3.2099554538726807, loss=2.9987239837646484
I0309 19:39:33.826312 139902746892096 spec.py:321] Evaluating on the training split.
I0309 19:39:45.545770 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 19:40:05.939436 139902746892096 spec.py:349] Evaluating on the test split.
I0309 19:40:07.620317 139902746892096 submission_runner.py:411] Time since start: 139130.65s, 	Step: 287433, 	{'train/accuracy': 0.88832026720047, 'train/loss': 0.41000136733055115, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 128598.95075941086, 'total_duration': 139130.64910507202, 'accumulated_submission_time': 128598.95075941086, 'accumulated_eval_time': 10499.648500919342, 'accumulated_logging_time': 16.957338094711304}
I0309 19:40:07.691549 139708415854336 logging_writer.py:48] [287433] accumulated_eval_time=10499.648501, accumulated_logging_time=16.957338, accumulated_submission_time=128598.950759, global_step=287433, preemption_count=0, score=128598.950759, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=139130.649105, train/accuracy=0.888320, train/loss=0.410001, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 19:40:34.475152 139708407461632 logging_writer.py:48] [287500] global_step=287500, grad_norm=3.0464189052581787, loss=2.2292113304138184
I0309 19:41:18.660616 139708415854336 logging_writer.py:48] [287600] global_step=287600, grad_norm=3.6518638134002686, loss=3.0785722732543945
I0309 19:42:03.978571 139708407461632 logging_writer.py:48] [287700] global_step=287700, grad_norm=3.093174934387207, loss=1.1613376140594482
I0309 19:42:50.052937 139708415854336 logging_writer.py:48] [287800] global_step=287800, grad_norm=3.06195068359375, loss=1.4313024282455444
I0309 19:43:35.391339 139708407461632 logging_writer.py:48] [287900] global_step=287900, grad_norm=3.2204720973968506, loss=1.4281210899353027
I0309 19:44:20.724142 139708415854336 logging_writer.py:48] [288000] global_step=288000, grad_norm=3.6671414375305176, loss=3.222820281982422
I0309 19:45:06.144422 139708407461632 logging_writer.py:48] [288100] global_step=288100, grad_norm=3.2752490043640137, loss=1.300114393234253
I0309 19:45:51.653981 139708415854336 logging_writer.py:48] [288200] global_step=288200, grad_norm=3.0887551307678223, loss=2.038177967071533
I0309 19:46:37.298872 139708407461632 logging_writer.py:48] [288300] global_step=288300, grad_norm=4.171693325042725, loss=3.0953922271728516
I0309 19:47:07.952974 139902746892096 spec.py:321] Evaluating on the training split.
I0309 19:47:19.457274 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 19:47:41.776627 139902746892096 spec.py:349] Evaluating on the test split.
I0309 19:47:43.444600 139902746892096 submission_runner.py:411] Time since start: 139586.47s, 	Step: 288369, 	{'train/accuracy': 0.8876171708106995, 'train/loss': 0.4147033095359802, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 129019.15416574478, 'total_duration': 139586.47339582443, 'accumulated_submission_time': 129019.15416574478, 'accumulated_eval_time': 10535.140134096146, 'accumulated_logging_time': 17.03790783882141}
I0309 19:47:43.503564 139708415854336 logging_writer.py:48] [288369] accumulated_eval_time=10535.140134, accumulated_logging_time=17.037908, accumulated_submission_time=129019.154166, global_step=288369, preemption_count=0, score=129019.154166, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=139586.473396, train/accuracy=0.887617, train/loss=0.414703, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 19:47:56.096139 139708407461632 logging_writer.py:48] [288400] global_step=288400, grad_norm=2.9231553077697754, loss=1.0713328123092651
I0309 19:48:37.738881 139708415854336 logging_writer.py:48] [288500] global_step=288500, grad_norm=3.061910629272461, loss=1.7960515022277832
I0309 19:49:23.292925 139708407461632 logging_writer.py:48] [288600] global_step=288600, grad_norm=3.5925285816192627, loss=1.063704490661621
I0309 19:50:08.896991 139708415854336 logging_writer.py:48] [288700] global_step=288700, grad_norm=2.9799447059631348, loss=2.2439794540405273
I0309 19:50:54.048318 139708407461632 logging_writer.py:48] [288800] global_step=288800, grad_norm=3.242687940597534, loss=1.154404878616333
I0309 19:51:39.501437 139708415854336 logging_writer.py:48] [288900] global_step=288900, grad_norm=3.0285677909851074, loss=1.2424253225326538
I0309 19:52:25.124458 139708407461632 logging_writer.py:48] [289000] global_step=289000, grad_norm=2.8148272037506104, loss=1.6140941381454468
I0309 19:53:10.447431 139708415854336 logging_writer.py:48] [289100] global_step=289100, grad_norm=3.4236433506011963, loss=1.439483642578125
I0309 19:53:55.906759 139708407461632 logging_writer.py:48] [289200] global_step=289200, grad_norm=2.942559003829956, loss=2.081249952316284
I0309 19:54:41.381381 139708415854336 logging_writer.py:48] [289300] global_step=289300, grad_norm=3.0005440711975098, loss=1.135141372680664
I0309 19:54:43.822474 139902746892096 spec.py:321] Evaluating on the training split.
I0309 19:54:55.354834 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 19:55:16.170023 139902746892096 spec.py:349] Evaluating on the test split.
I0309 19:55:17.855925 139902746892096 submission_runner.py:411] Time since start: 140040.88s, 	Step: 289307, 	{'train/accuracy': 0.887011706829071, 'train/loss': 0.42254722118377686, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 129439.41557979584, 'total_duration': 140040.8846886158, 'accumulated_submission_time': 129439.41557979584, 'accumulated_eval_time': 10569.173550605774, 'accumulated_logging_time': 17.106011629104614}
I0309 19:55:17.922694 139708407461632 logging_writer.py:48] [289307] accumulated_eval_time=10569.173551, accumulated_logging_time=17.106012, accumulated_submission_time=129439.415580, global_step=289307, preemption_count=0, score=129439.415580, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=140040.884689, train/accuracy=0.887012, train/loss=0.422547, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 19:55:55.313920 139708415854336 logging_writer.py:48] [289400] global_step=289400, grad_norm=2.9471840858459473, loss=1.253499984741211
I0309 19:56:40.698726 139708407461632 logging_writer.py:48] [289500] global_step=289500, grad_norm=2.9438319206237793, loss=1.3351178169250488
I0309 19:57:26.305902 139708415854336 logging_writer.py:48] [289600] global_step=289600, grad_norm=3.0014309883117676, loss=1.0180916786193848
I0309 19:58:11.985405 139708407461632 logging_writer.py:48] [289700] global_step=289700, grad_norm=3.9758803844451904, loss=2.413778305053711
I0309 19:58:56.993333 139708415854336 logging_writer.py:48] [289800] global_step=289800, grad_norm=3.154208183288574, loss=1.1567976474761963
I0309 19:59:42.493207 139708407461632 logging_writer.py:48] [289900] global_step=289900, grad_norm=3.0283164978027344, loss=2.130781650543213
I0309 20:00:27.550133 139708415854336 logging_writer.py:48] [290000] global_step=290000, grad_norm=3.144225835800171, loss=1.0054436922073364
I0309 20:01:12.650053 139708407461632 logging_writer.py:48] [290100] global_step=290100, grad_norm=3.474775552749634, loss=2.939882755279541
I0309 20:01:58.057319 139708415854336 logging_writer.py:48] [290200] global_step=290200, grad_norm=3.2289206981658936, loss=1.1700268983840942
I0309 20:02:18.026967 139902746892096 spec.py:321] Evaluating on the training split.
I0309 20:02:29.274278 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 20:02:50.129334 139902746892096 spec.py:349] Evaluating on the test split.
I0309 20:02:51.796993 139902746892096 submission_runner.py:411] Time since start: 140494.83s, 	Step: 290246, 	{'train/accuracy': 0.8878515362739563, 'train/loss': 0.4139612913131714, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 129859.462069273, 'total_duration': 140494.82578778267, 'accumulated_submission_time': 129859.462069273, 'accumulated_eval_time': 10602.943563699722, 'accumulated_logging_time': 17.181469678878784}
I0309 20:02:51.863389 139708407461632 logging_writer.py:48] [290246] accumulated_eval_time=10602.943564, accumulated_logging_time=17.181470, accumulated_submission_time=129859.462069, global_step=290246, preemption_count=0, score=129859.462069, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=140494.825788, train/accuracy=0.887852, train/loss=0.413961, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 20:03:13.736802 139708415854336 logging_writer.py:48] [290300] global_step=290300, grad_norm=3.3168141841888428, loss=2.765873432159424
I0309 20:03:56.625313 139708407461632 logging_writer.py:48] [290400] global_step=290400, grad_norm=3.0315678119659424, loss=1.1420291662216187
I0309 20:04:42.156797 139708415854336 logging_writer.py:48] [290500] global_step=290500, grad_norm=2.985562562942505, loss=1.0644046068191528
I0309 20:05:27.861707 139708407461632 logging_writer.py:48] [290600] global_step=290600, grad_norm=3.8930106163024902, loss=3.180058002471924
I0309 20:06:13.043759 139708415854336 logging_writer.py:48] [290700] global_step=290700, grad_norm=3.0814969539642334, loss=1.9983582496643066
I0309 20:06:58.461653 139708407461632 logging_writer.py:48] [290800] global_step=290800, grad_norm=3.0384528636932373, loss=2.3269126415252686
I0309 20:07:44.056921 139708415854336 logging_writer.py:48] [290900] global_step=290900, grad_norm=2.9466919898986816, loss=1.3717989921569824
I0309 20:08:29.540047 139708407461632 logging_writer.py:48] [291000] global_step=291000, grad_norm=3.465123414993286, loss=3.069736957550049
I0309 20:09:14.711890 139708415854336 logging_writer.py:48] [291100] global_step=291100, grad_norm=3.300222873687744, loss=1.1844090223312378
I0309 20:09:52.013428 139902746892096 spec.py:321] Evaluating on the training split.
I0309 20:10:03.292415 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 20:10:23.583232 139902746892096 spec.py:349] Evaluating on the test split.
I0309 20:10:25.259056 139902746892096 submission_runner.py:411] Time since start: 140948.29s, 	Step: 291184, 	{'train/accuracy': 0.8871288895606995, 'train/loss': 0.41430744528770447, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 130279.55357980728, 'total_duration': 140948.2878472805, 'accumulated_submission_time': 130279.55357980728, 'accumulated_eval_time': 10636.189188480377, 'accumulated_logging_time': 17.256900310516357}
I0309 20:10:25.330648 139708407461632 logging_writer.py:48] [291184] accumulated_eval_time=10636.189188, accumulated_logging_time=17.256900, accumulated_submission_time=130279.553580, global_step=291184, preemption_count=0, score=130279.553580, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=140948.287847, train/accuracy=0.887129, train/loss=0.414307, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 20:10:32.049223 139708415854336 logging_writer.py:48] [291200] global_step=291200, grad_norm=3.0092530250549316, loss=1.3955414295196533
I0309 20:11:13.356645 139708407461632 logging_writer.py:48] [291300] global_step=291300, grad_norm=3.2471022605895996, loss=2.7125844955444336
I0309 20:11:58.656892 139708415854336 logging_writer.py:48] [291400] global_step=291400, grad_norm=2.8563168048858643, loss=1.241490364074707
I0309 20:12:44.368996 139708407461632 logging_writer.py:48] [291500] global_step=291500, grad_norm=2.774885654449463, loss=1.8103853464126587
I0309 20:13:29.752628 139708415854336 logging_writer.py:48] [291600] global_step=291600, grad_norm=3.238521099090576, loss=1.208344578742981
I0309 20:14:14.966446 139708407461632 logging_writer.py:48] [291700] global_step=291700, grad_norm=3.0424225330352783, loss=1.9091771841049194
I0309 20:15:00.612915 139708415854336 logging_writer.py:48] [291800] global_step=291800, grad_norm=4.234744548797607, loss=3.1938390731811523
I0309 20:15:45.997789 139708407461632 logging_writer.py:48] [291900] global_step=291900, grad_norm=2.9129018783569336, loss=1.1990101337432861
I0309 20:16:31.599964 139708415854336 logging_writer.py:48] [292000] global_step=292000, grad_norm=2.720954656600952, loss=1.464017629623413
I0309 20:17:16.838596 139708407461632 logging_writer.py:48] [292100] global_step=292100, grad_norm=3.024390697479248, loss=1.1354007720947266
I0309 20:17:25.548618 139902746892096 spec.py:321] Evaluating on the training split.
I0309 20:17:37.225386 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 20:17:59.239884 139902746892096 spec.py:349] Evaluating on the test split.
I0309 20:18:00.913474 139902746892096 submission_runner.py:411] Time since start: 141403.94s, 	Step: 292121, 	{'train/accuracy': 0.8910741806030273, 'train/loss': 0.4123877286911011, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 130699.70896553993, 'total_duration': 141403.94225287437, 'accumulated_submission_time': 130699.70896553993, 'accumulated_eval_time': 10671.554021835327, 'accumulated_logging_time': 17.342145442962646}
I0309 20:18:00.981710 139708415854336 logging_writer.py:48] [292121] accumulated_eval_time=10671.554022, accumulated_logging_time=17.342145, accumulated_submission_time=130699.708966, global_step=292121, preemption_count=0, score=130699.708966, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=141403.942253, train/accuracy=0.891074, train/loss=0.412388, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 20:18:32.446291 139708407461632 logging_writer.py:48] [292200] global_step=292200, grad_norm=3.40226149559021, loss=1.1928404569625854
I0309 20:19:16.986135 139708415854336 logging_writer.py:48] [292300] global_step=292300, grad_norm=3.2516489028930664, loss=2.385655403137207
I0309 20:20:02.382994 139708407461632 logging_writer.py:48] [292400] global_step=292400, grad_norm=3.25816011428833, loss=1.1574305295944214
I0309 20:20:47.729697 139708415854336 logging_writer.py:48] [292500] global_step=292500, grad_norm=3.4608700275421143, loss=1.0375542640686035
I0309 20:21:32.927746 139708407461632 logging_writer.py:48] [292600] global_step=292600, grad_norm=3.0770928859710693, loss=2.0953850746154785
I0309 20:22:18.150482 139708415854336 logging_writer.py:48] [292700] global_step=292700, grad_norm=3.6681532859802246, loss=1.1157519817352295
I0309 20:23:03.885488 139708407461632 logging_writer.py:48] [292800] global_step=292800, grad_norm=4.0287017822265625, loss=3.2808501720428467
I0309 20:23:49.097054 139708415854336 logging_writer.py:48] [292900] global_step=292900, grad_norm=3.53916597366333, loss=1.0798778533935547
I0309 20:24:34.537604 139708407461632 logging_writer.py:48] [293000] global_step=293000, grad_norm=3.1021945476531982, loss=1.2950023412704468
I0309 20:25:01.061355 139902746892096 spec.py:321] Evaluating on the training split.
I0309 20:25:12.345170 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 20:25:35.029512 139902746892096 spec.py:349] Evaluating on the test split.
I0309 20:25:36.698156 139902746892096 submission_runner.py:411] Time since start: 141859.73s, 	Step: 293060, 	{'train/accuracy': 0.8864648342132568, 'train/loss': 0.422519326210022, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 131119.73110198975, 'total_duration': 141859.72695946693, 'accumulated_submission_time': 131119.73110198975, 'accumulated_eval_time': 10707.190835475922, 'accumulated_logging_time': 17.419295072555542}
I0309 20:25:36.758068 139708415854336 logging_writer.py:48] [293060] accumulated_eval_time=10707.190835, accumulated_logging_time=17.419295, accumulated_submission_time=131119.731102, global_step=293060, preemption_count=0, score=131119.731102, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=141859.726959, train/accuracy=0.886465, train/loss=0.422519, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 20:25:52.890748 139708407461632 logging_writer.py:48] [293100] global_step=293100, grad_norm=3.015483856201172, loss=1.6068769693374634
I0309 20:26:34.751704 139708415854336 logging_writer.py:48] [293200] global_step=293200, grad_norm=3.1799516677856445, loss=1.2108967304229736
I0309 20:27:20.574322 139708407461632 logging_writer.py:48] [293300] global_step=293300, grad_norm=3.149613618850708, loss=1.0871411561965942
I0309 20:28:06.457870 139708415854336 logging_writer.py:48] [293400] global_step=293400, grad_norm=4.147152900695801, loss=3.289767026901245
I0309 20:28:51.838346 139708407461632 logging_writer.py:48] [293500] global_step=293500, grad_norm=3.380932092666626, loss=2.9306726455688477
I0309 20:29:36.910432 139708415854336 logging_writer.py:48] [293600] global_step=293600, grad_norm=3.0290825366973877, loss=1.4630860090255737
I0309 20:30:22.412232 139708407461632 logging_writer.py:48] [293700] global_step=293700, grad_norm=3.1210525035858154, loss=2.6964540481567383
I0309 20:31:07.834170 139708415854336 logging_writer.py:48] [293800] global_step=293800, grad_norm=3.0290913581848145, loss=1.7849950790405273
I0309 20:31:53.130454 139708407461632 logging_writer.py:48] [293900] global_step=293900, grad_norm=2.691802501678467, loss=1.8217930793762207
I0309 20:32:36.920241 139902746892096 spec.py:321] Evaluating on the training split.
I0309 20:32:48.202385 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 20:33:11.105112 139902746892096 spec.py:349] Evaluating on the test split.
I0309 20:33:12.780608 139902746892096 submission_runner.py:411] Time since start: 142315.81s, 	Step: 293998, 	{'train/accuracy': 0.8893359303474426, 'train/loss': 0.4128582775592804, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 131539.8347415924, 'total_duration': 142315.80940794945, 'accumulated_submission_time': 131539.8347415924, 'accumulated_eval_time': 10743.051213026047, 'accumulated_logging_time': 17.48875403404236}
I0309 20:33:12.838939 139708415854336 logging_writer.py:48] [293998] accumulated_eval_time=10743.051213, accumulated_logging_time=17.488754, accumulated_submission_time=131539.834742, global_step=293998, preemption_count=0, score=131539.834742, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=142315.809408, train/accuracy=0.889336, train/loss=0.412858, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 20:33:14.026241 139708407461632 logging_writer.py:48] [294000] global_step=294000, grad_norm=3.0817158222198486, loss=2.370306968688965
I0309 20:33:54.131863 139708415854336 logging_writer.py:48] [294100] global_step=294100, grad_norm=3.031766176223755, loss=1.2954847812652588
I0309 20:34:39.458688 139708407461632 logging_writer.py:48] [294200] global_step=294200, grad_norm=3.111970901489258, loss=1.1794613599777222
I0309 20:35:25.384788 139708415854336 logging_writer.py:48] [294300] global_step=294300, grad_norm=3.322589159011841, loss=2.826022148132324
I0309 20:36:10.894697 139708407461632 logging_writer.py:48] [294400] global_step=294400, grad_norm=2.915379047393799, loss=2.3438491821289062
I0309 20:36:56.306828 139708415854336 logging_writer.py:48] [294500] global_step=294500, grad_norm=2.962409496307373, loss=1.1394332647323608
I0309 20:37:42.214424 139708407461632 logging_writer.py:48] [294600] global_step=294600, grad_norm=2.9135494232177734, loss=1.0850460529327393
I0309 20:38:27.485219 139708415854336 logging_writer.py:48] [294700] global_step=294700, grad_norm=3.1407313346862793, loss=1.0858190059661865
I0309 20:39:13.236137 139708407461632 logging_writer.py:48] [294800] global_step=294800, grad_norm=3.049798011779785, loss=2.1710169315338135
I0309 20:39:58.776304 139708415854336 logging_writer.py:48] [294900] global_step=294900, grad_norm=3.256180763244629, loss=1.0691051483154297
I0309 20:40:12.882512 139902746892096 spec.py:321] Evaluating on the training split.
I0309 20:40:24.394436 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 20:40:46.562350 139902746892096 spec.py:349] Evaluating on the test split.
I0309 20:40:48.249717 139902746892096 submission_runner.py:411] Time since start: 142771.28s, 	Step: 294933, 	{'train/accuracy': 0.8885351419448853, 'train/loss': 0.41563349962234497, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 131959.82090997696, 'total_duration': 142771.27851748466, 'accumulated_submission_time': 131959.82090997696, 'accumulated_eval_time': 10778.418419837952, 'accumulated_logging_time': 17.556360244750977}
I0309 20:40:48.309072 139708407461632 logging_writer.py:48] [294933] accumulated_eval_time=10778.418420, accumulated_logging_time=17.556360, accumulated_submission_time=131959.820910, global_step=294933, preemption_count=0, score=131959.820910, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=142771.278517, train/accuracy=0.888535, train/loss=0.415633, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 20:41:15.057497 139708415854336 logging_writer.py:48] [295000] global_step=295000, grad_norm=3.243414878845215, loss=1.195875644683838
I0309 20:41:58.926235 139708407461632 logging_writer.py:48] [295100] global_step=295100, grad_norm=2.8939144611358643, loss=1.2471892833709717
I0309 20:42:44.329741 139708415854336 logging_writer.py:48] [295200] global_step=295200, grad_norm=3.4329679012298584, loss=1.0967590808868408
I0309 20:43:30.022075 139708407461632 logging_writer.py:48] [295300] global_step=295300, grad_norm=3.216092586517334, loss=1.2079755067825317
I0309 20:44:15.110674 139708415854336 logging_writer.py:48] [295400] global_step=295400, grad_norm=3.141446590423584, loss=2.4051711559295654
I0309 20:45:00.373159 139708407461632 logging_writer.py:48] [295500] global_step=295500, grad_norm=2.8272593021392822, loss=1.2345755100250244
I0309 20:45:45.691676 139708415854336 logging_writer.py:48] [295600] global_step=295600, grad_norm=2.989593267440796, loss=1.342085838317871
I0309 20:46:30.753522 139708407461632 logging_writer.py:48] [295700] global_step=295700, grad_norm=3.4152169227600098, loss=1.0863362550735474
I0309 20:47:16.136342 139708415854336 logging_writer.py:48] [295800] global_step=295800, grad_norm=3.2228991985321045, loss=1.6740084886550903
I0309 20:47:48.529510 139902746892096 spec.py:321] Evaluating on the training split.
I0309 20:48:00.074242 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 20:48:20.641155 139902746892096 spec.py:349] Evaluating on the test split.
I0309 20:48:22.329993 139902746892096 submission_runner.py:411] Time since start: 143225.36s, 	Step: 295873, 	{'train/accuracy': 0.8855273127555847, 'train/loss': 0.4239533543586731, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 132379.98327803612, 'total_duration': 143225.35876965523, 'accumulated_submission_time': 132379.98327803612, 'accumulated_eval_time': 10812.218878507614, 'accumulated_logging_time': 17.62444758415222}
I0309 20:48:22.402902 139708407461632 logging_writer.py:48] [295873] accumulated_eval_time=10812.218879, accumulated_logging_time=17.624448, accumulated_submission_time=132379.983278, global_step=295873, preemption_count=0, score=132379.983278, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=143225.358770, train/accuracy=0.885527, train/loss=0.423953, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 20:48:33.430504 139708415854336 logging_writer.py:48] [295900] global_step=295900, grad_norm=4.071808815002441, loss=3.272869348526001
I0309 20:49:15.510710 139708407461632 logging_writer.py:48] [296000] global_step=296000, grad_norm=3.158571481704712, loss=2.7891886234283447
I0309 20:50:00.787846 139708415854336 logging_writer.py:48] [296100] global_step=296100, grad_norm=3.26709246635437, loss=2.4361164569854736
I0309 20:50:46.427358 139708407461632 logging_writer.py:48] [296200] global_step=296200, grad_norm=3.148777484893799, loss=1.1242396831512451
I0309 20:51:31.816957 139708415854336 logging_writer.py:48] [296300] global_step=296300, grad_norm=3.9483132362365723, loss=3.1187734603881836
I0309 20:52:17.169901 139708407461632 logging_writer.py:48] [296400] global_step=296400, grad_norm=3.0808498859405518, loss=1.4622671604156494
I0309 20:53:02.652092 139708415854336 logging_writer.py:48] [296500] global_step=296500, grad_norm=3.1102592945098877, loss=2.092520236968994
I0309 20:53:48.646185 139708407461632 logging_writer.py:48] [296600] global_step=296600, grad_norm=3.068655490875244, loss=2.4201111793518066
I0309 20:54:33.929957 139708415854336 logging_writer.py:48] [296700] global_step=296700, grad_norm=3.254072904586792, loss=1.2829164266586304
I0309 20:55:19.315430 139708407461632 logging_writer.py:48] [296800] global_step=296800, grad_norm=3.0912845134735107, loss=1.3949494361877441
I0309 20:55:22.738324 139902746892096 spec.py:321] Evaluating on the training split.
I0309 20:55:34.284726 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 20:55:56.988876 139902746892096 spec.py:349] Evaluating on the test split.
I0309 20:55:58.657689 139902746892096 submission_runner.py:411] Time since start: 143681.69s, 	Step: 296809, 	{'train/accuracy': 0.8883007764816284, 'train/loss': 0.4127146899700165, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 132800.25978302956, 'total_duration': 143681.6864824295, 'accumulated_submission_time': 132800.25978302956, 'accumulated_eval_time': 10848.138242006302, 'accumulated_logging_time': 17.70807909965515}
I0309 20:55:58.717983 139708415854336 logging_writer.py:48] [296809] accumulated_eval_time=10848.138242, accumulated_logging_time=17.708079, accumulated_submission_time=132800.259783, global_step=296809, preemption_count=0, score=132800.259783, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=143681.686482, train/accuracy=0.888301, train/loss=0.412715, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 20:56:35.078611 139708407461632 logging_writer.py:48] [296900] global_step=296900, grad_norm=3.14271879196167, loss=1.0797452926635742
I0309 20:57:20.161830 139708415854336 logging_writer.py:48] [297000] global_step=297000, grad_norm=3.284001588821411, loss=2.8427224159240723
I0309 20:58:05.672580 139708407461632 logging_writer.py:48] [297100] global_step=297100, grad_norm=2.995039701461792, loss=1.164513349533081
I0309 20:58:50.929963 139708415854336 logging_writer.py:48] [297200] global_step=297200, grad_norm=3.0829899311065674, loss=2.290073871612549
I0309 20:59:36.076737 139708407461632 logging_writer.py:48] [297300] global_step=297300, grad_norm=3.028425693511963, loss=1.541504144668579
I0309 21:00:21.562402 139708415854336 logging_writer.py:48] [297400] global_step=297400, grad_norm=3.176342725753784, loss=1.194345474243164
I0309 21:01:06.522751 139708407461632 logging_writer.py:48] [297500] global_step=297500, grad_norm=3.2928662300109863, loss=1.3534188270568848
I0309 21:01:52.071737 139708415854336 logging_writer.py:48] [297600] global_step=297600, grad_norm=3.1456782817840576, loss=1.1642374992370605
I0309 21:02:37.390580 139708407461632 logging_writer.py:48] [297700] global_step=297700, grad_norm=2.9296181201934814, loss=1.936826229095459
I0309 21:02:58.834669 139902746892096 spec.py:321] Evaluating on the training split.
I0309 21:03:09.941758 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 21:03:31.695619 139902746892096 spec.py:349] Evaluating on the test split.
I0309 21:03:33.377282 139902746892096 submission_runner.py:411] Time since start: 144136.41s, 	Step: 297749, 	{'train/accuracy': 0.8908202648162842, 'train/loss': 0.405164510011673, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 133220.3181180954, 'total_duration': 144136.4060664177, 'accumulated_submission_time': 133220.3181180954, 'accumulated_eval_time': 10882.680842399597, 'accumulated_logging_time': 17.776843547821045}
I0309 21:03:33.445918 139708415854336 logging_writer.py:48] [297749] accumulated_eval_time=10882.680842, accumulated_logging_time=17.776844, accumulated_submission_time=133220.318118, global_step=297749, preemption_count=0, score=133220.318118, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=144136.406066, train/accuracy=0.890820, train/loss=0.405165, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 21:03:53.920299 139708407461632 logging_writer.py:48] [297800] global_step=297800, grad_norm=3.21203875541687, loss=1.2075564861297607
I0309 21:04:36.812295 139708415854336 logging_writer.py:48] [297900] global_step=297900, grad_norm=2.987710475921631, loss=2.1490046977996826
I0309 21:05:22.119396 139708407461632 logging_writer.py:48] [298000] global_step=298000, grad_norm=3.0583322048187256, loss=1.1402089595794678
I0309 21:06:07.205366 139708415854336 logging_writer.py:48] [298100] global_step=298100, grad_norm=3.069854497909546, loss=1.7467586994171143
I0309 21:06:52.172864 139708407461632 logging_writer.py:48] [298200] global_step=298200, grad_norm=3.0476033687591553, loss=1.1155773401260376
I0309 21:07:37.685725 139708415854336 logging_writer.py:48] [298300] global_step=298300, grad_norm=2.898719549179077, loss=1.152319312095642
I0309 21:08:22.782268 139708407461632 logging_writer.py:48] [298400] global_step=298400, grad_norm=3.4998056888580322, loss=1.3404972553253174
I0309 21:09:07.905767 139708415854336 logging_writer.py:48] [298500] global_step=298500, grad_norm=3.2049288749694824, loss=2.721177339553833
I0309 21:09:53.246214 139708407461632 logging_writer.py:48] [298600] global_step=298600, grad_norm=2.840688705444336, loss=2.0568437576293945
I0309 21:10:33.569227 139902746892096 spec.py:321] Evaluating on the training split.
I0309 21:10:45.075191 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 21:11:05.568530 139902746892096 spec.py:349] Evaluating on the test split.
I0309 21:11:07.251239 139902746892096 submission_runner.py:411] Time since start: 144590.28s, 	Step: 298691, 	{'train/accuracy': 0.8869335651397705, 'train/loss': 0.4187902808189392, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 133640.38061594963, 'total_duration': 144590.28002810478, 'accumulated_submission_time': 133640.38061594963, 'accumulated_eval_time': 10916.362850904465, 'accumulated_logging_time': 17.857324361801147}
I0309 21:11:07.326354 139708415854336 logging_writer.py:48] [298691] accumulated_eval_time=10916.362851, accumulated_logging_time=17.857324, accumulated_submission_time=133640.380616, global_step=298691, preemption_count=0, score=133640.380616, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=144590.280028, train/accuracy=0.886934, train/loss=0.418790, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 21:11:11.262049 139708407461632 logging_writer.py:48] [298700] global_step=298700, grad_norm=2.998033046722412, loss=1.8603847026824951
I0309 21:11:52.189249 139708415854336 logging_writer.py:48] [298800] global_step=298800, grad_norm=3.077603340148926, loss=1.061367154121399
I0309 21:12:37.227984 139708407461632 logging_writer.py:48] [298900] global_step=298900, grad_norm=3.1361289024353027, loss=1.149179220199585
I0309 21:13:22.321992 139708415854336 logging_writer.py:48] [299000] global_step=299000, grad_norm=4.156148910522461, loss=3.137498378753662
I0309 21:14:08.001097 139708407461632 logging_writer.py:48] [299100] global_step=299100, grad_norm=3.6939187049865723, loss=3.039240598678589
I0309 21:14:53.363264 139708415854336 logging_writer.py:48] [299200] global_step=299200, grad_norm=2.9002015590667725, loss=1.0472313165664673
I0309 21:15:38.839290 139708407461632 logging_writer.py:48] [299300] global_step=299300, grad_norm=3.640068292617798, loss=3.165632963180542
I0309 21:16:23.987452 139708415854336 logging_writer.py:48] [299400] global_step=299400, grad_norm=3.9807522296905518, loss=3.1941170692443848
I0309 21:17:09.119652 139708407461632 logging_writer.py:48] [299500] global_step=299500, grad_norm=3.149458169937134, loss=1.2228808403015137
I0309 21:17:54.466225 139708415854336 logging_writer.py:48] [299600] global_step=299600, grad_norm=3.193965196609497, loss=1.9104055166244507
I0309 21:18:07.376338 139902746892096 spec.py:321] Evaluating on the training split.
I0309 21:18:19.117280 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 21:18:39.242846 139902746892096 spec.py:349] Evaluating on the test split.
I0309 21:18:40.926269 139902746892096 submission_runner.py:411] Time since start: 145043.96s, 	Step: 299630, 	{'train/accuracy': 0.8880468606948853, 'train/loss': 0.4154566526412964, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 134060.37126111984, 'total_duration': 145043.9550564289, 'accumulated_submission_time': 134060.37126111984, 'accumulated_eval_time': 10949.912775278091, 'accumulated_logging_time': 17.94206404685974}
I0309 21:18:40.999830 139708407461632 logging_writer.py:48] [299630] accumulated_eval_time=10949.912775, accumulated_logging_time=17.942064, accumulated_submission_time=134060.371261, global_step=299630, preemption_count=0, score=134060.371261, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=145043.955056, train/accuracy=0.888047, train/loss=0.415457, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 21:19:08.934474 139708415854336 logging_writer.py:48] [299700] global_step=299700, grad_norm=3.2737581729888916, loss=1.2032499313354492
I0309 21:19:52.825861 139708407461632 logging_writer.py:48] [299800] global_step=299800, grad_norm=3.0911240577697754, loss=1.0637166500091553
I0309 21:20:37.935749 139708415854336 logging_writer.py:48] [299900] global_step=299900, grad_norm=3.180307626724243, loss=1.8856470584869385
I0309 21:21:23.321763 139708407461632 logging_writer.py:48] [300000] global_step=300000, grad_norm=2.9526965618133545, loss=1.2556042671203613
I0309 21:22:08.334999 139708415854336 logging_writer.py:48] [300100] global_step=300100, grad_norm=3.556124210357666, loss=2.199211597442627
I0309 21:22:53.396858 139708407461632 logging_writer.py:48] [300200] global_step=300200, grad_norm=3.6961076259613037, loss=3.104130744934082
I0309 21:23:38.930742 139708415854336 logging_writer.py:48] [300300] global_step=300300, grad_norm=2.8732364177703857, loss=1.3431215286254883
I0309 21:24:23.892007 139708407461632 logging_writer.py:48] [300400] global_step=300400, grad_norm=3.1291708946228027, loss=2.542614698410034
I0309 21:25:09.248984 139708415854336 logging_writer.py:48] [300500] global_step=300500, grad_norm=3.1247000694274902, loss=1.2449783086776733
I0309 21:25:40.983024 139902746892096 spec.py:321] Evaluating on the training split.
I0309 21:25:52.368541 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 21:26:13.438868 139902746892096 spec.py:349] Evaluating on the test split.
I0309 21:26:15.122732 139902746892096 submission_runner.py:411] Time since start: 145498.15s, 	Step: 300572, 	{'train/accuracy': 0.8863085508346558, 'train/loss': 0.4205401539802551, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 134480.29626059532, 'total_duration': 145498.15152049065, 'accumulated_submission_time': 134480.29626059532, 'accumulated_eval_time': 10984.052475690842, 'accumulated_logging_time': 18.025188207626343}
I0309 21:26:15.185218 139708407461632 logging_writer.py:48] [300572] accumulated_eval_time=10984.052476, accumulated_logging_time=18.025188, accumulated_submission_time=134480.296261, global_step=300572, preemption_count=0, score=134480.296261, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=145498.151520, train/accuracy=0.886309, train/loss=0.420540, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 21:26:26.595261 139708415854336 logging_writer.py:48] [300600] global_step=300600, grad_norm=3.2914555072784424, loss=2.9682517051696777
I0309 21:27:08.315364 139708407461632 logging_writer.py:48] [300700] global_step=300700, grad_norm=3.1002962589263916, loss=1.1192306280136108
I0309 21:27:53.813510 139708415854336 logging_writer.py:48] [300800] global_step=300800, grad_norm=2.9130947589874268, loss=1.1803021430969238
I0309 21:28:39.807823 139708407461632 logging_writer.py:48] [300900] global_step=300900, grad_norm=3.1171422004699707, loss=1.1779934167861938
I0309 21:29:25.167565 139708415854336 logging_writer.py:48] [301000] global_step=301000, grad_norm=3.0005695819854736, loss=1.2545949220657349
I0309 21:30:10.462607 139708407461632 logging_writer.py:48] [301100] global_step=301100, grad_norm=2.9737939834594727, loss=1.0966914892196655
I0309 21:30:55.668198 139708415854336 logging_writer.py:48] [301200] global_step=301200, grad_norm=3.4539012908935547, loss=1.091697335243225
I0309 21:31:40.962321 139708407461632 logging_writer.py:48] [301300] global_step=301300, grad_norm=4.02894926071167, loss=3.1608996391296387
I0309 21:32:26.499649 139708415854336 logging_writer.py:48] [301400] global_step=301400, grad_norm=3.3346304893493652, loss=1.3262771368026733
I0309 21:33:11.669254 139708407461632 logging_writer.py:48] [301500] global_step=301500, grad_norm=3.0565876960754395, loss=1.8277838230133057
I0309 21:33:15.216086 139902746892096 spec.py:321] Evaluating on the training split.
I0309 21:33:26.517873 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 21:33:48.071659 139902746892096 spec.py:349] Evaluating on the test split.
I0309 21:33:49.746073 139902746892096 submission_runner.py:411] Time since start: 145952.77s, 	Step: 301509, 	{'train/accuracy': 0.8903906345367432, 'train/loss': 0.4110255241394043, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 134900.26795220375, 'total_duration': 145952.77487421036, 'accumulated_submission_time': 134900.26795220375, 'accumulated_eval_time': 11018.582464933395, 'accumulated_logging_time': 18.097609281539917}
I0309 21:33:49.805115 139708415854336 logging_writer.py:48] [301509] accumulated_eval_time=11018.582465, accumulated_logging_time=18.097609, accumulated_submission_time=134900.267952, global_step=301509, preemption_count=0, score=134900.267952, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=145952.774874, train/accuracy=0.890391, train/loss=0.411026, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 21:34:26.002393 139708407461632 logging_writer.py:48] [301600] global_step=301600, grad_norm=2.929861068725586, loss=1.2805347442626953
I0309 21:35:10.795264 139708415854336 logging_writer.py:48] [301700] global_step=301700, grad_norm=3.1768300533294678, loss=1.0614053010940552
I0309 21:35:56.278762 139708407461632 logging_writer.py:48] [301800] global_step=301800, grad_norm=2.8936879634857178, loss=1.1079506874084473
I0309 21:36:41.472687 139708415854336 logging_writer.py:48] [301900] global_step=301900, grad_norm=3.4341132640838623, loss=1.3512907028198242
I0309 21:37:26.697006 139708407461632 logging_writer.py:48] [302000] global_step=302000, grad_norm=2.8729262351989746, loss=1.885530710220337
I0309 21:38:12.230344 139708415854336 logging_writer.py:48] [302100] global_step=302100, grad_norm=3.2843520641326904, loss=1.9576687812805176
I0309 21:38:57.125876 139708407461632 logging_writer.py:48] [302200] global_step=302200, grad_norm=3.4569737911224365, loss=2.6316447257995605
I0309 21:39:42.791012 139708415854336 logging_writer.py:48] [302300] global_step=302300, grad_norm=2.923301935195923, loss=1.0800888538360596
I0309 21:40:27.796593 139708407461632 logging_writer.py:48] [302400] global_step=302400, grad_norm=3.0445854663848877, loss=1.538258671760559
I0309 21:40:49.971625 139902746892096 spec.py:321] Evaluating on the training split.
I0309 21:41:01.195664 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 21:41:25.298088 139902746892096 spec.py:349] Evaluating on the test split.
I0309 21:41:26.966019 139902746892096 submission_runner.py:411] Time since start: 146409.99s, 	Step: 302451, 	{'train/accuracy': 0.8894921541213989, 'train/loss': 0.40963980555534363, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 135320.3759112358, 'total_duration': 146409.99482226372, 'accumulated_submission_time': 135320.3759112358, 'accumulated_eval_time': 11055.576867341995, 'accumulated_logging_time': 18.165199279785156}
I0309 21:41:27.027487 139708415854336 logging_writer.py:48] [302451] accumulated_eval_time=11055.576867, accumulated_logging_time=18.165199, accumulated_submission_time=135320.375911, global_step=302451, preemption_count=0, score=135320.375911, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=146409.994822, train/accuracy=0.889492, train/loss=0.409640, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 21:41:46.701778 139708407461632 logging_writer.py:48] [302500] global_step=302500, grad_norm=3.3982837200164795, loss=1.206038236618042
I0309 21:42:29.344687 139708415854336 logging_writer.py:48] [302600] global_step=302600, grad_norm=3.2788145542144775, loss=2.2344274520874023
I0309 21:43:14.900261 139708407461632 logging_writer.py:48] [302700] global_step=302700, grad_norm=2.9064817428588867, loss=1.0299323797225952
I0309 21:44:00.439634 139708415854336 logging_writer.py:48] [302800] global_step=302800, grad_norm=3.1315596103668213, loss=1.059410810470581
I0309 21:44:45.675409 139708407461632 logging_writer.py:48] [302900] global_step=302900, grad_norm=2.99397349357605, loss=1.1422574520111084
I0309 21:45:30.898723 139708415854336 logging_writer.py:48] [303000] global_step=303000, grad_norm=3.1628053188323975, loss=1.9003320932388306
I0309 21:46:16.095805 139708407461632 logging_writer.py:48] [303100] global_step=303100, grad_norm=3.24468994140625, loss=1.0848381519317627
I0309 21:47:01.283220 139708415854336 logging_writer.py:48] [303200] global_step=303200, grad_norm=3.283198118209839, loss=1.783407211303711
I0309 21:47:46.715071 139708407461632 logging_writer.py:48] [303300] global_step=303300, grad_norm=3.5351641178131104, loss=1.3016644716262817
I0309 21:48:27.221666 139902746892096 spec.py:321] Evaluating on the training split.
I0309 21:48:38.575543 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 21:49:00.302878 139902746892096 spec.py:349] Evaluating on the test split.
I0309 21:49:01.978964 139902746892096 submission_runner.py:411] Time since start: 146865.01s, 	Step: 303392, 	{'train/accuracy': 0.88832026720047, 'train/loss': 0.413810133934021, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 135740.50975704193, 'total_duration': 146865.0077443123, 'accumulated_submission_time': 135740.50975704193, 'accumulated_eval_time': 11090.334149360657, 'accumulated_logging_time': 18.237561464309692}
I0309 21:49:02.049930 139708415854336 logging_writer.py:48] [303392] accumulated_eval_time=11090.334149, accumulated_logging_time=18.237561, accumulated_submission_time=135740.509757, global_step=303392, preemption_count=0, score=135740.509757, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=146865.007744, train/accuracy=0.888320, train/loss=0.413810, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 21:49:05.586121 139708407461632 logging_writer.py:48] [303400] global_step=303400, grad_norm=3.7492825984954834, loss=2.939439296722412
I0309 21:49:46.286046 139708415854336 logging_writer.py:48] [303500] global_step=303500, grad_norm=3.310807228088379, loss=1.0773100852966309
I0309 21:50:31.115793 139708407461632 logging_writer.py:48] [303600] global_step=303600, grad_norm=3.2266452312469482, loss=2.8117401599884033
I0309 21:51:16.230459 139708415854336 logging_writer.py:48] [303700] global_step=303700, grad_norm=2.9842007160186768, loss=1.0684047937393188
I0309 21:52:01.759065 139708407461632 logging_writer.py:48] [303800] global_step=303800, grad_norm=2.9442577362060547, loss=1.3772891759872437
I0309 21:52:46.708911 139708415854336 logging_writer.py:48] [303900] global_step=303900, grad_norm=4.102848529815674, loss=2.882519483566284
I0309 21:53:31.807005 139708407461632 logging_writer.py:48] [304000] global_step=304000, grad_norm=3.2872822284698486, loss=1.6042343378067017
I0309 21:54:17.460339 139708415854336 logging_writer.py:48] [304100] global_step=304100, grad_norm=3.598548412322998, loss=3.2547519207000732
I0309 21:55:02.591884 139708407461632 logging_writer.py:48] [304200] global_step=304200, grad_norm=2.7262303829193115, loss=1.5843007564544678
I0309 21:55:47.943438 139708415854336 logging_writer.py:48] [304300] global_step=304300, grad_norm=3.179964303970337, loss=1.1388790607452393
I0309 21:56:02.043162 139902746892096 spec.py:321] Evaluating on the training split.
I0309 21:56:14.162216 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 21:56:36.415719 139902746892096 spec.py:349] Evaluating on the test split.
I0309 21:56:38.091468 139902746892096 submission_runner.py:411] Time since start: 147321.12s, 	Step: 304333, 	{'train/accuracy': 0.8908593654632568, 'train/loss': 0.4046581983566284, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 136160.44328808784, 'total_duration': 147321.12025094032, 'accumulated_submission_time': 136160.44328808784, 'accumulated_eval_time': 11126.382422208786, 'accumulated_logging_time': 18.318288803100586}
I0309 21:56:38.160538 139708407461632 logging_writer.py:48] [304333] accumulated_eval_time=11126.382422, accumulated_logging_time=18.318289, accumulated_submission_time=136160.443288, global_step=304333, preemption_count=0, score=136160.443288, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=147321.120251, train/accuracy=0.890859, train/loss=0.404658, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 21:57:04.908924 139708415854336 logging_writer.py:48] [304400] global_step=304400, grad_norm=3.365478992462158, loss=1.0484490394592285
I0309 21:57:48.716398 139708407461632 logging_writer.py:48] [304500] global_step=304500, grad_norm=3.237489938735962, loss=1.062212347984314
I0309 21:58:34.197160 139708415854336 logging_writer.py:48] [304600] global_step=304600, grad_norm=2.9811110496520996, loss=2.2511560916900635
I0309 21:59:19.792515 139708407461632 logging_writer.py:48] [304700] global_step=304700, grad_norm=2.978260040283203, loss=1.6920582056045532
I0309 22:00:04.934078 139708415854336 logging_writer.py:48] [304800] global_step=304800, grad_norm=2.790886163711548, loss=1.1040675640106201
I0309 22:00:50.247115 139708407461632 logging_writer.py:48] [304900] global_step=304900, grad_norm=4.030690670013428, loss=3.206305503845215
I0309 22:01:35.501263 139708415854336 logging_writer.py:48] [305000] global_step=305000, grad_norm=3.256333589553833, loss=1.1972311735153198
I0309 22:02:20.851567 139708407461632 logging_writer.py:48] [305100] global_step=305100, grad_norm=3.9449996948242188, loss=3.141465663909912
I0309 22:03:06.091127 139708415854336 logging_writer.py:48] [305200] global_step=305200, grad_norm=3.1685214042663574, loss=1.1515883207321167
I0309 22:03:38.189030 139902746892096 spec.py:321] Evaluating on the training split.
I0309 22:03:49.722929 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 22:04:11.905880 139902746892096 spec.py:349] Evaluating on the test split.
I0309 22:04:13.581661 139902746892096 submission_runner.py:411] Time since start: 147776.61s, 	Step: 305272, 	{'train/accuracy': 0.885546863079071, 'train/loss': 0.41975247859954834, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 136580.41247677803, 'total_duration': 147776.61046028137, 'accumulated_submission_time': 136580.41247677803, 'accumulated_eval_time': 11161.775073289871, 'accumulated_logging_time': 18.397958040237427}
I0309 22:04:13.642817 139708407461632 logging_writer.py:48] [305272] accumulated_eval_time=11161.775073, accumulated_logging_time=18.397958, accumulated_submission_time=136580.412477, global_step=305272, preemption_count=0, score=136580.412477, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=147776.610460, train/accuracy=0.885547, train/loss=0.419752, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 22:04:25.047557 139708415854336 logging_writer.py:48] [305300] global_step=305300, grad_norm=3.290428638458252, loss=1.1340229511260986
I0309 22:05:07.001048 139708407461632 logging_writer.py:48] [305400] global_step=305400, grad_norm=3.2014284133911133, loss=2.917024612426758
I0309 22:05:52.145346 139708415854336 logging_writer.py:48] [305500] global_step=305500, grad_norm=2.9553191661834717, loss=1.4832284450531006
I0309 22:06:37.682659 139708407461632 logging_writer.py:48] [305600] global_step=305600, grad_norm=3.4106125831604004, loss=1.1302659511566162
I0309 22:07:23.120853 139708415854336 logging_writer.py:48] [305700] global_step=305700, grad_norm=3.148484468460083, loss=1.1360350847244263
I0309 22:08:08.514046 139708407461632 logging_writer.py:48] [305800] global_step=305800, grad_norm=3.345237970352173, loss=1.1523914337158203
I0309 22:08:53.724171 139708415854336 logging_writer.py:48] [305900] global_step=305900, grad_norm=3.1237986087799072, loss=2.005511999130249
I0309 22:09:38.899289 139708407461632 logging_writer.py:48] [306000] global_step=306000, grad_norm=3.029489278793335, loss=1.0849461555480957
I0309 22:10:24.436625 139708415854336 logging_writer.py:48] [306100] global_step=306100, grad_norm=3.8546791076660156, loss=3.3071162700653076
I0309 22:11:09.609542 139708407461632 logging_writer.py:48] [306200] global_step=306200, grad_norm=2.8479220867156982, loss=1.6428329944610596
I0309 22:11:13.762238 139902746892096 spec.py:321] Evaluating on the training split.
I0309 22:11:25.197157 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 22:11:47.287875 139902746892096 spec.py:349] Evaluating on the test split.
I0309 22:11:48.961098 139902746892096 submission_runner.py:411] Time since start: 148231.99s, 	Step: 306211, 	{'train/accuracy': 0.8883788585662842, 'train/loss': 0.41313502192497253, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 137000.47409558296, 'total_duration': 148231.98989629745, 'accumulated_submission_time': 137000.47409558296, 'accumulated_eval_time': 11196.973927736282, 'accumulated_logging_time': 18.467971086502075}
I0309 22:11:49.023952 139708415854336 logging_writer.py:48] [306211] accumulated_eval_time=11196.973928, accumulated_logging_time=18.467971, accumulated_submission_time=137000.474096, global_step=306211, preemption_count=0, score=137000.474096, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=148231.989896, train/accuracy=0.888379, train/loss=0.413135, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 22:12:24.420618 139708407461632 logging_writer.py:48] [306300] global_step=306300, grad_norm=2.9671125411987305, loss=1.1308821439743042
I0309 22:13:09.805367 139708415854336 logging_writer.py:48] [306400] global_step=306400, grad_norm=3.5779659748077393, loss=3.221283435821533
I0309 22:13:55.092792 139708407461632 logging_writer.py:48] [306500] global_step=306500, grad_norm=2.863541603088379, loss=1.651995301246643
I0309 22:14:40.707460 139708415854336 logging_writer.py:48] [306600] global_step=306600, grad_norm=3.468568801879883, loss=1.352116584777832
I0309 22:15:25.799756 139708407461632 logging_writer.py:48] [306700] global_step=306700, grad_norm=2.9694156646728516, loss=1.8820830583572388
I0309 22:16:11.077148 139708415854336 logging_writer.py:48] [306800] global_step=306800, grad_norm=2.8496720790863037, loss=2.165086269378662
I0309 22:16:56.345477 139708407461632 logging_writer.py:48] [306900] global_step=306900, grad_norm=3.761651039123535, loss=1.0717493295669556
I0309 22:17:41.893984 139708415854336 logging_writer.py:48] [307000] global_step=307000, grad_norm=3.324301242828369, loss=1.1182515621185303
I0309 22:18:27.129299 139708407461632 logging_writer.py:48] [307100] global_step=307100, grad_norm=3.9130656719207764, loss=3.2724740505218506
I0309 22:18:49.114188 139902746892096 spec.py:321] Evaluating on the training split.
I0309 22:19:00.461285 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 22:19:22.366862 139902746892096 spec.py:349] Evaluating on the test split.
I0309 22:19:24.037181 139902746892096 submission_runner.py:411] Time since start: 148687.07s, 	Step: 307151, 	{'train/accuracy': 0.8883984088897705, 'train/loss': 0.41334328055381775, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 137420.50700616837, 'total_duration': 148687.06598305702, 'accumulated_submission_time': 137420.50700616837, 'accumulated_eval_time': 11231.89693236351, 'accumulated_logging_time': 18.539098262786865}
I0309 22:19:24.097875 139708415854336 logging_writer.py:48] [307151] accumulated_eval_time=11231.896932, accumulated_logging_time=18.539098, accumulated_submission_time=137420.507006, global_step=307151, preemption_count=0, score=137420.507006, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=148687.065983, train/accuracy=0.888398, train/loss=0.413343, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 22:19:43.762288 139708407461632 logging_writer.py:48] [307200] global_step=307200, grad_norm=3.259969711303711, loss=1.130712628364563
I0309 22:20:26.702659 139708415854336 logging_writer.py:48] [307300] global_step=307300, grad_norm=3.154677152633667, loss=1.7141412496566772
I0309 22:21:11.935063 139708407461632 logging_writer.py:48] [307400] global_step=307400, grad_norm=3.6610279083251953, loss=3.1402742862701416
I0309 22:21:57.453450 139708415854336 logging_writer.py:48] [307500] global_step=307500, grad_norm=3.08292555809021, loss=1.4104362726211548
I0309 22:22:43.064190 139708407461632 logging_writer.py:48] [307600] global_step=307600, grad_norm=2.935652732849121, loss=2.273036003112793
I0309 22:23:28.469081 139708415854336 logging_writer.py:48] [307700] global_step=307700, grad_norm=3.0635666847229004, loss=2.434262752532959
I0309 22:24:13.944696 139708407461632 logging_writer.py:48] [307800] global_step=307800, grad_norm=4.712049961090088, loss=3.084583282470703
I0309 22:24:59.278901 139708415854336 logging_writer.py:48] [307900] global_step=307900, grad_norm=3.0197267532348633, loss=1.1245886087417603
I0309 22:25:44.581181 139708407461632 logging_writer.py:48] [308000] global_step=308000, grad_norm=3.355808973312378, loss=1.1032097339630127
I0309 22:26:24.426548 139902746892096 spec.py:321] Evaluating on the training split.
I0309 22:26:35.900339 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 22:26:57.872416 139902746892096 spec.py:349] Evaluating on the test split.
I0309 22:26:59.543673 139902746892096 submission_runner.py:411] Time since start: 149142.57s, 	Step: 308089, 	{'train/accuracy': 0.8885155916213989, 'train/loss': 0.4139094352722168, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 137840.77836227417, 'total_duration': 149142.5724697113, 'accumulated_submission_time': 137840.77836227417, 'accumulated_eval_time': 11267.014060974121, 'accumulated_logging_time': 18.608031034469604}
I0309 22:26:59.605408 139708415854336 logging_writer.py:48] [308089] accumulated_eval_time=11267.014061, accumulated_logging_time=18.608031, accumulated_submission_time=137840.778362, global_step=308089, preemption_count=0, score=137840.778362, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=149142.572470, train/accuracy=0.888516, train/loss=0.413909, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 22:27:04.327324 139708407461632 logging_writer.py:48] [308100] global_step=308100, grad_norm=3.260769844055176, loss=1.025676965713501
I0309 22:27:45.102429 139708415854336 logging_writer.py:48] [308200] global_step=308200, grad_norm=3.166804075241089, loss=1.2135951519012451
I0309 22:28:30.238515 139708407461632 logging_writer.py:48] [308300] global_step=308300, grad_norm=3.04361891746521, loss=1.829050064086914
I0309 22:29:15.831221 139708415854336 logging_writer.py:48] [308400] global_step=308400, grad_norm=3.339404582977295, loss=2.4292962551116943
I0309 22:30:01.049945 139708407461632 logging_writer.py:48] [308500] global_step=308500, grad_norm=4.050168037414551, loss=3.1348471641540527
I0309 22:30:46.183731 139708415854336 logging_writer.py:48] [308600] global_step=308600, grad_norm=2.9038312435150146, loss=1.4460670948028564
I0309 22:31:31.432344 139708407461632 logging_writer.py:48] [308700] global_step=308700, grad_norm=3.4178426265716553, loss=2.8503992557525635
I0309 22:32:16.630490 139708415854336 logging_writer.py:48] [308800] global_step=308800, grad_norm=2.789304494857788, loss=1.8219009637832642
I0309 22:33:01.702162 139708407461632 logging_writer.py:48] [308900] global_step=308900, grad_norm=2.855816602706909, loss=1.3385231494903564
I0309 22:33:47.179517 139708415854336 logging_writer.py:48] [309000] global_step=309000, grad_norm=3.135829210281372, loss=1.1118520498275757
I0309 22:33:59.853270 139902746892096 spec.py:321] Evaluating on the training split.
I0309 22:34:11.302999 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 22:34:33.811164 139902746892096 spec.py:349] Evaluating on the test split.
I0309 22:34:35.478463 139902746892096 submission_runner.py:411] Time since start: 149598.51s, 	Step: 309029, 	{'train/accuracy': 0.8866991996765137, 'train/loss': 0.41851291060447693, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 138260.96691298485, 'total_duration': 149598.50726366043, 'accumulated_submission_time': 138260.96691298485, 'accumulated_eval_time': 11302.639257907867, 'accumulated_logging_time': 18.680636405944824}
I0309 22:34:35.549545 139708407461632 logging_writer.py:48] [309029] accumulated_eval_time=11302.639258, accumulated_logging_time=18.680636, accumulated_submission_time=138260.966913, global_step=309029, preemption_count=0, score=138260.966913, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=149598.507264, train/accuracy=0.886699, train/loss=0.418513, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 22:35:03.874774 139708415854336 logging_writer.py:48] [309100] global_step=309100, grad_norm=2.968864679336548, loss=1.3404067754745483
I0309 22:35:47.797679 139708407461632 logging_writer.py:48] [309200] global_step=309200, grad_norm=3.993687629699707, loss=3.1535158157348633
I0309 22:36:33.325069 139708415854336 logging_writer.py:48] [309300] global_step=309300, grad_norm=3.0156736373901367, loss=0.9747718572616577
I0309 22:37:18.917423 139708407461632 logging_writer.py:48] [309400] global_step=309400, grad_norm=2.72221302986145, loss=1.7154953479766846
I0309 22:38:04.219190 139708415854336 logging_writer.py:48] [309500] global_step=309500, grad_norm=3.1556739807128906, loss=1.1608436107635498
I0309 22:38:49.757146 139708407461632 logging_writer.py:48] [309600] global_step=309600, grad_norm=3.0540952682495117, loss=1.9776864051818848
I0309 22:39:34.849378 139708415854336 logging_writer.py:48] [309700] global_step=309700, grad_norm=3.1468429565429688, loss=1.5911120176315308
I0309 22:40:20.191218 139708407461632 logging_writer.py:48] [309800] global_step=309800, grad_norm=3.02866530418396, loss=1.052980899810791
I0309 22:41:05.683781 139708415854336 logging_writer.py:48] [309900] global_step=309900, grad_norm=3.1523289680480957, loss=1.1673977375030518
I0309 22:41:35.674244 139902746892096 spec.py:321] Evaluating on the training split.
I0309 22:41:47.071203 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 22:42:07.040561 139902746892096 spec.py:349] Evaluating on the test split.
I0309 22:42:08.720449 139902746892096 submission_runner.py:411] Time since start: 150051.75s, 	Step: 309968, 	{'train/accuracy': 0.8897656202316284, 'train/loss': 0.41061508655548096, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 138681.03453612328, 'total_duration': 150051.74921894073, 'accumulated_submission_time': 138681.03453612328, 'accumulated_eval_time': 11335.685423135757, 'accumulated_logging_time': 18.76006007194519}
I0309 22:42:08.796658 139708407461632 logging_writer.py:48] [309968] accumulated_eval_time=11335.685423, accumulated_logging_time=18.760060, accumulated_submission_time=138681.034536, global_step=309968, preemption_count=0, score=138681.034536, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=150051.749219, train/accuracy=0.889766, train/loss=0.410615, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 22:42:21.787928 139708415854336 logging_writer.py:48] [310000] global_step=310000, grad_norm=2.9043989181518555, loss=1.035913109779358
I0309 22:43:04.128115 139708407461632 logging_writer.py:48] [310100] global_step=310100, grad_norm=3.105372667312622, loss=1.2343932390213013
I0309 22:43:49.443103 139708415854336 logging_writer.py:48] [310200] global_step=310200, grad_norm=3.7150299549102783, loss=3.132413148880005
I0309 22:44:34.983777 139708407461632 logging_writer.py:48] [310300] global_step=310300, grad_norm=3.1566877365112305, loss=1.1988801956176758
I0309 22:45:20.223947 139708415854336 logging_writer.py:48] [310400] global_step=310400, grad_norm=2.9514167308807373, loss=1.3060383796691895
I0309 22:46:05.273350 139708407461632 logging_writer.py:48] [310500] global_step=310500, grad_norm=3.123286008834839, loss=2.2830004692077637
I0309 22:46:50.577617 139708415854336 logging_writer.py:48] [310600] global_step=310600, grad_norm=3.706918716430664, loss=1.185662031173706
I0309 22:47:36.096237 139708407461632 logging_writer.py:48] [310700] global_step=310700, grad_norm=3.0269956588745117, loss=1.1454523801803589
I0309 22:48:21.501762 139708415854336 logging_writer.py:48] [310800] global_step=310800, grad_norm=3.0072648525238037, loss=2.080393075942993
I0309 22:49:06.699967 139708407461632 logging_writer.py:48] [310900] global_step=310900, grad_norm=3.0278525352478027, loss=1.7545931339263916
I0309 22:49:09.134804 139902746892096 spec.py:321] Evaluating on the training split.
I0309 22:49:20.517357 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 22:49:40.497232 139902746892096 spec.py:349] Evaluating on the test split.
I0309 22:49:42.186338 139902746892096 submission_runner.py:411] Time since start: 150505.22s, 	Step: 310907, 	{'train/accuracy': 0.8888671398162842, 'train/loss': 0.4129617512226105, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 139101.31498932838, 'total_duration': 150505.21512961388, 'accumulated_submission_time': 139101.31498932838, 'accumulated_eval_time': 11368.736935138702, 'accumulated_logging_time': 18.845513105392456}
I0309 22:49:42.260129 139708415854336 logging_writer.py:48] [310907] accumulated_eval_time=11368.736935, accumulated_logging_time=18.845513, accumulated_submission_time=139101.314989, global_step=310907, preemption_count=0, score=139101.314989, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=150505.215130, train/accuracy=0.888867, train/loss=0.412962, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 22:50:20.407341 139708407461632 logging_writer.py:48] [311000] global_step=311000, grad_norm=2.90817928314209, loss=1.5157057046890259
I0309 22:51:05.499069 139708415854336 logging_writer.py:48] [311100] global_step=311100, grad_norm=3.334742784500122, loss=1.2290143966674805
I0309 22:51:50.994605 139708407461632 logging_writer.py:48] [311200] global_step=311200, grad_norm=3.037529468536377, loss=2.3590173721313477
I0309 22:52:36.526133 139708415854336 logging_writer.py:48] [311300] global_step=311300, grad_norm=3.539177179336548, loss=2.7515180110931396
I0309 22:53:21.888951 139708407461632 logging_writer.py:48] [311400] global_step=311400, grad_norm=3.214824676513672, loss=2.5630340576171875
I0309 22:54:07.231741 139708415854336 logging_writer.py:48] [311500] global_step=311500, grad_norm=3.032094955444336, loss=1.3529609441757202
I0309 22:54:52.945143 139708407461632 logging_writer.py:48] [311600] global_step=311600, grad_norm=3.7328972816467285, loss=2.672682046890259
I0309 22:55:38.674043 139708415854336 logging_writer.py:48] [311700] global_step=311700, grad_norm=3.624662160873413, loss=3.1742734909057617
I0309 22:56:24.244249 139708407461632 logging_writer.py:48] [311800] global_step=311800, grad_norm=3.5260322093963623, loss=1.4640824794769287
I0309 22:56:42.586901 139902746892096 spec.py:321] Evaluating on the training split.
I0309 22:56:53.868580 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 22:57:15.226069 139902746892096 spec.py:349] Evaluating on the test split.
I0309 22:57:16.904628 139902746892096 submission_runner.py:411] Time since start: 150959.93s, 	Step: 311842, 	{'train/accuracy': 0.8851171731948853, 'train/loss': 0.4197098910808563, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 139521.58361434937, 'total_duration': 150959.9334256649, 'accumulated_submission_time': 139521.58361434937, 'accumulated_eval_time': 11403.054658412933, 'accumulated_logging_time': 18.928631067276}
I0309 22:57:16.965284 139708415854336 logging_writer.py:48] [311842] accumulated_eval_time=11403.054658, accumulated_logging_time=18.928631, accumulated_submission_time=139521.583614, global_step=311842, preemption_count=0, score=139521.583614, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=150959.933426, train/accuracy=0.885117, train/loss=0.419710, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 22:57:40.316329 139708407461632 logging_writer.py:48] [311900] global_step=311900, grad_norm=3.2765071392059326, loss=1.1555325984954834
I0309 22:58:23.693663 139708415854336 logging_writer.py:48] [312000] global_step=312000, grad_norm=3.0893917083740234, loss=1.218557357788086
I0309 22:59:09.044328 139708407461632 logging_writer.py:48] [312100] global_step=312100, grad_norm=3.09645676612854, loss=2.7720179557800293
I0309 22:59:54.432887 139708415854336 logging_writer.py:48] [312200] global_step=312200, grad_norm=2.965970039367676, loss=1.9578970670700073
I0309 23:00:39.679003 139708407461632 logging_writer.py:48] [312300] global_step=312300, grad_norm=2.9573264122009277, loss=1.0910755395889282
I0309 23:01:25.272279 139708415854336 logging_writer.py:48] [312400] global_step=312400, grad_norm=3.365204334259033, loss=1.4228739738464355
I0309 23:02:10.496488 139708407461632 logging_writer.py:48] [312500] global_step=312500, grad_norm=3.3041293621063232, loss=1.2460684776306152
I0309 23:02:55.992759 139708415854336 logging_writer.py:48] [312600] global_step=312600, grad_norm=3.061737060546875, loss=1.108373761177063
I0309 23:03:41.424650 139708407461632 logging_writer.py:48] [312700] global_step=312700, grad_norm=3.2715835571289062, loss=1.0450036525726318
I0309 23:04:17.376406 139902746892096 spec.py:321] Evaluating on the training split.
I0309 23:04:28.598540 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 23:04:49.760493 139902746892096 spec.py:349] Evaluating on the test split.
I0309 23:04:51.427743 139902746892096 submission_runner.py:411] Time since start: 151414.46s, 	Step: 312780, 	{'train/accuracy': 0.8889452815055847, 'train/loss': 0.41509246826171875, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 139941.93662166595, 'total_duration': 151414.4565434456, 'accumulated_submission_time': 139941.93662166595, 'accumulated_eval_time': 11437.10601568222, 'accumulated_logging_time': 18.999311447143555}
I0309 23:04:51.489115 139708415854336 logging_writer.py:48] [312780] accumulated_eval_time=11437.106016, accumulated_logging_time=18.999311, accumulated_submission_time=139941.936622, global_step=312780, preemption_count=0, score=139941.936622, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=151414.456543, train/accuracy=0.888945, train/loss=0.415092, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 23:04:59.755773 139708407461632 logging_writer.py:48] [312800] global_step=312800, grad_norm=3.000552177429199, loss=1.094749927520752
I0309 23:05:40.615368 139708415854336 logging_writer.py:48] [312900] global_step=312900, grad_norm=3.249215841293335, loss=1.0765990018844604
I0309 23:06:25.855369 139708407461632 logging_writer.py:48] [313000] global_step=313000, grad_norm=3.039242744445801, loss=1.1322264671325684
I0309 23:07:11.411162 139708415854336 logging_writer.py:48] [313100] global_step=313100, grad_norm=3.367781162261963, loss=1.0964035987854004
I0309 23:07:56.540955 139708407461632 logging_writer.py:48] [313200] global_step=313200, grad_norm=3.2623467445373535, loss=1.1313331127166748
I0309 23:08:41.961539 139708415854336 logging_writer.py:48] [313300] global_step=313300, grad_norm=3.5104854106903076, loss=3.014998435974121
I0309 23:09:27.373178 139708407461632 logging_writer.py:48] [313400] global_step=313400, grad_norm=2.9015140533447266, loss=1.5256482362747192
I0309 23:10:12.408982 139708415854336 logging_writer.py:48] [313500] global_step=313500, grad_norm=3.1300604343414307, loss=1.2456300258636475
I0309 23:10:57.605373 139708407461632 logging_writer.py:48] [313600] global_step=313600, grad_norm=3.1393635272979736, loss=1.2600632905960083
I0309 23:11:42.776547 139708415854336 logging_writer.py:48] [313700] global_step=313700, grad_norm=3.0300025939941406, loss=1.254272699356079
I0309 23:11:51.816208 139902746892096 spec.py:321] Evaluating on the training split.
I0309 23:12:02.964748 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 23:12:23.332665 139902746892096 spec.py:349] Evaluating on the test split.
I0309 23:12:25.016984 139902746892096 submission_runner.py:411] Time since start: 151868.05s, 	Step: 313722, 	{'train/accuracy': 0.88734370470047, 'train/loss': 0.41925010085105896, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 140362.2047946453, 'total_duration': 151868.0457689762, 'accumulated_submission_time': 140362.2047946453, 'accumulated_eval_time': 11470.306761026382, 'accumulated_logging_time': 19.070598363876343}
I0309 23:12:25.089224 139708407461632 logging_writer.py:48] [313722] accumulated_eval_time=11470.306761, accumulated_logging_time=19.070598, accumulated_submission_time=140362.204795, global_step=313722, preemption_count=0, score=140362.204795, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=151868.045769, train/accuracy=0.887344, train/loss=0.419250, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 23:12:56.176249 139708415854336 logging_writer.py:48] [313800] global_step=313800, grad_norm=3.09566330909729, loss=2.512179374694824
I0309 23:13:40.525563 139708407461632 logging_writer.py:48] [313900] global_step=313900, grad_norm=3.1360113620758057, loss=1.2327075004577637
I0309 23:14:25.907323 139708415854336 logging_writer.py:48] [314000] global_step=314000, grad_norm=3.3307077884674072, loss=1.6314561367034912
I0309 23:15:11.840699 139708407461632 logging_writer.py:48] [314100] global_step=314100, grad_norm=3.151524305343628, loss=1.1773712635040283
I0309 23:15:56.779625 139708415854336 logging_writer.py:48] [314200] global_step=314200, grad_norm=3.3950023651123047, loss=2.745741844177246
I0309 23:16:42.154300 139708407461632 logging_writer.py:48] [314300] global_step=314300, grad_norm=3.0424001216888428, loss=1.107576608657837
I0309 23:17:27.474376 139708415854336 logging_writer.py:48] [314400] global_step=314400, grad_norm=3.0873751640319824, loss=1.100148320198059
I0309 23:18:12.667472 139708407461632 logging_writer.py:48] [314500] global_step=314500, grad_norm=3.2170937061309814, loss=1.143301010131836
I0309 23:18:57.820221 139708415854336 logging_writer.py:48] [314600] global_step=314600, grad_norm=3.5037479400634766, loss=2.9222376346588135
I0309 23:19:25.032648 139902746892096 spec.py:321] Evaluating on the training split.
I0309 23:19:36.351119 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 23:19:58.975379 139902746892096 spec.py:349] Evaluating on the test split.
I0309 23:20:00.661296 139902746892096 submission_runner.py:411] Time since start: 152323.69s, 	Step: 314662, 	{'train/accuracy': 0.887011706829071, 'train/loss': 0.41875016689300537, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 140782.08658885956, 'total_duration': 152323.69007754326, 'accumulated_submission_time': 140782.08658885956, 'accumulated_eval_time': 11505.935393810272, 'accumulated_logging_time': 19.155100107192993}
I0309 23:20:00.734970 139708407461632 logging_writer.py:48] [314662] accumulated_eval_time=11505.935394, accumulated_logging_time=19.155100, accumulated_submission_time=140782.086589, global_step=314662, preemption_count=0, score=140782.086589, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=152323.690078, train/accuracy=0.887012, train/loss=0.418750, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 23:20:16.075796 139708415854336 logging_writer.py:48] [314700] global_step=314700, grad_norm=3.3117547035217285, loss=2.5609326362609863
I0309 23:20:57.969916 139708407461632 logging_writer.py:48] [314800] global_step=314800, grad_norm=3.2131054401397705, loss=2.5234270095825195
I0309 23:21:43.414276 139708415854336 logging_writer.py:48] [314900] global_step=314900, grad_norm=3.8322556018829346, loss=3.0883069038391113
I0309 23:22:28.688336 139708407461632 logging_writer.py:48] [315000] global_step=315000, grad_norm=3.0531041622161865, loss=1.1221301555633545
I0309 23:23:14.083980 139708415854336 logging_writer.py:48] [315100] global_step=315100, grad_norm=3.0920021533966064, loss=1.139479160308838
I0309 23:23:59.352544 139708407461632 logging_writer.py:48] [315200] global_step=315200, grad_norm=2.8803160190582275, loss=2.1166462898254395
I0309 23:24:44.827410 139708415854336 logging_writer.py:48] [315300] global_step=315300, grad_norm=3.176241159439087, loss=1.0954978466033936
I0309 23:25:30.129175 139708407461632 logging_writer.py:48] [315400] global_step=315400, grad_norm=3.4379351139068604, loss=2.981215000152588
I0309 23:26:15.545451 139708415854336 logging_writer.py:48] [315500] global_step=315500, grad_norm=2.8827342987060547, loss=1.10990571975708
I0309 23:27:00.913617 139708407461632 logging_writer.py:48] [315600] global_step=315600, grad_norm=3.0932559967041016, loss=2.168471097946167
I0309 23:27:00.924298 139902746892096 spec.py:321] Evaluating on the training split.
I0309 23:27:12.446144 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 23:27:35.106718 139902746892096 spec.py:349] Evaluating on the test split.
I0309 23:27:36.784949 139902746892096 submission_runner.py:411] Time since start: 152779.81s, 	Step: 315601, 	{'train/accuracy': 0.8889257907867432, 'train/loss': 0.4117574989795685, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 141202.2172217369, 'total_duration': 152779.8137331009, 'accumulated_submission_time': 141202.2172217369, 'accumulated_eval_time': 11541.796007156372, 'accumulated_logging_time': 19.238603115081787}
I0309 23:27:36.857433 139708415854336 logging_writer.py:48] [315601] accumulated_eval_time=11541.796007, accumulated_logging_time=19.238603, accumulated_submission_time=141202.217222, global_step=315601, preemption_count=0, score=141202.217222, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=152779.813733, train/accuracy=0.888926, train/loss=0.411757, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 23:28:16.472813 139708407461632 logging_writer.py:48] [315700] global_step=315700, grad_norm=3.748556137084961, loss=3.2111973762512207
I0309 23:29:01.649541 139708415854336 logging_writer.py:48] [315800] global_step=315800, grad_norm=3.0624139308929443, loss=1.0818926095962524
I0309 23:29:46.973378 139708407461632 logging_writer.py:48] [315900] global_step=315900, grad_norm=3.2429354190826416, loss=1.0830727815628052
I0309 23:30:32.252957 139708415854336 logging_writer.py:48] [316000] global_step=316000, grad_norm=3.298490524291992, loss=2.9690370559692383
I0309 23:31:17.105676 139708407461632 logging_writer.py:48] [316100] global_step=316100, grad_norm=3.039161443710327, loss=1.1559187173843384
I0309 23:32:02.502877 139708415854336 logging_writer.py:48] [316200] global_step=316200, grad_norm=3.255002498626709, loss=1.3486212491989136
I0309 23:32:47.946486 139708407461632 logging_writer.py:48] [316300] global_step=316300, grad_norm=3.112504482269287, loss=1.1621536016464233
I0309 23:33:33.121477 139708415854336 logging_writer.py:48] [316400] global_step=316400, grad_norm=3.3988912105560303, loss=1.158071756362915
I0309 23:34:18.742090 139708407461632 logging_writer.py:48] [316500] global_step=316500, grad_norm=3.251594066619873, loss=2.1280431747436523
I0309 23:34:37.120026 139902746892096 spec.py:321] Evaluating on the training split.
I0309 23:34:48.600728 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 23:35:09.172783 139902746892096 spec.py:349] Evaluating on the test split.
I0309 23:35:10.852838 139902746892096 submission_runner.py:411] Time since start: 153233.88s, 	Step: 316542, 	{'train/accuracy': 0.8873632550239563, 'train/loss': 0.4215485751628876, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 141622.4211435318, 'total_duration': 153233.88162970543, 'accumulated_submission_time': 141622.4211435318, 'accumulated_eval_time': 11575.5288336277, 'accumulated_logging_time': 19.320276975631714}
I0309 23:35:10.930488 139708415854336 logging_writer.py:48] [316542] accumulated_eval_time=11575.528834, accumulated_logging_time=19.320277, accumulated_submission_time=141622.421144, global_step=316542, preemption_count=0, score=141622.421144, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=153233.881630, train/accuracy=0.887363, train/loss=0.421549, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 23:35:34.134278 139708407461632 logging_writer.py:48] [316600] global_step=316600, grad_norm=3.0980119705200195, loss=1.1618446111679077
I0309 23:36:18.366368 139708415854336 logging_writer.py:48] [316700] global_step=316700, grad_norm=3.023207902908325, loss=1.1555628776550293
I0309 23:37:03.862124 139708407461632 logging_writer.py:48] [316800] global_step=316800, grad_norm=3.2934553623199463, loss=1.6132004261016846
I0309 23:37:49.818204 139708415854336 logging_writer.py:48] [316900] global_step=316900, grad_norm=3.0912976264953613, loss=1.750420331954956
I0309 23:38:35.031536 139708407461632 logging_writer.py:48] [317000] global_step=317000, grad_norm=3.297325372695923, loss=2.742631435394287
I0309 23:39:20.355283 139708415854336 logging_writer.py:48] [317100] global_step=317100, grad_norm=2.9846434593200684, loss=1.266871690750122
I0309 23:40:05.539406 139708407461632 logging_writer.py:48] [317200] global_step=317200, grad_norm=3.3129208087921143, loss=2.8341588973999023
I0309 23:40:51.078258 139708415854336 logging_writer.py:48] [317300] global_step=317300, grad_norm=3.4149441719055176, loss=1.1213895082473755
I0309 23:41:36.824178 139708407461632 logging_writer.py:48] [317400] global_step=317400, grad_norm=3.3024768829345703, loss=2.9836838245391846
I0309 23:42:10.995281 139902746892096 spec.py:321] Evaluating on the training split.
I0309 23:42:22.527411 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 23:42:42.384219 139902746892096 spec.py:349] Evaluating on the test split.
I0309 23:42:44.072454 139902746892096 submission_runner.py:411] Time since start: 153687.10s, 	Step: 317477, 	{'train/accuracy': 0.88929682970047, 'train/loss': 0.40979132056236267, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 142042.42588067055, 'total_duration': 153687.1012263298, 'accumulated_submission_time': 142042.42588067055, 'accumulated_eval_time': 11608.605984210968, 'accumulated_logging_time': 19.408986806869507}
I0309 23:42:44.150568 139708415854336 logging_writer.py:48] [317477] accumulated_eval_time=11608.605984, accumulated_logging_time=19.408987, accumulated_submission_time=142042.425881, global_step=317477, preemption_count=0, score=142042.425881, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=153687.101226, train/accuracy=0.889297, train/loss=0.409791, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 23:42:53.597522 139708407461632 logging_writer.py:48] [317500] global_step=317500, grad_norm=3.0468509197235107, loss=1.543068528175354
I0309 23:43:35.811316 139708415854336 logging_writer.py:48] [317600] global_step=317600, grad_norm=2.8817102909088135, loss=1.8656290769577026
I0309 23:44:21.263624 139708407461632 logging_writer.py:48] [317700] global_step=317700, grad_norm=3.1768712997436523, loss=1.1146219968795776
I0309 23:45:06.966329 139708415854336 logging_writer.py:48] [317800] global_step=317800, grad_norm=3.2491841316223145, loss=2.313263416290283
I0309 23:45:52.412729 139708407461632 logging_writer.py:48] [317900] global_step=317900, grad_norm=2.945502519607544, loss=1.8832790851593018
I0309 23:46:37.503239 139708415854336 logging_writer.py:48] [318000] global_step=318000, grad_norm=2.9676854610443115, loss=1.1852960586547852
I0309 23:47:22.784314 139708407461632 logging_writer.py:48] [318100] global_step=318100, grad_norm=3.099736213684082, loss=1.0656107664108276
I0309 23:48:08.069270 139708415854336 logging_writer.py:48] [318200] global_step=318200, grad_norm=4.23372745513916, loss=3.2659361362457275
I0309 23:48:53.504360 139708407461632 logging_writer.py:48] [318300] global_step=318300, grad_norm=3.082930088043213, loss=1.0821120738983154
I0309 23:49:38.541966 139708415854336 logging_writer.py:48] [318400] global_step=318400, grad_norm=2.9649174213409424, loss=1.0474992990493774
I0309 23:49:44.157124 139902746892096 spec.py:321] Evaluating on the training split.
I0309 23:49:55.429009 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 23:50:17.711289 139902746892096 spec.py:349] Evaluating on the test split.
I0309 23:50:19.378489 139902746892096 submission_runner.py:411] Time since start: 154142.41s, 	Step: 318414, 	{'train/accuracy': 0.8886523246765137, 'train/loss': 0.42033693194389343, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 142462.3735461235, 'total_duration': 154142.40728020668, 'accumulated_submission_time': 142462.3735461235, 'accumulated_eval_time': 11643.827334403992, 'accumulated_logging_time': 19.497087478637695}
I0309 23:50:19.442503 139708407461632 logging_writer.py:48] [318414] accumulated_eval_time=11643.827334, accumulated_logging_time=19.497087, accumulated_submission_time=142462.373546, global_step=318414, preemption_count=0, score=142462.373546, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=154142.407280, train/accuracy=0.888652, train/loss=0.420337, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 23:50:53.658769 139708415854336 logging_writer.py:48] [318500] global_step=318500, grad_norm=3.2849199771881104, loss=1.1914489269256592
I0309 23:51:38.110203 139708407461632 logging_writer.py:48] [318600] global_step=318600, grad_norm=2.8112425804138184, loss=1.0437180995941162
I0309 23:52:23.292048 139708415854336 logging_writer.py:48] [318700] global_step=318700, grad_norm=2.886169195175171, loss=1.6713498830795288
I0309 23:53:08.482125 139708407461632 logging_writer.py:48] [318800] global_step=318800, grad_norm=3.544654607772827, loss=3.0761077404022217
I0309 23:53:53.625123 139708415854336 logging_writer.py:48] [318900] global_step=318900, grad_norm=3.4421725273132324, loss=1.0362567901611328
I0309 23:54:39.086206 139708407461632 logging_writer.py:48] [319000] global_step=319000, grad_norm=3.3809773921966553, loss=1.1086150407791138
I0309 23:55:24.518809 139708415854336 logging_writer.py:48] [319100] global_step=319100, grad_norm=2.9923110008239746, loss=1.2888129949569702
I0309 23:56:09.791438 139708407461632 logging_writer.py:48] [319200] global_step=319200, grad_norm=3.826815128326416, loss=3.228719711303711
I0309 23:56:54.817327 139708415854336 logging_writer.py:48] [319300] global_step=319300, grad_norm=3.1292901039123535, loss=1.5281639099121094
I0309 23:57:19.713944 139902746892096 spec.py:321] Evaluating on the training split.
I0309 23:57:31.324407 139902746892096 spec.py:333] Evaluating on the validation split.
I0309 23:57:52.302709 139902746892096 spec.py:349] Evaluating on the test split.
I0309 23:57:53.971953 139902746892096 submission_runner.py:411] Time since start: 154597.00s, 	Step: 319357, 	{'train/accuracy': 0.8871288895606995, 'train/loss': 0.4173811674118042, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 142882.58603477478, 'total_duration': 154597.0007481575, 'accumulated_submission_time': 142882.58603477478, 'accumulated_eval_time': 11678.085348844528, 'accumulated_logging_time': 19.57066559791565}
I0309 23:57:54.036945 139708407461632 logging_writer.py:48] [319357] accumulated_eval_time=11678.085349, accumulated_logging_time=19.570666, accumulated_submission_time=142882.586035, global_step=319357, preemption_count=0, score=142882.586035, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=154597.000748, train/accuracy=0.887129, train/loss=0.417381, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0309 23:58:11.365887 139708415854336 logging_writer.py:48] [319400] global_step=319400, grad_norm=2.952317237854004, loss=1.2939021587371826
I0309 23:58:53.595673 139708407461632 logging_writer.py:48] [319500] global_step=319500, grad_norm=3.2813796997070312, loss=2.6328718662261963
I0309 23:59:38.967663 139708415854336 logging_writer.py:48] [319600] global_step=319600, grad_norm=3.136317014694214, loss=1.1532154083251953
I0310 00:00:24.428777 139708407461632 logging_writer.py:48] [319700] global_step=319700, grad_norm=3.299365997314453, loss=3.0552852153778076
I0310 00:01:09.549522 139708415854336 logging_writer.py:48] [319800] global_step=319800, grad_norm=3.124056100845337, loss=1.3659415245056152
I0310 00:01:54.750987 139708407461632 logging_writer.py:48] [319900] global_step=319900, grad_norm=3.0796451568603516, loss=1.1259307861328125
I0310 00:02:39.956018 139708415854336 logging_writer.py:48] [320000] global_step=320000, grad_norm=2.9962899684906006, loss=2.2653019428253174
I0310 00:03:25.088370 139708407461632 logging_writer.py:48] [320100] global_step=320100, grad_norm=3.080113649368286, loss=1.3482943773269653
I0310 00:04:10.216479 139708415854336 logging_writer.py:48] [320200] global_step=320200, grad_norm=3.266089677810669, loss=1.1434812545776367
I0310 00:04:54.339214 139902746892096 spec.py:321] Evaluating on the training split.
I0310 00:05:06.051492 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 00:05:30.018106 139902746892096 spec.py:349] Evaluating on the test split.
I0310 00:05:31.690465 139902746892096 submission_runner.py:411] Time since start: 155054.72s, 	Step: 320299, 	{'train/accuracy': 0.8873046636581421, 'train/loss': 0.41342973709106445, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 143302.82303833961, 'total_duration': 155054.7192568779, 'accumulated_submission_time': 143302.82303833961, 'accumulated_eval_time': 11715.436604499817, 'accumulated_logging_time': 19.651818990707397}
I0310 00:05:31.754557 139708407461632 logging_writer.py:48] [320299] accumulated_eval_time=11715.436604, accumulated_logging_time=19.651819, accumulated_submission_time=143302.823038, global_step=320299, preemption_count=0, score=143302.823038, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=155054.719257, train/accuracy=0.887305, train/loss=0.413430, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 00:05:32.543633 139708415854336 logging_writer.py:48] [320300] global_step=320300, grad_norm=2.8917949199676514, loss=1.4661306142807007
I0310 00:06:12.672451 139708407461632 logging_writer.py:48] [320400] global_step=320400, grad_norm=3.055205821990967, loss=0.9864709973335266
I0310 00:06:57.887145 139708415854336 logging_writer.py:48] [320500] global_step=320500, grad_norm=3.693190097808838, loss=3.1618709564208984
I0310 00:07:43.496104 139708407461632 logging_writer.py:48] [320600] global_step=320600, grad_norm=3.1028189659118652, loss=1.4239563941955566
I0310 00:08:28.790762 139708415854336 logging_writer.py:48] [320700] global_step=320700, grad_norm=2.939267158508301, loss=1.6502654552459717
I0310 00:09:14.090678 139708407461632 logging_writer.py:48] [320800] global_step=320800, grad_norm=3.075169324874878, loss=1.1743806600570679
I0310 00:09:59.554216 139708415854336 logging_writer.py:48] [320900] global_step=320900, grad_norm=2.9089157581329346, loss=1.8698813915252686
I0310 00:10:44.480320 139708407461632 logging_writer.py:48] [321000] global_step=321000, grad_norm=3.03311824798584, loss=1.8239375352859497
I0310 00:11:29.934109 139708415854336 logging_writer.py:48] [321100] global_step=321100, grad_norm=3.3695547580718994, loss=1.2127515077590942
I0310 00:12:14.895041 139708407461632 logging_writer.py:48] [321200] global_step=321200, grad_norm=3.1116085052490234, loss=1.2838811874389648
I0310 00:12:31.705078 139902746892096 spec.py:321] Evaluating on the training split.
I0310 00:12:43.256920 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 00:13:05.364734 139902746892096 spec.py:349] Evaluating on the test split.
I0310 00:13:07.033375 139902746892096 submission_runner.py:411] Time since start: 155510.06s, 	Step: 321239, 	{'train/accuracy': 0.8871288895606995, 'train/loss': 0.42032068967819214, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 143722.71570396423, 'total_duration': 155510.06215786934, 'accumulated_submission_time': 143722.71570396423, 'accumulated_eval_time': 11750.764872550964, 'accumulated_logging_time': 19.725261211395264}
I0310 00:13:07.106759 139708415854336 logging_writer.py:48] [321239] accumulated_eval_time=11750.764873, accumulated_logging_time=19.725261, accumulated_submission_time=143722.715704, global_step=321239, preemption_count=0, score=143722.715704, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=155510.062158, train/accuracy=0.887129, train/loss=0.420321, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 00:13:31.492298 139708407461632 logging_writer.py:48] [321300] global_step=321300, grad_norm=3.0402684211730957, loss=1.103683352470398
I0310 00:14:14.996652 139708415854336 logging_writer.py:48] [321400] global_step=321400, grad_norm=3.644706964492798, loss=3.0032665729522705
I0310 00:15:00.135361 139708407461632 logging_writer.py:48] [321500] global_step=321500, grad_norm=2.779555320739746, loss=1.207543134689331
I0310 00:15:46.241481 139708415854336 logging_writer.py:48] [321600] global_step=321600, grad_norm=3.2554709911346436, loss=2.1451408863067627
I0310 00:16:31.416375 139708407461632 logging_writer.py:48] [321700] global_step=321700, grad_norm=2.9443862438201904, loss=1.5021919012069702
I0310 00:17:17.152617 139708415854336 logging_writer.py:48] [321800] global_step=321800, grad_norm=3.1794543266296387, loss=2.2219223976135254
I0310 00:18:02.591551 139708407461632 logging_writer.py:48] [321900] global_step=321900, grad_norm=3.2271461486816406, loss=1.3066555261611938
I0310 00:18:48.254407 139708415854336 logging_writer.py:48] [322000] global_step=322000, grad_norm=5.024909496307373, loss=1.1558504104614258
I0310 00:19:33.576135 139708407461632 logging_writer.py:48] [322100] global_step=322100, grad_norm=3.4938554763793945, loss=3.0044031143188477
I0310 00:20:07.306804 139902746892096 spec.py:321] Evaluating on the training split.
I0310 00:20:18.819109 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 00:20:38.533581 139902746892096 spec.py:349] Evaluating on the test split.
I0310 00:20:40.214930 139902746892096 submission_runner.py:411] Time since start: 155963.24s, 	Step: 322176, 	{'train/accuracy': 0.8892187476158142, 'train/loss': 0.4106161892414093, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 144142.85748529434, 'total_duration': 155963.2437081337, 'accumulated_submission_time': 144142.85748529434, 'accumulated_eval_time': 11783.672978162766, 'accumulated_logging_time': 19.807963609695435}
I0310 00:20:40.288622 139708415854336 logging_writer.py:48] [322176] accumulated_eval_time=11783.672978, accumulated_logging_time=19.807964, accumulated_submission_time=144142.857485, global_step=322176, preemption_count=0, score=144142.857485, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=155963.243708, train/accuracy=0.889219, train/loss=0.410616, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 00:20:50.136778 139708407461632 logging_writer.py:48] [322200] global_step=322200, grad_norm=3.095975637435913, loss=1.433962345123291
I0310 00:21:31.990324 139708415854336 logging_writer.py:48] [322300] global_step=322300, grad_norm=4.209433078765869, loss=3.2735254764556885
I0310 00:22:17.216518 139708407461632 logging_writer.py:48] [322400] global_step=322400, grad_norm=3.13207745552063, loss=1.2257978916168213
I0310 00:23:02.146722 139708415854336 logging_writer.py:48] [322500] global_step=322500, grad_norm=3.2474236488342285, loss=1.1126285791397095
I0310 00:23:47.589873 139708407461632 logging_writer.py:48] [322600] global_step=322600, grad_norm=3.0175836086273193, loss=1.1612331867218018
I0310 00:24:33.174647 139708415854336 logging_writer.py:48] [322700] global_step=322700, grad_norm=3.182460308074951, loss=1.16512930393219
I0310 00:25:18.951198 139708407461632 logging_writer.py:48] [322800] global_step=322800, grad_norm=3.363901376724243, loss=2.896369695663452
I0310 00:26:04.348116 139708415854336 logging_writer.py:48] [322900] global_step=322900, grad_norm=3.354630470275879, loss=1.118319034576416
I0310 00:26:49.691877 139708407461632 logging_writer.py:48] [323000] global_step=323000, grad_norm=3.185636043548584, loss=1.4069862365722656
I0310 00:27:35.152710 139708415854336 logging_writer.py:48] [323100] global_step=323100, grad_norm=3.211043357849121, loss=1.210914969444275
I0310 00:27:40.263181 139902746892096 spec.py:321] Evaluating on the training split.
I0310 00:27:51.870976 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 00:28:14.696817 139902746892096 spec.py:349] Evaluating on the test split.
I0310 00:28:16.369799 139902746892096 submission_runner.py:411] Time since start: 156419.40s, 	Step: 323113, 	{'train/accuracy': 0.8874218463897705, 'train/loss': 0.41405174136161804, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 144562.77267432213, 'total_duration': 156419.39860010147, 'accumulated_submission_time': 144562.77267432213, 'accumulated_eval_time': 11819.779586553574, 'accumulated_logging_time': 19.89247989654541}
I0310 00:28:16.435859 139708407461632 logging_writer.py:48] [323113] accumulated_eval_time=11819.779587, accumulated_logging_time=19.892480, accumulated_submission_time=144562.772674, global_step=323113, preemption_count=0, score=144562.772674, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=156419.398600, train/accuracy=0.887422, train/loss=0.414052, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 00:28:51.054838 139708415854336 logging_writer.py:48] [323200] global_step=323200, grad_norm=3.1856882572174072, loss=1.221361517906189
I0310 00:29:36.210502 139708407461632 logging_writer.py:48] [323300] global_step=323300, grad_norm=3.4859657287597656, loss=2.901524066925049
I0310 00:30:21.645543 139708415854336 logging_writer.py:48] [323400] global_step=323400, grad_norm=3.003023624420166, loss=1.1916069984436035
I0310 00:31:07.006126 139708407461632 logging_writer.py:48] [323500] global_step=323500, grad_norm=3.2447736263275146, loss=1.818740725517273
I0310 00:31:52.106146 139708415854336 logging_writer.py:48] [323600] global_step=323600, grad_norm=3.2315425872802734, loss=1.085146188735962
I0310 00:32:37.365074 139708407461632 logging_writer.py:48] [323700] global_step=323700, grad_norm=3.251016616821289, loss=1.1257457733154297
I0310 00:33:22.555182 139708415854336 logging_writer.py:48] [323800] global_step=323800, grad_norm=2.9447667598724365, loss=1.47674560546875
I0310 00:34:07.684048 139708407461632 logging_writer.py:48] [323900] global_step=323900, grad_norm=3.3347158432006836, loss=2.8464839458465576
I0310 00:34:52.942586 139708415854336 logging_writer.py:48] [324000] global_step=324000, grad_norm=4.089286804199219, loss=3.195049524307251
I0310 00:35:16.787572 139902746892096 spec.py:321] Evaluating on the training split.
I0310 00:35:27.987063 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 00:35:49.354830 139902746892096 spec.py:349] Evaluating on the test split.
I0310 00:35:51.025773 139902746892096 submission_runner.py:411] Time since start: 156874.05s, 	Step: 324054, 	{'train/accuracy': 0.8886327743530273, 'train/loss': 0.416003555059433, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 144983.06563448906, 'total_duration': 156874.05456399918, 'accumulated_submission_time': 144983.06563448906, 'accumulated_eval_time': 11854.017767190933, 'accumulated_logging_time': 19.968931913375854}
I0310 00:35:51.089403 139708407461632 logging_writer.py:48] [324054] accumulated_eval_time=11854.017767, accumulated_logging_time=19.968932, accumulated_submission_time=144983.065634, global_step=324054, preemption_count=0, score=144983.065634, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=156874.054564, train/accuracy=0.888633, train/loss=0.416004, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 00:36:09.570131 139708415854336 logging_writer.py:48] [324100] global_step=324100, grad_norm=3.097102165222168, loss=1.28524911403656
I0310 00:36:51.855411 139708407461632 logging_writer.py:48] [324200] global_step=324200, grad_norm=4.7640156745910645, loss=1.1680561304092407
I0310 00:37:37.442551 139708415854336 logging_writer.py:48] [324300] global_step=324300, grad_norm=3.0110745429992676, loss=1.2089664936065674
I0310 00:38:23.324704 139708407461632 logging_writer.py:48] [324400] global_step=324400, grad_norm=3.163731336593628, loss=1.136315941810608
I0310 00:39:08.446750 139708415854336 logging_writer.py:48] [324500] global_step=324500, grad_norm=3.1788017749786377, loss=1.1107418537139893
I0310 00:39:53.974702 139708407461632 logging_writer.py:48] [324600] global_step=324600, grad_norm=2.716538667678833, loss=1.789463758468628
I0310 00:40:39.180169 139708415854336 logging_writer.py:48] [324700] global_step=324700, grad_norm=3.2011590003967285, loss=1.1436249017715454
I0310 00:41:24.446949 139708407461632 logging_writer.py:48] [324800] global_step=324800, grad_norm=3.8688013553619385, loss=3.123091459274292
I0310 00:42:09.987043 139708415854336 logging_writer.py:48] [324900] global_step=324900, grad_norm=2.838756561279297, loss=1.6227335929870605
I0310 00:42:51.111684 139902746892096 spec.py:321] Evaluating on the training split.
I0310 00:43:02.520287 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 00:43:25.712121 139902746892096 spec.py:349] Evaluating on the test split.
I0310 00:43:27.389130 139902746892096 submission_runner.py:411] Time since start: 157330.42s, 	Step: 324993, 	{'train/accuracy': 0.890429675579071, 'train/loss': 0.413343220949173, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 145403.02961182594, 'total_duration': 157330.41791248322, 'accumulated_submission_time': 145403.02961182594, 'accumulated_eval_time': 11890.295197963715, 'accumulated_logging_time': 20.042044162750244}
I0310 00:43:27.465153 139708407461632 logging_writer.py:48] [324993] accumulated_eval_time=11890.295198, accumulated_logging_time=20.042044, accumulated_submission_time=145403.029612, global_step=324993, preemption_count=0, score=145403.029612, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=157330.417912, train/accuracy=0.890430, train/loss=0.413343, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 00:43:30.615479 139708415854336 logging_writer.py:48] [325000] global_step=325000, grad_norm=3.9415061473846436, loss=2.1594178676605225
I0310 00:44:11.069381 139708407461632 logging_writer.py:48] [325100] global_step=325100, grad_norm=3.714625120162964, loss=3.248886823654175
I0310 00:44:56.347562 139708415854336 logging_writer.py:48] [325200] global_step=325200, grad_norm=3.117993116378784, loss=1.1604746580123901
I0310 00:45:41.832005 139708407461632 logging_writer.py:48] [325300] global_step=325300, grad_norm=2.9196321964263916, loss=1.4514961242675781
I0310 00:46:27.399304 139708415854336 logging_writer.py:48] [325400] global_step=325400, grad_norm=2.833832025527954, loss=2.0607428550720215
I0310 00:47:12.430094 139708407461632 logging_writer.py:48] [325500] global_step=325500, grad_norm=3.928797483444214, loss=3.3179049491882324
I0310 00:47:58.085779 139708415854336 logging_writer.py:48] [325600] global_step=325600, grad_norm=3.192023992538452, loss=2.5519986152648926
I0310 00:48:43.373700 139708407461632 logging_writer.py:48] [325700] global_step=325700, grad_norm=3.1412901878356934, loss=2.3588526248931885
I0310 00:49:28.734670 139708415854336 logging_writer.py:48] [325800] global_step=325800, grad_norm=2.8714709281921387, loss=2.221179962158203
I0310 00:50:14.072290 139708407461632 logging_writer.py:48] [325900] global_step=325900, grad_norm=3.1684701442718506, loss=1.1796473264694214
I0310 00:50:27.744804 139902746892096 spec.py:321] Evaluating on the training split.
I0310 00:50:39.260315 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 00:51:03.065672 139902746892096 spec.py:349] Evaluating on the test split.
I0310 00:51:04.740354 139902746892096 submission_runner.py:411] Time since start: 157787.77s, 	Step: 325932, 	{'train/accuracy': 0.8891210556030273, 'train/loss': 0.4105064570903778, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 145823.25002264977, 'total_duration': 157787.76915454865, 'accumulated_submission_time': 145823.25002264977, 'accumulated_eval_time': 11927.290768623352, 'accumulated_logging_time': 20.128324270248413}
I0310 00:51:04.805685 139708415854336 logging_writer.py:48] [325932] accumulated_eval_time=11927.290769, accumulated_logging_time=20.128324, accumulated_submission_time=145823.250023, global_step=325932, preemption_count=0, score=145823.250023, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=157787.769155, train/accuracy=0.889121, train/loss=0.410506, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 00:51:32.066764 139708407461632 logging_writer.py:48] [326000] global_step=326000, grad_norm=3.003444194793701, loss=1.0857305526733398
I0310 00:52:15.856693 139708415854336 logging_writer.py:48] [326100] global_step=326100, grad_norm=3.0221078395843506, loss=1.1003190279006958
I0310 00:53:01.349830 139708407461632 logging_writer.py:48] [326200] global_step=326200, grad_norm=3.376309871673584, loss=1.1971032619476318
I0310 00:53:46.833652 139708415854336 logging_writer.py:48] [326300] global_step=326300, grad_norm=2.994328498840332, loss=1.0398578643798828
I0310 00:54:32.274702 139708407461632 logging_writer.py:48] [326400] global_step=326400, grad_norm=3.1177656650543213, loss=1.1155575513839722
I0310 00:55:17.273402 139708415854336 logging_writer.py:48] [326500] global_step=326500, grad_norm=3.172104597091675, loss=2.7316718101501465
I0310 00:56:03.047521 139708407461632 logging_writer.py:48] [326600] global_step=326600, grad_norm=3.22835111618042, loss=2.6887640953063965
I0310 00:56:48.231117 139708415854336 logging_writer.py:48] [326700] global_step=326700, grad_norm=3.4965524673461914, loss=1.1485662460327148
I0310 00:57:33.742033 139708407461632 logging_writer.py:48] [326800] global_step=326800, grad_norm=3.6718029975891113, loss=2.6016428470611572
I0310 00:58:04.913938 139902746892096 spec.py:321] Evaluating on the training split.
I0310 00:58:16.440044 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 00:58:38.697386 139902746892096 spec.py:349] Evaluating on the test split.
I0310 00:58:40.369697 139902746892096 submission_runner.py:411] Time since start: 158243.40s, 	Step: 326870, 	{'train/accuracy': 0.8895702958106995, 'train/loss': 0.40923407673835754, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 146243.3017590046, 'total_duration': 158243.3984901905, 'accumulated_submission_time': 146243.3017590046, 'accumulated_eval_time': 11962.746523618698, 'accumulated_logging_time': 20.202043771743774}
I0310 00:58:40.433492 139708415854336 logging_writer.py:48] [326870] accumulated_eval_time=11962.746524, accumulated_logging_time=20.202044, accumulated_submission_time=146243.301759, global_step=326870, preemption_count=0, score=146243.301759, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=158243.398490, train/accuracy=0.889570, train/loss=0.409234, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 00:58:52.633623 139708407461632 logging_writer.py:48] [326900] global_step=326900, grad_norm=3.2220871448516846, loss=1.1622974872589111
I0310 00:59:34.315933 139708415854336 logging_writer.py:48] [327000] global_step=327000, grad_norm=3.217750072479248, loss=1.4041080474853516
I0310 01:00:19.814471 139708407461632 logging_writer.py:48] [327100] global_step=327100, grad_norm=3.3001606464385986, loss=1.543511152267456
I0310 01:01:05.661965 139708415854336 logging_writer.py:48] [327200] global_step=327200, grad_norm=3.1125152111053467, loss=1.1148432493209839
I0310 01:01:50.805511 139708407461632 logging_writer.py:48] [327300] global_step=327300, grad_norm=3.453916549682617, loss=2.87636661529541
I0310 01:02:36.204714 139708415854336 logging_writer.py:48] [327400] global_step=327400, grad_norm=3.172888994216919, loss=1.1164575815200806
I0310 01:03:21.454078 139708407461632 logging_writer.py:48] [327500] global_step=327500, grad_norm=2.976005792617798, loss=1.1538398265838623
I0310 01:04:06.921498 139708415854336 logging_writer.py:48] [327600] global_step=327600, grad_norm=2.994178295135498, loss=1.58831787109375
I0310 01:04:52.234003 139708407461632 logging_writer.py:48] [327700] global_step=327700, grad_norm=2.986759901046753, loss=1.0002226829528809
I0310 01:05:37.999186 139708415854336 logging_writer.py:48] [327800] global_step=327800, grad_norm=3.6510353088378906, loss=3.2904751300811768
I0310 01:05:40.531746 139902746892096 spec.py:321] Evaluating on the training split.
I0310 01:05:52.667418 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 01:06:14.822778 139902746892096 spec.py:349] Evaluating on the test split.
I0310 01:06:16.504931 139902746892096 submission_runner.py:411] Time since start: 158699.53s, 	Step: 327807, 	{'train/accuracy': 0.88720703125, 'train/loss': 0.4160758852958679, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 146663.34315609932, 'total_duration': 158699.53372955322, 'accumulated_submission_time': 146663.34315609932, 'accumulated_eval_time': 11998.719693899155, 'accumulated_logging_time': 20.274394273757935}
I0310 01:06:16.572362 139708407461632 logging_writer.py:48] [327807] accumulated_eval_time=11998.719694, accumulated_logging_time=20.274394, accumulated_submission_time=146663.343156, global_step=327807, preemption_count=0, score=146663.343156, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=158699.533730, train/accuracy=0.887207, train/loss=0.416076, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 01:06:53.973907 139708415854336 logging_writer.py:48] [327900] global_step=327900, grad_norm=3.10140323638916, loss=1.382251501083374
I0310 01:07:39.586195 139708407461632 logging_writer.py:48] [328000] global_step=328000, grad_norm=3.136552095413208, loss=1.0904569625854492
I0310 01:08:25.334554 139708415854336 logging_writer.py:48] [328100] global_step=328100, grad_norm=3.350571632385254, loss=1.291779637336731
I0310 01:09:10.896180 139708407461632 logging_writer.py:48] [328200] global_step=328200, grad_norm=3.6182572841644287, loss=2.987290620803833
I0310 01:09:56.256050 139708415854336 logging_writer.py:48] [328300] global_step=328300, grad_norm=2.857116460800171, loss=1.7285654544830322
I0310 01:10:41.556678 139708407461632 logging_writer.py:48] [328400] global_step=328400, grad_norm=3.1562106609344482, loss=1.2298771142959595
I0310 01:11:26.750802 139708415854336 logging_writer.py:48] [328500] global_step=328500, grad_norm=3.648092746734619, loss=1.6618353128433228
I0310 01:12:12.165317 139708407461632 logging_writer.py:48] [328600] global_step=328600, grad_norm=3.229823589324951, loss=2.6021130084991455
I0310 01:12:57.467128 139708415854336 logging_writer.py:48] [328700] global_step=328700, grad_norm=2.8566367626190186, loss=1.1012760400772095
I0310 01:13:16.709494 139902746892096 spec.py:321] Evaluating on the training split.
I0310 01:13:28.263711 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 01:13:49.906212 139902746892096 spec.py:349] Evaluating on the test split.
I0310 01:13:51.589088 139902746892096 submission_runner.py:411] Time since start: 159154.62s, 	Step: 328744, 	{'train/accuracy': 0.8877929449081421, 'train/loss': 0.4134314954280853, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 147083.42253422737, 'total_duration': 159154.61788129807, 'accumulated_submission_time': 147083.42253422737, 'accumulated_eval_time': 12033.599283218384, 'accumulated_logging_time': 20.351003408432007}
I0310 01:13:51.654181 139708407461632 logging_writer.py:48] [328744] accumulated_eval_time=12033.599283, accumulated_logging_time=20.351003, accumulated_submission_time=147083.422534, global_step=328744, preemption_count=0, score=147083.422534, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=159154.617881, train/accuracy=0.887793, train/loss=0.413431, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 01:14:14.082956 139708415854336 logging_writer.py:48] [328800] global_step=328800, grad_norm=3.2503795623779297, loss=2.6771860122680664
I0310 01:14:57.806907 139708407461632 logging_writer.py:48] [328900] global_step=328900, grad_norm=3.0808005332946777, loss=1.1824824810028076
I0310 01:15:42.990113 139708415854336 logging_writer.py:48] [329000] global_step=329000, grad_norm=3.083509683609009, loss=1.3818814754486084
I0310 01:16:29.099172 139708407461632 logging_writer.py:48] [329100] global_step=329100, grad_norm=3.3249571323394775, loss=1.1545758247375488
I0310 01:17:14.370474 139708415854336 logging_writer.py:48] [329200] global_step=329200, grad_norm=3.052145481109619, loss=1.1580190658569336
I0310 01:17:59.602861 139708407461632 logging_writer.py:48] [329300] global_step=329300, grad_norm=2.9527876377105713, loss=1.191101312637329
I0310 01:18:44.966691 139708415854336 logging_writer.py:48] [329400] global_step=329400, grad_norm=3.191117525100708, loss=1.1246534585952759
I0310 01:19:30.126309 139708407461632 logging_writer.py:48] [329500] global_step=329500, grad_norm=3.0396523475646973, loss=1.1822645664215088
I0310 01:20:15.533761 139708415854336 logging_writer.py:48] [329600] global_step=329600, grad_norm=3.6027634143829346, loss=2.853928565979004
I0310 01:20:51.866296 139902746892096 spec.py:321] Evaluating on the training split.
I0310 01:21:03.341552 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 01:21:24.592398 139902746892096 spec.py:349] Evaluating on the test split.
I0310 01:21:26.276436 139902746892096 submission_runner.py:411] Time since start: 159609.31s, 	Step: 329682, 	{'train/accuracy': 0.8869140148162842, 'train/loss': 0.4140605926513672, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 147503.57654500008, 'total_duration': 159609.30521917343, 'accumulated_submission_time': 147503.57654500008, 'accumulated_eval_time': 12068.009401798248, 'accumulated_logging_time': 20.425606727600098}
I0310 01:21:26.344252 139708407461632 logging_writer.py:48] [329682] accumulated_eval_time=12068.009402, accumulated_logging_time=20.425607, accumulated_submission_time=147503.576545, global_step=329682, preemption_count=0, score=147503.576545, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=159609.305219, train/accuracy=0.886914, train/loss=0.414061, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 01:21:33.813622 139708415854336 logging_writer.py:48] [329700] global_step=329700, grad_norm=3.0397400856018066, loss=2.1939940452575684
I0310 01:22:15.180622 139708407461632 logging_writer.py:48] [329800] global_step=329800, grad_norm=3.35758376121521, loss=1.1197121143341064
I0310 01:23:00.248251 139708415854336 logging_writer.py:48] [329900] global_step=329900, grad_norm=3.198228120803833, loss=1.2001924514770508
I0310 01:23:45.614727 139708407461632 logging_writer.py:48] [330000] global_step=330000, grad_norm=3.0109026432037354, loss=1.0925594568252563
I0310 01:24:32.123581 139708415854336 logging_writer.py:48] [330100] global_step=330100, grad_norm=3.3784120082855225, loss=1.4105883836746216
I0310 01:25:17.246576 139708407461632 logging_writer.py:48] [330200] global_step=330200, grad_norm=2.9908933639526367, loss=1.8636302947998047
I0310 01:26:02.742032 139708415854336 logging_writer.py:48] [330300] global_step=330300, grad_norm=2.8621504306793213, loss=0.9641519784927368
I0310 01:26:47.966704 139708407461632 logging_writer.py:48] [330400] global_step=330400, grad_norm=3.397540807723999, loss=1.5405046939849854
I0310 01:27:33.326463 139708415854336 logging_writer.py:48] [330500] global_step=330500, grad_norm=3.010772705078125, loss=2.0971364974975586
I0310 01:28:18.537977 139708407461632 logging_writer.py:48] [330600] global_step=330600, grad_norm=3.6452717781066895, loss=3.111612319946289
I0310 01:28:26.347951 139902746892096 spec.py:321] Evaluating on the training split.
I0310 01:28:37.897138 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 01:29:00.621581 139902746892096 spec.py:349] Evaluating on the test split.
I0310 01:29:02.296925 139902746892096 submission_runner.py:411] Time since start: 160065.33s, 	Step: 330619, 	{'train/accuracy': 0.890429675579071, 'train/loss': 0.40680992603302, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 147923.52029657364, 'total_duration': 160065.32572340965, 'accumulated_submission_time': 147923.52029657364, 'accumulated_eval_time': 12103.958374977112, 'accumulated_logging_time': 20.504555225372314}
I0310 01:29:02.361421 139708415854336 logging_writer.py:48] [330619] accumulated_eval_time=12103.958375, accumulated_logging_time=20.504555, accumulated_submission_time=147923.520297, global_step=330619, preemption_count=0, score=147923.520297, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=160065.325723, train/accuracy=0.890430, train/loss=0.406810, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 01:29:34.593836 139708407461632 logging_writer.py:48] [330700] global_step=330700, grad_norm=3.2007319927215576, loss=1.1060138940811157
I0310 01:30:19.111835 139708415854336 logging_writer.py:48] [330800] global_step=330800, grad_norm=3.4152185916900635, loss=1.0739929676055908
I0310 01:31:04.427700 139708407461632 logging_writer.py:48] [330900] global_step=330900, grad_norm=3.0405046939849854, loss=1.2453635931015015
I0310 01:31:49.703824 139708415854336 logging_writer.py:48] [331000] global_step=331000, grad_norm=3.328996181488037, loss=1.113634467124939
I0310 01:32:34.844664 139708407461632 logging_writer.py:48] [331100] global_step=331100, grad_norm=3.469697952270508, loss=2.999325752258301
I0310 01:33:20.239679 139708415854336 logging_writer.py:48] [331200] global_step=331200, grad_norm=3.0844340324401855, loss=2.4783148765563965
I0310 01:34:05.531825 139708407461632 logging_writer.py:48] [331300] global_step=331300, grad_norm=3.0224735736846924, loss=1.0861833095550537
I0310 01:34:50.797913 139708415854336 logging_writer.py:48] [331400] global_step=331400, grad_norm=3.0939929485321045, loss=1.2133214473724365
I0310 01:35:36.023480 139708407461632 logging_writer.py:48] [331500] global_step=331500, grad_norm=3.350102663040161, loss=1.2162529230117798
I0310 01:36:02.552346 139902746892096 spec.py:321] Evaluating on the training split.
I0310 01:36:13.855739 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 01:36:34.458587 139902746892096 spec.py:349] Evaluating on the test split.
I0310 01:36:36.139991 139902746892096 submission_runner.py:411] Time since start: 160519.17s, 	Step: 331560, 	{'train/accuracy': 0.8872460722923279, 'train/loss': 0.4223819673061371, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 148343.6543688774, 'total_duration': 160519.1687822342, 'accumulated_submission_time': 148343.6543688774, 'accumulated_eval_time': 12137.546013832092, 'accumulated_logging_time': 20.577284812927246}
I0310 01:36:36.210338 139708415854336 logging_writer.py:48] [331560] accumulated_eval_time=12137.546014, accumulated_logging_time=20.577285, accumulated_submission_time=148343.654369, global_step=331560, preemption_count=0, score=148343.654369, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=160519.168782, train/accuracy=0.887246, train/loss=0.422382, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 01:36:52.344755 139708407461632 logging_writer.py:48] [331600] global_step=331600, grad_norm=3.0721285343170166, loss=1.6644266843795776
I0310 01:37:35.116969 139708415854336 logging_writer.py:48] [331700] global_step=331700, grad_norm=3.1356360912323, loss=2.193112850189209
I0310 01:38:20.576302 139708407461632 logging_writer.py:48] [331800] global_step=331800, grad_norm=3.326897621154785, loss=2.8117077350616455
I0310 01:39:06.071154 139708415854336 logging_writer.py:48] [331900] global_step=331900, grad_norm=2.9863815307617188, loss=1.0247724056243896
I0310 01:39:51.293250 139708407461632 logging_writer.py:48] [332000] global_step=332000, grad_norm=3.644808053970337, loss=3.07285475730896
I0310 01:40:36.718430 139708415854336 logging_writer.py:48] [332100] global_step=332100, grad_norm=3.62432861328125, loss=3.155665397644043
I0310 01:41:22.143452 139708407461632 logging_writer.py:48] [332200] global_step=332200, grad_norm=3.1234560012817383, loss=1.1101185083389282
I0310 01:42:07.546169 139708415854336 logging_writer.py:48] [332300] global_step=332300, grad_norm=3.5754613876342773, loss=3.2112975120544434
I0310 01:42:53.115532 139708407461632 logging_writer.py:48] [332400] global_step=332400, grad_norm=2.768266439437866, loss=1.5470654964447021
I0310 01:43:36.331120 139902746892096 spec.py:321] Evaluating on the training split.
I0310 01:43:47.926056 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 01:44:10.570652 139902746892096 spec.py:349] Evaluating on the test split.
I0310 01:44:12.239987 139902746892096 submission_runner.py:411] Time since start: 160975.27s, 	Step: 332497, 	{'train/accuracy': 0.88783198595047, 'train/loss': 0.4149302840232849, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 148763.71720457077, 'total_duration': 160975.2687857151, 'accumulated_submission_time': 148763.71720457077, 'accumulated_eval_time': 12173.454895019531, 'accumulated_logging_time': 20.656660318374634}
I0310 01:44:12.305328 139708415854336 logging_writer.py:48] [332497] accumulated_eval_time=12173.454895, accumulated_logging_time=20.656660, accumulated_submission_time=148763.717205, global_step=332497, preemption_count=0, score=148763.717205, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=160975.268786, train/accuracy=0.887832, train/loss=0.414930, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 01:44:13.880275 139708407461632 logging_writer.py:48] [332500] global_step=332500, grad_norm=4.257015705108643, loss=3.2039313316345215
I0310 01:44:54.444339 139708415854336 logging_writer.py:48] [332600] global_step=332600, grad_norm=3.043030023574829, loss=1.1021363735198975
I0310 01:45:39.443545 139708407461632 logging_writer.py:48] [332700] global_step=332700, grad_norm=3.1597707271575928, loss=1.1854151487350464
I0310 01:46:25.391672 139708415854336 logging_writer.py:48] [332800] global_step=332800, grad_norm=2.7789251804351807, loss=1.4707838296890259
I0310 01:47:10.914889 139708407461632 logging_writer.py:48] [332900] global_step=332900, grad_norm=3.1430442333221436, loss=2.619995355606079
I0310 01:47:56.291614 139708415854336 logging_writer.py:48] [333000] global_step=333000, grad_norm=3.0915937423706055, loss=1.1654131412506104
I0310 01:48:41.702648 139708407461632 logging_writer.py:48] [333100] global_step=333100, grad_norm=3.0339272022247314, loss=1.315340280532837
I0310 01:49:26.956577 139708415854336 logging_writer.py:48] [333200] global_step=333200, grad_norm=3.0098512172698975, loss=1.997846245765686
I0310 01:50:12.465225 139708407461632 logging_writer.py:48] [333300] global_step=333300, grad_norm=3.1806111335754395, loss=2.6791718006134033
I0310 01:50:57.635576 139708415854336 logging_writer.py:48] [333400] global_step=333400, grad_norm=3.1106069087982178, loss=1.5891526937484741
I0310 01:51:12.575390 139902746892096 spec.py:321] Evaluating on the training split.
I0310 01:51:23.965065 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 01:51:45.990920 139902746892096 spec.py:349] Evaluating on the test split.
I0310 01:51:47.668062 139902746892096 submission_runner.py:411] Time since start: 161430.70s, 	Step: 333435, 	{'train/accuracy': 0.8897656202316284, 'train/loss': 0.40816745162010193, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 149183.92890954018, 'total_duration': 161430.69685649872, 'accumulated_submission_time': 149183.92890954018, 'accumulated_eval_time': 12208.547565460205, 'accumulated_logging_time': 20.73137640953064}
I0310 01:51:47.732604 139708407461632 logging_writer.py:48] [333435] accumulated_eval_time=12208.547565, accumulated_logging_time=20.731376, accumulated_submission_time=149183.928910, global_step=333435, preemption_count=0, score=149183.928910, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=161430.696856, train/accuracy=0.889766, train/loss=0.408167, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 01:52:13.835106 139708415854336 logging_writer.py:48] [333500] global_step=333500, grad_norm=3.0768661499023438, loss=1.2405695915222168
I0310 01:52:57.865005 139708407461632 logging_writer.py:48] [333600] global_step=333600, grad_norm=3.1083791255950928, loss=2.3591766357421875
I0310 01:53:43.229392 139708415854336 logging_writer.py:48] [333700] global_step=333700, grad_norm=2.9295103549957275, loss=1.5881493091583252
I0310 01:54:28.517967 139708407461632 logging_writer.py:48] [333800] global_step=333800, grad_norm=3.02783203125, loss=1.106845498085022
I0310 01:55:13.889554 139708415854336 logging_writer.py:48] [333900] global_step=333900, grad_norm=3.2617738246917725, loss=1.0741262435913086
I0310 01:55:59.186772 139708407461632 logging_writer.py:48] [334000] global_step=334000, grad_norm=2.9709057807922363, loss=2.1409127712249756
I0310 01:56:44.661713 139708415854336 logging_writer.py:48] [334100] global_step=334100, grad_norm=2.9305760860443115, loss=1.4923453330993652
I0310 01:57:30.062753 139708407461632 logging_writer.py:48] [334200] global_step=334200, grad_norm=3.487696409225464, loss=2.933779239654541
I0310 01:58:15.576120 139708415854336 logging_writer.py:48] [334300] global_step=334300, grad_norm=3.2745838165283203, loss=1.1724284887313843
I0310 01:58:47.956624 139902746892096 spec.py:321] Evaluating on the training split.
I0310 01:58:59.363248 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 01:59:21.195762 139902746892096 spec.py:349] Evaluating on the test split.
I0310 01:59:22.873861 139902746892096 submission_runner.py:411] Time since start: 161885.90s, 	Step: 334373, 	{'train/accuracy': 0.8862499594688416, 'train/loss': 0.4207873046398163, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 149604.09540700912, 'total_duration': 161885.90264344215, 'accumulated_submission_time': 149604.09540700912, 'accumulated_eval_time': 12243.464797735214, 'accumulated_logging_time': 20.8051335811615}
I0310 01:59:22.949856 139708407461632 logging_writer.py:48] [334373] accumulated_eval_time=12243.464798, accumulated_logging_time=20.805134, accumulated_submission_time=149604.095407, global_step=334373, preemption_count=0, score=149604.095407, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=161885.902643, train/accuracy=0.886250, train/loss=0.420787, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 01:59:33.970187 139708415854336 logging_writer.py:48] [334400] global_step=334400, grad_norm=3.0132551193237305, loss=1.7991008758544922
I0310 02:00:15.851777 139708407461632 logging_writer.py:48] [334500] global_step=334500, grad_norm=2.7254366874694824, loss=1.8303152322769165
I0310 02:01:01.216095 139708415854336 logging_writer.py:48] [334600] global_step=334600, grad_norm=3.178373098373413, loss=1.57106614112854
I0310 02:01:46.972572 139708407461632 logging_writer.py:48] [334700] global_step=334700, grad_norm=2.85299015045166, loss=2.0576231479644775
I0310 02:02:32.181677 139708415854336 logging_writer.py:48] [334800] global_step=334800, grad_norm=3.8818588256835938, loss=1.1926872730255127
I0310 02:03:17.424690 139708407461632 logging_writer.py:48] [334900] global_step=334900, grad_norm=3.108090400695801, loss=2.5025784969329834
I0310 02:04:02.803084 139708415854336 logging_writer.py:48] [335000] global_step=335000, grad_norm=3.9522345066070557, loss=3.246903657913208
I0310 02:04:47.994271 139708407461632 logging_writer.py:48] [335100] global_step=335100, grad_norm=2.915807008743286, loss=2.122537612915039
I0310 02:05:33.479335 139708415854336 logging_writer.py:48] [335200] global_step=335200, grad_norm=3.46560001373291, loss=3.0698294639587402
I0310 02:06:18.929379 139708407461632 logging_writer.py:48] [335300] global_step=335300, grad_norm=3.600764274597168, loss=3.157421588897705
I0310 02:06:23.217966 139902746892096 spec.py:321] Evaluating on the training split.
I0310 02:06:34.674046 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 02:06:55.405991 139902746892096 spec.py:349] Evaluating on the test split.
I0310 02:06:57.094890 139902746892096 submission_runner.py:411] Time since start: 162340.12s, 	Step: 335311, 	{'train/accuracy': 0.8875390291213989, 'train/loss': 0.4163751006126404, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 150024.3045117855, 'total_duration': 162340.12366628647, 'accumulated_submission_time': 150024.3045117855, 'accumulated_eval_time': 12277.341688632965, 'accumulated_logging_time': 20.890724897384644}
I0310 02:06:57.165470 139708415854336 logging_writer.py:48] [335311] accumulated_eval_time=12277.341689, accumulated_logging_time=20.890725, accumulated_submission_time=150024.304512, global_step=335311, preemption_count=0, score=150024.304512, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=162340.123666, train/accuracy=0.887539, train/loss=0.416375, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 02:07:32.964250 139708407461632 logging_writer.py:48] [335400] global_step=335400, grad_norm=2.8806912899017334, loss=2.3490352630615234
I0310 02:08:18.084590 139708415854336 logging_writer.py:48] [335500] global_step=335500, grad_norm=3.127288579940796, loss=1.1104273796081543
I0310 02:09:03.621260 139708407461632 logging_writer.py:48] [335600] global_step=335600, grad_norm=3.274799108505249, loss=1.8524664640426636
I0310 02:09:48.854138 139708415854336 logging_writer.py:48] [335700] global_step=335700, grad_norm=2.967543840408325, loss=1.1204248666763306
I0310 02:10:34.238130 139708407461632 logging_writer.py:48] [335800] global_step=335800, grad_norm=3.025564670562744, loss=1.2030413150787354
I0310 02:11:19.646614 139708415854336 logging_writer.py:48] [335900] global_step=335900, grad_norm=3.380873203277588, loss=1.1929430961608887
I0310 02:12:04.904320 139708407461632 logging_writer.py:48] [336000] global_step=336000, grad_norm=3.4515388011932373, loss=1.3540748357772827
I0310 02:12:50.484136 139708415854336 logging_writer.py:48] [336100] global_step=336100, grad_norm=3.3254947662353516, loss=1.3655941486358643
I0310 02:13:35.708723 139708407461632 logging_writer.py:48] [336200] global_step=336200, grad_norm=3.1838300228118896, loss=1.994606852531433
I0310 02:13:57.446002 139902746892096 spec.py:321] Evaluating on the training split.
I0310 02:14:08.864180 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 02:14:30.159309 139902746892096 spec.py:349] Evaluating on the test split.
I0310 02:14:31.831029 139902746892096 submission_runner.py:411] Time since start: 162794.86s, 	Step: 336250, 	{'train/accuracy': 0.8867382407188416, 'train/loss': 0.4137430191040039, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 150444.52754735947, 'total_duration': 162794.85982775688, 'accumulated_submission_time': 150444.52754735947, 'accumulated_eval_time': 12311.726724147797, 'accumulated_logging_time': 20.969918489456177}
I0310 02:14:31.895996 139708415854336 logging_writer.py:48] [336250] accumulated_eval_time=12311.726724, accumulated_logging_time=20.969918, accumulated_submission_time=150444.527547, global_step=336250, preemption_count=0, score=150444.527547, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=162794.859828, train/accuracy=0.886738, train/loss=0.413743, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 02:14:51.986699 139708407461632 logging_writer.py:48] [336300] global_step=336300, grad_norm=2.9896016120910645, loss=1.3763023614883423
I0310 02:15:34.786518 139708415854336 logging_writer.py:48] [336400] global_step=336400, grad_norm=3.014570713043213, loss=1.6810873746871948
I0310 02:16:20.046799 139708407461632 logging_writer.py:48] [336500] global_step=336500, grad_norm=3.5535645484924316, loss=2.7121694087982178
I0310 02:17:05.761456 139708415854336 logging_writer.py:48] [336600] global_step=336600, grad_norm=3.1690235137939453, loss=2.8142101764678955
I0310 02:17:51.211515 139708407461632 logging_writer.py:48] [336700] global_step=336700, grad_norm=3.0403172969818115, loss=1.1901642084121704
I0310 02:18:36.558040 139708415854336 logging_writer.py:48] [336800] global_step=336800, grad_norm=3.1797678470611572, loss=1.1349844932556152
I0310 02:19:21.834424 139708407461632 logging_writer.py:48] [336900] global_step=336900, grad_norm=3.0550131797790527, loss=1.130913257598877
I0310 02:20:07.072589 139708415854336 logging_writer.py:48] [337000] global_step=337000, grad_norm=2.764563798904419, loss=2.0474181175231934
I0310 02:20:52.676057 139708407461632 logging_writer.py:48] [337100] global_step=337100, grad_norm=2.986974000930786, loss=1.147872805595398
I0310 02:21:32.324867 139902746892096 spec.py:321] Evaluating on the training split.
I0310 02:21:43.603532 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 02:22:04.289333 139902746892096 spec.py:349] Evaluating on the test split.
I0310 02:22:05.969946 139902746892096 submission_runner.py:411] Time since start: 163249.00s, 	Step: 337189, 	{'train/accuracy': 0.888476550579071, 'train/loss': 0.4186500012874603, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 150864.89817857742, 'total_duration': 163248.99874138832, 'accumulated_submission_time': 150864.89817857742, 'accumulated_eval_time': 12345.37182021141, 'accumulated_logging_time': 21.044143676757812}
I0310 02:22:06.037417 139708415854336 logging_writer.py:48] [337189] accumulated_eval_time=12345.371820, accumulated_logging_time=21.044144, accumulated_submission_time=150864.898179, global_step=337189, preemption_count=0, score=150864.898179, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=163248.998741, train/accuracy=0.888477, train/loss=0.418650, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 02:22:10.759527 139708407461632 logging_writer.py:48] [337200] global_step=337200, grad_norm=3.289970874786377, loss=2.246769905090332
I0310 02:22:51.410950 139708415854336 logging_writer.py:48] [337300] global_step=337300, grad_norm=3.1192727088928223, loss=2.562119960784912
I0310 02:23:36.390561 139708407461632 logging_writer.py:48] [337400] global_step=337400, grad_norm=3.178426742553711, loss=1.125673532485962
I0310 02:24:21.670909 139708415854336 logging_writer.py:48] [337500] global_step=337500, grad_norm=3.0609359741210938, loss=1.2519029378890991
I0310 02:25:07.208648 139708407461632 logging_writer.py:48] [337600] global_step=337600, grad_norm=2.6859781742095947, loss=1.235040545463562
I0310 02:25:52.365995 139708415854336 logging_writer.py:48] [337700] global_step=337700, grad_norm=2.940584421157837, loss=1.0679538249969482
I0310 02:26:37.756721 139708407461632 logging_writer.py:48] [337800] global_step=337800, grad_norm=2.847919225692749, loss=1.3997292518615723
I0310 02:27:23.119117 139708415854336 logging_writer.py:48] [337900] global_step=337900, grad_norm=3.139692544937134, loss=1.1800212860107422
I0310 02:28:08.469966 139708407461632 logging_writer.py:48] [338000] global_step=338000, grad_norm=3.241403579711914, loss=1.5646650791168213
I0310 02:28:53.830399 139708415854336 logging_writer.py:48] [338100] global_step=338100, grad_norm=3.0823328495025635, loss=2.4490809440612793
I0310 02:29:06.180673 139902746892096 spec.py:321] Evaluating on the training split.
I0310 02:29:17.615696 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 02:29:40.337921 139902746892096 spec.py:349] Evaluating on the test split.
I0310 02:29:42.005544 139902746892096 submission_runner.py:411] Time since start: 163705.03s, 	Step: 338129, 	{'train/accuracy': 0.8878124952316284, 'train/loss': 0.41412225365638733, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 151284.9808397293, 'total_duration': 163705.03434443474, 'accumulated_submission_time': 151284.9808397293, 'accumulated_eval_time': 12381.196682929993, 'accumulated_logging_time': 21.122995376586914}
I0310 02:29:42.071231 139708407461632 logging_writer.py:48] [338129] accumulated_eval_time=12381.196683, accumulated_logging_time=21.122995, accumulated_submission_time=151284.980840, global_step=338129, preemption_count=0, score=151284.980840, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=163705.034344, train/accuracy=0.887812, train/loss=0.414122, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 02:30:10.387960 139708415854336 logging_writer.py:48] [338200] global_step=338200, grad_norm=3.2285168170928955, loss=1.114380121231079
I0310 02:30:54.219365 139708407461632 logging_writer.py:48] [338300] global_step=338300, grad_norm=3.3372626304626465, loss=2.351761817932129
I0310 02:31:39.780763 139708415854336 logging_writer.py:48] [338400] global_step=338400, grad_norm=2.8402962684631348, loss=1.5515941381454468
I0310 02:32:25.003756 139708407461632 logging_writer.py:48] [338500] global_step=338500, grad_norm=2.834106206893921, loss=1.039225459098816
I0310 02:33:10.157579 139708415854336 logging_writer.py:48] [338600] global_step=338600, grad_norm=3.0592546463012695, loss=1.5466516017913818
I0310 02:33:55.533079 139708407461632 logging_writer.py:48] [338700] global_step=338700, grad_norm=2.981459617614746, loss=1.147017240524292
I0310 02:34:40.863203 139708415854336 logging_writer.py:48] [338800] global_step=338800, grad_norm=3.2106690406799316, loss=0.9997532963752747
I0310 02:35:26.169110 139708407461632 logging_writer.py:48] [338900] global_step=338900, grad_norm=3.2077245712280273, loss=1.0738046169281006
I0310 02:36:11.401563 139708415854336 logging_writer.py:48] [339000] global_step=339000, grad_norm=3.127243995666504, loss=2.2168924808502197
I0310 02:36:42.406659 139902746892096 spec.py:321] Evaluating on the training split.
I0310 02:36:53.816110 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 02:37:16.319497 139902746892096 spec.py:349] Evaluating on the test split.
I0310 02:37:17.995358 139902746892096 submission_runner.py:411] Time since start: 164161.02s, 	Step: 339070, 	{'train/accuracy': 0.8898437023162842, 'train/loss': 0.40944549441337585, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 151705.25899219513, 'total_duration': 164161.02415442467, 'accumulated_submission_time': 151705.25899219513, 'accumulated_eval_time': 12416.785385370255, 'accumulated_logging_time': 21.197551250457764}
I0310 02:37:18.061891 139708407461632 logging_writer.py:48] [339070] accumulated_eval_time=12416.785385, accumulated_logging_time=21.197551, accumulated_submission_time=151705.258992, global_step=339070, preemption_count=0, score=151705.258992, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=164161.024154, train/accuracy=0.889844, train/loss=0.409445, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 02:37:30.260183 139708415854336 logging_writer.py:48] [339100] global_step=339100, grad_norm=3.5132155418395996, loss=1.5990263223648071
I0310 02:38:11.905031 139708407461632 logging_writer.py:48] [339200] global_step=339200, grad_norm=2.823631763458252, loss=1.4915425777435303
I0310 02:38:57.229671 139708415854336 logging_writer.py:48] [339300] global_step=339300, grad_norm=3.0924808979034424, loss=1.0367438793182373
I0310 02:39:42.715638 139708407461632 logging_writer.py:48] [339400] global_step=339400, grad_norm=3.078639507293701, loss=1.101052165031433
I0310 02:40:27.743952 139708415854336 logging_writer.py:48] [339500] global_step=339500, grad_norm=3.2049150466918945, loss=1.127024531364441
I0310 02:41:13.087843 139708407461632 logging_writer.py:48] [339600] global_step=339600, grad_norm=3.0059609413146973, loss=1.9999520778656006
I0310 02:41:58.474239 139708415854336 logging_writer.py:48] [339700] global_step=339700, grad_norm=2.876094341278076, loss=1.1239922046661377
I0310 02:42:43.658618 139708407461632 logging_writer.py:48] [339800] global_step=339800, grad_norm=3.1209521293640137, loss=1.2714447975158691
I0310 02:43:29.027333 139708415854336 logging_writer.py:48] [339900] global_step=339900, grad_norm=2.895728349685669, loss=1.3207433223724365
I0310 02:44:14.074038 139708407461632 logging_writer.py:48] [340000] global_step=340000, grad_norm=3.0223348140716553, loss=1.228219985961914
I0310 02:44:18.267696 139902746892096 spec.py:321] Evaluating on the training split.
I0310 02:44:29.853869 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 02:44:51.575295 139902746892096 spec.py:349] Evaluating on the test split.
I0310 02:44:53.234679 139902746892096 submission_runner.py:411] Time since start: 164616.26s, 	Step: 340011, 	{'train/accuracy': 0.8881640434265137, 'train/loss': 0.419726699590683, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 152125.4069724083, 'total_duration': 164616.2634806633, 'accumulated_submission_time': 152125.4069724083, 'accumulated_eval_time': 12451.752354383469, 'accumulated_logging_time': 21.272735834121704}
I0310 02:44:53.304820 139708415854336 logging_writer.py:48] [340011] accumulated_eval_time=12451.752354, accumulated_logging_time=21.272736, accumulated_submission_time=152125.406972, global_step=340011, preemption_count=0, score=152125.406972, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=164616.263481, train/accuracy=0.888164, train/loss=0.419727, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 02:45:28.705018 139708407461632 logging_writer.py:48] [340100] global_step=340100, grad_norm=3.2540576457977295, loss=1.163694143295288
I0310 02:46:13.898775 139708415854336 logging_writer.py:48] [340200] global_step=340200, grad_norm=2.9777896404266357, loss=1.8986433744430542
I0310 02:46:59.721412 139708407461632 logging_writer.py:48] [340300] global_step=340300, grad_norm=2.930776596069336, loss=1.2008781433105469
I0310 02:47:45.502758 139708415854336 logging_writer.py:48] [340400] global_step=340400, grad_norm=3.1009440422058105, loss=1.6362380981445312
I0310 02:48:31.185155 139708407461632 logging_writer.py:48] [340500] global_step=340500, grad_norm=3.0808827877044678, loss=1.2124650478363037
I0310 02:49:16.467267 139708415854336 logging_writer.py:48] [340600] global_step=340600, grad_norm=2.978452205657959, loss=2.013010263442993
I0310 02:50:01.715387 139708407461632 logging_writer.py:48] [340700] global_step=340700, grad_norm=3.195777416229248, loss=1.452290654182434
I0310 02:50:47.012705 139708415854336 logging_writer.py:48] [340800] global_step=340800, grad_norm=3.1279332637786865, loss=1.8299400806427002
I0310 02:51:32.396953 139708407461632 logging_writer.py:48] [340900] global_step=340900, grad_norm=3.1068246364593506, loss=1.1120870113372803
I0310 02:51:53.335416 139902746892096 spec.py:321] Evaluating on the training split.
I0310 02:52:05.016186 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 02:52:26.764867 139902746892096 spec.py:349] Evaluating on the test split.
I0310 02:52:28.447824 139902746892096 submission_runner.py:411] Time since start: 165071.48s, 	Step: 340948, 	{'train/accuracy': 0.8879492282867432, 'train/loss': 0.4159083962440491, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 152545.37711572647, 'total_duration': 165071.47660183907, 'accumulated_submission_time': 152545.37711572647, 'accumulated_eval_time': 12486.864793300629, 'accumulated_logging_time': 21.353898286819458}
I0310 02:52:28.526805 139708415854336 logging_writer.py:48] [340948] accumulated_eval_time=12486.864793, accumulated_logging_time=21.353898, accumulated_submission_time=152545.377116, global_step=340948, preemption_count=0, score=152545.377116, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=165071.476602, train/accuracy=0.887949, train/loss=0.415908, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 02:52:49.403586 139708407461632 logging_writer.py:48] [341000] global_step=341000, grad_norm=3.04325532913208, loss=1.0861972570419312
I0310 02:53:32.851426 139708415854336 logging_writer.py:48] [341100] global_step=341100, grad_norm=3.0909924507141113, loss=2.1673483848571777
I0310 02:54:18.081265 139708407461632 logging_writer.py:48] [341200] global_step=341200, grad_norm=3.302924871444702, loss=2.9504528045654297
I0310 02:55:03.523586 139708415854336 logging_writer.py:48] [341300] global_step=341300, grad_norm=3.051386594772339, loss=1.1478863954544067
I0310 02:55:48.809317 139708407461632 logging_writer.py:48] [341400] global_step=341400, grad_norm=3.2654781341552734, loss=2.9952707290649414
I0310 02:56:34.365267 139708415854336 logging_writer.py:48] [341500] global_step=341500, grad_norm=2.9530487060546875, loss=1.047743320465088
I0310 02:57:20.256206 139708407461632 logging_writer.py:48] [341600] global_step=341600, grad_norm=3.0281498432159424, loss=1.206743836402893
I0310 02:58:05.702517 139708415854336 logging_writer.py:48] [341700] global_step=341700, grad_norm=3.313765048980713, loss=1.6184213161468506
I0310 02:58:50.976994 139708407461632 logging_writer.py:48] [341800] global_step=341800, grad_norm=2.9384615421295166, loss=1.3259717226028442
I0310 02:59:28.897373 139902746892096 spec.py:321] Evaluating on the training split.
I0310 02:59:40.602692 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 03:00:03.393433 139902746892096 spec.py:349] Evaluating on the test split.
I0310 03:00:05.065347 139902746892096 submission_runner.py:411] Time since start: 165528.09s, 	Step: 341885, 	{'train/accuracy': 0.8882616758346558, 'train/loss': 0.4191540777683258, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 152965.68938064575, 'total_duration': 165528.09414219856, 'accumulated_submission_time': 152965.68938064575, 'accumulated_eval_time': 12523.032780647278, 'accumulated_logging_time': 21.442599773406982}
I0310 03:00:05.131255 139708415854336 logging_writer.py:48] [341885] accumulated_eval_time=12523.032781, accumulated_logging_time=21.442600, accumulated_submission_time=152965.689381, global_step=341885, preemption_count=0, score=152965.689381, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=165528.094142, train/accuracy=0.888262, train/loss=0.419154, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 03:00:11.431091 139708407461632 logging_writer.py:48] [341900] global_step=341900, grad_norm=3.2025723457336426, loss=1.189630389213562
I0310 03:00:52.162407 139708415854336 logging_writer.py:48] [342000] global_step=342000, grad_norm=3.8620834350585938, loss=3.1634745597839355
I0310 03:01:37.403788 139708407461632 logging_writer.py:48] [342100] global_step=342100, grad_norm=3.02913236618042, loss=1.0755417346954346
I0310 03:02:23.257225 139708415854336 logging_writer.py:48] [342200] global_step=342200, grad_norm=3.1459481716156006, loss=1.544661521911621
I0310 03:03:08.537685 139708407461632 logging_writer.py:48] [342300] global_step=342300, grad_norm=2.8037490844726562, loss=1.10672926902771
I0310 03:03:53.748046 139708415854336 logging_writer.py:48] [342400] global_step=342400, grad_norm=3.109853982925415, loss=1.6634043455123901
I0310 03:04:38.952327 139708407461632 logging_writer.py:48] [342500] global_step=342500, grad_norm=3.023078680038452, loss=1.1816580295562744
I0310 03:05:24.384646 139708415854336 logging_writer.py:48] [342600] global_step=342600, grad_norm=3.7585670948028564, loss=2.756570339202881
I0310 03:06:09.552417 139708407461632 logging_writer.py:48] [342700] global_step=342700, grad_norm=3.1589102745056152, loss=1.0612661838531494
I0310 03:06:54.939672 139708415854336 logging_writer.py:48] [342800] global_step=342800, grad_norm=3.2583258152008057, loss=1.153100609779358
I0310 03:07:05.164416 139902746892096 spec.py:321] Evaluating on the training split.
I0310 03:07:16.572226 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 03:07:38.868450 139902746892096 spec.py:349] Evaluating on the test split.
I0310 03:07:40.544105 139902746892096 submission_runner.py:411] Time since start: 165983.57s, 	Step: 342824, 	{'train/accuracy': 0.8852733969688416, 'train/loss': 0.42138731479644775, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 153385.66485118866, 'total_duration': 165983.57289791107, 'accumulated_submission_time': 153385.66485118866, 'accumulated_eval_time': 12558.412452220917, 'accumulated_logging_time': 21.517127513885498}
I0310 03:07:40.612755 139708407461632 logging_writer.py:48] [342824] accumulated_eval_time=12558.412452, accumulated_logging_time=21.517128, accumulated_submission_time=153385.664851, global_step=342824, preemption_count=0, score=153385.664851, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=165983.572898, train/accuracy=0.885273, train/loss=0.421387, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 03:08:10.922365 139708415854336 logging_writer.py:48] [342900] global_step=342900, grad_norm=3.0706095695495605, loss=1.1225619316101074
I0310 03:08:54.994327 139708407461632 logging_writer.py:48] [343000] global_step=343000, grad_norm=3.070140838623047, loss=2.59578013420105
I0310 03:09:40.416530 139708415854336 logging_writer.py:48] [343100] global_step=343100, grad_norm=3.3729145526885986, loss=2.88319993019104
I0310 03:10:25.915420 139708407461632 logging_writer.py:48] [343200] global_step=343200, grad_norm=2.914761781692505, loss=2.2056193351745605
I0310 03:11:11.114994 139708415854336 logging_writer.py:48] [343300] global_step=343300, grad_norm=2.9819929599761963, loss=1.3091247081756592
I0310 03:11:56.261353 139708407461632 logging_writer.py:48] [343400] global_step=343400, grad_norm=3.096317768096924, loss=1.3980828523635864
I0310 03:12:41.543423 139708415854336 logging_writer.py:48] [343500] global_step=343500, grad_norm=3.7245285511016846, loss=1.1785972118377686
I0310 03:13:26.699664 139708407461632 logging_writer.py:48] [343600] global_step=343600, grad_norm=3.1747257709503174, loss=1.4250057935714722
I0310 03:14:11.948032 139708415854336 logging_writer.py:48] [343700] global_step=343700, grad_norm=3.072810411453247, loss=1.3982515335083008
I0310 03:14:40.749339 139902746892096 spec.py:321] Evaluating on the training split.
I0310 03:14:52.347584 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 03:15:13.851376 139902746892096 spec.py:349] Evaluating on the test split.
I0310 03:15:15.533302 139902746892096 submission_runner.py:411] Time since start: 166438.56s, 	Step: 343765, 	{'train/accuracy': 0.8892382383346558, 'train/loss': 0.4125251770019531, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 153805.74320554733, 'total_duration': 166438.56208109856, 'accumulated_submission_time': 153805.74320554733, 'accumulated_eval_time': 12593.196396112442, 'accumulated_logging_time': 21.595152139663696}
I0310 03:15:15.612485 139708407461632 logging_writer.py:48] [343765] accumulated_eval_time=12593.196396, accumulated_logging_time=21.595152, accumulated_submission_time=153805.743206, global_step=343765, preemption_count=0, score=153805.743206, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=166438.562081, train/accuracy=0.889238, train/loss=0.412525, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 03:15:29.771840 139708415854336 logging_writer.py:48] [343800] global_step=343800, grad_norm=3.4192566871643066, loss=2.8547208309173584
I0310 03:16:12.000143 139708407461632 logging_writer.py:48] [343900] global_step=343900, grad_norm=3.156306028366089, loss=1.0378458499908447
I0310 03:16:57.276085 139708415854336 logging_writer.py:48] [344000] global_step=344000, grad_norm=3.016785144805908, loss=2.4950461387634277
I0310 03:17:43.229212 139708407461632 logging_writer.py:48] [344100] global_step=344100, grad_norm=2.8799259662628174, loss=1.2511680126190186
I0310 03:18:28.463771 139708415854336 logging_writer.py:48] [344200] global_step=344200, grad_norm=3.201087236404419, loss=1.154435634613037
I0310 03:19:13.780887 139708407461632 logging_writer.py:48] [344300] global_step=344300, grad_norm=3.126215934753418, loss=1.4032552242279053
I0310 03:19:59.234055 139708415854336 logging_writer.py:48] [344400] global_step=344400, grad_norm=3.155036687850952, loss=2.4371275901794434
I0310 03:20:44.331390 139708407461632 logging_writer.py:48] [344500] global_step=344500, grad_norm=2.9561760425567627, loss=1.2527700662612915
I0310 03:21:29.779581 139708415854336 logging_writer.py:48] [344600] global_step=344600, grad_norm=2.8717803955078125, loss=1.1003507375717163
I0310 03:22:14.942768 139708407461632 logging_writer.py:48] [344700] global_step=344700, grad_norm=3.0649478435516357, loss=1.107163906097412
I0310 03:22:15.987435 139902746892096 spec.py:321] Evaluating on the training split.
I0310 03:22:27.467357 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 03:22:49.195353 139902746892096 spec.py:349] Evaluating on the test split.
I0310 03:22:50.867244 139902746892096 submission_runner.py:411] Time since start: 166893.90s, 	Step: 344704, 	{'train/accuracy': 0.8899218440055847, 'train/loss': 0.41085392236709595, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 154226.0605700016, 'total_duration': 166893.89604711533, 'accumulated_submission_time': 154226.0605700016, 'accumulated_eval_time': 12628.076207399368, 'accumulated_logging_time': 21.683309316635132}
I0310 03:22:50.934220 139708415854336 logging_writer.py:48] [344704] accumulated_eval_time=12628.076207, accumulated_logging_time=21.683309, accumulated_submission_time=154226.060570, global_step=344704, preemption_count=0, score=154226.060570, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=166893.896047, train/accuracy=0.889922, train/loss=0.410854, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 03:23:29.524995 139708407461632 logging_writer.py:48] [344800] global_step=344800, grad_norm=3.2038822174072266, loss=1.2058396339416504
I0310 03:24:14.521534 139708415854336 logging_writer.py:48] [344900] global_step=344900, grad_norm=3.028491973876953, loss=1.8430984020233154
I0310 03:24:59.920710 139708407461632 logging_writer.py:48] [345000] global_step=345000, grad_norm=3.142887830734253, loss=1.021026372909546
I0310 03:25:45.407143 139708415854336 logging_writer.py:48] [345100] global_step=345100, grad_norm=3.326366424560547, loss=3.118610143661499
I0310 03:26:30.612215 139708407461632 logging_writer.py:48] [345200] global_step=345200, grad_norm=2.8930280208587646, loss=1.4987858533859253
I0310 03:27:16.168217 139708415854336 logging_writer.py:48] [345300] global_step=345300, grad_norm=3.215751886367798, loss=1.0970778465270996
I0310 03:28:01.970276 139708407461632 logging_writer.py:48] [345400] global_step=345400, grad_norm=2.973620891571045, loss=2.1283397674560547
I0310 03:28:47.247810 139708415854336 logging_writer.py:48] [345500] global_step=345500, grad_norm=2.835411310195923, loss=1.8028644323349
I0310 03:29:32.527484 139708407461632 logging_writer.py:48] [345600] global_step=345600, grad_norm=3.7191216945648193, loss=3.0078699588775635
I0310 03:29:51.219444 139902746892096 spec.py:321] Evaluating on the training split.
I0310 03:30:02.903574 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 03:30:25.230934 139902746892096 spec.py:349] Evaluating on the test split.
I0310 03:30:26.901709 139902746892096 submission_runner.py:411] Time since start: 167349.93s, 	Step: 345643, 	{'train/accuracy': 0.8858007788658142, 'train/loss': 0.41924089193344116, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 154646.28801751137, 'total_duration': 167349.93050813675, 'accumulated_submission_time': 154646.28801751137, 'accumulated_eval_time': 12663.758479118347, 'accumulated_logging_time': 21.758626222610474}
I0310 03:30:26.967804 139708415854336 logging_writer.py:48] [345643] accumulated_eval_time=12663.758479, accumulated_logging_time=21.758626, accumulated_submission_time=154646.288018, global_step=345643, preemption_count=0, score=154646.288018, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=167349.930508, train/accuracy=0.885801, train/loss=0.419241, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 03:30:49.792851 139708407461632 logging_writer.py:48] [345700] global_step=345700, grad_norm=3.152078151702881, loss=1.4281938076019287
I0310 03:31:33.202631 139708415854336 logging_writer.py:48] [345800] global_step=345800, grad_norm=3.0301401615142822, loss=1.6634650230407715
I0310 03:32:18.811570 139708407461632 logging_writer.py:48] [345900] global_step=345900, grad_norm=2.9581663608551025, loss=1.42721688747406
I0310 03:33:04.612452 139708415854336 logging_writer.py:48] [346000] global_step=346000, grad_norm=3.0799059867858887, loss=1.15412175655365
I0310 03:33:49.849136 139708407461632 logging_writer.py:48] [346100] global_step=346100, grad_norm=3.32473087310791, loss=1.1519147157669067
I0310 03:34:35.260881 139708415854336 logging_writer.py:48] [346200] global_step=346200, grad_norm=3.053981304168701, loss=0.9850691556930542
I0310 03:35:20.603711 139708407461632 logging_writer.py:48] [346300] global_step=346300, grad_norm=3.296889066696167, loss=2.51751708984375
I0310 03:36:05.931948 139708415854336 logging_writer.py:48] [346400] global_step=346400, grad_norm=3.146012783050537, loss=1.0711606740951538
I0310 03:36:51.307969 139708407461632 logging_writer.py:48] [346500] global_step=346500, grad_norm=3.4347660541534424, loss=1.7290149927139282
I0310 03:37:27.134876 139902746892096 spec.py:321] Evaluating on the training split.
I0310 03:37:38.770596 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 03:38:00.028390 139902746892096 spec.py:349] Evaluating on the test split.
I0310 03:38:01.704821 139902746892096 submission_runner.py:411] Time since start: 167804.73s, 	Step: 346580, 	{'train/accuracy': 0.8874218463897705, 'train/loss': 0.414071649312973, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 155066.39670681953, 'total_duration': 167804.73359775543, 'accumulated_submission_time': 155066.39670681953, 'accumulated_eval_time': 12698.328409194946, 'accumulated_logging_time': 21.834535837173462}
I0310 03:38:01.786769 139708415854336 logging_writer.py:48] [346580] accumulated_eval_time=12698.328409, accumulated_logging_time=21.834536, accumulated_submission_time=155066.396707, global_step=346580, preemption_count=0, score=155066.396707, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=167804.733598, train/accuracy=0.887422, train/loss=0.414072, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 03:38:10.053922 139708407461632 logging_writer.py:48] [346600] global_step=346600, grad_norm=3.9263832569122314, loss=3.2607333660125732
I0310 03:38:51.672936 139708415854336 logging_writer.py:48] [346700] global_step=346700, grad_norm=3.988523483276367, loss=3.2968597412109375
I0310 03:39:37.132059 139708407461632 logging_writer.py:48] [346800] global_step=346800, grad_norm=3.1423656940460205, loss=1.0626211166381836
I0310 03:40:22.483777 139708415854336 logging_writer.py:48] [346900] global_step=346900, grad_norm=3.283169746398926, loss=1.121403694152832
I0310 03:41:07.645623 139708407461632 logging_writer.py:48] [347000] global_step=347000, grad_norm=2.9420547485351562, loss=1.9374744892120361
I0310 03:41:52.974481 139708415854336 logging_writer.py:48] [347100] global_step=347100, grad_norm=3.000828742980957, loss=1.1046472787857056
I0310 03:42:38.317218 139708407461632 logging_writer.py:48] [347200] global_step=347200, grad_norm=3.143156051635742, loss=1.3599951267242432
I0310 03:43:23.585463 139708415854336 logging_writer.py:48] [347300] global_step=347300, grad_norm=3.8821802139282227, loss=3.2104291915893555
I0310 03:44:08.988074 139708407461632 logging_writer.py:48] [347400] global_step=347400, grad_norm=2.96712327003479, loss=1.2763737440109253
I0310 03:44:54.238466 139708415854336 logging_writer.py:48] [347500] global_step=347500, grad_norm=3.250044822692871, loss=2.7585926055908203
I0310 03:45:02.043700 139902746892096 spec.py:321] Evaluating on the training split.
I0310 03:45:13.429955 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 03:45:34.548785 139902746892096 spec.py:349] Evaluating on the test split.
I0310 03:45:36.234122 139902746892096 submission_runner.py:411] Time since start: 168259.26s, 	Step: 347519, 	{'train/accuracy': 0.8876953125, 'train/loss': 0.4181354343891144, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 155486.5955555439, 'total_duration': 168259.2629210949, 'accumulated_submission_time': 155486.5955555439, 'accumulated_eval_time': 12732.518855333328, 'accumulated_logging_time': 21.925792455673218}
I0310 03:45:36.302168 139708407461632 logging_writer.py:48] [347519] accumulated_eval_time=12732.518855, accumulated_logging_time=21.925792, accumulated_submission_time=155486.595556, global_step=347519, preemption_count=0, score=155486.595556, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=168259.262921, train/accuracy=0.887695, train/loss=0.418135, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 03:46:08.668633 139708415854336 logging_writer.py:48] [347600] global_step=347600, grad_norm=3.2669873237609863, loss=1.176187515258789
I0310 03:46:53.590228 139708407461632 logging_writer.py:48] [347700] global_step=347700, grad_norm=2.852369546890259, loss=1.0209823846817017
I0310 03:47:39.222636 139708415854336 logging_writer.py:48] [347800] global_step=347800, grad_norm=3.3415651321411133, loss=1.1002895832061768
I0310 03:48:25.049631 139708407461632 logging_writer.py:48] [347900] global_step=347900, grad_norm=2.920302391052246, loss=2.060180187225342
I0310 03:49:10.300184 139708415854336 logging_writer.py:48] [348000] global_step=348000, grad_norm=3.927311658859253, loss=3.2098464965820312
I0310 03:49:55.687819 139708407461632 logging_writer.py:48] [348100] global_step=348100, grad_norm=3.9382152557373047, loss=3.2079660892486572
I0310 03:50:41.057261 139708415854336 logging_writer.py:48] [348200] global_step=348200, grad_norm=3.2030279636383057, loss=1.1001324653625488
I0310 03:51:26.513386 139708407461632 logging_writer.py:48] [348300] global_step=348300, grad_norm=3.1761550903320312, loss=1.1870383024215698
I0310 03:52:11.811316 139708415854336 logging_writer.py:48] [348400] global_step=348400, grad_norm=3.1530821323394775, loss=1.370174527168274
I0310 03:52:36.559001 139902746892096 spec.py:321] Evaluating on the training split.
I0310 03:52:48.062927 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 03:53:09.149893 139902746892096 spec.py:349] Evaluating on the test split.
I0310 03:53:10.831078 139902746892096 submission_runner.py:411] Time since start: 168713.86s, 	Step: 348456, 	{'train/accuracy': 0.8907812237739563, 'train/loss': 0.41011130809783936, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 155906.79409885406, 'total_duration': 168713.85986709595, 'accumulated_submission_time': 155906.79409885406, 'accumulated_eval_time': 12766.790929794312, 'accumulated_logging_time': 22.002236366271973}
I0310 03:53:10.913752 139708407461632 logging_writer.py:48] [348456] accumulated_eval_time=12766.790930, accumulated_logging_time=22.002236, accumulated_submission_time=155906.794099, global_step=348456, preemption_count=0, score=155906.794099, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=168713.859867, train/accuracy=0.890781, train/loss=0.410111, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 03:53:28.638087 139708415854336 logging_writer.py:48] [348500] global_step=348500, grad_norm=3.242319107055664, loss=2.7617619037628174
I0310 03:54:11.425151 139708407461632 logging_writer.py:48] [348600] global_step=348600, grad_norm=3.0177125930786133, loss=1.1997236013412476
I0310 03:54:56.680878 139708415854336 logging_writer.py:48] [348700] global_step=348700, grad_norm=3.9691543579101562, loss=3.1872875690460205
I0310 03:55:41.831365 139708407461632 logging_writer.py:48] [348800] global_step=348800, grad_norm=3.226020097732544, loss=1.333219289779663
I0310 03:56:27.103093 139708415854336 logging_writer.py:48] [348900] global_step=348900, grad_norm=2.96811842918396, loss=2.5234227180480957
I0310 03:57:12.355743 139708407461632 logging_writer.py:48] [349000] global_step=349000, grad_norm=3.498387575149536, loss=3.0042364597320557
I0310 03:57:58.000711 139708415854336 logging_writer.py:48] [349100] global_step=349100, grad_norm=3.0774898529052734, loss=1.171956181526184
I0310 03:58:43.190015 139708407461632 logging_writer.py:48] [349200] global_step=349200, grad_norm=3.190307140350342, loss=1.0807931423187256
I0310 03:59:28.522809 139708415854336 logging_writer.py:48] [349300] global_step=349300, grad_norm=2.9959969520568848, loss=1.1101034879684448
I0310 04:00:10.884208 139902746892096 spec.py:321] Evaluating on the training split.
I0310 04:00:22.410010 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 04:00:45.415374 139902746892096 spec.py:349] Evaluating on the test split.
I0310 04:00:47.088087 139902746892096 submission_runner.py:411] Time since start: 169170.12s, 	Step: 349395, 	{'train/accuracy': 0.88978511095047, 'train/loss': 0.4093369245529175, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 156326.7063846588, 'total_duration': 169170.11686944962, 'accumulated_submission_time': 156326.7063846588, 'accumulated_eval_time': 12802.994801282883, 'accumulated_logging_time': 22.09450626373291}
I0310 04:00:47.163594 139708407461632 logging_writer.py:48] [349395] accumulated_eval_time=12802.994801, accumulated_logging_time=22.094506, accumulated_submission_time=156326.706385, global_step=349395, preemption_count=0, score=156326.706385, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=169170.116869, train/accuracy=0.889785, train/loss=0.409337, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 04:00:49.521359 139708415854336 logging_writer.py:48] [349400] global_step=349400, grad_norm=3.1036672592163086, loss=2.2954561710357666
I0310 04:01:29.548658 139708407461632 logging_writer.py:48] [349500] global_step=349500, grad_norm=3.1390600204467773, loss=1.0366805791854858
I0310 04:02:14.869681 139708415854336 logging_writer.py:48] [349600] global_step=349600, grad_norm=3.3629298210144043, loss=1.1243040561676025
I0310 04:03:00.787380 139708407461632 logging_writer.py:48] [349700] global_step=349700, grad_norm=3.3609161376953125, loss=2.76479172706604
I0310 04:03:45.899437 139708415854336 logging_writer.py:48] [349800] global_step=349800, grad_norm=3.134645462036133, loss=1.1398721933364868
I0310 04:04:31.235998 139708407461632 logging_writer.py:48] [349900] global_step=349900, grad_norm=3.073371648788452, loss=1.1265931129455566
I0310 04:05:16.729276 139708415854336 logging_writer.py:48] [350000] global_step=350000, grad_norm=3.3745079040527344, loss=2.7023868560791016
I0310 04:06:02.059422 139708407461632 logging_writer.py:48] [350100] global_step=350100, grad_norm=3.358651638031006, loss=2.3748409748077393
I0310 04:06:47.253850 139708415854336 logging_writer.py:48] [350200] global_step=350200, grad_norm=3.4730358123779297, loss=1.1062312126159668
I0310 04:07:32.586563 139708407461632 logging_writer.py:48] [350300] global_step=350300, grad_norm=4.432878017425537, loss=3.2066049575805664
I0310 04:07:47.191302 139902746892096 spec.py:321] Evaluating on the training split.
I0310 04:07:58.480331 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 04:08:18.104507 139902746892096 spec.py:349] Evaluating on the test split.
I0310 04:08:19.781749 139902746892096 submission_runner.py:411] Time since start: 169622.81s, 	Step: 350333, 	{'train/accuracy': 0.8917187452316284, 'train/loss': 0.40313830971717834, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 156746.6764435768, 'total_duration': 169622.81052684784, 'accumulated_submission_time': 156746.6764435768, 'accumulated_eval_time': 12835.585213184357, 'accumulated_logging_time': 22.179222583770752}
I0310 04:08:19.865548 139708415854336 logging_writer.py:48] [350333] accumulated_eval_time=12835.585213, accumulated_logging_time=22.179223, accumulated_submission_time=156746.676444, global_step=350333, preemption_count=0, score=156746.676444, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=169622.810527, train/accuracy=0.891719, train/loss=0.403138, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 04:08:46.612097 139708407461632 logging_writer.py:48] [350400] global_step=350400, grad_norm=2.884978771209717, loss=1.2191041707992554
I0310 04:09:31.263213 139708415854336 logging_writer.py:48] [350500] global_step=350500, grad_norm=2.989109516143799, loss=1.2511091232299805
I0310 04:10:16.670589 139708407461632 logging_writer.py:48] [350600] global_step=350600, grad_norm=3.2282333374023438, loss=2.8565118312835693
I0310 04:11:02.266127 139708415854336 logging_writer.py:48] [350700] global_step=350700, grad_norm=3.594367504119873, loss=1.1838186979293823
I0310 04:11:47.391558 139708407461632 logging_writer.py:48] [350800] global_step=350800, grad_norm=3.0999841690063477, loss=1.0768446922302246
I0310 04:12:32.651219 139708415854336 logging_writer.py:48] [350900] global_step=350900, grad_norm=3.050595283508301, loss=2.4914066791534424
I0310 04:13:17.883809 139708407461632 logging_writer.py:48] [351000] global_step=351000, grad_norm=3.14921236038208, loss=2.7247400283813477
I0310 04:14:03.202384 139708415854336 logging_writer.py:48] [351100] global_step=351100, grad_norm=3.1462485790252686, loss=1.139041543006897
I0310 04:14:48.659205 139708407461632 logging_writer.py:48] [351200] global_step=351200, grad_norm=3.1816420555114746, loss=1.103379487991333
I0310 04:15:19.894054 139902746892096 spec.py:321] Evaluating on the training split.
I0310 04:15:32.039334 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 04:15:52.457473 139902746892096 spec.py:349] Evaluating on the test split.
I0310 04:15:54.158033 139902746892096 submission_runner.py:411] Time since start: 170077.19s, 	Step: 351271, 	{'train/accuracy': 0.88818359375, 'train/loss': 0.4155411422252655, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 157166.64605093002, 'total_duration': 170077.18683290482, 'accumulated_submission_time': 157166.64605093002, 'accumulated_eval_time': 12869.849185228348, 'accumulated_logging_time': 22.272608995437622}
I0310 04:15:54.225154 139708415854336 logging_writer.py:48] [351271] accumulated_eval_time=12869.849185, accumulated_logging_time=22.272609, accumulated_submission_time=157166.646051, global_step=351271, preemption_count=0, score=157166.646051, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=170077.186833, train/accuracy=0.888184, train/loss=0.415541, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 04:16:06.024900 139708407461632 logging_writer.py:48] [351300] global_step=351300, grad_norm=3.050121307373047, loss=1.1732709407806396
I0310 04:16:47.246803 139708415854336 logging_writer.py:48] [351400] global_step=351400, grad_norm=2.8544368743896484, loss=2.203104019165039
I0310 04:17:32.555304 139708407461632 logging_writer.py:48] [351500] global_step=351500, grad_norm=3.6873927116394043, loss=2.5335021018981934
I0310 04:18:18.413007 139708415854336 logging_writer.py:48] [351600] global_step=351600, grad_norm=3.268454074859619, loss=1.2692993879318237
I0310 04:19:03.946842 139708407461632 logging_writer.py:48] [351700] global_step=351700, grad_norm=3.124774932861328, loss=1.2069244384765625
I0310 04:19:49.421059 139708415854336 logging_writer.py:48] [351800] global_step=351800, grad_norm=3.7994329929351807, loss=3.128286361694336
I0310 04:20:34.652788 139708407461632 logging_writer.py:48] [351900] global_step=351900, grad_norm=3.833925724029541, loss=2.57177734375
I0310 04:21:19.990251 139708415854336 logging_writer.py:48] [352000] global_step=352000, grad_norm=3.2916502952575684, loss=2.782991886138916
I0310 04:22:05.092043 139708407461632 logging_writer.py:48] [352100] global_step=352100, grad_norm=3.687978744506836, loss=2.6187710762023926
I0310 04:22:50.406256 139708415854336 logging_writer.py:48] [352200] global_step=352200, grad_norm=2.83925724029541, loss=1.3596203327178955
I0310 04:22:54.708150 139902746892096 spec.py:321] Evaluating on the training split.
I0310 04:23:06.071464 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 04:23:26.668519 139902746892096 spec.py:349] Evaluating on the test split.
I0310 04:23:28.350266 139902746892096 submission_runner.py:411] Time since start: 170531.38s, 	Step: 352211, 	{'train/accuracy': 0.88671875, 'train/loss': 0.41791388392448425, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 157587.0714263916, 'total_duration': 170531.37905216217, 'accumulated_submission_time': 157587.0714263916, 'accumulated_eval_time': 12903.491284608841, 'accumulated_logging_time': 22.348501205444336}
I0310 04:23:28.434200 139708407461632 logging_writer.py:48] [352211] accumulated_eval_time=12903.491285, accumulated_logging_time=22.348501, accumulated_submission_time=157587.071426, global_step=352211, preemption_count=0, score=157587.071426, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=170531.379052, train/accuracy=0.886719, train/loss=0.417914, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 04:24:04.090815 139708415854336 logging_writer.py:48] [352300] global_step=352300, grad_norm=3.192645788192749, loss=1.281962275505066
I0310 04:24:48.951714 139708407461632 logging_writer.py:48] [352400] global_step=352400, grad_norm=3.4502053260803223, loss=2.3621432781219482
I0310 04:25:34.227225 139708415854336 logging_writer.py:48] [352500] global_step=352500, grad_norm=3.3215603828430176, loss=1.7589408159255981
I0310 04:26:19.807361 139708407461632 logging_writer.py:48] [352600] global_step=352600, grad_norm=3.0048625469207764, loss=1.0908787250518799
I0310 04:27:05.142910 139708415854336 logging_writer.py:48] [352700] global_step=352700, grad_norm=3.3436756134033203, loss=1.1411949396133423
I0310 04:27:50.442754 139708407461632 logging_writer.py:48] [352800] global_step=352800, grad_norm=3.946824789047241, loss=3.205179452896118
I0310 04:28:36.566442 139708415854336 logging_writer.py:48] [352900] global_step=352900, grad_norm=4.086084842681885, loss=3.209017038345337
I0310 04:29:21.937445 139708407461632 logging_writer.py:48] [353000] global_step=353000, grad_norm=3.0599007606506348, loss=2.412580966949463
I0310 04:30:07.191348 139708415854336 logging_writer.py:48] [353100] global_step=353100, grad_norm=2.9243087768554688, loss=1.150527000427246
I0310 04:30:28.669595 139902746892096 spec.py:321] Evaluating on the training split.
I0310 04:30:40.183752 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 04:31:02.578303 139902746892096 spec.py:349] Evaluating on the test split.
I0310 04:31:04.243050 139902746892096 submission_runner.py:411] Time since start: 170987.27s, 	Step: 353149, 	{'train/accuracy': 0.8885351419448853, 'train/loss': 0.4137209355831146, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 158007.24652957916, 'total_duration': 170987.2718143463, 'accumulated_submission_time': 158007.24652957916, 'accumulated_eval_time': 12939.064716339111, 'accumulated_logging_time': 22.443691968917847}
I0310 04:31:04.312049 139708407461632 logging_writer.py:48] [353149] accumulated_eval_time=12939.064716, accumulated_logging_time=22.443692, accumulated_submission_time=158007.246530, global_step=353149, preemption_count=0, score=158007.246530, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=170987.271814, train/accuracy=0.888535, train/loss=0.413721, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 04:31:24.766955 139708415854336 logging_writer.py:48] [353200] global_step=353200, grad_norm=3.622483015060425, loss=1.123407006263733
I0310 04:32:07.703742 139708407461632 logging_writer.py:48] [353300] global_step=353300, grad_norm=2.979633092880249, loss=1.0987427234649658
I0310 04:32:53.013334 139708415854336 logging_writer.py:48] [353400] global_step=353400, grad_norm=3.2720398902893066, loss=1.1294728517532349
I0310 04:33:38.424492 139708407461632 logging_writer.py:48] [353500] global_step=353500, grad_norm=3.646801710128784, loss=3.025348424911499
I0310 04:34:23.529105 139708415854336 logging_writer.py:48] [353600] global_step=353600, grad_norm=3.1371171474456787, loss=1.1363451480865479
I0310 04:35:08.755824 139708407461632 logging_writer.py:48] [353700] global_step=353700, grad_norm=3.514713764190674, loss=2.7015466690063477
I0310 04:35:53.769292 139708415854336 logging_writer.py:48] [353800] global_step=353800, grad_norm=3.2734475135803223, loss=1.355559229850769
I0310 04:36:38.954065 139708407461632 logging_writer.py:48] [353900] global_step=353900, grad_norm=3.228473663330078, loss=2.5429513454437256
I0310 04:37:24.115017 139708415854336 logging_writer.py:48] [354000] global_step=354000, grad_norm=3.0500142574310303, loss=1.4493992328643799
I0310 04:38:04.396959 139902746892096 spec.py:321] Evaluating on the training split.
I0310 04:38:15.719663 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 04:38:38.742219 139902746892096 spec.py:349] Evaluating on the test split.
I0310 04:38:40.411134 139902746892096 submission_runner.py:411] Time since start: 171443.44s, 	Step: 354090, 	{'train/accuracy': 0.8870312571525574, 'train/loss': 0.4134266972541809, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 158427.2736890316, 'total_duration': 171443.43993115425, 'accumulated_submission_time': 158427.2736890316, 'accumulated_eval_time': 12975.078892946243, 'accumulated_logging_time': 22.521716356277466}
I0310 04:38:40.479584 139708407461632 logging_writer.py:48] [354090] accumulated_eval_time=12975.078893, accumulated_logging_time=22.521716, accumulated_submission_time=158427.273689, global_step=354090, preemption_count=0, score=158427.273689, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=171443.439931, train/accuracy=0.887031, train/loss=0.413427, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 04:38:44.816843 139708415854336 logging_writer.py:48] [354100] global_step=354100, grad_norm=3.0059380531311035, loss=2.036337375640869
I0310 04:39:25.011924 139708407461632 logging_writer.py:48] [354200] global_step=354200, grad_norm=2.941502571105957, loss=1.2232557535171509
I0310 04:40:10.173602 139708415854336 logging_writer.py:48] [354300] global_step=354300, grad_norm=3.0003256797790527, loss=2.5360071659088135
I0310 04:40:55.831871 139708407461632 logging_writer.py:48] [354400] global_step=354400, grad_norm=3.474355697631836, loss=3.0922129154205322
I0310 04:41:40.971385 139708415854336 logging_writer.py:48] [354500] global_step=354500, grad_norm=3.065728187561035, loss=1.077280044555664
I0310 04:42:26.219600 139708407461632 logging_writer.py:48] [354600] global_step=354600, grad_norm=3.0620570182800293, loss=2.5464606285095215
I0310 04:43:11.642997 139708415854336 logging_writer.py:48] [354700] global_step=354700, grad_norm=2.8365345001220703, loss=1.234528660774231
I0310 04:43:56.782386 139708407461632 logging_writer.py:48] [354800] global_step=354800, grad_norm=3.9654381275177, loss=3.2041172981262207
I0310 04:44:41.898401 139708415854336 logging_writer.py:48] [354900] global_step=354900, grad_norm=3.422415256500244, loss=2.824066400527954
I0310 04:45:26.951225 139708407461632 logging_writer.py:48] [355000] global_step=355000, grad_norm=3.228057622909546, loss=1.1375658512115479
I0310 04:45:40.622728 139902746892096 spec.py:321] Evaluating on the training split.
I0310 04:45:52.281696 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 04:46:14.632159 139902746892096 spec.py:349] Evaluating on the test split.
I0310 04:46:16.310007 139902746892096 submission_runner.py:411] Time since start: 171899.34s, 	Step: 355032, 	{'train/accuracy': 0.8900585770606995, 'train/loss': 0.41066959500312805, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 158847.3594338894, 'total_duration': 171899.33880877495, 'accumulated_submission_time': 158847.3594338894, 'accumulated_eval_time': 13010.76617193222, 'accumulated_logging_time': 22.5986111164093}
I0310 04:46:16.378902 139708415854336 logging_writer.py:48] [355032] accumulated_eval_time=13010.766172, accumulated_logging_time=22.598611, accumulated_submission_time=158847.359434, global_step=355032, preemption_count=0, score=158847.359434, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=171899.338809, train/accuracy=0.890059, train/loss=0.410670, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 04:46:43.749502 139708407461632 logging_writer.py:48] [355100] global_step=355100, grad_norm=3.2709221839904785, loss=1.1452467441558838
I0310 04:47:27.412272 139708415854336 logging_writer.py:48] [355200] global_step=355200, grad_norm=3.1256167888641357, loss=1.056884765625
I0310 04:48:12.870539 139708407461632 logging_writer.py:48] [355300] global_step=355300, grad_norm=2.932251453399658, loss=2.2100400924682617
I0310 04:48:58.517409 139708415854336 logging_writer.py:48] [355400] global_step=355400, grad_norm=3.0379106998443604, loss=2.4816949367523193
I0310 04:49:43.812034 139708407461632 logging_writer.py:48] [355500] global_step=355500, grad_norm=3.0211431980133057, loss=1.1148388385772705
I0310 04:50:29.390648 139708415854336 logging_writer.py:48] [355600] global_step=355600, grad_norm=3.0794105529785156, loss=1.1351752281188965
I0310 04:51:14.828279 139708407461632 logging_writer.py:48] [355700] global_step=355700, grad_norm=3.0332138538360596, loss=1.0802165269851685
I0310 04:52:00.070590 139708415854336 logging_writer.py:48] [355800] global_step=355800, grad_norm=3.295987367630005, loss=1.1333696842193604
I0310 04:52:45.179996 139708407461632 logging_writer.py:48] [355900] global_step=355900, grad_norm=2.941096544265747, loss=1.1755403280258179
I0310 04:53:16.525452 139902746892096 spec.py:321] Evaluating on the training split.
I0310 04:53:28.039623 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 04:53:49.763056 139902746892096 spec.py:349] Evaluating on the test split.
I0310 04:53:51.436099 139902746892096 submission_runner.py:411] Time since start: 172354.46s, 	Step: 355971, 	{'train/accuracy': 0.8856835961341858, 'train/loss': 0.42187395691871643, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 159267.44840097427, 'total_duration': 172354.46488285065, 'accumulated_submission_time': 159267.44840097427, 'accumulated_eval_time': 13045.676797151566, 'accumulated_logging_time': 22.675989627838135}
I0310 04:53:51.505980 139708415854336 logging_writer.py:48] [355971] accumulated_eval_time=13045.676797, accumulated_logging_time=22.675990, accumulated_submission_time=159267.448401, global_step=355971, preemption_count=0, score=159267.448401, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=172354.464883, train/accuracy=0.885684, train/loss=0.421874, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 04:54:03.318609 139708407461632 logging_writer.py:48] [356000] global_step=356000, grad_norm=3.170440435409546, loss=1.5476484298706055
I0310 04:54:45.094302 139708415854336 logging_writer.py:48] [356100] global_step=356100, grad_norm=3.6930384635925293, loss=3.1987111568450928
I0310 04:55:30.310592 139708407461632 logging_writer.py:48] [356200] global_step=356200, grad_norm=3.051637887954712, loss=1.4557965993881226
I0310 04:56:15.809732 139708415854336 logging_writer.py:48] [356300] global_step=356300, grad_norm=2.994255542755127, loss=1.1663894653320312
I0310 04:57:01.294030 139708407461632 logging_writer.py:48] [356400] global_step=356400, grad_norm=3.199054479598999, loss=1.2633944749832153
I0310 04:57:46.776988 139708415854336 logging_writer.py:48] [356500] global_step=356500, grad_norm=3.7036285400390625, loss=3.246647596359253
I0310 04:58:32.495738 139708407461632 logging_writer.py:48] [356600] global_step=356600, grad_norm=3.1480422019958496, loss=1.1383812427520752
I0310 04:59:17.504330 139708415854336 logging_writer.py:48] [356700] global_step=356700, grad_norm=3.20293927192688, loss=2.671353340148926
I0310 05:00:02.770600 139708407461632 logging_writer.py:48] [356800] global_step=356800, grad_norm=3.363191843032837, loss=1.2874771356582642
I0310 05:00:47.955369 139708415854336 logging_writer.py:48] [356900] global_step=356900, grad_norm=3.036217451095581, loss=1.0563781261444092
I0310 05:00:51.662815 139902746892096 spec.py:321] Evaluating on the training split.
I0310 05:01:03.132670 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 05:01:25.778275 139902746892096 spec.py:349] Evaluating on the test split.
I0310 05:01:27.446672 139902746892096 submission_runner.py:411] Time since start: 172810.48s, 	Step: 356910, 	{'train/accuracy': 0.8889843821525574, 'train/loss': 0.4116753935813904, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 159687.5464732647, 'total_duration': 172810.47546195984, 'accumulated_submission_time': 159687.5464732647, 'accumulated_eval_time': 13081.4606487751, 'accumulated_logging_time': 22.755434274673462}
I0310 05:01:27.519469 139708407461632 logging_writer.py:48] [356910] accumulated_eval_time=13081.460649, accumulated_logging_time=22.755434, accumulated_submission_time=159687.546473, global_step=356910, preemption_count=0, score=159687.546473, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=172810.475462, train/accuracy=0.888984, train/loss=0.411675, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 05:02:03.431055 139708415854336 logging_writer.py:48] [357000] global_step=357000, grad_norm=3.1782171726226807, loss=1.293607473373413
I0310 05:02:48.588722 139708407461632 logging_writer.py:48] [357100] global_step=357100, grad_norm=3.705625057220459, loss=3.101425886154175
I0310 05:03:33.885584 139708415854336 logging_writer.py:48] [357200] global_step=357200, grad_norm=3.170837163925171, loss=1.5703178644180298
I0310 05:04:19.211922 139708407461632 logging_writer.py:48] [357300] global_step=357300, grad_norm=3.2524662017822266, loss=2.3902933597564697
I0310 05:05:04.276987 139708415854336 logging_writer.py:48] [357400] global_step=357400, grad_norm=3.2730274200439453, loss=2.6437597274780273
I0310 05:05:49.683294 139708407461632 logging_writer.py:48] [357500] global_step=357500, grad_norm=3.1523585319519043, loss=1.2685707807540894
I0310 05:06:35.077172 139708415854336 logging_writer.py:48] [357600] global_step=357600, grad_norm=3.0995187759399414, loss=2.3130366802215576
I0310 05:07:20.334402 139708407461632 logging_writer.py:48] [357700] global_step=357700, grad_norm=2.9933857917785645, loss=2.07869553565979
I0310 05:08:05.839607 139708415854336 logging_writer.py:48] [357800] global_step=357800, grad_norm=3.043135643005371, loss=1.0946106910705566
I0310 05:08:27.622902 139902746892096 spec.py:321] Evaluating on the training split.
I0310 05:08:39.111984 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 05:09:01.925903 139902746892096 spec.py:349] Evaluating on the test split.
I0310 05:09:03.596350 139902746892096 submission_runner.py:411] Time since start: 173266.63s, 	Step: 357849, 	{'train/accuracy': 0.8860546946525574, 'train/loss': 0.41632598638534546, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 160107.59187602997, 'total_duration': 173266.62514972687, 'accumulated_submission_time': 160107.59187602997, 'accumulated_eval_time': 13117.434097290039, 'accumulated_logging_time': 22.83749270439148}
I0310 05:09:03.666939 139708407461632 logging_writer.py:48] [357849] accumulated_eval_time=13117.434097, accumulated_logging_time=22.837493, accumulated_submission_time=160107.591876, global_step=357849, preemption_count=0, score=160107.591876, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=173266.625150, train/accuracy=0.886055, train/loss=0.416326, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 05:09:24.143327 139708415854336 logging_writer.py:48] [357900] global_step=357900, grad_norm=2.980881929397583, loss=1.847606897354126
I0310 05:10:07.377519 139708407461632 logging_writer.py:48] [358000] global_step=358000, grad_norm=2.904440402984619, loss=1.1666548252105713
I0310 05:10:52.749392 139708415854336 logging_writer.py:48] [358100] global_step=358100, grad_norm=2.827721357345581, loss=1.5427056550979614
I0310 05:11:38.511848 139708407461632 logging_writer.py:48] [358200] global_step=358200, grad_norm=3.2360076904296875, loss=1.1629024744033813
I0310 05:12:23.931441 139708415854336 logging_writer.py:48] [358300] global_step=358300, grad_norm=3.204939603805542, loss=1.107906460762024
I0310 05:13:09.255083 139708407461632 logging_writer.py:48] [358400] global_step=358400, grad_norm=3.9099276065826416, loss=3.137819766998291
I0310 05:13:54.660547 139708415854336 logging_writer.py:48] [358500] global_step=358500, grad_norm=3.2697699069976807, loss=2.459941864013672
I0310 05:14:39.910542 139708407461632 logging_writer.py:48] [358600] global_step=358600, grad_norm=3.4394171237945557, loss=1.1405844688415527
I0310 05:15:25.343113 139708415854336 logging_writer.py:48] [358700] global_step=358700, grad_norm=3.1420764923095703, loss=1.4676272869110107
I0310 05:16:03.971127 139902746892096 spec.py:321] Evaluating on the training split.
I0310 05:16:15.611851 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 05:16:36.432221 139902746892096 spec.py:349] Evaluating on the test split.
I0310 05:16:38.119610 139902746892096 submission_runner.py:411] Time since start: 173721.15s, 	Step: 358787, 	{'train/accuracy': 0.8892577886581421, 'train/loss': 0.4093237519264221, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 160527.83763742447, 'total_duration': 173721.14839720726, 'accumulated_submission_time': 160527.83763742447, 'accumulated_eval_time': 13151.582559347153, 'accumulated_logging_time': 22.91771149635315}
I0310 05:16:38.204443 139708407461632 logging_writer.py:48] [358787] accumulated_eval_time=13151.582559, accumulated_logging_time=22.917711, accumulated_submission_time=160527.837637, global_step=358787, preemption_count=0, score=160527.837637, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=173721.148397, train/accuracy=0.889258, train/loss=0.409324, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 05:16:43.719314 139708415854336 logging_writer.py:48] [358800] global_step=358800, grad_norm=2.967935562133789, loss=1.6659481525421143
I0310 05:17:25.058993 139708407461632 logging_writer.py:48] [358900] global_step=358900, grad_norm=2.975142240524292, loss=2.09438419342041
I0310 05:18:10.510109 139708415854336 logging_writer.py:48] [359000] global_step=359000, grad_norm=3.325875759124756, loss=1.1399985551834106
I0310 05:18:56.054829 139708407461632 logging_writer.py:48] [359100] global_step=359100, grad_norm=2.885784387588501, loss=1.4184093475341797
I0310 05:19:41.620963 139708415854336 logging_writer.py:48] [359200] global_step=359200, grad_norm=3.0009217262268066, loss=1.2051655054092407
I0310 05:20:26.700508 139708407461632 logging_writer.py:48] [359300] global_step=359300, grad_norm=3.1922149658203125, loss=1.2500574588775635
I0310 05:21:11.999368 139708415854336 logging_writer.py:48] [359400] global_step=359400, grad_norm=2.8913586139678955, loss=1.2807937860488892
I0310 05:21:57.133429 139708407461632 logging_writer.py:48] [359500] global_step=359500, grad_norm=3.1690680980682373, loss=1.2485281229019165
I0310 05:22:42.400928 139708415854336 logging_writer.py:48] [359600] global_step=359600, grad_norm=2.8198630809783936, loss=1.815322756767273
I0310 05:23:27.599290 139708407461632 logging_writer.py:48] [359700] global_step=359700, grad_norm=2.9323954582214355, loss=1.1056733131408691
I0310 05:23:38.138165 139902746892096 spec.py:321] Evaluating on the training split.
I0310 05:23:49.479965 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 05:24:12.158848 139902746892096 spec.py:349] Evaluating on the test split.
I0310 05:24:13.834145 139902746892096 submission_runner.py:411] Time since start: 174176.86s, 	Step: 359725, 	{'train/accuracy': 0.8866406083106995, 'train/loss': 0.4209662973880768, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 160947.71241402626, 'total_duration': 174176.86294460297, 'accumulated_submission_time': 160947.71241402626, 'accumulated_eval_time': 13187.27852511406, 'accumulated_logging_time': 23.012847900390625}
I0310 05:24:13.902766 139708415854336 logging_writer.py:48] [359725] accumulated_eval_time=13187.278525, accumulated_logging_time=23.012848, accumulated_submission_time=160947.712414, global_step=359725, preemption_count=0, score=160947.712414, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=174176.862945, train/accuracy=0.886641, train/loss=0.420966, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 05:24:43.809949 139708407461632 logging_writer.py:48] [359800] global_step=359800, grad_norm=3.123721122741699, loss=1.0884995460510254
I0310 05:25:27.651093 139708415854336 logging_writer.py:48] [359900] global_step=359900, grad_norm=2.945488691329956, loss=1.5759310722351074
I0310 05:26:12.800924 139708407461632 logging_writer.py:48] [360000] global_step=360000, grad_norm=2.8434903621673584, loss=1.4281527996063232
I0310 05:26:58.191341 139708415854336 logging_writer.py:48] [360100] global_step=360100, grad_norm=3.10366153717041, loss=1.1044543981552124
I0310 05:27:43.534235 139708407461632 logging_writer.py:48] [360200] global_step=360200, grad_norm=4.1878509521484375, loss=3.087461471557617
I0310 05:28:28.835757 139708415854336 logging_writer.py:48] [360300] global_step=360300, grad_norm=3.466932535171509, loss=3.124668598175049
I0310 05:29:14.506671 139708407461632 logging_writer.py:48] [360400] global_step=360400, grad_norm=3.2527525424957275, loss=1.0853711366653442
I0310 05:29:59.681244 139708415854336 logging_writer.py:48] [360500] global_step=360500, grad_norm=5.477329730987549, loss=3.2505757808685303
I0310 05:30:45.091398 139708407461632 logging_writer.py:48] [360600] global_step=360600, grad_norm=4.01435661315918, loss=3.2686216831207275
I0310 05:31:13.903914 139902746892096 spec.py:321] Evaluating on the training split.
I0310 05:31:25.550113 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 05:31:48.361442 139902746892096 spec.py:349] Evaluating on the test split.
I0310 05:31:50.031218 139902746892096 submission_runner.py:411] Time since start: 174633.06s, 	Step: 360665, 	{'train/accuracy': 0.8871093392372131, 'train/loss': 0.42082682251930237, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 161367.6559035778, 'total_duration': 174633.06001091003, 'accumulated_submission_time': 161367.6559035778, 'accumulated_eval_time': 13223.405816078186, 'accumulated_logging_time': 23.090537786483765}
I0310 05:31:50.101351 139708415854336 logging_writer.py:48] [360665] accumulated_eval_time=13223.405816, accumulated_logging_time=23.090538, accumulated_submission_time=161367.655904, global_step=360665, preemption_count=0, score=161367.655904, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=174633.060011, train/accuracy=0.887109, train/loss=0.420827, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 05:32:04.281138 139708407461632 logging_writer.py:48] [360700] global_step=360700, grad_norm=3.1237754821777344, loss=1.9586724042892456
I0310 05:32:46.216506 139708415854336 logging_writer.py:48] [360800] global_step=360800, grad_norm=2.9397072792053223, loss=2.0117671489715576
I0310 05:33:31.603162 139708407461632 logging_writer.py:48] [360900] global_step=360900, grad_norm=3.218538284301758, loss=1.259080410003662
I0310 05:34:17.359163 139708415854336 logging_writer.py:48] [361000] global_step=361000, grad_norm=3.0440962314605713, loss=1.1036256551742554
I0310 05:35:02.587954 139708407461632 logging_writer.py:48] [361100] global_step=361100, grad_norm=3.2220845222473145, loss=1.1913622617721558
I0310 05:35:48.030879 139708415854336 logging_writer.py:48] [361200] global_step=361200, grad_norm=3.1984028816223145, loss=1.2447224855422974
I0310 05:36:33.426286 139708407461632 logging_writer.py:48] [361300] global_step=361300, grad_norm=3.150020122528076, loss=1.4662244319915771
I0310 05:37:18.725311 139708415854336 logging_writer.py:48] [361400] global_step=361400, grad_norm=3.2472023963928223, loss=1.1580158472061157
I0310 05:38:04.332298 139708407461632 logging_writer.py:48] [361500] global_step=361500, grad_norm=3.669631242752075, loss=3.176553249359131
I0310 05:38:50.014595 139708415854336 logging_writer.py:48] [361600] global_step=361600, grad_norm=2.859480619430542, loss=1.6834988594055176
I0310 05:38:50.159631 139902746892096 spec.py:321] Evaluating on the training split.
I0310 05:39:01.658620 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 05:39:24.364610 139902746892096 spec.py:349] Evaluating on the test split.
I0310 05:39:26.040621 139902746892096 submission_runner.py:411] Time since start: 175089.07s, 	Step: 361602, 	{'train/accuracy': 0.8885937333106995, 'train/loss': 0.41361212730407715, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 161787.65626049042, 'total_duration': 175089.06940317154, 'accumulated_submission_time': 161787.65626049042, 'accumulated_eval_time': 13259.286784172058, 'accumulated_logging_time': 23.16886854171753}
I0310 05:39:26.120157 139708407461632 logging_writer.py:48] [361602] accumulated_eval_time=13259.286784, accumulated_logging_time=23.168869, accumulated_submission_time=161787.656260, global_step=361602, preemption_count=0, score=161787.656260, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=175089.069403, train/accuracy=0.888594, train/loss=0.413612, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 05:40:05.590367 139708415854336 logging_writer.py:48] [361700] global_step=361700, grad_norm=3.5223090648651123, loss=1.1054855585098267
I0310 05:40:50.789281 139708407461632 logging_writer.py:48] [361800] global_step=361800, grad_norm=3.079888343811035, loss=1.257667064666748
I0310 05:41:36.353972 139708415854336 logging_writer.py:48] [361900] global_step=361900, grad_norm=2.9116311073303223, loss=2.497826099395752
I0310 05:42:21.654246 139708407461632 logging_writer.py:48] [362000] global_step=362000, grad_norm=3.167691469192505, loss=1.1874196529388428
I0310 05:43:07.155261 139708415854336 logging_writer.py:48] [362100] global_step=362100, grad_norm=3.633084774017334, loss=2.903717517852783
I0310 05:43:52.586937 139708407461632 logging_writer.py:48] [362200] global_step=362200, grad_norm=3.2060720920562744, loss=3.00006103515625
I0310 05:44:37.698514 139708415854336 logging_writer.py:48] [362300] global_step=362300, grad_norm=3.357445478439331, loss=1.1307547092437744
I0310 05:45:23.125543 139708407461632 logging_writer.py:48] [362400] global_step=362400, grad_norm=3.0289394855499268, loss=1.3286656141281128
I0310 05:46:08.433762 139708415854336 logging_writer.py:48] [362500] global_step=362500, grad_norm=3.2975950241088867, loss=1.791283130645752
I0310 05:46:26.252795 139902746892096 spec.py:321] Evaluating on the training split.
I0310 05:46:37.698991 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 05:47:00.964396 139902746892096 spec.py:349] Evaluating on the test split.
I0310 05:47:02.639388 139902746892096 submission_runner.py:411] Time since start: 175545.67s, 	Step: 362541, 	{'train/accuracy': 0.8882616758346558, 'train/loss': 0.4161655008792877, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 162207.73133540154, 'total_duration': 175545.66818594933, 'accumulated_submission_time': 162207.73133540154, 'accumulated_eval_time': 13295.67337846756, 'accumulated_logging_time': 23.257583141326904}
I0310 05:47:02.710051 139708407461632 logging_writer.py:48] [362541] accumulated_eval_time=13295.673378, accumulated_logging_time=23.257583, accumulated_submission_time=162207.731335, global_step=362541, preemption_count=0, score=162207.731335, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=175545.668186, train/accuracy=0.888262, train/loss=0.416166, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 05:47:26.339415 139708415854336 logging_writer.py:48] [362600] global_step=362600, grad_norm=3.169921875, loss=1.1907671689987183
I0310 05:48:10.220598 139708407461632 logging_writer.py:48] [362700] global_step=362700, grad_norm=3.1881415843963623, loss=2.3069047927856445
I0310 05:48:55.472254 139708415854336 logging_writer.py:48] [362800] global_step=362800, grad_norm=3.080085277557373, loss=1.4272820949554443
I0310 05:49:41.285742 139708407461632 logging_writer.py:48] [362900] global_step=362900, grad_norm=2.9573161602020264, loss=1.0173401832580566
I0310 05:50:26.243159 139708415854336 logging_writer.py:48] [363000] global_step=363000, grad_norm=3.1982595920562744, loss=1.1823503971099854
I0310 05:51:11.888660 139708407461632 logging_writer.py:48] [363100] global_step=363100, grad_norm=3.3944902420043945, loss=1.1915148496627808
I0310 05:51:57.302042 139708415854336 logging_writer.py:48] [363200] global_step=363200, grad_norm=3.042850971221924, loss=1.3705124855041504
I0310 05:52:42.507342 139708407461632 logging_writer.py:48] [363300] global_step=363300, grad_norm=3.217926025390625, loss=1.1500787734985352
I0310 05:53:27.814369 139708415854336 logging_writer.py:48] [363400] global_step=363400, grad_norm=3.1986308097839355, loss=1.1675176620483398
I0310 05:54:02.830503 139902746892096 spec.py:321] Evaluating on the training split.
I0310 05:54:14.278685 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 05:54:38.723508 139902746892096 spec.py:349] Evaluating on the test split.
I0310 05:54:40.399573 139902746892096 submission_runner.py:411] Time since start: 176003.43s, 	Step: 363479, 	{'train/accuracy': 0.8877929449081421, 'train/loss': 0.41626253724098206, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 162627.79280495644, 'total_duration': 176003.4283466339, 'accumulated_submission_time': 162627.79280495644, 'accumulated_eval_time': 13333.242425441742, 'accumulated_logging_time': 23.33720898628235}
I0310 05:54:40.478360 139708407461632 logging_writer.py:48] [363479] accumulated_eval_time=13333.242425, accumulated_logging_time=23.337209, accumulated_submission_time=162627.792805, global_step=363479, preemption_count=0, score=162627.792805, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=176003.428347, train/accuracy=0.887793, train/loss=0.416263, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 05:54:49.131922 139708415854336 logging_writer.py:48] [363500] global_step=363500, grad_norm=3.0589778423309326, loss=1.2826405763626099
I0310 05:55:30.453204 139708407461632 logging_writer.py:48] [363600] global_step=363600, grad_norm=3.2311441898345947, loss=1.1649550199508667
I0310 05:56:15.620825 139708415854336 logging_writer.py:48] [363700] global_step=363700, grad_norm=3.8394925594329834, loss=3.191986560821533
I0310 05:57:00.874833 139708407461632 logging_writer.py:48] [363800] global_step=363800, grad_norm=3.058946132659912, loss=1.5340896844863892
I0310 05:57:46.552614 139708415854336 logging_writer.py:48] [363900] global_step=363900, grad_norm=3.83536696434021, loss=3.167654275894165
I0310 05:58:31.851791 139708407461632 logging_writer.py:48] [364000] global_step=364000, grad_norm=3.116819143295288, loss=1.0277574062347412
I0310 05:59:17.736361 139708415854336 logging_writer.py:48] [364100] global_step=364100, grad_norm=3.153850555419922, loss=1.0875129699707031
I0310 06:00:02.936312 139708407461632 logging_writer.py:48] [364200] global_step=364200, grad_norm=2.9194600582122803, loss=1.1112655401229858
I0310 06:00:48.326277 139708415854336 logging_writer.py:48] [364300] global_step=364300, grad_norm=3.0859715938568115, loss=1.1812045574188232
I0310 06:01:33.783054 139708407461632 logging_writer.py:48] [364400] global_step=364400, grad_norm=3.0552611351013184, loss=1.094285488128662
I0310 06:01:40.670885 139902746892096 spec.py:321] Evaluating on the training split.
I0310 06:01:52.011811 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 06:02:12.614578 139902746892096 spec.py:349] Evaluating on the test split.
I0310 06:02:14.293882 139902746892096 submission_runner.py:411] Time since start: 176457.32s, 	Step: 364417, 	{'train/accuracy': 0.8885741829872131, 'train/loss': 0.41587018966674805, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 163047.926517725, 'total_duration': 176457.32267189026, 'accumulated_submission_time': 163047.926517725, 'accumulated_eval_time': 13366.86541056633, 'accumulated_logging_time': 23.42582654953003}
I0310 06:02:14.379023 139708415854336 logging_writer.py:48] [364417] accumulated_eval_time=13366.865411, accumulated_logging_time=23.425827, accumulated_submission_time=163047.926518, global_step=364417, preemption_count=0, score=163047.926518, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=176457.322672, train/accuracy=0.888574, train/loss=0.415870, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 06:02:47.759968 139708407461632 logging_writer.py:48] [364500] global_step=364500, grad_norm=3.2090086936950684, loss=1.130712866783142
I0310 06:03:33.156890 139708415854336 logging_writer.py:48] [364600] global_step=364600, grad_norm=3.1400930881500244, loss=1.0997700691223145
I0310 06:04:18.700764 139708407461632 logging_writer.py:48] [364700] global_step=364700, grad_norm=3.0371274948120117, loss=2.0100972652435303
I0310 06:05:04.389107 139708415854336 logging_writer.py:48] [364800] global_step=364800, grad_norm=2.8410871028900146, loss=0.9810107350349426
I0310 06:05:49.571635 139708407461632 logging_writer.py:48] [364900] global_step=364900, grad_norm=3.2799746990203857, loss=2.4366166591644287
I0310 06:06:35.073765 139708415854336 logging_writer.py:48] [365000] global_step=365000, grad_norm=2.8254988193511963, loss=1.81489896774292
I0310 06:07:20.647928 139708407461632 logging_writer.py:48] [365100] global_step=365100, grad_norm=2.9817817211151123, loss=1.0795660018920898
I0310 06:08:06.270502 139708415854336 logging_writer.py:48] [365200] global_step=365200, grad_norm=3.3324191570281982, loss=2.7651376724243164
I0310 06:08:51.734514 139708407461632 logging_writer.py:48] [365300] global_step=365300, grad_norm=3.098135232925415, loss=1.0247255563735962
I0310 06:09:14.473369 139902746892096 spec.py:321] Evaluating on the training split.
I0310 06:09:26.005715 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 06:09:47.849201 139902746892096 spec.py:349] Evaluating on the test split.
I0310 06:09:49.529673 139902746892096 submission_runner.py:411] Time since start: 176912.56s, 	Step: 365351, 	{'train/accuracy': 0.887499988079071, 'train/loss': 0.4217265546321869, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 163467.9629137516, 'total_duration': 176912.55845713615, 'accumulated_submission_time': 163467.9629137516, 'accumulated_eval_time': 13401.921688556671, 'accumulated_logging_time': 23.520427465438843}
I0310 06:09:49.608360 139708415854336 logging_writer.py:48] [365351] accumulated_eval_time=13401.921689, accumulated_logging_time=23.520427, accumulated_submission_time=163467.962914, global_step=365351, preemption_count=0, score=163467.962914, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=176912.558457, train/accuracy=0.887500, train/loss=0.421727, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 06:10:09.283360 139708407461632 logging_writer.py:48] [365400] global_step=365400, grad_norm=3.3550987243652344, loss=1.0539718866348267
I0310 06:10:51.777734 139708415854336 logging_writer.py:48] [365500] global_step=365500, grad_norm=2.939513921737671, loss=2.2098679542541504
I0310 06:11:36.994149 139708407461632 logging_writer.py:48] [365600] global_step=365600, grad_norm=3.1735293865203857, loss=1.2070423364639282
I0310 06:12:22.550528 139708415854336 logging_writer.py:48] [365700] global_step=365700, grad_norm=2.878750801086426, loss=2.175741195678711
I0310 06:13:07.825759 139708407461632 logging_writer.py:48] [365800] global_step=365800, grad_norm=3.3499374389648438, loss=2.7910072803497314
I0310 06:13:53.303589 139708415854336 logging_writer.py:48] [365900] global_step=365900, grad_norm=3.1601006984710693, loss=2.16245174407959
I0310 06:14:38.612408 139708407461632 logging_writer.py:48] [366000] global_step=366000, grad_norm=3.31913423538208, loss=1.1357195377349854
I0310 06:15:24.012507 139708415854336 logging_writer.py:48] [366100] global_step=366100, grad_norm=3.037949323654175, loss=1.1522655487060547
I0310 06:16:09.422205 139708407461632 logging_writer.py:48] [366200] global_step=366200, grad_norm=3.1171963214874268, loss=1.1417858600616455
I0310 06:16:49.662607 139902746892096 spec.py:321] Evaluating on the training split.
I0310 06:17:00.872963 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 06:17:22.355336 139902746892096 spec.py:349] Evaluating on the test split.
I0310 06:17:24.027488 139902746892096 submission_runner.py:411] Time since start: 177367.06s, 	Step: 366291, 	{'train/accuracy': 0.8881444931030273, 'train/loss': 0.4135344624519348, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 163887.9586417675, 'total_duration': 177367.0562813282, 'accumulated_submission_time': 163887.9586417675, 'accumulated_eval_time': 13436.286578655243, 'accumulated_logging_time': 23.60895323753357}
I0310 06:17:24.097431 139708415854336 logging_writer.py:48] [366291] accumulated_eval_time=13436.286579, accumulated_logging_time=23.608953, accumulated_submission_time=163887.958642, global_step=366291, preemption_count=0, score=163887.958642, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=177367.056281, train/accuracy=0.888144, train/loss=0.413534, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 06:17:28.042807 139708407461632 logging_writer.py:48] [366300] global_step=366300, grad_norm=4.668236255645752, loss=3.178295850753784
I0310 06:18:08.794464 139708415854336 logging_writer.py:48] [366400] global_step=366400, grad_norm=3.3419113159179688, loss=2.8820278644561768
I0310 06:18:53.912276 139708407461632 logging_writer.py:48] [366500] global_step=366500, grad_norm=3.4834227561950684, loss=1.082544207572937
I0310 06:19:39.330765 139708415854336 logging_writer.py:48] [366600] global_step=366600, grad_norm=3.0734357833862305, loss=1.8200249671936035
I0310 06:20:24.700595 139708407461632 logging_writer.py:48] [366700] global_step=366700, grad_norm=3.2398810386657715, loss=1.161851167678833
I0310 06:21:09.846615 139708415854336 logging_writer.py:48] [366800] global_step=366800, grad_norm=3.3056933879852295, loss=1.1833131313323975
I0310 06:21:55.432862 139708407461632 logging_writer.py:48] [366900] global_step=366900, grad_norm=3.068537712097168, loss=1.9379454851150513
I0310 06:22:40.433041 139708415854336 logging_writer.py:48] [367000] global_step=367000, grad_norm=3.0811805725097656, loss=2.4237327575683594
I0310 06:23:25.844889 139708407461632 logging_writer.py:48] [367100] global_step=367100, grad_norm=4.118616580963135, loss=3.104456663131714
I0310 06:24:11.413342 139708415854336 logging_writer.py:48] [367200] global_step=367200, grad_norm=3.3062312602996826, loss=2.1405935287475586
I0310 06:24:24.066333 139902746892096 spec.py:321] Evaluating on the training split.
I0310 06:24:35.555890 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 06:24:56.116542 139902746892096 spec.py:349] Evaluating on the test split.
I0310 06:24:57.803173 139902746892096 submission_runner.py:411] Time since start: 177820.83s, 	Step: 367230, 	{'train/accuracy': 0.8866796493530273, 'train/loss': 0.41895416378974915, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 164307.8692791462, 'total_duration': 177820.83196425438, 'accumulated_submission_time': 164307.8692791462, 'accumulated_eval_time': 13470.023416519165, 'accumulated_logging_time': 23.688948154449463}
I0310 06:24:57.876996 139708407461632 logging_writer.py:48] [367230] accumulated_eval_time=13470.023417, accumulated_logging_time=23.688948, accumulated_submission_time=164307.869279, global_step=367230, preemption_count=0, score=164307.869279, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=177820.831964, train/accuracy=0.886680, train/loss=0.418954, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 06:25:25.816496 139708415854336 logging_writer.py:48] [367300] global_step=367300, grad_norm=3.229048252105713, loss=2.70402455329895
I0310 06:26:09.922774 139708407461632 logging_writer.py:48] [367400] global_step=367400, grad_norm=3.1229677200317383, loss=1.3496226072311401
I0310 06:26:54.955081 139708415854336 logging_writer.py:48] [367500] global_step=367500, grad_norm=2.9203057289123535, loss=2.16435170173645
I0310 06:27:40.476676 139708407461632 logging_writer.py:48] [367600] global_step=367600, grad_norm=2.9940035343170166, loss=1.206003189086914
I0310 06:28:25.776241 139708415854336 logging_writer.py:48] [367700] global_step=367700, grad_norm=3.0239756107330322, loss=1.9090220928192139
I0310 06:29:10.754387 139708407461632 logging_writer.py:48] [367800] global_step=367800, grad_norm=3.1228556632995605, loss=1.5254251956939697
I0310 06:29:56.595762 139708415854336 logging_writer.py:48] [367900] global_step=367900, grad_norm=3.0460727214813232, loss=1.95694899559021
I0310 06:30:41.817873 139708407461632 logging_writer.py:48] [368000] global_step=368000, grad_norm=3.0971426963806152, loss=1.1455042362213135
I0310 06:31:27.167282 139708415854336 logging_writer.py:48] [368100] global_step=368100, grad_norm=3.7090036869049072, loss=2.814120292663574
I0310 06:31:58.155169 139902746892096 spec.py:321] Evaluating on the training split.
I0310 06:32:09.757991 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 06:32:31.211712 139902746892096 spec.py:349] Evaluating on the test split.
I0310 06:32:32.884432 139902746892096 submission_runner.py:411] Time since start: 178275.91s, 	Step: 368170, 	{'train/accuracy': 0.8882421851158142, 'train/loss': 0.41343486309051514, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 164728.0875532627, 'total_duration': 178275.9132180214, 'accumulated_submission_time': 164728.0875532627, 'accumulated_eval_time': 13504.752668857574, 'accumulated_logging_time': 23.773596048355103}
I0310 06:32:32.955235 139708407461632 logging_writer.py:48] [368170] accumulated_eval_time=13504.752669, accumulated_logging_time=23.773596, accumulated_submission_time=164728.087553, global_step=368170, preemption_count=0, score=164728.087553, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=178275.913218, train/accuracy=0.888242, train/loss=0.413435, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 06:32:45.149615 139708415854336 logging_writer.py:48] [368200] global_step=368200, grad_norm=2.8591465950012207, loss=1.7052381038665771
I0310 06:33:26.959565 139708407461632 logging_writer.py:48] [368300] global_step=368300, grad_norm=3.0268843173980713, loss=1.134922742843628
I0310 06:34:12.277424 139708415854336 logging_writer.py:48] [368400] global_step=368400, grad_norm=4.724244594573975, loss=2.653535842895508
I0310 06:34:57.768449 139708407461632 logging_writer.py:48] [368500] global_step=368500, grad_norm=3.210218667984009, loss=1.6923928260803223
I0310 06:35:43.039528 139708415854336 logging_writer.py:48] [368600] global_step=368600, grad_norm=3.6594371795654297, loss=3.0981318950653076
I0310 06:36:28.240337 139708407461632 logging_writer.py:48] [368700] global_step=368700, grad_norm=3.201456308364868, loss=1.3643745183944702
I0310 06:37:13.725484 139708415854336 logging_writer.py:48] [368800] global_step=368800, grad_norm=3.4291486740112305, loss=2.1873230934143066
I0310 06:37:58.894828 139708407461632 logging_writer.py:48] [368900] global_step=368900, grad_norm=3.364023447036743, loss=1.1564117670059204
I0310 06:38:44.181629 139708415854336 logging_writer.py:48] [369000] global_step=369000, grad_norm=2.889457941055298, loss=2.2000041007995605
I0310 06:39:29.882895 139708407461632 logging_writer.py:48] [369100] global_step=369100, grad_norm=2.816127061843872, loss=1.4271574020385742
I0310 06:39:33.171094 139902746892096 spec.py:321] Evaluating on the training split.
I0310 06:39:44.420999 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 06:40:05.813363 139902746892096 spec.py:349] Evaluating on the test split.
I0310 06:40:07.491324 139902746892096 submission_runner.py:411] Time since start: 178730.52s, 	Step: 369109, 	{'train/accuracy': 0.8899999856948853, 'train/loss': 0.41148337721824646, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 165148.242303133, 'total_duration': 178730.52012515068, 'accumulated_submission_time': 165148.242303133, 'accumulated_eval_time': 13539.072884321213, 'accumulated_logging_time': 23.85645580291748}
I0310 06:40:07.563074 139708415854336 logging_writer.py:48] [369109] accumulated_eval_time=13539.072884, accumulated_logging_time=23.856456, accumulated_submission_time=165148.242303, global_step=369109, preemption_count=0, score=165148.242303, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=178730.520125, train/accuracy=0.890000, train/loss=0.411483, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 06:40:44.177016 139708407461632 logging_writer.py:48] [369200] global_step=369200, grad_norm=3.171825408935547, loss=1.158854365348816
I0310 06:41:29.214589 139708415854336 logging_writer.py:48] [369300] global_step=369300, grad_norm=2.984631061553955, loss=1.988570213317871
I0310 06:42:14.731851 139708407461632 logging_writer.py:48] [369400] global_step=369400, grad_norm=2.8614039421081543, loss=1.479842185974121
I0310 06:43:00.143403 139708415854336 logging_writer.py:48] [369500] global_step=369500, grad_norm=3.53867506980896, loss=2.825117588043213
I0310 06:43:45.623577 139708407461632 logging_writer.py:48] [369600] global_step=369600, grad_norm=3.1443355083465576, loss=1.739086389541626
I0310 06:44:31.285257 139708415854336 logging_writer.py:48] [369700] global_step=369700, grad_norm=3.167745590209961, loss=1.0422120094299316
I0310 06:45:16.709905 139708407461632 logging_writer.py:48] [369800] global_step=369800, grad_norm=3.8105077743530273, loss=3.0245211124420166
I0310 06:46:02.037815 139708415854336 logging_writer.py:48] [369900] global_step=369900, grad_norm=3.1715641021728516, loss=2.087381601333618
I0310 06:46:47.416570 139708407461632 logging_writer.py:48] [370000] global_step=370000, grad_norm=3.6906239986419678, loss=1.3639265298843384
I0310 06:47:07.534579 139902746892096 spec.py:321] Evaluating on the training split.
I0310 06:47:19.232501 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 06:47:40.909105 139902746892096 spec.py:349] Evaluating on the test split.
I0310 06:47:42.585769 139902746892096 submission_runner.py:411] Time since start: 179185.61s, 	Step: 370046, 	{'train/accuracy': 0.888476550579071, 'train/loss': 0.41124534606933594, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 165568.15756726265, 'total_duration': 179185.61457157135, 'accumulated_submission_time': 165568.15756726265, 'accumulated_eval_time': 13574.1240670681, 'accumulated_logging_time': 23.93623661994934}
I0310 06:47:42.656659 139708415854336 logging_writer.py:48] [370046] accumulated_eval_time=13574.124067, accumulated_logging_time=23.936237, accumulated_submission_time=165568.157567, global_step=370046, preemption_count=0, score=165568.157567, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=179185.614572, train/accuracy=0.888477, train/loss=0.411245, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 06:48:04.295569 139708407461632 logging_writer.py:48] [370100] global_step=370100, grad_norm=3.0775418281555176, loss=1.9446876049041748
I0310 06:48:47.386659 139708415854336 logging_writer.py:48] [370200] global_step=370200, grad_norm=3.1666550636291504, loss=1.4001080989837646
I0310 06:49:32.822836 139708407461632 logging_writer.py:48] [370300] global_step=370300, grad_norm=3.111259937286377, loss=1.0803837776184082
I0310 06:50:18.736333 139708415854336 logging_writer.py:48] [370400] global_step=370400, grad_norm=3.2398135662078857, loss=1.4148914813995361
I0310 06:51:03.924877 139708407461632 logging_writer.py:48] [370500] global_step=370500, grad_norm=3.53266978263855, loss=1.7044382095336914
I0310 06:51:49.395519 139708415854336 logging_writer.py:48] [370600] global_step=370600, grad_norm=2.9117746353149414, loss=1.0839639902114868
I0310 06:52:34.773232 139708407461632 logging_writer.py:48] [370700] global_step=370700, grad_norm=2.998735189437866, loss=1.0759857892990112
I0310 06:53:20.067036 139708415854336 logging_writer.py:48] [370800] global_step=370800, grad_norm=2.9594552516937256, loss=1.4547204971313477
I0310 06:54:05.296729 139708407461632 logging_writer.py:48] [370900] global_step=370900, grad_norm=3.289842367172241, loss=1.0979888439178467
I0310 06:54:43.043815 139902746892096 spec.py:321] Evaluating on the training split.
I0310 06:54:54.408323 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 06:55:14.903332 139902746892096 spec.py:349] Evaluating on the test split.
I0310 06:55:16.590513 139902746892096 submission_runner.py:411] Time since start: 179639.62s, 	Step: 370985, 	{'train/accuracy': 0.8868359327316284, 'train/loss': 0.4194703996181488, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 165988.48790717125, 'total_duration': 179639.6193087101, 'accumulated_submission_time': 165988.48790717125, 'accumulated_eval_time': 13607.670770168304, 'accumulated_logging_time': 24.01575541496277}
I0310 06:55:16.661410 139708415854336 logging_writer.py:48] [370985] accumulated_eval_time=13607.670770, accumulated_logging_time=24.015755, accumulated_submission_time=165988.487907, global_step=370985, preemption_count=0, score=165988.487907, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=179639.619309, train/accuracy=0.886836, train/loss=0.419470, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 06:55:22.961153 139708407461632 logging_writer.py:48] [371000] global_step=371000, grad_norm=3.7533676624298096, loss=3.205155372619629
I0310 06:56:03.980909 139708415854336 logging_writer.py:48] [371100] global_step=371100, grad_norm=3.071082830429077, loss=2.4428353309631348
I0310 06:56:48.981652 139708407461632 logging_writer.py:48] [371200] global_step=371200, grad_norm=3.5015432834625244, loss=3.0158281326293945
I0310 06:57:34.226342 139708415854336 logging_writer.py:48] [371300] global_step=371300, grad_norm=2.9657704830169678, loss=1.9325273036956787
I0310 06:58:19.574100 139708407461632 logging_writer.py:48] [371400] global_step=371400, grad_norm=2.7625112533569336, loss=1.7540502548217773
I0310 06:59:04.444677 139708415854336 logging_writer.py:48] [371500] global_step=371500, grad_norm=3.0313222408294678, loss=2.338151216506958
I0310 06:59:49.914908 139708407461632 logging_writer.py:48] [371600] global_step=371600, grad_norm=3.081549882888794, loss=1.1108360290527344
I0310 07:00:34.968686 139708415854336 logging_writer.py:48] [371700] global_step=371700, grad_norm=3.4444141387939453, loss=1.0879367589950562
I0310 07:01:20.315535 139708407461632 logging_writer.py:48] [371800] global_step=371800, grad_norm=3.354109048843384, loss=1.229358196258545
I0310 07:02:05.706978 139708415854336 logging_writer.py:48] [371900] global_step=371900, grad_norm=3.3232460021972656, loss=2.7659478187561035
I0310 07:02:16.689545 139902746892096 spec.py:321] Evaluating on the training split.
I0310 07:02:28.138162 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 07:02:49.383593 139902746892096 spec.py:349] Evaluating on the test split.
I0310 07:02:51.058198 139902746892096 submission_runner.py:411] Time since start: 180094.09s, 	Step: 371926, 	{'train/accuracy': 0.8890234231948853, 'train/loss': 0.41407310962677, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 166408.45634293556, 'total_duration': 180094.0869989395, 'accumulated_submission_time': 166408.45634293556, 'accumulated_eval_time': 13642.039429187775, 'accumulated_logging_time': 24.097371816635132}
I0310 07:02:51.128095 139708407461632 logging_writer.py:48] [371926] accumulated_eval_time=13642.039429, accumulated_logging_time=24.097372, accumulated_submission_time=166408.456343, global_step=371926, preemption_count=0, score=166408.456343, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=180094.086999, train/accuracy=0.889023, train/loss=0.414073, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 07:03:20.628306 139708415854336 logging_writer.py:48] [372000] global_step=372000, grad_norm=3.2140069007873535, loss=1.1174860000610352
I0310 07:04:04.731600 139708407461632 logging_writer.py:48] [372100] global_step=372100, grad_norm=3.1419641971588135, loss=1.7036722898483276
I0310 07:04:50.077091 139708415854336 logging_writer.py:48] [372200] global_step=372200, grad_norm=3.3774406909942627, loss=1.4144622087478638
I0310 07:05:35.367473 139708407461632 logging_writer.py:48] [372300] global_step=372300, grad_norm=3.190317392349243, loss=1.1283915042877197
I0310 07:06:20.625516 139708415854336 logging_writer.py:48] [372400] global_step=372400, grad_norm=3.1567907333374023, loss=1.110558032989502
I0310 07:07:05.704972 139708407461632 logging_writer.py:48] [372500] global_step=372500, grad_norm=3.161867141723633, loss=1.8347160816192627
I0310 07:07:51.004476 139708415854336 logging_writer.py:48] [372600] global_step=372600, grad_norm=2.9597220420837402, loss=2.4961187839508057
I0310 07:08:36.040487 139708407461632 logging_writer.py:48] [372700] global_step=372700, grad_norm=3.5407793521881104, loss=1.1476383209228516
I0310 07:09:21.202692 139708415854336 logging_writer.py:48] [372800] global_step=372800, grad_norm=3.984179973602295, loss=3.254793167114258
I0310 07:09:51.067661 139902746892096 spec.py:321] Evaluating on the training split.
I0310 07:10:02.487922 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 07:10:23.534916 139902746892096 spec.py:349] Evaluating on the test split.
I0310 07:10:25.211773 139902746892096 submission_runner.py:411] Time since start: 180548.24s, 	Step: 372867, 	{'train/accuracy': 0.8907421827316284, 'train/loss': 0.4095943868160248, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 166828.33721232414, 'total_duration': 180548.2405500412, 'accumulated_submission_time': 166828.33721232414, 'accumulated_eval_time': 13676.183526039124, 'accumulated_logging_time': 24.177289485931396}
I0310 07:10:25.293586 139708407461632 logging_writer.py:48] [372867] accumulated_eval_time=13676.183526, accumulated_logging_time=24.177289, accumulated_submission_time=166828.337212, global_step=372867, preemption_count=0, score=166828.337212, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=180548.240550, train/accuracy=0.890742, train/loss=0.409594, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 07:10:38.689720 139708415854336 logging_writer.py:48] [372900] global_step=372900, grad_norm=3.1027917861938477, loss=1.1650323867797852
I0310 07:11:20.677717 139708407461632 logging_writer.py:48] [373000] global_step=373000, grad_norm=3.4890778064727783, loss=2.9659829139709473
I0310 07:12:05.881216 139708415854336 logging_writer.py:48] [373100] global_step=373100, grad_norm=4.989220142364502, loss=3.2967681884765625
I0310 07:12:51.171291 139708407461632 logging_writer.py:48] [373200] global_step=373200, grad_norm=3.0141053199768066, loss=2.1001625061035156
I0310 07:13:36.277577 139708415854336 logging_writer.py:48] [373300] global_step=373300, grad_norm=3.084724187850952, loss=2.496145725250244
I0310 07:14:21.503314 139708407461632 logging_writer.py:48] [373400] global_step=373400, grad_norm=3.8401927947998047, loss=3.2806649208068848
I0310 07:15:07.166077 139708415854336 logging_writer.py:48] [373500] global_step=373500, grad_norm=3.0676305294036865, loss=1.4053142070770264
I0310 07:15:52.336798 139708407461632 logging_writer.py:48] [373600] global_step=373600, grad_norm=3.1293439865112305, loss=1.0905818939208984
I0310 07:16:38.596541 139708415854336 logging_writer.py:48] [373700] global_step=373700, grad_norm=3.2142539024353027, loss=1.9098873138427734
I0310 07:17:23.628018 139708407461632 logging_writer.py:48] [373800] global_step=373800, grad_norm=3.078307867050171, loss=1.1618058681488037
I0310 07:17:25.607817 139902746892096 spec.py:321] Evaluating on the training split.
I0310 07:17:37.421556 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 07:17:58.395801 139902746892096 spec.py:349] Evaluating on the test split.
I0310 07:18:00.072841 139902746892096 submission_runner.py:411] Time since start: 181003.10s, 	Step: 373806, 	{'train/accuracy': 0.8892773389816284, 'train/loss': 0.4130665361881256, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 167248.5930762291, 'total_duration': 181003.10163211823, 'accumulated_submission_time': 167248.5930762291, 'accumulated_eval_time': 13710.648540973663, 'accumulated_logging_time': 24.268741846084595}
I0310 07:18:00.143671 139708415854336 logging_writer.py:48] [373806] accumulated_eval_time=13710.648541, accumulated_logging_time=24.268742, accumulated_submission_time=167248.593076, global_step=373806, preemption_count=0, score=167248.593076, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=181003.101632, train/accuracy=0.889277, train/loss=0.413067, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 07:18:37.994668 139708407461632 logging_writer.py:48] [373900] global_step=373900, grad_norm=3.0310850143432617, loss=1.759913682937622
I0310 07:19:23.098684 139708415854336 logging_writer.py:48] [374000] global_step=374000, grad_norm=3.879600763320923, loss=3.2606310844421387
I0310 07:20:08.687509 139708407461632 logging_writer.py:48] [374100] global_step=374100, grad_norm=3.712893486022949, loss=3.2396240234375
I0310 07:20:54.396681 139708415854336 logging_writer.py:48] [374200] global_step=374200, grad_norm=3.0255556106567383, loss=1.0474278926849365
I0310 07:21:39.680864 139708407461632 logging_writer.py:48] [374300] global_step=374300, grad_norm=2.899505853652954, loss=1.468148112297058
I0310 07:22:25.062037 139708415854336 logging_writer.py:48] [374400] global_step=374400, grad_norm=3.238717794418335, loss=2.7754831314086914
I0310 07:23:10.384988 139708407461632 logging_writer.py:48] [374500] global_step=374500, grad_norm=2.980767011642456, loss=1.1262240409851074
I0310 07:23:55.695933 139708415854336 logging_writer.py:48] [374600] global_step=374600, grad_norm=3.377490282058716, loss=2.187957286834717
I0310 07:24:40.894680 139708407461632 logging_writer.py:48] [374700] global_step=374700, grad_norm=3.0126960277557373, loss=1.0410401821136475
I0310 07:25:00.430824 139902746892096 spec.py:321] Evaluating on the training split.
I0310 07:25:12.445978 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 07:25:33.886223 139902746892096 spec.py:349] Evaluating on the test split.
I0310 07:25:35.551248 139902746892096 submission_runner.py:411] Time since start: 181458.58s, 	Step: 374745, 	{'train/accuracy': 0.8873242139816284, 'train/loss': 0.4112350344657898, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 167668.823564291, 'total_duration': 181458.58004879951, 'accumulated_submission_time': 167668.823564291, 'accumulated_eval_time': 13745.768962621689, 'accumulated_logging_time': 24.34765338897705}
I0310 07:25:35.621594 139708415854336 logging_writer.py:48] [374745] accumulated_eval_time=13745.768963, accumulated_logging_time=24.347653, accumulated_submission_time=167668.823564, global_step=374745, preemption_count=0, score=167668.823564, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=181458.580049, train/accuracy=0.887324, train/loss=0.411235, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 07:25:57.659754 139708407461632 logging_writer.py:48] [374800] global_step=374800, grad_norm=3.213029623031616, loss=2.566624879837036
I0310 07:26:40.447218 139708415854336 logging_writer.py:48] [374900] global_step=374900, grad_norm=3.0192196369171143, loss=1.074939489364624
I0310 07:27:25.623839 139708407461632 logging_writer.py:48] [375000] global_step=375000, grad_norm=2.8654448986053467, loss=1.0739893913269043
I0310 07:28:11.242907 139708415854336 logging_writer.py:48] [375100] global_step=375100, grad_norm=3.2790589332580566, loss=1.3338987827301025
I0310 07:28:56.653713 139708407461632 logging_writer.py:48] [375200] global_step=375200, grad_norm=3.2275028228759766, loss=2.191646099090576
I0310 07:29:42.015037 139708415854336 logging_writer.py:48] [375300] global_step=375300, grad_norm=4.02137565612793, loss=3.217926025390625
I0310 07:30:27.401381 139708407461632 logging_writer.py:48] [375400] global_step=375400, grad_norm=3.1027183532714844, loss=1.0806479454040527
I0310 07:31:12.545115 139708415854336 logging_writer.py:48] [375500] global_step=375500, grad_norm=3.1013927459716797, loss=1.1735963821411133
I0310 07:31:57.752235 139708407461632 logging_writer.py:48] [375600] global_step=375600, grad_norm=3.5179014205932617, loss=2.1849465370178223
I0310 07:32:35.662261 139902746892096 spec.py:321] Evaluating on the training split.
I0310 07:32:47.096239 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 07:33:08.759112 139902746892096 spec.py:349] Evaluating on the test split.
I0310 07:33:10.436238 139902746892096 submission_runner.py:411] Time since start: 181913.47s, 	Step: 375685, 	{'train/accuracy': 0.8878905773162842, 'train/loss': 0.4148047864437103, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 168088.80501580238, 'total_duration': 181913.4650197029, 'accumulated_submission_time': 168088.80501580238, 'accumulated_eval_time': 13780.542924642563, 'accumulated_logging_time': 24.428040742874146}
I0310 07:33:10.515611 139708415854336 logging_writer.py:48] [375685] accumulated_eval_time=13780.542925, accumulated_logging_time=24.428041, accumulated_submission_time=168088.805016, global_step=375685, preemption_count=0, score=168088.805016, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=181913.465020, train/accuracy=0.887891, train/loss=0.414805, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 07:33:16.819304 139708407461632 logging_writer.py:48] [375700] global_step=375700, grad_norm=3.3802103996276855, loss=2.8833017349243164
I0310 07:33:57.889311 139708415854336 logging_writer.py:48] [375800] global_step=375800, grad_norm=3.3133535385131836, loss=1.2655342817306519
I0310 07:34:42.857488 139708407461632 logging_writer.py:48] [375900] global_step=375900, grad_norm=3.0480127334594727, loss=1.057127833366394
I0310 07:35:28.323839 139708415854336 logging_writer.py:48] [376000] global_step=376000, grad_norm=3.1838743686676025, loss=1.0513086318969727
I0310 07:36:13.549402 139708407461632 logging_writer.py:48] [376100] global_step=376100, grad_norm=3.1979663372039795, loss=2.3466084003448486
I0310 07:36:58.834079 139708415854336 logging_writer.py:48] [376200] global_step=376200, grad_norm=3.3750252723693848, loss=1.246689796447754
I0310 07:37:44.278310 139708407461632 logging_writer.py:48] [376300] global_step=376300, grad_norm=3.139671564102173, loss=1.2731997966766357
I0310 07:38:29.419001 139708415854336 logging_writer.py:48] [376400] global_step=376400, grad_norm=3.130075693130493, loss=1.0757102966308594
I0310 07:39:15.062896 139708407461632 logging_writer.py:48] [376500] global_step=376500, grad_norm=3.067950963973999, loss=1.520457148551941
I0310 07:40:00.285949 139708415854336 logging_writer.py:48] [376600] global_step=376600, grad_norm=3.3248865604400635, loss=1.1264537572860718
I0310 07:40:10.788870 139902746892096 spec.py:321] Evaluating on the training split.
I0310 07:40:22.944408 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 07:40:45.015013 139902746892096 spec.py:349] Evaluating on the test split.
I0310 07:40:46.677357 139902746892096 submission_runner.py:411] Time since start: 182369.71s, 	Step: 376625, 	{'train/accuracy': 0.8874218463897705, 'train/loss': 0.41583871841430664, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 168509.0180311203, 'total_duration': 182369.70614933968, 'accumulated_submission_time': 168509.0180311203, 'accumulated_eval_time': 13816.431394577026, 'accumulated_logging_time': 24.518927574157715}
I0310 07:40:46.756955 139708407461632 logging_writer.py:48] [376625] accumulated_eval_time=13816.431395, accumulated_logging_time=24.518928, accumulated_submission_time=168509.018031, global_step=376625, preemption_count=0, score=168509.018031, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=182369.706149, train/accuracy=0.887422, train/loss=0.415839, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 07:41:16.670144 139708415854336 logging_writer.py:48] [376700] global_step=376700, grad_norm=3.620035409927368, loss=2.8598644733428955
I0310 07:42:00.835457 139708407461632 logging_writer.py:48] [376800] global_step=376800, grad_norm=3.249785900115967, loss=1.222162127494812
I0310 07:42:46.369731 139708415854336 logging_writer.py:48] [376900] global_step=376900, grad_norm=3.755046844482422, loss=1.194535493850708
I0310 07:43:31.672598 139708407461632 logging_writer.py:48] [377000] global_step=377000, grad_norm=3.5620832443237305, loss=2.3560097217559814
I0310 07:44:17.091986 139708415854336 logging_writer.py:48] [377100] global_step=377100, grad_norm=3.0625975131988525, loss=1.1288100481033325
I0310 07:45:02.375748 139708407461632 logging_writer.py:48] [377200] global_step=377200, grad_norm=3.224614143371582, loss=2.5851073265075684
I0310 07:45:47.426010 139708415854336 logging_writer.py:48] [377300] global_step=377300, grad_norm=2.9534714221954346, loss=1.0254467725753784
I0310 07:46:32.947955 139708407461632 logging_writer.py:48] [377400] global_step=377400, grad_norm=2.92677903175354, loss=1.1877784729003906
I0310 07:47:18.332237 139708415854336 logging_writer.py:48] [377500] global_step=377500, grad_norm=3.223407745361328, loss=1.993593692779541
I0310 07:47:47.011611 139902746892096 spec.py:321] Evaluating on the training split.
I0310 07:47:58.434822 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 07:48:19.386671 139902746892096 spec.py:349] Evaluating on the test split.
I0310 07:48:21.081741 139902746892096 submission_runner.py:411] Time since start: 182824.11s, 	Step: 377565, 	{'train/accuracy': 0.8898242115974426, 'train/loss': 0.407855361700058, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 168929.21570897102, 'total_duration': 182824.11052680016, 'accumulated_submission_time': 168929.21570897102, 'accumulated_eval_time': 13850.501540184021, 'accumulated_logging_time': 24.606639623641968}
I0310 07:48:21.160245 139708407461632 logging_writer.py:48] [377565] accumulated_eval_time=13850.501540, accumulated_logging_time=24.606640, accumulated_submission_time=168929.215709, global_step=377565, preemption_count=0, score=168929.215709, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=182824.110527, train/accuracy=0.889824, train/loss=0.407855, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 07:48:35.319571 139708415854336 logging_writer.py:48] [377600] global_step=377600, grad_norm=3.860788583755493, loss=3.1441752910614014
I0310 07:49:17.535217 139708407461632 logging_writer.py:48] [377700] global_step=377700, grad_norm=2.802020788192749, loss=1.165140151977539
I0310 07:50:02.909703 139708415854336 logging_writer.py:48] [377800] global_step=377800, grad_norm=2.891242742538452, loss=1.6731221675872803
I0310 07:50:48.633785 139708407461632 logging_writer.py:48] [377900] global_step=377900, grad_norm=2.8139662742614746, loss=1.0368624925613403
I0310 07:51:33.684593 139708415854336 logging_writer.py:48] [378000] global_step=378000, grad_norm=3.106646776199341, loss=2.6745285987854004
I0310 07:52:18.932368 139708407461632 logging_writer.py:48] [378100] global_step=378100, grad_norm=3.2995152473449707, loss=1.1233599185943604
I0310 07:53:04.429470 139708415854336 logging_writer.py:48] [378200] global_step=378200, grad_norm=3.2360169887542725, loss=2.4039361476898193
I0310 07:53:49.419515 139708407461632 logging_writer.py:48] [378300] global_step=378300, grad_norm=3.080754041671753, loss=1.5223814249038696
I0310 07:54:34.776188 139708415854336 logging_writer.py:48] [378400] global_step=378400, grad_norm=3.1995747089385986, loss=1.6100777387619019
I0310 07:55:20.050144 139708407461632 logging_writer.py:48] [378500] global_step=378500, grad_norm=3.038071632385254, loss=1.3892532587051392
I0310 07:55:21.100319 139902746892096 spec.py:321] Evaluating on the training split.
I0310 07:55:32.661435 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 07:55:52.676294 139902746892096 spec.py:349] Evaluating on the test split.
I0310 07:55:54.361093 139902746892096 submission_runner.py:411] Time since start: 183277.39s, 	Step: 378504, 	{'train/accuracy': 0.8873632550239563, 'train/loss': 0.41827499866485596, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 169349.09867095947, 'total_duration': 183277.38987207413, 'accumulated_submission_time': 169349.09867095947, 'accumulated_eval_time': 13883.762291669846, 'accumulated_logging_time': 24.693422079086304}
I0310 07:55:54.445642 139708415854336 logging_writer.py:48] [378504] accumulated_eval_time=13883.762292, accumulated_logging_time=24.693422, accumulated_submission_time=169349.098671, global_step=378504, preemption_count=0, score=169349.098671, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=183277.389872, train/accuracy=0.887363, train/loss=0.418275, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 07:56:33.284415 139708407461632 logging_writer.py:48] [378600] global_step=378600, grad_norm=2.980600118637085, loss=1.3697764873504639
I0310 07:57:18.459033 139708415854336 logging_writer.py:48] [378700] global_step=378700, grad_norm=3.2189323902130127, loss=1.1313872337341309
I0310 07:58:03.789616 139708407461632 logging_writer.py:48] [378800] global_step=378800, grad_norm=2.954066038131714, loss=1.8364348411560059
I0310 07:58:49.277882 139708415854336 logging_writer.py:48] [378900] global_step=378900, grad_norm=3.0777626037597656, loss=1.1715755462646484
I0310 07:59:34.590566 139708407461632 logging_writer.py:48] [379000] global_step=379000, grad_norm=3.1476972103118896, loss=1.081308364868164
I0310 08:00:20.124228 139708415854336 logging_writer.py:48] [379100] global_step=379100, grad_norm=3.2543249130249023, loss=1.1406209468841553
I0310 08:01:05.402364 139708407461632 logging_writer.py:48] [379200] global_step=379200, grad_norm=3.0695571899414062, loss=1.0856831073760986
I0310 08:01:50.704255 139708415854336 logging_writer.py:48] [379300] global_step=379300, grad_norm=3.454335927963257, loss=2.918037176132202
I0310 08:02:36.101114 139708407461632 logging_writer.py:48] [379400] global_step=379400, grad_norm=3.235586643218994, loss=1.1809661388397217
I0310 08:02:54.793227 139902746892096 spec.py:321] Evaluating on the training split.
I0310 08:03:06.099455 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 08:03:28.660418 139902746892096 spec.py:349] Evaluating on the test split.
I0310 08:03:30.325793 139902746892096 submission_runner.py:411] Time since start: 183733.35s, 	Step: 379443, 	{'train/accuracy': 0.88880854845047, 'train/loss': 0.4128335416316986, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 169769.38675570488, 'total_duration': 183733.3545923233, 'accumulated_submission_time': 169769.38675570488, 'accumulated_eval_time': 13919.294860601425, 'accumulated_logging_time': 24.787896394729614}
I0310 08:03:30.402944 139708415854336 logging_writer.py:48] [379443] accumulated_eval_time=13919.294861, accumulated_logging_time=24.787896, accumulated_submission_time=169769.386756, global_step=379443, preemption_count=0, score=169769.386756, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=183733.354592, train/accuracy=0.888809, train/loss=0.412834, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 08:03:53.242541 139708407461632 logging_writer.py:48] [379500] global_step=379500, grad_norm=3.2167341709136963, loss=1.1886792182922363
I0310 08:04:36.168615 139708415854336 logging_writer.py:48] [379600] global_step=379600, grad_norm=3.2992005348205566, loss=1.1403274536132812
I0310 08:05:21.580982 139708407461632 logging_writer.py:48] [379700] global_step=379700, grad_norm=2.9117746353149414, loss=1.4442014694213867
I0310 08:06:07.141992 139708415854336 logging_writer.py:48] [379800] global_step=379800, grad_norm=2.9112613201141357, loss=1.1339170932769775
I0310 08:06:52.202205 139708407461632 logging_writer.py:48] [379900] global_step=379900, grad_norm=3.2574431896209717, loss=1.167916178703308
I0310 08:07:37.599377 139708415854336 logging_writer.py:48] [380000] global_step=380000, grad_norm=3.2679786682128906, loss=1.2013041973114014
I0310 08:08:22.915060 139708407461632 logging_writer.py:48] [380100] global_step=380100, grad_norm=3.0687272548675537, loss=1.142269253730774
I0310 08:09:08.254681 139708415854336 logging_writer.py:48] [380200] global_step=380200, grad_norm=3.4725394248962402, loss=2.9404053688049316
I0310 08:09:53.507493 139708407461632 logging_writer.py:48] [380300] global_step=380300, grad_norm=3.0782923698425293, loss=1.5952404737472534
I0310 08:10:30.594431 139902746892096 spec.py:321] Evaluating on the training split.
I0310 08:10:41.829406 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 08:11:03.202608 139902746892096 spec.py:349] Evaluating on the test split.
I0310 08:11:04.881315 139902746892096 submission_runner.py:411] Time since start: 184187.91s, 	Step: 380383, 	{'train/accuracy': 0.8867577910423279, 'train/loss': 0.4163850247859955, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 170189.518556118, 'total_duration': 184187.91010832787, 'accumulated_submission_time': 170189.518556118, 'accumulated_eval_time': 13953.581751823425, 'accumulated_logging_time': 24.875013828277588}
I0310 08:11:04.953655 139708415854336 logging_writer.py:48] [380383] accumulated_eval_time=13953.581752, accumulated_logging_time=24.875014, accumulated_submission_time=170189.518556, global_step=380383, preemption_count=0, score=170189.518556, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=184187.910108, train/accuracy=0.886758, train/loss=0.416385, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 08:11:12.030531 139708407461632 logging_writer.py:48] [380400] global_step=380400, grad_norm=3.206167459487915, loss=1.3236373662948608
I0310 08:11:53.208404 139708415854336 logging_writer.py:48] [380500] global_step=380500, grad_norm=3.1502814292907715, loss=1.1877164840698242
I0310 08:12:38.376137 139708407461632 logging_writer.py:48] [380600] global_step=380600, grad_norm=3.10662579536438, loss=1.4447354078292847
I0310 08:13:23.763626 139708415854336 logging_writer.py:48] [380700] global_step=380700, grad_norm=3.5587146282196045, loss=2.7188847064971924
I0310 08:14:08.981590 139708407461632 logging_writer.py:48] [380800] global_step=380800, grad_norm=3.2191367149353027, loss=1.1120861768722534
I0310 08:14:53.976144 139708415854336 logging_writer.py:48] [380900] global_step=380900, grad_norm=3.038147211074829, loss=1.1512515544891357
I0310 08:15:39.311316 139708407461632 logging_writer.py:48] [381000] global_step=381000, grad_norm=3.250403881072998, loss=1.1033735275268555
I0310 08:16:24.458607 139708415854336 logging_writer.py:48] [381100] global_step=381100, grad_norm=3.6871566772460938, loss=3.0904452800750732
I0310 08:17:09.614853 139708407461632 logging_writer.py:48] [381200] global_step=381200, grad_norm=3.775951862335205, loss=3.0372226238250732
I0310 08:17:54.866990 139708415854336 logging_writer.py:48] [381300] global_step=381300, grad_norm=3.3511743545532227, loss=1.192883849143982
I0310 08:18:04.974221 139902746892096 spec.py:321] Evaluating on the training split.
I0310 08:18:16.630058 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 08:18:37.214278 139902746892096 spec.py:349] Evaluating on the test split.
I0310 08:18:38.901356 139902746892096 submission_runner.py:411] Time since start: 184641.93s, 	Step: 381324, 	{'train/accuracy': 0.8875781297683716, 'train/loss': 0.41504546999931335, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 170609.48133444786, 'total_duration': 184641.93013739586, 'accumulated_submission_time': 170609.48133444786, 'accumulated_eval_time': 13987.508892059326, 'accumulated_logging_time': 24.955965995788574}
I0310 08:18:38.984797 139708407461632 logging_writer.py:48] [381324] accumulated_eval_time=13987.508892, accumulated_logging_time=24.955966, accumulated_submission_time=170609.481334, global_step=381324, preemption_count=0, score=170609.481334, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=184641.930137, train/accuracy=0.887578, train/loss=0.415045, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 08:19:09.269004 139708415854336 logging_writer.py:48] [381400] global_step=381400, grad_norm=3.065847635269165, loss=1.1896698474884033
I0310 08:19:53.952673 139708407461632 logging_writer.py:48] [381500] global_step=381500, grad_norm=3.159421682357788, loss=1.676334023475647
I0310 08:20:39.397905 139708415854336 logging_writer.py:48] [381600] global_step=381600, grad_norm=3.0957095623016357, loss=2.1442410945892334
I0310 08:21:24.475825 139708407461632 logging_writer.py:48] [381700] global_step=381700, grad_norm=3.019073247909546, loss=1.1471521854400635
I0310 08:22:09.486816 139708415854336 logging_writer.py:48] [381800] global_step=381800, grad_norm=3.1457462310791016, loss=1.3701252937316895
I0310 08:22:54.879474 139708407461632 logging_writer.py:48] [381900] global_step=381900, grad_norm=3.5082409381866455, loss=1.9051902294158936
I0310 08:23:39.959378 139708415854336 logging_writer.py:48] [382000] global_step=382000, grad_norm=3.317254066467285, loss=1.1943899393081665
I0310 08:24:25.184077 139708407461632 logging_writer.py:48] [382100] global_step=382100, grad_norm=3.5409207344055176, loss=1.1048412322998047
I0310 08:25:10.428116 139708415854336 logging_writer.py:48] [382200] global_step=382200, grad_norm=3.0905580520629883, loss=1.2270704507827759
I0310 08:25:38.954990 139902746892096 spec.py:321] Evaluating on the training split.
I0310 08:25:50.426018 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 08:26:11.102372 139902746892096 spec.py:349] Evaluating on the test split.
I0310 08:26:12.787438 139902746892096 submission_runner.py:411] Time since start: 185095.82s, 	Step: 382265, 	{'train/accuracy': 0.8875585794448853, 'train/loss': 0.41507232189178467, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 171029.39256739616, 'total_duration': 185095.81622838974, 'accumulated_submission_time': 171029.39256739616, 'accumulated_eval_time': 14021.341363430023, 'accumulated_logging_time': 25.048585176467896}
I0310 08:26:12.864026 139708407461632 logging_writer.py:48] [382265] accumulated_eval_time=14021.341363, accumulated_logging_time=25.048585, accumulated_submission_time=171029.392567, global_step=382265, preemption_count=0, score=171029.392567, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=185095.816228, train/accuracy=0.887559, train/loss=0.415072, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 08:26:27.038207 139708415854336 logging_writer.py:48] [382300] global_step=382300, grad_norm=4.038887977600098, loss=3.2142860889434814
I0310 08:27:09.254454 139708407461632 logging_writer.py:48] [382400] global_step=382400, grad_norm=3.8341662883758545, loss=3.0877838134765625
I0310 08:27:54.707466 139708415854336 logging_writer.py:48] [382500] global_step=382500, grad_norm=3.2635350227355957, loss=2.191725492477417
I0310 08:28:40.087999 139708407461632 logging_writer.py:48] [382600] global_step=382600, grad_norm=3.0079734325408936, loss=1.2413707971572876
I0310 08:29:25.439733 139708415854336 logging_writer.py:48] [382700] global_step=382700, grad_norm=3.2692761421203613, loss=1.8469115495681763
I0310 08:30:10.356070 139708407461632 logging_writer.py:48] [382800] global_step=382800, grad_norm=4.451322078704834, loss=1.591339349746704
I0310 08:30:55.909333 139708415854336 logging_writer.py:48] [382900] global_step=382900, grad_norm=2.7214081287384033, loss=1.056334376335144
I0310 08:31:41.115102 139708407461632 logging_writer.py:48] [383000] global_step=383000, grad_norm=3.0150182247161865, loss=1.2533633708953857
I0310 08:32:26.413015 139708415854336 logging_writer.py:48] [383100] global_step=383100, grad_norm=3.052696466445923, loss=1.070275068283081
I0310 08:33:11.625554 139708407461632 logging_writer.py:48] [383200] global_step=383200, grad_norm=3.271338701248169, loss=1.0551282167434692
I0310 08:33:13.119113 139902746892096 spec.py:321] Evaluating on the training split.
I0310 08:33:24.506165 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 08:33:46.357774 139902746892096 spec.py:349] Evaluating on the test split.
I0310 08:33:48.030300 139902746892096 submission_runner.py:411] Time since start: 185551.06s, 	Step: 383205, 	{'train/accuracy': 0.8885546922683716, 'train/loss': 0.4139377176761627, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 171449.58943510056, 'total_duration': 185551.0590927601, 'accumulated_submission_time': 171449.58943510056, 'accumulated_eval_time': 14056.252525568008, 'accumulated_logging_time': 25.13417649269104}
I0310 08:33:48.102920 139708415854336 logging_writer.py:48] [383205] accumulated_eval_time=14056.252526, accumulated_logging_time=25.134176, accumulated_submission_time=171449.589435, global_step=383205, preemption_count=0, score=171449.589435, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=185551.059093, train/accuracy=0.888555, train/loss=0.413938, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 08:34:26.318339 139708407461632 logging_writer.py:48] [383300] global_step=383300, grad_norm=2.9095897674560547, loss=1.0213886499404907
I0310 08:35:11.507550 139708415854336 logging_writer.py:48] [383400] global_step=383400, grad_norm=3.859055280685425, loss=3.1929750442504883
I0310 08:35:56.873658 139708407461632 logging_writer.py:48] [383500] global_step=383500, grad_norm=4.052315711975098, loss=3.3740415573120117
I0310 08:36:42.198814 139708415854336 logging_writer.py:48] [383600] global_step=383600, grad_norm=3.0198159217834473, loss=1.5731329917907715
I0310 08:37:27.387900 139708407461632 logging_writer.py:48] [383700] global_step=383700, grad_norm=3.0654330253601074, loss=1.1966750621795654
I0310 08:38:13.094952 139708415854336 logging_writer.py:48] [383800] global_step=383800, grad_norm=3.2788102626800537, loss=1.0829377174377441
I0310 08:38:58.322412 139708407461632 logging_writer.py:48] [383900] global_step=383900, grad_norm=3.196446418762207, loss=1.2328391075134277
I0310 08:39:43.390008 139708415854336 logging_writer.py:48] [384000] global_step=384000, grad_norm=3.1217894554138184, loss=1.1011234521865845
I0310 08:40:28.953877 139708407461632 logging_writer.py:48] [384100] global_step=384100, grad_norm=3.145768404006958, loss=1.171036720275879
I0310 08:40:48.119842 139902746892096 spec.py:321] Evaluating on the training split.
I0310 08:40:59.476545 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 08:41:20.396511 139902746892096 spec.py:349] Evaluating on the test split.
I0310 08:41:22.077388 139902746892096 submission_runner.py:411] Time since start: 186005.11s, 	Step: 384144, 	{'train/accuracy': 0.8873242139816284, 'train/loss': 0.4182506799697876, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 171869.548060894, 'total_duration': 186005.1061720848, 'accumulated_submission_time': 171869.548060894, 'accumulated_eval_time': 14090.210060834885, 'accumulated_logging_time': 25.21590518951416}
I0310 08:41:22.159224 139708415854336 logging_writer.py:48] [384144] accumulated_eval_time=14090.210061, accumulated_logging_time=25.215905, accumulated_submission_time=171869.548061, global_step=384144, preemption_count=0, score=171869.548061, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=186005.106172, train/accuracy=0.887324, train/loss=0.418251, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 08:41:44.597582 139708407461632 logging_writer.py:48] [384200] global_step=384200, grad_norm=2.9208602905273438, loss=1.0649970769882202
I0310 08:42:27.756271 139708415854336 logging_writer.py:48] [384300] global_step=384300, grad_norm=3.042682409286499, loss=1.1113919019699097
I0310 08:43:12.919507 139708407461632 logging_writer.py:48] [384400] global_step=384400, grad_norm=3.211560010910034, loss=2.531731367111206
I0310 08:43:58.461767 139708415854336 logging_writer.py:48] [384500] global_step=384500, grad_norm=3.0554659366607666, loss=1.1201382875442505
I0310 08:44:43.867638 139708407461632 logging_writer.py:48] [384600] global_step=384600, grad_norm=3.46830677986145, loss=1.0651121139526367
I0310 08:45:29.303738 139708415854336 logging_writer.py:48] [384700] global_step=384700, grad_norm=3.130049228668213, loss=2.3994219303131104
I0310 08:46:14.728529 139708407461632 logging_writer.py:48] [384800] global_step=384800, grad_norm=3.340090751647949, loss=1.0768109560012817
I0310 08:46:59.865856 139708415854336 logging_writer.py:48] [384900] global_step=384900, grad_norm=3.7286341190338135, loss=1.6112467050552368
I0310 08:47:45.327930 139708407461632 logging_writer.py:48] [385000] global_step=385000, grad_norm=2.868243932723999, loss=1.244990348815918
I0310 08:48:22.137157 139902746892096 spec.py:321] Evaluating on the training split.
I0310 08:48:33.478241 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 08:48:55.512359 139902746892096 spec.py:349] Evaluating on the test split.
I0310 08:48:57.182111 139902746892096 submission_runner.py:411] Time since start: 186460.21s, 	Step: 385083, 	{'train/accuracy': 0.8873828053474426, 'train/loss': 0.4178923964500427, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 172289.46645498276, 'total_duration': 186460.21090960503, 'accumulated_submission_time': 172289.46645498276, 'accumulated_eval_time': 14125.25502538681, 'accumulated_logging_time': 25.30760169029236}
I0310 08:48:57.254990 139708415854336 logging_writer.py:48] [385083] accumulated_eval_time=14125.255025, accumulated_logging_time=25.307602, accumulated_submission_time=172289.466455, global_step=385083, preemption_count=0, score=172289.466455, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=186460.210910, train/accuracy=0.887383, train/loss=0.417892, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 08:49:04.344515 139708407461632 logging_writer.py:48] [385100] global_step=385100, grad_norm=3.0505011081695557, loss=1.5298124551773071
I0310 08:49:44.825479 139708415854336 logging_writer.py:48] [385200] global_step=385200, grad_norm=3.491973400115967, loss=1.1779615879058838
I0310 08:50:29.905956 139708407461632 logging_writer.py:48] [385300] global_step=385300, grad_norm=3.224233865737915, loss=2.22477650642395
I0310 08:51:15.731135 139708415854336 logging_writer.py:48] [385400] global_step=385400, grad_norm=3.748196840286255, loss=2.810770034790039
I0310 08:52:01.016476 139708407461632 logging_writer.py:48] [385500] global_step=385500, grad_norm=3.118365526199341, loss=1.1914104223251343
I0310 08:52:46.179340 139708415854336 logging_writer.py:48] [385600] global_step=385600, grad_norm=2.97554874420166, loss=1.0822871923446655
I0310 08:53:31.500539 139708407461632 logging_writer.py:48] [385700] global_step=385700, grad_norm=3.125800609588623, loss=1.0844368934631348
I0310 08:54:16.710021 139708415854336 logging_writer.py:48] [385800] global_step=385800, grad_norm=3.090693712234497, loss=1.064462661743164
I0310 08:55:01.840565 139708407461632 logging_writer.py:48] [385900] global_step=385900, grad_norm=3.3253750801086426, loss=1.3210813999176025
I0310 08:55:46.912110 139708415854336 logging_writer.py:48] [386000] global_step=386000, grad_norm=2.9531288146972656, loss=1.0474215745925903
I0310 08:55:57.477709 139902746892096 spec.py:321] Evaluating on the training split.
I0310 08:56:08.827047 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 08:56:29.054756 139902746892096 spec.py:349] Evaluating on the test split.
I0310 08:56:30.744123 139902746892096 submission_runner.py:411] Time since start: 186913.77s, 	Step: 386025, 	{'train/accuracy': 0.8904687166213989, 'train/loss': 0.41166284680366516, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 172709.62967181206, 'total_duration': 186913.77291107178, 'accumulated_submission_time': 172709.62967181206, 'accumulated_eval_time': 14158.521426916122, 'accumulated_logging_time': 25.390140295028687}
I0310 08:56:30.834351 139708407461632 logging_writer.py:48] [386025] accumulated_eval_time=14158.521427, accumulated_logging_time=25.390140, accumulated_submission_time=172709.629672, global_step=386025, preemption_count=0, score=172709.629672, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=186913.772911, train/accuracy=0.890469, train/loss=0.411663, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 08:57:00.767417 139708415854336 logging_writer.py:48] [386100] global_step=386100, grad_norm=3.0616207122802734, loss=1.5266417264938354
I0310 08:57:45.364714 139708407461632 logging_writer.py:48] [386200] global_step=386200, grad_norm=3.1686222553253174, loss=1.1076265573501587
I0310 08:58:30.429165 139708415854336 logging_writer.py:48] [386300] global_step=386300, grad_norm=3.1096575260162354, loss=1.8622740507125854
I0310 08:59:15.960383 139708407461632 logging_writer.py:48] [386400] global_step=386400, grad_norm=3.1465697288513184, loss=1.100003957748413
I0310 09:00:01.105465 139708415854336 logging_writer.py:48] [386500] global_step=386500, grad_norm=3.468151569366455, loss=1.156671166419983
I0310 09:00:46.524505 139708407461632 logging_writer.py:48] [386600] global_step=386600, grad_norm=3.2322676181793213, loss=1.6849297285079956
I0310 09:01:32.234609 139708415854336 logging_writer.py:48] [386700] global_step=386700, grad_norm=4.012192249298096, loss=1.1066747903823853
I0310 09:02:17.458528 139708407461632 logging_writer.py:48] [386800] global_step=386800, grad_norm=3.278810501098633, loss=1.0407357215881348
I0310 09:03:02.896418 139708415854336 logging_writer.py:48] [386900] global_step=386900, grad_norm=3.094054937362671, loss=2.3607964515686035
I0310 09:03:30.976219 139902746892096 spec.py:321] Evaluating on the training split.
I0310 09:03:42.478771 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 09:04:04.211247 139902746892096 spec.py:349] Evaluating on the test split.
I0310 09:04:05.882837 139902746892096 submission_runner.py:411] Time since start: 187368.91s, 	Step: 386964, 	{'train/accuracy': 0.8865038752555847, 'train/loss': 0.42187249660491943, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 173129.71286416054, 'total_duration': 187368.9116373062, 'accumulated_submission_time': 173129.71286416054, 'accumulated_eval_time': 14193.428065538406, 'accumulated_logging_time': 25.49014639854431}
I0310 09:04:05.957927 139708407461632 logging_writer.py:48] [386964] accumulated_eval_time=14193.428066, accumulated_logging_time=25.490146, accumulated_submission_time=173129.712864, global_step=386964, preemption_count=0, score=173129.712864, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=187368.911637, train/accuracy=0.886504, train/loss=0.421872, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 09:04:20.518275 139708415854336 logging_writer.py:48] [387000] global_step=387000, grad_norm=3.106348991394043, loss=1.1472493410110474
I0310 09:05:02.638635 139708407461632 logging_writer.py:48] [387100] global_step=387100, grad_norm=2.9805071353912354, loss=1.1908915042877197
I0310 09:05:48.133248 139708415854336 logging_writer.py:48] [387200] global_step=387200, grad_norm=2.8136868476867676, loss=1.2832468748092651
I0310 09:06:33.790507 139708407461632 logging_writer.py:48] [387300] global_step=387300, grad_norm=3.2353029251098633, loss=1.6091721057891846
I0310 09:07:19.233575 139708415854336 logging_writer.py:48] [387400] global_step=387400, grad_norm=3.45674467086792, loss=3.0127313137054443
I0310 09:08:05.018531 139708407461632 logging_writer.py:48] [387500] global_step=387500, grad_norm=3.2297279834747314, loss=2.3169264793395996
I0310 09:08:50.398028 139708415854336 logging_writer.py:48] [387600] global_step=387600, grad_norm=3.120818614959717, loss=1.0391021966934204
I0310 09:09:36.089356 139708407461632 logging_writer.py:48] [387700] global_step=387700, grad_norm=2.8821470737457275, loss=2.2669320106506348
I0310 09:10:21.186017 139708415854336 logging_writer.py:48] [387800] global_step=387800, grad_norm=3.1393637657165527, loss=1.504626989364624
I0310 09:11:06.118408 139902746892096 spec.py:321] Evaluating on the training split.
I0310 09:11:17.472525 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 09:11:38.605501 139902746892096 spec.py:349] Evaluating on the test split.
I0310 09:11:40.289965 139902746892096 submission_runner.py:411] Time since start: 187823.32s, 	Step: 387900, 	{'train/accuracy': 0.888476550579071, 'train/loss': 0.4137118458747864, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 173549.81622862816, 'total_duration': 187823.31874752045, 'accumulated_submission_time': 173549.81622862816, 'accumulated_eval_time': 14227.59961605072, 'accumulated_logging_time': 25.574037551879883}
I0310 09:11:40.380816 139708407461632 logging_writer.py:48] [387900] accumulated_eval_time=14227.599616, accumulated_logging_time=25.574038, accumulated_submission_time=173549.816229, global_step=387900, preemption_count=0, score=173549.816229, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=187823.318748, train/accuracy=0.888477, train/loss=0.413712, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 09:11:40.781227 139708415854336 logging_writer.py:48] [387900] global_step=387900, grad_norm=3.2422361373901367, loss=2.3588337898254395
I0310 09:12:21.126920 139708407461632 logging_writer.py:48] [388000] global_step=388000, grad_norm=3.164773941040039, loss=2.259960174560547
I0310 09:13:06.548172 139708415854336 logging_writer.py:48] [388100] global_step=388100, grad_norm=3.191375970840454, loss=1.5905860662460327
I0310 09:13:52.227748 139708407461632 logging_writer.py:48] [388200] global_step=388200, grad_norm=3.3321406841278076, loss=1.3797353506088257
I0310 09:14:37.825044 139708415854336 logging_writer.py:48] [388300] global_step=388300, grad_norm=3.051866292953491, loss=1.081249475479126
I0310 09:15:23.110603 139708407461632 logging_writer.py:48] [388400] global_step=388400, grad_norm=3.321597099304199, loss=2.7075860500335693
I0310 09:16:08.570981 139708415854336 logging_writer.py:48] [388500] global_step=388500, grad_norm=3.0967159271240234, loss=1.5273395776748657
I0310 09:16:53.815974 139708407461632 logging_writer.py:48] [388600] global_step=388600, grad_norm=3.5486223697662354, loss=1.6125956773757935
I0310 09:17:39.306673 139708415854336 logging_writer.py:48] [388700] global_step=388700, grad_norm=3.0660178661346436, loss=1.1129703521728516
I0310 09:18:24.662977 139708407461632 logging_writer.py:48] [388800] global_step=388800, grad_norm=3.1551549434661865, loss=1.0397117137908936
I0310 09:18:40.635119 139902746892096 spec.py:321] Evaluating on the training split.
I0310 09:18:51.902549 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 09:19:13.154475 139902746892096 spec.py:349] Evaluating on the test split.
I0310 09:19:14.833459 139902746892096 submission_runner.py:411] Time since start: 188277.86s, 	Step: 388837, 	{'train/accuracy': 0.8871679306030273, 'train/loss': 0.4193262457847595, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 173970.01164507866, 'total_duration': 188277.86225795746, 'accumulated_submission_time': 173970.01164507866, 'accumulated_eval_time': 14261.797946691513, 'accumulated_logging_time': 25.674198389053345}
I0310 09:19:14.911279 139708415854336 logging_writer.py:48] [388837] accumulated_eval_time=14261.797947, accumulated_logging_time=25.674198, accumulated_submission_time=173970.011645, global_step=388837, preemption_count=0, score=173970.011645, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=188277.862258, train/accuracy=0.887168, train/loss=0.419326, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 09:19:40.085793 139708407461632 logging_writer.py:48] [388900] global_step=388900, grad_norm=3.974170446395874, loss=3.172229290008545
I0310 09:20:23.546334 139708415854336 logging_writer.py:48] [389000] global_step=389000, grad_norm=3.4170408248901367, loss=1.212841272354126
I0310 09:21:09.507230 139708407461632 logging_writer.py:48] [389100] global_step=389100, grad_norm=4.816650867462158, loss=1.1238834857940674
I0310 09:21:54.726118 139708415854336 logging_writer.py:48] [389200] global_step=389200, grad_norm=3.409351110458374, loss=2.9596967697143555
I0310 09:22:39.930579 139708407461632 logging_writer.py:48] [389300] global_step=389300, grad_norm=3.056473970413208, loss=1.1063722372055054
I0310 09:23:25.167726 139708415854336 logging_writer.py:48] [389400] global_step=389400, grad_norm=3.3898487091064453, loss=1.1534209251403809
I0310 09:24:10.441987 139708407461632 logging_writer.py:48] [389500] global_step=389500, grad_norm=2.7903544902801514, loss=1.0038783550262451
I0310 09:24:55.776515 139708415854336 logging_writer.py:48] [389600] global_step=389600, grad_norm=2.8853487968444824, loss=1.2793418169021606
I0310 09:25:41.143453 139708407461632 logging_writer.py:48] [389700] global_step=389700, grad_norm=2.8928136825561523, loss=1.975641131401062
I0310 09:26:15.054606 139902746892096 spec.py:321] Evaluating on the training split.
I0310 09:26:26.394032 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 09:26:46.820451 139902746892096 spec.py:349] Evaluating on the test split.
I0310 09:26:48.513618 139902746892096 submission_runner.py:411] Time since start: 188731.54s, 	Step: 389777, 	{'train/accuracy': 0.8882616758346558, 'train/loss': 0.41691356897354126, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 174390.0960047245, 'total_duration': 188731.5424156189, 'accumulated_submission_time': 174390.0960047245, 'accumulated_eval_time': 14295.256955862045, 'accumulated_logging_time': 25.760523080825806}
I0310 09:26:48.588198 139708415854336 logging_writer.py:48] [389777] accumulated_eval_time=14295.256956, accumulated_logging_time=25.760523, accumulated_submission_time=174390.096005, global_step=389777, preemption_count=0, score=174390.096005, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=188731.542416, train/accuracy=0.888262, train/loss=0.416914, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 09:26:58.048064 139708407461632 logging_writer.py:48] [389800] global_step=389800, grad_norm=3.108238458633423, loss=2.418750762939453
I0310 09:27:39.307505 139708415854336 logging_writer.py:48] [389900] global_step=389900, grad_norm=3.0955517292022705, loss=1.1279128789901733
I0310 09:28:24.435029 139708407461632 logging_writer.py:48] [390000] global_step=390000, grad_norm=3.116898536682129, loss=1.0974411964416504
I0310 09:29:09.664723 139708415854336 logging_writer.py:48] [390100] global_step=390100, grad_norm=2.9606149196624756, loss=1.3399522304534912
I0310 09:29:55.245102 139708407461632 logging_writer.py:48] [390200] global_step=390200, grad_norm=3.4079689979553223, loss=1.04609215259552
I0310 09:30:40.497756 139708415854336 logging_writer.py:48] [390300] global_step=390300, grad_norm=3.376837730407715, loss=1.0689754486083984
I0310 09:31:25.923589 139708407461632 logging_writer.py:48] [390400] global_step=390400, grad_norm=3.078362226486206, loss=1.1249715089797974
I0310 09:32:11.220458 139708415854336 logging_writer.py:48] [390500] global_step=390500, grad_norm=3.81058406829834, loss=3.3042314052581787
I0310 09:32:56.429441 139708407461632 logging_writer.py:48] [390600] global_step=390600, grad_norm=3.0005781650543213, loss=1.2993268966674805
I0310 09:33:41.611943 139708415854336 logging_writer.py:48] [390700] global_step=390700, grad_norm=3.137113332748413, loss=1.2295749187469482
I0310 09:33:48.519019 139902746892096 spec.py:321] Evaluating on the training split.
I0310 09:33:59.847728 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 09:34:20.237918 139902746892096 spec.py:349] Evaluating on the test split.
I0310 09:34:21.914306 139902746892096 submission_runner.py:411] Time since start: 189184.94s, 	Step: 390717, 	{'train/accuracy': 0.8859570026397705, 'train/loss': 0.41968971490859985, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 174809.967348814, 'total_duration': 189184.9430897236, 'accumulated_submission_time': 174809.967348814, 'accumulated_eval_time': 14328.652228593826, 'accumulated_logging_time': 25.845441102981567}
I0310 09:34:22.001847 139708407461632 logging_writer.py:48] [390717] accumulated_eval_time=14328.652229, accumulated_logging_time=25.845441, accumulated_submission_time=174809.967349, global_step=390717, preemption_count=0, score=174809.967349, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=189184.943090, train/accuracy=0.885957, train/loss=0.419690, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 09:34:55.131861 139708415854336 logging_writer.py:48] [390800] global_step=390800, grad_norm=5.152223110198975, loss=3.2641420364379883
I0310 09:35:39.986300 139708407461632 logging_writer.py:48] [390900] global_step=390900, grad_norm=3.310293674468994, loss=1.2170451879501343
I0310 09:36:25.208027 139708415854336 logging_writer.py:48] [391000] global_step=391000, grad_norm=3.3204755783081055, loss=1.2494221925735474
I0310 09:37:10.619657 139708407461632 logging_writer.py:48] [391100] global_step=391100, grad_norm=3.091665506362915, loss=1.849334955215454
I0310 09:37:55.811100 139708415854336 logging_writer.py:48] [391200] global_step=391200, grad_norm=3.07779860496521, loss=1.210035800933838
I0310 09:38:41.116066 139708407461632 logging_writer.py:48] [391300] global_step=391300, grad_norm=3.0506935119628906, loss=1.0388399362564087
I0310 09:39:26.569997 139708415854336 logging_writer.py:48] [391400] global_step=391400, grad_norm=3.1854066848754883, loss=1.6965314149856567
I0310 09:40:11.758224 139708407461632 logging_writer.py:48] [391500] global_step=391500, grad_norm=3.341848611831665, loss=1.1728136539459229
I0310 09:40:57.407284 139708415854336 logging_writer.py:48] [391600] global_step=391600, grad_norm=3.165933847427368, loss=1.1719725131988525
I0310 09:41:22.032105 139902746892096 spec.py:321] Evaluating on the training split.
I0310 09:41:33.303286 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 09:41:55.221609 139902746892096 spec.py:349] Evaluating on the test split.
I0310 09:41:56.895992 139902746892096 submission_runner.py:411] Time since start: 189639.92s, 	Step: 391656, 	{'train/accuracy': 0.8898046612739563, 'train/loss': 0.4087473452091217, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 175229.94012641907, 'total_duration': 189639.92478442192, 'accumulated_submission_time': 175229.94012641907, 'accumulated_eval_time': 14363.516121864319, 'accumulated_logging_time': 25.941320419311523}
I0310 09:41:56.974460 139708407461632 logging_writer.py:48] [391656] accumulated_eval_time=14363.516122, accumulated_logging_time=25.941320, accumulated_submission_time=175229.940126, global_step=391656, preemption_count=0, score=175229.940126, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=189639.924784, train/accuracy=0.889805, train/loss=0.408747, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 09:42:14.678344 139708415854336 logging_writer.py:48] [391700] global_step=391700, grad_norm=3.3543570041656494, loss=1.3431804180145264
I0310 09:42:57.331495 139708407461632 logging_writer.py:48] [391800] global_step=391800, grad_norm=3.419522762298584, loss=1.1274151802062988
I0310 09:43:42.608531 139708415854336 logging_writer.py:48] [391900] global_step=391900, grad_norm=3.1205177307128906, loss=1.1609269380569458
I0310 09:44:28.077730 139708407461632 logging_writer.py:48] [392000] global_step=392000, grad_norm=4.899351119995117, loss=3.2600767612457275
I0310 09:45:13.118484 139708415854336 logging_writer.py:48] [392100] global_step=392100, grad_norm=3.1396467685699463, loss=2.434634208679199
I0310 09:45:58.361443 139708407461632 logging_writer.py:48] [392200] global_step=392200, grad_norm=3.1938531398773193, loss=1.1417919397354126
I0310 09:46:43.610136 139708415854336 logging_writer.py:48] [392300] global_step=392300, grad_norm=3.0323967933654785, loss=1.6359556913375854
I0310 09:47:28.867461 139708407461632 logging_writer.py:48] [392400] global_step=392400, grad_norm=3.521157741546631, loss=1.1965595483779907
I0310 09:48:14.296790 139708415854336 logging_writer.py:48] [392500] global_step=392500, grad_norm=3.2288148403167725, loss=1.310210943222046
I0310 09:48:57.070866 139902746892096 spec.py:321] Evaluating on the training split.
I0310 09:49:08.267825 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 09:49:29.875141 139902746892096 spec.py:349] Evaluating on the test split.
I0310 09:49:31.552568 139902746892096 submission_runner.py:411] Time since start: 190094.58s, 	Step: 392596, 	{'train/accuracy': 0.8891991972923279, 'train/loss': 0.41116905212402344, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 175649.9788453579, 'total_duration': 190094.58134031296, 'accumulated_submission_time': 175649.9788453579, 'accumulated_eval_time': 14397.997807979584, 'accumulated_logging_time': 26.028414011001587}
I0310 09:49:31.643222 139708407461632 logging_writer.py:48] [392596] accumulated_eval_time=14397.997808, accumulated_logging_time=26.028414, accumulated_submission_time=175649.978845, global_step=392596, preemption_count=0, score=175649.978845, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=190094.581340, train/accuracy=0.889199, train/loss=0.411169, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 09:49:33.616012 139708415854336 logging_writer.py:48] [392600] global_step=392600, grad_norm=3.1964876651763916, loss=1.0996918678283691
I0310 09:50:13.725227 139708407461632 logging_writer.py:48] [392700] global_step=392700, grad_norm=3.392922878265381, loss=2.724684715270996
I0310 09:50:58.912652 139708415854336 logging_writer.py:48] [392800] global_step=392800, grad_norm=2.858468532562256, loss=1.741662621498108
I0310 09:51:44.707989 139708407461632 logging_writer.py:48] [392900] global_step=392900, grad_norm=3.0684282779693604, loss=1.7120211124420166
I0310 09:52:30.164791 139708415854336 logging_writer.py:48] [393000] global_step=393000, grad_norm=3.0735654830932617, loss=1.1978543996810913
I0310 09:53:15.367780 139708407461632 logging_writer.py:48] [393100] global_step=393100, grad_norm=3.271263599395752, loss=2.5621893405914307
I0310 09:54:00.980339 139708415854336 logging_writer.py:48] [393200] global_step=393200, grad_norm=3.079526424407959, loss=1.1335381269454956
I0310 09:54:46.189987 139708407461632 logging_writer.py:48] [393300] global_step=393300, grad_norm=3.1833529472351074, loss=1.2098937034606934
I0310 09:55:31.511654 139708415854336 logging_writer.py:48] [393400] global_step=393400, grad_norm=3.252793312072754, loss=1.0867640972137451
I0310 09:56:16.731972 139708407461632 logging_writer.py:48] [393500] global_step=393500, grad_norm=3.087031841278076, loss=1.1589105129241943
I0310 09:56:31.792033 139902746892096 spec.py:321] Evaluating on the training split.
I0310 09:56:43.005959 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 09:57:03.335013 139902746892096 spec.py:349] Evaluating on the test split.
I0310 09:57:05.021852 139902746892096 submission_runner.py:411] Time since start: 190548.05s, 	Step: 393535, 	{'train/accuracy': 0.8874413967132568, 'train/loss': 0.4186658263206482, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 176070.06787610054, 'total_duration': 190548.05064105988, 'accumulated_submission_time': 176070.06787610054, 'accumulated_eval_time': 14431.227613449097, 'accumulated_logging_time': 26.12958836555481}
I0310 09:57:05.114207 139708415854336 logging_writer.py:48] [393535] accumulated_eval_time=14431.227613, accumulated_logging_time=26.129588, accumulated_submission_time=176070.067876, global_step=393535, preemption_count=0, score=176070.067876, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=190548.050641, train/accuracy=0.887441, train/loss=0.418666, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 09:57:31.095143 139708407461632 logging_writer.py:48] [393600] global_step=393600, grad_norm=2.9540164470672607, loss=1.781057357788086
I0310 09:58:15.018009 139708415854336 logging_writer.py:48] [393700] global_step=393700, grad_norm=3.650953769683838, loss=3.168290138244629
I0310 09:59:00.149737 139708407461632 logging_writer.py:48] [393800] global_step=393800, grad_norm=2.9657840728759766, loss=1.0949652194976807
I0310 09:59:45.638464 139708415854336 logging_writer.py:48] [393900] global_step=393900, grad_norm=3.244992733001709, loss=1.1099679470062256
I0310 10:00:30.703747 139708407461632 logging_writer.py:48] [394000] global_step=394000, grad_norm=3.025280237197876, loss=1.0911359786987305
I0310 10:01:16.192791 139708415854336 logging_writer.py:48] [394100] global_step=394100, grad_norm=3.0232343673706055, loss=1.106003999710083
I0310 10:02:01.581960 139708407461632 logging_writer.py:48] [394200] global_step=394200, grad_norm=2.9301440715789795, loss=1.761781930923462
I0310 10:02:46.523841 139708415854336 logging_writer.py:48] [394300] global_step=394300, grad_norm=3.271775960922241, loss=2.1317996978759766
I0310 10:03:31.877213 139708407461632 logging_writer.py:48] [394400] global_step=394400, grad_norm=3.2122695446014404, loss=1.114463448524475
I0310 10:04:05.454481 139902746892096 spec.py:321] Evaluating on the training split.
I0310 10:04:17.295588 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 10:04:39.718168 139902746892096 spec.py:349] Evaluating on the test split.
I0310 10:04:41.395154 139902746892096 submission_runner.py:411] Time since start: 191004.42s, 	Step: 394476, 	{'train/accuracy': 0.8876562118530273, 'train/loss': 0.4162094295024872, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 176490.34965968132, 'total_duration': 191004.4239397049, 'accumulated_submission_time': 176490.34965968132, 'accumulated_eval_time': 14467.168281316757, 'accumulated_logging_time': 26.231871128082275}
I0310 10:04:41.479558 139708415854336 logging_writer.py:48] [394476] accumulated_eval_time=14467.168281, accumulated_logging_time=26.231871, accumulated_submission_time=176490.349660, global_step=394476, preemption_count=0, score=176490.349660, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=191004.423940, train/accuracy=0.887656, train/loss=0.416209, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 10:04:51.323335 139708407461632 logging_writer.py:48] [394500] global_step=394500, grad_norm=3.0363686084747314, loss=1.3437613248825073
I0310 10:05:32.463693 139708415854336 logging_writer.py:48] [394600] global_step=394600, grad_norm=3.4285759925842285, loss=2.697969675064087
I0310 10:06:17.628423 139708407461632 logging_writer.py:48] [394700] global_step=394700, grad_norm=3.1546449661254883, loss=1.320487380027771
I0310 10:07:03.004697 139708415854336 logging_writer.py:48] [394800] global_step=394800, grad_norm=3.1743085384368896, loss=1.2238045930862427
I0310 10:07:48.256184 139708407461632 logging_writer.py:48] [394900] global_step=394900, grad_norm=3.099384307861328, loss=1.090673565864563
I0310 10:08:33.429958 139708415854336 logging_writer.py:48] [395000] global_step=395000, grad_norm=3.162266731262207, loss=1.1519792079925537
I0310 10:09:18.819448 139708407461632 logging_writer.py:48] [395100] global_step=395100, grad_norm=3.054429769515991, loss=1.0753505229949951
I0310 10:10:04.094852 139708415854336 logging_writer.py:48] [395200] global_step=395200, grad_norm=3.1096644401550293, loss=2.6359617710113525
I0310 10:10:49.242913 139708407461632 logging_writer.py:48] [395300] global_step=395300, grad_norm=3.2513864040374756, loss=1.1403038501739502
I0310 10:11:34.722741 139708415854336 logging_writer.py:48] [395400] global_step=395400, grad_norm=3.951289653778076, loss=3.3969802856445312
I0310 10:11:41.698212 139902746892096 spec.py:321] Evaluating on the training split.
I0310 10:11:53.033753 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 10:12:13.309113 139902746892096 spec.py:349] Evaluating on the test split.
I0310 10:12:14.996868 139902746892096 submission_runner.py:411] Time since start: 191458.03s, 	Step: 395417, 	{'train/accuracy': 0.8920117020606995, 'train/loss': 0.4101240336894989, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 176910.50852513313, 'total_duration': 191458.02564930916, 'accumulated_submission_time': 176910.50852513313, 'accumulated_eval_time': 14500.466915845871, 'accumulated_logging_time': 26.326695442199707}
I0310 10:12:15.092073 139708407461632 logging_writer.py:48] [395417] accumulated_eval_time=14500.466916, accumulated_logging_time=26.326695, accumulated_submission_time=176910.508525, global_step=395417, preemption_count=0, score=176910.508525, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=191458.025649, train/accuracy=0.892012, train/loss=0.410124, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 10:12:48.188797 139708415854336 logging_writer.py:48] [395500] global_step=395500, grad_norm=3.2649025917053223, loss=1.2632395029067993
I0310 10:13:33.154437 139708407461632 logging_writer.py:48] [395600] global_step=395600, grad_norm=3.1065797805786133, loss=1.5132510662078857
I0310 10:14:18.852749 139708415854336 logging_writer.py:48] [395700] global_step=395700, grad_norm=3.73964262008667, loss=3.2262868881225586
I0310 10:15:04.297205 139708407461632 logging_writer.py:48] [395800] global_step=395800, grad_norm=3.0104262828826904, loss=2.3607981204986572
I0310 10:15:49.654515 139708415854336 logging_writer.py:48] [395900] global_step=395900, grad_norm=3.4534406661987305, loss=1.1947417259216309
I0310 10:16:34.968572 139708407461632 logging_writer.py:48] [396000] global_step=396000, grad_norm=3.118934392929077, loss=1.1023812294006348
I0310 10:17:20.179972 139708415854336 logging_writer.py:48] [396100] global_step=396100, grad_norm=3.5946006774902344, loss=3.2072365283966064
I0310 10:18:05.513412 139708407461632 logging_writer.py:48] [396200] global_step=396200, grad_norm=3.66096830368042, loss=2.6724841594696045
I0310 10:18:50.802859 139708415854336 logging_writer.py:48] [396300] global_step=396300, grad_norm=3.213423490524292, loss=1.1310738325119019
I0310 10:19:15.420340 139902746892096 spec.py:321] Evaluating on the training split.
I0310 10:19:26.826110 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 10:19:48.632467 139902746892096 spec.py:349] Evaluating on the test split.
I0310 10:19:50.297670 139902746892096 submission_runner.py:411] Time since start: 191913.33s, 	Step: 396356, 	{'train/accuracy': 0.88880854845047, 'train/loss': 0.409117728471756, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 177330.77657341957, 'total_duration': 191913.3264658451, 'accumulated_submission_time': 177330.77657341957, 'accumulated_eval_time': 14535.344248533249, 'accumulated_logging_time': 26.433138370513916}
I0310 10:19:50.372562 139708407461632 logging_writer.py:48] [396356] accumulated_eval_time=14535.344249, accumulated_logging_time=26.433138, accumulated_submission_time=177330.776573, global_step=396356, preemption_count=0, score=177330.776573, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=191913.326466, train/accuracy=0.888809, train/loss=0.409118, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 10:20:08.074856 139708415854336 logging_writer.py:48] [396400] global_step=396400, grad_norm=3.0608394145965576, loss=1.720299243927002
I0310 10:20:50.138310 139708407461632 logging_writer.py:48] [396500] global_step=396500, grad_norm=2.9595794677734375, loss=1.146644949913025
I0310 10:21:35.829701 139708415854336 logging_writer.py:48] [396600] global_step=396600, grad_norm=3.0776219367980957, loss=1.1418737173080444
I0310 10:22:21.547364 139708407461632 logging_writer.py:48] [396700] global_step=396700, grad_norm=3.430737018585205, loss=1.8797916173934937
I0310 10:23:06.670225 139708415854336 logging_writer.py:48] [396800] global_step=396800, grad_norm=2.8958213329315186, loss=1.128061056137085
I0310 10:23:52.035999 139708407461632 logging_writer.py:48] [396900] global_step=396900, grad_norm=3.244028091430664, loss=2.9038138389587402
I0310 10:24:37.197706 139708415854336 logging_writer.py:48] [397000] global_step=397000, grad_norm=3.0008645057678223, loss=1.1344420909881592
I0310 10:25:22.632128 139708407461632 logging_writer.py:48] [397100] global_step=397100, grad_norm=3.111231803894043, loss=2.424278974533081
I0310 10:26:08.018721 139708415854336 logging_writer.py:48] [397200] global_step=397200, grad_norm=2.9306931495666504, loss=2.677752733230591
I0310 10:26:50.603106 139902746892096 spec.py:321] Evaluating on the training split.
I0310 10:27:02.245705 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 10:27:24.908573 139902746892096 spec.py:349] Evaluating on the test split.
I0310 10:27:26.583420 139902746892096 submission_runner.py:411] Time since start: 192369.61s, 	Step: 397296, 	{'train/accuracy': 0.8882226347923279, 'train/loss': 0.4123573303222656, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 177750.9477841854, 'total_duration': 192369.61221718788, 'accumulated_submission_time': 177750.9477841854, 'accumulated_eval_time': 14571.324562311172, 'accumulated_logging_time': 26.5184805393219}
I0310 10:27:26.661545 139708407461632 logging_writer.py:48] [397296] accumulated_eval_time=14571.324562, accumulated_logging_time=26.518481, accumulated_submission_time=177750.947784, global_step=397296, preemption_count=0, score=177750.947784, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=192369.612217, train/accuracy=0.888223, train/loss=0.412357, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 10:27:28.631746 139708415854336 logging_writer.py:48] [397300] global_step=397300, grad_norm=3.360727310180664, loss=1.1354522705078125
I0310 10:28:08.692811 139708407461632 logging_writer.py:48] [397400] global_step=397400, grad_norm=3.1102304458618164, loss=1.1079062223434448
I0310 10:28:53.629205 139708415854336 logging_writer.py:48] [397500] global_step=397500, grad_norm=3.30295729637146, loss=1.2182519435882568
I0310 10:29:38.690508 139708407461632 logging_writer.py:48] [397600] global_step=397600, grad_norm=3.25278902053833, loss=2.349097728729248
I0310 10:30:24.455630 139708415854336 logging_writer.py:48] [397700] global_step=397700, grad_norm=3.7231709957122803, loss=3.1095359325408936
I0310 10:31:09.662085 139708407461632 logging_writer.py:48] [397800] global_step=397800, grad_norm=2.6929993629455566, loss=1.4802260398864746
I0310 10:31:55.295620 139708415854336 logging_writer.py:48] [397900] global_step=397900, grad_norm=3.8463618755340576, loss=2.884040117263794
I0310 10:32:40.684564 139708407461632 logging_writer.py:48] [398000] global_step=398000, grad_norm=3.0690293312072754, loss=1.8880692720413208
I0310 10:33:26.236301 139708415854336 logging_writer.py:48] [398100] global_step=398100, grad_norm=3.0760746002197266, loss=1.0977380275726318
I0310 10:34:11.264735 139708407461632 logging_writer.py:48] [398200] global_step=398200, grad_norm=3.2222177982330322, loss=2.1105048656463623
I0310 10:34:26.772723 139902746892096 spec.py:321] Evaluating on the training split.
I0310 10:34:38.923247 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 10:35:00.905915 139902746892096 spec.py:349] Evaluating on the test split.
I0310 10:35:02.579419 139902746892096 submission_runner.py:411] Time since start: 192825.61s, 	Step: 398236, 	{'train/accuracy': 0.88880854845047, 'train/loss': 0.4158269166946411, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 178170.99898934364, 'total_duration': 192825.60821533203, 'accumulated_submission_time': 178170.99898934364, 'accumulated_eval_time': 14607.131270647049, 'accumulated_logging_time': 26.60717272758484}
I0310 10:35:02.659016 139708415854336 logging_writer.py:48] [398236] accumulated_eval_time=14607.131271, accumulated_logging_time=26.607173, accumulated_submission_time=178170.998989, global_step=398236, preemption_count=0, score=178170.998989, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=192825.608215, train/accuracy=0.888809, train/loss=0.415827, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 10:35:28.229294 139708407461632 logging_writer.py:48] [398300] global_step=398300, grad_norm=2.9405436515808105, loss=1.5134215354919434
I0310 10:36:12.094310 139708415854336 logging_writer.py:48] [398400] global_step=398400, grad_norm=3.405574321746826, loss=3.047016143798828
I0310 10:36:57.711753 139708407461632 logging_writer.py:48] [398500] global_step=398500, grad_norm=2.957561731338501, loss=1.0940914154052734
I0310 10:37:43.529039 139708415854336 logging_writer.py:48] [398600] global_step=398600, grad_norm=3.104555368423462, loss=1.1957885026931763
I0310 10:38:28.918506 139708407461632 logging_writer.py:48] [398700] global_step=398700, grad_norm=3.047468900680542, loss=1.3784173727035522
I0310 10:39:14.024828 139708415854336 logging_writer.py:48] [398800] global_step=398800, grad_norm=3.307959794998169, loss=1.1388235092163086
I0310 10:39:59.526176 139708407461632 logging_writer.py:48] [398900] global_step=398900, grad_norm=3.1008083820343018, loss=2.570309638977051
I0310 10:40:44.886952 139708415854336 logging_writer.py:48] [399000] global_step=399000, grad_norm=2.948805332183838, loss=1.8254948854446411
I0310 10:41:30.532679 139708407461632 logging_writer.py:48] [399100] global_step=399100, grad_norm=2.968033790588379, loss=1.4492417573928833
I0310 10:42:02.881845 139902746892096 spec.py:321] Evaluating on the training split.
I0310 10:42:14.549407 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 10:42:36.549984 139902746892096 spec.py:349] Evaluating on the test split.
I0310 10:42:38.221192 139902746892096 submission_runner.py:411] Time since start: 193281.25s, 	Step: 399173, 	{'train/accuracy': 0.8885546922683716, 'train/loss': 0.40943875908851624, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 178591.1631834507, 'total_duration': 193281.24997234344, 'accumulated_submission_time': 178591.1631834507, 'accumulated_eval_time': 14642.470609903336, 'accumulated_logging_time': 26.696512460708618}
I0310 10:42:38.314773 139708415854336 logging_writer.py:48] [399173] accumulated_eval_time=14642.470610, accumulated_logging_time=26.696512, accumulated_submission_time=178591.163183, global_step=399173, preemption_count=0, score=178591.163183, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=193281.249972, train/accuracy=0.888555, train/loss=0.409439, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 10:42:49.333741 139708407461632 logging_writer.py:48] [399200] global_step=399200, grad_norm=3.0407931804656982, loss=1.1000316143035889
I0310 10:43:31.350065 139708415854336 logging_writer.py:48] [399300] global_step=399300, grad_norm=2.958853244781494, loss=1.128265142440796
I0310 10:44:16.574748 139708407461632 logging_writer.py:48] [399400] global_step=399400, grad_norm=3.1015913486480713, loss=1.3583707809448242
I0310 10:45:02.000136 139708415854336 logging_writer.py:48] [399500] global_step=399500, grad_norm=3.2607929706573486, loss=1.1704661846160889
I0310 10:45:47.119372 139708407461632 logging_writer.py:48] [399600] global_step=399600, grad_norm=3.185133934020996, loss=1.41682767868042
I0310 10:46:32.406269 139708415854336 logging_writer.py:48] [399700] global_step=399700, grad_norm=2.9847323894500732, loss=2.4581844806671143
I0310 10:47:17.693632 139708407461632 logging_writer.py:48] [399800] global_step=399800, grad_norm=3.0591585636138916, loss=1.391434669494629
I0310 10:48:02.827099 139708415854336 logging_writer.py:48] [399900] global_step=399900, grad_norm=3.016411542892456, loss=1.2958256006240845
I0310 10:48:48.080520 139708407461632 logging_writer.py:48] [400000] global_step=400000, grad_norm=2.8774771690368652, loss=1.4552186727523804
I0310 10:49:33.105170 139708415854336 logging_writer.py:48] [400100] global_step=400100, grad_norm=2.9644408226013184, loss=1.234277606010437
I0310 10:49:38.260815 139902746892096 spec.py:321] Evaluating on the training split.
I0310 10:49:49.607678 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 10:50:11.825818 139902746892096 spec.py:349] Evaluating on the test split.
I0310 10:50:13.496452 139902746892096 submission_runner.py:411] Time since start: 193736.53s, 	Step: 400113, 	{'train/accuracy': 0.8860546946525574, 'train/loss': 0.41684088110923767, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 179011.04693436623, 'total_duration': 193736.52523708344, 'accumulated_submission_time': 179011.04693436623, 'accumulated_eval_time': 14677.706232786179, 'accumulated_logging_time': 26.80361557006836}
I0310 10:50:13.577120 139708407461632 logging_writer.py:48] [400113] accumulated_eval_time=14677.706233, accumulated_logging_time=26.803616, accumulated_submission_time=179011.046934, global_step=400113, preemption_count=0, score=179011.046934, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=193736.525237, train/accuracy=0.886055, train/loss=0.416841, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 10:50:48.201218 139708415854336 logging_writer.py:48] [400200] global_step=400200, grad_norm=3.4514026641845703, loss=3.197817087173462
I0310 10:51:33.112615 139708407461632 logging_writer.py:48] [400300] global_step=400300, grad_norm=3.4436192512512207, loss=1.671440601348877
I0310 10:52:18.739628 139708415854336 logging_writer.py:48] [400400] global_step=400400, grad_norm=3.638444662094116, loss=3.242786407470703
I0310 10:53:04.401786 139708407461632 logging_writer.py:48] [400500] global_step=400500, grad_norm=3.802384853363037, loss=3.276475191116333
I0310 10:53:49.707180 139708415854336 logging_writer.py:48] [400600] global_step=400600, grad_norm=3.558046579360962, loss=2.974430799484253
I0310 10:54:35.090020 139708407461632 logging_writer.py:48] [400700] global_step=400700, grad_norm=3.0294055938720703, loss=1.4032223224639893
I0310 10:55:20.234344 139708415854336 logging_writer.py:48] [400800] global_step=400800, grad_norm=3.370678186416626, loss=3.065950632095337
I0310 10:56:05.725916 139708407461632 logging_writer.py:48] [400900] global_step=400900, grad_norm=3.049229383468628, loss=1.1205004453659058
I0310 10:56:50.973856 139708415854336 logging_writer.py:48] [401000] global_step=401000, grad_norm=3.1274936199188232, loss=1.1007487773895264
I0310 10:57:13.762277 139902746892096 spec.py:321] Evaluating on the training split.
I0310 10:57:24.937219 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 10:57:45.440207 139902746892096 spec.py:349] Evaluating on the test split.
I0310 10:57:47.133419 139902746892096 submission_runner.py:411] Time since start: 194190.16s, 	Step: 401052, 	{'train/accuracy': 0.8895898461341858, 'train/loss': 0.4095662832260132, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 179431.1743233204, 'total_duration': 194190.1621646881, 'accumulated_submission_time': 179431.1743233204, 'accumulated_eval_time': 14711.077335596085, 'accumulated_logging_time': 26.893108129501343}
I0310 10:57:47.265934 139708407461632 logging_writer.py:48] [401052] accumulated_eval_time=14711.077336, accumulated_logging_time=26.893108, accumulated_submission_time=179431.174323, global_step=401052, preemption_count=0, score=179431.174323, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=194190.162165, train/accuracy=0.889590, train/loss=0.409566, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 10:58:06.592208 139708415854336 logging_writer.py:48] [401100] global_step=401100, grad_norm=2.8268983364105225, loss=1.8787620067596436
I0310 10:58:49.561166 139708407461632 logging_writer.py:48] [401200] global_step=401200, grad_norm=3.0478625297546387, loss=1.219115972518921
I0310 10:59:34.326171 139708415854336 logging_writer.py:48] [401300] global_step=401300, grad_norm=3.471649169921875, loss=2.859696388244629
I0310 11:00:19.609303 139708407461632 logging_writer.py:48] [401400] global_step=401400, grad_norm=3.2981913089752197, loss=2.697550058364868
I0310 11:01:04.671117 139708415854336 logging_writer.py:48] [401500] global_step=401500, grad_norm=3.1568329334259033, loss=1.1463292837142944
I0310 11:01:49.978197 139708407461632 logging_writer.py:48] [401600] global_step=401600, grad_norm=3.225083827972412, loss=1.1149590015411377
I0310 11:02:35.555771 139708415854336 logging_writer.py:48] [401700] global_step=401700, grad_norm=3.177436351776123, loss=1.7373578548431396
I0310 11:03:20.751657 139708407461632 logging_writer.py:48] [401800] global_step=401800, grad_norm=3.0317955017089844, loss=1.681048035621643
I0310 11:04:06.013071 139708415854336 logging_writer.py:48] [401900] global_step=401900, grad_norm=2.975292921066284, loss=1.498000979423523
I0310 11:04:47.405309 139902746892096 spec.py:321] Evaluating on the training split.
I0310 11:04:58.936512 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 11:05:20.772602 139902746892096 spec.py:349] Evaluating on the test split.
I0310 11:05:22.445206 139902746892096 submission_runner.py:411] Time since start: 194645.47s, 	Step: 401993, 	{'train/accuracy': 0.8876367211341858, 'train/loss': 0.41779905557632446, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 179851.25026535988, 'total_duration': 194645.4740064144, 'accumulated_submission_time': 179851.25026535988, 'accumulated_eval_time': 14746.11722946167, 'accumulated_logging_time': 27.03999924659729}
I0310 11:05:22.519953 139708407461632 logging_writer.py:48] [401993] accumulated_eval_time=14746.117229, accumulated_logging_time=27.039999, accumulated_submission_time=179851.250265, global_step=401993, preemption_count=0, score=179851.250265, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=194645.474006, train/accuracy=0.887637, train/loss=0.417799, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 11:05:25.675301 139708415854336 logging_writer.py:48] [402000] global_step=402000, grad_norm=3.138983964920044, loss=1.5218262672424316
I0310 11:06:06.427367 139708407461632 logging_writer.py:48] [402100] global_step=402100, grad_norm=3.152827262878418, loss=1.2114263772964478
I0310 11:06:51.458400 139708415854336 logging_writer.py:48] [402200] global_step=402200, grad_norm=3.224618673324585, loss=2.5498859882354736
I0310 11:07:37.287979 139708407461632 logging_writer.py:48] [402300] global_step=402300, grad_norm=3.079371213912964, loss=2.0683867931365967
I0310 11:08:22.883030 139708415854336 logging_writer.py:48] [402400] global_step=402400, grad_norm=2.9042844772338867, loss=1.2103567123413086
I0310 11:09:08.120419 139708407461632 logging_writer.py:48] [402500] global_step=402500, grad_norm=2.914621591567993, loss=1.1398890018463135
I0310 11:09:53.330530 139708415854336 logging_writer.py:48] [402600] global_step=402600, grad_norm=3.0308139324188232, loss=1.0802223682403564
I0310 11:10:38.636888 139708407461632 logging_writer.py:48] [402700] global_step=402700, grad_norm=3.7011144161224365, loss=2.9349629878997803
I0310 11:11:23.881886 139708415854336 logging_writer.py:48] [402800] global_step=402800, grad_norm=3.4314911365509033, loss=2.8681137561798096
I0310 11:12:09.634441 139708407461632 logging_writer.py:48] [402900] global_step=402900, grad_norm=3.185641050338745, loss=1.129725694656372
I0310 11:12:22.677304 139902746892096 spec.py:321] Evaluating on the training split.
I0310 11:12:34.098539 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 11:12:54.565907 139902746892096 spec.py:349] Evaluating on the test split.
I0310 11:12:56.250199 139902746892096 submission_runner.py:411] Time since start: 195099.28s, 	Step: 402930, 	{'train/accuracy': 0.8872265219688416, 'train/loss': 0.4172936677932739, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 180271.34703087807, 'total_duration': 195099.27899551392, 'accumulated_submission_time': 180271.34703087807, 'accumulated_eval_time': 14779.690134763718, 'accumulated_logging_time': 27.126314640045166}
I0310 11:12:56.325260 139708415854336 logging_writer.py:48] [402930] accumulated_eval_time=14779.690135, accumulated_logging_time=27.126315, accumulated_submission_time=180271.347031, global_step=402930, preemption_count=0, score=180271.347031, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=195099.278996, train/accuracy=0.887227, train/loss=0.417294, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 11:13:24.255594 139708407461632 logging_writer.py:48] [403000] global_step=403000, grad_norm=3.4423089027404785, loss=1.1801655292510986
I0310 11:14:08.532974 139708415854336 logging_writer.py:48] [403100] global_step=403100, grad_norm=3.8648221492767334, loss=3.257797956466675
I0310 11:14:53.988861 139708407461632 logging_writer.py:48] [403200] global_step=403200, grad_norm=3.128530740737915, loss=1.186025619506836
I0310 11:15:39.732887 139708415854336 logging_writer.py:48] [403300] global_step=403300, grad_norm=3.1937143802642822, loss=2.9599225521087646
I0310 11:16:24.983492 139708407461632 logging_writer.py:48] [403400] global_step=403400, grad_norm=3.2168750762939453, loss=1.0737476348876953
I0310 11:17:10.303949 139708415854336 logging_writer.py:48] [403500] global_step=403500, grad_norm=3.076291561126709, loss=2.229391098022461
I0310 11:17:55.700714 139708407461632 logging_writer.py:48] [403600] global_step=403600, grad_norm=3.7573626041412354, loss=3.2796006202697754
I0310 11:18:40.856272 139708415854336 logging_writer.py:48] [403700] global_step=403700, grad_norm=3.1569108963012695, loss=1.1701749563217163
I0310 11:19:25.967154 139708407461632 logging_writer.py:48] [403800] global_step=403800, grad_norm=2.8816258907318115, loss=1.9873600006103516
I0310 11:19:56.380517 139902746892096 spec.py:321] Evaluating on the training split.
I0310 11:20:07.808748 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 11:20:28.861624 139902746892096 spec.py:349] Evaluating on the test split.
I0310 11:20:30.539375 139902746892096 submission_runner.py:411] Time since start: 195553.57s, 	Step: 403869, 	{'train/accuracy': 0.8881054520606995, 'train/loss': 0.4169188141822815, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 180691.3440322876, 'total_duration': 195553.56817293167, 'accumulated_submission_time': 180691.3440322876, 'accumulated_eval_time': 14813.849033117294, 'accumulated_logging_time': 27.21081781387329}
I0310 11:20:30.617548 139708415854336 logging_writer.py:48] [403869] accumulated_eval_time=14813.849033, accumulated_logging_time=27.210818, accumulated_submission_time=180691.344032, global_step=403869, preemption_count=0, score=180691.344032, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=195553.568173, train/accuracy=0.888105, train/loss=0.416919, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 11:20:43.216335 139708407461632 logging_writer.py:48] [403900] global_step=403900, grad_norm=3.2247748374938965, loss=1.2269699573516846
I0310 11:21:25.148377 139708415854336 logging_writer.py:48] [404000] global_step=404000, grad_norm=3.403221845626831, loss=2.3029370307922363
I0310 11:22:10.093444 139708407461632 logging_writer.py:48] [404100] global_step=404100, grad_norm=2.798032283782959, loss=1.9493926763534546
I0310 11:22:56.157294 139708415854336 logging_writer.py:48] [404200] global_step=404200, grad_norm=3.2004997730255127, loss=1.8664733171463013
I0310 11:23:41.451211 139708407461632 logging_writer.py:48] [404300] global_step=404300, grad_norm=4.290378570556641, loss=3.31946063041687
I0310 11:24:26.925771 139708415854336 logging_writer.py:48] [404400] global_step=404400, grad_norm=3.0546247959136963, loss=1.4518646001815796
I0310 11:25:12.481403 139708407461632 logging_writer.py:48] [404500] global_step=404500, grad_norm=2.9996538162231445, loss=1.019594669342041
I0310 11:25:57.750049 139708415854336 logging_writer.py:48] [404600] global_step=404600, grad_norm=3.0331993103027344, loss=1.0646508932113647
I0310 11:26:42.997009 139708407461632 logging_writer.py:48] [404700] global_step=404700, grad_norm=3.1724681854248047, loss=1.7755792140960693
I0310 11:27:28.621766 139708415854336 logging_writer.py:48] [404800] global_step=404800, grad_norm=3.0828793048858643, loss=2.582111120223999
I0310 11:27:30.713841 139902746892096 spec.py:321] Evaluating on the training split.
I0310 11:27:42.099515 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 11:28:05.049335 139902746892096 spec.py:349] Evaluating on the test split.
I0310 11:28:06.725084 139902746892096 submission_runner.py:411] Time since start: 196009.75s, 	Step: 404806, 	{'train/accuracy': 0.8880273103713989, 'train/loss': 0.4125872850418091, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 181111.38258075714, 'total_duration': 196009.753885746, 'accumulated_submission_time': 181111.38258075714, 'accumulated_eval_time': 14849.860277414322, 'accumulated_logging_time': 27.298006534576416}
I0310 11:28:06.800741 139708407461632 logging_writer.py:48] [404806] accumulated_eval_time=14849.860277, accumulated_logging_time=27.298007, accumulated_submission_time=181111.382581, global_step=404806, preemption_count=0, score=181111.382581, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=196009.753886, train/accuracy=0.888027, train/loss=0.412587, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 11:28:44.173705 139708415854336 logging_writer.py:48] [404900] global_step=404900, grad_norm=3.2273504734039307, loss=1.113093376159668
I0310 11:29:29.294483 139708407461632 logging_writer.py:48] [405000] global_step=405000, grad_norm=3.3735930919647217, loss=1.1590467691421509
I0310 11:30:14.671463 139708415854336 logging_writer.py:48] [405100] global_step=405100, grad_norm=2.9306540489196777, loss=1.1502422094345093
I0310 11:31:00.328144 139708407461632 logging_writer.py:48] [405200] global_step=405200, grad_norm=3.053633689880371, loss=1.3745615482330322
I0310 11:31:45.577507 139708415854336 logging_writer.py:48] [405300] global_step=405300, grad_norm=3.128298282623291, loss=1.2179182767868042
I0310 11:32:31.235983 139708407461632 logging_writer.py:48] [405400] global_step=405400, grad_norm=3.2473137378692627, loss=2.470026969909668
I0310 11:33:16.295028 139708415854336 logging_writer.py:48] [405500] global_step=405500, grad_norm=3.0127511024475098, loss=1.909744381904602
I0310 11:34:01.455076 139708407461632 logging_writer.py:48] [405600] global_step=405600, grad_norm=2.8755757808685303, loss=1.1396628618240356
I0310 11:34:46.838253 139708415854336 logging_writer.py:48] [405700] global_step=405700, grad_norm=2.9682724475860596, loss=1.5767035484313965
I0310 11:35:06.844055 139902746892096 spec.py:321] Evaluating on the training split.
I0310 11:35:18.161091 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 11:35:41.564461 139902746892096 spec.py:349] Evaluating on the test split.
I0310 11:35:43.246777 139902746892096 submission_runner.py:411] Time since start: 196466.28s, 	Step: 405746, 	{'train/accuracy': 0.8882226347923279, 'train/loss': 0.4126043915748596, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 181531.36791729927, 'total_duration': 196466.27555465698, 'accumulated_submission_time': 181531.36791729927, 'accumulated_eval_time': 14886.262979507446, 'accumulated_logging_time': 27.382766485214233}
I0310 11:35:43.334758 139708407461632 logging_writer.py:48] [405746] accumulated_eval_time=14886.262980, accumulated_logging_time=27.382766, accumulated_submission_time=181531.367917, global_step=405746, preemption_count=0, score=181531.367917, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=196466.275555, train/accuracy=0.888223, train/loss=0.412604, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 11:36:04.965233 139708415854336 logging_writer.py:48] [405800] global_step=405800, grad_norm=2.999213457107544, loss=1.4322099685668945
I0310 11:36:47.894079 139708407461632 logging_writer.py:48] [405900] global_step=405900, grad_norm=3.738698720932007, loss=3.149299144744873
I0310 11:37:33.361443 139708415854336 logging_writer.py:48] [406000] global_step=406000, grad_norm=3.1359355449676514, loss=2.4288337230682373
I0310 11:38:19.056384 139708407461632 logging_writer.py:48] [406100] global_step=406100, grad_norm=2.9177699089050293, loss=1.3073477745056152
I0310 11:39:04.013716 139708415854336 logging_writer.py:48] [406200] global_step=406200, grad_norm=3.2032978534698486, loss=1.179631233215332
I0310 11:39:49.156978 139708407461632 logging_writer.py:48] [406300] global_step=406300, grad_norm=2.7668960094451904, loss=1.4273113012313843
I0310 11:40:34.514834 139708415854336 logging_writer.py:48] [406400] global_step=406400, grad_norm=2.9347198009490967, loss=1.9258801937103271
I0310 11:41:19.564932 139708407461632 logging_writer.py:48] [406500] global_step=406500, grad_norm=3.1598589420318604, loss=2.6416492462158203
I0310 11:42:04.892032 139708415854336 logging_writer.py:48] [406600] global_step=406600, grad_norm=3.292085647583008, loss=2.900258779525757
I0310 11:42:43.297526 139902746892096 spec.py:321] Evaluating on the training split.
I0310 11:42:54.745836 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 11:43:15.986006 139902746892096 spec.py:349] Evaluating on the test split.
I0310 11:43:17.666478 139902746892096 submission_runner.py:411] Time since start: 196920.70s, 	Step: 406686, 	{'train/accuracy': 0.8878124952316284, 'train/loss': 0.41458389163017273, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 181951.2723581791, 'total_duration': 196920.69525766373, 'accumulated_submission_time': 181951.2723581791, 'accumulated_eval_time': 14920.631911039352, 'accumulated_logging_time': 27.48050045967102}
I0310 11:43:17.758002 139708407461632 logging_writer.py:48] [406686] accumulated_eval_time=14920.631911, accumulated_logging_time=27.480500, accumulated_submission_time=181951.272358, global_step=406686, preemption_count=0, score=181951.272358, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=196920.695258, train/accuracy=0.887812, train/loss=0.414584, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 11:43:23.659595 139708415854336 logging_writer.py:48] [406700] global_step=406700, grad_norm=3.251617193222046, loss=2.9707319736480713
I0310 11:44:04.758815 139708407461632 logging_writer.py:48] [406800] global_step=406800, grad_norm=3.159844160079956, loss=1.2749282121658325
I0310 11:44:50.244077 139708415854336 logging_writer.py:48] [406900] global_step=406900, grad_norm=5.2065629959106445, loss=3.3046841621398926
I0310 11:45:35.982090 139708407461632 logging_writer.py:48] [407000] global_step=407000, grad_norm=3.3179919719696045, loss=1.0865012407302856
I0310 11:46:21.398418 139708415854336 logging_writer.py:48] [407100] global_step=407100, grad_norm=3.7645950317382812, loss=3.0652191638946533
I0310 11:47:06.581656 139708407461632 logging_writer.py:48] [407200] global_step=407200, grad_norm=3.1276655197143555, loss=1.526815414428711
I0310 11:47:52.262372 139708415854336 logging_writer.py:48] [407300] global_step=407300, grad_norm=3.0908939838409424, loss=2.132347822189331
I0310 11:48:37.457925 139708407461632 logging_writer.py:48] [407400] global_step=407400, grad_norm=3.4848554134368896, loss=3.1682844161987305
I0310 11:49:22.774945 139708415854336 logging_writer.py:48] [407500] global_step=407500, grad_norm=2.881446599960327, loss=1.0822429656982422
I0310 11:50:08.040275 139708407461632 logging_writer.py:48] [407600] global_step=407600, grad_norm=3.4231693744659424, loss=1.1878852844238281
I0310 11:50:18.094578 139902746892096 spec.py:321] Evaluating on the training split.
I0310 11:50:29.527733 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 11:50:50.766630 139902746892096 spec.py:349] Evaluating on the test split.
I0310 11:50:52.444638 139902746892096 submission_runner.py:411] Time since start: 197375.47s, 	Step: 407624, 	{'train/accuracy': 0.8862109184265137, 'train/loss': 0.419201523065567, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 182371.54948091507, 'total_duration': 197375.47342300415, 'accumulated_submission_time': 182371.54948091507, 'accumulated_eval_time': 14954.981954574585, 'accumulated_logging_time': 27.58235263824463}
I0310 11:50:52.530022 139708415854336 logging_writer.py:48] [407624] accumulated_eval_time=14954.981955, accumulated_logging_time=27.582353, accumulated_submission_time=182371.549481, global_step=407624, preemption_count=0, score=182371.549481, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=197375.473423, train/accuracy=0.886211, train/loss=0.419202, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 11:51:22.842399 139708407461632 logging_writer.py:48] [407700] global_step=407700, grad_norm=3.2014529705047607, loss=2.7868802547454834
I0310 11:52:07.215866 139708415854336 logging_writer.py:48] [407800] global_step=407800, grad_norm=3.292515993118286, loss=1.1123117208480835
I0310 11:52:52.662606 139708407461632 logging_writer.py:48] [407900] global_step=407900, grad_norm=3.0178797245025635, loss=1.4629769325256348
I0310 11:53:38.562159 139708415854336 logging_writer.py:48] [408000] global_step=408000, grad_norm=3.363340139389038, loss=1.0503917932510376
I0310 11:54:23.746564 139708407461632 logging_writer.py:48] [408100] global_step=408100, grad_norm=5.693099498748779, loss=1.300186038017273
I0310 11:55:09.129406 139708415854336 logging_writer.py:48] [408200] global_step=408200, grad_norm=3.017364263534546, loss=2.2084147930145264
I0310 11:55:54.386112 139708407461632 logging_writer.py:48] [408300] global_step=408300, grad_norm=4.062980651855469, loss=1.2239582538604736
I0310 11:56:40.001107 139708415854336 logging_writer.py:48] [408400] global_step=408400, grad_norm=3.0793819427490234, loss=1.195020318031311
I0310 11:57:25.312623 139708407461632 logging_writer.py:48] [408500] global_step=408500, grad_norm=3.825615644454956, loss=3.156895160675049
I0310 11:57:52.855947 139902746892096 spec.py:321] Evaluating on the training split.
I0310 11:58:04.022252 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 11:58:26.805980 139902746892096 spec.py:349] Evaluating on the test split.
I0310 11:58:28.485832 139902746892096 submission_runner.py:411] Time since start: 197831.51s, 	Step: 408562, 	{'train/accuracy': 0.8884179592132568, 'train/loss': 0.4154849052429199, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 182791.8166847229, 'total_duration': 197831.5146150589, 'accumulated_submission_time': 182791.8166847229, 'accumulated_eval_time': 14990.611825942993, 'accumulated_logging_time': 27.677670001983643}
I0310 11:58:28.570717 139708415854336 logging_writer.py:48] [408562] accumulated_eval_time=14990.611826, accumulated_logging_time=27.677670, accumulated_submission_time=182791.816685, global_step=408562, preemption_count=0, score=182791.816685, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=197831.514615, train/accuracy=0.888418, train/loss=0.415485, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 11:58:43.976651 139708407461632 logging_writer.py:48] [408600] global_step=408600, grad_norm=2.996222496032715, loss=1.1242806911468506
I0310 11:59:25.769485 139708415854336 logging_writer.py:48] [408700] global_step=408700, grad_norm=3.0726139545440674, loss=1.2701505422592163
I0310 12:00:11.001913 139708407461632 logging_writer.py:48] [408800] global_step=408800, grad_norm=3.1790850162506104, loss=1.075355887413025
I0310 12:00:56.411849 139708415854336 logging_writer.py:48] [408900] global_step=408900, grad_norm=3.493884325027466, loss=1.2763689756393433
I0310 12:01:41.664726 139708407461632 logging_writer.py:48] [409000] global_step=409000, grad_norm=3.2194466590881348, loss=2.1934282779693604
I0310 12:02:26.906432 139708415854336 logging_writer.py:48] [409100] global_step=409100, grad_norm=2.7423977851867676, loss=1.6152764558792114
I0310 12:03:12.582718 139708407461632 logging_writer.py:48] [409200] global_step=409200, grad_norm=3.328972339630127, loss=2.0822389125823975
I0310 12:03:57.579152 139708415854336 logging_writer.py:48] [409300] global_step=409300, grad_norm=3.259075880050659, loss=1.186493158340454
I0310 12:04:42.902035 139708407461632 logging_writer.py:48] [409400] global_step=409400, grad_norm=3.030517578125, loss=1.5460244417190552
I0310 12:05:28.155038 139708415854336 logging_writer.py:48] [409500] global_step=409500, grad_norm=4.006690502166748, loss=3.0627989768981934
I0310 12:05:28.746492 139902746892096 spec.py:321] Evaluating on the training split.
I0310 12:05:40.290773 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 12:06:02.429076 139902746892096 spec.py:349] Evaluating on the test split.
I0310 12:06:04.101373 139902746892096 submission_runner.py:411] Time since start: 198287.13s, 	Step: 409503, 	{'train/accuracy': 0.8890624642372131, 'train/loss': 0.41266676783561707, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 183211.93514060974, 'total_duration': 198287.1301677227, 'accumulated_submission_time': 183211.93514060974, 'accumulated_eval_time': 15025.966703891754, 'accumulated_logging_time': 27.77146291732788}
I0310 12:06:04.180182 139708407461632 logging_writer.py:48] [409503] accumulated_eval_time=15025.966704, accumulated_logging_time=27.771463, accumulated_submission_time=183211.935141, global_step=409503, preemption_count=0, score=183211.935141, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=198287.130168, train/accuracy=0.889062, train/loss=0.412667, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 12:06:43.596966 139708415854336 logging_writer.py:48] [409600] global_step=409600, grad_norm=2.747635841369629, loss=1.1188114881515503
I0310 12:07:28.941588 139708407461632 logging_writer.py:48] [409700] global_step=409700, grad_norm=2.9913723468780518, loss=1.0773186683654785
I0310 12:08:14.432587 139708415854336 logging_writer.py:48] [409800] global_step=409800, grad_norm=3.0245330333709717, loss=1.9927635192871094
I0310 12:08:59.826069 139708407461632 logging_writer.py:48] [409900] global_step=409900, grad_norm=2.96891713142395, loss=1.1015660762786865
I0310 12:09:45.007768 139708415854336 logging_writer.py:48] [410000] global_step=410000, grad_norm=2.941680431365967, loss=2.220104694366455
I0310 12:10:30.276033 139708407461632 logging_writer.py:48] [410100] global_step=410100, grad_norm=3.4812676906585693, loss=3.056548833847046
I0310 12:11:15.481884 139708415854336 logging_writer.py:48] [410200] global_step=410200, grad_norm=3.073154926300049, loss=1.0385124683380127
I0310 12:12:00.910408 139708407461632 logging_writer.py:48] [410300] global_step=410300, grad_norm=2.940136671066284, loss=1.2252894639968872
I0310 12:12:46.564643 139708415854336 logging_writer.py:48] [410400] global_step=410400, grad_norm=3.057582378387451, loss=1.0623782873153687
I0310 12:13:04.445949 139902746892096 spec.py:321] Evaluating on the training split.
I0310 12:13:15.920516 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 12:13:38.206607 139902746892096 spec.py:349] Evaluating on the test split.
I0310 12:13:39.878954 139902746892096 submission_runner.py:411] Time since start: 198742.91s, 	Step: 410441, 	{'train/accuracy': 0.8878710865974426, 'train/loss': 0.4203723073005676, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 183632.14395451546, 'total_duration': 198742.9077372551, 'accumulated_submission_time': 183632.14395451546, 'accumulated_eval_time': 15061.399684906006, 'accumulated_logging_time': 27.8588445186615}
I0310 12:13:39.969841 139708407461632 logging_writer.py:48] [410441] accumulated_eval_time=15061.399685, accumulated_logging_time=27.858845, accumulated_submission_time=183632.143955, global_step=410441, preemption_count=0, score=183632.143955, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=198742.907737, train/accuracy=0.887871, train/loss=0.420372, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 12:14:03.577629 139708415854336 logging_writer.py:48] [410500] global_step=410500, grad_norm=3.0653367042541504, loss=2.360617160797119
I0310 12:14:46.706967 139708407461632 logging_writer.py:48] [410600] global_step=410600, grad_norm=2.950953483581543, loss=1.0021920204162598
I0310 12:15:32.104522 139708415854336 logging_writer.py:48] [410700] global_step=410700, grad_norm=2.6910946369171143, loss=1.246790885925293
I0310 12:16:17.744913 139708407461632 logging_writer.py:48] [410800] global_step=410800, grad_norm=2.786463737487793, loss=1.0645662546157837
I0310 12:17:03.048386 139708415854336 logging_writer.py:48] [410900] global_step=410900, grad_norm=4.063104629516602, loss=2.973391056060791
I0310 12:17:48.477682 139708407461632 logging_writer.py:48] [411000] global_step=411000, grad_norm=4.103443145751953, loss=3.3785436153411865
I0310 12:18:33.817226 139708415854336 logging_writer.py:48] [411100] global_step=411100, grad_norm=3.9286584854125977, loss=3.1800198554992676
I0310 12:19:19.587653 139708407461632 logging_writer.py:48] [411200] global_step=411200, grad_norm=3.3249289989471436, loss=1.0583879947662354
I0310 12:20:04.695685 139708415854336 logging_writer.py:48] [411300] global_step=411300, grad_norm=3.377866268157959, loss=2.766214370727539
I0310 12:20:39.943853 139902746892096 spec.py:321] Evaluating on the training split.
I0310 12:20:51.533443 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 12:21:12.603671 139902746892096 spec.py:349] Evaluating on the test split.
I0310 12:21:14.290847 139902746892096 submission_runner.py:411] Time since start: 199197.32s, 	Step: 411379, 	{'train/accuracy': 0.8880273103713989, 'train/loss': 0.41521868109703064, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 184052.06006407738, 'total_duration': 199197.31963682175, 'accumulated_submission_time': 184052.06006407738, 'accumulated_eval_time': 15095.746675729752, 'accumulated_logging_time': 27.959092378616333}
I0310 12:21:14.369637 139708407461632 logging_writer.py:48] [411379] accumulated_eval_time=15095.746676, accumulated_logging_time=27.959092, accumulated_submission_time=184052.060064, global_step=411379, preemption_count=0, score=184052.060064, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=199197.319637, train/accuracy=0.888027, train/loss=0.415219, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 12:21:23.034018 139708415854336 logging_writer.py:48] [411400] global_step=411400, grad_norm=3.236759901046753, loss=2.4268109798431396
I0310 12:22:04.405910 139708407461632 logging_writer.py:48] [411500] global_step=411500, grad_norm=4.360347747802734, loss=3.2706830501556396
I0310 12:22:49.640037 139708415854336 logging_writer.py:48] [411600] global_step=411600, grad_norm=3.2835183143615723, loss=1.3639110326766968
I0310 12:23:35.313455 139708407461632 logging_writer.py:48] [411700] global_step=411700, grad_norm=3.4650635719299316, loss=1.0775519609451294
I0310 12:24:20.591150 139708415854336 logging_writer.py:48] [411800] global_step=411800, grad_norm=3.0845537185668945, loss=1.1620978116989136
I0310 12:25:05.902826 139708407461632 logging_writer.py:48] [411900] global_step=411900, grad_norm=3.2061927318573, loss=1.0718377828598022
I0310 12:25:51.347509 139708415854336 logging_writer.py:48] [412000] global_step=412000, grad_norm=3.101140260696411, loss=1.0143463611602783
I0310 12:26:36.848780 139708407461632 logging_writer.py:48] [412100] global_step=412100, grad_norm=2.9760425090789795, loss=2.52284574508667
I0310 12:27:21.917693 139708415854336 logging_writer.py:48] [412200] global_step=412200, grad_norm=3.122790813446045, loss=1.5864719152450562
I0310 12:28:07.500575 139708407461632 logging_writer.py:48] [412300] global_step=412300, grad_norm=3.213956594467163, loss=1.3576993942260742
I0310 12:28:14.499564 139902746892096 spec.py:321] Evaluating on the training split.
I0310 12:28:26.057744 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 12:28:48.094199 139902746892096 spec.py:349] Evaluating on the test split.
I0310 12:28:49.775964 139902746892096 submission_runner.py:411] Time since start: 199652.80s, 	Step: 412317, 	{'train/accuracy': 0.8885546922683716, 'train/loss': 0.4164588153362274, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 184472.1326699257, 'total_duration': 199652.8047463894, 'accumulated_submission_time': 184472.1326699257, 'accumulated_eval_time': 15131.023107528687, 'accumulated_logging_time': 28.047126054763794}
I0310 12:28:49.864969 139708415854336 logging_writer.py:48] [412317] accumulated_eval_time=15131.023108, accumulated_logging_time=28.047126, accumulated_submission_time=184472.132670, global_step=412317, preemption_count=0, score=184472.132670, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=199652.804746, train/accuracy=0.888555, train/loss=0.416459, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 12:29:23.094443 139708407461632 logging_writer.py:48] [412400] global_step=412400, grad_norm=3.127636194229126, loss=1.1168932914733887
I0310 12:30:07.637590 139708415854336 logging_writer.py:48] [412500] global_step=412500, grad_norm=3.231738567352295, loss=1.1557092666625977
I0310 12:30:52.788090 139708407461632 logging_writer.py:48] [412600] global_step=412600, grad_norm=2.8066048622131348, loss=1.2644667625427246
I0310 12:31:38.441410 139708415854336 logging_writer.py:48] [412700] global_step=412700, grad_norm=2.958195447921753, loss=1.3301297426223755
I0310 12:32:23.562961 139708407461632 logging_writer.py:48] [412800] global_step=412800, grad_norm=3.0678274631500244, loss=2.51678729057312
I0310 12:33:08.904140 139708415854336 logging_writer.py:48] [412900] global_step=412900, grad_norm=3.0108158588409424, loss=1.115287184715271
I0310 12:33:54.380889 139708407461632 logging_writer.py:48] [413000] global_step=413000, grad_norm=3.272447347640991, loss=1.2443417310714722
I0310 12:34:39.478491 139708415854336 logging_writer.py:48] [413100] global_step=413100, grad_norm=2.7872061729431152, loss=2.145280122756958
I0310 12:35:24.835730 139708407461632 logging_writer.py:48] [413200] global_step=413200, grad_norm=3.276346206665039, loss=1.204528570175171
I0310 12:35:49.895267 139902746892096 spec.py:321] Evaluating on the training split.
I0310 12:36:01.278076 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 12:36:22.002225 139902746892096 spec.py:349] Evaluating on the test split.
I0310 12:36:23.690859 139902746892096 submission_runner.py:411] Time since start: 200106.72s, 	Step: 413257, 	{'train/accuracy': 0.8854687213897705, 'train/loss': 0.4227531850337982, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 184892.1034603119, 'total_duration': 200106.71964406967, 'accumulated_submission_time': 184892.1034603119, 'accumulated_eval_time': 15164.818674087524, 'accumulated_logging_time': 28.145170211791992}
I0310 12:36:23.771109 139708415854336 logging_writer.py:48] [413257] accumulated_eval_time=15164.818674, accumulated_logging_time=28.145170, accumulated_submission_time=184892.103460, global_step=413257, preemption_count=0, score=184892.103460, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=200106.719644, train/accuracy=0.885469, train/loss=0.422753, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 12:36:41.089400 139708407461632 logging_writer.py:48] [413300] global_step=413300, grad_norm=3.2903213500976562, loss=1.0298553705215454
I0310 12:37:23.702457 139708415854336 logging_writer.py:48] [413400] global_step=413400, grad_norm=3.0177783966064453, loss=1.107670783996582
I0310 12:38:09.262013 139708407461632 logging_writer.py:48] [413500] global_step=413500, grad_norm=3.189115524291992, loss=1.0455759763717651
I0310 12:38:54.748082 139708415854336 logging_writer.py:48] [413600] global_step=413600, grad_norm=2.812941312789917, loss=1.0343677997589111
I0310 12:39:40.033929 139708407461632 logging_writer.py:48] [413700] global_step=413700, grad_norm=3.03425931930542, loss=1.1318862438201904
I0310 12:40:25.356321 139708415854336 logging_writer.py:48] [413800] global_step=413800, grad_norm=2.910398006439209, loss=1.0703994035720825
I0310 12:41:11.119906 139708407461632 logging_writer.py:48] [413900] global_step=413900, grad_norm=3.1349265575408936, loss=1.6285772323608398
I0310 12:41:56.132782 139708415854336 logging_writer.py:48] [414000] global_step=414000, grad_norm=3.3394997119903564, loss=1.150100827217102
I0310 12:42:41.318765 139708407461632 logging_writer.py:48] [414100] global_step=414100, grad_norm=3.1229593753814697, loss=1.1909444332122803
I0310 12:43:24.106142 139902746892096 spec.py:321] Evaluating on the training split.
I0310 12:43:35.485918 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 12:43:56.418452 139902746892096 spec.py:349] Evaluating on the test split.
I0310 12:43:58.102015 139902746892096 submission_runner.py:411] Time since start: 200561.13s, 	Step: 414195, 	{'train/accuracy': 0.8895312547683716, 'train/loss': 0.41166308522224426, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 185312.38032460213, 'total_duration': 200561.13081026077, 'accumulated_submission_time': 185312.38032460213, 'accumulated_eval_time': 15198.814550161362, 'accumulated_logging_time': 28.235026121139526}
I0310 12:43:58.181772 139708415854336 logging_writer.py:48] [414195] accumulated_eval_time=15198.814550, accumulated_logging_time=28.235026, accumulated_submission_time=185312.380325, global_step=414195, preemption_count=0, score=185312.380325, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=200561.130810, train/accuracy=0.889531, train/loss=0.411663, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 12:44:00.543130 139708407461632 logging_writer.py:48] [414200] global_step=414200, grad_norm=3.2228786945343018, loss=3.005786180496216
I0310 12:44:41.174599 139708415854336 logging_writer.py:48] [414300] global_step=414300, grad_norm=3.1230647563934326, loss=1.139512062072754
I0310 12:45:26.433025 139708407461632 logging_writer.py:48] [414400] global_step=414400, grad_norm=2.9866654872894287, loss=1.0545796155929565
I0310 12:46:12.116853 139708415854336 logging_writer.py:48] [414500] global_step=414500, grad_norm=3.1145689487457275, loss=2.663700580596924
I0310 12:46:57.585280 139708407461632 logging_writer.py:48] [414600] global_step=414600, grad_norm=3.3406660556793213, loss=1.1285053491592407
I0310 12:47:43.206080 139708415854336 logging_writer.py:48] [414700] global_step=414700, grad_norm=2.9725170135498047, loss=1.0889825820922852
I0310 12:48:28.689288 139708407461632 logging_writer.py:48] [414800] global_step=414800, grad_norm=3.139355182647705, loss=1.2529528141021729
I0310 12:49:13.820244 139708415854336 logging_writer.py:48] [414900] global_step=414900, grad_norm=3.3894803524017334, loss=2.5568251609802246
I0310 12:49:59.284352 139708407461632 logging_writer.py:48] [415000] global_step=415000, grad_norm=3.1120176315307617, loss=2.7848217487335205
I0310 12:50:44.502476 139708415854336 logging_writer.py:48] [415100] global_step=415100, grad_norm=3.7925493717193604, loss=3.1683943271636963
I0310 12:50:58.155560 139902746892096 spec.py:321] Evaluating on the training split.
I0310 12:51:09.513097 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 12:51:31.409297 139902746892096 spec.py:349] Evaluating on the test split.
I0310 12:51:33.081905 139902746892096 submission_runner.py:411] Time since start: 201016.11s, 	Step: 415132, 	{'train/accuracy': 0.8891991972923279, 'train/loss': 0.41304099559783936, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 185732.29559206963, 'total_duration': 201016.11069083214, 'accumulated_submission_time': 185732.29559206963, 'accumulated_eval_time': 15233.74088358879, 'accumulated_logging_time': 28.325153589248657}
I0310 12:51:33.168544 139708407461632 logging_writer.py:48] [415132] accumulated_eval_time=15233.740884, accumulated_logging_time=28.325154, accumulated_submission_time=185732.295592, global_step=415132, preemption_count=0, score=185732.295592, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=201016.110691, train/accuracy=0.889199, train/loss=0.413041, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 12:52:00.314922 139708415854336 logging_writer.py:48] [415200] global_step=415200, grad_norm=3.7208452224731445, loss=2.8977794647216797
I0310 12:52:44.179783 139708407461632 logging_writer.py:48] [415300] global_step=415300, grad_norm=3.0816123485565186, loss=1.148555874824524
I0310 12:53:29.642455 139708415854336 logging_writer.py:48] [415400] global_step=415400, grad_norm=2.8952698707580566, loss=1.1156435012817383
I0310 12:54:15.267578 139708407461632 logging_writer.py:48] [415500] global_step=415500, grad_norm=3.2916665077209473, loss=1.198467493057251
I0310 12:55:00.331758 139708415854336 logging_writer.py:48] [415600] global_step=415600, grad_norm=3.4312827587127686, loss=1.082571268081665
I0310 12:55:45.667186 139708407461632 logging_writer.py:48] [415700] global_step=415700, grad_norm=3.049821615219116, loss=1.072762131690979
I0310 12:56:30.584703 139708415854336 logging_writer.py:48] [415800] global_step=415800, grad_norm=3.0711519718170166, loss=2.5278308391571045
I0310 12:57:15.972400 139708407461632 logging_writer.py:48] [415900] global_step=415900, grad_norm=3.2304718494415283, loss=1.1401766538619995
I0310 12:58:01.343171 139708415854336 logging_writer.py:48] [416000] global_step=416000, grad_norm=3.334650754928589, loss=1.502694845199585
I0310 12:58:33.458309 139902746892096 spec.py:321] Evaluating on the training split.
I0310 12:58:44.809437 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 12:59:06.321275 139902746892096 spec.py:349] Evaluating on the test split.
I0310 12:59:07.996510 139902746892096 submission_runner.py:411] Time since start: 201471.03s, 	Step: 416073, 	{'train/accuracy': 0.8877929449081421, 'train/loss': 0.416156530380249, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 186152.5260412693, 'total_duration': 201471.02529382706, 'accumulated_submission_time': 186152.5260412693, 'accumulated_eval_time': 15268.279075622559, 'accumulated_logging_time': 28.421623706817627}
I0310 12:59:08.074108 139708407461632 logging_writer.py:48] [416073] accumulated_eval_time=15268.279076, accumulated_logging_time=28.421624, accumulated_submission_time=186152.526041, global_step=416073, preemption_count=0, score=186152.526041, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=201471.025294, train/accuracy=0.887793, train/loss=0.416157, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 12:59:19.096529 139708415854336 logging_writer.py:48] [416100] global_step=416100, grad_norm=3.3971352577209473, loss=1.1336270570755005
I0310 13:00:00.617050 139708407461632 logging_writer.py:48] [416200] global_step=416200, grad_norm=3.2735230922698975, loss=1.1437411308288574
I0310 13:00:45.432048 139708415854336 logging_writer.py:48] [416300] global_step=416300, grad_norm=2.8189830780029297, loss=1.4232860803604126
I0310 13:01:30.568835 139708407461632 logging_writer.py:48] [416400] global_step=416400, grad_norm=3.0274782180786133, loss=1.1559096574783325
I0310 13:02:15.821375 139708415854336 logging_writer.py:48] [416500] global_step=416500, grad_norm=3.256866931915283, loss=1.16326904296875
I0310 13:03:00.821566 139708407461632 logging_writer.py:48] [416600] global_step=416600, grad_norm=2.9713294506073, loss=1.6035594940185547
I0310 13:03:46.320229 139708415854336 logging_writer.py:48] [416700] global_step=416700, grad_norm=3.3280022144317627, loss=1.0863391160964966
I0310 13:04:31.361685 139708407461632 logging_writer.py:48] [416800] global_step=416800, grad_norm=3.2591145038604736, loss=2.319030523300171
I0310 13:05:16.640688 139708415854336 logging_writer.py:48] [416900] global_step=416900, grad_norm=2.753192186355591, loss=1.6135159730911255
I0310 13:06:01.601660 139708407461632 logging_writer.py:48] [417000] global_step=417000, grad_norm=3.4810783863067627, loss=3.041224241256714
I0310 13:06:08.066566 139902746892096 spec.py:321] Evaluating on the training split.
I0310 13:06:19.551129 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 13:06:41.020543 139902746892096 spec.py:349] Evaluating on the test split.
I0310 13:06:42.695324 139902746892096 submission_runner.py:411] Time since start: 201925.72s, 	Step: 417016, 	{'train/accuracy': 0.8877539038658142, 'train/loss': 0.41653478145599365, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 186572.45982551575, 'total_duration': 201925.7241203785, 'accumulated_submission_time': 186572.45982551575, 'accumulated_eval_time': 15302.907843351364, 'accumulated_logging_time': 28.508391618728638}
I0310 13:06:42.772236 139708415854336 logging_writer.py:48] [417016] accumulated_eval_time=15302.907843, accumulated_logging_time=28.508392, accumulated_submission_time=186572.459826, global_step=417016, preemption_count=0, score=186572.459826, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=201925.724120, train/accuracy=0.887754, train/loss=0.416535, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 13:07:16.486951 139708407461632 logging_writer.py:48] [417100] global_step=417100, grad_norm=3.3583104610443115, loss=3.040904998779297
I0310 13:08:01.671452 139708415854336 logging_writer.py:48] [417200] global_step=417200, grad_norm=2.9062798023223877, loss=1.013998031616211
I0310 13:08:46.702769 139708407461632 logging_writer.py:48] [417300] global_step=417300, grad_norm=2.891303539276123, loss=1.2653005123138428
I0310 13:09:31.951182 139708415854336 logging_writer.py:48] [417400] global_step=417400, grad_norm=3.2837188243865967, loss=1.214368224143982
I0310 13:10:17.124376 139708407461632 logging_writer.py:48] [417500] global_step=417500, grad_norm=2.99194073677063, loss=1.2553900480270386
I0310 13:11:02.365435 139708415854336 logging_writer.py:48] [417600] global_step=417600, grad_norm=3.06826114654541, loss=1.0448940992355347
I0310 13:11:47.839658 139708407461632 logging_writer.py:48] [417700] global_step=417700, grad_norm=3.8073642253875732, loss=3.1283416748046875
I0310 13:12:32.883587 139708415854336 logging_writer.py:48] [417800] global_step=417800, grad_norm=2.954176187515259, loss=1.772324800491333
I0310 13:13:18.501829 139708407461632 logging_writer.py:48] [417900] global_step=417900, grad_norm=3.8788957595825195, loss=3.312197685241699
I0310 13:13:42.727017 139902746892096 spec.py:321] Evaluating on the training split.
I0310 13:13:54.140381 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 13:14:17.244066 139902746892096 spec.py:349] Evaluating on the test split.
I0310 13:14:18.913745 139902746892096 submission_runner.py:411] Time since start: 202381.94s, 	Step: 417955, 	{'train/accuracy': 0.88832026720047, 'train/loss': 0.41332384943962097, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 186992.08824324608, 'total_duration': 202381.94254612923, 'accumulated_submission_time': 186992.08824324608, 'accumulated_eval_time': 15339.094573259354, 'accumulated_logging_time': 28.86305069923401}
I0310 13:14:18.994134 139708415854336 logging_writer.py:48] [417955] accumulated_eval_time=15339.094573, accumulated_logging_time=28.863051, accumulated_submission_time=186992.088243, global_step=417955, preemption_count=0, score=186992.088243, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=202381.942546, train/accuracy=0.888320, train/loss=0.413324, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 13:14:37.102425 139708407461632 logging_writer.py:48] [418000] global_step=418000, grad_norm=2.9272725582122803, loss=1.562826156616211
I0310 13:15:19.522725 139708415854336 logging_writer.py:48] [418100] global_step=418100, grad_norm=3.507146120071411, loss=3.102217435836792
I0310 13:16:04.608047 139708407461632 logging_writer.py:48] [418200] global_step=418200, grad_norm=3.53897762298584, loss=3.1287841796875
I0310 13:16:50.108366 139708415854336 logging_writer.py:48] [418300] global_step=418300, grad_norm=2.957427740097046, loss=1.070624589920044
I0310 13:17:35.429294 139708407461632 logging_writer.py:48] [418400] global_step=418400, grad_norm=3.193091869354248, loss=1.3859941959381104
I0310 13:18:21.070959 139708415854336 logging_writer.py:48] [418500] global_step=418500, grad_norm=3.054886817932129, loss=2.362030267715454
I0310 13:19:06.126391 139708407461632 logging_writer.py:48] [418600] global_step=418600, grad_norm=3.0468544960021973, loss=1.1495537757873535
I0310 13:19:51.209810 139708415854336 logging_writer.py:48] [418700] global_step=418700, grad_norm=3.082200765609741, loss=1.0735247135162354
I0310 13:20:36.630100 139708407461632 logging_writer.py:48] [418800] global_step=418800, grad_norm=3.043954372406006, loss=1.1902744770050049
I0310 13:21:19.022799 139902746892096 spec.py:321] Evaluating on the training split.
I0310 13:21:30.297398 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 13:21:50.971754 139902746892096 spec.py:349] Evaluating on the test split.
I0310 13:21:52.644610 139902746892096 submission_runner.py:411] Time since start: 202835.67s, 	Step: 418895, 	{'train/accuracy': 0.8907421827316284, 'train/loss': 0.412113755941391, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 187412.05957746506, 'total_duration': 202835.67338371277, 'accumulated_submission_time': 187412.05957746506, 'accumulated_eval_time': 15372.716371774673, 'accumulated_logging_time': 28.951866388320923}
I0310 13:21:52.732986 139708415854336 logging_writer.py:48] [418895] accumulated_eval_time=15372.716372, accumulated_logging_time=28.951866, accumulated_submission_time=187412.059577, global_step=418895, preemption_count=0, score=187412.059577, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=202835.673384, train/accuracy=0.890742, train/loss=0.412114, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 13:21:55.094478 139708407461632 logging_writer.py:48] [418900] global_step=418900, grad_norm=3.207301139831543, loss=1.8565213680267334
I0310 13:22:35.314607 139708415854336 logging_writer.py:48] [419000] global_step=419000, grad_norm=2.9027907848358154, loss=1.9398095607757568
I0310 13:23:20.435930 139708407461632 logging_writer.py:48] [419100] global_step=419100, grad_norm=2.8056538105010986, loss=1.4001660346984863
I0310 13:24:06.256756 139708415854336 logging_writer.py:48] [419200] global_step=419200, grad_norm=3.0907440185546875, loss=1.1626070737838745
I0310 13:24:51.482443 139708407461632 logging_writer.py:48] [419300] global_step=419300, grad_norm=3.2345340251922607, loss=1.049936294555664
I0310 13:25:36.658162 139708415854336 logging_writer.py:48] [419400] global_step=419400, grad_norm=3.010685443878174, loss=1.0659711360931396
I0310 13:26:21.933773 139708407461632 logging_writer.py:48] [419500] global_step=419500, grad_norm=3.110389471054077, loss=1.5063080787658691
I0310 13:27:07.080155 139708415854336 logging_writer.py:48] [419600] global_step=419600, grad_norm=3.133012533187866, loss=2.094703197479248
I0310 13:27:52.303029 139708407461632 logging_writer.py:48] [419700] global_step=419700, grad_norm=3.025665521621704, loss=1.1331720352172852
I0310 13:28:37.583278 139708415854336 logging_writer.py:48] [419800] global_step=419800, grad_norm=3.051520824432373, loss=1.7791969776153564
I0310 13:28:52.787423 139902746892096 spec.py:321] Evaluating on the training split.
I0310 13:29:04.191930 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 13:29:26.006112 139902746892096 spec.py:349] Evaluating on the test split.
I0310 13:29:27.687010 139902746892096 submission_runner.py:411] Time since start: 203290.72s, 	Step: 419835, 	{'train/accuracy': 0.8883398175239563, 'train/loss': 0.41080543398857117, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 187832.055000782, 'total_duration': 203290.7157907486, 'accumulated_submission_time': 187832.055000782, 'accumulated_eval_time': 15407.615942239761, 'accumulated_logging_time': 29.050409078598022}
I0310 13:29:27.774885 139708407461632 logging_writer.py:48] [419835] accumulated_eval_time=15407.615942, accumulated_logging_time=29.050409, accumulated_submission_time=187832.055001, global_step=419835, preemption_count=0, score=187832.055001, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=203290.715791, train/accuracy=0.888340, train/loss=0.410805, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 13:29:53.750922 139708415854336 logging_writer.py:48] [419900] global_step=419900, grad_norm=3.323965311050415, loss=1.115432858467102
I0310 13:30:37.609560 139708407461632 logging_writer.py:48] [420000] global_step=420000, grad_norm=3.2937192916870117, loss=1.0824495553970337
I0310 13:31:22.820916 139708415854336 logging_writer.py:48] [420100] global_step=420100, grad_norm=2.996492862701416, loss=1.0829780101776123
I0310 13:32:08.139957 139708407461632 logging_writer.py:48] [420200] global_step=420200, grad_norm=4.128267288208008, loss=3.088994264602661
I0310 13:32:53.353587 139708415854336 logging_writer.py:48] [420300] global_step=420300, grad_norm=3.3798084259033203, loss=1.1065003871917725
I0310 13:33:38.894035 139708407461632 logging_writer.py:48] [420400] global_step=420400, grad_norm=2.9524776935577393, loss=1.1030592918395996
I0310 13:34:24.010537 139708415854336 logging_writer.py:48] [420500] global_step=420500, grad_norm=3.342643976211548, loss=2.7774384021759033
I0310 13:35:09.021391 139708407461632 logging_writer.py:48] [420600] global_step=420600, grad_norm=3.5080506801605225, loss=1.1802793741226196
I0310 13:35:54.335232 139708415854336 logging_writer.py:48] [420700] global_step=420700, grad_norm=3.3633882999420166, loss=1.2705581188201904
I0310 13:36:27.954148 139902746892096 spec.py:321] Evaluating on the training split.
I0310 13:36:39.457786 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 13:37:01.134590 139902746892096 spec.py:349] Evaluating on the test split.
I0310 13:37:02.809134 139902746892096 submission_runner.py:411] Time since start: 203745.84s, 	Step: 420776, 	{'train/accuracy': 0.8899804353713989, 'train/loss': 0.4098820686340332, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 188252.17577075958, 'total_duration': 203745.83793091774, 'accumulated_submission_time': 188252.17577075958, 'accumulated_eval_time': 15442.47093296051, 'accumulated_logging_time': 29.147966384887695}
I0310 13:37:02.888630 139708407461632 logging_writer.py:48] [420776] accumulated_eval_time=15442.470933, accumulated_logging_time=29.147966, accumulated_submission_time=188252.175771, global_step=420776, preemption_count=0, score=188252.175771, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=203745.837931, train/accuracy=0.889980, train/loss=0.409882, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 13:37:12.730451 139708415854336 logging_writer.py:48] [420800] global_step=420800, grad_norm=3.1356565952301025, loss=1.6964904069900513
I0310 13:37:54.512093 139708407461632 logging_writer.py:48] [420900] global_step=420900, grad_norm=3.2694709300994873, loss=1.2130903005599976
I0310 13:38:39.905473 139708415854336 logging_writer.py:48] [421000] global_step=421000, grad_norm=4.053950786590576, loss=3.2065930366516113
I0310 13:39:25.454304 139708407461632 logging_writer.py:48] [421100] global_step=421100, grad_norm=2.831995964050293, loss=1.9497802257537842
I0310 13:40:10.611693 139708415854336 logging_writer.py:48] [421200] global_step=421200, grad_norm=3.215139865875244, loss=2.151956558227539
I0310 13:40:55.873636 139708407461632 logging_writer.py:48] [421300] global_step=421300, grad_norm=3.1512813568115234, loss=1.170913815498352
I0310 13:41:41.047734 139708415854336 logging_writer.py:48] [421400] global_step=421400, grad_norm=3.055555582046509, loss=1.7880717515945435
I0310 13:42:26.203412 139708407461632 logging_writer.py:48] [421500] global_step=421500, grad_norm=3.1249032020568848, loss=2.463299036026001
I0310 13:43:11.274813 139708415854336 logging_writer.py:48] [421600] global_step=421600, grad_norm=3.2180254459381104, loss=2.7391865253448486
I0310 13:43:57.196765 139708407461632 logging_writer.py:48] [421700] global_step=421700, grad_norm=3.0806632041931152, loss=1.6916954517364502
I0310 13:44:03.184739 139902746892096 spec.py:321] Evaluating on the training split.
I0310 13:44:15.289375 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 13:44:37.228313 139902746892096 spec.py:349] Evaluating on the test split.
I0310 13:44:38.896503 139902746892096 submission_runner.py:411] Time since start: 204201.93s, 	Step: 421715, 	{'train/accuracy': 0.8877539038658142, 'train/loss': 0.41214266419410706, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 188672.41037154198, 'total_duration': 204201.92528772354, 'accumulated_submission_time': 188672.41037154198, 'accumulated_eval_time': 15478.182670593262, 'accumulated_logging_time': 29.23967170715332}
I0310 13:44:38.975994 139708415854336 logging_writer.py:48] [421715] accumulated_eval_time=15478.182671, accumulated_logging_time=29.239672, accumulated_submission_time=188672.410372, global_step=421715, preemption_count=0, score=188672.410372, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=204201.925288, train/accuracy=0.887754, train/loss=0.412143, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 13:45:12.830905 139708407461632 logging_writer.py:48] [421800] global_step=421800, grad_norm=3.1764509677886963, loss=1.1205425262451172
I0310 13:45:57.234077 139708415854336 logging_writer.py:48] [421900] global_step=421900, grad_norm=3.424755334854126, loss=2.61279034614563
I0310 13:46:42.960506 139708407461632 logging_writer.py:48] [422000] global_step=422000, grad_norm=3.0999066829681396, loss=1.1031200885772705
I0310 13:47:28.427166 139708415854336 logging_writer.py:48] [422100] global_step=422100, grad_norm=3.659566879272461, loss=3.2269644737243652
I0310 13:48:13.933933 139708407461632 logging_writer.py:48] [422200] global_step=422200, grad_norm=3.1152303218841553, loss=1.4053441286087036
I0310 13:48:59.514423 139708415854336 logging_writer.py:48] [422300] global_step=422300, grad_norm=2.865996837615967, loss=1.0788589715957642
I0310 13:49:44.840335 139708407461632 logging_writer.py:48] [422400] global_step=422400, grad_norm=3.1917946338653564, loss=1.1356830596923828
I0310 13:50:30.273503 139708415854336 logging_writer.py:48] [422500] global_step=422500, grad_norm=3.0178143978118896, loss=1.0214653015136719
I0310 13:51:15.559247 139708407461632 logging_writer.py:48] [422600] global_step=422600, grad_norm=2.9586386680603027, loss=1.5761942863464355
I0310 13:51:39.296688 139902746892096 spec.py:321] Evaluating on the training split.
I0310 13:51:50.649904 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 13:52:14.409987 139902746892096 spec.py:349] Evaluating on the test split.
I0310 13:52:16.084556 139902746892096 submission_runner.py:411] Time since start: 204659.11s, 	Step: 422654, 	{'train/accuracy': 0.8885155916213989, 'train/loss': 0.41493117809295654, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 189092.67406320572, 'total_duration': 204659.1133544445, 'accumulated_submission_time': 189092.67406320572, 'accumulated_eval_time': 15514.970543146133, 'accumulated_logging_time': 29.327571868896484}
I0310 13:52:16.161693 139708415854336 logging_writer.py:48] [422654] accumulated_eval_time=15514.970543, accumulated_logging_time=29.327572, accumulated_submission_time=189092.674063, global_step=422654, preemption_count=0, score=189092.674063, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=204659.113354, train/accuracy=0.888516, train/loss=0.414931, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 13:52:34.668216 139708407461632 logging_writer.py:48] [422700] global_step=422700, grad_norm=3.0549371242523193, loss=1.1337401866912842
I0310 13:53:17.371676 139708415854336 logging_writer.py:48] [422800] global_step=422800, grad_norm=3.219641923904419, loss=1.1618800163269043
I0310 13:54:02.780261 139708407461632 logging_writer.py:48] [422900] global_step=422900, grad_norm=3.1468729972839355, loss=2.026402711868286
I0310 13:54:48.147338 139708415854336 logging_writer.py:48] [423000] global_step=423000, grad_norm=3.053020477294922, loss=1.137281894683838
I0310 13:55:33.403288 139708407461632 logging_writer.py:48] [423100] global_step=423100, grad_norm=3.216942310333252, loss=1.0876471996307373
I0310 13:56:18.695095 139708415854336 logging_writer.py:48] [423200] global_step=423200, grad_norm=3.1595962047576904, loss=2.0876269340515137
I0310 13:57:03.932075 139708407461632 logging_writer.py:48] [423300] global_step=423300, grad_norm=3.306267023086548, loss=1.1522570848464966
I0310 13:57:49.454379 139708415854336 logging_writer.py:48] [423400] global_step=423400, grad_norm=3.658585786819458, loss=1.1639275550842285
I0310 13:58:34.989719 139708407461632 logging_writer.py:48] [423500] global_step=423500, grad_norm=3.146658182144165, loss=1.2662310600280762
I0310 13:59:16.559499 139902746892096 spec.py:321] Evaluating on the training split.
I0310 13:59:27.958991 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 13:59:49.409769 139902746892096 spec.py:349] Evaluating on the test split.
I0310 13:59:51.078016 139902746892096 submission_runner.py:411] Time since start: 205114.11s, 	Step: 423593, 	{'train/accuracy': 0.88671875, 'train/loss': 0.41452547907829285, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 189513.01320648193, 'total_duration': 205114.10681796074, 'accumulated_submission_time': 189513.01320648193, 'accumulated_eval_time': 15549.48907828331, 'accumulated_logging_time': 29.4142906665802}
I0310 13:59:51.156277 139708415854336 logging_writer.py:48] [423593] accumulated_eval_time=15549.489078, accumulated_logging_time=29.414291, accumulated_submission_time=189513.013206, global_step=423593, preemption_count=0, score=189513.013206, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=205114.106818, train/accuracy=0.886719, train/loss=0.414525, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 13:59:54.307782 139708407461632 logging_writer.py:48] [423600] global_step=423600, grad_norm=3.3890395164489746, loss=2.7199013233184814
I0310 14:00:34.432391 139708415854336 logging_writer.py:48] [423700] global_step=423700, grad_norm=3.338122844696045, loss=2.3486595153808594
I0310 14:01:19.399003 139708407461632 logging_writer.py:48] [423800] global_step=423800, grad_norm=4.166135311126709, loss=3.2273316383361816
I0310 14:02:04.735923 139708415854336 logging_writer.py:48] [423900] global_step=423900, grad_norm=3.0966429710388184, loss=1.2783952951431274
I0310 14:02:50.289436 139708407461632 logging_writer.py:48] [424000] global_step=424000, grad_norm=3.5263211727142334, loss=2.122337818145752
I0310 14:03:35.450006 139708415854336 logging_writer.py:48] [424100] global_step=424100, grad_norm=2.9853858947753906, loss=1.3917979001998901
I0310 14:04:20.971205 139708407461632 logging_writer.py:48] [424200] global_step=424200, grad_norm=3.22660493850708, loss=1.0818833112716675
I0310 14:05:06.142123 139708415854336 logging_writer.py:48] [424300] global_step=424300, grad_norm=3.1225645542144775, loss=1.1434636116027832
I0310 14:05:51.117681 139708407461632 logging_writer.py:48] [424400] global_step=424400, grad_norm=3.206097364425659, loss=1.0723681449890137
I0310 14:06:36.415972 139708415854336 logging_writer.py:48] [424500] global_step=424500, grad_norm=2.908212423324585, loss=1.6557066440582275
I0310 14:06:51.397595 139902746892096 spec.py:321] Evaluating on the training split.
I0310 14:07:02.913997 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 14:07:24.697065 139902746892096 spec.py:349] Evaluating on the test split.
I0310 14:07:26.372170 139902746892096 submission_runner.py:411] Time since start: 205569.40s, 	Step: 424535, 	{'train/accuracy': 0.8887695074081421, 'train/loss': 0.4101097881793976, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 189933.19627666473, 'total_duration': 205569.40095114708, 'accumulated_submission_time': 189933.19627666473, 'accumulated_eval_time': 15584.463649272919, 'accumulated_logging_time': 29.50173306465149}
I0310 14:07:26.463458 139708407461632 logging_writer.py:48] [424535] accumulated_eval_time=15584.463649, accumulated_logging_time=29.501733, accumulated_submission_time=189933.196277, global_step=424535, preemption_count=0, score=189933.196277, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=205569.400951, train/accuracy=0.888770, train/loss=0.410110, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 14:07:52.425640 139708415854336 logging_writer.py:48] [424600] global_step=424600, grad_norm=3.1073131561279297, loss=1.7979404926300049
I0310 14:08:36.627203 139708407461632 logging_writer.py:48] [424700] global_step=424700, grad_norm=3.423081398010254, loss=3.004514694213867
I0310 14:09:22.176316 139708415854336 logging_writer.py:48] [424800] global_step=424800, grad_norm=3.3639097213745117, loss=1.1565876007080078
I0310 14:10:07.778049 139708407461632 logging_writer.py:48] [424900] global_step=424900, grad_norm=3.028108835220337, loss=1.155220627784729
I0310 14:10:52.987388 139708415854336 logging_writer.py:48] [425000] global_step=425000, grad_norm=3.192298412322998, loss=2.157419443130493
I0310 14:11:38.101121 139708407461632 logging_writer.py:48] [425100] global_step=425100, grad_norm=2.9106712341308594, loss=1.1205523014068604
I0310 14:12:23.341749 139708415854336 logging_writer.py:48] [425200] global_step=425200, grad_norm=3.2057998180389404, loss=1.2132251262664795
I0310 14:13:08.390498 139708407461632 logging_writer.py:48] [425300] global_step=425300, grad_norm=3.064363479614258, loss=2.6662206649780273
I0310 14:13:53.910722 139708415854336 logging_writer.py:48] [425400] global_step=425400, grad_norm=3.194960832595825, loss=1.0495749711990356
I0310 14:14:26.380813 139902746892096 spec.py:321] Evaluating on the training split.
I0310 14:14:37.876896 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 14:15:01.420934 139902746892096 spec.py:349] Evaluating on the test split.
I0310 14:15:03.093491 139902746892096 submission_runner.py:411] Time since start: 206026.12s, 	Step: 425473, 	{'train/accuracy': 0.8880859017372131, 'train/loss': 0.4189074635505676, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 190353.0560581684, 'total_duration': 206026.12229013443, 'accumulated_submission_time': 190353.0560581684, 'accumulated_eval_time': 15621.176329135895, 'accumulated_logging_time': 29.602285385131836}
I0310 14:15:03.172635 139708407461632 logging_writer.py:48] [425473] accumulated_eval_time=15621.176329, accumulated_logging_time=29.602285, accumulated_submission_time=190353.056058, global_step=425473, preemption_count=0, score=190353.056058, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=206026.122290, train/accuracy=0.888086, train/loss=0.418907, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 14:15:14.192494 139708415854336 logging_writer.py:48] [425500] global_step=425500, grad_norm=3.5455281734466553, loss=3.179119825363159
I0310 14:15:55.912798 139708407461632 logging_writer.py:48] [425600] global_step=425600, grad_norm=3.101087808609009, loss=2.3857622146606445
I0310 14:16:41.343707 139708415854336 logging_writer.py:48] [425700] global_step=425700, grad_norm=3.113281488418579, loss=2.046415328979492
I0310 14:17:27.257244 139708407461632 logging_writer.py:48] [425800] global_step=425800, grad_norm=3.3484089374542236, loss=2.809704303741455
I0310 14:18:13.054708 139708415854336 logging_writer.py:48] [425900] global_step=425900, grad_norm=3.0189061164855957, loss=2.0421218872070312
I0310 14:18:58.547429 139708407461632 logging_writer.py:48] [426000] global_step=426000, grad_norm=3.3443477153778076, loss=2.903146266937256
I0310 14:19:44.274283 139708415854336 logging_writer.py:48] [426100] global_step=426100, grad_norm=4.308341026306152, loss=3.286571979522705
I0310 14:20:29.797286 139708407461632 logging_writer.py:48] [426200] global_step=426200, grad_norm=4.252791881561279, loss=3.3050551414489746
I0310 14:21:15.264001 139708415854336 logging_writer.py:48] [426300] global_step=426300, grad_norm=3.224061965942383, loss=1.9822642803192139
I0310 14:22:00.524905 139708407461632 logging_writer.py:48] [426400] global_step=426400, grad_norm=3.1586248874664307, loss=1.0568792819976807
I0310 14:22:03.356725 139902746892096 spec.py:321] Evaluating on the training split.
I0310 14:22:14.709699 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 14:22:37.343122 139902746892096 spec.py:349] Evaluating on the test split.
I0310 14:22:39.007007 139902746892096 submission_runner.py:411] Time since start: 206482.04s, 	Step: 426408, 	{'train/accuracy': 0.8875585794448853, 'train/loss': 0.41545385122299194, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 190773.18273472786, 'total_duration': 206482.0358054638, 'accumulated_submission_time': 190773.18273472786, 'accumulated_eval_time': 15656.826608181, 'accumulated_logging_time': 29.69010305404663}
I0310 14:22:39.091949 139708415854336 logging_writer.py:48] [426408] accumulated_eval_time=15656.826608, accumulated_logging_time=29.690103, accumulated_submission_time=190773.182735, global_step=426408, preemption_count=0, score=190773.182735, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=206482.035805, train/accuracy=0.887559, train/loss=0.415454, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 14:23:15.872993 139708407461632 logging_writer.py:48] [426500] global_step=426500, grad_norm=3.3188512325286865, loss=2.9053831100463867
I0310 14:24:01.077260 139708415854336 logging_writer.py:48] [426600] global_step=426600, grad_norm=3.3791539669036865, loss=1.3077061176300049
I0310 14:24:46.659709 139708407461632 logging_writer.py:48] [426700] global_step=426700, grad_norm=4.259946346282959, loss=3.228367567062378
I0310 14:25:31.966262 139708415854336 logging_writer.py:48] [426800] global_step=426800, grad_norm=3.266864776611328, loss=1.5147587060928345
I0310 14:26:17.129964 139708407461632 logging_writer.py:48] [426900] global_step=426900, grad_norm=3.436039686203003, loss=3.0774853229522705
I0310 14:27:02.512166 139708415854336 logging_writer.py:48] [427000] global_step=427000, grad_norm=3.189397096633911, loss=1.148956060409546
I0310 14:27:47.904855 139708407461632 logging_writer.py:48] [427100] global_step=427100, grad_norm=3.8366475105285645, loss=3.312112331390381
I0310 14:28:33.031318 139708415854336 logging_writer.py:48] [427200] global_step=427200, grad_norm=3.188523769378662, loss=1.279537558555603
I0310 14:29:18.359107 139708407461632 logging_writer.py:48] [427300] global_step=427300, grad_norm=3.146547555923462, loss=1.1924965381622314
I0310 14:29:39.049824 139902746892096 spec.py:321] Evaluating on the training split.
I0310 14:29:50.336504 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 14:30:12.246307 139902746892096 spec.py:349] Evaluating on the test split.
I0310 14:30:13.926544 139902746892096 submission_runner.py:411] Time since start: 206936.96s, 	Step: 427347, 	{'train/accuracy': 0.8865429759025574, 'train/loss': 0.41640082001686096, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 191193.0832927227, 'total_duration': 206936.95532655716, 'accumulated_submission_time': 191193.0832927227, 'accumulated_eval_time': 15691.703318595886, 'accumulated_logging_time': 29.783590078353882}
I0310 14:30:14.016636 139708415854336 logging_writer.py:48] [427347] accumulated_eval_time=15691.703319, accumulated_logging_time=29.783590, accumulated_submission_time=191193.083293, global_step=427347, preemption_count=0, score=191193.083293, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=206936.955327, train/accuracy=0.886543, train/loss=0.416401, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 14:30:35.283021 139708407461632 logging_writer.py:48] [427400] global_step=427400, grad_norm=3.4915428161621094, loss=3.2396624088287354
I0310 14:31:18.381978 139708415854336 logging_writer.py:48] [427500] global_step=427500, grad_norm=3.4372897148132324, loss=2.86978816986084
I0310 14:32:03.620187 139708407461632 logging_writer.py:48] [427600] global_step=427600, grad_norm=3.0888607501983643, loss=1.0999737977981567
I0310 14:32:49.515396 139708415854336 logging_writer.py:48] [427700] global_step=427700, grad_norm=3.131924867630005, loss=1.7412484884262085
I0310 14:33:34.847984 139708407461632 logging_writer.py:48] [427800] global_step=427800, grad_norm=3.631540298461914, loss=3.1620349884033203
I0310 14:34:20.559521 139708415854336 logging_writer.py:48] [427900] global_step=427900, grad_norm=3.138343095779419, loss=1.1735880374908447
I0310 14:35:06.084280 139708407461632 logging_writer.py:48] [428000] global_step=428000, grad_norm=3.118856906890869, loss=2.6957900524139404
I0310 14:35:51.243694 139708415854336 logging_writer.py:48] [428100] global_step=428100, grad_norm=3.118281841278076, loss=1.2957062721252441
I0310 14:36:36.827161 139708407461632 logging_writer.py:48] [428200] global_step=428200, grad_norm=2.9245734214782715, loss=1.1447489261627197
I0310 14:37:14.227552 139902746892096 spec.py:321] Evaluating on the training split.
I0310 14:37:25.840935 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 14:37:48.332480 139902746892096 spec.py:349] Evaluating on the test split.
I0310 14:37:50.013821 139902746892096 submission_runner.py:411] Time since start: 207393.04s, 	Step: 428284, 	{'train/accuracy': 0.8866796493530273, 'train/loss': 0.4184194803237915, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 191613.23265123367, 'total_duration': 207393.04261374474, 'accumulated_submission_time': 191613.23265123367, 'accumulated_eval_time': 15727.489582061768, 'accumulated_logging_time': 29.886178016662598}
I0310 14:37:50.095043 139708415854336 logging_writer.py:48] [428284] accumulated_eval_time=15727.489582, accumulated_logging_time=29.886178, accumulated_submission_time=191613.232651, global_step=428284, preemption_count=0, score=191613.232651, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=207393.042614, train/accuracy=0.886680, train/loss=0.418419, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 14:37:56.779441 139708407461632 logging_writer.py:48] [428300] global_step=428300, grad_norm=2.7906622886657715, loss=1.9998466968536377
I0310 14:38:37.731485 139708415854336 logging_writer.py:48] [428400] global_step=428400, grad_norm=3.323742628097534, loss=2.5751724243164062
I0310 14:39:22.995788 139708407461632 logging_writer.py:48] [428500] global_step=428500, grad_norm=3.29685378074646, loss=1.1055312156677246
I0310 14:40:08.436276 139708415854336 logging_writer.py:48] [428600] global_step=428600, grad_norm=3.181385040283203, loss=1.2130706310272217
I0310 14:40:53.649035 139708407461632 logging_writer.py:48] [428700] global_step=428700, grad_norm=3.028475761413574, loss=1.069735050201416
I0310 14:41:39.032876 139708415854336 logging_writer.py:48] [428800] global_step=428800, grad_norm=2.8271172046661377, loss=1.5846151113510132
I0310 14:42:24.611287 139708407461632 logging_writer.py:48] [428900] global_step=428900, grad_norm=5.662742614746094, loss=3.1570751667022705
I0310 14:43:09.824951 139708415854336 logging_writer.py:48] [429000] global_step=429000, grad_norm=3.863624334335327, loss=1.2281548976898193
I0310 14:43:54.955052 139708407461632 logging_writer.py:48] [429100] global_step=429100, grad_norm=3.6515793800354004, loss=3.2006564140319824
I0310 14:44:40.745223 139708415854336 logging_writer.py:48] [429200] global_step=429200, grad_norm=3.1712160110473633, loss=1.644622802734375
I0310 14:44:50.373650 139902746892096 spec.py:321] Evaluating on the training split.
I0310 14:45:01.786744 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 14:45:23.050876 139902746892096 spec.py:349] Evaluating on the test split.
I0310 14:45:24.737149 139902746892096 submission_runner.py:411] Time since start: 207847.77s, 	Step: 429223, 	{'train/accuracy': 0.8891991972923279, 'train/loss': 0.411479115486145, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 192033.45323348045, 'total_duration': 207847.76594495773, 'accumulated_submission_time': 192033.45323348045, 'accumulated_eval_time': 15761.853075504303, 'accumulated_logging_time': 29.976338148117065}
I0310 14:45:24.815999 139708407461632 logging_writer.py:48] [429223] accumulated_eval_time=15761.853076, accumulated_logging_time=29.976338, accumulated_submission_time=192033.453233, global_step=429223, preemption_count=0, score=192033.453233, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=207847.765945, train/accuracy=0.889199, train/loss=0.411479, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 14:45:55.514992 139708415854336 logging_writer.py:48] [429300] global_step=429300, grad_norm=3.0487945079803467, loss=1.1888682842254639
I0310 14:46:40.268619 139708407461632 logging_writer.py:48] [429400] global_step=429400, grad_norm=3.308574914932251, loss=1.1445506811141968
I0310 14:47:25.895041 139708415854336 logging_writer.py:48] [429500] global_step=429500, grad_norm=2.9840962886810303, loss=1.359371304512024
I0310 14:48:11.703425 139708407461632 logging_writer.py:48] [429600] global_step=429600, grad_norm=3.143935441970825, loss=1.6092660427093506
I0310 14:48:56.884952 139708415854336 logging_writer.py:48] [429700] global_step=429700, grad_norm=3.016688585281372, loss=1.0548404455184937
I0310 14:49:42.289754 139708407461632 logging_writer.py:48] [429800] global_step=429800, grad_norm=3.7526538372039795, loss=1.1871860027313232
I0310 14:50:27.386567 139708415854336 logging_writer.py:48] [429900] global_step=429900, grad_norm=3.298513650894165, loss=1.0692849159240723
I0310 14:51:12.926870 139708407461632 logging_writer.py:48] [430000] global_step=430000, grad_norm=3.0468544960021973, loss=1.3569144010543823
I0310 14:51:58.254680 139708415854336 logging_writer.py:48] [430100] global_step=430100, grad_norm=3.0181617736816406, loss=1.8406319618225098
I0310 14:52:24.800263 139902746892096 spec.py:321] Evaluating on the training split.
I0310 14:52:36.177211 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 14:52:58.243374 139902746892096 spec.py:349] Evaluating on the test split.
I0310 14:52:59.913453 139902746892096 submission_runner.py:411] Time since start: 208302.94s, 	Step: 430160, 	{'train/accuracy': 0.8868749737739563, 'train/loss': 0.4184442162513733, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 192453.38086915016, 'total_duration': 208302.94225239754, 'accumulated_submission_time': 192453.38086915016, 'accumulated_eval_time': 15796.966268777847, 'accumulated_logging_time': 30.063382387161255}
I0310 14:52:59.993162 139708407461632 logging_writer.py:48] [430160] accumulated_eval_time=15796.966269, accumulated_logging_time=30.063382, accumulated_submission_time=192453.380869, global_step=430160, preemption_count=0, score=192453.380869, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=208302.942252, train/accuracy=0.886875, train/loss=0.418444, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 14:53:16.133919 139708415854336 logging_writer.py:48] [430200] global_step=430200, grad_norm=3.136671543121338, loss=1.2817821502685547
I0310 14:53:58.395116 139708407461632 logging_writer.py:48] [430300] global_step=430300, grad_norm=3.2533376216888428, loss=1.1454622745513916
I0310 14:54:44.056387 139708415854336 logging_writer.py:48] [430400] global_step=430400, grad_norm=3.0348124504089355, loss=1.8643475770950317
I0310 14:55:29.912669 139708407461632 logging_writer.py:48] [430500] global_step=430500, grad_norm=3.5725438594818115, loss=3.176806688308716
I0310 14:56:15.273117 139708415854336 logging_writer.py:48] [430600] global_step=430600, grad_norm=2.8233895301818848, loss=1.9627883434295654
I0310 14:57:00.559086 139708407461632 logging_writer.py:48] [430700] global_step=430700, grad_norm=3.7164433002471924, loss=3.111232280731201
I0310 14:57:45.988365 139708415854336 logging_writer.py:48] [430800] global_step=430800, grad_norm=3.5689492225646973, loss=1.3782800436019897
I0310 14:58:31.067549 139708407461632 logging_writer.py:48] [430900] global_step=430900, grad_norm=3.210742950439453, loss=1.1582690477371216
I0310 14:59:16.578283 139708415854336 logging_writer.py:48] [431000] global_step=431000, grad_norm=3.3171193599700928, loss=2.792492151260376
I0310 14:59:59.991669 139902746892096 spec.py:321] Evaluating on the training split.
I0310 15:00:11.203906 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 15:00:32.724854 139902746892096 spec.py:349] Evaluating on the test split.
I0310 15:00:34.401950 139902746892096 submission_runner.py:411] Time since start: 208757.43s, 	Step: 431097, 	{'train/accuracy': 0.8900390267372131, 'train/loss': 0.4117555618286133, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 192873.32185602188, 'total_duration': 208757.43074917793, 'accumulated_submission_time': 192873.32185602188, 'accumulated_eval_time': 15831.376549959183, 'accumulated_logging_time': 30.151485204696655}
I0310 15:00:34.482635 139708407461632 logging_writer.py:48] [431097] accumulated_eval_time=15831.376550, accumulated_logging_time=30.151485, accumulated_submission_time=192873.321856, global_step=431097, preemption_count=0, score=192873.321856, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=208757.430749, train/accuracy=0.890039, train/loss=0.411756, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 15:00:36.064878 139708415854336 logging_writer.py:48] [431100] global_step=431100, grad_norm=3.043713331222534, loss=1.1650577783584595
I0310 15:01:16.271629 139708407461632 logging_writer.py:48] [431200] global_step=431200, grad_norm=3.0476536750793457, loss=2.366323947906494
I0310 15:02:01.516271 139708415854336 logging_writer.py:48] [431300] global_step=431300, grad_norm=3.3874287605285645, loss=1.1633989810943604
I0310 15:02:46.969025 139708407461632 logging_writer.py:48] [431400] global_step=431400, grad_norm=2.988401174545288, loss=1.2610926628112793
I0310 15:03:32.593981 139708415854336 logging_writer.py:48] [431500] global_step=431500, grad_norm=3.0145866870880127, loss=1.3586528301239014
I0310 15:04:17.470558 139708407461632 logging_writer.py:48] [431600] global_step=431600, grad_norm=3.117406129837036, loss=1.0800583362579346
I0310 15:05:02.995501 139708415854336 logging_writer.py:48] [431700] global_step=431700, grad_norm=3.053321123123169, loss=2.056675910949707
I0310 15:05:48.387594 139708407461632 logging_writer.py:48] [431800] global_step=431800, grad_norm=3.2517101764678955, loss=2.569643497467041
I0310 15:06:33.664929 139708415854336 logging_writer.py:48] [431900] global_step=431900, grad_norm=3.645535707473755, loss=2.7958192825317383
I0310 15:07:19.077695 139708407461632 logging_writer.py:48] [432000] global_step=432000, grad_norm=2.9039220809936523, loss=1.1752796173095703
I0310 15:07:34.761147 139902746892096 spec.py:321] Evaluating on the training split.
I0310 15:07:46.155939 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 15:08:06.096274 139902746892096 spec.py:349] Evaluating on the test split.
I0310 15:08:07.772438 139902746892096 submission_runner.py:411] Time since start: 209210.80s, 	Step: 432036, 	{'train/accuracy': 0.8875781297683716, 'train/loss': 0.41674911975860596, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 193293.54080319405, 'total_duration': 209210.80121159554, 'accumulated_submission_time': 193293.54080319405, 'accumulated_eval_time': 15864.387818336487, 'accumulated_logging_time': 30.242802619934082}
I0310 15:08:07.870136 139708415854336 logging_writer.py:48] [432036] accumulated_eval_time=15864.387818, accumulated_logging_time=30.242803, accumulated_submission_time=193293.540803, global_step=432036, preemption_count=0, score=193293.540803, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=209210.801212, train/accuracy=0.887578, train/loss=0.416749, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 15:08:33.474223 139708407461632 logging_writer.py:48] [432100] global_step=432100, grad_norm=3.361741781234741, loss=2.2748470306396484
I0310 15:09:17.571130 139708415854336 logging_writer.py:48] [432200] global_step=432200, grad_norm=3.3326306343078613, loss=1.0769325494766235
I0310 15:10:02.841881 139708407461632 logging_writer.py:48] [432300] global_step=432300, grad_norm=2.826695680618286, loss=1.9439682960510254
I0310 15:10:48.089719 139708415854336 logging_writer.py:48] [432400] global_step=432400, grad_norm=3.32888126373291, loss=1.1970959901809692
I0310 15:11:33.164545 139708407461632 logging_writer.py:48] [432500] global_step=432500, grad_norm=3.1084988117218018, loss=2.3028790950775146
I0310 15:12:18.478299 139708415854336 logging_writer.py:48] [432600] global_step=432600, grad_norm=3.825160503387451, loss=3.2852160930633545
I0310 15:13:03.934771 139708407461632 logging_writer.py:48] [432700] global_step=432700, grad_norm=3.1929643154144287, loss=1.5169990062713623
I0310 15:13:49.173550 139708415854336 logging_writer.py:48] [432800] global_step=432800, grad_norm=3.0118942260742188, loss=1.43653404712677
I0310 15:14:34.638090 139708407461632 logging_writer.py:48] [432900] global_step=432900, grad_norm=3.0669398307800293, loss=1.1259928941726685
I0310 15:15:07.962506 139902746892096 spec.py:321] Evaluating on the training split.
I0310 15:15:19.337354 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 15:15:39.562040 139902746892096 spec.py:349] Evaluating on the test split.
I0310 15:15:41.241317 139902746892096 submission_runner.py:411] Time since start: 209664.27s, 	Step: 432975, 	{'train/accuracy': 0.88880854845047, 'train/loss': 0.4180719554424286, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 193713.57505464554, 'total_duration': 209664.27010130882, 'accumulated_submission_time': 193713.57505464554, 'accumulated_eval_time': 15897.666623830795, 'accumulated_logging_time': 30.3501980304718}
I0310 15:15:41.334214 139708415854336 logging_writer.py:48] [432975] accumulated_eval_time=15897.666624, accumulated_logging_time=30.350198, accumulated_submission_time=193713.575055, global_step=432975, preemption_count=0, score=193713.575055, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=209664.270101, train/accuracy=0.888809, train/loss=0.418072, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 15:15:51.578940 139708407461632 logging_writer.py:48] [433000] global_step=433000, grad_norm=3.2858026027679443, loss=2.8542490005493164
I0310 15:16:32.871634 139708415854336 logging_writer.py:48] [433100] global_step=433100, grad_norm=3.768481492996216, loss=3.2569377422332764
I0310 15:17:18.119683 139708407461632 logging_writer.py:48] [433200] global_step=433200, grad_norm=2.869486093521118, loss=1.313313603401184
I0310 15:18:03.818053 139708415854336 logging_writer.py:48] [433300] global_step=433300, grad_norm=3.1901514530181885, loss=1.6275537014007568
I0310 15:18:49.241017 139708407461632 logging_writer.py:48] [433400] global_step=433400, grad_norm=3.0521976947784424, loss=1.0786254405975342
I0310 15:19:34.664798 139708415854336 logging_writer.py:48] [433500] global_step=433500, grad_norm=3.9931576251983643, loss=2.9176297187805176
I0310 15:20:20.070420 139708407461632 logging_writer.py:48] [433600] global_step=433600, grad_norm=3.168853998184204, loss=1.1775497198104858
I0310 15:21:05.178373 139708415854336 logging_writer.py:48] [433700] global_step=433700, grad_norm=3.2908990383148193, loss=2.748401641845703
I0310 15:21:50.657311 139708407461632 logging_writer.py:48] [433800] global_step=433800, grad_norm=3.4079954624176025, loss=1.0631227493286133
I0310 15:22:36.239785 139708415854336 logging_writer.py:48] [433900] global_step=433900, grad_norm=3.046222448348999, loss=2.5132923126220703
I0310 15:22:41.383780 139902746892096 spec.py:321] Evaluating on the training split.
I0310 15:22:52.704808 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 15:23:14.820229 139902746892096 spec.py:349] Evaluating on the test split.
I0310 15:23:16.486669 139902746892096 submission_runner.py:411] Time since start: 210119.52s, 	Step: 433913, 	{'train/accuracy': 0.8876367211341858, 'train/loss': 0.4186112880706787, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 194133.56434631348, 'total_duration': 210119.51546907425, 'accumulated_submission_time': 194133.56434631348, 'accumulated_eval_time': 15932.76951098442, 'accumulated_logging_time': 30.454039573669434}
I0310 15:23:16.567113 139708407461632 logging_writer.py:48] [433913] accumulated_eval_time=15932.769511, accumulated_logging_time=30.454040, accumulated_submission_time=194133.564346, global_step=433913, preemption_count=0, score=194133.564346, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=210119.515469, train/accuracy=0.887637, train/loss=0.418611, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 15:23:51.365865 139708415854336 logging_writer.py:48] [434000] global_step=434000, grad_norm=3.1935434341430664, loss=2.257290840148926
I0310 15:24:35.832404 139708407461632 logging_writer.py:48] [434100] global_step=434100, grad_norm=3.6318085193634033, loss=3.201115131378174
I0310 15:25:21.687845 139708415854336 logging_writer.py:48] [434200] global_step=434200, grad_norm=3.2845284938812256, loss=2.7793378829956055
I0310 15:26:07.276791 139708407461632 logging_writer.py:48] [434300] global_step=434300, grad_norm=2.9818472862243652, loss=1.0991995334625244
I0310 15:26:52.368623 139708415854336 logging_writer.py:48] [434400] global_step=434400, grad_norm=3.5434532165527344, loss=3.1114871501922607
I0310 15:27:38.019747 139708407461632 logging_writer.py:48] [434500] global_step=434500, grad_norm=3.1481871604919434, loss=1.0019614696502686
I0310 15:28:23.345141 139708415854336 logging_writer.py:48] [434600] global_step=434600, grad_norm=3.19667649269104, loss=1.226793646812439
I0310 15:29:08.397702 139708407461632 logging_writer.py:48] [434700] global_step=434700, grad_norm=3.249880075454712, loss=2.649214744567871
I0310 15:29:53.692407 139708415854336 logging_writer.py:48] [434800] global_step=434800, grad_norm=3.7090890407562256, loss=3.189422369003296
I0310 15:30:16.527989 139902746892096 spec.py:321] Evaluating on the training split.
I0310 15:30:27.735224 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 15:30:49.055243 139902746892096 spec.py:349] Evaluating on the test split.
I0310 15:30:50.736495 139902746892096 submission_runner.py:411] Time since start: 210573.77s, 	Step: 434852, 	{'train/accuracy': 0.8868945240974426, 'train/loss': 0.4147317111492157, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 194553.46647405624, 'total_duration': 210573.76526474953, 'accumulated_submission_time': 194553.46647405624, 'accumulated_eval_time': 15966.977990627289, 'accumulated_logging_time': 30.544241189956665}
I0310 15:30:50.818776 139708407461632 logging_writer.py:48] [434852] accumulated_eval_time=15966.977991, accumulated_logging_time=30.544241, accumulated_submission_time=194553.466474, global_step=434852, preemption_count=0, score=194553.466474, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=210573.765265, train/accuracy=0.886895, train/loss=0.414732, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 15:31:10.110465 139708415854336 logging_writer.py:48] [434900] global_step=434900, grad_norm=2.860374927520752, loss=1.0833232402801514
I0310 15:31:52.765400 139708407461632 logging_writer.py:48] [435000] global_step=435000, grad_norm=3.329094171524048, loss=2.7763383388519287
I0310 15:32:37.984996 139708415854336 logging_writer.py:48] [435100] global_step=435100, grad_norm=2.8169403076171875, loss=1.6696500778198242
I0310 15:33:23.411904 139708407461632 logging_writer.py:48] [435200] global_step=435200, grad_norm=2.997633934020996, loss=1.0816270112991333
I0310 15:34:08.620014 139708415854336 logging_writer.py:48] [435300] global_step=435300, grad_norm=3.3532536029815674, loss=2.815673351287842
I0310 15:34:54.063596 139708407461632 logging_writer.py:48] [435400] global_step=435400, grad_norm=3.3302102088928223, loss=1.1448830366134644
I0310 15:35:39.390567 139708415854336 logging_writer.py:48] [435500] global_step=435500, grad_norm=3.5030674934387207, loss=1.5297865867614746
I0310 15:36:24.656441 139708407461632 logging_writer.py:48] [435600] global_step=435600, grad_norm=3.3077147006988525, loss=1.5949106216430664
I0310 15:37:10.181101 139708415854336 logging_writer.py:48] [435700] global_step=435700, grad_norm=3.8649070262908936, loss=3.255566358566284
I0310 15:37:50.825300 139902746892096 spec.py:321] Evaluating on the training split.
I0310 15:38:02.251433 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 15:38:23.732609 139902746892096 spec.py:349] Evaluating on the test split.
I0310 15:38:25.409435 139902746892096 submission_runner.py:411] Time since start: 211028.44s, 	Step: 435791, 	{'train/accuracy': 0.8885546922683716, 'train/loss': 0.41643431782722473, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 194973.41510796547, 'total_duration': 211028.43823552132, 'accumulated_submission_time': 194973.41510796547, 'accumulated_eval_time': 16001.562130689621, 'accumulated_logging_time': 30.63562846183777}
I0310 15:38:25.489871 139708407461632 logging_writer.py:48] [435791] accumulated_eval_time=16001.562131, accumulated_logging_time=30.635628, accumulated_submission_time=194973.415108, global_step=435791, preemption_count=0, score=194973.415108, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=211028.438236, train/accuracy=0.888555, train/loss=0.416434, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 15:38:29.421055 139708415854336 logging_writer.py:48] [435800] global_step=435800, grad_norm=2.9089181423187256, loss=1.2184324264526367
I0310 15:39:10.410177 139708407461632 logging_writer.py:48] [435900] global_step=435900, grad_norm=3.0843379497528076, loss=1.2864489555358887
I0310 15:39:55.752931 139708415854336 logging_writer.py:48] [436000] global_step=436000, grad_norm=3.105302572250366, loss=2.43519926071167
I0310 15:40:41.168240 139708407461632 logging_writer.py:48] [436100] global_step=436100, grad_norm=3.4979782104492188, loss=3.1143922805786133
I0310 15:41:26.287485 139708415854336 logging_writer.py:48] [436200] global_step=436200, grad_norm=3.369933605194092, loss=2.698535919189453
I0310 15:42:11.528012 139708407461632 logging_writer.py:48] [436300] global_step=436300, grad_norm=3.170403242111206, loss=1.074312448501587
I0310 15:42:57.122800 139708415854336 logging_writer.py:48] [436400] global_step=436400, grad_norm=3.9738128185272217, loss=3.239776134490967
I0310 15:43:42.393553 139708407461632 logging_writer.py:48] [436500] global_step=436500, grad_norm=3.143380880355835, loss=1.107056975364685
I0310 15:44:27.451910 139708415854336 logging_writer.py:48] [436600] global_step=436600, grad_norm=2.821290969848633, loss=1.3719470500946045
I0310 15:45:13.194036 139708407461632 logging_writer.py:48] [436700] global_step=436700, grad_norm=3.974365711212158, loss=3.24873423576355
I0310 15:45:25.448650 139902746892096 spec.py:321] Evaluating on the training split.
I0310 15:45:36.807006 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 15:45:59.680552 139902746892096 spec.py:349] Evaluating on the test split.
I0310 15:46:01.362857 139902746892096 submission_runner.py:411] Time since start: 211484.39s, 	Step: 436729, 	{'train/accuracy': 0.8863476514816284, 'train/loss': 0.4209548830986023, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 195393.3149046898, 'total_duration': 211484.39165091515, 'accumulated_submission_time': 195393.3149046898, 'accumulated_eval_time': 16037.476316690445, 'accumulated_logging_time': 30.72657537460327}
I0310 15:46:01.447228 139708415854336 logging_writer.py:48] [436729] accumulated_eval_time=16037.476317, accumulated_logging_time=30.726575, accumulated_submission_time=195393.314905, global_step=436729, preemption_count=0, score=195393.314905, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=211484.391651, train/accuracy=0.886348, train/loss=0.420955, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 15:46:29.778446 139708407461632 logging_writer.py:48] [436800] global_step=436800, grad_norm=2.918196439743042, loss=1.5095421075820923
I0310 15:47:13.854151 139708415854336 logging_writer.py:48] [436900] global_step=436900, grad_norm=3.110555410385132, loss=1.545130968093872
I0310 15:47:59.499116 139708407461632 logging_writer.py:48] [437000] global_step=437000, grad_norm=2.893109083175659, loss=1.0744264125823975
I0310 15:48:45.005569 139708415854336 logging_writer.py:48] [437100] global_step=437100, grad_norm=3.0017333030700684, loss=1.0722477436065674
I0310 15:49:30.248260 139708407461632 logging_writer.py:48] [437200] global_step=437200, grad_norm=3.798650026321411, loss=3.30094838142395
I0310 15:50:15.588395 139708415854336 logging_writer.py:48] [437300] global_step=437300, grad_norm=3.1213767528533936, loss=1.16450834274292
I0310 15:51:00.686932 139708407461632 logging_writer.py:48] [437400] global_step=437400, grad_norm=2.7410190105438232, loss=1.6206821203231812
I0310 15:51:46.143322 139708415854336 logging_writer.py:48] [437500] global_step=437500, grad_norm=3.9039318561553955, loss=3.1902883052825928
I0310 15:52:31.316493 139708407461632 logging_writer.py:48] [437600] global_step=437600, grad_norm=3.2960970401763916, loss=1.1274813413619995
I0310 15:53:01.395952 139902746892096 spec.py:321] Evaluating on the training split.
I0310 15:53:12.785882 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 15:53:34.854892 139902746892096 spec.py:349] Evaluating on the test split.
I0310 15:53:36.528348 139902746892096 submission_runner.py:411] Time since start: 211939.56s, 	Step: 437668, 	{'train/accuracy': 0.8892577886581421, 'train/loss': 0.41338232159614563, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 195813.20502996445, 'total_duration': 211939.55713367462, 'accumulated_submission_time': 195813.20502996445, 'accumulated_eval_time': 16072.608691453934, 'accumulated_logging_time': 30.82048726081848}
I0310 15:53:36.623267 139708415854336 logging_writer.py:48] [437668] accumulated_eval_time=16072.608691, accumulated_logging_time=30.820487, accumulated_submission_time=195813.205030, global_step=437668, preemption_count=0, score=195813.205030, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=211939.557134, train/accuracy=0.889258, train/loss=0.413382, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 15:53:49.616348 139708407461632 logging_writer.py:48] [437700] global_step=437700, grad_norm=3.1856369972229004, loss=2.597531795501709
I0310 15:54:31.593358 139708415854336 logging_writer.py:48] [437800] global_step=437800, grad_norm=3.15775203704834, loss=2.3644134998321533
I0310 15:55:16.867996 139708407461632 logging_writer.py:48] [437900] global_step=437900, grad_norm=3.3145484924316406, loss=1.1017005443572998
I0310 15:56:02.384711 139708415854336 logging_writer.py:48] [438000] global_step=438000, grad_norm=4.185922622680664, loss=3.314201593399048
I0310 15:56:47.749075 139708407461632 logging_writer.py:48] [438100] global_step=438100, grad_norm=2.9322118759155273, loss=1.4187490940093994
I0310 15:57:33.251586 139708415854336 logging_writer.py:48] [438200] global_step=438200, grad_norm=2.9625988006591797, loss=1.153174638748169
I0310 15:58:18.661089 139708407461632 logging_writer.py:48] [438300] global_step=438300, grad_norm=2.998861074447632, loss=1.3152035474777222
I0310 15:59:03.946204 139708415854336 logging_writer.py:48] [438400] global_step=438400, grad_norm=3.4267423152923584, loss=2.700427770614624
I0310 15:59:49.415564 139708407461632 logging_writer.py:48] [438500] global_step=438500, grad_norm=2.8962416648864746, loss=1.5348871946334839
I0310 16:00:34.826090 139708415854336 logging_writer.py:48] [438600] global_step=438600, grad_norm=2.9985930919647217, loss=1.103497862815857
I0310 16:00:36.766296 139902746892096 spec.py:321] Evaluating on the training split.
I0310 16:00:48.132696 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 16:01:08.844394 139902746892096 spec.py:349] Evaluating on the test split.
I0310 16:01:10.526525 139902746892096 submission_runner.py:411] Time since start: 212393.56s, 	Step: 438606, 	{'train/accuracy': 0.8886327743530273, 'train/loss': 0.4106488823890686, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 196233.2885670662, 'total_duration': 212393.555311203, 'accumulated_submission_time': 196233.2885670662, 'accumulated_eval_time': 16106.368917942047, 'accumulated_logging_time': 30.92542052268982}
I0310 16:01:10.631294 139708407461632 logging_writer.py:48] [438606] accumulated_eval_time=16106.368918, accumulated_logging_time=30.925421, accumulated_submission_time=196233.288567, global_step=438606, preemption_count=0, score=196233.288567, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=212393.555311, train/accuracy=0.888633, train/loss=0.410649, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 16:01:48.412521 139708415854336 logging_writer.py:48] [438700] global_step=438700, grad_norm=2.866339921951294, loss=1.8278800249099731
I0310 16:02:33.476693 139708407461632 logging_writer.py:48] [438800] global_step=438800, grad_norm=3.144735813140869, loss=1.1269372701644897
I0310 16:03:18.750417 139708415854336 logging_writer.py:48] [438900] global_step=438900, grad_norm=3.4092726707458496, loss=2.989715337753296
I0310 16:04:04.504707 139708407461632 logging_writer.py:48] [439000] global_step=439000, grad_norm=3.2685022354125977, loss=1.058619499206543
I0310 16:04:49.626707 139708415854336 logging_writer.py:48] [439100] global_step=439100, grad_norm=3.1787068843841553, loss=2.6199870109558105
I0310 16:05:35.327735 139708407461632 logging_writer.py:48] [439200] global_step=439200, grad_norm=3.164680242538452, loss=1.1466233730316162
I0310 16:06:20.550769 139708415854336 logging_writer.py:48] [439300] global_step=439300, grad_norm=3.628005027770996, loss=2.8900768756866455
I0310 16:07:05.716566 139708407461632 logging_writer.py:48] [439400] global_step=439400, grad_norm=3.0104002952575684, loss=1.2915287017822266
I0310 16:07:51.256157 139708415854336 logging_writer.py:48] [439500] global_step=439500, grad_norm=3.056077003479004, loss=2.2430977821350098
I0310 16:08:10.855180 139902746892096 spec.py:321] Evaluating on the training split.
I0310 16:08:22.237750 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 16:08:44.614373 139902746892096 spec.py:349] Evaluating on the test split.
I0310 16:08:46.291275 139902746892096 submission_runner.py:411] Time since start: 212849.32s, 	Step: 439545, 	{'train/accuracy': 0.8883788585662842, 'train/loss': 0.4163420498371124, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 196653.45367574692, 'total_duration': 212849.32005739212, 'accumulated_submission_time': 196653.45367574692, 'accumulated_eval_time': 16141.804993867874, 'accumulated_logging_time': 31.039782285690308}
I0310 16:08:46.389475 139708407461632 logging_writer.py:48] [439545] accumulated_eval_time=16141.804994, accumulated_logging_time=31.039782, accumulated_submission_time=196653.453676, global_step=439545, preemption_count=0, score=196653.453676, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=212849.320057, train/accuracy=0.888379, train/loss=0.416342, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 16:09:08.426784 139708415854336 logging_writer.py:48] [439600] global_step=439600, grad_norm=2.921666383743286, loss=2.3564019203186035
I0310 16:09:51.456049 139708407461632 logging_writer.py:48] [439700] global_step=439700, grad_norm=2.747103691101074, loss=1.719204068183899
I0310 16:10:36.768320 139708415854336 logging_writer.py:48] [439800] global_step=439800, grad_norm=3.429277181625366, loss=2.468745470046997
I0310 16:11:22.383172 139708407461632 logging_writer.py:48] [439900] global_step=439900, grad_norm=2.888126850128174, loss=1.2143292427062988
I0310 16:12:07.624718 139708415854336 logging_writer.py:48] [440000] global_step=440000, grad_norm=3.1656625270843506, loss=1.1114022731781006
I0310 16:12:52.990280 139708407461632 logging_writer.py:48] [440100] global_step=440100, grad_norm=3.147024631500244, loss=1.5129765272140503
I0310 16:13:38.592323 139708415854336 logging_writer.py:48] [440200] global_step=440200, grad_norm=3.2232797145843506, loss=1.9398000240325928
I0310 16:14:23.896604 139708407461632 logging_writer.py:48] [440300] global_step=440300, grad_norm=3.301818609237671, loss=1.1804786920547485
I0310 16:15:09.334049 139708415854336 logging_writer.py:48] [440400] global_step=440400, grad_norm=4.152761459350586, loss=3.1970341205596924
I0310 16:15:46.502382 139902746892096 spec.py:321] Evaluating on the training split.
I0310 16:15:58.030756 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 16:16:20.498077 139902746892096 spec.py:349] Evaluating on the test split.
I0310 16:16:22.171721 139902746892096 submission_runner.py:411] Time since start: 213305.20s, 	Step: 440484, 	{'train/accuracy': 0.8870507478713989, 'train/loss': 0.41588273644447327, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 197073.50745630264, 'total_duration': 213305.20050811768, 'accumulated_submission_time': 197073.50745630264, 'accumulated_eval_time': 16177.474328279495, 'accumulated_logging_time': 31.147948503494263}
I0310 16:16:22.254580 139708407461632 logging_writer.py:48] [440484] accumulated_eval_time=16177.474328, accumulated_logging_time=31.147949, accumulated_submission_time=197073.507456, global_step=440484, preemption_count=0, score=197073.507456, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=213305.200508, train/accuracy=0.887051, train/loss=0.415883, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 16:16:28.940150 139708415854336 logging_writer.py:48] [440500] global_step=440500, grad_norm=3.056370735168457, loss=1.2379200458526611
I0310 16:17:09.586793 139708407461632 logging_writer.py:48] [440600] global_step=440600, grad_norm=3.6128718852996826, loss=3.2099215984344482
I0310 16:17:55.134208 139708415854336 logging_writer.py:48] [440700] global_step=440700, grad_norm=2.971198320388794, loss=1.0918819904327393
I0310 16:18:40.424142 139708407461632 logging_writer.py:48] [440800] global_step=440800, grad_norm=2.8750083446502686, loss=2.0868566036224365
I0310 16:19:25.879799 139708415854336 logging_writer.py:48] [440900] global_step=440900, grad_norm=3.805849313735962, loss=2.990187406539917
I0310 16:20:11.243451 139708407461632 logging_writer.py:48] [441000] global_step=441000, grad_norm=2.9327900409698486, loss=2.4103500843048096
I0310 16:20:56.538819 139708415854336 logging_writer.py:48] [441100] global_step=441100, grad_norm=3.3103928565979004, loss=1.379262924194336
I0310 16:21:41.845920 139708407461632 logging_writer.py:48] [441200] global_step=441200, grad_norm=3.1851229667663574, loss=1.7572228908538818
I0310 16:22:27.202847 139708415854336 logging_writer.py:48] [441300] global_step=441300, grad_norm=3.5919265747070312, loss=3.1246650218963623
I0310 16:23:12.545905 139708407461632 logging_writer.py:48] [441400] global_step=441400, grad_norm=3.1421172618865967, loss=1.1086509227752686
I0310 16:23:22.280066 139902746892096 spec.py:321] Evaluating on the training split.
I0310 16:23:33.912177 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 16:23:56.738103 139902746892096 spec.py:349] Evaluating on the test split.
I0310 16:23:58.409598 139902746892096 submission_runner.py:411] Time since start: 213761.44s, 	Step: 441423, 	{'train/accuracy': 0.8891406059265137, 'train/loss': 0.41353046894073486, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 197493.4744989872, 'total_duration': 213761.4383919239, 'accumulated_submission_time': 197493.4744989872, 'accumulated_eval_time': 16213.603892564774, 'accumulated_logging_time': 31.240306854248047}
I0310 16:23:58.495584 139708415854336 logging_writer.py:48] [441423] accumulated_eval_time=16213.603893, accumulated_logging_time=31.240307, accumulated_submission_time=197493.474499, global_step=441423, preemption_count=0, score=197493.474499, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=213761.438392, train/accuracy=0.889141, train/loss=0.413530, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 16:24:29.169294 139708407461632 logging_writer.py:48] [441500] global_step=441500, grad_norm=2.934182643890381, loss=1.2281969785690308
I0310 16:25:13.702553 139708415854336 logging_writer.py:48] [441600] global_step=441600, grad_norm=3.3203461170196533, loss=1.5174685716629028
I0310 16:25:59.486339 139708407461632 logging_writer.py:48] [441700] global_step=441700, grad_norm=3.310626745223999, loss=2.129248857498169
I0310 16:26:45.243375 139708415854336 logging_writer.py:48] [441800] global_step=441800, grad_norm=2.836251735687256, loss=1.8719520568847656
I0310 16:27:30.349694 139708407461632 logging_writer.py:48] [441900] global_step=441900, grad_norm=2.9527945518493652, loss=1.0948288440704346
I0310 16:28:15.895042 139708415854336 logging_writer.py:48] [442000] global_step=442000, grad_norm=3.279743194580078, loss=1.0963759422302246
I0310 16:29:00.987138 139708407461632 logging_writer.py:48] [442100] global_step=442100, grad_norm=3.0245578289031982, loss=1.108848214149475
I0310 16:29:46.498561 139708415854336 logging_writer.py:48] [442200] global_step=442200, grad_norm=3.1608948707580566, loss=1.1109976768493652
I0310 16:30:31.807619 139708407461632 logging_writer.py:48] [442300] global_step=442300, grad_norm=2.9274191856384277, loss=2.304713249206543
I0310 16:30:58.632958 139902746892096 spec.py:321] Evaluating on the training split.
I0310 16:31:10.284173 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 16:31:31.752846 139902746892096 spec.py:349] Evaluating on the test split.
I0310 16:31:33.428059 139902746892096 submission_runner.py:411] Time since start: 214216.46s, 	Step: 442361, 	{'train/accuracy': 0.8888866901397705, 'train/loss': 0.4150472581386566, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 197913.55271673203, 'total_duration': 214216.4568591118, 'accumulated_submission_time': 197913.55271673203, 'accumulated_eval_time': 16248.398986577988, 'accumulated_logging_time': 31.336047172546387}
I0310 16:31:33.508937 139708415854336 logging_writer.py:48] [442361] accumulated_eval_time=16248.398987, accumulated_logging_time=31.336047, accumulated_submission_time=197913.552717, global_step=442361, preemption_count=0, score=197913.552717, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=214216.456859, train/accuracy=0.888887, train/loss=0.415047, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 16:31:49.247374 139708407461632 logging_writer.py:48] [442400] global_step=442400, grad_norm=4.238633632659912, loss=3.2790424823760986
I0310 16:32:32.124730 139708415854336 logging_writer.py:48] [442500] global_step=442500, grad_norm=4.4663405418396, loss=1.229111909866333
I0310 16:33:17.254772 139708407461632 logging_writer.py:48] [442600] global_step=442600, grad_norm=2.8886168003082275, loss=1.120510220527649
I0310 16:34:02.691602 139708415854336 logging_writer.py:48] [442700] global_step=442700, grad_norm=2.8677947521209717, loss=1.7319416999816895
I0310 16:34:48.065125 139708407461632 logging_writer.py:48] [442800] global_step=442800, grad_norm=3.0200557708740234, loss=1.312540054321289
I0310 16:35:33.491951 139708415854336 logging_writer.py:48] [442900] global_step=442900, grad_norm=2.9820263385772705, loss=1.1570268869400024
I0310 16:36:19.247790 139708407461632 logging_writer.py:48] [443000] global_step=443000, grad_norm=3.0482757091522217, loss=1.2403581142425537
I0310 16:37:04.709891 139708415854336 logging_writer.py:48] [443100] global_step=443100, grad_norm=2.726857900619507, loss=1.613399863243103
I0310 16:37:50.062695 139708407461632 logging_writer.py:48] [443200] global_step=443200, grad_norm=3.262021064758301, loss=1.1223740577697754
I0310 16:38:33.760051 139902746892096 spec.py:321] Evaluating on the training split.
I0310 16:38:45.246129 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 16:39:07.367259 139902746892096 spec.py:349] Evaluating on the test split.
I0310 16:39:09.034105 139902746892096 submission_runner.py:411] Time since start: 214672.06s, 	Step: 443298, 	{'train/accuracy': 0.8919921517372131, 'train/loss': 0.4033390283584595, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 198333.74651145935, 'total_duration': 214672.06289815903, 'accumulated_submission_time': 198333.74651145935, 'accumulated_eval_time': 16283.67305135727, 'accumulated_logging_time': 31.426677703857422}
I0310 16:39:09.115440 139708415854336 logging_writer.py:48] [443298] accumulated_eval_time=16283.673051, accumulated_logging_time=31.426678, accumulated_submission_time=198333.746511, global_step=443298, preemption_count=0, score=198333.746511, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=214672.062898, train/accuracy=0.891992, train/loss=0.403339, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 16:39:10.305312 139708407461632 logging_writer.py:48] [443300] global_step=443300, grad_norm=3.199195623397827, loss=1.2125967741012573
I0310 16:39:50.823396 139708415854336 logging_writer.py:48] [443400] global_step=443400, grad_norm=3.0563716888427734, loss=1.1576510667800903
I0310 16:40:36.096815 139708407461632 logging_writer.py:48] [443500] global_step=443500, grad_norm=3.4780406951904297, loss=1.1158887147903442
I0310 16:41:21.436583 139708415854336 logging_writer.py:48] [443600] global_step=443600, grad_norm=2.992030620574951, loss=1.6651051044464111
I0310 16:42:06.731114 139708407461632 logging_writer.py:48] [443700] global_step=443700, grad_norm=2.9858055114746094, loss=1.1648318767547607
I0310 16:42:51.992828 139708415854336 logging_writer.py:48] [443800] global_step=443800, grad_norm=2.982396125793457, loss=1.8021061420440674
I0310 16:43:37.389114 139708407461632 logging_writer.py:48] [443900] global_step=443900, grad_norm=3.285959005355835, loss=1.0901501178741455
I0310 16:44:23.075258 139708415854336 logging_writer.py:48] [444000] global_step=444000, grad_norm=2.9096627235412598, loss=1.1719028949737549
I0310 16:45:08.503344 139708407461632 logging_writer.py:48] [444100] global_step=444100, grad_norm=4.188847541809082, loss=1.864946961402893
I0310 16:45:54.215878 139708415854336 logging_writer.py:48] [444200] global_step=444200, grad_norm=3.2308506965637207, loss=1.140557050704956
I0310 16:46:09.421319 139902746892096 spec.py:321] Evaluating on the training split.
I0310 16:46:20.797606 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 16:46:44.049326 139902746892096 spec.py:349] Evaluating on the test split.
I0310 16:46:45.720744 139902746892096 submission_runner.py:411] Time since start: 215128.75s, 	Step: 444235, 	{'train/accuracy': 0.8896874785423279, 'train/loss': 0.4114967882633209, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 198753.99509072304, 'total_duration': 215128.74952864647, 'accumulated_submission_time': 198753.99509072304, 'accumulated_eval_time': 16319.972463607788, 'accumulated_logging_time': 31.51662302017212}
I0310 16:46:45.815907 139708407461632 logging_writer.py:48] [444235] accumulated_eval_time=16319.972464, accumulated_logging_time=31.516623, accumulated_submission_time=198753.995091, global_step=444235, preemption_count=0, score=198753.995091, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=215128.749529, train/accuracy=0.889687, train/loss=0.411497, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 16:47:11.806362 139708415854336 logging_writer.py:48] [444300] global_step=444300, grad_norm=3.1403696537017822, loss=1.20834219455719
I0310 16:47:55.498424 139708407461632 logging_writer.py:48] [444400] global_step=444400, grad_norm=3.220186233520508, loss=1.133327603340149
I0310 16:48:40.821068 139708415854336 logging_writer.py:48] [444500] global_step=444500, grad_norm=4.176665782928467, loss=3.0637848377227783
I0310 16:49:26.575711 139708407461632 logging_writer.py:48] [444600] global_step=444600, grad_norm=3.333254098892212, loss=2.553684711456299
I0310 16:50:11.759532 139708415854336 logging_writer.py:48] [444700] global_step=444700, grad_norm=2.9589996337890625, loss=2.0621602535247803
I0310 16:50:57.285725 139708407461632 logging_writer.py:48] [444800] global_step=444800, grad_norm=3.190788745880127, loss=1.3624852895736694
I0310 16:51:42.445611 139708415854336 logging_writer.py:48] [444900] global_step=444900, grad_norm=3.205941915512085, loss=1.0926417112350464
I0310 16:52:27.862915 139708407461632 logging_writer.py:48] [445000] global_step=445000, grad_norm=5.638694763183594, loss=1.247877597808838
I0310 16:53:13.209971 139708415854336 logging_writer.py:48] [445100] global_step=445100, grad_norm=3.28402042388916, loss=1.2682182788848877
I0310 16:53:46.041416 139902746892096 spec.py:321] Evaluating on the training split.
I0310 16:53:58.023899 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 16:54:19.911882 139902746892096 spec.py:349] Evaluating on the test split.
I0310 16:54:21.585696 139902746892096 submission_runner.py:411] Time since start: 215584.61s, 	Step: 445174, 	{'train/accuracy': 0.8869726657867432, 'train/loss': 0.4166136384010315, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 199174.16241502762, 'total_duration': 215584.61447906494, 'accumulated_submission_time': 199174.16241502762, 'accumulated_eval_time': 16355.516724586487, 'accumulated_logging_time': 31.621028661727905}
I0310 16:54:21.680830 139708407461632 logging_writer.py:48] [445174] accumulated_eval_time=16355.516725, accumulated_logging_time=31.621029, accumulated_submission_time=199174.162415, global_step=445174, preemption_count=0, score=199174.162415, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=215584.614479, train/accuracy=0.886973, train/loss=0.416614, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 16:54:32.308178 139708415854336 logging_writer.py:48] [445200] global_step=445200, grad_norm=3.1661019325256348, loss=1.2612590789794922
I0310 16:55:13.600716 139708407461632 logging_writer.py:48] [445300] global_step=445300, grad_norm=3.1981112957000732, loss=1.2229828834533691
I0310 16:55:59.025408 139708415854336 logging_writer.py:48] [445400] global_step=445400, grad_norm=3.3423867225646973, loss=2.7470591068267822
I0310 16:56:44.943041 139708407461632 logging_writer.py:48] [445500] global_step=445500, grad_norm=3.4467689990997314, loss=3.133586883544922
I0310 16:57:30.523580 139708415854336 logging_writer.py:48] [445600] global_step=445600, grad_norm=3.4196159839630127, loss=2.9592058658599854
I0310 16:58:15.674734 139708407461632 logging_writer.py:48] [445700] global_step=445700, grad_norm=3.2120347023010254, loss=1.757978081703186
I0310 16:59:01.339520 139708415854336 logging_writer.py:48] [445800] global_step=445800, grad_norm=3.3960821628570557, loss=3.0561208724975586
I0310 16:59:46.684599 139708407461632 logging_writer.py:48] [445900] global_step=445900, grad_norm=2.949305295944214, loss=1.8962815999984741
I0310 17:00:32.227677 139708415854336 logging_writer.py:48] [446000] global_step=446000, grad_norm=3.446120262145996, loss=1.1285759210586548
I0310 17:01:17.630182 139708407461632 logging_writer.py:48] [446100] global_step=446100, grad_norm=3.3018064498901367, loss=2.662396192550659
I0310 17:01:21.878368 139902746892096 spec.py:321] Evaluating on the training split.
I0310 17:01:33.411894 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 17:01:56.055284 139902746892096 spec.py:349] Evaluating on the test split.
I0310 17:01:57.729131 139902746892096 submission_runner.py:411] Time since start: 216040.76s, 	Step: 446111, 	{'train/accuracy': 0.8872851133346558, 'train/loss': 0.4126180410385132, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 199594.30109024048, 'total_duration': 216040.75790715218, 'accumulated_submission_time': 199594.30109024048, 'accumulated_eval_time': 16391.367474079132, 'accumulated_logging_time': 31.726098775863647}
I0310 17:01:57.822089 139708415854336 logging_writer.py:48] [446111] accumulated_eval_time=16391.367474, accumulated_logging_time=31.726099, accumulated_submission_time=199594.301090, global_step=446111, preemption_count=0, score=199594.301090, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=216040.757907, train/accuracy=0.887285, train/loss=0.412618, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 17:02:33.396526 139708407461632 logging_writer.py:48] [446200] global_step=446200, grad_norm=3.043644428253174, loss=2.1482017040252686
I0310 17:03:18.627045 139708415854336 logging_writer.py:48] [446300] global_step=446300, grad_norm=3.1695032119750977, loss=1.4288861751556396
I0310 17:04:03.905469 139708407461632 logging_writer.py:48] [446400] global_step=446400, grad_norm=2.978200912475586, loss=1.0908799171447754
I0310 17:04:49.653981 139708415854336 logging_writer.py:48] [446500] global_step=446500, grad_norm=3.209455728530884, loss=1.0503581762313843
I0310 17:05:34.995874 139708407461632 logging_writer.py:48] [446600] global_step=446600, grad_norm=2.9251914024353027, loss=1.066338300704956
I0310 17:06:20.565260 139708415854336 logging_writer.py:48] [446700] global_step=446700, grad_norm=3.7168123722076416, loss=3.0335946083068848
I0310 17:07:05.965392 139708407461632 logging_writer.py:48] [446800] global_step=446800, grad_norm=3.6935696601867676, loss=3.0420186519622803
I0310 17:07:51.332387 139708415854336 logging_writer.py:48] [446900] global_step=446900, grad_norm=3.1789276599884033, loss=1.1010088920593262
I0310 17:08:36.686962 139708407461632 logging_writer.py:48] [447000] global_step=447000, grad_norm=3.3347604274749756, loss=1.1041182279586792
I0310 17:08:58.220204 139902746892096 spec.py:321] Evaluating on the training split.
I0310 17:09:09.727335 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 17:09:32.453705 139902746892096 spec.py:349] Evaluating on the test split.
I0310 17:09:34.129364 139902746892096 submission_runner.py:411] Time since start: 216497.16s, 	Step: 447049, 	{'train/accuracy': 0.88734370470047, 'train/loss': 0.41566184163093567, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 200014.64024281502, 'total_duration': 216497.15816378593, 'accumulated_submission_time': 200014.64024281502, 'accumulated_eval_time': 16427.27664041519, 'accumulated_logging_time': 31.828654527664185}
I0310 17:09:34.212643 139708415854336 logging_writer.py:48] [447049] accumulated_eval_time=16427.276640, accumulated_logging_time=31.828655, accumulated_submission_time=200014.640243, global_step=447049, preemption_count=0, score=200014.640243, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=216497.158164, train/accuracy=0.887344, train/loss=0.415662, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 17:09:54.681941 139708407461632 logging_writer.py:48] [447100] global_step=447100, grad_norm=3.100693702697754, loss=1.8037022352218628
I0310 17:10:37.914545 139708415854336 logging_writer.py:48] [447200] global_step=447200, grad_norm=3.28159236907959, loss=1.2591701745986938
I0310 17:11:23.337919 139708407461632 logging_writer.py:48] [447300] global_step=447300, grad_norm=2.9322593212127686, loss=2.0434494018554688
I0310 17:12:09.026079 139708415854336 logging_writer.py:48] [447400] global_step=447400, grad_norm=3.2635889053344727, loss=2.345662832260132
I0310 17:12:54.229404 139708407461632 logging_writer.py:48] [447500] global_step=447500, grad_norm=3.246006727218628, loss=1.9611059427261353
I0310 17:13:39.429213 139708415854336 logging_writer.py:48] [447600] global_step=447600, grad_norm=3.305402994155884, loss=1.1213701963424683
I0310 17:14:25.308152 139708407461632 logging_writer.py:48] [447700] global_step=447700, grad_norm=3.093493938446045, loss=2.017643928527832
I0310 17:15:10.485328 139708415854336 logging_writer.py:48] [447800] global_step=447800, grad_norm=3.07711124420166, loss=1.4896111488342285
I0310 17:15:56.123003 139708407461632 logging_writer.py:48] [447900] global_step=447900, grad_norm=3.0593950748443604, loss=2.018218517303467
I0310 17:16:34.441522 139902746892096 spec.py:321] Evaluating on the training split.
I0310 17:16:45.852785 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 17:17:08.673991 139902746892096 spec.py:349] Evaluating on the test split.
I0310 17:17:10.339708 139902746892096 submission_runner.py:411] Time since start: 216953.37s, 	Step: 447986, 	{'train/accuracy': 0.8880078196525574, 'train/loss': 0.41072872281074524, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 200434.81145572662, 'total_duration': 216953.36850714684, 'accumulated_submission_time': 200434.81145572662, 'accumulated_eval_time': 16463.17485022545, 'accumulated_logging_time': 31.92135524749756}
I0310 17:17:10.421776 139708415854336 logging_writer.py:48] [447986] accumulated_eval_time=16463.174850, accumulated_logging_time=31.921355, accumulated_submission_time=200434.811456, global_step=447986, preemption_count=0, score=200434.811456, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=216953.368507, train/accuracy=0.888008, train/loss=0.410729, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 17:17:16.319964 139708407461632 logging_writer.py:48] [448000] global_step=448000, grad_norm=3.141627550125122, loss=1.3084710836410522
I0310 17:17:57.278950 139708415854336 logging_writer.py:48] [448100] global_step=448100, grad_norm=2.8192129135131836, loss=1.3705792427062988
I0310 17:18:42.620944 139708407461632 logging_writer.py:48] [448200] global_step=448200, grad_norm=3.0483016967773438, loss=1.043860912322998
I0310 17:19:28.148167 139708415854336 logging_writer.py:48] [448300] global_step=448300, grad_norm=3.2296528816223145, loss=2.7089831829071045
I0310 17:20:13.532846 139708407461632 logging_writer.py:48] [448400] global_step=448400, grad_norm=3.099066734313965, loss=1.5095553398132324
I0310 17:20:58.948010 139708415854336 logging_writer.py:48] [448500] global_step=448500, grad_norm=2.8255534172058105, loss=1.4085018634796143
I0310 17:21:44.519819 139708407461632 logging_writer.py:48] [448600] global_step=448600, grad_norm=2.974071741104126, loss=1.0659681558609009
I0310 17:22:29.826847 139708415854336 logging_writer.py:48] [448700] global_step=448700, grad_norm=2.8314285278320312, loss=1.8575525283813477
I0310 17:23:15.159296 139708407461632 logging_writer.py:48] [448800] global_step=448800, grad_norm=3.0258328914642334, loss=1.3009421825408936
I0310 17:24:00.498065 139708415854336 logging_writer.py:48] [448900] global_step=448900, grad_norm=3.1478166580200195, loss=1.7511857748031616
I0310 17:24:10.736535 139902746892096 spec.py:321] Evaluating on the training split.
I0310 17:24:22.274573 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 17:24:45.094911 139902746892096 spec.py:349] Evaluating on the test split.
I0310 17:24:46.768400 139902746892096 submission_runner.py:411] Time since start: 217409.80s, 	Step: 448924, 	{'train/accuracy': 0.8885155916213989, 'train/loss': 0.41902875900268555, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 200855.06634163857, 'total_duration': 217409.7971892357, 'accumulated_submission_time': 200855.06634163857, 'accumulated_eval_time': 16499.20669221878, 'accumulated_logging_time': 32.01465153694153}
I0310 17:24:46.853984 139708407461632 logging_writer.py:48] [448924] accumulated_eval_time=16499.206692, accumulated_logging_time=32.014652, accumulated_submission_time=200855.066342, global_step=448924, preemption_count=0, score=200855.066342, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=217409.797189, train/accuracy=0.888516, train/loss=0.419029, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 17:25:17.165948 139708415854336 logging_writer.py:48] [449000] global_step=449000, grad_norm=3.1298351287841797, loss=1.1669564247131348
I0310 17:26:01.529549 139708407461632 logging_writer.py:48] [449100] global_step=449100, grad_norm=2.9612441062927246, loss=1.140135645866394
I0310 17:26:47.194065 139708415854336 logging_writer.py:48] [449200] global_step=449200, grad_norm=2.9627044200897217, loss=1.7223713397979736
I0310 17:27:32.561807 139708407461632 logging_writer.py:48] [449300] global_step=449300, grad_norm=2.984724521636963, loss=1.8836861848831177
I0310 17:28:17.876738 139708415854336 logging_writer.py:48] [449400] global_step=449400, grad_norm=3.0197200775146484, loss=1.064861536026001
I0310 17:29:03.476473 139708407461632 logging_writer.py:48] [449500] global_step=449500, grad_norm=2.7987570762634277, loss=1.4810009002685547
I0310 17:29:48.869099 139708415854336 logging_writer.py:48] [449600] global_step=449600, grad_norm=3.2971837520599365, loss=1.2877914905548096
I0310 17:30:34.202438 139708407461632 logging_writer.py:48] [449700] global_step=449700, grad_norm=4.363536834716797, loss=3.263956308364868
I0310 17:31:19.522970 139708415854336 logging_writer.py:48] [449800] global_step=449800, grad_norm=3.0817391872406006, loss=2.545083999633789
I0310 17:31:46.773198 139902746892096 spec.py:321] Evaluating on the training split.
I0310 17:31:58.242401 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 17:32:20.329457 139902746892096 spec.py:349] Evaluating on the test split.
I0310 17:32:22.001187 139902746892096 submission_runner.py:411] Time since start: 217865.03s, 	Step: 449862, 	{'train/accuracy': 0.8880664110183716, 'train/loss': 0.4124629497528076, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 201274.92774033546, 'total_duration': 217865.02998900414, 'accumulated_submission_time': 201274.92774033546, 'accumulated_eval_time': 16534.43468928337, 'accumulated_logging_time': 32.109177350997925}
I0310 17:32:22.083518 139708407461632 logging_writer.py:48] [449862] accumulated_eval_time=16534.434689, accumulated_logging_time=32.109177, accumulated_submission_time=201274.927740, global_step=449862, preemption_count=0, score=201274.927740, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=217865.029989, train/accuracy=0.888066, train/loss=0.412463, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 17:32:37.437031 139708415854336 logging_writer.py:48] [449900] global_step=449900, grad_norm=3.374347448348999, loss=2.8983917236328125
I0310 17:33:20.033698 139708407461632 logging_writer.py:48] [450000] global_step=450000, grad_norm=3.1289379596710205, loss=1.3348467350006104
I0310 17:34:05.037961 139708415854336 logging_writer.py:48] [450100] global_step=450100, grad_norm=3.173734188079834, loss=1.1509498357772827
I0310 17:34:50.511381 139708407461632 logging_writer.py:48] [450200] global_step=450200, grad_norm=3.143458604812622, loss=1.5147539377212524
I0310 17:35:36.009625 139708415854336 logging_writer.py:48] [450300] global_step=450300, grad_norm=3.22772479057312, loss=2.044660806655884
I0310 17:36:21.403760 139708407461632 logging_writer.py:48] [450400] global_step=450400, grad_norm=3.8281702995300293, loss=3.307929515838623
I0310 17:37:07.024642 139708415854336 logging_writer.py:48] [450500] global_step=450500, grad_norm=3.1573474407196045, loss=1.1561493873596191
I0310 17:37:52.611877 139708407461632 logging_writer.py:48] [450600] global_step=450600, grad_norm=3.1460843086242676, loss=1.1674163341522217
I0310 17:38:38.148360 139708415854336 logging_writer.py:48] [450700] global_step=450700, grad_norm=3.217857837677002, loss=1.1348533630371094
I0310 17:39:22.177547 139902746892096 spec.py:321] Evaluating on the training split.
I0310 17:39:33.811185 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 17:39:56.636509 139902746892096 spec.py:349] Evaluating on the test split.
I0310 17:39:58.308900 139902746892096 submission_runner.py:411] Time since start: 218321.34s, 	Step: 450799, 	{'train/accuracy': 0.8866210579872131, 'train/loss': 0.4189048409461975, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 201694.9643316269, 'total_duration': 218321.33767843246, 'accumulated_submission_time': 201694.9643316269, 'accumulated_eval_time': 16570.566035032272, 'accumulated_logging_time': 32.20013737678528}
I0310 17:39:58.406614 139708407461632 logging_writer.py:48] [450799] accumulated_eval_time=16570.566035, accumulated_logging_time=32.200137, accumulated_submission_time=201694.964332, global_step=450799, preemption_count=0, score=201694.964332, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=218321.337678, train/accuracy=0.886621, train/loss=0.418905, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 17:39:59.207006 139708415854336 logging_writer.py:48] [450800] global_step=450800, grad_norm=3.223737955093384, loss=1.6724117994308472
I0310 17:40:39.374940 139708407461632 logging_writer.py:48] [450900] global_step=450900, grad_norm=3.036113739013672, loss=1.0836758613586426
I0310 17:41:24.659465 139708415854336 logging_writer.py:48] [451000] global_step=451000, grad_norm=3.098788022994995, loss=1.107522964477539
I0310 17:42:10.408704 139708407461632 logging_writer.py:48] [451100] global_step=451100, grad_norm=3.748962163925171, loss=3.24843168258667
I0310 17:42:55.839588 139708415854336 logging_writer.py:48] [451200] global_step=451200, grad_norm=3.37774658203125, loss=1.2332035303115845
I0310 17:43:41.192309 139708407461632 logging_writer.py:48] [451300] global_step=451300, grad_norm=3.001375675201416, loss=1.193215250968933
I0310 17:44:26.696356 139708415854336 logging_writer.py:48] [451400] global_step=451400, grad_norm=2.8641984462738037, loss=1.164687991142273
I0310 17:45:12.187264 139708407461632 logging_writer.py:48] [451500] global_step=451500, grad_norm=4.204545497894287, loss=2.989469051361084
I0310 17:45:57.508464 139708415854336 logging_writer.py:48] [451600] global_step=451600, grad_norm=2.9469001293182373, loss=2.277216672897339
I0310 17:46:43.416754 139708407461632 logging_writer.py:48] [451700] global_step=451700, grad_norm=3.1525115966796875, loss=1.9663159847259521
I0310 17:46:58.743680 139902746892096 spec.py:321] Evaluating on the training split.
I0310 17:47:10.287390 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 17:47:33.184680 139902746892096 spec.py:349] Evaluating on the test split.
I0310 17:47:34.853201 139902746892096 submission_runner.py:411] Time since start: 218777.88s, 	Step: 451735, 	{'train/accuracy': 0.8884375095367432, 'train/loss': 0.4119004011154175, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 202115.24264860153, 'total_duration': 218777.88199329376, 'accumulated_submission_time': 202115.24264860153, 'accumulated_eval_time': 16606.675546884537, 'accumulated_logging_time': 32.308313846588135}
I0310 17:47:34.938348 139708415854336 logging_writer.py:48] [451735] accumulated_eval_time=16606.675547, accumulated_logging_time=32.308314, accumulated_submission_time=202115.242649, global_step=451735, preemption_count=0, score=202115.242649, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=218777.881993, train/accuracy=0.888438, train/loss=0.411900, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 17:48:00.911461 139708407461632 logging_writer.py:48] [451800] global_step=451800, grad_norm=3.154639720916748, loss=1.1563650369644165
I0310 17:48:44.662609 139708415854336 logging_writer.py:48] [451900] global_step=451900, grad_norm=3.62935471534729, loss=2.5783779621124268
I0310 17:49:30.081630 139708407461632 logging_writer.py:48] [452000] global_step=452000, grad_norm=3.00287127494812, loss=1.1711112260818481
I0310 17:50:15.351170 139708415854336 logging_writer.py:48] [452100] global_step=452100, grad_norm=3.09334397315979, loss=1.9798249006271362
I0310 17:51:00.598667 139708407461632 logging_writer.py:48] [452200] global_step=452200, grad_norm=2.9474642276763916, loss=2.037470579147339
I0310 17:51:45.844331 139708415854336 logging_writer.py:48] [452300] global_step=452300, grad_norm=4.338242530822754, loss=2.8776984214782715
I0310 17:52:31.039088 139708407461632 logging_writer.py:48] [452400] global_step=452400, grad_norm=3.560786247253418, loss=3.0283620357513428
I0310 17:53:16.390027 139708415854336 logging_writer.py:48] [452500] global_step=452500, grad_norm=3.086451768875122, loss=2.2173283100128174
I0310 17:54:01.693035 139708407461632 logging_writer.py:48] [452600] global_step=452600, grad_norm=2.897512912750244, loss=1.585126519203186
I0310 17:54:34.958734 139902746892096 spec.py:321] Evaluating on the training split.
I0310 17:54:46.489673 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 17:55:08.580059 139902746892096 spec.py:349] Evaluating on the test split.
I0310 17:55:10.252276 139902746892096 submission_runner.py:411] Time since start: 219233.28s, 	Step: 452675, 	{'train/accuracy': 0.8880664110183716, 'train/loss': 0.41462060809135437, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 202535.2054517269, 'total_duration': 219233.2810678482, 'accumulated_submission_time': 202535.2054517269, 'accumulated_eval_time': 16641.969081640244, 'accumulated_logging_time': 32.402332067489624}
I0310 17:55:10.335422 139708415854336 logging_writer.py:48] [452675] accumulated_eval_time=16641.969082, accumulated_logging_time=32.402332, accumulated_submission_time=202535.205452, global_step=452675, preemption_count=0, score=202535.205452, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=219233.281068, train/accuracy=0.888066, train/loss=0.414621, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 17:55:20.575269 139708407461632 logging_writer.py:48] [452700] global_step=452700, grad_norm=2.9599828720092773, loss=2.393728494644165
I0310 17:56:02.370755 139708415854336 logging_writer.py:48] [452800] global_step=452800, grad_norm=3.0703017711639404, loss=1.7121766805648804
I0310 17:56:47.759061 139708407461632 logging_writer.py:48] [452900] global_step=452900, grad_norm=5.028557300567627, loss=1.629989504814148
I0310 17:57:33.355562 139708415854336 logging_writer.py:48] [453000] global_step=453000, grad_norm=2.9226770401000977, loss=1.2718892097473145
I0310 17:58:18.596724 139708407461632 logging_writer.py:48] [453100] global_step=453100, grad_norm=3.5209357738494873, loss=1.0873459577560425
I0310 17:59:03.764880 139708415854336 logging_writer.py:48] [453200] global_step=453200, grad_norm=3.2398762702941895, loss=1.1736928224563599
I0310 17:59:49.168522 139708407461632 logging_writer.py:48] [453300] global_step=453300, grad_norm=2.9219777584075928, loss=1.8195961713790894
I0310 18:00:34.469904 139708415854336 logging_writer.py:48] [453400] global_step=453400, grad_norm=3.03450083732605, loss=1.1744952201843262
I0310 18:01:19.740868 139708407461632 logging_writer.py:48] [453500] global_step=453500, grad_norm=3.1348345279693604, loss=2.0199666023254395
I0310 18:02:05.079686 139708415854336 logging_writer.py:48] [453600] global_step=453600, grad_norm=3.878648281097412, loss=2.933678150177002
I0310 18:02:10.693154 139902746892096 spec.py:321] Evaluating on the training split.
I0310 18:02:22.113549 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 18:02:42.805313 139902746892096 spec.py:349] Evaluating on the test split.
I0310 18:02:44.486463 139902746892096 submission_runner.py:411] Time since start: 219687.52s, 	Step: 453614, 	{'train/accuracy': 0.8863085508346558, 'train/loss': 0.41980284452438354, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 202955.50574207306, 'total_duration': 219687.51525568962, 'accumulated_submission_time': 202955.50574207306, 'accumulated_eval_time': 16675.76238799095, 'accumulated_logging_time': 32.494225025177}
I0310 18:02:44.576745 139708407461632 logging_writer.py:48] [453614] accumulated_eval_time=16675.762388, accumulated_logging_time=32.494225, accumulated_submission_time=202955.505742, global_step=453614, preemption_count=0, score=202955.505742, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=219687.515256, train/accuracy=0.886309, train/loss=0.419803, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 18:03:19.091393 139708415854336 logging_writer.py:48] [453700] global_step=453700, grad_norm=3.0263683795928955, loss=1.2100896835327148
I0310 18:04:04.321688 139708407461632 logging_writer.py:48] [453800] global_step=453800, grad_norm=3.1839916706085205, loss=1.178407907485962
I0310 18:04:49.716178 139708415854336 logging_writer.py:48] [453900] global_step=453900, grad_norm=3.956146478652954, loss=3.2445664405822754
I0310 18:05:35.033608 139708407461632 logging_writer.py:48] [454000] global_step=454000, grad_norm=2.9764750003814697, loss=1.0736453533172607
I0310 18:06:20.189528 139708415854336 logging_writer.py:48] [454100] global_step=454100, grad_norm=3.2605910301208496, loss=2.2845988273620605
I0310 18:07:05.582736 139708407461632 logging_writer.py:48] [454200] global_step=454200, grad_norm=3.1224753856658936, loss=1.1402537822723389
I0310 18:07:50.956742 139708415854336 logging_writer.py:48] [454300] global_step=454300, grad_norm=3.2585904598236084, loss=1.2021983861923218
I0310 18:08:36.390812 139708407461632 logging_writer.py:48] [454400] global_step=454400, grad_norm=3.7102818489074707, loss=3.10911226272583
I0310 18:09:21.782034 139708415854336 logging_writer.py:48] [454500] global_step=454500, grad_norm=3.2791385650634766, loss=2.1091678142547607
I0310 18:09:44.761117 139902746892096 spec.py:321] Evaluating on the training split.
I0310 18:09:56.269092 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 18:10:19.754549 139902746892096 spec.py:349] Evaluating on the test split.
I0310 18:10:21.417266 139902746892096 submission_runner.py:411] Time since start: 220144.45s, 	Step: 454552, 	{'train/accuracy': 0.8874413967132568, 'train/loss': 0.418776273727417, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 203375.63128709793, 'total_duration': 220144.4460659027, 'accumulated_submission_time': 203375.63128709793, 'accumulated_eval_time': 16712.418529748917, 'accumulated_logging_time': 32.59436345100403}
I0310 18:10:21.500719 139708407461632 logging_writer.py:48] [454552] accumulated_eval_time=16712.418530, accumulated_logging_time=32.594363, accumulated_submission_time=203375.631287, global_step=454552, preemption_count=0, score=203375.631287, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=220144.446066, train/accuracy=0.887441, train/loss=0.418776, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 18:10:40.801290 139708415854336 logging_writer.py:48] [454600] global_step=454600, grad_norm=3.372676134109497, loss=1.2019116878509521
I0310 18:11:23.169599 139708407461632 logging_writer.py:48] [454700] global_step=454700, grad_norm=3.489588737487793, loss=1.168338656425476
I0310 18:12:08.754350 139708415854336 logging_writer.py:48] [454800] global_step=454800, grad_norm=3.6899728775024414, loss=3.053682565689087
I0310 18:12:54.380354 139708407461632 logging_writer.py:48] [454900] global_step=454900, grad_norm=3.130537271499634, loss=2.5220584869384766
I0310 18:13:39.270498 139708415854336 logging_writer.py:48] [455000] global_step=455000, grad_norm=3.5592236518859863, loss=2.9320998191833496
I0310 18:14:24.407599 139708407461632 logging_writer.py:48] [455100] global_step=455100, grad_norm=3.6851398944854736, loss=3.1338582038879395
I0310 18:15:09.807680 139708415854336 logging_writer.py:48] [455200] global_step=455200, grad_norm=3.127645969390869, loss=1.2010774612426758
I0310 18:15:55.090892 139708407461632 logging_writer.py:48] [455300] global_step=455300, grad_norm=3.416186809539795, loss=3.025926351547241
I0310 18:16:40.843516 139708415854336 logging_writer.py:48] [455400] global_step=455400, grad_norm=2.7770895957946777, loss=1.9277563095092773
I0310 18:17:21.841516 139902746892096 spec.py:321] Evaluating on the training split.
I0310 18:17:33.538004 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 18:17:56.299676 139902746892096 spec.py:349] Evaluating on the test split.
I0310 18:17:57.972383 139902746892096 submission_runner.py:411] Time since start: 220601.00s, 	Step: 455492, 	{'train/accuracy': 0.8889843821525574, 'train/loss': 0.41404807567596436, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 203795.91160702705, 'total_duration': 220601.00116944313, 'accumulated_submission_time': 203795.91160702705, 'accumulated_eval_time': 16748.549387454987, 'accumulated_logging_time': 32.69032335281372}
I0310 18:17:58.056799 139708407461632 logging_writer.py:48] [455492] accumulated_eval_time=16748.549387, accumulated_logging_time=32.690323, accumulated_submission_time=203795.911607, global_step=455492, preemption_count=0, score=203795.911607, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=220601.001169, train/accuracy=0.888984, train/loss=0.414048, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 18:18:01.600238 139708415854336 logging_writer.py:48] [455500] global_step=455500, grad_norm=3.1304168701171875, loss=1.0841255187988281
I0310 18:18:42.059057 139708407461632 logging_writer.py:48] [455600] global_step=455600, grad_norm=2.8744258880615234, loss=1.0254961252212524
I0310 18:19:27.319334 139708415854336 logging_writer.py:48] [455700] global_step=455700, grad_norm=3.2973217964172363, loss=1.149204134941101
I0310 18:20:13.056739 139708407461632 logging_writer.py:48] [455800] global_step=455800, grad_norm=3.254509449005127, loss=2.7721168994903564
I0310 18:20:58.565227 139708415854336 logging_writer.py:48] [455900] global_step=455900, grad_norm=3.1572282314300537, loss=1.1666208505630493
I0310 18:21:43.839743 139708407461632 logging_writer.py:48] [456000] global_step=456000, grad_norm=3.1119003295898438, loss=1.3104724884033203
I0310 18:22:29.538694 139708415854336 logging_writer.py:48] [456100] global_step=456100, grad_norm=2.9938230514526367, loss=2.22182559967041
I0310 18:23:14.598599 139708407461632 logging_writer.py:48] [456200] global_step=456200, grad_norm=3.3358795642852783, loss=2.0072081089019775
I0310 18:23:59.773925 139708415854336 logging_writer.py:48] [456300] global_step=456300, grad_norm=3.8911728858947754, loss=3.22841477394104
I0310 18:24:44.952186 139708407461632 logging_writer.py:48] [456400] global_step=456400, grad_norm=3.1192727088928223, loss=1.1854228973388672
I0310 18:24:58.185551 139902746892096 spec.py:321] Evaluating on the training split.
I0310 18:25:09.709099 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 18:25:31.922482 139902746892096 spec.py:349] Evaluating on the test split.
I0310 18:25:33.585722 139902746892096 submission_runner.py:411] Time since start: 221056.61s, 	Step: 456431, 	{'train/accuracy': 0.8893945217132568, 'train/loss': 0.41152775287628174, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 204215.98255586624, 'total_duration': 221056.6145207882, 'accumulated_submission_time': 204215.98255586624, 'accumulated_eval_time': 16783.94956278801, 'accumulated_logging_time': 32.78437304496765}
I0310 18:25:33.670132 139708415854336 logging_writer.py:48] [456431] accumulated_eval_time=16783.949563, accumulated_logging_time=32.784373, accumulated_submission_time=204215.982556, global_step=456431, preemption_count=0, score=204215.982556, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=221056.614521, train/accuracy=0.889395, train/loss=0.411528, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 18:26:01.216195 139708407461632 logging_writer.py:48] [456500] global_step=456500, grad_norm=2.965261459350586, loss=2.110846757888794
I0310 18:26:45.680323 139708415854336 logging_writer.py:48] [456600] global_step=456600, grad_norm=3.2710771560668945, loss=1.0881694555282593
I0310 18:27:30.916103 139708407461632 logging_writer.py:48] [456700] global_step=456700, grad_norm=3.254101514816284, loss=1.0812135934829712
I0310 18:28:16.639235 139708415854336 logging_writer.py:48] [456800] global_step=456800, grad_norm=2.9918246269226074, loss=2.3442859649658203
I0310 18:29:01.942447 139708407461632 logging_writer.py:48] [456900] global_step=456900, grad_norm=3.7988150119781494, loss=3.252331018447876
I0310 18:29:47.179258 139708415854336 logging_writer.py:48] [457000] global_step=457000, grad_norm=2.99971866607666, loss=1.0611968040466309
I0310 18:30:32.361644 139708407461632 logging_writer.py:48] [457100] global_step=457100, grad_norm=3.2643520832061768, loss=1.114505648612976
I0310 18:31:17.745422 139708415854336 logging_writer.py:48] [457200] global_step=457200, grad_norm=3.270076274871826, loss=1.1595771312713623
I0310 18:32:02.784684 139708407461632 logging_writer.py:48] [457300] global_step=457300, grad_norm=3.3059945106506348, loss=1.1136767864227295
I0310 18:32:33.630746 139902746892096 spec.py:321] Evaluating on the training split.
I0310 18:32:45.140256 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 18:33:07.298023 139902746892096 spec.py:349] Evaluating on the test split.
I0310 18:33:08.963758 139902746892096 submission_runner.py:411] Time since start: 221511.99s, 	Step: 457370, 	{'train/accuracy': 0.8887109160423279, 'train/loss': 0.4161616861820221, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 204635.8847630024, 'total_duration': 221511.99255919456, 'accumulated_submission_time': 204635.8847630024, 'accumulated_eval_time': 16819.28258252144, 'accumulated_logging_time': 32.87864851951599}
I0310 18:33:09.049054 139708415854336 logging_writer.py:48] [457370] accumulated_eval_time=16819.282583, accumulated_logging_time=32.878649, accumulated_submission_time=204635.884763, global_step=457370, preemption_count=0, score=204635.884763, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=221511.992559, train/accuracy=0.888711, train/loss=0.416162, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 18:33:21.250923 139708407461632 logging_writer.py:48] [457400] global_step=457400, grad_norm=3.2281155586242676, loss=1.165215253829956
I0310 18:34:02.706850 139708415854336 logging_writer.py:48] [457500] global_step=457500, grad_norm=3.0912909507751465, loss=1.1955201625823975
I0310 18:34:48.028386 139708407461632 logging_writer.py:48] [457600] global_step=457600, grad_norm=3.185384750366211, loss=1.6665494441986084
I0310 18:35:33.588036 139708415854336 logging_writer.py:48] [457700] global_step=457700, grad_norm=3.241856813430786, loss=2.8408310413360596
I0310 18:36:18.867274 139708407461632 logging_writer.py:48] [457800] global_step=457800, grad_norm=3.177708625793457, loss=2.1486198902130127
I0310 18:37:03.997400 139708415854336 logging_writer.py:48] [457900] global_step=457900, grad_norm=3.1971781253814697, loss=1.1300814151763916
I0310 18:37:50.011277 139708407461632 logging_writer.py:48] [458000] global_step=458000, grad_norm=3.2571942806243896, loss=1.1810650825500488
I0310 18:38:35.430326 139708415854336 logging_writer.py:48] [458100] global_step=458100, grad_norm=3.0501010417938232, loss=1.0237643718719482
I0310 18:39:21.126764 139708407461632 logging_writer.py:48] [458200] global_step=458200, grad_norm=3.1554548740386963, loss=1.1797279119491577
I0310 18:40:06.577200 139708415854336 logging_writer.py:48] [458300] global_step=458300, grad_norm=2.8635151386260986, loss=2.1131513118743896
I0310 18:40:09.010203 139902746892096 spec.py:321] Evaluating on the training split.
I0310 18:40:20.513771 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 18:40:41.498796 139902746892096 spec.py:349] Evaluating on the test split.
I0310 18:40:43.178741 139902746892096 submission_runner.py:411] Time since start: 221966.21s, 	Step: 458307, 	{'train/accuracy': 0.8866796493530273, 'train/loss': 0.42128390073776245, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 205055.78770446777, 'total_duration': 221966.20752811432, 'accumulated_submission_time': 205055.78770446777, 'accumulated_eval_time': 16853.451112270355, 'accumulated_logging_time': 32.97318458557129}
I0310 18:40:43.262194 139708407461632 logging_writer.py:48] [458307] accumulated_eval_time=16853.451112, accumulated_logging_time=32.973185, accumulated_submission_time=205055.787704, global_step=458307, preemption_count=0, score=205055.787704, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=221966.207528, train/accuracy=0.886680, train/loss=0.421284, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 18:41:21.000169 139708415854336 logging_writer.py:48] [458400] global_step=458400, grad_norm=3.0437045097351074, loss=1.2047061920166016
I0310 18:42:06.354200 139708407461632 logging_writer.py:48] [458500] global_step=458500, grad_norm=3.1454670429229736, loss=2.771141767501831
I0310 18:42:51.877106 139708415854336 logging_writer.py:48] [458600] global_step=458600, grad_norm=3.1393914222717285, loss=1.3914871215820312
I0310 18:43:37.414003 139708407461632 logging_writer.py:48] [458700] global_step=458700, grad_norm=3.046785593032837, loss=1.8011000156402588
I0310 18:44:22.907401 139708415854336 logging_writer.py:48] [458800] global_step=458800, grad_norm=2.9071767330169678, loss=2.2717223167419434
I0310 18:45:08.170186 139708407461632 logging_writer.py:48] [458900] global_step=458900, grad_norm=3.125443696975708, loss=1.1026581525802612
I0310 18:45:53.612063 139708415854336 logging_writer.py:48] [459000] global_step=459000, grad_norm=2.9149508476257324, loss=1.8992187976837158
I0310 18:46:38.942292 139708407461632 logging_writer.py:48] [459100] global_step=459100, grad_norm=3.236753225326538, loss=1.064092755317688
I0310 18:47:24.682204 139708415854336 logging_writer.py:48] [459200] global_step=459200, grad_norm=3.8439159393310547, loss=3.1486904621124268
I0310 18:47:43.202726 139902746892096 spec.py:321] Evaluating on the training split.
I0310 18:47:54.746469 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 18:48:16.470193 139902746892096 spec.py:349] Evaluating on the test split.
I0310 18:48:18.141806 139902746892096 submission_runner.py:411] Time since start: 222421.17s, 	Step: 459242, 	{'train/accuracy': 0.8888671398162842, 'train/loss': 0.4136143922805786, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 205475.67133545876, 'total_duration': 222421.1706006527, 'accumulated_submission_time': 205475.67133545876, 'accumulated_eval_time': 16888.39018678665, 'accumulated_logging_time': 33.064754486083984}
I0310 18:48:18.225497 139708407461632 logging_writer.py:48] [459242] accumulated_eval_time=16888.390187, accumulated_logging_time=33.064754, accumulated_submission_time=205475.671335, global_step=459242, preemption_count=0, score=205475.671335, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=222421.170601, train/accuracy=0.888867, train/loss=0.413614, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 18:48:41.450346 139708415854336 logging_writer.py:48] [459300] global_step=459300, grad_norm=3.148900270462036, loss=2.0394721031188965
I0310 18:49:24.983170 139708407461632 logging_writer.py:48] [459400] global_step=459400, grad_norm=3.031947135925293, loss=2.405029296875
I0310 18:50:10.402716 139708415854336 logging_writer.py:48] [459500] global_step=459500, grad_norm=2.974402904510498, loss=1.1316337585449219
I0310 18:50:55.957734 139708407461632 logging_writer.py:48] [459600] global_step=459600, grad_norm=3.5310301780700684, loss=3.0503053665161133
I0310 18:51:41.150573 139708415854336 logging_writer.py:48] [459700] global_step=459700, grad_norm=3.0401763916015625, loss=1.024314522743225
I0310 18:52:26.788387 139708407461632 logging_writer.py:48] [459800] global_step=459800, grad_norm=3.054025650024414, loss=1.3203707933425903
I0310 18:53:12.449571 139708415854336 logging_writer.py:48] [459900] global_step=459900, grad_norm=2.7672998905181885, loss=1.5468354225158691
I0310 18:53:57.488500 139708407461632 logging_writer.py:48] [460000] global_step=460000, grad_norm=3.589618444442749, loss=3.0541129112243652
I0310 18:54:42.746219 139708415854336 logging_writer.py:48] [460100] global_step=460100, grad_norm=3.5754525661468506, loss=1.959543228149414
I0310 18:55:18.318243 139902746892096 spec.py:321] Evaluating on the training split.
I0310 18:55:29.837142 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 18:55:51.078222 139902746892096 spec.py:349] Evaluating on the test split.
I0310 18:55:52.756499 139902746892096 submission_runner.py:411] Time since start: 222875.79s, 	Step: 460179, 	{'train/accuracy': 0.8866991996765137, 'train/loss': 0.41916623711586, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 205895.7057659626, 'total_duration': 222875.7852590084, 'accumulated_submission_time': 205895.7057659626, 'accumulated_eval_time': 16922.828406095505, 'accumulated_logging_time': 33.157649517059326}
I0310 18:55:52.855107 139708407461632 logging_writer.py:48] [460179] accumulated_eval_time=16922.828406, accumulated_logging_time=33.157650, accumulated_submission_time=205895.705766, global_step=460179, preemption_count=0, score=205895.705766, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=222875.785259, train/accuracy=0.886699, train/loss=0.419166, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 18:56:01.517723 139708415854336 logging_writer.py:48] [460200] global_step=460200, grad_norm=3.1620869636535645, loss=1.095341444015503
I0310 18:56:42.677961 139708407461632 logging_writer.py:48] [460300] global_step=460300, grad_norm=3.1758530139923096, loss=1.2312734127044678
I0310 18:57:27.766951 139708415854336 logging_writer.py:48] [460400] global_step=460400, grad_norm=2.997464656829834, loss=1.189460039138794
I0310 18:58:13.498548 139708407461632 logging_writer.py:48] [460500] global_step=460500, grad_norm=3.38700008392334, loss=1.0933290719985962
I0310 18:58:59.726945 139708415854336 logging_writer.py:48] [460600] global_step=460600, grad_norm=3.446646213531494, loss=1.1872135400772095
I0310 18:59:45.073818 139708407461632 logging_writer.py:48] [460700] global_step=460700, grad_norm=3.996159315109253, loss=3.083770275115967
I0310 19:00:30.272296 139708415854336 logging_writer.py:48] [460800] global_step=460800, grad_norm=2.8276913166046143, loss=1.9880285263061523
I0310 19:01:15.471785 139708407461632 logging_writer.py:48] [460900] global_step=460900, grad_norm=3.2896318435668945, loss=1.1880744695663452
I0310 19:02:00.791376 139708415854336 logging_writer.py:48] [461000] global_step=461000, grad_norm=2.8904008865356445, loss=2.1453804969787598
I0310 19:02:45.894938 139708407461632 logging_writer.py:48] [461100] global_step=461100, grad_norm=2.8094193935394287, loss=1.3218255043029785
I0310 19:02:53.153895 139902746892096 spec.py:321] Evaluating on the training split.
I0310 19:03:04.620540 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 19:03:26.660322 139902746892096 spec.py:349] Evaluating on the test split.
I0310 19:03:28.330492 139902746892096 submission_runner.py:411] Time since start: 223331.36s, 	Step: 461118, 	{'train/accuracy': 0.8881054520606995, 'train/loss': 0.41157087683677673, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 206315.94537830353, 'total_duration': 223331.3592851162, 'accumulated_submission_time': 206315.94537830353, 'accumulated_eval_time': 16958.004980802536, 'accumulated_logging_time': 33.26661014556885}
I0310 19:03:28.416677 139708415854336 logging_writer.py:48] [461118] accumulated_eval_time=16958.004981, accumulated_logging_time=33.266610, accumulated_submission_time=206315.945378, global_step=461118, preemption_count=0, score=206315.945378, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=223331.359285, train/accuracy=0.888105, train/loss=0.411571, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 19:04:01.080984 139708407461632 logging_writer.py:48] [461200] global_step=461200, grad_norm=3.34830904006958, loss=2.0312328338623047
I0310 19:04:45.542295 139708415854336 logging_writer.py:48] [461300] global_step=461300, grad_norm=3.465369462966919, loss=1.1948162317276
I0310 19:05:30.957045 139708407461632 logging_writer.py:48] [461400] global_step=461400, grad_norm=3.028874635696411, loss=1.4671955108642578
I0310 19:06:16.241328 139708415854336 logging_writer.py:48] [461500] global_step=461500, grad_norm=2.9891364574432373, loss=1.84224534034729
I0310 19:07:01.430939 139708407461632 logging_writer.py:48] [461600] global_step=461600, grad_norm=3.112619638442993, loss=1.4177297353744507
I0310 19:07:47.236870 139708415854336 logging_writer.py:48] [461700] global_step=461700, grad_norm=3.15903902053833, loss=1.267295479774475
I0310 19:08:32.643053 139708407461632 logging_writer.py:48] [461800] global_step=461800, grad_norm=3.4037978649139404, loss=1.8700791597366333
I0310 19:09:17.951592 139708415854336 logging_writer.py:48] [461900] global_step=461900, grad_norm=3.590993881225586, loss=1.2021856307983398
I0310 19:10:03.522149 139708407461632 logging_writer.py:48] [462000] global_step=462000, grad_norm=2.8629791736602783, loss=1.2009426355361938
I0310 19:10:28.619304 139902746892096 spec.py:321] Evaluating on the training split.
I0310 19:10:40.132699 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 19:11:03.920501 139902746892096 spec.py:349] Evaluating on the test split.
I0310 19:11:05.585963 139902746892096 submission_runner.py:411] Time since start: 223788.61s, 	Step: 462057, 	{'train/accuracy': 0.8887499570846558, 'train/loss': 0.41572850942611694, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 206736.0901172161, 'total_duration': 223788.6147623062, 'accumulated_submission_time': 206736.0901172161, 'accumulated_eval_time': 16994.971632242203, 'accumulated_logging_time': 33.36212921142578}
I0310 19:11:05.669498 139708415854336 logging_writer.py:48] [462057] accumulated_eval_time=16994.971632, accumulated_logging_time=33.362129, accumulated_submission_time=206736.090117, global_step=462057, preemption_count=0, score=206736.090117, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=223788.614762, train/accuracy=0.888750, train/loss=0.415729, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 19:11:22.973156 139708407461632 logging_writer.py:48] [462100] global_step=462100, grad_norm=3.0694446563720703, loss=1.316113829612732
I0310 19:12:05.633906 139708415854336 logging_writer.py:48] [462200] global_step=462200, grad_norm=3.972775936126709, loss=3.252315044403076
I0310 19:12:50.987245 139708407461632 logging_writer.py:48] [462300] global_step=462300, grad_norm=3.4862167835235596, loss=2.9193739891052246
I0310 19:13:36.593023 139708415854336 logging_writer.py:48] [462400] global_step=462400, grad_norm=5.218794345855713, loss=3.347097873687744
I0310 19:14:21.768265 139708407461632 logging_writer.py:48] [462500] global_step=462500, grad_norm=3.524256944656372, loss=1.8378548622131348
I0310 19:15:07.126399 139708415854336 logging_writer.py:48] [462600] global_step=462600, grad_norm=3.008476972579956, loss=1.5354658365249634
I0310 19:15:52.512343 139708407461632 logging_writer.py:48] [462700] global_step=462700, grad_norm=3.0590734481811523, loss=1.0123999118804932
I0310 19:16:37.698346 139708415854336 logging_writer.py:48] [462800] global_step=462800, grad_norm=3.1146352291107178, loss=1.1187976598739624
I0310 19:17:23.021982 139708407461632 logging_writer.py:48] [462900] global_step=462900, grad_norm=2.979085922241211, loss=1.141276240348816
I0310 19:18:05.873366 139902746892096 spec.py:321] Evaluating on the training split.
I0310 19:18:17.694632 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 19:18:40.047675 139902746892096 spec.py:349] Evaluating on the test split.
I0310 19:18:41.719183 139902746892096 submission_runner.py:411] Time since start: 224244.75s, 	Step: 462995, 	{'train/accuracy': 0.8881054520606995, 'train/loss': 0.4158811569213867, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 207156.234416008, 'total_duration': 224244.74796533585, 'accumulated_submission_time': 207156.234416008, 'accumulated_eval_time': 17030.81745314598, 'accumulated_logging_time': 33.45702028274536}
I0310 19:18:41.813722 139708415854336 logging_writer.py:48] [462995] accumulated_eval_time=17030.817453, accumulated_logging_time=33.457020, accumulated_submission_time=207156.234416, global_step=462995, preemption_count=0, score=207156.234416, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=224244.747965, train/accuracy=0.888105, train/loss=0.415881, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 19:18:44.180277 139708407461632 logging_writer.py:48] [463000] global_step=463000, grad_norm=2.866764783859253, loss=2.0679142475128174
I0310 19:19:24.548334 139708415854336 logging_writer.py:48] [463100] global_step=463100, grad_norm=3.4176783561706543, loss=3.174481153488159
I0310 19:20:09.800601 139708407461632 logging_writer.py:48] [463200] global_step=463200, grad_norm=2.905413866043091, loss=1.4061872959136963
I0310 19:20:55.573150 139708415854336 logging_writer.py:48] [463300] global_step=463300, grad_norm=2.8424808979034424, loss=2.2751898765563965
I0310 19:21:40.945597 139708407461632 logging_writer.py:48] [463400] global_step=463400, grad_norm=3.0592591762542725, loss=1.078029751777649
I0310 19:22:26.210417 139708415854336 logging_writer.py:48] [463500] global_step=463500, grad_norm=3.1386728286743164, loss=1.232790231704712
I0310 19:23:11.808107 139708407461632 logging_writer.py:48] [463600] global_step=463600, grad_norm=3.256458044052124, loss=2.7410736083984375
I0310 19:23:57.141397 139708415854336 logging_writer.py:48] [463700] global_step=463700, grad_norm=2.972909450531006, loss=1.2016390562057495
I0310 19:24:42.574725 139708407461632 logging_writer.py:48] [463800] global_step=463800, grad_norm=3.1635098457336426, loss=2.535461187362671
I0310 19:25:27.995344 139708415854336 logging_writer.py:48] [463900] global_step=463900, grad_norm=3.128347396850586, loss=1.1808382272720337
I0310 19:25:41.826108 139902746892096 spec.py:321] Evaluating on the training split.
I0310 19:25:53.278069 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 19:26:15.815386 139902746892096 spec.py:349] Evaluating on the test split.
I0310 19:26:17.480945 139902746892096 submission_runner.py:411] Time since start: 224700.51s, 	Step: 463932, 	{'train/accuracy': 0.8861132860183716, 'train/loss': 0.41760244965553284, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 207576.18787121773, 'total_duration': 224700.50974440575, 'accumulated_submission_time': 207576.18787121773, 'accumulated_eval_time': 17066.472289562225, 'accumulated_logging_time': 33.561460733413696}
I0310 19:26:17.564288 139708407461632 logging_writer.py:48] [463932] accumulated_eval_time=17066.472290, accumulated_logging_time=33.561461, accumulated_submission_time=207576.187871, global_step=463932, preemption_count=0, score=207576.187871, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=224700.509744, train/accuracy=0.886113, train/loss=0.417602, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 19:26:44.709957 139708415854336 logging_writer.py:48] [464000] global_step=464000, grad_norm=2.928515672683716, loss=1.274334192276001
I0310 19:27:29.055771 139708407461632 logging_writer.py:48] [464100] global_step=464100, grad_norm=3.1145098209381104, loss=1.9257853031158447
I0310 19:28:15.070761 139708415854336 logging_writer.py:48] [464200] global_step=464200, grad_norm=3.87164306640625, loss=1.041624665260315
I0310 19:29:00.906368 139708407461632 logging_writer.py:48] [464300] global_step=464300, grad_norm=3.88189959526062, loss=3.292006254196167
I0310 19:29:46.122937 139708415854336 logging_writer.py:48] [464400] global_step=464400, grad_norm=3.14353084564209, loss=1.4348044395446777
I0310 19:30:31.690640 139708407461632 logging_writer.py:48] [464500] global_step=464500, grad_norm=3.2386488914489746, loss=1.4216467142105103
I0310 19:31:17.026452 139708415854336 logging_writer.py:48] [464600] global_step=464600, grad_norm=3.033806324005127, loss=1.1168826818466187
I0310 19:32:02.382744 139708407461632 logging_writer.py:48] [464700] global_step=464700, grad_norm=2.9563703536987305, loss=1.4280726909637451
I0310 19:32:48.017233 139708415854336 logging_writer.py:48] [464800] global_step=464800, grad_norm=3.0872156620025635, loss=1.1477476358413696
I0310 19:33:17.753045 139902746892096 spec.py:321] Evaluating on the training split.
I0310 19:33:29.204314 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 19:33:50.558478 139902746892096 spec.py:349] Evaluating on the test split.
I0310 19:33:52.222470 139902746892096 submission_runner.py:411] Time since start: 225155.25s, 	Step: 464867, 	{'train/accuracy': 0.889941394329071, 'train/loss': 0.4139235317707062, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 207996.31915855408, 'total_duration': 225155.25125455856, 'accumulated_submission_time': 207996.31915855408, 'accumulated_eval_time': 17100.941737413406, 'accumulated_logging_time': 33.65341758728027}
I0310 19:33:52.308241 139708407461632 logging_writer.py:48] [464867] accumulated_eval_time=17100.941737, accumulated_logging_time=33.653418, accumulated_submission_time=207996.319159, global_step=464867, preemption_count=0, score=207996.319159, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=225155.251255, train/accuracy=0.889941, train/loss=0.413924, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 19:34:05.683299 139708415854336 logging_writer.py:48] [464900] global_step=464900, grad_norm=3.1324524879455566, loss=1.1215109825134277
I0310 19:34:47.417260 139708407461632 logging_writer.py:48] [465000] global_step=465000, grad_norm=2.9186301231384277, loss=1.482743263244629
I0310 19:35:32.704973 139708415854336 logging_writer.py:48] [465100] global_step=465100, grad_norm=3.129314661026001, loss=2.1133739948272705
I0310 19:36:17.931980 139708407461632 logging_writer.py:48] [465200] global_step=465200, grad_norm=3.2517294883728027, loss=1.2092665433883667
I0310 19:37:03.590845 139708415854336 logging_writer.py:48] [465300] global_step=465300, grad_norm=3.1187705993652344, loss=2.78068470954895
I0310 19:37:48.878616 139708407461632 logging_writer.py:48] [465400] global_step=465400, grad_norm=3.031071662902832, loss=1.1074696779251099
I0310 19:38:35.186525 139708415854336 logging_writer.py:48] [465500] global_step=465500, grad_norm=3.5337507724761963, loss=2.736947536468506
I0310 19:39:20.548662 139708407461632 logging_writer.py:48] [465600] global_step=465600, grad_norm=3.648676633834839, loss=1.1926207542419434
I0310 19:40:05.953201 139708415854336 logging_writer.py:48] [465700] global_step=465700, grad_norm=3.7670936584472656, loss=1.1678386926651
I0310 19:40:51.393905 139708407461632 logging_writer.py:48] [465800] global_step=465800, grad_norm=3.348544120788574, loss=3.0431385040283203
I0310 19:40:52.429626 139902746892096 spec.py:321] Evaluating on the training split.
I0310 19:41:03.870530 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 19:41:26.564088 139902746892096 spec.py:349] Evaluating on the test split.
I0310 19:41:28.235362 139902746892096 submission_runner.py:411] Time since start: 225611.26s, 	Step: 465804, 	{'train/accuracy': 0.8892577886581421, 'train/loss': 0.4131757915019989, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 208416.38221096992, 'total_duration': 225611.26416301727, 'accumulated_submission_time': 208416.38221096992, 'accumulated_eval_time': 17136.74747443199, 'accumulated_logging_time': 33.74782729148865}
I0310 19:41:28.319892 139708415854336 logging_writer.py:48] [465804] accumulated_eval_time=17136.747474, accumulated_logging_time=33.747827, accumulated_submission_time=208416.382211, global_step=465804, preemption_count=0, score=208416.382211, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=225611.264163, train/accuracy=0.889258, train/loss=0.413176, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 19:42:07.057474 139708407461632 logging_writer.py:48] [465900] global_step=465900, grad_norm=3.0307118892669678, loss=1.0935239791870117
I0310 19:42:52.269295 139708415854336 logging_writer.py:48] [466000] global_step=466000, grad_norm=3.221174955368042, loss=2.6976163387298584
I0310 19:43:37.830907 139708407461632 logging_writer.py:48] [466100] global_step=466100, grad_norm=3.072756290435791, loss=1.0910392999649048
I0310 19:44:23.166107 139708415854336 logging_writer.py:48] [466200] global_step=466200, grad_norm=2.9129884243011475, loss=2.0505123138427734
I0310 19:45:08.295561 139708407461632 logging_writer.py:48] [466300] global_step=466300, grad_norm=2.9370760917663574, loss=1.4695658683776855
I0310 19:45:53.800109 139708415854336 logging_writer.py:48] [466400] global_step=466400, grad_norm=5.376997470855713, loss=3.1935830116271973
I0310 19:46:39.010744 139708407461632 logging_writer.py:48] [466500] global_step=466500, grad_norm=3.040086269378662, loss=1.4718376398086548
I0310 19:47:24.272352 139708415854336 logging_writer.py:48] [466600] global_step=466600, grad_norm=3.001277446746826, loss=1.0992536544799805
I0310 19:48:10.280550 139708407461632 logging_writer.py:48] [466700] global_step=466700, grad_norm=4.053226470947266, loss=3.2480409145355225
I0310 19:48:28.426971 139902746892096 spec.py:321] Evaluating on the training split.
I0310 19:48:39.964571 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 19:49:03.021699 139902746892096 spec.py:349] Evaluating on the test split.
I0310 19:49:04.696599 139902746892096 submission_runner.py:411] Time since start: 226067.73s, 	Step: 466742, 	{'train/accuracy': 0.8897460699081421, 'train/loss': 0.40749502182006836, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 208836.4301431179, 'total_duration': 226067.72537398338, 'accumulated_submission_time': 208836.4301431179, 'accumulated_eval_time': 17173.017083644867, 'accumulated_logging_time': 33.8416645526886}
I0310 19:49:04.793489 139708415854336 logging_writer.py:48] [466742] accumulated_eval_time=17173.017084, accumulated_logging_time=33.841665, accumulated_submission_time=208836.430143, global_step=466742, preemption_count=0, score=208836.430143, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=226067.725374, train/accuracy=0.889746, train/loss=0.407495, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 19:49:28.010704 139708407461632 logging_writer.py:48] [466800] global_step=466800, grad_norm=3.163120985031128, loss=1.041691780090332
I0310 19:50:11.334977 139708415854336 logging_writer.py:48] [466900] global_step=466900, grad_norm=3.3064985275268555, loss=2.7242932319641113
I0310 19:50:56.648686 139708407461632 logging_writer.py:48] [467000] global_step=467000, grad_norm=3.080197811126709, loss=1.3836514949798584
I0310 19:51:42.148170 139708415854336 logging_writer.py:48] [467100] global_step=467100, grad_norm=4.4202141761779785, loss=3.3715858459472656
I0310 19:52:27.556521 139708407461632 logging_writer.py:48] [467200] global_step=467200, grad_norm=5.590909957885742, loss=1.063386082649231
I0310 19:53:13.027132 139708415854336 logging_writer.py:48] [467300] global_step=467300, grad_norm=3.1248061656951904, loss=1.5528957843780518
I0310 19:53:58.342311 139708407461632 logging_writer.py:48] [467400] global_step=467400, grad_norm=3.6599700450897217, loss=1.214653730392456
I0310 19:54:43.419469 139708415854336 logging_writer.py:48] [467500] global_step=467500, grad_norm=3.177931308746338, loss=1.113919973373413
I0310 19:55:28.449760 139708407461632 logging_writer.py:48] [467600] global_step=467600, grad_norm=3.010988473892212, loss=1.7927316427230835
I0310 19:56:05.242955 139902746892096 spec.py:321] Evaluating on the training split.
I0310 19:56:17.029392 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 19:56:38.523737 139902746892096 spec.py:349] Evaluating on the test split.
I0310 19:56:40.193439 139902746892096 submission_runner.py:411] Time since start: 226523.22s, 	Step: 467682, 	{'train/accuracy': 0.88929682970047, 'train/loss': 0.41145119071006775, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 209256.81998324394, 'total_duration': 226523.2222290039, 'accumulated_submission_time': 209256.81998324394, 'accumulated_eval_time': 17207.967556238174, 'accumulated_logging_time': 33.94882917404175}
I0310 19:56:40.283370 139708415854336 logging_writer.py:48] [467682] accumulated_eval_time=17207.967556, accumulated_logging_time=33.948829, accumulated_submission_time=209256.819983, global_step=467682, preemption_count=0, score=209256.819983, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=226523.222229, train/accuracy=0.889297, train/loss=0.411451, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 19:56:47.753385 139708407461632 logging_writer.py:48] [467700] global_step=467700, grad_norm=3.346599817276001, loss=1.1009204387664795
I0310 19:57:29.135703 139708415854336 logging_writer.py:48] [467800] global_step=467800, grad_norm=3.050182342529297, loss=1.1091102361679077
I0310 19:58:14.353958 139708407461632 logging_writer.py:48] [467900] global_step=467900, grad_norm=3.2606723308563232, loss=1.068730115890503
I0310 19:59:00.026686 139708415854336 logging_writer.py:48] [468000] global_step=468000, grad_norm=2.9428749084472656, loss=1.0696322917938232
I0310 19:59:45.271755 139708407461632 logging_writer.py:48] [468100] global_step=468100, grad_norm=3.094475030899048, loss=1.1622066497802734
I0310 20:00:30.495649 139708415854336 logging_writer.py:48] [468200] global_step=468200, grad_norm=3.2662618160247803, loss=2.76871919631958
I0310 20:01:16.131283 139708407461632 logging_writer.py:48] [468300] global_step=468300, grad_norm=3.1775970458984375, loss=1.0852428674697876
I0310 20:02:01.729996 139708415854336 logging_writer.py:48] [468400] global_step=468400, grad_norm=3.8907008171081543, loss=3.2639944553375244
I0310 20:02:47.160703 139708407461632 logging_writer.py:48] [468500] global_step=468500, grad_norm=3.5513980388641357, loss=3.005951404571533
I0310 20:03:32.603739 139708415854336 logging_writer.py:48] [468600] global_step=468600, grad_norm=2.9630634784698486, loss=1.0848195552825928
I0310 20:03:40.392611 139902746892096 spec.py:321] Evaluating on the training split.
I0310 20:03:52.646524 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 20:04:12.996352 139902746892096 spec.py:349] Evaluating on the test split.
I0310 20:04:14.681874 139902746892096 submission_runner.py:411] Time since start: 226977.71s, 	Step: 468619, 	{'train/accuracy': 0.8893749713897705, 'train/loss': 0.4095381200313568, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 209676.87094807625, 'total_duration': 226977.71065473557, 'accumulated_submission_time': 209676.87094807625, 'accumulated_eval_time': 17242.25678539276, 'accumulated_logging_time': 34.04836964607239}
I0310 20:04:14.776276 139708407461632 logging_writer.py:48] [468619] accumulated_eval_time=17242.256785, accumulated_logging_time=34.048370, accumulated_submission_time=209676.870948, global_step=468619, preemption_count=0, score=209676.870948, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=226977.710655, train/accuracy=0.889375, train/loss=0.409538, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 20:04:47.049952 139708415854336 logging_writer.py:48] [468700] global_step=468700, grad_norm=3.755206823348999, loss=1.1456406116485596
I0310 20:05:31.830212 139708407461632 logging_writer.py:48] [468800] global_step=468800, grad_norm=3.947096347808838, loss=3.2192773818969727
I0310 20:06:17.006278 139708415854336 logging_writer.py:48] [468900] global_step=468900, grad_norm=3.409250497817993, loss=2.871922254562378
I0310 20:07:02.352347 139708407461632 logging_writer.py:48] [469000] global_step=469000, grad_norm=3.0595858097076416, loss=2.6446568965911865
I0310 20:07:47.577946 139708415854336 logging_writer.py:48] [469100] global_step=469100, grad_norm=3.2044105529785156, loss=2.638104200363159
I0310 20:08:32.991930 139708407461632 logging_writer.py:48] [469200] global_step=469200, grad_norm=3.052727699279785, loss=1.113426923751831
I0310 20:09:18.381908 139708415854336 logging_writer.py:48] [469300] global_step=469300, grad_norm=2.9665780067443848, loss=1.2328883409500122
I0310 20:10:03.782069 139708407461632 logging_writer.py:48] [469400] global_step=469400, grad_norm=3.4959211349487305, loss=3.130110263824463
I0310 20:10:49.271136 139708415854336 logging_writer.py:48] [469500] global_step=469500, grad_norm=3.3006234169006348, loss=1.0819084644317627
I0310 20:11:15.085345 139902746892096 spec.py:321] Evaluating on the training split.
I0310 20:11:26.629786 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 20:11:48.287237 139902746892096 spec.py:349] Evaluating on the test split.
I0310 20:11:49.963583 139902746892096 submission_runner.py:411] Time since start: 227432.99s, 	Step: 469559, 	{'train/accuracy': 0.8883788585662842, 'train/loss': 0.41384100914001465, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 210097.1221292019, 'total_duration': 227432.9923608303, 'accumulated_submission_time': 210097.1221292019, 'accumulated_eval_time': 17277.135021448135, 'accumulated_logging_time': 34.15190577507019}
I0310 20:11:50.059261 139708407461632 logging_writer.py:48] [469559] accumulated_eval_time=17277.135021, accumulated_logging_time=34.151906, accumulated_submission_time=210097.122129, global_step=469559, preemption_count=0, score=210097.122129, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=227432.992361, train/accuracy=0.888379, train/loss=0.413841, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 20:12:06.597193 139708415854336 logging_writer.py:48] [469600] global_step=469600, grad_norm=3.961087465286255, loss=3.1476893424987793
I0310 20:12:49.196315 139708407461632 logging_writer.py:48] [469700] global_step=469700, grad_norm=3.475206136703491, loss=3.0175421237945557
I0310 20:13:34.868860 139708415854336 logging_writer.py:48] [469800] global_step=469800, grad_norm=3.2020938396453857, loss=2.0141754150390625
I0310 20:14:20.298873 139708407461632 logging_writer.py:48] [469900] global_step=469900, grad_norm=2.9256181716918945, loss=1.6339306831359863
I0310 20:15:05.554479 139708415854336 logging_writer.py:48] [470000] global_step=470000, grad_norm=3.1057138442993164, loss=1.1249761581420898
I0310 20:15:51.321326 139708407461632 logging_writer.py:48] [470100] global_step=470100, grad_norm=2.6811585426330566, loss=1.059707760810852
I0310 20:16:36.952341 139708415854336 logging_writer.py:48] [470200] global_step=470200, grad_norm=3.2470970153808594, loss=2.9884607791900635
I0310 20:17:22.489848 139708407461632 logging_writer.py:48] [470300] global_step=470300, grad_norm=3.536808490753174, loss=1.2040146589279175
I0310 20:18:08.134179 139708415854336 logging_writer.py:48] [470400] global_step=470400, grad_norm=3.017228364944458, loss=2.2835474014282227
I0310 20:18:50.181777 139902746892096 spec.py:321] Evaluating on the training split.
I0310 20:19:01.550747 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 20:19:24.464915 139902746892096 spec.py:349] Evaluating on the test split.
I0310 20:19:26.137104 139902746892096 submission_runner.py:411] Time since start: 227889.17s, 	Step: 470494, 	{'train/accuracy': 0.8877343535423279, 'train/loss': 0.41281256079673767, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 210517.18369412422, 'total_duration': 227889.16590118408, 'accumulated_submission_time': 210517.18369412422, 'accumulated_eval_time': 17313.09035563469, 'accumulated_logging_time': 34.259888887405396}
I0310 20:19:26.223444 139708407461632 logging_writer.py:48] [470494] accumulated_eval_time=17313.090356, accumulated_logging_time=34.259889, accumulated_submission_time=210517.183694, global_step=470494, preemption_count=0, score=210517.183694, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=227889.165901, train/accuracy=0.887734, train/loss=0.412813, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 20:19:28.983865 139708415854336 logging_writer.py:48] [470500] global_step=470500, grad_norm=3.6162056922912598, loss=1.2672350406646729
I0310 20:20:08.855907 139708407461632 logging_writer.py:48] [470600] global_step=470600, grad_norm=3.0506694316864014, loss=1.3189709186553955
I0310 20:20:54.080663 139708415854336 logging_writer.py:48] [470700] global_step=470700, grad_norm=2.9178524017333984, loss=1.28843092918396
I0310 20:21:39.719708 139708407461632 logging_writer.py:48] [470800] global_step=470800, grad_norm=3.074829339981079, loss=2.2738969326019287
I0310 20:22:25.405112 139708415854336 logging_writer.py:48] [470900] global_step=470900, grad_norm=2.9962828159332275, loss=1.1111568212509155
I0310 20:23:10.446048 139708407461632 logging_writer.py:48] [471000] global_step=471000, grad_norm=3.235729217529297, loss=1.1754138469696045
I0310 20:23:55.840960 139708415854336 logging_writer.py:48] [471100] global_step=471100, grad_norm=2.9060187339782715, loss=1.695732593536377
I0310 20:24:40.983435 139708407461632 logging_writer.py:48] [471200] global_step=471200, grad_norm=3.392587184906006, loss=1.157320261001587
I0310 20:25:26.298706 139708415854336 logging_writer.py:48] [471300] global_step=471300, grad_norm=2.8545823097229004, loss=1.0755696296691895
I0310 20:26:11.476481 139708407461632 logging_writer.py:48] [471400] global_step=471400, grad_norm=3.155125856399536, loss=2.494745969772339
I0310 20:26:26.421647 139902746892096 spec.py:321] Evaluating on the training split.
I0310 20:26:37.734065 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 20:27:01.081324 139902746892096 spec.py:349] Evaluating on the test split.
I0310 20:27:02.744833 139902746892096 submission_runner.py:411] Time since start: 228345.77s, 	Step: 471435, 	{'train/accuracy': 0.8878124952316284, 'train/loss': 0.4137357473373413, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 210937.3222939968, 'total_duration': 228345.77362632751, 'accumulated_submission_time': 210937.3222939968, 'accumulated_eval_time': 17349.413534879684, 'accumulated_logging_time': 34.35630774497986}
I0310 20:27:02.829402 139708415854336 logging_writer.py:48] [471435] accumulated_eval_time=17349.413535, accumulated_logging_time=34.356308, accumulated_submission_time=210937.322294, global_step=471435, preemption_count=0, score=210937.322294, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=228345.773626, train/accuracy=0.887812, train/loss=0.413736, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 20:27:28.803059 139708407461632 logging_writer.py:48] [471500] global_step=471500, grad_norm=3.3452556133270264, loss=2.9728193283081055
I0310 20:28:12.425928 139708415854336 logging_writer.py:48] [471600] global_step=471600, grad_norm=3.298950433731079, loss=2.8740978240966797
I0310 20:28:58.289366 139708407461632 logging_writer.py:48] [471700] global_step=471700, grad_norm=3.577326774597168, loss=3.1922059059143066
I0310 20:29:43.588158 139708415854336 logging_writer.py:48] [471800] global_step=471800, grad_norm=3.5704894065856934, loss=2.909029960632324
I0310 20:30:28.513480 139708407461632 logging_writer.py:48] [471900] global_step=471900, grad_norm=2.94905424118042, loss=1.680274486541748
I0310 20:31:13.835713 139708415854336 logging_writer.py:48] [472000] global_step=472000, grad_norm=4.062570571899414, loss=3.219414234161377
I0310 20:31:59.017103 139708407461632 logging_writer.py:48] [472100] global_step=472100, grad_norm=2.967029094696045, loss=2.3932244777679443
I0310 20:32:44.242819 139708415854336 logging_writer.py:48] [472200] global_step=472200, grad_norm=2.981851816177368, loss=1.1193299293518066
I0310 20:33:29.449751 139708407461632 logging_writer.py:48] [472300] global_step=472300, grad_norm=3.39782452583313, loss=2.646771192550659
I0310 20:34:03.124255 139902746892096 spec.py:321] Evaluating on the training split.
I0310 20:34:14.555567 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 20:34:36.403162 139902746892096 spec.py:349] Evaluating on the test split.
I0310 20:34:38.079485 139902746892096 submission_runner.py:411] Time since start: 228801.11s, 	Step: 472376, 	{'train/accuracy': 0.8859961032867432, 'train/loss': 0.42180758714675903, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 211357.55799984932, 'total_duration': 228801.10828518867, 'accumulated_submission_time': 211357.55799984932, 'accumulated_eval_time': 17384.368765592575, 'accumulated_logging_time': 34.45077323913574}
I0310 20:34:38.165612 139708415854336 logging_writer.py:48] [472376] accumulated_eval_time=17384.368766, accumulated_logging_time=34.450773, accumulated_submission_time=211357.558000, global_step=472376, preemption_count=0, score=211357.558000, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=228801.108285, train/accuracy=0.885996, train/loss=0.421808, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 20:34:48.006560 139708407461632 logging_writer.py:48] [472400] global_step=472400, grad_norm=3.492500066757202, loss=3.1349823474884033
I0310 20:35:29.781106 139708415854336 logging_writer.py:48] [472500] global_step=472500, grad_norm=2.9825901985168457, loss=1.501161813735962
I0310 20:36:14.702777 139708407461632 logging_writer.py:48] [472600] global_step=472600, grad_norm=3.1491236686706543, loss=1.1344847679138184
I0310 20:37:00.003132 139708415854336 logging_writer.py:48] [472700] global_step=472700, grad_norm=3.1045145988464355, loss=1.1546220779418945
I0310 20:37:45.465386 139708407461632 logging_writer.py:48] [472800] global_step=472800, grad_norm=3.8517463207244873, loss=3.1105384826660156
I0310 20:38:30.557960 139708415854336 logging_writer.py:48] [472900] global_step=472900, grad_norm=2.958021879196167, loss=2.241255760192871
I0310 20:39:16.049130 139708407461632 logging_writer.py:48] [473000] global_step=473000, grad_norm=4.241323947906494, loss=3.3172342777252197
I0310 20:40:01.204391 139708415854336 logging_writer.py:48] [473100] global_step=473100, grad_norm=3.2869176864624023, loss=1.1260325908660889
I0310 20:40:46.503748 139708407461632 logging_writer.py:48] [473200] global_step=473200, grad_norm=3.12652587890625, loss=1.0616408586502075
I0310 20:41:32.020984 139708415854336 logging_writer.py:48] [473300] global_step=473300, grad_norm=3.2693324089050293, loss=2.0181050300598145
I0310 20:41:38.361549 139902746892096 spec.py:321] Evaluating on the training split.
I0310 20:41:50.021947 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 20:42:12.448385 139902746892096 spec.py:349] Evaluating on the test split.
I0310 20:42:14.116541 139902746892096 submission_runner.py:411] Time since start: 229257.15s, 	Step: 473316, 	{'train/accuracy': 0.8886523246765137, 'train/loss': 0.41294628381729126, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 211777.69554495811, 'total_duration': 229257.14534235, 'accumulated_submission_time': 211777.69554495811, 'accumulated_eval_time': 17420.123772144318, 'accumulated_logging_time': 34.5459508895874}
I0310 20:42:14.201956 139708407461632 logging_writer.py:48] [473316] accumulated_eval_time=17420.123772, accumulated_logging_time=34.545951, accumulated_submission_time=211777.695545, global_step=473316, preemption_count=0, score=211777.695545, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=229257.145342, train/accuracy=0.888652, train/loss=0.412946, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 20:42:47.620347 139708415854336 logging_writer.py:48] [473400] global_step=473400, grad_norm=3.2511541843414307, loss=1.327604055404663
I0310 20:43:32.677383 139708407461632 logging_writer.py:48] [473500] global_step=473500, grad_norm=2.951314926147461, loss=2.3762218952178955
I0310 20:44:17.960267 139708415854336 logging_writer.py:48] [473600] global_step=473600, grad_norm=2.9097838401794434, loss=0.9733250141143799
I0310 20:45:03.330442 139708407461632 logging_writer.py:48] [473700] global_step=473700, grad_norm=3.8143904209136963, loss=3.1603825092315674
I0310 20:45:48.476058 139708415854336 logging_writer.py:48] [473800] global_step=473800, grad_norm=2.7993645668029785, loss=1.563413143157959
I0310 20:46:33.983802 139708407461632 logging_writer.py:48] [473900] global_step=473900, grad_norm=3.042057514190674, loss=1.833189845085144
I0310 20:47:19.400287 139708415854336 logging_writer.py:48] [474000] global_step=474000, grad_norm=3.6256489753723145, loss=2.842599630355835
I0310 20:48:04.808022 139708407461632 logging_writer.py:48] [474100] global_step=474100, grad_norm=2.947817325592041, loss=1.127787709236145
I0310 20:48:50.361179 139708415854336 logging_writer.py:48] [474200] global_step=474200, grad_norm=2.8355021476745605, loss=2.1126413345336914
I0310 20:49:14.499146 139902746892096 spec.py:321] Evaluating on the training split.
I0310 20:49:26.007769 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 20:49:46.343941 139902746892096 spec.py:349] Evaluating on the test split.
I0310 20:49:48.019291 139902746892096 submission_runner.py:411] Time since start: 229711.05s, 	Step: 474255, 	{'train/accuracy': 0.8865429759025574, 'train/loss': 0.4211982488632202, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 212197.9356815815, 'total_duration': 229711.0480697155, 'accumulated_submission_time': 212197.9356815815, 'accumulated_eval_time': 17453.64389872551, 'accumulated_logging_time': 34.639634132385254}
I0310 20:49:48.142394 139708407461632 logging_writer.py:48] [474255] accumulated_eval_time=17453.643899, accumulated_logging_time=34.639634, accumulated_submission_time=212197.935682, global_step=474255, preemption_count=0, score=212197.935682, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=229711.048070, train/accuracy=0.886543, train/loss=0.421198, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 20:50:06.250508 139708415854336 logging_writer.py:48] [474300] global_step=474300, grad_norm=3.0949599742889404, loss=1.163172960281372
I0310 20:50:49.364252 139708407461632 logging_writer.py:48] [474400] global_step=474400, grad_norm=3.0964298248291016, loss=1.2154381275177002
I0310 20:51:34.638490 139708415854336 logging_writer.py:48] [474500] global_step=474500, grad_norm=3.1638031005859375, loss=1.0512721538543701
I0310 20:52:20.511436 139708407461632 logging_writer.py:48] [474600] global_step=474600, grad_norm=3.107203722000122, loss=1.5723016262054443
I0310 20:53:05.586073 139708415854336 logging_writer.py:48] [474700] global_step=474700, grad_norm=3.3605799674987793, loss=1.8820677995681763
I0310 20:53:51.035476 139708407461632 logging_writer.py:48] [474800] global_step=474800, grad_norm=3.1785171031951904, loss=1.1299238204956055
I0310 20:54:36.421448 139708415854336 logging_writer.py:48] [474900] global_step=474900, grad_norm=3.214022636413574, loss=1.8458470106124878
I0310 20:55:21.944324 139708407461632 logging_writer.py:48] [475000] global_step=475000, grad_norm=3.168883800506592, loss=1.1236618757247925
I0310 20:56:07.376767 139708415854336 logging_writer.py:48] [475100] global_step=475100, grad_norm=2.9290382862091064, loss=1.0538500547409058
I0310 20:56:48.159674 139902746892096 spec.py:321] Evaluating on the training split.
I0310 20:56:59.734441 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 20:57:22.542072 139902746892096 spec.py:349] Evaluating on the test split.
I0310 20:57:24.217809 139902746892096 submission_runner.py:411] Time since start: 230167.25s, 	Step: 475192, 	{'train/accuracy': 0.888671875, 'train/loss': 0.4101220667362213, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 212617.89036035538, 'total_duration': 230167.2466096878, 'accumulated_submission_time': 212617.89036035538, 'accumulated_eval_time': 17489.702049016953, 'accumulated_logging_time': 34.77641224861145}
I0310 20:57:24.303248 139708407461632 logging_writer.py:48] [475192] accumulated_eval_time=17489.702049, accumulated_logging_time=34.776412, accumulated_submission_time=212617.890360, global_step=475192, preemption_count=0, score=212617.890360, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=230167.246610, train/accuracy=0.888672, train/loss=0.410122, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 20:57:27.843619 139708415854336 logging_writer.py:48] [475200] global_step=475200, grad_norm=3.016268253326416, loss=1.2148233652114868
I0310 20:58:08.112414 139708407461632 logging_writer.py:48] [475300] global_step=475300, grad_norm=3.0653488636016846, loss=1.1314128637313843
I0310 20:58:53.229234 139708415854336 logging_writer.py:48] [475400] global_step=475400, grad_norm=3.1057491302490234, loss=1.8809528350830078
I0310 20:59:38.814066 139708407461632 logging_writer.py:48] [475500] global_step=475500, grad_norm=3.2894861698150635, loss=2.650538444519043
I0310 21:00:23.973443 139708415854336 logging_writer.py:48] [475600] global_step=475600, grad_norm=3.3395988941192627, loss=2.0709447860717773
I0310 21:01:09.208532 139708407461632 logging_writer.py:48] [475700] global_step=475700, grad_norm=3.421186923980713, loss=1.151105523109436
I0310 21:01:54.484610 139708415854336 logging_writer.py:48] [475800] global_step=475800, grad_norm=3.2938807010650635, loss=1.1948646306991577
I0310 21:02:39.745322 139708407461632 logging_writer.py:48] [475900] global_step=475900, grad_norm=3.087944984436035, loss=2.104569673538208
I0310 21:03:24.927209 139708415854336 logging_writer.py:48] [476000] global_step=476000, grad_norm=3.166283130645752, loss=1.143746018409729
I0310 21:04:10.034471 139708407461632 logging_writer.py:48] [476100] global_step=476100, grad_norm=3.5636279582977295, loss=2.857154130935669
I0310 21:04:24.573128 139902746892096 spec.py:321] Evaluating on the training split.
I0310 21:04:35.831698 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 21:04:57.397286 139902746892096 spec.py:349] Evaluating on the test split.
I0310 21:04:59.079347 139902746892096 submission_runner.py:411] Time since start: 230622.11s, 	Step: 476134, 	{'train/accuracy': 0.8873632550239563, 'train/loss': 0.415277898311615, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 213038.1022245884, 'total_duration': 230622.10814762115, 'accumulated_submission_time': 213038.1022245884, 'accumulated_eval_time': 17524.20827102661, 'accumulated_logging_time': 34.870837688446045}
I0310 21:04:59.164643 139708415854336 logging_writer.py:48] [476134] accumulated_eval_time=17524.208271, accumulated_logging_time=34.870838, accumulated_submission_time=213038.102225, global_step=476134, preemption_count=0, score=213038.102225, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=230622.108148, train/accuracy=0.887363, train/loss=0.415278, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 21:05:25.539655 139708407461632 logging_writer.py:48] [476200] global_step=476200, grad_norm=3.077721357345581, loss=1.1313976049423218
I0310 21:06:09.458866 139708415854336 logging_writer.py:48] [476300] global_step=476300, grad_norm=4.251828193664551, loss=3.2089176177978516
I0310 21:06:54.653356 139708407461632 logging_writer.py:48] [476400] global_step=476400, grad_norm=3.4292995929718018, loss=1.3037337064743042
I0310 21:07:40.220173 139708415854336 logging_writer.py:48] [476500] global_step=476500, grad_norm=3.3600950241088867, loss=1.1841280460357666
I0310 21:08:25.214770 139708407461632 logging_writer.py:48] [476600] global_step=476600, grad_norm=3.8205220699310303, loss=2.904066562652588
I0310 21:09:10.594633 139708415854336 logging_writer.py:48] [476700] global_step=476700, grad_norm=2.9533884525299072, loss=1.2960747480392456
I0310 21:09:55.999811 139708407461632 logging_writer.py:48] [476800] global_step=476800, grad_norm=3.1144089698791504, loss=1.0924396514892578
I0310 21:10:41.179736 139708415854336 logging_writer.py:48] [476900] global_step=476900, grad_norm=3.110599994659424, loss=1.2149492502212524
I0310 21:11:26.482410 139708407461632 logging_writer.py:48] [477000] global_step=477000, grad_norm=3.6287364959716797, loss=3.2517309188842773
I0310 21:11:59.242008 139902746892096 spec.py:321] Evaluating on the training split.
I0310 21:12:10.634870 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 21:12:32.436064 139902746892096 spec.py:349] Evaluating on the test split.
I0310 21:12:34.103617 139902746892096 submission_runner.py:411] Time since start: 231077.13s, 	Step: 477074, 	{'train/accuracy': 0.8871874809265137, 'train/loss': 0.4179157018661499, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 213458.12177467346, 'total_duration': 231077.1324160099, 'accumulated_submission_time': 213458.12177467346, 'accumulated_eval_time': 17559.069897413254, 'accumulated_logging_time': 34.964377880096436}
I0310 21:12:34.189895 139708415854336 logging_writer.py:48] [477074] accumulated_eval_time=17559.069897, accumulated_logging_time=34.964378, accumulated_submission_time=213458.121775, global_step=477074, preemption_count=0, score=213458.121775, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=231077.132416, train/accuracy=0.887187, train/loss=0.417916, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 21:12:44.819968 139708407461632 logging_writer.py:48] [477100] global_step=477100, grad_norm=3.306145191192627, loss=2.7395856380462646
I0310 21:13:26.497215 139708415854336 logging_writer.py:48] [477200] global_step=477200, grad_norm=3.1658146381378174, loss=1.1149288415908813
I0310 21:14:11.798699 139708407461632 logging_writer.py:48] [477300] global_step=477300, grad_norm=2.8156652450561523, loss=1.6779117584228516
I0310 21:14:57.268668 139708415854336 logging_writer.py:48] [477400] global_step=477400, grad_norm=3.0745599269866943, loss=1.0885803699493408
I0310 21:15:42.391457 139708407461632 logging_writer.py:48] [477500] global_step=477500, grad_norm=3.013789415359497, loss=2.556906223297119
I0310 21:16:27.520586 139708415854336 logging_writer.py:48] [477600] global_step=477600, grad_norm=3.375582695007324, loss=2.6314659118652344
I0310 21:17:12.768335 139708407461632 logging_writer.py:48] [477700] global_step=477700, grad_norm=3.3157687187194824, loss=1.3502953052520752
I0310 21:17:58.173888 139708415854336 logging_writer.py:48] [477800] global_step=477800, grad_norm=3.28236985206604, loss=1.1426455974578857
I0310 21:18:43.024183 139708407461632 logging_writer.py:48] [477900] global_step=477900, grad_norm=3.893784761428833, loss=3.3123679161071777
I0310 21:19:28.924149 139708415854336 logging_writer.py:48] [478000] global_step=478000, grad_norm=3.086310386657715, loss=1.0956733226776123
I0310 21:19:34.511197 139902746892096 spec.py:321] Evaluating on the training split.
I0310 21:19:45.987561 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 21:20:07.758126 139902746892096 spec.py:349] Evaluating on the test split.
I0310 21:20:09.430590 139902746892096 submission_runner.py:411] Time since start: 231532.46s, 	Step: 478014, 	{'train/accuracy': 0.8897460699081421, 'train/loss': 0.41139480471611023, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 213878.38510799408, 'total_duration': 231532.45939064026, 'accumulated_submission_time': 213878.38510799408, 'accumulated_eval_time': 17593.989289283752, 'accumulated_logging_time': 35.059608697891235}
I0310 21:20:09.517673 139708407461632 logging_writer.py:48] [478014] accumulated_eval_time=17593.989289, accumulated_logging_time=35.059609, accumulated_submission_time=213878.385108, global_step=478014, preemption_count=0, score=213878.385108, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=231532.459391, train/accuracy=0.889746, train/loss=0.411395, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 21:20:43.773596 139708415854336 logging_writer.py:48] [478100] global_step=478100, grad_norm=3.0311708450317383, loss=2.678926467895508
I0310 21:21:29.279322 139708407461632 logging_writer.py:48] [478200] global_step=478200, grad_norm=2.9979407787323, loss=1.267397403717041
I0310 21:22:14.585529 139708415854336 logging_writer.py:48] [478300] global_step=478300, grad_norm=2.7402937412261963, loss=1.4986813068389893
I0310 21:23:00.089614 139708407461632 logging_writer.py:48] [478400] global_step=478400, grad_norm=3.889899253845215, loss=3.1533689498901367
I0310 21:23:45.324501 139708415854336 logging_writer.py:48] [478500] global_step=478500, grad_norm=4.081711292266846, loss=1.1310425996780396
I0310 21:24:30.937562 139708407461632 logging_writer.py:48] [478600] global_step=478600, grad_norm=3.8694913387298584, loss=3.017766237258911
I0310 21:25:16.283529 139708415854336 logging_writer.py:48] [478700] global_step=478700, grad_norm=2.8802919387817383, loss=1.2026628255844116
I0310 21:26:01.838530 139708407461632 logging_writer.py:48] [478800] global_step=478800, grad_norm=3.2429208755493164, loss=2.8407392501831055
I0310 21:26:47.144215 139708415854336 logging_writer.py:48] [478900] global_step=478900, grad_norm=3.33522629737854, loss=2.6529088020324707
I0310 21:27:09.462723 139902746892096 spec.py:321] Evaluating on the training split.
I0310 21:27:20.947980 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 21:27:43.903954 139902746892096 spec.py:349] Evaluating on the test split.
I0310 21:27:45.578279 139902746892096 submission_runner.py:411] Time since start: 231988.61s, 	Step: 478951, 	{'train/accuracy': 0.8887109160423279, 'train/loss': 0.4161091148853302, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 214298.27096557617, 'total_duration': 231988.60705900192, 'accumulated_submission_time': 214298.27096557617, 'accumulated_eval_time': 17630.104825258255, 'accumulated_logging_time': 35.1567656993866}
I0310 21:27:45.684815 139708407461632 logging_writer.py:48] [478951] accumulated_eval_time=17630.104825, accumulated_logging_time=35.156766, accumulated_submission_time=214298.270966, global_step=478951, preemption_count=0, score=214298.270966, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=231988.607059, train/accuracy=0.888711, train/loss=0.416109, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 21:28:05.359261 139708415854336 logging_writer.py:48] [479000] global_step=479000, grad_norm=3.0305776596069336, loss=2.5051865577697754
I0310 21:28:47.687467 139708407461632 logging_writer.py:48] [479100] global_step=479100, grad_norm=2.9042372703552246, loss=1.3066438436508179
I0310 21:29:33.284964 139708415854336 logging_writer.py:48] [479200] global_step=479200, grad_norm=3.078601121902466, loss=1.0269882678985596
I0310 21:30:18.675350 139708407461632 logging_writer.py:48] [479300] global_step=479300, grad_norm=3.366764783859253, loss=1.1714904308319092
I0310 21:31:03.701191 139708415854336 logging_writer.py:48] [479400] global_step=479400, grad_norm=2.9708261489868164, loss=1.9025211334228516
I0310 21:31:49.067031 139708407461632 logging_writer.py:48] [479500] global_step=479500, grad_norm=3.043473720550537, loss=1.409525752067566
I0310 21:32:34.353730 139708415854336 logging_writer.py:48] [479600] global_step=479600, grad_norm=3.3767266273498535, loss=1.138045310974121
I0310 21:33:19.513195 139708407461632 logging_writer.py:48] [479700] global_step=479700, grad_norm=3.648146867752075, loss=2.9183928966522217
I0310 21:34:04.641516 139708415854336 logging_writer.py:48] [479800] global_step=479800, grad_norm=4.361907482147217, loss=3.1518731117248535
I0310 21:34:45.681755 139902746892096 spec.py:321] Evaluating on the training split.
I0310 21:34:56.962360 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 21:35:18.665068 139902746892096 spec.py:349] Evaluating on the test split.
I0310 21:35:20.348843 139902746892096 submission_runner.py:411] Time since start: 232443.38s, 	Step: 479893, 	{'train/accuracy': 0.8892577886581421, 'train/loss': 0.4139748215675354, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 214718.205119133, 'total_duration': 232443.3776421547, 'accumulated_submission_time': 214718.205119133, 'accumulated_eval_time': 17664.77192544937, 'accumulated_logging_time': 35.27648377418518}
I0310 21:35:20.436643 139708407461632 logging_writer.py:48] [479893] accumulated_eval_time=17664.771925, accumulated_logging_time=35.276484, accumulated_submission_time=214718.205119, global_step=479893, preemption_count=0, score=214718.205119, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=232443.377642, train/accuracy=0.889258, train/loss=0.413975, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 21:35:23.586036 139708415854336 logging_writer.py:48] [479900] global_step=479900, grad_norm=2.9771106243133545, loss=1.3007687330245972
I0310 21:36:04.202393 139708407461632 logging_writer.py:48] [480000] global_step=480000, grad_norm=3.6111860275268555, loss=2.96213960647583
I0310 21:36:49.088660 139708415854336 logging_writer.py:48] [480100] global_step=480100, grad_norm=3.2432188987731934, loss=2.8316988945007324
I0310 21:37:34.658717 139708407461632 logging_writer.py:48] [480200] global_step=480200, grad_norm=3.1049890518188477, loss=1.2365349531173706
I0310 21:38:20.151634 139708415854336 logging_writer.py:48] [480300] global_step=480300, grad_norm=3.146299362182617, loss=1.0925374031066895
I0310 21:39:04.998890 139708407461632 logging_writer.py:48] [480400] global_step=480400, grad_norm=3.538897752761841, loss=3.1307578086853027
I0310 21:39:50.523458 139708415854336 logging_writer.py:48] [480500] global_step=480500, grad_norm=2.811314582824707, loss=1.2033307552337646
I0310 21:40:35.517157 139708407461632 logging_writer.py:48] [480600] global_step=480600, grad_norm=3.103577136993408, loss=1.2018496990203857
I0310 21:41:20.820713 139708415854336 logging_writer.py:48] [480700] global_step=480700, grad_norm=2.9098172187805176, loss=1.2269585132598877
I0310 21:42:05.992934 139708407461632 logging_writer.py:48] [480800] global_step=480800, grad_norm=3.3794760704040527, loss=1.1426124572753906
I0310 21:42:20.557444 139902746892096 spec.py:321] Evaluating on the training split.
I0310 21:42:31.906666 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 21:42:52.686242 139902746892096 spec.py:349] Evaluating on the test split.
I0310 21:42:54.359888 139902746892096 submission_runner.py:411] Time since start: 232897.39s, 	Step: 480834, 	{'train/accuracy': 0.8868163824081421, 'train/loss': 0.4217853844165802, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 215138.26736354828, 'total_duration': 232897.38868904114, 'accumulated_submission_time': 215138.26736354828, 'accumulated_eval_time': 17698.574368476868, 'accumulated_logging_time': 35.3733856678009}
I0310 21:42:54.454000 139708415854336 logging_writer.py:48] [480834] accumulated_eval_time=17698.574368, accumulated_logging_time=35.373386, accumulated_submission_time=215138.267364, global_step=480834, preemption_count=0, score=215138.267364, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=232897.388689, train/accuracy=0.886816, train/loss=0.421785, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 21:43:20.827439 139708407461632 logging_writer.py:48] [480900] global_step=480900, grad_norm=3.2272303104400635, loss=2.65321946144104
I0310 21:44:04.595603 139708415854336 logging_writer.py:48] [481000] global_step=481000, grad_norm=2.915597677230835, loss=1.1743654012680054
I0310 21:44:49.961857 139708407461632 logging_writer.py:48] [481100] global_step=481100, grad_norm=4.31417989730835, loss=3.2359514236450195
I0310 21:45:35.340490 139708415854336 logging_writer.py:48] [481200] global_step=481200, grad_norm=3.7444427013397217, loss=2.7484123706817627
I0310 21:46:20.383289 139708407461632 logging_writer.py:48] [481300] global_step=481300, grad_norm=3.117217540740967, loss=1.1697289943695068
I0310 21:47:05.769505 139708415854336 logging_writer.py:48] [481400] global_step=481400, grad_norm=3.3962197303771973, loss=2.5307457447052
I0310 21:47:51.187284 139708407461632 logging_writer.py:48] [481500] global_step=481500, grad_norm=3.2254531383514404, loss=1.7675634622573853
I0310 21:48:36.195240 139708415854336 logging_writer.py:48] [481600] global_step=481600, grad_norm=2.9902639389038086, loss=1.2061835527420044
I0310 21:49:21.712290 139708407461632 logging_writer.py:48] [481700] global_step=481700, grad_norm=2.943406105041504, loss=1.9723081588745117
I0310 21:49:54.518790 139902746892096 spec.py:321] Evaluating on the training split.
I0310 21:50:05.987890 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 21:50:28.430196 139902746892096 spec.py:349] Evaluating on the test split.
I0310 21:50:30.104849 139902746892096 submission_runner.py:411] Time since start: 233353.13s, 	Step: 481774, 	{'train/accuracy': 0.8877929449081421, 'train/loss': 0.41647791862487793, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 215558.27237558365, 'total_duration': 233353.13364720345, 'accumulated_submission_time': 215558.27237558365, 'accumulated_eval_time': 17734.160432100296, 'accumulated_logging_time': 35.47701930999756}
I0310 21:50:30.192959 139708415854336 logging_writer.py:48] [481774] accumulated_eval_time=17734.160432, accumulated_logging_time=35.477019, accumulated_submission_time=215558.272376, global_step=481774, preemption_count=0, score=215558.272376, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=233353.133647, train/accuracy=0.887793, train/loss=0.416478, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 21:50:40.809812 139708407461632 logging_writer.py:48] [481800] global_step=481800, grad_norm=3.1073994636535645, loss=2.581362247467041
I0310 21:51:22.254545 139708415854336 logging_writer.py:48] [481900] global_step=481900, grad_norm=3.2566299438476562, loss=1.5086326599121094
I0310 21:52:07.614607 139708407461632 logging_writer.py:48] [482000] global_step=482000, grad_norm=3.1777896881103516, loss=1.6749423742294312
I0310 21:52:53.214231 139708415854336 logging_writer.py:48] [482100] global_step=482100, grad_norm=3.976856231689453, loss=3.156785011291504
I0310 21:53:38.536380 139708407461632 logging_writer.py:48] [482200] global_step=482200, grad_norm=3.519848585128784, loss=3.007401466369629
I0310 21:54:23.836018 139708415854336 logging_writer.py:48] [482300] global_step=482300, grad_norm=3.4363625049591064, loss=2.9995548725128174
I0310 21:55:09.169256 139708407461632 logging_writer.py:48] [482400] global_step=482400, grad_norm=3.1097426414489746, loss=1.0586742162704468
I0310 21:55:54.577251 139708415854336 logging_writer.py:48] [482500] global_step=482500, grad_norm=3.014277219772339, loss=1.047114610671997
I0310 21:56:39.978834 139708407461632 logging_writer.py:48] [482600] global_step=482600, grad_norm=3.458132266998291, loss=1.6125084161758423
I0310 21:57:25.590186 139708415854336 logging_writer.py:48] [482700] global_step=482700, grad_norm=2.958207368850708, loss=1.0475177764892578
I0310 21:57:30.252249 139902746892096 spec.py:321] Evaluating on the training split.
I0310 21:57:41.888839 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 21:58:04.468901 139902746892096 spec.py:349] Evaluating on the test split.
I0310 21:58:06.141352 139902746892096 submission_runner.py:411] Time since start: 233809.17s, 	Step: 482712, 	{'train/accuracy': 0.8885351419448853, 'train/loss': 0.4163164794445038, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 215978.2754702568, 'total_duration': 233809.17014741898, 'accumulated_submission_time': 215978.2754702568, 'accumulated_eval_time': 17770.049526929855, 'accumulated_logging_time': 35.57323598861694}
I0310 21:58:06.236945 139708407461632 logging_writer.py:48] [482712] accumulated_eval_time=17770.049527, accumulated_logging_time=35.573236, accumulated_submission_time=215978.275470, global_step=482712, preemption_count=0, score=215978.275470, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=233809.170147, train/accuracy=0.888535, train/loss=0.416316, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 21:58:41.317630 139708415854336 logging_writer.py:48] [482800] global_step=482800, grad_norm=3.7419426441192627, loss=3.044649600982666
I0310 21:59:26.421150 139708407461632 logging_writer.py:48] [482900] global_step=482900, grad_norm=3.3174984455108643, loss=1.1101839542388916
I0310 22:00:11.875240 139708415854336 logging_writer.py:48] [483000] global_step=483000, grad_norm=2.9461991786956787, loss=1.4182322025299072
I0310 22:00:57.343666 139708407461632 logging_writer.py:48] [483100] global_step=483100, grad_norm=2.9137184619903564, loss=1.1148921251296997
I0310 22:01:42.366750 139708415854336 logging_writer.py:48] [483200] global_step=483200, grad_norm=3.071467399597168, loss=1.078281283378601
I0310 22:02:27.714953 139708407461632 logging_writer.py:48] [483300] global_step=483300, grad_norm=3.1792449951171875, loss=2.215456008911133
I0310 22:03:13.185051 139708415854336 logging_writer.py:48] [483400] global_step=483400, grad_norm=3.527895212173462, loss=1.2505967617034912
I0310 22:03:58.368570 139708407461632 logging_writer.py:48] [483500] global_step=483500, grad_norm=3.6828994750976562, loss=2.973159074783325
I0310 22:04:43.519979 139708415854336 logging_writer.py:48] [483600] global_step=483600, grad_norm=3.0205841064453125, loss=1.1856987476348877
I0310 22:05:06.405030 139902746892096 spec.py:321] Evaluating on the training split.
I0310 22:05:17.922672 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 22:05:38.167751 139902746892096 spec.py:349] Evaluating on the test split.
I0310 22:05:39.856400 139902746892096 submission_runner.py:411] Time since start: 234262.89s, 	Step: 483652, 	{'train/accuracy': 0.8861523270606995, 'train/loss': 0.4175904095172882, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 216398.384973526, 'total_duration': 234262.88517141342, 'accumulated_submission_time': 216398.384973526, 'accumulated_eval_time': 17803.500860214233, 'accumulated_logging_time': 35.67725419998169}
I0310 22:05:39.965709 139708407461632 logging_writer.py:48] [483652] accumulated_eval_time=17803.500860, accumulated_logging_time=35.677254, accumulated_submission_time=216398.384974, global_step=483652, preemption_count=0, score=216398.384974, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=234262.885171, train/accuracy=0.886152, train/loss=0.417590, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 22:05:59.254757 139708415854336 logging_writer.py:48] [483700] global_step=483700, grad_norm=3.1743600368499756, loss=1.0455174446105957
I0310 22:06:42.574238 139708407461632 logging_writer.py:48] [483800] global_step=483800, grad_norm=3.287628173828125, loss=1.358038067817688
I0310 22:07:28.001548 139708415854336 logging_writer.py:48] [483900] global_step=483900, grad_norm=3.033400058746338, loss=1.993541955947876
I0310 22:08:13.450038 139708407461632 logging_writer.py:48] [484000] global_step=484000, grad_norm=3.167971611022949, loss=1.1450074911117554
I0310 22:08:58.711974 139708415854336 logging_writer.py:48] [484100] global_step=484100, grad_norm=3.4270758628845215, loss=1.2891000509262085
I0310 22:09:44.124591 139708407461632 logging_writer.py:48] [484200] global_step=484200, grad_norm=3.164175510406494, loss=1.2090721130371094
I0310 22:10:29.895572 139708415854336 logging_writer.py:48] [484300] global_step=484300, grad_norm=3.03326678276062, loss=1.2229270935058594
I0310 22:11:15.343247 139708407461632 logging_writer.py:48] [484400] global_step=484400, grad_norm=3.8102352619171143, loss=3.1734957695007324
I0310 22:12:00.636461 139708415854336 logging_writer.py:48] [484500] global_step=484500, grad_norm=3.1455719470977783, loss=1.1142325401306152
I0310 22:12:40.228948 139902746892096 spec.py:321] Evaluating on the training split.
I0310 22:12:51.665004 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 22:13:13.679429 139902746892096 spec.py:349] Evaluating on the test split.
I0310 22:13:15.346072 139902746892096 submission_runner.py:411] Time since start: 234718.37s, 	Step: 484589, 	{'train/accuracy': 0.8881054520606995, 'train/loss': 0.41433796286582947, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 216818.5877084732, 'total_duration': 234718.37487339973, 'accumulated_submission_time': 216818.5877084732, 'accumulated_eval_time': 17838.617998600006, 'accumulated_logging_time': 35.79676175117493}
I0310 22:13:15.433233 139708407461632 logging_writer.py:48] [484589] accumulated_eval_time=17838.617999, accumulated_logging_time=35.796762, accumulated_submission_time=216818.587708, global_step=484589, preemption_count=0, score=216818.587708, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=234718.374873, train/accuracy=0.888105, train/loss=0.414338, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 22:13:20.152656 139708415854336 logging_writer.py:48] [484600] global_step=484600, grad_norm=3.2729380130767822, loss=1.2359609603881836
I0310 22:14:01.021203 139708407461632 logging_writer.py:48] [484700] global_step=484700, grad_norm=3.077674150466919, loss=1.2183326482772827
I0310 22:14:46.060080 139708415854336 logging_writer.py:48] [484800] global_step=484800, grad_norm=3.134622573852539, loss=1.0990983247756958
I0310 22:15:31.530914 139708407461632 logging_writer.py:48] [484900] global_step=484900, grad_norm=3.3255367279052734, loss=3.097362518310547
I0310 22:16:16.887400 139708415854336 logging_writer.py:48] [485000] global_step=485000, grad_norm=3.125039577484131, loss=1.8036507368087769
I0310 22:17:02.072576 139708407461632 logging_writer.py:48] [485100] global_step=485100, grad_norm=3.6516289710998535, loss=3.1207520961761475
I0310 22:17:47.339961 139708415854336 logging_writer.py:48] [485200] global_step=485200, grad_norm=3.3014605045318604, loss=1.6228687763214111
I0310 22:18:32.643979 139708407461632 logging_writer.py:48] [485300] global_step=485300, grad_norm=2.987459659576416, loss=1.9120595455169678
I0310 22:19:17.742072 139708415854336 logging_writer.py:48] [485400] global_step=485400, grad_norm=3.815859079360962, loss=2.6779043674468994
I0310 22:20:03.139241 139708407461632 logging_writer.py:48] [485500] global_step=485500, grad_norm=3.100926160812378, loss=2.4106006622314453
I0310 22:20:15.473004 139902746892096 spec.py:321] Evaluating on the training split.
I0310 22:20:26.768500 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 22:20:49.748393 139902746892096 spec.py:349] Evaluating on the test split.
I0310 22:20:51.423174 139902746892096 submission_runner.py:411] Time since start: 235174.45s, 	Step: 485529, 	{'train/accuracy': 0.8881640434265137, 'train/loss': 0.41480955481529236, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 217238.56854653358, 'total_duration': 235174.45196986198, 'accumulated_submission_time': 217238.56854653358, 'accumulated_eval_time': 17874.568171977997, 'accumulated_logging_time': 35.89333629608154}
I0310 22:20:51.510256 139708415854336 logging_writer.py:48] [485529] accumulated_eval_time=17874.568172, accumulated_logging_time=35.893336, accumulated_submission_time=217238.568547, global_step=485529, preemption_count=0, score=217238.568547, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=235174.451970, train/accuracy=0.888164, train/loss=0.414810, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 22:21:19.829131 139708407461632 logging_writer.py:48] [485600] global_step=485600, grad_norm=3.173264265060425, loss=1.3300015926361084
I0310 22:22:03.619106 139708415854336 logging_writer.py:48] [485700] global_step=485700, grad_norm=4.535269737243652, loss=3.1870932579040527
I0310 22:22:48.907406 139708407461632 logging_writer.py:48] [485800] global_step=485800, grad_norm=3.172865152359009, loss=1.2302610874176025
I0310 22:23:34.535682 139708415854336 logging_writer.py:48] [485900] global_step=485900, grad_norm=3.3525712490081787, loss=2.779088020324707
I0310 22:24:19.713554 139708407461632 logging_writer.py:48] [486000] global_step=486000, grad_norm=3.278911828994751, loss=1.1604154109954834
I0310 22:25:04.937072 139708415854336 logging_writer.py:48] [486100] global_step=486100, grad_norm=3.5596773624420166, loss=1.1377873420715332
I0310 22:25:50.003233 139708407461632 logging_writer.py:48] [486200] global_step=486200, grad_norm=3.1234586238861084, loss=1.3279328346252441
I0310 22:26:35.343988 139708415854336 logging_writer.py:48] [486300] global_step=486300, grad_norm=3.2225513458251953, loss=2.8286757469177246
I0310 22:27:20.556093 139708407461632 logging_writer.py:48] [486400] global_step=486400, grad_norm=3.1032984256744385, loss=2.599468946456909
I0310 22:27:51.701537 139902746892096 spec.py:321] Evaluating on the training split.
I0310 22:28:03.084752 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 22:28:23.705781 139902746892096 spec.py:349] Evaluating on the test split.
I0310 22:28:25.383030 139902746892096 submission_runner.py:411] Time since start: 235628.41s, 	Step: 486470, 	{'train/accuracy': 0.88818359375, 'train/loss': 0.41647249460220337, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 217658.7013375759, 'total_duration': 235628.41182494164, 'accumulated_submission_time': 217658.7013375759, 'accumulated_eval_time': 17908.249662160873, 'accumulated_logging_time': 35.98907494544983}
I0310 22:28:25.471694 139708415854336 logging_writer.py:48] [486470] accumulated_eval_time=17908.249662, accumulated_logging_time=35.989075, accumulated_submission_time=217658.701338, global_step=486470, preemption_count=0, score=217658.701338, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=235628.411825, train/accuracy=0.888184, train/loss=0.416472, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 22:28:37.669825 139708407461632 logging_writer.py:48] [486500] global_step=486500, grad_norm=3.4391844272613525, loss=1.0338624715805054
I0310 22:29:19.501707 139708415854336 logging_writer.py:48] [486600] global_step=486600, grad_norm=3.568274736404419, loss=1.1703848838806152
I0310 22:30:04.844564 139708407461632 logging_writer.py:48] [486700] global_step=486700, grad_norm=3.18978214263916, loss=2.4765870571136475
I0310 22:30:50.341141 139708415854336 logging_writer.py:48] [486800] global_step=486800, grad_norm=3.060682535171509, loss=1.0712076425552368
I0310 22:31:35.653788 139708407461632 logging_writer.py:48] [486900] global_step=486900, grad_norm=3.2328763008117676, loss=1.4240431785583496
I0310 22:32:21.012019 139708415854336 logging_writer.py:48] [487000] global_step=487000, grad_norm=3.259744882583618, loss=1.1041957139968872
I0310 22:33:06.288028 139708407461632 logging_writer.py:48] [487100] global_step=487100, grad_norm=3.3406312465667725, loss=1.1284817457199097
I0310 22:33:51.689933 139708415854336 logging_writer.py:48] [487200] global_step=487200, grad_norm=4.055943012237549, loss=1.0771931409835815
I0310 22:34:37.033881 139708407461632 logging_writer.py:48] [487300] global_step=487300, grad_norm=3.7255301475524902, loss=3.1472654342651367
I0310 22:35:22.366554 139708415854336 logging_writer.py:48] [487400] global_step=487400, grad_norm=3.0657150745391846, loss=1.0839565992355347
I0310 22:35:25.635494 139902746892096 spec.py:321] Evaluating on the training split.
I0310 22:35:37.158717 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 22:35:56.592764 139902746892096 spec.py:349] Evaluating on the test split.
I0310 22:35:58.277678 139902746892096 submission_runner.py:411] Time since start: 236081.31s, 	Step: 487409, 	{'train/accuracy': 0.8879492282867432, 'train/loss': 0.4153375029563904, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 218078.80738782883, 'total_duration': 236081.30646657944, 'accumulated_submission_time': 218078.80738782883, 'accumulated_eval_time': 17940.89183330536, 'accumulated_logging_time': 36.08620810508728}
I0310 22:35:58.386520 139708407461632 logging_writer.py:48] [487409] accumulated_eval_time=17940.891833, accumulated_logging_time=36.086208, accumulated_submission_time=218078.807388, global_step=487409, preemption_count=0, score=218078.807388, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=236081.306467, train/accuracy=0.887949, train/loss=0.415338, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 22:36:35.424789 139708415854336 logging_writer.py:48] [487500] global_step=487500, grad_norm=3.2446606159210205, loss=1.4913183450698853
I0310 22:37:20.601527 139708407461632 logging_writer.py:48] [487600] global_step=487600, grad_norm=2.7982535362243652, loss=1.6222119331359863
I0310 22:38:05.828005 139708415854336 logging_writer.py:48] [487700] global_step=487700, grad_norm=3.262382984161377, loss=1.1826765537261963
I0310 22:38:51.348251 139708407461632 logging_writer.py:48] [487800] global_step=487800, grad_norm=2.9439449310302734, loss=1.094899296760559
I0310 22:39:36.468989 139708415854336 logging_writer.py:48] [487900] global_step=487900, grad_norm=3.3824474811553955, loss=1.1324400901794434
I0310 22:40:22.143135 139708407461632 logging_writer.py:48] [488000] global_step=488000, grad_norm=3.294548273086548, loss=1.1318033933639526
I0310 22:41:07.610174 139708415854336 logging_writer.py:48] [488100] global_step=488100, grad_norm=3.1894829273223877, loss=2.1978211402893066
I0310 22:41:52.847540 139708407461632 logging_writer.py:48] [488200] global_step=488200, grad_norm=3.3133704662323, loss=2.4077110290527344
I0310 22:42:38.311367 139708415854336 logging_writer.py:48] [488300] global_step=488300, grad_norm=3.20121169090271, loss=2.786679983139038
I0310 22:42:58.386718 139902746892096 spec.py:321] Evaluating on the training split.
I0310 22:43:09.642406 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 22:43:32.515563 139902746892096 spec.py:349] Evaluating on the test split.
I0310 22:43:34.182185 139902746892096 submission_runner.py:411] Time since start: 236537.21s, 	Step: 488346, 	{'train/accuracy': 0.8887695074081421, 'train/loss': 0.4143279790878296, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 218498.74874806404, 'total_duration': 236537.2109837532, 'accumulated_submission_time': 218498.74874806404, 'accumulated_eval_time': 17976.68732571602, 'accumulated_logging_time': 36.20460081100464}
I0310 22:43:34.268199 139708407461632 logging_writer.py:48] [488346] accumulated_eval_time=17976.687326, accumulated_logging_time=36.204601, accumulated_submission_time=218498.748748, global_step=488346, preemption_count=0, score=218498.748748, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=236537.210984, train/accuracy=0.888770, train/loss=0.414328, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 22:43:55.908798 139708415854336 logging_writer.py:48] [488400] global_step=488400, grad_norm=3.594531297683716, loss=2.9105782508850098
I0310 22:44:38.948022 139708407461632 logging_writer.py:48] [488500] global_step=488500, grad_norm=3.0610744953155518, loss=2.5601372718811035
I0310 22:45:24.579800 139708415854336 logging_writer.py:48] [488600] global_step=488600, grad_norm=3.145927906036377, loss=1.1857123374938965
I0310 22:46:10.382629 139708407461632 logging_writer.py:48] [488700] global_step=488700, grad_norm=3.080341339111328, loss=1.1725401878356934
I0310 22:46:55.621666 139708415854336 logging_writer.py:48] [488800] global_step=488800, grad_norm=3.6715025901794434, loss=3.0897934436798096
I0310 22:47:41.045194 139708407461632 logging_writer.py:48] [488900] global_step=488900, grad_norm=3.019272565841675, loss=1.7541098594665527
I0310 22:48:26.526241 139708415854336 logging_writer.py:48] [489000] global_step=489000, grad_norm=3.014371395111084, loss=2.382524251937866
I0310 22:49:11.694470 139708407461632 logging_writer.py:48] [489100] global_step=489100, grad_norm=3.0573484897613525, loss=1.8271780014038086
I0310 22:49:57.248480 139708415854336 logging_writer.py:48] [489200] global_step=489200, grad_norm=3.1142330169677734, loss=1.177919626235962
I0310 22:50:34.252719 139902746892096 spec.py:321] Evaluating on the training split.
I0310 22:50:45.668210 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 22:51:08.389191 139902746892096 spec.py:349] Evaluating on the test split.
I0310 22:51:10.054799 139902746892096 submission_runner.py:411] Time since start: 236993.08s, 	Step: 489283, 	{'train/accuracy': 0.8891015648841858, 'train/loss': 0.4146701693534851, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 218918.67112493515, 'total_duration': 236993.08359861374, 'accumulated_submission_time': 218918.67112493515, 'accumulated_eval_time': 18012.489437818527, 'accumulated_logging_time': 36.3038694858551}
I0310 22:51:10.144011 139708407461632 logging_writer.py:48] [489283] accumulated_eval_time=18012.489438, accumulated_logging_time=36.303869, accumulated_submission_time=218918.671125, global_step=489283, preemption_count=0, score=218918.671125, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=236993.083599, train/accuracy=0.889102, train/loss=0.414670, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 22:51:17.218984 139708415854336 logging_writer.py:48] [489300] global_step=489300, grad_norm=2.9819283485412598, loss=1.046614170074463
I0310 22:51:57.662277 139708407461632 logging_writer.py:48] [489400] global_step=489400, grad_norm=2.862165927886963, loss=1.1771583557128906
I0310 22:52:42.722586 139708415854336 logging_writer.py:48] [489500] global_step=489500, grad_norm=2.8890058994293213, loss=1.1590875387191772
I0310 22:53:28.194554 139708407461632 logging_writer.py:48] [489600] global_step=489600, grad_norm=3.2695775032043457, loss=1.1926946640014648
I0310 22:54:13.395106 139708415854336 logging_writer.py:48] [489700] global_step=489700, grad_norm=3.3021228313446045, loss=2.8138463497161865
I0310 22:54:58.509880 139708407461632 logging_writer.py:48] [489800] global_step=489800, grad_norm=3.027965545654297, loss=2.342721939086914
I0310 22:55:44.073523 139708415854336 logging_writer.py:48] [489900] global_step=489900, grad_norm=3.344339609146118, loss=1.1089469194412231
I0310 22:56:28.792782 139708407461632 logging_writer.py:48] [490000] global_step=490000, grad_norm=4.077399730682373, loss=3.162623167037964
I0310 22:57:14.086985 139708415854336 logging_writer.py:48] [490100] global_step=490100, grad_norm=3.3105058670043945, loss=2.195920467376709
I0310 22:57:59.422954 139708407461632 logging_writer.py:48] [490200] global_step=490200, grad_norm=3.6210625171661377, loss=1.0957509279251099
I0310 22:58:10.359618 139902746892096 spec.py:321] Evaluating on the training split.
I0310 22:58:21.700085 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 22:58:43.596620 139902746892096 spec.py:349] Evaluating on the test split.
I0310 22:58:45.277097 139902746892096 submission_runner.py:411] Time since start: 237448.31s, 	Step: 490226, 	{'train/accuracy': 0.8903515338897705, 'train/loss': 0.4075874984264374, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 219338.82824254036, 'total_duration': 237448.30587911606, 'accumulated_submission_time': 219338.82824254036, 'accumulated_eval_time': 18047.406898736954, 'accumulated_logging_time': 36.40238857269287}
I0310 22:58:45.378235 139708415854336 logging_writer.py:48] [490226] accumulated_eval_time=18047.406899, accumulated_logging_time=36.402389, accumulated_submission_time=219338.828243, global_step=490226, preemption_count=0, score=219338.828243, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=237448.305879, train/accuracy=0.890352, train/loss=0.407587, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 22:59:14.887315 139708407461632 logging_writer.py:48] [490300] global_step=490300, grad_norm=3.4783966541290283, loss=3.0277698040008545
I0310 22:59:59.416215 139708415854336 logging_writer.py:48] [490400] global_step=490400, grad_norm=3.0038015842437744, loss=1.0816152095794678
I0310 23:00:45.093440 139708407461632 logging_writer.py:48] [490500] global_step=490500, grad_norm=3.108278512954712, loss=2.136672258377075
I0310 23:01:30.574715 139708415854336 logging_writer.py:48] [490600] global_step=490600, grad_norm=3.452209234237671, loss=1.1801934242248535
I0310 23:02:15.869412 139708407461632 logging_writer.py:48] [490700] global_step=490700, grad_norm=3.1703336238861084, loss=1.321083903312683
I0310 23:03:01.395297 139708415854336 logging_writer.py:48] [490800] global_step=490800, grad_norm=3.1330296993255615, loss=1.5739452838897705
I0310 23:03:46.610384 139708407461632 logging_writer.py:48] [490900] global_step=490900, grad_norm=3.3879408836364746, loss=1.2879555225372314
I0310 23:04:31.845106 139708415854336 logging_writer.py:48] [491000] global_step=491000, grad_norm=3.1220016479492188, loss=1.135175347328186
I0310 23:05:16.985441 139708407461632 logging_writer.py:48] [491100] global_step=491100, grad_norm=3.3901548385620117, loss=1.155503749847412
I0310 23:05:45.299625 139902746892096 spec.py:321] Evaluating on the training split.
I0310 23:05:56.723615 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 23:06:18.715799 139902746892096 spec.py:349] Evaluating on the test split.
I0310 23:06:20.388942 139902746892096 submission_runner.py:411] Time since start: 237903.42s, 	Step: 491164, 	{'train/accuracy': 0.8893554210662842, 'train/loss': 0.40992864966392517, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 219758.6913704872, 'total_duration': 237903.41774082184, 'accumulated_submission_time': 219758.6913704872, 'accumulated_eval_time': 18082.4962182045, 'accumulated_logging_time': 36.51316452026367}
I0310 23:06:20.478528 139708415854336 logging_writer.py:48] [491164] accumulated_eval_time=18082.496218, accumulated_logging_time=36.513165, accumulated_submission_time=219758.691370, global_step=491164, preemption_count=0, score=219758.691370, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=237903.417741, train/accuracy=0.889355, train/loss=0.409929, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 23:06:35.053304 139708407461632 logging_writer.py:48] [491200] global_step=491200, grad_norm=3.073364496231079, loss=1.0975360870361328
I0310 23:07:17.069966 139708415854336 logging_writer.py:48] [491300] global_step=491300, grad_norm=2.9206650257110596, loss=1.129706621170044
I0310 23:08:02.456103 139708407461632 logging_writer.py:48] [491400] global_step=491400, grad_norm=3.0047178268432617, loss=1.0950658321380615
I0310 23:08:48.041245 139708415854336 logging_writer.py:48] [491500] global_step=491500, grad_norm=2.9735398292541504, loss=1.068549394607544
I0310 23:09:33.353526 139708407461632 logging_writer.py:48] [491600] global_step=491600, grad_norm=3.0486674308776855, loss=1.1705217361450195
I0310 23:10:18.824318 139708415854336 logging_writer.py:48] [491700] global_step=491700, grad_norm=3.3460512161254883, loss=1.177071452140808
I0310 23:11:04.294997 139708407461632 logging_writer.py:48] [491800] global_step=491800, grad_norm=2.989365577697754, loss=2.0309395790100098
I0310 23:11:49.383775 139708415854336 logging_writer.py:48] [491900] global_step=491900, grad_norm=3.2908802032470703, loss=1.1442978382110596
I0310 23:12:34.774352 139708407461632 logging_writer.py:48] [492000] global_step=492000, grad_norm=3.6146934032440186, loss=3.1254541873931885
I0310 23:13:20.143320 139708415854336 logging_writer.py:48] [492100] global_step=492100, grad_norm=3.2263412475585938, loss=2.112041711807251
I0310 23:13:20.732290 139902746892096 spec.py:321] Evaluating on the training split.
I0310 23:13:32.994191 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 23:13:53.738551 139902746892096 spec.py:349] Evaluating on the test split.
I0310 23:13:55.404199 139902746892096 submission_runner.py:411] Time since start: 238358.43s, 	Step: 492103, 	{'train/accuracy': 0.8878515362739563, 'train/loss': 0.4151861369609833, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 220178.88720822334, 'total_duration': 238358.4329917431, 'accumulated_submission_time': 220178.88720822334, 'accumulated_eval_time': 18117.168116807938, 'accumulated_logging_time': 36.611567735672}
I0310 23:13:55.491284 139708407461632 logging_writer.py:48] [492103] accumulated_eval_time=18117.168117, accumulated_logging_time=36.611568, accumulated_submission_time=220178.887208, global_step=492103, preemption_count=0, score=220178.887208, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=238358.432992, train/accuracy=0.887852, train/loss=0.415186, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 23:14:34.249292 139708415854336 logging_writer.py:48] [492200] global_step=492200, grad_norm=3.154290199279785, loss=1.0426172018051147
I0310 23:15:19.190081 139708407461632 logging_writer.py:48] [492300] global_step=492300, grad_norm=3.9606833457946777, loss=3.143202304840088
I0310 23:16:04.531087 139708415854336 logging_writer.py:48] [492400] global_step=492400, grad_norm=3.2661147117614746, loss=1.163354516029358
I0310 23:16:49.891652 139708407461632 logging_writer.py:48] [492500] global_step=492500, grad_norm=3.15643310546875, loss=1.211525321006775
I0310 23:17:35.098260 139708415854336 logging_writer.py:48] [492600] global_step=492600, grad_norm=3.142915725708008, loss=1.173194169998169
I0310 23:18:20.145439 139708407461632 logging_writer.py:48] [492700] global_step=492700, grad_norm=3.0259575843811035, loss=1.2843996286392212
I0310 23:19:05.306048 139708415854336 logging_writer.py:48] [492800] global_step=492800, grad_norm=2.793081760406494, loss=2.381006956100464
I0310 23:19:50.420950 139708407461632 logging_writer.py:48] [492900] global_step=492900, grad_norm=2.9306585788726807, loss=1.034239649772644
I0310 23:20:35.872573 139708415854336 logging_writer.py:48] [493000] global_step=493000, grad_norm=3.260469436645508, loss=2.6869778633117676
I0310 23:20:55.420570 139902746892096 spec.py:321] Evaluating on the training split.
I0310 23:21:06.879067 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 23:21:28.342385 139902746892096 spec.py:349] Evaluating on the test split.
I0310 23:21:30.019803 139902746892096 submission_runner.py:411] Time since start: 238813.05s, 	Step: 493045, 	{'train/accuracy': 0.8883984088897705, 'train/loss': 0.414981484413147, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 220598.75923585892, 'total_duration': 238813.04859733582, 'accumulated_submission_time': 220598.75923585892, 'accumulated_eval_time': 18151.76734852791, 'accumulated_logging_time': 36.70686912536621}
I0310 23:21:30.107802 139708407461632 logging_writer.py:48] [493045] accumulated_eval_time=18151.767349, accumulated_logging_time=36.706869, accumulated_submission_time=220598.759236, global_step=493045, preemption_count=0, score=220598.759236, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=238813.048597, train/accuracy=0.888398, train/loss=0.414981, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 23:21:52.155375 139708415854336 logging_writer.py:48] [493100] global_step=493100, grad_norm=3.6738603115081787, loss=3.126171112060547
I0310 23:22:35.568153 139708407461632 logging_writer.py:48] [493200] global_step=493200, grad_norm=3.230109930038452, loss=1.1184935569763184
I0310 23:23:21.214725 139708415854336 logging_writer.py:48] [493300] global_step=493300, grad_norm=3.2876315116882324, loss=1.0642955303192139
I0310 23:24:07.052793 139708407461632 logging_writer.py:48] [493400] global_step=493400, grad_norm=2.8608222007751465, loss=1.5203889608383179
I0310 23:24:52.164198 139708415854336 logging_writer.py:48] [493500] global_step=493500, grad_norm=3.0197958946228027, loss=1.2725526094436646
I0310 23:25:37.401536 139708407461632 logging_writer.py:48] [493600] global_step=493600, grad_norm=3.081127643585205, loss=1.172549843788147
I0310 23:26:22.651292 139708415854336 logging_writer.py:48] [493700] global_step=493700, grad_norm=3.1056668758392334, loss=1.1273380517959595
I0310 23:27:07.802736 139708407461632 logging_writer.py:48] [493800] global_step=493800, grad_norm=3.131664991378784, loss=2.4348230361938477
I0310 23:27:53.113929 139708415854336 logging_writer.py:48] [493900] global_step=493900, grad_norm=3.2109878063201904, loss=1.3508244752883911
I0310 23:28:30.075492 139902746892096 spec.py:321] Evaluating on the training split.
I0310 23:28:41.571905 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 23:29:02.517843 139902746892096 spec.py:349] Evaluating on the test split.
I0310 23:29:04.204463 139902746892096 submission_runner.py:411] Time since start: 239267.23s, 	Step: 493983, 	{'train/accuracy': 0.8869531154632568, 'train/loss': 0.4125227630138397, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 221018.6685461998, 'total_duration': 239267.23322939873, 'accumulated_submission_time': 221018.6685461998, 'accumulated_eval_time': 18185.896293401718, 'accumulated_logging_time': 36.804692029953}
I0310 23:29:04.304629 139708407461632 logging_writer.py:48] [493983] accumulated_eval_time=18185.896293, accumulated_logging_time=36.804692, accumulated_submission_time=221018.668546, global_step=493983, preemption_count=0, score=221018.668546, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=239267.233229, train/accuracy=0.886953, train/loss=0.412523, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 23:29:11.392910 139708415854336 logging_writer.py:48] [494000] global_step=494000, grad_norm=3.124028444290161, loss=1.1332650184631348
I0310 23:29:52.776783 139708407461632 logging_writer.py:48] [494100] global_step=494100, grad_norm=2.9528820514678955, loss=1.2334811687469482
I0310 23:30:38.208881 139708415854336 logging_writer.py:48] [494200] global_step=494200, grad_norm=3.007310628890991, loss=1.0773366689682007
I0310 23:31:23.660160 139708407461632 logging_writer.py:48] [494300] global_step=494300, grad_norm=2.7949657440185547, loss=1.8638789653778076
I0310 23:32:08.999482 139708415854336 logging_writer.py:48] [494400] global_step=494400, grad_norm=3.3287134170532227, loss=1.1936571598052979
I0310 23:32:54.249914 139708407461632 logging_writer.py:48] [494500] global_step=494500, grad_norm=3.892911195755005, loss=3.1703760623931885
I0310 23:33:39.794804 139708415854336 logging_writer.py:48] [494600] global_step=494600, grad_norm=3.1069273948669434, loss=1.173740267753601
I0310 23:34:25.046384 139708407461632 logging_writer.py:48] [494700] global_step=494700, grad_norm=3.1363797187805176, loss=1.1233898401260376
I0310 23:35:10.320208 139708415854336 logging_writer.py:48] [494800] global_step=494800, grad_norm=3.372645378112793, loss=1.1790409088134766
I0310 23:35:55.539204 139708407461632 logging_writer.py:48] [494900] global_step=494900, grad_norm=3.644484519958496, loss=2.875690460205078
I0310 23:36:04.330844 139902746892096 spec.py:321] Evaluating on the training split.
I0310 23:36:15.652896 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 23:36:37.949140 139902746892096 spec.py:349] Evaluating on the test split.
I0310 23:36:39.617959 139902746892096 submission_runner.py:411] Time since start: 239722.65s, 	Step: 494921, 	{'train/accuracy': 0.8883007764816284, 'train/loss': 0.4121655523777008, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 221438.6368880272, 'total_duration': 239722.64675951004, 'accumulated_submission_time': 221438.6368880272, 'accumulated_eval_time': 18221.183396816254, 'accumulated_logging_time': 36.91400504112244}
I0310 23:36:39.708515 139708415854336 logging_writer.py:48] [494921] accumulated_eval_time=18221.183397, accumulated_logging_time=36.914005, accumulated_submission_time=221438.636888, global_step=494921, preemption_count=0, score=221438.636888, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=239722.646760, train/accuracy=0.888301, train/loss=0.412166, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 23:37:11.162454 139708407461632 logging_writer.py:48] [495000] global_step=495000, grad_norm=2.9365077018737793, loss=1.7218228578567505
I0310 23:37:55.382103 139708415854336 logging_writer.py:48] [495100] global_step=495100, grad_norm=3.8056957721710205, loss=3.0520801544189453
I0310 23:38:40.473489 139708407461632 logging_writer.py:48] [495200] global_step=495200, grad_norm=3.397078037261963, loss=1.2494465112686157
I0310 23:39:26.025960 139708415854336 logging_writer.py:48] [495300] global_step=495300, grad_norm=2.914255142211914, loss=1.3376593589782715
I0310 23:40:11.126809 139708407461632 logging_writer.py:48] [495400] global_step=495400, grad_norm=3.0740833282470703, loss=1.0627449750900269
I0310 23:40:56.678206 139708415854336 logging_writer.py:48] [495500] global_step=495500, grad_norm=2.7830092906951904, loss=1.171433925628662
I0310 23:41:42.077784 139708407461632 logging_writer.py:48] [495600] global_step=495600, grad_norm=3.049790143966675, loss=1.3540762662887573
I0310 23:42:27.511011 139708415854336 logging_writer.py:48] [495700] global_step=495700, grad_norm=3.0181069374084473, loss=1.5632121562957764
I0310 23:43:12.948231 139708407461632 logging_writer.py:48] [495800] global_step=495800, grad_norm=3.063352108001709, loss=2.257267951965332
I0310 23:43:39.835665 139902746892096 spec.py:321] Evaluating on the training split.
I0310 23:43:51.373300 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 23:44:14.559911 139902746892096 spec.py:349] Evaluating on the test split.
I0310 23:44:16.235126 139902746892096 submission_runner.py:411] Time since start: 240179.26s, 	Step: 495861, 	{'train/accuracy': 0.88832026720047, 'train/loss': 0.41517317295074463, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 221858.705681324, 'total_duration': 240179.26391124725, 'accumulated_submission_time': 221858.705681324, 'accumulated_eval_time': 18257.582832574844, 'accumulated_logging_time': 37.0140278339386}
I0310 23:44:16.335542 139708415854336 logging_writer.py:48] [495861] accumulated_eval_time=18257.582833, accumulated_logging_time=37.014028, accumulated_submission_time=221858.705681, global_step=495861, preemption_count=0, score=221858.705681, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=240179.263911, train/accuracy=0.888320, train/loss=0.415173, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 23:44:32.080604 139708407461632 logging_writer.py:48] [495900] global_step=495900, grad_norm=2.864772081375122, loss=1.6385254859924316
I0310 23:45:14.039957 139708415854336 logging_writer.py:48] [496000] global_step=496000, grad_norm=3.1737725734710693, loss=1.1404075622558594
I0310 23:45:59.591293 139708407461632 logging_writer.py:48] [496100] global_step=496100, grad_norm=2.720940113067627, loss=1.0631895065307617
I0310 23:46:45.213478 139708415854336 logging_writer.py:48] [496200] global_step=496200, grad_norm=3.2800369262695312, loss=1.0968996286392212
I0310 23:47:30.505059 139708407461632 logging_writer.py:48] [496300] global_step=496300, grad_norm=3.0816099643707275, loss=2.420799970626831
I0310 23:48:15.890811 139708415854336 logging_writer.py:48] [496400] global_step=496400, grad_norm=3.222933053970337, loss=2.440675735473633
I0310 23:49:01.359714 139708407461632 logging_writer.py:48] [496500] global_step=496500, grad_norm=3.083146095275879, loss=2.062939167022705
I0310 23:49:46.394906 139708415854336 logging_writer.py:48] [496600] global_step=496600, grad_norm=3.048645496368408, loss=1.4580607414245605
I0310 23:50:31.937942 139708407461632 logging_writer.py:48] [496700] global_step=496700, grad_norm=3.2841081619262695, loss=1.2164435386657715
I0310 23:51:16.304693 139902746892096 spec.py:321] Evaluating on the training split.
I0310 23:51:27.882186 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 23:51:49.721037 139902746892096 spec.py:349] Evaluating on the test split.
I0310 23:51:51.396831 139902746892096 submission_runner.py:411] Time since start: 240634.43s, 	Step: 496799, 	{'train/accuracy': 0.88783198595047, 'train/loss': 0.4155504107475281, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 222278.61622595787, 'total_duration': 240634.42562317848, 'accumulated_submission_time': 222278.61622595787, 'accumulated_eval_time': 18292.674980401993, 'accumulated_logging_time': 37.12426042556763}
I0310 23:51:51.489371 139708415854336 logging_writer.py:48] [496799] accumulated_eval_time=18292.674980, accumulated_logging_time=37.124260, accumulated_submission_time=222278.616226, global_step=496799, preemption_count=0, score=222278.616226, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=240634.425623, train/accuracy=0.887832, train/loss=0.415550, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 23:51:52.280711 139708407461632 logging_writer.py:48] [496800] global_step=496800, grad_norm=2.925269365310669, loss=2.265007257461548
I0310 23:52:32.302603 139708415854336 logging_writer.py:48] [496900] global_step=496900, grad_norm=3.267965316772461, loss=1.2067599296569824
I0310 23:53:17.569899 139708407461632 logging_writer.py:48] [497000] global_step=497000, grad_norm=3.2057149410247803, loss=1.097614049911499
I0310 23:54:03.265952 139708415854336 logging_writer.py:48] [497100] global_step=497100, grad_norm=2.954730749130249, loss=1.7544068098068237
I0310 23:54:48.712501 139708407461632 logging_writer.py:48] [497200] global_step=497200, grad_norm=2.9445321559906006, loss=1.0927283763885498
I0310 23:55:34.205560 139708415854336 logging_writer.py:48] [497300] global_step=497300, grad_norm=3.068509101867676, loss=1.168166995048523
I0310 23:56:19.607108 139708407461632 logging_writer.py:48] [497400] global_step=497400, grad_norm=2.828342914581299, loss=1.3246585130691528
I0310 23:57:04.653844 139708415854336 logging_writer.py:48] [497500] global_step=497500, grad_norm=3.405630111694336, loss=1.2669909000396729
I0310 23:57:50.021777 139708407461632 logging_writer.py:48] [497600] global_step=497600, grad_norm=3.2927584648132324, loss=1.1761432886123657
I0310 23:58:35.342902 139708415854336 logging_writer.py:48] [497700] global_step=497700, grad_norm=3.011096954345703, loss=2.4641683101654053
I0310 23:58:51.547261 139902746892096 spec.py:321] Evaluating on the training split.
I0310 23:59:03.207290 139902746892096 spec.py:333] Evaluating on the validation split.
I0310 23:59:26.274497 139902746892096 spec.py:349] Evaluating on the test split.
I0310 23:59:27.946156 139902746892096 submission_runner.py:411] Time since start: 241090.97s, 	Step: 497738, 	{'train/accuracy': 0.8870507478713989, 'train/loss': 0.4186049699783325, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 222698.61618041992, 'total_duration': 241090.97495818138, 'accumulated_submission_time': 222698.61618041992, 'accumulated_eval_time': 18329.073879480362, 'accumulated_logging_time': 37.22601556777954}
I0310 23:59:28.035845 139708407461632 logging_writer.py:48] [497738] accumulated_eval_time=18329.073879, accumulated_logging_time=37.226016, accumulated_submission_time=222698.616180, global_step=497738, preemption_count=0, score=222698.616180, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=241090.974958, train/accuracy=0.887051, train/loss=0.418605, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0310 23:59:52.843527 139708415854336 logging_writer.py:48] [497800] global_step=497800, grad_norm=3.0893139839172363, loss=2.715487003326416
I0311 00:00:36.247782 139708407461632 logging_writer.py:48] [497900] global_step=497900, grad_norm=2.9989993572235107, loss=1.5772275924682617
I0311 00:01:21.887195 139708415854336 logging_writer.py:48] [498000] global_step=498000, grad_norm=2.98490047454834, loss=1.472720980644226
I0311 00:02:07.443435 139708407461632 logging_writer.py:48] [498100] global_step=498100, grad_norm=3.046924591064453, loss=1.1564325094223022
I0311 00:02:52.662616 139708415854336 logging_writer.py:48] [498200] global_step=498200, grad_norm=3.1801302433013916, loss=2.3466944694519043
I0311 00:03:37.963306 139708407461632 logging_writer.py:48] [498300] global_step=498300, grad_norm=3.744720697402954, loss=2.944857120513916
I0311 00:04:23.109541 139708415854336 logging_writer.py:48] [498400] global_step=498400, grad_norm=2.905163526535034, loss=1.795188546180725
I0311 00:05:08.705887 139708407461632 logging_writer.py:48] [498500] global_step=498500, grad_norm=2.9853460788726807, loss=1.1029218435287476
I0311 00:05:54.064322 139708415854336 logging_writer.py:48] [498600] global_step=498600, grad_norm=2.9873340129852295, loss=1.572463870048523
I0311 00:06:28.257026 139902746892096 spec.py:321] Evaluating on the training split.
I0311 00:06:39.861460 139902746892096 spec.py:333] Evaluating on the validation split.
I0311 00:07:01.341305 139902746892096 spec.py:349] Evaluating on the test split.
I0311 00:07:03.017356 139902746892096 submission_runner.py:411] Time since start: 241546.05s, 	Step: 498677, 	{'train/accuracy': 0.8886913657188416, 'train/loss': 0.412122905254364, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 223118.7795138359, 'total_duration': 241546.0461564064, 'accumulated_submission_time': 223118.7795138359, 'accumulated_eval_time': 18363.83420419693, 'accumulated_logging_time': 37.32507681846619}
I0311 00:07:03.106936 139708407461632 logging_writer.py:48] [498677] accumulated_eval_time=18363.834204, accumulated_logging_time=37.325077, accumulated_submission_time=223118.779514, global_step=498677, preemption_count=0, score=223118.779514, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=241546.046156, train/accuracy=0.888691, train/loss=0.412123, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0311 00:07:12.554462 139708415854336 logging_writer.py:48] [498700] global_step=498700, grad_norm=4.059969902038574, loss=3.1540513038635254
I0311 00:07:54.553428 139708407461632 logging_writer.py:48] [498800] global_step=498800, grad_norm=3.4789223670959473, loss=3.1743669509887695
I0311 00:08:39.786190 139708415854336 logging_writer.py:48] [498900] global_step=498900, grad_norm=3.018346071243286, loss=1.3188531398773193
I0311 00:09:25.267776 139708407461632 logging_writer.py:48] [499000] global_step=499000, grad_norm=3.87101674079895, loss=3.2806992530822754
I0311 00:10:10.774332 139708415854336 logging_writer.py:48] [499100] global_step=499100, grad_norm=2.893882989883423, loss=1.3308696746826172
I0311 00:10:56.335675 139708407461632 logging_writer.py:48] [499200] global_step=499200, grad_norm=2.999054431915283, loss=2.3193178176879883
I0311 00:11:41.472235 139708415854336 logging_writer.py:48] [499300] global_step=499300, grad_norm=3.41579270362854, loss=1.6739792823791504
I0311 00:12:26.620571 139708407461632 logging_writer.py:48] [499400] global_step=499400, grad_norm=3.0740163326263428, loss=1.5130078792572021
I0311 00:13:11.855445 139708415854336 logging_writer.py:48] [499500] global_step=499500, grad_norm=3.240323543548584, loss=1.1050188541412354
I0311 00:13:57.122866 139708407461632 logging_writer.py:48] [499600] global_step=499600, grad_norm=3.0164191722869873, loss=1.1019258499145508
I0311 00:14:03.179627 139902746892096 spec.py:321] Evaluating on the training split.
I0311 00:14:14.602116 139902746892096 spec.py:333] Evaluating on the validation split.
I0311 00:14:37.291917 139902746892096 spec.py:349] Evaluating on the test split.
I0311 00:14:38.965520 139902746892096 submission_runner.py:411] Time since start: 242001.99s, 	Step: 499615, 	{'train/accuracy': 0.88734370470047, 'train/loss': 0.4127151072025299, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 223538.7945408821, 'total_duration': 242001.99430322647, 'accumulated_submission_time': 223538.7945408821, 'accumulated_eval_time': 18399.62007832527, 'accumulated_logging_time': 37.42348909378052}
I0311 00:14:39.066506 139708415854336 logging_writer.py:48] [499615] accumulated_eval_time=18399.620078, accumulated_logging_time=37.423489, accumulated_submission_time=223538.794541, global_step=499615, preemption_count=0, score=223538.794541, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=242001.994303, train/accuracy=0.887344, train/loss=0.412715, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0311 00:15:12.894945 139708407461632 logging_writer.py:48] [499700] global_step=499700, grad_norm=4.089073181152344, loss=3.112631320953369
I0311 00:15:57.734979 139708415854336 logging_writer.py:48] [499800] global_step=499800, grad_norm=3.110781192779541, loss=2.2485952377319336
I0311 00:16:43.109531 139708407461632 logging_writer.py:48] [499900] global_step=499900, grad_norm=3.687429189682007, loss=3.030904531478882
I0311 00:17:28.339520 139708415854336 logging_writer.py:48] [500000] global_step=500000, grad_norm=3.03763747215271, loss=1.168853998184204
I0311 00:18:13.648872 139708407461632 logging_writer.py:48] [500100] global_step=500100, grad_norm=3.2120325565338135, loss=1.2125049829483032
I0311 00:18:58.947076 139708415854336 logging_writer.py:48] [500200] global_step=500200, grad_norm=2.8394274711608887, loss=1.822195053100586
I0311 00:19:44.127855 139708407461632 logging_writer.py:48] [500300] global_step=500300, grad_norm=3.2304303646087646, loss=2.0435068607330322
I0311 00:20:29.340315 139708415854336 logging_writer.py:48] [500400] global_step=500400, grad_norm=3.3152875900268555, loss=1.1031852960586548
I0311 00:21:14.836275 139708407461632 logging_writer.py:48] [500500] global_step=500500, grad_norm=3.2169103622436523, loss=1.4304587841033936
I0311 00:21:39.179764 139902746892096 spec.py:321] Evaluating on the training split.
I0311 00:21:50.531757 139902746892096 spec.py:333] Evaluating on the validation split.
I0311 00:22:12.702028 139902746892096 spec.py:349] Evaluating on the test split.
I0311 00:22:14.379018 139902746892096 submission_runner.py:411] Time since start: 242457.41s, 	Step: 500556, 	{'train/accuracy': 0.8880664110183716, 'train/loss': 0.41742244362831116, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 223958.84918498993, 'total_duration': 242457.40779733658, 'accumulated_submission_time': 223958.84918498993, 'accumulated_eval_time': 18434.819314956665, 'accumulated_logging_time': 37.533724308013916}
I0311 00:22:14.482776 139708415854336 logging_writer.py:48] [500556] accumulated_eval_time=18434.819315, accumulated_logging_time=37.533724, accumulated_submission_time=223958.849185, global_step=500556, preemption_count=0, score=223958.849185, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=242457.407797, train/accuracy=0.888066, train/loss=0.417422, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0311 00:22:32.184842 139708407461632 logging_writer.py:48] [500600] global_step=500600, grad_norm=3.409005641937256, loss=2.572885036468506
I0311 00:23:14.497074 139708415854336 logging_writer.py:48] [500700] global_step=500700, grad_norm=3.0638511180877686, loss=1.5681192874908447
I0311 00:23:59.792742 139708407461632 logging_writer.py:48] [500800] global_step=500800, grad_norm=2.970970869064331, loss=1.1950548887252808
I0311 00:24:45.019016 139708415854336 logging_writer.py:48] [500900] global_step=500900, grad_norm=3.1493382453918457, loss=1.2001539468765259
I0311 00:25:30.358668 139708407461632 logging_writer.py:48] [501000] global_step=501000, grad_norm=3.3900723457336426, loss=1.1747106313705444
I0311 00:26:15.905261 139708415854336 logging_writer.py:48] [501100] global_step=501100, grad_norm=3.141972303390503, loss=1.1157876253128052
I0311 00:27:01.038352 139708407461632 logging_writer.py:48] [501200] global_step=501200, grad_norm=3.413095474243164, loss=1.1243903636932373
I0311 00:27:46.476327 139708415854336 logging_writer.py:48] [501300] global_step=501300, grad_norm=3.343459367752075, loss=1.056075096130371
I0311 00:28:31.877725 139708407461632 logging_writer.py:48] [501400] global_step=501400, grad_norm=3.0552830696105957, loss=1.7374905347824097
I0311 00:29:14.537522 139902746892096 spec.py:321] Evaluating on the training split.
I0311 00:29:26.087562 139902746892096 spec.py:333] Evaluating on the validation split.
I0311 00:29:48.285028 139902746892096 spec.py:349] Evaluating on the test split.
I0311 00:29:49.959444 139902746892096 submission_runner.py:411] Time since start: 242912.99s, 	Step: 501496, 	{'train/accuracy': 0.8877148032188416, 'train/loss': 0.4193105101585388, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 224378.8450908661, 'total_duration': 242912.9882376194, 'accumulated_submission_time': 224378.8450908661, 'accumulated_eval_time': 18470.24125123024, 'accumulated_logging_time': 37.64730954170227}
I0311 00:29:50.047994 139708415854336 logging_writer.py:48] [501496] accumulated_eval_time=18470.241251, accumulated_logging_time=37.647310, accumulated_submission_time=224378.845091, global_step=501496, preemption_count=0, score=224378.845091, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=242912.988238, train/accuracy=0.887715, train/loss=0.419311, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0311 00:29:52.014700 139708407461632 logging_writer.py:48] [501500] global_step=501500, grad_norm=3.124561071395874, loss=1.274139165878296
I0311 00:30:32.302877 139708415854336 logging_writer.py:48] [501600] global_step=501600, grad_norm=2.9778809547424316, loss=1.3235056400299072
I0311 00:31:17.639101 139708407461632 logging_writer.py:48] [501700] global_step=501700, grad_norm=3.1596457958221436, loss=2.572166919708252
I0311 00:32:03.203568 139708415854336 logging_writer.py:48] [501800] global_step=501800, grad_norm=3.4825804233551025, loss=3.1231980323791504
I0311 00:32:48.449416 139708407461632 logging_writer.py:48] [501900] global_step=501900, grad_norm=3.2263312339782715, loss=1.1582664251327515
I0311 00:33:33.572528 139708415854336 logging_writer.py:48] [502000] global_step=502000, grad_norm=2.7158217430114746, loss=1.0729135274887085
I0311 00:34:18.899661 139708407461632 logging_writer.py:48] [502100] global_step=502100, grad_norm=3.438305139541626, loss=2.598271369934082
I0311 00:35:03.862831 139708415854336 logging_writer.py:48] [502200] global_step=502200, grad_norm=3.2417895793914795, loss=1.173897385597229
I0311 00:35:49.245469 139708407461632 logging_writer.py:48] [502300] global_step=502300, grad_norm=2.961275339126587, loss=2.255175828933716
I0311 00:36:34.651409 139708415854336 logging_writer.py:48] [502400] global_step=502400, grad_norm=3.1925153732299805, loss=2.309023857116699
I0311 00:36:50.132421 139902746892096 spec.py:321] Evaluating on the training split.
I0311 00:37:01.625194 139902746892096 spec.py:333] Evaluating on the validation split.
I0311 00:37:23.988964 139902746892096 spec.py:349] Evaluating on the test split.
I0311 00:37:25.662298 139902746892096 submission_runner.py:411] Time since start: 243368.69s, 	Step: 502436, 	{'train/accuracy': 0.8877733945846558, 'train/loss': 0.4132482409477234, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 224798.8704931736, 'total_duration': 243368.6910982132, 'accumulated_submission_time': 224798.8704931736, 'accumulated_eval_time': 18505.77114391327, 'accumulated_logging_time': 37.74493646621704}
I0311 00:37:25.752479 139708407461632 logging_writer.py:48] [502436] accumulated_eval_time=18505.771144, accumulated_logging_time=37.744936, accumulated_submission_time=224798.870493, global_step=502436, preemption_count=0, score=224798.870493, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=243368.691098, train/accuracy=0.887773, train/loss=0.413248, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0311 00:37:51.312531 139708415854336 logging_writer.py:48] [502500] global_step=502500, grad_norm=2.9298624992370605, loss=2.3803980350494385
I0311 00:38:34.877269 139708407461632 logging_writer.py:48] [502600] global_step=502600, grad_norm=2.820143461227417, loss=1.0192943811416626
I0311 00:39:20.078775 139708415854336 logging_writer.py:48] [502700] global_step=502700, grad_norm=3.188744068145752, loss=2.485177755355835
I0311 00:40:05.560031 139708407461632 logging_writer.py:48] [502800] global_step=502800, grad_norm=3.153771162033081, loss=1.0643866062164307
I0311 00:40:50.779900 139708415854336 logging_writer.py:48] [502900] global_step=502900, grad_norm=3.1147620677948, loss=2.661318778991699
I0311 00:41:36.463346 139708407461632 logging_writer.py:48] [503000] global_step=503000, grad_norm=4.145978927612305, loss=3.308760643005371
I0311 00:42:21.902166 139708415854336 logging_writer.py:48] [503100] global_step=503100, grad_norm=3.125396490097046, loss=1.627146601676941
I0311 00:43:07.015383 139708407461632 logging_writer.py:48] [503200] global_step=503200, grad_norm=3.7228429317474365, loss=2.9846227169036865
I0311 00:43:52.445169 139708415854336 logging_writer.py:48] [503300] global_step=503300, grad_norm=3.1264290809631348, loss=2.643170118331909
I0311 00:44:26.066252 139902746892096 spec.py:321] Evaluating on the training split.
I0311 00:44:37.610490 139902746892096 spec.py:333] Evaluating on the validation split.
I0311 00:44:58.524696 139902746892096 spec.py:349] Evaluating on the test split.
I0311 00:45:00.209011 139902746892096 submission_runner.py:411] Time since start: 243823.24s, 	Step: 503376, 	{'train/accuracy': 0.8895312547683716, 'train/loss': 0.41039058566093445, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 225219.12637400627, 'total_duration': 243823.23779582977, 'accumulated_submission_time': 225219.12637400627, 'accumulated_eval_time': 18539.91388988495, 'accumulated_logging_time': 37.84392428398132}
I0311 00:45:00.316401 139708407461632 logging_writer.py:48] [503376] accumulated_eval_time=18539.913890, accumulated_logging_time=37.843924, accumulated_submission_time=225219.126374, global_step=503376, preemption_count=0, score=225219.126374, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=243823.237796, train/accuracy=0.889531, train/loss=0.410391, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0311 00:45:10.164250 139708415854336 logging_writer.py:48] [503400] global_step=503400, grad_norm=3.216054916381836, loss=1.2020083665847778
I0311 00:45:51.800689 139708407461632 logging_writer.py:48] [503500] global_step=503500, grad_norm=3.0103676319122314, loss=1.8034898042678833
I0311 00:46:37.295763 139708415854336 logging_writer.py:48] [503600] global_step=503600, grad_norm=3.8177387714385986, loss=3.1295549869537354
I0311 00:47:23.046909 139708407461632 logging_writer.py:48] [503700] global_step=503700, grad_norm=3.0513906478881836, loss=1.1169852018356323
I0311 00:48:08.611449 139708415854336 logging_writer.py:48] [503800] global_step=503800, grad_norm=3.2338573932647705, loss=1.1558878421783447
I0311 00:48:53.629084 139708407461632 logging_writer.py:48] [503900] global_step=503900, grad_norm=3.000908851623535, loss=1.1064012050628662
I0311 00:49:39.198616 139708415854336 logging_writer.py:48] [504000] global_step=504000, grad_norm=2.9828720092773438, loss=2.513848304748535
I0311 00:50:24.616630 139708407461632 logging_writer.py:48] [504100] global_step=504100, grad_norm=3.12926983833313, loss=2.8602616786956787
I0311 00:51:11.007328 139708415854336 logging_writer.py:48] [504200] global_step=504200, grad_norm=2.781899929046631, loss=1.7035695314407349
I0311 00:51:56.434692 139708407461632 logging_writer.py:48] [504300] global_step=504300, grad_norm=3.098649263381958, loss=1.0811439752578735
I0311 00:52:00.576227 139902746892096 spec.py:321] Evaluating on the training split.
I0311 00:52:12.233021 139902746892096 spec.py:333] Evaluating on the validation split.
I0311 00:52:35.110519 139902746892096 spec.py:349] Evaluating on the test split.
I0311 00:52:36.783120 139902746892096 submission_runner.py:411] Time since start: 244279.81s, 	Step: 504311, 	{'train/accuracy': 0.8867968320846558, 'train/loss': 0.42240333557128906, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 225639.3281033039, 'total_duration': 244279.811917305, 'accumulated_submission_time': 225639.3281033039, 'accumulated_eval_time': 18576.120767831802, 'accumulated_logging_time': 37.96061396598816}
I0311 00:52:36.875859 139708415854336 logging_writer.py:48] [504311] accumulated_eval_time=18576.120768, accumulated_logging_time=37.960614, accumulated_submission_time=225639.328103, global_step=504311, preemption_count=0, score=225639.328103, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=244279.811917, train/accuracy=0.886797, train/loss=0.422403, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0311 00:53:12.460796 139708407461632 logging_writer.py:48] [504400] global_step=504400, grad_norm=3.37587571144104, loss=2.8121402263641357
I0311 00:53:57.667926 139708415854336 logging_writer.py:48] [504500] global_step=504500, grad_norm=3.2441511154174805, loss=1.0214736461639404
I0311 00:54:43.272016 139708407461632 logging_writer.py:48] [504600] global_step=504600, grad_norm=3.2677974700927734, loss=1.2540640830993652
I0311 00:55:28.891266 139708415854336 logging_writer.py:48] [504700] global_step=504700, grad_norm=2.9671506881713867, loss=1.3303109407424927
I0311 00:56:14.195566 139708407461632 logging_writer.py:48] [504800] global_step=504800, grad_norm=3.5367658138275146, loss=1.0637532472610474
I0311 00:56:59.746866 139708415854336 logging_writer.py:48] [504900] global_step=504900, grad_norm=2.7441611289978027, loss=1.6270227432250977
I0311 00:57:45.085551 139708407461632 logging_writer.py:48] [505000] global_step=505000, grad_norm=3.2531394958496094, loss=1.0733436346054077
I0311 00:58:30.425967 139708415854336 logging_writer.py:48] [505100] global_step=505100, grad_norm=2.991619110107422, loss=1.1506495475769043
I0311 00:59:15.816473 139708407461632 logging_writer.py:48] [505200] global_step=505200, grad_norm=3.097743034362793, loss=0.9831578731536865
I0311 00:59:36.791236 139902746892096 spec.py:321] Evaluating on the training split.
I0311 00:59:48.020490 139902746892096 spec.py:333] Evaluating on the validation split.
I0311 01:00:11.573336 139902746892096 spec.py:349] Evaluating on the test split.
I0311 01:00:13.247869 139902746892096 submission_runner.py:411] Time since start: 244736.28s, 	Step: 505248, 	{'train/accuracy': 0.888671875, 'train/loss': 0.41712310910224915, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 226059.18442249298, 'total_duration': 244736.27666950226, 'accumulated_submission_time': 226059.18442249298, 'accumulated_eval_time': 18612.577392101288, 'accumulated_logging_time': 38.06256675720215}
I0311 01:00:13.336816 139708415854336 logging_writer.py:48] [505248] accumulated_eval_time=18612.577392, accumulated_logging_time=38.062567, accumulated_submission_time=226059.184422, global_step=505248, preemption_count=0, score=226059.184422, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=244736.276670, train/accuracy=0.888672, train/loss=0.417123, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0311 01:00:34.253846 139708407461632 logging_writer.py:48] [505300] global_step=505300, grad_norm=2.946213722229004, loss=1.4514784812927246
I0311 01:01:16.930387 139708415854336 logging_writer.py:48] [505400] global_step=505400, grad_norm=3.089423179626465, loss=2.5848214626312256
I0311 01:02:02.768221 139708407461632 logging_writer.py:48] [505500] global_step=505500, grad_norm=3.202972412109375, loss=1.0842827558517456
I0311 01:02:48.903711 139708415854336 logging_writer.py:48] [505600] global_step=505600, grad_norm=3.0897505283355713, loss=1.0911078453063965
I0311 01:03:34.118204 139708407461632 logging_writer.py:48] [505700] global_step=505700, grad_norm=3.4275224208831787, loss=1.2080775499343872
I0311 01:04:19.627265 139708415854336 logging_writer.py:48] [505800] global_step=505800, grad_norm=3.1361749172210693, loss=1.001927375793457
I0311 01:05:04.900382 139708407461632 logging_writer.py:48] [505900] global_step=505900, grad_norm=3.224942922592163, loss=1.4199708700180054
I0311 01:05:50.015937 139708415854336 logging_writer.py:48] [506000] global_step=506000, grad_norm=3.716522455215454, loss=3.2409441471099854
I0311 01:06:35.292017 139708407461632 logging_writer.py:48] [506100] global_step=506100, grad_norm=3.0926401615142822, loss=1.1324310302734375
I0311 01:07:13.370341 139902746892096 spec.py:321] Evaluating on the training split.
I0311 01:07:24.884003 139902746892096 spec.py:333] Evaluating on the validation split.
I0311 01:07:47.566135 139902746892096 spec.py:349] Evaluating on the test split.
I0311 01:07:49.231870 139902746892096 submission_runner.py:411] Time since start: 245192.26s, 	Step: 506186, 	{'train/accuracy': 0.88832026720047, 'train/loss': 0.41861864924430847, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 226479.16041707993, 'total_duration': 245192.26065707207, 'accumulated_submission_time': 226479.16041707993, 'accumulated_eval_time': 18648.438907146454, 'accumulated_logging_time': 38.15953755378723}
I0311 01:07:49.325641 139708415854336 logging_writer.py:48] [506186] accumulated_eval_time=18648.438907, accumulated_logging_time=38.159538, accumulated_submission_time=226479.160417, global_step=506186, preemption_count=0, score=226479.160417, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=245192.260657, train/accuracy=0.888320, train/loss=0.418619, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0311 01:07:55.232114 139708407461632 logging_writer.py:48] [506200] global_step=506200, grad_norm=3.41780686378479, loss=3.092726230621338
I0311 01:08:35.879394 139708415854336 logging_writer.py:48] [506300] global_step=506300, grad_norm=2.997971773147583, loss=2.0242879390716553
I0311 01:09:20.909447 139708407461632 logging_writer.py:48] [506400] global_step=506400, grad_norm=3.2153093814849854, loss=1.780045747756958
I0311 01:10:06.085356 139708415854336 logging_writer.py:48] [506500] global_step=506500, grad_norm=3.149930238723755, loss=1.3009250164031982
I0311 01:10:51.445268 139708407461632 logging_writer.py:48] [506600] global_step=506600, grad_norm=3.150498628616333, loss=1.1248810291290283
I0311 01:11:36.683736 139708415854336 logging_writer.py:48] [506700] global_step=506700, grad_norm=3.1662251949310303, loss=1.4675486087799072
I0311 01:12:22.119853 139708407461632 logging_writer.py:48] [506800] global_step=506800, grad_norm=4.615971088409424, loss=3.2372612953186035
I0311 01:13:07.242110 139708415854336 logging_writer.py:48] [506900] global_step=506900, grad_norm=3.0450327396392822, loss=1.083239197731018
I0311 01:13:52.448214 139708407461632 logging_writer.py:48] [507000] global_step=507000, grad_norm=2.9591681957244873, loss=1.3507003784179688
I0311 01:14:37.922716 139708415854336 logging_writer.py:48] [507100] global_step=507100, grad_norm=3.1546008586883545, loss=1.0575435161590576
I0311 01:14:49.241278 139902746892096 spec.py:321] Evaluating on the training split.
I0311 01:15:00.739004 139902746892096 spec.py:333] Evaluating on the validation split.
I0311 01:15:22.476364 139902746892096 spec.py:349] Evaluating on the test split.
I0311 01:15:24.153559 139902746892096 submission_runner.py:411] Time since start: 245647.18s, 	Step: 507127, 	{'train/accuracy': 0.8861913681030273, 'train/loss': 0.4204094707965851, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 226899.0175216198, 'total_duration': 245647.18233394623, 'accumulated_submission_time': 226899.0175216198, 'accumulated_eval_time': 18683.35116648674, 'accumulated_logging_time': 38.262371301651}
I0311 01:15:24.255750 139708407461632 logging_writer.py:48] [507127] accumulated_eval_time=18683.351166, accumulated_logging_time=38.262371, accumulated_submission_time=226899.017522, global_step=507127, preemption_count=0, score=226899.017522, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=245647.182334, train/accuracy=0.886191, train/loss=0.420409, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0311 01:15:53.378232 139708415854336 logging_writer.py:48] [507200] global_step=507200, grad_norm=3.416083812713623, loss=1.1732006072998047
I0311 01:16:38.060457 139708407461632 logging_writer.py:48] [507300] global_step=507300, grad_norm=2.9359781742095947, loss=1.0873655080795288
I0311 01:17:23.604858 139708415854336 logging_writer.py:48] [507400] global_step=507400, grad_norm=3.3749101161956787, loss=3.009300470352173
I0311 01:18:09.260499 139708407461632 logging_writer.py:48] [507500] global_step=507500, grad_norm=3.3106558322906494, loss=2.744432210922241
I0311 01:18:54.405198 139708415854336 logging_writer.py:48] [507600] global_step=507600, grad_norm=4.26457405090332, loss=3.122026205062866
I0311 01:19:39.548737 139708407461632 logging_writer.py:48] [507700] global_step=507700, grad_norm=3.2464330196380615, loss=1.3740131855010986
I0311 01:20:24.912859 139708415854336 logging_writer.py:48] [507800] global_step=507800, grad_norm=3.210139513015747, loss=2.202829599380493
I0311 01:21:09.997552 139708407461632 logging_writer.py:48] [507900] global_step=507900, grad_norm=3.2035717964172363, loss=1.6844971179962158
I0311 01:21:55.745815 139708415854336 logging_writer.py:48] [508000] global_step=508000, grad_norm=3.0173444747924805, loss=1.012274146080017
I0311 01:22:24.448776 139902746892096 spec.py:321] Evaluating on the training split.
I0311 01:22:35.980065 139902746892096 spec.py:333] Evaluating on the validation split.
I0311 01:22:59.210626 139902746892096 spec.py:349] Evaluating on the test split.
I0311 01:23:00.882643 139902746892096 submission_runner.py:411] Time since start: 246103.91s, 	Step: 508065, 	{'train/accuracy': 0.8902148008346558, 'train/loss': 0.40682268142700195, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 227319.15297055244, 'total_duration': 246103.91144418716, 'accumulated_submission_time': 227319.15297055244, 'accumulated_eval_time': 18719.78503012657, 'accumulated_logging_time': 38.373663663864136}
I0311 01:23:00.974477 139708407461632 logging_writer.py:48] [508065] accumulated_eval_time=18719.785030, accumulated_logging_time=38.373664, accumulated_submission_time=227319.152971, global_step=508065, preemption_count=0, score=227319.152971, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=246103.911444, train/accuracy=0.890215, train/loss=0.406823, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0311 01:23:15.145292 139708415854336 logging_writer.py:48] [508100] global_step=508100, grad_norm=3.704298734664917, loss=3.253225326538086
I0311 01:23:57.069957 139708407461632 logging_writer.py:48] [508200] global_step=508200, grad_norm=3.4464006423950195, loss=1.1515475511550903
I0311 01:24:42.308195 139708415854336 logging_writer.py:48] [508300] global_step=508300, grad_norm=3.0961456298828125, loss=1.1007546186447144
I0311 01:25:27.883882 139708407461632 logging_writer.py:48] [508400] global_step=508400, grad_norm=3.0216832160949707, loss=1.1764075756072998
I0311 01:26:13.040953 139708415854336 logging_writer.py:48] [508500] global_step=508500, grad_norm=2.7973575592041016, loss=2.0966436862945557
I0311 01:26:58.281736 139708407461632 logging_writer.py:48] [508600] global_step=508600, grad_norm=3.2375967502593994, loss=1.1135444641113281
I0311 01:27:44.052414 139708415854336 logging_writer.py:48] [508700] global_step=508700, grad_norm=3.1019442081451416, loss=1.185253620147705
I0311 01:28:29.145264 139708407461632 logging_writer.py:48] [508800] global_step=508800, grad_norm=3.8465969562530518, loss=3.2405402660369873
I0311 01:29:14.473647 139708415854336 logging_writer.py:48] [508900] global_step=508900, grad_norm=3.729947805404663, loss=3.1560580730438232
I0311 01:29:59.723476 139708407461632 logging_writer.py:48] [509000] global_step=509000, grad_norm=3.2093894481658936, loss=1.0263420343399048
I0311 01:30:01.259459 139902746892096 spec.py:321] Evaluating on the training split.
I0311 01:30:12.698557 139902746892096 spec.py:333] Evaluating on the validation split.
I0311 01:30:33.448570 139902746892096 spec.py:349] Evaluating on the test split.
I0311 01:30:35.142888 139902746892096 submission_runner.py:411] Time since start: 246558.17s, 	Step: 509005, 	{'train/accuracy': 0.8861913681030273, 'train/loss': 0.420152872800827, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 227739.37865519524, 'total_duration': 246558.1716852188, 'accumulated_submission_time': 227739.37865519524, 'accumulated_eval_time': 18753.6684820652, 'accumulated_logging_time': 38.47547960281372}
I0311 01:30:35.240521 139708415854336 logging_writer.py:48] [509005] accumulated_eval_time=18753.668482, accumulated_logging_time=38.475480, accumulated_submission_time=227739.378655, global_step=509005, preemption_count=0, score=227739.378655, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=246558.171685, train/accuracy=0.886191, train/loss=0.420153, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0311 01:31:13.725083 139708407461632 logging_writer.py:48] [509100] global_step=509100, grad_norm=3.0125856399536133, loss=1.1045763492584229
I0311 01:31:58.953994 139708415854336 logging_writer.py:48] [509200] global_step=509200, grad_norm=2.9973151683807373, loss=1.0776009559631348
I0311 01:32:44.346681 139708407461632 logging_writer.py:48] [509300] global_step=509300, grad_norm=3.1354188919067383, loss=2.16745924949646
I0311 01:33:29.997340 139708415854336 logging_writer.py:48] [509400] global_step=509400, grad_norm=3.192701578140259, loss=1.1427628993988037
I0311 01:34:15.119082 139708407461632 logging_writer.py:48] [509500] global_step=509500, grad_norm=2.9776268005371094, loss=1.2269790172576904
I0311 01:35:00.643032 139708415854336 logging_writer.py:48] [509600] global_step=509600, grad_norm=2.9968278408050537, loss=1.622204303741455
I0311 01:35:45.744262 139708407461632 logging_writer.py:48] [509700] global_step=509700, grad_norm=3.2684099674224854, loss=2.4761765003204346
I0311 01:36:30.967135 139708415854336 logging_writer.py:48] [509800] global_step=509800, grad_norm=2.975921630859375, loss=1.0354957580566406
I0311 01:37:16.359568 139708407461632 logging_writer.py:48] [509900] global_step=509900, grad_norm=3.025583505630493, loss=1.0416228771209717
I0311 01:37:35.178910 139902746892096 spec.py:321] Evaluating on the training split.
I0311 01:37:46.800609 139902746892096 spec.py:333] Evaluating on the validation split.
I0311 01:38:09.687796 139902746892096 spec.py:349] Evaluating on the test split.
I0311 01:38:11.364530 139902746892096 submission_runner.py:411] Time since start: 247014.39s, 	Step: 509943, 	{'train/accuracy': 0.8894726634025574, 'train/loss': 0.4081210494041443, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 228159.25987029076, 'total_duration': 247014.39329242706, 'accumulated_submission_time': 228159.25987029076, 'accumulated_eval_time': 18789.85410284996, 'accumulated_logging_time': 38.581543922424316}
I0311 01:38:11.468938 139708415854336 logging_writer.py:48] [509943] accumulated_eval_time=18789.854103, accumulated_logging_time=38.581544, accumulated_submission_time=228159.259870, global_step=509943, preemption_count=0, score=228159.259870, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=247014.393292, train/accuracy=0.889473, train/loss=0.408121, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0311 01:38:34.308298 139708407461632 logging_writer.py:48] [510000] global_step=510000, grad_norm=2.960465431213379, loss=1.3108855485916138
I0311 01:39:17.411661 139708415854336 logging_writer.py:48] [510100] global_step=510100, grad_norm=3.1705124378204346, loss=1.1130741834640503
I0311 01:40:02.806001 139708407461632 logging_writer.py:48] [510200] global_step=510200, grad_norm=3.1718902587890625, loss=1.067874550819397
I0311 01:40:48.547192 139708415854336 logging_writer.py:48] [510300] global_step=510300, grad_norm=3.0052452087402344, loss=1.0917690992355347
I0311 01:41:33.739324 139708407461632 logging_writer.py:48] [510400] global_step=510400, grad_norm=4.003192901611328, loss=3.2246899604797363
I0311 01:42:19.437197 139708415854336 logging_writer.py:48] [510500] global_step=510500, grad_norm=3.534935235977173, loss=3.22796368598938
I0311 01:43:05.317068 139708407461632 logging_writer.py:48] [510600] global_step=510600, grad_norm=2.9022302627563477, loss=2.1924057006835938
I0311 01:43:50.388355 139708415854336 logging_writer.py:48] [510700] global_step=510700, grad_norm=3.2036330699920654, loss=1.2053862810134888
I0311 01:44:36.027326 139708407461632 logging_writer.py:48] [510800] global_step=510800, grad_norm=2.9738378524780273, loss=2.3800039291381836
I0311 01:45:11.669180 139902746892096 spec.py:321] Evaluating on the training split.
I0311 01:45:23.047789 139902746892096 spec.py:333] Evaluating on the validation split.
I0311 01:45:44.664413 139902746892096 spec.py:349] Evaluating on the test split.
I0311 01:45:46.343807 139902746892096 submission_runner.py:411] Time since start: 247469.37s, 	Step: 510880, 	{'train/accuracy': 0.8873046636581421, 'train/loss': 0.41740089654922485, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 228579.4013376236, 'total_duration': 247469.3725888729, 'accumulated_submission_time': 228579.4013376236, 'accumulated_eval_time': 18824.528720617294, 'accumulated_logging_time': 38.695571184158325}
I0311 01:45:46.443545 139708415854336 logging_writer.py:48] [510880] accumulated_eval_time=18824.528721, accumulated_logging_time=38.695571, accumulated_submission_time=228579.401338, global_step=510880, preemption_count=0, score=228579.401338, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=247469.372589, train/accuracy=0.887305, train/loss=0.417401, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0311 01:45:54.713922 139708407461632 logging_writer.py:48] [510900] global_step=510900, grad_norm=3.1575074195861816, loss=1.475570797920227
I0311 01:46:36.096197 139708415854336 logging_writer.py:48] [511000] global_step=511000, grad_norm=3.1359355449676514, loss=1.136002540588379
I0311 01:47:21.393668 139708407461632 logging_writer.py:48] [511100] global_step=511100, grad_norm=3.051908016204834, loss=1.053102731704712
I0311 01:48:06.921274 139708415854336 logging_writer.py:48] [511200] global_step=511200, grad_norm=3.341738224029541, loss=1.1425361633300781
I0311 01:48:51.988290 139708407461632 logging_writer.py:48] [511300] global_step=511300, grad_norm=2.830315113067627, loss=1.5662307739257812
I0311 01:49:37.429692 139708415854336 logging_writer.py:48] [511400] global_step=511400, grad_norm=2.837181806564331, loss=1.5874993801116943
I0311 01:50:23.181329 139708407461632 logging_writer.py:48] [511500] global_step=511500, grad_norm=3.0364842414855957, loss=1.2497079372406006
I0311 01:51:08.666718 139708415854336 logging_writer.py:48] [511600] global_step=511600, grad_norm=3.041247606277466, loss=1.1335417032241821
I0311 01:51:54.147434 139708407461632 logging_writer.py:48] [511700] global_step=511700, grad_norm=2.949091911315918, loss=2.4001057147979736
I0311 01:52:39.692691 139708415854336 logging_writer.py:48] [511800] global_step=511800, grad_norm=3.3086891174316406, loss=1.1128369569778442
I0311 01:52:46.517645 139902746892096 spec.py:321] Evaluating on the training split.
I0311 01:52:58.084139 139902746892096 spec.py:333] Evaluating on the validation split.
I0311 01:53:20.982785 139902746892096 spec.py:349] Evaluating on the test split.
I0311 01:53:22.652982 139902746892096 submission_runner.py:411] Time since start: 247925.68s, 	Step: 511817, 	{'train/accuracy': 0.8889257907867432, 'train/loss': 0.41287484765052795, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 228999.4161813259, 'total_duration': 247925.6817638874, 'accumulated_submission_time': 228999.4161813259, 'accumulated_eval_time': 18860.66403913498, 'accumulated_logging_time': 38.80521607398987}
I0311 01:53:22.757786 139708407461632 logging_writer.py:48] [511817] accumulated_eval_time=18860.664039, accumulated_logging_time=38.805216, accumulated_submission_time=228999.416181, global_step=511817, preemption_count=0, score=228999.416181, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=247925.681764, train/accuracy=0.888926, train/loss=0.412875, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0311 01:53:55.788065 139708415854336 logging_writer.py:48] [511900] global_step=511900, grad_norm=3.2944722175598145, loss=1.1516692638397217
I0311 01:54:40.547353 139708407461632 logging_writer.py:48] [512000] global_step=512000, grad_norm=3.0321691036224365, loss=1.0828700065612793
I0311 01:55:26.017633 139708415854336 logging_writer.py:48] [512100] global_step=512100, grad_norm=3.1150295734405518, loss=1.3397583961486816
I0311 01:56:11.429579 139708407461632 logging_writer.py:48] [512200] global_step=512200, grad_norm=3.2918686866760254, loss=2.5069310665130615
I0311 01:56:56.788570 139708415854336 logging_writer.py:48] [512300] global_step=512300, grad_norm=3.119537115097046, loss=1.1290324926376343
I0311 01:57:42.335934 139708407461632 logging_writer.py:48] [512400] global_step=512400, grad_norm=2.894726276397705, loss=2.160546064376831
I0311 01:58:27.634752 139708415854336 logging_writer.py:48] [512500] global_step=512500, grad_norm=3.2347934246063232, loss=2.6902570724487305
I0311 01:59:13.101263 139708407461632 logging_writer.py:48] [512600] global_step=512600, grad_norm=3.2343873977661133, loss=2.295916795730591
I0311 01:59:58.539690 139708415854336 logging_writer.py:48] [512700] global_step=512700, grad_norm=3.642547607421875, loss=3.026029586791992
I0311 02:00:22.754896 139902746892096 spec.py:321] Evaluating on the training split.
I0311 02:00:34.151445 139902746892096 spec.py:333] Evaluating on the validation split.
I0311 02:00:55.256938 139902746892096 spec.py:349] Evaluating on the test split.
I0311 02:00:56.939961 139902746892096 submission_runner.py:411] Time since start: 248379.97s, 	Step: 512755, 	{'train/accuracy': 0.8876953125, 'train/loss': 0.4195302724838257, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 229419.35469794273, 'total_duration': 248379.96874928474, 'accumulated_submission_time': 229419.35469794273, 'accumulated_eval_time': 18894.84908437729, 'accumulated_logging_time': 38.919668197631836}
I0311 02:00:57.032555 139708407461632 logging_writer.py:48] [512755] accumulated_eval_time=18894.849084, accumulated_logging_time=38.919668, accumulated_submission_time=229419.354698, global_step=512755, preemption_count=0, score=229419.354698, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=248379.968749, train/accuracy=0.887695, train/loss=0.419530, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0311 02:01:15.144689 139708415854336 logging_writer.py:48] [512800] global_step=512800, grad_norm=4.127535343170166, loss=3.2659378051757812
I0311 02:01:58.068140 139708407461632 logging_writer.py:48] [512900] global_step=512900, grad_norm=3.0786056518554688, loss=1.0663540363311768
I0311 02:02:43.827273 139708415854336 logging_writer.py:48] [513000] global_step=513000, grad_norm=2.8880455493927, loss=1.7310595512390137
I0311 02:03:29.489209 139708407461632 logging_writer.py:48] [513100] global_step=513100, grad_norm=3.12996768951416, loss=1.099166750907898
I0311 02:04:15.021928 139708415854336 logging_writer.py:48] [513200] global_step=513200, grad_norm=3.0493741035461426, loss=1.0380680561065674
I0311 02:05:00.395743 139708407461632 logging_writer.py:48] [513300] global_step=513300, grad_norm=3.2052977085113525, loss=1.6543437242507935
I0311 02:05:45.839837 139708415854336 logging_writer.py:48] [513400] global_step=513400, grad_norm=3.3224830627441406, loss=2.4275879859924316
I0311 02:06:31.115947 139708407461632 logging_writer.py:48] [513500] global_step=513500, grad_norm=3.868864059448242, loss=3.2308359146118164
I0311 02:07:16.705718 139708415854336 logging_writer.py:48] [513600] global_step=513600, grad_norm=3.119448661804199, loss=1.8268738985061646
I0311 02:07:57.016321 139902746892096 spec.py:321] Evaluating on the training split.
I0311 02:08:08.386100 139902746892096 spec.py:333] Evaluating on the validation split.
I0311 02:08:29.221731 139902746892096 spec.py:349] Evaluating on the test split.
I0311 02:08:30.892646 139902746892096 submission_runner.py:411] Time since start: 248833.92s, 	Step: 513690, 	{'train/accuracy': 0.8895312547683716, 'train/loss': 0.4111018180847168, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 229839.27992606163, 'total_duration': 248833.9214372635, 'accumulated_submission_time': 229839.27992606163, 'accumulated_eval_time': 18928.725402593613, 'accumulated_logging_time': 39.0208375453949}
I0311 02:08:30.983706 139708407461632 logging_writer.py:48] [513690] accumulated_eval_time=18928.725403, accumulated_logging_time=39.020838, accumulated_submission_time=229839.279926, global_step=513690, preemption_count=0, score=229839.279926, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=248833.921437, train/accuracy=0.889531, train/loss=0.411102, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0311 02:08:35.315515 139708415854336 logging_writer.py:48] [513700] global_step=513700, grad_norm=3.0727994441986084, loss=2.061554431915283
I0311 02:09:16.310207 139708407461632 logging_writer.py:48] [513800] global_step=513800, grad_norm=3.4232804775238037, loss=1.2317607402801514
I0311 02:10:01.413712 139708415854336 logging_writer.py:48] [513900] global_step=513900, grad_norm=3.498446226119995, loss=1.0677202939987183
I0311 02:10:47.094091 139708407461632 logging_writer.py:48] [514000] global_step=514000, grad_norm=3.2861530780792236, loss=1.027953028678894
I0311 02:11:32.785688 139708415854336 logging_writer.py:48] [514100] global_step=514100, grad_norm=3.9867820739746094, loss=3.3425068855285645
I0311 02:12:17.702135 139708407461632 logging_writer.py:48] [514200] global_step=514200, grad_norm=2.785496473312378, loss=1.8539296388626099
I0311 02:13:04.079803 139708415854336 logging_writer.py:48] [514300] global_step=514300, grad_norm=3.0437846183776855, loss=2.278329372406006
I0311 02:13:49.810376 139708407461632 logging_writer.py:48] [514400] global_step=514400, grad_norm=3.2909014225006104, loss=1.1477084159851074
I0311 02:14:35.491535 139708415854336 logging_writer.py:48] [514500] global_step=514500, grad_norm=3.2133326530456543, loss=1.0726770162582397
I0311 02:15:21.217568 139708407461632 logging_writer.py:48] [514600] global_step=514600, grad_norm=3.325921058654785, loss=1.2482125759124756
I0311 02:15:30.977210 139902746892096 spec.py:321] Evaluating on the training split.
I0311 02:15:42.578358 139902746892096 spec.py:333] Evaluating on the validation split.
I0311 02:16:05.985491 139902746892096 spec.py:349] Evaluating on the test split.
I0311 02:16:07.658795 139902746892096 submission_runner.py:411] Time since start: 249290.69s, 	Step: 514623, 	{'train/accuracy': 0.8900585770606995, 'train/loss': 0.4098351299762726, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 230259.215269804, 'total_duration': 249290.68759441376, 'accumulated_submission_time': 230259.215269804, 'accumulated_eval_time': 18965.406985521317, 'accumulated_logging_time': 39.12135338783264}
I0311 02:16:07.753194 139708415854336 logging_writer.py:48] [514623] accumulated_eval_time=18965.406986, accumulated_logging_time=39.121353, accumulated_submission_time=230259.215270, global_step=514623, preemption_count=0, score=230259.215270, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=249290.687594, train/accuracy=0.890059, train/loss=0.409835, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0311 02:16:38.444897 139708407461632 logging_writer.py:48] [514700] global_step=514700, grad_norm=2.943889617919922, loss=1.2375808954238892
I0311 02:17:23.128756 139708415854336 logging_writer.py:48] [514800] global_step=514800, grad_norm=3.15655255317688, loss=2.632094144821167
I0311 02:18:08.792605 139708407461632 logging_writer.py:48] [514900] global_step=514900, grad_norm=3.1024608612060547, loss=1.944523811340332
I0311 02:18:54.370974 139708415854336 logging_writer.py:48] [515000] global_step=515000, grad_norm=3.0563414096832275, loss=1.1071733236312866
I0311 02:19:39.762364 139708407461632 logging_writer.py:48] [515100] global_step=515100, grad_norm=2.99696683883667, loss=1.0156540870666504
I0311 02:20:24.957089 139708415854336 logging_writer.py:48] [515200] global_step=515200, grad_norm=3.376433849334717, loss=1.0616700649261475
I0311 02:21:10.379015 139708407461632 logging_writer.py:48] [515300] global_step=515300, grad_norm=3.9424338340759277, loss=2.727674961090088
I0311 02:21:55.593384 139708415854336 logging_writer.py:48] [515400] global_step=515400, grad_norm=3.1226561069488525, loss=1.3292391300201416
I0311 02:22:41.117351 139708407461632 logging_writer.py:48] [515500] global_step=515500, grad_norm=2.873553991317749, loss=1.1960991621017456
I0311 02:23:07.669333 139902746892096 spec.py:321] Evaluating on the training split.
I0311 02:23:19.746182 139902746892096 spec.py:333] Evaluating on the validation split.
I0311 02:23:42.703077 139902746892096 spec.py:349] Evaluating on the test split.
I0311 02:23:44.373145 139902746892096 submission_runner.py:411] Time since start: 249747.40s, 	Step: 515560, 	{'train/accuracy': 0.8890234231948853, 'train/loss': 0.41008222103118896, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 230679.0721449852, 'total_duration': 249747.4019293785, 'accumulated_submission_time': 230679.0721449852, 'accumulated_eval_time': 19002.11077594757, 'accumulated_logging_time': 39.22586536407471}
I0311 02:23:44.477083 139708415854336 logging_writer.py:48] [515560] accumulated_eval_time=19002.110776, accumulated_logging_time=39.225865, accumulated_submission_time=230679.072145, global_step=515560, preemption_count=0, score=230679.072145, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=249747.401929, train/accuracy=0.889023, train/loss=0.410082, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0311 02:24:00.611982 139708407461632 logging_writer.py:48] [515600] global_step=515600, grad_norm=3.216409921646118, loss=1.1798919439315796
I0311 02:24:42.707394 139708415854336 logging_writer.py:48] [515700] global_step=515700, grad_norm=2.887897491455078, loss=1.967888355255127
I0311 02:25:27.990049 139708407461632 logging_writer.py:48] [515800] global_step=515800, grad_norm=2.9841785430908203, loss=1.1191147565841675
I0311 02:26:13.601857 139708415854336 logging_writer.py:48] [515900] global_step=515900, grad_norm=3.135422706604004, loss=1.1924035549163818
I0311 02:26:58.924959 139708407461632 logging_writer.py:48] [516000] global_step=516000, grad_norm=3.274160861968994, loss=1.1547964811325073
I0311 02:27:44.774516 139708415854336 logging_writer.py:48] [516100] global_step=516100, grad_norm=3.4849109649658203, loss=1.1627460718154907
I0311 02:28:30.440872 139708407461632 logging_writer.py:48] [516200] global_step=516200, grad_norm=3.1655633449554443, loss=2.7842605113983154
I0311 02:29:15.551919 139708415854336 logging_writer.py:48] [516300] global_step=516300, grad_norm=3.3438236713409424, loss=1.2958803176879883
I0311 02:30:00.998525 139708407461632 logging_writer.py:48] [516400] global_step=516400, grad_norm=2.9650583267211914, loss=1.1556905508041382
I0311 02:30:44.701370 139902746892096 spec.py:321] Evaluating on the training split.
I0311 02:30:56.266780 139902746892096 spec.py:333] Evaluating on the validation split.
I0311 02:31:18.710164 139902746892096 spec.py:349] Evaluating on the test split.
I0311 02:31:20.388312 139902746892096 submission_runner.py:411] Time since start: 250203.42s, 	Step: 516498, 	{'train/accuracy': 0.8893945217132568, 'train/loss': 0.40792348980903625, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 231099.23832178116, 'total_duration': 250203.41710281372, 'accumulated_submission_time': 231099.23832178116, 'accumulated_eval_time': 19037.79772377014, 'accumulated_logging_time': 39.339415073394775}
I0311 02:31:20.481712 139708415854336 logging_writer.py:48] [516498] accumulated_eval_time=19037.797724, accumulated_logging_time=39.339415, accumulated_submission_time=231099.238322, global_step=516498, preemption_count=0, score=231099.238322, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=250203.417103, train/accuracy=0.889395, train/loss=0.407923, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0311 02:31:21.669176 139708407461632 logging_writer.py:48] [516500] global_step=516500, grad_norm=3.1063942909240723, loss=1.0843840837478638
I0311 02:32:01.907528 139708415854336 logging_writer.py:48] [516600] global_step=516600, grad_norm=3.202193260192871, loss=1.1605627536773682
I0311 02:32:46.795545 139708407461632 logging_writer.py:48] [516700] global_step=516700, grad_norm=3.1726670265197754, loss=1.803443193435669
I0311 02:33:32.420775 139708415854336 logging_writer.py:48] [516800] global_step=516800, grad_norm=3.2909493446350098, loss=1.1279759407043457
I0311 02:34:17.998390 139708407461632 logging_writer.py:48] [516900] global_step=516900, grad_norm=2.9809865951538086, loss=1.4239405393600464
I0311 02:35:03.413396 139708415854336 logging_writer.py:48] [517000] global_step=517000, grad_norm=2.9261486530303955, loss=2.124335289001465
I0311 02:35:48.824066 139708407461632 logging_writer.py:48] [517100] global_step=517100, grad_norm=3.005117177963257, loss=2.191626787185669
I0311 02:36:34.078022 139708415854336 logging_writer.py:48] [517200] global_step=517200, grad_norm=3.3867268562316895, loss=1.87632155418396
I0311 02:37:19.336279 139708407461632 logging_writer.py:48] [517300] global_step=517300, grad_norm=3.164745569229126, loss=1.63564932346344
I0311 02:38:04.806579 139708415854336 logging_writer.py:48] [517400] global_step=517400, grad_norm=2.7103097438812256, loss=1.0005861520767212
I0311 02:38:20.720006 139902746892096 spec.py:321] Evaluating on the training split.
I0311 02:38:32.119198 139902746892096 spec.py:333] Evaluating on the validation split.
I0311 02:38:53.190418 139902746892096 spec.py:349] Evaluating on the test split.
I0311 02:38:54.860908 139902746892096 submission_runner.py:411] Time since start: 250657.89s, 	Step: 517437, 	{'train/accuracy': 0.8845898509025574, 'train/loss': 0.42180606722831726, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 231519.41894960403, 'total_duration': 250657.88970041275, 'accumulated_submission_time': 231519.41894960403, 'accumulated_eval_time': 19071.938607931137, 'accumulated_logging_time': 39.44188857078552}
I0311 02:38:54.953430 139708407461632 logging_writer.py:48] [517437] accumulated_eval_time=19071.938608, accumulated_logging_time=39.441889, accumulated_submission_time=231519.418950, global_step=517437, preemption_count=0, score=231519.418950, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=250657.889700, train/accuracy=0.884590, train/loss=0.421806, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0311 02:39:20.141342 139708415854336 logging_writer.py:48] [517500] global_step=517500, grad_norm=3.460252046585083, loss=1.1420331001281738
I0311 02:40:04.063692 139708407461632 logging_writer.py:48] [517600] global_step=517600, grad_norm=2.970247745513916, loss=1.340313196182251
I0311 02:40:49.498607 139708415854336 logging_writer.py:48] [517700] global_step=517700, grad_norm=3.605637788772583, loss=2.5545878410339355
I0311 02:41:35.111889 139708407461632 logging_writer.py:48] [517800] global_step=517800, grad_norm=3.3177332878112793, loss=2.426452398300171
I0311 02:42:20.502135 139708415854336 logging_writer.py:48] [517900] global_step=517900, grad_norm=2.8485231399536133, loss=2.0421409606933594
I0311 02:43:06.028607 139708407461632 logging_writer.py:48] [518000] global_step=518000, grad_norm=3.33842134475708, loss=2.6559255123138428
I0311 02:43:51.376051 139708415854336 logging_writer.py:48] [518100] global_step=518100, grad_norm=3.5420608520507812, loss=3.087958812713623
I0311 02:44:36.345720 139708407461632 logging_writer.py:48] [518200] global_step=518200, grad_norm=3.3084466457366943, loss=1.134605884552002
I0311 02:45:21.874258 139708415854336 logging_writer.py:48] [518300] global_step=518300, grad_norm=3.099668025970459, loss=1.1258304119110107
I0311 02:45:55.079819 139902746892096 spec.py:321] Evaluating on the training split.
I0311 02:46:06.685418 139902746892096 spec.py:333] Evaluating on the validation split.
I0311 02:46:29.596885 139902746892096 spec.py:349] Evaluating on the test split.
I0311 02:46:31.281637 139902746892096 submission_runner.py:411] Time since start: 251114.31s, 	Step: 518375, 	{'train/accuracy': 0.8900195360183716, 'train/loss': 0.4088830351829529, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 231939.48673439026, 'total_duration': 251114.31041026115, 'accumulated_submission_time': 231939.48673439026, 'accumulated_eval_time': 19108.140387535095, 'accumulated_logging_time': 39.543895959854126}
I0311 02:46:31.386936 139708407461632 logging_writer.py:48] [518375] accumulated_eval_time=19108.140388, accumulated_logging_time=39.543896, accumulated_submission_time=231939.486734, global_step=518375, preemption_count=0, score=231939.486734, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=251114.310410, train/accuracy=0.890020, train/loss=0.408883, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0311 02:46:41.611341 139708415854336 logging_writer.py:48] [518400] global_step=518400, grad_norm=3.4900851249694824, loss=1.294420599937439
I0311 02:47:22.972944 139708407461632 logging_writer.py:48] [518500] global_step=518500, grad_norm=3.2114691734313965, loss=1.2311843633651733
I0311 02:48:08.327272 139708415854336 logging_writer.py:48] [518600] global_step=518600, grad_norm=4.036221981048584, loss=3.230081558227539
I0311 02:48:54.179943 139708407461632 logging_writer.py:48] [518700] global_step=518700, grad_norm=3.050539970397949, loss=1.0457453727722168
I0311 02:49:39.427224 139708415854336 logging_writer.py:48] [518800] global_step=518800, grad_norm=3.309354543685913, loss=1.349867820739746
I0311 02:50:24.753468 139708407461632 logging_writer.py:48] [518900] global_step=518900, grad_norm=3.003774404525757, loss=1.0537351369857788
I0311 02:51:10.519212 139708415854336 logging_writer.py:48] [519000] global_step=519000, grad_norm=3.4042458534240723, loss=2.1892690658569336
I0311 02:51:55.916335 139708407461632 logging_writer.py:48] [519100] global_step=519100, grad_norm=3.1713602542877197, loss=2.2032623291015625
I0311 02:52:41.289125 139708415854336 logging_writer.py:48] [519200] global_step=519200, grad_norm=3.0585854053497314, loss=1.840882658958435
I0311 02:53:27.105479 139708407461632 logging_writer.py:48] [519300] global_step=519300, grad_norm=3.109262466430664, loss=1.0809835195541382
I0311 02:53:32.013003 139902746892096 spec.py:321] Evaluating on the training split.
I0311 02:53:43.554660 139902746892096 spec.py:333] Evaluating on the validation split.
I0311 02:54:06.052171 139902746892096 spec.py:349] Evaluating on the test split.
I0311 02:54:07.728553 139902746892096 submission_runner.py:411] Time since start: 251570.76s, 	Step: 519311, 	{'train/accuracy': 0.8872656226158142, 'train/loss': 0.4180852770805359, 'validation/accuracy': 0.7828800082206726, 'validation/loss': 0.8518542051315308, 'validation/num_examples': 50000, 'test/accuracy': 0.6663000583648682, 'test/loss': 1.4581419229507446, 'test/num_examples': 10000, 'score': 232360.05365729332, 'total_duration': 251570.7573211193, 'accumulated_submission_time': 232360.05365729332, 'accumulated_eval_time': 19143.85590171814, 'accumulated_logging_time': 39.65925049781799}
I0311 02:54:07.837641 139708415854336 logging_writer.py:48] [519311] accumulated_eval_time=19143.855902, accumulated_logging_time=39.659250, accumulated_submission_time=232360.053657, global_step=519311, preemption_count=0, score=232360.053657, test/accuracy=0.666300, test/loss=1.458142, test/num_examples=10000, total_duration=251570.757321, train/accuracy=0.887266, train/loss=0.418085, validation/accuracy=0.782880, validation/loss=0.851854, validation/num_examples=50000
I0311 02:54:43.443334 139708407461632 logging_writer.py:48] [519400] global_step=519400, grad_norm=3.174729585647583, loss=1.8287463188171387
I0311 02:55:28.724050 139708415854336 logging_writer.py:48] [519500] global_step=519500, grad_norm=3.3089427947998047, loss=1.826370358467102
I0311 02:56:14.386515 139708407461632 logging_writer.py:48] [519600] global_step=519600, grad_norm=3.3654115200042725, loss=1.370991587638855
I0311 02:56:59.963761 139708415854336 logging_writer.py:48] [519700] global_step=519700, grad_norm=3.586743116378784, loss=2.496328115463257
I0311 02:57:28.370319 139708407461632 logging_writer.py:48] [519764] global_step=519764, preemption_count=0, score=232560.439424
I0311 02:57:28.993344 139902746892096 checkpoints.py:490] Saving checkpoint at step: 519764
I0311 02:57:30.445269 139902746892096 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification_self_tuning/study_4/imagenet_vit_jax/trial_1/checkpoint_519764
I0311 02:57:30.475798 139902746892096 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification_self_tuning/study_4/imagenet_vit_jax/trial_1/checkpoint_519764.
I0311 02:57:31.601288 139902746892096 submission_runner.py:676] Final imagenet_vit score: 232560.43942379951
