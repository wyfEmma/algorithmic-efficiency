python3 submission_runner.py --framework=jax --workload=ogbg --submission_path=prize_qualification_baselines/self_tuning/jax_nadamw_full_budget.py --data_dir=/data/ogbg --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=prize_qualification_self_tuning/study_4 --overwrite=true --save_checkpoints=false --rng_seed=2953769335 --max_global_steps=240000 --tuning_ruleset=self 2>&1 | tee -a /logs/ogbg_jax_03-11-2024-03-14-14.log
I0311 03:14:35.571759 139789500200768 logger_utils.py:76] Creating experiment directory at /experiment_runs/prize_qualification_self_tuning/study_4/ogbg_jax.
I0311 03:14:36.570481 139789500200768 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I0311 03:14:36.572195 139789500200768 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0311 03:14:36.572450 139789500200768 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0311 03:14:37.438125 139789500200768 submission_runner.py:605] Creating directory at /experiment_runs/prize_qualification_self_tuning/study_4/ogbg_jax/trial_1.
I0311 03:14:37.634479 139789500200768 submission_runner.py:206] Initializing dataset.
I0311 03:14:37.920470 139789500200768 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0311 03:14:37.924716 139789500200768 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow_datasets/core/reader.py:101: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.counter(...)` instead.
W0311 03:14:38.170786 139789500200768 deprecation.py:364] From /usr/local/lib/python3.8/dist-packages/tensorflow_datasets/core/reader.py:101: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.counter(...)` instead.
I0311 03:14:38.230224 139789500200768 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0311 03:14:38.302531 139789500200768 submission_runner.py:213] Initializing model.
I0311 03:14:42.926051 139789500200768 submission_runner.py:255] Initializing optimizer.
I0311 03:14:43.558435 139789500200768 submission_runner.py:262] Initializing metrics bundle.
I0311 03:14:43.558629 139789500200768 submission_runner.py:280] Initializing checkpoint and logger.
I0311 03:14:43.559267 139789500200768 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification_self_tuning/study_4/ogbg_jax/trial_1 with prefix checkpoint_
I0311 03:14:43.559401 139789500200768 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification_self_tuning/study_4/ogbg_jax/trial_1/meta_data_0.json.
I0311 03:14:43.559583 139789500200768 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0311 03:14:43.559642 139789500200768 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0311 03:14:43.873945 139789500200768 logger_utils.py:220] Unable to record git information. Continuing without it.
I0311 03:14:44.161737 139789500200768 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification_self_tuning/study_4/ogbg_jax/trial_1/flags_0.json.
I0311 03:14:44.172719 139789500200768 submission_runner.py:314] Starting training loop.
I0311 03:15:02.773640 139624278124288 logging_writer.py:48] [0] global_step=0, grad_norm=1.8646773099899292, loss=0.7976973056793213
I0311 03:15:02.788758 139789500200768 spec.py:321] Evaluating on the training split.
I0311 03:15:02.795161 139789500200768 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0311 03:15:02.799757 139789500200768 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0311 03:15:02.868779 139789500200768 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0311 03:16:55.352753 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 03:16:55.356287 139789500200768 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0311 03:16:55.360215 139789500200768 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0311 03:16:55.424257 139789500200768 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split validation, from /data/ogbg/ogbg_molpcba/0.1.3
I0311 03:18:27.249483 139789500200768 spec.py:349] Evaluating on the test split.
I0311 03:18:27.654725 139789500200768 dataset_info.py:736] Load pre-computed DatasetInfo (eg: splits, num examples,...) from GCS: ogbg_molpcba/0.1.3
I0311 03:18:28.847961 139789500200768 dataset_info.py:578] Load dataset info from /tmp/tmpu2wqyspqtfds
I0311 03:18:28.851238 139789500200768 dataset_info.py:669] Fields info.[description, release_notes, splits, module_name] from disk and from code do not match. Keeping the one from code.
I0311 03:18:28.851705 139789500200768 dataset_builder.py:593] Generating dataset ogbg_molpcba (/root/data/ogbg_molpcba/0.1.3)
Downloading and preparing dataset 37.70 MiB (download: 37.70 MiB, generated: 822.53 MiB, total: 860.23 MiB) to /root/data/ogbg_molpcba/0.1.3...
Dl Completed...: 0 url [00:00, ? url/s]
Dl Size...: 0 MiB [00:00, ? MiB/s][A

Extraction completed...: 0 file [00:00, ? file/s][A[AI0311 03:18:30.022291 139789500200768 download_manager.py:400] Downloading https://snap.stanford.edu/ogb/data/graphproppred/csv_mol_download/pcba.zip into /root/data/downloads/snap.stan.edu_ogb_grap_csv_mol_down_pcbapc4I82Cv1THcU-IggPHK8IHZ8qM-BJ3VDk-q_rtqrf4.zip.tmp.751a5d9d83da41889e7e98db8c7904d4...
Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]
Dl Size...: 0 MiB [00:00, ? MiB/s][A

Extraction completed...: 0 file [00:00, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]
Dl Size...:   0%|          | 0/37 [00:00<?, ? MiB/s][A

Extraction completed...: 0 file [00:00, ? file/s][A[A
Dl Size...:   3%|▎         | 1/37 [00:02<01:43,  2.89s/ MiB][ADl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]
Dl Size...:   3%|▎         | 1/37 [00:02<01:43,  2.89s/ MiB][A

Extraction completed...: 0 file [00:02, ? file/s][A[A
Dl Size...:   5%|▌         | 2/37 [00:03<00:54,  1.56s/ MiB][ADl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]
Dl Size...:   5%|▌         | 2/37 [00:03<00:54,  1.56s/ MiB][A

Extraction completed...: 0 file [00:03, ? file/s][A[A
Dl Size...:   8%|▊         | 3/37 [00:03<00:33,  1.01 MiB/s][ADl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]
Dl Size...:   8%|▊         | 3/37 [00:03<00:33,  1.01 MiB/s][A

Extraction completed...: 0 file [00:03, ? file/s][A[A
Dl Size...:  11%|█         | 4/37 [00:04<00:23,  1.39 MiB/s][ADl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]
Dl Size...:  11%|█         | 4/37 [00:04<00:23,  1.39 MiB/s][A

Extraction completed...: 0 file [00:04, ? file/s][A[A
Dl Size...:  14%|█▎        | 5/37 [00:04<00:16,  1.93 MiB/s][ADl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]
Dl Size...:  14%|█▎        | 5/37 [00:04<00:16,  1.93 MiB/s][A

Extraction completed...: 0 file [00:04, ? file/s][A[A
Dl Size...:  16%|█▌        | 6/37 [00:04<00:13,  2.24 MiB/s][ADl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]
Dl Size...:  16%|█▌        | 6/37 [00:04<00:13,  2.24 MiB/s][A

Extraction completed...: 0 file [00:04, ? file/s][A[A
Dl Size...:  19%|█▉        | 7/37 [00:04<00:10,  2.84 MiB/s][ADl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]
Dl Size...:  19%|█▉        | 7/37 [00:04<00:10,  2.84 MiB/s][A

Extraction completed...: 0 file [00:04, ? file/s][A[A
Dl Size...:  22%|██▏       | 8/37 [00:04<00:08,  3.47 MiB/s][ADl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]
Dl Size...:  22%|██▏       | 8/37 [00:04<00:08,  3.47 MiB/s][A

Extraction completed...: 0 file [00:04, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]
Dl Size...:  24%|██▍       | 9/37 [00:04<00:08,  3.47 MiB/s][A

Extraction completed...: 0 file [00:04, ? file/s][A[A
Dl Size...:  27%|██▋       | 10/37 [00:05<00:05,  5.22 MiB/s][ADl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]
Dl Size...:  27%|██▋       | 10/37 [00:05<00:05,  5.22 MiB/s][A

Extraction completed...: 0 file [00:05, ? file/s][A[A
Dl Size...:  30%|██▉       | 11/37 [00:05<00:04,  5.59 MiB/s][ADl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]
Dl Size...:  30%|██▉       | 11/37 [00:05<00:04,  5.59 MiB/s][A

Extraction completed...: 0 file [00:05, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]
Dl Size...:  32%|███▏      | 12/37 [00:05<00:04,  5.59 MiB/s][A

Extraction completed...: 0 file [00:05, ? file/s][A[A
Dl Size...:  35%|███▌      | 13/37 [00:05<00:03,  7.30 MiB/s][ADl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]
Dl Size...:  35%|███▌      | 13/37 [00:05<00:03,  7.30 MiB/s][A

Extraction completed...: 0 file [00:05, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]
Dl Size...:  38%|███▊      | 14/37 [00:05<00:03,  7.30 MiB/s][A

Extraction completed...: 0 file [00:05, ? file/s][A[A
Dl Size...:  41%|████      | 15/37 [00:05<00:02,  8.66 MiB/s][ADl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]
Dl Size...:  41%|████      | 15/37 [00:05<00:02,  8.66 MiB/s][A

Extraction completed...: 0 file [00:05, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]
Dl Size...:  43%|████▎     | 16/37 [00:05<00:02,  8.66 MiB/s][A

Extraction completed...: 0 file [00:05, ? file/s][A[A
Dl Size...:  46%|████▌     | 17/37 [00:05<00:02,  9.79 MiB/s][ADl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]
Dl Size...:  46%|████▌     | 17/37 [00:05<00:02,  9.79 MiB/s][A

Extraction completed...: 0 file [00:05, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]
Dl Size...:  49%|████▊     | 18/37 [00:05<00:01,  9.79 MiB/s][A

Extraction completed...: 0 file [00:05, ? file/s][A[A
Dl Size...:  51%|█████▏    | 19/37 [00:05<00:01, 10.72 MiB/s][ADl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]
Dl Size...:  51%|█████▏    | 19/37 [00:05<00:01, 10.72 MiB/s][A

Extraction completed...: 0 file [00:05, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]
Dl Size...:  54%|█████▍    | 20/37 [00:05<00:01, 10.72 MiB/s][A

Extraction completed...: 0 file [00:05, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]
Dl Size...:  57%|█████▋    | 21/37 [00:05<00:01, 10.72 MiB/s][A

Extraction completed...: 0 file [00:05, ? file/s][A[A
Dl Size...:  59%|█████▉    | 22/37 [00:06<00:01, 13.05 MiB/s][ADl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]
Dl Size...:  59%|█████▉    | 22/37 [00:06<00:01, 13.05 MiB/s][A

Extraction completed...: 0 file [00:06, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]
Dl Size...:  62%|██████▏   | 23/37 [00:06<00:01, 13.05 MiB/s][A

Extraction completed...: 0 file [00:06, ? file/s][A[A
Dl Size...:  65%|██████▍   | 24/37 [00:06<00:00, 13.22 MiB/s][ADl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]
Dl Size...:  65%|██████▍   | 24/37 [00:06<00:00, 13.22 MiB/s][A

Extraction completed...: 0 file [00:06, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]
Dl Size...:  68%|██████▊   | 25/37 [00:06<00:00, 13.22 MiB/s][A

Extraction completed...: 0 file [00:06, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]
Dl Size...:  70%|███████   | 26/37 [00:06<00:00, 13.22 MiB/s][A

Extraction completed...: 0 file [00:06, ? file/s][A[A
Dl Size...:  73%|███████▎  | 27/37 [00:06<00:00, 14.90 MiB/s][ADl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]
Dl Size...:  73%|███████▎  | 27/37 [00:06<00:00, 14.90 MiB/s][A

Extraction completed...: 0 file [00:06, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]
Dl Size...:  76%|███████▌  | 28/37 [00:06<00:00, 14.90 MiB/s][A

Extraction completed...: 0 file [00:06, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]
Dl Size...:  78%|███████▊  | 29/37 [00:06<00:00, 14.90 MiB/s][A

Extraction completed...: 0 file [00:06, ? file/s][A[A
Dl Size...:  81%|████████  | 30/37 [00:06<00:00, 16.18 MiB/s][ADl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]
Dl Size...:  81%|████████  | 30/37 [00:06<00:00, 16.18 MiB/s][A

Extraction completed...: 0 file [00:06, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]
Dl Size...:  84%|████████▍ | 31/37 [00:06<00:00, 16.18 MiB/s][A

Extraction completed...: 0 file [00:06, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]
Dl Size...:  86%|████████▋ | 32/37 [00:06<00:00, 16.18 MiB/s][A

Extraction completed...: 0 file [00:06, ? file/s][A[A
Dl Size...:  89%|████████▉ | 33/37 [00:06<00:00, 16.92 MiB/s][ADl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]
Dl Size...:  89%|████████▉ | 33/37 [00:06<00:00, 16.92 MiB/s][A

Extraction completed...: 0 file [00:06, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]
Dl Size...:  92%|█████████▏| 34/37 [00:06<00:00, 16.92 MiB/s][A

Extraction completed...: 0 file [00:06, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]
Dl Size...:  95%|█████████▍| 35/37 [00:06<00:00, 16.92 MiB/s][A

Extraction completed...: 0 file [00:06, ? file/s][A[A
Dl Size...:  97%|█████████▋| 36/37 [00:06<00:00, 17.59 MiB/s][ADl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]
Dl Size...:  97%|█████████▋| 36/37 [00:06<00:00, 17.59 MiB/s][A

Extraction completed...: 0 file [00:06, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]
Dl Size...: 100%|██████████| 37/37 [00:06<00:00, 17.59 MiB/s][A

Extraction completed...: 0 file [00:06, ? file/s][A[ADl Completed...: 100%|██████████| 1/1 [00:06<00:00,  6.86s/ url]Dl Completed...: 100%|██████████| 1/1 [00:06<00:00,  6.86s/ url]
Dl Size...: 100%|██████████| 37/37 [00:06<00:00, 17.59 MiB/s][A

Extraction completed...: 0 file [00:06, ? file/s][A[ADl Completed...: 100%|██████████| 1/1 [00:06<00:00,  6.86s/ url]
Dl Size...: 100%|██████████| 37/37 [00:06<00:00, 17.59 MiB/s][A

Extraction completed...:   0%|          | 0/1 [00:06<?, ? file/s][A[ADl Completed...: 100%|██████████| 1/1 [00:06<00:00,  6.86s/ url]
Dl Size...: 100%|██████████| 37/37 [00:06<00:00, 17.59 MiB/s][A

Extraction completed...:   0%|          | 0/2 [00:06<?, ? file/s][A[ADl Completed...: 100%|██████████| 1/1 [00:06<00:00,  6.86s/ url]
Dl Size...: 100%|██████████| 37/37 [00:06<00:00, 17.59 MiB/s][A

Extraction completed...:   0%|          | 0/3 [00:06<?, ? file/s][A[ADl Completed...: 100%|██████████| 1/1 [00:06<00:00,  6.86s/ url]
Dl Size...: 100%|██████████| 37/37 [00:06<00:00, 17.59 MiB/s][A

Extraction completed...:   0%|          | 0/4 [00:06<?, ? file/s][A[ADl Completed...: 100%|██████████| 1/1 [00:06<00:00,  6.86s/ url]
Dl Size...: 100%|██████████| 37/37 [00:06<00:00, 17.59 MiB/s][A

Extraction completed...:   0%|          | 0/5 [00:06<?, ? file/s][A[ADl Completed...: 100%|██████████| 1/1 [00:06<00:00,  6.86s/ url]
Dl Size...: 100%|██████████| 37/37 [00:06<00:00, 17.59 MiB/s][A

Extraction completed...:   0%|          | 0/6 [00:06<?, ? file/s][A[ADl Completed...: 100%|██████████| 1/1 [00:06<00:00,  6.86s/ url]
Dl Size...: 100%|██████████| 37/37 [00:06<00:00, 17.59 MiB/s][A

Extraction completed...:   0%|          | 0/7 [00:06<?, ? file/s][A[ADl Completed...: 100%|██████████| 1/1 [00:06<00:00,  6.86s/ url]
Dl Size...: 100%|██████████| 37/37 [00:06<00:00, 17.59 MiB/s][A

Extraction completed...:   0%|          | 0/8 [00:06<?, ? file/s][A[ADl Completed...: 100%|██████████| 1/1 [00:06<00:00,  6.86s/ url]
Dl Size...: 100%|██████████| 37/37 [00:06<00:00, 17.59 MiB/s][A

Extraction completed...:   0%|          | 0/9 [00:06<?, ? file/s][A[ADl Completed...: 100%|██████████| 1/1 [00:06<00:00,  6.86s/ url]
Dl Size...: 100%|██████████| 37/37 [00:06<00:00, 17.59 MiB/s][A

Extraction completed...:   0%|          | 0/10 [00:06<?, ? file/s][A[ADl Completed...: 100%|██████████| 1/1 [00:06<00:00,  6.86s/ url]
Dl Size...: 100%|██████████| 37/37 [00:06<00:00, 17.59 MiB/s][A

Extraction completed...:   0%|          | 0/11 [00:06<?, ? file/s][A[ADl Completed...: 100%|██████████| 1/1 [00:06<00:00,  6.86s/ url]
Dl Size...: 100%|██████████| 37/37 [00:06<00:00, 17.59 MiB/s][A

Extraction completed...:   0%|          | 0/12 [00:06<?, ? file/s][A[A

Extraction completed...:   8%|▊         | 1/12 [00:06<01:16,  6.99s/ file][A[ADl Completed...: 100%|██████████| 1/1 [00:06<00:00,  6.86s/ url]
Dl Size...: 100%|██████████| 37/37 [00:06<00:00, 17.59 MiB/s][A

Extraction completed...:   8%|▊         | 1/12 [00:06<01:16,  6.99s/ file][A[ADl Completed...: 100%|██████████| 1/1 [00:06<00:00,  6.86s/ url]
Dl Size...: 100%|██████████| 37/37 [00:06<00:00, 17.59 MiB/s][A

Extraction completed...:  17%|█▋        | 2/12 [00:06<01:09,  6.99s/ file][A[ADl Completed...: 100%|██████████| 1/1 [00:06<00:00,  6.86s/ url]
Dl Size...: 100%|██████████| 37/37 [00:06<00:00, 17.59 MiB/s][A

Extraction completed...:  25%|██▌       | 3/12 [00:06<01:02,  6.99s/ file][A[ADl Completed...: 100%|██████████| 1/1 [00:06<00:00,  6.86s/ url]
Dl Size...: 100%|██████████| 37/37 [00:06<00:00, 17.59 MiB/s][A

Extraction completed...:  33%|███▎      | 4/12 [00:06<00:55,  6.99s/ file][A[ADl Completed...: 100%|██████████| 1/1 [00:06<00:00,  6.86s/ url]
Dl Size...: 100%|██████████| 37/37 [00:06<00:00, 17.59 MiB/s][A

Extraction completed...:  42%|████▏     | 5/12 [00:06<00:48,  6.99s/ file][A[ADl Completed...: 100%|██████████| 1/1 [00:06<00:00,  6.86s/ url]
Dl Size...: 100%|██████████| 37/37 [00:06<00:00, 17.59 MiB/s][A

Extraction completed...:  50%|█████     | 6/12 [00:06<00:41,  6.99s/ file][A[ADl Completed...: 100%|██████████| 1/1 [00:06<00:00,  6.86s/ url]
Dl Size...: 100%|██████████| 37/37 [00:06<00:00, 17.59 MiB/s][A

Extraction completed...:  58%|█████▊    | 7/12 [00:06<00:34,  6.99s/ file][A[ADl Completed...: 100%|██████████| 1/1 [00:06<00:00,  6.86s/ url]
Dl Size...: 100%|██████████| 37/37 [00:06<00:00, 17.59 MiB/s][A

Extraction completed...:  67%|██████▋   | 8/12 [00:06<00:27,  6.99s/ file][A[ADl Completed...: 100%|██████████| 1/1 [00:06<00:00,  6.86s/ url]
Dl Size...: 100%|██████████| 37/37 [00:06<00:00, 17.59 MiB/s][A

Extraction completed...:  75%|███████▌  | 9/12 [00:06<00:20,  6.99s/ file][A[ADl Completed...: 100%|██████████| 1/1 [00:06<00:00,  6.86s/ url]
Dl Size...: 100%|██████████| 37/37 [00:06<00:00, 17.59 MiB/s][A

Extraction completed...:  83%|████████▎ | 10/12 [00:06<00:13,  6.99s/ file][A[ADl Completed...: 100%|██████████| 1/1 [00:06<00:00,  6.86s/ url]
Dl Size...: 100%|██████████| 37/37 [00:06<00:00, 17.59 MiB/s][A

Extraction completed...:  92%|█████████▏| 11/12 [00:06<00:06,  6.99s/ file][A[ADl Completed...: 100%|██████████| 1/1 [00:06<00:00,  6.86s/ url]
Dl Size...: 100%|██████████| 37/37 [00:06<00:00, 17.59 MiB/s][A

Extraction completed...: 100%|██████████| 12/12 [00:06<00:00,  6.99s/ file][A[AExtraction completed...: 100%|██████████| 12/12 [00:06<00:00,  1.72 file/s]
Dl Size...: 100%|██████████| 37/37 [00:06<00:00,  5.29 MiB/s]
Dl Completed...: 100%|██████████| 1/1 [00:06<00:00,  6.99s/ url]
Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]
Generating train examples...:   0%|          | 0/350343 [00:00<?, ? examples/s][A
Generating train examples...:   0%|          | 86/350343 [00:00<06:47, 858.73 examples/s][A
Generating train examples...:   0%|          | 172/350343 [00:00<07:22, 792.06 examples/s][A
Generating train examples...:   0%|          | 403/350343 [00:00<04:00, 1453.87 examples/s][A
Generating train examples...:   0%|          | 635/350343 [00:00<03:15, 1786.00 examples/s][A
Generating train examples...:   0%|          | 846/350343 [00:00<03:03, 1900.00 examples/s][A
Generating train examples...:   0%|          | 1082/350343 [00:00<02:50, 2053.50 examples/s][A
Generating train examples...:   0%|          | 1314/350343 [00:00<02:43, 2138.61 examples/s][A
Generating train examples...:   0%|          | 1557/350343 [00:00<02:36, 2228.77 examples/s][A
Generating train examples...:   1%|          | 1794/350343 [00:00<02:33, 2270.49 examples/s][A
Generating train examples...:   1%|          | 2022/350343 [00:01<02:34, 2258.54 examples/s][A
Generating train examples...:   1%|          | 2261/350343 [00:01<02:31, 2297.81 examples/s][A
Generating train examples...:   1%|          | 2504/350343 [00:01<02:28, 2337.34 examples/s][A
Generating train examples...:   1%|          | 2742/350343 [00:01<02:28, 2348.37 examples/s][A
Generating train examples...:   1%|          | 2977/350343 [00:01<02:28, 2342.90 examples/s][A
Generating train examples...:   1%|          | 3214/350343 [00:01<02:27, 2348.50 examples/s][A
Generating train examples...:   1%|          | 3451/350343 [00:01<02:27, 2354.22 examples/s][A
Generating train examples...:   1%|          | 3693/350343 [00:01<02:26, 2372.82 examples/s][A
Generating train examples...:   1%|          | 3931/350343 [00:01<02:26, 2366.63 examples/s][A
Generating train examples...:   1%|          | 4168/350343 [00:01<02:27, 2348.84 examples/s][A
Generating train examples...:   1%|▏         | 4403/350343 [00:02<02:32, 2274.50 examples/s][A
Generating train examples...:   1%|▏         | 4636/350343 [00:02<02:35, 2218.62 examples/s][A
Generating train examples...:   1%|▏         | 4878/350343 [00:02<02:31, 2274.64 examples/s][A
Generating train examples...:   1%|▏         | 5113/350343 [00:02<02:30, 2296.50 examples/s][A
Generating train examples...:   2%|▏         | 5352/350343 [00:02<02:28, 2323.40 examples/s][A
Generating train examples...:   2%|▏         | 5594/350343 [00:02<02:26, 2351.48 examples/s][A
Generating train examples...:   2%|▏         | 5830/350343 [00:02<02:26, 2353.32 examples/s][A
Generating train examples...:   2%|▏         | 6067/350343 [00:02<02:26, 2357.62 examples/s][A
Generating train examples...:   2%|▏         | 6304/350343 [00:02<02:25, 2361.11 examples/s][A
Generating train examples...:   2%|▏         | 6542/350343 [00:02<02:25, 2365.69 examples/s][A
Generating train examples...:   2%|▏         | 6779/350343 [00:03<02:30, 2289.38 examples/s][A
Generating train examples...:   2%|▏         | 7018/350343 [00:03<02:28, 2317.11 examples/s][A
Generating train examples...:   2%|▏         | 7254/350343 [00:03<02:27, 2328.30 examples/s][A
Generating train examples...:   2%|▏         | 7489/350343 [00:03<02:26, 2332.55 examples/s][A
Generating train examples...:   2%|▏         | 7733/350343 [00:03<02:25, 2362.69 examples/s][A
Generating train examples...:   2%|▏         | 7970/350343 [00:03<02:25, 2356.58 examples/s][A
Generating train examples...:   2%|▏         | 8207/350343 [00:03<02:25, 2358.78 examples/s][A
Generating train examples...:   2%|▏         | 8443/350343 [00:03<02:29, 2282.51 examples/s][A
Generating train examples...:   2%|▏         | 8683/350343 [00:03<02:27, 2313.86 examples/s][A
Generating train examples...:   3%|▎         | 8915/350343 [00:04<02:54, 1959.99 examples/s][A
Generating train examples...:   3%|▎         | 9157/350343 [00:04<02:44, 2079.43 examples/s][A
Generating train examples...:   3%|▎         | 9392/350343 [00:04<02:38, 2152.94 examples/s][A
Generating train examples...:   3%|▎         | 9629/350343 [00:04<02:34, 2211.43 examples/s][A
Generating train examples...:   3%|▎         | 9856/350343 [00:04<02:33, 2218.91 examples/s][A
Generating train examples...:   3%|▎         | 10084/350343 [00:04<02:32, 2235.88 examples/s][A
Generating train examples...:   3%|▎         | 10324/350343 [00:04<02:28, 2282.21 examples/s][A
Generating train examples...:   3%|▎         | 10562/350343 [00:04<02:27, 2310.62 examples/s][A
Generating train examples...:   3%|▎         | 10803/350343 [00:04<02:25, 2339.58 examples/s][A
Generating train examples...:   3%|▎         | 11046/350343 [00:04<02:23, 2365.41 examples/s][A
Generating train examples...:   3%|▎         | 11284/350343 [00:05<02:57, 1914.29 examples/s][A
Generating train examples...:   3%|▎         | 11518/350343 [00:05<02:47, 2021.46 examples/s][A
Generating train examples...:   3%|▎         | 11759/350343 [00:05<02:39, 2124.77 examples/s][A
Generating train examples...:   3%|▎         | 11992/350343 [00:05<02:35, 2180.32 examples/s][A
Generating train examples...:   3%|▎         | 12230/350343 [00:05<02:31, 2236.75 examples/s][A
Generating train examples...:   4%|▎         | 12468/350343 [00:05<02:28, 2277.14 examples/s][A
Generating train examples...:   4%|▎         | 12700/350343 [00:05<02:32, 2220.01 examples/s][A
Generating train examples...:   4%|▎         | 12937/350343 [00:05<02:29, 2261.98 examples/s][A
Generating train examples...:   4%|▍         | 13173/350343 [00:05<02:27, 2290.33 examples/s][A
Generating train examples...:   4%|▍         | 13407/350343 [00:06<02:26, 2303.87 examples/s][A
Generating train examples...:   4%|▍         | 13648/350343 [00:06<02:24, 2335.06 examples/s][A
Generating train examples...:   4%|▍         | 13886/350343 [00:06<02:23, 2346.04 examples/s][A
Generating train examples...:   4%|▍         | 14122/350343 [00:06<02:23, 2345.95 examples/s][A
Generating train examples...:   4%|▍         | 14358/350343 [00:06<02:23, 2348.89 examples/s][A
Generating train examples...:   4%|▍         | 14597/350343 [00:06<02:22, 2360.49 examples/s][A
Generating train examples...:   4%|▍         | 14834/350343 [00:06<02:22, 2361.57 examples/s][A
Generating train examples...:   4%|▍         | 15076/350343 [00:06<02:20, 2378.35 examples/s][A
Generating train examples...:   4%|▍         | 15314/350343 [00:06<02:20, 2378.62 examples/s][A
Generating train examples...:   4%|▍         | 15552/350343 [00:06<02:20, 2376.04 examples/s][A
Generating train examples...:   5%|▍         | 15790/350343 [00:07<02:21, 2370.17 examples/s][A
Generating train examples...:   5%|▍         | 16028/350343 [00:07<02:20, 2372.05 examples/s][A
Generating train examples...:   5%|▍         | 16266/350343 [00:07<02:21, 2359.02 examples/s][A
Generating train examples...:   5%|▍         | 16502/350343 [00:07<02:26, 2274.39 examples/s][A
Generating train examples...:   5%|▍         | 16738/350343 [00:07<02:25, 2298.31 examples/s][A
Generating train examples...:   5%|▍         | 16979/350343 [00:07<02:23, 2329.36 examples/s][A
Generating train examples...:   5%|▍         | 17214/350343 [00:07<02:22, 2335.33 examples/s][A
Generating train examples...:   5%|▍         | 17457/350343 [00:07<02:20, 2361.49 examples/s][A
Generating train examples...:   5%|▌         | 17697/350343 [00:07<02:20, 2371.84 examples/s][A
Generating train examples...:   5%|▌         | 17935/350343 [00:07<02:20, 2358.95 examples/s][A
Generating train examples...:   5%|▌         | 18172/350343 [00:08<02:20, 2356.89 examples/s][A
Generating train examples...:   5%|▌         | 18408/350343 [00:08<02:20, 2354.51 examples/s][A
Generating train examples...:   5%|▌         | 18653/350343 [00:08<02:19, 2381.72 examples/s][A
Generating train examples...:   5%|▌         | 18892/350343 [00:08<02:20, 2358.35 examples/s][A
Generating train examples...:   5%|▌         | 19131/350343 [00:08<02:19, 2365.99 examples/s][A
Generating train examples...:   6%|▌         | 19368/350343 [00:08<02:21, 2344.85 examples/s][A
Generating train examples...:   6%|▌         | 19603/350343 [00:08<02:25, 2278.06 examples/s][A
Generating train examples...:   6%|▌         | 19832/350343 [00:08<02:27, 2238.10 examples/s][A
Generating train examples...:   6%|▌         | 20057/350343 [00:08<02:27, 2240.48 examples/s][A
Generating train examples...:   6%|▌         | 20290/350343 [00:08<02:25, 2264.37 examples/s][A
Generating train examples...:   6%|▌         | 20522/350343 [00:09<02:29, 2205.42 examples/s][A
Generating train examples...:   6%|▌         | 20759/350343 [00:09<02:26, 2251.22 examples/s][A
Generating train examples...:   6%|▌         | 20999/350343 [00:09<02:23, 2292.70 examples/s][A
Generating train examples...:   6%|▌         | 21236/350343 [00:09<02:22, 2314.63 examples/s][A
Generating train examples...:   6%|▌         | 21477/350343 [00:09<02:20, 2342.33 examples/s][A
Generating train examples...:   6%|▌         | 21712/350343 [00:09<02:20, 2343.51 examples/s][A
Generating train examples...:   6%|▋         | 21947/350343 [00:09<02:20, 2338.48 examples/s][A
Generating train examples...:   6%|▋         | 22182/350343 [00:09<02:20, 2340.86 examples/s][A
Generating train examples...:   6%|▋         | 22419/350343 [00:09<02:19, 2347.99 examples/s][A
Generating train examples...:   6%|▋         | 22654/350343 [00:09<02:19, 2342.30 examples/s][A
Generating train examples...:   7%|▋         | 22889/350343 [00:10<02:19, 2339.69 examples/s][A
Generating train examples...:   7%|▋         | 23131/350343 [00:10<02:18, 2361.00 examples/s][A
Generating train examples...:   7%|▋         | 23372/350343 [00:10<02:17, 2374.91 examples/s][A
Generating train examples...:   7%|▋         | 23610/350343 [00:10<02:17, 2367.79 examples/s][A
Generating train examples...:   7%|▋         | 23852/350343 [00:10<02:17, 2381.73 examples/s][A
Generating train examples...:   7%|▋         | 24091/350343 [00:10<02:17, 2367.56 examples/s][A
Generating train examples...:   7%|▋         | 24328/350343 [00:10<02:23, 2275.08 examples/s][A
Generating train examples...:   7%|▋         | 24569/350343 [00:10<02:20, 2312.95 examples/s][A
Generating train examples...:   7%|▋         | 24812/350343 [00:10<02:18, 2346.94 examples/s][A
Generating train examples...:   7%|▋         | 25048/350343 [00:11<02:19, 2340.12 examples/s][A
Generating train examples...:   7%|▋         | 25283/350343 [00:11<02:19, 2338.07 examples/s][A
Generating train examples...:   7%|▋         | 25518/350343 [00:11<02:18, 2340.99 examples/s][A
Generating train examples...:   7%|▋         | 25760/350343 [00:11<02:17, 2363.46 examples/s][A
Generating train examples...:   7%|▋         | 25997/350343 [00:11<02:17, 2353.38 examples/s][A
Generating train examples...:   7%|▋         | 26239/350343 [00:11<02:16, 2370.20 examples/s][A
Generating train examples...:   8%|▊         | 26480/350343 [00:11<02:15, 2381.78 examples/s][A
Generating train examples...:   8%|▊         | 26719/350343 [00:11<02:15, 2383.12 examples/s][A
Generating train examples...:   8%|▊         | 26958/350343 [00:11<02:15, 2380.97 examples/s][A
Generating train examples...:   8%|▊         | 27198/350343 [00:11<02:15, 2385.28 examples/s][A
Generating train examples...:   8%|▊         | 27437/350343 [00:12<02:16, 2366.50 examples/s][A
Generating train examples...:   8%|▊         | 27676/350343 [00:12<02:16, 2372.18 examples/s][A
Generating train examples...:   8%|▊         | 27914/350343 [00:12<02:20, 2287.39 examples/s][A
Generating train examples...:   8%|▊         | 28153/350343 [00:12<02:19, 2317.04 examples/s][A
Generating train examples...:   8%|▊         | 28389/350343 [00:12<02:18, 2327.33 examples/s][A
Generating train examples...:   8%|▊         | 28623/350343 [00:12<02:18, 2322.01 examples/s][A
Generating train examples...:   8%|▊         | 28860/350343 [00:12<02:17, 2334.47 examples/s][A
Generating train examples...:   8%|▊         | 29094/350343 [00:12<02:23, 2234.06 examples/s][A
Generating train examples...:   8%|▊         | 29319/350343 [00:12<02:23, 2237.56 examples/s][A
Generating train examples...:   8%|▊         | 29544/350343 [00:12<02:24, 2227.48 examples/s][A
Generating train examples...:   8%|▊         | 29773/350343 [00:13<02:22, 2244.15 examples/s][A
Generating train examples...:   9%|▊         | 30000/350343 [00:13<02:22, 2249.50 examples/s][A
Generating train examples...:   9%|▊         | 30226/350343 [00:13<02:22, 2246.52 examples/s][A
Generating train examples...:   9%|▊         | 30451/350343 [00:13<02:22, 2246.16 examples/s][A
Generating train examples...:   9%|▉         | 30679/350343 [00:13<02:21, 2255.07 examples/s][A
Generating train examples...:   9%|▉         | 30905/350343 [00:13<02:21, 2255.82 examples/s][A
Generating train examples...:   9%|▉         | 31137/350343 [00:13<02:20, 2272.49 examples/s][A
Generating train examples...:   9%|▉         | 31378/350343 [00:13<02:18, 2311.25 examples/s][A
Generating train examples...:   9%|▉         | 31610/350343 [00:13<02:17, 2313.68 examples/s][A
Generating train examples...:   9%|▉         | 31842/350343 [00:13<02:18, 2302.60 examples/s][A
Generating train examples...:   9%|▉         | 32073/350343 [00:14<02:18, 2298.09 examples/s][A
Generating train examples...:   9%|▉         | 32310/350343 [00:14<02:17, 2318.49 examples/s][A
Generating train examples...:   9%|▉         | 32549/350343 [00:14<02:15, 2338.07 examples/s][A
Generating train examples...:   9%|▉         | 32783/350343 [00:14<02:16, 2327.74 examples/s][A
Generating train examples...:   9%|▉         | 33027/350343 [00:14<02:14, 2360.13 examples/s][A
Generating train examples...:   9%|▉         | 33264/350343 [00:14<02:14, 2357.13 examples/s][A
Generating train examples...:  10%|▉         | 33500/350343 [00:14<02:14, 2348.03 examples/s][A
Generating train examples...:  10%|▉         | 33735/350343 [00:14<02:21, 2244.14 examples/s][A
Generating train examples...:  10%|▉         | 33977/350343 [00:14<02:17, 2293.81 examples/s][A
Generating train examples...:  10%|▉         | 34208/350343 [00:14<02:17, 2297.88 examples/s][A
Generating train examples...:  10%|▉         | 34445/350343 [00:15<02:16, 2316.98 examples/s][A
Generating train examples...:  10%|▉         | 34680/350343 [00:15<02:15, 2325.79 examples/s][A
Generating train examples...:  10%|▉         | 34917/350343 [00:15<02:14, 2338.84 examples/s][A
Generating train examples...:  10%|█         | 35152/350343 [00:15<02:15, 2323.32 examples/s][A
Generating train examples...:  10%|█         | 35385/350343 [00:15<02:16, 2307.93 examples/s][A
Generating train examples...:  10%|█         | 35625/350343 [00:15<02:14, 2335.16 examples/s][A
Generating train examples...:  10%|█         | 35860/350343 [00:15<02:14, 2338.90 examples/s][A
Generating train examples...:  10%|█         | 36098/350343 [00:15<02:13, 2350.90 examples/s][A
Generating train examples...:  10%|█         | 36338/350343 [00:15<02:12, 2364.28 examples/s][A
Generating train examples...:  10%|█         | 36575/350343 [00:15<02:12, 2364.72 examples/s][A
Generating train examples...:  11%|█         | 36812/350343 [00:16<02:12, 2365.80 examples/s][A
Generating train examples...:  11%|█         | 37053/350343 [00:16<02:11, 2377.37 examples/s][A
Generating train examples...:  11%|█         | 37291/350343 [00:16<02:11, 2376.62 examples/s][A
Generating train examples...:  11%|█         | 37529/350343 [00:16<02:12, 2363.77 examples/s][A
Generating train examples...:  11%|█         | 37766/350343 [00:16<02:18, 2263.59 examples/s][A
Generating train examples...:  11%|█         | 38005/350343 [00:16<02:15, 2297.63 examples/s][A
Generating train examples...:  11%|█         | 38237/350343 [00:16<02:15, 2302.48 examples/s][A
Generating train examples...:  11%|█         | 38468/350343 [00:16<02:15, 2304.38 examples/s][A
Generating train examples...:  11%|█         | 38701/350343 [00:16<02:14, 2310.99 examples/s][A
Generating train examples...:  11%|█         | 38934/350343 [00:17<02:14, 2316.08 examples/s][A
Generating train examples...:  11%|█         | 39174/350343 [00:17<02:13, 2338.98 examples/s][A
Generating train examples...:  11%|█         | 39409/350343 [00:17<02:13, 2336.52 examples/s][A
Generating train examples...:  11%|█▏        | 39648/350343 [00:17<02:12, 2350.41 examples/s][A
Generating train examples...:  11%|█▏        | 39885/350343 [00:17<02:11, 2355.29 examples/s][A
Generating train examples...:  11%|█▏        | 40122/350343 [00:17<02:11, 2358.54 examples/s][A
Generating train examples...:  12%|█▏        | 40358/350343 [00:17<02:11, 2353.88 examples/s][A
Generating train examples...:  12%|█▏        | 40594/350343 [00:17<02:11, 2354.25 examples/s][A
Generating train examples...:  12%|█▏        | 40834/350343 [00:17<02:10, 2367.71 examples/s][A
Generating train examples...:  12%|█▏        | 41071/350343 [00:17<02:11, 2356.33 examples/s][A
Generating train examples...:  12%|█▏        | 41307/350343 [00:18<02:11, 2357.17 examples/s][A
Generating train examples...:  12%|█▏        | 41549/350343 [00:18<02:10, 2374.55 examples/s][A
Generating train examples...:  12%|█▏        | 41787/350343 [00:18<02:14, 2288.71 examples/s][A
Generating train examples...:  12%|█▏        | 42022/350343 [00:18<02:13, 2306.22 examples/s][A
Generating train examples...:  12%|█▏        | 42261/350343 [00:18<02:12, 2329.73 examples/s][A
Generating train examples...:  12%|█▏        | 42499/350343 [00:18<02:11, 2343.59 examples/s][A
Generating train examples...:  12%|█▏        | 42734/350343 [00:18<02:11, 2332.70 examples/s][A
Generating train examples...:  12%|█▏        | 42973/350343 [00:18<02:10, 2347.43 examples/s][A
Generating train examples...:  12%|█▏        | 43208/350343 [00:18<02:11, 2343.13 examples/s][A
Generating train examples...:  12%|█▏        | 43443/350343 [00:18<02:11, 2337.04 examples/s][A
Generating train examples...:  12%|█▏        | 43679/350343 [00:19<02:10, 2342.45 examples/s][A
Generating train examples...:  13%|█▎        | 43914/350343 [00:19<02:11, 2334.88 examples/s][A
Generating train examples...:  13%|█▎        | 44152/350343 [00:19<02:10, 2346.91 examples/s][A
Generating train examples...:  13%|█▎        | 44390/350343 [00:19<02:09, 2354.96 examples/s][A
Generating train examples...:  13%|█▎        | 44626/350343 [00:19<02:10, 2345.61 examples/s][A
Generating train examples...:  13%|█▎        | 44861/350343 [00:19<02:10, 2341.92 examples/s][A
Generating train examples...:  13%|█▎        | 45102/350343 [00:19<02:09, 2361.38 examples/s][A
Generating train examples...:  13%|█▎        | 45339/350343 [00:19<02:09, 2361.79 examples/s][A
Generating train examples...:  13%|█▎        | 45579/350343 [00:19<02:13, 2284.68 examples/s][A
Generating train examples...:  13%|█▎        | 45810/350343 [00:19<02:12, 2290.45 examples/s][A
Generating train examples...:  13%|█▎        | 46044/350343 [00:20<02:12, 2304.78 examples/s][A
Generating train examples...:  13%|█▎        | 46275/350343 [00:20<02:11, 2305.87 examples/s][A
Generating train examples...:  13%|█▎        | 46508/350343 [00:20<02:11, 2311.50 examples/s][A
Generating train examples...:  13%|█▎        | 46740/350343 [00:20<02:11, 2303.55 examples/s][A
Generating train examples...:  13%|█▎        | 46973/350343 [00:20<02:11, 2310.15 examples/s][A
Generating train examples...:  13%|█▎        | 47209/350343 [00:20<02:10, 2323.46 examples/s][A
Generating train examples...:  14%|█▎        | 47445/350343 [00:20<02:09, 2332.66 examples/s][A
Generating train examples...:  14%|█▎        | 47686/350343 [00:20<02:08, 2355.27 examples/s][A
Generating train examples...:  14%|█▎        | 47922/350343 [00:20<02:09, 2342.26 examples/s][A
Generating train examples...:  14%|█▎        | 48157/350343 [00:20<02:09, 2332.58 examples/s][A
Generating train examples...:  14%|█▍        | 48391/350343 [00:21<02:09, 2333.64 examples/s][A
Generating train examples...:  14%|█▍        | 48638/350343 [00:21<02:07, 2372.75 examples/s][A
Generating train examples...:  14%|█▍        | 48877/350343 [00:21<02:06, 2376.53 examples/s][A
Generating train examples...:  14%|█▍        | 49115/350343 [00:21<02:07, 2358.40 examples/s][A
Generating train examples...:  14%|█▍        | 49351/350343 [00:21<02:07, 2357.55 examples/s][A
Generating train examples...:  14%|█▍        | 49587/350343 [00:21<02:08, 2348.64 examples/s][A
Generating train examples...:  14%|█▍        | 49822/350343 [00:21<02:08, 2343.54 examples/s][A
Generating train examples...:  14%|█▍        | 50057/350343 [00:21<02:13, 2243.23 examples/s][A
Generating train examples...:  14%|█▍        | 50288/350343 [00:21<02:12, 2261.51 examples/s][A
Generating train examples...:  14%|█▍        | 50515/350343 [00:21<02:12, 2255.74 examples/s][A
Generating train examples...:  14%|█▍        | 50753/350343 [00:22<02:10, 2291.59 examples/s][A
Generating train examples...:  15%|█▍        | 50988/350343 [00:22<02:09, 2308.73 examples/s][A
Generating train examples...:  15%|█▍        | 51224/350343 [00:22<02:08, 2323.44 examples/s][A
Generating train examples...:  15%|█▍        | 51460/350343 [00:22<02:08, 2332.69 examples/s][A
Generating train examples...:  15%|█▍        | 51704/350343 [00:22<02:06, 2363.11 examples/s][A
Generating train examples...:  15%|█▍        | 51941/350343 [00:22<02:06, 2358.31 examples/s][A
Generating train examples...:  15%|█▍        | 52177/350343 [00:22<02:07, 2341.67 examples/s][A
Generating train examples...:  15%|█▍        | 52414/350343 [00:22<02:06, 2349.92 examples/s][A
Generating train examples...:  15%|█▌        | 52650/350343 [00:22<02:08, 2324.33 examples/s][A
Generating train examples...:  15%|█▌        | 52884/350343 [00:22<02:07, 2328.28 examples/s][A
Generating train examples...:  15%|█▌        | 53117/350343 [00:23<02:07, 2322.72 examples/s][A
Generating train examples...:  15%|█▌        | 53354/350343 [00:23<02:07, 2335.67 examples/s][A
Generating train examples...:  15%|█▌        | 53588/350343 [00:23<02:07, 2335.22 examples/s][A
Generating train examples...:  15%|█▌        | 53822/350343 [00:23<02:06, 2335.88 examples/s][A
Generating train examples...:  15%|█▌        | 54056/350343 [00:23<02:07, 2328.49 examples/s][A
Generating train examples...:  15%|█▌        | 54295/350343 [00:23<02:06, 2345.92 examples/s][A
Generating train examples...:  16%|█▌        | 54530/350343 [00:23<02:09, 2277.63 examples/s][A
Generating train examples...:  16%|█▌        | 54765/350343 [00:23<02:08, 2296.53 examples/s][A
Generating train examples...:  16%|█▌        | 55006/350343 [00:23<02:06, 2329.91 examples/s][A
Generating train examples...:  16%|█▌        | 55240/350343 [00:23<02:06, 2330.23 examples/s][A
Generating train examples...:  16%|█▌        | 55480/350343 [00:24<02:05, 2349.60 examples/s][A
Generating train examples...:  16%|█▌        | 55716/350343 [00:24<02:06, 2324.38 examples/s][A
Generating train examples...:  16%|█▌        | 55949/350343 [00:24<02:06, 2324.16 examples/s][A
Generating train examples...:  16%|█▌        | 56185/350343 [00:24<02:06, 2332.80 examples/s][A
Generating train examples...:  16%|█▌        | 56424/350343 [00:24<02:05, 2348.82 examples/s][A
Generating train examples...:  16%|█▌        | 56660/350343 [00:24<02:04, 2350.20 examples/s][A
Generating train examples...:  16%|█▌        | 56898/350343 [00:24<02:04, 2357.51 examples/s][A
Generating train examples...:  16%|█▋        | 57138/350343 [00:24<02:03, 2367.71 examples/s][A
Generating train examples...:  16%|█▋        | 57376/350343 [00:24<02:03, 2368.85 examples/s][A
Generating train examples...:  16%|█▋        | 57613/350343 [00:24<02:03, 2366.99 examples/s][A
Generating train examples...:  17%|█▋        | 57850/350343 [00:25<02:05, 2327.14 examples/s][A
Generating train examples...:  17%|█▋        | 58083/350343 [00:25<02:13, 2196.86 examples/s][A
Generating train examples...:  17%|█▋        | 58305/350343 [00:25<02:14, 2168.85 examples/s][A
Generating train examples...:  17%|█▋        | 58532/350343 [00:25<02:12, 2196.76 examples/s][A
Generating train examples...:  17%|█▋        | 58766/350343 [00:25<02:10, 2238.12 examples/s][A
Generating train examples...:  17%|█▋        | 59003/350343 [00:25<02:08, 2275.21 examples/s][A
Generating train examples...:  17%|█▋        | 59232/350343 [00:25<02:11, 2212.70 examples/s][A
Generating train examples...:  17%|█▋        | 59462/350343 [00:25<02:10, 2236.77 examples/s][A
Generating train examples...:  17%|█▋        | 59703/350343 [00:25<02:07, 2286.43 examples/s][A
Generating train examples...:  17%|█▋        | 59941/350343 [00:26<02:05, 2311.94 examples/s][A
Generating train examples...:  17%|█▋        | 60180/350343 [00:26<02:04, 2333.06 examples/s][A
Generating train examples...:  17%|█▋        | 60414/350343 [00:26<02:04, 2331.46 examples/s][A
Generating train examples...:  17%|█▋        | 60649/350343 [00:26<02:04, 2334.56 examples/s][A
Generating train examples...:  17%|█▋        | 60883/350343 [00:26<02:05, 2314.52 examples/s][A
Generating train examples...:  17%|█▋        | 61118/350343 [00:26<02:04, 2323.53 examples/s][A
Generating train examples...:  18%|█▊        | 61352/350343 [00:26<02:04, 2327.89 examples/s][A
Generating train examples...:  18%|█▊        | 61585/350343 [00:26<02:04, 2316.85 examples/s][A
Generating train examples...:  18%|█▊        | 61819/350343 [00:26<02:04, 2322.10 examples/s][A
Generating train examples...:  18%|█▊        | 62053/350343 [00:26<02:03, 2326.59 examples/s][A
Generating train examples...:  18%|█▊        | 62286/350343 [00:27<02:03, 2325.09 examples/s][A
Generating train examples...:  18%|█▊        | 62524/350343 [00:27<02:03, 2339.48 examples/s][A
Generating train examples...:  18%|█▊        | 62762/350343 [00:27<02:02, 2349.51 examples/s][A
Generating train examples...:  18%|█▊        | 62997/350343 [00:27<02:03, 2323.00 examples/s][A
Generating train examples...:  18%|█▊        | 63231/350343 [00:27<02:03, 2326.26 examples/s][A
Generating train examples...:  18%|█▊        | 63466/350343 [00:27<02:02, 2333.02 examples/s][A
Generating train examples...:  18%|█▊        | 63701/350343 [00:27<02:07, 2256.40 examples/s][A
Generating train examples...:  18%|█▊        | 63943/350343 [00:27<02:04, 2301.02 examples/s][A
Generating train examples...:  18%|█▊        | 64186/350343 [00:27<02:02, 2338.88 examples/s][A
Generating train examples...:  18%|█▊        | 64424/350343 [00:27<02:01, 2349.36 examples/s][A
Generating train examples...:  18%|█▊        | 64664/350343 [00:28<02:00, 2362.93 examples/s][A
Generating train examples...:  19%|█▊        | 64903/350343 [00:28<02:00, 2369.68 examples/s][A
Generating train examples...:  19%|█▊        | 65145/350343 [00:28<01:59, 2383.28 examples/s][A
Generating train examples...:  19%|█▊        | 65384/350343 [00:28<02:00, 2373.95 examples/s][A
Generating train examples...:  19%|█▊        | 65622/350343 [00:28<02:00, 2367.48 examples/s][A
Generating train examples...:  19%|█▉        | 65859/350343 [00:28<02:00, 2364.87 examples/s][A
Generating train examples...:  19%|█▉        | 66096/350343 [00:28<02:00, 2365.20 examples/s][A
Generating train examples...:  19%|█▉        | 66333/350343 [00:28<02:00, 2360.99 examples/s][A
Generating train examples...:  19%|█▉        | 66574/350343 [00:28<01:59, 2374.62 examples/s][A
Generating train examples...:  19%|█▉        | 66813/350343 [00:28<01:59, 2376.54 examples/s][A
Generating train examples...:  19%|█▉        | 67051/350343 [00:29<01:59, 2374.41 examples/s][A
Generating train examples...:  19%|█▉        | 67289/350343 [00:29<02:03, 2296.53 examples/s][A
Generating train examples...:  19%|█▉        | 67528/350343 [00:29<02:01, 2323.03 examples/s][A
Generating train examples...:  19%|█▉        | 67766/350343 [00:29<02:00, 2338.01 examples/s][A
Generating train examples...:  19%|█▉        | 68004/350343 [00:29<02:00, 2347.74 examples/s][A
Generating train examples...:  19%|█▉        | 68240/350343 [00:29<02:01, 2326.88 examples/s][A
Generating train examples...:  20%|█▉        | 68476/350343 [00:29<02:00, 2335.83 examples/s][A
Generating train examples...:  20%|█▉        | 68714/350343 [00:29<01:59, 2346.92 examples/s][A
Generating train examples...:  20%|█▉        | 68949/350343 [00:29<02:00, 2338.72 examples/s][A
Generating train examples...:  20%|█▉        | 69183/350343 [00:29<02:00, 2330.10 examples/s][A
Generating train examples...:  20%|█▉        | 69417/350343 [00:30<02:01, 2315.40 examples/s][A
Generating train examples...:  20%|█▉        | 69655/350343 [00:30<02:00, 2332.97 examples/s][A
Generating train examples...:  20%|█▉        | 69891/350343 [00:30<01:59, 2339.38 examples/s][A
Generating train examples...:  20%|██        | 70128/350343 [00:30<01:59, 2346.76 examples/s][A
Generating train examples...:  20%|██        | 70363/350343 [00:30<01:59, 2347.44 examples/s][A
Generating train examples...:  20%|██        | 70598/350343 [00:30<02:00, 2329.87 examples/s][A
Generating train examples...:  20%|██        | 70837/350343 [00:30<01:59, 2345.55 examples/s][A
Generating train examples...:  20%|██        | 71075/350343 [00:30<01:58, 2354.93 examples/s][A
Generating train examples...:  20%|██        | 71311/350343 [00:30<02:03, 2266.92 examples/s][A
Generating train examples...:  20%|██        | 71548/350343 [00:31<02:01, 2294.64 examples/s][A
Generating train examples...:  20%|██        | 71784/350343 [00:31<02:00, 2311.94 examples/s][A
Generating train examples...:  21%|██        | 72024/350343 [00:31<01:59, 2336.00 examples/s][A
Generating train examples...:  21%|██        | 72263/350343 [00:31<01:58, 2349.33 examples/s][A
Generating train examples...:  21%|██        | 72499/350343 [00:31<01:58, 2341.88 examples/s][A
Generating train examples...:  21%|██        | 72741/350343 [00:31<01:57, 2364.40 examples/s][A
Generating train examples...:  21%|██        | 72978/350343 [00:31<01:58, 2345.98 examples/s][A
Generating train examples...:  21%|██        | 73214/350343 [00:31<01:58, 2347.99 examples/s][A
Generating train examples...:  21%|██        | 73449/350343 [00:31<01:58, 2340.79 examples/s][A
Generating train examples...:  21%|██        | 73689/350343 [00:31<01:57, 2357.19 examples/s][A
Generating train examples...:  21%|██        | 73925/350343 [00:32<01:57, 2345.87 examples/s][A
Generating train examples...:  21%|██        | 74160/350343 [00:32<01:57, 2343.29 examples/s][A
Generating train examples...:  21%|██        | 74395/350343 [00:32<01:57, 2340.82 examples/s][A
Generating train examples...:  21%|██▏       | 74633/350343 [00:32<01:57, 2352.20 examples/s][A
Generating train examples...:  21%|██▏       | 74871/350343 [00:32<01:56, 2358.80 examples/s][A
Generating train examples...:  21%|██▏       | 75112/350343 [00:32<02:00, 2281.73 examples/s][A
Generating train examples...:  22%|██▏       | 75351/350343 [00:32<01:58, 2312.12 examples/s][A
Generating train examples...:  22%|██▏       | 75590/350343 [00:32<01:57, 2334.94 examples/s][A
Generating train examples...:  22%|██▏       | 75834/350343 [00:32<01:56, 2365.51 examples/s][A
Generating train examples...:  22%|██▏       | 76071/350343 [00:32<01:57, 2340.65 examples/s][A
Generating train examples...:  22%|██▏       | 76309/350343 [00:33<01:56, 2350.16 examples/s][A
Generating train examples...:  22%|██▏       | 76545/350343 [00:33<01:56, 2348.84 examples/s][A
Generating train examples...:  22%|██▏       | 76787/350343 [00:33<01:55, 2369.17 examples/s][A
Generating train examples...:  22%|██▏       | 77025/350343 [00:33<01:56, 2351.93 examples/s][A
Generating train examples...:  22%|██▏       | 77267/350343 [00:33<01:55, 2370.80 examples/s][A
Generating train examples...:  22%|██▏       | 77505/350343 [00:33<01:55, 2362.77 examples/s][A
Generating train examples...:  22%|██▏       | 77742/350343 [00:33<01:55, 2364.81 examples/s][A
Generating train examples...:  22%|██▏       | 77979/350343 [00:33<01:55, 2348.87 examples/s][A
Generating train examples...:  22%|██▏       | 78216/350343 [00:33<01:55, 2354.00 examples/s][A
Generating train examples...:  22%|██▏       | 78452/350343 [00:33<01:55, 2346.42 examples/s][A
Generating train examples...:  22%|██▏       | 78687/350343 [00:34<01:56, 2337.66 examples/s][A
Generating train examples...:  23%|██▎       | 78921/350343 [00:34<02:00, 2252.97 examples/s][A
Generating train examples...:  23%|██▎       | 79158/350343 [00:34<01:58, 2286.38 examples/s][A
Generating train examples...:  23%|██▎       | 79394/350343 [00:34<01:57, 2307.31 examples/s][A
Generating train examples...:  23%|██▎       | 79633/350343 [00:34<01:56, 2330.27 examples/s][A
Generating train examples...:  23%|██▎       | 79874/350343 [00:34<01:54, 2351.91 examples/s][A
Generating train examples...:  23%|██▎       | 80115/350343 [00:34<01:54, 2367.07 examples/s][A
Generating train examples...:  23%|██▎       | 80361/350343 [00:34<01:52, 2393.90 examples/s][A
Generating train examples...:  23%|██▎       | 80601/350343 [00:34<01:53, 2374.23 examples/s][A
Generating train examples...:  23%|██▎       | 80839/350343 [00:34<01:53, 2373.87 examples/s][A
Generating train examples...:  23%|██▎       | 81077/350343 [00:35<01:53, 2369.35 examples/s][A
Generating train examples...:  23%|██▎       | 81315/350343 [00:35<01:54, 2356.68 examples/s][A
Generating train examples...:  23%|██▎       | 81551/350343 [00:35<01:54, 2354.52 examples/s][A
Generating train examples...:  23%|██▎       | 81787/350343 [00:35<01:55, 2330.14 examples/s][A
Generating train examples...:  23%|██▎       | 82026/350343 [00:35<01:54, 2347.72 examples/s][A
Generating train examples...:  23%|██▎       | 82265/350343 [00:35<01:53, 2359.91 examples/s][A
Generating train examples...:  24%|██▎       | 82502/350343 [00:35<01:58, 2261.39 examples/s][A
Generating train examples...:  24%|██▎       | 82735/350343 [00:35<01:57, 2279.02 examples/s][A
Generating train examples...:  24%|██▎       | 82970/350343 [00:35<01:56, 2299.12 examples/s][A
Generating train examples...:  24%|██▍       | 83208/350343 [00:35<01:55, 2321.27 examples/s][A
Generating train examples...:  24%|██▍       | 83447/350343 [00:36<01:54, 2340.42 examples/s][A
Generating train examples...:  24%|██▍       | 83685/350343 [00:36<01:53, 2351.33 examples/s][A
Generating train examples...:  24%|██▍       | 83921/350343 [00:36<01:53, 2341.45 examples/s][A
Generating train examples...:  24%|██▍       | 84158/350343 [00:36<01:53, 2348.42 examples/s][A
Generating train examples...:  24%|██▍       | 84396/350343 [00:36<01:52, 2356.67 examples/s][A
Generating train examples...:  24%|██▍       | 84632/350343 [00:36<01:53, 2345.02 examples/s][A
Generating train examples...:  24%|██▍       | 84871/350343 [00:36<01:52, 2356.70 examples/s][A
Generating train examples...:  24%|██▍       | 85109/350343 [00:36<01:52, 2362.17 examples/s][A
Generating train examples...:  24%|██▍       | 85346/350343 [00:36<01:52, 2362.79 examples/s][A
Generating train examples...:  24%|██▍       | 85583/350343 [00:36<01:52, 2363.37 examples/s][A
Generating train examples...:  24%|██▍       | 85825/350343 [00:37<01:51, 2378.56 examples/s][A
Generating train examples...:  25%|██▍       | 86063/350343 [00:37<01:51, 2362.41 examples/s][A
Generating train examples...:  25%|██▍       | 86300/350343 [00:37<01:55, 2278.77 examples/s][A
Generating train examples...:  25%|██▍       | 86539/350343 [00:37<01:54, 2309.96 examples/s][A
Generating train examples...:  25%|██▍       | 86773/350343 [00:37<01:53, 2317.87 examples/s][A
Generating train examples...:  25%|██▍       | 87006/350343 [00:37<01:54, 2308.13 examples/s][A
Generating train examples...:  25%|██▍       | 87239/350343 [00:37<01:53, 2312.87 examples/s][A
Generating train examples...:  25%|██▍       | 87471/350343 [00:37<01:53, 2314.12 examples/s][A
Generating train examples...:  25%|██▌       | 87713/350343 [00:37<01:51, 2345.54 examples/s][A
Generating train examples...:  25%|██▌       | 87952/350343 [00:38<01:51, 2356.76 examples/s][A
Generating train examples...:  25%|██▌       | 88188/350343 [00:38<01:51, 2346.01 examples/s][A
Generating train examples...:  25%|██▌       | 88423/350343 [00:38<01:51, 2345.76 examples/s][A
Generating train examples...:  25%|██▌       | 88658/350343 [00:38<01:51, 2346.29 examples/s][A
Generating train examples...:  25%|██▌       | 88893/350343 [00:38<01:51, 2337.23 examples/s][A
Generating train examples...:  25%|██▌       | 89132/350343 [00:38<01:51, 2350.80 examples/s][A
Generating train examples...:  26%|██▌       | 89368/350343 [00:38<01:51, 2339.53 examples/s][A
Generating train examples...:  26%|██▌       | 89607/350343 [00:38<01:50, 2352.34 examples/s][A
Generating train examples...:  26%|██▌       | 89848/350343 [00:38<01:49, 2369.28 examples/s][A
Generating train examples...:  26%|██▌       | 90085/350343 [00:38<01:49, 2367.18 examples/s][A
Generating train examples...:  26%|██▌       | 90322/350343 [00:39<01:55, 2245.86 examples/s][A
Generating train examples...:  26%|██▌       | 90561/350343 [00:39<01:53, 2285.70 examples/s][A
Generating train examples...:  26%|██▌       | 90796/350343 [00:39<01:52, 2304.35 examples/s][A
Generating train examples...:  26%|██▌       | 91033/350343 [00:39<01:51, 2323.24 examples/s][A
Generating train examples...:  26%|██▌       | 91266/350343 [00:39<01:51, 2321.19 examples/s][A
Generating train examples...:  26%|██▌       | 91499/350343 [00:39<01:52, 2309.74 examples/s][A
Generating train examples...:  26%|██▌       | 91731/350343 [00:39<01:52, 2307.46 examples/s][A
Generating train examples...:  26%|██▌       | 91963/350343 [00:39<01:51, 2309.47 examples/s][A
Generating train examples...:  26%|██▋       | 92203/350343 [00:39<01:50, 2334.59 examples/s][A
Generating train examples...:  26%|██▋       | 92443/350343 [00:39<01:49, 2352.08 examples/s][A
Generating train examples...:  26%|██▋       | 92682/350343 [00:40<01:49, 2362.61 examples/s][A
Generating train examples...:  27%|██▋       | 92919/350343 [00:40<01:49, 2359.28 examples/s][A
Generating train examples...:  27%|██▋       | 93159/350343 [00:40<01:48, 2370.49 examples/s][A
Generating train examples...:  27%|██▋       | 93397/350343 [00:40<01:48, 2365.29 examples/s][A
Generating train examples...:  27%|██▋       | 93634/350343 [00:40<01:49, 2351.36 examples/s][A
Generating train examples...:  27%|██▋       | 93870/350343 [00:40<01:49, 2347.35 examples/s][A
Generating train examples...:  27%|██▋       | 94110/350343 [00:40<01:48, 2361.10 examples/s][A
Generating train examples...:  27%|██▋       | 94347/350343 [00:40<01:52, 2266.08 examples/s][A
Generating train examples...:  27%|██▋       | 94575/350343 [00:40<01:52, 2264.47 examples/s][A
Generating train examples...:  27%|██▋       | 94815/350343 [00:40<01:50, 2302.71 examples/s][A
Generating train examples...:  27%|██▋       | 95057/350343 [00:41<01:49, 2336.92 examples/s][A
Generating train examples...:  27%|██▋       | 95292/350343 [00:41<01:49, 2330.66 examples/s][A
Generating train examples...:  27%|██▋       | 95528/350343 [00:41<01:49, 2337.54 examples/s][A
Generating train examples...:  27%|██▋       | 95762/350343 [00:41<01:48, 2338.20 examples/s][A
Generating train examples...:  27%|██▋       | 96002/350343 [00:41<01:48, 2354.33 examples/s][A
Generating train examples...:  27%|██▋       | 96238/350343 [00:41<01:48, 2337.50 examples/s][A
Generating train examples...:  28%|██▊       | 96478/350343 [00:41<01:47, 2355.49 examples/s][A
Generating train examples...:  28%|██▊       | 96716/350343 [00:41<01:47, 2361.94 examples/s][A
Generating train examples...:  28%|██▊       | 96953/350343 [00:41<01:47, 2358.13 examples/s][A
Generating train examples...:  28%|██▊       | 97189/350343 [00:41<01:48, 2336.72 examples/s][A
Generating train examples...:  28%|██▊       | 97425/350343 [00:42<01:47, 2343.30 examples/s][A
Generating train examples...:  28%|██▊       | 97667/350343 [00:42<01:46, 2364.07 examples/s][A
Generating train examples...:  28%|██▊       | 97904/350343 [00:42<01:47, 2358.40 examples/s][A
Generating train examples...:  28%|██▊       | 98142/350343 [00:42<01:46, 2364.14 examples/s][A
Generating train examples...:  28%|██▊       | 98379/350343 [00:42<01:50, 2287.95 examples/s][A
Generating train examples...:  28%|██▊       | 98615/350343 [00:42<01:49, 2307.55 examples/s][A
Generating train examples...:  28%|██▊       | 98855/350343 [00:42<01:47, 2334.28 examples/s][A
Generating train examples...:  28%|██▊       | 99091/350343 [00:42<01:47, 2339.27 examples/s][A
Generating train examples...:  28%|██▊       | 99326/350343 [00:42<01:47, 2337.26 examples/s][A
Generating train examples...:  28%|██▊       | 99564/350343 [00:42<01:46, 2347.37 examples/s][A
Generating train examples...:  28%|██▊       | 99799/350343 [00:43<01:47, 2324.69 examples/s][A
Generating train examples...:  29%|██▊       | 100032/350343 [00:43<01:52, 2222.38 examples/s][A
Generating train examples...:  29%|██▊       | 100256/350343 [00:43<01:53, 2208.39 examples/s][A
Generating train examples...:  29%|██▊       | 100482/350343 [00:43<01:52, 2222.68 examples/s][A
Generating train examples...:  29%|██▊       | 100711/350343 [00:43<01:51, 2241.56 examples/s][A
Generating train examples...:  29%|██▉       | 100942/350343 [00:43<01:50, 2260.45 examples/s][A
Generating train examples...:  29%|██▉       | 101171/350343 [00:43<01:49, 2267.66 examples/s][A
Generating train examples...:  29%|██▉       | 101404/350343 [00:43<01:48, 2284.39 examples/s][A
Generating train examples...:  29%|██▉       | 101633/350343 [00:43<01:50, 2255.90 examples/s][A
Generating train examples...:  29%|██▉       | 101868/350343 [00:44<01:48, 2282.40 examples/s][A
Generating train examples...:  29%|██▉       | 102105/350343 [00:44<01:47, 2308.22 examples/s][A
Generating train examples...:  29%|██▉       | 102344/350343 [00:44<01:46, 2332.44 examples/s][A
Generating train examples...:  29%|██▉       | 102580/350343 [00:44<01:45, 2339.00 examples/s][A
Generating train examples...:  29%|██▉       | 102817/350343 [00:44<01:45, 2347.34 examples/s][A
Generating train examples...:  29%|██▉       | 103056/350343 [00:44<01:44, 2357.77 examples/s][A
Generating train examples...:  29%|██▉       | 103292/350343 [00:44<01:45, 2341.97 examples/s][A
Generating train examples...:  30%|██▉       | 103527/350343 [00:44<01:50, 2238.59 examples/s][A
Generating train examples...:  30%|██▉       | 103764/350343 [00:44<01:48, 2274.78 examples/s][A
Generating train examples...:  30%|██▉       | 103999/350343 [00:44<01:47, 2296.15 examples/s][A
Generating train examples...:  30%|██▉       | 104232/350343 [00:45<01:46, 2305.55 examples/s][A
Generating train examples...:  30%|██▉       | 104471/350343 [00:45<01:45, 2329.20 examples/s][A
Generating train examples...:  30%|██▉       | 104706/350343 [00:45<01:45, 2335.35 examples/s][A
Generating train examples...:  30%|██▉       | 104946/350343 [00:45<01:44, 2353.24 examples/s][A
Generating train examples...:  30%|███       | 105187/350343 [00:45<01:43, 2367.32 examples/s][A
Generating train examples...:  30%|███       | 105424/350343 [00:45<01:44, 2353.04 examples/s][A
Generating train examples...:  30%|███       | 105662/350343 [00:45<01:43, 2360.78 examples/s][A
Generating train examples...:  30%|███       | 105901/350343 [00:45<01:43, 2366.97 examples/s][A
Generating train examples...:  30%|███       | 106141/350343 [00:45<01:42, 2376.66 examples/s][A
Generating train examples...:  30%|███       | 106380/350343 [00:45<01:42, 2379.10 examples/s][A
Generating train examples...:  30%|███       | 106618/350343 [00:46<01:43, 2360.84 examples/s][A
Generating train examples...:  31%|███       | 106855/350343 [00:46<01:43, 2351.30 examples/s][A
Generating train examples...:  31%|███       | 107096/350343 [00:46<01:42, 2367.07 examples/s][A
Generating train examples...:  31%|███       | 107334/350343 [00:46<01:46, 2284.33 examples/s][A
Generating train examples...:  31%|███       | 107571/350343 [00:46<01:45, 2308.59 examples/s][A
Generating train examples...:  31%|███       | 107811/350343 [00:46<01:43, 2332.80 examples/s][A
Generating train examples...:  31%|███       | 108048/350343 [00:46<01:43, 2341.69 examples/s][A
Generating train examples...:  31%|███       | 108287/350343 [00:46<01:42, 2354.46 examples/s][A
Generating train examples...:  31%|███       | 108523/350343 [00:46<01:42, 2353.53 examples/s][A
Generating train examples...:  31%|███       | 108759/350343 [00:46<01:42, 2348.25 examples/s][A
Generating train examples...:  31%|███       | 108994/350343 [00:47<01:44, 2316.81 examples/s][A
Generating train examples...:  31%|███       | 109237/350343 [00:47<01:42, 2347.70 examples/s][A
Generating train examples...:  31%|███       | 109476/350343 [00:47<01:42, 2358.76 examples/s][A
Generating train examples...:  31%|███▏      | 109712/350343 [00:47<01:42, 2339.44 examples/s][A
Generating train examples...:  31%|███▏      | 109947/350343 [00:47<01:42, 2342.34 examples/s][A
Generating train examples...:  31%|███▏      | 110182/350343 [00:47<01:42, 2333.39 examples/s][A
Generating train examples...:  32%|███▏      | 110425/350343 [00:47<01:41, 2360.63 examples/s][A
Generating train examples...:  32%|███▏      | 110662/350343 [00:47<01:41, 2353.30 examples/s][A
Generating train examples...:  32%|███▏      | 110899/350343 [00:47<01:41, 2357.92 examples/s][A
Generating train examples...:  32%|███▏      | 111135/350343 [00:47<01:45, 2260.68 examples/s][A
Generating train examples...:  32%|███▏      | 111376/350343 [00:48<01:43, 2303.69 examples/s][A
Generating train examples...:  32%|███▏      | 111621/350343 [00:48<01:41, 2344.72 examples/s][A
Generating train examples...:  32%|███▏      | 111858/350343 [00:48<01:41, 2350.77 examples/s][A
Generating train examples...:  32%|███▏      | 112094/350343 [00:48<01:42, 2318.78 examples/s][A
Generating train examples...:  32%|███▏      | 112330/350343 [00:48<01:42, 2329.55 examples/s][A
Generating train examples...:  32%|███▏      | 112568/350343 [00:48<01:41, 2342.43 examples/s][A
Generating train examples...:  32%|███▏      | 112806/350343 [00:48<01:40, 2352.80 examples/s][A
Generating train examples...:  32%|███▏      | 113042/350343 [00:48<01:40, 2352.82 examples/s][A
Generating train examples...:  32%|███▏      | 113282/350343 [00:48<01:40, 2365.89 examples/s][A
Generating train examples...:  32%|███▏      | 113519/350343 [00:48<01:40, 2361.96 examples/s][A
Generating train examples...:  32%|███▏      | 113757/350343 [00:49<01:39, 2366.67 examples/s][A
Generating train examples...:  33%|███▎      | 113998/350343 [00:49<01:39, 2377.78 examples/s][A
Generating train examples...:  33%|███▎      | 114238/350343 [00:49<01:39, 2382.51 examples/s][A
Generating train examples...:  33%|███▎      | 114477/350343 [00:49<01:38, 2384.01 examples/s][A
Generating train examples...:  33%|███▎      | 114716/350343 [00:49<01:42, 2288.63 examples/s][A
Generating train examples...:  33%|███▎      | 114951/350343 [00:49<01:42, 2305.81 examples/s][A
Generating train examples...:  33%|███▎      | 115187/350343 [00:49<01:41, 2319.99 examples/s][A
Generating train examples...:  33%|███▎      | 115420/350343 [00:49<01:41, 2322.05 examples/s][A
Generating train examples...:  33%|███▎      | 115658/350343 [00:49<01:40, 2338.80 examples/s][A
Generating train examples...:  33%|███▎      | 115893/350343 [00:50<01:40, 2341.17 examples/s][A
Generating train examples...:  33%|███▎      | 116129/350343 [00:50<01:39, 2345.36 examples/s][A
Generating train examples...:  33%|███▎      | 116366/350343 [00:50<01:39, 2350.87 examples/s][A
Generating train examples...:  33%|███▎      | 116602/350343 [00:50<01:39, 2351.50 examples/s][A
Generating train examples...:  33%|███▎      | 116838/350343 [00:50<01:39, 2351.51 examples/s][A
Generating train examples...:  33%|███▎      | 117074/350343 [00:50<01:39, 2352.99 examples/s][A
Generating train examples...:  33%|███▎      | 117310/350343 [00:50<01:39, 2352.63 examples/s][A
Generating train examples...:  34%|███▎      | 117546/350343 [00:50<01:39, 2349.13 examples/s][A
Generating train examples...:  34%|███▎      | 117784/350343 [00:50<01:38, 2358.13 examples/s][A
Generating train examples...:  34%|███▎      | 118023/350343 [00:50<01:38, 2367.26 examples/s][A
Generating train examples...:  34%|███▍      | 118260/350343 [00:51<01:38, 2364.78 examples/s][A
Generating train examples...:  34%|███▍      | 118497/350343 [00:51<01:38, 2356.41 examples/s][A
Generating train examples...:  34%|███▍      | 118733/350343 [00:51<01:38, 2351.75 examples/s][A
Generating train examples...:  34%|███▍      | 118969/350343 [00:51<01:41, 2268.42 examples/s][A
Generating train examples...:  34%|███▍      | 119203/350343 [00:51<01:40, 2288.61 examples/s][A
Generating train examples...:  34%|███▍      | 119434/350343 [00:51<01:40, 2292.88 examples/s][A
Generating train examples...:  34%|███▍      | 119664/350343 [00:51<01:40, 2293.65 examples/s][A
Generating train examples...:  34%|███▍      | 119901/350343 [00:51<01:39, 2315.90 examples/s][A
Generating train examples...:  34%|███▍      | 120142/350343 [00:51<01:38, 2343.36 examples/s][A
Generating train examples...:  34%|███▍      | 120377/350343 [00:51<01:38, 2344.77 examples/s][A
Generating train examples...:  34%|███▍      | 120612/350343 [00:52<01:38, 2325.24 examples/s][A
Generating train examples...:  34%|███▍      | 120846/350343 [00:52<01:38, 2327.31 examples/s][A
Generating train examples...:  35%|███▍      | 121084/350343 [00:52<01:37, 2341.80 examples/s][A
Generating train examples...:  35%|███▍      | 121319/350343 [00:52<01:37, 2340.83 examples/s][A
Generating train examples...:  35%|███▍      | 121554/350343 [00:52<01:38, 2330.73 examples/s][A
Generating train examples...:  35%|███▍      | 121790/350343 [00:52<01:37, 2337.07 examples/s][A
Generating train examples...:  35%|███▍      | 122029/350343 [00:52<01:37, 2350.83 examples/s][A
Generating train examples...:  35%|███▍      | 122265/350343 [00:52<01:36, 2352.21 examples/s][A
Generating train examples...:  35%|███▍      | 122501/350343 [00:52<01:37, 2344.31 examples/s][A
Generating train examples...:  35%|███▌      | 122736/350343 [00:52<01:37, 2345.11 examples/s][A
Generating train examples...:  35%|███▌      | 122971/350343 [00:53<01:37, 2326.69 examples/s][A
Generating train examples...:  35%|███▌      | 123204/350343 [00:53<01:38, 2303.95 examples/s][A
Generating train examples...:  35%|███▌      | 123436/350343 [00:53<01:41, 2242.52 examples/s][A
Generating train examples...:  35%|███▌      | 123670/350343 [00:53<01:39, 2269.99 examples/s][A
Generating train examples...:  35%|███▌      | 123908/350343 [00:53<01:38, 2302.17 examples/s][A
Generating train examples...:  35%|███▌      | 124141/350343 [00:53<01:37, 2309.69 examples/s][A
Generating train examples...:  36%|███▌      | 124377/350343 [00:53<01:37, 2323.41 examples/s][A
Generating train examples...:  36%|███▌      | 124611/350343 [00:53<01:37, 2325.98 examples/s][A
Generating train examples...:  36%|███▌      | 124850/350343 [00:53<01:36, 2343.34 examples/s][A
Generating train examples...:  36%|███▌      | 125085/350343 [00:53<01:38, 2277.47 examples/s][A
Generating train examples...:  36%|███▌      | 125320/350343 [00:54<01:37, 2296.80 examples/s][A
Generating train examples...:  36%|███▌      | 125557/350343 [00:54<01:37, 2316.18 examples/s][A
Generating train examples...:  36%|███▌      | 125792/350343 [00:54<01:36, 2326.12 examples/s][A
Generating train examples...:  36%|███▌      | 126029/350343 [00:54<01:35, 2338.27 examples/s][A
Generating train examples...:  36%|███▌      | 126263/350343 [00:54<01:36, 2332.81 examples/s][A
Generating train examples...:  36%|███▌      | 126497/350343 [00:54<01:35, 2334.92 examples/s][A
Generating train examples...:  36%|███▌      | 126735/350343 [00:54<01:35, 2346.39 examples/s][A
Generating train examples...:  36%|███▌      | 126981/350343 [00:54<01:33, 2378.67 examples/s][A
Generating train examples...:  36%|███▋      | 127225/350343 [00:54<01:33, 2395.18 examples/s][A
Generating train examples...:  36%|███▋      | 127465/350343 [00:54<01:36, 2303.01 examples/s][A
Generating train examples...:  36%|███▋      | 127700/350343 [00:55<01:36, 2313.59 examples/s][A
Generating train examples...:  37%|███▋      | 127944/350343 [00:55<01:34, 2348.03 examples/s][A
Generating train examples...:  37%|███▋      | 128182/350343 [00:55<01:34, 2355.73 examples/s][A
Generating train examples...:  37%|███▋      | 128418/350343 [00:55<01:34, 2339.60 examples/s][A
Generating train examples...:  37%|███▋      | 128655/350343 [00:55<01:34, 2346.19 examples/s][A
Generating train examples...:  37%|███▋      | 128892/350343 [00:55<01:34, 2352.21 examples/s][A
Generating train examples...:  37%|███▋      | 129132/350343 [00:55<01:33, 2364.01 examples/s][A
Generating train examples...:  37%|███▋      | 129370/350343 [00:55<01:33, 2368.65 examples/s][A
Generating train examples...:  37%|███▋      | 129607/350343 [00:55<01:33, 2368.39 examples/s][A
Generating train examples...:  37%|███▋      | 129844/350343 [00:55<01:33, 2364.36 examples/s][A
Generating train examples...:  37%|███▋      | 130082/350343 [00:56<01:33, 2367.54 examples/s][A
Generating train examples...:  37%|███▋      | 130323/350343 [00:56<01:32, 2378.42 examples/s][A
Generating train examples...:  37%|███▋      | 130561/350343 [00:56<01:32, 2372.56 examples/s][A
Generating train examples...:  37%|███▋      | 130799/350343 [00:56<01:32, 2365.69 examples/s][A
Generating train examples...:  37%|███▋      | 131039/350343 [00:56<01:32, 2375.74 examples/s][A
Generating train examples...:  37%|███▋      | 131277/350343 [00:56<01:35, 2284.63 examples/s][A
Generating train examples...:  38%|███▊      | 131516/350343 [00:56<01:34, 2313.40 examples/s][A
Generating train examples...:  38%|███▊      | 131757/350343 [00:56<01:33, 2341.61 examples/s][A
Generating train examples...:  38%|███▊      | 131998/350343 [00:56<01:32, 2359.45 examples/s][A
Generating train examples...:  38%|███▊      | 132238/350343 [00:56<01:32, 2369.85 examples/s][A
Generating train examples...:  38%|███▊      | 132478/350343 [00:57<01:31, 2378.61 examples/s][A
Generating train examples...:  38%|███▊      | 132718/350343 [00:57<01:31, 2383.37 examples/s][A
Generating train examples...:  38%|███▊      | 132959/350343 [00:57<01:30, 2389.09 examples/s][A
Generating train examples...:  38%|███▊      | 133199/350343 [00:57<01:31, 2374.83 examples/s][A
Generating train examples...:  38%|███▊      | 133439/350343 [00:57<01:31, 2380.93 examples/s][A
Generating train examples...:  38%|███▊      | 133678/350343 [00:57<01:31, 2370.76 examples/s][A
Generating train examples...:  38%|███▊      | 133918/350343 [00:57<01:30, 2379.39 examples/s][A
Generating train examples...:  38%|███▊      | 134156/350343 [00:57<01:31, 2369.63 examples/s][A
Generating train examples...:  38%|███▊      | 134393/350343 [00:57<01:31, 2365.84 examples/s][A
Generating train examples...:  38%|███▊      | 134630/350343 [00:58<01:31, 2354.16 examples/s][A
Generating train examples...:  38%|███▊      | 134866/350343 [00:58<01:35, 2255.65 examples/s][A
Generating train examples...:  39%|███▊      | 135104/350343 [00:58<01:33, 2289.96 examples/s][A
Generating train examples...:  39%|███▊      | 135340/350343 [00:58<01:33, 2308.73 examples/s][A
Generating train examples...:  39%|███▊      | 135572/350343 [00:58<01:33, 2304.05 examples/s][A
Generating train examples...:  39%|███▉      | 135806/350343 [00:58<01:32, 2314.38 examples/s][A
Generating train examples...:  39%|███▉      | 136041/350343 [00:58<01:32, 2322.95 examples/s][A
Generating train examples...:  39%|███▉      | 136274/350343 [00:58<01:32, 2317.18 examples/s][A
Generating train examples...:  39%|███▉      | 136507/350343 [00:58<01:32, 2319.12 examples/s][A
Generating train examples...:  39%|███▉      | 136752/350343 [00:58<01:30, 2356.09 examples/s][A
Generating train examples...:  39%|███▉      | 136991/350343 [00:59<01:30, 2365.85 examples/s][A
Generating train examples...:  39%|███▉      | 137228/350343 [00:59<01:30, 2354.84 examples/s][A
Generating train examples...:  39%|███▉      | 137464/350343 [00:59<01:30, 2353.60 examples/s][A
Generating train examples...:  39%|███▉      | 137700/350343 [00:59<01:30, 2342.66 examples/s][A
Generating train examples...:  39%|███▉      | 137935/350343 [00:59<01:30, 2343.75 examples/s][A
Generating train examples...:  39%|███▉      | 138170/350343 [00:59<01:30, 2343.09 examples/s][A
Generating train examples...:  40%|███▉      | 138408/350343 [00:59<01:30, 2352.89 examples/s][A
Generating train examples...:  40%|███▉      | 138648/350343 [00:59<01:29, 2365.68 examples/s][A
Generating train examples...:  40%|███▉      | 138885/350343 [00:59<01:33, 2272.88 examples/s][A
Generating train examples...:  40%|███▉      | 139123/350343 [00:59<01:31, 2302.35 examples/s][A
Generating train examples...:  40%|███▉      | 139357/350343 [01:00<01:31, 2313.13 examples/s][A
Generating train examples...:  40%|███▉      | 139595/350343 [01:00<01:30, 2331.59 examples/s][A
Generating train examples...:  40%|███▉      | 139831/350343 [01:00<01:29, 2339.56 examples/s][A
Generating train examples...:  40%|███▉      | 140068/350343 [01:00<01:29, 2347.30 examples/s][A
Generating train examples...:  40%|████      | 140307/350343 [01:00<01:29, 2359.72 examples/s][A
Generating train examples...:  40%|████      | 140544/350343 [01:00<01:28, 2358.44 examples/s][A
Generating train examples...:  40%|████      | 140788/350343 [01:00<01:27, 2381.55 examples/s][A
Generating train examples...:  40%|████      | 141027/350343 [01:00<01:28, 2378.23 examples/s][A
Generating train examples...:  40%|████      | 141270/350343 [01:00<01:27, 2392.47 examples/s][A
Generating train examples...:  40%|████      | 141510/350343 [01:00<01:27, 2381.74 examples/s][A
Generating train examples...:  40%|████      | 141749/350343 [01:01<01:27, 2373.35 examples/s][A
Generating train examples...:  41%|████      | 141987/350343 [01:01<01:27, 2372.80 examples/s][A
Generating train examples...:  41%|████      | 142225/350343 [01:01<01:28, 2347.66 examples/s][A
Generating train examples...:  41%|████      | 142460/350343 [01:01<01:36, 2162.67 examples/s][A
Generating train examples...:  41%|████      | 142688/350343 [01:01<01:34, 2193.72 examples/s][A
Generating train examples...:  41%|████      | 142920/350343 [01:01<01:33, 2229.68 examples/s][A
Generating train examples...:  41%|████      | 143150/350343 [01:01<01:32, 2247.85 examples/s][A
Generating train examples...:  41%|████      | 143380/350343 [01:01<01:31, 2261.24 examples/s][A
Generating train examples...:  41%|████      | 143611/350343 [01:01<01:30, 2273.98 examples/s][A
Generating train examples...:  41%|████      | 143840/350343 [01:01<01:30, 2277.46 examples/s][A
Generating train examples...:  41%|████      | 144078/350343 [01:02<01:29, 2306.97 examples/s][A
Generating train examples...:  41%|████      | 144312/350343 [01:02<01:28, 2315.94 examples/s][A
Generating train examples...:  41%|████▏     | 144544/350343 [01:02<01:29, 2311.38 examples/s][A
Generating train examples...:  41%|████▏     | 144776/350343 [01:02<01:29, 2305.78 examples/s][A
Generating train examples...:  41%|████▏     | 145007/350343 [01:02<01:29, 2306.68 examples/s][A
Generating train examples...:  41%|████▏     | 145244/350343 [01:02<01:28, 2324.33 examples/s][A
Generating train examples...:  42%|████▏     | 145482/350343 [01:02<01:27, 2338.69 examples/s][A
Generating train examples...:  42%|████▏     | 145716/350343 [01:02<01:28, 2325.12 examples/s][A
Generating train examples...:  42%|████▏     | 145953/350343 [01:02<01:27, 2338.07 examples/s][A
Generating train examples...:  42%|████▏     | 146187/350343 [01:02<01:27, 2330.81 examples/s][A
Generating train examples...:  42%|████▏     | 146424/350343 [01:03<01:27, 2341.52 examples/s][A
Generating train examples...:  42%|████▏     | 146666/350343 [01:03<01:26, 2362.84 examples/s][A
Generating train examples...:  42%|████▏     | 146903/350343 [01:03<01:26, 2350.93 examples/s][A
Generating train examples...:  42%|████▏     | 147142/350343 [01:03<01:26, 2361.05 examples/s][A
Generating train examples...:  42%|████▏     | 147380/350343 [01:03<01:25, 2364.93 examples/s][A
Generating train examples...:  42%|████▏     | 147617/350343 [01:03<01:29, 2264.45 examples/s][A
Generating train examples...:  42%|████▏     | 147854/350343 [01:03<01:28, 2294.75 examples/s][A
Generating train examples...:  42%|████▏     | 148096/350343 [01:03<01:26, 2331.03 examples/s][A
Generating train examples...:  42%|████▏     | 148336/350343 [01:03<01:25, 2350.46 examples/s][A
Generating train examples...:  42%|████▏     | 148576/350343 [01:04<01:25, 2365.10 examples/s][A
Generating train examples...:  42%|████▏     | 148813/350343 [01:04<01:25, 2363.35 examples/s][A
Generating train examples...:  43%|████▎     | 149055/350343 [01:04<01:24, 2377.85 examples/s][A
Generating train examples...:  43%|████▎     | 149293/350343 [01:04<01:25, 2360.35 examples/s][A
Generating train examples...:  43%|████▎     | 149530/350343 [01:04<01:25, 2353.65 examples/s][A
Generating train examples...:  43%|████▎     | 149766/350343 [01:04<01:25, 2354.02 examples/s][A
Generating train examples...:  43%|████▎     | 150002/350343 [01:04<01:25, 2348.44 examples/s][A
Generating train examples...:  43%|████▎     | 150237/350343 [01:04<01:25, 2346.10 examples/s][A
Generating train examples...:  43%|████▎     | 150472/350343 [01:04<01:25, 2344.06 examples/s][A
Generating train examples...:  43%|████▎     | 150708/350343 [01:04<01:25, 2347.05 examples/s][A
Generating train examples...:  43%|████▎     | 150943/350343 [01:05<01:25, 2337.14 examples/s][A
Generating train examples...:  43%|████▎     | 151182/350343 [01:05<01:24, 2350.72 examples/s][A
Generating train examples...:  43%|████▎     | 151418/350343 [01:05<01:28, 2259.41 examples/s][A
Generating train examples...:  43%|████▎     | 151663/350343 [01:05<01:25, 2314.22 examples/s][A
Generating train examples...:  43%|████▎     | 151896/350343 [01:05<01:25, 2311.43 examples/s][A
Generating train examples...:  43%|████▎     | 152129/350343 [01:05<01:25, 2315.58 examples/s][A
Generating train examples...:  43%|████▎     | 152366/350343 [01:05<01:24, 2330.04 examples/s][A
Generating train examples...:  44%|████▎     | 152604/350343 [01:05<01:24, 2343.14 examples/s][A
Generating train examples...:  44%|████▎     | 152841/350343 [01:05<01:24, 2349.96 examples/s][A
Generating train examples...:  44%|████▎     | 153077/350343 [01:05<01:23, 2352.69 examples/s][A
Generating train examples...:  44%|████▍     | 153315/350343 [01:06<01:23, 2359.25 examples/s][A
Generating train examples...:  44%|████▍     | 153555/350343 [01:06<01:23, 2368.99 examples/s][A
Generating train examples...:  44%|████▍     | 153795/350343 [01:06<01:22, 2377.44 examples/s][A
Generating train examples...:  44%|████▍     | 154033/350343 [01:06<01:22, 2371.92 examples/s][A
Generating train examples...:  44%|████▍     | 154273/350343 [01:06<01:22, 2378.60 examples/s][A
Generating train examples...:  44%|████▍     | 154511/350343 [01:06<01:22, 2368.03 examples/s][A
Generating train examples...:  44%|████▍     | 154748/350343 [01:06<01:22, 2365.87 examples/s][A
Generating train examples...:  44%|████▍     | 154985/350343 [01:06<01:23, 2347.87 examples/s][A
Generating train examples...:  44%|████▍     | 155220/350343 [01:06<01:25, 2275.94 examples/s][A
Generating train examples...:  44%|████▍     | 155457/350343 [01:06<01:24, 2302.14 examples/s][A
Generating train examples...:  44%|████▍     | 155695/350343 [01:07<01:23, 2323.00 examples/s][A
Generating train examples...:  45%|████▍     | 155928/350343 [01:07<01:23, 2318.43 examples/s][A
Generating train examples...:  45%|████▍     | 156161/350343 [01:07<01:24, 2309.97 examples/s][A
Generating train examples...:  45%|████▍     | 156394/350343 [01:07<01:23, 2314.62 examples/s][A
Generating train examples...:  45%|████▍     | 156626/350343 [01:07<01:24, 2305.34 examples/s][A
Generating train examples...:  45%|████▍     | 156862/350343 [01:07<01:23, 2319.62 examples/s][A
Generating train examples...:  45%|████▍     | 157096/350343 [01:07<01:23, 2325.34 examples/s][A
Generating train examples...:  45%|████▍     | 157329/350343 [01:07<01:24, 2294.12 examples/s][A
Generating train examples...:  45%|████▍     | 157559/350343 [01:07<01:25, 2249.44 examples/s][A
Generating train examples...:  45%|████▌     | 157785/350343 [01:07<01:27, 2201.60 examples/s][A
Generating train examples...:  45%|████▌     | 158006/350343 [01:08<01:27, 2202.12 examples/s][A
Generating train examples...:  45%|████▌     | 158237/350343 [01:08<01:26, 2231.65 examples/s][A
Generating train examples...:  45%|████▌     | 158472/350343 [01:08<01:24, 2265.64 examples/s][A
Generating train examples...:  45%|████▌     | 158708/350343 [01:08<01:23, 2291.84 examples/s][A
Generating train examples...:  45%|████▌     | 158944/350343 [01:08<01:22, 2310.77 examples/s][A
Generating train examples...:  45%|████▌     | 159176/350343 [01:08<01:22, 2309.72 examples/s][A
Generating train examples...:  46%|████▌     | 159409/350343 [01:08<01:22, 2315.45 examples/s][A
Generating train examples...:  46%|████▌     | 159647/350343 [01:08<01:21, 2332.25 examples/s][A
Generating train examples...:  46%|████▌     | 159884/350343 [01:08<01:21, 2343.32 examples/s][A
Generating train examples...:  46%|████▌     | 160120/350343 [01:08<01:21, 2347.37 examples/s][A
Generating train examples...:  46%|████▌     | 160359/350343 [01:09<01:23, 2269.17 examples/s][A
Generating train examples...:  46%|████▌     | 160597/350343 [01:09<01:22, 2299.57 examples/s][A
Generating train examples...:  46%|████▌     | 160829/350343 [01:09<01:22, 2304.24 examples/s][A
Generating train examples...:  46%|████▌     | 161065/350343 [01:09<01:21, 2319.22 examples/s][A
Generating train examples...:  46%|████▌     | 161300/350343 [01:09<01:21, 2327.77 examples/s][A
Generating train examples...:  46%|████▌     | 161533/350343 [01:09<01:21, 2323.37 examples/s][A
Generating train examples...:  46%|████▌     | 161768/350343 [01:09<01:20, 2330.45 examples/s][A
Generating train examples...:  46%|████▌     | 162005/350343 [01:09<01:20, 2339.87 examples/s][A
Generating train examples...:  46%|████▋     | 162241/350343 [01:09<01:20, 2345.60 examples/s][A
Generating train examples...:  46%|████▋     | 162481/350343 [01:09<01:19, 2360.10 examples/s][A
Generating train examples...:  46%|████▋     | 162718/350343 [01:10<01:19, 2359.14 examples/s][A
Generating train examples...:  47%|████▋     | 162960/350343 [01:10<01:18, 2376.94 examples/s][A
Generating train examples...:  47%|████▋     | 163198/350343 [01:10<01:18, 2372.89 examples/s][A
Generating train examples...:  47%|████▋     | 163436/350343 [01:10<01:19, 2361.40 examples/s][A
Generating train examples...:  47%|████▋     | 163673/350343 [01:10<01:19, 2343.75 examples/s][A
Generating train examples...:  47%|████▋     | 163911/350343 [01:10<01:19, 2354.42 examples/s][A
Generating train examples...:  47%|████▋     | 164147/350343 [01:10<01:19, 2354.88 examples/s][A
Generating train examples...:  47%|████▋     | 164384/350343 [01:10<01:18, 2358.42 examples/s][A
Generating train examples...:  47%|████▋     | 164620/350343 [01:10<01:21, 2275.66 examples/s][A
Generating train examples...:  47%|████▋     | 164857/350343 [01:11<01:20, 2302.30 examples/s][A
Generating train examples...:  47%|████▋     | 165088/350343 [01:11<01:20, 2297.03 examples/s][A
Generating train examples...:  47%|████▋     | 165319/350343 [01:11<01:20, 2296.54 examples/s][A
Generating train examples...:  47%|████▋     | 165552/350343 [01:11<01:20, 2305.19 examples/s][A
Generating train examples...:  47%|████▋     | 165783/350343 [01:11<01:20, 2300.96 examples/s][A
Generating train examples...:  47%|████▋     | 166016/350343 [01:11<01:19, 2309.37 examples/s][A
Generating train examples...:  47%|████▋     | 166249/350343 [01:11<01:19, 2312.78 examples/s][A
Generating train examples...:  48%|████▊     | 166489/350343 [01:11<01:18, 2337.49 examples/s][A
Generating train examples...:  48%|████▊     | 166730/350343 [01:11<01:17, 2359.01 examples/s][A
Generating train examples...:  48%|████▊     | 166972/350343 [01:11<01:17, 2375.74 examples/s][A
Generating train examples...:  48%|████▊     | 167210/350343 [01:12<01:17, 2360.97 examples/s][A
Generating train examples...:  48%|████▊     | 167447/350343 [01:12<01:17, 2363.57 examples/s][A
Generating train examples...:  48%|████▊     | 167687/350343 [01:12<01:16, 2372.29 examples/s][A
Generating train examples...:  48%|████▊     | 167925/350343 [01:12<01:17, 2350.69 examples/s][A
Generating train examples...:  48%|████▊     | 168161/350343 [01:12<01:17, 2341.59 examples/s][A
Generating train examples...:  48%|████▊     | 168396/350343 [01:12<01:17, 2342.86 examples/s][A
Generating train examples...:  48%|████▊     | 168631/350343 [01:12<01:17, 2333.28 examples/s][A
Generating train examples...:  48%|████▊     | 168865/350343 [01:12<01:20, 2252.40 examples/s][A
Generating train examples...:  48%|████▊     | 169095/350343 [01:12<01:20, 2264.24 examples/s][A
Generating train examples...:  48%|████▊     | 169325/350343 [01:12<01:19, 2272.85 examples/s][A
Generating train examples...:  48%|████▊     | 169557/350343 [01:13<01:19, 2286.63 examples/s][A
Generating train examples...:  48%|████▊     | 169794/350343 [01:13<01:18, 2311.04 examples/s][A
Generating train examples...:  49%|████▊     | 170036/350343 [01:13<01:16, 2343.15 examples/s][A
Generating train examples...:  49%|████▊     | 170271/350343 [01:13<01:17, 2338.46 examples/s][A
Generating train examples...:  49%|████▊     | 170505/350343 [01:13<01:17, 2318.13 examples/s][A
Generating train examples...:  49%|████▊     | 170737/350343 [01:13<01:19, 2255.38 examples/s][A
Generating train examples...:  49%|████▉     | 170963/350343 [01:13<01:19, 2253.68 examples/s][A
Generating train examples...:  49%|████▉     | 171192/350343 [01:13<01:19, 2263.91 examples/s][A
Generating train examples...:  49%|████▉     | 171420/350343 [01:13<01:18, 2266.84 examples/s][A
Generating train examples...:  49%|████▉     | 171647/350343 [01:13<01:18, 2262.93 examples/s][A
Generating train examples...:  49%|████▉     | 171874/350343 [01:14<01:18, 2262.58 examples/s][A
Generating train examples...:  49%|████▉     | 172101/350343 [01:14<01:19, 2254.75 examples/s][A
Generating train examples...:  49%|████▉     | 172327/350343 [01:14<01:18, 2254.25 examples/s][A
Generating train examples...:  49%|████▉     | 172553/350343 [01:14<01:19, 2245.84 examples/s][A
Generating train examples...:  49%|████▉     | 172783/350343 [01:14<01:18, 2259.47 examples/s][A
Generating train examples...:  49%|████▉     | 173020/350343 [01:14<01:17, 2289.91 examples/s][A
Generating train examples...:  49%|████▉     | 173252/350343 [01:14<01:17, 2298.12 examples/s][A
Generating train examples...:  50%|████▉     | 173492/350343 [01:14<01:16, 2326.48 examples/s][A
Generating train examples...:  50%|████▉     | 173733/350343 [01:14<01:15, 2349.64 examples/s][A
Generating train examples...:  50%|████▉     | 173972/350343 [01:14<01:14, 2359.99 examples/s][A
Generating train examples...:  50%|████▉     | 174209/350343 [01:15<01:14, 2361.83 examples/s][A
Generating train examples...:  50%|████▉     | 174446/350343 [01:15<01:14, 2357.03 examples/s][A
Generating train examples...:  50%|████▉     | 174682/350343 [01:15<01:17, 2271.61 examples/s][A
Generating train examples...:  50%|████▉     | 174910/350343 [01:15<01:17, 2272.93 examples/s][A
Generating train examples...:  50%|████▉     | 175152/350343 [01:15<01:15, 2315.38 examples/s][A
Generating train examples...:  50%|█████     | 175387/350343 [01:15<01:15, 2325.37 examples/s][A
Generating train examples...:  50%|█████     | 175624/350343 [01:15<01:14, 2336.51 examples/s][A
Generating train examples...:  50%|█████     | 175861/350343 [01:15<01:14, 2345.98 examples/s][A
Generating train examples...:  50%|█████     | 176097/350343 [01:15<01:14, 2349.23 examples/s][A
Generating train examples...:  50%|█████     | 176333/350343 [01:15<01:14, 2339.87 examples/s][A
Generating train examples...:  50%|█████     | 176568/350343 [01:16<01:14, 2335.74 examples/s][A
Generating train examples...:  50%|█████     | 176805/350343 [01:16<01:13, 2345.24 examples/s][A
Generating train examples...:  51%|█████     | 177040/350343 [01:16<01:13, 2343.05 examples/s][A
Generating train examples...:  51%|█████     | 177275/350343 [01:16<01:14, 2336.55 examples/s][A
Generating train examples...:  51%|█████     | 177509/350343 [01:16<01:13, 2336.43 examples/s][A
Generating train examples...:  51%|█████     | 177753/350343 [01:16<01:12, 2364.55 examples/s][A
Generating train examples...:  51%|█████     | 177991/350343 [01:16<01:12, 2367.91 examples/s][A
Generating train examples...:  51%|█████     | 178233/350343 [01:16<01:12, 2382.65 examples/s][A
Generating train examples...:  51%|█████     | 178472/350343 [01:16<01:12, 2367.59 examples/s][A
Generating train examples...:  51%|█████     | 178709/350343 [01:16<01:15, 2287.86 examples/s][A
Generating train examples...:  51%|█████     | 178941/350343 [01:17<01:14, 2297.19 examples/s][A
Generating train examples...:  51%|█████     | 179180/350343 [01:17<01:13, 2322.76 examples/s][A
Generating train examples...:  51%|█████     | 179419/350343 [01:17<01:13, 2340.64 examples/s][A
Generating train examples...:  51%|█████▏    | 179654/350343 [01:17<01:13, 2332.46 examples/s][A
Generating train examples...:  51%|█████▏    | 179888/350343 [01:17<01:13, 2331.51 examples/s][A
Generating train examples...:  51%|█████▏    | 180122/350343 [01:17<01:13, 2330.64 examples/s][A
Generating train examples...:  51%|█████▏    | 180357/350343 [01:17<01:12, 2334.44 examples/s][A
Generating train examples...:  52%|█████▏    | 180599/350343 [01:17<01:11, 2359.94 examples/s][A
Generating train examples...:  52%|█████▏    | 180836/350343 [01:17<01:11, 2356.21 examples/s][A
Generating train examples...:  52%|█████▏    | 181072/350343 [01:18<01:12, 2349.25 examples/s][A
Generating train examples...:  52%|█████▏    | 181309/350343 [01:18<01:11, 2354.18 examples/s][A
Generating train examples...:  52%|█████▏    | 181547/350343 [01:18<01:11, 2361.30 examples/s][A
Generating train examples...:  52%|█████▏    | 181784/350343 [01:18<01:11, 2363.39 examples/s][A
Generating train examples...:  52%|█████▏    | 182021/350343 [01:18<01:11, 2361.07 examples/s][A
Generating train examples...:  52%|█████▏    | 182262/350343 [01:18<01:10, 2374.93 examples/s][A
Generating train examples...:  52%|█████▏    | 182500/350343 [01:18<01:10, 2370.68 examples/s][A
Generating train examples...:  52%|█████▏    | 182738/350343 [01:18<01:13, 2284.31 examples/s][A
Generating train examples...:  52%|█████▏    | 182968/350343 [01:18<01:13, 2287.94 examples/s][A
Generating train examples...:  52%|█████▏    | 183204/350343 [01:18<01:12, 2307.99 examples/s][A
Generating train examples...:  52%|█████▏    | 183437/350343 [01:19<01:12, 2311.85 examples/s][A
Generating train examples...:  52%|█████▏    | 183671/350343 [01:19<01:11, 2318.85 examples/s][A
Generating train examples...:  52%|█████▏    | 183909/350343 [01:19<01:11, 2335.05 examples/s][A
Generating train examples...:  53%|█████▎    | 184150/350343 [01:19<01:10, 2356.76 examples/s][A
Generating train examples...:  53%|█████▎    | 184386/350343 [01:19<01:10, 2350.97 examples/s][A
Generating train examples...:  53%|█████▎    | 184622/350343 [01:19<01:11, 2328.69 examples/s][A
Generating train examples...:  53%|█████▎    | 184855/350343 [01:19<01:11, 2309.20 examples/s][A
Generating train examples...:  53%|█████▎    | 185096/350343 [01:19<01:10, 2336.93 examples/s][A
Generating train examples...:  53%|█████▎    | 185330/350343 [01:19<01:10, 2329.33 examples/s][A
Generating train examples...:  53%|█████▎    | 185564/350343 [01:19<01:10, 2331.10 examples/s][A
Generating train examples...:  53%|█████▎    | 185798/350343 [01:20<01:10, 2330.13 examples/s][A
Generating train examples...:  53%|█████▎    | 186032/350343 [01:20<01:10, 2331.05 examples/s][A
Generating train examples...:  53%|█████▎    | 186269/350343 [01:20<01:10, 2341.23 examples/s][A
Generating train examples...:  53%|█████▎    | 186504/350343 [01:20<01:10, 2338.92 examples/s][A
Generating train examples...:  53%|█████▎    | 186741/350343 [01:20<01:09, 2346.85 examples/s][A
Generating train examples...:  53%|█████▎    | 186976/350343 [01:20<01:11, 2273.13 examples/s][A
Generating train examples...:  53%|█████▎    | 187222/350343 [01:20<01:10, 2325.27 examples/s][A
Generating train examples...:  54%|█████▎    | 187459/350343 [01:20<01:09, 2336.06 examples/s][A
Generating train examples...:  54%|█████▎    | 187697/350343 [01:20<01:09, 2347.74 examples/s][A
Generating train examples...:  54%|█████▎    | 187937/350343 [01:20<01:08, 2361.72 examples/s][A
Generating train examples...:  54%|█████▎    | 188174/350343 [01:21<01:08, 2362.38 examples/s][A
Generating train examples...:  54%|█████▍    | 188411/350343 [01:21<01:08, 2356.31 examples/s][A
Generating train examples...:  54%|█████▍    | 188649/350343 [01:21<01:08, 2361.84 examples/s][A
Generating train examples...:  54%|█████▍    | 188886/350343 [01:21<01:08, 2361.31 examples/s][A
Generating train examples...:  54%|█████▍    | 189125/350343 [01:21<01:08, 2367.50 examples/s][A
Generating train examples...:  54%|█████▍    | 189363/350343 [01:21<01:07, 2369.43 examples/s][A
Generating train examples...:  54%|█████▍    | 189600/350343 [01:21<01:07, 2369.46 examples/s][A
Generating train examples...:  54%|█████▍    | 189837/350343 [01:21<01:09, 2317.32 examples/s][A
Generating train examples...:  54%|█████▍    | 190078/350343 [01:21<01:08, 2343.15 examples/s][A
Generating train examples...:  54%|█████▍    | 190317/350343 [01:21<01:07, 2355.99 examples/s][A
Generating train examples...:  54%|█████▍    | 190554/350343 [01:22<01:09, 2286.52 examples/s][A
Generating train examples...:  54%|█████▍    | 190793/350343 [01:22<01:08, 2315.74 examples/s][A
Generating train examples...:  55%|█████▍    | 191031/350343 [01:22<01:08, 2333.68 examples/s][A
Generating train examples...:  55%|█████▍    | 191268/350343 [01:22<01:07, 2343.42 examples/s][A
Generating train examples...:  55%|█████▍    | 191503/350343 [01:22<01:07, 2343.60 examples/s][A
Generating train examples...:  55%|█████▍    | 191738/350343 [01:22<01:07, 2342.41 examples/s][A
Generating train examples...:  55%|█████▍    | 191977/350343 [01:22<01:07, 2354.88 examples/s][A
Generating train examples...:  55%|█████▍    | 192213/350343 [01:22<01:07, 2347.27 examples/s][A
Generating train examples...:  55%|█████▍    | 192453/350343 [01:22<01:06, 2362.53 examples/s][A
Generating train examples...:  55%|█████▌    | 192690/350343 [01:22<01:07, 2351.01 examples/s][A
Generating train examples...:  55%|█████▌    | 192926/350343 [01:23<01:07, 2343.13 examples/s][A
Generating train examples...:  55%|█████▌    | 193164/350343 [01:23<01:06, 2351.93 examples/s][A
Generating train examples...:  55%|█████▌    | 193406/350343 [01:23<01:06, 2371.62 examples/s][A
Generating train examples...:  55%|█████▌    | 193644/350343 [01:23<01:06, 2360.99 examples/s][A
Generating train examples...:  55%|█████▌    | 193881/350343 [01:23<01:06, 2346.53 examples/s][A
Generating train examples...:  55%|█████▌    | 194116/350343 [01:23<01:06, 2339.92 examples/s][A
Generating train examples...:  55%|█████▌    | 194351/350343 [01:23<01:06, 2341.97 examples/s][A
Generating train examples...:  56%|█████▌    | 194586/350343 [01:23<01:09, 2249.13 examples/s][A
Generating train examples...:  56%|█████▌    | 194819/350343 [01:23<01:08, 2270.94 examples/s][A
Generating train examples...:  56%|█████▌    | 195052/350343 [01:23<01:07, 2287.56 examples/s][A
Generating train examples...:  56%|█████▌    | 195288/350343 [01:24<01:07, 2307.58 examples/s][A
Generating train examples...:  56%|█████▌    | 195527/350343 [01:24<01:06, 2330.72 examples/s][A
Generating train examples...:  56%|█████▌    | 195763/350343 [01:24<01:06, 2338.12 examples/s][A
Generating train examples...:  56%|█████▌    | 195998/350343 [01:24<01:05, 2341.14 examples/s][A
Generating train examples...:  56%|█████▌    | 196235/350343 [01:24<01:05, 2348.28 examples/s][A
Generating train examples...:  56%|█████▌    | 196470/350343 [01:24<01:05, 2334.68 examples/s][A
Generating train examples...:  56%|█████▌    | 196706/350343 [01:24<01:05, 2340.08 examples/s][A
Generating train examples...:  56%|█████▌    | 196941/350343 [01:24<01:05, 2341.27 examples/s][A
Generating train examples...:  56%|█████▋    | 197179/350343 [01:24<01:05, 2352.51 examples/s][A
Generating train examples...:  56%|█████▋    | 197418/350343 [01:24<01:04, 2363.28 examples/s][A
Generating train examples...:  56%|█████▋    | 197655/350343 [01:25<01:04, 2362.29 examples/s][A
Generating train examples...:  56%|█████▋    | 197892/350343 [01:25<01:05, 2335.51 examples/s][A
Generating train examples...:  57%|█████▋    | 198126/350343 [01:25<01:06, 2281.51 examples/s][A
Generating train examples...:  57%|█████▋    | 198361/350343 [01:25<01:06, 2299.23 examples/s][A
Generating train examples...:  57%|█████▋    | 198594/350343 [01:25<01:05, 2306.24 examples/s][A
Generating train examples...:  57%|█████▋    | 198835/350343 [01:25<01:04, 2336.08 examples/s][A
Generating train examples...:  57%|█████▋    | 199069/350343 [01:25<01:07, 2235.65 examples/s][A
Generating train examples...:  57%|█████▋    | 199301/350343 [01:25<01:06, 2258.73 examples/s][A
Generating train examples...:  57%|█████▋    | 199532/350343 [01:25<01:06, 2272.11 examples/s][A
Generating train examples...:  57%|█████▋    | 199766/350343 [01:26<01:05, 2290.92 examples/s][A
Generating train examples...:  57%|█████▋    | 199999/350343 [01:26<01:05, 2302.38 examples/s][A
Generating train examples...:  57%|█████▋    | 200235/350343 [01:26<01:04, 2319.32 examples/s][A
Generating train examples...:  57%|█████▋    | 200472/350343 [01:26<01:04, 2333.37 examples/s][A
Generating train examples...:  57%|█████▋    | 200713/350343 [01:26<01:03, 2355.76 examples/s][A
Generating train examples...:  57%|█████▋    | 200949/350343 [01:26<01:03, 2356.44 examples/s][A
Generating train examples...:  57%|█████▋    | 201185/350343 [01:26<01:03, 2352.47 examples/s][A
Generating train examples...:  57%|█████▋    | 201421/350343 [01:26<01:03, 2349.29 examples/s][A
Generating train examples...:  58%|█████▊    | 201659/350343 [01:26<01:03, 2358.32 examples/s][A
Generating train examples...:  58%|█████▊    | 201897/350343 [01:26<01:02, 2364.59 examples/s][A
Generating train examples...:  58%|█████▊    | 202134/350343 [01:27<01:03, 2351.53 examples/s][A
Generating train examples...:  58%|█████▊    | 202370/350343 [01:27<01:03, 2341.83 examples/s][A
Generating train examples...:  58%|█████▊    | 202610/350343 [01:27<01:02, 2357.67 examples/s][A
Generating train examples...:  58%|█████▊    | 202848/350343 [01:27<01:02, 2363.46 examples/s][A
Generating train examples...:  58%|█████▊    | 203085/350343 [01:27<01:04, 2267.99 examples/s][A
Generating train examples...:  58%|█████▊    | 203323/350343 [01:27<01:03, 2298.07 examples/s][A
Generating train examples...:  58%|█████▊    | 203554/350343 [01:27<01:03, 2300.06 examples/s][A
Generating train examples...:  58%|█████▊    | 203793/350343 [01:27<01:03, 2325.21 examples/s][A
Generating train examples...:  58%|█████▊    | 204026/350343 [01:27<01:02, 2322.80 examples/s][A
Generating train examples...:  58%|█████▊    | 204259/350343 [01:27<01:03, 2315.22 examples/s][A
Generating train examples...:  58%|█████▊    | 204493/350343 [01:28<01:02, 2321.22 examples/s][A
Generating train examples...:  58%|█████▊    | 204727/350343 [01:28<01:02, 2325.46 examples/s][A
Generating train examples...:  59%|█████▊    | 204962/350343 [01:28<01:02, 2332.25 examples/s][A
Generating train examples...:  59%|█████▊    | 205196/350343 [01:28<01:02, 2320.21 examples/s][A
Generating train examples...:  59%|█████▊    | 205435/350343 [01:28<01:01, 2340.79 examples/s][A
Generating train examples...:  59%|█████▊    | 205670/350343 [01:28<01:01, 2341.58 examples/s][A
Generating train examples...:  59%|█████▉    | 205905/350343 [01:28<01:01, 2339.27 examples/s][A
Generating train examples...:  59%|█████▉    | 206139/350343 [01:28<01:01, 2334.85 examples/s][A
Generating train examples...:  59%|█████▉    | 206378/350343 [01:28<01:01, 2350.04 examples/s][A
Generating train examples...:  59%|█████▉    | 206617/350343 [01:28<01:00, 2360.72 examples/s][A
Generating train examples...:  59%|█████▉    | 206854/350343 [01:29<01:00, 2356.65 examples/s][A
Generating train examples...:  59%|█████▉    | 207094/350343 [01:29<01:00, 2368.75 examples/s][A
Generating train examples...:  59%|█████▉    | 207331/350343 [01:29<01:02, 2286.22 examples/s][A
Generating train examples...:  59%|█████▉    | 207562/350343 [01:29<01:02, 2292.38 examples/s][A
Generating train examples...:  59%|█████▉    | 207800/350343 [01:29<01:01, 2316.73 examples/s][A
Generating train examples...:  59%|█████▉    | 208033/350343 [01:29<01:01, 2318.77 examples/s][A
Generating train examples...:  59%|█████▉    | 208270/350343 [01:29<01:00, 2333.54 examples/s][A
Generating train examples...:  60%|█████▉    | 208511/350343 [01:29<01:00, 2356.13 examples/s][A
Generating train examples...:  60%|█████▉    | 208751/350343 [01:29<00:59, 2367.27 examples/s][A
Generating train examples...:  60%|█████▉    | 208988/350343 [01:29<01:00, 2350.86 examples/s][A
Generating train examples...:  60%|█████▉    | 209224/350343 [01:30<01:00, 2344.51 examples/s][A
Generating train examples...:  60%|█████▉    | 209461/350343 [01:30<00:59, 2350.89 examples/s][A
Generating train examples...:  60%|█████▉    | 209699/350343 [01:30<00:59, 2357.51 examples/s][A
Generating train examples...:  60%|█████▉    | 209935/350343 [01:30<00:59, 2343.94 examples/s][A
Generating train examples...:  60%|█████▉    | 210173/350343 [01:30<00:59, 2352.02 examples/s][A
Generating train examples...:  60%|██████    | 210416/350343 [01:30<00:58, 2372.60 examples/s][A
Generating train examples...:  60%|██████    | 210654/350343 [01:30<00:59, 2358.31 examples/s][A
Generating train examples...:  60%|██████    | 210890/350343 [01:30<00:59, 2352.85 examples/s][A
Generating train examples...:  60%|██████    | 211130/350343 [01:30<01:00, 2283.02 examples/s][A
Generating train examples...:  60%|██████    | 211362/350343 [01:30<01:00, 2293.41 examples/s][A
Generating train examples...:  60%|██████    | 211597/350343 [01:31<01:00, 2309.93 examples/s][A
Generating train examples...:  60%|██████    | 211835/350343 [01:31<00:59, 2330.30 examples/s][A
Generating train examples...:  61%|██████    | 212073/350343 [01:31<00:58, 2344.00 examples/s][A
Generating train examples...:  61%|██████    | 212308/350343 [01:31<00:59, 2336.95 examples/s][A
Generating train examples...:  61%|██████    | 212542/350343 [01:31<00:59, 2334.33 examples/s][A
Generating train examples...:  61%|██████    | 212776/350343 [01:31<00:58, 2334.90 examples/s][A
Generating train examples...:  61%|██████    | 213015/350343 [01:31<00:58, 2349.31 examples/s][A
Generating train examples...:  61%|██████    | 213251/350343 [01:31<00:58, 2352.00 examples/s][A
Generating train examples...:  61%|██████    | 213487/350343 [01:31<00:58, 2349.70 examples/s][A
Generating train examples...:  61%|██████    | 213732/350343 [01:31<00:57, 2378.06 examples/s][A
Generating train examples...:  61%|██████    | 213971/350343 [01:32<00:57, 2379.20 examples/s][A
Generating train examples...:  61%|██████    | 214213/350343 [01:32<00:56, 2391.06 examples/s][A
Generating train examples...:  61%|██████    | 214453/350343 [01:32<00:57, 2364.41 examples/s][A
Generating train examples...:  61%|██████▏   | 214690/350343 [01:32<00:57, 2346.05 examples/s][A
Generating train examples...:  61%|██████▏   | 214927/350343 [01:32<00:57, 2352.27 examples/s][A
Generating train examples...:  61%|██████▏   | 215163/350343 [01:32<00:59, 2262.67 examples/s][A
Generating train examples...:  61%|██████▏   | 215394/350343 [01:32<00:59, 2274.63 examples/s][A
Generating train examples...:  62%|██████▏   | 215628/350343 [01:32<00:58, 2292.46 examples/s][A
Generating train examples...:  62%|██████▏   | 215869/350343 [01:32<00:57, 2325.84 examples/s][A
Generating train examples...:  62%|██████▏   | 216102/350343 [01:33<00:57, 2323.75 examples/s][A
Generating train examples...:  62%|██████▏   | 216335/350343 [01:33<00:57, 2315.04 examples/s][A
Generating train examples...:  62%|██████▏   | 216571/350343 [01:33<00:57, 2326.67 examples/s][A
Generating train examples...:  62%|██████▏   | 216807/350343 [01:33<00:57, 2336.27 examples/s][A
Generating train examples...:  62%|██████▏   | 217041/350343 [01:33<00:57, 2330.75 examples/s][A
Generating train examples...:  62%|██████▏   | 217280/350343 [01:33<00:56, 2347.67 examples/s][A
Generating train examples...:  62%|██████▏   | 217517/350343 [01:33<00:56, 2352.60 examples/s][A
Generating train examples...:  62%|██████▏   | 217757/350343 [01:33<00:56, 2364.31 examples/s][A
Generating train examples...:  62%|██████▏   | 217994/350343 [01:33<00:56, 2359.77 examples/s][A
Generating train examples...:  62%|██████▏   | 218230/350343 [01:33<00:55, 2359.47 examples/s][A
Generating train examples...:  62%|██████▏   | 218466/350343 [01:34<00:56, 2348.47 examples/s][A
Generating train examples...:  62%|██████▏   | 218701/350343 [01:34<00:56, 2339.22 examples/s][A
Generating train examples...:  62%|██████▏   | 218938/350343 [01:34<00:55, 2346.70 examples/s][A
Generating train examples...:  63%|██████▎   | 219176/350343 [01:34<00:55, 2355.77 examples/s][A
Generating train examples...:  63%|██████▎   | 219412/350343 [01:34<00:57, 2262.17 examples/s][A
Generating train examples...:  63%|██████▎   | 219649/350343 [01:34<00:57, 2292.50 examples/s][A
Generating train examples...:  63%|██████▎   | 219887/350343 [01:34<00:56, 2317.58 examples/s][A
Generating train examples...:  63%|██████▎   | 220132/350343 [01:34<00:55, 2354.43 examples/s][A
Generating train examples...:  63%|██████▎   | 220368/350343 [01:34<00:55, 2353.36 examples/s][A
Generating train examples...:  63%|██████▎   | 220604/350343 [01:34<00:55, 2350.50 examples/s][A
Generating train examples...:  63%|██████▎   | 220844/350343 [01:35<00:54, 2364.17 examples/s][A
Generating train examples...:  63%|██████▎   | 221081/350343 [01:35<00:54, 2354.85 examples/s][A
Generating train examples...:  63%|██████▎   | 221320/350343 [01:35<00:54, 2364.36 examples/s][A
Generating train examples...:  63%|██████▎   | 221557/350343 [01:35<00:54, 2353.41 examples/s][A
Generating train examples...:  63%|██████▎   | 221793/350343 [01:35<00:54, 2351.08 examples/s][A
Generating train examples...:  63%|██████▎   | 222035/350343 [01:35<00:54, 2369.33 examples/s][A
Generating train examples...:  63%|██████▎   | 222274/350343 [01:35<00:53, 2375.40 examples/s][A
Generating train examples...:  64%|██████▎   | 222512/350343 [01:35<00:54, 2355.05 examples/s][A
Generating train examples...:  64%|██████▎   | 222749/350343 [01:35<00:54, 2358.91 examples/s][A
Generating train examples...:  64%|██████▎   | 222987/350343 [01:35<00:53, 2362.90 examples/s][A
Generating train examples...:  64%|██████▎   | 223224/350343 [01:36<00:56, 2262.84 examples/s][A
Generating train examples...:  64%|██████▍   | 223461/350343 [01:36<00:55, 2293.68 examples/s][A
Generating train examples...:  64%|██████▍   | 223700/350343 [01:36<00:54, 2319.87 examples/s][A
Generating train examples...:  64%|██████▍   | 223935/350343 [01:36<00:54, 2328.61 examples/s][A
Generating train examples...:  64%|██████▍   | 224172/350343 [01:36<00:53, 2340.25 examples/s][A
Generating train examples...:  64%|██████▍   | 224407/350343 [01:36<00:53, 2339.27 examples/s][A
Generating train examples...:  64%|██████▍   | 224647/350343 [01:36<00:53, 2356.07 examples/s][A
Generating train examples...:  64%|██████▍   | 224886/350343 [01:36<00:53, 2363.61 examples/s][A
Generating train examples...:  64%|██████▍   | 225125/350343 [01:36<00:52, 2370.43 examples/s][A
Generating train examples...:  64%|██████▍   | 225364/350343 [01:36<00:52, 2375.28 examples/s][A
Generating train examples...:  64%|██████▍   | 225602/350343 [01:37<00:52, 2368.35 examples/s][A
Generating train examples...:  64%|██████▍   | 225840/350343 [01:37<00:52, 2369.92 examples/s][A
Generating train examples...:  65%|██████▍   | 226078/350343 [01:37<00:52, 2363.32 examples/s][A
Generating train examples...:  65%|██████▍   | 226315/350343 [01:37<00:52, 2353.12 examples/s][A
Generating train examples...:  65%|██████▍   | 226551/350343 [01:37<00:52, 2346.77 examples/s][A
Generating train examples...:  65%|██████▍   | 226788/350343 [01:37<00:52, 2352.13 examples/s][A
Generating train examples...:  65%|██████▍   | 227024/350343 [01:37<00:54, 2268.66 examples/s][A
Generating train examples...:  65%|██████▍   | 227256/350343 [01:37<00:53, 2283.42 examples/s][A
Generating train examples...:  65%|██████▍   | 227492/350343 [01:37<00:53, 2305.46 examples/s][A
Generating train examples...:  65%|██████▌   | 227723/350343 [01:37<00:53, 2305.60 examples/s][A
Generating train examples...:  65%|██████▌   | 227967/350343 [01:38<00:52, 2345.39 examples/s][A
Generating train examples...:  65%|██████▌   | 228203/350343 [01:38<00:52, 2348.20 examples/s][A
Generating train examples...:  65%|██████▌   | 228438/350343 [01:38<00:51, 2344.35 examples/s][A
Generating train examples...:  65%|██████▌   | 228673/350343 [01:38<00:51, 2342.86 examples/s][A
Generating train examples...:  65%|██████▌   | 228913/350343 [01:38<00:51, 2359.54 examples/s][A
Generating train examples...:  65%|██████▌   | 229150/350343 [01:38<00:51, 2353.27 examples/s][A
Generating train examples...:  65%|██████▌   | 229386/350343 [01:38<00:51, 2329.54 examples/s][A
Generating train examples...:  66%|██████▌   | 229624/350343 [01:38<00:51, 2343.16 examples/s][A
Generating train examples...:  66%|██████▌   | 229863/350343 [01:38<00:51, 2355.45 examples/s][A
Generating train examples...:  66%|██████▌   | 230099/350343 [01:38<00:51, 2353.80 examples/s][A
Generating train examples...:  66%|██████▌   | 230335/350343 [01:39<00:51, 2341.26 examples/s][A
Generating train examples...:  66%|██████▌   | 230570/350343 [01:39<00:51, 2341.23 examples/s][A
Generating train examples...:  66%|██████▌   | 230805/350343 [01:39<00:51, 2338.14 examples/s][A
Generating train examples...:  66%|██████▌   | 231040/350343 [01:39<00:52, 2259.38 examples/s][A
Generating train examples...:  66%|██████▌   | 231278/350343 [01:39<00:51, 2292.36 examples/s][A
Generating train examples...:  66%|██████▌   | 231516/350343 [01:39<00:51, 2317.26 examples/s][A
Generating train examples...:  66%|██████▌   | 231755/350343 [01:39<00:50, 2338.33 examples/s][A
Generating train examples...:  66%|██████▌   | 231991/350343 [01:39<00:50, 2344.48 examples/s][A
Generating train examples...:  66%|██████▋   | 232228/350343 [01:39<00:50, 2351.73 examples/s][A
Generating train examples...:  66%|██████▋   | 232465/350343 [01:40<00:50, 2356.70 examples/s][A
Generating train examples...:  66%|██████▋   | 232701/350343 [01:40<00:50, 2337.26 examples/s][A
Generating train examples...:  66%|██████▋   | 232936/350343 [01:40<00:50, 2341.00 examples/s][A
Generating train examples...:  67%|██████▋   | 233175/350343 [01:40<00:49, 2353.14 examples/s][A
Generating train examples...:  67%|██████▋   | 233411/350343 [01:40<00:49, 2352.00 examples/s][A
Generating train examples...:  67%|██████▋   | 233647/350343 [01:40<00:49, 2342.49 examples/s][A
Generating train examples...:  67%|██████▋   | 233886/350343 [01:40<00:49, 2355.17 examples/s][A
Generating train examples...:  67%|██████▋   | 234123/350343 [01:40<00:49, 2359.04 examples/s][A
Generating train examples...:  67%|██████▋   | 234359/350343 [01:40<00:49, 2352.47 examples/s][A
Generating train examples...:  67%|██████▋   | 234599/350343 [01:40<00:48, 2366.61 examples/s][A
Generating train examples...:  67%|██████▋   | 234838/350343 [01:41<00:48, 2372.73 examples/s][A
Generating train examples...:  67%|██████▋   | 235076/350343 [01:41<00:50, 2282.36 examples/s][A
Generating train examples...:  67%|██████▋   | 235305/350343 [01:41<00:50, 2282.70 examples/s][A
Generating train examples...:  67%|██████▋   | 235543/350343 [01:41<00:49, 2310.38 examples/s][A
Generating train examples...:  67%|██████▋   | 235784/350343 [01:41<00:48, 2338.48 examples/s][A
Generating train examples...:  67%|██████▋   | 236019/350343 [01:41<00:48, 2341.34 examples/s][A
Generating train examples...:  67%|██████▋   | 236254/350343 [01:41<00:48, 2338.51 examples/s][A
Generating train examples...:  68%|██████▊   | 236489/350343 [01:41<00:48, 2341.85 examples/s][A
Generating train examples...:  68%|██████▊   | 236724/350343 [01:41<00:48, 2329.97 examples/s][A
Generating train examples...:  68%|██████▊   | 236958/350343 [01:41<00:48, 2320.73 examples/s][A
Generating train examples...:  68%|██████▊   | 237192/350343 [01:42<00:48, 2325.09 examples/s][A
Generating train examples...:  68%|██████▊   | 237429/350343 [01:42<00:48, 2335.69 examples/s][A
Generating train examples...:  68%|██████▊   | 237666/350343 [01:42<00:48, 2345.88 examples/s][A
Generating train examples...:  68%|██████▊   | 237905/350343 [01:42<00:47, 2358.68 examples/s][A
Generating train examples...:  68%|██████▊   | 238141/350343 [01:42<00:47, 2357.46 examples/s][A
Generating train examples...:  68%|██████▊   | 238377/350343 [01:42<00:47, 2338.98 examples/s][A
Generating train examples...:  68%|██████▊   | 238611/350343 [01:42<00:47, 2337.29 examples/s][A
Generating train examples...:  68%|██████▊   | 238845/350343 [01:42<00:47, 2333.01 examples/s][A
Generating train examples...:  68%|██████▊   | 239079/350343 [01:42<00:47, 2333.32 examples/s][A
Generating train examples...:  68%|██████▊   | 239313/350343 [01:42<00:47, 2332.89 examples/s][A
Generating train examples...:  68%|██████▊   | 239547/350343 [01:43<00:48, 2263.24 examples/s][A
Generating train examples...:  68%|██████▊   | 239778/350343 [01:43<00:48, 2275.10 examples/s][A
Generating train examples...:  69%|██████▊   | 240014/350343 [01:43<00:47, 2299.26 examples/s][A
Generating train examples...:  69%|██████▊   | 240246/350343 [01:43<00:47, 2304.73 examples/s][A
Generating train examples...:  69%|██████▊   | 240483/350343 [01:43<00:47, 2322.41 examples/s][A
Generating train examples...:  69%|██████▊   | 240716/350343 [01:43<00:47, 2320.05 examples/s][A
Generating train examples...:  69%|██████▉   | 240949/350343 [01:43<00:47, 2310.59 examples/s][A
Generating train examples...:  69%|██████▉   | 241181/350343 [01:43<00:47, 2307.94 examples/s][A
Generating train examples...:  69%|██████▉   | 241412/350343 [01:43<00:48, 2254.68 examples/s][A
Generating train examples...:  69%|██████▉   | 241638/350343 [01:43<00:49, 2191.42 examples/s][A
Generating train examples...:  69%|██████▉   | 241862/350343 [01:44<00:49, 2204.67 examples/s][A
Generating train examples...:  69%|██████▉   | 242093/350343 [01:44<00:48, 2233.62 examples/s][A
Generating train examples...:  69%|██████▉   | 242318/350343 [01:44<00:48, 2238.33 examples/s][A
Generating train examples...:  69%|██████▉   | 242543/350343 [01:44<00:48, 2239.46 examples/s][A
Generating train examples...:  69%|██████▉   | 242773/350343 [01:44<00:47, 2255.69 examples/s][A
Generating train examples...:  69%|██████▉   | 242999/350343 [01:44<00:47, 2256.27 examples/s][A
Generating train examples...:  69%|██████▉   | 243228/350343 [01:44<00:47, 2264.95 examples/s][A
Generating train examples...:  69%|██████▉   | 243461/350343 [01:44<00:46, 2282.31 examples/s][A
Generating train examples...:  70%|██████▉   | 243690/350343 [01:44<00:46, 2274.95 examples/s][A
Generating train examples...:  70%|██████▉   | 243918/350343 [01:44<00:46, 2273.35 examples/s][A
Generating train examples...:  70%|██████▉   | 244146/350343 [01:45<00:46, 2268.63 examples/s][A
Generating train examples...:  70%|██████▉   | 244373/350343 [01:45<00:46, 2262.15 examples/s][A
Generating train examples...:  70%|██████▉   | 244600/350343 [01:45<00:46, 2258.62 examples/s][A
Generating train examples...:  70%|██████▉   | 244833/350343 [01:45<00:46, 2278.23 examples/s][A
Generating train examples...:  70%|██████▉   | 245071/350343 [01:45<00:45, 2307.44 examples/s][A
Generating train examples...:  70%|███████   | 245307/350343 [01:45<00:45, 2322.60 examples/s][A
Generating train examples...:  70%|███████   | 245540/350343 [01:45<00:45, 2323.44 examples/s][A
Generating train examples...:  70%|███████   | 245778/350343 [01:45<00:44, 2338.18 examples/s][A
Generating train examples...:  70%|███████   | 246012/350343 [01:45<00:44, 2336.32 examples/s][A
Generating train examples...:  70%|███████   | 246249/350343 [01:45<00:46, 2256.59 examples/s][A
Generating train examples...:  70%|███████   | 246486/350343 [01:46<00:45, 2287.96 examples/s][A
Generating train examples...:  70%|███████   | 246727/350343 [01:46<00:44, 2322.47 examples/s][A
Generating train examples...:  70%|███████   | 246963/350343 [01:46<00:44, 2333.36 examples/s][A
Generating train examples...:  71%|███████   | 247197/350343 [01:46<00:44, 2334.32 examples/s][A
Generating train examples...:  71%|███████   | 247441/350343 [01:46<00:43, 2363.59 examples/s][A
Generating train examples...:  71%|███████   | 247682/350343 [01:46<00:43, 2375.54 examples/s][A
Generating train examples...:  71%|███████   | 247920/350343 [01:46<00:43, 2364.37 examples/s][A
Generating train examples...:  71%|███████   | 248158/350343 [01:46<00:43, 2367.48 examples/s][A
Generating train examples...:  71%|███████   | 248395/350343 [01:46<00:43, 2367.88 examples/s][A
Generating train examples...:  71%|███████   | 248632/350343 [01:46<00:43, 2346.26 examples/s][A
Generating train examples...:  71%|███████   | 248867/350343 [01:47<00:43, 2344.17 examples/s][A
Generating train examples...:  71%|███████   | 249102/350343 [01:47<00:43, 2335.89 examples/s][A
Generating train examples...:  71%|███████   | 249336/350343 [01:47<00:43, 2332.55 examples/s][A
Generating train examples...:  71%|███████   | 249570/350343 [01:47<00:43, 2330.39 examples/s][A
Generating train examples...:  71%|███████▏  | 249808/350343 [01:47<00:42, 2344.80 examples/s][A
Generating train examples...:  71%|███████▏  | 250044/350343 [01:47<00:42, 2349.20 examples/s][A
Generating train examples...:  71%|███████▏  | 250279/350343 [01:47<00:43, 2280.66 examples/s][A
Generating train examples...:  72%|███████▏  | 250517/350343 [01:47<00:43, 2308.31 examples/s][A
Generating train examples...:  72%|███████▏  | 250753/350343 [01:47<00:42, 2322.10 examples/s][A
Generating train examples...:  72%|███████▏  | 250989/350343 [01:48<00:42, 2331.53 examples/s][A
Generating train examples...:  72%|███████▏  | 251226/350343 [01:48<00:42, 2341.78 examples/s][A
Generating train examples...:  72%|███████▏  | 251461/350343 [01:48<00:42, 2329.60 examples/s][A
Generating train examples...:  72%|███████▏  | 251703/350343 [01:48<00:41, 2355.35 examples/s][A
Generating train examples...:  72%|███████▏  | 251939/350343 [01:48<00:42, 2339.27 examples/s][A
Generating train examples...:  72%|███████▏  | 252181/350343 [01:48<00:41, 2361.31 examples/s][A
Generating train examples...:  72%|███████▏  | 252420/350343 [01:48<00:41, 2367.73 examples/s][A
Generating train examples...:  72%|███████▏  | 252657/350343 [01:48<00:41, 2366.34 examples/s][A
Generating train examples...:  72%|███████▏  | 252894/350343 [01:48<00:41, 2366.39 examples/s][A
Generating train examples...:  72%|███████▏  | 253133/350343 [01:48<00:40, 2371.31 examples/s][A
Generating train examples...:  72%|███████▏  | 253371/350343 [01:49<00:40, 2368.52 examples/s][A
Generating train examples...:  72%|███████▏  | 253608/350343 [01:49<00:40, 2364.17 examples/s][A
Generating train examples...:  72%|███████▏  | 253847/350343 [01:49<00:40, 2370.93 examples/s][A
Generating train examples...:  73%|███████▎  | 254085/350343 [01:49<00:42, 2281.04 examples/s][A
Generating train examples...:  73%|███████▎  | 254314/350343 [01:49<00:42, 2282.85 examples/s][A
Generating train examples...:  73%|███████▎  | 254544/350343 [01:49<00:41, 2285.34 examples/s][A
Generating train examples...:  73%|███████▎  | 254777/350343 [01:49<00:41, 2296.99 examples/s][A
Generating train examples...:  73%|███████▎  | 255014/350343 [01:49<00:41, 2317.07 examples/s][A
Generating train examples...:  73%|███████▎  | 255255/350343 [01:49<00:40, 2344.49 examples/s][A
Generating train examples...:  73%|███████▎  | 255496/350343 [01:49<00:40, 2361.95 examples/s][A
Generating train examples...:  73%|███████▎  | 255733/350343 [01:50<00:40, 2339.98 examples/s][A
Generating train examples...:  73%|███████▎  | 255968/350343 [01:50<00:40, 2336.04 examples/s][A
Generating train examples...:  73%|███████▎  | 256209/350343 [01:50<00:39, 2356.12 examples/s][A
Generating train examples...:  73%|███████▎  | 256445/350343 [01:50<00:39, 2348.34 examples/s][A
Generating train examples...:  73%|███████▎  | 256685/350343 [01:50<00:39, 2363.28 examples/s][A
Generating train examples...:  73%|███████▎  | 256927/350343 [01:50<00:39, 2377.72 examples/s][A
Generating train examples...:  73%|███████▎  | 257165/350343 [01:50<00:39, 2365.63 examples/s][A
Generating train examples...:  73%|███████▎  | 257402/350343 [01:50<00:39, 2356.68 examples/s][A
Generating train examples...:  74%|███████▎  | 257639/350343 [01:50<00:39, 2359.64 examples/s][A
Generating train examples...:  74%|███████▎  | 257879/350343 [01:50<00:40, 2284.36 examples/s][A
Generating train examples...:  74%|███████▎  | 258114/350343 [01:51<00:40, 2301.64 examples/s][A
Generating train examples...:  74%|███████▎  | 258348/350343 [01:51<00:39, 2310.17 examples/s][A
Generating train examples...:  74%|███████▍  | 258583/350343 [01:51<00:39, 2321.43 examples/s][A
Generating train examples...:  74%|███████▍  | 258816/350343 [01:51<00:39, 2319.19 examples/s][A
Generating train examples...:  74%|███████▍  | 259056/350343 [01:51<00:38, 2342.59 examples/s][A
Generating train examples...:  74%|███████▍  | 259295/350343 [01:51<00:38, 2355.41 examples/s][A
Generating train examples...:  74%|███████▍  | 259531/350343 [01:51<00:38, 2349.59 examples/s][A
Generating train examples...:  74%|███████▍  | 259767/350343 [01:51<00:38, 2352.27 examples/s][A
Generating train examples...:  74%|███████▍  | 260004/350343 [01:51<00:38, 2356.58 examples/s][A
Generating train examples...:  74%|███████▍  | 260240/350343 [01:51<00:38, 2356.04 examples/s][A
Generating train examples...:  74%|███████▍  | 260477/350343 [01:52<00:38, 2358.63 examples/s][A
Generating train examples...:  74%|███████▍  | 260713/350343 [01:52<00:38, 2357.51 examples/s][A
Generating train examples...:  74%|███████▍  | 260951/350343 [01:52<00:37, 2362.75 examples/s][A
Generating train examples...:  75%|███████▍  | 261190/350343 [01:52<00:37, 2370.15 examples/s][A
Generating train examples...:  75%|███████▍  | 261430/350343 [01:52<00:37, 2378.16 examples/s][A
Generating train examples...:  75%|███████▍  | 261668/350343 [01:52<00:37, 2366.01 examples/s][A
Generating train examples...:  75%|███████▍  | 261905/350343 [01:52<00:38, 2281.27 examples/s][A
Generating train examples...:  75%|███████▍  | 262145/350343 [01:52<00:38, 2313.83 examples/s][A
Generating train examples...:  75%|███████▍  | 262378/350343 [01:52<00:37, 2315.87 examples/s][A
Generating train examples...:  75%|███████▍  | 262612/350343 [01:52<00:37, 2320.17 examples/s][A
Generating train examples...:  75%|███████▌  | 262845/350343 [01:53<00:37, 2321.66 examples/s][A
Generating train examples...:  75%|███████▌  | 263079/350343 [01:53<00:37, 2325.49 examples/s][A
Generating train examples...:  75%|███████▌  | 263317/350343 [01:53<00:37, 2339.56 examples/s][A
Generating train examples...:  75%|███████▌  | 263552/350343 [01:53<00:37, 2334.68 examples/s][A
Generating train examples...:  75%|███████▌  | 263786/350343 [01:53<00:37, 2330.13 examples/s][A
Generating train examples...:  75%|███████▌  | 264020/350343 [01:53<00:37, 2322.44 examples/s][A
Generating train examples...:  75%|███████▌  | 264253/350343 [01:53<00:37, 2306.21 examples/s][A
Generating train examples...:  75%|███████▌  | 264484/350343 [01:53<00:37, 2292.20 examples/s][A
Generating train examples...:  76%|███████▌  | 264714/350343 [01:53<00:37, 2287.29 examples/s][A
Generating train examples...:  76%|███████▌  | 264944/350343 [01:53<00:37, 2290.38 examples/s][A
Generating train examples...:  76%|███████▌  | 265174/350343 [01:54<00:37, 2291.31 examples/s][A
Generating train examples...:  76%|███████▌  | 265415/350343 [01:54<00:36, 2325.68 examples/s][A
Generating train examples...:  76%|███████▌  | 265655/350343 [01:54<00:36, 2345.98 examples/s][A
Generating train examples...:  76%|███████▌  | 265891/350343 [01:54<00:35, 2349.10 examples/s][A
Generating train examples...:  76%|███████▌  | 266126/350343 [01:54<00:35, 2346.81 examples/s][A
Generating train examples...:  76%|███████▌  | 266363/350343 [01:54<00:35, 2352.14 examples/s][A
Generating train examples...:  76%|███████▌  | 266599/350343 [01:54<00:36, 2268.20 examples/s][A
Generating train examples...:  76%|███████▌  | 266838/350343 [01:54<00:36, 2303.42 examples/s][A
Generating train examples...:  76%|███████▌  | 267077/350343 [01:54<00:35, 2326.85 examples/s][A
Generating train examples...:  76%|███████▋  | 267313/350343 [01:54<00:35, 2335.00 examples/s][A
Generating train examples...:  76%|███████▋  | 267547/350343 [01:55<00:35, 2318.17 examples/s][A
Generating train examples...:  76%|███████▋  | 267788/350343 [01:55<00:35, 2343.11 examples/s][A
Generating train examples...:  77%|███████▋  | 268023/350343 [01:55<00:35, 2334.22 examples/s][A
Generating train examples...:  77%|███████▋  | 268259/350343 [01:55<00:35, 2339.17 examples/s][A
Generating train examples...:  77%|███████▋  | 268494/350343 [01:55<00:34, 2341.04 examples/s][A
Generating train examples...:  77%|███████▋  | 268733/350343 [01:55<00:34, 2353.18 examples/s][A
Generating train examples...:  77%|███████▋  | 268975/350343 [01:55<00:34, 2371.78 examples/s][A
Generating train examples...:  77%|███████▋  | 269213/350343 [01:55<00:34, 2373.40 examples/s][A
Generating train examples...:  77%|███████▋  | 269451/350343 [01:55<00:34, 2368.19 examples/s][A
Generating train examples...:  77%|███████▋  | 269693/350343 [01:56<00:33, 2382.68 examples/s][A
Generating train examples...:  77%|███████▋  | 269932/350343 [01:56<00:33, 2375.83 examples/s][A
Generating train examples...:  77%|███████▋  | 270170/350343 [01:56<00:33, 2370.15 examples/s][A
Generating train examples...:  77%|███████▋  | 270408/350343 [01:56<00:35, 2269.44 examples/s][A
Generating train examples...:  77%|███████▋  | 270643/350343 [01:56<00:34, 2292.16 examples/s][A
Generating train examples...:  77%|███████▋  | 270877/350343 [01:56<00:34, 2304.31 examples/s][A
Generating train examples...:  77%|███████▋  | 271117/350343 [01:56<00:33, 2331.03 examples/s][A
Generating train examples...:  77%|███████▋  | 271357/350343 [01:56<00:33, 2349.57 examples/s][A
Generating train examples...:  78%|███████▊  | 271594/350343 [01:56<00:33, 2353.15 examples/s][A
Generating train examples...:  78%|███████▊  | 271833/350343 [01:56<00:33, 2364.08 examples/s][A
Generating train examples...:  78%|███████▊  | 272074/350343 [01:57<00:32, 2376.93 examples/s][A
Generating train examples...:  78%|███████▊  | 272314/350343 [01:57<00:32, 2383.20 examples/s][A
Generating train examples...:  78%|███████▊  | 272553/350343 [01:57<00:32, 2380.43 examples/s][A
Generating train examples...:  78%|███████▊  | 272792/350343 [01:57<00:32, 2377.42 examples/s][A
Generating train examples...:  78%|███████▊  | 273030/350343 [01:57<00:32, 2373.48 examples/s][A
Generating train examples...:  78%|███████▊  | 273268/350343 [01:57<00:32, 2361.96 examples/s][A
Generating train examples...:  78%|███████▊  | 273505/350343 [01:57<00:32, 2340.90 examples/s][A
Generating train examples...:  78%|███████▊  | 273745/350343 [01:57<00:32, 2357.29 examples/s][A
Generating train examples...:  78%|███████▊  | 273981/350343 [01:57<00:33, 2271.21 examples/s][A
Generating train examples...:  78%|███████▊  | 274209/350343 [01:57<00:33, 2273.04 examples/s][A
Generating train examples...:  78%|███████▊  | 274445/350343 [01:58<00:33, 2296.97 examples/s][A
Generating train examples...:  78%|███████▊  | 274676/350343 [01:58<00:32, 2298.17 examples/s][A
Generating train examples...:  78%|███████▊  | 274916/350343 [01:58<00:32, 2327.24 examples/s][A
Generating train examples...:  79%|███████▊  | 275156/350343 [01:58<00:32, 2347.70 examples/s][A
Generating train examples...:  79%|███████▊  | 275396/350343 [01:58<00:31, 2362.47 examples/s][A
Generating train examples...:  79%|███████▊  | 275635/350343 [01:58<00:31, 2368.33 examples/s][A
Generating train examples...:  79%|███████▊  | 275872/350343 [01:58<00:31, 2352.77 examples/s][A
Generating train examples...:  79%|███████▉  | 276110/350343 [01:58<00:31, 2360.82 examples/s][A
Generating train examples...:  79%|███████▉  | 276347/350343 [01:58<00:31, 2362.54 examples/s][A
Generating train examples...:  79%|███████▉  | 276584/350343 [01:58<00:31, 2363.57 examples/s][A
Generating train examples...:  79%|███████▉  | 276821/350343 [01:59<00:31, 2358.04 examples/s][A
Generating train examples...:  79%|███████▉  | 277057/350343 [01:59<00:31, 2349.91 examples/s][A
Generating train examples...:  79%|███████▉  | 277301/350343 [01:59<00:30, 2374.97 examples/s][A
Generating train examples...:  79%|███████▉  | 277543/350343 [01:59<00:30, 2387.17 examples/s][A
Generating train examples...:  79%|███████▉  | 277782/350343 [01:59<00:31, 2277.26 examples/s][A
Generating train examples...:  79%|███████▉  | 278025/350343 [01:59<00:31, 2319.76 examples/s][A
Generating train examples...:  79%|███████▉  | 278261/350343 [01:59<00:30, 2331.47 examples/s][A
Generating train examples...:  79%|███████▉  | 278505/350343 [01:59<00:30, 2362.80 examples/s][A
Generating train examples...:  80%|███████▉  | 278742/350343 [01:59<00:30, 2358.17 examples/s][A
Generating train examples...:  80%|███████▉  | 278979/350343 [01:59<00:30, 2351.57 examples/s][A
Generating train examples...:  80%|███████▉  | 279215/350343 [02:00<00:30, 2325.91 examples/s][A
Generating train examples...:  80%|███████▉  | 279448/350343 [02:00<00:31, 2264.38 examples/s][A
Generating train examples...:  80%|███████▉  | 279675/350343 [02:00<00:32, 2203.34 examples/s][A
Generating train examples...:  80%|███████▉  | 279902/350343 [02:00<00:31, 2220.25 examples/s][A
Generating train examples...:  80%|███████▉  | 280131/350343 [02:00<00:31, 2239.20 examples/s][A
Generating train examples...:  80%|████████  | 280360/350343 [02:00<00:31, 2251.99 examples/s][A
Generating train examples...:  80%|████████  | 280586/350343 [02:00<00:30, 2252.40 examples/s][A
Generating train examples...:  80%|████████  | 280819/350343 [02:00<00:30, 2274.18 examples/s][A
Generating train examples...:  80%|████████  | 281049/350343 [02:00<00:30, 2280.43 examples/s][A
Generating train examples...:  80%|████████  | 281278/350343 [02:00<00:30, 2265.73 examples/s][A
Generating train examples...:  80%|████████  | 281505/350343 [02:01<00:30, 2254.48 examples/s][A
Generating train examples...:  80%|████████  | 281731/350343 [02:01<00:30, 2253.03 examples/s][A
Generating train examples...:  80%|████████  | 281958/350343 [02:01<00:30, 2255.67 examples/s][A
Generating train examples...:  81%|████████  | 282187/350343 [02:01<00:30, 2263.63 examples/s][A
Generating train examples...:  81%|████████  | 282414/350343 [02:01<00:30, 2252.65 examples/s][A
Generating train examples...:  81%|████████  | 282641/350343 [02:01<00:30, 2256.32 examples/s][A
Generating train examples...:  81%|████████  | 282868/350343 [02:01<00:29, 2260.36 examples/s][A
Generating train examples...:  81%|████████  | 283095/350343 [02:01<00:29, 2254.32 examples/s][A
Generating train examples...:  81%|████████  | 283322/350343 [02:01<00:29, 2257.95 examples/s][A
Generating train examples...:  81%|████████  | 283548/350343 [02:02<00:29, 2254.41 examples/s][A
Generating train examples...:  81%|████████  | 283781/350343 [02:02<00:29, 2274.73 examples/s][A
Generating train examples...:  81%|████████  | 284013/350343 [02:02<00:29, 2285.57 examples/s][A
Generating train examples...:  81%|████████  | 284247/350343 [02:02<00:28, 2301.50 examples/s][A
Generating train examples...:  81%|████████  | 284478/350343 [02:02<00:28, 2285.30 examples/s][A
Generating train examples...:  81%|████████▏ | 284707/350343 [02:02<00:29, 2218.92 examples/s][A
Generating train examples...:  81%|████████▏ | 284940/350343 [02:02<00:29, 2251.14 examples/s][A
Generating train examples...:  81%|████████▏ | 285170/350343 [02:02<00:28, 2264.61 examples/s][A
Generating train examples...:  81%|████████▏ | 285408/350343 [02:02<00:28, 2296.84 examples/s][A
Generating train examples...:  82%|████████▏ | 285648/350343 [02:02<00:27, 2325.85 examples/s][A
Generating train examples...:  82%|████████▏ | 285886/350343 [02:03<00:27, 2339.93 examples/s][A
Generating train examples...:  82%|████████▏ | 286122/350343 [02:03<00:27, 2345.68 examples/s][A
Generating train examples...:  82%|████████▏ | 286357/350343 [02:03<00:27, 2342.70 examples/s][A
Generating train examples...:  82%|████████▏ | 286599/350343 [02:03<00:26, 2364.68 examples/s][A
Generating train examples...:  82%|████████▏ | 286838/350343 [02:03<00:26, 2371.18 examples/s][A
Generating train examples...:  82%|████████▏ | 287076/350343 [02:03<00:27, 2337.06 examples/s][A
Generating train examples...:  82%|████████▏ | 287310/350343 [02:03<00:27, 2318.73 examples/s][A
Generating train examples...:  82%|████████▏ | 287546/350343 [02:03<00:26, 2330.66 examples/s][A
Generating train examples...:  82%|████████▏ | 287785/350343 [02:03<00:26, 2345.67 examples/s][A
Generating train examples...:  82%|████████▏ | 288029/350343 [02:03<00:26, 2373.24 examples/s][A
Generating train examples...:  82%|████████▏ | 288270/350343 [02:04<00:26, 2382.45 examples/s][A
Generating train examples...:  82%|████████▏ | 288509/350343 [02:04<00:27, 2287.98 examples/s][A
Generating train examples...:  82%|████████▏ | 288742/350343 [02:04<00:26, 2298.39 examples/s][A
Generating train examples...:  82%|████████▏ | 288973/350343 [02:04<00:26, 2295.46 examples/s][A
Generating train examples...:  83%|████████▎ | 289204/350343 [02:04<00:26, 2298.20 examples/s][A
Generating train examples...:  83%|████████▎ | 289436/350343 [02:04<00:26, 2302.65 examples/s][A
Generating train examples...:  83%|████████▎ | 289667/350343 [02:04<00:26, 2298.00 examples/s][A
Generating train examples...:  83%|████████▎ | 289897/350343 [02:04<00:26, 2291.50 examples/s][A
Generating train examples...:  83%|████████▎ | 290129/350343 [02:04<00:26, 2298.43 examples/s][A
Generating train examples...:  83%|████████▎ | 290360/350343 [02:04<00:26, 2300.24 examples/s][A
Generating train examples...:  83%|████████▎ | 290591/350343 [02:05<00:25, 2300.14 examples/s][A
Generating train examples...:  83%|████████▎ | 290823/350343 [02:05<00:25, 2305.28 examples/s][A
Generating train examples...:  83%|████████▎ | 291064/350343 [02:05<00:25, 2335.39 examples/s][A
Generating train examples...:  83%|████████▎ | 291303/350343 [02:05<00:25, 2350.98 examples/s][A
Generating train examples...:  83%|████████▎ | 291539/350343 [02:05<00:25, 2346.61 examples/s][A
Generating train examples...:  83%|████████▎ | 291774/350343 [02:05<00:25, 2329.85 examples/s][A
Generating train examples...:  83%|████████▎ | 292008/350343 [02:05<00:25, 2327.31 examples/s][A
Generating train examples...:  83%|████████▎ | 292245/350343 [02:05<00:24, 2339.99 examples/s][A
Generating train examples...:  83%|████████▎ | 292480/350343 [02:05<00:24, 2340.25 examples/s][A
Generating train examples...:  84%|████████▎ | 292715/350343 [02:05<00:24, 2333.95 examples/s][A
Generating train examples...:  84%|████████▎ | 292949/350343 [02:06<00:24, 2321.85 examples/s][A
Generating train examples...:  84%|████████▎ | 293182/350343 [02:06<00:24, 2321.66 examples/s][A
Generating train examples...:  84%|████████▍ | 293415/350343 [02:06<00:24, 2317.68 examples/s][A
Generating train examples...:  84%|████████▍ | 293647/350343 [02:06<00:24, 2312.45 examples/s][A
Generating train examples...:  84%|████████▍ | 293879/350343 [02:06<00:25, 2246.64 examples/s][A
Generating train examples...:  84%|████████▍ | 294114/350343 [02:06<00:24, 2274.66 examples/s][A
Generating train examples...:  84%|████████▍ | 294357/350343 [02:06<00:24, 2319.61 examples/s][A
Generating train examples...:  84%|████████▍ | 294597/350343 [02:06<00:23, 2342.77 examples/s][A
Generating train examples...:  84%|████████▍ | 294835/350343 [02:06<00:23, 2353.02 examples/s][A
Generating train examples...:  84%|████████▍ | 295074/350343 [02:06<00:23, 2361.35 examples/s][A
Generating train examples...:  84%|████████▍ | 295311/350343 [02:07<00:23, 2346.38 examples/s][A
Generating train examples...:  84%|████████▍ | 295551/350343 [02:07<00:23, 2361.57 examples/s][A
Generating train examples...:  84%|████████▍ | 295788/350343 [02:07<00:23, 2348.94 examples/s][A
Generating train examples...:  84%|████████▍ | 296023/350343 [02:07<00:23, 2334.45 examples/s][A
Generating train examples...:  85%|████████▍ | 296258/350343 [02:07<00:23, 2338.72 examples/s][A
Generating train examples...:  85%|████████▍ | 296493/350343 [02:07<00:23, 2341.20 examples/s][A
Generating train examples...:  85%|████████▍ | 296734/350343 [02:07<00:22, 2360.11 examples/s][A
Generating train examples...:  85%|████████▍ | 296973/350343 [02:07<00:22, 2367.12 examples/s][A
Generating train examples...:  85%|████████▍ | 297210/350343 [02:07<00:22, 2349.79 examples/s][A
Generating train examples...:  85%|████████▍ | 297446/350343 [02:07<00:22, 2343.94 examples/s][A
Generating train examples...:  85%|████████▍ | 297681/350343 [02:08<00:24, 2181.00 examples/s][A
Generating train examples...:  85%|████████▌ | 297902/350343 [02:08<00:24, 2148.45 examples/s][A
Generating train examples...:  85%|████████▌ | 298125/350343 [02:08<00:24, 2171.48 examples/s][A
Generating train examples...:  85%|████████▌ | 298353/350343 [02:08<00:23, 2199.96 examples/s][A
Generating train examples...:  85%|████████▌ | 298580/350343 [02:08<00:23, 2220.12 examples/s][A
Generating train examples...:  85%|████████▌ | 298821/350343 [02:08<00:22, 2274.15 examples/s][A
Generating train examples...:  85%|████████▌ | 299054/350343 [02:08<00:22, 2289.11 examples/s][A
Generating train examples...:  85%|████████▌ | 299284/350343 [02:08<00:22, 2290.74 examples/s][A
Generating train examples...:  85%|████████▌ | 299517/350343 [02:08<00:22, 2301.51 examples/s][A
Generating train examples...:  86%|████████▌ | 299751/350343 [02:09<00:21, 2312.95 examples/s][A
Generating train examples...:  86%|████████▌ | 299983/350343 [02:09<00:21, 2308.32 examples/s][A
Generating train examples...:  86%|████████▌ | 300223/350343 [02:09<00:21, 2334.81 examples/s][A
Generating train examples...:  86%|████████▌ | 300464/350343 [02:09<00:21, 2355.30 examples/s][A
Generating train examples...:  86%|████████▌ | 300703/350343 [02:09<00:20, 2363.81 examples/s][A
Generating train examples...:  86%|████████▌ | 300940/350343 [02:09<00:21, 2351.45 examples/s][A
Generating train examples...:  86%|████████▌ | 301176/350343 [02:09<00:20, 2343.72 examples/s][A
Generating train examples...:  86%|████████▌ | 301417/350343 [02:09<00:20, 2361.33 examples/s][A
Generating train examples...:  86%|████████▌ | 301655/350343 [02:09<00:20, 2365.49 examples/s][A
Generating train examples...:  86%|████████▌ | 301892/350343 [02:09<00:20, 2363.77 examples/s][A
Generating train examples...:  86%|████████▌ | 302129/350343 [02:10<00:20, 2352.29 examples/s][A
Generating train examples...:  86%|████████▋ | 302365/350343 [02:10<00:20, 2323.34 examples/s][A
Generating train examples...:  86%|████████▋ | 302598/350343 [02:10<00:21, 2241.40 examples/s][A
Generating train examples...:  86%|████████▋ | 302832/350343 [02:10<00:20, 2268.57 examples/s][A
Generating train examples...:  87%|████████▋ | 303067/350343 [02:10<00:20, 2290.97 examples/s][A
Generating train examples...:  87%|████████▋ | 303304/350343 [02:10<00:20, 2312.75 examples/s][A
Generating train examples...:  87%|████████▋ | 303541/350343 [02:10<00:20, 2328.76 examples/s][A
Generating train examples...:  87%|████████▋ | 303777/350343 [02:10<00:19, 2336.33 examples/s][A
Generating train examples...:  87%|████████▋ | 304012/350343 [02:10<00:19, 2338.92 examples/s][A
Generating train examples...:  87%|████████▋ | 304252/350343 [02:10<00:19, 2356.92 examples/s][A
Generating train examples...:  87%|████████▋ | 304488/350343 [02:11<00:19, 2345.60 examples/s][A
Generating train examples...:  87%|████████▋ | 304723/350343 [02:11<00:19, 2343.14 examples/s][A
Generating train examples...:  87%|████████▋ | 304961/350343 [02:11<00:19, 2353.40 examples/s][A
Generating train examples...:  87%|████████▋ | 305197/350343 [02:11<00:19, 2342.36 examples/s][A
Generating train examples...:  87%|████████▋ | 305434/350343 [02:11<00:19, 2349.99 examples/s][A
Generating train examples...:  87%|████████▋ | 305671/350343 [02:11<00:18, 2355.21 examples/s][A
Generating train examples...:  87%|████████▋ | 305907/350343 [02:11<00:18, 2353.33 examples/s][A
Generating train examples...:  87%|████████▋ | 306143/350343 [02:11<00:18, 2351.79 examples/s][A
Generating train examples...:  87%|████████▋ | 306379/350343 [02:11<00:18, 2341.73 examples/s][A
Generating train examples...:  88%|████████▊ | 306614/350343 [02:11<00:19, 2292.12 examples/s][A
Generating train examples...:  88%|████████▊ | 306844/350343 [02:12<00:20, 2163.85 examples/s][A
Generating train examples...:  88%|████████▊ | 307071/350343 [02:12<00:19, 2191.97 examples/s][A
Generating train examples...:  88%|████████▊ | 307300/350343 [02:12<00:19, 2218.89 examples/s][A
Generating train examples...:  88%|████████▊ | 307530/350343 [02:12<00:19, 2241.42 examples/s][A
Generating train examples...:  88%|████████▊ | 307765/350343 [02:12<00:18, 2272.29 examples/s][A
Generating train examples...:  88%|████████▊ | 307993/350343 [02:12<00:18, 2273.28 examples/s][A
Generating train examples...:  88%|████████▊ | 308221/350343 [02:12<00:18, 2269.14 examples/s][A
Generating train examples...:  88%|████████▊ | 308449/350343 [02:12<00:18, 2270.45 examples/s][A
Generating train examples...:  88%|████████▊ | 308685/350343 [02:12<00:18, 2294.86 examples/s][A
Generating train examples...:  88%|████████▊ | 308915/350343 [02:12<00:18, 2287.47 examples/s][A
Generating train examples...:  88%|████████▊ | 309144/350343 [02:13<00:18, 2287.58 examples/s][A
Generating train examples...:  88%|████████▊ | 309373/350343 [02:13<00:17, 2283.25 examples/s][A
Generating train examples...:  88%|████████▊ | 309602/350343 [02:13<00:17, 2282.00 examples/s][A
Generating train examples...:  88%|████████▊ | 309837/350343 [02:13<00:17, 2301.19 examples/s][A
Generating train examples...:  89%|████████▊ | 310070/350343 [02:13<00:17, 2308.83 examples/s][A
Generating train examples...:  89%|████████▊ | 310301/350343 [02:13<00:17, 2296.71 examples/s][A
Generating train examples...:  89%|████████▊ | 310536/350343 [02:13<00:17, 2311.51 examples/s][A
Generating train examples...:  89%|████████▊ | 310772/350343 [02:13<00:17, 2323.87 examples/s][A
Generating train examples...:  89%|████████▉ | 311008/350343 [02:13<00:16, 2332.62 examples/s][A
Generating train examples...:  89%|████████▉ | 311242/350343 [02:13<00:16, 2330.03 examples/s][A
Generating train examples...:  89%|████████▉ | 311476/350343 [02:14<00:16, 2326.29 examples/s][A
Generating train examples...:  89%|████████▉ | 311710/350343 [02:14<00:16, 2330.15 examples/s][A
Generating train examples...:  89%|████████▉ | 311944/350343 [02:14<00:16, 2301.58 examples/s][A
Generating train examples...:  89%|████████▉ | 312175/350343 [02:14<00:17, 2234.86 examples/s][A
Generating train examples...:  89%|████████▉ | 312409/350343 [02:14<00:16, 2264.65 examples/s][A
Generating train examples...:  89%|████████▉ | 312637/350343 [02:14<00:16, 2267.63 examples/s][A
Generating train examples...:  89%|████████▉ | 312871/350343 [02:14<00:16, 2287.88 examples/s][A
Generating train examples...:  89%|████████▉ | 313101/350343 [02:14<00:16, 2195.29 examples/s][A
Generating train examples...:  89%|████████▉ | 313331/350343 [02:14<00:16, 2225.35 examples/s][A
Generating train examples...:  90%|████████▉ | 313557/350343 [02:15<00:16, 2234.08 examples/s][A
Generating train examples...:  90%|████████▉ | 313781/350343 [02:15<00:17, 2142.68 examples/s][A
Generating train examples...:  90%|████████▉ | 313997/350343 [02:15<00:16, 2145.56 examples/s][A
Generating train examples...:  90%|████████▉ | 314233/350343 [02:15<00:16, 2206.20 examples/s][A
Generating train examples...:  90%|████████▉ | 314472/350343 [02:15<00:15, 2258.91 examples/s][A
Generating train examples...:  90%|████████▉ | 314709/350343 [02:15<00:15, 2290.18 examples/s][A
Generating train examples...:  90%|████████▉ | 314944/350343 [02:15<00:15, 2306.96 examples/s][A
Generating train examples...:  90%|████████▉ | 315178/350343 [02:15<00:15, 2315.29 examples/s][A
Generating train examples...:  90%|█████████ | 315414/350343 [02:15<00:15, 2328.56 examples/s][A
Generating train examples...:  90%|█████████ | 315651/350343 [02:15<00:14, 2339.94 examples/s][A
Generating train examples...:  90%|█████████ | 315891/350343 [02:16<00:14, 2356.67 examples/s][A
Generating train examples...:  90%|█████████ | 316127/350343 [02:16<00:14, 2353.97 examples/s][A
Generating train examples...:  90%|█████████ | 316363/350343 [02:16<00:14, 2350.85 examples/s][A
Generating train examples...:  90%|█████████ | 316600/350343 [02:16<00:14, 2355.44 examples/s][A
Generating train examples...:  90%|█████████ | 316836/350343 [02:16<00:14, 2345.05 examples/s][A
Generating train examples...:  91%|█████████ | 317071/350343 [02:16<00:14, 2326.92 examples/s][A
Generating train examples...:  91%|█████████ | 317304/350343 [02:16<00:14, 2315.15 examples/s][A
Generating train examples...:  91%|█████████ | 317544/350343 [02:16<00:14, 2339.01 examples/s][A
Generating train examples...:  91%|█████████ | 317778/350343 [02:16<00:13, 2333.80 examples/s][A
Generating train examples...:  91%|█████████ | 318016/350343 [02:16<00:13, 2345.18 examples/s][A
Generating train examples...:  91%|█████████ | 318251/350343 [02:17<00:14, 2239.33 examples/s][A
Generating train examples...:  91%|█████████ | 318483/350343 [02:17<00:14, 2262.43 examples/s][A
Generating train examples...:  91%|█████████ | 318722/350343 [02:17<00:13, 2299.01 examples/s][A
Generating train examples...:  91%|█████████ | 318953/350343 [02:17<00:13, 2297.98 examples/s][A
Generating train examples...:  91%|█████████ | 319194/350343 [02:17<00:13, 2330.49 examples/s][A
Generating train examples...:  91%|█████████ | 319428/350343 [02:17<00:13, 2314.67 examples/s][A
Generating train examples...:  91%|█████████ | 319660/350343 [02:17<00:13, 2314.95 examples/s][A
Generating train examples...:  91%|█████████▏| 319893/350343 [02:17<00:13, 2317.29 examples/s][A
Generating train examples...:  91%|█████████▏| 320135/350343 [02:17<00:12, 2346.42 examples/s][A
Generating train examples...:  91%|█████████▏| 320373/350343 [02:17<00:12, 2355.04 examples/s][A
Generating train examples...:  92%|█████████▏| 320609/350343 [02:18<00:12, 2346.53 examples/s][A
Generating train examples...:  92%|█████████▏| 320846/350343 [02:18<00:12, 2351.98 examples/s][A
Generating train examples...:  92%|█████████▏| 321082/350343 [02:18<00:12, 2343.70 examples/s][A
Generating train examples...:  92%|█████████▏| 321317/350343 [02:18<00:12, 2337.16 examples/s][A
Generating train examples...:  92%|█████████▏| 321556/350343 [02:18<00:12, 2351.10 examples/s][A
Generating train examples...:  92%|█████████▏| 321797/350343 [02:18<00:12, 2366.76 examples/s][A
Generating train examples...:  92%|█████████▏| 322035/350343 [02:18<00:11, 2368.70 examples/s][A
Generating train examples...:  92%|█████████▏| 322272/350343 [02:18<00:12, 2283.09 examples/s][A
Generating train examples...:  92%|█████████▏| 322510/350343 [02:18<00:12, 2310.28 examples/s][A
Generating train examples...:  92%|█████████▏| 322748/350343 [02:18<00:11, 2330.20 examples/s][A
Generating train examples...:  92%|█████████▏| 322982/350343 [02:19<00:11, 2322.54 examples/s][A
Generating train examples...:  92%|█████████▏| 323218/350343 [02:19<00:11, 2331.73 examples/s][A
Generating train examples...:  92%|█████████▏| 323457/350343 [02:19<00:11, 2346.80 examples/s][A
Generating train examples...:  92%|█████████▏| 323692/350343 [02:19<00:11, 2346.53 examples/s][A
Generating train examples...:  92%|█████████▏| 323927/350343 [02:19<00:11, 2331.75 examples/s][A
Generating train examples...:  93%|█████████▎| 324165/350343 [02:19<00:11, 2343.49 examples/s][A
Generating train examples...:  93%|█████████▎| 324401/350343 [02:19<00:11, 2347.43 examples/s][A
Generating train examples...:  93%|█████████▎| 324641/350343 [02:19<00:10, 2361.75 examples/s][A
Generating train examples...:  93%|█████████▎| 324881/350343 [02:19<00:10, 2372.50 examples/s][A
Generating train examples...:  93%|█████████▎| 325119/350343 [02:19<00:10, 2304.65 examples/s][A
Generating train examples...:  93%|█████████▎| 325350/350343 [02:20<00:11, 2236.03 examples/s][A
Generating train examples...:  93%|█████████▎| 325575/350343 [02:20<00:11, 2235.04 examples/s][A
Generating train examples...:  93%|█████████▎| 325801/350343 [02:20<00:10, 2241.72 examples/s][A
Generating train examples...:  93%|█████████▎| 326031/350343 [02:20<00:10, 2256.96 examples/s][A
Generating train examples...:  93%|█████████▎| 326257/350343 [02:20<00:10, 2251.74 examples/s][A
Generating train examples...:  93%|█████████▎| 326486/350343 [02:20<00:10, 2260.49 examples/s][A
Generating train examples...:  93%|█████████▎| 326714/350343 [02:20<00:10, 2265.84 examples/s][A
Generating train examples...:  93%|█████████▎| 326941/350343 [02:20<00:10, 2258.37 examples/s][A
Generating train examples...:  93%|█████████▎| 327175/350343 [02:20<00:10, 2282.52 examples/s][A
Generating train examples...:  93%|█████████▎| 327410/350343 [02:21<00:09, 2302.14 examples/s][A
Generating train examples...:  94%|█████████▎| 327641/350343 [02:21<00:10, 2227.60 examples/s][A
Generating train examples...:  94%|█████████▎| 327878/350343 [02:21<00:09, 2268.64 examples/s][A
Generating train examples...:  94%|█████████▎| 328112/350343 [02:21<00:09, 2288.51 examples/s][A
Generating train examples...:  94%|█████████▎| 328351/350343 [02:21<00:09, 2317.57 examples/s][A
Generating train examples...:  94%|█████████▍| 328587/350343 [02:21<00:09, 2330.16 examples/s][A
Generating train examples...:  94%|█████████▍| 328821/350343 [02:21<00:09, 2324.24 examples/s][A
Generating train examples...:  94%|█████████▍| 329056/350343 [02:21<00:09, 2329.28 examples/s][A
Generating train examples...:  94%|█████████▍| 329290/350343 [02:21<00:09, 2329.45 examples/s][A
Generating train examples...:  94%|█████████▍| 329524/350343 [02:21<00:09, 2238.30 examples/s][A
Generating train examples...:  94%|█████████▍| 329749/350343 [02:22<00:09, 2231.45 examples/s][A
Generating train examples...:  94%|█████████▍| 329973/350343 [02:22<00:09, 2132.73 examples/s][A
Generating train examples...:  94%|█████████▍| 330190/350343 [02:22<00:09, 2143.25 examples/s][A
Generating train examples...:  94%|█████████▍| 330417/350343 [02:22<00:09, 2178.86 examples/s][A
Generating train examples...:  94%|█████████▍| 330652/350343 [02:22<00:08, 2228.76 examples/s][A
Generating train examples...:  94%|█████████▍| 330885/350343 [02:22<00:08, 2257.95 examples/s][A
Generating train examples...:  95%|█████████▍| 331115/350343 [02:22<00:08, 2270.05 examples/s][A
Generating train examples...:  95%|█████████▍| 331344/350343 [02:22<00:08, 2274.04 examples/s][A
Generating train examples...:  95%|█████████▍| 331577/350343 [02:22<00:08, 2289.92 examples/s][A
Generating train examples...:  95%|█████████▍| 331808/350343 [02:22<00:08, 2293.85 examples/s][A
Generating train examples...:  95%|█████████▍| 332038/350343 [02:23<00:08, 2261.10 examples/s][A
Generating train examples...:  95%|█████████▍| 332271/350343 [02:23<00:07, 2280.52 examples/s][A
Generating train examples...:  95%|█████████▍| 332509/350343 [02:23<00:07, 2308.42 examples/s][A
Generating train examples...:  95%|█████████▍| 332743/350343 [02:23<00:07, 2315.48 examples/s][A
Generating train examples...:  95%|█████████▌| 332975/350343 [02:23<00:07, 2314.16 examples/s][A
Generating train examples...:  95%|█████████▌| 333208/350343 [02:23<00:07, 2318.86 examples/s][A
Generating train examples...:  95%|█████████▌| 333447/350343 [02:23<00:07, 2337.96 examples/s][A
Generating train examples...:  95%|█████████▌| 333681/350343 [02:23<00:07, 2260.62 examples/s][A
Generating train examples...:  95%|█████████▌| 333917/350343 [02:23<00:07, 2289.27 examples/s][A
Generating train examples...:  95%|█████████▌| 334147/350343 [02:23<00:07, 2290.45 examples/s][A
Generating train examples...:  95%|█████████▌| 334383/350343 [02:24<00:06, 2309.92 examples/s][A
Generating train examples...:  96%|█████████▌| 334615/350343 [02:24<00:06, 2310.51 examples/s][A
Generating train examples...:  96%|█████████▌| 334847/350343 [02:24<00:06, 2307.43 examples/s][A
Generating train examples...:  96%|█████████▌| 335081/350343 [02:24<00:06, 2316.71 examples/s][A
Generating train examples...:  96%|█████████▌| 335322/350343 [02:24<00:06, 2342.68 examples/s][A
Generating train examples...:  96%|█████████▌| 335560/350343 [02:24<00:06, 2353.25 examples/s][A
Generating train examples...:  96%|█████████▌| 335800/350343 [02:24<00:06, 2365.39 examples/s][A
Generating train examples...:  96%|█████████▌| 336037/350343 [02:24<00:06, 2355.73 examples/s][A
Generating train examples...:  96%|█████████▌| 336277/350343 [02:24<00:05, 2368.69 examples/s][A
Generating train examples...:  96%|█████████▌| 336514/350343 [02:24<00:05, 2350.33 examples/s][A
Generating train examples...:  96%|█████████▌| 336750/350343 [02:25<00:05, 2348.36 examples/s][A
Generating train examples...:  96%|█████████▌| 336985/350343 [02:25<00:05, 2347.37 examples/s][A
Generating train examples...:  96%|█████████▋| 337220/350343 [02:25<00:05, 2313.74 examples/s][A
Generating train examples...:  96%|█████████▋| 337452/350343 [02:25<00:05, 2189.63 examples/s][A
Generating train examples...:  96%|█████████▋| 337673/350343 [02:25<00:05, 2144.38 examples/s][A
Generating train examples...:  96%|█████████▋| 337907/350343 [02:25<00:05, 2199.70 examples/s][A
Generating train examples...:  97%|█████████▋| 338141/350343 [02:25<00:05, 2239.30 examples/s][A
Generating train examples...:  97%|█████████▋| 338371/350343 [02:25<00:05, 2191.20 examples/s][A
Generating train examples...:  97%|█████████▋| 338609/350343 [02:25<00:05, 2243.20 examples/s][A
Generating train examples...:  97%|█████████▋| 338843/350343 [02:26<00:05, 2270.29 examples/s][A
Generating train examples...:  97%|█████████▋| 339077/350343 [02:26<00:04, 2288.82 examples/s][A
Generating train examples...:  97%|█████████▋| 339315/350343 [02:26<00:04, 2314.94 examples/s][A
Generating train examples...:  97%|█████████▋| 339556/350343 [02:26<00:04, 2341.60 examples/s][A
Generating train examples...:  97%|█████████▋| 339791/350343 [02:26<00:04, 2332.73 examples/s][A
Generating train examples...:  97%|█████████▋| 340025/350343 [02:26<00:04, 2328.67 examples/s][A
Generating train examples...:  97%|█████████▋| 340259/350343 [02:26<00:04, 2319.77 examples/s][A
Generating train examples...:  97%|█████████▋| 340495/350343 [02:26<00:04, 2330.81 examples/s][A
Generating train examples...:  97%|█████████▋| 340729/350343 [02:26<00:04, 2331.85 examples/s][A
Generating train examples...:  97%|█████████▋| 340965/350343 [02:26<00:04, 2338.07 examples/s][A
Generating train examples...:  97%|█████████▋| 341201/350343 [02:27<00:03, 2344.18 examples/s][A
Generating train examples...:  97%|█████████▋| 341436/350343 [02:27<00:03, 2325.50 examples/s][A
Generating train examples...:  98%|█████████▊| 341669/350343 [02:27<00:03, 2314.58 examples/s][A
Generating train examples...:  98%|█████████▊| 341901/350343 [02:27<00:03, 2185.65 examples/s][A
Generating train examples...:  98%|█████████▊| 342132/350343 [02:27<00:03, 2219.23 examples/s][A
Generating train examples...:  98%|█████████▊| 342363/350343 [02:27<00:03, 2244.18 examples/s][A
Generating train examples...:  98%|█████████▊| 342597/350343 [02:27<00:03, 2270.62 examples/s][A
Generating train examples...:  98%|█████████▊| 342828/350343 [02:27<00:03, 2282.17 examples/s][A
Generating train examples...:  98%|█████████▊| 343062/350343 [02:27<00:03, 2297.16 examples/s][A
Generating train examples...:  98%|█████████▊| 343293/350343 [02:27<00:03, 2227.34 examples/s][A
Generating train examples...:  98%|█████████▊| 343526/350343 [02:28<00:03, 2255.57 examples/s][A
Generating train examples...:  98%|█████████▊| 343761/350343 [02:28<00:02, 2283.15 examples/s][A
Generating train examples...:  98%|█████████▊| 344001/350343 [02:28<00:02, 2316.07 examples/s][A
Generating train examples...:  98%|█████████▊| 344233/350343 [02:28<00:02, 2301.56 examples/s][A
Generating train examples...:  98%|█████████▊| 344464/350343 [02:28<00:02, 2298.39 examples/s][A
Generating train examples...:  98%|█████████▊| 344695/350343 [02:28<00:02, 2297.49 examples/s][A
Generating train examples...:  98%|█████████▊| 344933/350343 [02:28<00:02, 2321.07 examples/s][A
Generating train examples...:  99%|█████████▊| 345172/350343 [02:28<00:02, 2339.45 examples/s][A
Generating train examples...:  99%|█████████▊| 345414/350343 [02:28<00:02, 2361.44 examples/s][A
Generating train examples...:  99%|█████████▊| 345652/350343 [02:28<00:01, 2366.17 examples/s][A
Generating train examples...:  99%|█████████▊| 345891/350343 [02:29<00:01, 2370.90 examples/s][A
Generating train examples...:  99%|█████████▉| 346129/350343 [02:29<00:01, 2371.52 examples/s][A
Generating train examples...:  99%|█████████▉| 346372/350343 [02:29<00:01, 2388.58 examples/s][A
Generating train examples...:  99%|█████████▉| 346611/350343 [02:29<00:01, 2370.03 examples/s][A
Generating train examples...:  99%|█████████▉| 346852/350343 [02:29<00:01, 2381.34 examples/s][A
Generating train examples...:  99%|█████████▉| 347091/350343 [02:29<00:01, 2372.29 examples/s][A
Generating train examples...:  99%|█████████▉| 347329/350343 [02:29<00:01, 2282.64 examples/s][A
Generating train examples...:  99%|█████████▉| 347570/350343 [02:29<00:01, 2317.25 examples/s][A
Generating train examples...:  99%|█████████▉| 347806/350343 [02:29<00:01, 2328.01 examples/s][A
Generating train examples...:  99%|█████████▉| 348040/350343 [02:29<00:00, 2327.56 examples/s][A
Generating train examples...:  99%|█████████▉| 348274/350343 [02:30<00:00, 2328.14 examples/s][A
Generating train examples...:  99%|█████████▉| 348514/350343 [02:30<00:00, 2347.59 examples/s][A
Generating train examples...: 100%|█████████▉| 348749/350343 [02:30<00:00, 2340.15 examples/s][A
Generating train examples...: 100%|█████████▉| 348984/350343 [02:30<00:00, 2341.19 examples/s][A
Generating train examples...: 100%|█████████▉| 349219/350343 [02:30<00:00, 2341.98 examples/s][A
Generating train examples...: 100%|█████████▉| 349454/350343 [02:30<00:00, 2341.20 examples/s][A
Generating train examples...: 100%|█████████▉| 349689/350343 [02:30<00:00, 2338.15 examples/s][A
Generating train examples...: 100%|█████████▉| 349929/350343 [02:30<00:00, 2355.59 examples/s][A
Generating train examples...: 100%|█████████▉| 350166/350343 [02:30<00:00, 2359.76 examples/s][A
                                                                                              [A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteVNVHA5/ogbg_molpcba-train.tfrecord*...:   0%|          | 0/350343 [00:00<?, ? examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteVNVHA5/ogbg_molpcba-train.tfrecord*...:   0%|          | 1/350343 [00:00<39:21:22,  2.47 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteVNVHA5/ogbg_molpcba-train.tfrecord*...:   4%|▍         | 14729/350343 [00:00<00:08, 38445.99 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteVNVHA5/ogbg_molpcba-train.tfrecord*...:   9%|▊         | 29933/350343 [00:00<00:04, 69298.06 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteVNVHA5/ogbg_molpcba-train.tfrecord*...:  13%|█▎        | 45216/350343 [00:00<00:03, 92647.09 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteVNVHA5/ogbg_molpcba-train.tfrecord*...:  17%|█▋        | 60718/350343 [00:00<00:02, 110444.94 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteVNVHA5/ogbg_molpcba-train.tfrecord*...:  22%|██▏       | 76378/350343 [00:00<00:02, 123809.58 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteVNVHA5/ogbg_molpcba-train.tfrecord*...:  26%|██▌       | 91778/350343 [00:01<00:01, 132642.50 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteVNVHA5/ogbg_molpcba-train.tfrecord*...:  31%|███       | 107636/350343 [00:01<00:01, 140285.64 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteVNVHA5/ogbg_molpcba-train.tfrecord*...:  35%|███▌      | 123228/350343 [00:01<00:01, 144915.85 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteVNVHA5/ogbg_molpcba-train.tfrecord*...:  40%|███▉      | 138801/350343 [00:01<00:01, 148121.39 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteVNVHA5/ogbg_molpcba-train.tfrecord*...:  44%|████▍     | 154616/350343 [00:01<00:01, 151109.06 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteVNVHA5/ogbg_molpcba-train.tfrecord*...:  49%|████▊     | 170487/350343 [00:01<00:01, 153375.06 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteVNVHA5/ogbg_molpcba-train.tfrecord*...:  53%|█████▎    | 186354/350343 [00:01<00:01, 154955.16 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteVNVHA5/ogbg_molpcba-train.tfrecord*...:  58%|█████▊    | 202358/350343 [00:01<00:00, 156474.73 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteVNVHA5/ogbg_molpcba-train.tfrecord*...:  62%|██████▏   | 218145/350343 [00:01<00:00, 156802.69 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteVNVHA5/ogbg_molpcba-train.tfrecord*...:  67%|██████▋   | 234198/350343 [00:01<00:00, 157916.93 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteVNVHA5/ogbg_molpcba-train.tfrecord*...:  71%|███████▏  | 250134/350343 [00:02<00:00, 158346.40 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteVNVHA5/ogbg_molpcba-train.tfrecord*...:  76%|███████▌  | 266182/350343 [00:02<00:00, 158982.65 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteVNVHA5/ogbg_molpcba-train.tfrecord*...:  81%|████████  | 282256/350343 [00:02<00:00, 159506.77 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteVNVHA5/ogbg_molpcba-train.tfrecord*...:  85%|████████▌ | 298390/350343 [00:02<00:00, 160052.68 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteVNVHA5/ogbg_molpcba-train.tfrecord*...:  90%|████████▉ | 314412/350343 [00:02<00:00, 159983.37 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteVNVHA5/ogbg_molpcba-train.tfrecord*...:  94%|█████████▍| 330454/350343 [00:02<00:00, 160112.25 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteVNVHA5/ogbg_molpcba-train.tfrecord*...:  99%|█████████▉| 346474/350343 [00:02<00:00, 159740.17 examples/s][A
                                                                                                                                                            [AI0311 03:21:26.919368 139789500200768 writer.py:301] Done writing /root/data/ogbg_molpcba/0.1.3.incompleteVNVHA5/ogbg_molpcba-train.tfrecord*. Number of examples: 350343 (shards: [43793, 43793, 43793, 43793, 43792, 43793, 43793, 43793])
Generating splits...:  33%|███▎      | 1/3 [02:33<05:07, 153.71s/ splits]
Generating validation examples...:   0%|          | 0/43793 [00:00<?, ? examples/s][A
Generating validation examples...:   0%|          | 108/43793 [00:00<00:43, 1005.16 examples/s][A
Generating validation examples...:   1%|          | 333/43793 [00:00<00:25, 1712.96 examples/s][A
Generating validation examples...:   1%|▏         | 560/43793 [00:00<00:22, 1959.96 examples/s][A
Generating validation examples...:   2%|▏         | 786/43793 [00:00<00:20, 2074.38 examples/s][A
Generating validation examples...:   2%|▏         | 1014/43793 [00:00<00:19, 2147.11 examples/s][A
Generating validation examples...:   3%|▎         | 1233/43793 [00:00<00:19, 2159.22 examples/s][A
Generating validation examples...:   3%|▎         | 1458/43793 [00:00<00:19, 2186.30 examples/s][A
Generating validation examples...:   4%|▍         | 1679/43793 [00:00<00:19, 2193.79 examples/s][A
Generating validation examples...:   4%|▍         | 1904/43793 [00:00<00:18, 2209.36 examples/s][A
Generating validation examples...:   5%|▍         | 2126/43793 [00:01<00:18, 2210.71 examples/s][A
Generating validation examples...:   5%|▌         | 2348/43793 [00:01<00:18, 2196.08 examples/s][A
Generating validation examples...:   6%|▌         | 2576/43793 [00:01<00:18, 2220.80 examples/s][A
Generating validation examples...:   6%|▋         | 2805/43793 [00:01<00:18, 2239.45 examples/s][A
Generating validation examples...:   7%|▋         | 3034/43793 [00:01<00:18, 2253.08 examples/s][A
Generating validation examples...:   7%|▋         | 3261/43793 [00:01<00:17, 2255.92 examples/s][A
Generating validation examples...:   8%|▊         | 3487/43793 [00:01<00:18, 2235.07 examples/s][A
Generating validation examples...:   8%|▊         | 3711/43793 [00:01<00:17, 2230.72 examples/s][A
Generating validation examples...:   9%|▉         | 3939/43793 [00:01<00:17, 2244.71 examples/s][A
Generating validation examples...:  10%|▉         | 4164/43793 [00:01<00:17, 2240.91 examples/s][A
Generating validation examples...:  10%|█         | 4389/43793 [00:02<00:17, 2242.21 examples/s][A
Generating validation examples...:  11%|█         | 4616/43793 [00:02<00:17, 2249.58 examples/s][A
Generating validation examples...:  11%|█         | 4846/43793 [00:02<00:17, 2263.11 examples/s][A
Generating validation examples...:  12%|█▏        | 5076/43793 [00:02<00:17, 2272.21 examples/s][A
Generating validation examples...:  12%|█▏        | 5305/43793 [00:02<00:16, 2276.73 examples/s][A
Generating validation examples...:  13%|█▎        | 5533/43793 [00:02<00:16, 2264.69 examples/s][A
Generating validation examples...:  13%|█▎        | 5760/43793 [00:02<00:16, 2249.92 examples/s][A
Generating validation examples...:  14%|█▎        | 5986/43793 [00:02<00:16, 2247.08 examples/s][A
Generating validation examples...:  14%|█▍        | 6211/43793 [00:02<00:16, 2242.50 examples/s][A
Generating validation examples...:  15%|█▍        | 6436/43793 [00:02<00:16, 2238.03 examples/s][A
Generating validation examples...:  15%|█▌        | 6660/43793 [00:03<00:16, 2237.44 examples/s][A
Generating validation examples...:  16%|█▌        | 6885/43793 [00:03<00:16, 2240.23 examples/s][A
Generating validation examples...:  16%|█▌        | 7111/43793 [00:03<00:16, 2244.75 examples/s][A
Generating validation examples...:  17%|█▋        | 7338/43793 [00:03<00:16, 2250.08 examples/s][A
Generating validation examples...:  17%|█▋        | 7564/43793 [00:03<00:16, 2242.00 examples/s][A
Generating validation examples...:  18%|█▊        | 7790/43793 [00:03<00:16, 2246.97 examples/s][A
Generating validation examples...:  18%|█▊        | 8015/43793 [00:03<00:16, 2228.34 examples/s][A
Generating validation examples...:  19%|█▉        | 8242/43793 [00:03<00:15, 2238.80 examples/s][A
Generating validation examples...:  19%|█▉        | 8467/43793 [00:03<00:15, 2239.93 examples/s][A
Generating validation examples...:  20%|█▉        | 8696/43793 [00:03<00:15, 2252.26 examples/s][A
Generating validation examples...:  20%|██        | 8922/43793 [00:04<00:15, 2251.60 examples/s][A
Generating validation examples...:  21%|██        | 9148/43793 [00:04<00:15, 2253.71 examples/s][A
Generating validation examples...:  21%|██▏       | 9375/43793 [00:04<00:15, 2256.97 examples/s][A
Generating validation examples...:  22%|██▏       | 9603/43793 [00:04<00:15, 2262.61 examples/s][A
Generating validation examples...:  22%|██▏       | 9831/43793 [00:04<00:14, 2266.77 examples/s][A
Generating validation examples...:  23%|██▎       | 10058/43793 [00:04<00:14, 2263.23 examples/s][A
Generating validation examples...:  23%|██▎       | 10286/43793 [00:04<00:14, 2266.40 examples/s][A
Generating validation examples...:  24%|██▍       | 10516/43793 [00:04<00:14, 2274.78 examples/s][A
Generating validation examples...:  25%|██▍       | 10744/43793 [00:04<00:14, 2273.72 examples/s][A
Generating validation examples...:  25%|██▌       | 10972/43793 [00:04<00:14, 2268.84 examples/s][A
Generating validation examples...:  26%|██▌       | 11199/43793 [00:05<00:14, 2268.71 examples/s][A
Generating validation examples...:  26%|██▌       | 11429/43793 [00:05<00:14, 2276.00 examples/s][A
Generating validation examples...:  27%|██▋       | 11660/43793 [00:05<00:14, 2283.85 examples/s][A
Generating validation examples...:  27%|██▋       | 11889/43793 [00:05<00:14, 2254.01 examples/s][A
Generating validation examples...:  28%|██▊       | 12115/43793 [00:05<00:14, 2251.05 examples/s][A
Generating validation examples...:  28%|██▊       | 12341/43793 [00:05<00:14, 2173.80 examples/s][A
Generating validation examples...:  29%|██▊       | 12561/43793 [00:05<00:14, 2180.22 examples/s][A
Generating validation examples...:  29%|██▉       | 12787/43793 [00:05<00:14, 2202.16 examples/s][A
Generating validation examples...:  30%|██▉       | 13014/43793 [00:05<00:13, 2219.40 examples/s][A
Generating validation examples...:  30%|███       | 13246/43793 [00:05<00:13, 2248.50 examples/s][A
Generating validation examples...:  31%|███       | 13474/43793 [00:06<00:13, 2257.30 examples/s][A
Generating validation examples...:  31%|███▏      | 13700/43793 [00:06<00:13, 2257.28 examples/s][A
Generating validation examples...:  32%|███▏      | 13932/43793 [00:06<00:13, 2273.16 examples/s][A
Generating validation examples...:  32%|███▏      | 14160/43793 [00:06<00:13, 2271.64 examples/s][A
Generating validation examples...:  33%|███▎      | 14388/43793 [00:06<00:12, 2266.88 examples/s][A
Generating validation examples...:  33%|███▎      | 14615/43793 [00:06<00:12, 2263.38 examples/s][A
Generating validation examples...:  34%|███▍      | 14842/43793 [00:06<00:12, 2252.28 examples/s][A
Generating validation examples...:  34%|███▍      | 15071/43793 [00:06<00:12, 2263.14 examples/s][A
Generating validation examples...:  35%|███▍      | 15300/43793 [00:06<00:12, 2269.30 examples/s][A
Generating validation examples...:  35%|███▌      | 15527/43793 [00:06<00:12, 2262.01 examples/s][A
Generating validation examples...:  36%|███▌      | 15754/43793 [00:07<00:12, 2257.01 examples/s][A
Generating validation examples...:  36%|███▋      | 15981/43793 [00:07<00:12, 2260.44 examples/s][A
Generating validation examples...:  37%|███▋      | 16209/43793 [00:07<00:12, 2264.79 examples/s][A
Generating validation examples...:  38%|███▊      | 16436/43793 [00:07<00:12, 2262.32 examples/s][A
Generating validation examples...:  38%|███▊      | 16663/43793 [00:07<00:12, 2257.04 examples/s][A
Generating validation examples...:  39%|███▊      | 16890/43793 [00:07<00:11, 2260.42 examples/s][A
Generating validation examples...:  39%|███▉      | 17117/43793 [00:07<00:11, 2251.15 examples/s][A
Generating validation examples...:  40%|███▉      | 17343/43793 [00:07<00:11, 2242.76 examples/s][A
Generating validation examples...:  40%|████      | 17568/43793 [00:07<00:11, 2212.22 examples/s][A
Generating validation examples...:  41%|████      | 17790/43793 [00:07<00:11, 2212.77 examples/s][A
Generating validation examples...:  41%|████      | 18012/43793 [00:08<00:11, 2211.86 examples/s][A
Generating validation examples...:  42%|████▏     | 18235/43793 [00:08<00:11, 2214.34 examples/s][A
Generating validation examples...:  42%|████▏     | 18460/43793 [00:08<00:11, 2222.07 examples/s][A
Generating validation examples...:  43%|████▎     | 18684/43793 [00:08<00:11, 2225.76 examples/s][A
Generating validation examples...:  43%|████▎     | 18907/43793 [00:08<00:11, 2224.99 examples/s][A
Generating validation examples...:  44%|████▎     | 19130/43793 [00:08<00:11, 2220.57 examples/s][A
Generating validation examples...:  44%|████▍     | 19353/43793 [00:08<00:11, 2209.04 examples/s][A
Generating validation examples...:  45%|████▍     | 19576/43793 [00:08<00:10, 2214.83 examples/s][A
Generating validation examples...:  45%|████▌     | 19799/43793 [00:08<00:10, 2218.36 examples/s][A
Generating validation examples...:  46%|████▌     | 20022/43793 [00:08<00:10, 2220.16 examples/s][A
Generating validation examples...:  46%|████▌     | 20247/43793 [00:09<00:10, 2227.02 examples/s][A
Generating validation examples...:  47%|████▋     | 20470/43793 [00:09<00:10, 2221.01 examples/s][A
Generating validation examples...:  47%|████▋     | 20698/43793 [00:09<00:10, 2236.71 examples/s][A
Generating validation examples...:  48%|████▊     | 20925/43793 [00:09<00:10, 2246.16 examples/s][A
Generating validation examples...:  48%|████▊     | 21150/43793 [00:09<00:10, 2240.53 examples/s][A
Generating validation examples...:  49%|████▉     | 21375/43793 [00:09<00:10, 2228.79 examples/s][A
Generating validation examples...:  49%|████▉     | 21605/43793 [00:09<00:09, 2249.68 examples/s][A
Generating validation examples...:  50%|████▉     | 21833/43793 [00:09<00:09, 2257.75 examples/s][A
Generating validation examples...:  50%|█████     | 22060/43793 [00:09<00:09, 2261.28 examples/s][A
Generating validation examples...:  51%|█████     | 22290/43793 [00:09<00:09, 2269.98 examples/s][A
Generating validation examples...:  51%|█████▏    | 22519/43793 [00:10<00:09, 2274.47 examples/s][A
Generating validation examples...:  52%|█████▏    | 22747/43793 [00:10<00:09, 2265.44 examples/s][A
Generating validation examples...:  52%|█████▏    | 22974/43793 [00:10<00:09, 2254.44 examples/s][A
Generating validation examples...:  53%|█████▎    | 23201/43793 [00:10<00:09, 2256.91 examples/s][A
Generating validation examples...:  53%|█████▎    | 23427/43793 [00:10<00:09, 2250.92 examples/s][A
Generating validation examples...:  54%|█████▍    | 23653/43793 [00:10<00:08, 2243.57 examples/s][A
Generating validation examples...:  55%|█████▍    | 23878/43793 [00:10<00:08, 2240.81 examples/s][A
Generating validation examples...:  55%|█████▌    | 24111/43793 [00:10<00:08, 2265.10 examples/s][A
Generating validation examples...:  56%|█████▌    | 24339/43793 [00:10<00:08, 2269.31 examples/s][A
Generating validation examples...:  56%|█████▌    | 24566/43793 [00:11<00:09, 2134.33 examples/s][A
Generating validation examples...:  57%|█████▋    | 24788/43793 [00:11<00:09, 2089.36 examples/s][A
Generating validation examples...:  57%|█████▋    | 25009/43793 [00:11<00:08, 2106.08 examples/s][A
Generating validation examples...:  58%|█████▊    | 25233/43793 [00:11<00:08, 2143.30 examples/s][A
Generating validation examples...:  58%|█████▊    | 25455/43793 [00:11<00:08, 2144.74 examples/s][A
Generating validation examples...:  59%|█████▊    | 25677/43793 [00:11<00:08, 2159.78 examples/s][A
Generating validation examples...:  59%|█████▉    | 25897/43793 [00:11<00:08, 2159.62 examples/s][A
Generating validation examples...:  60%|█████▉    | 26118/43793 [00:11<00:08, 2174.39 examples/s][A
Generating validation examples...:  60%|██████    | 26340/43793 [00:11<00:08, 2180.70 examples/s][A
Generating validation examples...:  61%|██████    | 26566/43793 [00:11<00:07, 2203.37 examples/s][A
Generating validation examples...:  61%|██████    | 26790/43793 [00:12<00:07, 2212.89 examples/s][A
Generating validation examples...:  62%|██████▏   | 27017/43793 [00:12<00:07, 2229.06 examples/s][A
Generating validation examples...:  62%|██████▏   | 27248/43793 [00:12<00:07, 2251.80 examples/s][A
Generating validation examples...:  63%|██████▎   | 27474/43793 [00:12<00:07, 2246.96 examples/s][A
Generating validation examples...:  63%|██████▎   | 27699/43793 [00:12<00:07, 2245.94 examples/s][A
Generating validation examples...:  64%|██████▍   | 27926/43793 [00:12<00:07, 2250.97 examples/s][A
Generating validation examples...:  64%|██████▍   | 28152/43793 [00:12<00:07, 2220.03 examples/s][A
Generating validation examples...:  65%|██████▍   | 28375/43793 [00:12<00:06, 2207.45 examples/s][A
Generating validation examples...:  65%|██████▌   | 28596/43793 [00:12<00:06, 2204.41 examples/s][A
Generating validation examples...:  66%|██████▌   | 28817/43793 [00:12<00:06, 2194.53 examples/s][A
Generating validation examples...:  66%|██████▋   | 29041/43793 [00:13<00:06, 2207.65 examples/s][A
Generating validation examples...:  67%|██████▋   | 29271/43793 [00:13<00:06, 2232.04 examples/s][A
Generating validation examples...:  67%|██████▋   | 29495/43793 [00:13<00:06, 2214.62 examples/s][A
Generating validation examples...:  68%|██████▊   | 29720/43793 [00:13<00:06, 2224.40 examples/s][A
Generating validation examples...:  68%|██████▊   | 29946/43793 [00:13<00:06, 2233.92 examples/s][A
Generating validation examples...:  69%|██████▉   | 30172/43793 [00:13<00:06, 2240.38 examples/s][A
Generating validation examples...:  69%|██████▉   | 30398/43793 [00:13<00:05, 2245.73 examples/s][A
Generating validation examples...:  70%|██████▉   | 30623/43793 [00:13<00:05, 2221.49 examples/s][A
Generating validation examples...:  70%|███████   | 30851/43793 [00:13<00:05, 2237.27 examples/s][A
Generating validation examples...:  71%|███████   | 31082/43793 [00:13<00:05, 2256.53 examples/s][A
Generating validation examples...:  71%|███████▏  | 31308/43793 [00:14<00:05, 2255.33 examples/s][A
Generating validation examples...:  72%|███████▏  | 31535/43793 [00:14<00:05, 2259.41 examples/s][A
Generating validation examples...:  73%|███████▎  | 31762/43793 [00:14<00:05, 2261.03 examples/s][A
Generating validation examples...:  73%|███████▎  | 31989/43793 [00:14<00:05, 2252.80 examples/s][A
Generating validation examples...:  74%|███████▎  | 32216/43793 [00:14<00:05, 2257.46 examples/s][A
Generating validation examples...:  74%|███████▍  | 32442/43793 [00:14<00:05, 2256.74 examples/s][A
Generating validation examples...:  75%|███████▍  | 32668/43793 [00:14<00:04, 2256.41 examples/s][A
Generating validation examples...:  75%|███████▌  | 32894/43793 [00:14<00:04, 2232.48 examples/s][A
Generating validation examples...:  76%|███████▌  | 33118/43793 [00:14<00:04, 2228.17 examples/s][A
Generating validation examples...:  76%|███████▌  | 33341/43793 [00:14<00:04, 2221.08 examples/s][A
Generating validation examples...:  77%|███████▋  | 33564/43793 [00:15<00:04, 2220.67 examples/s][A
Generating validation examples...:  77%|███████▋  | 33787/43793 [00:15<00:04, 2196.33 examples/s][A
Generating validation examples...:  78%|███████▊  | 34016/43793 [00:15<00:04, 2222.60 examples/s][A
Generating validation examples...:  78%|███████▊  | 34241/43793 [00:15<00:04, 2229.46 examples/s][A
Generating validation examples...:  79%|███████▊  | 34468/43793 [00:15<00:04, 2239.97 examples/s][A
Generating validation examples...:  79%|███████▉  | 34699/43793 [00:15<00:04, 2259.88 examples/s][A
Generating validation examples...:  80%|███████▉  | 34929/43793 [00:15<00:03, 2271.39 examples/s][A
Generating validation examples...:  80%|████████  | 35157/43793 [00:15<00:03, 2250.79 examples/s][A
Generating validation examples...:  81%|████████  | 35383/43793 [00:15<00:03, 2202.01 examples/s][A
Generating validation examples...:  81%|████████▏ | 35607/43793 [00:15<00:03, 2213.10 examples/s][A
Generating validation examples...:  82%|████████▏ | 35836/43793 [00:16<00:03, 2235.55 examples/s][A
Generating validation examples...:  82%|████████▏ | 36068/43793 [00:16<00:03, 2258.97 examples/s][A
Generating validation examples...:  83%|████████▎ | 36297/43793 [00:16<00:03, 2266.83 examples/s][A
Generating validation examples...:  83%|████████▎ | 36524/43793 [00:16<00:03, 2263.71 examples/s][A
Generating validation examples...:  84%|████████▍ | 36751/43793 [00:16<00:03, 2258.03 examples/s][A
Generating validation examples...:  84%|████████▍ | 36983/43793 [00:16<00:02, 2274.23 examples/s][A
Generating validation examples...:  85%|████████▍ | 37211/43793 [00:16<00:02, 2264.66 examples/s][A
Generating validation examples...:  85%|████████▌ | 37438/43793 [00:16<00:02, 2259.07 examples/s][A
Generating validation examples...:  86%|████████▌ | 37665/43793 [00:16<00:02, 2174.34 examples/s][A
Generating validation examples...:  87%|████████▋ | 37887/43793 [00:17<00:02, 2185.32 examples/s][A
Generating validation examples...:  87%|████████▋ | 38112/43793 [00:17<00:02, 2203.02 examples/s][A
Generating validation examples...:  88%|████████▊ | 38338/43793 [00:17<00:02, 2218.60 examples/s][A
Generating validation examples...:  88%|████████▊ | 38563/43793 [00:17<00:02, 2227.36 examples/s][A
Generating validation examples...:  89%|████████▊ | 38788/43793 [00:17<00:02, 2232.01 examples/s][A
Generating validation examples...:  89%|████████▉ | 39014/43793 [00:17<00:02, 2238.51 examples/s][A
Generating validation examples...:  90%|████████▉ | 39241/43793 [00:17<00:02, 2246.30 examples/s][A
Generating validation examples...:  90%|█████████ | 39469/43793 [00:17<00:01, 2253.97 examples/s][A
Generating validation examples...:  91%|█████████ | 39702/43793 [00:17<00:01, 2276.06 examples/s][A
Generating validation examples...:  91%|█████████ | 39930/43793 [00:17<00:01, 2276.36 examples/s][A
Generating validation examples...:  92%|█████████▏| 40160/43793 [00:18<00:01, 2281.45 examples/s][A
Generating validation examples...:  92%|█████████▏| 40389/43793 [00:18<00:01, 2282.88 examples/s][A
Generating validation examples...:  93%|█████████▎| 40620/43793 [00:18<00:01, 2290.96 examples/s][A
Generating validation examples...:  93%|█████████▎| 40850/43793 [00:18<00:01, 2281.44 examples/s][A
Generating validation examples...:  94%|█████████▍| 41082/43793 [00:18<00:01, 2292.49 examples/s][A
Generating validation examples...:  94%|█████████▍| 41312/43793 [00:18<00:01, 2277.29 examples/s][A
Generating validation examples...:  95%|█████████▍| 41540/43793 [00:18<00:00, 2274.76 examples/s][A
Generating validation examples...:  95%|█████████▌| 41768/43793 [00:18<00:00, 2242.28 examples/s][A
Generating validation examples...:  96%|█████████▌| 41995/43793 [00:18<00:00, 2248.51 examples/s][A
Generating validation examples...:  96%|█████████▋| 42224/43793 [00:18<00:00, 2260.50 examples/s][A
Generating validation examples...:  97%|█████████▋| 42451/43793 [00:19<00:00, 2244.78 examples/s][A
Generating validation examples...:  97%|█████████▋| 42682/43793 [00:19<00:00, 2261.70 examples/s][A
Generating validation examples...:  98%|█████████▊| 42909/43793 [00:19<00:00, 2255.56 examples/s][A
Generating validation examples...:  98%|█████████▊| 43136/43793 [00:19<00:00, 2258.27 examples/s][A
Generating validation examples...:  99%|█████████▉| 43366/43793 [00:19<00:00, 2270.61 examples/s][A
Generating validation examples...: 100%|█████████▉| 43594/43793 [00:19<00:00, 2255.76 examples/s][A
                                                                                                 [A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteVNVHA5/ogbg_molpcba-validation.tfrecord*...:   0%|          | 0/43793 [00:00<?, ? examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteVNVHA5/ogbg_molpcba-validation.tfrecord*...:  24%|██▍       | 10439/43793 [00:00<00:00, 104376.16 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteVNVHA5/ogbg_molpcba-validation.tfrecord*...:  60%|█████▉    | 26156/43793 [00:00<00:00, 135423.29 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteVNVHA5/ogbg_molpcba-validation.tfrecord*...:  96%|█████████▌| 42053/43793 [00:00<00:00, 146171.50 examples/s][A
                                                                                                                                                               [AI0311 03:21:46.926424 139789500200768 writer.py:301] Done writing /root/data/ogbg_molpcba/0.1.3.incompleteVNVHA5/ogbg_molpcba-validation.tfrecord*. Number of examples: 43793 (shards: [43793])
Generating splits...:  67%|██████▋   | 2/3 [02:53<01:15, 75.02s/ splits] 
Generating test examples...:   0%|          | 0/43793 [00:00<?, ? examples/s][A
Generating test examples...:   0%|          | 171/43793 [00:00<00:25, 1705.80 examples/s][A
Generating test examples...:   1%|          | 393/43793 [00:00<00:21, 2006.74 examples/s][A
Generating test examples...:   1%|▏         | 624/43793 [00:00<00:20, 2144.80 examples/s][A
Generating test examples...:   2%|▏         | 850/43793 [00:00<00:19, 2187.57 examples/s][A
Generating test examples...:   2%|▏         | 1077/43793 [00:00<00:19, 2216.68 examples/s][A
Generating test examples...:   3%|▎         | 1304/43793 [00:00<00:19, 2234.37 examples/s][A
Generating test examples...:   3%|▎         | 1528/43793 [00:00<00:18, 2234.44 examples/s][A
Generating test examples...:   4%|▍         | 1759/43793 [00:00<00:18, 2256.34 examples/s][A
Generating test examples...:   5%|▍         | 1987/43793 [00:00<00:18, 2262.92 examples/s][A
Generating test examples...:   5%|▌         | 2217/43793 [00:01<00:18, 2274.08 examples/s][A
Generating test examples...:   6%|▌         | 2446/43793 [00:01<00:18, 2278.59 examples/s][A
Generating test examples...:   6%|▌         | 2674/43793 [00:01<00:18, 2245.11 examples/s][A
Generating test examples...:   7%|▋         | 2906/43793 [00:01<00:18, 2265.88 examples/s][A
Generating test examples...:   7%|▋         | 3133/43793 [00:01<00:17, 2266.68 examples/s][A
Generating test examples...:   8%|▊         | 3360/43793 [00:01<00:17, 2267.14 examples/s][A
Generating test examples...:   8%|▊         | 3589/43793 [00:01<00:17, 2273.47 examples/s][A
Generating test examples...:   9%|▊         | 3817/43793 [00:01<00:17, 2252.96 examples/s][A
Generating test examples...:   9%|▉         | 4043/43793 [00:01<00:17, 2242.71 examples/s][A
Generating test examples...:  10%|▉         | 4271/43793 [00:01<00:17, 2252.92 examples/s][A
Generating test examples...:  10%|█         | 4502/43793 [00:02<00:17, 2269.82 examples/s][A
Generating test examples...:  11%|█         | 4734/43793 [00:02<00:17, 2283.62 examples/s][A
Generating test examples...:  11%|█▏        | 4967/43793 [00:02<00:16, 2296.13 examples/s][A
Generating test examples...:  12%|█▏        | 5200/43793 [00:02<00:17, 2220.68 examples/s][A
Generating test examples...:  12%|█▏        | 5423/43793 [00:02<00:17, 2219.61 examples/s][A
Generating test examples...:  13%|█▎        | 5650/43793 [00:02<00:17, 2233.29 examples/s][A
Generating test examples...:  13%|█▎        | 5880/43793 [00:02<00:16, 2252.08 examples/s][A
Generating test examples...:  14%|█▍        | 6106/43793 [00:02<00:16, 2241.36 examples/s][A
Generating test examples...:  14%|█▍        | 6338/43793 [00:02<00:16, 2262.44 examples/s][A
Generating test examples...:  15%|█▍        | 6565/43793 [00:02<00:16, 2252.46 examples/s][A
Generating test examples...:  16%|█▌        | 6791/43793 [00:03<00:16, 2253.24 examples/s][A
Generating test examples...:  16%|█▌        | 7023/43793 [00:03<00:16, 2271.40 examples/s][A
Generating test examples...:  17%|█▋        | 7251/43793 [00:03<00:16, 2269.46 examples/s][A
Generating test examples...:  17%|█▋        | 7484/43793 [00:03<00:15, 2286.60 examples/s][A
Generating test examples...:  18%|█▊        | 7715/43793 [00:03<00:15, 2293.03 examples/s][A
Generating test examples...:  18%|█▊        | 7945/43793 [00:03<00:15, 2292.01 examples/s][A
Generating test examples...:  19%|█▊        | 8175/43793 [00:03<00:15, 2293.02 examples/s][A
Generating test examples...:  19%|█▉        | 8405/43793 [00:03<00:15, 2273.90 examples/s][A
Generating test examples...:  20%|█▉        | 8634/43793 [00:03<00:15, 2277.10 examples/s][A
Generating test examples...:  20%|██        | 8862/43793 [00:03<00:15, 2274.45 examples/s][A
Generating test examples...:  21%|██        | 9090/43793 [00:04<00:15, 2274.85 examples/s][A
Generating test examples...:  21%|██▏       | 9322/43793 [00:04<00:15, 2286.02 examples/s][A
Generating test examples...:  22%|██▏       | 9552/43793 [00:04<00:14, 2287.94 examples/s][A
Generating test examples...:  22%|██▏       | 9781/43793 [00:04<00:14, 2277.14 examples/s][A
Generating test examples...:  23%|██▎       | 10013/43793 [00:04<00:14, 2288.69 examples/s][A
Generating test examples...:  23%|██▎       | 10243/43793 [00:04<00:14, 2289.95 examples/s][A
Generating test examples...:  24%|██▍       | 10475/43793 [00:04<00:14, 2297.09 examples/s][A
Generating test examples...:  24%|██▍       | 10705/43793 [00:04<00:14, 2295.06 examples/s][A
Generating test examples...:  25%|██▍       | 10938/43793 [00:04<00:14, 2303.94 examples/s][A
Generating test examples...:  26%|██▌       | 11169/43793 [00:04<00:14, 2267.72 examples/s][A
Generating test examples...:  26%|██▌       | 11403/43793 [00:05<00:14, 2289.05 examples/s][A
Generating test examples...:  27%|██▋       | 11633/43793 [00:05<00:14, 2279.35 examples/s][A
Generating test examples...:  27%|██▋       | 11862/43793 [00:05<00:14, 2203.92 examples/s][A
Generating test examples...:  28%|██▊       | 12092/43793 [00:05<00:14, 2231.69 examples/s][A
Generating test examples...:  28%|██▊       | 12322/43793 [00:05<00:13, 2251.27 examples/s][A
Generating test examples...:  29%|██▊       | 12551/43793 [00:05<00:13, 2261.33 examples/s][A
Generating test examples...:  29%|██▉       | 12781/43793 [00:05<00:13, 2271.32 examples/s][A
Generating test examples...:  30%|██▉       | 13009/43793 [00:05<00:13, 2257.98 examples/s][A
Generating test examples...:  30%|███       | 13235/43793 [00:05<00:13, 2249.43 examples/s][A
Generating test examples...:  31%|███       | 13461/43793 [00:05<00:13, 2242.77 examples/s][A
Generating test examples...:  31%|███▏      | 13691/43793 [00:06<00:13, 2258.62 examples/s][A
Generating test examples...:  32%|███▏      | 13923/43793 [00:06<00:13, 2275.50 examples/s][A
Generating test examples...:  32%|███▏      | 14152/43793 [00:06<00:13, 2277.62 examples/s][A
Generating test examples...:  33%|███▎      | 14384/43793 [00:06<00:12, 2287.82 examples/s][A
Generating test examples...:  33%|███▎      | 14613/43793 [00:06<00:12, 2275.58 examples/s][A
Generating test examples...:  34%|███▍      | 14841/43793 [00:06<00:12, 2275.43 examples/s][A
Generating test examples...:  34%|███▍      | 15069/43793 [00:06<00:12, 2270.39 examples/s][A
Generating test examples...:  35%|███▍      | 15297/43793 [00:06<00:12, 2224.63 examples/s][A
Generating test examples...:  35%|███▌      | 15524/43793 [00:06<00:12, 2237.42 examples/s][A
Generating test examples...:  36%|███▌      | 15748/43793 [00:06<00:12, 2228.08 examples/s][A
Generating test examples...:  36%|███▋      | 15977/43793 [00:07<00:12, 2246.11 examples/s][A
Generating test examples...:  37%|███▋      | 16209/43793 [00:07<00:12, 2266.15 examples/s][A
Generating test examples...:  38%|███▊      | 16436/43793 [00:07<00:12, 2261.52 examples/s][A
Generating test examples...:  38%|███▊      | 16664/43793 [00:07<00:11, 2265.30 examples/s][A
Generating test examples...:  39%|███▊      | 16891/43793 [00:07<00:11, 2253.56 examples/s][A
Generating test examples...:  39%|███▉      | 17117/43793 [00:07<00:11, 2254.69 examples/s][A
Generating test examples...:  40%|███▉      | 17343/43793 [00:07<00:11, 2254.36 examples/s][A
Generating test examples...:  40%|████      | 17576/43793 [00:07<00:11, 2276.26 examples/s][A
Generating test examples...:  41%|████      | 17804/43793 [00:07<00:11, 2275.75 examples/s][A
Generating test examples...:  41%|████      | 18032/43793 [00:07<00:11, 2276.81 examples/s][A
Generating test examples...:  42%|████▏     | 18263/43793 [00:08<00:11, 2285.19 examples/s][A
Generating test examples...:  42%|████▏     | 18492/43793 [00:08<00:11, 2277.09 examples/s][A
Generating test examples...:  43%|████▎     | 18720/43793 [00:08<00:11, 2269.16 examples/s][A
Generating test examples...:  43%|████▎     | 18949/43793 [00:08<00:10, 2274.25 examples/s][A
Generating test examples...:  44%|████▍     | 19181/43793 [00:08<00:10, 2287.02 examples/s][A
Generating test examples...:  44%|████▍     | 19410/43793 [00:08<00:11, 2193.68 examples/s][A
Generating test examples...:  45%|████▍     | 19635/43793 [00:08<00:10, 2207.81 examples/s][A
Generating test examples...:  45%|████▌     | 19860/43793 [00:08<00:10, 2218.91 examples/s][A
Generating test examples...:  46%|████▌     | 20094/43793 [00:08<00:10, 2253.42 examples/s][A
Generating test examples...:  46%|████▋     | 20321/43793 [00:09<00:10, 2255.85 examples/s][A
Generating test examples...:  47%|████▋     | 20547/43793 [00:09<00:10, 2254.89 examples/s][A
Generating test examples...:  47%|████▋     | 20776/43793 [00:09<00:10, 2264.22 examples/s][A
Generating test examples...:  48%|████▊     | 21006/43793 [00:09<00:10, 2273.38 examples/s][A
Generating test examples...:  48%|████▊     | 21234/43793 [00:09<00:09, 2264.52 examples/s][A
Generating test examples...:  49%|████▉     | 21466/43793 [00:09<00:09, 2279.51 examples/s][A
Generating test examples...:  50%|████▉     | 21699/43793 [00:09<00:09, 2291.86 examples/s][A
Generating test examples...:  50%|█████     | 21932/43793 [00:09<00:09, 2301.35 examples/s][A
Generating test examples...:  51%|█████     | 22163/43793 [00:09<00:09, 2292.92 examples/s][A
Generating test examples...:  51%|█████     | 22393/43793 [00:09<00:09, 2294.07 examples/s][A
Generating test examples...:  52%|█████▏    | 22623/43793 [00:10<00:09, 2290.49 examples/s][A
Generating test examples...:  52%|█████▏    | 22853/43793 [00:10<00:09, 2288.95 examples/s][A
Generating test examples...:  53%|█████▎    | 23086/43793 [00:10<00:09, 2298.81 examples/s][A
Generating test examples...:  53%|█████▎    | 23316/43793 [00:10<00:08, 2296.68 examples/s][A
Generating test examples...:  54%|█████▍    | 23546/43793 [00:10<00:08, 2288.85 examples/s][A
Generating test examples...:  54%|█████▍    | 23779/43793 [00:10<00:08, 2299.24 examples/s][A
Generating test examples...:  55%|█████▍    | 24009/43793 [00:10<00:08, 2296.74 examples/s][A
Generating test examples...:  55%|█████▌    | 24239/43793 [00:10<00:08, 2278.00 examples/s][A
Generating test examples...:  56%|█████▌    | 24474/43793 [00:10<00:08, 2298.80 examples/s][A
Generating test examples...:  56%|█████▋    | 24708/43793 [00:10<00:08, 2310.65 examples/s][A
Generating test examples...:  57%|█████▋    | 24940/43793 [00:11<00:08, 2308.55 examples/s][A
Generating test examples...:  57%|█████▋    | 25171/43793 [00:11<00:08, 2304.56 examples/s][A
Generating test examples...:  58%|█████▊    | 25402/43793 [00:11<00:07, 2303.64 examples/s][A
Generating test examples...:  59%|█████▊    | 25633/43793 [00:11<00:08, 2202.22 examples/s][A
Generating test examples...:  59%|█████▉    | 25859/43793 [00:11<00:08, 2218.46 examples/s][A
Generating test examples...:  60%|█████▉    | 26090/43793 [00:11<00:07, 2242.46 examples/s][A
Generating test examples...:  60%|██████    | 26321/43793 [00:11<00:07, 2260.67 examples/s][A
Generating test examples...:  61%|██████    | 26548/43793 [00:11<00:07, 2245.55 examples/s][A
Generating test examples...:  61%|██████    | 26773/43793 [00:11<00:07, 2226.95 examples/s][A
Generating test examples...:  62%|██████▏   | 26996/43793 [00:11<00:07, 2208.16 examples/s][A
Generating test examples...:  62%|██████▏   | 27217/43793 [00:12<00:07, 2198.17 examples/s][A
Generating test examples...:  63%|██████▎   | 27437/43793 [00:12<00:07, 2190.42 examples/s][A
Generating test examples...:  63%|██████▎   | 27657/43793 [00:12<00:07, 2183.85 examples/s][A
Generating test examples...:  64%|██████▎   | 27876/43793 [00:12<00:07, 2183.29 examples/s][A
Generating test examples...:  64%|██████▍   | 28095/43793 [00:12<00:07, 2183.91 examples/s][A
Generating test examples...:  65%|██████▍   | 28324/43793 [00:12<00:06, 2214.05 examples/s][A
Generating test examples...:  65%|██████▌   | 28548/43793 [00:12<00:06, 2221.15 examples/s][A
Generating test examples...:  66%|██████▌   | 28771/43793 [00:12<00:06, 2210.73 examples/s][A
Generating test examples...:  66%|██████▌   | 28999/43793 [00:12<00:06, 2230.96 examples/s][A
Generating test examples...:  67%|██████▋   | 29226/43793 [00:12<00:06, 2240.50 examples/s][A
Generating test examples...:  67%|██████▋   | 29453/43793 [00:13<00:06, 2248.92 examples/s][A
Generating test examples...:  68%|██████▊   | 29681/43793 [00:13<00:06, 2255.86 examples/s][A
Generating test examples...:  68%|██████▊   | 29907/43793 [00:13<00:06, 2252.33 examples/s][A
Generating test examples...:  69%|██████▉   | 30137/43793 [00:13<00:06, 2264.80 examples/s][A
Generating test examples...:  69%|██████▉   | 30364/43793 [00:13<00:05, 2261.94 examples/s][A
Generating test examples...:  70%|██████▉   | 30592/43793 [00:13<00:05, 2265.60 examples/s][A
Generating test examples...:  70%|███████   | 30821/43793 [00:13<00:05, 2271.71 examples/s][A
Generating test examples...:  71%|███████   | 31049/43793 [00:13<00:05, 2259.34 examples/s][A
Generating test examples...:  71%|███████▏  | 31275/43793 [00:13<00:05, 2247.96 examples/s][A
Generating test examples...:  72%|███████▏  | 31500/43793 [00:13<00:05, 2243.38 examples/s][A
Generating test examples...:  72%|███████▏  | 31725/43793 [00:14<00:05, 2242.65 examples/s][A
Generating test examples...:  73%|███████▎  | 31952/43793 [00:14<00:05, 2248.76 examples/s][A
Generating test examples...:  73%|███████▎  | 32185/43793 [00:14<00:05, 2270.99 examples/s][A
Generating test examples...:  74%|███████▍  | 32413/43793 [00:14<00:05, 2246.02 examples/s][A
Generating test examples...:  75%|███████▍  | 32638/43793 [00:14<00:04, 2235.20 examples/s][A
Generating test examples...:  75%|███████▌  | 32867/43793 [00:14<00:04, 2248.70 examples/s][A
Generating test examples...:  76%|███████▌  | 33092/43793 [00:14<00:04, 2214.69 examples/s][A
Generating test examples...:  76%|███████▌  | 33314/43793 [00:14<00:04, 2174.94 examples/s][A
Generating test examples...:  77%|███████▋  | 33532/43793 [00:14<00:04, 2160.78 examples/s][A
Generating test examples...:  77%|███████▋  | 33751/43793 [00:14<00:04, 2168.74 examples/s][A
Generating test examples...:  78%|███████▊  | 33976/43793 [00:15<00:04, 2192.59 examples/s][A
Generating test examples...:  78%|███████▊  | 34196/43793 [00:15<00:04, 2185.47 examples/s][A
Generating test examples...:  79%|███████▊  | 34427/43793 [00:15<00:04, 2220.93 examples/s][A
Generating test examples...:  79%|███████▉  | 34662/43793 [00:15<00:04, 2258.80 examples/s][A
Generating test examples...:  80%|███████▉  | 34888/43793 [00:15<00:03, 2255.12 examples/s][A
Generating test examples...:  80%|████████  | 35118/43793 [00:15<00:03, 2267.03 examples/s][A
Generating test examples...:  81%|████████  | 35350/43793 [00:15<00:03, 2281.78 examples/s][A
Generating test examples...:  81%|████████  | 35579/43793 [00:15<00:03, 2281.85 examples/s][A
Generating test examples...:  82%|████████▏ | 35808/43793 [00:15<00:03, 2275.41 examples/s][A
Generating test examples...:  82%|████████▏ | 36038/43793 [00:15<00:03, 2280.67 examples/s][A
Generating test examples...:  83%|████████▎ | 36269/43793 [00:16<00:03, 2287.12 examples/s][A
Generating test examples...:  83%|████████▎ | 36498/43793 [00:16<00:03, 2283.49 examples/s][A
Generating test examples...:  84%|████████▍ | 36730/43793 [00:16<00:03, 2293.67 examples/s][A
Generating test examples...:  84%|████████▍ | 36960/43793 [00:16<00:03, 2180.72 examples/s][A
Generating test examples...:  85%|████████▍ | 37186/43793 [00:16<00:02, 2203.11 examples/s][A
Generating test examples...:  85%|████████▌ | 37419/43793 [00:16<00:02, 2240.04 examples/s][A
Generating test examples...:  86%|████████▌ | 37644/43793 [00:16<00:02, 2240.43 examples/s][A
Generating test examples...:  86%|████████▋ | 37874/43793 [00:16<00:02, 2257.94 examples/s][A
Generating test examples...:  87%|████████▋ | 38106/43793 [00:16<00:02, 2275.26 examples/s][A
Generating test examples...:  88%|████████▊ | 38336/43793 [00:17<00:02, 2280.47 examples/s][A
Generating test examples...:  88%|████████▊ | 38565/43793 [00:17<00:02, 2280.46 examples/s][A
Generating test examples...:  89%|████████▊ | 38794/43793 [00:17<00:02, 2282.93 examples/s][A
Generating test examples...:  89%|████████▉ | 39023/43793 [00:17<00:02, 2264.31 examples/s][A
Generating test examples...:  90%|████████▉ | 39254/43793 [00:17<00:01, 2277.60 examples/s][A
Generating test examples...:  90%|█████████ | 39482/43793 [00:17<00:01, 2271.95 examples/s][A
Generating test examples...:  91%|█████████ | 39719/43793 [00:17<00:01, 2300.83 examples/s][A
Generating test examples...:  91%|█████████ | 39950/43793 [00:17<00:01, 2278.76 examples/s][A
Generating test examples...:  92%|█████████▏| 40178/43793 [00:17<00:01, 2267.76 examples/s][A
Generating test examples...:  92%|█████████▏| 40405/43793 [00:17<00:01, 2265.47 examples/s][A
Generating test examples...:  93%|█████████▎| 40632/43793 [00:18<00:01, 2253.95 examples/s][A
Generating test examples...:  93%|█████████▎| 40860/43793 [00:18<00:01, 2260.20 examples/s][A
Generating test examples...:  94%|█████████▍| 41087/43793 [00:18<00:01, 2255.70 examples/s][A
Generating test examples...:  94%|█████████▍| 41314/43793 [00:18<00:01, 2257.14 examples/s][A
Generating test examples...:  95%|█████████▍| 41540/43793 [00:18<00:01, 2229.75 examples/s][A
Generating test examples...:  95%|█████████▌| 41766/43793 [00:18<00:00, 2238.32 examples/s][A
Generating test examples...:  96%|█████████▌| 41995/43793 [00:18<00:00, 2252.20 examples/s][A
Generating test examples...:  96%|█████████▋| 42221/43793 [00:18<00:00, 2240.30 examples/s][A
Generating test examples...:  97%|█████████▋| 42454/43793 [00:18<00:00, 2266.18 examples/s][A
Generating test examples...:  97%|█████████▋| 42683/43793 [00:18<00:00, 2273.20 examples/s][A
Generating test examples...:  98%|█████████▊| 42911/43793 [00:19<00:00, 2268.69 examples/s][A
Generating test examples...:  99%|█████████▊| 43138/43793 [00:19<00:00, 2263.55 examples/s][A
Generating test examples...:  99%|█████████▉| 43365/43793 [00:19<00:00, 2245.54 examples/s][A
Generating test examples...: 100%|█████████▉| 43591/43793 [00:19<00:00, 2247.93 examples/s][A
                                                                                           [A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteVNVHA5/ogbg_molpcba-test.tfrecord*...:   0%|          | 0/43793 [00:00<?, ? examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteVNVHA5/ogbg_molpcba-test.tfrecord*...:  24%|██▎       | 10385/43793 [00:00<00:00, 103841.93 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteVNVHA5/ogbg_molpcba-test.tfrecord*...:  60%|█████▉    | 26102/43793 [00:00<00:00, 135203.82 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteVNVHA5/ogbg_molpcba-test.tfrecord*...:  92%|█████████▏| 40083/43793 [00:00<00:00, 137302.13 examples/s][A
                                                                                                                                                         [AI0311 03:22:06.702039 139789500200768 writer.py:301] Done writing /root/data/ogbg_molpcba/0.1.3.incompleteVNVHA5/ogbg_molpcba-test.tfrecord*. Number of examples: 43793 (shards: [43793])
Generating splits...: 100%|██████████| 3/3 [03:13<00:00, 49.80s/ splits]                                                                        I0311 03:22:06.787315 139789500200768 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split test, from /root/data/ogbg_molpcba/0.1.3
I0311 03:23:41.286519 139789500200768 submission_runner.py:411] Time since start: 537.11s, 	Step: 1, 	{'train/accuracy': 0.47024616599082947, 'train/loss': 0.7943987846374512, 'train/mean_average_precision': 0.021934454010009253, 'validation/accuracy': 0.47222936153411865, 'validation/loss': 0.7926825284957886, 'validation/mean_average_precision': 0.024972205256454275, 'validation/num_examples': 43793, 'test/accuracy': 0.4722304046154022, 'test/loss': 0.7914055585861206, 'test/mean_average_precision': 0.027065315125327314, 'test/num_examples': 43793, 'score': 18.616023302078247, 'total_duration': 537.1137669086456, 'accumulated_submission_time': 18.616023302078247, 'accumulated_eval_time': 518.4977009296417, 'accumulated_logging_time': 0}
I0311 03:23:41.304275 139620668196608 logging_writer.py:48] [1] accumulated_eval_time=518.497701, accumulated_logging_time=0, accumulated_submission_time=18.616023, global_step=1, preemption_count=0, score=18.616023, test/accuracy=0.472230, test/loss=0.791406, test/mean_average_precision=0.027065, test/num_examples=43793, total_duration=537.113767, train/accuracy=0.470246, train/loss=0.794399, train/mean_average_precision=0.021934, validation/accuracy=0.472229, validation/loss=0.792683, validation/mean_average_precision=0.024972, validation/num_examples=43793
I0311 03:24:12.134058 139621870597888 logging_writer.py:48] [100] global_step=100, grad_norm=0.3078884482383728, loss=0.27934712171554565
I0311 03:24:43.405768 139620668196608 logging_writer.py:48] [200] global_step=200, grad_norm=0.09656967967748642, loss=0.11043957620859146
I0311 03:25:14.823390 139621870597888 logging_writer.py:48] [300] global_step=300, grad_norm=0.030756168067455292, loss=0.06730964779853821
I0311 03:25:45.894483 139620668196608 logging_writer.py:48] [400] global_step=400, grad_norm=0.015522286295890808, loss=0.05785487964749336
I0311 03:26:16.769391 139621870597888 logging_writer.py:48] [500] global_step=500, grad_norm=0.03782977536320686, loss=0.05768154188990593
I0311 03:26:48.163066 139620668196608 logging_writer.py:48] [600] global_step=600, grad_norm=0.025269072502851486, loss=0.052063580602407455
I0311 03:27:18.968253 139621870597888 logging_writer.py:48] [700] global_step=700, grad_norm=0.041985251009464264, loss=0.050622545182704926
I0311 03:27:41.542163 139789500200768 spec.py:321] Evaluating on the training split.
I0311 03:29:30.536491 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 03:29:33.464037 139789500200768 spec.py:349] Evaluating on the test split.
I0311 03:29:36.328969 139789500200768 submission_runner.py:411] Time since start: 892.16s, 	Step: 775, 	{'train/accuracy': 0.9868525862693787, 'train/loss': 0.05085231736302376, 'train/mean_average_precision': 0.05798055082425799, 'validation/accuracy': 0.9842725992202759, 'validation/loss': 0.05991297587752342, 'validation/mean_average_precision': 0.05585357709415071, 'validation/num_examples': 43793, 'test/accuracy': 0.9832780957221985, 'test/loss': 0.06298837810754776, 'test/mean_average_precision': 0.05773436903943875, 'test/num_examples': 43793, 'score': 258.824720621109, 'total_duration': 892.1560852527618, 'accumulated_submission_time': 258.824720621109, 'accumulated_eval_time': 633.2843382358551, 'accumulated_logging_time': 0.028377771377563477}
I0311 03:29:36.346114 139622146934528 logging_writer.py:48] [775] accumulated_eval_time=633.284338, accumulated_logging_time=0.028378, accumulated_submission_time=258.824721, global_step=775, preemption_count=0, score=258.824721, test/accuracy=0.983278, test/loss=0.062988, test/mean_average_precision=0.057734, test/num_examples=43793, total_duration=892.156085, train/accuracy=0.986853, train/loss=0.050852, train/mean_average_precision=0.057981, validation/accuracy=0.984273, validation/loss=0.059913, validation/mean_average_precision=0.055854, validation/num_examples=43793
I0311 03:29:44.238864 139622155327232 logging_writer.py:48] [800] global_step=800, grad_norm=0.02188405580818653, loss=0.05359134078025818
I0311 03:30:14.548280 139622146934528 logging_writer.py:48] [900] global_step=900, grad_norm=0.02159855142235756, loss=0.047255877405405045
I0311 03:30:45.138523 139622155327232 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.021191248670220375, loss=0.051142167299985886
I0311 03:31:15.571910 139622146934528 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.0296014416962862, loss=0.049905795603990555
I0311 03:31:46.495327 139622155327232 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.036168232560157776, loss=0.04697285592556
I0311 03:32:17.201309 139622146934528 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.032942600548267365, loss=0.048868510872125626
I0311 03:32:48.116906 139622155327232 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.02215576171875, loss=0.04667961597442627
I0311 03:33:18.863767 139622146934528 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.03232578933238983, loss=0.04492184519767761
I0311 03:33:36.608546 139789500200768 spec.py:321] Evaluating on the training split.
I0311 03:35:26.908980 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 03:35:29.956310 139789500200768 spec.py:349] Evaluating on the test split.
I0311 03:35:32.958642 139789500200768 submission_runner.py:411] Time since start: 1248.79s, 	Step: 1559, 	{'train/accuracy': 0.987420916557312, 'train/loss': 0.04602539539337158, 'train/mean_average_precision': 0.11023784264345188, 'validation/accuracy': 0.9847450852394104, 'validation/loss': 0.05513370782136917, 'validation/mean_average_precision': 0.11082575545975201, 'validation/num_examples': 43793, 'test/accuracy': 0.983700156211853, 'test/loss': 0.05848104506731033, 'test/mean_average_precision': 0.10550865069542789, 'test/num_examples': 43793, 'score': 499.05750465393066, 'total_duration': 1248.7858963012695, 'accumulated_submission_time': 499.05750465393066, 'accumulated_eval_time': 749.6343984603882, 'accumulated_logging_time': 0.056328535079956055}
I0311 03:35:32.973484 139621801113344 logging_writer.py:48] [1559] accumulated_eval_time=749.634398, accumulated_logging_time=0.056329, accumulated_submission_time=499.057505, global_step=1559, preemption_count=0, score=499.057505, test/accuracy=0.983700, test/loss=0.058481, test/mean_average_precision=0.105509, test/num_examples=43793, total_duration=1248.785896, train/accuracy=0.987421, train/loss=0.046025, train/mean_average_precision=0.110238, validation/accuracy=0.984745, validation/loss=0.055134, validation/mean_average_precision=0.110826, validation/num_examples=43793
I0311 03:35:45.912900 139622048081664 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.014416244812309742, loss=0.04279879480600357
I0311 03:36:17.039835 139621801113344 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.01997256651520729, loss=0.04879666492342949
I0311 03:36:47.888922 139622048081664 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.014934474602341652, loss=0.041791483759880066
I0311 03:37:18.944715 139621801113344 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.02194252423942089, loss=0.04594054073095322
I0311 03:37:50.243731 139622048081664 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.022137364372611046, loss=0.048723168671131134
I0311 03:38:21.098326 139621801113344 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.019521798938512802, loss=0.04847294092178345
I0311 03:38:51.781797 139622048081664 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.010599609464406967, loss=0.041090287268161774
I0311 03:39:22.265742 139621801113344 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.02020532637834549, loss=0.04424012079834938
I0311 03:39:33.029988 139789500200768 spec.py:321] Evaluating on the training split.
I0311 03:41:26.344950 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 03:41:29.362740 139789500200768 spec.py:349] Evaluating on the test split.
I0311 03:41:32.289397 139789500200768 submission_runner.py:411] Time since start: 1608.12s, 	Step: 2336, 	{'train/accuracy': 0.9876879453659058, 'train/loss': 0.044227346777915955, 'train/mean_average_precision': 0.13989243539821986, 'validation/accuracy': 0.985008955001831, 'validation/loss': 0.053349219262599945, 'validation/mean_average_precision': 0.13552906441336143, 'validation/num_examples': 43793, 'test/accuracy': 0.9839503169059753, 'test/loss': 0.05677831918001175, 'test/mean_average_precision': 0.13096821466968986, 'test/num_examples': 43793, 'score': 739.084522485733, 'total_duration': 1608.116632938385, 'accumulated_submission_time': 739.084522485733, 'accumulated_eval_time': 868.8937499523163, 'accumulated_logging_time': 0.0814664363861084}
I0311 03:41:32.304063 139622146934528 logging_writer.py:48] [2336] accumulated_eval_time=868.893750, accumulated_logging_time=0.081466, accumulated_submission_time=739.084522, global_step=2336, preemption_count=0, score=739.084522, test/accuracy=0.983950, test/loss=0.056778, test/mean_average_precision=0.130968, test/num_examples=43793, total_duration=1608.116633, train/accuracy=0.987688, train/loss=0.044227, train/mean_average_precision=0.139892, validation/accuracy=0.985009, validation/loss=0.053349, validation/mean_average_precision=0.135529, validation/num_examples=43793
I0311 03:41:52.862108 139622155327232 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.012645607814192772, loss=0.044106625020504
I0311 03:42:23.744404 139622146934528 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.010583815164864063, loss=0.043286699801683426
I0311 03:42:54.573137 139622155327232 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.018204404041171074, loss=0.04000943526625633
I0311 03:43:25.195788 139622146934528 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.010222379118204117, loss=0.0381079725921154
I0311 03:43:55.867318 139622155327232 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.014100455678999424, loss=0.04297007992863655
I0311 03:44:26.606159 139622146934528 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.015874797478318214, loss=0.041063617914915085
I0311 03:44:57.291668 139622155327232 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.017260925844311714, loss=0.043759237974882126
I0311 03:45:27.968312 139622146934528 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.012613797560334206, loss=0.04119733348488808
I0311 03:45:32.576488 139789500200768 spec.py:321] Evaluating on the training split.
I0311 03:47:26.598037 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 03:47:29.661985 139789500200768 spec.py:349] Evaluating on the test split.
I0311 03:47:32.609824 139789500200768 submission_runner.py:411] Time since start: 1968.44s, 	Step: 3116, 	{'train/accuracy': 0.9880595207214355, 'train/loss': 0.041854601353406906, 'train/mean_average_precision': 0.1689057449872897, 'validation/accuracy': 0.9852123260498047, 'validation/loss': 0.05064059793949127, 'validation/mean_average_precision': 0.15374830032629236, 'validation/num_examples': 43793, 'test/accuracy': 0.9842274785041809, 'test/loss': 0.05347682535648346, 'test/mean_average_precision': 0.15432886069503893, 'test/num_examples': 43793, 'score': 979.3258407115936, 'total_duration': 1968.4370694160461, 'accumulated_submission_time': 979.3258407115936, 'accumulated_eval_time': 988.9270553588867, 'accumulated_logging_time': 0.10827064514160156}
I0311 03:47:32.624855 139621809506048 logging_writer.py:48] [3116] accumulated_eval_time=988.927055, accumulated_logging_time=0.108271, accumulated_submission_time=979.325841, global_step=3116, preemption_count=0, score=979.325841, test/accuracy=0.984227, test/loss=0.053477, test/mean_average_precision=0.154329, test/num_examples=43793, total_duration=1968.437069, train/accuracy=0.988060, train/loss=0.041855, train/mean_average_precision=0.168906, validation/accuracy=0.985212, validation/loss=0.050641, validation/mean_average_precision=0.153748, validation/num_examples=43793
I0311 03:47:58.646781 139621870597888 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.032688263803720474, loss=0.043305981904268265
I0311 03:48:29.281586 139621809506048 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.020250476896762848, loss=0.04084211587905884
I0311 03:49:00.124976 139621870597888 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.013885729014873505, loss=0.04137987643480301
I0311 03:49:31.050164 139621809506048 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.015520627610385418, loss=0.04366542026400566
I0311 03:50:01.682268 139621870597888 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.016135042533278465, loss=0.04562946781516075
I0311 03:50:32.687255 139621809506048 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.012693610042333603, loss=0.047731589525938034
I0311 03:51:03.760707 139621870597888 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.013772018253803253, loss=0.04249744117259979
I0311 03:51:32.912894 139789500200768 spec.py:321] Evaluating on the training split.
I0311 03:53:27.567152 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 03:53:30.580128 139789500200768 spec.py:349] Evaluating on the test split.
I0311 03:53:33.503881 139789500200768 submission_runner.py:411] Time since start: 2329.33s, 	Step: 3895, 	{'train/accuracy': 0.9883913993835449, 'train/loss': 0.03992282599210739, 'train/mean_average_precision': 0.20093586195102411, 'validation/accuracy': 0.9854932427406311, 'validation/loss': 0.049285680055618286, 'validation/mean_average_precision': 0.1780816860356048, 'validation/num_examples': 43793, 'test/accuracy': 0.9845648407936096, 'test/loss': 0.0519956611096859, 'test/mean_average_precision': 0.17284906810367456, 'test/num_examples': 43793, 'score': 1219.583377122879, 'total_duration': 2329.331063270569, 'accumulated_submission_time': 1219.583377122879, 'accumulated_eval_time': 1109.51793384552, 'accumulated_logging_time': 0.1344447135925293}
I0311 03:53:33.519113 139622048081664 logging_writer.py:48] [3895] accumulated_eval_time=1109.517934, accumulated_logging_time=0.134445, accumulated_submission_time=1219.583377, global_step=3895, preemption_count=0, score=1219.583377, test/accuracy=0.984565, test/loss=0.051996, test/mean_average_precision=0.172849, test/num_examples=43793, total_duration=2329.331063, train/accuracy=0.988391, train/loss=0.039923, train/mean_average_precision=0.200936, validation/accuracy=0.985493, validation/loss=0.049286, validation/mean_average_precision=0.178082, validation/num_examples=43793
I0311 03:53:35.373346 139622155327232 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.01686202920973301, loss=0.03521553799510002
I0311 03:54:06.353674 139622048081664 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.012475978583097458, loss=0.03662366047501564
I0311 03:54:37.298322 139622155327232 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.012740670703351498, loss=0.04113024100661278
I0311 03:55:08.694280 139622048081664 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.020528286695480347, loss=0.04022962972521782
I0311 03:55:39.689454 139622155327232 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.017358645796775818, loss=0.04404779151082039
I0311 03:56:10.705519 139622048081664 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.013162706047296524, loss=0.03937355801463127
I0311 03:56:41.434765 139622155327232 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.012197352945804596, loss=0.03552427142858505
I0311 03:57:12.501824 139622048081664 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.013195283710956573, loss=0.03850522264838219
I0311 03:57:33.725133 139789500200768 spec.py:321] Evaluating on the training split.
I0311 03:59:28.536324 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 03:59:31.491821 139789500200768 spec.py:349] Evaluating on the test split.
I0311 03:59:34.417889 139789500200768 submission_runner.py:411] Time since start: 2690.25s, 	Step: 4669, 	{'train/accuracy': 0.9887118339538574, 'train/loss': 0.03858122602105141, 'train/mean_average_precision': 0.22564615878118954, 'validation/accuracy': 0.9857218265533447, 'validation/loss': 0.047950953245162964, 'validation/mean_average_precision': 0.1956819743386989, 'validation/num_examples': 43793, 'test/accuracy': 0.9848319292068481, 'test/loss': 0.05060854181647301, 'test/mean_average_precision': 0.19379429790271613, 'test/num_examples': 43793, 'score': 1459.7593297958374, 'total_duration': 2690.2451293468475, 'accumulated_submission_time': 1459.7593297958374, 'accumulated_eval_time': 1230.2106416225433, 'accumulated_logging_time': 0.1602764129638672}
I0311 03:59:34.433288 139628679296768 logging_writer.py:48] [4669] accumulated_eval_time=1230.210642, accumulated_logging_time=0.160276, accumulated_submission_time=1459.759330, global_step=4669, preemption_count=0, score=1459.759330, test/accuracy=0.984832, test/loss=0.050609, test/mean_average_precision=0.193794, test/num_examples=43793, total_duration=2690.245129, train/accuracy=0.988712, train/loss=0.038581, train/mean_average_precision=0.225646, validation/accuracy=0.985722, validation/loss=0.047951, validation/mean_average_precision=0.195682, validation/num_examples=43793
I0311 03:59:44.458260 139727860639488 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.014690771698951721, loss=0.036702562123537064
I0311 04:00:15.702112 139628679296768 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.009823843836784363, loss=0.03469915688037872
I0311 04:00:46.979627 139727860639488 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.011351492255926132, loss=0.03696366399526596
I0311 04:01:17.786240 139628679296768 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.010516407899558544, loss=0.04175000637769699
I0311 04:01:49.101904 139727860639488 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.013677626848220825, loss=0.041341982781887054
I0311 04:02:20.503234 139628679296768 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.011904696002602577, loss=0.03827822580933571
I0311 04:02:51.938558 139727860639488 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.01394274178892374, loss=0.040699511766433716
I0311 04:03:23.074113 139628679296768 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.01798577420413494, loss=0.04094959795475006
I0311 04:03:34.535573 139789500200768 spec.py:321] Evaluating on the training split.
I0311 04:05:31.281105 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 04:05:34.293353 139789500200768 spec.py:349] Evaluating on the test split.
I0311 04:05:37.296981 139789500200768 submission_runner.py:411] Time since start: 3053.12s, 	Step: 5437, 	{'train/accuracy': 0.9889028668403625, 'train/loss': 0.037836384028196335, 'train/mean_average_precision': 0.24131117112429018, 'validation/accuracy': 0.9858987927436829, 'validation/loss': 0.0474938340485096, 'validation/mean_average_precision': 0.20850823042146568, 'validation/num_examples': 43793, 'test/accuracy': 0.9849759340286255, 'test/loss': 0.050428878515958786, 'test/mean_average_precision': 0.2005436096187317, 'test/num_examples': 43793, 'score': 1699.8314542770386, 'total_duration': 3053.124225139618, 'accumulated_submission_time': 1699.8314542770386, 'accumulated_eval_time': 1352.9720075130463, 'accumulated_logging_time': 0.18658971786499023}
I0311 04:05:37.312334 139628670904064 logging_writer.py:48] [5437] accumulated_eval_time=1352.972008, accumulated_logging_time=0.186590, accumulated_submission_time=1699.831454, global_step=5437, preemption_count=0, score=1699.831454, test/accuracy=0.984976, test/loss=0.050429, test/mean_average_precision=0.200544, test/num_examples=43793, total_duration=3053.124225, train/accuracy=0.988903, train/loss=0.037836, train/mean_average_precision=0.241311, validation/accuracy=0.985899, validation/loss=0.047494, validation/mean_average_precision=0.208508, validation/num_examples=43793
I0311 04:05:57.432869 139727869032192 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.0184171125292778, loss=0.04136727377772331
I0311 04:06:29.067107 139628670904064 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.017799751833081245, loss=0.03874426707625389
I0311 04:07:00.454216 139727869032192 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.02098291553556919, loss=0.03479815274477005
I0311 04:07:32.371155 139628670904064 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.014699405990540981, loss=0.037633806467056274
I0311 04:08:06.141468 139727869032192 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.015847571194171906, loss=0.03898179158568382
I0311 04:08:39.252506 139628670904064 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.012245576828718185, loss=0.03282809257507324
I0311 04:09:12.583995 139727869032192 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.01500390749424696, loss=0.03916935622692108
I0311 04:09:37.426290 139789500200768 spec.py:321] Evaluating on the training split.
I0311 04:11:37.967415 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 04:11:40.965734 139789500200768 spec.py:349] Evaluating on the test split.
I0311 04:11:43.916396 139789500200768 submission_runner.py:411] Time since start: 3419.74s, 	Step: 6176, 	{'train/accuracy': 0.9892878532409668, 'train/loss': 0.03627763316035271, 'train/mean_average_precision': 0.2747439853818164, 'validation/accuracy': 0.9860436916351318, 'validation/loss': 0.04658976197242737, 'validation/mean_average_precision': 0.21270261908527863, 'validation/num_examples': 43793, 'test/accuracy': 0.9852021336555481, 'test/loss': 0.04921684041619301, 'test/mean_average_precision': 0.21136306353536272, 'test/num_examples': 43793, 'score': 1939.9151394367218, 'total_duration': 3419.7436504364014, 'accumulated_submission_time': 1939.9151394367218, 'accumulated_eval_time': 1479.4620878696442, 'accumulated_logging_time': 0.21283316612243652}
I0311 04:11:43.932401 139622180505344 logging_writer.py:48] [6176] accumulated_eval_time=1479.462088, accumulated_logging_time=0.212833, accumulated_submission_time=1939.915139, global_step=6176, preemption_count=0, score=1939.915139, test/accuracy=0.985202, test/loss=0.049217, test/mean_average_precision=0.211363, test/num_examples=43793, total_duration=3419.743650, train/accuracy=0.989288, train/loss=0.036278, train/mean_average_precision=0.274744, validation/accuracy=0.986044, validation/loss=0.046590, validation/mean_average_precision=0.212703, validation/num_examples=43793
I0311 04:11:52.349730 139628679296768 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.01843773014843464, loss=0.039013832807540894
I0311 04:12:23.796631 139622180505344 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.020777657628059387, loss=0.03738570958375931
I0311 04:12:55.450546 139628679296768 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.012477493844926357, loss=0.03893212229013443
I0311 04:13:27.192695 139622180505344 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.01263970322906971, loss=0.04114798456430435
I0311 04:13:58.581438 139628679296768 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.014895269647240639, loss=0.037675321102142334
I0311 04:14:29.727404 139622180505344 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.018494417890906334, loss=0.03727196156978607
I0311 04:15:00.837161 139628679296768 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.02141030877828598, loss=0.04122822359204292
I0311 04:15:31.911491 139622180505344 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.01231329794973135, loss=0.033608805388212204
I0311 04:15:44.157211 139789500200768 spec.py:321] Evaluating on the training split.
I0311 04:17:43.656741 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 04:17:46.729390 139789500200768 spec.py:349] Evaluating on the test split.
I0311 04:17:49.779413 139789500200768 submission_runner.py:411] Time since start: 3785.61s, 	Step: 6941, 	{'train/accuracy': 0.9895052313804626, 'train/loss': 0.03547140210866928, 'train/mean_average_precision': 0.2851274273494, 'validation/accuracy': 0.9862803816795349, 'validation/loss': 0.046073075383901596, 'validation/mean_average_precision': 0.22775998970112277, 'validation/num_examples': 43793, 'test/accuracy': 0.9854455590248108, 'test/loss': 0.0487179309129715, 'test/mean_average_precision': 0.22638465450753256, 'test/num_examples': 43793, 'score': 2180.108886241913, 'total_duration': 3785.6066641807556, 'accumulated_submission_time': 2180.108886241913, 'accumulated_eval_time': 1605.0842487812042, 'accumulated_logging_time': 0.24019217491149902}
I0311 04:17:49.796153 139727860639488 logging_writer.py:48] [6941] accumulated_eval_time=1605.084249, accumulated_logging_time=0.240192, accumulated_submission_time=2180.108886, global_step=6941, preemption_count=0, score=2180.108886, test/accuracy=0.985446, test/loss=0.048718, test/mean_average_precision=0.226385, test/num_examples=43793, total_duration=3785.606664, train/accuracy=0.989505, train/loss=0.035471, train/mean_average_precision=0.285127, validation/accuracy=0.986280, validation/loss=0.046073, validation/mean_average_precision=0.227760, validation/num_examples=43793
I0311 04:18:08.244107 139727869032192 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.017832137644290924, loss=0.038042787462472916
I0311 04:18:38.889961 139727860639488 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.015173325315117836, loss=0.036552801728248596
I0311 04:19:09.989996 139727869032192 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.019740555435419083, loss=0.03866492211818695
I0311 04:19:40.906116 139727860639488 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.016713257879018784, loss=0.03446472808718681
I0311 04:20:11.695396 139727869032192 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.014358737505972385, loss=0.040532175451517105
I0311 04:20:42.596329 139727860639488 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.015418772585690022, loss=0.035790566354990005
I0311 04:21:13.376546 139727869032192 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.013530107215046883, loss=0.03592349961400032
I0311 04:21:44.155533 139727860639488 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.016590818762779236, loss=0.036439310759305954
I0311 04:21:50.000625 139789500200768 spec.py:321] Evaluating on the training split.
I0311 04:23:48.492739 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 04:23:51.525351 139789500200768 spec.py:349] Evaluating on the test split.
I0311 04:23:54.484086 139789500200768 submission_runner.py:411] Time since start: 4150.31s, 	Step: 7720, 	{'train/accuracy': 0.9895861744880676, 'train/loss': 0.034861691296100616, 'train/mean_average_precision': 0.29633644317566893, 'validation/accuracy': 0.9863311052322388, 'validation/loss': 0.04608482867479324, 'validation/mean_average_precision': 0.2284525234305878, 'validation/num_examples': 43793, 'test/accuracy': 0.98549485206604, 'test/loss': 0.0486711822450161, 'test/mean_average_precision': 0.2260540501382554, 'test/num_examples': 43793, 'score': 2420.2818851470947, 'total_duration': 4150.311340808868, 'accumulated_submission_time': 2420.2818851470947, 'accumulated_eval_time': 1729.5676703453064, 'accumulated_logging_time': 0.26764941215515137}
I0311 04:23:54.500614 139628670904064 logging_writer.py:48] [7720] accumulated_eval_time=1729.567670, accumulated_logging_time=0.267649, accumulated_submission_time=2420.281885, global_step=7720, preemption_count=0, score=2420.281885, test/accuracy=0.985495, test/loss=0.048671, test/mean_average_precision=0.226054, test/num_examples=43793, total_duration=4150.311341, train/accuracy=0.989586, train/loss=0.034862, train/mean_average_precision=0.296336, validation/accuracy=0.986331, validation/loss=0.046085, validation/mean_average_precision=0.228453, validation/num_examples=43793
I0311 04:24:19.396769 139628679296768 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.018505215644836426, loss=0.03563621640205383
I0311 04:24:50.474899 139628670904064 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.018618883565068245, loss=0.040403272956609726
I0311 04:25:21.157002 139628679296768 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.017266949638724327, loss=0.03473220765590668
I0311 04:25:52.068471 139628670904064 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.015004190616309643, loss=0.03394213318824768
I0311 04:26:22.853879 139628679296768 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.02415589988231659, loss=0.0355399027466774
I0311 04:26:53.887579 139628670904064 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.015252789482474327, loss=0.03385382518172264
I0311 04:27:24.838911 139628679296768 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.025522859767079353, loss=0.03517574444413185
I0311 04:27:54.778236 139789500200768 spec.py:321] Evaluating on the training split.
I0311 04:29:50.114650 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 04:29:53.159027 139789500200768 spec.py:349] Evaluating on the test split.
I0311 04:29:56.117436 139789500200768 submission_runner.py:411] Time since start: 4511.94s, 	Step: 8498, 	{'train/accuracy': 0.9899796843528748, 'train/loss': 0.033533308655023575, 'train/mean_average_precision': 0.3374593375725701, 'validation/accuracy': 0.9864756464958191, 'validation/loss': 0.04531529173254967, 'validation/mean_average_precision': 0.23742502999916543, 'validation/num_examples': 43793, 'test/accuracy': 0.9855883717536926, 'test/loss': 0.047956012189388275, 'test/mean_average_precision': 0.2365790668525406, 'test/num_examples': 43793, 'score': 2660.528244495392, 'total_duration': 4511.944679737091, 'accumulated_submission_time': 2660.528244495392, 'accumulated_eval_time': 1850.9068267345428, 'accumulated_logging_time': 0.2948906421661377}
I0311 04:29:56.134578 139622180505344 logging_writer.py:48] [8498] accumulated_eval_time=1850.906827, accumulated_logging_time=0.294891, accumulated_submission_time=2660.528244, global_step=8498, preemption_count=0, score=2660.528244, test/accuracy=0.985588, test/loss=0.047956, test/mean_average_precision=0.236579, test/num_examples=43793, total_duration=4511.944680, train/accuracy=0.989980, train/loss=0.033533, train/mean_average_precision=0.337459, validation/accuracy=0.986476, validation/loss=0.045315, validation/mean_average_precision=0.237425, validation/num_examples=43793
I0311 04:29:57.130862 139727869032192 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.016709107905626297, loss=0.03462706506252289
I0311 04:30:28.270936 139622180505344 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.019595613703131676, loss=0.04279540106654167
I0311 04:30:59.260277 139727869032192 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.024088634178042412, loss=0.03582935407757759
I0311 04:31:30.637727 139622180505344 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.019351301714777946, loss=0.035596929490566254
I0311 04:32:01.768178 139727869032192 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.018432289361953735, loss=0.03550618886947632
I0311 04:32:32.891332 139622180505344 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.023027213290333748, loss=0.03795682266354561
I0311 04:33:03.923049 139727869032192 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.017030399292707443, loss=0.035485684871673584
I0311 04:33:35.205699 139622180505344 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.01501175481826067, loss=0.03222217410802841
I0311 04:33:56.240044 139789500200768 spec.py:321] Evaluating on the training split.
I0311 04:35:56.023133 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 04:35:59.470949 139789500200768 spec.py:349] Evaluating on the test split.
I0311 04:36:02.847820 139789500200768 submission_runner.py:411] Time since start: 4878.68s, 	Step: 9268, 	{'train/accuracy': 0.9901212453842163, 'train/loss': 0.03311290591955185, 'train/mean_average_precision': 0.3409681409111439, 'validation/accuracy': 0.9865267872810364, 'validation/loss': 0.04503150284290314, 'validation/mean_average_precision': 0.23488928743004914, 'validation/num_examples': 43793, 'test/accuracy': 0.9857029318809509, 'test/loss': 0.04746432602405548, 'test/mean_average_precision': 0.24841293517930735, 'test/num_examples': 43793, 'score': 2900.601298093796, 'total_duration': 4878.675051450729, 'accumulated_submission_time': 2900.601298093796, 'accumulated_eval_time': 1977.5145423412323, 'accumulated_logging_time': 0.32437801361083984}
I0311 04:36:02.867998 139628670904064 logging_writer.py:48] [9268] accumulated_eval_time=1977.514542, accumulated_logging_time=0.324378, accumulated_submission_time=2900.601298, global_step=9268, preemption_count=0, score=2900.601298, test/accuracy=0.985703, test/loss=0.047464, test/mean_average_precision=0.248413, test/num_examples=43793, total_duration=4878.675051, train/accuracy=0.990121, train/loss=0.033113, train/mean_average_precision=0.340968, validation/accuracy=0.986527, validation/loss=0.045032, validation/mean_average_precision=0.234889, validation/num_examples=43793
I0311 04:36:13.763645 139628679296768 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.01617901772260666, loss=0.03221495822072029
I0311 04:36:46.735690 139628670904064 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.01660521887242794, loss=0.03270021080970764
I0311 04:37:19.972857 139628679296768 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.020508665591478348, loss=0.03506234660744667
I0311 04:37:53.041250 139628670904064 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.01711212284862995, loss=0.03796197474002838
I0311 04:38:25.918731 139628679296768 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.02088836394250393, loss=0.035006213933229446
I0311 04:38:58.365441 139628670904064 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.01881636306643486, loss=0.032788775861263275
I0311 04:39:30.800446 139628679296768 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.01473244745284319, loss=0.02722114510834217
I0311 04:40:03.121316 139789500200768 spec.py:321] Evaluating on the training split.
I0311 04:42:03.828768 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 04:42:06.872269 139789500200768 spec.py:349] Evaluating on the test split.
I0311 04:42:09.844277 139789500200768 submission_runner.py:411] Time since start: 5245.67s, 	Step: 10000, 	{'train/accuracy': 0.9901310801506042, 'train/loss': 0.03306126222014427, 'train/mean_average_precision': 0.33981560149830237, 'validation/accuracy': 0.9864979386329651, 'validation/loss': 0.04513760283589363, 'validation/mean_average_precision': 0.2469973556483602, 'validation/num_examples': 43793, 'test/accuracy': 0.9857020974159241, 'test/loss': 0.04766290262341499, 'test/mean_average_precision': 0.24791485135059496, 'test/num_examples': 43793, 'score': 3140.8176939487457, 'total_duration': 5245.671521663666, 'accumulated_submission_time': 3140.8176939487457, 'accumulated_eval_time': 2104.2374680042267, 'accumulated_logging_time': 0.3568861484527588}
I0311 04:42:09.861157 139622180505344 logging_writer.py:48] [10000] accumulated_eval_time=2104.237468, accumulated_logging_time=0.356886, accumulated_submission_time=3140.817694, global_step=10000, preemption_count=0, score=3140.817694, test/accuracy=0.985702, test/loss=0.047663, test/mean_average_precision=0.247915, test/num_examples=43793, total_duration=5245.671522, train/accuracy=0.990131, train/loss=0.033061, train/mean_average_precision=0.339816, validation/accuracy=0.986498, validation/loss=0.045138, validation/mean_average_precision=0.246997, validation/num_examples=43793
I0311 04:42:10.184037 139727869032192 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.01835593208670616, loss=0.031510401517152786
I0311 04:42:41.681193 139622180505344 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.023045385256409645, loss=0.03248397633433342
I0311 04:43:12.805289 139727869032192 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.02430298924446106, loss=0.03141080588102341
I0311 04:43:43.970289 139622180505344 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.018553130328655243, loss=0.02976205013692379
I0311 04:44:15.647084 139727869032192 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.01868303492665291, loss=0.03565839305520058
I0311 04:44:46.747281 139622180505344 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.026639722287654877, loss=0.03215748444199562
I0311 04:45:17.605806 139727869032192 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.045279402285814285, loss=0.03771544247865677
I0311 04:45:48.246749 139622180505344 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.019178498536348343, loss=0.03486577048897743
I0311 04:46:10.025230 139789500200768 spec.py:321] Evaluating on the training split.
I0311 04:48:10.191317 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 04:48:13.195087 139789500200768 spec.py:349] Evaluating on the test split.
I0311 04:48:16.122355 139789500200768 submission_runner.py:411] Time since start: 5611.95s, 	Step: 10771, 	{'train/accuracy': 0.9900743365287781, 'train/loss': 0.03314041346311569, 'train/mean_average_precision': 0.3361895117138637, 'validation/accuracy': 0.9865519404411316, 'validation/loss': 0.044738687574863434, 'validation/mean_average_precision': 0.24407130772474123, 'validation/num_examples': 43793, 'test/accuracy': 0.9857050180435181, 'test/loss': 0.04721212759613991, 'test/mean_average_precision': 0.24730224073459342, 'test/num_examples': 43793, 'score': 3380.950745820999, 'total_duration': 5611.949606180191, 'accumulated_submission_time': 3380.950745820999, 'accumulated_eval_time': 2230.334555387497, 'accumulated_logging_time': 0.384890079498291}
I0311 04:48:16.139301 139628670904064 logging_writer.py:48] [10771] accumulated_eval_time=2230.334555, accumulated_logging_time=0.384890, accumulated_submission_time=3380.950746, global_step=10771, preemption_count=0, score=3380.950746, test/accuracy=0.985705, test/loss=0.047212, test/mean_average_precision=0.247302, test/num_examples=43793, total_duration=5611.949606, train/accuracy=0.990074, train/loss=0.033140, train/mean_average_precision=0.336190, validation/accuracy=0.986552, validation/loss=0.044739, validation/mean_average_precision=0.244071, validation/num_examples=43793
I0311 04:48:25.765013 139727860639488 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.022746264934539795, loss=0.035639937967061996
I0311 04:48:56.685651 139628670904064 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.02361142821609974, loss=0.03481009975075722
I0311 04:49:27.678653 139727860639488 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.022575100883841515, loss=0.03223955258727074
I0311 04:49:58.613964 139628670904064 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.022151347249746323, loss=0.031638868153095245
I0311 04:50:29.435199 139727860639488 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.02519989386200905, loss=0.034684572368860245
I0311 04:51:00.379940 139628670904064 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.02206813171505928, loss=0.03283355385065079
I0311 04:51:31.168078 139727860639488 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.02164425328373909, loss=0.03283054754137993
I0311 04:52:02.018034 139628670904064 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.02842392772436142, loss=0.03228067606687546
I0311 04:52:16.213356 139789500200768 spec.py:321] Evaluating on the training split.
I0311 04:54:15.670114 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 04:54:18.666868 139789500200768 spec.py:349] Evaluating on the test split.
I0311 04:54:21.599037 139789500200768 submission_runner.py:411] Time since start: 5977.43s, 	Step: 11547, 	{'train/accuracy': 0.990300178527832, 'train/loss': 0.03246564418077469, 'train/mean_average_precision': 0.3550998255123031, 'validation/accuracy': 0.9865734577178955, 'validation/loss': 0.04457039013504982, 'validation/mean_average_precision': 0.2514523384657378, 'validation/num_examples': 43793, 'test/accuracy': 0.9857008457183838, 'test/loss': 0.04713666811585426, 'test/mean_average_precision': 0.24371194041154445, 'test/num_examples': 43793, 'score': 3620.993992805481, 'total_duration': 5977.426261425018, 'accumulated_submission_time': 3620.993992805481, 'accumulated_eval_time': 2355.720167160034, 'accumulated_logging_time': 0.4129753112792969}
I0311 04:54:21.619863 139622180505344 logging_writer.py:48] [11547] accumulated_eval_time=2355.720167, accumulated_logging_time=0.412975, accumulated_submission_time=3620.993993, global_step=11547, preemption_count=0, score=3620.993993, test/accuracy=0.985701, test/loss=0.047137, test/mean_average_precision=0.243712, test/num_examples=43793, total_duration=5977.426261, train/accuracy=0.990300, train/loss=0.032466, train/mean_average_precision=0.355100, validation/accuracy=0.986573, validation/loss=0.044570, validation/mean_average_precision=0.251452, validation/num_examples=43793
I0311 04:54:38.377870 139628679296768 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.028569884598255157, loss=0.03251056745648384
I0311 04:55:08.982460 139622180505344 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.01917974278330803, loss=0.03489474579691887
I0311 04:55:40.123505 139628679296768 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.02406986616551876, loss=0.03281417116522789
I0311 04:56:11.317623 139622180505344 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.026586858555674553, loss=0.03317505493760109
I0311 04:56:42.754442 139628679296768 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.023860467597842216, loss=0.033380039036273956
I0311 04:57:14.219643 139622180505344 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.023723283782601357, loss=0.0348333865404129
I0311 04:57:45.774286 139628679296768 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.03005644492805004, loss=0.03119511529803276
I0311 04:58:16.855199 139622180505344 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.021843258291482925, loss=0.03229335695505142
I0311 04:58:21.826284 139789500200768 spec.py:321] Evaluating on the training split.
I0311 05:00:19.037985 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 05:00:22.011852 139789500200768 spec.py:349] Evaluating on the test split.
I0311 05:00:24.930570 139789500200768 submission_runner.py:411] Time since start: 6340.76s, 	Step: 12317, 	{'train/accuracy': 0.9902815222740173, 'train/loss': 0.032408569008111954, 'train/mean_average_precision': 0.3652259174491668, 'validation/accuracy': 0.9865154027938843, 'validation/loss': 0.04486195743083954, 'validation/mean_average_precision': 0.24539731313897778, 'validation/num_examples': 43793, 'test/accuracy': 0.9856165647506714, 'test/loss': 0.047537025064229965, 'test/mean_average_precision': 0.24648157724123573, 'test/num_examples': 43793, 'score': 3861.169105052948, 'total_duration': 6340.75782251358, 'accumulated_submission_time': 3861.169105052948, 'accumulated_eval_time': 2478.8244092464447, 'accumulated_logging_time': 0.4461667537689209}
I0311 05:00:24.947566 139727860639488 logging_writer.py:48] [12317] accumulated_eval_time=2478.824409, accumulated_logging_time=0.446167, accumulated_submission_time=3861.169105, global_step=12317, preemption_count=0, score=3861.169105, test/accuracy=0.985617, test/loss=0.047537, test/mean_average_precision=0.246482, test/num_examples=43793, total_duration=6340.757823, train/accuracy=0.990282, train/loss=0.032409, train/mean_average_precision=0.365226, validation/accuracy=0.986515, validation/loss=0.044862, validation/mean_average_precision=0.245397, validation/num_examples=43793
I0311 05:00:51.160099 139727869032192 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.02925083041191101, loss=0.03633750230073929
I0311 05:01:22.131120 139727860639488 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.03676715865731239, loss=0.033463943749666214
I0311 05:01:53.048028 139727869032192 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.026781147345900536, loss=0.03332706168293953
I0311 05:02:23.911493 139727860639488 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.024671418592333794, loss=0.033928655087947845
I0311 05:02:54.393912 139727869032192 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.022829921916127205, loss=0.0353202149271965
I0311 05:03:24.956243 139727860639488 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.027240177616477013, loss=0.03392080217599869
I0311 05:03:55.934620 139727869032192 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.0292709581553936, loss=0.036737266927957535
I0311 05:04:25.135775 139789500200768 spec.py:321] Evaluating on the training split.
I0311 05:06:23.752639 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 05:06:26.762816 139789500200768 spec.py:349] Evaluating on the test split.
I0311 05:06:29.753050 139789500200768 submission_runner.py:411] Time since start: 6705.58s, 	Step: 13096, 	{'train/accuracy': 0.9903441071510315, 'train/loss': 0.03184124454855919, 'train/mean_average_precision': 0.3631558729401631, 'validation/accuracy': 0.9867634773254395, 'validation/loss': 0.04434294253587723, 'validation/mean_average_precision': 0.253112694139392, 'validation/num_examples': 43793, 'test/accuracy': 0.9858625531196594, 'test/loss': 0.04703953489661217, 'test/mean_average_precision': 0.2526472636480409, 'test/num_examples': 43793, 'score': 4101.32523059845, 'total_duration': 6705.580302476883, 'accumulated_submission_time': 4101.32523059845, 'accumulated_eval_time': 2603.4416444301605, 'accumulated_logging_time': 0.4753603935241699}
I0311 05:06:29.770552 139622180505344 logging_writer.py:48] [13096] accumulated_eval_time=2603.441644, accumulated_logging_time=0.475360, accumulated_submission_time=4101.325231, global_step=13096, preemption_count=0, score=4101.325231, test/accuracy=0.985863, test/loss=0.047040, test/mean_average_precision=0.252647, test/num_examples=43793, total_duration=6705.580302, train/accuracy=0.990344, train/loss=0.031841, train/mean_average_precision=0.363156, validation/accuracy=0.986763, validation/loss=0.044343, validation/mean_average_precision=0.253113, validation/num_examples=43793
I0311 05:06:31.425719 139628679296768 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.02526792883872986, loss=0.032702863216400146
I0311 05:07:02.524552 139622180505344 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.030921198427677155, loss=0.03049100749194622
I0311 05:07:34.327924 139628679296768 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.027878794819116592, loss=0.031722377985715866
I0311 05:08:05.402771 139622180505344 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.027568217366933823, loss=0.030065316706895828
I0311 05:08:36.463586 139628679296768 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.027866696938872337, loss=0.029247118160128593
I0311 05:09:07.678906 139622180505344 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.02892855368554592, loss=0.03343064710497856
I0311 05:09:38.881034 139628679296768 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.033838722854852676, loss=0.032886672765016556
I0311 05:10:10.259648 139622180505344 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.043418824672698975, loss=0.03296675160527229
I0311 05:10:29.990143 139789500200768 spec.py:321] Evaluating on the training split.
I0311 05:12:30.350250 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 05:12:33.328625 139789500200768 spec.py:349] Evaluating on the test split.
I0311 05:12:36.236135 139789500200768 submission_runner.py:411] Time since start: 7072.06s, 	Step: 13864, 	{'train/accuracy': 0.9905263781547546, 'train/loss': 0.031641941517591476, 'train/mean_average_precision': 0.38300378100119425, 'validation/accuracy': 0.9866445064544678, 'validation/loss': 0.044306594878435135, 'validation/mean_average_precision': 0.2587238987096111, 'validation/num_examples': 43793, 'test/accuracy': 0.985772430896759, 'test/loss': 0.046745408326387405, 'test/mean_average_precision': 0.25912207352122685, 'test/num_examples': 43793, 'score': 4341.513201236725, 'total_duration': 7072.06339097023, 'accumulated_submission_time': 4341.513201236725, 'accumulated_eval_time': 2729.6876018047333, 'accumulated_logging_time': 0.5052244663238525}
I0311 05:12:36.253464 139628670904064 logging_writer.py:48] [13864] accumulated_eval_time=2729.687602, accumulated_logging_time=0.505224, accumulated_submission_time=4341.513201, global_step=13864, preemption_count=0, score=4341.513201, test/accuracy=0.985772, test/loss=0.046745, test/mean_average_precision=0.259122, test/num_examples=43793, total_duration=7072.063391, train/accuracy=0.990526, train/loss=0.031642, train/mean_average_precision=0.383004, validation/accuracy=0.986645, validation/loss=0.044307, validation/mean_average_precision=0.258724, validation/num_examples=43793
I0311 05:12:47.715945 139727869032192 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.028736118227243423, loss=0.034909553825855255
I0311 05:13:18.621793 139628670904064 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.036926183849573135, loss=0.030608341097831726
I0311 05:13:49.705237 139727869032192 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.03371092677116394, loss=0.03299374878406525
I0311 05:14:20.834266 139628670904064 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.02975030615925789, loss=0.03297078236937523
I0311 05:14:52.169473 139727869032192 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.03805661201477051, loss=0.03358449786901474
I0311 05:15:23.836647 139628670904064 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.04080894589424133, loss=0.02775608003139496
I0311 05:15:55.295715 139727869032192 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.0388842336833477, loss=0.03221820294857025
I0311 05:16:26.408637 139628670904064 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.03315975144505501, loss=0.03259633108973503
I0311 05:16:36.540882 139789500200768 spec.py:321] Evaluating on the training split.
I0311 05:18:36.738192 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 05:18:39.747001 139789500200768 spec.py:349] Evaluating on the test split.
I0311 05:18:42.700263 139789500200768 submission_runner.py:411] Time since start: 7438.53s, 	Step: 14634, 	{'train/accuracy': 0.990631103515625, 'train/loss': 0.030873339623212814, 'train/mean_average_precision': 0.3843530163345709, 'validation/accuracy': 0.9867780804634094, 'validation/loss': 0.04493802413344383, 'validation/mean_average_precision': 0.25320764048344646, 'validation/num_examples': 43793, 'test/accuracy': 0.9859017133712769, 'test/loss': 0.047769758850336075, 'test/mean_average_precision': 0.25176877628054134, 'test/num_examples': 43793, 'score': 4581.769669532776, 'total_duration': 7438.527512073517, 'accumulated_submission_time': 4581.769669532776, 'accumulated_eval_time': 2855.8469376564026, 'accumulated_logging_time': 0.5335369110107422}
I0311 05:18:42.717859 139622180505344 logging_writer.py:48] [14634] accumulated_eval_time=2855.846938, accumulated_logging_time=0.533537, accumulated_submission_time=4581.769670, global_step=14634, preemption_count=0, score=4581.769670, test/accuracy=0.985902, test/loss=0.047770, test/mean_average_precision=0.251769, test/num_examples=43793, total_duration=7438.527512, train/accuracy=0.990631, train/loss=0.030873, train/mean_average_precision=0.384353, validation/accuracy=0.986778, validation/loss=0.044938, validation/mean_average_precision=0.253208, validation/num_examples=43793
I0311 05:19:03.569163 139727860639488 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.03896864876151085, loss=0.03454415872693062
I0311 05:19:35.735254 139622180505344 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.03609446436166763, loss=0.03253314644098282
I0311 05:20:07.164320 139727860639488 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.02780383639037609, loss=0.031617794185876846
I0311 05:20:38.875711 139622180505344 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.03962229564785957, loss=0.03347964212298393
I0311 05:21:10.348226 139727860639488 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.0427815243601799, loss=0.033187225461006165
I0311 05:21:41.999781 139622180505344 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.035949692130088806, loss=0.03266667574644089
I0311 05:22:13.474344 139727860639488 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.03397197276353836, loss=0.030725719407200813
I0311 05:22:42.971904 139789500200768 spec.py:321] Evaluating on the training split.
I0311 05:24:44.471365 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 05:24:47.513431 139789500200768 spec.py:349] Evaluating on the test split.
I0311 05:24:50.512552 139789500200768 submission_runner.py:411] Time since start: 7806.34s, 	Step: 15395, 	{'train/accuracy': 0.9906738996505737, 'train/loss': 0.03051711805164814, 'train/mean_average_precision': 0.40603327689300567, 'validation/accuracy': 0.9868982434272766, 'validation/loss': 0.043945930898189545, 'validation/mean_average_precision': 0.2693682103284675, 'validation/num_examples': 43793, 'test/accuracy': 0.9859628081321716, 'test/loss': 0.046863578259944916, 'test/mean_average_precision': 0.25872254489079627, 'test/num_examples': 43793, 'score': 4821.992245674133, 'total_duration': 7806.339802980423, 'accumulated_submission_time': 4821.992245674133, 'accumulated_eval_time': 2983.3875534534454, 'accumulated_logging_time': 0.5630576610565186}
I0311 05:24:50.530403 139628670904064 logging_writer.py:48] [15395] accumulated_eval_time=2983.387553, accumulated_logging_time=0.563058, accumulated_submission_time=4821.992246, global_step=15395, preemption_count=0, score=4821.992246, test/accuracy=0.985963, test/loss=0.046864, test/mean_average_precision=0.258723, test/num_examples=43793, total_duration=7806.339803, train/accuracy=0.990674, train/loss=0.030517, train/mean_average_precision=0.406033, validation/accuracy=0.986898, validation/loss=0.043946, validation/mean_average_precision=0.269368, validation/num_examples=43793
I0311 05:24:52.432742 139628679296768 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.03578059375286102, loss=0.03390485793352127
I0311 05:25:23.801459 139628670904064 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.03643225505948067, loss=0.03225358575582504
I0311 05:25:55.082378 139628679296768 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.04652632027864456, loss=0.031691521406173706
I0311 05:26:26.082318 139628670904064 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.041072599589824677, loss=0.029278595000505447
I0311 05:26:57.285932 139628679296768 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.030256066471338272, loss=0.031500447541475296
I0311 05:27:28.536871 139628670904064 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.03954267501831055, loss=0.0326162651181221
I0311 05:27:59.752954 139628679296768 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.03847534954547882, loss=0.033421698957681656
I0311 05:28:30.647735 139628670904064 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.0326361209154129, loss=0.03346356749534607
I0311 05:28:50.630454 139789500200768 spec.py:321] Evaluating on the training split.
I0311 05:30:51.342926 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 05:30:54.425819 139789500200768 spec.py:349] Evaluating on the test split.
I0311 05:30:57.424528 139789500200768 submission_runner.py:411] Time since start: 8173.25s, 	Step: 16165, 	{'train/accuracy': 0.9908897876739502, 'train/loss': 0.029918527230620384, 'train/mean_average_precision': 0.41900699967008126, 'validation/accuracy': 0.9867841601371765, 'validation/loss': 0.04417656362056732, 'validation/mean_average_precision': 0.26255118382380965, 'validation/num_examples': 43793, 'test/accuracy': 0.9859126806259155, 'test/loss': 0.0468367300927639, 'test/mean_average_precision': 0.26178150211622314, 'test/num_examples': 43793, 'score': 5062.061771631241, 'total_duration': 8173.251757383347, 'accumulated_submission_time': 5062.061771631241, 'accumulated_eval_time': 3110.181565284729, 'accumulated_logging_time': 0.5918211936950684}
I0311 05:30:57.443403 139622180505344 logging_writer.py:48] [16165] accumulated_eval_time=3110.181565, accumulated_logging_time=0.591821, accumulated_submission_time=5062.061772, global_step=16165, preemption_count=0, score=5062.061772, test/accuracy=0.985913, test/loss=0.046837, test/mean_average_precision=0.261782, test/num_examples=43793, total_duration=8173.251757, train/accuracy=0.990890, train/loss=0.029919, train/mean_average_precision=0.419007, validation/accuracy=0.986784, validation/loss=0.044177, validation/mean_average_precision=0.262551, validation/num_examples=43793
I0311 05:31:08.744391 139727869032192 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.04049210995435715, loss=0.03032802976667881
I0311 05:31:39.985506 139622180505344 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.043399956077337265, loss=0.033848922699689865
I0311 05:32:11.059898 139727869032192 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.04665227606892586, loss=0.034418512135744095
I0311 05:32:42.174673 139622180505344 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.03213901445269585, loss=0.034289851784706116
I0311 05:33:13.248191 139727869032192 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.03147616982460022, loss=0.03505822643637657
I0311 05:33:44.395612 139622180505344 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.04188275709748268, loss=0.034935787320137024
I0311 05:34:15.186794 139727869032192 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.03370156139135361, loss=0.0351349301636219
I0311 05:34:46.206545 139622180505344 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.04442726820707321, loss=0.03268294781446457
I0311 05:34:57.629739 139789500200768 spec.py:321] Evaluating on the training split.
I0311 05:36:58.663050 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 05:37:01.719259 139789500200768 spec.py:349] Evaluating on the test split.
I0311 05:37:04.660471 139789500200768 submission_runner.py:411] Time since start: 8540.49s, 	Step: 16938, 	{'train/accuracy': 0.9907991886138916, 'train/loss': 0.03001684509217739, 'train/mean_average_precision': 0.41737968675430603, 'validation/accuracy': 0.9867289662361145, 'validation/loss': 0.04451462998986244, 'validation/mean_average_precision': 0.26258477333179575, 'validation/num_examples': 43793, 'test/accuracy': 0.985939621925354, 'test/loss': 0.047058578580617905, 'test/mean_average_precision': 0.2587865116364578, 'test/num_examples': 43793, 'score': 5302.2172219753265, 'total_duration': 8540.487695932388, 'accumulated_submission_time': 5302.2172219753265, 'accumulated_eval_time': 3237.2122271060944, 'accumulated_logging_time': 0.621873140335083}
I0311 05:37:04.678236 139628670904064 logging_writer.py:48] [16938] accumulated_eval_time=3237.212227, accumulated_logging_time=0.621873, accumulated_submission_time=5302.217222, global_step=16938, preemption_count=0, score=5302.217222, test/accuracy=0.985940, test/loss=0.047059, test/mean_average_precision=0.258787, test/num_examples=43793, total_duration=8540.487696, train/accuracy=0.990799, train/loss=0.030017, train/mean_average_precision=0.417380, validation/accuracy=0.986729, validation/loss=0.044515, validation/mean_average_precision=0.262585, validation/num_examples=43793
I0311 05:37:24.111669 139727860639488 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.035049159079790115, loss=0.03171947970986366
I0311 05:37:54.810909 139628670904064 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.03750142082571983, loss=0.03154244273900986
I0311 05:38:25.788876 139727860639488 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.04024513438344002, loss=0.03023315779864788
I0311 05:38:57.085201 139628670904064 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.05490827187895775, loss=0.032179612666368484
I0311 05:39:27.746525 139727860639488 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.03155209496617317, loss=0.028493283316493034
I0311 05:39:58.719793 139628670904064 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.04344774782657623, loss=0.029690634459257126
I0311 05:40:29.668643 139727860639488 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.05723816901445389, loss=0.03252863511443138
I0311 05:41:00.900238 139628670904064 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.04870300740003586, loss=0.03195134177803993
I0311 05:41:04.977333 139789500200768 spec.py:321] Evaluating on the training split.
I0311 05:43:03.073820 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 05:43:06.061462 139789500200768 spec.py:349] Evaluating on the test split.
I0311 05:43:08.995683 139789500200768 submission_runner.py:411] Time since start: 8904.82s, 	Step: 17714, 	{'train/accuracy': 0.991073489189148, 'train/loss': 0.029269535094499588, 'train/mean_average_precision': 0.431056186751704, 'validation/accuracy': 0.9868012070655823, 'validation/loss': 0.04434310272336006, 'validation/mean_average_precision': 0.26219888341893227, 'validation/num_examples': 43793, 'test/accuracy': 0.985929548740387, 'test/loss': 0.04710589349269867, 'test/mean_average_precision': 0.25902150874371566, 'test/num_examples': 43793, 'score': 5542.485919952393, 'total_duration': 8904.822938919067, 'accumulated_submission_time': 5542.485919952393, 'accumulated_eval_time': 3361.2305388450623, 'accumulated_logging_time': 0.6503303050994873}
I0311 05:43:09.013845 139622180505344 logging_writer.py:48] [17714] accumulated_eval_time=3361.230539, accumulated_logging_time=0.650330, accumulated_submission_time=5542.485920, global_step=17714, preemption_count=0, score=5542.485920, test/accuracy=0.985930, test/loss=0.047106, test/mean_average_precision=0.259022, test/num_examples=43793, total_duration=8904.822939, train/accuracy=0.991073, train/loss=0.029270, train/mean_average_precision=0.431056, validation/accuracy=0.986801, validation/loss=0.044343, validation/mean_average_precision=0.262199, validation/num_examples=43793
I0311 05:43:36.388024 139727869032192 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.05709412693977356, loss=0.03019886277616024
I0311 05:44:08.345566 139622180505344 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.0354161374270916, loss=0.03275410458445549
I0311 05:44:40.024860 139727869032192 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.05040185898542404, loss=0.0339503139257431
I0311 05:45:11.332886 139622180505344 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.044120676815509796, loss=0.033017903566360474
I0311 05:45:43.007061 139727869032192 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.04689883813261986, loss=0.03374020755290985
I0311 05:46:14.535675 139622180505344 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.05923790857195854, loss=0.030184846371412277
I0311 05:46:46.401834 139727869032192 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.11360560357570648, loss=0.03417680785059929
I0311 05:47:09.100684 139789500200768 spec.py:321] Evaluating on the training split.
I0311 05:49:07.166472 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 05:49:10.162779 139789500200768 spec.py:349] Evaluating on the test split.
I0311 05:49:13.124540 139789500200768 submission_runner.py:411] Time since start: 9268.95s, 	Step: 18472, 	{'train/accuracy': 0.991197407245636, 'train/loss': 0.028720958158373833, 'train/mean_average_precision': 0.4492757997726974, 'validation/accuracy': 0.9868823885917664, 'validation/loss': 0.04432648420333862, 'validation/mean_average_precision': 0.2703646829249263, 'validation/num_examples': 43793, 'test/accuracy': 0.986065149307251, 'test/loss': 0.04700955003499985, 'test/mean_average_precision': 0.26593542396439124, 'test/num_examples': 43793, 'score': 5782.542509555817, 'total_duration': 9268.951786994934, 'accumulated_submission_time': 5782.542509555817, 'accumulated_eval_time': 3485.2543499469757, 'accumulated_logging_time': 0.6788928508758545}
I0311 05:49:13.143659 139628670904064 logging_writer.py:48] [18472] accumulated_eval_time=3485.254350, accumulated_logging_time=0.678893, accumulated_submission_time=5782.542510, global_step=18472, preemption_count=0, score=5782.542510, test/accuracy=0.986065, test/loss=0.047010, test/mean_average_precision=0.265935, test/num_examples=43793, total_duration=9268.951787, train/accuracy=0.991197, train/loss=0.028721, train/mean_average_precision=0.449276, validation/accuracy=0.986882, validation/loss=0.044326, validation/mean_average_precision=0.270365, validation/num_examples=43793
I0311 05:49:22.205172 139628679296768 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.039092861115932465, loss=0.032494813203811646
I0311 05:49:53.082782 139628670904064 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.04208644479513168, loss=0.034713659435510635
I0311 05:50:24.302535 139628679296768 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.04335826262831688, loss=0.031728800386190414
I0311 05:50:55.684766 139628670904064 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.03719666600227356, loss=0.03047303482890129
I0311 05:51:26.987682 139628679296768 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.03500986099243164, loss=0.030274352058768272
I0311 05:51:57.775864 139628670904064 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.04565969109535217, loss=0.0314125157892704
I0311 05:52:28.850634 139628679296768 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.04257707670331001, loss=0.031191399320960045
I0311 05:52:59.752414 139628670904064 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.03900117799639702, loss=0.030076956376433372
I0311 05:53:13.234467 139789500200768 spec.py:321] Evaluating on the training split.
I0311 05:55:11.481849 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 05:55:14.448702 139789500200768 spec.py:349] Evaluating on the test split.
I0311 05:55:17.383654 139789500200768 submission_runner.py:411] Time since start: 9633.21s, 	Step: 19245, 	{'train/accuracy': 0.9910871386528015, 'train/loss': 0.029056798666715622, 'train/mean_average_precision': 0.44187565556708325, 'validation/accuracy': 0.9868450164794922, 'validation/loss': 0.04456828907132149, 'validation/mean_average_precision': 0.2730459174791392, 'validation/num_examples': 43793, 'test/accuracy': 0.9859371185302734, 'test/loss': 0.04744438827037811, 'test/mean_average_precision': 0.2659620129994766, 'test/num_examples': 43793, 'score': 6022.6024379730225, 'total_duration': 9633.210906744003, 'accumulated_submission_time': 6022.6024379730225, 'accumulated_eval_time': 3609.403496026993, 'accumulated_logging_time': 0.7088520526885986}
I0311 05:55:17.402259 139622180505344 logging_writer.py:48] [19245] accumulated_eval_time=3609.403496, accumulated_logging_time=0.708852, accumulated_submission_time=6022.602438, global_step=19245, preemption_count=0, score=6022.602438, test/accuracy=0.985937, test/loss=0.047444, test/mean_average_precision=0.265962, test/num_examples=43793, total_duration=9633.210907, train/accuracy=0.991087, train/loss=0.029057, train/mean_average_precision=0.441876, validation/accuracy=0.986845, validation/loss=0.044568, validation/mean_average_precision=0.273046, validation/num_examples=43793
I0311 05:55:34.824536 139727869032192 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.04089880734682083, loss=0.031906310468912125
I0311 05:56:05.504668 139622180505344 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.036539655178785324, loss=0.03006482683122158
I0311 05:56:36.344121 139727869032192 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.05197891220450401, loss=0.032084207981824875
I0311 05:57:07.347038 139622180505344 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.04145286977291107, loss=0.028386320918798447
I0311 05:57:38.528804 139727869032192 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.0551544614136219, loss=0.0337638333439827
I0311 05:58:09.348531 139622180505344 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.04878927767276764, loss=0.032872769981622696
I0311 05:58:40.115548 139727869032192 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.040774181485176086, loss=0.03263210877776146
I0311 05:59:10.849458 139622180505344 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.05685688927769661, loss=0.03429228067398071
I0311 05:59:17.428477 139789500200768 spec.py:321] Evaluating on the training split.
I0311 06:01:20.234874 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 06:01:23.204574 139789500200768 spec.py:349] Evaluating on the test split.
I0311 06:01:26.147514 139789500200768 submission_runner.py:411] Time since start: 10001.97s, 	Step: 20022, 	{'train/accuracy': 0.9911937713623047, 'train/loss': 0.028831778094172478, 'train/mean_average_precision': 0.4492746429492774, 'validation/accuracy': 0.9869363903999329, 'validation/loss': 0.04398409649729729, 'validation/mean_average_precision': 0.2670309948303253, 'validation/num_examples': 43793, 'test/accuracy': 0.9860432744026184, 'test/loss': 0.04657575860619545, 'test/mean_average_precision': 0.2620121333398993, 'test/num_examples': 43793, 'score': 6262.597642183304, 'total_duration': 10001.974766492844, 'accumulated_submission_time': 6262.597642183304, 'accumulated_eval_time': 3738.1225106716156, 'accumulated_logging_time': 0.7383873462677002}
I0311 06:01:26.166404 139628670904064 logging_writer.py:48] [20022] accumulated_eval_time=3738.122511, accumulated_logging_time=0.738387, accumulated_submission_time=6262.597642, global_step=20022, preemption_count=0, score=6262.597642, test/accuracy=0.986043, test/loss=0.046576, test/mean_average_precision=0.262012, test/num_examples=43793, total_duration=10001.974766, train/accuracy=0.991194, train/loss=0.028832, train/mean_average_precision=0.449275, validation/accuracy=0.986936, validation/loss=0.043984, validation/mean_average_precision=0.267031, validation/num_examples=43793
I0311 06:01:50.676646 139628679296768 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.0498468279838562, loss=0.03516068309545517
I0311 06:02:21.963180 139628670904064 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.03678169846534729, loss=0.02606922760605812
I0311 06:02:53.086485 139628679296768 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.0404876209795475, loss=0.03085167519748211
I0311 06:03:24.410750 139628670904064 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.03810467571020126, loss=0.03126870095729828
I0311 06:03:55.874377 139628679296768 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.053004540503025055, loss=0.029602298513054848
I0311 06:04:27.196588 139628670904064 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.04747463017702103, loss=0.02928990125656128
I0311 06:04:58.408937 139628679296768 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.04461606219410896, loss=0.0282576996833086
I0311 06:05:26.355074 139789500200768 spec.py:321] Evaluating on the training split.
I0311 06:07:25.187282 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 06:07:28.282776 139789500200768 spec.py:349] Evaluating on the test split.
I0311 06:07:31.301964 139789500200768 submission_runner.py:411] Time since start: 10367.13s, 	Step: 20790, 	{'train/accuracy': 0.9911454319953918, 'train/loss': 0.028866467997431755, 'train/mean_average_precision': 0.43754581940107656, 'validation/accuracy': 0.9868552088737488, 'validation/loss': 0.04410266876220703, 'validation/mean_average_precision': 0.26749663135732027, 'validation/num_examples': 43793, 'test/accuracy': 0.9860184192657471, 'test/loss': 0.046742070466279984, 'test/mean_average_precision': 0.2635152227722349, 'test/num_examples': 43793, 'score': 6502.755940914154, 'total_duration': 10367.129216194153, 'accumulated_submission_time': 6502.755940914154, 'accumulated_eval_time': 3863.0693638324738, 'accumulated_logging_time': 0.7679581642150879}
I0311 06:07:31.320854 139622180505344 logging_writer.py:48] [20790] accumulated_eval_time=3863.069364, accumulated_logging_time=0.767958, accumulated_submission_time=6502.755941, global_step=20790, preemption_count=0, score=6502.755941, test/accuracy=0.986018, test/loss=0.046742, test/mean_average_precision=0.263515, test/num_examples=43793, total_duration=10367.129216, train/accuracy=0.991145, train/loss=0.028866, train/mean_average_precision=0.437546, validation/accuracy=0.986855, validation/loss=0.044103, validation/mean_average_precision=0.267497, validation/num_examples=43793
I0311 06:07:34.845502 139727860639488 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.053407084196805954, loss=0.031145647168159485
I0311 06:08:06.307943 139622180505344 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.049091190099716187, loss=0.031148742884397507
I0311 06:08:37.643488 139727860639488 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.03863910958170891, loss=0.029876291751861572
I0311 06:09:08.433595 139622180505344 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.04376750811934471, loss=0.03155893087387085
I0311 06:09:39.495302 139727860639488 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.042773861438035965, loss=0.028299476951360703
I0311 06:10:10.744073 139622180505344 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.0843065083026886, loss=0.028320923447608948
I0311 06:10:41.686944 139727860639488 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.044575128704309464, loss=0.031837157905101776
I0311 06:11:12.521606 139622180505344 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.052257224917411804, loss=0.030357547104358673
I0311 06:11:31.477497 139789500200768 spec.py:321] Evaluating on the training split.
I0311 06:13:32.872062 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 06:13:35.838734 139789500200768 spec.py:349] Evaluating on the test split.
I0311 06:13:38.772134 139789500200768 submission_runner.py:411] Time since start: 10734.60s, 	Step: 21562, 	{'train/accuracy': 0.9909815192222595, 'train/loss': 0.029510999098420143, 'train/mean_average_precision': 0.4286785680240972, 'validation/accuracy': 0.9869201183319092, 'validation/loss': 0.04432511329650879, 'validation/mean_average_precision': 0.2658507401426963, 'validation/num_examples': 43793, 'test/accuracy': 0.9859952330589294, 'test/loss': 0.04716325178742409, 'test/mean_average_precision': 0.26155488899120855, 'test/num_examples': 43793, 'score': 6742.881019592285, 'total_duration': 10734.599383115768, 'accumulated_submission_time': 6742.881019592285, 'accumulated_eval_time': 3990.363956451416, 'accumulated_logging_time': 0.7991714477539062}
I0311 06:13:38.791157 139628670904064 logging_writer.py:48] [21562] accumulated_eval_time=3990.363956, accumulated_logging_time=0.799171, accumulated_submission_time=6742.881020, global_step=21562, preemption_count=0, score=6742.881020, test/accuracy=0.985995, test/loss=0.047163, test/mean_average_precision=0.261555, test/num_examples=43793, total_duration=10734.599383, train/accuracy=0.990982, train/loss=0.029511, train/mean_average_precision=0.428679, validation/accuracy=0.986920, validation/loss=0.044325, validation/mean_average_precision=0.265851, validation/num_examples=43793
I0311 06:13:50.901704 139628679296768 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.042397111654281616, loss=0.028124865144491196
I0311 06:14:22.255904 139628670904064 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.05905785784125328, loss=0.02997501939535141
I0311 06:14:53.176914 139628679296768 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.04345855116844177, loss=0.03185408189892769
I0311 06:15:24.408322 139628670904064 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.04191413149237633, loss=0.026922328397631645
I0311 06:15:55.374667 139628679296768 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.04263392090797424, loss=0.026012521237134933
I0311 06:16:26.484520 139628670904064 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.05140551179647446, loss=0.03325917199254036
I0311 06:16:57.999417 139628679296768 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.06115509197115898, loss=0.03097684308886528
I0311 06:17:29.443850 139628670904064 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.05360492318868637, loss=0.03120747208595276
I0311 06:17:38.793964 139789500200768 spec.py:321] Evaluating on the training split.
I0311 06:19:38.780387 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 06:19:41.794500 139789500200768 spec.py:349] Evaluating on the test split.
I0311 06:19:44.740094 139789500200768 submission_runner.py:411] Time since start: 11100.57s, 	Step: 22331, 	{'train/accuracy': 0.9911559224128723, 'train/loss': 0.028979644179344177, 'train/mean_average_precision': 0.43084326357553937, 'validation/accuracy': 0.9867740273475647, 'validation/loss': 0.04402102157473564, 'validation/mean_average_precision': 0.2722770316354325, 'validation/num_examples': 43793, 'test/accuracy': 0.9860284924507141, 'test/loss': 0.046666767448186874, 'test/mean_average_precision': 0.2643371726781699, 'test/num_examples': 43793, 'score': 6982.853446960449, 'total_duration': 11100.56733751297, 'accumulated_submission_time': 6982.853446960449, 'accumulated_eval_time': 4116.310038328171, 'accumulated_logging_time': 0.8293313980102539}
I0311 06:19:44.758974 139727860639488 logging_writer.py:48] [22331] accumulated_eval_time=4116.310038, accumulated_logging_time=0.829331, accumulated_submission_time=6982.853447, global_step=22331, preemption_count=0, score=6982.853447, test/accuracy=0.986028, test/loss=0.046667, test/mean_average_precision=0.264337, test/num_examples=43793, total_duration=11100.567338, train/accuracy=0.991156, train/loss=0.028980, train/mean_average_precision=0.430843, validation/accuracy=0.986774, validation/loss=0.044021, validation/mean_average_precision=0.272277, validation/num_examples=43793
I0311 06:20:06.513700 139727869032192 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.04509316757321358, loss=0.0320516936480999
I0311 06:20:37.438981 139727860639488 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.05448726564645767, loss=0.02968648448586464
I0311 06:21:08.784944 139727869032192 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.04705921560525894, loss=0.032695282250642776
I0311 06:21:40.110822 139727860639488 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.05189184844493866, loss=0.031958017498254776
I0311 06:22:11.459490 139727869032192 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.04602672532200813, loss=0.029457706958055496
I0311 06:22:42.549668 139727860639488 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.06411668658256531, loss=0.03296484798192978
I0311 06:23:13.637526 139727869032192 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.04761310666799545, loss=0.030194979161024094
I0311 06:23:45.018441 139727860639488 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.049989454448223114, loss=0.030194446444511414
I0311 06:23:45.023310 139789500200768 spec.py:321] Evaluating on the training split.
I0311 06:25:47.298287 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 06:25:50.300129 139789500200768 spec.py:349] Evaluating on the test split.
I0311 06:25:53.257699 139789500200768 submission_runner.py:411] Time since start: 11469.08s, 	Step: 23101, 	{'train/accuracy': 0.9911812543869019, 'train/loss': 0.02899666503071785, 'train/mean_average_precision': 0.4412407234838167, 'validation/accuracy': 0.9868909120559692, 'validation/loss': 0.04426807165145874, 'validation/mean_average_precision': 0.2685556335456881, 'validation/num_examples': 43793, 'test/accuracy': 0.9861287474632263, 'test/loss': 0.04692790284752846, 'test/mean_average_precision': 0.26591284633085405, 'test/num_examples': 43793, 'score': 7223.086431264877, 'total_duration': 11469.08495092392, 'accumulated_submission_time': 7223.086431264877, 'accumulated_eval_time': 4244.544364929199, 'accumulated_logging_time': 0.8605177402496338}
I0311 06:25:53.277314 139622180505344 logging_writer.py:48] [23101] accumulated_eval_time=4244.544365, accumulated_logging_time=0.860518, accumulated_submission_time=7223.086431, global_step=23101, preemption_count=0, score=7223.086431, test/accuracy=0.986129, test/loss=0.046928, test/mean_average_precision=0.265913, test/num_examples=43793, total_duration=11469.084951, train/accuracy=0.991181, train/loss=0.028997, train/mean_average_precision=0.441241, validation/accuracy=0.986891, validation/loss=0.044268, validation/mean_average_precision=0.268556, validation/num_examples=43793
I0311 06:26:25.137907 139628670904064 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.06394678354263306, loss=0.030162686482071877
I0311 06:26:57.832628 139622180505344 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.04966413229703903, loss=0.03207086771726608
I0311 06:27:30.871627 139628670904064 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.050710905343294144, loss=0.031196333467960358
I0311 06:28:03.675394 139622180505344 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.054075807332992554, loss=0.02865089662373066
I0311 06:28:36.346318 139628670904064 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.05456971004605293, loss=0.028907397761940956
I0311 06:29:08.818721 139622180505344 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.04864701256155968, loss=0.0322406068444252
I0311 06:29:41.371876 139628670904064 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.054827652871608734, loss=0.030170204117894173
I0311 06:29:53.547051 139789500200768 spec.py:321] Evaluating on the training split.
I0311 06:31:57.085392 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 06:32:00.099646 139789500200768 spec.py:349] Evaluating on the test split.
I0311 06:32:03.135335 139789500200768 submission_runner.py:411] Time since start: 11838.96s, 	Step: 23839, 	{'train/accuracy': 0.9912705421447754, 'train/loss': 0.028407489880919456, 'train/mean_average_precision': 0.4519112047921446, 'validation/accuracy': 0.98692786693573, 'validation/loss': 0.0439685694873333, 'validation/mean_average_precision': 0.27231112162831644, 'validation/num_examples': 43793, 'test/accuracy': 0.9860807657241821, 'test/loss': 0.04666711390018463, 'test/mean_average_precision': 0.2664010170784851, 'test/num_examples': 43793, 'score': 7463.321610689163, 'total_duration': 11838.962498188019, 'accumulated_submission_time': 7463.321610689163, 'accumulated_eval_time': 4374.132519721985, 'accumulated_logging_time': 0.8918707370758057}
I0311 06:32:03.155361 139628679296768 logging_writer.py:48] [23839] accumulated_eval_time=4374.132520, accumulated_logging_time=0.891871, accumulated_submission_time=7463.321611, global_step=23839, preemption_count=0, score=7463.321611, test/accuracy=0.986081, test/loss=0.046667, test/mean_average_precision=0.266401, test/num_examples=43793, total_duration=11838.962498, train/accuracy=0.991271, train/loss=0.028407, train/mean_average_precision=0.451911, validation/accuracy=0.986928, validation/loss=0.043969, validation/mean_average_precision=0.272311, validation/num_examples=43793
I0311 06:32:22.685112 139727860639488 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.0527830608189106, loss=0.03314729034900665
I0311 06:32:54.077124 139628679296768 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.05852475389838219, loss=0.030272232368588448
I0311 06:33:25.527984 139727860639488 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.05082668736577034, loss=0.030897540971636772
I0311 06:33:56.849948 139628679296768 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.0497005358338356, loss=0.02790362760424614
I0311 06:34:28.297474 139727860639488 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.050381697714328766, loss=0.027689186856150627
I0311 06:34:59.548897 139628679296768 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.05721225216984749, loss=0.027028659358620644
I0311 06:35:30.830935 139727860639488 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.06929130852222443, loss=0.02716638892889023
I0311 06:36:02.297843 139628679296768 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.06336674839258194, loss=0.03092697262763977
I0311 06:36:03.259468 139789500200768 spec.py:321] Evaluating on the training split.
I0311 06:38:03.209963 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 06:38:06.199554 139789500200768 spec.py:349] Evaluating on the test split.
I0311 06:38:09.170244 139789500200768 submission_runner.py:411] Time since start: 12205.00s, 	Step: 24604, 	{'train/accuracy': 0.9913958311080933, 'train/loss': 0.027923747897148132, 'train/mean_average_precision': 0.46296279208897295, 'validation/accuracy': 0.9869635701179504, 'validation/loss': 0.04363136738538742, 'validation/mean_average_precision': 0.27925501348754844, 'validation/num_examples': 43793, 'test/accuracy': 0.9861123561859131, 'test/loss': 0.046518225222826004, 'test/mean_average_precision': 0.27152485168942353, 'test/num_examples': 43793, 'score': 7703.395177125931, 'total_duration': 12204.99735713005, 'accumulated_submission_time': 7703.395177125931, 'accumulated_eval_time': 4500.043113708496, 'accumulated_logging_time': 0.9227774143218994}
I0311 06:38:09.189353 139622180505344 logging_writer.py:48] [24604] accumulated_eval_time=4500.043114, accumulated_logging_time=0.922777, accumulated_submission_time=7703.395177, global_step=24604, preemption_count=0, score=7703.395177, test/accuracy=0.986112, test/loss=0.046518, test/mean_average_precision=0.271525, test/num_examples=43793, total_duration=12204.997357, train/accuracy=0.991396, train/loss=0.027924, train/mean_average_precision=0.462963, validation/accuracy=0.986964, validation/loss=0.043631, validation/mean_average_precision=0.279255, validation/num_examples=43793
I0311 06:38:39.456654 139727869032192 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.06890393793582916, loss=0.030488023534417152
I0311 06:39:10.714666 139622180505344 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.049004748463630676, loss=0.02896042913198471
I0311 06:39:41.808513 139727869032192 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.05417434871196747, loss=0.03268708288669586
I0311 06:40:12.759926 139622180505344 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.06743146479129791, loss=0.027826866135001183
I0311 06:40:43.788071 139727869032192 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.07130715996026993, loss=0.0328010618686676
I0311 06:41:15.113840 139622180505344 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.0573643334209919, loss=0.02886788919568062
I0311 06:41:46.160170 139727869032192 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.0514107272028923, loss=0.030063392594456673
I0311 06:42:09.308533 139789500200768 spec.py:321] Evaluating on the training split.
I0311 06:44:07.569812 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 06:44:10.541057 139789500200768 spec.py:349] Evaluating on the test split.
I0311 06:44:13.470272 139789500200768 submission_runner.py:411] Time since start: 12569.30s, 	Step: 25376, 	{'train/accuracy': 0.9914031624794006, 'train/loss': 0.027938485145568848, 'train/mean_average_precision': 0.4564729812272185, 'validation/accuracy': 0.9869404435157776, 'validation/loss': 0.0441245399415493, 'validation/mean_average_precision': 0.27998706840613086, 'validation/num_examples': 43793, 'test/accuracy': 0.9861026406288147, 'test/loss': 0.046784572303295135, 'test/mean_average_precision': 0.2682354559966355, 'test/num_examples': 43793, 'score': 7943.4845950603485, 'total_duration': 12569.297526597977, 'accumulated_submission_time': 7943.4845950603485, 'accumulated_eval_time': 4624.204813718796, 'accumulated_logging_time': 0.9522502422332764}
I0311 06:44:13.490397 139628670904064 logging_writer.py:48] [25376] accumulated_eval_time=4624.204814, accumulated_logging_time=0.952250, accumulated_submission_time=7943.484595, global_step=25376, preemption_count=0, score=7943.484595, test/accuracy=0.986103, test/loss=0.046785, test/mean_average_precision=0.268235, test/num_examples=43793, total_duration=12569.297527, train/accuracy=0.991403, train/loss=0.027938, train/mean_average_precision=0.456473, validation/accuracy=0.986940, validation/loss=0.044125, validation/mean_average_precision=0.279987, validation/num_examples=43793
I0311 06:44:21.210491 139727860639488 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.07494595646858215, loss=0.029565248638391495
I0311 06:44:52.256412 139628670904064 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.06931989639997482, loss=0.03197351098060608
I0311 06:45:23.631031 139727860639488 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.04197137802839279, loss=0.030093716457486153
I0311 06:45:55.035474 139628670904064 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.04919438809156418, loss=0.028197281062602997
I0311 06:46:26.801806 139727860639488 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.04889032617211342, loss=0.030399011448025703
I0311 06:46:58.277982 139628670904064 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.05418853461742401, loss=0.02931905724108219
I0311 06:47:29.929507 139727860639488 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.04724150896072388, loss=0.028431866317987442
I0311 06:48:01.178068 139628670904064 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.054474350064992905, loss=0.029832536354660988
I0311 06:48:13.621397 139789500200768 spec.py:321] Evaluating on the training split.
I0311 06:50:17.694413 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 06:50:20.733319 139789500200768 spec.py:349] Evaluating on the test split.
I0311 06:50:23.729736 139789500200768 submission_runner.py:411] Time since start: 12939.56s, 	Step: 26141, 	{'train/accuracy': 0.9914870262145996, 'train/loss': 0.02753416821360588, 'train/mean_average_precision': 0.47677597126947274, 'validation/accuracy': 0.9869298934936523, 'validation/loss': 0.04399753361940384, 'validation/mean_average_precision': 0.2779713734008719, 'validation/num_examples': 43793, 'test/accuracy': 0.986050009727478, 'test/loss': 0.04683008790016174, 'test/mean_average_precision': 0.2667560083856059, 'test/num_examples': 43793, 'score': 8183.584435462952, 'total_duration': 12939.556988477707, 'accumulated_submission_time': 8183.584435462952, 'accumulated_eval_time': 4754.313112735748, 'accumulated_logging_time': 0.9845142364501953}
I0311 06:50:23.750322 139628679296768 logging_writer.py:48] [26141] accumulated_eval_time=4754.313113, accumulated_logging_time=0.984514, accumulated_submission_time=8183.584435, global_step=26141, preemption_count=0, score=8183.584435, test/accuracy=0.986050, test/loss=0.046830, test/mean_average_precision=0.266756, test/num_examples=43793, total_duration=12939.556988, train/accuracy=0.991487, train/loss=0.027534, train/mean_average_precision=0.476776, validation/accuracy=0.986930, validation/loss=0.043998, validation/mean_average_precision=0.277971, validation/num_examples=43793
I0311 06:50:42.917611 139727869032192 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.04873056709766388, loss=0.030783122405409813
I0311 06:51:14.071845 139628679296768 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.0668143481016159, loss=0.030556099489331245
I0311 06:51:45.137506 139727869032192 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.07093395292758942, loss=0.03432279825210571
I0311 06:52:16.364994 139628679296768 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.07787972688674927, loss=0.026356935501098633
I0311 06:52:47.323048 139727869032192 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.057598572224378586, loss=0.03542245551943779
I0311 06:53:18.704796 139628679296768 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.05171205848455429, loss=0.02939028851687908
I0311 06:53:49.900665 139727869032192 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.04979923740029335, loss=0.03174672648310661
I0311 06:54:21.293505 139628679296768 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.05369536951184273, loss=0.029750801622867584
I0311 06:54:23.796936 139789500200768 spec.py:321] Evaluating on the training split.
I0311 06:56:24.010797 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 06:56:27.046642 139789500200768 spec.py:349] Evaluating on the test split.
I0311 06:56:30.058436 139789500200768 submission_runner.py:411] Time since start: 13305.89s, 	Step: 26909, 	{'train/accuracy': 0.9915395975112915, 'train/loss': 0.02723928913474083, 'train/mean_average_precision': 0.4861751748239109, 'validation/accuracy': 0.9869457483291626, 'validation/loss': 0.04400632157921791, 'validation/mean_average_precision': 0.28381370704270764, 'validation/num_examples': 43793, 'test/accuracy': 0.9861173629760742, 'test/loss': 0.04688054695725441, 'test/mean_average_precision': 0.27175133973816584, 'test/num_examples': 43793, 'score': 8423.600947618484, 'total_duration': 13305.885575532913, 'accumulated_submission_time': 8423.600947618484, 'accumulated_eval_time': 4880.574460268021, 'accumulated_logging_time': 1.0156822204589844}
I0311 06:56:30.078389 139628670904064 logging_writer.py:48] [26909] accumulated_eval_time=4880.574460, accumulated_logging_time=1.015682, accumulated_submission_time=8423.600948, global_step=26909, preemption_count=0, score=8423.600948, test/accuracy=0.986117, test/loss=0.046881, test/mean_average_precision=0.271751, test/num_examples=43793, total_duration=13305.885576, train/accuracy=0.991540, train/loss=0.027239, train/mean_average_precision=0.486175, validation/accuracy=0.986946, validation/loss=0.044006, validation/mean_average_precision=0.283814, validation/num_examples=43793
I0311 06:56:58.856400 139727860639488 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.05007471516728401, loss=0.028549915179610252
I0311 06:57:30.043169 139628670904064 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.06219062954187393, loss=0.03592660650610924
I0311 06:58:01.084352 139727860639488 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.0672229528427124, loss=0.034623924642801285
I0311 06:58:32.459602 139628670904064 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.0657230094075203, loss=0.03174862638115883
I0311 06:59:03.606892 139727860639488 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.08149717003107071, loss=0.02896489016711712
I0311 06:59:34.721478 139628670904064 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.05304010212421417, loss=0.0279280673712492
I0311 07:00:05.853678 139727860639488 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.04920794442296028, loss=0.03230557218194008
I0311 07:00:30.313612 139789500200768 spec.py:321] Evaluating on the training split.
I0311 07:02:32.180837 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 07:02:35.178400 139789500200768 spec.py:349] Evaluating on the test split.
I0311 07:02:38.236520 139789500200768 submission_runner.py:411] Time since start: 13674.06s, 	Step: 27679, 	{'train/accuracy': 0.9918515682220459, 'train/loss': 0.02632072940468788, 'train/mean_average_precision': 0.5115942328205016, 'validation/accuracy': 0.9870147109031677, 'validation/loss': 0.043874554336071014, 'validation/mean_average_precision': 0.2798143819556658, 'validation/num_examples': 43793, 'test/accuracy': 0.9861114621162415, 'test/loss': 0.04666158929467201, 'test/mean_average_precision': 0.274525427657463, 'test/num_examples': 43793, 'score': 8663.806197404861, 'total_duration': 13674.063752174377, 'accumulated_submission_time': 8663.806197404861, 'accumulated_eval_time': 5008.497307538986, 'accumulated_logging_time': 1.046447992324829}
I0311 07:02:38.257828 139622180505344 logging_writer.py:48] [27679] accumulated_eval_time=5008.497308, accumulated_logging_time=1.046448, accumulated_submission_time=8663.806197, global_step=27679, preemption_count=0, score=8663.806197, test/accuracy=0.986111, test/loss=0.046662, test/mean_average_precision=0.274525, test/num_examples=43793, total_duration=13674.063752, train/accuracy=0.991852, train/loss=0.026321, train/mean_average_precision=0.511594, validation/accuracy=0.987015, validation/loss=0.043875, validation/mean_average_precision=0.279814, validation/num_examples=43793
I0311 07:02:45.159671 139628679296768 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.060538388788700104, loss=0.03309464082121849
I0311 07:03:16.390527 139622180505344 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.05542678385972977, loss=0.02973783388733864
I0311 07:03:47.513200 139628679296768 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.05008714646100998, loss=0.031159954145550728
I0311 07:04:18.751196 139622180505344 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.05849974974989891, loss=0.02894354611635208
I0311 07:04:49.887953 139628679296768 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.05272615700960159, loss=0.02901075966656208
I0311 07:05:20.956071 139622180505344 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.0692911371588707, loss=0.029723377898335457
I0311 07:05:51.996501 139628679296768 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.05200052261352539, loss=0.02696273662149906
I0311 07:06:23.162871 139622180505344 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.06276087462902069, loss=0.03157110512256622
I0311 07:06:38.272522 139789500200768 spec.py:321] Evaluating on the training split.
I0311 07:08:35.189369 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 07:08:38.252032 139789500200768 spec.py:349] Evaluating on the test split.
I0311 07:08:41.239408 139789500200768 submission_runner.py:411] Time since start: 14037.07s, 	Step: 28450, 	{'train/accuracy': 0.9919523000717163, 'train/loss': 0.026040490716695786, 'train/mean_average_precision': 0.5119545931912086, 'validation/accuracy': 0.9868450164794922, 'validation/loss': 0.04379187524318695, 'validation/mean_average_precision': 0.2751911119907508, 'validation/num_examples': 43793, 'test/accuracy': 0.986026406288147, 'test/loss': 0.046833716332912445, 'test/mean_average_precision': 0.2677213978781376, 'test/num_examples': 43793, 'score': 8903.79060792923, 'total_duration': 14037.0665640831, 'accumulated_submission_time': 8903.79060792923, 'accumulated_eval_time': 5131.4640600681305, 'accumulated_logging_time': 1.0783584117889404}
I0311 07:08:41.259440 139628670904064 logging_writer.py:48] [28450] accumulated_eval_time=5131.464060, accumulated_logging_time=1.078358, accumulated_submission_time=8903.790608, global_step=28450, preemption_count=0, score=8903.790608, test/accuracy=0.986026, test/loss=0.046834, test/mean_average_precision=0.267721, test/num_examples=43793, total_duration=14037.066564, train/accuracy=0.991952, train/loss=0.026040, train/mean_average_precision=0.511955, validation/accuracy=0.986845, validation/loss=0.043792, validation/mean_average_precision=0.275191, validation/num_examples=43793
I0311 07:08:57.076638 139727860639488 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.051024436950683594, loss=0.026766669005155563
I0311 07:09:28.324187 139628670904064 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.05798593908548355, loss=0.03181733936071396
I0311 07:09:59.144112 139727860639488 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.06001349538564682, loss=0.026320239529013634
I0311 07:10:30.087639 139628670904064 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.05097915232181549, loss=0.024083077907562256
I0311 07:11:01.331293 139727860639488 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.04777517542243004, loss=0.025574710220098495
I0311 07:11:32.272293 139628670904064 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.06422862410545349, loss=0.03166184201836586
I0311 07:12:03.822382 139727860639488 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.05971767380833626, loss=0.029796969145536423
I0311 07:12:34.836995 139628670904064 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.05253055319190025, loss=0.03195212036371231
I0311 07:12:41.352859 139789500200768 spec.py:321] Evaluating on the training split.
I0311 07:14:40.203977 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 07:14:43.289728 139789500200768 spec.py:349] Evaluating on the test split.
I0311 07:14:46.307425 139789500200768 submission_runner.py:411] Time since start: 14402.13s, 	Step: 29222, 	{'train/accuracy': 0.9917731881141663, 'train/loss': 0.026522187516093254, 'train/mean_average_precision': 0.49747430777921375, 'validation/accuracy': 0.9870484471321106, 'validation/loss': 0.044064465910196304, 'validation/mean_average_precision': 0.28172652129326164, 'validation/num_examples': 43793, 'test/accuracy': 0.9861489534378052, 'test/loss': 0.04704549163579941, 'test/mean_average_precision': 0.27135695229590756, 'test/num_examples': 43793, 'score': 9143.852503299713, 'total_duration': 14402.134669065475, 'accumulated_submission_time': 9143.852503299713, 'accumulated_eval_time': 5256.418573856354, 'accumulated_logging_time': 1.1103498935699463}
I0311 07:14:46.329006 139622180505344 logging_writer.py:48] [29222] accumulated_eval_time=5256.418574, accumulated_logging_time=1.110350, accumulated_submission_time=9143.852503, global_step=29222, preemption_count=0, score=9143.852503, test/accuracy=0.986149, test/loss=0.047045, test/mean_average_precision=0.271357, test/num_examples=43793, total_duration=14402.134669, train/accuracy=0.991773, train/loss=0.026522, train/mean_average_precision=0.497474, validation/accuracy=0.987048, validation/loss=0.044064, validation/mean_average_precision=0.281727, validation/num_examples=43793
I0311 07:15:10.807222 139727869032192 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.05268210545182228, loss=0.029239224269986153
I0311 07:15:41.898860 139622180505344 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.05758921056985855, loss=0.030507072806358337
I0311 07:16:13.077265 139727869032192 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.05151021480560303, loss=0.02633734419941902
I0311 07:16:44.061995 139622180505344 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.051778264343738556, loss=0.02950185537338257
I0311 07:17:15.080953 139727869032192 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.056094445288181305, loss=0.02864747680723667
I0311 07:17:46.635380 139622180505344 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.06321387737989426, loss=0.027108032256364822
I0311 07:18:18.011600 139727869032192 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.05598972737789154, loss=0.027314944192767143
I0311 07:18:46.500025 139789500200768 spec.py:321] Evaluating on the training split.
I0311 07:20:46.136885 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 07:20:49.169281 139789500200768 spec.py:349] Evaluating on the test split.
I0311 07:20:52.193738 139789500200768 submission_runner.py:411] Time since start: 14768.02s, 	Step: 29990, 	{'train/accuracy': 0.9917317628860474, 'train/loss': 0.026847893372178078, 'train/mean_average_precision': 0.48323453691289237, 'validation/accuracy': 0.9871158003807068, 'validation/loss': 0.04365718737244606, 'validation/mean_average_precision': 0.274096221644849, 'validation/num_examples': 43793, 'test/accuracy': 0.98624587059021, 'test/loss': 0.04660336673259735, 'test/mean_average_precision': 0.27322129509925747, 'test/num_examples': 43793, 'score': 9383.99340891838, 'total_duration': 14768.02099108696, 'accumulated_submission_time': 9383.99340891838, 'accumulated_eval_time': 5382.11224770546, 'accumulated_logging_time': 1.1425657272338867}
I0311 07:20:52.214500 139628670904064 logging_writer.py:48] [29990] accumulated_eval_time=5382.112248, accumulated_logging_time=1.142566, accumulated_submission_time=9383.993409, global_step=29990, preemption_count=0, score=9383.993409, test/accuracy=0.986246, test/loss=0.046603, test/mean_average_precision=0.273221, test/num_examples=43793, total_duration=14768.020991, train/accuracy=0.991732, train/loss=0.026848, train/mean_average_precision=0.483235, validation/accuracy=0.987116, validation/loss=0.043657, validation/mean_average_precision=0.274096, validation/num_examples=43793
I0311 07:20:55.810844 139727860639488 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.05446308106184006, loss=0.02951609157025814
I0311 07:21:27.154254 139628670904064 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.06206800788640976, loss=0.02909320592880249
I0311 07:21:58.761403 139727860639488 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.05253695696592331, loss=0.02511165849864483
I0311 07:22:30.575909 139628670904064 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.05428816005587578, loss=0.0329306498169899
I0311 07:23:02.249008 139727860639488 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.051603544503450394, loss=0.027474742382764816
I0311 07:23:33.379786 139628670904064 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.06684870272874832, loss=0.031484100967645645
I0311 07:24:04.749396 139727860639488 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.06068214401602745, loss=0.029959706589579582
I0311 07:24:35.910623 139628670904064 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.07208223640918732, loss=0.031163180246949196
I0311 07:24:52.478336 139789500200768 spec.py:321] Evaluating on the training split.
I0311 07:26:48.930430 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 07:26:51.966953 139789500200768 spec.py:349] Evaluating on the test split.
I0311 07:26:54.955082 139789500200768 submission_runner.py:411] Time since start: 15130.78s, 	Step: 30754, 	{'train/accuracy': 0.9916686415672302, 'train/loss': 0.02702081948518753, 'train/mean_average_precision': 0.48696540905126645, 'validation/accuracy': 0.9869489669799805, 'validation/loss': 0.04419269785284996, 'validation/mean_average_precision': 0.27857723267896634, 'validation/num_examples': 43793, 'test/accuracy': 0.9861165285110474, 'test/loss': 0.04707059636712074, 'test/mean_average_precision': 0.273177578598428, 'test/num_examples': 43793, 'score': 9624.226995944977, 'total_duration': 15130.782220840454, 'accumulated_submission_time': 9624.226995944977, 'accumulated_eval_time': 5504.588839054108, 'accumulated_logging_time': 1.1740312576293945}
I0311 07:26:54.975892 139622180505344 logging_writer.py:48] [30754] accumulated_eval_time=5504.588839, accumulated_logging_time=1.174031, accumulated_submission_time=9624.226996, global_step=30754, preemption_count=0, score=9624.226996, test/accuracy=0.986117, test/loss=0.047071, test/mean_average_precision=0.273178, test/num_examples=43793, total_duration=15130.782221, train/accuracy=0.991669, train/loss=0.027021, train/mean_average_precision=0.486965, validation/accuracy=0.986949, validation/loss=0.044193, validation/mean_average_precision=0.278577, validation/num_examples=43793
I0311 07:27:09.652400 139628679296768 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.06266994774341583, loss=0.02669716253876686
I0311 07:27:40.993301 139622180505344 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.052852485328912735, loss=0.02737237699329853
I0311 07:28:12.122415 139628679296768 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.05890645086765289, loss=0.027605179697275162
I0311 07:28:43.291618 139622180505344 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.06714973598718643, loss=0.02991008386015892
I0311 07:29:14.337240 139628679296768 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.061933401972055435, loss=0.030872037634253502
I0311 07:29:45.766931 139622180505344 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.052509553730487823, loss=0.029200034216046333
I0311 07:30:16.606048 139628679296768 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.061782337725162506, loss=0.0258846003562212
I0311 07:30:47.442796 139622180505344 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.05141104385256767, loss=0.02751070261001587
I0311 07:30:55.096762 139789500200768 spec.py:321] Evaluating on the training split.
I0311 07:32:55.712738 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 07:32:58.735278 139789500200768 spec.py:349] Evaluating on the test split.
I0311 07:33:01.723294 139789500200768 submission_runner.py:411] Time since start: 15497.55s, 	Step: 31525, 	{'train/accuracy': 0.9916601181030273, 'train/loss': 0.026939183473587036, 'train/mean_average_precision': 0.4903063582821136, 'validation/accuracy': 0.9869124293327332, 'validation/loss': 0.04422162473201752, 'validation/mean_average_precision': 0.28009328281795953, 'validation/num_examples': 43793, 'test/accuracy': 0.9860710501670837, 'test/loss': 0.04695940390229225, 'test/mean_average_precision': 0.2723993864317921, 'test/num_examples': 43793, 'score': 9864.3176279068, 'total_duration': 15497.550547361374, 'accumulated_submission_time': 9864.3176279068, 'accumulated_eval_time': 5631.215336561203, 'accumulated_logging_time': 1.205542802810669}
I0311 07:33:01.744484 139622172112640 logging_writer.py:48] [31525] accumulated_eval_time=5631.215337, accumulated_logging_time=1.205543, accumulated_submission_time=9864.317628, global_step=31525, preemption_count=0, score=9864.317628, test/accuracy=0.986071, test/loss=0.046959, test/mean_average_precision=0.272399, test/num_examples=43793, total_duration=15497.550547, train/accuracy=0.991660, train/loss=0.026939, train/mean_average_precision=0.490306, validation/accuracy=0.986912, validation/loss=0.044222, validation/mean_average_precision=0.280093, validation/num_examples=43793
I0311 07:33:25.690881 139727869032192 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.06688831746578217, loss=0.029890136793255806
I0311 07:33:57.268916 139622172112640 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.07183541357517242, loss=0.03129516914486885
I0311 07:34:28.022848 139727869032192 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.06399283558130264, loss=0.030735425651073456
I0311 07:34:59.472269 139622172112640 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.05149482563138008, loss=0.025743689388036728
I0311 07:35:30.564238 139727869032192 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.05685250461101532, loss=0.029663898050785065
I0311 07:36:01.821236 139622172112640 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.06061195209622383, loss=0.029288193210959435
I0311 07:36:33.223625 139727869032192 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.07988832145929337, loss=0.0323600210249424
I0311 07:37:01.889840 139789500200768 spec.py:321] Evaluating on the training split.
I0311 07:39:01.010565 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 07:39:04.046809 139789500200768 spec.py:349] Evaluating on the test split.
I0311 07:39:07.049730 139789500200768 submission_runner.py:411] Time since start: 15862.88s, 	Step: 32293, 	{'train/accuracy': 0.9916957020759583, 'train/loss': 0.02683577686548233, 'train/mean_average_precision': 0.4875452796120583, 'validation/accuracy': 0.986953854560852, 'validation/loss': 0.043989598751068115, 'validation/mean_average_precision': 0.28286279777184314, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.04671858996152878, 'test/mean_average_precision': 0.27279993801565044, 'test/num_examples': 43793, 'score': 10104.432535648346, 'total_duration': 15862.876982688904, 'accumulated_submission_time': 10104.432535648346, 'accumulated_eval_time': 5756.375189065933, 'accumulated_logging_time': 1.2379601001739502}
I0311 07:39:07.070765 139622180505344 logging_writer.py:48] [32293] accumulated_eval_time=5756.375189, accumulated_logging_time=1.237960, accumulated_submission_time=10104.432536, global_step=32293, preemption_count=0, score=10104.432536, test/accuracy=0.986133, test/loss=0.046719, test/mean_average_precision=0.272800, test/num_examples=43793, total_duration=15862.876983, train/accuracy=0.991696, train/loss=0.026836, train/mean_average_precision=0.487545, validation/accuracy=0.986954, validation/loss=0.043990, validation/mean_average_precision=0.282863, validation/num_examples=43793
I0311 07:39:09.573691 139628670904064 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.0543997697532177, loss=0.027288902550935745
I0311 07:39:40.759689 139622180505344 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.07285287231206894, loss=0.027554325759410858
I0311 07:40:11.847614 139628670904064 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.06203503906726837, loss=0.02998235821723938
I0311 07:40:43.014512 139622180505344 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.07264436781406403, loss=0.030222825706005096
I0311 07:41:14.105980 139628670904064 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.0709146186709404, loss=0.028924554586410522
I0311 07:41:45.229044 139622180505344 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.057396382093429565, loss=0.029429571703076363
I0311 07:42:16.469827 139628670904064 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.057061225175857544, loss=0.030097881332039833
I0311 07:42:47.512602 139622180505344 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.0659283995628357, loss=0.025893503800034523
I0311 07:43:07.057991 139789500200768 spec.py:321] Evaluating on the training split.
I0311 07:45:09.856889 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 07:45:12.934922 139789500200768 spec.py:349] Evaluating on the test split.
I0311 07:45:15.892965 139789500200768 submission_runner.py:411] Time since start: 16231.72s, 	Step: 33063, 	{'train/accuracy': 0.9917645454406738, 'train/loss': 0.026396647095680237, 'train/mean_average_precision': 0.4937483853881236, 'validation/accuracy': 0.9870102405548096, 'validation/loss': 0.04406227171421051, 'validation/mean_average_precision': 0.2843081741127006, 'validation/num_examples': 43793, 'test/accuracy': 0.9861969947814941, 'test/loss': 0.04697497934103012, 'test/mean_average_precision': 0.27228021818470605, 'test/num_examples': 43793, 'score': 10344.389446496964, 'total_duration': 16231.720217704773, 'accumulated_submission_time': 10344.389446496964, 'accumulated_eval_time': 5885.2101221084595, 'accumulated_logging_time': 1.26981782913208}
I0311 07:45:15.914373 139622172112640 logging_writer.py:48] [33063] accumulated_eval_time=5885.210122, accumulated_logging_time=1.269818, accumulated_submission_time=10344.389446, global_step=33063, preemption_count=0, score=10344.389446, test/accuracy=0.986197, test/loss=0.046975, test/mean_average_precision=0.272280, test/num_examples=43793, total_duration=16231.720218, train/accuracy=0.991765, train/loss=0.026397, train/mean_average_precision=0.493748, validation/accuracy=0.987010, validation/loss=0.044062, validation/mean_average_precision=0.284308, validation/num_examples=43793
I0311 07:45:27.851871 139727869032192 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.055132120847702026, loss=0.027665194123983383
I0311 07:45:59.311432 139622172112640 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.06578561663627625, loss=0.03111039660871029
I0311 07:46:30.462391 139727869032192 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.05482471361756325, loss=0.027237532660365105
I0311 07:47:01.487829 139622172112640 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.05910678580403328, loss=0.02712169475853443
I0311 07:47:32.648313 139727869032192 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.06562290340662003, loss=0.026948178187012672
I0311 07:48:03.512800 139622172112640 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.057708583772182465, loss=0.03121454454958439
I0311 07:48:34.509759 139727869032192 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.06721490621566772, loss=0.026879139244556427
I0311 07:49:05.454557 139622172112640 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.0549871064722538, loss=0.02877132035791874
I0311 07:49:15.995180 139789500200768 spec.py:321] Evaluating on the training split.
I0311 07:51:15.137632 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 07:51:18.153893 139789500200768 spec.py:349] Evaluating on the test split.
I0311 07:51:21.139378 139789500200768 submission_runner.py:411] Time since start: 16596.97s, 	Step: 33835, 	{'train/accuracy': 0.9917652606964111, 'train/loss': 0.026541104540228844, 'train/mean_average_precision': 0.4958937299340971, 'validation/accuracy': 0.9870187640190125, 'validation/loss': 0.04407906159758568, 'validation/mean_average_precision': 0.27932303559589555, 'validation/num_examples': 43793, 'test/accuracy': 0.986108124256134, 'test/loss': 0.04690178856253624, 'test/mean_average_precision': 0.2736489972800847, 'test/num_examples': 43793, 'score': 10584.439611911774, 'total_duration': 16596.966630220413, 'accumulated_submission_time': 10584.439611911774, 'accumulated_eval_time': 6010.354276895523, 'accumulated_logging_time': 1.3019187450408936}
I0311 07:51:21.160772 139628670904064 logging_writer.py:48] [33835] accumulated_eval_time=6010.354277, accumulated_logging_time=1.301919, accumulated_submission_time=10584.439612, global_step=33835, preemption_count=0, score=10584.439612, test/accuracy=0.986108, test/loss=0.046902, test/mean_average_precision=0.273649, test/num_examples=43793, total_duration=16596.966630, train/accuracy=0.991765, train/loss=0.026541, train/mean_average_precision=0.495894, validation/accuracy=0.987019, validation/loss=0.044079, validation/mean_average_precision=0.279323, validation/num_examples=43793
I0311 07:51:41.802210 139628679296768 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.070853590965271, loss=0.029647838324308395
I0311 07:52:12.964147 139628670904064 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.08290451020002365, loss=0.03175065666437149
I0311 07:52:44.248547 139628679296768 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.07814928144216537, loss=0.02780134789645672
I0311 07:53:15.211704 139628670904064 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.05473950132727623, loss=0.026151573285460472
I0311 07:53:46.491918 139628679296768 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.06740298867225647, loss=0.031592581421136856
I0311 07:54:17.626327 139628670904064 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.06437163054943085, loss=0.03077828511595726
I0311 07:54:48.501061 139628679296768 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.0592452697455883, loss=0.030639177188277245
I0311 07:55:19.725601 139628670904064 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.08146058768033981, loss=0.029084092006087303
I0311 07:55:21.302636 139789500200768 spec.py:321] Evaluating on the training split.
I0311 07:57:25.628391 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 07:57:28.766590 139789500200768 spec.py:349] Evaluating on the test split.
I0311 07:57:31.736145 139789500200768 submission_runner.py:411] Time since start: 16967.56s, 	Step: 34606, 	{'train/accuracy': 0.9919253587722778, 'train/loss': 0.025892183184623718, 'train/mean_average_precision': 0.5138001849053772, 'validation/accuracy': 0.9870142936706543, 'validation/loss': 0.04429777339100838, 'validation/mean_average_precision': 0.2835272431791837, 'validation/num_examples': 43793, 'test/accuracy': 0.9861780405044556, 'test/loss': 0.04711897298693657, 'test/mean_average_precision': 0.27525119724462127, 'test/num_examples': 43793, 'score': 10824.551196575165, 'total_duration': 16967.56339740753, 'accumulated_submission_time': 10824.551196575165, 'accumulated_eval_time': 6140.787740945816, 'accumulated_logging_time': 1.333965539932251}
I0311 07:57:31.758192 139622172112640 logging_writer.py:48] [34606] accumulated_eval_time=6140.787741, accumulated_logging_time=1.333966, accumulated_submission_time=10824.551197, global_step=34606, preemption_count=0, score=10824.551197, test/accuracy=0.986178, test/loss=0.047119, test/mean_average_precision=0.275251, test/num_examples=43793, total_duration=16967.563397, train/accuracy=0.991925, train/loss=0.025892, train/mean_average_precision=0.513800, validation/accuracy=0.987014, validation/loss=0.044298, validation/mean_average_precision=0.283527, validation/num_examples=43793
I0311 07:58:01.627854 139622180505344 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.06458484381437302, loss=0.030742639675736427
I0311 07:58:32.963973 139622172112640 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.07078976184129715, loss=0.026700377464294434
I0311 07:59:04.299226 139622180505344 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.06224238872528076, loss=0.02579897828400135
I0311 07:59:35.768528 139622172112640 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.06167567893862724, loss=0.02963358536362648
I0311 08:00:07.166429 139622180505344 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.05950544402003288, loss=0.02428237535059452
I0311 08:00:38.632220 139622172112640 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.06823205202817917, loss=0.030207274481654167
I0311 08:01:10.146846 139622180505344 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.06739340722560883, loss=0.027172954753041267
I0311 08:01:31.980986 139789500200768 spec.py:321] Evaluating on the training split.
I0311 08:03:31.245256 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 08:03:34.310685 139789500200768 spec.py:349] Evaluating on the test split.
I0311 08:03:37.327737 139789500200768 submission_runner.py:411] Time since start: 17333.15s, 	Step: 35370, 	{'train/accuracy': 0.99210524559021, 'train/loss': 0.02542589232325554, 'train/mean_average_precision': 0.520013516105533, 'validation/accuracy': 0.9870370626449585, 'validation/loss': 0.044192180037498474, 'validation/mean_average_precision': 0.2838264860463516, 'validation/num_examples': 43793, 'test/accuracy': 0.9861679077148438, 'test/loss': 0.04706428945064545, 'test/mean_average_precision': 0.27231865203898387, 'test/num_examples': 43793, 'score': 11064.743356227875, 'total_duration': 17333.15499162674, 'accumulated_submission_time': 11064.743356227875, 'accumulated_eval_time': 6266.13445353508, 'accumulated_logging_time': 1.3671300411224365}
I0311 08:03:37.349549 139621870597888 logging_writer.py:48] [35370] accumulated_eval_time=6266.134454, accumulated_logging_time=1.367130, accumulated_submission_time=11064.743356, global_step=35370, preemption_count=0, score=11064.743356, test/accuracy=0.986168, test/loss=0.047064, test/mean_average_precision=0.272319, test/num_examples=43793, total_duration=17333.154992, train/accuracy=0.992105, train/loss=0.025426, train/mean_average_precision=0.520014, validation/accuracy=0.987037, validation/loss=0.044192, validation/mean_average_precision=0.283826, validation/num_examples=43793
I0311 08:03:47.312837 139628679296768 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.06965197622776031, loss=0.025841936469078064
I0311 08:04:18.512817 139621870597888 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.057345472276210785, loss=0.028717409819364548
I0311 08:04:49.993995 139628679296768 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.06349799036979675, loss=0.028278794139623642
I0311 08:05:21.473114 139621870597888 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.05098889768123627, loss=0.02614460699260235
I0311 08:05:52.875497 139628679296768 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.06466458737850189, loss=0.026217417791485786
I0311 08:06:24.342784 139621870597888 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.08236787468194962, loss=0.027203528210520744
I0311 08:06:55.803598 139628679296768 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.0667770728468895, loss=0.029321229085326195
I0311 08:07:27.430912 139621870597888 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.06650471687316895, loss=0.0277489572763443
I0311 08:07:37.474154 139789500200768 spec.py:321] Evaluating on the training split.
I0311 08:09:35.477452 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 08:09:38.473825 139789500200768 spec.py:349] Evaluating on the test split.
I0311 08:09:41.444200 139789500200768 submission_runner.py:411] Time since start: 17697.27s, 	Step: 36133, 	{'train/accuracy': 0.9921540021896362, 'train/loss': 0.025215448811650276, 'train/mean_average_precision': 0.5257004962214487, 'validation/accuracy': 0.9870886206626892, 'validation/loss': 0.04423440620303154, 'validation/mean_average_precision': 0.2862508491770593, 'validation/num_examples': 43793, 'test/accuracy': 0.9861965775489807, 'test/loss': 0.047443073242902756, 'test/mean_average_precision': 0.2730475080736813, 'test/num_examples': 43793, 'score': 11304.83763718605, 'total_duration': 17697.271452903748, 'accumulated_submission_time': 11304.83763718605, 'accumulated_eval_time': 6390.104465007782, 'accumulated_logging_time': 1.4000084400177002}
I0311 08:09:41.465980 139622172112640 logging_writer.py:48] [36133] accumulated_eval_time=6390.104465, accumulated_logging_time=1.400008, accumulated_submission_time=11304.837637, global_step=36133, preemption_count=0, score=11304.837637, test/accuracy=0.986197, test/loss=0.047443, test/mean_average_precision=0.273048, test/num_examples=43793, total_duration=17697.271453, train/accuracy=0.992154, train/loss=0.025215, train/mean_average_precision=0.525700, validation/accuracy=0.987089, validation/loss=0.044234, validation/mean_average_precision=0.286251, validation/num_examples=43793
I0311 08:10:02.689994 139622180505344 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.06342488527297974, loss=0.029210185632109642
I0311 08:10:33.536118 139622172112640 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.07939911633729935, loss=0.03372439369559288
I0311 08:11:04.573071 139622180505344 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.07168526202440262, loss=0.025924144312739372
I0311 08:11:35.589185 139622172112640 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.06341090798377991, loss=0.027288759127259254
I0311 08:12:06.420573 139622180505344 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.0717773362994194, loss=0.028642307966947556
I0311 08:12:37.135326 139622172112640 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.06513672322034836, loss=0.028566652908921242
I0311 08:13:08.654954 139622180505344 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.06628230959177017, loss=0.029207885265350342
I0311 08:13:39.830057 139622172112640 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.06597647070884705, loss=0.02559506706893444
I0311 08:13:41.753189 139789500200768 spec.py:321] Evaluating on the training split.
I0311 08:15:39.603493 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 08:15:42.622966 139789500200768 spec.py:349] Evaluating on the test split.
I0311 08:15:45.621000 139789500200768 submission_runner.py:411] Time since start: 18061.45s, 	Step: 36907, 	{'train/accuracy': 0.9923199415206909, 'train/loss': 0.024498628452420235, 'train/mean_average_precision': 0.5594185517547302, 'validation/accuracy': 0.9871170520782471, 'validation/loss': 0.04424981027841568, 'validation/mean_average_precision': 0.2851949177370656, 'validation/num_examples': 43793, 'test/accuracy': 0.9861688017845154, 'test/loss': 0.04755162075161934, 'test/mean_average_precision': 0.2679133358897493, 'test/num_examples': 43793, 'score': 11545.093611717224, 'total_duration': 18061.448248147964, 'accumulated_submission_time': 11545.093611717224, 'accumulated_eval_time': 6513.972229719162, 'accumulated_logging_time': 1.4337913990020752}
I0311 08:15:45.643602 139621870597888 logging_writer.py:48] [36907] accumulated_eval_time=6513.972230, accumulated_logging_time=1.433791, accumulated_submission_time=11545.093612, global_step=36907, preemption_count=0, score=11545.093612, test/accuracy=0.986169, test/loss=0.047552, test/mean_average_precision=0.267913, test/num_examples=43793, total_duration=18061.448248, train/accuracy=0.992320, train/loss=0.024499, train/mean_average_precision=0.559419, validation/accuracy=0.987117, validation/loss=0.044250, validation/mean_average_precision=0.285195, validation/num_examples=43793
I0311 08:16:14.705289 139628679296768 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.05911963805556297, loss=0.027538295835256577
I0311 08:16:45.959064 139621870597888 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.06570044904947281, loss=0.025445938110351562
I0311 08:17:17.032891 139628679296768 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.06788597255945206, loss=0.029427381232380867
I0311 08:17:48.151744 139621870597888 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.06764654815196991, loss=0.027303552255034447
I0311 08:18:19.707470 139628679296768 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.08968859165906906, loss=0.02809061110019684
I0311 08:18:51.328102 139621870597888 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.0629630908370018, loss=0.029201049357652664
I0311 08:19:22.989204 139628679296768 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.06563954055309296, loss=0.029880322515964508
I0311 08:19:45.705547 139789500200768 spec.py:321] Evaluating on the training split.
I0311 08:21:42.278884 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 08:21:45.354595 139789500200768 spec.py:349] Evaluating on the test split.
I0311 08:21:48.321516 139789500200768 submission_runner.py:411] Time since start: 18424.15s, 	Step: 37673, 	{'train/accuracy': 0.9923747181892395, 'train/loss': 0.024407749995589256, 'train/mean_average_precision': 0.5358389174143836, 'validation/accuracy': 0.9870086312294006, 'validation/loss': 0.04475032538175583, 'validation/mean_average_precision': 0.2843611275052901, 'validation/num_examples': 43793, 'test/accuracy': 0.9860794544219971, 'test/loss': 0.04776091128587723, 'test/mean_average_precision': 0.2707454139816693, 'test/num_examples': 43793, 'score': 11785.124882698059, 'total_duration': 18424.14876651764, 'accumulated_submission_time': 11785.124882698059, 'accumulated_eval_time': 6636.588159322739, 'accumulated_logging_time': 1.467538833618164}
I0311 08:21:48.343758 139622172112640 logging_writer.py:48] [37673] accumulated_eval_time=6636.588159, accumulated_logging_time=1.467539, accumulated_submission_time=11785.124883, global_step=37673, preemption_count=0, score=11785.124883, test/accuracy=0.986079, test/loss=0.047761, test/mean_average_precision=0.270745, test/num_examples=43793, total_duration=18424.148767, train/accuracy=0.992375, train/loss=0.024408, train/mean_average_precision=0.535839, validation/accuracy=0.987009, validation/loss=0.044750, validation/mean_average_precision=0.284361, validation/num_examples=43793
I0311 08:21:57.176215 139622180505344 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.06355555355548859, loss=0.027859436348080635
I0311 08:22:29.041554 139622172112640 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.06648535281419754, loss=0.026628950610756874
I0311 08:23:00.573739 139622180505344 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.0629795640707016, loss=0.028692608699202538
I0311 08:23:31.566113 139622172112640 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.06942006945610046, loss=0.029989859089255333
I0311 08:24:02.922396 139622180505344 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.06512890011072159, loss=0.026674816384911537
I0311 08:24:34.243180 139622172112640 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.066659115254879, loss=0.02712325006723404
I0311 08:25:05.477843 139622180505344 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.07672765851020813, loss=0.029724566265940666
I0311 08:25:36.635508 139622172112640 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.06226792186498642, loss=0.026646750047802925
I0311 08:25:48.539959 139789500200768 spec.py:321] Evaluating on the training split.
I0311 08:27:46.418177 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 08:27:49.456110 139789500200768 spec.py:349] Evaluating on the test split.
I0311 08:27:52.479905 139789500200768 submission_runner.py:411] Time since start: 18788.31s, 	Step: 38439, 	{'train/accuracy': 0.9925680160522461, 'train/loss': 0.023779582232236862, 'train/mean_average_precision': 0.5684574426282603, 'validation/accuracy': 0.9871584177017212, 'validation/loss': 0.04420415684580803, 'validation/mean_average_precision': 0.28866362399726003, 'validation/num_examples': 43793, 'test/accuracy': 0.9862340688705444, 'test/loss': 0.04719763249158859, 'test/mean_average_precision': 0.27555334978163376, 'test/num_examples': 43793, 'score': 12025.290924072266, 'total_duration': 18788.307156085968, 'accumulated_submission_time': 12025.290924072266, 'accumulated_eval_time': 6760.528062582016, 'accumulated_logging_time': 1.5004465579986572}
I0311 08:27:52.503273 139621870597888 logging_writer.py:48] [38439] accumulated_eval_time=6760.528063, accumulated_logging_time=1.500447, accumulated_submission_time=12025.290924, global_step=38439, preemption_count=0, score=12025.290924, test/accuracy=0.986234, test/loss=0.047198, test/mean_average_precision=0.275553, test/num_examples=43793, total_duration=18788.307156, train/accuracy=0.992568, train/loss=0.023780, train/mean_average_precision=0.568457, validation/accuracy=0.987158, validation/loss=0.044204, validation/mean_average_precision=0.288664, validation/num_examples=43793
I0311 08:28:11.742505 139628670904064 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.06450726091861725, loss=0.026971392333507538
I0311 08:28:42.820568 139621870597888 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.06452438980340958, loss=0.025636058300733566
I0311 08:29:14.137456 139628670904064 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.06494483351707458, loss=0.028798192739486694
I0311 08:29:45.541776 139621870597888 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.06403207033872604, loss=0.02891198731958866
I0311 08:30:16.721151 139628670904064 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.06993724405765533, loss=0.02473294921219349
I0311 08:30:47.832709 139621870597888 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.07181739062070847, loss=0.02830473892390728
I0311 08:31:18.866986 139628670904064 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.07043452560901642, loss=0.02991577982902527
I0311 08:31:49.981802 139621870597888 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.057871874421834946, loss=0.025042900815606117
I0311 08:31:52.487367 139789500200768 spec.py:321] Evaluating on the training split.
I0311 08:33:50.116342 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 08:33:53.147318 139789500200768 spec.py:349] Evaluating on the test split.
I0311 08:33:56.117893 139789500200768 submission_runner.py:411] Time since start: 19151.95s, 	Step: 39209, 	{'train/accuracy': 0.99250328540802, 'train/loss': 0.0239628404378891, 'train/mean_average_precision': 0.5541409386821077, 'validation/accuracy': 0.98708575963974, 'validation/loss': 0.04442518949508667, 'validation/mean_average_precision': 0.283720789086509, 'validation/num_examples': 43793, 'test/accuracy': 0.9862378239631653, 'test/loss': 0.04747000336647034, 'test/mean_average_precision': 0.2816381130732368, 'test/num_examples': 43793, 'score': 12265.243272781372, 'total_duration': 19151.945145130157, 'accumulated_submission_time': 12265.243272781372, 'accumulated_eval_time': 6884.158543348312, 'accumulated_logging_time': 1.536177396774292}
I0311 08:33:56.140472 139622180505344 logging_writer.py:48] [39209] accumulated_eval_time=6884.158543, accumulated_logging_time=1.536177, accumulated_submission_time=12265.243273, global_step=39209, preemption_count=0, score=12265.243273, test/accuracy=0.986238, test/loss=0.047470, test/mean_average_precision=0.281638, test/num_examples=43793, total_duration=19151.945145, train/accuracy=0.992503, train/loss=0.023963, train/mean_average_precision=0.554141, validation/accuracy=0.987086, validation/loss=0.044425, validation/mean_average_precision=0.283721, validation/num_examples=43793
I0311 08:34:24.832796 139628679296768 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.0643988847732544, loss=0.02700818143785
I0311 08:34:56.223984 139622180505344 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.0708920955657959, loss=0.024391252547502518
I0311 08:35:27.198508 139628679296768 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.07093833386898041, loss=0.02645372413098812
I0311 08:35:58.130957 139622180505344 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.06996430456638336, loss=0.026869434863328934
I0311 08:36:29.474045 139628679296768 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.06680574268102646, loss=0.02937065251171589
I0311 08:37:00.392158 139622180505344 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.0682927668094635, loss=0.02824936993420124
I0311 08:37:31.830992 139628679296768 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.07474282383918762, loss=0.02841055393218994
I0311 08:37:56.192210 139789500200768 spec.py:321] Evaluating on the training split.
I0311 08:39:53.938849 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 08:39:57.047443 139789500200768 spec.py:349] Evaluating on the test split.
I0311 08:40:00.113279 139789500200768 submission_runner.py:411] Time since start: 19515.94s, 	Step: 39979, 	{'train/accuracy': 0.9925006031990051, 'train/loss': 0.024188725277781487, 'train/mean_average_precision': 0.5570633831413991, 'validation/accuracy': 0.9870301485061646, 'validation/loss': 0.04469914734363556, 'validation/mean_average_precision': 0.28286701109031426, 'validation/num_examples': 43793, 'test/accuracy': 0.9860883355140686, 'test/loss': 0.047764796763658524, 'test/mean_average_precision': 0.28429391342194754, 'test/num_examples': 43793, 'score': 12505.26472067833, 'total_duration': 19515.94053053856, 'accumulated_submission_time': 12505.26472067833, 'accumulated_eval_time': 7008.079576253891, 'accumulated_logging_time': 1.56976318359375}
I0311 08:40:00.135954 139621870597888 logging_writer.py:48] [39979] accumulated_eval_time=7008.079576, accumulated_logging_time=1.569763, accumulated_submission_time=12505.264721, global_step=39979, preemption_count=0, score=12505.264721, test/accuracy=0.986088, test/loss=0.047765, test/mean_average_precision=0.284294, test/num_examples=43793, total_duration=19515.940531, train/accuracy=0.992501, train/loss=0.024189, train/mean_average_precision=0.557063, validation/accuracy=0.987030, validation/loss=0.044699, validation/mean_average_precision=0.282867, validation/num_examples=43793
I0311 08:40:07.013993 139622172112640 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.06879891455173492, loss=0.028964903205633163
I0311 08:40:38.594357 139621870597888 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.08942320942878723, loss=0.025304153561592102
I0311 08:41:12.052089 139622172112640 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.07206001877784729, loss=0.02580196224153042
I0311 08:41:45.130990 139621870597888 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.06641750037670135, loss=0.025615666061639786
I0311 08:42:18.497282 139622172112640 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.10133054107427597, loss=0.0289427749812603
I0311 08:42:51.640847 139621870597888 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.07396448403596878, loss=0.027114614844322205
I0311 08:43:23.420457 139622172112640 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.08639900386333466, loss=0.02872949093580246
I0311 08:43:55.129289 139621870597888 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.05737801268696785, loss=0.02473723329603672
I0311 08:44:00.122774 139789500200768 spec.py:321] Evaluating on the training split.
I0311 08:45:57.687314 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 08:46:00.672971 139789500200768 spec.py:349] Evaluating on the test split.
I0311 08:46:03.661416 139789500200768 submission_runner.py:411] Time since start: 19879.49s, 	Step: 40717, 	{'train/accuracy': 0.9924003481864929, 'train/loss': 0.024326428771018982, 'train/mean_average_precision': 0.5494295396144823, 'validation/accuracy': 0.9869505763053894, 'validation/loss': 0.044512081891298294, 'validation/mean_average_precision': 0.28068610229665536, 'validation/num_examples': 43793, 'test/accuracy': 0.9861072897911072, 'test/loss': 0.0472981296479702, 'test/mean_average_precision': 0.2748004192246521, 'test/num_examples': 43793, 'score': 12745.222152233124, 'total_duration': 19879.488668203354, 'accumulated_submission_time': 12745.222152233124, 'accumulated_eval_time': 7131.618180990219, 'accumulated_logging_time': 1.6031405925750732}
I0311 08:46:03.684359 139628670904064 logging_writer.py:48] [40717] accumulated_eval_time=7131.618181, accumulated_logging_time=1.603141, accumulated_submission_time=12745.222152, global_step=40717, preemption_count=0, score=12745.222152, test/accuracy=0.986107, test/loss=0.047298, test/mean_average_precision=0.274800, test/num_examples=43793, total_duration=19879.488668, train/accuracy=0.992400, train/loss=0.024326, train/mean_average_precision=0.549430, validation/accuracy=0.986951, validation/loss=0.044512, validation/mean_average_precision=0.280686, validation/num_examples=43793
I0311 08:46:30.094608 139628679296768 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.07821153104305267, loss=0.024849722161889076
I0311 08:47:01.200162 139628670904064 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.0718703642487526, loss=0.028688019141554832
I0311 08:47:32.967474 139628679296768 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.06634534895420074, loss=0.02516034059226513
I0311 08:48:04.173706 139628670904064 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.06945185363292694, loss=0.02422257885336876
I0311 08:48:35.310340 139628679296768 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.06486696749925613, loss=0.026626549661159515
I0311 08:49:06.477004 139628670904064 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.09720765054225922, loss=0.02687925472855568
I0311 08:49:37.566143 139628679296768 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.07153091579675674, loss=0.0228994470089674
I0311 08:50:03.803515 139789500200768 spec.py:321] Evaluating on the training split.
I0311 08:52:05.531866 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 08:52:08.606666 139789500200768 spec.py:349] Evaluating on the test split.
I0311 08:52:11.619507 139789500200768 submission_runner.py:411] Time since start: 20247.45s, 	Step: 41485, 	{'train/accuracy': 0.9922370314598083, 'train/loss': 0.024809887632727623, 'train/mean_average_precision': 0.5382047059952417, 'validation/accuracy': 0.9869790077209473, 'validation/loss': 0.04488864541053772, 'validation/mean_average_precision': 0.29048437753305895, 'validation/num_examples': 43793, 'test/accuracy': 0.986151933670044, 'test/loss': 0.04785619303584099, 'test/mean_average_precision': 0.2822238579045632, 'test/num_examples': 43793, 'score': 12985.309354305267, 'total_duration': 20247.44675898552, 'accumulated_submission_time': 12985.309354305267, 'accumulated_eval_time': 7259.434130430222, 'accumulated_logging_time': 1.6383109092712402}
I0311 08:52:11.642504 139622172112640 logging_writer.py:48] [41485] accumulated_eval_time=7259.434130, accumulated_logging_time=1.638311, accumulated_submission_time=12985.309354, global_step=41485, preemption_count=0, score=12985.309354, test/accuracy=0.986152, test/loss=0.047856, test/mean_average_precision=0.282224, test/num_examples=43793, total_duration=20247.446759, train/accuracy=0.992237, train/loss=0.024810, train/mean_average_precision=0.538205, validation/accuracy=0.986979, validation/loss=0.044889, validation/mean_average_precision=0.290484, validation/num_examples=43793
I0311 08:52:16.723176 139622180505344 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.07023001462221146, loss=0.022366903722286224
I0311 08:52:48.326853 139622172112640 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.06138322502374649, loss=0.025607654824852943
I0311 08:53:19.795083 139622180505344 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.07639501988887787, loss=0.02855464443564415
I0311 08:53:51.016299 139622172112640 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.09422528743743896, loss=0.023540841415524483
I0311 08:54:22.206406 139622180505344 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.09462689608335495, loss=0.02572394348680973
I0311 08:54:53.567669 139622172112640 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.06458985060453415, loss=0.02339666336774826
I0311 08:55:24.844897 139622180505344 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.07168314605951309, loss=0.025222180411219597
I0311 08:55:56.262501 139622172112640 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.06741953641176224, loss=0.026615289971232414
I0311 08:56:11.691597 139789500200768 spec.py:321] Evaluating on the training split.
I0311 08:58:12.168867 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 08:58:15.257961 139789500200768 spec.py:349] Evaluating on the test split.
I0311 08:58:18.265217 139789500200768 submission_runner.py:411] Time since start: 20614.09s, 	Step: 42250, 	{'train/accuracy': 0.992445707321167, 'train/loss': 0.024151816964149475, 'train/mean_average_precision': 0.5460987084725499, 'validation/accuracy': 0.98708575963974, 'validation/loss': 0.04485373944044113, 'validation/mean_average_precision': 0.287187447261353, 'validation/num_examples': 43793, 'test/accuracy': 0.9862955808639526, 'test/loss': 0.04780704528093338, 'test/mean_average_precision': 0.28304548365999127, 'test/num_examples': 43793, 'score': 13225.327796459198, 'total_duration': 20614.092463970184, 'accumulated_submission_time': 13225.327796459198, 'accumulated_eval_time': 7386.007725477219, 'accumulated_logging_time': 1.672457218170166}
I0311 08:58:18.288598 139628670904064 logging_writer.py:48] [42250] accumulated_eval_time=7386.007725, accumulated_logging_time=1.672457, accumulated_submission_time=13225.327796, global_step=42250, preemption_count=0, score=13225.327796, test/accuracy=0.986296, test/loss=0.047807, test/mean_average_precision=0.283045, test/num_examples=43793, total_duration=20614.092464, train/accuracy=0.992446, train/loss=0.024152, train/mean_average_precision=0.546099, validation/accuracy=0.987086, validation/loss=0.044854, validation/mean_average_precision=0.287187, validation/num_examples=43793
I0311 08:58:34.372807 139628679296768 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.08510758727788925, loss=0.0278947576880455
I0311 08:59:05.898424 139628670904064 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.08129177242517471, loss=0.027161777019500732
I0311 08:59:37.111001 139628679296768 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.08787801861763, loss=0.025371160358190536
I0311 09:00:08.578819 139628670904064 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.0797303318977356, loss=0.026261279359459877
I0311 09:00:40.085977 139628679296768 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.07204374670982361, loss=0.02472609095275402
I0311 09:01:11.442803 139628670904064 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.07070910185575485, loss=0.02967841923236847
I0311 09:01:43.638252 139628679296768 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.07671856880187988, loss=0.025446640327572823
I0311 09:02:16.185615 139628670904064 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.0683106854557991, loss=0.02493395283818245
I0311 09:02:18.471432 139789500200768 spec.py:321] Evaluating on the training split.
I0311 09:04:26.271437 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 09:04:29.728201 139789500200768 spec.py:349] Evaluating on the test split.
I0311 09:04:33.079889 139789500200768 submission_runner.py:411] Time since start: 20988.91s, 	Step: 43008, 	{'train/accuracy': 0.9924915432929993, 'train/loss': 0.023939501494169235, 'train/mean_average_precision': 0.5532173934177393, 'validation/accuracy': 0.9871170520782471, 'validation/loss': 0.04468199610710144, 'validation/mean_average_precision': 0.2867559415482638, 'validation/num_examples': 43793, 'test/accuracy': 0.9862471222877502, 'test/loss': 0.04774852097034454, 'test/mean_average_precision': 0.2772697280132931, 'test/num_examples': 43793, 'score': 13465.478171348572, 'total_duration': 20988.90712785721, 'accumulated_submission_time': 13465.478171348572, 'accumulated_eval_time': 7520.616132259369, 'accumulated_logging_time': 1.7076139450073242}
I0311 09:04:33.106959 139621870597888 logging_writer.py:48] [43008] accumulated_eval_time=7520.616132, accumulated_logging_time=1.707614, accumulated_submission_time=13465.478171, global_step=43008, preemption_count=0, score=13465.478171, test/accuracy=0.986247, test/loss=0.047749, test/mean_average_precision=0.277270, test/num_examples=43793, total_duration=20988.907128, train/accuracy=0.992492, train/loss=0.023940, train/mean_average_precision=0.553217, validation/accuracy=0.987117, validation/loss=0.044682, validation/mean_average_precision=0.286756, validation/num_examples=43793
I0311 09:05:02.564002 139622172112640 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.07246480137109756, loss=0.02726135216653347
I0311 09:05:34.147209 139621870597888 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.07015997171401978, loss=0.02581297606229782
I0311 09:06:05.785598 139622172112640 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.06157220900058746, loss=0.024664873257279396
I0311 09:06:37.391367 139621870597888 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.08066990971565247, loss=0.02605592831969261
I0311 09:07:08.890315 139622172112640 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.07836119830608368, loss=0.02578955702483654
I0311 09:07:40.666662 139621870597888 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.07810655236244202, loss=0.027507146820425987
I0311 09:08:12.003245 139622172112640 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.07057306170463562, loss=0.025776326656341553
I0311 09:08:33.080704 139789500200768 spec.py:321] Evaluating on the training split.
I0311 09:10:30.110473 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 09:10:33.222363 139789500200768 spec.py:349] Evaluating on the test split.
I0311 09:10:36.226819 139789500200768 submission_runner.py:411] Time since start: 21352.05s, 	Step: 43769, 	{'train/accuracy': 0.9925384521484375, 'train/loss': 0.02369183860719204, 'train/mean_average_precision': 0.5701476633664833, 'validation/accuracy': 0.9872027039527893, 'validation/loss': 0.04474662244319916, 'validation/mean_average_precision': 0.289002886959356, 'validation/num_examples': 43793, 'test/accuracy': 0.986272394657135, 'test/loss': 0.04795881360769272, 'test/mean_average_precision': 0.27967659025454555, 'test/num_examples': 43793, 'score': 13705.421085119247, 'total_duration': 21352.05406999588, 'accumulated_submission_time': 13705.421085119247, 'accumulated_eval_time': 7643.762209892273, 'accumulated_logging_time': 1.7461457252502441}
I0311 09:10:36.250494 139628670904064 logging_writer.py:48] [43769] accumulated_eval_time=7643.762210, accumulated_logging_time=1.746146, accumulated_submission_time=13705.421085, global_step=43769, preemption_count=0, score=13705.421085, test/accuracy=0.986272, test/loss=0.047959, test/mean_average_precision=0.279677, test/num_examples=43793, total_duration=21352.054070, train/accuracy=0.992538, train/loss=0.023692, train/mean_average_precision=0.570148, validation/accuracy=0.987203, validation/loss=0.044747, validation/mean_average_precision=0.289003, validation/num_examples=43793
I0311 09:10:46.366598 139628679296768 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.07018069922924042, loss=0.025502566248178482
I0311 09:11:18.063922 139628670904064 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.06979493796825409, loss=0.025414839386940002
I0311 09:11:49.616319 139628679296768 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.06745690852403641, loss=0.024119572713971138
I0311 09:12:20.802670 139628670904064 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.0822606161236763, loss=0.029720591381192207
I0311 09:12:52.207135 139628679296768 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.07689688354730606, loss=0.026284994557499886
I0311 09:13:23.384460 139628670904064 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.07564868777990341, loss=0.02468041330575943
I0311 09:13:54.808664 139628679296768 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.072455495595932, loss=0.027125218883156776
I0311 09:14:25.925446 139628670904064 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.07974101603031158, loss=0.024005873128771782
I0311 09:14:36.510369 139789500200768 spec.py:321] Evaluating on the training split.
I0311 09:16:33.139665 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 09:16:36.288517 139789500200768 spec.py:349] Evaluating on the test split.
I0311 09:16:39.420821 139789500200768 submission_runner.py:411] Time since start: 21715.25s, 	Step: 44535, 	{'train/accuracy': 0.9927001595497131, 'train/loss': 0.023185497149825096, 'train/mean_average_precision': 0.5688899401380112, 'validation/accuracy': 0.9870370626449585, 'validation/loss': 0.04471414536237717, 'validation/mean_average_precision': 0.29342218863011227, 'validation/num_examples': 43793, 'test/accuracy': 0.9860782027244568, 'test/loss': 0.047963697463274, 'test/mean_average_precision': 0.2759291178161914, 'test/num_examples': 43793, 'score': 13945.649666070938, 'total_duration': 21715.24806857109, 'accumulated_submission_time': 13945.649666070938, 'accumulated_eval_time': 7766.672614097595, 'accumulated_logging_time': 1.7812371253967285}
I0311 09:16:39.444492 139621870597888 logging_writer.py:48] [44535] accumulated_eval_time=7766.672614, accumulated_logging_time=1.781237, accumulated_submission_time=13945.649666, global_step=44535, preemption_count=0, score=13945.649666, test/accuracy=0.986078, test/loss=0.047964, test/mean_average_precision=0.275929, test/num_examples=43793, total_duration=21715.248069, train/accuracy=0.992700, train/loss=0.023185, train/mean_average_precision=0.568890, validation/accuracy=0.987037, validation/loss=0.044714, validation/mean_average_precision=0.293422, validation/num_examples=43793
I0311 09:17:00.291862 139622180505344 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.08987566083669662, loss=0.027989977970719337
I0311 09:17:31.640002 139621870597888 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.08950531482696533, loss=0.02704855054616928
I0311 09:18:02.869926 139622180505344 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.08785820007324219, loss=0.025929642841219902
I0311 09:18:34.080684 139621870597888 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.07827771455049515, loss=0.024638287723064423
I0311 09:19:05.781408 139622180505344 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.07977865636348724, loss=0.026827072724699974
I0311 09:19:37.165827 139621870597888 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.09274688363075256, loss=0.02932650037109852
I0311 09:20:08.651347 139622180505344 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.08339694142341614, loss=0.027573317289352417
I0311 09:20:39.492312 139789500200768 spec.py:321] Evaluating on the training split.
I0311 09:22:39.415361 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 09:22:42.492599 139789500200768 spec.py:349] Evaluating on the test split.
I0311 09:22:45.492532 139789500200768 submission_runner.py:411] Time since start: 22081.32s, 	Step: 45299, 	{'train/accuracy': 0.9928914308547974, 'train/loss': 0.022592060267925262, 'train/mean_average_precision': 0.5915234600156412, 'validation/accuracy': 0.9871146082878113, 'validation/loss': 0.04499609395861626, 'validation/mean_average_precision': 0.29086285799273925, 'validation/num_examples': 43793, 'test/accuracy': 0.9862774610519409, 'test/loss': 0.0480165034532547, 'test/mean_average_precision': 0.27684241058067216, 'test/num_examples': 43793, 'score': 14185.667350292206, 'total_duration': 22081.319782733917, 'accumulated_submission_time': 14185.667350292206, 'accumulated_eval_time': 7892.67279791832, 'accumulated_logging_time': 1.8158187866210938}
I0311 09:22:45.516026 139622172112640 logging_writer.py:48] [45299] accumulated_eval_time=7892.672798, accumulated_logging_time=1.815819, accumulated_submission_time=14185.667350, global_step=45299, preemption_count=0, score=14185.667350, test/accuracy=0.986277, test/loss=0.048017, test/mean_average_precision=0.276842, test/num_examples=43793, total_duration=22081.319783, train/accuracy=0.992891, train/loss=0.022592, train/mean_average_precision=0.591523, validation/accuracy=0.987115, validation/loss=0.044996, validation/mean_average_precision=0.290863, validation/num_examples=43793
I0311 09:22:46.170944 139628670904064 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.07512228935956955, loss=0.023699074983596802
I0311 09:23:17.276846 139622172112640 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.0651317685842514, loss=0.022214476019144058
I0311 09:23:48.243970 139628670904064 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.0883677750825882, loss=0.025793634355068207
I0311 09:24:19.565149 139622172112640 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.09461716562509537, loss=0.030005525797605515
I0311 09:24:50.723943 139628670904064 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.06614562124013901, loss=0.023446831852197647
I0311 09:25:21.561927 139622172112640 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.07758399099111557, loss=0.024254085496068
I0311 09:25:52.825267 139628670904064 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.07614008337259293, loss=0.02340339496731758
I0311 09:26:23.700891 139622172112640 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.0881551057100296, loss=0.026357602328062057
I0311 09:26:45.796507 139789500200768 spec.py:321] Evaluating on the training split.
I0311 09:28:46.411280 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 09:28:49.424803 139789500200768 spec.py:349] Evaluating on the test split.
I0311 09:28:52.443381 139789500200768 submission_runner.py:411] Time since start: 22448.27s, 	Step: 46072, 	{'train/accuracy': 0.9930112957954407, 'train/loss': 0.022161442786455154, 'train/mean_average_precision': 0.5958331862615327, 'validation/accuracy': 0.98707115650177, 'validation/loss': 0.04508213326334953, 'validation/mean_average_precision': 0.2912427689904615, 'validation/num_examples': 43793, 'test/accuracy': 0.9862134456634521, 'test/loss': 0.04813104495406151, 'test/mean_average_precision': 0.27775174980010936, 'test/num_examples': 43793, 'score': 14425.914602994919, 'total_duration': 22448.270634174347, 'accumulated_submission_time': 14425.914602994919, 'accumulated_eval_time': 8019.319634437561, 'accumulated_logging_time': 1.8533244132995605}
I0311 09:28:52.467804 139621870597888 logging_writer.py:48] [46072] accumulated_eval_time=8019.319634, accumulated_logging_time=1.853324, accumulated_submission_time=14425.914603, global_step=46072, preemption_count=0, score=14425.914603, test/accuracy=0.986213, test/loss=0.048131, test/mean_average_precision=0.277752, test/num_examples=43793, total_duration=22448.270634, train/accuracy=0.993011, train/loss=0.022161, train/mean_average_precision=0.595833, validation/accuracy=0.987071, validation/loss=0.045082, validation/mean_average_precision=0.291243, validation/num_examples=43793
I0311 09:29:01.578701 139628679296768 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.09079943597316742, loss=0.025981152430176735
I0311 09:29:32.843116 139621870597888 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.08414404094219208, loss=0.02391277626156807
I0311 09:30:04.346842 139628679296768 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.06765373051166534, loss=0.020909737795591354
I0311 09:30:35.575892 139621870597888 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.08706037700176239, loss=0.024771811440587044
I0311 09:31:06.864131 139628679296768 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.07148383557796478, loss=0.021939493715763092
I0311 09:31:38.214283 139621870597888 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.08230309933423996, loss=0.02373228408396244
I0311 09:32:09.795464 139628679296768 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.07026024162769318, loss=0.02287650667130947
I0311 09:32:41.166317 139621870597888 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.07942339032888412, loss=0.024696333333849907
I0311 09:32:52.678683 139789500200768 spec.py:321] Evaluating on the training split.
I0311 09:34:50.383000 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 09:34:53.459574 139789500200768 spec.py:349] Evaluating on the test split.
I0311 09:34:56.455350 139789500200768 submission_runner.py:411] Time since start: 22812.28s, 	Step: 46838, 	{'train/accuracy': 0.9932113885879517, 'train/loss': 0.02150786854326725, 'train/mean_average_precision': 0.6120571175636954, 'validation/accuracy': 0.9871028065681458, 'validation/loss': 0.04514069855213165, 'validation/mean_average_precision': 0.2953311430028864, 'validation/num_examples': 43793, 'test/accuracy': 0.9862639904022217, 'test/loss': 0.048252418637275696, 'test/mean_average_precision': 0.27882228078059207, 'test/num_examples': 43793, 'score': 14666.095184326172, 'total_duration': 22812.28260588646, 'accumulated_submission_time': 14666.095184326172, 'accumulated_eval_time': 8143.096262216568, 'accumulated_logging_time': 1.8887808322906494}
I0311 09:34:56.479534 139622180505344 logging_writer.py:48] [46838] accumulated_eval_time=8143.096262, accumulated_logging_time=1.888781, accumulated_submission_time=14666.095184, global_step=46838, preemption_count=0, score=14666.095184, test/accuracy=0.986264, test/loss=0.048252, test/mean_average_precision=0.278822, test/num_examples=43793, total_duration=22812.282606, train/accuracy=0.993211, train/loss=0.021508, train/mean_average_precision=0.612057, validation/accuracy=0.987103, validation/loss=0.045141, validation/mean_average_precision=0.295331, validation/num_examples=43793
I0311 09:35:16.399577 139628670904064 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.07689250260591507, loss=0.025275304913520813
I0311 09:35:47.643292 139622180505344 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.06972937285900116, loss=0.02149537391960621
I0311 09:36:18.798933 139628670904064 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.10760170221328735, loss=0.028539007529616356
I0311 09:36:50.016761 139622180505344 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.07639224827289581, loss=0.022854434326291084
I0311 09:37:21.276621 139628670904064 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.07916386425495148, loss=0.026885194703936577
I0311 09:37:52.868493 139622180505344 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.07926086336374283, loss=0.022707132622599602
I0311 09:38:23.777268 139628670904064 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.08191665261983871, loss=0.02628496289253235
I0311 09:38:54.695460 139622180505344 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.0898958221077919, loss=0.0250686202198267
I0311 09:38:56.550108 139789500200768 spec.py:321] Evaluating on the training split.
I0311 09:40:53.459554 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 09:40:56.496595 139789500200768 spec.py:349] Evaluating on the test split.
I0311 09:40:59.444687 139789500200768 submission_runner.py:411] Time since start: 23175.27s, 	Step: 47607, 	{'train/accuracy': 0.9933398962020874, 'train/loss': 0.02112414874136448, 'train/mean_average_precision': 0.6331789829490917, 'validation/accuracy': 0.9870001077651978, 'validation/loss': 0.04567553848028183, 'validation/mean_average_precision': 0.29153234846397424, 'validation/num_examples': 43793, 'test/accuracy': 0.9861923456192017, 'test/loss': 0.048628274351358414, 'test/mean_average_precision': 0.26881960350041634, 'test/num_examples': 43793, 'score': 14906.135026216507, 'total_duration': 23175.271939516068, 'accumulated_submission_time': 14906.135026216507, 'accumulated_eval_time': 8265.990796804428, 'accumulated_logging_time': 1.9238817691802979}
I0311 09:40:59.468614 139621870597888 logging_writer.py:48] [47607] accumulated_eval_time=8265.990797, accumulated_logging_time=1.923882, accumulated_submission_time=14906.135026, global_step=47607, preemption_count=0, score=14906.135026, test/accuracy=0.986192, test/loss=0.048628, test/mean_average_precision=0.268820, test/num_examples=43793, total_duration=23175.271940, train/accuracy=0.993340, train/loss=0.021124, train/mean_average_precision=0.633179, validation/accuracy=0.987000, validation/loss=0.045676, validation/mean_average_precision=0.291532, validation/num_examples=43793
I0311 09:41:28.540491 139628679296768 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.07899085432291031, loss=0.022342925891280174
I0311 09:41:59.720663 139621870597888 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.07205147296190262, loss=0.023189572617411613
I0311 09:42:30.707743 139628679296768 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.09373174607753754, loss=0.028174540027976036
I0311 09:43:02.067392 139621870597888 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.07778922468423843, loss=0.02249642089009285
I0311 09:43:33.178997 139628679296768 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.08470778167247772, loss=0.024848781526088715
I0311 09:44:04.046541 139621870597888 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.08573190122842789, loss=0.026566721498966217
I0311 09:44:35.345803 139628679296768 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.09715331345796585, loss=0.025449303910136223
I0311 09:44:59.633710 139789500200768 spec.py:321] Evaluating on the training split.
I0311 09:46:54.062557 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 09:46:57.114967 139789500200768 spec.py:349] Evaluating on the test split.
I0311 09:47:00.109549 139789500200768 submission_runner.py:411] Time since start: 23535.94s, 	Step: 48379, 	{'train/accuracy': 0.9934019446372986, 'train/loss': 0.020989738404750824, 'train/mean_average_precision': 0.6255306974010649, 'validation/accuracy': 0.9870269298553467, 'validation/loss': 0.04553539678454399, 'validation/mean_average_precision': 0.2903014903988803, 'validation/num_examples': 43793, 'test/accuracy': 0.9862968325614929, 'test/loss': 0.0484311506152153, 'test/mean_average_precision': 0.2783990005573538, 'test/num_examples': 43793, 'score': 15146.269136428833, 'total_duration': 23535.936802864075, 'accumulated_submission_time': 15146.269136428833, 'accumulated_eval_time': 8386.46659874916, 'accumulated_logging_time': 1.9589176177978516}
I0311 09:47:00.134012 139622172112640 logging_writer.py:48] [48379] accumulated_eval_time=8386.466599, accumulated_logging_time=1.958918, accumulated_submission_time=15146.269136, global_step=48379, preemption_count=0, score=15146.269136, test/accuracy=0.986297, test/loss=0.048431, test/mean_average_precision=0.278399, test/num_examples=43793, total_duration=23535.936803, train/accuracy=0.993402, train/loss=0.020990, train/mean_average_precision=0.625531, validation/accuracy=0.987027, validation/loss=0.045535, validation/mean_average_precision=0.290301, validation/num_examples=43793
I0311 09:47:07.135371 139622180505344 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.09162095934152603, loss=0.024382779374718666
I0311 09:47:38.747845 139622172112640 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.0841069296002388, loss=0.02130630612373352
I0311 09:48:09.791239 139622180505344 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.10024585574865341, loss=0.02772274613380432
I0311 09:48:41.066919 139622172112640 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.0747404620051384, loss=0.022455789148807526
I0311 09:49:12.031071 139622180505344 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.07896343618631363, loss=0.020557425916194916
I0311 09:49:42.933947 139622172112640 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.08540945500135422, loss=0.025814762338995934
I0311 09:50:14.622898 139622180505344 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.08580166101455688, loss=0.02304115705192089
I0311 09:50:45.894312 139622172112640 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.09742782264947891, loss=0.025091761723160744
I0311 09:51:00.373011 139789500200768 spec.py:321] Evaluating on the training split.
I0311 09:52:54.137046 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 09:52:57.180439 139789500200768 spec.py:349] Evaluating on the test split.
I0311 09:53:00.203446 139789500200768 submission_runner.py:411] Time since start: 23896.03s, 	Step: 49148, 	{'train/accuracy': 0.9931447505950928, 'train/loss': 0.021765442565083504, 'train/mean_average_precision': 0.6131731781560671, 'validation/accuracy': 0.986970067024231, 'validation/loss': 0.04550040140748024, 'validation/mean_average_precision': 0.2893361191203478, 'validation/num_examples': 43793, 'test/accuracy': 0.9860782027244568, 'test/loss': 0.0487363375723362, 'test/mean_average_precision': 0.2771831528863973, 'test/num_examples': 43793, 'score': 15386.475734949112, 'total_duration': 23896.030689239502, 'accumulated_submission_time': 15386.475734949112, 'accumulated_eval_time': 8506.296981334686, 'accumulated_logging_time': 1.9959444999694824}
I0311 09:53:00.227935 139628670904064 logging_writer.py:48] [49148] accumulated_eval_time=8506.296981, accumulated_logging_time=1.995944, accumulated_submission_time=15386.475735, global_step=49148, preemption_count=0, score=15386.475735, test/accuracy=0.986078, test/loss=0.048736, test/mean_average_precision=0.277183, test/num_examples=43793, total_duration=23896.030689, train/accuracy=0.993145, train/loss=0.021765, train/mean_average_precision=0.613173, validation/accuracy=0.986970, validation/loss=0.045500, validation/mean_average_precision=0.289336, validation/num_examples=43793
I0311 09:53:16.732198 139628679296768 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.08822425454854965, loss=0.02527732029557228
I0311 09:53:47.658540 139628670904064 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.1006007120013237, loss=0.027239371091127396
I0311 09:54:18.758905 139628679296768 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.0896420106291771, loss=0.024529971182346344
I0311 09:54:49.892617 139628670904064 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.09593446552753448, loss=0.024517351761460304
I0311 09:55:21.059140 139628679296768 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.08850791305303574, loss=0.022270359098911285
I0311 09:55:52.342875 139628670904064 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.08417005091905594, loss=0.024968929588794708
I0311 09:56:23.289614 139628679296768 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.08374852687120438, loss=0.023547103628516197
I0311 09:56:54.408690 139628670904064 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.09287882596254349, loss=0.023654133081436157
I0311 09:57:00.479366 139789500200768 spec.py:321] Evaluating on the training split.
I0311 09:58:58.421652 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 09:59:01.547451 139789500200768 spec.py:349] Evaluating on the test split.
I0311 09:59:04.551513 139789500200768 submission_runner.py:411] Time since start: 24260.38s, 	Step: 49921, 	{'train/accuracy': 0.9931887984275818, 'train/loss': 0.021556342020630836, 'train/mean_average_precision': 0.5925207424069308, 'validation/accuracy': 0.9870281219482422, 'validation/loss': 0.04564758017659187, 'validation/mean_average_precision': 0.2885576907983909, 'validation/num_examples': 43793, 'test/accuracy': 0.9861999154090881, 'test/loss': 0.04878488555550575, 'test/mean_average_precision': 0.2782597699814962, 'test/num_examples': 43793, 'score': 15626.695341348648, 'total_duration': 24260.378756046295, 'accumulated_submission_time': 15626.695341348648, 'accumulated_eval_time': 8630.36907529831, 'accumulated_logging_time': 2.0322859287261963}
I0311 09:59:04.576068 139621870597888 logging_writer.py:48] [49921] accumulated_eval_time=8630.369075, accumulated_logging_time=2.032286, accumulated_submission_time=15626.695341, global_step=49921, preemption_count=0, score=15626.695341, test/accuracy=0.986200, test/loss=0.048785, test/mean_average_precision=0.278260, test/num_examples=43793, total_duration=24260.378756, train/accuracy=0.993189, train/loss=0.021556, train/mean_average_precision=0.592521, validation/accuracy=0.987028, validation/loss=0.045648, validation/mean_average_precision=0.288558, validation/num_examples=43793
I0311 09:59:29.364494 139622172112640 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.09034278988838196, loss=0.022343400865793228
I0311 10:00:00.582505 139621870597888 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.09659864008426666, loss=0.02483825944364071
I0311 10:00:31.610444 139622172112640 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.08479160815477371, loss=0.023927301168441772
I0311 10:01:02.677788 139621870597888 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.09598784893751144, loss=0.02585023082792759
I0311 10:01:33.836602 139622172112640 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.0893891379237175, loss=0.024898340925574303
I0311 10:02:05.042018 139621870597888 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.08726856112480164, loss=0.024436723440885544
I0311 10:02:36.069442 139622172112640 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.09080729633569717, loss=0.024771781638264656
I0311 10:03:04.778202 139789500200768 spec.py:321] Evaluating on the training split.
I0311 10:05:01.764051 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 10:05:04.819669 139789500200768 spec.py:349] Evaluating on the test split.
I0311 10:05:07.863056 139789500200768 submission_runner.py:411] Time since start: 24623.69s, 	Step: 50693, 	{'train/accuracy': 0.9932281374931335, 'train/loss': 0.021493710577487946, 'train/mean_average_precision': 0.6125680428291371, 'validation/accuracy': 0.9870500564575195, 'validation/loss': 0.0457317978143692, 'validation/mean_average_precision': 0.2902672784161562, 'validation/num_examples': 43793, 'test/accuracy': 0.9861531853675842, 'test/loss': 0.04879327863454819, 'test/mean_average_precision': 0.2808486112206802, 'test/num_examples': 43793, 'score': 15866.866524219513, 'total_duration': 24623.690307617188, 'accumulated_submission_time': 15866.866524219513, 'accumulated_eval_time': 8753.453888893127, 'accumulated_logging_time': 2.068065881729126}
I0311 10:05:07.888292 139628670904064 logging_writer.py:48] [50693] accumulated_eval_time=8753.453889, accumulated_logging_time=2.068066, accumulated_submission_time=15866.866524, global_step=50693, preemption_count=0, score=15866.866524, test/accuracy=0.986153, test/loss=0.048793, test/mean_average_precision=0.280849, test/num_examples=43793, total_duration=24623.690308, train/accuracy=0.993228, train/loss=0.021494, train/mean_average_precision=0.612568, validation/accuracy=0.987050, validation/loss=0.045732, validation/mean_average_precision=0.290267, validation/num_examples=43793
I0311 10:05:10.466651 139628679296768 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.10073015093803406, loss=0.025111133232712746
I0311 10:05:41.862654 139628670904064 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.0965225100517273, loss=0.026663603261113167
I0311 10:06:13.120563 139628679296768 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.09394806623458862, loss=0.023586470633745193
I0311 10:06:44.115252 139628670904064 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.0917208269238472, loss=0.02293219044804573
I0311 10:07:15.106598 139628679296768 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.1078118234872818, loss=0.027563706040382385
I0311 10:07:46.564882 139628670904064 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.0830615907907486, loss=0.024519002065062523
I0311 10:08:17.802095 139628679296768 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.09262246638536453, loss=0.02223208174109459
I0311 10:08:49.523218 139628670904064 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.0983477383852005, loss=0.026211664080619812
I0311 10:09:08.015315 139789500200768 spec.py:321] Evaluating on the training split.
I0311 10:10:59.838375 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 10:11:02.937261 139789500200768 spec.py:349] Evaluating on the test split.
I0311 10:11:05.928030 139789500200768 submission_runner.py:411] Time since start: 24981.76s, 	Step: 51460, 	{'train/accuracy': 0.9931707382202148, 'train/loss': 0.021562300622463226, 'train/mean_average_precision': 0.6086612239107657, 'validation/accuracy': 0.9869388341903687, 'validation/loss': 0.045888785272836685, 'validation/mean_average_precision': 0.28756521101560656, 'validation/num_examples': 43793, 'test/accuracy': 0.9861097931861877, 'test/loss': 0.048993151634931564, 'test/mean_average_precision': 0.2756912633800584, 'test/num_examples': 43793, 'score': 16106.96294260025, 'total_duration': 24981.755274295807, 'accumulated_submission_time': 16106.96294260025, 'accumulated_eval_time': 8871.36655497551, 'accumulated_logging_time': 2.1043295860290527}
I0311 10:11:05.953851 139621870597888 logging_writer.py:48] [51460] accumulated_eval_time=8871.366555, accumulated_logging_time=2.104330, accumulated_submission_time=16106.962943, global_step=51460, preemption_count=0, score=16106.962943, test/accuracy=0.986110, test/loss=0.048993, test/mean_average_precision=0.275691, test/num_examples=43793, total_duration=24981.755274, train/accuracy=0.993171, train/loss=0.021562, train/mean_average_precision=0.608661, validation/accuracy=0.986939, validation/loss=0.045889, validation/mean_average_precision=0.287565, validation/num_examples=43793
I0311 10:11:18.599911 139622172112640 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.09150203317403793, loss=0.023245804011821747
I0311 10:11:49.702955 139621870597888 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.1050427034497261, loss=0.02318037860095501
I0311 10:12:20.933351 139622172112640 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.1219475120306015, loss=0.024826085194945335
I0311 10:12:52.241979 139621870597888 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.08473391830921173, loss=0.02294442430138588
I0311 10:13:23.614321 139622172112640 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.0977165624499321, loss=0.026028715074062347
I0311 10:13:54.673949 139621870597888 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.09642763435840607, loss=0.023738263174891472
I0311 10:14:25.748480 139622172112640 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.0854465588927269, loss=0.023958399891853333
I0311 10:14:56.700265 139621870597888 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.08495703339576721, loss=0.021166488528251648
I0311 10:15:05.968923 139789500200768 spec.py:321] Evaluating on the training split.
I0311 10:17:02.793546 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 10:17:05.849241 139789500200768 spec.py:349] Evaluating on the test split.
I0311 10:17:08.822692 139789500200768 submission_runner.py:411] Time since start: 25344.65s, 	Step: 52231, 	{'train/accuracy': 0.9933059811592102, 'train/loss': 0.021193090826272964, 'train/mean_average_precision': 0.6209554499918561, 'validation/accuracy': 0.9869749546051025, 'validation/loss': 0.04572819173336029, 'validation/mean_average_precision': 0.2894453169966877, 'validation/num_examples': 43793, 'test/accuracy': 0.986139714717865, 'test/loss': 0.04903361573815346, 'test/mean_average_precision': 0.27923242313841695, 'test/num_examples': 43793, 'score': 16346.9478225708, 'total_duration': 25344.649944782257, 'accumulated_submission_time': 16346.9478225708, 'accumulated_eval_time': 8994.220281600952, 'accumulated_logging_time': 2.1409432888031006}
I0311 10:17:08.847813 139622180505344 logging_writer.py:48] [52231] accumulated_eval_time=8994.220282, accumulated_logging_time=2.140943, accumulated_submission_time=16346.947823, global_step=52231, preemption_count=0, score=16346.947823, test/accuracy=0.986140, test/loss=0.049034, test/mean_average_precision=0.279232, test/num_examples=43793, total_duration=25344.649945, train/accuracy=0.993306, train/loss=0.021193, train/mean_average_precision=0.620955, validation/accuracy=0.986975, validation/loss=0.045728, validation/mean_average_precision=0.289445, validation/num_examples=43793
I0311 10:17:30.722542 139628670904064 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.09869610518217087, loss=0.02385680377483368
I0311 10:18:01.742962 139622180505344 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.09535038471221924, loss=0.023707397282123566
I0311 10:18:33.330640 139628670904064 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.09247389435768127, loss=0.02377976104617119
I0311 10:19:04.622548 139622180505344 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.10570959001779556, loss=0.023254234343767166
I0311 10:19:35.692900 139628670904064 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.09432518482208252, loss=0.02431950718164444
I0311 10:20:06.700644 139622180505344 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.08338653296232224, loss=0.022100284695625305
I0311 10:20:37.740596 139628670904064 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.10347525775432587, loss=0.02465907484292984
I0311 10:21:08.829721 139622180505344 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.08773504197597504, loss=0.021656757220625877
I0311 10:21:08.835773 139789500200768 spec.py:321] Evaluating on the training split.
I0311 10:23:05.492379 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 10:23:08.551894 139789500200768 spec.py:349] Evaluating on the test split.
I0311 10:23:11.522222 139789500200768 submission_runner.py:411] Time since start: 25707.35s, 	Step: 53001, 	{'train/accuracy': 0.9933321475982666, 'train/loss': 0.020837854593992233, 'train/mean_average_precision': 0.6346190961639615, 'validation/accuracy': 0.9871361255645752, 'validation/loss': 0.04646424576640129, 'validation/mean_average_precision': 0.2905150610562712, 'validation/num_examples': 43793, 'test/accuracy': 0.9862247705459595, 'test/loss': 0.04974513500928879, 'test/mean_average_precision': 0.2790904600309866, 'test/num_examples': 43793, 'score': 16586.905706882477, 'total_duration': 25707.349474668503, 'accumulated_submission_time': 16586.905706882477, 'accumulated_eval_time': 9116.906694173813, 'accumulated_logging_time': 2.1766245365142822}
I0311 10:23:11.547202 139621870597888 logging_writer.py:48] [53001] accumulated_eval_time=9116.906694, accumulated_logging_time=2.176625, accumulated_submission_time=16586.905707, global_step=53001, preemption_count=0, score=16586.905707, test/accuracy=0.986225, test/loss=0.049745, test/mean_average_precision=0.279090, test/num_examples=43793, total_duration=25707.349475, train/accuracy=0.993332, train/loss=0.020838, train/mean_average_precision=0.634619, validation/accuracy=0.987136, validation/loss=0.046464, validation/mean_average_precision=0.290515, validation/num_examples=43793
I0311 10:23:42.866511 139628679296768 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.09140589833259583, loss=0.022812608629465103
I0311 10:24:13.964634 139621870597888 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.10257444530725479, loss=0.02550682984292507
I0311 10:24:46.239273 139628679296768 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.1023794636130333, loss=0.02350812591612339
I0311 10:25:19.661952 139621870597888 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.09827279299497604, loss=0.023786744102835655
I0311 10:25:52.371691 139628679296768 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.08581298589706421, loss=0.02049664780497551
I0311 10:26:24.943913 139621870597888 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.09986289590597153, loss=0.024339156225323677
I0311 10:26:57.312080 139628679296768 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.09129013866186142, loss=0.021796150133013725
I0311 10:27:11.678470 139789500200768 spec.py:321] Evaluating on the training split.
I0311 10:29:07.753563 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 10:29:10.835420 139789500200768 spec.py:349] Evaluating on the test split.
I0311 10:29:13.825062 139789500200768 submission_runner.py:411] Time since start: 26069.65s, 	Step: 53745, 	{'train/accuracy': 0.9934971332550049, 'train/loss': 0.020465897396206856, 'train/mean_average_precision': 0.630007503667221, 'validation/accuracy': 0.9869733452796936, 'validation/loss': 0.04617712274193764, 'validation/mean_average_precision': 0.28698648735918236, 'validation/num_examples': 43793, 'test/accuracy': 0.9861270785331726, 'test/loss': 0.04953198507428169, 'test/mean_average_precision': 0.2741914971743095, 'test/num_examples': 43793, 'score': 16827.004104614258, 'total_duration': 26069.652314662933, 'accumulated_submission_time': 16827.004104614258, 'accumulated_eval_time': 9239.053257226944, 'accumulated_logging_time': 2.212446451187134}
I0311 10:29:13.851672 139622172112640 logging_writer.py:48] [53745] accumulated_eval_time=9239.053257, accumulated_logging_time=2.212446, accumulated_submission_time=16827.004105, global_step=53745, preemption_count=0, score=16827.004105, test/accuracy=0.986127, test/loss=0.049532, test/mean_average_precision=0.274191, test/num_examples=43793, total_duration=26069.652315, train/accuracy=0.993497, train/loss=0.020466, train/mean_average_precision=0.630008, validation/accuracy=0.986973, validation/loss=0.046177, validation/mean_average_precision=0.286986, validation/num_examples=43793
I0311 10:29:31.715173 139622180505344 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.11306518316268921, loss=0.024706123396754265
I0311 10:30:03.088614 139622172112640 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.09584679454565048, loss=0.023239104077219963
I0311 10:30:34.698803 139622180505344 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.10508429259061813, loss=0.023231804370880127
I0311 10:31:05.940581 139622172112640 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.10456322133541107, loss=0.024188391864299774
I0311 10:31:37.229811 139622180505344 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.10151930153369904, loss=0.025590624660253525
I0311 10:32:08.361294 139622172112640 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.10153379291296005, loss=0.023780666291713715
I0311 10:32:39.436799 139622180505344 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.1018117368221283, loss=0.022857865318655968
I0311 10:33:10.828352 139622172112640 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.0841514989733696, loss=0.020005669444799423
I0311 10:33:14.002130 139789500200768 spec.py:321] Evaluating on the training split.
I0311 10:35:06.055996 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 10:35:09.103102 139789500200768 spec.py:349] Evaluating on the test split.
I0311 10:35:12.166649 139789500200768 submission_runner.py:411] Time since start: 26427.99s, 	Step: 54511, 	{'train/accuracy': 0.9936056733131409, 'train/loss': 0.020094292238354683, 'train/mean_average_precision': 0.6474843711686171, 'validation/accuracy': 0.9870001077651978, 'validation/loss': 0.04663432762026787, 'validation/mean_average_precision': 0.29249571124037116, 'validation/num_examples': 43793, 'test/accuracy': 0.9862344861030579, 'test/loss': 0.04997370392084122, 'test/mean_average_precision': 0.275257343295017, 'test/num_examples': 43793, 'score': 17067.124797344208, 'total_duration': 26427.993901014328, 'accumulated_submission_time': 17067.124797344208, 'accumulated_eval_time': 9357.21773147583, 'accumulated_logging_time': 2.249485731124878}
I0311 10:35:12.191953 139621870597888 logging_writer.py:48] [54511] accumulated_eval_time=9357.217731, accumulated_logging_time=2.249486, accumulated_submission_time=17067.124797, global_step=54511, preemption_count=0, score=17067.124797, test/accuracy=0.986234, test/loss=0.049974, test/mean_average_precision=0.275257, test/num_examples=43793, total_duration=26427.993901, train/accuracy=0.993606, train/loss=0.020094, train/mean_average_precision=0.647484, validation/accuracy=0.987000, validation/loss=0.046634, validation/mean_average_precision=0.292496, validation/num_examples=43793
I0311 10:35:41.554329 139628670904064 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.09415384382009506, loss=0.021848052740097046
I0311 10:36:12.899283 139621870597888 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.10031426697969437, loss=0.022871797904372215
I0311 10:36:44.199084 139628670904064 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.09905754029750824, loss=0.02424733340740204
I0311 10:37:15.643829 139621870597888 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.11206517368555069, loss=0.025448273867368698
I0311 10:37:47.103859 139628670904064 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.11142069101333618, loss=0.02295447513461113
I0311 10:38:19.814289 139621870597888 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.09754174947738647, loss=0.021687256172299385
I0311 10:38:52.563565 139628670904064 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.10030464828014374, loss=0.023146407678723335
I0311 10:39:12.257672 139789500200768 spec.py:321] Evaluating on the training split.
I0311 10:41:12.109551 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 10:41:15.162676 139789500200768 spec.py:349] Evaluating on the test split.
I0311 10:41:18.160351 139789500200768 submission_runner.py:411] Time since start: 26793.99s, 	Step: 55261, 	{'train/accuracy': 0.9938763976097107, 'train/loss': 0.019259542226791382, 'train/mean_average_precision': 0.6639851968755207, 'validation/accuracy': 0.987063467502594, 'validation/loss': 0.04681715741753578, 'validation/mean_average_precision': 0.28829801160658247, 'validation/num_examples': 43793, 'test/accuracy': 0.986182689666748, 'test/loss': 0.04995269328355789, 'test/mean_average_precision': 0.2779944627561785, 'test/num_examples': 43793, 'score': 17307.159264326096, 'total_duration': 26793.98760509491, 'accumulated_submission_time': 17307.159264326096, 'accumulated_eval_time': 9483.120379209518, 'accumulated_logging_time': 2.2856380939483643}
I0311 10:41:18.186324 139622172112640 logging_writer.py:48] [55261] accumulated_eval_time=9483.120379, accumulated_logging_time=2.285638, accumulated_submission_time=17307.159264, global_step=55261, preemption_count=0, score=17307.159264, test/accuracy=0.986183, test/loss=0.049953, test/mean_average_precision=0.277994, test/num_examples=43793, total_duration=26793.987605, train/accuracy=0.993876, train/loss=0.019260, train/mean_average_precision=0.663985, validation/accuracy=0.987063, validation/loss=0.046817, validation/mean_average_precision=0.288298, validation/num_examples=43793
I0311 10:41:30.718607 139628679296768 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.09995575994253159, loss=0.0234929621219635
I0311 10:42:01.666017 139622172112640 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.10088278353214264, loss=0.02474330924451351
I0311 10:42:32.999034 139628679296768 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.09793782234191895, loss=0.022950660437345505
I0311 10:43:04.125805 139622172112640 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.1087976023554802, loss=0.02307995967566967
I0311 10:43:34.986492 139628679296768 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.1141979843378067, loss=0.023226244375109673
I0311 10:44:06.379485 139622172112640 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.11360057443380356, loss=0.02403097413480282
I0311 10:44:37.562514 139628679296768 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.08531658351421356, loss=0.0184507817029953
I0311 10:45:08.821439 139622172112640 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.12767159938812256, loss=0.02334846369922161
I0311 10:45:18.284658 139789500200768 spec.py:321] Evaluating on the training split.
I0311 10:47:12.356145 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 10:47:15.799310 139789500200768 spec.py:349] Evaluating on the test split.
I0311 10:47:19.177654 139789500200768 submission_runner.py:411] Time since start: 27155.00s, 	Step: 56031, 	{'train/accuracy': 0.9941009283065796, 'train/loss': 0.01869196444749832, 'train/mean_average_precision': 0.6720966607253281, 'validation/accuracy': 0.9870317578315735, 'validation/loss': 0.046952150762081146, 'validation/mean_average_precision': 0.28628593022749427, 'validation/num_examples': 43793, 'test/accuracy': 0.9861670732498169, 'test/loss': 0.050098955631256104, 'test/mean_average_precision': 0.27749577902928824, 'test/num_examples': 43793, 'score': 17547.22730565071, 'total_duration': 27155.00487589836, 'accumulated_submission_time': 17547.22730565071, 'accumulated_eval_time': 9604.013299703598, 'accumulated_logging_time': 2.322486400604248}
I0311 10:47:19.208473 139622180505344 logging_writer.py:48] [56031] accumulated_eval_time=9604.013300, accumulated_logging_time=2.322486, accumulated_submission_time=17547.227306, global_step=56031, preemption_count=0, score=17547.227306, test/accuracy=0.986167, test/loss=0.050099, test/mean_average_precision=0.277496, test/num_examples=43793, total_duration=27155.004876, train/accuracy=0.994101, train/loss=0.018692, train/mean_average_precision=0.672097, validation/accuracy=0.987032, validation/loss=0.046952, validation/mean_average_precision=0.286286, validation/num_examples=43793
I0311 10:47:42.136709 139628670904064 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.0935504287481308, loss=0.01913481578230858
I0311 10:48:14.997492 139622180505344 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.10017954558134079, loss=0.023696063086390495
I0311 10:48:47.504410 139628670904064 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.1189633384346962, loss=0.024355506524443626
I0311 10:49:20.031749 139622180505344 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.12484390288591385, loss=0.02212078869342804
I0311 10:49:52.347632 139628670904064 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.09744738787412643, loss=0.019215328618884087
I0311 10:50:24.542349 139622180505344 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.10725004225969315, loss=0.02162654884159565
I0311 10:50:57.025404 139628670904064 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.09642574936151505, loss=0.022597484290599823
I0311 10:51:19.270658 139789500200768 spec.py:321] Evaluating on the training split.
I0311 10:53:22.498239 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 10:53:25.917077 139789500200768 spec.py:349] Evaluating on the test split.
I0311 10:53:29.292224 139789500200768 submission_runner.py:411] Time since start: 27525.12s, 	Step: 56768, 	{'train/accuracy': 0.9941667318344116, 'train/loss': 0.01827869936823845, 'train/mean_average_precision': 0.6992927322278004, 'validation/accuracy': 0.9869331121444702, 'validation/loss': 0.04693625494837761, 'validation/mean_average_precision': 0.29089760537266196, 'validation/num_examples': 43793, 'test/accuracy': 0.9861464500427246, 'test/loss': 0.05004053935408592, 'test/mean_average_precision': 0.2790140354084867, 'test/num_examples': 43793, 'score': 17787.253559589386, 'total_duration': 27525.11945939064, 'accumulated_submission_time': 17787.253559589386, 'accumulated_eval_time': 9734.03482222557, 'accumulated_logging_time': 2.3649609088897705}
I0311 10:53:29.321829 139622172112640 logging_writer.py:48] [56768] accumulated_eval_time=9734.034822, accumulated_logging_time=2.364961, accumulated_submission_time=17787.253560, global_step=56768, preemption_count=0, score=17787.253560, test/accuracy=0.986146, test/loss=0.050041, test/mean_average_precision=0.279014, test/num_examples=43793, total_duration=27525.119459, train/accuracy=0.994167, train/loss=0.018279, train/mean_average_precision=0.699293, validation/accuracy=0.986933, validation/loss=0.046936, validation/mean_average_precision=0.290898, validation/num_examples=43793
I0311 10:53:40.085063 139628679296768 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.09351906925439835, loss=0.024311773478984833
I0311 10:54:12.571011 139622172112640 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.10136087238788605, loss=0.019830815494060516
I0311 10:54:44.038383 139628679296768 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.11943841725587845, loss=0.022296341136097908
I0311 10:55:15.250631 139622172112640 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.10457980632781982, loss=0.02123214304447174
I0311 10:55:46.558447 139628679296768 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.10298509150743484, loss=0.02150634489953518
I0311 10:56:18.097501 139622172112640 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.1106550320982933, loss=0.02365342155098915
I0311 10:56:49.618528 139628679296768 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.12113351374864578, loss=0.02185237966477871
I0311 10:57:20.981600 139622172112640 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.1185372993350029, loss=0.023136047646403313
I0311 10:57:29.581093 139789500200768 spec.py:321] Evaluating on the training split.
I0311 10:59:23.600328 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 10:59:26.667523 139789500200768 spec.py:349] Evaluating on the test split.
I0311 10:59:29.656210 139789500200768 submission_runner.py:411] Time since start: 27885.48s, 	Step: 57528, 	{'train/accuracy': 0.994340181350708, 'train/loss': 0.017951402813196182, 'train/mean_average_precision': 0.6863589255944273, 'validation/accuracy': 0.9869924187660217, 'validation/loss': 0.04685588553547859, 'validation/mean_average_precision': 0.2878277138915034, 'validation/num_examples': 43793, 'test/accuracy': 0.9862096309661865, 'test/loss': 0.050077904015779495, 'test/mean_average_precision': 0.2783108176366595, 'test/num_examples': 43793, 'score': 18027.479197263718, 'total_duration': 27885.483307123184, 'accumulated_submission_time': 18027.479197263718, 'accumulated_eval_time': 9854.109743356705, 'accumulated_logging_time': 2.407958745956421}
I0311 10:59:29.683065 139622180505344 logging_writer.py:48] [57528] accumulated_eval_time=9854.109743, accumulated_logging_time=2.407959, accumulated_submission_time=18027.479197, global_step=57528, preemption_count=0, score=18027.479197, test/accuracy=0.986210, test/loss=0.050078, test/mean_average_precision=0.278311, test/num_examples=43793, total_duration=27885.483307, train/accuracy=0.994340, train/loss=0.017951, train/mean_average_precision=0.686359, validation/accuracy=0.986992, validation/loss=0.046856, validation/mean_average_precision=0.287828, validation/num_examples=43793
I0311 10:59:52.423372 139628670904064 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.09909903258085251, loss=0.021408403292298317
I0311 11:00:23.571118 139622180505344 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.11000552773475647, loss=0.02140074409544468
I0311 11:00:54.461776 139628670904064 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.10163608938455582, loss=0.019801199436187744
I0311 11:01:25.789485 139622180505344 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.10667837411165237, loss=0.023871684446930885
I0311 11:01:57.227556 139628670904064 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.12417648732662201, loss=0.02281259559094906
I0311 11:02:28.528496 139622180505344 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.10319942235946655, loss=0.019436435773968697
I0311 11:02:59.975283 139628670904064 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.1125142052769661, loss=0.01977595128118992
I0311 11:03:29.726111 139789500200768 spec.py:321] Evaluating on the training split.
I0311 11:05:31.481506 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 11:05:34.545129 139789500200768 spec.py:349] Evaluating on the test split.
I0311 11:05:37.544317 139789500200768 submission_runner.py:411] Time since start: 28253.37s, 	Step: 58296, 	{'train/accuracy': 0.9941099286079407, 'train/loss': 0.018635477870702744, 'train/mean_average_precision': 0.6760400525936513, 'validation/accuracy': 0.9870346188545227, 'validation/loss': 0.047081682831048965, 'validation/mean_average_precision': 0.284984456051923, 'validation/num_examples': 43793, 'test/accuracy': 0.9860777854919434, 'test/loss': 0.0504041463136673, 'test/mean_average_precision': 0.2747297670159299, 'test/num_examples': 43793, 'score': 18267.49107837677, 'total_duration': 28253.37156844139, 'accumulated_submission_time': 18267.49107837677, 'accumulated_eval_time': 9981.927907466888, 'accumulated_logging_time': 2.44576358795166}
I0311 11:05:37.571403 139621870597888 logging_writer.py:48] [58296] accumulated_eval_time=9981.927907, accumulated_logging_time=2.445764, accumulated_submission_time=18267.491078, global_step=58296, preemption_count=0, score=18267.491078, test/accuracy=0.986078, test/loss=0.050404, test/mean_average_precision=0.274730, test/num_examples=43793, total_duration=28253.371568, train/accuracy=0.994110, train/loss=0.018635, train/mean_average_precision=0.676040, validation/accuracy=0.987035, validation/loss=0.047082, validation/mean_average_precision=0.284984, validation/num_examples=43793
I0311 11:05:39.157793 139628679296768 logging_writer.py:48] [58300] global_step=58300, grad_norm=0.11541005969047546, loss=0.020947329699993134
I0311 11:06:10.348673 139621870597888 logging_writer.py:48] [58400] global_step=58400, grad_norm=0.1060137003660202, loss=0.023334316909313202
I0311 11:06:41.905361 139628679296768 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.10654318332672119, loss=0.021699456498026848
I0311 11:07:13.283502 139621870597888 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.11445910483598709, loss=0.024552831426262856
I0311 11:07:44.681592 139628679296768 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.11519976705312729, loss=0.020643938332796097
I0311 11:08:16.082960 139621870597888 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.12907037138938904, loss=0.02232581377029419
I0311 11:08:47.425574 139628679296768 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.10509324818849564, loss=0.01932188682258129
I0311 11:09:18.609730 139621870597888 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.127949059009552, loss=0.023621389642357826
I0311 11:09:37.704325 139789500200768 spec.py:321] Evaluating on the training split.
I0311 11:11:29.989881 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 11:11:33.011812 139789500200768 spec.py:349] Evaluating on the test split.
I0311 11:11:36.020643 139789500200768 submission_runner.py:411] Time since start: 28611.85s, 	Step: 59062, 	{'train/accuracy': 0.9940509796142578, 'train/loss': 0.01852925680577755, 'train/mean_average_precision': 0.6800617030023898, 'validation/accuracy': 0.9870293140411377, 'validation/loss': 0.04753362014889717, 'validation/mean_average_precision': 0.2872965751470687, 'validation/num_examples': 43793, 'test/accuracy': 0.9862083792686462, 'test/loss': 0.05086510628461838, 'test/mean_average_precision': 0.2728735942641346, 'test/num_examples': 43793, 'score': 18507.593081474304, 'total_duration': 28611.847897052765, 'accumulated_submission_time': 18507.593081474304, 'accumulated_eval_time': 10100.244191408157, 'accumulated_logging_time': 2.483938217163086}
I0311 11:11:36.048260 139622172112640 logging_writer.py:48] [59062] accumulated_eval_time=10100.244191, accumulated_logging_time=2.483938, accumulated_submission_time=18507.593081, global_step=59062, preemption_count=0, score=18507.593081, test/accuracy=0.986208, test/loss=0.050865, test/mean_average_precision=0.272874, test/num_examples=43793, total_duration=28611.847897, train/accuracy=0.994051, train/loss=0.018529, train/mean_average_precision=0.680062, validation/accuracy=0.987029, validation/loss=0.047534, validation/mean_average_precision=0.287297, validation/num_examples=43793
I0311 11:11:48.424049 139622180505344 logging_writer.py:48] [59100] global_step=59100, grad_norm=0.12756648659706116, loss=0.023288460448384285
I0311 11:12:20.122685 139622172112640 logging_writer.py:48] [59200] global_step=59200, grad_norm=0.11777907609939575, loss=0.023890918120741844
I0311 11:12:51.606439 139622180505344 logging_writer.py:48] [59300] global_step=59300, grad_norm=0.1196984350681305, loss=0.01880572736263275
I0311 11:13:22.610944 139622172112640 logging_writer.py:48] [59400] global_step=59400, grad_norm=0.11515819281339645, loss=0.023073378950357437
I0311 11:13:53.877929 139622180505344 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.11142463237047195, loss=0.021763307973742485
I0311 11:14:25.114425 139622172112640 logging_writer.py:48] [59600] global_step=59600, grad_norm=0.11913741379976273, loss=0.022269295528531075
I0311 11:14:56.137600 139622180505344 logging_writer.py:48] [59700] global_step=59700, grad_norm=0.13113096356391907, loss=0.02358963154256344
I0311 11:15:27.257364 139622172112640 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.11305580288171768, loss=0.021828843280673027
I0311 11:15:36.235728 139789500200768 spec.py:321] Evaluating on the training split.
I0311 11:17:28.712789 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 11:17:32.061151 139789500200768 spec.py:349] Evaluating on the test split.
I0311 11:17:35.164688 139789500200768 submission_runner.py:411] Time since start: 28970.99s, 	Step: 59830, 	{'train/accuracy': 0.9940648674964905, 'train/loss': 0.018450187519192696, 'train/mean_average_precision': 0.6847460980138358, 'validation/accuracy': 0.9869866967201233, 'validation/loss': 0.04754917696118355, 'validation/mean_average_precision': 0.2900075163292792, 'validation/num_examples': 43793, 'test/accuracy': 0.9861797094345093, 'test/loss': 0.050867557525634766, 'test/mean_average_precision': 0.2815177285535439, 'test/num_examples': 43793, 'score': 18747.74967765808, 'total_duration': 28970.991935491562, 'accumulated_submission_time': 18747.74967765808, 'accumulated_eval_time': 10219.173105716705, 'accumulated_logging_time': 2.522862672805786}
I0311 11:17:35.191810 139621870597888 logging_writer.py:48] [59830] accumulated_eval_time=10219.173106, accumulated_logging_time=2.522863, accumulated_submission_time=18747.749678, global_step=59830, preemption_count=0, score=18747.749678, test/accuracy=0.986180, test/loss=0.050868, test/mean_average_precision=0.281518, test/num_examples=43793, total_duration=28970.991935, train/accuracy=0.994065, train/loss=0.018450, train/mean_average_precision=0.684746, validation/accuracy=0.986987, validation/loss=0.047549, validation/mean_average_precision=0.290008, validation/num_examples=43793
I0311 11:17:57.361947 139628679296768 logging_writer.py:48] [59900] global_step=59900, grad_norm=0.10243681818246841, loss=0.020707042887806892
I0311 11:18:28.527036 139621870597888 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.10149572044610977, loss=0.01851348951458931
I0311 11:18:59.415737 139628679296768 logging_writer.py:48] [60100] global_step=60100, grad_norm=0.11173610389232635, loss=0.021856101229786873
I0311 11:19:30.669599 139621870597888 logging_writer.py:48] [60200] global_step=60200, grad_norm=0.12095486372709274, loss=0.020951202139258385
I0311 11:20:01.729913 139628679296768 logging_writer.py:48] [60300] global_step=60300, grad_norm=0.1157752126455307, loss=0.021851561963558197
I0311 11:20:32.861023 139621870597888 logging_writer.py:48] [60400] global_step=60400, grad_norm=0.11190852522850037, loss=0.021380815654993057
I0311 11:21:04.235230 139628679296768 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.11641806364059448, loss=0.02277258038520813
I0311 11:21:35.457217 139621870597888 logging_writer.py:48] [60600] global_step=60600, grad_norm=0.12324412912130356, loss=0.022407397627830505
I0311 11:21:35.462224 139789500200768 spec.py:321] Evaluating on the training split.
I0311 11:23:29.435461 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 11:23:32.538094 139789500200768 spec.py:349] Evaluating on the test split.
I0311 11:23:35.557207 139789500200768 submission_runner.py:411] Time since start: 29331.38s, 	Step: 60601, 	{'train/accuracy': 0.9941041469573975, 'train/loss': 0.018240880221128464, 'train/mean_average_precision': 0.6792949123470332, 'validation/accuracy': 0.9869810342788696, 'validation/loss': 0.04807710275053978, 'validation/mean_average_precision': 0.2883566277049855, 'validation/num_examples': 43793, 'test/accuracy': 0.9862108826637268, 'test/loss': 0.05136479437351227, 'test/mean_average_precision': 0.277457402308432, 'test/num_examples': 43793, 'score': 18987.989778280258, 'total_duration': 29331.38431572914, 'accumulated_submission_time': 18987.989778280258, 'accumulated_eval_time': 10339.2678835392, 'accumulated_logging_time': 2.560737371444702}
I0311 11:23:35.583351 139622180505344 logging_writer.py:48] [60601] accumulated_eval_time=10339.267884, accumulated_logging_time=2.560737, accumulated_submission_time=18987.989778, global_step=60601, preemption_count=0, score=18987.989778, test/accuracy=0.986211, test/loss=0.051365, test/mean_average_precision=0.277457, test/num_examples=43793, total_duration=29331.384316, train/accuracy=0.994104, train/loss=0.018241, train/mean_average_precision=0.679295, validation/accuracy=0.986981, validation/loss=0.048077, validation/mean_average_precision=0.288357, validation/num_examples=43793
I0311 11:24:06.661915 139628670904064 logging_writer.py:48] [60700] global_step=60700, grad_norm=0.11222686618566513, loss=0.022346150130033493
I0311 11:24:37.791261 139622180505344 logging_writer.py:48] [60800] global_step=60800, grad_norm=0.14499028027057648, loss=0.023528404533863068
I0311 11:25:09.184139 139628670904064 logging_writer.py:48] [60900] global_step=60900, grad_norm=0.13065338134765625, loss=0.02108089067041874
I0311 11:25:40.603467 139622180505344 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.11402299255132675, loss=0.020373672246932983
I0311 11:26:11.735888 139628670904064 logging_writer.py:48] [61100] global_step=61100, grad_norm=0.11849430203437805, loss=0.020587991923093796
I0311 11:26:42.879179 139622180505344 logging_writer.py:48] [61200] global_step=61200, grad_norm=0.1173708587884903, loss=0.01915508508682251
I0311 11:27:14.002802 139628670904064 logging_writer.py:48] [61300] global_step=61300, grad_norm=0.12593474984169006, loss=0.021922141313552856
I0311 11:27:35.562683 139789500200768 spec.py:321] Evaluating on the training split.
I0311 11:29:26.984497 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 11:29:30.015646 139789500200768 spec.py:349] Evaluating on the test split.
I0311 11:29:33.046642 139789500200768 submission_runner.py:411] Time since start: 29688.87s, 	Step: 61370, 	{'train/accuracy': 0.9942519664764404, 'train/loss': 0.017891153693199158, 'train/mean_average_precision': 0.6891838281450775, 'validation/accuracy': 0.9869696497917175, 'validation/loss': 0.048091426491737366, 'validation/mean_average_precision': 0.29077759394154923, 'validation/num_examples': 43793, 'test/accuracy': 0.9860908389091492, 'test/loss': 0.051525529474020004, 'test/mean_average_precision': 0.27580721306580536, 'test/num_examples': 43793, 'score': 19227.93788957596, 'total_duration': 29688.873893022537, 'accumulated_submission_time': 19227.93788957596, 'accumulated_eval_time': 10456.751803159714, 'accumulated_logging_time': 2.598947525024414}
I0311 11:29:33.073675 139621870597888 logging_writer.py:48] [61370] accumulated_eval_time=10456.751803, accumulated_logging_time=2.598948, accumulated_submission_time=19227.937890, global_step=61370, preemption_count=0, score=19227.937890, test/accuracy=0.986091, test/loss=0.051526, test/mean_average_precision=0.275807, test/num_examples=43793, total_duration=29688.873893, train/accuracy=0.994252, train/loss=0.017891, train/mean_average_precision=0.689184, validation/accuracy=0.986970, validation/loss=0.048091, validation/mean_average_precision=0.290778, validation/num_examples=43793
I0311 11:29:42.707893 139622172112640 logging_writer.py:48] [61400] global_step=61400, grad_norm=0.13446402549743652, loss=0.02223590388894081
I0311 11:30:13.907478 139621870597888 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.11315588653087616, loss=0.02045660838484764
I0311 11:30:45.405039 139622172112640 logging_writer.py:48] [61600] global_step=61600, grad_norm=0.13343478739261627, loss=0.019740071147680283
I0311 11:31:16.703821 139621870597888 logging_writer.py:48] [61700] global_step=61700, grad_norm=0.15120232105255127, loss=0.022513626143336296
I0311 11:31:47.589778 139622172112640 logging_writer.py:48] [61800] global_step=61800, grad_norm=0.12578082084655762, loss=0.02264031022787094
I0311 11:32:18.577268 139621870597888 logging_writer.py:48] [61900] global_step=61900, grad_norm=0.10655757039785385, loss=0.020296510308980942
I0311 11:32:49.790746 139622172112640 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.11717845499515533, loss=0.020831994712352753
I0311 11:33:20.669696 139621870597888 logging_writer.py:48] [62100] global_step=62100, grad_norm=0.10909316688776016, loss=0.019101234152913094
I0311 11:33:33.151583 139789500200768 spec.py:321] Evaluating on the training split.
I0311 11:35:27.810719 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 11:35:30.859827 139789500200768 spec.py:349] Evaluating on the test split.
I0311 11:35:33.863050 139789500200768 submission_runner.py:411] Time since start: 30049.69s, 	Step: 62141, 	{'train/accuracy': 0.9941074252128601, 'train/loss': 0.018122250214219093, 'train/mean_average_precision': 0.6837093136990084, 'validation/accuracy': 0.9870021343231201, 'validation/loss': 0.048569802194833755, 'validation/mean_average_precision': 0.28823224451288115, 'validation/num_examples': 43793, 'test/accuracy': 0.9862163662910461, 'test/loss': 0.05191672220826149, 'test/mean_average_precision': 0.27297115370326647, 'test/num_examples': 43793, 'score': 19467.98549079895, 'total_duration': 30049.69030022621, 'accumulated_submission_time': 19467.98549079895, 'accumulated_eval_time': 10577.463264465332, 'accumulated_logging_time': 2.6371090412139893}
I0311 11:35:33.890039 139622180505344 logging_writer.py:48] [62141] accumulated_eval_time=10577.463264, accumulated_logging_time=2.637109, accumulated_submission_time=19467.985491, global_step=62141, preemption_count=0, score=19467.985491, test/accuracy=0.986216, test/loss=0.051917, test/mean_average_precision=0.272971, test/num_examples=43793, total_duration=30049.690300, train/accuracy=0.994107, train/loss=0.018122, train/mean_average_precision=0.683709, validation/accuracy=0.987002, validation/loss=0.048570, validation/mean_average_precision=0.288232, validation/num_examples=43793
I0311 11:35:52.586775 139628679296768 logging_writer.py:48] [62200] global_step=62200, grad_norm=0.1141463965177536, loss=0.018629910424351692
I0311 11:36:23.731448 139622180505344 logging_writer.py:48] [62300] global_step=62300, grad_norm=0.10836343467235565, loss=0.01697678118944168
I0311 11:36:54.600522 139628679296768 logging_writer.py:48] [62400] global_step=62400, grad_norm=0.1203482449054718, loss=0.019709326326847076
I0311 11:37:25.663520 139622180505344 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.14252305030822754, loss=0.020458199083805084
I0311 11:37:56.936986 139628679296768 logging_writer.py:48] [62600] global_step=62600, grad_norm=0.12485121190547943, loss=0.017854470759630203
I0311 11:38:28.012122 139622180505344 logging_writer.py:48] [62700] global_step=62700, grad_norm=0.11541976034641266, loss=0.018961375579237938
I0311 11:38:58.867987 139628679296768 logging_writer.py:48] [62800] global_step=62800, grad_norm=0.12605342268943787, loss=0.019367707893252373
I0311 11:39:29.956149 139622180505344 logging_writer.py:48] [62900] global_step=62900, grad_norm=0.1207292228937149, loss=0.019180867820978165
I0311 11:39:33.914510 139789500200768 spec.py:321] Evaluating on the training split.
I0311 11:41:27.511636 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 11:41:30.518669 139789500200768 spec.py:349] Evaluating on the test split.
I0311 11:41:33.556888 139789500200768 submission_runner.py:411] Time since start: 30409.38s, 	Step: 62914, 	{'train/accuracy': 0.9943687319755554, 'train/loss': 0.017458561807870865, 'train/mean_average_precision': 0.710569110240898, 'validation/accuracy': 0.9870545268058777, 'validation/loss': 0.04856282100081444, 'validation/mean_average_precision': 0.2889291577087489, 'validation/num_examples': 43793, 'test/accuracy': 0.9861868619918823, 'test/loss': 0.052145179361104965, 'test/mean_average_precision': 0.2739277181367657, 'test/num_examples': 43793, 'score': 19707.979488134384, 'total_duration': 30409.384122371674, 'accumulated_submission_time': 19707.979488134384, 'accumulated_eval_time': 10697.105581521988, 'accumulated_logging_time': 2.6750237941741943}
I0311 11:41:33.583900 139622172112640 logging_writer.py:48] [62914] accumulated_eval_time=10697.105582, accumulated_logging_time=2.675024, accumulated_submission_time=19707.979488, global_step=62914, preemption_count=0, score=19707.979488, test/accuracy=0.986187, test/loss=0.052145, test/mean_average_precision=0.273928, test/num_examples=43793, total_duration=30409.384122, train/accuracy=0.994369, train/loss=0.017459, train/mean_average_precision=0.710569, validation/accuracy=0.987055, validation/loss=0.048563, validation/mean_average_precision=0.288929, validation/num_examples=43793
I0311 11:42:01.145635 139628670904064 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.13092413544654846, loss=0.019823838025331497
I0311 11:42:32.181165 139622172112640 logging_writer.py:48] [63100] global_step=63100, grad_norm=0.11570194363594055, loss=0.02019633539021015
I0311 11:43:03.099108 139628670904064 logging_writer.py:48] [63200] global_step=63200, grad_norm=0.11821030080318451, loss=0.02046024613082409
I0311 11:43:34.329214 139622172112640 logging_writer.py:48] [63300] global_step=63300, grad_norm=0.12409710884094238, loss=0.0206234659999609
I0311 11:44:05.561624 139628670904064 logging_writer.py:48] [63400] global_step=63400, grad_norm=0.12751708924770355, loss=0.021475130692124367
I0311 11:44:36.653192 139622172112640 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.12937116622924805, loss=0.021358584985136986
I0311 11:45:07.683920 139628670904064 logging_writer.py:48] [63600] global_step=63600, grad_norm=0.1450124830007553, loss=0.022685518488287926
I0311 11:45:33.699974 139789500200768 spec.py:321] Evaluating on the training split.
I0311 11:47:28.056244 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 11:47:31.185221 139789500200768 spec.py:349] Evaluating on the test split.
I0311 11:47:34.194301 139789500200768 submission_runner.py:411] Time since start: 30770.02s, 	Step: 63683, 	{'train/accuracy': 0.9945791959762573, 'train/loss': 0.016849100589752197, 'train/mean_average_precision': 0.7172802330353509, 'validation/accuracy': 0.9870272874832153, 'validation/loss': 0.04886254295706749, 'validation/mean_average_precision': 0.2913743414060199, 'validation/num_examples': 43793, 'test/accuracy': 0.9861218333244324, 'test/loss': 0.05235057696700096, 'test/mean_average_precision': 0.2753913641660267, 'test/num_examples': 43793, 'score': 19948.06530070305, 'total_duration': 30770.02154660225, 'accumulated_submission_time': 19948.06530070305, 'accumulated_eval_time': 10817.599865198135, 'accumulated_logging_time': 2.7128069400787354}
I0311 11:47:34.222208 139621870597888 logging_writer.py:48] [63683] accumulated_eval_time=10817.599865, accumulated_logging_time=2.712807, accumulated_submission_time=19948.065301, global_step=63683, preemption_count=0, score=19948.065301, test/accuracy=0.986122, test/loss=0.052351, test/mean_average_precision=0.275391, test/num_examples=43793, total_duration=30770.021547, train/accuracy=0.994579, train/loss=0.016849, train/mean_average_precision=0.717280, validation/accuracy=0.987027, validation/loss=0.048863, validation/mean_average_precision=0.291374, validation/num_examples=43793
I0311 11:47:40.025531 139628679296768 logging_writer.py:48] [63700] global_step=63700, grad_norm=0.12467613816261292, loss=0.020126234740018845
I0311 11:48:11.569584 139621870597888 logging_writer.py:48] [63800] global_step=63800, grad_norm=0.13719965517520905, loss=0.021862508729100227
I0311 11:48:42.767482 139628679296768 logging_writer.py:48] [63900] global_step=63900, grad_norm=0.12920139729976654, loss=0.0195060595870018
I0311 11:49:13.964338 139621870597888 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.12385192513465881, loss=0.017938556149601936
I0311 11:49:45.005896 139628679296768 logging_writer.py:48] [64100] global_step=64100, grad_norm=0.1357075572013855, loss=0.020063752308487892
I0311 11:50:16.302110 139621870597888 logging_writer.py:48] [64200] global_step=64200, grad_norm=0.12918049097061157, loss=0.021528879180550575
I0311 11:50:47.355985 139628679296768 logging_writer.py:48] [64300] global_step=64300, grad_norm=0.1279096007347107, loss=0.021296534687280655
I0311 11:51:18.749606 139621870597888 logging_writer.py:48] [64400] global_step=64400, grad_norm=0.12001083046197891, loss=0.018504176288843155
I0311 11:51:34.381620 139789500200768 spec.py:321] Evaluating on the training split.
I0311 11:53:27.249426 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 11:53:30.258457 139789500200768 spec.py:349] Evaluating on the test split.
I0311 11:53:33.227017 139789500200768 submission_runner.py:411] Time since start: 31129.05s, 	Step: 64451, 	{'train/accuracy': 0.9945203065872192, 'train/loss': 0.01688815839588642, 'train/mean_average_precision': 0.7186517583486293, 'validation/accuracy': 0.9870195984840393, 'validation/loss': 0.04916929081082344, 'validation/mean_average_precision': 0.2907579213342952, 'validation/num_examples': 43793, 'test/accuracy': 0.9862428903579712, 'test/loss': 0.052708618342876434, 'test/mean_average_precision': 0.27606586085011925, 'test/num_examples': 43793, 'score': 20188.194657087326, 'total_duration': 31129.054266929626, 'accumulated_submission_time': 20188.194657087326, 'accumulated_eval_time': 10936.445219993591, 'accumulated_logging_time': 2.7514443397521973}
I0311 11:53:33.254456 139622172112640 logging_writer.py:48] [64451] accumulated_eval_time=10936.445220, accumulated_logging_time=2.751444, accumulated_submission_time=20188.194657, global_step=64451, preemption_count=0, score=20188.194657, test/accuracy=0.986243, test/loss=0.052709, test/mean_average_precision=0.276066, test/num_examples=43793, total_duration=31129.054267, train/accuracy=0.994520, train/loss=0.016888, train/mean_average_precision=0.718652, validation/accuracy=0.987020, validation/loss=0.049169, validation/mean_average_precision=0.290758, validation/num_examples=43793
I0311 11:53:48.638500 139622180505344 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.12412349134683609, loss=0.021311137825250626
I0311 11:54:19.724136 139622172112640 logging_writer.py:48] [64600] global_step=64600, grad_norm=0.11714760959148407, loss=0.018245814368128777
I0311 11:54:50.891227 139622180505344 logging_writer.py:48] [64700] global_step=64700, grad_norm=0.13248233497142792, loss=0.018677663058042526
I0311 11:55:21.942354 139622172112640 logging_writer.py:48] [64800] global_step=64800, grad_norm=0.15656067430973053, loss=0.023500297218561172
I0311 11:55:52.866650 139622180505344 logging_writer.py:48] [64900] global_step=64900, grad_norm=0.11358169466257095, loss=0.01890050619840622
I0311 11:56:24.194384 139622172112640 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.13799846172332764, loss=0.01922973059117794
I0311 11:56:55.629719 139622180505344 logging_writer.py:48] [65100] global_step=65100, grad_norm=0.12960606813430786, loss=0.020149312913417816
I0311 11:57:26.777587 139622172112640 logging_writer.py:48] [65200] global_step=65200, grad_norm=0.12907540798187256, loss=0.021810226142406464
I0311 11:57:33.403672 139789500200768 spec.py:321] Evaluating on the training split.
I0311 11:59:26.857128 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 11:59:29.872150 139789500200768 spec.py:349] Evaluating on the test split.
I0311 11:59:32.894074 139789500200768 submission_runner.py:411] Time since start: 31488.72s, 	Step: 65222, 	{'train/accuracy': 0.9947992563247681, 'train/loss': 0.016018958762288094, 'train/mean_average_precision': 0.7327114869218326, 'validation/accuracy': 0.9871072769165039, 'validation/loss': 0.049162641167640686, 'validation/mean_average_precision': 0.28769506812764, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05266168713569641, 'test/mean_average_precision': 0.2729145729121758, 'test/num_examples': 43793, 'score': 20428.312505960464, 'total_duration': 31488.721318006516, 'accumulated_submission_time': 20428.312505960464, 'accumulated_eval_time': 11055.935570955276, 'accumulated_logging_time': 2.790802240371704}
I0311 11:59:32.921192 139628670904064 logging_writer.py:48] [65222] accumulated_eval_time=11055.935571, accumulated_logging_time=2.790802, accumulated_submission_time=20428.312506, global_step=65222, preemption_count=0, score=20428.312506, test/accuracy=0.986211, test/loss=0.052662, test/mean_average_precision=0.272915, test/num_examples=43793, total_duration=31488.721318, train/accuracy=0.994799, train/loss=0.016019, train/mean_average_precision=0.732711, validation/accuracy=0.987107, validation/loss=0.049163, validation/mean_average_precision=0.287695, validation/num_examples=43793
I0311 11:59:57.715796 139628679296768 logging_writer.py:48] [65300] global_step=65300, grad_norm=0.14120984077453613, loss=0.02060677669942379
I0311 12:00:28.661314 139628670904064 logging_writer.py:48] [65400] global_step=65400, grad_norm=0.15995223820209503, loss=0.01950746215879917
I0311 12:00:59.862571 139628679296768 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.12670548260211945, loss=0.016375966370105743
I0311 12:01:30.605422 139628670904064 logging_writer.py:48] [65600] global_step=65600, grad_norm=0.1388908177614212, loss=0.01928565837442875
I0311 12:02:01.813112 139628679296768 logging_writer.py:48] [65700] global_step=65700, grad_norm=0.14013910293579102, loss=0.022404400631785393
I0311 12:02:32.840627 139628670904064 logging_writer.py:48] [65800] global_step=65800, grad_norm=0.1304558366537094, loss=0.018914885818958282
I0311 12:03:03.664997 139628679296768 logging_writer.py:48] [65900] global_step=65900, grad_norm=0.12741105258464813, loss=0.01886274479329586
I0311 12:03:33.057981 139789500200768 spec.py:321] Evaluating on the training split.
I0311 12:05:26.576554 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 12:05:29.612853 139789500200768 spec.py:349] Evaluating on the test split.
I0311 12:05:32.618322 139789500200768 submission_runner.py:411] Time since start: 31848.45s, 	Step: 65996, 	{'train/accuracy': 0.9950602650642395, 'train/loss': 0.015398657880723476, 'train/mean_average_precision': 0.7444797052813373, 'validation/accuracy': 0.9870184063911438, 'validation/loss': 0.04950360953807831, 'validation/mean_average_precision': 0.28588816281787016, 'validation/num_examples': 43793, 'test/accuracy': 0.9861881732940674, 'test/loss': 0.05297314003109932, 'test/mean_average_precision': 0.2759885855868024, 'test/num_examples': 43793, 'score': 20668.418220043182, 'total_duration': 31848.445571899414, 'accumulated_submission_time': 20668.418220043182, 'accumulated_eval_time': 11175.49587225914, 'accumulated_logging_time': 2.8298258781433105}
I0311 12:05:32.647190 139621870597888 logging_writer.py:48] [65996] accumulated_eval_time=11175.495872, accumulated_logging_time=2.829826, accumulated_submission_time=20668.418220, global_step=65996, preemption_count=0, score=20668.418220, test/accuracy=0.986188, test/loss=0.052973, test/mean_average_precision=0.275989, test/num_examples=43793, total_duration=31848.445572, train/accuracy=0.995060, train/loss=0.015399, train/mean_average_precision=0.744480, validation/accuracy=0.987018, validation/loss=0.049504, validation/mean_average_precision=0.285888, validation/num_examples=43793
I0311 12:05:34.205433 139622180505344 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.12712576985359192, loss=0.018824543803930283
I0311 12:06:05.299245 139621870597888 logging_writer.py:48] [66100] global_step=66100, grad_norm=0.13645541667938232, loss=0.017559241503477097
I0311 12:06:36.285135 139622180505344 logging_writer.py:48] [66200] global_step=66200, grad_norm=0.12158820033073425, loss=0.01773597113788128
I0311 12:07:07.269585 139621870597888 logging_writer.py:48] [66300] global_step=66300, grad_norm=0.1329161822795868, loss=0.0206998810172081
I0311 12:07:38.688144 139622180505344 logging_writer.py:48] [66400] global_step=66400, grad_norm=0.14147748053073883, loss=0.019200876355171204
I0311 12:08:09.609714 139621870597888 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.15073813498020172, loss=0.019800690934062004
I0311 12:08:40.483852 139622180505344 logging_writer.py:48] [66600] global_step=66600, grad_norm=0.13680648803710938, loss=0.020132683217525482
I0311 12:09:11.771487 139621870597888 logging_writer.py:48] [66700] global_step=66700, grad_norm=0.14048677682876587, loss=0.01916913129389286
I0311 12:09:32.871618 139789500200768 spec.py:321] Evaluating on the training split.
I0311 12:11:27.023235 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 12:11:30.087633 139789500200768 spec.py:349] Evaluating on the test split.
I0311 12:11:33.096463 139789500200768 submission_runner.py:411] Time since start: 32208.92s, 	Step: 66768, 	{'train/accuracy': 0.9951653480529785, 'train/loss': 0.015196551568806171, 'train/mean_average_precision': 0.7628244662934002, 'validation/accuracy': 0.9869709014892578, 'validation/loss': 0.04945063963532448, 'validation/mean_average_precision': 0.284574734141852, 'validation/num_examples': 43793, 'test/accuracy': 0.9861089587211609, 'test/loss': 0.052991099655628204, 'test/mean_average_precision': 0.2741048176481009, 'test/num_examples': 43793, 'score': 20908.612892866135, 'total_duration': 32208.923696279526, 'accumulated_submission_time': 20908.612892866135, 'accumulated_eval_time': 11295.720662355423, 'accumulated_logging_time': 2.86948561668396}
I0311 12:11:33.126031 139622172112640 logging_writer.py:48] [66768] accumulated_eval_time=11295.720662, accumulated_logging_time=2.869486, accumulated_submission_time=20908.612893, global_step=66768, preemption_count=0, score=20908.612893, test/accuracy=0.986109, test/loss=0.052991, test/mean_average_precision=0.274105, test/num_examples=43793, total_duration=32208.923696, train/accuracy=0.995165, train/loss=0.015197, train/mean_average_precision=0.762824, validation/accuracy=0.986971, validation/loss=0.049451, validation/mean_average_precision=0.284575, validation/num_examples=43793
I0311 12:11:43.592636 139628670904064 logging_writer.py:48] [66800] global_step=66800, grad_norm=0.15907490253448486, loss=0.02209220640361309
I0311 12:12:15.230497 139622172112640 logging_writer.py:48] [66900] global_step=66900, grad_norm=0.12696315348148346, loss=0.019196033477783203
I0311 12:12:45.971865 139628670904064 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.1482933759689331, loss=0.02066405862569809
I0311 12:13:16.933854 139622172112640 logging_writer.py:48] [67100] global_step=67100, grad_norm=0.14318260550498962, loss=0.021940691396594048
I0311 12:13:47.741377 139628670904064 logging_writer.py:48] [67200] global_step=67200, grad_norm=0.15290386974811554, loss=0.018273325636982918
I0311 12:14:18.695075 139622172112640 logging_writer.py:48] [67300] global_step=67300, grad_norm=0.13168585300445557, loss=0.019059479236602783
I0311 12:14:49.823167 139628670904064 logging_writer.py:48] [67400] global_step=67400, grad_norm=0.12343299388885498, loss=0.01925678551197052
I0311 12:15:20.811167 139622172112640 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.16103848814964294, loss=0.019925421103835106
I0311 12:15:33.161766 139789500200768 spec.py:321] Evaluating on the training split.
I0311 12:17:28.825185 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 12:17:31.885749 139789500200768 spec.py:349] Evaluating on the test split.
I0311 12:17:34.862714 139789500200768 submission_runner.py:411] Time since start: 32570.69s, 	Step: 67541, 	{'train/accuracy': 0.995209276676178, 'train/loss': 0.015028637833893299, 'train/mean_average_precision': 0.7566962851795804, 'validation/accuracy': 0.9869514107704163, 'validation/loss': 0.04956110939383507, 'validation/mean_average_precision': 0.2874051538617712, 'validation/num_examples': 43793, 'test/accuracy': 0.9860563278198242, 'test/loss': 0.05303581804037094, 'test/mean_average_precision': 0.27888116810711416, 'test/num_examples': 43793, 'score': 21148.618430376053, 'total_duration': 32570.689962387085, 'accumulated_submission_time': 21148.618430376053, 'accumulated_eval_time': 11417.421566963196, 'accumulated_logging_time': 2.909762382507324}
I0311 12:17:34.890625 139622180505344 logging_writer.py:48] [67541] accumulated_eval_time=11417.421567, accumulated_logging_time=2.909762, accumulated_submission_time=21148.618430, global_step=67541, preemption_count=0, score=21148.618430, test/accuracy=0.986056, test/loss=0.053036, test/mean_average_precision=0.278881, test/num_examples=43793, total_duration=32570.689962, train/accuracy=0.995209, train/loss=0.015029, train/mean_average_precision=0.756696, validation/accuracy=0.986951, validation/loss=0.049561, validation/mean_average_precision=0.287405, validation/num_examples=43793
I0311 12:17:53.359879 139628679296768 logging_writer.py:48] [67600] global_step=67600, grad_norm=0.1507679671049118, loss=0.01990405097603798
I0311 12:18:24.544402 139622180505344 logging_writer.py:48] [67700] global_step=67700, grad_norm=0.12694403529167175, loss=0.019141050055623055
I0311 12:18:55.740820 139628679296768 logging_writer.py:48] [67800] global_step=67800, grad_norm=0.13551115989685059, loss=0.018976671621203423
I0311 12:19:26.913624 139622180505344 logging_writer.py:48] [67900] global_step=67900, grad_norm=0.13480985164642334, loss=0.0168666560202837
I0311 12:19:57.895313 139628679296768 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.1440642774105072, loss=0.019905779510736465
I0311 12:20:29.002785 139622180505344 logging_writer.py:48] [68100] global_step=68100, grad_norm=0.14367835223674774, loss=0.019226165488362312
I0311 12:21:00.183482 139628679296768 logging_writer.py:48] [68200] global_step=68200, grad_norm=0.13627773523330688, loss=0.017826497554779053
I0311 12:21:31.261270 139622180505344 logging_writer.py:48] [68300] global_step=68300, grad_norm=0.14772433042526245, loss=0.01821143552660942
I0311 12:21:35.015404 139789500200768 spec.py:321] Evaluating on the training split.
I0311 12:23:29.235920 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 12:23:32.260761 139789500200768 spec.py:349] Evaluating on the test split.
I0311 12:23:35.234716 139789500200768 submission_runner.py:411] Time since start: 32931.06s, 	Step: 68313, 	{'train/accuracy': 0.9951818585395813, 'train/loss': 0.015097702853381634, 'train/mean_average_precision': 0.7568026094827762, 'validation/accuracy': 0.9868556261062622, 'validation/loss': 0.049604371190071106, 'validation/mean_average_precision': 0.2851188808682722, 'validation/num_examples': 43793, 'test/accuracy': 0.9860062003135681, 'test/loss': 0.05310899391770363, 'test/mean_average_precision': 0.27431121784486373, 'test/num_examples': 43793, 'score': 21388.713089466095, 'total_duration': 32931.06196761131, 'accumulated_submission_time': 21388.713089466095, 'accumulated_eval_time': 11537.640836715698, 'accumulated_logging_time': 2.9481992721557617}
I0311 12:23:35.262473 139621870597888 logging_writer.py:48] [68313] accumulated_eval_time=11537.640837, accumulated_logging_time=2.948199, accumulated_submission_time=21388.713089, global_step=68313, preemption_count=0, score=21388.713089, test/accuracy=0.986006, test/loss=0.053109, test/mean_average_precision=0.274311, test/num_examples=43793, total_duration=32931.061968, train/accuracy=0.995182, train/loss=0.015098, train/mean_average_precision=0.756803, validation/accuracy=0.986856, validation/loss=0.049604, validation/mean_average_precision=0.285119, validation/num_examples=43793
I0311 12:24:02.858671 139628670904064 logging_writer.py:48] [68400] global_step=68400, grad_norm=0.1375705599784851, loss=0.018907640129327774
I0311 12:24:34.204961 139621870597888 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.14763489365577698, loss=0.018463097512722015
I0311 12:25:05.431259 139628670904064 logging_writer.py:48] [68600] global_step=68600, grad_norm=0.1233009546995163, loss=0.016305040568113327
I0311 12:25:36.517084 139621870597888 logging_writer.py:48] [68700] global_step=68700, grad_norm=0.14987999200820923, loss=0.018501101061701775
I0311 12:26:07.840671 139628670904064 logging_writer.py:48] [68800] global_step=68800, grad_norm=0.126092791557312, loss=0.01791733130812645
I0311 12:26:39.250316 139621870597888 logging_writer.py:48] [68900] global_step=68900, grad_norm=0.13735537230968475, loss=0.017522023990750313
I0311 12:27:10.344706 139628670904064 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.13451111316680908, loss=0.02066652849316597
I0311 12:27:35.247054 139789500200768 spec.py:321] Evaluating on the training split.
I0311 12:29:27.284018 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 12:29:30.301854 139789500200768 spec.py:349] Evaluating on the test split.
I0311 12:29:33.322252 139789500200768 submission_runner.py:411] Time since start: 33289.15s, 	Step: 69080, 	{'train/accuracy': 0.9952203035354614, 'train/loss': 0.014993236400187016, 'train/mean_average_precision': 0.7534647243648879, 'validation/accuracy': 0.9869457483291626, 'validation/loss': 0.0499713309109211, 'validation/mean_average_precision': 0.2849290280465007, 'validation/num_examples': 43793, 'test/accuracy': 0.9861005544662476, 'test/loss': 0.05352845788002014, 'test/mean_average_precision': 0.27518389738457233, 'test/num_examples': 43793, 'score': 21628.667563438416, 'total_duration': 33289.1495051384, 'accumulated_submission_time': 21628.667563438416, 'accumulated_eval_time': 11655.715996980667, 'accumulated_logging_time': 2.9867284297943115}
I0311 12:29:33.350205 139622180505344 logging_writer.py:48] [69080] accumulated_eval_time=11655.715997, accumulated_logging_time=2.986728, accumulated_submission_time=21628.667563, global_step=69080, preemption_count=0, score=21628.667563, test/accuracy=0.986101, test/loss=0.053528, test/mean_average_precision=0.275184, test/num_examples=43793, total_duration=33289.149505, train/accuracy=0.995220, train/loss=0.014993, train/mean_average_precision=0.753465, validation/accuracy=0.986946, validation/loss=0.049971, validation/mean_average_precision=0.284929, validation/num_examples=43793
I0311 12:29:39.934911 139628679296768 logging_writer.py:48] [69100] global_step=69100, grad_norm=0.13052749633789062, loss=0.01874181255698204
I0311 12:30:11.171514 139622180505344 logging_writer.py:48] [69200] global_step=69200, grad_norm=0.1219816654920578, loss=0.015777625143527985
I0311 12:30:42.645989 139628679296768 logging_writer.py:48] [69300] global_step=69300, grad_norm=0.14243990182876587, loss=0.020394112914800644
I0311 12:31:15.377614 139622180505344 logging_writer.py:48] [69400] global_step=69400, grad_norm=0.13655176758766174, loss=0.019176779314875603
I0311 12:31:47.847329 139628679296768 logging_writer.py:48] [69500] global_step=69500, grad_norm=0.14337417483329773, loss=0.019118763506412506
I0311 12:32:20.444732 139622180505344 logging_writer.py:48] [69600] global_step=69600, grad_norm=0.13282455503940582, loss=0.01878512278199196
I0311 12:32:53.028342 139628679296768 logging_writer.py:48] [69700] global_step=69700, grad_norm=0.1458844542503357, loss=0.019689980894327164
I0311 12:33:25.719010 139622180505344 logging_writer.py:48] [69800] global_step=69800, grad_norm=0.15172500908374786, loss=0.020835593342781067
I0311 12:33:33.536040 139789500200768 spec.py:321] Evaluating on the training split.
I0311 12:35:38.435674 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 12:35:41.874343 139789500200768 spec.py:349] Evaluating on the test split.
I0311 12:35:45.261724 139789500200768 submission_runner.py:411] Time since start: 33661.09s, 	Step: 69825, 	{'train/accuracy': 0.9949262142181396, 'train/loss': 0.015657195821404457, 'train/mean_average_precision': 0.7358463544646767, 'validation/accuracy': 0.986976146697998, 'validation/loss': 0.050358448177576065, 'validation/mean_average_precision': 0.285283862791445, 'validation/num_examples': 43793, 'test/accuracy': 0.9861228466033936, 'test/loss': 0.05389023199677467, 'test/mean_average_precision': 0.2756676195806692, 'test/num_examples': 43793, 'score': 21868.81844639778, 'total_duration': 33661.08895611763, 'accumulated_submission_time': 21868.81844639778, 'accumulated_eval_time': 11787.441627502441, 'accumulated_logging_time': 3.0268402099609375}
I0311 12:35:45.295561 139622172112640 logging_writer.py:48] [69825] accumulated_eval_time=11787.441628, accumulated_logging_time=3.026840, accumulated_submission_time=21868.818446, global_step=69825, preemption_count=0, score=21868.818446, test/accuracy=0.986123, test/loss=0.053890, test/mean_average_precision=0.275668, test/num_examples=43793, total_duration=33661.088956, train/accuracy=0.994926, train/loss=0.015657, train/mean_average_precision=0.735846, validation/accuracy=0.986976, validation/loss=0.050358, validation/mean_average_precision=0.285284, validation/num_examples=43793
I0311 12:36:10.470434 139628670904064 logging_writer.py:48] [69900] global_step=69900, grad_norm=0.146382138133049, loss=0.019456829875707626
I0311 12:36:43.218860 139622172112640 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.1454162895679474, loss=0.016737276688218117
I0311 12:37:15.969266 139628670904064 logging_writer.py:48] [70100] global_step=70100, grad_norm=0.13043969869613647, loss=0.019490646198391914
I0311 12:37:48.607357 139622172112640 logging_writer.py:48] [70200] global_step=70200, grad_norm=0.11634072661399841, loss=0.01715617999434471
I0311 12:38:21.242799 139628670904064 logging_writer.py:48] [70300] global_step=70300, grad_norm=0.15246640145778656, loss=0.016207141801714897
I0311 12:38:53.448551 139622172112640 logging_writer.py:48] [70400] global_step=70400, grad_norm=0.12418387085199356, loss=0.01712518185377121
I0311 12:39:25.803097 139628670904064 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.13200703263282776, loss=0.018290702253580093
I0311 12:39:45.578835 139789500200768 spec.py:321] Evaluating on the training split.
I0311 12:41:42.593272 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 12:41:46.130840 139789500200768 spec.py:349] Evaluating on the test split.
I0311 12:41:49.530167 139789500200768 submission_runner.py:411] Time since start: 34025.36s, 	Step: 70562, 	{'train/accuracy': 0.9949321746826172, 'train/loss': 0.01567414216697216, 'train/mean_average_precision': 0.7396729190456879, 'validation/accuracy': 0.9868730306625366, 'validation/loss': 0.050321001559495926, 'validation/mean_average_precision': 0.28472577276448835, 'validation/num_examples': 43793, 'test/accuracy': 0.9860192537307739, 'test/loss': 0.05383049696683884, 'test/mean_average_precision': 0.2737792846556728, 'test/num_examples': 43793, 'score': 22109.06586909294, 'total_duration': 34025.35740637779, 'accumulated_submission_time': 22109.06586909294, 'accumulated_eval_time': 11911.392924785614, 'accumulated_logging_time': 3.0721487998962402}
I0311 12:41:49.562072 139621870597888 logging_writer.py:48] [70562] accumulated_eval_time=11911.392925, accumulated_logging_time=3.072149, accumulated_submission_time=22109.065869, global_step=70562, preemption_count=0, score=22109.065869, test/accuracy=0.986019, test/loss=0.053830, test/mean_average_precision=0.273779, test/num_examples=43793, total_duration=34025.357406, train/accuracy=0.994932, train/loss=0.015674, train/mean_average_precision=0.739673, validation/accuracy=0.986873, validation/loss=0.050321, validation/mean_average_precision=0.284726, validation/num_examples=43793
I0311 12:42:02.275125 139628679296768 logging_writer.py:48] [70600] global_step=70600, grad_norm=0.14178083837032318, loss=0.017603516578674316
I0311 12:42:34.586935 139621870597888 logging_writer.py:48] [70700] global_step=70700, grad_norm=0.14671970903873444, loss=0.01900399848818779
I0311 12:43:07.179193 139628679296768 logging_writer.py:48] [70800] global_step=70800, grad_norm=0.14120417833328247, loss=0.017522845417261124
I0311 12:43:39.658542 139621870597888 logging_writer.py:48] [70900] global_step=70900, grad_norm=0.15437613427639008, loss=0.02095593325793743
I0311 12:44:12.044275 139628679296768 logging_writer.py:48] [71000] global_step=71000, grad_norm=0.14607253670692444, loss=0.01904900185763836
I0311 12:44:44.401793 139621870597888 logging_writer.py:48] [71100] global_step=71100, grad_norm=0.13145025074481964, loss=0.016755778342485428
I0311 12:45:17.067601 139628679296768 logging_writer.py:48] [71200] global_step=71200, grad_norm=0.13486464321613312, loss=0.018914546817541122
I0311 12:45:49.364819 139621870597888 logging_writer.py:48] [71300] global_step=71300, grad_norm=0.155784472823143, loss=0.019296975806355476
I0311 12:45:49.717466 139789500200768 spec.py:321] Evaluating on the training split.
I0311 12:47:52.891773 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 12:47:55.983619 139789500200768 spec.py:349] Evaluating on the test split.
I0311 12:47:58.977998 139789500200768 submission_runner.py:411] Time since start: 34394.81s, 	Step: 71302, 	{'train/accuracy': 0.9951404333114624, 'train/loss': 0.015093795955181122, 'train/mean_average_precision': 0.753908089745251, 'validation/accuracy': 0.9869737029075623, 'validation/loss': 0.05020499974489212, 'validation/mean_average_precision': 0.28967990362220764, 'validation/num_examples': 43793, 'test/accuracy': 0.9860963225364685, 'test/loss': 0.05377104505896568, 'test/mean_average_precision': 0.27544795722023047, 'test/num_examples': 43793, 'score': 22349.18640422821, 'total_duration': 34394.80511713028, 'accumulated_submission_time': 22349.18640422821, 'accumulated_eval_time': 12040.653289079666, 'accumulated_logging_time': 3.115537643432617}
I0311 12:47:59.007590 139622172112640 logging_writer.py:48] [71302] accumulated_eval_time=12040.653289, accumulated_logging_time=3.115538, accumulated_submission_time=22349.186404, global_step=71302, preemption_count=0, score=22349.186404, test/accuracy=0.986096, test/loss=0.053771, test/mean_average_precision=0.275448, test/num_examples=43793, total_duration=34394.805117, train/accuracy=0.995140, train/loss=0.015094, train/mean_average_precision=0.753908, validation/accuracy=0.986974, validation/loss=0.050205, validation/mean_average_precision=0.289680, validation/num_examples=43793
I0311 12:48:30.537305 139622180505344 logging_writer.py:48] [71400] global_step=71400, grad_norm=0.13119223713874817, loss=0.017146984115242958
I0311 12:49:02.241437 139622172112640 logging_writer.py:48] [71500] global_step=71500, grad_norm=0.1292615383863449, loss=0.017255302518606186
I0311 12:49:34.029342 139622180505344 logging_writer.py:48] [71600] global_step=71600, grad_norm=0.14031803607940674, loss=0.018291540443897247
I0311 12:50:05.254843 139622172112640 logging_writer.py:48] [71700] global_step=71700, grad_norm=0.13284777104854584, loss=0.01843985728919506
I0311 12:50:36.405171 139622180505344 logging_writer.py:48] [71800] global_step=71800, grad_norm=0.14553391933441162, loss=0.018466060981154442
I0311 12:51:07.375023 139622172112640 logging_writer.py:48] [71900] global_step=71900, grad_norm=0.13821941614151, loss=0.01881873793900013
I0311 12:51:38.359071 139622180505344 logging_writer.py:48] [72000] global_step=72000, grad_norm=0.14106649160385132, loss=0.015498788096010685
I0311 12:51:59.268138 139789500200768 spec.py:321] Evaluating on the training split.
I0311 12:53:57.355925 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 12:54:00.442087 139789500200768 spec.py:349] Evaluating on the test split.
I0311 12:54:03.479212 139789500200768 submission_runner.py:411] Time since start: 34759.31s, 	Step: 72069, 	{'train/accuracy': 0.9952371716499329, 'train/loss': 0.014763647690415382, 'train/mean_average_precision': 0.7646365713289918, 'validation/accuracy': 0.9869250059127808, 'validation/loss': 0.050302352756261826, 'validation/mean_average_precision': 0.28536721658847597, 'validation/num_examples': 43793, 'test/accuracy': 0.9861177802085876, 'test/loss': 0.053827133029699326, 'test/mean_average_precision': 0.27476908436635195, 'test/num_examples': 43793, 'score': 22589.416313409805, 'total_duration': 34759.30646586418, 'accumulated_submission_time': 22589.416313409805, 'accumulated_eval_time': 12164.864319086075, 'accumulated_logging_time': 3.1560232639312744}
I0311 12:54:03.508132 139621870597888 logging_writer.py:48] [72069] accumulated_eval_time=12164.864319, accumulated_logging_time=3.156023, accumulated_submission_time=22589.416313, global_step=72069, preemption_count=0, score=22589.416313, test/accuracy=0.986118, test/loss=0.053827, test/mean_average_precision=0.274769, test/num_examples=43793, total_duration=34759.306466, train/accuracy=0.995237, train/loss=0.014764, train/mean_average_precision=0.764637, validation/accuracy=0.986925, validation/loss=0.050302, validation/mean_average_precision=0.285367, validation/num_examples=43793
I0311 12:54:13.488907 139628670904064 logging_writer.py:48] [72100] global_step=72100, grad_norm=0.1554553508758545, loss=0.019983811303973198
I0311 12:54:44.660365 139621870597888 logging_writer.py:48] [72200] global_step=72200, grad_norm=0.13284310698509216, loss=0.01924431510269642
I0311 12:55:15.823691 139628670904064 logging_writer.py:48] [72300] global_step=72300, grad_norm=0.1363237202167511, loss=0.015722544863820076
I0311 12:55:47.157327 139621870597888 logging_writer.py:48] [72400] global_step=72400, grad_norm=0.1394759863615036, loss=0.019253600388765335
I0311 12:56:18.285619 139628670904064 logging_writer.py:48] [72500] global_step=72500, grad_norm=0.1433296799659729, loss=0.01714388281106949
I0311 12:56:49.242139 139621870597888 logging_writer.py:48] [72600] global_step=72600, grad_norm=0.14248785376548767, loss=0.018737206235527992
I0311 12:57:20.875942 139628670904064 logging_writer.py:48] [72700] global_step=72700, grad_norm=0.13337011635303497, loss=0.018213137984275818
I0311 12:57:52.012045 139621870597888 logging_writer.py:48] [72800] global_step=72800, grad_norm=0.1272306889295578, loss=0.016119128093123436
I0311 12:58:03.488204 139789500200768 spec.py:321] Evaluating on the training split.
I0311 12:59:58.866460 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 13:00:01.997021 139789500200768 spec.py:349] Evaluating on the test split.
I0311 13:00:04.983650 139789500200768 submission_runner.py:411] Time since start: 35120.81s, 	Step: 72838, 	{'train/accuracy': 0.9951958656311035, 'train/loss': 0.014718987978994846, 'train/mean_average_precision': 0.7631483641737393, 'validation/accuracy': 0.9870277047157288, 'validation/loss': 0.050460074096918106, 'validation/mean_average_precision': 0.28478578473269156, 'validation/num_examples': 43793, 'test/accuracy': 0.9861805438995361, 'test/loss': 0.05405411124229431, 'test/mean_average_precision': 0.27597924107438254, 'test/num_examples': 43793, 'score': 22829.366702079773, 'total_duration': 35120.810903549194, 'accumulated_submission_time': 22829.366702079773, 'accumulated_eval_time': 12286.359723567963, 'accumulated_logging_time': 3.1960184574127197}
I0311 13:00:05.013016 139622180505344 logging_writer.py:48] [72838] accumulated_eval_time=12286.359724, accumulated_logging_time=3.196018, accumulated_submission_time=22829.366702, global_step=72838, preemption_count=0, score=22829.366702, test/accuracy=0.986181, test/loss=0.054054, test/mean_average_precision=0.275979, test/num_examples=43793, total_duration=35120.810904, train/accuracy=0.995196, train/loss=0.014719, train/mean_average_precision=0.763148, validation/accuracy=0.987028, validation/loss=0.050460, validation/mean_average_precision=0.284786, validation/num_examples=43793
I0311 13:00:25.068539 139628679296768 logging_writer.py:48] [72900] global_step=72900, grad_norm=0.16428717970848083, loss=0.02091980166733265
I0311 13:00:56.450595 139622180505344 logging_writer.py:48] [73000] global_step=73000, grad_norm=0.13972817361354828, loss=0.019820228219032288
I0311 13:01:28.076646 139628679296768 logging_writer.py:48] [73100] global_step=73100, grad_norm=0.15015263855457306, loss=0.01830405741930008
I0311 13:01:59.230918 139622180505344 logging_writer.py:48] [73200] global_step=73200, grad_norm=0.1547928899526596, loss=0.01938553899526596
I0311 13:02:30.623848 139628679296768 logging_writer.py:48] [73300] global_step=73300, grad_norm=0.14421528577804565, loss=0.019138239324092865
I0311 13:03:02.285254 139622180505344 logging_writer.py:48] [73400] global_step=73400, grad_norm=0.14894072711467743, loss=0.017787355929613113
I0311 13:03:33.753983 139628679296768 logging_writer.py:48] [73500] global_step=73500, grad_norm=0.15233002603054047, loss=0.02223573438823223
I0311 13:04:05.298936 139622180505344 logging_writer.py:48] [73600] global_step=73600, grad_norm=0.13711461424827576, loss=0.017702249810099602
I0311 13:04:05.304007 139789500200768 spec.py:321] Evaluating on the training split.
I0311 13:06:00.822449 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 13:06:03.872233 139789500200768 spec.py:349] Evaluating on the test split.
I0311 13:06:06.892319 139789500200768 submission_runner.py:411] Time since start: 35482.72s, 	Step: 73601, 	{'train/accuracy': 0.9955556988716125, 'train/loss': 0.013952000066637993, 'train/mean_average_precision': 0.7762335065818747, 'validation/accuracy': 0.9869396090507507, 'validation/loss': 0.0503431037068367, 'validation/mean_average_precision': 0.2852072589426013, 'validation/num_examples': 43793, 'test/accuracy': 0.9861106276512146, 'test/loss': 0.05399731546640396, 'test/mean_average_precision': 0.27490015615353236, 'test/num_examples': 43793, 'score': 23069.627014398575, 'total_duration': 35482.71941781044, 'accumulated_submission_time': 23069.627014398575, 'accumulated_eval_time': 12407.947816610336, 'accumulated_logging_time': 3.2364907264709473}
I0311 13:06:06.921557 139621870597888 logging_writer.py:48] [73601] accumulated_eval_time=12407.947817, accumulated_logging_time=3.236491, accumulated_submission_time=23069.627014, global_step=73601, preemption_count=0, score=23069.627014, test/accuracy=0.986111, test/loss=0.053997, test/mean_average_precision=0.274900, test/num_examples=43793, total_duration=35482.719418, train/accuracy=0.995556, train/loss=0.013952, train/mean_average_precision=0.776234, validation/accuracy=0.986940, validation/loss=0.050343, validation/mean_average_precision=0.285207, validation/num_examples=43793
I0311 13:06:38.522496 139628670904064 logging_writer.py:48] [73700] global_step=73700, grad_norm=0.13919052481651306, loss=0.01753877103328705
I0311 13:07:10.141837 139621870597888 logging_writer.py:48] [73800] global_step=73800, grad_norm=0.16353973746299744, loss=0.01908526010811329
I0311 13:07:41.964741 139628670904064 logging_writer.py:48] [73900] global_step=73900, grad_norm=0.13947570323944092, loss=0.017440898343920708
I0311 13:08:13.386353 139621870597888 logging_writer.py:48] [74000] global_step=74000, grad_norm=0.13551880419254303, loss=0.017229538410902023
I0311 13:08:45.065750 139628670904064 logging_writer.py:48] [74100] global_step=74100, grad_norm=0.1457095593214035, loss=0.01931633986532688
I0311 13:09:16.893539 139621870597888 logging_writer.py:48] [74200] global_step=74200, grad_norm=0.14165686070919037, loss=0.0173286534845829
I0311 13:09:48.248442 139628670904064 logging_writer.py:48] [74300] global_step=74300, grad_norm=0.15660536289215088, loss=0.01774875819683075
I0311 13:10:07.130351 139789500200768 spec.py:321] Evaluating on the training split.
I0311 13:11:59.632793 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 13:12:02.699411 139789500200768 spec.py:349] Evaluating on the test split.
I0311 13:12:05.731554 139789500200768 submission_runner.py:411] Time since start: 35841.56s, 	Step: 74361, 	{'train/accuracy': 0.9955661296844482, 'train/loss': 0.013913517817854881, 'train/mean_average_precision': 0.7747334711189504, 'validation/accuracy': 0.9869696497917175, 'validation/loss': 0.05052773654460907, 'validation/mean_average_precision': 0.2868443094424433, 'validation/num_examples': 43793, 'test/accuracy': 0.9861372113227844, 'test/loss': 0.05415481701493263, 'test/mean_average_precision': 0.2747839274994807, 'test/num_examples': 43793, 'score': 23309.805548667908, 'total_duration': 35841.558666944504, 'accumulated_submission_time': 23309.805548667908, 'accumulated_eval_time': 12526.548835277557, 'accumulated_logging_time': 3.2765581607818604}
I0311 13:12:05.762866 139622172112640 logging_writer.py:48] [74361] accumulated_eval_time=12526.548835, accumulated_logging_time=3.276558, accumulated_submission_time=23309.805549, global_step=74361, preemption_count=0, score=23309.805549, test/accuracy=0.986137, test/loss=0.054155, test/mean_average_precision=0.274784, test/num_examples=43793, total_duration=35841.558667, train/accuracy=0.995566, train/loss=0.013914, train/mean_average_precision=0.774733, validation/accuracy=0.986970, validation/loss=0.050528, validation/mean_average_precision=0.286844, validation/num_examples=43793
I0311 13:12:18.735272 139622180505344 logging_writer.py:48] [74400] global_step=74400, grad_norm=0.13160762190818787, loss=0.017281457781791687
I0311 13:12:50.067090 139622172112640 logging_writer.py:48] [74500] global_step=74500, grad_norm=0.137088805437088, loss=0.0168735571205616
I0311 13:13:21.351652 139622180505344 logging_writer.py:48] [74600] global_step=74600, grad_norm=0.12797440588474274, loss=0.01622854545712471
I0311 13:13:52.816641 139622172112640 logging_writer.py:48] [74700] global_step=74700, grad_norm=0.1424657106399536, loss=0.017406832426786423
I0311 13:14:24.124955 139622180505344 logging_writer.py:48] [74800] global_step=74800, grad_norm=0.15309567749500275, loss=0.018422622233629227
I0311 13:14:55.369687 139622172112640 logging_writer.py:48] [74900] global_step=74900, grad_norm=0.13895532488822937, loss=0.01692168042063713
I0311 13:15:26.826042 139622180505344 logging_writer.py:48] [75000] global_step=75000, grad_norm=0.14840328693389893, loss=0.017759380862116814
I0311 13:15:58.206046 139622172112640 logging_writer.py:48] [75100] global_step=75100, grad_norm=0.14440971612930298, loss=0.01765919104218483
I0311 13:16:05.757271 139789500200768 spec.py:321] Evaluating on the training split.
I0311 13:18:00.423343 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 13:18:03.492591 139789500200768 spec.py:349] Evaluating on the test split.
I0311 13:18:06.552170 139789500200768 submission_runner.py:411] Time since start: 36202.38s, 	Step: 75125, 	{'train/accuracy': 0.9955736994743347, 'train/loss': 0.01394970528781414, 'train/mean_average_precision': 0.7811614897240826, 'validation/accuracy': 0.9869436621665955, 'validation/loss': 0.05055380240082741, 'validation/mean_average_precision': 0.2867632035727815, 'validation/num_examples': 43793, 'test/accuracy': 0.9861240983009338, 'test/loss': 0.05423346906900406, 'test/mean_average_precision': 0.2750307266695363, 'test/num_examples': 43793, 'score': 23549.76802420616, 'total_duration': 36202.37942099571, 'accumulated_submission_time': 23549.76802420616, 'accumulated_eval_time': 12647.343688488007, 'accumulated_logging_time': 3.3206896781921387}
I0311 13:18:06.582626 139621870597888 logging_writer.py:48] [75125] accumulated_eval_time=12647.343688, accumulated_logging_time=3.320690, accumulated_submission_time=23549.768024, global_step=75125, preemption_count=0, score=23549.768024, test/accuracy=0.986124, test/loss=0.054233, test/mean_average_precision=0.275031, test/num_examples=43793, total_duration=36202.379421, train/accuracy=0.995574, train/loss=0.013950, train/mean_average_precision=0.781161, validation/accuracy=0.986944, validation/loss=0.050554, validation/mean_average_precision=0.286763, validation/num_examples=43793
I0311 13:18:30.504134 139628679296768 logging_writer.py:48] [75200] global_step=75200, grad_norm=0.12534265220165253, loss=0.016836995258927345
I0311 13:19:02.107095 139621870597888 logging_writer.py:48] [75300] global_step=75300, grad_norm=0.14927180111408234, loss=0.017989249899983406
I0311 13:19:33.804559 139628679296768 logging_writer.py:48] [75400] global_step=75400, grad_norm=0.13409091532230377, loss=0.01656273379921913
I0311 13:20:05.311073 139621870597888 logging_writer.py:48] [75500] global_step=75500, grad_norm=0.15447981655597687, loss=0.017962124198675156
I0311 13:20:37.083526 139628679296768 logging_writer.py:48] [75600] global_step=75600, grad_norm=0.14594975113868713, loss=0.01803978905081749
I0311 13:21:08.640585 139621870597888 logging_writer.py:48] [75700] global_step=75700, grad_norm=0.15071630477905273, loss=0.017880180850625038
I0311 13:21:40.743044 139628679296768 logging_writer.py:48] [75800] global_step=75800, grad_norm=0.1473553329706192, loss=0.019859254360198975
I0311 13:22:06.629785 139789500200768 spec.py:321] Evaluating on the training split.
I0311 13:23:59.966727 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 13:24:03.018513 139789500200768 spec.py:349] Evaluating on the test split.
I0311 13:24:05.993315 139789500200768 submission_runner.py:411] Time since start: 36561.82s, 	Step: 75883, 	{'train/accuracy': 0.9955554008483887, 'train/loss': 0.01381759624928236, 'train/mean_average_precision': 0.7852192319353195, 'validation/accuracy': 0.9869920015335083, 'validation/loss': 0.05057290568947792, 'validation/mean_average_precision': 0.28785123642933713, 'validation/num_examples': 43793, 'test/accuracy': 0.9861254096031189, 'test/loss': 0.054279033094644547, 'test/mean_average_precision': 0.27503527275053985, 'test/num_examples': 43793, 'score': 23789.78480911255, 'total_duration': 36561.82056903839, 'accumulated_submission_time': 23789.78480911255, 'accumulated_eval_time': 12766.707190036774, 'accumulated_logging_time': 3.362133741378784}
I0311 13:24:06.024478 139622172112640 logging_writer.py:48] [75883] accumulated_eval_time=12766.707190, accumulated_logging_time=3.362134, accumulated_submission_time=23789.784809, global_step=75883, preemption_count=0, score=23789.784809, test/accuracy=0.986125, test/loss=0.054279, test/mean_average_precision=0.275035, test/num_examples=43793, total_duration=36561.820569, train/accuracy=0.995555, train/loss=0.013818, train/mean_average_precision=0.785219, validation/accuracy=0.986992, validation/loss=0.050573, validation/mean_average_precision=0.287851, validation/num_examples=43793
I0311 13:24:11.695692 139622180505344 logging_writer.py:48] [75900] global_step=75900, grad_norm=0.14779901504516602, loss=0.017151322215795517
I0311 13:24:42.899236 139622172112640 logging_writer.py:48] [76000] global_step=76000, grad_norm=0.14618799090385437, loss=0.017988163977861404
I0311 13:25:14.048311 139622180505344 logging_writer.py:48] [76100] global_step=76100, grad_norm=0.13525773584842682, loss=0.01636320725083351
I0311 13:25:45.109576 139622172112640 logging_writer.py:48] [76200] global_step=76200, grad_norm=0.16498342156410217, loss=0.017647799104452133
I0311 13:26:16.068027 139622180505344 logging_writer.py:48] [76300] global_step=76300, grad_norm=0.1538926213979721, loss=0.019278038293123245
I0311 13:26:47.106235 139622172112640 logging_writer.py:48] [76400] global_step=76400, grad_norm=0.14645205438137054, loss=0.018072903156280518
I0311 13:27:18.255563 139622180505344 logging_writer.py:48] [76500] global_step=76500, grad_norm=0.15151503682136536, loss=0.019931897521018982
I0311 13:27:49.414526 139622172112640 logging_writer.py:48] [76600] global_step=76600, grad_norm=0.13444234430789948, loss=0.016007984057068825
I0311 13:28:06.049958 139789500200768 spec.py:321] Evaluating on the training split.
I0311 13:30:01.828618 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 13:30:04.933126 139789500200768 spec.py:349] Evaluating on the test split.
I0311 13:30:07.937605 139789500200768 submission_runner.py:411] Time since start: 36923.76s, 	Step: 76655, 	{'train/accuracy': 0.9955092668533325, 'train/loss': 0.014035946689546108, 'train/mean_average_precision': 0.7789031097164165, 'validation/accuracy': 0.98692786693573, 'validation/loss': 0.050566427409648895, 'validation/mean_average_precision': 0.28578041503215484, 'validation/num_examples': 43793, 'test/accuracy': 0.9861195087432861, 'test/loss': 0.05423501133918762, 'test/mean_average_precision': 0.2757211049940482, 'test/num_examples': 43793, 'score': 24029.778205871582, 'total_duration': 36923.764858961105, 'accumulated_submission_time': 24029.778205871582, 'accumulated_eval_time': 12888.594792842865, 'accumulated_logging_time': 3.4054219722747803}
I0311 13:30:07.967208 139621870597888 logging_writer.py:48] [76655] accumulated_eval_time=12888.594793, accumulated_logging_time=3.405422, accumulated_submission_time=24029.778206, global_step=76655, preemption_count=0, score=24029.778206, test/accuracy=0.986120, test/loss=0.054235, test/mean_average_precision=0.275721, test/num_examples=43793, total_duration=36923.764859, train/accuracy=0.995509, train/loss=0.014036, train/mean_average_precision=0.778903, validation/accuracy=0.986928, validation/loss=0.050566, validation/mean_average_precision=0.285780, validation/num_examples=43793
I0311 13:30:22.342054 139628670904064 logging_writer.py:48] [76700] global_step=76700, grad_norm=0.16421709954738617, loss=0.019197875633835793
I0311 13:30:53.654947 139621870597888 logging_writer.py:48] [76800] global_step=76800, grad_norm=0.14005599915981293, loss=0.01874164305627346
I0311 13:31:25.095500 139628670904064 logging_writer.py:48] [76900] global_step=76900, grad_norm=0.14719101786613464, loss=0.018650205805897713
I0311 13:31:56.390233 139621870597888 logging_writer.py:48] [77000] global_step=77000, grad_norm=0.15455305576324463, loss=0.02008047327399254
I0311 13:32:27.655450 139628670904064 logging_writer.py:48] [77100] global_step=77100, grad_norm=0.14791347086429596, loss=0.018916254863142967
I0311 13:32:58.844222 139621870597888 logging_writer.py:48] [77200] global_step=77200, grad_norm=0.17024260759353638, loss=0.018978042528033257
I0311 13:33:29.747321 139628670904064 logging_writer.py:48] [77300] global_step=77300, grad_norm=0.13921524584293365, loss=0.016596248373389244
I0311 13:34:00.756614 139621870597888 logging_writer.py:48] [77400] global_step=77400, grad_norm=0.1312447041273117, loss=0.016609206795692444
I0311 13:34:07.957609 139789500200768 spec.py:321] Evaluating on the training split.
I0311 13:36:02.299317 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 13:36:05.301612 139789500200768 spec.py:349] Evaluating on the test split.
I0311 13:36:08.263239 139789500200768 submission_runner.py:411] Time since start: 37284.09s, 	Step: 77424, 	{'train/accuracy': 0.9954677224159241, 'train/loss': 0.014050626195967197, 'train/mean_average_precision': 0.7791727016973256, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050620876252651215, 'validation/mean_average_precision': 0.28584319074274717, 'validation/num_examples': 43793, 'test/accuracy': 0.9861531853675842, 'test/loss': 0.054288722574710846, 'test/mean_average_precision': 0.2760953723677083, 'test/num_examples': 43793, 'score': 24269.738109350204, 'total_duration': 37284.09049272537, 'accumulated_submission_time': 24269.738109350204, 'accumulated_eval_time': 13008.900380373001, 'accumulated_logging_time': 3.4460644721984863}
I0311 13:36:08.292626 139622180505344 logging_writer.py:48] [77424] accumulated_eval_time=13008.900380, accumulated_logging_time=3.446064, accumulated_submission_time=24269.738109, global_step=77424, preemption_count=0, score=24269.738109, test/accuracy=0.986153, test/loss=0.054289, test/mean_average_precision=0.276095, test/num_examples=43793, total_duration=37284.090493, train/accuracy=0.995468, train/loss=0.014051, train/mean_average_precision=0.779173, validation/accuracy=0.986956, validation/loss=0.050621, validation/mean_average_precision=0.285843, validation/num_examples=43793
I0311 13:36:32.520797 139628679296768 logging_writer.py:48] [77500] global_step=77500, grad_norm=0.1302979588508606, loss=0.017077704891562462
I0311 13:37:03.863764 139622180505344 logging_writer.py:48] [77600] global_step=77600, grad_norm=0.1505095660686493, loss=0.01867210678756237
I0311 13:37:35.099793 139628679296768 logging_writer.py:48] [77700] global_step=77700, grad_norm=0.15695509314537048, loss=0.01927368901669979
I0311 13:38:06.699768 139622180505344 logging_writer.py:48] [77800] global_step=77800, grad_norm=0.14729870855808258, loss=0.018033891916275024
I0311 13:38:37.972163 139628679296768 logging_writer.py:48] [77900] global_step=77900, grad_norm=0.14737637341022491, loss=0.017073554918169975
I0311 13:39:09.312720 139622180505344 logging_writer.py:48] [78000] global_step=78000, grad_norm=0.14780236780643463, loss=0.019722403958439827
I0311 13:39:40.601369 139628679296768 logging_writer.py:48] [78100] global_step=78100, grad_norm=0.16656962037086487, loss=0.01691766083240509
I0311 13:40:08.532042 139789500200768 spec.py:321] Evaluating on the training split.
I0311 13:42:00.319611 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 13:42:03.332212 139789500200768 spec.py:349] Evaluating on the test split.
I0311 13:42:07.734698 139789500200768 submission_runner.py:411] Time since start: 37643.56s, 	Step: 78189, 	{'train/accuracy': 0.9954847693443298, 'train/loss': 0.014059551991522312, 'train/mean_average_precision': 0.7697898776753651, 'validation/accuracy': 0.9869655966758728, 'validation/loss': 0.05060245096683502, 'validation/mean_average_precision': 0.2866528137796882, 'validation/num_examples': 43793, 'test/accuracy': 0.9861258268356323, 'test/loss': 0.05427347868680954, 'test/mean_average_precision': 0.27556272047751773, 'test/num_examples': 43793, 'score': 24509.947502613068, 'total_duration': 37643.561952352524, 'accumulated_submission_time': 24509.947502613068, 'accumulated_eval_time': 13128.102998018265, 'accumulated_logging_time': 3.4863169193267822}
I0311 13:42:07.766161 139622172112640 logging_writer.py:48] [78189] accumulated_eval_time=13128.102998, accumulated_logging_time=3.486317, accumulated_submission_time=24509.947503, global_step=78189, preemption_count=0, score=24509.947503, test/accuracy=0.986126, test/loss=0.054273, test/mean_average_precision=0.275563, test/num_examples=43793, total_duration=37643.561952, train/accuracy=0.995485, train/loss=0.014060, train/mean_average_precision=0.769790, validation/accuracy=0.986966, validation/loss=0.050602, validation/mean_average_precision=0.286653, validation/num_examples=43793
I0311 13:42:11.921328 139628670904064 logging_writer.py:48] [78200] global_step=78200, grad_norm=0.13830149173736572, loss=0.018125144764780998
I0311 13:42:43.313980 139622172112640 logging_writer.py:48] [78300] global_step=78300, grad_norm=0.13533014059066772, loss=0.016218138858675957
I0311 13:43:14.566469 139628670904064 logging_writer.py:48] [78400] global_step=78400, grad_norm=0.14842617511749268, loss=0.01875208504498005
I0311 13:43:45.965650 139622172112640 logging_writer.py:48] [78500] global_step=78500, grad_norm=0.17096048593521118, loss=0.022580206394195557
I0311 13:44:17.497371 139628670904064 logging_writer.py:48] [78600] global_step=78600, grad_norm=0.14041079580783844, loss=0.016589367762207985
I0311 13:44:48.843683 139622172112640 logging_writer.py:48] [78700] global_step=78700, grad_norm=0.14099398255348206, loss=0.017188090831041336
I0311 13:45:20.363618 139628670904064 logging_writer.py:48] [78800] global_step=78800, grad_norm=0.1464960128068924, loss=0.019311485812067986
I0311 13:45:51.987556 139622172112640 logging_writer.py:48] [78900] global_step=78900, grad_norm=0.14309187233448029, loss=0.016645651310682297
I0311 13:46:07.801798 139789500200768 spec.py:321] Evaluating on the training split.
I0311 13:48:01.681888 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 13:48:04.740814 139789500200768 spec.py:349] Evaluating on the test split.
I0311 13:48:07.780448 139789500200768 submission_runner.py:411] Time since start: 38003.61s, 	Step: 78951, 	{'train/accuracy': 0.9955748915672302, 'train/loss': 0.013974305242300034, 'train/mean_average_precision': 0.7815056272749845, 'validation/accuracy': 0.9869611263275146, 'validation/loss': 0.050583042204380035, 'validation/mean_average_precision': 0.2863825448521734, 'validation/num_examples': 43793, 'test/accuracy': 0.986123263835907, 'test/loss': 0.05425333231687546, 'test/mean_average_precision': 0.27618787776337544, 'test/num_examples': 43793, 'score': 24749.636902093887, 'total_duration': 38003.607684612274, 'accumulated_submission_time': 24749.636902093887, 'accumulated_eval_time': 13248.081588506699, 'accumulated_logging_time': 3.8449010848999023}
I0311 13:48:07.810550 139621870597888 logging_writer.py:48] [78951] accumulated_eval_time=13248.081589, accumulated_logging_time=3.844901, accumulated_submission_time=24749.636902, global_step=78951, preemption_count=0, score=24749.636902, test/accuracy=0.986123, test/loss=0.054253, test/mean_average_precision=0.276188, test/num_examples=43793, total_duration=38003.607685, train/accuracy=0.995575, train/loss=0.013974, train/mean_average_precision=0.781506, validation/accuracy=0.986961, validation/loss=0.050583, validation/mean_average_precision=0.286383, validation/num_examples=43793
I0311 13:48:23.397095 139628679296768 logging_writer.py:48] [79000] global_step=79000, grad_norm=0.14285944402217865, loss=0.0178296510130167
I0311 13:48:54.426680 139621870597888 logging_writer.py:48] [79100] global_step=79100, grad_norm=0.14894366264343262, loss=0.017639927566051483
I0311 13:49:25.540395 139628679296768 logging_writer.py:48] [79200] global_step=79200, grad_norm=0.17877739667892456, loss=0.02050083689391613
I0311 13:49:56.769091 139621870597888 logging_writer.py:48] [79300] global_step=79300, grad_norm=0.14961928129196167, loss=0.017083028331398964
I0311 13:50:28.090674 139628679296768 logging_writer.py:48] [79400] global_step=79400, grad_norm=0.1464155614376068, loss=0.019129762426018715
I0311 13:50:59.276858 139621870597888 logging_writer.py:48] [79500] global_step=79500, grad_norm=0.17183837294578552, loss=0.01832372322678566
I0311 13:51:30.783025 139628679296768 logging_writer.py:48] [79600] global_step=79600, grad_norm=0.14532038569450378, loss=0.016975048929452896
I0311 13:52:02.104075 139621870597888 logging_writer.py:48] [79700] global_step=79700, grad_norm=0.14919359982013702, loss=0.018381550908088684
I0311 13:52:07.852342 139789500200768 spec.py:321] Evaluating on the training split.
I0311 13:54:04.968315 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 13:54:07.982964 139789500200768 spec.py:349] Evaluating on the test split.
I0311 13:54:10.945197 139789500200768 submission_runner.py:411] Time since start: 38366.77s, 	Step: 79719, 	{'train/accuracy': 0.9955337047576904, 'train/loss': 0.01384957879781723, 'train/mean_average_precision': 0.7799917830671548, 'validation/accuracy': 0.9869562983512878, 'validation/loss': 0.05058920010924339, 'validation/mean_average_precision': 0.2869632430767811, 'validation/num_examples': 43793, 'test/accuracy': 0.9861325621604919, 'test/loss': 0.05426281690597534, 'test/mean_average_precision': 0.27601922213642033, 'test/num_examples': 43793, 'score': 24989.647536993027, 'total_duration': 38366.772450208664, 'accumulated_submission_time': 24989.647536993027, 'accumulated_eval_time': 13371.174401044846, 'accumulated_logging_time': 3.8868765830993652}
I0311 13:54:10.975047 139622180505344 logging_writer.py:48] [79719] accumulated_eval_time=13371.174401, accumulated_logging_time=3.886877, accumulated_submission_time=24989.647537, global_step=79719, preemption_count=0, score=24989.647537, test/accuracy=0.986133, test/loss=0.054263, test/mean_average_precision=0.276019, test/num_examples=43793, total_duration=38366.772450, train/accuracy=0.995534, train/loss=0.013850, train/mean_average_precision=0.779992, validation/accuracy=0.986956, validation/loss=0.050589, validation/mean_average_precision=0.286963, validation/num_examples=43793
I0311 13:54:36.478358 139628670904064 logging_writer.py:48] [79800] global_step=79800, grad_norm=0.1488317847251892, loss=0.01743050292134285
I0311 13:55:07.778096 139622180505344 logging_writer.py:48] [79900] global_step=79900, grad_norm=0.14190612733364105, loss=0.01748543046414852
I0311 13:55:39.232675 139628670904064 logging_writer.py:48] [80000] global_step=80000, grad_norm=0.1638948768377304, loss=0.019672663882374763
I0311 13:56:10.411357 139622180505344 logging_writer.py:48] [80100] global_step=80100, grad_norm=0.13132105767726898, loss=0.017847253009676933
I0311 13:56:41.965641 139628670904064 logging_writer.py:48] [80200] global_step=80200, grad_norm=0.16156642138957977, loss=0.020123150199651718
I0311 13:57:13.123671 139622180505344 logging_writer.py:48] [80300] global_step=80300, grad_norm=0.1445094347000122, loss=0.01808004453778267
I0311 13:57:44.439723 139628670904064 logging_writer.py:48] [80400] global_step=80400, grad_norm=0.1445695161819458, loss=0.017344923689961433
I0311 13:58:11.107095 139789500200768 spec.py:321] Evaluating on the training split.
I0311 14:00:00.366230 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 14:00:03.442393 139789500200768 spec.py:349] Evaluating on the test split.
I0311 14:00:06.436972 139789500200768 submission_runner.py:411] Time since start: 38722.26s, 	Step: 80486, 	{'train/accuracy': 0.9955554008483887, 'train/loss': 0.013935456052422523, 'train/mean_average_precision': 0.7850682347056999, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.28651567181688725, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.27600045847683485, 'test/num_examples': 43793, 'score': 25229.74752855301, 'total_duration': 38722.26422739029, 'accumulated_submission_time': 25229.74752855301, 'accumulated_eval_time': 13486.504242897034, 'accumulated_logging_time': 3.929654359817505}
I0311 14:00:06.467033 139621870597888 logging_writer.py:48] [80486] accumulated_eval_time=13486.504243, accumulated_logging_time=3.929654, accumulated_submission_time=25229.747529, global_step=80486, preemption_count=0, score=25229.747529, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.276000, test/num_examples=43793, total_duration=38722.264227, train/accuracy=0.995555, train/loss=0.013935, train/mean_average_precision=0.785068, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286516, validation/num_examples=43793
I0311 14:00:11.153599 139628679296768 logging_writer.py:48] [80500] global_step=80500, grad_norm=0.1469058096408844, loss=0.017364464700222015
I0311 14:00:42.906032 139621870597888 logging_writer.py:48] [80600] global_step=80600, grad_norm=0.1434023231267929, loss=0.017203355208039284
I0311 14:01:14.767180 139628679296768 logging_writer.py:48] [80700] global_step=80700, grad_norm=0.14100702106952667, loss=0.016993682831525803
I0311 14:01:46.106295 139621870597888 logging_writer.py:48] [80800] global_step=80800, grad_norm=0.14224191009998322, loss=0.017393523827195168
I0311 14:02:18.353439 139628679296768 logging_writer.py:48] [80900] global_step=80900, grad_norm=0.14008758962154388, loss=0.015803184360265732
I0311 14:02:51.332233 139621870597888 logging_writer.py:48] [81000] global_step=81000, grad_norm=0.12840938568115234, loss=0.016760947182774544
I0311 14:03:24.118110 139628679296768 logging_writer.py:48] [81100] global_step=81100, grad_norm=0.13794928789138794, loss=0.01723637804389
I0311 14:03:56.516265 139621870597888 logging_writer.py:48] [81200] global_step=81200, grad_norm=0.1271679699420929, loss=0.016560383141040802
I0311 14:04:06.620497 139789500200768 spec.py:321] Evaluating on the training split.
I0311 14:06:14.646020 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 14:06:18.134605 139789500200768 spec.py:349] Evaluating on the test split.
I0311 14:06:21.515296 139789500200768 submission_runner.py:411] Time since start: 39097.34s, 	Step: 81232, 	{'train/accuracy': 0.9955120086669922, 'train/loss': 0.013953101821243763, 'train/mean_average_precision': 0.776054432927252, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.2865305392511518, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.27597567677572693, 'test/num_examples': 43793, 'score': 25469.8677110672, 'total_duration': 39097.34252977371, 'accumulated_submission_time': 25469.8677110672, 'accumulated_eval_time': 13621.399010181427, 'accumulated_logging_time': 3.971792221069336}
I0311 14:06:21.550699 139622180505344 logging_writer.py:48] [81232] accumulated_eval_time=13621.399010, accumulated_logging_time=3.971792, accumulated_submission_time=25469.867711, global_step=81232, preemption_count=0, score=25469.867711, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.275976, test/num_examples=43793, total_duration=39097.342530, train/accuracy=0.995512, train/loss=0.013953, train/mean_average_precision=0.776054, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286531, validation/num_examples=43793
I0311 14:06:44.060444 139628670904064 logging_writer.py:48] [81300] global_step=81300, grad_norm=0.130514457821846, loss=0.017115240916609764
I0311 14:07:16.949574 139622180505344 logging_writer.py:48] [81400] global_step=81400, grad_norm=0.15879477560520172, loss=0.015845224261283875
I0311 14:07:49.706316 139628670904064 logging_writer.py:48] [81500] global_step=81500, grad_norm=0.14660575985908508, loss=0.01596878655254841
I0311 14:08:22.635744 139622180505344 logging_writer.py:48] [81600] global_step=81600, grad_norm=0.12736444175243378, loss=0.01571282371878624
I0311 14:08:55.096842 139628670904064 logging_writer.py:48] [81700] global_step=81700, grad_norm=0.14686168730258942, loss=0.018404392525553703
I0311 14:09:27.630545 139622180505344 logging_writer.py:48] [81800] global_step=81800, grad_norm=0.139408141374588, loss=0.014296951703727245
I0311 14:09:59.844187 139628670904064 logging_writer.py:48] [81900] global_step=81900, grad_norm=0.13966867327690125, loss=0.017957717180252075
I0311 14:10:21.796639 139789500200768 spec.py:321] Evaluating on the training split.
I0311 14:12:24.087551 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 14:12:27.544232 139789500200768 spec.py:349] Evaluating on the test split.
I0311 14:12:30.905940 139789500200768 submission_runner.py:411] Time since start: 39466.73s, 	Step: 81968, 	{'train/accuracy': 0.995551586151123, 'train/loss': 0.013901996426284313, 'train/mean_average_precision': 0.7857464322595502, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.28650710633603355, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426059290766716, 'test/mean_average_precision': 0.2760720686538725, 'test/num_examples': 43793, 'score': 25710.078585147858, 'total_duration': 39466.73317718506, 'accumulated_submission_time': 25710.078585147858, 'accumulated_eval_time': 13750.508263587952, 'accumulated_logging_time': 4.0181121826171875}
I0311 14:12:30.940751 139621870597888 logging_writer.py:48] [81968] accumulated_eval_time=13750.508264, accumulated_logging_time=4.018112, accumulated_submission_time=25710.078585, global_step=81968, preemption_count=0, score=25710.078585, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.276072, test/num_examples=43793, total_duration=39466.733177, train/accuracy=0.995552, train/loss=0.013902, train/mean_average_precision=0.785746, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286507, validation/num_examples=43793
I0311 14:12:41.755678 139622172112640 logging_writer.py:48] [82000] global_step=82000, grad_norm=0.13611160218715668, loss=0.015167874284088612
I0311 14:13:14.224206 139621870597888 logging_writer.py:48] [82100] global_step=82100, grad_norm=0.15470686554908752, loss=0.017756076529622078
I0311 14:13:46.738306 139622172112640 logging_writer.py:48] [82200] global_step=82200, grad_norm=0.1421719640493393, loss=0.017741968855261803
I0311 14:14:19.388673 139621870597888 logging_writer.py:48] [82300] global_step=82300, grad_norm=0.13700421154499054, loss=0.01659516803920269
I0311 14:14:51.784832 139622172112640 logging_writer.py:48] [82400] global_step=82400, grad_norm=0.15188483893871307, loss=0.018121197819709778
I0311 14:15:24.192739 139621870597888 logging_writer.py:48] [82500] global_step=82500, grad_norm=0.1574091911315918, loss=0.017904218286275864
I0311 14:15:56.446837 139622172112640 logging_writer.py:48] [82600] global_step=82600, grad_norm=0.14856743812561035, loss=0.01920136623084545
I0311 14:16:29.159100 139621870597888 logging_writer.py:48] [82700] global_step=82700, grad_norm=0.15035347640514374, loss=0.01865415833890438
I0311 14:16:31.069250 139789500200768 spec.py:321] Evaluating on the training split.
I0311 14:18:29.521591 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 14:18:32.994835 139789500200768 spec.py:349] Evaluating on the test split.
I0311 14:18:36.442785 139789500200768 submission_runner.py:411] Time since start: 39832.27s, 	Step: 82707, 	{'train/accuracy': 0.9955469369888306, 'train/loss': 0.013970274478197098, 'train/mean_average_precision': 0.7702644447155856, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.2866412038804638, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.27594910047859617, 'test/num_examples': 43793, 'score': 25950.171499967575, 'total_duration': 39832.270023822784, 'accumulated_submission_time': 25950.171499967575, 'accumulated_eval_time': 13875.881750106812, 'accumulated_logging_time': 4.0646750926971436}
I0311 14:18:36.479467 139628670904064 logging_writer.py:48] [82707] accumulated_eval_time=13875.881750, accumulated_logging_time=4.064675, accumulated_submission_time=25950.171500, global_step=82707, preemption_count=0, score=25950.171500, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.275949, test/num_examples=43793, total_duration=39832.270024, train/accuracy=0.995547, train/loss=0.013970, train/mean_average_precision=0.770264, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286641, validation/num_examples=43793
I0311 14:19:07.371733 139628679296768 logging_writer.py:48] [82800] global_step=82800, grad_norm=0.14679618179798126, loss=0.019366571679711342
I0311 14:19:40.126017 139628670904064 logging_writer.py:48] [82900] global_step=82900, grad_norm=0.13650444149971008, loss=0.017535056918859482
I0311 14:20:13.003588 139628679296768 logging_writer.py:48] [83000] global_step=83000, grad_norm=0.1574336290359497, loss=0.01848343200981617
I0311 14:20:46.023884 139628670904064 logging_writer.py:48] [83100] global_step=83100, grad_norm=0.14016136527061462, loss=0.017976926639676094
I0311 14:21:19.282678 139628679296768 logging_writer.py:48] [83200] global_step=83200, grad_norm=0.16145533323287964, loss=0.018152812495827675
I0311 14:21:52.388174 139628670904064 logging_writer.py:48] [83300] global_step=83300, grad_norm=0.15702566504478455, loss=0.01736241951584816
I0311 14:22:25.536763 139628679296768 logging_writer.py:48] [83400] global_step=83400, grad_norm=0.1404390037059784, loss=0.0181171465665102
I0311 14:22:36.733878 139789500200768 spec.py:321] Evaluating on the training split.
I0311 14:24:42.139626 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 14:24:45.658265 139789500200768 spec.py:349] Evaluating on the test split.
I0311 14:24:49.122051 139789500200768 submission_runner.py:411] Time since start: 40204.95s, 	Step: 83435, 	{'train/accuracy': 0.9955602884292603, 'train/loss': 0.013871967792510986, 'train/mean_average_precision': 0.7792143024381376, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.2867899905995227, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.2759817029818188, 'test/num_examples': 43793, 'score': 26190.390765428543, 'total_duration': 40204.94928979874, 'accumulated_submission_time': 26190.390765428543, 'accumulated_eval_time': 14008.269878864288, 'accumulated_logging_time': 4.11327862739563}
I0311 14:24:49.157403 139621870597888 logging_writer.py:48] [83435] accumulated_eval_time=14008.269879, accumulated_logging_time=4.113279, accumulated_submission_time=26190.390765, global_step=83435, preemption_count=0, score=26190.390765, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.275982, test/num_examples=43793, total_duration=40204.949290, train/accuracy=0.995560, train/loss=0.013872, train/mean_average_precision=0.779214, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286790, validation/num_examples=43793
I0311 14:25:10.963216 139622172112640 logging_writer.py:48] [83500] global_step=83500, grad_norm=0.14551298320293427, loss=0.017690587788820267
I0311 14:25:43.642831 139621870597888 logging_writer.py:48] [83600] global_step=83600, grad_norm=0.16461040079593658, loss=0.020671728998422623
I0311 14:26:15.699659 139622172112640 logging_writer.py:48] [83700] global_step=83700, grad_norm=0.1665058135986328, loss=0.01990317925810814
I0311 14:26:47.377127 139621870597888 logging_writer.py:48] [83800] global_step=83800, grad_norm=0.14336267113685608, loss=0.017151866108179092
I0311 14:27:19.066949 139622172112640 logging_writer.py:48] [83900] global_step=83900, grad_norm=0.14548221230506897, loss=0.01798512227833271
I0311 14:27:51.021101 139621870597888 logging_writer.py:48] [84000] global_step=84000, grad_norm=0.1489383429288864, loss=0.01804688572883606
I0311 14:28:23.088721 139622172112640 logging_writer.py:48] [84100] global_step=84100, grad_norm=0.13519282639026642, loss=0.01799250952899456
I0311 14:28:49.275612 139789500200768 spec.py:321] Evaluating on the training split.
I0311 14:30:41.843056 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 14:30:44.979978 139789500200768 spec.py:349] Evaluating on the test split.
I0311 14:30:47.990923 139789500200768 submission_runner.py:411] Time since start: 40563.82s, 	Step: 84184, 	{'train/accuracy': 0.9955973625183105, 'train/loss': 0.013776498846709728, 'train/mean_average_precision': 0.783298819531164, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.286624042646645, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.2759793362653783, 'test/num_examples': 43793, 'score': 26430.476687908173, 'total_duration': 40563.81817674637, 'accumulated_submission_time': 26430.476687908173, 'accumulated_eval_time': 14126.985151052475, 'accumulated_logging_time': 4.160360813140869}
I0311 14:30:48.022252 139622180505344 logging_writer.py:48] [84184] accumulated_eval_time=14126.985151, accumulated_logging_time=4.160361, accumulated_submission_time=26430.476688, global_step=84184, preemption_count=0, score=26430.476688, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.275979, test/num_examples=43793, total_duration=40563.818177, train/accuracy=0.995597, train/loss=0.013776, train/mean_average_precision=0.783299, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286624, validation/num_examples=43793
I0311 14:30:53.395367 139628670904064 logging_writer.py:48] [84200] global_step=84200, grad_norm=0.15000708401203156, loss=0.018473254516720772
I0311 14:31:25.094168 139622180505344 logging_writer.py:48] [84300] global_step=84300, grad_norm=0.13673007488250732, loss=0.018533624708652496
I0311 14:31:56.931224 139628670904064 logging_writer.py:48] [84400] global_step=84400, grad_norm=0.1377221643924713, loss=0.01878138817846775
I0311 14:32:28.733794 139622180505344 logging_writer.py:48] [84500] global_step=84500, grad_norm=0.14962105453014374, loss=0.01762937754392624
I0311 14:33:00.200361 139628670904064 logging_writer.py:48] [84600] global_step=84600, grad_norm=0.12989351153373718, loss=0.017397722229361534
I0311 14:33:32.385345 139622180505344 logging_writer.py:48] [84700] global_step=84700, grad_norm=0.14960643649101257, loss=0.017292393371462822
I0311 14:34:04.175262 139628670904064 logging_writer.py:48] [84800] global_step=84800, grad_norm=0.13618329167366028, loss=0.01677532307803631
I0311 14:34:35.963514 139622180505344 logging_writer.py:48] [84900] global_step=84900, grad_norm=0.12573528289794922, loss=0.016140567138791084
I0311 14:34:48.145139 139789500200768 spec.py:321] Evaluating on the training split.
I0311 14:36:41.238370 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 14:36:44.708622 139789500200768 spec.py:349] Evaluating on the test split.
I0311 14:36:48.078835 139789500200768 submission_runner.py:411] Time since start: 40923.91s, 	Step: 84940, 	{'train/accuracy': 0.9954774379730225, 'train/loss': 0.014080786146223545, 'train/mean_average_precision': 0.7756743396009409, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.2864305899233596, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.2759710816592701, 'test/num_examples': 43793, 'score': 26670.56862926483, 'total_duration': 40923.905933618546, 'accumulated_submission_time': 26670.56862926483, 'accumulated_eval_time': 14246.918652057648, 'accumulated_logging_time': 4.203290939331055}
I0311 14:36:48.115529 139621870597888 logging_writer.py:48] [84940] accumulated_eval_time=14246.918652, accumulated_logging_time=4.203291, accumulated_submission_time=26670.568629, global_step=84940, preemption_count=0, score=26670.568629, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.275971, test/num_examples=43793, total_duration=40923.905934, train/accuracy=0.995477, train/loss=0.014081, train/mean_average_precision=0.775674, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286431, validation/num_examples=43793
I0311 14:37:08.292315 139622172112640 logging_writer.py:48] [85000] global_step=85000, grad_norm=0.12884770333766937, loss=0.016311289742588997
I0311 14:37:40.946645 139621870597888 logging_writer.py:48] [85100] global_step=85100, grad_norm=0.13367004692554474, loss=0.01680820807814598
I0311 14:38:13.475254 139622172112640 logging_writer.py:48] [85200] global_step=85200, grad_norm=0.12975718080997467, loss=0.015846002846956253
I0311 14:38:45.868581 139621870597888 logging_writer.py:48] [85300] global_step=85300, grad_norm=0.14676955342292786, loss=0.017738036811351776
I0311 14:39:17.110748 139622172112640 logging_writer.py:48] [85400] global_step=85400, grad_norm=0.14574581384658813, loss=0.017854610458016396
I0311 14:39:48.386663 139621870597888 logging_writer.py:48] [85500] global_step=85500, grad_norm=0.12912015616893768, loss=0.01676473394036293
I0311 14:40:19.596537 139622172112640 logging_writer.py:48] [85600] global_step=85600, grad_norm=0.15523572266101837, loss=0.0167503971606493
I0311 14:40:48.219226 139789500200768 spec.py:321] Evaluating on the training split.
I0311 14:42:42.217399 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 14:42:45.280865 139789500200768 spec.py:349] Evaluating on the test split.
I0311 14:42:48.283891 139789500200768 submission_runner.py:411] Time since start: 41284.11s, 	Step: 85693, 	{'train/accuracy': 0.9955883622169495, 'train/loss': 0.013791185803711414, 'train/mean_average_precision': 0.7841693580470048, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.2865949001344348, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.2759714642884614, 'test/num_examples': 43793, 'score': 26910.639630556107, 'total_duration': 41284.111137628555, 'accumulated_submission_time': 26910.639630556107, 'accumulated_eval_time': 14366.983271598816, 'accumulated_logging_time': 4.25146746635437}
I0311 14:42:48.314897 139628670904064 logging_writer.py:48] [85693] accumulated_eval_time=14366.983272, accumulated_logging_time=4.251467, accumulated_submission_time=26910.639631, global_step=85693, preemption_count=0, score=26910.639631, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.275971, test/num_examples=43793, total_duration=41284.111138, train/accuracy=0.995588, train/loss=0.013791, train/mean_average_precision=0.784169, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286595, validation/num_examples=43793
I0311 14:42:50.935217 139628679296768 logging_writer.py:48] [85700] global_step=85700, grad_norm=0.1508919596672058, loss=0.017393525689840317
I0311 14:43:22.460837 139628670904064 logging_writer.py:48] [85800] global_step=85800, grad_norm=0.13812372088432312, loss=0.01675950363278389
I0311 14:43:54.015918 139628679296768 logging_writer.py:48] [85900] global_step=85900, grad_norm=0.14890964329242706, loss=0.018211349844932556
I0311 14:44:25.668090 139628670904064 logging_writer.py:48] [86000] global_step=86000, grad_norm=0.14821425080299377, loss=0.017632436007261276
I0311 14:44:56.984147 139628679296768 logging_writer.py:48] [86100] global_step=86100, grad_norm=0.13754872977733612, loss=0.016004545614123344
I0311 14:45:28.392947 139628670904064 logging_writer.py:48] [86200] global_step=86200, grad_norm=0.14642184972763062, loss=0.017981186509132385
I0311 14:46:02.353881 139628679296768 logging_writer.py:48] [86300] global_step=86300, grad_norm=0.13805918395519257, loss=0.01818096451461315
I0311 14:46:33.720824 139628670904064 logging_writer.py:48] [86400] global_step=86400, grad_norm=0.15938624739646912, loss=0.01899651624262333
I0311 14:46:48.527220 139789500200768 spec.py:321] Evaluating on the training split.
I0311 14:48:43.700149 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 14:48:46.789605 139789500200768 spec.py:349] Evaluating on the test split.
I0311 14:48:49.820547 139789500200768 submission_runner.py:411] Time since start: 41645.65s, 	Step: 86448, 	{'train/accuracy': 0.9955260157585144, 'train/loss': 0.01402968168258667, 'train/mean_average_precision': 0.7727005352207057, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.28667042010570265, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.2759215759192164, 'test/num_examples': 43793, 'score': 27150.82177400589, 'total_duration': 41645.647800922394, 'accumulated_submission_time': 27150.82177400589, 'accumulated_eval_time': 14488.276557445526, 'accumulated_logging_time': 4.293117523193359}
I0311 14:48:49.858120 139621870597888 logging_writer.py:48] [86448] accumulated_eval_time=14488.276557, accumulated_logging_time=4.293118, accumulated_submission_time=27150.821774, global_step=86448, preemption_count=0, score=27150.821774, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.275922, test/num_examples=43793, total_duration=41645.647801, train/accuracy=0.995526, train/loss=0.014030, train/mean_average_precision=0.772701, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286670, validation/num_examples=43793
I0311 14:49:07.303576 139622180505344 logging_writer.py:48] [86500] global_step=86500, grad_norm=0.1297938972711563, loss=0.017685862258076668
I0311 14:49:39.120244 139621870597888 logging_writer.py:48] [86600] global_step=86600, grad_norm=0.14400634169578552, loss=0.01991269923746586
I0311 14:50:10.707275 139622180505344 logging_writer.py:48] [86700] global_step=86700, grad_norm=0.148019939661026, loss=0.017689427360892296
I0311 14:50:42.403009 139621870597888 logging_writer.py:48] [86800] global_step=86800, grad_norm=0.1664329469203949, loss=0.01855197176337242
I0311 14:51:13.896494 139622180505344 logging_writer.py:48] [86900] global_step=86900, grad_norm=0.14139804244041443, loss=0.017048733308911324
I0311 14:51:45.661498 139621870597888 logging_writer.py:48] [87000] global_step=87000, grad_norm=0.1436781883239746, loss=0.01675715669989586
I0311 14:52:17.650687 139622180505344 logging_writer.py:48] [87100] global_step=87100, grad_norm=0.1479218304157257, loss=0.01701373979449272
I0311 14:52:50.063528 139621870597888 logging_writer.py:48] [87200] global_step=87200, grad_norm=0.12676624953746796, loss=0.01761079765856266
I0311 14:52:50.068711 139789500200768 spec.py:321] Evaluating on the training split.
I0311 14:54:41.567824 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 14:54:44.683670 139789500200768 spec.py:349] Evaluating on the test split.
I0311 14:54:47.745319 139789500200768 submission_runner.py:411] Time since start: 42003.57s, 	Step: 87201, 	{'train/accuracy': 0.9955212473869324, 'train/loss': 0.014033936895430088, 'train/mean_average_precision': 0.7780336724337944, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.286602377318734, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426059290766716, 'test/mean_average_precision': 0.2760478402507823, 'test/num_examples': 43793, 'score': 27391.00008749962, 'total_duration': 42003.57257246971, 'accumulated_submission_time': 27391.00008749962, 'accumulated_eval_time': 14605.95310664177, 'accumulated_logging_time': 4.343390703201294}
I0311 14:54:47.776571 139628670904064 logging_writer.py:48] [87201] accumulated_eval_time=14605.953107, accumulated_logging_time=4.343391, accumulated_submission_time=27391.000087, global_step=87201, preemption_count=0, score=27391.000087, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.276048, test/num_examples=43793, total_duration=42003.572572, train/accuracy=0.995521, train/loss=0.014034, train/mean_average_precision=0.778034, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286602, validation/num_examples=43793
I0311 14:55:19.893866 139628679296768 logging_writer.py:48] [87300] global_step=87300, grad_norm=0.13799665868282318, loss=0.018044117838144302
I0311 14:55:51.959661 139628670904064 logging_writer.py:48] [87400] global_step=87400, grad_norm=0.14993895590305328, loss=0.017345353960990906
I0311 14:56:24.075141 139628679296768 logging_writer.py:48] [87500] global_step=87500, grad_norm=0.13812793791294098, loss=0.01958121359348297
I0311 14:56:56.162832 139628670904064 logging_writer.py:48] [87600] global_step=87600, grad_norm=0.13484376668930054, loss=0.016739865764975548
I0311 14:57:28.480868 139628679296768 logging_writer.py:48] [87700] global_step=87700, grad_norm=0.1315005123615265, loss=0.016081426292657852
I0311 14:58:01.603292 139628670904064 logging_writer.py:48] [87800] global_step=87800, grad_norm=0.14783672988414764, loss=0.017656536772847176
I0311 14:58:34.119049 139628679296768 logging_writer.py:48] [87900] global_step=87900, grad_norm=0.13125871121883392, loss=0.017773250117897987
I0311 14:58:47.812202 139789500200768 spec.py:321] Evaluating on the training split.
I0311 15:00:43.533152 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 15:00:46.594653 139789500200768 spec.py:349] Evaluating on the test split.
I0311 15:00:51.183599 139789500200768 submission_runner.py:411] Time since start: 42367.01s, 	Step: 87944, 	{'train/accuracy': 0.995576798915863, 'train/loss': 0.013769900426268578, 'train/mean_average_precision': 0.7864783435135866, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.28656722161560855, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.2760906757638992, 'test/num_examples': 43793, 'score': 27631.004123210907, 'total_duration': 42367.01085090637, 'accumulated_submission_time': 27631.004123210907, 'accumulated_eval_time': 14729.32446551323, 'accumulated_logging_time': 4.386620283126831}
I0311 15:00:51.217164 139621870597888 logging_writer.py:48] [87944] accumulated_eval_time=14729.324466, accumulated_logging_time=4.386620, accumulated_submission_time=27631.004123, global_step=87944, preemption_count=0, score=27631.004123, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.276091, test/num_examples=43793, total_duration=42367.010851, train/accuracy=0.995577, train/loss=0.013770, train/mean_average_precision=0.786478, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286567, validation/num_examples=43793
I0311 15:01:09.748733 139622180505344 logging_writer.py:48] [88000] global_step=88000, grad_norm=0.13894978165626526, loss=0.01766154170036316
I0311 15:01:41.684544 139621870597888 logging_writer.py:48] [88100] global_step=88100, grad_norm=0.13331834971904755, loss=0.0169163066893816
I0311 15:02:13.854087 139622180505344 logging_writer.py:48] [88200] global_step=88200, grad_norm=0.14597554504871368, loss=0.015141095034778118
I0311 15:02:45.702488 139621870597888 logging_writer.py:48] [88300] global_step=88300, grad_norm=0.15832632780075073, loss=0.021359510719776154
I0311 15:03:17.753034 139622180505344 logging_writer.py:48] [88400] global_step=88400, grad_norm=0.1297302097082138, loss=0.016710184514522552
I0311 15:03:49.770784 139621870597888 logging_writer.py:48] [88500] global_step=88500, grad_norm=0.1563669592142105, loss=0.017490217462182045
I0311 15:04:21.933297 139622180505344 logging_writer.py:48] [88600] global_step=88600, grad_norm=0.1438290923833847, loss=0.016872961074113846
I0311 15:04:51.190318 139789500200768 spec.py:321] Evaluating on the training split.
I0311 15:06:43.706305 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 15:06:46.843836 139789500200768 spec.py:349] Evaluating on the test split.
I0311 15:06:49.928060 139789500200768 submission_runner.py:411] Time since start: 42725.76s, 	Step: 88693, 	{'train/accuracy': 0.9955042600631714, 'train/loss': 0.01403342466801405, 'train/mean_average_precision': 0.7815081377763017, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.28642123782753076, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.27613584269231656, 'test/num_examples': 43793, 'score': 27870.94686794281, 'total_duration': 42725.75531196594, 'accumulated_submission_time': 27870.94686794281, 'accumulated_eval_time': 14848.06216621399, 'accumulated_logging_time': 4.431077480316162}
I0311 15:06:49.960848 139628670904064 logging_writer.py:48] [88693] accumulated_eval_time=14848.062166, accumulated_logging_time=4.431077, accumulated_submission_time=27870.946868, global_step=88693, preemption_count=0, score=27870.946868, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.276136, test/num_examples=43793, total_duration=42725.755312, train/accuracy=0.995504, train/loss=0.014033, train/mean_average_precision=0.781508, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286421, validation/num_examples=43793
I0311 15:06:52.539057 139628679296768 logging_writer.py:48] [88700] global_step=88700, grad_norm=0.15181294083595276, loss=0.018413959071040154
I0311 15:07:24.458267 139628670904064 logging_writer.py:48] [88800] global_step=88800, grad_norm=0.13604317605495453, loss=0.016784576699137688
I0311 15:07:56.562387 139628679296768 logging_writer.py:48] [88900] global_step=88900, grad_norm=0.134333997964859, loss=0.017499523237347603
I0311 15:08:28.573212 139628670904064 logging_writer.py:48] [89000] global_step=89000, grad_norm=0.14944739639759064, loss=0.020537961274385452
I0311 15:09:00.322777 139628679296768 logging_writer.py:48] [89100] global_step=89100, grad_norm=0.14518646895885468, loss=0.018970344215631485
I0311 15:09:32.102608 139628670904064 logging_writer.py:48] [89200] global_step=89200, grad_norm=0.15755198895931244, loss=0.018958216533064842
I0311 15:10:03.935849 139628679296768 logging_writer.py:48] [89300] global_step=89300, grad_norm=0.13653042912483215, loss=0.017019258812069893
I0311 15:10:35.651274 139628670904064 logging_writer.py:48] [89400] global_step=89400, grad_norm=0.14751455187797546, loss=0.017040248960256577
I0311 15:10:50.253264 139789500200768 spec.py:321] Evaluating on the training split.
I0311 15:12:45.189263 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 15:12:48.352711 139789500200768 spec.py:349] Evaluating on the test split.
I0311 15:12:51.357565 139789500200768 submission_runner.py:411] Time since start: 43087.18s, 	Step: 89447, 	{'train/accuracy': 0.9955434799194336, 'train/loss': 0.013801591470837593, 'train/mean_average_precision': 0.778073415456541, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.28657425356941235, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.27598515098768295, 'test/num_examples': 43793, 'score': 28111.208525419235, 'total_duration': 43087.18470454216, 'accumulated_submission_time': 28111.208525419235, 'accumulated_eval_time': 14969.166311979294, 'accumulated_logging_time': 4.4745166301727295}
I0311 15:12:51.390307 139622172112640 logging_writer.py:48] [89447] accumulated_eval_time=14969.166312, accumulated_logging_time=4.474517, accumulated_submission_time=28111.208525, global_step=89447, preemption_count=0, score=28111.208525, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.275985, test/num_examples=43793, total_duration=43087.184705, train/accuracy=0.995543, train/loss=0.013802, train/mean_average_precision=0.778073, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286574, validation/num_examples=43793
I0311 15:13:08.784198 139622180505344 logging_writer.py:48] [89500] global_step=89500, grad_norm=0.1476902812719345, loss=0.020132476463913918
I0311 15:13:40.590677 139622172112640 logging_writer.py:48] [89600] global_step=89600, grad_norm=0.13779793679714203, loss=0.018966302275657654
I0311 15:14:12.124572 139622180505344 logging_writer.py:48] [89700] global_step=89700, grad_norm=0.143035426735878, loss=0.017679978162050247
I0311 15:14:43.727076 139622172112640 logging_writer.py:48] [89800] global_step=89800, grad_norm=0.1329890340566635, loss=0.016140291467308998
I0311 15:15:15.474785 139622180505344 logging_writer.py:48] [89900] global_step=89900, grad_norm=0.131531223654747, loss=0.01634531468153
I0311 15:15:47.210570 139622172112640 logging_writer.py:48] [90000] global_step=90000, grad_norm=0.1417403519153595, loss=0.016597643494606018
I0311 15:16:18.944953 139622180505344 logging_writer.py:48] [90100] global_step=90100, grad_norm=0.1463373601436615, loss=0.01796957664191723
I0311 15:16:50.448465 139622172112640 logging_writer.py:48] [90200] global_step=90200, grad_norm=0.15935644507408142, loss=0.02049851045012474
I0311 15:16:51.412759 139789500200768 spec.py:321] Evaluating on the training split.
I0311 15:18:41.804467 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 15:18:44.857672 139789500200768 spec.py:349] Evaluating on the test split.
I0311 15:18:47.848968 139789500200768 submission_runner.py:411] Time since start: 43443.68s, 	Step: 90204, 	{'train/accuracy': 0.9955761432647705, 'train/loss': 0.013939591124653816, 'train/mean_average_precision': 0.7750910374084437, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.2865358350164018, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.27596577020136687, 'test/num_examples': 43793, 'score': 28351.200445890427, 'total_duration': 43443.676209926605, 'accumulated_submission_time': 28351.200445890427, 'accumulated_eval_time': 15085.602485895157, 'accumulated_logging_time': 4.517841577529907}
I0311 15:18:47.882599 139621870597888 logging_writer.py:48] [90204] accumulated_eval_time=15085.602486, accumulated_logging_time=4.517842, accumulated_submission_time=28351.200446, global_step=90204, preemption_count=0, score=28351.200446, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.275966, test/num_examples=43793, total_duration=43443.676210, train/accuracy=0.995576, train/loss=0.013940, train/mean_average_precision=0.775091, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286536, validation/num_examples=43793
I0311 15:19:18.928570 139628679296768 logging_writer.py:48] [90300] global_step=90300, grad_norm=0.13345210254192352, loss=0.01607576385140419
I0311 15:19:50.424924 139621870597888 logging_writer.py:48] [90400] global_step=90400, grad_norm=0.1376967430114746, loss=0.016235575079917908
I0311 15:20:22.145503 139628679296768 logging_writer.py:48] [90500] global_step=90500, grad_norm=0.13570408523082733, loss=0.01749083586037159
I0311 15:20:53.656948 139621870597888 logging_writer.py:48] [90600] global_step=90600, grad_norm=0.13352076709270477, loss=0.01803986355662346
I0311 15:21:25.249623 139628679296768 logging_writer.py:48] [90700] global_step=90700, grad_norm=0.12110541015863419, loss=0.015663810074329376
I0311 15:21:56.903563 139621870597888 logging_writer.py:48] [90800] global_step=90800, grad_norm=0.16714513301849365, loss=0.01868266426026821
I0311 15:22:28.522674 139628679296768 logging_writer.py:48] [90900] global_step=90900, grad_norm=0.1487022340297699, loss=0.01637222245335579
I0311 15:22:48.093118 139789500200768 spec.py:321] Evaluating on the training split.
I0311 15:24:42.266582 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 15:24:45.318808 139789500200768 spec.py:349] Evaluating on the test split.
I0311 15:24:48.315457 139789500200768 submission_runner.py:411] Time since start: 43804.14s, 	Step: 90963, 	{'train/accuracy': 0.9954948425292969, 'train/loss': 0.014080642722547054, 'train/mean_average_precision': 0.7802792739060775, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.28661713295096386, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.27599760922265065, 'test/num_examples': 43793, 'score': 28591.379506587982, 'total_duration': 43804.14271020889, 'accumulated_submission_time': 28591.379506587982, 'accumulated_eval_time': 15205.82478427887, 'accumulated_logging_time': 4.563338756561279}
I0311 15:24:48.347299 139622172112640 logging_writer.py:48] [90963] accumulated_eval_time=15205.824784, accumulated_logging_time=4.563339, accumulated_submission_time=28591.379507, global_step=90963, preemption_count=0, score=28591.379507, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.275998, test/num_examples=43793, total_duration=43804.142710, train/accuracy=0.995495, train/loss=0.014081, train/mean_average_precision=0.780279, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286617, validation/num_examples=43793
I0311 15:25:00.321150 139622180505344 logging_writer.py:48] [91000] global_step=91000, grad_norm=0.13295187056064606, loss=0.018453074619174004
I0311 15:25:31.722064 139622172112640 logging_writer.py:48] [91100] global_step=91100, grad_norm=0.1400526463985443, loss=0.01691257767379284
I0311 15:26:02.879957 139622180505344 logging_writer.py:48] [91200] global_step=91200, grad_norm=0.13899783790111542, loss=0.01696941629052162
I0311 15:26:34.206665 139622172112640 logging_writer.py:48] [91300] global_step=91300, grad_norm=0.14306916296482086, loss=0.01601300574839115
I0311 15:27:05.644911 139622180505344 logging_writer.py:48] [91400] global_step=91400, grad_norm=0.13065685331821442, loss=0.014877558685839176
I0311 15:27:37.247868 139622172112640 logging_writer.py:48] [91500] global_step=91500, grad_norm=0.13561956584453583, loss=0.016537435352802277
I0311 15:28:08.597161 139622180505344 logging_writer.py:48] [91600] global_step=91600, grad_norm=0.15998829901218414, loss=0.017760321497917175
I0311 15:28:40.243653 139622172112640 logging_writer.py:48] [91700] global_step=91700, grad_norm=0.13940460979938507, loss=0.017302440479397774
I0311 15:28:48.389319 139789500200768 spec.py:321] Evaluating on the training split.
I0311 15:30:38.873553 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 15:30:41.905868 139789500200768 spec.py:349] Evaluating on the test split.
I0311 15:30:44.903213 139789500200768 submission_runner.py:411] Time since start: 44160.73s, 	Step: 91727, 	{'train/accuracy': 0.9955697059631348, 'train/loss': 0.013826191425323486, 'train/mean_average_precision': 0.7810301983931045, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.2866110429605794, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.27599961688565555, 'test/num_examples': 43793, 'score': 28831.39079451561, 'total_duration': 44160.73046088219, 'accumulated_submission_time': 28831.39079451561, 'accumulated_eval_time': 15322.338630437851, 'accumulated_logging_time': 4.60586142539978}
I0311 15:30:44.936517 139621870597888 logging_writer.py:48] [91727] accumulated_eval_time=15322.338630, accumulated_logging_time=4.605861, accumulated_submission_time=28831.390795, global_step=91727, preemption_count=0, score=28831.390795, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.276000, test/num_examples=43793, total_duration=44160.730461, train/accuracy=0.995570, train/loss=0.013826, train/mean_average_precision=0.781030, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286611, validation/num_examples=43793
I0311 15:31:08.033537 139628679296768 logging_writer.py:48] [91800] global_step=91800, grad_norm=0.14618124067783356, loss=0.01767580211162567
I0311 15:31:39.583067 139621870597888 logging_writer.py:48] [91900] global_step=91900, grad_norm=0.12819570302963257, loss=0.016724977642297745
I0311 15:32:10.907040 139628679296768 logging_writer.py:48] [92000] global_step=92000, grad_norm=0.15750983357429504, loss=0.019393961876630783
I0311 15:32:42.087816 139621870597888 logging_writer.py:48] [92100] global_step=92100, grad_norm=0.1339586228132248, loss=0.016560258343815804
I0311 15:33:13.041599 139628679296768 logging_writer.py:48] [92200] global_step=92200, grad_norm=0.13238927721977234, loss=0.01620025560259819
I0311 15:33:44.352267 139621870597888 logging_writer.py:48] [92300] global_step=92300, grad_norm=0.1513386070728302, loss=0.020414067432284355
I0311 15:34:15.840096 139628679296768 logging_writer.py:48] [92400] global_step=92400, grad_norm=0.1322382241487503, loss=0.017705367878079414
I0311 15:34:45.133874 139789500200768 spec.py:321] Evaluating on the training split.
I0311 15:36:38.443092 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 15:36:41.540698 139789500200768 spec.py:349] Evaluating on the test split.
I0311 15:36:44.497900 139789500200768 submission_runner.py:411] Time since start: 44520.33s, 	Step: 92495, 	{'train/accuracy': 0.9955700039863586, 'train/loss': 0.013861620798707008, 'train/mean_average_precision': 0.7817951724672543, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.2865152886626495, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.2759411183774932, 'test/num_examples': 43793, 'score': 29071.55628156662, 'total_duration': 44520.325152397156, 'accumulated_submission_time': 29071.55628156662, 'accumulated_eval_time': 15441.70261669159, 'accumulated_logging_time': 4.650819540023804}
I0311 15:36:44.531220 139622172112640 logging_writer.py:48] [92495] accumulated_eval_time=15441.702617, accumulated_logging_time=4.650820, accumulated_submission_time=29071.556282, global_step=92495, preemption_count=0, score=29071.556282, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.275941, test/num_examples=43793, total_duration=44520.325152, train/accuracy=0.995570, train/loss=0.013862, train/mean_average_precision=0.781795, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286515, validation/num_examples=43793
I0311 15:36:46.468496 139628670904064 logging_writer.py:48] [92500] global_step=92500, grad_norm=0.14367400109767914, loss=0.017833871766924858
I0311 15:37:18.049501 139622172112640 logging_writer.py:48] [92600] global_step=92600, grad_norm=0.1459200084209442, loss=0.017788471654057503
I0311 15:37:49.726586 139628670904064 logging_writer.py:48] [92700] global_step=92700, grad_norm=0.14693355560302734, loss=0.016275174915790558
I0311 15:38:21.411002 139622172112640 logging_writer.py:48] [92800] global_step=92800, grad_norm=0.14852862060070038, loss=0.018363764509558678
I0311 15:38:52.994903 139628670904064 logging_writer.py:48] [92900] global_step=92900, grad_norm=0.1296669989824295, loss=0.015426257625222206
I0311 15:39:24.619403 139622172112640 logging_writer.py:48] [93000] global_step=93000, grad_norm=0.13651831448078156, loss=0.017821522429585457
I0311 15:39:56.027217 139628670904064 logging_writer.py:48] [93100] global_step=93100, grad_norm=0.14629405736923218, loss=0.01893504150211811
I0311 15:40:27.875193 139622172112640 logging_writer.py:48] [93200] global_step=93200, grad_norm=0.15851938724517822, loss=0.021175872534513474
I0311 15:40:44.772644 139789500200768 spec.py:321] Evaluating on the training split.
I0311 15:42:39.477056 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 15:42:42.535943 139789500200768 spec.py:349] Evaluating on the test split.
I0311 15:42:45.557933 139789500200768 submission_runner.py:411] Time since start: 44881.39s, 	Step: 93254, 	{'train/accuracy': 0.9955008029937744, 'train/loss': 0.014022867195308208, 'train/mean_average_precision': 0.7731956136988019, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.2864977609935052, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.2759367174136053, 'test/num_examples': 43793, 'score': 29311.766901016235, 'total_duration': 44881.385175704956, 'accumulated_submission_time': 29311.766901016235, 'accumulated_eval_time': 15562.487857341766, 'accumulated_logging_time': 4.694918155670166}
I0311 15:42:45.591721 139621870597888 logging_writer.py:48] [93254] accumulated_eval_time=15562.487857, accumulated_logging_time=4.694918, accumulated_submission_time=29311.766901, global_step=93254, preemption_count=0, score=29311.766901, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.275937, test/num_examples=43793, total_duration=44881.385176, train/accuracy=0.995501, train/loss=0.014023, train/mean_average_precision=0.773196, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286498, validation/num_examples=43793
I0311 15:43:00.792766 139622180505344 logging_writer.py:48] [93300] global_step=93300, grad_norm=0.14776837825775146, loss=0.01790001057088375
I0311 15:43:32.314661 139621870597888 logging_writer.py:48] [93400] global_step=93400, grad_norm=0.1555013209581375, loss=0.019711075350642204
I0311 15:44:03.798194 139622180505344 logging_writer.py:48] [93500] global_step=93500, grad_norm=0.15682263672351837, loss=0.02089698053896427
I0311 15:44:35.464390 139621870597888 logging_writer.py:48] [93600] global_step=93600, grad_norm=0.14662863314151764, loss=0.017833519726991653
I0311 15:45:06.698406 139622180505344 logging_writer.py:48] [93700] global_step=93700, grad_norm=0.14360155165195465, loss=0.016718000173568726
I0311 15:45:38.036252 139621870597888 logging_writer.py:48] [93800] global_step=93800, grad_norm=0.1579919457435608, loss=0.02012091875076294
I0311 15:46:09.510092 139622180505344 logging_writer.py:48] [93900] global_step=93900, grad_norm=0.13672158122062683, loss=0.016181686893105507
I0311 15:46:41.058329 139621870597888 logging_writer.py:48] [94000] global_step=94000, grad_norm=0.15173658728599548, loss=0.020389700308442116
I0311 15:46:45.828461 139789500200768 spec.py:321] Evaluating on the training split.
I0311 15:48:37.722881 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 15:48:40.750546 139789500200768 spec.py:349] Evaluating on the test split.
I0311 15:48:43.759342 139789500200768 submission_runner.py:411] Time since start: 45239.59s, 	Step: 94016, 	{'train/accuracy': 0.9955675005912781, 'train/loss': 0.01388111524283886, 'train/mean_average_precision': 0.7789318143991779, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.2865480551449149, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.27606673888065353, 'test/num_examples': 43793, 'score': 29551.97358584404, 'total_duration': 45239.58659052849, 'accumulated_submission_time': 29551.97358584404, 'accumulated_eval_time': 15680.41870713234, 'accumulated_logging_time': 4.739025115966797}
I0311 15:48:43.793500 139628670904064 logging_writer.py:48] [94016] accumulated_eval_time=15680.418707, accumulated_logging_time=4.739025, accumulated_submission_time=29551.973586, global_step=94016, preemption_count=0, score=29551.973586, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.276067, test/num_examples=43793, total_duration=45239.586591, train/accuracy=0.995568, train/loss=0.013881, train/mean_average_precision=0.778932, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286548, validation/num_examples=43793
I0311 15:49:10.343478 139628679296768 logging_writer.py:48] [94100] global_step=94100, grad_norm=0.16135063767433167, loss=0.019695833325386047
I0311 15:49:41.766184 139628670904064 logging_writer.py:48] [94200] global_step=94200, grad_norm=0.14936858415603638, loss=0.015793710947036743
I0311 15:50:13.265885 139628679296768 logging_writer.py:48] [94300] global_step=94300, grad_norm=0.16337993741035461, loss=0.01825939677655697
I0311 15:50:44.325220 139628670904064 logging_writer.py:48] [94400] global_step=94400, grad_norm=0.13976486027240753, loss=0.016941474750638008
I0311 15:51:15.806070 139628679296768 logging_writer.py:48] [94500] global_step=94500, grad_norm=0.13533101975917816, loss=0.016637180000543594
I0311 15:51:47.329982 139628670904064 logging_writer.py:48] [94600] global_step=94600, grad_norm=0.14546255767345428, loss=0.018017174676060677
I0311 15:52:18.483594 139628679296768 logging_writer.py:48] [94700] global_step=94700, grad_norm=0.13934588432312012, loss=0.01967310719192028
I0311 15:52:43.784205 139789500200768 spec.py:321] Evaluating on the training split.
I0311 15:54:38.848227 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 15:54:41.898487 139789500200768 spec.py:349] Evaluating on the test split.
I0311 15:54:44.887434 139789500200768 submission_runner.py:411] Time since start: 45600.71s, 	Step: 94782, 	{'train/accuracy': 0.9955399036407471, 'train/loss': 0.01397243607789278, 'train/mean_average_precision': 0.7753218829520573, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.2866137226228561, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.2759850102404555, 'test/num_examples': 43793, 'score': 29791.934020996094, 'total_duration': 45600.714686870575, 'accumulated_submission_time': 29791.934020996094, 'accumulated_eval_time': 15801.521898031235, 'accumulated_logging_time': 4.783658504486084}
I0311 15:54:44.920138 139621870597888 logging_writer.py:48] [94782] accumulated_eval_time=15801.521898, accumulated_logging_time=4.783659, accumulated_submission_time=29791.934021, global_step=94782, preemption_count=0, score=29791.934021, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.275985, test/num_examples=43793, total_duration=45600.714687, train/accuracy=0.995540, train/loss=0.013972, train/mean_average_precision=0.775322, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286614, validation/num_examples=43793
I0311 15:54:50.904381 139622172112640 logging_writer.py:48] [94800] global_step=94800, grad_norm=0.13282740116119385, loss=0.017811080440878868
I0311 15:55:22.471877 139621870597888 logging_writer.py:48] [94900] global_step=94900, grad_norm=0.16041986644268036, loss=0.018684370443224907
I0311 15:55:53.940182 139622172112640 logging_writer.py:48] [95000] global_step=95000, grad_norm=0.13918454945087433, loss=0.019039714708924294
I0311 15:56:25.153572 139621870597888 logging_writer.py:48] [95100] global_step=95100, grad_norm=0.14990685880184174, loss=0.01803772896528244
I0311 15:56:56.483130 139622172112640 logging_writer.py:48] [95200] global_step=95200, grad_norm=0.12611667811870575, loss=0.016292516142129898
I0311 15:57:27.973415 139621870597888 logging_writer.py:48] [95300] global_step=95300, grad_norm=0.16167615354061127, loss=0.018596719950437546
I0311 15:57:59.667921 139622172112640 logging_writer.py:48] [95400] global_step=95400, grad_norm=0.1433444768190384, loss=0.016209838911890984
I0311 15:58:31.340924 139621870597888 logging_writer.py:48] [95500] global_step=95500, grad_norm=0.14497129619121552, loss=0.01649710163474083
I0311 15:58:45.118453 139789500200768 spec.py:321] Evaluating on the training split.
I0311 16:00:34.382118 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 16:00:37.413082 139789500200768 spec.py:349] Evaluating on the test split.
I0311 16:00:40.451120 139789500200768 submission_runner.py:411] Time since start: 45956.28s, 	Step: 95545, 	{'train/accuracy': 0.9955657124519348, 'train/loss': 0.013877557590603828, 'train/mean_average_precision': 0.7811558660083017, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.28662541128754276, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.27602166018248514, 'test/num_examples': 43793, 'score': 30032.101594686508, 'total_duration': 45956.278367996216, 'accumulated_submission_time': 30032.101594686508, 'accumulated_eval_time': 15916.854518413544, 'accumulated_logging_time': 4.826842546463013}
I0311 16:00:40.484323 139628670904064 logging_writer.py:48] [95545] accumulated_eval_time=15916.854518, accumulated_logging_time=4.826843, accumulated_submission_time=30032.101595, global_step=95545, preemption_count=0, score=30032.101595, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.276022, test/num_examples=43793, total_duration=45956.278368, train/accuracy=0.995566, train/loss=0.013878, train/mean_average_precision=0.781156, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286625, validation/num_examples=43793
I0311 16:00:58.064683 139628679296768 logging_writer.py:48] [95600] global_step=95600, grad_norm=0.14126047492027283, loss=0.018525581806898117
I0311 16:01:29.456015 139628670904064 logging_writer.py:48] [95700] global_step=95700, grad_norm=0.14378827810287476, loss=0.01780441217124462
I0311 16:02:01.039215 139628679296768 logging_writer.py:48] [95800] global_step=95800, grad_norm=0.14564478397369385, loss=0.018727228045463562
I0311 16:02:32.590184 139628670904064 logging_writer.py:48] [95900] global_step=95900, grad_norm=0.12808065116405487, loss=0.017570560798048973
I0311 16:03:03.803851 139628679296768 logging_writer.py:48] [96000] global_step=96000, grad_norm=0.14088353514671326, loss=0.017689524218440056
I0311 16:03:35.266722 139628670904064 logging_writer.py:48] [96100] global_step=96100, grad_norm=0.12608855962753296, loss=0.016546398401260376
I0311 16:04:06.640124 139628679296768 logging_writer.py:48] [96200] global_step=96200, grad_norm=0.14999273419380188, loss=0.018358342349529266
I0311 16:04:38.110754 139628670904064 logging_writer.py:48] [96300] global_step=96300, grad_norm=0.13585101068019867, loss=0.016189398244023323
I0311 16:04:40.642076 139789500200768 spec.py:321] Evaluating on the training split.
I0311 16:06:32.809517 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 16:06:35.848743 139789500200768 spec.py:349] Evaluating on the test split.
I0311 16:06:38.837141 139789500200768 submission_runner.py:411] Time since start: 46314.66s, 	Step: 96309, 	{'train/accuracy': 0.9955589175224304, 'train/loss': 0.013802922330796719, 'train/mean_average_precision': 0.7868677053325954, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.28661016304410036, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.2760575986127702, 'test/num_examples': 43793, 'score': 30272.227252483368, 'total_duration': 46314.66438794136, 'accumulated_submission_time': 30272.227252483368, 'accumulated_eval_time': 16035.049534797668, 'accumulated_logging_time': 4.871997594833374}
I0311 16:06:38.870709 139621870597888 logging_writer.py:48] [96309] accumulated_eval_time=16035.049535, accumulated_logging_time=4.871998, accumulated_submission_time=30272.227252, global_step=96309, preemption_count=0, score=30272.227252, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.276058, test/num_examples=43793, total_duration=46314.664388, train/accuracy=0.995559, train/loss=0.013803, train/mean_average_precision=0.786868, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286610, validation/num_examples=43793
I0311 16:07:07.970071 139622180505344 logging_writer.py:48] [96400] global_step=96400, grad_norm=0.14119938015937805, loss=0.017475076019763947
I0311 16:07:39.778323 139621870597888 logging_writer.py:48] [96500] global_step=96500, grad_norm=0.13312436640262604, loss=0.015961723402142525
I0311 16:08:11.237404 139622180505344 logging_writer.py:48] [96600] global_step=96600, grad_norm=0.13814711570739746, loss=0.01851535402238369
I0311 16:08:43.032327 139621870597888 logging_writer.py:48] [96700] global_step=96700, grad_norm=0.1569664180278778, loss=0.019552113488316536
I0311 16:09:14.569036 139622180505344 logging_writer.py:48] [96800] global_step=96800, grad_norm=0.15254412591457367, loss=0.021007182076573372
I0311 16:09:46.133946 139621870597888 logging_writer.py:48] [96900] global_step=96900, grad_norm=0.15022355318069458, loss=0.017746875062584877
I0311 16:10:18.094596 139622180505344 logging_writer.py:48] [97000] global_step=97000, grad_norm=0.1360693722963333, loss=0.017040494829416275
I0311 16:10:38.886693 139789500200768 spec.py:321] Evaluating on the training split.
I0311 16:12:36.885546 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 16:12:39.978923 139789500200768 spec.py:349] Evaluating on the test split.
I0311 16:12:42.995192 139789500200768 submission_runner.py:411] Time since start: 46678.82s, 	Step: 97067, 	{'train/accuracy': 0.9955001473426819, 'train/loss': 0.014052250422537327, 'train/mean_average_precision': 0.7729460373532289, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.28668993511872576, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.2759726786264729, 'test/num_examples': 43793, 'score': 30512.211572647095, 'total_duration': 46678.82244515419, 'accumulated_submission_time': 30512.211572647095, 'accumulated_eval_time': 16159.157991409302, 'accumulated_logging_time': 4.9170825481414795}
I0311 16:12:43.028573 139628670904064 logging_writer.py:48] [97067] accumulated_eval_time=16159.157991, accumulated_logging_time=4.917083, accumulated_submission_time=30512.211573, global_step=97067, preemption_count=0, score=30512.211573, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.275973, test/num_examples=43793, total_duration=46678.822445, train/accuracy=0.995500, train/loss=0.014052, train/mean_average_precision=0.772946, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286690, validation/num_examples=43793
I0311 16:12:53.839437 139628679296768 logging_writer.py:48] [97100] global_step=97100, grad_norm=0.14251995086669922, loss=0.018860001116991043
I0311 16:13:25.352167 139628670904064 logging_writer.py:48] [97200] global_step=97200, grad_norm=0.12890103459358215, loss=0.016130974516272545
I0311 16:13:56.959219 139628679296768 logging_writer.py:48] [97300] global_step=97300, grad_norm=0.14528217911720276, loss=0.01918557845056057
I0311 16:14:28.374237 139628670904064 logging_writer.py:48] [97400] global_step=97400, grad_norm=0.15167157351970673, loss=0.015540581196546555
I0311 16:14:59.858221 139628679296768 logging_writer.py:48] [97500] global_step=97500, grad_norm=0.14429102838039398, loss=0.01848185807466507
I0311 16:15:31.258659 139628670904064 logging_writer.py:48] [97600] global_step=97600, grad_norm=0.13982394337654114, loss=0.017735786736011505
I0311 16:16:02.777079 139628679296768 logging_writer.py:48] [97700] global_step=97700, grad_norm=0.1344730705022812, loss=0.01669071801006794
I0311 16:16:34.408521 139628670904064 logging_writer.py:48] [97800] global_step=97800, grad_norm=0.1314430683851242, loss=0.016897009685635567
I0311 16:16:43.129575 139789500200768 spec.py:321] Evaluating on the training split.
I0311 16:18:37.000932 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 16:18:40.028803 139789500200768 spec.py:349] Evaluating on the test split.
I0311 16:18:43.053339 139789500200768 submission_runner.py:411] Time since start: 47038.88s, 	Step: 97829, 	{'train/accuracy': 0.9955435991287231, 'train/loss': 0.013885174877941608, 'train/mean_average_precision': 0.7760392545421367, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.2865191272693426, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.27611054885026437, 'test/num_examples': 43793, 'score': 30752.282462358475, 'total_duration': 47038.88059139252, 'accumulated_submission_time': 30752.282462358475, 'accumulated_eval_time': 16279.08171248436, 'accumulated_logging_time': 4.960994243621826}
I0311 16:18:43.086586 139621870597888 logging_writer.py:48] [97829] accumulated_eval_time=16279.081712, accumulated_logging_time=4.960994, accumulated_submission_time=30752.282462, global_step=97829, preemption_count=0, score=30752.282462, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.276111, test/num_examples=43793, total_duration=47038.880591, train/accuracy=0.995544, train/loss=0.013885, train/mean_average_precision=0.776039, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286519, validation/num_examples=43793
I0311 16:19:05.561894 139622180505344 logging_writer.py:48] [97900] global_step=97900, grad_norm=0.1264074444770813, loss=0.015851441770792007
I0311 16:19:37.322612 139621870597888 logging_writer.py:48] [98000] global_step=98000, grad_norm=0.14435115456581116, loss=0.017155414447188377
I0311 16:20:08.748064 139622180505344 logging_writer.py:48] [98100] global_step=98100, grad_norm=0.13946714997291565, loss=0.015737667679786682
I0311 16:20:40.679505 139621870597888 logging_writer.py:48] [98200] global_step=98200, grad_norm=0.14544792473316193, loss=0.018695160746574402
I0311 16:21:13.460942 139622180505344 logging_writer.py:48] [98300] global_step=98300, grad_norm=0.13007284700870514, loss=0.01725522056221962
I0311 16:21:46.847146 139621870597888 logging_writer.py:48] [98400] global_step=98400, grad_norm=0.1421443074941635, loss=0.016088327392935753
I0311 16:22:19.268802 139622180505344 logging_writer.py:48] [98500] global_step=98500, grad_norm=0.15117405354976654, loss=0.018990568816661835
I0311 16:22:43.345433 139789500200768 spec.py:321] Evaluating on the training split.
I0311 16:24:32.693153 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 16:24:37.286982 139789500200768 spec.py:349] Evaluating on the test split.
I0311 16:24:40.241637 139789500200768 submission_runner.py:411] Time since start: 47396.07s, 	Step: 98578, 	{'train/accuracy': 0.9955313801765442, 'train/loss': 0.013981902971863747, 'train/mean_average_precision': 0.7786825487258938, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.2864706683088275, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426059290766716, 'test/mean_average_precision': 0.2760305001928738, 'test/num_examples': 43793, 'score': 30992.51039814949, 'total_duration': 47396.06888914108, 'accumulated_submission_time': 30992.51039814949, 'accumulated_eval_time': 16395.97787475586, 'accumulated_logging_time': 5.005981922149658}
I0311 16:24:40.276081 139622172112640 logging_writer.py:48] [98578] accumulated_eval_time=16395.977875, accumulated_logging_time=5.005982, accumulated_submission_time=30992.510398, global_step=98578, preemption_count=0, score=30992.510398, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.276031, test/num_examples=43793, total_duration=47396.068889, train/accuracy=0.995531, train/loss=0.013982, train/mean_average_precision=0.778683, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286471, validation/num_examples=43793
I0311 16:24:47.775263 139628670904064 logging_writer.py:48] [98600] global_step=98600, grad_norm=0.14225806295871735, loss=0.015483015216886997
I0311 16:25:19.855130 139622172112640 logging_writer.py:48] [98700] global_step=98700, grad_norm=0.14715169370174408, loss=0.0189732126891613
I0311 16:25:51.892128 139628670904064 logging_writer.py:48] [98800] global_step=98800, grad_norm=0.14387346804141998, loss=0.01808297447860241
I0311 16:26:23.727234 139622172112640 logging_writer.py:48] [98900] global_step=98900, grad_norm=0.1520599126815796, loss=0.017066342756152153
I0311 16:26:54.958011 139628670904064 logging_writer.py:48] [99000] global_step=99000, grad_norm=0.13825441896915436, loss=0.01771421916782856
I0311 16:27:26.405494 139622172112640 logging_writer.py:48] [99100] global_step=99100, grad_norm=0.14825654029846191, loss=0.020206518471240997
I0311 16:27:58.094176 139628670904064 logging_writer.py:48] [99200] global_step=99200, grad_norm=0.1323072761297226, loss=0.016556618735194206
I0311 16:28:29.415493 139622172112640 logging_writer.py:48] [99300] global_step=99300, grad_norm=0.14296981692314148, loss=0.017674816772341728
I0311 16:28:40.384455 139789500200768 spec.py:321] Evaluating on the training split.
I0311 16:30:32.781760 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 16:30:35.790712 139789500200768 spec.py:349] Evaluating on the test split.
I0311 16:30:38.781398 139789500200768 submission_runner.py:411] Time since start: 47754.61s, 	Step: 99336, 	{'train/accuracy': 0.9955341815948486, 'train/loss': 0.01396910659968853, 'train/mean_average_precision': 0.7826914554191917, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.28655729105045646, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426059290766716, 'test/mean_average_precision': 0.27597604529584313, 'test/num_examples': 43793, 'score': 31232.58783340454, 'total_duration': 47754.608516693115, 'accumulated_submission_time': 31232.58783340454, 'accumulated_eval_time': 16514.374661684036, 'accumulated_logging_time': 5.0520899295806885}
I0311 16:30:38.815549 139621870597888 logging_writer.py:48] [99336] accumulated_eval_time=16514.374662, accumulated_logging_time=5.052090, accumulated_submission_time=31232.587833, global_step=99336, preemption_count=0, score=31232.587833, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.275976, test/num_examples=43793, total_duration=47754.608517, train/accuracy=0.995534, train/loss=0.013969, train/mean_average_precision=0.782691, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286557, validation/num_examples=43793
I0311 16:30:59.497335 139628679296768 logging_writer.py:48] [99400] global_step=99400, grad_norm=0.14105454087257385, loss=0.016944749280810356
I0311 16:31:30.827404 139621870597888 logging_writer.py:48] [99500] global_step=99500, grad_norm=0.1356879025697708, loss=0.01684664748609066
I0311 16:32:02.179163 139628679296768 logging_writer.py:48] [99600] global_step=99600, grad_norm=0.13775284588336945, loss=0.0162685364484787
I0311 16:32:33.560951 139621870597888 logging_writer.py:48] [99700] global_step=99700, grad_norm=0.14332246780395508, loss=0.017664510756731033
I0311 16:33:04.698747 139628679296768 logging_writer.py:48] [99800] global_step=99800, grad_norm=0.1350686103105545, loss=0.016929220408201218
I0311 16:33:35.877525 139621870597888 logging_writer.py:48] [99900] global_step=99900, grad_norm=0.15789614617824554, loss=0.017531204968690872
I0311 16:34:07.043821 139628679296768 logging_writer.py:48] [100000] global_step=100000, grad_norm=0.1403551548719406, loss=0.01709277555346489
I0311 16:34:38.303697 139621870597888 logging_writer.py:48] [100100] global_step=100100, grad_norm=0.1385125368833542, loss=0.01645631156861782
I0311 16:34:38.953136 139789500200768 spec.py:321] Evaluating on the training split.
I0311 16:36:31.915222 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 16:36:34.932189 139789500200768 spec.py:349] Evaluating on the test split.
I0311 16:36:37.922061 139789500200768 submission_runner.py:411] Time since start: 48113.75s, 	Step: 100103, 	{'train/accuracy': 0.9955401420593262, 'train/loss': 0.01389030460268259, 'train/mean_average_precision': 0.7793055337576846, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.2865208102988978, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.27595623146942866, 'test/num_examples': 43793, 'score': 31472.69523191452, 'total_duration': 48113.74931025505, 'accumulated_submission_time': 31472.69523191452, 'accumulated_eval_time': 16633.34353852272, 'accumulated_logging_time': 5.097035884857178}
I0311 16:36:37.956502 139622180505344 logging_writer.py:48] [100103] accumulated_eval_time=16633.343539, accumulated_logging_time=5.097036, accumulated_submission_time=31472.695232, global_step=100103, preemption_count=0, score=31472.695232, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.275956, test/num_examples=43793, total_duration=48113.749310, train/accuracy=0.995540, train/loss=0.013890, train/mean_average_precision=0.779306, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286521, validation/num_examples=43793
I0311 16:37:08.822199 139628670904064 logging_writer.py:48] [100200] global_step=100200, grad_norm=0.1399451047182083, loss=0.017001977190375328
I0311 16:37:40.801798 139622180505344 logging_writer.py:48] [100300] global_step=100300, grad_norm=0.15453390777111053, loss=0.017861954867839813
I0311 16:38:12.104162 139628670904064 logging_writer.py:48] [100400] global_step=100400, grad_norm=0.13936030864715576, loss=0.018890872597694397
I0311 16:38:43.392359 139622180505344 logging_writer.py:48] [100500] global_step=100500, grad_norm=0.13030345737934113, loss=0.01621614582836628
I0311 16:39:14.413787 139628670904064 logging_writer.py:48] [100600] global_step=100600, grad_norm=0.1359909474849701, loss=0.01658277027308941
I0311 16:39:45.592293 139622180505344 logging_writer.py:48] [100700] global_step=100700, grad_norm=0.14856821298599243, loss=0.018199017271399498
I0311 16:40:16.766035 139628670904064 logging_writer.py:48] [100800] global_step=100800, grad_norm=0.15774840116500854, loss=0.018448857590556145
I0311 16:40:37.944520 139789500200768 spec.py:321] Evaluating on the training split.
I0311 16:42:30.339107 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 16:42:33.392105 139789500200768 spec.py:349] Evaluating on the test split.
I0311 16:42:36.400838 139789500200768 submission_runner.py:411] Time since start: 48472.23s, 	Step: 100869, 	{'train/accuracy': 0.9955520629882812, 'train/loss': 0.013956233859062195, 'train/mean_average_precision': 0.7847198707800941, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.2865390644148356, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.2760779355054802, 'test/num_examples': 43793, 'score': 31712.652955770493, 'total_duration': 48472.228076696396, 'accumulated_submission_time': 31712.652955770493, 'accumulated_eval_time': 16751.799800157547, 'accumulated_logging_time': 5.141988754272461}
I0311 16:42:36.436598 139621870597888 logging_writer.py:48] [100869] accumulated_eval_time=16751.799800, accumulated_logging_time=5.141989, accumulated_submission_time=31712.652956, global_step=100869, preemption_count=0, score=31712.652956, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.276078, test/num_examples=43793, total_duration=48472.228077, train/accuracy=0.995552, train/loss=0.013956, train/mean_average_precision=0.784720, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286539, validation/num_examples=43793
I0311 16:42:46.518002 139622172112640 logging_writer.py:48] [100900] global_step=100900, grad_norm=0.1369309276342392, loss=0.0179321076720953
I0311 16:43:17.667701 139621870597888 logging_writer.py:48] [101000] global_step=101000, grad_norm=0.14142127335071564, loss=0.021110570058226585
I0311 16:43:49.424370 139622172112640 logging_writer.py:48] [101100] global_step=101100, grad_norm=0.15379446744918823, loss=0.017694616690278053
I0311 16:44:21.172947 139621870597888 logging_writer.py:48] [101200] global_step=101200, grad_norm=0.1605834662914276, loss=0.01572624407708645
I0311 16:44:52.615286 139622172112640 logging_writer.py:48] [101300] global_step=101300, grad_norm=0.14573997259140015, loss=0.01774575188755989
I0311 16:45:23.859185 139621870597888 logging_writer.py:48] [101400] global_step=101400, grad_norm=0.14303308725357056, loss=0.016861537471413612
I0311 16:45:54.949419 139622172112640 logging_writer.py:48] [101500] global_step=101500, grad_norm=0.16356272995471954, loss=0.018706802278757095
I0311 16:46:26.291544 139621870597888 logging_writer.py:48] [101600] global_step=101600, grad_norm=0.1431238204240799, loss=0.01842503435909748
I0311 16:46:36.567260 139789500200768 spec.py:321] Evaluating on the training split.
I0311 16:48:27.958299 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 16:48:31.030453 139789500200768 spec.py:349] Evaluating on the test split.
I0311 16:48:34.003675 139789500200768 submission_runner.py:411] Time since start: 48829.83s, 	Step: 101634, 	{'train/accuracy': 0.9955708980560303, 'train/loss': 0.01379525475203991, 'train/mean_average_precision': 0.7813605957526542, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.2866145553852413, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.2759564045633718, 'test/num_examples': 43793, 'score': 31952.75311899185, 'total_duration': 48829.83093047142, 'accumulated_submission_time': 31952.75311899185, 'accumulated_eval_time': 16869.23617386818, 'accumulated_logging_time': 5.1884894371032715}
I0311 16:48:34.038054 139622180505344 logging_writer.py:48] [101634] accumulated_eval_time=16869.236174, accumulated_logging_time=5.188489, accumulated_submission_time=31952.753119, global_step=101634, preemption_count=0, score=31952.753119, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.275956, test/num_examples=43793, total_duration=48829.830930, train/accuracy=0.995571, train/loss=0.013795, train/mean_average_precision=0.781361, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286615, validation/num_examples=43793
I0311 16:48:55.249049 139628670904064 logging_writer.py:48] [101700] global_step=101700, grad_norm=0.14448291063308716, loss=0.017144428566098213
I0311 16:49:26.832080 139622180505344 logging_writer.py:48] [101800] global_step=101800, grad_norm=0.13677793741226196, loss=0.017145508900284767
I0311 16:49:58.047385 139628670904064 logging_writer.py:48] [101900] global_step=101900, grad_norm=0.14918792247772217, loss=0.017582539469003677
I0311 16:50:29.278285 139622180505344 logging_writer.py:48] [102000] global_step=102000, grad_norm=0.13618282973766327, loss=0.017659712582826614
I0311 16:51:00.581158 139628670904064 logging_writer.py:48] [102100] global_step=102100, grad_norm=0.12727949023246765, loss=0.017161421477794647
I0311 16:51:31.946186 139622180505344 logging_writer.py:48] [102200] global_step=102200, grad_norm=0.13643063604831696, loss=0.018839862197637558
I0311 16:52:03.313380 139628670904064 logging_writer.py:48] [102300] global_step=102300, grad_norm=0.1407705545425415, loss=0.01757960207760334
I0311 16:52:34.068078 139789500200768 spec.py:321] Evaluating on the training split.
I0311 16:54:29.358158 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 16:54:32.385401 139789500200768 spec.py:349] Evaluating on the test split.
I0311 16:54:35.420489 139789500200768 submission_runner.py:411] Time since start: 49191.25s, 	Step: 102400, 	{'train/accuracy': 0.9955251216888428, 'train/loss': 0.013961590826511383, 'train/mean_average_precision': 0.770412098402832, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.28648523717673197, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.276152526270037, 'test/num_examples': 43793, 'score': 32192.752415180206, 'total_duration': 49191.247725725174, 'accumulated_submission_time': 32192.752415180206, 'accumulated_eval_time': 16990.588547229767, 'accumulated_logging_time': 5.23371148109436}
I0311 16:54:35.454611 139622172112640 logging_writer.py:48] [102400] accumulated_eval_time=16990.588547, accumulated_logging_time=5.233711, accumulated_submission_time=32192.752415, global_step=102400, preemption_count=0, score=32192.752415, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.276153, test/num_examples=43793, total_duration=49191.247726, train/accuracy=0.995525, train/loss=0.013962, train/mean_average_precision=0.770412, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286485, validation/num_examples=43793
I0311 16:54:35.804703 139628679296768 logging_writer.py:48] [102400] global_step=102400, grad_norm=0.14363330602645874, loss=0.017498428001999855
I0311 16:55:07.357995 139622172112640 logging_writer.py:48] [102500] global_step=102500, grad_norm=0.1403442919254303, loss=0.016363050788640976
I0311 16:55:38.997472 139628679296768 logging_writer.py:48] [102600] global_step=102600, grad_norm=0.12955431640148163, loss=0.016862256452441216
I0311 16:56:10.672100 139622172112640 logging_writer.py:48] [102700] global_step=102700, grad_norm=0.13184574246406555, loss=0.017720306292176247
I0311 16:56:42.353578 139628679296768 logging_writer.py:48] [102800] global_step=102800, grad_norm=0.14911773800849915, loss=0.018076734617352486
I0311 16:57:13.845911 139622172112640 logging_writer.py:48] [102900] global_step=102900, grad_norm=0.14364518225193024, loss=0.01777416281402111
I0311 16:57:45.586992 139628679296768 logging_writer.py:48] [103000] global_step=103000, grad_norm=0.13352559506893158, loss=0.015578052029013634
I0311 16:58:17.298513 139622172112640 logging_writer.py:48] [103100] global_step=103100, grad_norm=0.16065256297588348, loss=0.017151257023215294
I0311 16:58:35.608161 139789500200768 spec.py:321] Evaluating on the training split.
I0311 17:00:25.938281 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 17:00:28.986159 139789500200768 spec.py:349] Evaluating on the test split.
I0311 17:00:32.027926 139789500200768 submission_runner.py:411] Time since start: 49547.86s, 	Step: 103158, 	{'train/accuracy': 0.9955227375030518, 'train/loss': 0.01404392346739769, 'train/mean_average_precision': 0.773106773764879, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.2867109029952526, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.27599418885740407, 'test/num_examples': 43793, 'score': 32432.875032186508, 'total_duration': 49547.85517024994, 'accumulated_submission_time': 32432.875032186508, 'accumulated_eval_time': 17107.008259534836, 'accumulated_logging_time': 5.2788918018341064}
I0311 17:00:32.062872 139621870597888 logging_writer.py:48] [103158] accumulated_eval_time=17107.008260, accumulated_logging_time=5.278892, accumulated_submission_time=32432.875032, global_step=103158, preemption_count=0, score=32432.875032, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.275994, test/num_examples=43793, total_duration=49547.855170, train/accuracy=0.995523, train/loss=0.014044, train/mean_average_precision=0.773107, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286711, validation/num_examples=43793
I0311 17:00:45.795402 139628670904064 logging_writer.py:48] [103200] global_step=103200, grad_norm=0.1341940462589264, loss=0.01739625260233879
I0311 17:01:17.026167 139621870597888 logging_writer.py:48] [103300] global_step=103300, grad_norm=0.1498204469680786, loss=0.0192592553794384
I0311 17:01:48.649426 139628670904064 logging_writer.py:48] [103400] global_step=103400, grad_norm=0.13107042014598846, loss=0.017618374899029732
I0311 17:02:20.267438 139621870597888 logging_writer.py:48] [103500] global_step=103500, grad_norm=0.1448456346988678, loss=0.017474938184022903
I0311 17:02:51.948915 139628670904064 logging_writer.py:48] [103600] global_step=103600, grad_norm=0.1400473266839981, loss=0.018356265500187874
I0311 17:03:23.389988 139621870597888 logging_writer.py:48] [103700] global_step=103700, grad_norm=0.1386118084192276, loss=0.018118686974048615
I0311 17:03:54.735764 139628670904064 logging_writer.py:48] [103800] global_step=103800, grad_norm=0.19460414350032806, loss=0.017230181023478508
I0311 17:04:26.363436 139621870597888 logging_writer.py:48] [103900] global_step=103900, grad_norm=0.1542075127363205, loss=0.019072428345680237
I0311 17:04:32.050926 139789500200768 spec.py:321] Evaluating on the training split.
I0311 17:06:17.181378 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 17:06:20.190119 139789500200768 spec.py:349] Evaluating on the test split.
I0311 17:06:23.185885 139789500200768 submission_runner.py:411] Time since start: 49899.01s, 	Step: 103919, 	{'train/accuracy': 0.9955666065216064, 'train/loss': 0.013838540762662888, 'train/mean_average_precision': 0.7811419869738975, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.2865256056236226, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.2759413262208593, 'test/num_examples': 43793, 'score': 32672.832125663757, 'total_duration': 49899.01314020157, 'accumulated_submission_time': 32672.832125663757, 'accumulated_eval_time': 17218.14317536354, 'accumulated_logging_time': 5.324789047241211}
I0311 17:06:23.220624 139622180505344 logging_writer.py:48] [103919] accumulated_eval_time=17218.143175, accumulated_logging_time=5.324789, accumulated_submission_time=32672.832126, global_step=103919, preemption_count=0, score=32672.832126, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.275941, test/num_examples=43793, total_duration=49899.013140, train/accuracy=0.995567, train/loss=0.013839, train/mean_average_precision=0.781142, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286526, validation/num_examples=43793
I0311 17:06:48.923504 139628679296768 logging_writer.py:48] [104000] global_step=104000, grad_norm=0.14251463115215302, loss=0.017828356474637985
I0311 17:07:20.267138 139622180505344 logging_writer.py:48] [104100] global_step=104100, grad_norm=0.14874432981014252, loss=0.01727447472512722
I0311 17:07:51.484412 139628679296768 logging_writer.py:48] [104200] global_step=104200, grad_norm=0.1492161899805069, loss=0.017040858045220375
I0311 17:08:22.750383 139622180505344 logging_writer.py:48] [104300] global_step=104300, grad_norm=0.13751298189163208, loss=0.016503723338246346
I0311 17:08:53.716206 139628679296768 logging_writer.py:48] [104400] global_step=104400, grad_norm=0.12938399612903595, loss=0.016208941116929054
I0311 17:09:24.753623 139622180505344 logging_writer.py:48] [104500] global_step=104500, grad_norm=0.13431581854820251, loss=0.018167763948440552
I0311 17:09:56.118105 139628679296768 logging_writer.py:48] [104600] global_step=104600, grad_norm=0.13093215227127075, loss=0.01625317707657814
I0311 17:10:23.198714 139789500200768 spec.py:321] Evaluating on the training split.
I0311 17:12:17.704272 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 17:12:20.761811 139789500200768 spec.py:349] Evaluating on the test split.
I0311 17:12:23.738365 139789500200768 submission_runner.py:411] Time since start: 50259.57s, 	Step: 104688, 	{'train/accuracy': 0.9955680966377258, 'train/loss': 0.013852370902895927, 'train/mean_average_precision': 0.7850848461487768, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.28656039860562493, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.2759689009568245, 'test/num_examples': 43793, 'score': 32912.77941918373, 'total_duration': 50259.56562113762, 'accumulated_submission_time': 32912.77941918373, 'accumulated_eval_time': 17338.682789564133, 'accumulated_logging_time': 5.370456695556641}
I0311 17:12:23.772634 139621870597888 logging_writer.py:48] [104688] accumulated_eval_time=17338.682790, accumulated_logging_time=5.370457, accumulated_submission_time=32912.779419, global_step=104688, preemption_count=0, score=32912.779419, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.275969, test/num_examples=43793, total_duration=50259.565621, train/accuracy=0.995568, train/loss=0.013852, train/mean_average_precision=0.785085, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286560, validation/num_examples=43793
I0311 17:12:27.833316 139622172112640 logging_writer.py:48] [104700] global_step=104700, grad_norm=0.15229550004005432, loss=0.0170336551964283
I0311 17:12:58.972628 139621870597888 logging_writer.py:48] [104800] global_step=104800, grad_norm=0.15870718657970428, loss=0.019259287044405937
I0311 17:13:30.877994 139622172112640 logging_writer.py:48] [104900] global_step=104900, grad_norm=0.16316069662570953, loss=0.022391118109226227
I0311 17:14:03.017087 139621870597888 logging_writer.py:48] [105000] global_step=105000, grad_norm=0.14429503679275513, loss=0.018789976835250854
I0311 17:14:35.015806 139622172112640 logging_writer.py:48] [105100] global_step=105100, grad_norm=0.13934966921806335, loss=0.019886232912540436
I0311 17:15:06.815810 139621870597888 logging_writer.py:48] [105200] global_step=105200, grad_norm=0.15954162180423737, loss=0.019325433298945427
I0311 17:15:38.244951 139622172112640 logging_writer.py:48] [105300] global_step=105300, grad_norm=0.1472567617893219, loss=0.017763106152415276
I0311 17:16:09.461256 139621870597888 logging_writer.py:48] [105400] global_step=105400, grad_norm=0.13075962662696838, loss=0.016909634694457054
I0311 17:16:23.986053 139789500200768 spec.py:321] Evaluating on the training split.
I0311 17:18:17.360610 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 17:18:20.411524 139789500200768 spec.py:349] Evaluating on the test split.
I0311 17:18:23.451651 139789500200768 submission_runner.py:411] Time since start: 50619.28s, 	Step: 105447, 	{'train/accuracy': 0.9955039620399475, 'train/loss': 0.013995528221130371, 'train/mean_average_precision': 0.7774106373615576, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.28652165998540224, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.27605249639452245, 'test/num_examples': 43793, 'score': 33152.962929964066, 'total_duration': 50619.27890300751, 'accumulated_submission_time': 33152.962929964066, 'accumulated_eval_time': 17458.148350954056, 'accumulated_logging_time': 5.415530681610107}
I0311 17:18:23.486654 139622180505344 logging_writer.py:48] [105447] accumulated_eval_time=17458.148351, accumulated_logging_time=5.415531, accumulated_submission_time=33152.962930, global_step=105447, preemption_count=0, score=33152.962930, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.276052, test/num_examples=43793, total_duration=50619.278903, train/accuracy=0.995504, train/loss=0.013996, train/mean_average_precision=0.777411, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286522, validation/num_examples=43793
I0311 17:18:40.572272 139628679296768 logging_writer.py:48] [105500] global_step=105500, grad_norm=0.1407560408115387, loss=0.0181027390062809
I0311 17:19:12.739279 139622180505344 logging_writer.py:48] [105600] global_step=105600, grad_norm=0.1371437907218933, loss=0.017518455162644386
I0311 17:19:44.744581 139628679296768 logging_writer.py:48] [105700] global_step=105700, grad_norm=0.159873828291893, loss=0.019140291959047318
I0311 17:20:17.729282 139622180505344 logging_writer.py:48] [105800] global_step=105800, grad_norm=0.14779432117938995, loss=0.018725130707025528
I0311 17:20:50.859565 139628679296768 logging_writer.py:48] [105900] global_step=105900, grad_norm=0.14494165778160095, loss=0.017950911074876785
I0311 17:21:23.840205 139622180505344 logging_writer.py:48] [106000] global_step=106000, grad_norm=0.15593737363815308, loss=0.018835319206118584
I0311 17:21:57.142171 139628679296768 logging_writer.py:48] [106100] global_step=106100, grad_norm=0.14141923189163208, loss=0.01949489489197731
I0311 17:22:23.458518 139789500200768 spec.py:321] Evaluating on the training split.
I0311 17:24:20.563706 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 17:24:23.998594 139789500200768 spec.py:349] Evaluating on the test split.
I0311 17:24:27.452010 139789500200768 submission_runner.py:411] Time since start: 50983.28s, 	Step: 106180, 	{'train/accuracy': 0.9955573678016663, 'train/loss': 0.013854365795850754, 'train/mean_average_precision': 0.7817870350219891, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.28664194307397634, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.2759944644891478, 'test/num_examples': 43793, 'score': 33392.90033054352, 'total_duration': 50983.27924466133, 'accumulated_submission_time': 33392.90033054352, 'accumulated_eval_time': 17582.141822576523, 'accumulated_logging_time': 5.462770223617554}
I0311 17:24:27.494557 139621870597888 logging_writer.py:48] [106180] accumulated_eval_time=17582.141823, accumulated_logging_time=5.462770, accumulated_submission_time=33392.900331, global_step=106180, preemption_count=0, score=33392.900331, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.275994, test/num_examples=43793, total_duration=50983.279245, train/accuracy=0.995557, train/loss=0.013854, train/mean_average_precision=0.781787, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286642, validation/num_examples=43793
I0311 17:24:34.486756 139622172112640 logging_writer.py:48] [106200] global_step=106200, grad_norm=0.15333759784698486, loss=0.018395544961094856
I0311 17:25:07.322546 139621870597888 logging_writer.py:48] [106300] global_step=106300, grad_norm=0.13103342056274414, loss=0.018621275201439857
I0311 17:25:40.375429 139622172112640 logging_writer.py:48] [106400] global_step=106400, grad_norm=0.1483517736196518, loss=0.019457485526800156
I0311 17:26:13.138981 139621870597888 logging_writer.py:48] [106500] global_step=106500, grad_norm=0.1541869342327118, loss=0.019526246935129166
I0311 17:26:46.167242 139622172112640 logging_writer.py:48] [106600] global_step=106600, grad_norm=0.1370464414358139, loss=0.016263453289866447
I0311 17:27:18.152602 139621870597888 logging_writer.py:48] [106700] global_step=106700, grad_norm=0.1507401317358017, loss=0.020771564915776253
I0311 17:27:50.405495 139622172112640 logging_writer.py:48] [106800] global_step=106800, grad_norm=0.1495266854763031, loss=0.017264973372220993
I0311 17:28:22.200864 139621870597888 logging_writer.py:48] [106900] global_step=106900, grad_norm=0.15672974288463593, loss=0.019330862909555435
I0311 17:28:27.648045 139789500200768 spec.py:321] Evaluating on the training split.
I0311 17:30:18.223443 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 17:30:21.260945 139789500200768 spec.py:349] Evaluating on the test split.
I0311 17:30:24.237792 139789500200768 submission_runner.py:411] Time since start: 51340.07s, 	Step: 106918, 	{'train/accuracy': 0.9955280423164368, 'train/loss': 0.014052228070795536, 'train/mean_average_precision': 0.7661217680770456, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.28655629138775107, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426059290766716, 'test/mean_average_precision': 0.27596062679952144, 'test/num_examples': 43793, 'score': 33633.02038526535, 'total_duration': 51340.06503534317, 'accumulated_submission_time': 33633.02038526535, 'accumulated_eval_time': 17698.731519460678, 'accumulated_logging_time': 5.51738166809082}
I0311 17:30:24.273150 139628670904064 logging_writer.py:48] [106918] accumulated_eval_time=17698.731519, accumulated_logging_time=5.517382, accumulated_submission_time=33633.020385, global_step=106918, preemption_count=0, score=33633.020385, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.275961, test/num_examples=43793, total_duration=51340.065035, train/accuracy=0.995528, train/loss=0.014052, train/mean_average_precision=0.766122, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286556, validation/num_examples=43793
I0311 17:30:50.169816 139628679296768 logging_writer.py:48] [107000] global_step=107000, grad_norm=0.13820649683475494, loss=0.017387814819812775
I0311 17:31:21.601709 139628670904064 logging_writer.py:48] [107100] global_step=107100, grad_norm=0.14419907331466675, loss=0.018703626468777657
I0311 17:31:52.909347 139628679296768 logging_writer.py:48] [107200] global_step=107200, grad_norm=0.14206252992153168, loss=0.017459699884057045
I0311 17:32:24.550355 139628670904064 logging_writer.py:48] [107300] global_step=107300, grad_norm=0.1518271416425705, loss=0.019712308421730995
I0311 17:32:56.340515 139628679296768 logging_writer.py:48] [107400] global_step=107400, grad_norm=0.13776937127113342, loss=0.016678782179951668
I0311 17:33:28.068171 139628670904064 logging_writer.py:48] [107500] global_step=107500, grad_norm=0.1383821964263916, loss=0.014421912841498852
I0311 17:33:59.945381 139628679296768 logging_writer.py:48] [107600] global_step=107600, grad_norm=0.13979367911815643, loss=0.017103590071201324
I0311 17:34:24.237885 139789500200768 spec.py:321] Evaluating on the training split.
I0311 17:36:13.995561 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 17:36:17.018651 139789500200768 spec.py:349] Evaluating on the test split.
I0311 17:36:19.967219 139789500200768 submission_runner.py:411] Time since start: 51695.79s, 	Step: 107677, 	{'train/accuracy': 0.9955880641937256, 'train/loss': 0.013822034001350403, 'train/mean_average_precision': 0.779899470061354, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.2865691722887861, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426059290766716, 'test/mean_average_precision': 0.27601527500498824, 'test/num_examples': 43793, 'score': 33872.95367479324, 'total_duration': 51695.79447507858, 'accumulated_submission_time': 33872.95367479324, 'accumulated_eval_time': 17814.46081638336, 'accumulated_logging_time': 5.564499616622925}
I0311 17:36:20.002715 139621870597888 logging_writer.py:48] [107677] accumulated_eval_time=17814.460816, accumulated_logging_time=5.564500, accumulated_submission_time=33872.953675, global_step=107677, preemption_count=0, score=33872.953675, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.276015, test/num_examples=43793, total_duration=51695.794475, train/accuracy=0.995588, train/loss=0.013822, train/mean_average_precision=0.779899, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286569, validation/num_examples=43793
I0311 17:36:27.823758 139622172112640 logging_writer.py:48] [107700] global_step=107700, grad_norm=0.1412135660648346, loss=0.018174026161432266
I0311 17:36:58.839802 139621870597888 logging_writer.py:48] [107800] global_step=107800, grad_norm=0.13530489802360535, loss=0.016225453466176987
I0311 17:37:30.037142 139622172112640 logging_writer.py:48] [107900] global_step=107900, grad_norm=0.16686370968818665, loss=0.01934252493083477
I0311 17:38:01.914341 139621870597888 logging_writer.py:48] [108000] global_step=108000, grad_norm=0.13703903555870056, loss=0.01671702042222023
I0311 17:38:33.410602 139622172112640 logging_writer.py:48] [108100] global_step=108100, grad_norm=0.13943395018577576, loss=0.017845869064331055
I0311 17:39:04.964630 139621870597888 logging_writer.py:48] [108200] global_step=108200, grad_norm=0.1444694846868515, loss=0.017915183678269386
I0311 17:39:36.347016 139622172112640 logging_writer.py:48] [108300] global_step=108300, grad_norm=0.13515962660312653, loss=0.0169113427400589
I0311 17:40:07.643800 139621870597888 logging_writer.py:48] [108400] global_step=108400, grad_norm=0.13359729945659637, loss=0.01556491944938898
I0311 17:40:20.244239 139789500200768 spec.py:321] Evaluating on the training split.
I0311 17:42:07.165102 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 17:42:10.197372 139789500200768 spec.py:349] Evaluating on the test split.
I0311 17:42:13.151811 139789500200768 submission_runner.py:411] Time since start: 52048.98s, 	Step: 108441, 	{'train/accuracy': 0.9955666065216064, 'train/loss': 0.013839383609592915, 'train/mean_average_precision': 0.7841374720767258, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.2864342504751349, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.2759831511515541, 'test/num_examples': 43793, 'score': 34113.1648209095, 'total_duration': 52048.97903227806, 'accumulated_submission_time': 34113.1648209095, 'accumulated_eval_time': 17927.368314504623, 'accumulated_logging_time': 5.611015796661377}
I0311 17:42:13.186521 139622180505344 logging_writer.py:48] [108441] accumulated_eval_time=17927.368315, accumulated_logging_time=5.611016, accumulated_submission_time=34113.164821, global_step=108441, preemption_count=0, score=34113.164821, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.275983, test/num_examples=43793, total_duration=52048.979032, train/accuracy=0.995567, train/loss=0.013839, train/mean_average_precision=0.784137, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286434, validation/num_examples=43793
I0311 17:42:32.114961 139628679296768 logging_writer.py:48] [108500] global_step=108500, grad_norm=0.13480369746685028, loss=0.017627697438001633
I0311 17:43:03.855981 139622180505344 logging_writer.py:48] [108600] global_step=108600, grad_norm=0.14832563698291779, loss=0.018529318273067474
I0311 17:43:35.347624 139628679296768 logging_writer.py:48] [108700] global_step=108700, grad_norm=0.13435938954353333, loss=0.0171941127628088
I0311 17:44:06.561815 139622180505344 logging_writer.py:48] [108800] global_step=108800, grad_norm=0.1566983163356781, loss=0.018583059310913086
I0311 17:44:37.832944 139628679296768 logging_writer.py:48] [108900] global_step=108900, grad_norm=0.1462988704442978, loss=0.016728827729821205
I0311 17:45:09.445634 139622180505344 logging_writer.py:48] [109000] global_step=109000, grad_norm=0.13355162739753723, loss=0.01757078804075718
I0311 17:45:40.893942 139628679296768 logging_writer.py:48] [109100] global_step=109100, grad_norm=0.16140010952949524, loss=0.01640862226486206
I0311 17:46:12.386836 139622180505344 logging_writer.py:48] [109200] global_step=109200, grad_norm=0.1336275339126587, loss=0.017221376299858093
I0311 17:46:13.334085 139789500200768 spec.py:321] Evaluating on the training split.
I0311 17:48:00.287532 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 17:48:03.312247 139789500200768 spec.py:349] Evaluating on the test split.
I0311 17:48:06.319794 139789500200768 submission_runner.py:411] Time since start: 52402.15s, 	Step: 109204, 	{'train/accuracy': 0.9955072999000549, 'train/loss': 0.01401020959019661, 'train/mean_average_precision': 0.781538035227237, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.28650508182639306, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.27600523660196896, 'test/num_examples': 43793, 'score': 34353.28302168846, 'total_duration': 52402.147035598755, 'accumulated_submission_time': 34353.28302168846, 'accumulated_eval_time': 18040.35396838188, 'accumulated_logging_time': 5.656085968017578}
I0311 17:48:06.356076 139622172112640 logging_writer.py:48] [109204] accumulated_eval_time=18040.353968, accumulated_logging_time=5.656086, accumulated_submission_time=34353.283022, global_step=109204, preemption_count=0, score=34353.283022, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.276005, test/num_examples=43793, total_duration=52402.147036, train/accuracy=0.995507, train/loss=0.014010, train/mean_average_precision=0.781538, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286505, validation/num_examples=43793
I0311 17:48:37.237278 139628670904064 logging_writer.py:48] [109300] global_step=109300, grad_norm=0.12707273662090302, loss=0.015118280425667763
I0311 17:49:09.066687 139622172112640 logging_writer.py:48] [109400] global_step=109400, grad_norm=0.15964394807815552, loss=0.017121149227023125
I0311 17:49:40.835544 139628670904064 logging_writer.py:48] [109500] global_step=109500, grad_norm=0.169251948595047, loss=0.01845589466392994
I0311 17:50:13.397940 139622172112640 logging_writer.py:48] [109600] global_step=109600, grad_norm=0.13629969954490662, loss=0.017034726217389107
I0311 17:50:46.100056 139628670904064 logging_writer.py:48] [109700] global_step=109700, grad_norm=0.15547415614128113, loss=0.017528031021356583
I0311 17:51:18.880603 139622172112640 logging_writer.py:48] [109800] global_step=109800, grad_norm=0.15445269644260406, loss=0.018629832193255424
I0311 17:51:51.507243 139628670904064 logging_writer.py:48] [109900] global_step=109900, grad_norm=0.14595873653888702, loss=0.0173735860735178
I0311 17:52:06.360124 139789500200768 spec.py:321] Evaluating on the training split.
I0311 17:54:07.495779 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 17:54:11.010175 139789500200768 spec.py:349] Evaluating on the test split.
I0311 17:54:14.481100 139789500200768 submission_runner.py:411] Time since start: 52770.31s, 	Step: 109946, 	{'train/accuracy': 0.9955655932426453, 'train/loss': 0.013794494792819023, 'train/mean_average_precision': 0.7802740098906389, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.28657301579377276, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.27601604227510423, 'test/num_examples': 43793, 'score': 34593.253633499146, 'total_duration': 52770.308326005936, 'accumulated_submission_time': 34593.253633499146, 'accumulated_eval_time': 18168.47488641739, 'accumulated_logging_time': 5.704323053359985}
I0311 17:54:14.523649 139622180505344 logging_writer.py:48] [109946] accumulated_eval_time=18168.474886, accumulated_logging_time=5.704323, accumulated_submission_time=34593.253633, global_step=109946, preemption_count=0, score=34593.253633, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.276016, test/num_examples=43793, total_duration=52770.308326, train/accuracy=0.995566, train/loss=0.013794, train/mean_average_precision=0.780274, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286573, validation/num_examples=43793
I0311 17:54:32.762590 139628679296768 logging_writer.py:48] [110000] global_step=110000, grad_norm=0.14331859350204468, loss=0.017246002331376076
I0311 17:55:05.862101 139622180505344 logging_writer.py:48] [110100] global_step=110100, grad_norm=0.14395266771316528, loss=0.017083432525396347
I0311 17:55:38.661795 139628679296768 logging_writer.py:48] [110200] global_step=110200, grad_norm=0.13847611844539642, loss=0.016442421823740005
I0311 17:56:11.382821 139622180505344 logging_writer.py:48] [110300] global_step=110300, grad_norm=0.1488649696111679, loss=0.021306166425347328
I0311 17:56:43.083029 139628679296768 logging_writer.py:48] [110400] global_step=110400, grad_norm=0.13558447360992432, loss=0.018682537600398064
I0311 17:57:14.813011 139622180505344 logging_writer.py:48] [110500] global_step=110500, grad_norm=0.15634344518184662, loss=0.020655866712331772
I0311 17:57:46.727308 139628679296768 logging_writer.py:48] [110600] global_step=110600, grad_norm=0.13001702725887299, loss=0.015822086483240128
I0311 17:58:14.533731 139789500200768 spec.py:321] Evaluating on the training split.
I0311 18:00:08.424074 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 18:00:11.516623 139789500200768 spec.py:349] Evaluating on the test split.
I0311 18:00:14.668784 139789500200768 submission_runner.py:411] Time since start: 53130.50s, 	Step: 110688, 	{'train/accuracy': 0.9955146312713623, 'train/loss': 0.01403243001550436, 'train/mean_average_precision': 0.7722557395643874, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.28671282761451994, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.27594933128037413, 'test/num_examples': 43793, 'score': 34833.229488134384, 'total_duration': 53130.4960372448, 'accumulated_submission_time': 34833.229488134384, 'accumulated_eval_time': 18288.60990166664, 'accumulated_logging_time': 5.759541034698486}
I0311 18:00:14.704847 139621870597888 logging_writer.py:48] [110688] accumulated_eval_time=18288.609902, accumulated_logging_time=5.759541, accumulated_submission_time=34833.229488, global_step=110688, preemption_count=0, score=34833.229488, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.275949, test/num_examples=43793, total_duration=53130.496037, train/accuracy=0.995515, train/loss=0.014032, train/mean_average_precision=0.772256, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286713, validation/num_examples=43793
I0311 18:00:18.959089 139628670904064 logging_writer.py:48] [110700] global_step=110700, grad_norm=0.1341012418270111, loss=0.015872405841946602
I0311 18:00:51.842170 139621870597888 logging_writer.py:48] [110800] global_step=110800, grad_norm=0.1498676836490631, loss=0.01893731765449047
I0311 18:01:23.522271 139628670904064 logging_writer.py:48] [110900] global_step=110900, grad_norm=0.14762158691883087, loss=0.01771477982401848
I0311 18:01:55.417230 139621870597888 logging_writer.py:48] [111000] global_step=111000, grad_norm=0.14595060050487518, loss=0.01928229071199894
I0311 18:02:27.244840 139628670904064 logging_writer.py:48] [111100] global_step=111100, grad_norm=0.15514910221099854, loss=0.016225291416049004
I0311 18:02:58.995192 139621870597888 logging_writer.py:48] [111200] global_step=111200, grad_norm=0.16338714957237244, loss=0.02038334682583809
I0311 18:03:30.397181 139628670904064 logging_writer.py:48] [111300] global_step=111300, grad_norm=0.16206514835357666, loss=0.01962081529200077
I0311 18:04:02.236283 139621870597888 logging_writer.py:48] [111400] global_step=111400, grad_norm=0.14024344086647034, loss=0.017334584146738052
I0311 18:04:14.832704 139789500200768 spec.py:321] Evaluating on the training split.
I0311 18:06:04.356858 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 18:06:07.413443 139789500200768 spec.py:349] Evaluating on the test split.
I0311 18:06:10.432580 139789500200768 submission_runner.py:411] Time since start: 53486.26s, 	Step: 111441, 	{'train/accuracy': 0.9955356121063232, 'train/loss': 0.014004101045429707, 'train/mean_average_precision': 0.7818724497668679, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.28648287570078523, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.2759526438099676, 'test/num_examples': 43793, 'score': 35073.32724404335, 'total_duration': 53486.25983309746, 'accumulated_submission_time': 35073.32724404335, 'accumulated_eval_time': 18404.20976114273, 'accumulated_logging_time': 5.80646538734436}
I0311 18:06:10.468573 139622172112640 logging_writer.py:48] [111441] accumulated_eval_time=18404.209761, accumulated_logging_time=5.806465, accumulated_submission_time=35073.327244, global_step=111441, preemption_count=0, score=35073.327244, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.275953, test/num_examples=43793, total_duration=53486.259833, train/accuracy=0.995536, train/loss=0.014004, train/mean_average_precision=0.781872, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286483, validation/num_examples=43793
I0311 18:06:29.411895 139622180505344 logging_writer.py:48] [111500] global_step=111500, grad_norm=0.12730708718299866, loss=0.015372339636087418
I0311 18:07:00.773081 139622172112640 logging_writer.py:48] [111600] global_step=111600, grad_norm=0.1354377269744873, loss=0.01793035864830017
I0311 18:07:32.586802 139622180505344 logging_writer.py:48] [111700] global_step=111700, grad_norm=0.14220967888832092, loss=0.01629016548395157
I0311 18:08:04.019038 139622172112640 logging_writer.py:48] [111800] global_step=111800, grad_norm=0.1355414092540741, loss=0.017040591686964035
I0311 18:08:35.346539 139622180505344 logging_writer.py:48] [111900] global_step=111900, grad_norm=0.1630122810602188, loss=0.020177755504846573
I0311 18:09:06.917554 139622172112640 logging_writer.py:48] [112000] global_step=112000, grad_norm=0.16257312893867493, loss=0.020444676280021667
I0311 18:09:38.038249 139622180505344 logging_writer.py:48] [112100] global_step=112100, grad_norm=0.13394781947135925, loss=0.015681462362408638
I0311 18:10:09.282639 139622172112640 logging_writer.py:48] [112200] global_step=112200, grad_norm=0.14616109430789948, loss=0.017805421724915504
I0311 18:10:10.531641 139789500200768 spec.py:321] Evaluating on the training split.
I0311 18:12:01.630503 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 18:12:06.363824 139789500200768 spec.py:349] Evaluating on the test split.
I0311 18:12:09.398357 139789500200768 submission_runner.py:411] Time since start: 53845.23s, 	Step: 112205, 	{'train/accuracy': 0.9955651760101318, 'train/loss': 0.013843481428921223, 'train/mean_average_precision': 0.7786537040391218, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.28645586817474816, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.276035341605793, 'test/num_examples': 43793, 'score': 35313.35783934593, 'total_duration': 53845.22560930252, 'accumulated_submission_time': 35313.35783934593, 'accumulated_eval_time': 18523.07643556595, 'accumulated_logging_time': 5.854486465454102}
I0311 18:12:09.435363 139621870597888 logging_writer.py:48] [112205] accumulated_eval_time=18523.076436, accumulated_logging_time=5.854486, accumulated_submission_time=35313.357839, global_step=112205, preemption_count=0, score=35313.357839, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.276035, test/num_examples=43793, total_duration=53845.225609, train/accuracy=0.995565, train/loss=0.013843, train/mean_average_precision=0.778654, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286456, validation/num_examples=43793
I0311 18:12:39.391098 139628679296768 logging_writer.py:48] [112300] global_step=112300, grad_norm=0.14909105002880096, loss=0.018056640401482582
I0311 18:13:11.107169 139621870597888 logging_writer.py:48] [112400] global_step=112400, grad_norm=0.13129523396492004, loss=0.017918817698955536
I0311 18:13:42.403157 139628679296768 logging_writer.py:48] [112500] global_step=112500, grad_norm=0.14740236103534698, loss=0.016618894413113594
I0311 18:14:13.942769 139621870597888 logging_writer.py:48] [112600] global_step=112600, grad_norm=0.1318434774875641, loss=0.017454057931900024
I0311 18:14:45.733918 139628679296768 logging_writer.py:48] [112700] global_step=112700, grad_norm=0.1392955780029297, loss=0.018637390807271004
I0311 18:15:17.138921 139621870597888 logging_writer.py:48] [112800] global_step=112800, grad_norm=0.12011022865772247, loss=0.016014451161026955
I0311 18:15:48.634570 139628679296768 logging_writer.py:48] [112900] global_step=112900, grad_norm=0.14124129712581635, loss=0.01842867210507393
I0311 18:16:09.401281 139789500200768 spec.py:321] Evaluating on the training split.
I0311 18:17:55.924592 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 18:17:58.944115 139789500200768 spec.py:349] Evaluating on the test split.
I0311 18:18:02.007425 139789500200768 submission_runner.py:411] Time since start: 54197.83s, 	Step: 112967, 	{'train/accuracy': 0.9955306053161621, 'train/loss': 0.013974021188914776, 'train/mean_average_precision': 0.7914633546862597, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.28660724281595434, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.27603297388379083, 'test/num_examples': 43793, 'score': 35553.29326963425, 'total_duration': 54197.83467626572, 'accumulated_submission_time': 35553.29326963425, 'accumulated_eval_time': 18635.68253469467, 'accumulated_logging_time': 5.9019293785095215}
I0311 18:18:02.043956 139622172112640 logging_writer.py:48] [112967] accumulated_eval_time=18635.682535, accumulated_logging_time=5.901929, accumulated_submission_time=35553.293270, global_step=112967, preemption_count=0, score=35553.293270, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.276033, test/num_examples=43793, total_duration=54197.834676, train/accuracy=0.995531, train/loss=0.013974, train/mean_average_precision=0.791463, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286607, validation/num_examples=43793
I0311 18:18:12.705422 139622180505344 logging_writer.py:48] [113000] global_step=113000, grad_norm=0.1364651769399643, loss=0.018696537241339684
I0311 18:18:44.015822 139622172112640 logging_writer.py:48] [113100] global_step=113100, grad_norm=0.12924160063266754, loss=0.016600847244262695
I0311 18:19:15.402696 139622180505344 logging_writer.py:48] [113200] global_step=113200, grad_norm=0.13245423138141632, loss=0.01823371648788452
I0311 18:19:46.689204 139622172112640 logging_writer.py:48] [113300] global_step=113300, grad_norm=0.14638619124889374, loss=0.01808846928179264
I0311 18:20:17.892527 139622180505344 logging_writer.py:48] [113400] global_step=113400, grad_norm=0.1369786411523819, loss=0.015043762512505054
I0311 18:20:50.034682 139622172112640 logging_writer.py:48] [113500] global_step=113500, grad_norm=0.15014345943927765, loss=0.01708080992102623
I0311 18:21:21.404495 139622180505344 logging_writer.py:48] [113600] global_step=113600, grad_norm=0.17557859420776367, loss=0.019497517496347427
I0311 18:21:52.706585 139622172112640 logging_writer.py:48] [113700] global_step=113700, grad_norm=0.17731623351573944, loss=0.01912948302924633
I0311 18:22:02.127065 139789500200768 spec.py:321] Evaluating on the training split.
I0311 18:23:45.482263 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 18:23:48.570739 139789500200768 spec.py:349] Evaluating on the test split.
I0311 18:23:53.324044 139789500200768 submission_runner.py:411] Time since start: 54549.15s, 	Step: 113731, 	{'train/accuracy': 0.9955313205718994, 'train/loss': 0.013878352008759975, 'train/mean_average_precision': 0.7689000334908849, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.28664612908889037, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.2759876579915408, 'test/num_examples': 43793, 'score': 35793.34585046768, 'total_duration': 54549.151299238205, 'accumulated_submission_time': 35793.34585046768, 'accumulated_eval_time': 18746.879474401474, 'accumulated_logging_time': 5.949110984802246}
I0311 18:23:53.362288 139621870597888 logging_writer.py:48] [113731] accumulated_eval_time=18746.879474, accumulated_logging_time=5.949111, accumulated_submission_time=35793.345850, global_step=113731, preemption_count=0, score=35793.345850, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.275988, test/num_examples=43793, total_duration=54549.151299, train/accuracy=0.995531, train/loss=0.013878, train/mean_average_precision=0.768900, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286646, validation/num_examples=43793
I0311 18:24:15.469215 139628679296768 logging_writer.py:48] [113800] global_step=113800, grad_norm=0.1418069750070572, loss=0.018123546615242958
I0311 18:24:47.202924 139621870597888 logging_writer.py:48] [113900] global_step=113900, grad_norm=0.13730394840240479, loss=0.018100827932357788
I0311 18:25:18.604858 139628679296768 logging_writer.py:48] [114000] global_step=114000, grad_norm=0.16550737619400024, loss=0.01847541145980358
I0311 18:25:50.402350 139621870597888 logging_writer.py:48] [114100] global_step=114100, grad_norm=0.14780038595199585, loss=0.01990617625415325
I0311 18:26:21.950237 139628679296768 logging_writer.py:48] [114200] global_step=114200, grad_norm=0.17678074538707733, loss=0.018632179126143456
I0311 18:26:53.551714 139621870597888 logging_writer.py:48] [114300] global_step=114300, grad_norm=0.1402750164270401, loss=0.017672482877969742
I0311 18:27:25.545010 139628679296768 logging_writer.py:48] [114400] global_step=114400, grad_norm=0.15717829763889313, loss=0.018044505268335342
I0311 18:27:53.327856 139789500200768 spec.py:321] Evaluating on the training split.
I0311 18:29:43.121022 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 18:29:46.238020 139789500200768 spec.py:349] Evaluating on the test split.
I0311 18:29:49.229596 139789500200768 submission_runner.py:411] Time since start: 54905.06s, 	Step: 114488, 	{'train/accuracy': 0.9955372214317322, 'train/loss': 0.01393947284668684, 'train/mean_average_precision': 0.7815741067321347, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.2865367019489142, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.2760602507676384, 'test/num_examples': 43793, 'score': 36033.280943632126, 'total_duration': 54905.05683708191, 'accumulated_submission_time': 36033.280943632126, 'accumulated_eval_time': 18862.78116440773, 'accumulated_logging_time': 5.997893810272217}
I0311 18:29:49.267367 139622172112640 logging_writer.py:48] [114488] accumulated_eval_time=18862.781164, accumulated_logging_time=5.997894, accumulated_submission_time=36033.280944, global_step=114488, preemption_count=0, score=36033.280944, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.276060, test/num_examples=43793, total_duration=54905.056837, train/accuracy=0.995537, train/loss=0.013939, train/mean_average_precision=0.781574, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286537, validation/num_examples=43793
I0311 18:29:53.388464 139628670904064 logging_writer.py:48] [114500] global_step=114500, grad_norm=0.14702507853507996, loss=0.01757328025996685
I0311 18:30:24.550298 139622172112640 logging_writer.py:48] [114600] global_step=114600, grad_norm=0.14402753114700317, loss=0.01809241995215416
I0311 18:30:55.775600 139628670904064 logging_writer.py:48] [114700] global_step=114700, grad_norm=0.15846717357635498, loss=0.0184722188860178
I0311 18:31:26.799153 139622172112640 logging_writer.py:48] [114800] global_step=114800, grad_norm=0.15162742137908936, loss=0.01814076118171215
I0311 18:31:58.148747 139628670904064 logging_writer.py:48] [114900] global_step=114900, grad_norm=0.147322878241539, loss=0.016593007370829582
I0311 18:32:29.229823 139622172112640 logging_writer.py:48] [115000] global_step=115000, grad_norm=0.14951938390731812, loss=0.016094563528895378
I0311 18:33:02.590044 139628670904064 logging_writer.py:48] [115100] global_step=115100, grad_norm=0.13627545535564423, loss=0.018310055136680603
I0311 18:33:34.331384 139622172112640 logging_writer.py:48] [115200] global_step=115200, grad_norm=0.13827595114707947, loss=0.017182249575853348
I0311 18:33:49.388214 139789500200768 spec.py:321] Evaluating on the training split.
I0311 18:35:35.594208 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 18:35:38.662594 139789500200768 spec.py:349] Evaluating on the test split.
I0311 18:35:41.650278 139789500200768 submission_runner.py:411] Time since start: 55257.48s, 	Step: 115249, 	{'train/accuracy': 0.9955669045448303, 'train/loss': 0.01395689882338047, 'train/mean_average_precision': 0.7696413411763118, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.2866747762429935, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.27593743283594613, 'test/num_examples': 43793, 'score': 36273.37165546417, 'total_duration': 55257.477509737015, 'accumulated_submission_time': 36273.37165546417, 'accumulated_eval_time': 18975.043164491653, 'accumulated_logging_time': 6.046472072601318}
I0311 18:35:41.686342 139621870597888 logging_writer.py:48] [115249] accumulated_eval_time=18975.043164, accumulated_logging_time=6.046472, accumulated_submission_time=36273.371655, global_step=115249, preemption_count=0, score=36273.371655, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.275937, test/num_examples=43793, total_duration=55257.477510, train/accuracy=0.995567, train/loss=0.013957, train/mean_average_precision=0.769641, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286675, validation/num_examples=43793
I0311 18:35:58.085984 139622180505344 logging_writer.py:48] [115300] global_step=115300, grad_norm=0.1514521986246109, loss=0.017543291673064232
I0311 18:36:29.137782 139621870597888 logging_writer.py:48] [115400] global_step=115400, grad_norm=0.1430312693119049, loss=0.018185636028647423
I0311 18:37:00.476820 139622180505344 logging_writer.py:48] [115500] global_step=115500, grad_norm=0.13852229714393616, loss=0.01852128468453884
I0311 18:37:31.870417 139621870597888 logging_writer.py:48] [115600] global_step=115600, grad_norm=0.1336391717195511, loss=0.016521338373422623
I0311 18:38:02.958852 139622180505344 logging_writer.py:48] [115700] global_step=115700, grad_norm=0.14541630446910858, loss=0.017515819519758224
I0311 18:38:34.097445 139621870597888 logging_writer.py:48] [115800] global_step=115800, grad_norm=0.14166252315044403, loss=0.016706697642803192
I0311 18:39:05.249634 139622180505344 logging_writer.py:48] [115900] global_step=115900, grad_norm=0.15752296149730682, loss=0.01678888313472271
I0311 18:39:36.289685 139621870597888 logging_writer.py:48] [116000] global_step=116000, grad_norm=0.14183013141155243, loss=0.019783305004239082
I0311 18:39:41.878743 139789500200768 spec.py:321] Evaluating on the training split.
I0311 18:41:26.148401 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 18:41:29.257111 139789500200768 spec.py:349] Evaluating on the test split.
I0311 18:41:32.261162 139789500200768 submission_runner.py:411] Time since start: 55608.09s, 	Step: 116019, 	{'train/accuracy': 0.9955641627311707, 'train/loss': 0.013859805651009083, 'train/mean_average_precision': 0.7850317787543281, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.28655513929099435, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.2760065789253968, 'test/num_examples': 43793, 'score': 36513.53369688988, 'total_duration': 55608.088416814804, 'accumulated_submission_time': 36513.53369688988, 'accumulated_eval_time': 19085.425540685654, 'accumulated_logging_time': 6.093016862869263}
I0311 18:41:32.297233 139628670904064 logging_writer.py:48] [116019] accumulated_eval_time=19085.425541, accumulated_logging_time=6.093017, accumulated_submission_time=36513.533697, global_step=116019, preemption_count=0, score=36513.533697, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.276007, test/num_examples=43793, total_duration=55608.088417, train/accuracy=0.995564, train/loss=0.013860, train/mean_average_precision=0.785032, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286555, validation/num_examples=43793
I0311 18:41:57.726239 139628679296768 logging_writer.py:48] [116100] global_step=116100, grad_norm=0.14353492856025696, loss=0.017368685454130173
I0311 18:42:28.707168 139628670904064 logging_writer.py:48] [116200] global_step=116200, grad_norm=0.16185489296913147, loss=0.020065223798155785
I0311 18:43:00.159230 139628679296768 logging_writer.py:48] [116300] global_step=116300, grad_norm=0.14862830936908722, loss=0.016911741346120834
I0311 18:43:31.052660 139628670904064 logging_writer.py:48] [116400] global_step=116400, grad_norm=0.15035457909107208, loss=0.01773335039615631
I0311 18:44:01.976667 139628679296768 logging_writer.py:48] [116500] global_step=116500, grad_norm=0.13505451381206512, loss=0.016153840348124504
I0311 18:44:33.187303 139628670904064 logging_writer.py:48] [116600] global_step=116600, grad_norm=0.13451944291591644, loss=0.016468342393636703
I0311 18:45:04.306436 139628679296768 logging_writer.py:48] [116700] global_step=116700, grad_norm=0.14335165917873383, loss=0.01967213675379753
I0311 18:45:32.280715 139789500200768 spec.py:321] Evaluating on the training split.
I0311 18:47:22.992855 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 18:47:26.033538 139789500200768 spec.py:349] Evaluating on the test split.
I0311 18:47:29.107191 139789500200768 submission_runner.py:411] Time since start: 55964.93s, 	Step: 116791, 	{'train/accuracy': 0.9955353140830994, 'train/loss': 0.013902532868087292, 'train/mean_average_precision': 0.7856174298674152, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.2865175104084884, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426059290766716, 'test/mean_average_precision': 0.2759737336301693, 'test/num_examples': 43793, 'score': 36753.48701620102, 'total_duration': 55964.93444156647, 'accumulated_submission_time': 36753.48701620102, 'accumulated_eval_time': 19202.251974105835, 'accumulated_logging_time': 6.139763593673706}
I0311 18:47:29.144160 139621870597888 logging_writer.py:48] [116791] accumulated_eval_time=19202.251974, accumulated_logging_time=6.139764, accumulated_submission_time=36753.487016, global_step=116791, preemption_count=0, score=36753.487016, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.275974, test/num_examples=43793, total_duration=55964.934442, train/accuracy=0.995535, train/loss=0.013903, train/mean_average_precision=0.785617, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286518, validation/num_examples=43793
I0311 18:47:32.357609 139622180505344 logging_writer.py:48] [116800] global_step=116800, grad_norm=0.1395254284143448, loss=0.017353860661387444
I0311 18:48:03.951306 139621870597888 logging_writer.py:48] [116900] global_step=116900, grad_norm=0.13749682903289795, loss=0.015182889997959137
I0311 18:48:35.194841 139622180505344 logging_writer.py:48] [117000] global_step=117000, grad_norm=0.14912910759449005, loss=0.018121439963579178
I0311 18:49:06.589616 139621870597888 logging_writer.py:48] [117100] global_step=117100, grad_norm=0.1508609801530838, loss=0.018528597429394722
I0311 18:49:38.140337 139622180505344 logging_writer.py:48] [117200] global_step=117200, grad_norm=0.13644669950008392, loss=0.01661808416247368
I0311 18:50:09.708213 139621870597888 logging_writer.py:48] [117300] global_step=117300, grad_norm=0.17415231466293335, loss=0.019512711092829704
I0311 18:50:41.046257 139622180505344 logging_writer.py:48] [117400] global_step=117400, grad_norm=0.17793622612953186, loss=0.022080624476075172
I0311 18:51:12.627623 139621870597888 logging_writer.py:48] [117500] global_step=117500, grad_norm=0.13240380585193634, loss=0.01694224216043949
I0311 18:51:29.283663 139789500200768 spec.py:321] Evaluating on the training split.
I0311 18:53:14.636221 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 18:53:17.688775 139789500200768 spec.py:349] Evaluating on the test split.
I0311 18:53:20.698570 139789500200768 submission_runner.py:411] Time since start: 56316.53s, 	Step: 117554, 	{'train/accuracy': 0.9955047965049744, 'train/loss': 0.014002995565533638, 'train/mean_average_precision': 0.7721958738934317, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.286516307077295, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.276063221490286, 'test/num_examples': 43793, 'score': 36993.59615969658, 'total_duration': 56316.52581310272, 'accumulated_submission_time': 36993.59615969658, 'accumulated_eval_time': 19313.66682934761, 'accumulated_logging_time': 6.1876304149627686}
I0311 18:53:20.735691 139622172112640 logging_writer.py:48] [117554] accumulated_eval_time=19313.666829, accumulated_logging_time=6.187630, accumulated_submission_time=36993.596160, global_step=117554, preemption_count=0, score=36993.596160, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.276063, test/num_examples=43793, total_duration=56316.525813, train/accuracy=0.995505, train/loss=0.014003, train/mean_average_precision=0.772196, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286516, validation/num_examples=43793
I0311 18:53:35.812389 139628679296768 logging_writer.py:48] [117600] global_step=117600, grad_norm=0.1342678815126419, loss=0.015124139375984669
I0311 18:54:06.937767 139622172112640 logging_writer.py:48] [117700] global_step=117700, grad_norm=0.13159072399139404, loss=0.01669112779200077
I0311 18:54:37.827994 139628679296768 logging_writer.py:48] [117800] global_step=117800, grad_norm=0.15953433513641357, loss=0.019754881039261818
I0311 18:55:09.262435 139622172112640 logging_writer.py:48] [117900] global_step=117900, grad_norm=0.15173302590847015, loss=0.017286770045757294
I0311 18:55:40.784158 139628679296768 logging_writer.py:48] [118000] global_step=118000, grad_norm=0.1324300765991211, loss=0.01618071086704731
I0311 18:56:12.310173 139622172112640 logging_writer.py:48] [118100] global_step=118100, grad_norm=0.14306744933128357, loss=0.017941368743777275
I0311 18:56:43.567820 139628679296768 logging_writer.py:48] [118200] global_step=118200, grad_norm=0.14316974580287933, loss=0.0192855317145586
I0311 18:57:14.763529 139622172112640 logging_writer.py:48] [118300] global_step=118300, grad_norm=0.13886211812496185, loss=0.016616592183709145
I0311 18:57:20.983092 139789500200768 spec.py:321] Evaluating on the training split.
I0311 18:59:11.602483 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 18:59:14.654886 139789500200768 spec.py:349] Evaluating on the test split.
I0311 18:59:17.650693 139789500200768 submission_runner.py:411] Time since start: 56673.48s, 	Step: 118321, 	{'train/accuracy': 0.9955202341079712, 'train/loss': 0.013966933824121952, 'train/mean_average_precision': 0.7832561135578842, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.28672363901143844, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.2760523527524613, 'test/num_examples': 43793, 'score': 37233.81376886368, 'total_duration': 56673.477946043015, 'accumulated_submission_time': 37233.81376886368, 'accumulated_eval_time': 19430.334386587143, 'accumulated_logging_time': 6.2353668212890625}
I0311 18:59:17.687957 139622180505344 logging_writer.py:48] [118321] accumulated_eval_time=19430.334387, accumulated_logging_time=6.235367, accumulated_submission_time=37233.813769, global_step=118321, preemption_count=0, score=37233.813769, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.276052, test/num_examples=43793, total_duration=56673.477946, train/accuracy=0.995520, train/loss=0.013967, train/mean_average_precision=0.783256, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286724, validation/num_examples=43793
I0311 18:59:43.089412 139628670904064 logging_writer.py:48] [118400] global_step=118400, grad_norm=0.14880982041358948, loss=0.019579840824007988
I0311 19:00:14.468221 139622180505344 logging_writer.py:48] [118500] global_step=118500, grad_norm=0.13294824957847595, loss=0.01673056371510029
I0311 19:00:46.108650 139628670904064 logging_writer.py:48] [118600] global_step=118600, grad_norm=0.14182132482528687, loss=0.018233977258205414
I0311 19:01:17.473065 139622180505344 logging_writer.py:48] [118700] global_step=118700, grad_norm=0.1367189735174179, loss=0.018947910517454147
I0311 19:01:48.861491 139628670904064 logging_writer.py:48] [118800] global_step=118800, grad_norm=0.1394556164741516, loss=0.01625012792646885
I0311 19:02:20.177467 139622180505344 logging_writer.py:48] [118900] global_step=118900, grad_norm=0.1255626678466797, loss=0.017358439043164253
I0311 19:02:51.390854 139628670904064 logging_writer.py:48] [119000] global_step=119000, grad_norm=0.15078555047512054, loss=0.018396129831671715
I0311 19:03:17.856282 139789500200768 spec.py:321] Evaluating on the training split.
I0311 19:05:11.371183 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 19:05:14.430739 139789500200768 spec.py:349] Evaluating on the test split.
I0311 19:05:17.415443 139789500200768 submission_runner.py:411] Time since start: 57033.24s, 	Step: 119085, 	{'train/accuracy': 0.9955582022666931, 'train/loss': 0.0139131685718894, 'train/mean_average_precision': 0.7770310732162424, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.2866907177414473, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.27594843181030515, 'test/num_examples': 43793, 'score': 37473.95171999931, 'total_duration': 57033.24269533157, 'accumulated_submission_time': 37473.95171999931, 'accumulated_eval_time': 19549.893506526947, 'accumulated_logging_time': 6.28296160697937}
I0311 19:05:17.452950 139621870597888 logging_writer.py:48] [119085] accumulated_eval_time=19549.893507, accumulated_logging_time=6.282962, accumulated_submission_time=37473.951720, global_step=119085, preemption_count=0, score=37473.951720, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.275948, test/num_examples=43793, total_duration=57033.242695, train/accuracy=0.995558, train/loss=0.013913, train/mean_average_precision=0.777031, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286691, validation/num_examples=43793
I0311 19:05:22.551358 139622172112640 logging_writer.py:48] [119100] global_step=119100, grad_norm=0.13611751794815063, loss=0.01680433377623558
I0311 19:05:54.031246 139621870597888 logging_writer.py:48] [119200] global_step=119200, grad_norm=0.14480111002922058, loss=0.017569798976182938
I0311 19:06:25.549330 139622172112640 logging_writer.py:48] [119300] global_step=119300, grad_norm=0.15330256521701813, loss=0.018001539632678032
I0311 19:06:57.112172 139621870597888 logging_writer.py:48] [119400] global_step=119400, grad_norm=0.11517342180013657, loss=0.013681740500032902
I0311 19:07:28.638021 139622172112640 logging_writer.py:48] [119500] global_step=119500, grad_norm=0.1326567828655243, loss=0.016587303951382637
I0311 19:07:59.938404 139621870597888 logging_writer.py:48] [119600] global_step=119600, grad_norm=0.13633333146572113, loss=0.0171215757727623
I0311 19:08:31.430494 139622172112640 logging_writer.py:48] [119700] global_step=119700, grad_norm=0.15703628957271576, loss=0.01977069303393364
I0311 19:09:02.587781 139621870597888 logging_writer.py:48] [119800] global_step=119800, grad_norm=0.15048737823963165, loss=0.018319785594940186
I0311 19:09:17.571300 139789500200768 spec.py:321] Evaluating on the training split.
I0311 19:11:04.343801 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 19:11:07.480885 139789500200768 spec.py:349] Evaluating on the test split.
I0311 19:11:10.612636 139789500200768 submission_runner.py:411] Time since start: 57386.44s, 	Step: 119849, 	{'train/accuracy': 0.9955252408981323, 'train/loss': 0.014038915745913982, 'train/mean_average_precision': 0.7764668111826432, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.2867658580304355, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.2760382490303594, 'test/num_examples': 43793, 'score': 37714.03889346123, 'total_duration': 57386.43988108635, 'accumulated_submission_time': 37714.03889346123, 'accumulated_eval_time': 19662.93479347229, 'accumulated_logging_time': 6.332173109054565}
I0311 19:11:10.650539 139622180505344 logging_writer.py:48] [119849] accumulated_eval_time=19662.934793, accumulated_logging_time=6.332173, accumulated_submission_time=37714.038893, global_step=119849, preemption_count=0, score=37714.038893, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.276038, test/num_examples=43793, total_duration=57386.439881, train/accuracy=0.995525, train/loss=0.014039, train/mean_average_precision=0.776467, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286766, validation/num_examples=43793
I0311 19:11:27.087626 139628670904064 logging_writer.py:48] [119900] global_step=119900, grad_norm=0.15439453721046448, loss=0.018414972350001335
I0311 19:11:58.688988 139622180505344 logging_writer.py:48] [120000] global_step=120000, grad_norm=0.14670205116271973, loss=0.018873892724514008
I0311 19:12:30.144406 139628670904064 logging_writer.py:48] [120100] global_step=120100, grad_norm=0.14179067313671112, loss=0.01646457426249981
I0311 19:13:01.277042 139622180505344 logging_writer.py:48] [120200] global_step=120200, grad_norm=0.13956588506698608, loss=0.015719829127192497
I0311 19:13:32.838832 139628670904064 logging_writer.py:48] [120300] global_step=120300, grad_norm=0.1892947256565094, loss=0.021244287490844727
I0311 19:14:04.312353 139622180505344 logging_writer.py:48] [120400] global_step=120400, grad_norm=0.1549336314201355, loss=0.019128447398543358
I0311 19:14:35.760198 139628670904064 logging_writer.py:48] [120500] global_step=120500, grad_norm=0.14092917740345, loss=0.018233196809887886
I0311 19:15:07.453705 139622180505344 logging_writer.py:48] [120600] global_step=120600, grad_norm=0.1336434781551361, loss=0.01477888785302639
I0311 19:15:10.888139 139789500200768 spec.py:321] Evaluating on the training split.
I0311 19:16:58.011137 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 19:17:01.057177 139789500200768 spec.py:349] Evaluating on the test split.
I0311 19:17:04.127901 139789500200768 submission_runner.py:411] Time since start: 57739.96s, 	Step: 120612, 	{'train/accuracy': 0.9956113696098328, 'train/loss': 0.013700583949685097, 'train/mean_average_precision': 0.7857608368347888, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.2865649723131295, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426059290766716, 'test/mean_average_precision': 0.27597957687888053, 'test/num_examples': 43793, 'score': 37954.24491381645, 'total_duration': 57739.95515346527, 'accumulated_submission_time': 37954.24491381645, 'accumulated_eval_time': 19776.17451095581, 'accumulated_logging_time': 6.382076978683472}
I0311 19:17:04.166261 139621870597888 logging_writer.py:48] [120612] accumulated_eval_time=19776.174511, accumulated_logging_time=6.382077, accumulated_submission_time=37954.244914, global_step=120612, preemption_count=0, score=37954.244914, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.275980, test/num_examples=43793, total_duration=57739.955153, train/accuracy=0.995611, train/loss=0.013701, train/mean_average_precision=0.785761, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286565, validation/num_examples=43793
I0311 19:17:32.278940 139628679296768 logging_writer.py:48] [120700] global_step=120700, grad_norm=0.14635196328163147, loss=0.018986202776432037
I0311 19:18:03.555207 139621870597888 logging_writer.py:48] [120800] global_step=120800, grad_norm=0.1373678296804428, loss=0.01675357110798359
I0311 19:18:34.936875 139628679296768 logging_writer.py:48] [120900] global_step=120900, grad_norm=0.14734593033790588, loss=0.017253782600164413
I0311 19:19:06.331181 139621870597888 logging_writer.py:48] [121000] global_step=121000, grad_norm=0.14923930168151855, loss=0.018595747649669647
I0311 19:19:37.758512 139628679296768 logging_writer.py:48] [121100] global_step=121100, grad_norm=0.13874678313732147, loss=0.018620146438479424
I0311 19:20:08.901435 139621870597888 logging_writer.py:48] [121200] global_step=121200, grad_norm=0.14271703362464905, loss=0.016750650480389595
I0311 19:20:40.166969 139628679296768 logging_writer.py:48] [121300] global_step=121300, grad_norm=0.1329575926065445, loss=0.01600475423038006
I0311 19:21:04.193529 139789500200768 spec.py:321] Evaluating on the training split.
I0311 19:22:52.699440 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 19:22:55.715320 139789500200768 spec.py:349] Evaluating on the test split.
I0311 19:22:58.695917 139789500200768 submission_runner.py:411] Time since start: 58094.52s, 	Step: 121378, 	{'train/accuracy': 0.9955203533172607, 'train/loss': 0.01398597750812769, 'train/mean_average_precision': 0.7798932588725074, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.2865813047482279, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.27593656622114493, 'test/num_examples': 43793, 'score': 38194.241970300674, 'total_duration': 58094.52316451073, 'accumulated_submission_time': 38194.241970300674, 'accumulated_eval_time': 19890.676852464676, 'accumulated_logging_time': 6.430763006210327}
I0311 19:22:58.734171 139622172112640 logging_writer.py:48] [121378] accumulated_eval_time=19890.676852, accumulated_logging_time=6.430763, accumulated_submission_time=38194.241970, global_step=121378, preemption_count=0, score=38194.241970, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.275937, test/num_examples=43793, total_duration=58094.523165, train/accuracy=0.995520, train/loss=0.013986, train/mean_average_precision=0.779893, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286581, validation/num_examples=43793
I0311 19:23:06.458503 139622180505344 logging_writer.py:48] [121400] global_step=121400, grad_norm=0.14573951065540314, loss=0.017324205487966537
I0311 19:23:39.587340 139622172112640 logging_writer.py:48] [121500] global_step=121500, grad_norm=0.13626305758953094, loss=0.01702689379453659
I0311 19:24:12.514126 139622180505344 logging_writer.py:48] [121600] global_step=121600, grad_norm=0.14341682195663452, loss=0.01845521107316017
I0311 19:24:45.529208 139622172112640 logging_writer.py:48] [121700] global_step=121700, grad_norm=0.152561217546463, loss=0.01675279624760151
I0311 19:25:18.487896 139622180505344 logging_writer.py:48] [121800] global_step=121800, grad_norm=0.14952708780765533, loss=0.018254397436976433
I0311 19:25:51.314065 139622172112640 logging_writer.py:48] [121900] global_step=121900, grad_norm=0.14642399549484253, loss=0.017982007935643196
I0311 19:26:23.930829 139622180505344 logging_writer.py:48] [122000] global_step=122000, grad_norm=0.15911270678043365, loss=0.019303979352116585
I0311 19:26:56.808966 139622172112640 logging_writer.py:48] [122100] global_step=122100, grad_norm=0.15786246955394745, loss=0.019681796431541443
I0311 19:26:58.790146 139789500200768 spec.py:321] Evaluating on the training split.
I0311 19:28:55.172588 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 19:28:58.664711 139789500200768 spec.py:349] Evaluating on the test split.
I0311 19:29:02.119888 139789500200768 submission_runner.py:411] Time since start: 58457.95s, 	Step: 122107, 	{'train/accuracy': 0.9955151081085205, 'train/loss': 0.013917292468249798, 'train/mean_average_precision': 0.7799444582331069, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.28650714427375207, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.27596513437867515, 'test/num_examples': 43793, 'score': 38434.26357674599, 'total_duration': 58457.947122097015, 'accumulated_submission_time': 38434.26357674599, 'accumulated_eval_time': 20014.006541490555, 'accumulated_logging_time': 6.4794793128967285}
I0311 19:29:02.163323 139621870597888 logging_writer.py:48] [122107] accumulated_eval_time=20014.006541, accumulated_logging_time=6.479479, accumulated_submission_time=38434.263577, global_step=122107, preemption_count=0, score=38434.263577, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.275965, test/num_examples=43793, total_duration=58457.947122, train/accuracy=0.995515, train/loss=0.013917, train/mean_average_precision=0.779944, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286507, validation/num_examples=43793
I0311 19:29:33.347521 139628670904064 logging_writer.py:48] [122200] global_step=122200, grad_norm=0.14356525242328644, loss=0.015246408060193062
I0311 19:30:06.389285 139621870597888 logging_writer.py:48] [122300] global_step=122300, grad_norm=0.13701291382312775, loss=0.01672334410250187
I0311 19:30:39.143227 139628670904064 logging_writer.py:48] [122400] global_step=122400, grad_norm=0.1595347821712494, loss=0.01946871541440487
I0311 19:31:12.209204 139621870597888 logging_writer.py:48] [122500] global_step=122500, grad_norm=0.16092370450496674, loss=0.01979122869670391
I0311 19:31:45.100828 139628670904064 logging_writer.py:48] [122600] global_step=122600, grad_norm=0.1342870146036148, loss=0.01723000779747963
I0311 19:32:17.982753 139621870597888 logging_writer.py:48] [122700] global_step=122700, grad_norm=0.14866892993450165, loss=0.01596333459019661
I0311 19:32:50.690207 139628670904064 logging_writer.py:48] [122800] global_step=122800, grad_norm=0.13352221250534058, loss=0.01687539368867874
I0311 19:33:02.135185 139789500200768 spec.py:321] Evaluating on the training split.
I0311 19:34:55.933534 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 19:34:58.990772 139789500200768 spec.py:349] Evaluating on the test split.
I0311 19:35:02.078288 139789500200768 submission_runner.py:411] Time since start: 58817.91s, 	Step: 122836, 	{'train/accuracy': 0.9955499768257141, 'train/loss': 0.013940073549747467, 'train/mean_average_precision': 0.7765179471599951, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.28649500996856847, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.2760162167495111, 'test/num_examples': 43793, 'score': 38674.200468063354, 'total_duration': 58817.90552377701, 'accumulated_submission_time': 38674.200468063354, 'accumulated_eval_time': 20133.949590206146, 'accumulated_logging_time': 6.534131288528442}
I0311 19:35:02.117129 139622172112640 logging_writer.py:48] [122836] accumulated_eval_time=20133.949590, accumulated_logging_time=6.534131, accumulated_submission_time=38674.200468, global_step=122836, preemption_count=0, score=38674.200468, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.276016, test/num_examples=43793, total_duration=58817.905524, train/accuracy=0.995550, train/loss=0.013940, train/mean_average_precision=0.776518, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286495, validation/num_examples=43793
I0311 19:35:22.598708 139622180505344 logging_writer.py:48] [122900] global_step=122900, grad_norm=0.12930363416671753, loss=0.015435784123837948
I0311 19:35:54.286479 139622172112640 logging_writer.py:48] [123000] global_step=123000, grad_norm=0.14357057213783264, loss=0.018237944692373276
I0311 19:36:25.736612 139622180505344 logging_writer.py:48] [123100] global_step=123100, grad_norm=0.14281973242759705, loss=0.018482690677046776
I0311 19:36:57.411474 139622172112640 logging_writer.py:48] [123200] global_step=123200, grad_norm=0.1625371128320694, loss=0.020026959478855133
I0311 19:37:28.805403 139622180505344 logging_writer.py:48] [123300] global_step=123300, grad_norm=0.1449107825756073, loss=0.018927687779068947
I0311 19:38:00.332008 139622172112640 logging_writer.py:48] [123400] global_step=123400, grad_norm=0.15019790828227997, loss=0.018595390021800995
I0311 19:38:31.809664 139622180505344 logging_writer.py:48] [123500] global_step=123500, grad_norm=0.15409007668495178, loss=0.016786420717835426
I0311 19:39:02.293344 139789500200768 spec.py:321] Evaluating on the training split.
I0311 19:40:47.404667 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 19:40:50.526563 139789500200768 spec.py:349] Evaluating on the test split.
I0311 19:40:55.443377 139789500200768 submission_runner.py:411] Time since start: 59171.27s, 	Step: 123599, 	{'train/accuracy': 0.9955602288246155, 'train/loss': 0.013960919342935085, 'train/mean_average_precision': 0.7762064006184903, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.2866554438585214, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.2759403551749374, 'test/num_examples': 43793, 'score': 38914.34498047829, 'total_duration': 59171.270610809326, 'accumulated_submission_time': 38914.34498047829, 'accumulated_eval_time': 20247.099562883377, 'accumulated_logging_time': 6.583909273147583}
I0311 19:40:55.482634 139628670904064 logging_writer.py:48] [123599] accumulated_eval_time=20247.099563, accumulated_logging_time=6.583909, accumulated_submission_time=38914.344980, global_step=123599, preemption_count=0, score=38914.344980, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.275940, test/num_examples=43793, total_duration=59171.270611, train/accuracy=0.995560, train/loss=0.013961, train/mean_average_precision=0.776206, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286655, validation/num_examples=43793
I0311 19:40:56.151658 139628679296768 logging_writer.py:48] [123600] global_step=123600, grad_norm=0.14026741683483124, loss=0.019550533965229988
I0311 19:41:27.621286 139628670904064 logging_writer.py:48] [123700] global_step=123700, grad_norm=0.1380583494901657, loss=0.019491927698254585
I0311 19:41:59.278822 139628679296768 logging_writer.py:48] [123800] global_step=123800, grad_norm=0.1434299349784851, loss=0.019014928489923477
I0311 19:42:31.166986 139628670904064 logging_writer.py:48] [123900] global_step=123900, grad_norm=0.1486840397119522, loss=0.01762719638645649
I0311 19:43:02.540693 139628679296768 logging_writer.py:48] [124000] global_step=124000, grad_norm=0.12680070102214813, loss=0.017107311636209488
I0311 19:43:34.255800 139628670904064 logging_writer.py:48] [124100] global_step=124100, grad_norm=0.12863919138908386, loss=0.01695629209280014
I0311 19:44:05.804346 139628679296768 logging_writer.py:48] [124200] global_step=124200, grad_norm=0.1554386019706726, loss=0.01836588606238365
I0311 19:44:37.494003 139628670904064 logging_writer.py:48] [124300] global_step=124300, grad_norm=0.1503893882036209, loss=0.0190103929489851
I0311 19:44:55.552987 139789500200768 spec.py:321] Evaluating on the training split.
I0311 19:46:44.389657 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 19:46:47.500769 139789500200768 spec.py:349] Evaluating on the test split.
I0311 19:46:50.584070 139789500200768 submission_runner.py:411] Time since start: 59526.41s, 	Step: 124357, 	{'train/accuracy': 0.9955296516418457, 'train/loss': 0.013915720395743847, 'train/mean_average_precision': 0.7814746632594798, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.28646866960599415, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.2759882452278814, 'test/num_examples': 43793, 'score': 39154.384144067764, 'total_duration': 59526.41132116318, 'accumulated_submission_time': 39154.384144067764, 'accumulated_eval_time': 20362.13060450554, 'accumulated_logging_time': 6.63392972946167}
I0311 19:46:50.623854 139621870597888 logging_writer.py:48] [124357] accumulated_eval_time=20362.130605, accumulated_logging_time=6.633930, accumulated_submission_time=39154.384144, global_step=124357, preemption_count=0, score=39154.384144, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.275988, test/num_examples=43793, total_duration=59526.411321, train/accuracy=0.995530, train/loss=0.013916, train/mean_average_precision=0.781475, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286469, validation/num_examples=43793
I0311 19:47:04.935371 139622172112640 logging_writer.py:48] [124400] global_step=124400, grad_norm=0.15267790853977203, loss=0.01951654814183712
I0311 19:47:37.249356 139621870597888 logging_writer.py:48] [124500] global_step=124500, grad_norm=0.14128606021404266, loss=0.017563220113515854
I0311 19:48:08.703963 139622172112640 logging_writer.py:48] [124600] global_step=124600, grad_norm=0.15322977304458618, loss=0.018784457817673683
I0311 19:48:40.034734 139621870597888 logging_writer.py:48] [124700] global_step=124700, grad_norm=0.12981745600700378, loss=0.015573588199913502
I0311 19:49:11.466213 139622172112640 logging_writer.py:48] [124800] global_step=124800, grad_norm=0.15875637531280518, loss=0.02024027518928051
I0311 19:49:42.998791 139621870597888 logging_writer.py:48] [124900] global_step=124900, grad_norm=0.13544757664203644, loss=0.017904534935951233
I0311 19:50:14.837678 139622172112640 logging_writer.py:48] [125000] global_step=125000, grad_norm=0.14656436443328857, loss=0.0182575024664402
I0311 19:50:46.347280 139621870597888 logging_writer.py:48] [125100] global_step=125100, grad_norm=0.1288948953151703, loss=0.016380546614527702
I0311 19:50:50.730312 139789500200768 spec.py:321] Evaluating on the training split.
I0311 19:52:36.859031 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 19:52:39.944968 139789500200768 spec.py:349] Evaluating on the test split.
I0311 19:52:44.824585 139789500200768 submission_runner.py:411] Time since start: 59880.65s, 	Step: 125115, 	{'train/accuracy': 0.9956167340278625, 'train/loss': 0.013716289773583412, 'train/mean_average_precision': 0.7831922579711271, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.2866426636904639, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426059290766716, 'test/mean_average_precision': 0.2759794268804273, 'test/num_examples': 43793, 'score': 39394.45890569687, 'total_duration': 59880.65182375908, 'accumulated_submission_time': 39394.45890569687, 'accumulated_eval_time': 20476.22482061386, 'accumulated_logging_time': 6.685439109802246}
I0311 19:52:44.864123 139628670904064 logging_writer.py:48] [125115] accumulated_eval_time=20476.224821, accumulated_logging_time=6.685439, accumulated_submission_time=39394.458906, global_step=125115, preemption_count=0, score=39394.458906, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.275979, test/num_examples=43793, total_duration=59880.651824, train/accuracy=0.995617, train/loss=0.013716, train/mean_average_precision=0.783192, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286643, validation/num_examples=43793
I0311 19:53:12.018070 139628679296768 logging_writer.py:48] [125200] global_step=125200, grad_norm=0.15693624317646027, loss=0.01755010150372982
I0311 19:53:43.373031 139628670904064 logging_writer.py:48] [125300] global_step=125300, grad_norm=0.1333693414926529, loss=0.01797146163880825
I0311 19:54:14.950148 139628679296768 logging_writer.py:48] [125400] global_step=125400, grad_norm=0.14486129581928253, loss=0.019209768623113632
I0311 19:54:46.554046 139628670904064 logging_writer.py:48] [125500] global_step=125500, grad_norm=0.1441972553730011, loss=0.018241725862026215
I0311 19:55:17.851724 139628679296768 logging_writer.py:48] [125600] global_step=125600, grad_norm=0.15752364695072174, loss=0.01834188774228096
I0311 19:55:49.334347 139628670904064 logging_writer.py:48] [125700] global_step=125700, grad_norm=0.14521457254886627, loss=0.018902968615293503
I0311 19:56:21.074479 139628679296768 logging_writer.py:48] [125800] global_step=125800, grad_norm=0.12966950237751007, loss=0.014833210967481136
I0311 19:56:45.105165 139789500200768 spec.py:321] Evaluating on the training split.
I0311 19:58:34.137290 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 19:58:37.227788 139789500200768 spec.py:349] Evaluating on the test split.
I0311 19:58:40.255132 139789500200768 submission_runner.py:411] Time since start: 60236.08s, 	Step: 125877, 	{'train/accuracy': 0.995496392250061, 'train/loss': 0.01407087966799736, 'train/mean_average_precision': 0.7815206147351226, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.2865493820973373, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.2759345952193906, 'test/num_examples': 43793, 'score': 39634.669246912, 'total_duration': 60236.08238339424, 'accumulated_submission_time': 39634.669246912, 'accumulated_eval_time': 20591.374747753143, 'accumulated_logging_time': 6.735987901687622}
I0311 19:58:40.294639 139622172112640 logging_writer.py:48] [125877] accumulated_eval_time=20591.374748, accumulated_logging_time=6.735988, accumulated_submission_time=39634.669247, global_step=125877, preemption_count=0, score=39634.669247, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.275935, test/num_examples=43793, total_duration=60236.082383, train/accuracy=0.995496, train/loss=0.014071, train/mean_average_precision=0.781521, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286549, validation/num_examples=43793
I0311 19:58:48.214879 139622180505344 logging_writer.py:48] [125900] global_step=125900, grad_norm=0.15225724875926971, loss=0.018361108377575874
I0311 19:59:19.540629 139622172112640 logging_writer.py:48] [126000] global_step=126000, grad_norm=0.153666153550148, loss=0.018349288031458855
I0311 19:59:51.272801 139622180505344 logging_writer.py:48] [126100] global_step=126100, grad_norm=0.14316579699516296, loss=0.018345030024647713
I0311 20:00:23.066678 139622172112640 logging_writer.py:48] [126200] global_step=126200, grad_norm=0.13781200349330902, loss=0.016187308356165886
I0311 20:00:54.828385 139622180505344 logging_writer.py:48] [126300] global_step=126300, grad_norm=0.1369403600692749, loss=0.017702646553516388
I0311 20:01:26.874800 139622172112640 logging_writer.py:48] [126400] global_step=126400, grad_norm=0.1583942472934723, loss=0.01875334605574608
I0311 20:01:58.378511 139622180505344 logging_writer.py:48] [126500] global_step=126500, grad_norm=0.15223784744739532, loss=0.01833503507077694
I0311 20:02:29.970420 139622172112640 logging_writer.py:48] [126600] global_step=126600, grad_norm=0.15936928987503052, loss=0.01895245909690857
I0311 20:02:40.436820 139789500200768 spec.py:321] Evaluating on the training split.
I0311 20:04:26.345252 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 20:04:29.417672 139789500200768 spec.py:349] Evaluating on the test split.
I0311 20:04:32.474960 139789500200768 submission_runner.py:411] Time since start: 60588.30s, 	Step: 126634, 	{'train/accuracy': 0.995526134967804, 'train/loss': 0.013944022357463837, 'train/mean_average_precision': 0.7840203056045634, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.2865940628883549, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.2759888347376683, 'test/num_examples': 43793, 'score': 39874.781376600266, 'total_duration': 60588.302213191986, 'accumulated_submission_time': 39874.781376600266, 'accumulated_eval_time': 20703.412845373154, 'accumulated_logging_time': 6.785962104797363}
I0311 20:04:32.514427 139628670904064 logging_writer.py:48] [126634] accumulated_eval_time=20703.412845, accumulated_logging_time=6.785962, accumulated_submission_time=39874.781377, global_step=126634, preemption_count=0, score=39874.781377, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.275989, test/num_examples=43793, total_duration=60588.302213, train/accuracy=0.995526, train/loss=0.013944, train/mean_average_precision=0.784020, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286594, validation/num_examples=43793
I0311 20:04:53.840643 139628679296768 logging_writer.py:48] [126700] global_step=126700, grad_norm=0.14303553104400635, loss=0.018341775983572006
I0311 20:05:25.852497 139628670904064 logging_writer.py:48] [126800] global_step=126800, grad_norm=0.1367635279893875, loss=0.01678827777504921
I0311 20:05:57.838922 139628679296768 logging_writer.py:48] [126900] global_step=126900, grad_norm=0.13332973420619965, loss=0.01703277789056301
I0311 20:06:29.618346 139628670904064 logging_writer.py:48] [127000] global_step=127000, grad_norm=0.15837864577770233, loss=0.019273534417152405
I0311 20:07:01.315773 139628679296768 logging_writer.py:48] [127100] global_step=127100, grad_norm=0.15002696216106415, loss=0.01784265786409378
I0311 20:07:33.160890 139628670904064 logging_writer.py:48] [127200] global_step=127200, grad_norm=0.15216143429279327, loss=0.0183392446488142
I0311 20:08:05.797558 139628679296768 logging_writer.py:48] [127300] global_step=127300, grad_norm=0.1316376030445099, loss=0.015478018671274185
I0311 20:08:32.538752 139789500200768 spec.py:321] Evaluating on the training split.
I0311 20:10:20.078560 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 20:10:23.170330 139789500200768 spec.py:349] Evaluating on the test split.
I0311 20:10:26.178796 139789500200768 submission_runner.py:411] Time since start: 60942.01s, 	Step: 127383, 	{'train/accuracy': 0.9955549836158752, 'train/loss': 0.013961422257125378, 'train/mean_average_precision': 0.7755276795761658, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.28649787309489877, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.2759701992733522, 'test/num_examples': 43793, 'score': 40114.7754137516, 'total_duration': 60942.006036281586, 'accumulated_submission_time': 40114.7754137516, 'accumulated_eval_time': 20817.052837133408, 'accumulated_logging_time': 6.836190938949585}
I0311 20:10:26.218536 139621870597888 logging_writer.py:48] [127383] accumulated_eval_time=20817.052837, accumulated_logging_time=6.836191, accumulated_submission_time=40114.775414, global_step=127383, preemption_count=0, score=40114.775414, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.275970, test/num_examples=43793, total_duration=60942.006036, train/accuracy=0.995555, train/loss=0.013961, train/mean_average_precision=0.775528, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286498, validation/num_examples=43793
I0311 20:10:32.182792 139622172112640 logging_writer.py:48] [127400] global_step=127400, grad_norm=0.15069887042045593, loss=0.015912439674139023
I0311 20:11:03.861974 139621870597888 logging_writer.py:48] [127500] global_step=127500, grad_norm=0.14279112219810486, loss=0.01828143000602722
I0311 20:11:35.122979 139622172112640 logging_writer.py:48] [127600] global_step=127600, grad_norm=0.15509077906608582, loss=0.01991862803697586
I0311 20:12:06.740002 139621870597888 logging_writer.py:48] [127700] global_step=127700, grad_norm=0.13526834547519684, loss=0.016253257170319557
I0311 20:12:37.936899 139622172112640 logging_writer.py:48] [127800] global_step=127800, grad_norm=0.13642337918281555, loss=0.01777409203350544
I0311 20:13:09.637255 139621870597888 logging_writer.py:48] [127900] global_step=127900, grad_norm=0.15183094143867493, loss=0.01708056963980198
I0311 20:13:41.107656 139622172112640 logging_writer.py:48] [128000] global_step=128000, grad_norm=0.14690223336219788, loss=0.015041936188936234
I0311 20:14:12.592533 139621870597888 logging_writer.py:48] [128100] global_step=128100, grad_norm=0.14477379620075226, loss=0.01948159746825695
I0311 20:14:26.213829 139789500200768 spec.py:321] Evaluating on the training split.
I0311 20:16:17.560745 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 20:16:20.625507 139789500200768 spec.py:349] Evaluating on the test split.
I0311 20:16:23.629082 139789500200768 submission_runner.py:411] Time since start: 61299.46s, 	Step: 128144, 	{'train/accuracy': 0.9955434203147888, 'train/loss': 0.013930249027907848, 'train/mean_average_precision': 0.7804785749072813, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.2865674623685516, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.27595313337877464, 'test/num_examples': 43793, 'score': 40354.73989057541, 'total_duration': 61299.456189870834, 'accumulated_submission_time': 40354.73989057541, 'accumulated_eval_time': 20934.467911720276, 'accumulated_logging_time': 6.886796712875366}
I0311 20:16:23.668781 139622180505344 logging_writer.py:48] [128144] accumulated_eval_time=20934.467912, accumulated_logging_time=6.886797, accumulated_submission_time=40354.739891, global_step=128144, preemption_count=0, score=40354.739891, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.275953, test/num_examples=43793, total_duration=61299.456190, train/accuracy=0.995543, train/loss=0.013930, train/mean_average_precision=0.780479, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286567, validation/num_examples=43793
I0311 20:16:41.479916 139628679296768 logging_writer.py:48] [128200] global_step=128200, grad_norm=0.15828073024749756, loss=0.017914369702339172
I0311 20:17:12.967678 139622180505344 logging_writer.py:48] [128300] global_step=128300, grad_norm=0.14981408417224884, loss=0.01666964404284954
I0311 20:17:44.288713 139628679296768 logging_writer.py:48] [128400] global_step=128400, grad_norm=0.14982664585113525, loss=0.017985880374908447
I0311 20:18:15.534576 139622180505344 logging_writer.py:48] [128500] global_step=128500, grad_norm=0.1315562129020691, loss=0.015508243814110756
I0311 20:18:46.823954 139628679296768 logging_writer.py:48] [128600] global_step=128600, grad_norm=0.16297036409378052, loss=0.017409702762961388
I0311 20:19:18.242280 139622180505344 logging_writer.py:48] [128700] global_step=128700, grad_norm=0.13709872961044312, loss=0.017168086022138596
I0311 20:19:49.929361 139628679296768 logging_writer.py:48] [128800] global_step=128800, grad_norm=0.15134964883327484, loss=0.016251053661108017
I0311 20:20:21.688998 139622180505344 logging_writer.py:48] [128900] global_step=128900, grad_norm=0.1430259346961975, loss=0.019382180646061897
I0311 20:20:23.890351 139789500200768 spec.py:321] Evaluating on the training split.
I0311 20:22:09.359464 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 20:22:12.409392 139789500200768 spec.py:349] Evaluating on the test split.
I0311 20:22:15.434804 139789500200768 submission_runner.py:411] Time since start: 61651.26s, 	Step: 128908, 	{'train/accuracy': 0.9955562949180603, 'train/loss': 0.01387965027242899, 'train/mean_average_precision': 0.7869704604464649, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.28654875207634695, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.2761565504820941, 'test/num_examples': 43793, 'score': 40594.930896520615, 'total_duration': 61651.26204371452, 'accumulated_submission_time': 40594.930896520615, 'accumulated_eval_time': 21046.012306928635, 'accumulated_logging_time': 6.93725061416626}
I0311 20:22:15.474233 139621870597888 logging_writer.py:48] [128908] accumulated_eval_time=21046.012307, accumulated_logging_time=6.937251, accumulated_submission_time=40594.930897, global_step=128908, preemption_count=0, score=40594.930897, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.276157, test/num_examples=43793, total_duration=61651.262044, train/accuracy=0.995556, train/loss=0.013880, train/mean_average_precision=0.786970, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286549, validation/num_examples=43793
I0311 20:22:45.064011 139622172112640 logging_writer.py:48] [129000] global_step=129000, grad_norm=0.13441286981105804, loss=0.017275666818022728
I0311 20:23:16.429290 139621870597888 logging_writer.py:48] [129100] global_step=129100, grad_norm=0.14622871577739716, loss=0.018971050158143044
I0311 20:23:47.917581 139622172112640 logging_writer.py:48] [129200] global_step=129200, grad_norm=0.12850579619407654, loss=0.01574069820344448
I0311 20:24:19.466014 139621870597888 logging_writer.py:48] [129300] global_step=129300, grad_norm=0.144431009888649, loss=0.016567612066864967
I0311 20:24:51.011994 139622172112640 logging_writer.py:48] [129400] global_step=129400, grad_norm=0.13847413659095764, loss=0.017536422237753868
I0311 20:25:22.205020 139621870597888 logging_writer.py:48] [129500] global_step=129500, grad_norm=0.12896373867988586, loss=0.016467416658997536
I0311 20:25:53.418225 139622172112640 logging_writer.py:48] [129600] global_step=129600, grad_norm=0.15353484451770782, loss=0.018869534134864807
I0311 20:26:15.725215 139789500200768 spec.py:321] Evaluating on the training split.
I0311 20:28:02.825390 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 20:28:07.689070 139789500200768 spec.py:349] Evaluating on the test split.
I0311 20:28:10.689651 139789500200768 submission_runner.py:411] Time since start: 62006.52s, 	Step: 129672, 	{'train/accuracy': 0.995521605014801, 'train/loss': 0.013931512832641602, 'train/mean_average_precision': 0.7730002351900508, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.2865531526774762, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.2760121926469126, 'test/num_examples': 43793, 'score': 40835.15206003189, 'total_duration': 62006.51690149307, 'accumulated_submission_time': 40835.15206003189, 'accumulated_eval_time': 21160.976698875427, 'accumulated_logging_time': 6.9871649742126465}
I0311 20:28:10.730528 139622180505344 logging_writer.py:48] [129672] accumulated_eval_time=21160.976699, accumulated_logging_time=6.987165, accumulated_submission_time=40835.152060, global_step=129672, preemption_count=0, score=40835.152060, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.276012, test/num_examples=43793, total_duration=62006.516901, train/accuracy=0.995522, train/loss=0.013932, train/mean_average_precision=0.773000, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286553, validation/num_examples=43793
I0311 20:28:19.806339 139628670904064 logging_writer.py:48] [129700] global_step=129700, grad_norm=0.16365043818950653, loss=0.017580028623342514
I0311 20:28:51.038792 139622180505344 logging_writer.py:48] [129800] global_step=129800, grad_norm=0.14049804210662842, loss=0.017115438356995583
I0311 20:29:22.145743 139628670904064 logging_writer.py:48] [129900] global_step=129900, grad_norm=0.1302502602338791, loss=0.015998711809515953
I0311 20:29:53.293806 139622180505344 logging_writer.py:48] [130000] global_step=130000, grad_norm=0.14517751336097717, loss=0.015667403116822243
I0311 20:30:24.948668 139628670904064 logging_writer.py:48] [130100] global_step=130100, grad_norm=0.1554725170135498, loss=0.018498793244361877
I0311 20:30:56.795927 139622180505344 logging_writer.py:48] [130200] global_step=130200, grad_norm=0.1579207330942154, loss=0.019303536042571068
I0311 20:31:28.492764 139628670904064 logging_writer.py:48] [130300] global_step=130300, grad_norm=0.15383842587471008, loss=0.019461583346128464
I0311 20:31:59.981256 139622180505344 logging_writer.py:48] [130400] global_step=130400, grad_norm=0.13082294166088104, loss=0.017713382840156555
I0311 20:32:10.989301 139789500200768 spec.py:321] Evaluating on the training split.
I0311 20:33:55.121131 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 20:33:58.136981 139789500200768 spec.py:349] Evaluating on the test split.
I0311 20:34:01.157665 139789500200768 submission_runner.py:411] Time since start: 62356.98s, 	Step: 130436, 	{'train/accuracy': 0.9955735206604004, 'train/loss': 0.013827841728925705, 'train/mean_average_precision': 0.7775615790059116, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.2866122372734342, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426059290766716, 'test/mean_average_precision': 0.2760328250730925, 'test/num_examples': 43793, 'score': 41075.380811452866, 'total_duration': 62356.984919548035, 'accumulated_submission_time': 41075.380811452866, 'accumulated_eval_time': 21271.14502310753, 'accumulated_logging_time': 7.038794279098511}
I0311 20:34:01.196557 139621870597888 logging_writer.py:48] [130436] accumulated_eval_time=21271.145023, accumulated_logging_time=7.038794, accumulated_submission_time=41075.380811, global_step=130436, preemption_count=0, score=41075.380811, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.276033, test/num_examples=43793, total_duration=62356.984920, train/accuracy=0.995574, train/loss=0.013828, train/mean_average_precision=0.777562, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286612, validation/num_examples=43793
I0311 20:34:21.622542 139622172112640 logging_writer.py:48] [130500] global_step=130500, grad_norm=0.1399955302476883, loss=0.017888283357024193
I0311 20:34:53.127971 139621870597888 logging_writer.py:48] [130600] global_step=130600, grad_norm=0.14138604700565338, loss=0.01857013627886772
I0311 20:35:24.771150 139622172112640 logging_writer.py:48] [130700] global_step=130700, grad_norm=0.13564792275428772, loss=0.017149562016129494
I0311 20:35:56.225183 139621870597888 logging_writer.py:48] [130800] global_step=130800, grad_norm=0.1388072371482849, loss=0.017099730670452118
I0311 20:36:27.525831 139622172112640 logging_writer.py:48] [130900] global_step=130900, grad_norm=0.15091009438037872, loss=0.018458260223269463
I0311 20:36:58.975203 139621870597888 logging_writer.py:48] [131000] global_step=131000, grad_norm=0.14227473735809326, loss=0.01817276142537594
I0311 20:37:30.364093 139622172112640 logging_writer.py:48] [131100] global_step=131100, grad_norm=0.17573505640029907, loss=0.019962824881076813
I0311 20:38:01.184863 139789500200768 spec.py:321] Evaluating on the training split.
I0311 20:39:48.777854 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 20:39:53.600162 139789500200768 spec.py:349] Evaluating on the test split.
I0311 20:39:56.569060 139789500200768 submission_runner.py:411] Time since start: 62712.40s, 	Step: 131200, 	{'train/accuracy': 0.9955013394355774, 'train/loss': 0.014089184813201427, 'train/mean_average_precision': 0.7765810385292532, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.28651420235985936, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.2760851045251188, 'test/num_examples': 43793, 'score': 41315.339268922806, 'total_duration': 62712.39630937576, 'accumulated_submission_time': 41315.339268922806, 'accumulated_eval_time': 21386.529180288315, 'accumulated_logging_time': 7.088238716125488}
I0311 20:39:56.610006 139628670904064 logging_writer.py:48] [131200] accumulated_eval_time=21386.529180, accumulated_logging_time=7.088239, accumulated_submission_time=41315.339269, global_step=131200, preemption_count=0, score=41315.339269, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.276085, test/num_examples=43793, total_duration=62712.396309, train/accuracy=0.995501, train/loss=0.014089, train/mean_average_precision=0.776581, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286514, validation/num_examples=43793
I0311 20:39:56.963114 139628679296768 logging_writer.py:48] [131200] global_step=131200, grad_norm=0.1341005563735962, loss=0.01675265096127987
I0311 20:40:28.019620 139628670904064 logging_writer.py:48] [131300] global_step=131300, grad_norm=0.16833120584487915, loss=0.021341845393180847
I0311 20:40:59.442086 139628679296768 logging_writer.py:48] [131400] global_step=131400, grad_norm=0.13879050314426422, loss=0.017947347834706306
I0311 20:41:30.727079 139628670904064 logging_writer.py:48] [131500] global_step=131500, grad_norm=0.14692991971969604, loss=0.015353777445852757
I0311 20:42:01.854113 139628679296768 logging_writer.py:48] [131600] global_step=131600, grad_norm=0.14755208790302277, loss=0.017994966357946396
I0311 20:42:33.357699 139628670904064 logging_writer.py:48] [131700] global_step=131700, grad_norm=0.14885124564170837, loss=0.016925500705838203
I0311 20:43:04.885040 139628679296768 logging_writer.py:48] [131800] global_step=131800, grad_norm=0.1419171392917633, loss=0.0176699161529541
I0311 20:43:36.076811 139628670904064 logging_writer.py:48] [131900] global_step=131900, grad_norm=0.15540601313114166, loss=0.01977025903761387
I0311 20:43:56.838052 139789500200768 spec.py:321] Evaluating on the training split.
I0311 20:45:37.902866 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 20:45:40.934068 139789500200768 spec.py:349] Evaluating on the test split.
I0311 20:45:43.938424 139789500200768 submission_runner.py:411] Time since start: 63059.77s, 	Step: 131968, 	{'train/accuracy': 0.9955359101295471, 'train/loss': 0.013963292352855206, 'train/mean_average_precision': 0.7859540350520396, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.2865155375239707, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.27602905995395377, 'test/num_examples': 43793, 'score': 41555.53718948364, 'total_duration': 63059.76567673683, 'accumulated_submission_time': 41555.53718948364, 'accumulated_eval_time': 21493.629511117935, 'accumulated_logging_time': 7.139894247055054}
I0311 20:45:43.979753 139621870597888 logging_writer.py:48] [131968] accumulated_eval_time=21493.629511, accumulated_logging_time=7.139894, accumulated_submission_time=41555.537189, global_step=131968, preemption_count=0, score=41555.537189, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.276029, test/num_examples=43793, total_duration=63059.765677, train/accuracy=0.995536, train/loss=0.013963, train/mean_average_precision=0.785954, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286516, validation/num_examples=43793
I0311 20:45:54.399675 139622180505344 logging_writer.py:48] [132000] global_step=132000, grad_norm=0.12969905138015747, loss=0.018359631299972534
I0311 20:46:25.668007 139621870597888 logging_writer.py:48] [132100] global_step=132100, grad_norm=0.13801302015781403, loss=0.018176760524511337
I0311 20:46:57.025774 139622180505344 logging_writer.py:48] [132200] global_step=132200, grad_norm=0.15404252707958221, loss=0.016163576394319534
I0311 20:47:28.430943 139621870597888 logging_writer.py:48] [132300] global_step=132300, grad_norm=0.14371350407600403, loss=0.018468016758561134
I0311 20:47:59.384453 139622180505344 logging_writer.py:48] [132400] global_step=132400, grad_norm=0.1482589840888977, loss=0.018625546246767044
I0311 20:48:30.663541 139621870597888 logging_writer.py:48] [132500] global_step=132500, grad_norm=0.14786741137504578, loss=0.01648636907339096
I0311 20:49:01.681735 139622180505344 logging_writer.py:48] [132600] global_step=132600, grad_norm=0.1427655965089798, loss=0.017136726528406143
I0311 20:49:33.169083 139621870597888 logging_writer.py:48] [132700] global_step=132700, grad_norm=0.16260790824890137, loss=0.017768535763025284
I0311 20:49:44.165619 139789500200768 spec.py:321] Evaluating on the training split.
I0311 20:51:28.208645 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 20:51:31.276095 139789500200768 spec.py:349] Evaluating on the test split.
I0311 20:51:36.066207 139789500200768 submission_runner.py:411] Time since start: 63411.89s, 	Step: 132736, 	{'train/accuracy': 0.9955663084983826, 'train/loss': 0.013797402381896973, 'train/mean_average_precision': 0.780957904115043, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.28649713901629664, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.2759484631551082, 'test/num_examples': 43793, 'score': 41795.693004608154, 'total_duration': 63411.89345788956, 'accumulated_submission_time': 41795.693004608154, 'accumulated_eval_time': 21605.530052900314, 'accumulated_logging_time': 7.191572189331055}
I0311 20:51:36.107609 139622172112640 logging_writer.py:48] [132736] accumulated_eval_time=21605.530053, accumulated_logging_time=7.191572, accumulated_submission_time=41795.693005, global_step=132736, preemption_count=0, score=41795.693005, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.275948, test/num_examples=43793, total_duration=63411.893458, train/accuracy=0.995566, train/loss=0.013797, train/mean_average_precision=0.780958, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286497, validation/num_examples=43793
I0311 20:51:56.337391 139628670904064 logging_writer.py:48] [132800] global_step=132800, grad_norm=0.16703490912914276, loss=0.017875470221042633
I0311 20:52:27.708786 139622172112640 logging_writer.py:48] [132900] global_step=132900, grad_norm=0.1521238088607788, loss=0.017595287412405014
I0311 20:52:58.826246 139628670904064 logging_writer.py:48] [133000] global_step=133000, grad_norm=0.1628783941268921, loss=0.020413164049386978
I0311 20:53:30.039111 139622172112640 logging_writer.py:48] [133100] global_step=133100, grad_norm=0.1413760781288147, loss=0.01721145212650299
I0311 20:54:01.456700 139628670904064 logging_writer.py:48] [133200] global_step=133200, grad_norm=0.13958020508289337, loss=0.017083769664168358
I0311 20:54:32.943531 139622172112640 logging_writer.py:48] [133300] global_step=133300, grad_norm=0.1677422672510147, loss=0.020334608852863312
I0311 20:55:04.642585 139628670904064 logging_writer.py:48] [133400] global_step=133400, grad_norm=0.14966820180416107, loss=0.017564835026860237
I0311 20:55:36.081588 139622172112640 logging_writer.py:48] [133500] global_step=133500, grad_norm=0.12804315984249115, loss=0.01709182932972908
I0311 20:55:36.086677 139789500200768 spec.py:321] Evaluating on the training split.
I0311 20:57:19.420473 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 20:57:22.515189 139789500200768 spec.py:349] Evaluating on the test split.
I0311 20:57:25.535843 139789500200768 submission_runner.py:411] Time since start: 63761.36s, 	Step: 133501, 	{'train/accuracy': 0.9955506920814514, 'train/loss': 0.013920744881033897, 'train/mean_average_precision': 0.7788296776529676, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.2866552551738057, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.27594367475452314, 'test/num_examples': 43793, 'score': 42035.6406815052, 'total_duration': 63761.363078832626, 'accumulated_submission_time': 42035.6406815052, 'accumulated_eval_time': 21714.979140996933, 'accumulated_logging_time': 7.2446510791778564}
I0311 20:57:25.577053 139621870597888 logging_writer.py:48] [133501] accumulated_eval_time=21714.979141, accumulated_logging_time=7.244651, accumulated_submission_time=42035.640682, global_step=133501, preemption_count=0, score=42035.640682, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.275944, test/num_examples=43793, total_duration=63761.363079, train/accuracy=0.995551, train/loss=0.013921, train/mean_average_precision=0.778830, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286655, validation/num_examples=43793
I0311 20:57:57.564644 139628679296768 logging_writer.py:48] [133600] global_step=133600, grad_norm=0.1293770670890808, loss=0.01681109145283699
I0311 20:58:28.747833 139621870597888 logging_writer.py:48] [133700] global_step=133700, grad_norm=0.1371220052242279, loss=0.017872318625450134
I0311 20:58:59.906144 139628679296768 logging_writer.py:48] [133800] global_step=133800, grad_norm=0.15434372425079346, loss=0.017598815262317657
I0311 20:59:31.263929 139621870597888 logging_writer.py:48] [133900] global_step=133900, grad_norm=0.13996252417564392, loss=0.019572848454117775
I0311 21:00:02.680747 139628679296768 logging_writer.py:48] [134000] global_step=134000, grad_norm=0.153749480843544, loss=0.01864716224372387
I0311 21:00:34.048689 139621870597888 logging_writer.py:48] [134100] global_step=134100, grad_norm=0.13766935467720032, loss=0.0186237171292305
I0311 21:01:05.336483 139628679296768 logging_writer.py:48] [134200] global_step=134200, grad_norm=0.15516647696495056, loss=0.01876772940158844
I0311 21:01:25.729541 139789500200768 spec.py:321] Evaluating on the training split.
I0311 21:03:07.871620 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 21:03:10.975248 139789500200768 spec.py:349] Evaluating on the test split.
I0311 21:03:13.983755 139789500200768 submission_runner.py:411] Time since start: 64109.81s, 	Step: 134266, 	{'train/accuracy': 0.995567262172699, 'train/loss': 0.013836799189448357, 'train/mean_average_precision': 0.7788725334472248, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.2864781178666014, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426059290766716, 'test/mean_average_precision': 0.27602731707863737, 'test/num_examples': 43793, 'score': 42275.762905836105, 'total_duration': 64109.81100869179, 'accumulated_submission_time': 42275.762905836105, 'accumulated_eval_time': 21823.23331308365, 'accumulated_logging_time': 7.296629428863525}
I0311 21:03:14.024299 139622172112640 logging_writer.py:48] [134266] accumulated_eval_time=21823.233313, accumulated_logging_time=7.296629, accumulated_submission_time=42275.762906, global_step=134266, preemption_count=0, score=42275.762906, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.276027, test/num_examples=43793, total_duration=64109.811009, train/accuracy=0.995567, train/loss=0.013837, train/mean_average_precision=0.778873, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286478, validation/num_examples=43793
I0311 21:03:25.163075 139628670904064 logging_writer.py:48] [134300] global_step=134300, grad_norm=0.14778658747673035, loss=0.01571636088192463
I0311 21:03:56.685292 139622172112640 logging_writer.py:48] [134400] global_step=134400, grad_norm=0.15233324468135834, loss=0.01805979013442993
I0311 21:04:28.162647 139628670904064 logging_writer.py:48] [134500] global_step=134500, grad_norm=0.17438995838165283, loss=0.02268456481397152
I0311 21:04:59.621286 139622172112640 logging_writer.py:48] [134600] global_step=134600, grad_norm=0.12776827812194824, loss=0.01652848720550537
I0311 21:05:32.343299 139628670904064 logging_writer.py:48] [134700] global_step=134700, grad_norm=0.14530876278877258, loss=0.018748143687844276
I0311 21:06:05.243210 139622172112640 logging_writer.py:48] [134800] global_step=134800, grad_norm=0.1269317865371704, loss=0.01740363799035549
I0311 21:06:37.924784 139628670904064 logging_writer.py:48] [134900] global_step=134900, grad_norm=0.13954302668571472, loss=0.017910117283463478
I0311 21:07:11.072568 139622172112640 logging_writer.py:48] [135000] global_step=135000, grad_norm=0.14367438852787018, loss=0.018808571621775627
I0311 21:07:14.207931 139789500200768 spec.py:321] Evaluating on the training split.
I0311 21:09:08.405077 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 21:09:11.826643 139789500200768 spec.py:349] Evaluating on the test split.
I0311 21:09:15.127194 139789500200768 submission_runner.py:411] Time since start: 64470.95s, 	Step: 135010, 	{'train/accuracy': 0.99552983045578, 'train/loss': 0.013988321647047997, 'train/mean_average_precision': 0.7758701306845568, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.28658756939426494, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.2760359550998511, 'test/num_examples': 43793, 'score': 42515.91206884384, 'total_duration': 64470.954426050186, 'accumulated_submission_time': 42515.91206884384, 'accumulated_eval_time': 21944.152539014816, 'accumulated_logging_time': 7.349959850311279}
I0311 21:09:15.174664 139622180505344 logging_writer.py:48] [135010] accumulated_eval_time=21944.152539, accumulated_logging_time=7.349960, accumulated_submission_time=42515.912069, global_step=135010, preemption_count=0, score=42515.912069, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.276036, test/num_examples=43793, total_duration=64470.954426, train/accuracy=0.995530, train/loss=0.013988, train/mean_average_precision=0.775870, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286588, validation/num_examples=43793
I0311 21:09:44.490652 139628679296768 logging_writer.py:48] [135100] global_step=135100, grad_norm=0.14596475660800934, loss=0.01929646171629429
I0311 21:10:17.063648 139622180505344 logging_writer.py:48] [135200] global_step=135200, grad_norm=0.150260791182518, loss=0.017394930124282837
I0311 21:10:49.381970 139628679296768 logging_writer.py:48] [135300] global_step=135300, grad_norm=0.1472320556640625, loss=0.017634013667702675
I0311 21:11:21.847319 139622180505344 logging_writer.py:48] [135400] global_step=135400, grad_norm=0.15079399943351746, loss=0.01891830936074257
I0311 21:11:54.123042 139628679296768 logging_writer.py:48] [135500] global_step=135500, grad_norm=0.11744263768196106, loss=0.01666058413684368
I0311 21:12:26.410193 139622180505344 logging_writer.py:48] [135600] global_step=135600, grad_norm=0.14578655362129211, loss=0.017414500936865807
I0311 21:12:58.515922 139628679296768 logging_writer.py:48] [135700] global_step=135700, grad_norm=0.13787943124771118, loss=0.018033334985375404
I0311 21:13:15.298309 139789500200768 spec.py:321] Evaluating on the training split.
I0311 21:15:02.755868 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 21:15:07.888900 139789500200768 spec.py:349] Evaluating on the test split.
I0311 21:15:10.967530 139789500200768 submission_runner.py:411] Time since start: 64826.79s, 	Step: 135753, 	{'train/accuracy': 0.9955183863639832, 'train/loss': 0.014059795998036861, 'train/mean_average_precision': 0.7834770621587747, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.2864998181212718, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.27609019193261725, 'test/num_examples': 43793, 'score': 42756.0000565052, 'total_duration': 64826.79478096962, 'accumulated_submission_time': 42756.0000565052, 'accumulated_eval_time': 22059.821719884872, 'accumulated_logging_time': 7.40891695022583}
I0311 21:15:11.009510 139621870597888 logging_writer.py:48] [135753] accumulated_eval_time=22059.821720, accumulated_logging_time=7.408917, accumulated_submission_time=42756.000057, global_step=135753, preemption_count=0, score=42756.000057, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.276090, test/num_examples=43793, total_duration=64826.794781, train/accuracy=0.995518, train/loss=0.014060, train/mean_average_precision=0.783477, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286500, validation/num_examples=43793
I0311 21:15:26.427778 139628670904064 logging_writer.py:48] [135800] global_step=135800, grad_norm=0.14926184713840485, loss=0.017922135069966316
I0311 21:15:58.427030 139621870597888 logging_writer.py:48] [135900] global_step=135900, grad_norm=0.15861599147319794, loss=0.020070699974894524
I0311 21:16:29.899264 139628670904064 logging_writer.py:48] [136000] global_step=136000, grad_norm=0.147419273853302, loss=0.015459471382200718
I0311 21:17:01.143957 139621870597888 logging_writer.py:48] [136100] global_step=136100, grad_norm=0.1416022926568985, loss=0.017290212213993073
I0311 21:17:32.683714 139628670904064 logging_writer.py:48] [136200] global_step=136200, grad_norm=0.14789770543575287, loss=0.018043385818600655
I0311 21:18:04.640295 139621870597888 logging_writer.py:48] [136300] global_step=136300, grad_norm=0.1425406038761139, loss=0.017422497272491455
I0311 21:18:36.194888 139628670904064 logging_writer.py:48] [136400] global_step=136400, grad_norm=0.16243164241313934, loss=0.01790444739162922
I0311 21:19:07.667753 139621870597888 logging_writer.py:48] [136500] global_step=136500, grad_norm=0.1497708261013031, loss=0.018317658454179764
I0311 21:19:11.124935 139789500200768 spec.py:321] Evaluating on the training split.
I0311 21:20:56.195102 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 21:20:59.286699 139789500200768 spec.py:349] Evaluating on the test split.
I0311 21:21:02.364205 139789500200768 submission_runner.py:411] Time since start: 65178.19s, 	Step: 136512, 	{'train/accuracy': 0.995557963848114, 'train/loss': 0.013795390725135803, 'train/mean_average_precision': 0.7795278572179078, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.2864227948563459, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.2759971165969379, 'test/num_examples': 43793, 'score': 42996.08527255058, 'total_duration': 65178.19145774841, 'accumulated_submission_time': 42996.08527255058, 'accumulated_eval_time': 22171.06094479561, 'accumulated_logging_time': 7.4614715576171875}
I0311 21:21:02.404409 139622172112640 logging_writer.py:48] [136512] accumulated_eval_time=22171.060945, accumulated_logging_time=7.461472, accumulated_submission_time=42996.085273, global_step=136512, preemption_count=0, score=42996.085273, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.275997, test/num_examples=43793, total_duration=65178.191458, train/accuracy=0.995558, train/loss=0.013795, train/mean_average_precision=0.779528, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286423, validation/num_examples=43793
I0311 21:21:30.532498 139628679296768 logging_writer.py:48] [136600] global_step=136600, grad_norm=0.13495101034641266, loss=0.014691459946334362
I0311 21:22:02.255772 139622172112640 logging_writer.py:48] [136700] global_step=136700, grad_norm=0.13962531089782715, loss=0.017364075407385826
I0311 21:22:33.783779 139628679296768 logging_writer.py:48] [136800] global_step=136800, grad_norm=0.15712618827819824, loss=0.020295068621635437
I0311 21:23:05.324764 139622172112640 logging_writer.py:48] [136900] global_step=136900, grad_norm=0.15852734446525574, loss=0.019697559997439384
I0311 21:23:36.991405 139628679296768 logging_writer.py:48] [137000] global_step=137000, grad_norm=0.1326228529214859, loss=0.018440498039126396
I0311 21:24:08.491244 139622172112640 logging_writer.py:48] [137100] global_step=137100, grad_norm=0.13115203380584717, loss=0.017703192308545113
I0311 21:24:39.928685 139628679296768 logging_writer.py:48] [137200] global_step=137200, grad_norm=0.13516193628311157, loss=0.0165841244161129
I0311 21:25:02.623905 139789500200768 spec.py:321] Evaluating on the training split.
I0311 21:26:47.894574 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 21:26:50.970173 139789500200768 spec.py:349] Evaluating on the test split.
I0311 21:26:55.807774 139789500200768 submission_runner.py:411] Time since start: 65531.64s, 	Step: 137273, 	{'train/accuracy': 0.9955756664276123, 'train/loss': 0.013854223303496838, 'train/mean_average_precision': 0.7806127574085215, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.28662013861570323, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.27592748898107045, 'test/num_examples': 43793, 'score': 43236.27346968651, 'total_duration': 65531.63501739502, 'accumulated_submission_time': 43236.27346968651, 'accumulated_eval_time': 22284.24476671219, 'accumulated_logging_time': 7.513568878173828}
I0311 21:26:55.850284 139621870597888 logging_writer.py:48] [137273] accumulated_eval_time=22284.244767, accumulated_logging_time=7.513569, accumulated_submission_time=43236.273470, global_step=137273, preemption_count=0, score=43236.273470, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.275927, test/num_examples=43793, total_duration=65531.635017, train/accuracy=0.995576, train/loss=0.013854, train/mean_average_precision=0.780613, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286620, validation/num_examples=43793
I0311 21:27:04.693624 139622180505344 logging_writer.py:48] [137300] global_step=137300, grad_norm=0.15354745090007782, loss=0.017631886526942253
I0311 21:27:36.506584 139621870597888 logging_writer.py:48] [137400] global_step=137400, grad_norm=0.13678377866744995, loss=0.017909808084368706
I0311 21:28:07.782161 139622180505344 logging_writer.py:48] [137500] global_step=137500, grad_norm=0.13359102606773376, loss=0.017763076350092888
I0311 21:28:39.006502 139621870597888 logging_writer.py:48] [137600] global_step=137600, grad_norm=0.15706275403499603, loss=0.01888319104909897
I0311 21:29:10.486407 139622180505344 logging_writer.py:48] [137700] global_step=137700, grad_norm=0.15251919627189636, loss=0.017560167238116264
I0311 21:29:41.953222 139621870597888 logging_writer.py:48] [137800] global_step=137800, grad_norm=0.13840757310390472, loss=0.018039608374238014
I0311 21:30:13.770524 139622180505344 logging_writer.py:48] [137900] global_step=137900, grad_norm=0.16453228890895844, loss=0.01776728220283985
I0311 21:30:45.425471 139621870597888 logging_writer.py:48] [138000] global_step=138000, grad_norm=0.13372156023979187, loss=0.017718560993671417
I0311 21:30:56.114797 139789500200768 spec.py:321] Evaluating on the training split.
I0311 21:32:40.802021 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 21:32:43.890309 139789500200768 spec.py:349] Evaluating on the test split.
I0311 21:32:46.928499 139789500200768 submission_runner.py:411] Time since start: 65882.76s, 	Step: 138035, 	{'train/accuracy': 0.9955047369003296, 'train/loss': 0.014028721489012241, 'train/mean_average_precision': 0.776828284611313, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.286563876407547, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.27597715595115024, 'test/num_examples': 43793, 'score': 43476.50746488571, 'total_duration': 65882.75573444366, 'accumulated_submission_time': 43476.50746488571, 'accumulated_eval_time': 22395.058416366577, 'accumulated_logging_time': 7.567029237747192}
I0311 21:32:46.970315 139628670904064 logging_writer.py:48] [138035] accumulated_eval_time=22395.058416, accumulated_logging_time=7.567029, accumulated_submission_time=43476.507465, global_step=138035, preemption_count=0, score=43476.507465, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.275977, test/num_examples=43793, total_duration=65882.755734, train/accuracy=0.995505, train/loss=0.014029, train/mean_average_precision=0.776828, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286564, validation/num_examples=43793
I0311 21:33:07.887861 139628679296768 logging_writer.py:48] [138100] global_step=138100, grad_norm=0.13604037463665009, loss=0.015391165390610695
I0311 21:33:38.941124 139628670904064 logging_writer.py:48] [138200] global_step=138200, grad_norm=0.14509645104408264, loss=0.017455097287893295
I0311 21:34:10.405143 139628679296768 logging_writer.py:48] [138300] global_step=138300, grad_norm=0.13750463724136353, loss=0.01886826381087303
I0311 21:34:41.794585 139628670904064 logging_writer.py:48] [138400] global_step=138400, grad_norm=0.13924062252044678, loss=0.018752356991171837
I0311 21:35:13.388806 139628679296768 logging_writer.py:48] [138500] global_step=138500, grad_norm=0.14759980142116547, loss=0.01836937479674816
I0311 21:35:44.607196 139628670904064 logging_writer.py:48] [138600] global_step=138600, grad_norm=0.16033141314983368, loss=0.019327286630868912
I0311 21:36:15.951267 139628679296768 logging_writer.py:48] [138700] global_step=138700, grad_norm=0.1411932110786438, loss=0.014206567779183388
I0311 21:36:47.048592 139628670904064 logging_writer.py:48] [138800] global_step=138800, grad_norm=0.1449238508939743, loss=0.01787920482456684
I0311 21:36:47.053558 139789500200768 spec.py:321] Evaluating on the training split.
I0311 21:38:31.032478 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 21:38:34.048400 139789500200768 spec.py:349] Evaluating on the test split.
I0311 21:38:38.912135 139789500200768 submission_runner.py:411] Time since start: 66234.74s, 	Step: 138801, 	{'train/accuracy': 0.9955438375473022, 'train/loss': 0.013939352706074715, 'train/mean_average_precision': 0.7791969983498022, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.2866075502097671, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426059290766716, 'test/mean_average_precision': 0.27602143679740193, 'test/num_examples': 43793, 'score': 43716.56052541733, 'total_duration': 66234.73925423622, 'accumulated_submission_time': 43716.56052541733, 'accumulated_eval_time': 22506.916797161102, 'accumulated_logging_time': 7.619933605194092}
I0311 21:38:38.954396 139621870597888 logging_writer.py:48] [138801] accumulated_eval_time=22506.916797, accumulated_logging_time=7.619934, accumulated_submission_time=43716.560525, global_step=138801, preemption_count=0, score=43716.560525, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.276021, test/num_examples=43793, total_duration=66234.739254, train/accuracy=0.995544, train/loss=0.013939, train/mean_average_precision=0.779197, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286608, validation/num_examples=43793
I0311 21:39:11.333388 139622172112640 logging_writer.py:48] [138900] global_step=138900, grad_norm=0.15072673559188843, loss=0.01972009427845478
I0311 21:39:42.547657 139621870597888 logging_writer.py:48] [139000] global_step=139000, grad_norm=0.15346591174602509, loss=0.01771865226328373
I0311 21:40:14.022857 139622172112640 logging_writer.py:48] [139100] global_step=139100, grad_norm=0.1453772783279419, loss=0.018521564081311226
I0311 21:40:45.285692 139621870597888 logging_writer.py:48] [139200] global_step=139200, grad_norm=0.13858361542224884, loss=0.01582319103181362
I0311 21:41:19.349949 139622172112640 logging_writer.py:48] [139300] global_step=139300, grad_norm=0.1385459303855896, loss=0.017758576199412346
I0311 21:41:52.458373 139621870597888 logging_writer.py:48] [139400] global_step=139400, grad_norm=0.13368657231330872, loss=0.017936108633875847
I0311 21:42:24.017091 139622172112640 logging_writer.py:48] [139500] global_step=139500, grad_norm=0.13573402166366577, loss=0.016904564574360847
I0311 21:42:39.190573 139789500200768 spec.py:321] Evaluating on the training split.
I0311 21:44:20.659528 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 21:44:23.748681 139789500200768 spec.py:349] Evaluating on the test split.
I0311 21:44:26.697342 139789500200768 submission_runner.py:411] Time since start: 66582.52s, 	Step: 139549, 	{'train/accuracy': 0.9955278635025024, 'train/loss': 0.014000042341649532, 'train/mean_average_precision': 0.7743114486500873, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.2867690866867088, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.2760094643162549, 'test/num_examples': 43793, 'score': 43956.76708507538, 'total_duration': 66582.52459478378, 'accumulated_submission_time': 43956.76708507538, 'accumulated_eval_time': 22614.423523902893, 'accumulated_logging_time': 7.672637939453125}
I0311 21:44:26.738813 139622180505344 logging_writer.py:48] [139549] accumulated_eval_time=22614.423524, accumulated_logging_time=7.672638, accumulated_submission_time=43956.767085, global_step=139549, preemption_count=0, score=43956.767085, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.276009, test/num_examples=43793, total_duration=66582.524595, train/accuracy=0.995528, train/loss=0.014000, train/mean_average_precision=0.774311, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286769, validation/num_examples=43793
I0311 21:44:43.214445 139628679296768 logging_writer.py:48] [139600] global_step=139600, grad_norm=0.12788192927837372, loss=0.01612171344459057
I0311 21:45:14.341753 139622180505344 logging_writer.py:48] [139700] global_step=139700, grad_norm=0.14923503994941711, loss=0.01780652441084385
I0311 21:45:45.501526 139628679296768 logging_writer.py:48] [139800] global_step=139800, grad_norm=0.1375550925731659, loss=0.017285341396927834
I0311 21:46:16.830931 139622180505344 logging_writer.py:48] [139900] global_step=139900, grad_norm=0.16500918567180634, loss=0.016362208873033524
I0311 21:46:47.750691 139628679296768 logging_writer.py:48] [140000] global_step=140000, grad_norm=0.1402207612991333, loss=0.019038591533899307
I0311 21:47:18.859282 139622180505344 logging_writer.py:48] [140100] global_step=140100, grad_norm=0.14412112534046173, loss=0.0192327369004488
I0311 21:47:50.046782 139628679296768 logging_writer.py:48] [140200] global_step=140200, grad_norm=0.1461644023656845, loss=0.017247291281819344
I0311 21:48:21.161258 139622180505344 logging_writer.py:48] [140300] global_step=140300, grad_norm=0.13927395641803741, loss=0.017872707918286324
I0311 21:48:26.797067 139789500200768 spec.py:321] Evaluating on the training split.
I0311 21:50:09.827816 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 21:50:12.883724 139789500200768 spec.py:349] Evaluating on the test split.
I0311 21:50:15.906067 139789500200768 submission_runner.py:411] Time since start: 66931.73s, 	Step: 140319, 	{'train/accuracy': 0.9955880641937256, 'train/loss': 0.013792039826512337, 'train/mean_average_precision': 0.7879608890674992, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.28647347798427675, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426059290766716, 'test/mean_average_precision': 0.2759517549835037, 'test/num_examples': 43793, 'score': 44196.79552650452, 'total_duration': 66931.73331785202, 'accumulated_submission_time': 44196.79552650452, 'accumulated_eval_time': 22723.532479286194, 'accumulated_logging_time': 7.724453449249268}
I0311 21:50:15.948279 139622172112640 logging_writer.py:48] [140319] accumulated_eval_time=22723.532479, accumulated_logging_time=7.724453, accumulated_submission_time=44196.795527, global_step=140319, preemption_count=0, score=44196.795527, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.275952, test/num_examples=43793, total_duration=66931.733318, train/accuracy=0.995588, train/loss=0.013792, train/mean_average_precision=0.787961, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286473, validation/num_examples=43793
I0311 21:50:41.891889 139628670904064 logging_writer.py:48] [140400] global_step=140400, grad_norm=0.1570948213338852, loss=0.019052822142839432
I0311 21:51:13.266569 139622172112640 logging_writer.py:48] [140500] global_step=140500, grad_norm=0.13945837318897247, loss=0.016284270212054253
I0311 21:51:45.064147 139628670904064 logging_writer.py:48] [140600] global_step=140600, grad_norm=0.13156373798847198, loss=0.018552465364336967
I0311 21:52:16.405586 139622172112640 logging_writer.py:48] [140700] global_step=140700, grad_norm=0.14976926147937775, loss=0.01911267824470997
I0311 21:52:47.809029 139628670904064 logging_writer.py:48] [140800] global_step=140800, grad_norm=0.15705177187919617, loss=0.017685720697045326
I0311 21:53:19.629847 139622172112640 logging_writer.py:48] [140900] global_step=140900, grad_norm=0.14046025276184082, loss=0.018608639016747475
I0311 21:53:50.955695 139628670904064 logging_writer.py:48] [141000] global_step=141000, grad_norm=0.15622404217720032, loss=0.01832328736782074
I0311 21:54:16.059545 139789500200768 spec.py:321] Evaluating on the training split.
I0311 21:56:01.841372 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 21:56:04.920727 139789500200768 spec.py:349] Evaluating on the test split.
I0311 21:56:07.925023 139789500200768 submission_runner.py:411] Time since start: 67283.75s, 	Step: 141081, 	{'train/accuracy': 0.9955501556396484, 'train/loss': 0.013860519044101238, 'train/mean_average_precision': 0.7867203846922961, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.2865235551818129, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.2760356463316414, 'test/num_examples': 43793, 'score': 44436.87666225433, 'total_duration': 67283.75227689743, 'accumulated_submission_time': 44436.87666225433, 'accumulated_eval_time': 22835.397917747498, 'accumulated_logging_time': 7.777060031890869}
I0311 21:56:07.966687 139622180505344 logging_writer.py:48] [141081] accumulated_eval_time=22835.397918, accumulated_logging_time=7.777060, accumulated_submission_time=44436.876662, global_step=141081, preemption_count=0, score=44436.876662, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.276036, test/num_examples=43793, total_duration=67283.752277, train/accuracy=0.995550, train/loss=0.013861, train/mean_average_precision=0.786720, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286524, validation/num_examples=43793
I0311 21:56:14.284523 139628679296768 logging_writer.py:48] [141100] global_step=141100, grad_norm=0.1316065639257431, loss=0.014862265437841415
I0311 21:56:46.139432 139622180505344 logging_writer.py:48] [141200] global_step=141200, grad_norm=0.14573627710342407, loss=0.016927890479564667
I0311 21:57:18.207961 139628679296768 logging_writer.py:48] [141300] global_step=141300, grad_norm=0.15686634182929993, loss=0.019272996112704277
I0311 21:57:49.927989 139622180505344 logging_writer.py:48] [141400] global_step=141400, grad_norm=0.1351405531167984, loss=0.016399528831243515
I0311 21:58:21.773852 139628679296768 logging_writer.py:48] [141500] global_step=141500, grad_norm=0.17372074723243713, loss=0.02035464160144329
I0311 21:58:53.458861 139622180505344 logging_writer.py:48] [141600] global_step=141600, grad_norm=0.1546502709388733, loss=0.020604675635695457
I0311 21:59:24.722778 139628679296768 logging_writer.py:48] [141700] global_step=141700, grad_norm=0.14441831409931183, loss=0.01704593002796173
I0311 21:59:56.492200 139622180505344 logging_writer.py:48] [141800] global_step=141800, grad_norm=0.13659103214740753, loss=0.01777869276702404
I0311 22:00:07.968852 139789500200768 spec.py:321] Evaluating on the training split.
I0311 22:01:49.096072 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 22:01:54.023377 139789500200768 spec.py:349] Evaluating on the test split.
I0311 22:01:57.070382 139789500200768 submission_runner.py:411] Time since start: 67632.90s, 	Step: 141837, 	{'train/accuracy': 0.9954988956451416, 'train/loss': 0.014038309454917908, 'train/mean_average_precision': 0.7701409373357102, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.28666304198103054, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.2760171261581809, 'test/num_examples': 43793, 'score': 44676.84755587578, 'total_duration': 67632.89763379097, 'accumulated_submission_time': 44676.84755587578, 'accumulated_eval_time': 22944.49940443039, 'accumulated_logging_time': 7.830647706985474}
I0311 22:01:57.113134 139621870597888 logging_writer.py:48] [141837] accumulated_eval_time=22944.499404, accumulated_logging_time=7.830648, accumulated_submission_time=44676.847556, global_step=141837, preemption_count=0, score=44676.847556, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.276017, test/num_examples=43793, total_duration=67632.897634, train/accuracy=0.995499, train/loss=0.014038, train/mean_average_precision=0.770141, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286663, validation/num_examples=43793
I0311 22:02:17.142357 139628670904064 logging_writer.py:48] [141900] global_step=141900, grad_norm=0.13935290277004242, loss=0.01727546751499176
I0311 22:02:48.616957 139621870597888 logging_writer.py:48] [142000] global_step=142000, grad_norm=0.1421452909708023, loss=0.01705174148082733
I0311 22:03:19.797900 139628670904064 logging_writer.py:48] [142100] global_step=142100, grad_norm=0.12914280593395233, loss=0.017256569117307663
I0311 22:03:50.936231 139621870597888 logging_writer.py:48] [142200] global_step=142200, grad_norm=0.15000303089618683, loss=0.01900937594473362
I0311 22:04:21.973446 139628670904064 logging_writer.py:48] [142300] global_step=142300, grad_norm=0.1446407437324524, loss=0.01911449246108532
I0311 22:04:53.146020 139621870597888 logging_writer.py:48] [142400] global_step=142400, grad_norm=0.1444694697856903, loss=0.01873774081468582
I0311 22:05:24.147858 139628670904064 logging_writer.py:48] [142500] global_step=142500, grad_norm=0.1281140148639679, loss=0.016578366979956627
I0311 22:05:55.085807 139621870597888 logging_writer.py:48] [142600] global_step=142600, grad_norm=0.14863690733909607, loss=0.016628995537757874
I0311 22:05:57.289371 139789500200768 spec.py:321] Evaluating on the training split.
I0311 22:07:39.562633 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 22:07:42.641883 139789500200768 spec.py:349] Evaluating on the test split.
I0311 22:07:45.654051 139789500200768 submission_runner.py:411] Time since start: 67981.48s, 	Step: 142608, 	{'train/accuracy': 0.9955378770828247, 'train/loss': 0.013882885687053204, 'train/mean_average_precision': 0.7796853291287564, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.28645223265282793, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.27596771552929106, 'test/num_examples': 43793, 'score': 44916.99302315712, 'total_duration': 67981.48130369186, 'accumulated_submission_time': 44916.99302315712, 'accumulated_eval_time': 23052.86403822899, 'accumulated_logging_time': 7.88399076461792}
I0311 22:07:45.697151 139622172112640 logging_writer.py:48] [142608] accumulated_eval_time=23052.864038, accumulated_logging_time=7.883991, accumulated_submission_time=44916.993023, global_step=142608, preemption_count=0, score=44916.993023, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.275968, test/num_examples=43793, total_duration=67981.481304, train/accuracy=0.995538, train/loss=0.013883, train/mean_average_precision=0.779685, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286452, validation/num_examples=43793
I0311 22:08:15.391017 139628679296768 logging_writer.py:48] [142700] global_step=142700, grad_norm=0.13600558042526245, loss=0.01616247184574604
I0311 22:08:47.339836 139622172112640 logging_writer.py:48] [142800] global_step=142800, grad_norm=0.14368964731693268, loss=0.017659271135926247
I0311 22:09:19.015963 139628679296768 logging_writer.py:48] [142900] global_step=142900, grad_norm=0.15352267026901245, loss=0.019323518499732018
I0311 22:09:50.878695 139622172112640 logging_writer.py:48] [143000] global_step=143000, grad_norm=0.14175041019916534, loss=0.01748836599290371
I0311 22:10:22.603069 139628679296768 logging_writer.py:48] [143100] global_step=143100, grad_norm=0.16185572743415833, loss=0.018261978402733803
I0311 22:10:54.452268 139622172112640 logging_writer.py:48] [143200] global_step=143200, grad_norm=0.15955665707588196, loss=0.01800893247127533
I0311 22:11:25.709596 139628679296768 logging_writer.py:48] [143300] global_step=143300, grad_norm=0.13482266664505005, loss=0.018107840791344643
I0311 22:11:45.711169 139789500200768 spec.py:321] Evaluating on the training split.
I0311 22:13:30.799071 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 22:13:33.900257 139789500200768 spec.py:349] Evaluating on the test split.
I0311 22:13:38.777276 139789500200768 submission_runner.py:411] Time since start: 68334.60s, 	Step: 143365, 	{'train/accuracy': 0.995568573474884, 'train/loss': 0.013904176652431488, 'train/mean_average_precision': 0.7736875430872415, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.2865738946807334, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.2759283175225142, 'test/num_examples': 43793, 'score': 45156.97582030296, 'total_duration': 68334.60452127457, 'accumulated_submission_time': 45156.97582030296, 'accumulated_eval_time': 23165.930099487305, 'accumulated_logging_time': 7.938454627990723}
I0311 22:13:38.820927 139621870597888 logging_writer.py:48] [143365] accumulated_eval_time=23165.930099, accumulated_logging_time=7.938455, accumulated_submission_time=45156.975820, global_step=143365, preemption_count=0, score=45156.975820, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.275928, test/num_examples=43793, total_duration=68334.604521, train/accuracy=0.995569, train/loss=0.013904, train/mean_average_precision=0.773688, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286574, validation/num_examples=43793
I0311 22:13:50.093582 139628670904064 logging_writer.py:48] [143400] global_step=143400, grad_norm=0.15142850577831268, loss=0.019504806026816368
I0311 22:14:21.342926 139621870597888 logging_writer.py:48] [143500] global_step=143500, grad_norm=0.15849719941616058, loss=0.018872566521167755
I0311 22:14:52.684046 139628670904064 logging_writer.py:48] [143600] global_step=143600, grad_norm=0.13989955186843872, loss=0.01759619079530239
I0311 22:15:24.021910 139621870597888 logging_writer.py:48] [143700] global_step=143700, grad_norm=0.12740041315555573, loss=0.016314778476953506
I0311 22:15:55.344053 139628670904064 logging_writer.py:48] [143800] global_step=143800, grad_norm=0.15185609459877014, loss=0.017832983285188675
I0311 22:16:26.663010 139621870597888 logging_writer.py:48] [143900] global_step=143900, grad_norm=0.14337408542633057, loss=0.01899314671754837
I0311 22:16:58.160931 139628670904064 logging_writer.py:48] [144000] global_step=144000, grad_norm=0.1544281244277954, loss=0.0174320787191391
I0311 22:17:30.047918 139621870597888 logging_writer.py:48] [144100] global_step=144100, grad_norm=0.1358804851770401, loss=0.01769459806382656
I0311 22:17:38.816411 139789500200768 spec.py:321] Evaluating on the training split.
I0311 22:19:19.837651 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 22:19:22.896266 139789500200768 spec.py:349] Evaluating on the test split.
I0311 22:19:25.842834 139789500200768 submission_runner.py:411] Time since start: 68681.67s, 	Step: 144129, 	{'train/accuracy': 0.9955149292945862, 'train/loss': 0.014027275145053864, 'train/mean_average_precision': 0.7838841100376359, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.28657804767817013, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426059290766716, 'test/mean_average_precision': 0.2759656585305678, 'test/num_examples': 43793, 'score': 45396.94107532501, 'total_duration': 68681.67007613182, 'accumulated_submission_time': 45396.94107532501, 'accumulated_eval_time': 23272.95647072792, 'accumulated_logging_time': 7.992566347122192}
I0311 22:19:25.886505 139622180505344 logging_writer.py:48] [144129] accumulated_eval_time=23272.956471, accumulated_logging_time=7.992566, accumulated_submission_time=45396.941075, global_step=144129, preemption_count=0, score=45396.941075, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.275966, test/num_examples=43793, total_duration=68681.670076, train/accuracy=0.995515, train/loss=0.014027, train/mean_average_precision=0.783884, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286578, validation/num_examples=43793
I0311 22:19:48.297904 139628679296768 logging_writer.py:48] [144200] global_step=144200, grad_norm=0.1475556492805481, loss=0.020729828625917435
I0311 22:20:19.573052 139622180505344 logging_writer.py:48] [144300] global_step=144300, grad_norm=0.1331857293844223, loss=0.017171617597341537
I0311 22:20:50.580702 139628679296768 logging_writer.py:48] [144400] global_step=144400, grad_norm=0.135929137468338, loss=0.016233673319220543
I0311 22:21:21.932816 139622180505344 logging_writer.py:48] [144500] global_step=144500, grad_norm=0.142819344997406, loss=0.018704965710639954
I0311 22:21:52.991208 139628679296768 logging_writer.py:48] [144600] global_step=144600, grad_norm=0.13920079171657562, loss=0.018434494733810425
I0311 22:22:24.169301 139622180505344 logging_writer.py:48] [144700] global_step=144700, grad_norm=0.13730640709400177, loss=0.017736583948135376
I0311 22:22:55.032152 139628679296768 logging_writer.py:48] [144800] global_step=144800, grad_norm=0.14935638010501862, loss=0.017737699672579765
I0311 22:23:26.111700 139622180505344 logging_writer.py:48] [144900] global_step=144900, grad_norm=0.15504899621009827, loss=0.019937671720981598
I0311 22:23:26.116415 139789500200768 spec.py:321] Evaluating on the training split.
I0311 22:25:02.985380 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 22:25:06.252472 139789500200768 spec.py:349] Evaluating on the test split.
I0311 22:25:09.466423 139789500200768 submission_runner.py:411] Time since start: 69025.29s, 	Step: 144901, 	{'train/accuracy': 0.9955885410308838, 'train/loss': 0.01379482727497816, 'train/mean_average_precision': 0.781838371574247, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.2865226670030312, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426059290766716, 'test/mean_average_precision': 0.27600089157566543, 'test/num_examples': 43793, 'score': 45637.140635728836, 'total_duration': 69025.29367923737, 'accumulated_submission_time': 45637.140635728836, 'accumulated_eval_time': 23376.306419849396, 'accumulated_logging_time': 8.046898603439331}
I0311 22:25:09.508759 139622172112640 logging_writer.py:48] [144901] accumulated_eval_time=23376.306420, accumulated_logging_time=8.046899, accumulated_submission_time=45637.140636, global_step=144901, preemption_count=0, score=45637.140636, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.276001, test/num_examples=43793, total_duration=69025.293679, train/accuracy=0.995589, train/loss=0.013795, train/mean_average_precision=0.781838, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286523, validation/num_examples=43793
I0311 22:25:40.445682 139628670904064 logging_writer.py:48] [145000] global_step=145000, grad_norm=0.14492109417915344, loss=0.017828697338700294
I0311 22:26:11.549714 139622172112640 logging_writer.py:48] [145100] global_step=145100, grad_norm=0.14260739088058472, loss=0.01896350085735321
I0311 22:26:42.645600 139628670904064 logging_writer.py:48] [145200] global_step=145200, grad_norm=0.15785925090312958, loss=0.016961125656962395
I0311 22:27:13.720453 139622172112640 logging_writer.py:48] [145300] global_step=145300, grad_norm=0.14375105500221252, loss=0.017686093226075172
I0311 22:27:44.938055 139628670904064 logging_writer.py:48] [145400] global_step=145400, grad_norm=0.13847975432872772, loss=0.015614915639162064
I0311 22:28:16.143787 139622172112640 logging_writer.py:48] [145500] global_step=145500, grad_norm=0.13772766292095184, loss=0.01594753935933113
I0311 22:28:47.037324 139628670904064 logging_writer.py:48] [145600] global_step=145600, grad_norm=0.1578962355852127, loss=0.01927725225687027
I0311 22:29:09.755479 139789500200768 spec.py:321] Evaluating on the training split.
I0311 22:30:52.821395 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 22:30:55.920886 139789500200768 spec.py:349] Evaluating on the test split.
I0311 22:30:58.954385 139789500200768 submission_runner.py:411] Time since start: 69374.78s, 	Step: 145674, 	{'train/accuracy': 0.995506227016449, 'train/loss': 0.014016961678862572, 'train/mean_average_precision': 0.7722065956630976, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.28650109272623636, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.276041130417176, 'test/num_examples': 43793, 'score': 45877.35671019554, 'total_duration': 69374.78163695335, 'accumulated_submission_time': 45877.35671019554, 'accumulated_eval_time': 23485.505282640457, 'accumulated_logging_time': 8.099945545196533}
I0311 22:30:58.997863 139621870597888 logging_writer.py:48] [145674] accumulated_eval_time=23485.505283, accumulated_logging_time=8.099946, accumulated_submission_time=45877.356710, global_step=145674, preemption_count=0, score=45877.356710, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.276041, test/num_examples=43793, total_duration=69374.781637, train/accuracy=0.995506, train/loss=0.014017, train/mean_average_precision=0.772207, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286501, validation/num_examples=43793
I0311 22:31:07.364193 139628679296768 logging_writer.py:48] [145700] global_step=145700, grad_norm=0.13667215406894684, loss=0.01713716611266136
I0311 22:31:38.259248 139621870597888 logging_writer.py:48] [145800] global_step=145800, grad_norm=0.15212945640087128, loss=0.01652567833662033
I0311 22:32:09.944103 139628679296768 logging_writer.py:48] [145900] global_step=145900, grad_norm=0.14775530993938446, loss=0.019858870655298233
I0311 22:32:41.133013 139621870597888 logging_writer.py:48] [146000] global_step=146000, grad_norm=0.13307438790798187, loss=0.015005555003881454
I0311 22:33:12.317951 139628679296768 logging_writer.py:48] [146100] global_step=146100, grad_norm=0.13930177688598633, loss=0.016621189191937447
I0311 22:33:43.452590 139621870597888 logging_writer.py:48] [146200] global_step=146200, grad_norm=0.1584865003824234, loss=0.016727708280086517
I0311 22:34:14.670947 139628679296768 logging_writer.py:48] [146300] global_step=146300, grad_norm=0.14422187209129333, loss=0.01954079419374466
I0311 22:34:46.013846 139621870597888 logging_writer.py:48] [146400] global_step=146400, grad_norm=0.13342024385929108, loss=0.015744324773550034
I0311 22:34:59.012596 139789500200768 spec.py:321] Evaluating on the training split.
I0311 22:36:40.970829 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 22:36:44.060455 139789500200768 spec.py:349] Evaluating on the test split.
I0311 22:36:47.048253 139789500200768 submission_runner.py:411] Time since start: 69722.88s, 	Step: 146443, 	{'train/accuracy': 0.9955160021781921, 'train/loss': 0.013951887376606464, 'train/mean_average_precision': 0.7824941855650105, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.286583906969833, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.27593177842395994, 'test/num_examples': 43793, 'score': 46117.34045815468, 'total_duration': 69722.8755030632, 'accumulated_submission_time': 46117.34045815468, 'accumulated_eval_time': 23593.540898799896, 'accumulated_logging_time': 8.154316425323486}
I0311 22:36:47.090914 139622172112640 logging_writer.py:48] [146443] accumulated_eval_time=23593.540899, accumulated_logging_time=8.154316, accumulated_submission_time=46117.340458, global_step=146443, preemption_count=0, score=46117.340458, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.275932, test/num_examples=43793, total_duration=69722.875503, train/accuracy=0.995516, train/loss=0.013952, train/mean_average_precision=0.782494, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286584, validation/num_examples=43793
I0311 22:37:05.007319 139628670904064 logging_writer.py:48] [146500] global_step=146500, grad_norm=0.15023526549339294, loss=0.01965455897152424
I0311 22:37:36.576336 139622172112640 logging_writer.py:48] [146600] global_step=146600, grad_norm=0.14572444558143616, loss=0.018770424649119377
I0311 22:38:07.828831 139628670904064 logging_writer.py:48] [146700] global_step=146700, grad_norm=0.14376993477344513, loss=0.01813766546547413
I0311 22:38:38.908301 139622172112640 logging_writer.py:48] [146800] global_step=146800, grad_norm=0.1404389888048172, loss=0.018584895879030228
I0311 22:39:10.028762 139628670904064 logging_writer.py:48] [146900] global_step=146900, grad_norm=0.13700871169567108, loss=0.017801756039261818
I0311 22:39:40.975610 139622172112640 logging_writer.py:48] [147000] global_step=147000, grad_norm=0.12876830995082855, loss=0.017153728753328323
I0311 22:40:12.200707 139628670904064 logging_writer.py:48] [147100] global_step=147100, grad_norm=0.13506802916526794, loss=0.018300926312804222
I0311 22:40:42.962846 139622172112640 logging_writer.py:48] [147200] global_step=147200, grad_norm=0.1583094596862793, loss=0.018305137753486633
I0311 22:40:47.300331 139789500200768 spec.py:321] Evaluating on the training split.
I0311 22:42:29.212198 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 22:42:32.252701 139789500200768 spec.py:349] Evaluating on the test split.
I0311 22:42:35.241797 139789500200768 submission_runner.py:411] Time since start: 70071.07s, 	Step: 147215, 	{'train/accuracy': 0.9956077337265015, 'train/loss': 0.013790295459330082, 'train/mean_average_precision': 0.7769709209696616, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.28641898446741215, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.27596930348809867, 'test/num_examples': 43793, 'score': 46357.51933145523, 'total_duration': 70071.06904149055, 'accumulated_submission_time': 46357.51933145523, 'accumulated_eval_time': 23701.482311964035, 'accumulated_logging_time': 8.207560777664185}
I0311 22:42:35.283535 139622180505344 logging_writer.py:48] [147215] accumulated_eval_time=23701.482312, accumulated_logging_time=8.207561, accumulated_submission_time=46357.519331, global_step=147215, preemption_count=0, score=46357.519331, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.275969, test/num_examples=43793, total_duration=70071.069041, train/accuracy=0.995608, train/loss=0.013790, train/mean_average_precision=0.776971, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286419, validation/num_examples=43793
I0311 22:43:02.137709 139628679296768 logging_writer.py:48] [147300] global_step=147300, grad_norm=0.15316654741764069, loss=0.019531000405550003
I0311 22:43:33.226600 139622180505344 logging_writer.py:48] [147400] global_step=147400, grad_norm=0.14423900842666626, loss=0.01777617447078228
I0311 22:44:04.225288 139628679296768 logging_writer.py:48] [147500] global_step=147500, grad_norm=0.15630444884300232, loss=0.021740945056080818
I0311 22:44:35.226039 139622180505344 logging_writer.py:48] [147600] global_step=147600, grad_norm=0.14696551859378815, loss=0.018429145216941833
I0311 22:45:06.150138 139628679296768 logging_writer.py:48] [147700] global_step=147700, grad_norm=0.13435842096805573, loss=0.01687098667025566
I0311 22:45:37.467460 139622180505344 logging_writer.py:48] [147800] global_step=147800, grad_norm=0.14996238052845, loss=0.0191278625279665
I0311 22:46:08.328976 139628679296768 logging_writer.py:48] [147900] global_step=147900, grad_norm=0.1657177358865738, loss=0.02003798820078373
I0311 22:46:35.494284 139789500200768 spec.py:321] Evaluating on the training split.
I0311 22:48:16.324868 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 22:48:19.426206 139789500200768 spec.py:349] Evaluating on the test split.
I0311 22:48:22.428338 139789500200768 submission_runner.py:411] Time since start: 70418.26s, 	Step: 147989, 	{'train/accuracy': 0.9954946637153625, 'train/loss': 0.014093567617237568, 'train/mean_average_precision': 0.7765513850956582, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.28653992410093787, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.27596771645486534, 'test/num_examples': 43793, 'score': 46597.699701309204, 'total_duration': 70418.25558948517, 'accumulated_submission_time': 46597.699701309204, 'accumulated_eval_time': 23808.41632771492, 'accumulated_logging_time': 8.259890794754028}
I0311 22:48:22.470340 139621870597888 logging_writer.py:48] [147989] accumulated_eval_time=23808.416328, accumulated_logging_time=8.259891, accumulated_submission_time=46597.699701, global_step=147989, preemption_count=0, score=46597.699701, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.275968, test/num_examples=43793, total_duration=70418.255589, train/accuracy=0.995495, train/loss=0.014094, train/mean_average_precision=0.776551, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286540, validation/num_examples=43793
I0311 22:48:26.239000 139622172112640 logging_writer.py:48] [148000] global_step=148000, grad_norm=0.1510910987854004, loss=0.019269797950983047
I0311 22:48:57.091954 139621870597888 logging_writer.py:48] [148100] global_step=148100, grad_norm=0.1545323133468628, loss=0.017945226281881332
I0311 22:49:27.801104 139622172112640 logging_writer.py:48] [148200] global_step=148200, grad_norm=0.13828763365745544, loss=0.016288088634610176
I0311 22:49:58.463156 139621870597888 logging_writer.py:48] [148300] global_step=148300, grad_norm=0.15330199897289276, loss=0.01992386393249035
I0311 22:50:29.577350 139622172112640 logging_writer.py:48] [148400] global_step=148400, grad_norm=0.15167175233364105, loss=0.018384743481874466
I0311 22:51:00.710695 139621870597888 logging_writer.py:48] [148500] global_step=148500, grad_norm=0.13122573494911194, loss=0.01785392127931118
I0311 22:51:31.766187 139622172112640 logging_writer.py:48] [148600] global_step=148600, grad_norm=0.14211365580558777, loss=0.017388688400387764
I0311 22:52:02.647954 139621870597888 logging_writer.py:48] [148700] global_step=148700, grad_norm=0.1401021033525467, loss=0.01803305558860302
I0311 22:52:22.719900 139789500200768 spec.py:321] Evaluating on the training split.
I0311 22:54:14.352786 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 22:54:17.756615 139789500200768 spec.py:349] Evaluating on the test split.
I0311 22:54:21.136835 139789500200768 submission_runner.py:411] Time since start: 70776.96s, 	Step: 148766, 	{'train/accuracy': 0.9955323934555054, 'train/loss': 0.013901354745030403, 'train/mean_average_precision': 0.7849211130504072, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.28653870800324505, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.2759949770797097, 'test/num_examples': 43793, 'score': 46837.91800737381, 'total_duration': 70776.96406245232, 'accumulated_submission_time': 46837.91800737381, 'accumulated_eval_time': 23926.83319377899, 'accumulated_logging_time': 8.31392502784729}
I0311 22:54:21.190918 139622180505344 logging_writer.py:48] [148766] accumulated_eval_time=23926.833194, accumulated_logging_time=8.313925, accumulated_submission_time=46837.918007, global_step=148766, preemption_count=0, score=46837.918007, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.275995, test/num_examples=43793, total_duration=70776.964062, train/accuracy=0.995532, train/loss=0.013901, train/mean_average_precision=0.784921, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286539, validation/num_examples=43793
I0311 22:54:32.677787 139628679296768 logging_writer.py:48] [148800] global_step=148800, grad_norm=0.1396043747663498, loss=0.01668049395084381
I0311 22:55:05.471195 139622180505344 logging_writer.py:48] [148900] global_step=148900, grad_norm=0.1412944793701172, loss=0.019198937341570854
I0311 22:55:38.337209 139628679296768 logging_writer.py:48] [149000] global_step=149000, grad_norm=0.13748101890087128, loss=0.014869105070829391
I0311 22:56:11.075866 139622180505344 logging_writer.py:48] [149100] global_step=149100, grad_norm=0.12880739569664001, loss=0.016688985750079155
I0311 22:56:43.736026 139628679296768 logging_writer.py:48] [149200] global_step=149200, grad_norm=0.14710181951522827, loss=0.018634917214512825
I0311 22:57:16.185775 139622180505344 logging_writer.py:48] [149300] global_step=149300, grad_norm=0.14025558531284332, loss=0.01780637539923191
I0311 22:57:48.634182 139628679296768 logging_writer.py:48] [149400] global_step=149400, grad_norm=0.15634752810001373, loss=0.017400408163666725
I0311 22:58:21.103644 139622180505344 logging_writer.py:48] [149500] global_step=149500, grad_norm=0.17033956944942474, loss=0.018985508009791374
I0311 22:58:21.438683 139789500200768 spec.py:321] Evaluating on the training split.
I0311 23:00:12.868447 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 23:00:16.254880 139789500200768 spec.py:349] Evaluating on the test split.
I0311 23:00:19.635214 139789500200768 submission_runner.py:411] Time since start: 71135.46s, 	Step: 149502, 	{'train/accuracy': 0.9955655932426453, 'train/loss': 0.01386684738099575, 'train/mean_average_precision': 0.7834967595954778, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.28652665907909447, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.2759343881185401, 'test/num_examples': 43793, 'score': 47078.129519701004, 'total_duration': 71135.46232533455, 'accumulated_submission_time': 47078.129519701004, 'accumulated_eval_time': 24045.02954697609, 'accumulated_logging_time': 8.379383087158203}
I0311 23:00:19.683851 139621870597888 logging_writer.py:48] [149502] accumulated_eval_time=24045.029547, accumulated_logging_time=8.379383, accumulated_submission_time=47078.129520, global_step=149502, preemption_count=0, score=47078.129520, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.275934, test/num_examples=43793, total_duration=71135.462325, train/accuracy=0.995566, train/loss=0.013867, train/mean_average_precision=0.783497, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286527, validation/num_examples=43793
I0311 23:00:51.477002 139622172112640 logging_writer.py:48] [149600] global_step=149600, grad_norm=0.136098250746727, loss=0.01703512854874134
I0311 23:01:23.935345 139621870597888 logging_writer.py:48] [149700] global_step=149700, grad_norm=0.15013694763183594, loss=0.017945965752005577
I0311 23:01:56.285839 139622172112640 logging_writer.py:48] [149800] global_step=149800, grad_norm=0.1426125466823578, loss=0.017803866416215897
I0311 23:02:28.866781 139621870597888 logging_writer.py:48] [149900] global_step=149900, grad_norm=0.13672979176044464, loss=0.01850396953523159
I0311 23:03:01.365612 139622172112640 logging_writer.py:48] [150000] global_step=150000, grad_norm=0.12513381242752075, loss=0.01600399985909462
I0311 23:03:35.962279 139621870597888 logging_writer.py:48] [150100] global_step=150100, grad_norm=0.12648841738700867, loss=0.01563354954123497
I0311 23:04:10.982692 139622172112640 logging_writer.py:48] [150200] global_step=150200, grad_norm=0.1379876434803009, loss=0.018140872940421104
I0311 23:04:19.967809 139789500200768 spec.py:321] Evaluating on the training split.
I0311 23:06:10.774390 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 23:06:14.325410 139789500200768 spec.py:349] Evaluating on the test split.
I0311 23:06:17.705973 139789500200768 submission_runner.py:411] Time since start: 71493.53s, 	Step: 150227, 	{'train/accuracy': 0.9955136179924011, 'train/loss': 0.013995732180774212, 'train/mean_average_precision': 0.7791588996594946, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.2867352038186914, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426059290766716, 'test/mean_average_precision': 0.27601307039895073, 'test/num_examples': 43793, 'score': 47318.37801861763, 'total_duration': 71493.53320717812, 'accumulated_submission_time': 47318.37801861763, 'accumulated_eval_time': 24162.76766180992, 'accumulated_logging_time': 8.43949007987976}
I0311 23:06:17.756189 139628670904064 logging_writer.py:48] [150227] accumulated_eval_time=24162.767662, accumulated_logging_time=8.439490, accumulated_submission_time=47318.378019, global_step=150227, preemption_count=0, score=47318.378019, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.276013, test/num_examples=43793, total_duration=71493.533207, train/accuracy=0.995514, train/loss=0.013996, train/mean_average_precision=0.779159, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286735, validation/num_examples=43793
I0311 23:06:43.513069 139628679296768 logging_writer.py:48] [150300] global_step=150300, grad_norm=0.14118751883506775, loss=0.015887940302491188
I0311 23:07:16.404773 139628670904064 logging_writer.py:48] [150400] global_step=150400, grad_norm=0.14533604681491852, loss=0.01605924218893051
I0311 23:07:48.120013 139628679296768 logging_writer.py:48] [150500] global_step=150500, grad_norm=0.14064089953899384, loss=0.01657864637672901
I0311 23:08:19.380026 139628670904064 logging_writer.py:48] [150600] global_step=150600, grad_norm=0.1587020456790924, loss=0.020307671278715134
I0311 23:08:50.803722 139628679296768 logging_writer.py:48] [150700] global_step=150700, grad_norm=0.1370825618505478, loss=0.016643913462758064
I0311 23:09:22.255969 139628670904064 logging_writer.py:48] [150800] global_step=150800, grad_norm=0.15144725143909454, loss=0.019786128774285316
I0311 23:09:53.422433 139628679296768 logging_writer.py:48] [150900] global_step=150900, grad_norm=0.13904651999473572, loss=0.01806986890733242
I0311 23:10:17.713947 139789500200768 spec.py:321] Evaluating on the training split.
I0311 23:12:00.446804 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 23:12:03.538348 139789500200768 spec.py:349] Evaluating on the test split.
I0311 23:12:06.545527 139789500200768 submission_runner.py:411] Time since start: 71842.37s, 	Step: 150979, 	{'train/accuracy': 0.9955903887748718, 'train/loss': 0.013775013387203217, 'train/mean_average_precision': 0.7747467790100341, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.2866331928863107, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.27592368250192684, 'test/num_examples': 43793, 'score': 47558.30167579651, 'total_duration': 71842.37278032303, 'accumulated_submission_time': 47558.30167579651, 'accumulated_eval_time': 24271.599202156067, 'accumulated_logging_time': 8.502744197845459}
I0311 23:12:06.589978 139622172112640 logging_writer.py:48] [150979] accumulated_eval_time=24271.599202, accumulated_logging_time=8.502744, accumulated_submission_time=47558.301676, global_step=150979, preemption_count=0, score=47558.301676, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.275924, test/num_examples=43793, total_duration=71842.372780, train/accuracy=0.995590, train/loss=0.013775, train/mean_average_precision=0.774747, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286633, validation/num_examples=43793
I0311 23:12:13.496967 139622180505344 logging_writer.py:48] [151000] global_step=151000, grad_norm=0.1693052351474762, loss=0.017715880647301674
I0311 23:12:44.576795 139622172112640 logging_writer.py:48] [151100] global_step=151100, grad_norm=0.14208519458770752, loss=0.01677815243601799
I0311 23:13:15.624925 139622180505344 logging_writer.py:48] [151200] global_step=151200, grad_norm=0.14898259937763214, loss=0.019467897713184357
I0311 23:13:46.579699 139622172112640 logging_writer.py:48] [151300] global_step=151300, grad_norm=0.13244883716106415, loss=0.01816246658563614
I0311 23:14:18.082358 139622180505344 logging_writer.py:48] [151400] global_step=151400, grad_norm=0.14536906778812408, loss=0.018297353759407997
I0311 23:14:49.233085 139622172112640 logging_writer.py:48] [151500] global_step=151500, grad_norm=0.15421707928180695, loss=0.019482454285025597
I0311 23:15:20.515731 139622180505344 logging_writer.py:48] [151600] global_step=151600, grad_norm=0.14783892035484314, loss=0.01759604550898075
I0311 23:15:51.383575 139622172112640 logging_writer.py:48] [151700] global_step=151700, grad_norm=0.1478271186351776, loss=0.018097370862960815
I0311 23:16:06.620552 139789500200768 spec.py:321] Evaluating on the training split.
I0311 23:17:48.418715 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 23:17:51.439674 139789500200768 spec.py:349] Evaluating on the test split.
I0311 23:17:54.436657 139789500200768 submission_runner.py:411] Time since start: 72190.26s, 	Step: 151750, 	{'train/accuracy': 0.9955427050590515, 'train/loss': 0.014013847336173058, 'train/mean_average_precision': 0.7803063834457855, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.28657613400541815, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.2760510306082031, 'test/num_examples': 43793, 'score': 47798.300828933716, 'total_duration': 72190.26391029358, 'accumulated_submission_time': 47798.300828933716, 'accumulated_eval_time': 24379.415264368057, 'accumulated_logging_time': 8.558963775634766}
I0311 23:17:54.480397 139621870597888 logging_writer.py:48] [151750] accumulated_eval_time=24379.415264, accumulated_logging_time=8.558964, accumulated_submission_time=47798.300829, global_step=151750, preemption_count=0, score=47798.300829, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.276051, test/num_examples=43793, total_duration=72190.263910, train/accuracy=0.995543, train/loss=0.014014, train/mean_average_precision=0.780306, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286576, validation/num_examples=43793
I0311 23:18:10.897476 139628679296768 logging_writer.py:48] [151800] global_step=151800, grad_norm=0.13260827958583832, loss=0.015225542709231377
I0311 23:18:41.967179 139621870597888 logging_writer.py:48] [151900] global_step=151900, grad_norm=0.132558673620224, loss=0.017348762601614
I0311 23:19:12.954275 139628679296768 logging_writer.py:48] [152000] global_step=152000, grad_norm=0.15404729545116425, loss=0.018199285492300987
I0311 23:19:43.884867 139621870597888 logging_writer.py:48] [152100] global_step=152100, grad_norm=0.15891624987125397, loss=0.018851587548851967
I0311 23:20:14.713409 139628679296768 logging_writer.py:48] [152200] global_step=152200, grad_norm=0.16361434757709503, loss=0.019924776628613472
I0311 23:20:45.558635 139621870597888 logging_writer.py:48] [152300] global_step=152300, grad_norm=0.15464375913143158, loss=0.019771480932831764
I0311 23:21:16.452872 139628679296768 logging_writer.py:48] [152400] global_step=152400, grad_norm=0.16933514177799225, loss=0.019599460065364838
I0311 23:21:47.706639 139621870597888 logging_writer.py:48] [152500] global_step=152500, grad_norm=0.11990699917078018, loss=0.017376575618982315
I0311 23:21:54.507697 139789500200768 spec.py:321] Evaluating on the training split.
I0311 23:23:34.121636 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 23:23:37.165688 139789500200768 spec.py:349] Evaluating on the test split.
I0311 23:23:40.151571 139789500200768 submission_runner.py:411] Time since start: 72535.98s, 	Step: 152523, 	{'train/accuracy': 0.9955410957336426, 'train/loss': 0.013931354507803917, 'train/mean_average_precision': 0.782091041865194, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.28661110597657197, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.275992589903956, 'test/num_examples': 43793, 'score': 48038.296209573746, 'total_duration': 72535.97880601883, 'accumulated_submission_time': 48038.296209573746, 'accumulated_eval_time': 24485.05907702446, 'accumulated_logging_time': 8.614659547805786}
I0311 23:23:40.196743 139622172112640 logging_writer.py:48] [152523] accumulated_eval_time=24485.059077, accumulated_logging_time=8.614660, accumulated_submission_time=48038.296210, global_step=152523, preemption_count=0, score=48038.296210, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.275993, test/num_examples=43793, total_duration=72535.978806, train/accuracy=0.995541, train/loss=0.013931, train/mean_average_precision=0.782091, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286611, validation/num_examples=43793
I0311 23:24:04.372263 139622180505344 logging_writer.py:48] [152600] global_step=152600, grad_norm=0.1409263163805008, loss=0.017103297635912895
I0311 23:24:35.627598 139622172112640 logging_writer.py:48] [152700] global_step=152700, grad_norm=0.1426672637462616, loss=0.015840746462345123
I0311 23:25:07.178552 139622180505344 logging_writer.py:48] [152800] global_step=152800, grad_norm=0.1570112407207489, loss=0.021594343706965446
I0311 23:25:38.379379 139622172112640 logging_writer.py:48] [152900] global_step=152900, grad_norm=0.14076754450798035, loss=0.017074525356292725
I0311 23:26:09.762957 139622180505344 logging_writer.py:48] [153000] global_step=153000, grad_norm=0.13168159127235413, loss=0.016588175669312477
I0311 23:26:40.982514 139622172112640 logging_writer.py:48] [153100] global_step=153100, grad_norm=0.16401183605194092, loss=0.02051514945924282
I0311 23:27:11.997429 139622180505344 logging_writer.py:48] [153200] global_step=153200, grad_norm=0.13662296533584595, loss=0.018200984224677086
I0311 23:27:40.401036 139789500200768 spec.py:321] Evaluating on the training split.
I0311 23:29:23.569523 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 23:29:26.627787 139789500200768 spec.py:349] Evaluating on the test split.
I0311 23:29:29.657053 139789500200768 submission_runner.py:411] Time since start: 72885.48s, 	Step: 153291, 	{'train/accuracy': 0.9955719709396362, 'train/loss': 0.013833555392920971, 'train/mean_average_precision': 0.7839643969392222, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.2865632471513083, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426059290766716, 'test/mean_average_precision': 0.27614041183661325, 'test/num_examples': 43793, 'score': 48278.4686756134, 'total_duration': 72885.48430514336, 'accumulated_submission_time': 48278.4686756134, 'accumulated_eval_time': 24594.315055131912, 'accumulated_logging_time': 8.67185354232788}
I0311 23:29:29.700931 139621870597888 logging_writer.py:48] [153291] accumulated_eval_time=24594.315055, accumulated_logging_time=8.671854, accumulated_submission_time=48278.468676, global_step=153291, preemption_count=0, score=48278.468676, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.276140, test/num_examples=43793, total_duration=72885.484305, train/accuracy=0.995572, train/loss=0.013834, train/mean_average_precision=0.783964, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286563, validation/num_examples=43793
I0311 23:29:33.081083 139628670904064 logging_writer.py:48] [153300] global_step=153300, grad_norm=0.13726061582565308, loss=0.018244795501232147
I0311 23:30:04.275547 139621870597888 logging_writer.py:48] [153400] global_step=153400, grad_norm=0.16471466422080994, loss=0.019058961421251297
I0311 23:30:35.022850 139628670904064 logging_writer.py:48] [153500] global_step=153500, grad_norm=0.14152297377586365, loss=0.019339537248015404
I0311 23:31:05.813309 139621870597888 logging_writer.py:48] [153600] global_step=153600, grad_norm=0.13177752494812012, loss=0.016732990741729736
I0311 23:31:36.548778 139628670904064 logging_writer.py:48] [153700] global_step=153700, grad_norm=0.13520365953445435, loss=0.017694547772407532
I0311 23:32:07.370475 139621870597888 logging_writer.py:48] [153800] global_step=153800, grad_norm=0.15339295566082, loss=0.018539074808359146
I0311 23:32:38.041011 139628670904064 logging_writer.py:48] [153900] global_step=153900, grad_norm=0.14825350046157837, loss=0.016741545870900154
I0311 23:33:09.140590 139621870597888 logging_writer.py:48] [154000] global_step=154000, grad_norm=0.14095179736614227, loss=0.016524318605661392
I0311 23:33:29.698220 139789500200768 spec.py:321] Evaluating on the training split.
I0311 23:35:09.881153 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 23:35:12.974352 139789500200768 spec.py:349] Evaluating on the test split.
I0311 23:35:15.967028 139789500200768 submission_runner.py:411] Time since start: 73231.79s, 	Step: 154068, 	{'train/accuracy': 0.9955121278762817, 'train/loss': 0.013970794156193733, 'train/mean_average_precision': 0.7735758383914202, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.28675174433599077, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.2759883245069869, 'test/num_examples': 43793, 'score': 48518.43424797058, 'total_duration': 73231.79427218437, 'accumulated_submission_time': 48518.43424797058, 'accumulated_eval_time': 24700.583837985992, 'accumulated_logging_time': 8.727308988571167}
I0311 23:35:16.010745 139622180505344 logging_writer.py:48] [154068] accumulated_eval_time=24700.583838, accumulated_logging_time=8.727309, accumulated_submission_time=48518.434248, global_step=154068, preemption_count=0, score=48518.434248, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.275988, test/num_examples=43793, total_duration=73231.794272, train/accuracy=0.995512, train/loss=0.013971, train/mean_average_precision=0.773576, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286752, validation/num_examples=43793
I0311 23:35:26.400968 139628679296768 logging_writer.py:48] [154100] global_step=154100, grad_norm=0.1515471488237381, loss=0.017489181831479073
I0311 23:35:57.533504 139622180505344 logging_writer.py:48] [154200] global_step=154200, grad_norm=0.1355099231004715, loss=0.0172945037484169
I0311 23:36:28.562982 139628679296768 logging_writer.py:48] [154300] global_step=154300, grad_norm=0.14314767718315125, loss=0.01886819861829281
I0311 23:36:59.623629 139622180505344 logging_writer.py:48] [154400] global_step=154400, grad_norm=0.16848425567150116, loss=0.01738569512963295
I0311 23:37:31.023760 139628679296768 logging_writer.py:48] [154500] global_step=154500, grad_norm=0.1339845508337021, loss=0.01659751683473587
I0311 23:38:02.092720 139622180505344 logging_writer.py:48] [154600] global_step=154600, grad_norm=0.13533176481723785, loss=0.01817512884736061
I0311 23:38:33.227020 139628679296768 logging_writer.py:48] [154700] global_step=154700, grad_norm=0.1339682936668396, loss=0.017120398581027985
I0311 23:39:04.422372 139622180505344 logging_writer.py:48] [154800] global_step=154800, grad_norm=0.15534278750419617, loss=0.019245320931077003
I0311 23:39:16.078301 139789500200768 spec.py:321] Evaluating on the training split.
I0311 23:41:01.322030 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 23:41:04.408295 139789500200768 spec.py:349] Evaluating on the test split.
I0311 23:41:07.398263 139789500200768 submission_runner.py:411] Time since start: 73583.23s, 	Step: 154839, 	{'train/accuracy': 0.9955089092254639, 'train/loss': 0.01399256568402052, 'train/mean_average_precision': 0.7794013870917653, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.2865083063356302, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.27620420651518673, 'test/num_examples': 43793, 'score': 48758.47049832344, 'total_duration': 73583.22550678253, 'accumulated_submission_time': 48758.47049832344, 'accumulated_eval_time': 24811.903750658035, 'accumulated_logging_time': 8.78290057182312}
I0311 23:41:07.441930 139621870597888 logging_writer.py:48] [154839] accumulated_eval_time=24811.903751, accumulated_logging_time=8.782901, accumulated_submission_time=48758.470498, global_step=154839, preemption_count=0, score=48758.470498, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.276204, test/num_examples=43793, total_duration=73583.225507, train/accuracy=0.995509, train/loss=0.013993, train/mean_average_precision=0.779401, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286508, validation/num_examples=43793
I0311 23:41:26.799293 139628670904064 logging_writer.py:48] [154900] global_step=154900, grad_norm=0.1412944197654724, loss=0.016445767134428024
I0311 23:41:57.497298 139621870597888 logging_writer.py:48] [155000] global_step=155000, grad_norm=0.1463623046875, loss=0.020832393318414688
I0311 23:42:28.772030 139628670904064 logging_writer.py:48] [155100] global_step=155100, grad_norm=0.1349106878042221, loss=0.016831636428833008
I0311 23:43:00.114346 139621870597888 logging_writer.py:48] [155200] global_step=155200, grad_norm=0.12506067752838135, loss=0.016072604805231094
I0311 23:43:31.284465 139628670904064 logging_writer.py:48] [155300] global_step=155300, grad_norm=0.1404649019241333, loss=0.01881360448896885
I0311 23:44:02.072465 139621870597888 logging_writer.py:48] [155400] global_step=155400, grad_norm=0.16005027294158936, loss=0.018737275153398514
I0311 23:44:32.793527 139628670904064 logging_writer.py:48] [155500] global_step=155500, grad_norm=0.13913211226463318, loss=0.016678784042596817
I0311 23:45:04.084598 139621870597888 logging_writer.py:48] [155600] global_step=155600, grad_norm=0.14697948098182678, loss=0.018072474747896194
I0311 23:45:07.510055 139789500200768 spec.py:321] Evaluating on the training split.
I0311 23:46:45.179162 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 23:46:48.228002 139789500200768 spec.py:349] Evaluating on the test split.
I0311 23:46:51.206676 139789500200768 submission_runner.py:411] Time since start: 73927.03s, 	Step: 155612, 	{'train/accuracy': 0.9955801963806152, 'train/loss': 0.013842207379639149, 'train/mean_average_precision': 0.7769311812947006, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.2865149377407092, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.2759311870209673, 'test/num_examples': 43793, 'score': 48998.50784397125, 'total_duration': 73927.03393220901, 'accumulated_submission_time': 48998.50784397125, 'accumulated_eval_time': 24915.600333452225, 'accumulated_logging_time': 8.837835311889648}
I0311 23:46:51.250864 139622180505344 logging_writer.py:48] [155612] accumulated_eval_time=24915.600333, accumulated_logging_time=8.837835, accumulated_submission_time=48998.507844, global_step=155612, preemption_count=0, score=48998.507844, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.275931, test/num_examples=43793, total_duration=73927.033932, train/accuracy=0.995580, train/loss=0.013842, train/mean_average_precision=0.776931, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286515, validation/num_examples=43793
I0311 23:47:19.887964 139628679296768 logging_writer.py:48] [155700] global_step=155700, grad_norm=0.1238759383559227, loss=0.015097794122993946
I0311 23:47:51.602321 139622180505344 logging_writer.py:48] [155800] global_step=155800, grad_norm=0.14520631730556488, loss=0.01690862886607647
I0311 23:48:22.681729 139628679296768 logging_writer.py:48] [155900] global_step=155900, grad_norm=0.13202305138111115, loss=0.016986271366477013
I0311 23:48:53.714491 139622180505344 logging_writer.py:48] [156000] global_step=156000, grad_norm=0.15563185513019562, loss=0.02070586197078228
I0311 23:49:24.816905 139628679296768 logging_writer.py:48] [156100] global_step=156100, grad_norm=0.16650117933750153, loss=0.020063569769263268
I0311 23:49:55.976917 139622180505344 logging_writer.py:48] [156200] global_step=156200, grad_norm=0.13523051142692566, loss=0.016676323488354683
I0311 23:50:27.618414 139628679296768 logging_writer.py:48] [156300] global_step=156300, grad_norm=0.1484699845314026, loss=0.016705799847841263
I0311 23:50:51.249188 139789500200768 spec.py:321] Evaluating on the training split.
I0311 23:52:34.799221 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 23:52:37.861636 139789500200768 spec.py:349] Evaluating on the test split.
I0311 23:52:40.862252 139789500200768 submission_runner.py:411] Time since start: 74276.69s, 	Step: 156377, 	{'train/accuracy': 0.9955509901046753, 'train/loss': 0.013957072049379349, 'train/mean_average_precision': 0.7760353013748329, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.2865370845662255, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.27602190875010923, 'test/num_examples': 43793, 'score': 49238.476517915726, 'total_duration': 74276.68950462341, 'accumulated_submission_time': 49238.476517915726, 'accumulated_eval_time': 25025.213356494904, 'accumulated_logging_time': 8.892682790756226}
I0311 23:52:40.906959 139621870597888 logging_writer.py:48] [156377] accumulated_eval_time=25025.213356, accumulated_logging_time=8.892683, accumulated_submission_time=49238.476518, global_step=156377, preemption_count=0, score=49238.476518, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.276022, test/num_examples=43793, total_duration=74276.689505, train/accuracy=0.995551, train/loss=0.013957, train/mean_average_precision=0.776035, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286537, validation/num_examples=43793
I0311 23:52:48.401628 139628670904064 logging_writer.py:48] [156400] global_step=156400, grad_norm=0.13929659128189087, loss=0.017724357545375824
I0311 23:53:19.554170 139621870597888 logging_writer.py:48] [156500] global_step=156500, grad_norm=0.14529092609882355, loss=0.01845785602927208
I0311 23:53:50.550541 139628670904064 logging_writer.py:48] [156600] global_step=156600, grad_norm=0.14331339299678802, loss=0.018961938098073006
I0311 23:54:21.845307 139621870597888 logging_writer.py:48] [156700] global_step=156700, grad_norm=0.1458098143339157, loss=0.018032336607575417
I0311 23:54:52.927675 139628670904064 logging_writer.py:48] [156800] global_step=156800, grad_norm=0.14235219359397888, loss=0.018121829256415367
I0311 23:55:24.030489 139621870597888 logging_writer.py:48] [156900] global_step=156900, grad_norm=0.15553335845470428, loss=0.019464761018753052
I0311 23:55:55.407763 139628670904064 logging_writer.py:48] [157000] global_step=157000, grad_norm=0.12803319096565247, loss=0.016892919316887856
I0311 23:56:26.767297 139621870597888 logging_writer.py:48] [157100] global_step=157100, grad_norm=0.1392490714788437, loss=0.017040828242897987
I0311 23:56:41.127119 139789500200768 spec.py:321] Evaluating on the training split.
I0311 23:58:19.878064 139789500200768 spec.py:333] Evaluating on the validation split.
I0311 23:58:22.961097 139789500200768 spec.py:349] Evaluating on the test split.
I0311 23:58:25.944136 139789500200768 submission_runner.py:411] Time since start: 74621.77s, 	Step: 157147, 	{'train/accuracy': 0.9955510497093201, 'train/loss': 0.013916567899286747, 'train/mean_average_precision': 0.7797898173653187, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.2865325905894223, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.2760101429392562, 'test/num_examples': 43793, 'score': 49478.666414260864, 'total_duration': 74621.77139091492, 'accumulated_submission_time': 49478.666414260864, 'accumulated_eval_time': 25130.03033232689, 'accumulated_logging_time': 8.947967529296875}
I0311 23:58:25.989176 139622172112640 logging_writer.py:48] [157147] accumulated_eval_time=25130.030332, accumulated_logging_time=8.947968, accumulated_submission_time=49478.666414, global_step=157147, preemption_count=0, score=49478.666414, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.276010, test/num_examples=43793, total_duration=74621.771391, train/accuracy=0.995551, train/loss=0.013917, train/mean_average_precision=0.779790, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286533, validation/num_examples=43793
I0311 23:58:43.057612 139622180505344 logging_writer.py:48] [157200] global_step=157200, grad_norm=0.1342567801475525, loss=0.01718471758067608
I0311 23:59:14.717546 139622172112640 logging_writer.py:48] [157300] global_step=157300, grad_norm=0.14167927205562592, loss=0.018315207213163376
I0311 23:59:46.532185 139622180505344 logging_writer.py:48] [157400] global_step=157400, grad_norm=0.13424494862556458, loss=0.01698349602520466
I0312 00:00:18.190686 139622172112640 logging_writer.py:48] [157500] global_step=157500, grad_norm=0.1390577107667923, loss=0.01779910735785961
I0312 00:00:49.612666 139622180505344 logging_writer.py:48] [157600] global_step=157600, grad_norm=0.1351810246706009, loss=0.01645171456038952
I0312 00:01:20.792007 139622172112640 logging_writer.py:48] [157700] global_step=157700, grad_norm=0.15067939460277557, loss=0.018714062869548798
I0312 00:01:52.038818 139622180505344 logging_writer.py:48] [157800] global_step=157800, grad_norm=0.1513640433549881, loss=0.01936885342001915
I0312 00:02:22.745848 139622172112640 logging_writer.py:48] [157900] global_step=157900, grad_norm=0.13988986611366272, loss=0.017087629064917564
I0312 00:02:26.188985 139789500200768 spec.py:321] Evaluating on the training split.
I0312 00:04:09.871648 139789500200768 spec.py:333] Evaluating on the validation split.
I0312 00:04:12.900279 139789500200768 spec.py:349] Evaluating on the test split.
I0312 00:04:15.870399 139789500200768 submission_runner.py:411] Time since start: 74971.70s, 	Step: 157912, 	{'train/accuracy': 0.9955184459686279, 'train/loss': 0.013942502439022064, 'train/mean_average_precision': 0.7842495488099699, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.28662290126281553, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.27595282023165874, 'test/num_examples': 43793, 'score': 49718.83649778366, 'total_duration': 74971.6976518631, 'accumulated_submission_time': 49718.83649778366, 'accumulated_eval_time': 25239.711701631546, 'accumulated_logging_time': 9.003626585006714}
I0312 00:04:15.914790 139628670904064 logging_writer.py:48] [157912] accumulated_eval_time=25239.711702, accumulated_logging_time=9.003627, accumulated_submission_time=49718.836498, global_step=157912, preemption_count=0, score=49718.836498, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.275953, test/num_examples=43793, total_duration=74971.697652, train/accuracy=0.995518, train/loss=0.013943, train/mean_average_precision=0.784250, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286623, validation/num_examples=43793
I0312 00:04:43.346333 139628679296768 logging_writer.py:48] [158000] global_step=158000, grad_norm=0.1464231014251709, loss=0.017992759123444557
I0312 00:05:14.494058 139628670904064 logging_writer.py:48] [158100] global_step=158100, grad_norm=0.14031343162059784, loss=0.017886938527226448
I0312 00:05:45.397774 139628679296768 logging_writer.py:48] [158200] global_step=158200, grad_norm=0.1489318609237671, loss=0.018740344792604446
I0312 00:06:16.197423 139628670904064 logging_writer.py:48] [158300] global_step=158300, grad_norm=0.1577051728963852, loss=0.02001386694610119
I0312 00:06:47.126630 139628679296768 logging_writer.py:48] [158400] global_step=158400, grad_norm=0.14634737372398376, loss=0.017975445836782455
I0312 00:07:17.908017 139628670904064 logging_writer.py:48] [158500] global_step=158500, grad_norm=0.13567370176315308, loss=0.01868174411356449
I0312 00:07:48.894629 139628679296768 logging_writer.py:48] [158600] global_step=158600, grad_norm=0.13478519022464752, loss=0.016374588012695312
I0312 00:08:16.091224 139789500200768 spec.py:321] Evaluating on the training split.
I0312 00:10:01.460587 139789500200768 spec.py:333] Evaluating on the validation split.
I0312 00:10:04.562005 139789500200768 spec.py:349] Evaluating on the test split.
I0312 00:10:07.613982 139789500200768 submission_runner.py:411] Time since start: 75323.44s, 	Step: 158689, 	{'train/accuracy': 0.9955437779426575, 'train/loss': 0.013895777054131031, 'train/mean_average_precision': 0.7807502432776531, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.28654159169874227, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.27595671718386705, 'test/num_examples': 43793, 'score': 49958.982744932175, 'total_duration': 75323.44123673439, 'accumulated_submission_time': 49958.982744932175, 'accumulated_eval_time': 25351.234421491623, 'accumulated_logging_time': 9.058332443237305}
I0312 00:10:07.659765 139621870597888 logging_writer.py:48] [158689] accumulated_eval_time=25351.234421, accumulated_logging_time=9.058332, accumulated_submission_time=49958.982745, global_step=158689, preemption_count=0, score=49958.982745, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.275957, test/num_examples=43793, total_duration=75323.441237, train/accuracy=0.995544, train/loss=0.013896, train/mean_average_precision=0.780750, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286542, validation/num_examples=43793
I0312 00:10:11.511231 139622172112640 logging_writer.py:48] [158700] global_step=158700, grad_norm=0.1450202316045761, loss=0.016631143167614937
I0312 00:10:42.642210 139621870597888 logging_writer.py:48] [158800] global_step=158800, grad_norm=0.13644172251224518, loss=0.018146006390452385
I0312 00:11:14.026486 139622172112640 logging_writer.py:48] [158900] global_step=158900, grad_norm=0.14925967156887054, loss=0.017674168571829796
I0312 00:11:45.333778 139621870597888 logging_writer.py:48] [159000] global_step=159000, grad_norm=0.15741686522960663, loss=0.019887829199433327
I0312 00:12:16.687917 139622172112640 logging_writer.py:48] [159100] global_step=159100, grad_norm=0.14195296168327332, loss=0.017550881952047348
I0312 00:12:47.897631 139621870597888 logging_writer.py:48] [159200] global_step=159200, grad_norm=0.1469079852104187, loss=0.01854233257472515
I0312 00:13:19.184719 139622172112640 logging_writer.py:48] [159300] global_step=159300, grad_norm=0.13407307863235474, loss=0.016139252111315727
I0312 00:13:50.453565 139621870597888 logging_writer.py:48] [159400] global_step=159400, grad_norm=0.13873742520809174, loss=0.014887770637869835
I0312 00:14:07.910067 139789500200768 spec.py:321] Evaluating on the training split.
I0312 00:15:50.115039 139789500200768 spec.py:333] Evaluating on the validation split.
I0312 00:15:53.167486 139789500200768 spec.py:349] Evaluating on the test split.
I0312 00:15:56.221986 139789500200768 submission_runner.py:411] Time since start: 75672.05s, 	Step: 159457, 	{'train/accuracy': 0.9955360293388367, 'train/loss': 0.013944956474006176, 'train/mean_average_precision': 0.7723770871421647, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.2867485931237354, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.2759523538735477, 'test/num_examples': 43793, 'score': 50199.202701091766, 'total_duration': 75672.04923796654, 'accumulated_submission_time': 50199.202701091766, 'accumulated_eval_time': 25459.5462975502, 'accumulated_logging_time': 9.114988565444946}
I0312 00:15:56.266888 139622180505344 logging_writer.py:48] [159457] accumulated_eval_time=25459.546298, accumulated_logging_time=9.114989, accumulated_submission_time=50199.202701, global_step=159457, preemption_count=0, score=50199.202701, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.275952, test/num_examples=43793, total_duration=75672.049238, train/accuracy=0.995536, train/loss=0.013945, train/mean_average_precision=0.772377, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286749, validation/num_examples=43793
I0312 00:16:10.362145 139628679296768 logging_writer.py:48] [159500] global_step=159500, grad_norm=0.1545584797859192, loss=0.017959976568818092
I0312 00:16:41.993175 139622180505344 logging_writer.py:48] [159600] global_step=159600, grad_norm=0.17294228076934814, loss=0.021198740229010582
I0312 00:17:13.200806 139628679296768 logging_writer.py:48] [159700] global_step=159700, grad_norm=0.1471593976020813, loss=0.018381865695118904
I0312 00:17:44.691743 139622180505344 logging_writer.py:48] [159800] global_step=159800, grad_norm=0.14751099050045013, loss=0.019427796825766563
I0312 00:18:15.920374 139628679296768 logging_writer.py:48] [159900] global_step=159900, grad_norm=0.13562224805355072, loss=0.015719041228294373
I0312 00:18:48.327132 139622180505344 logging_writer.py:48] [160000] global_step=160000, grad_norm=0.17218565940856934, loss=0.018908396363258362
I0312 00:19:20.824660 139628679296768 logging_writer.py:48] [160100] global_step=160100, grad_norm=0.15051501989364624, loss=0.015610136091709137
I0312 00:19:53.444923 139622180505344 logging_writer.py:48] [160200] global_step=160200, grad_norm=0.137649804353714, loss=0.01650826260447502
I0312 00:19:56.376771 139789500200768 spec.py:321] Evaluating on the training split.
I0312 00:21:44.144574 139789500200768 spec.py:333] Evaluating on the validation split.
I0312 00:21:47.621345 139789500200768 spec.py:349] Evaluating on the test split.
I0312 00:21:50.930237 139789500200768 submission_runner.py:411] Time since start: 76026.76s, 	Step: 160210, 	{'train/accuracy': 0.9955352544784546, 'train/loss': 0.013997252099215984, 'train/mean_average_precision': 0.7826859588401474, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.2866698257560256, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.2759910805896102, 'test/num_examples': 43793, 'score': 50439.278101205826, 'total_duration': 76026.75747394562, 'accumulated_submission_time': 50439.278101205826, 'accumulated_eval_time': 25574.099715471268, 'accumulated_logging_time': 9.172712564468384}
I0312 00:21:50.981648 139621870597888 logging_writer.py:48] [160210] accumulated_eval_time=25574.099715, accumulated_logging_time=9.172713, accumulated_submission_time=50439.278101, global_step=160210, preemption_count=0, score=50439.278101, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.275991, test/num_examples=43793, total_duration=76026.757474, train/accuracy=0.995535, train/loss=0.013997, train/mean_average_precision=0.782686, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286670, validation/num_examples=43793
I0312 00:22:20.733031 139628670904064 logging_writer.py:48] [160300] global_step=160300, grad_norm=0.1291581094264984, loss=0.01761290803551674
I0312 00:22:53.363306 139621870597888 logging_writer.py:48] [160400] global_step=160400, grad_norm=0.14929667115211487, loss=0.018871698528528214
I0312 00:23:26.165770 139628670904064 logging_writer.py:48] [160500] global_step=160500, grad_norm=0.14696818590164185, loss=0.01927679032087326
I0312 00:23:59.331441 139621870597888 logging_writer.py:48] [160600] global_step=160600, grad_norm=0.13709059357643127, loss=0.017048627138137817
I0312 00:24:31.998360 139628670904064 logging_writer.py:48] [160700] global_step=160700, grad_norm=0.13811151683330536, loss=0.01655762456357479
I0312 00:25:04.725435 139621870597888 logging_writer.py:48] [160800] global_step=160800, grad_norm=0.1522132158279419, loss=0.019150780513882637
I0312 00:25:37.614706 139628670904064 logging_writer.py:48] [160900] global_step=160900, grad_norm=0.13501936197280884, loss=0.015732524916529655
I0312 00:25:51.155081 139789500200768 spec.py:321] Evaluating on the training split.
I0312 00:27:43.490041 139789500200768 spec.py:333] Evaluating on the validation split.
I0312 00:27:46.598272 139789500200768 spec.py:349] Evaluating on the test split.
I0312 00:27:49.642113 139789500200768 submission_runner.py:411] Time since start: 76385.47s, 	Step: 160942, 	{'train/accuracy': 0.9955859184265137, 'train/loss': 0.013779065571725368, 'train/mean_average_precision': 0.7764327294859807, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.2865785827957917, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.2759355625368492, 'test/num_examples': 43793, 'score': 50679.41657733917, 'total_duration': 76385.46918559074, 'accumulated_submission_time': 50679.41657733917, 'accumulated_eval_time': 25692.586536169052, 'accumulated_logging_time': 9.235629558563232}
I0312 00:27:49.688779 139622180505344 logging_writer.py:48] [160942] accumulated_eval_time=25692.586536, accumulated_logging_time=9.235630, accumulated_submission_time=50679.416577, global_step=160942, preemption_count=0, score=50679.416577, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.275936, test/num_examples=43793, total_duration=76385.469186, train/accuracy=0.995586, train/loss=0.013779, train/mean_average_precision=0.776433, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286579, validation/num_examples=43793
I0312 00:28:08.362437 139628679296768 logging_writer.py:48] [161000] global_step=161000, grad_norm=0.14170436561107635, loss=0.01757809706032276
I0312 00:28:39.695182 139622180505344 logging_writer.py:48] [161100] global_step=161100, grad_norm=0.13998275995254517, loss=0.016414593905210495
I0312 00:29:11.048196 139628679296768 logging_writer.py:48] [161200] global_step=161200, grad_norm=0.15288718044757843, loss=0.019458994269371033
I0312 00:29:42.149732 139622180505344 logging_writer.py:48] [161300] global_step=161300, grad_norm=0.14727017283439636, loss=0.017451060935854912
I0312 00:30:13.407789 139628679296768 logging_writer.py:48] [161400] global_step=161400, grad_norm=0.14052796363830566, loss=0.017391836270689964
I0312 00:30:44.702315 139622180505344 logging_writer.py:48] [161500] global_step=161500, grad_norm=0.15001052618026733, loss=0.017599571496248245
I0312 00:31:16.190398 139628679296768 logging_writer.py:48] [161600] global_step=161600, grad_norm=0.1607905477285385, loss=0.018524089828133583
I0312 00:31:47.324832 139622180505344 logging_writer.py:48] [161700] global_step=161700, grad_norm=0.14999985694885254, loss=0.01872001588344574
I0312 00:31:49.822715 139789500200768 spec.py:321] Evaluating on the training split.
I0312 00:33:30.140352 139789500200768 spec.py:333] Evaluating on the validation split.
I0312 00:33:33.250990 139789500200768 spec.py:349] Evaluating on the test split.
I0312 00:33:36.265359 139789500200768 submission_runner.py:411] Time since start: 76732.09s, 	Step: 161709, 	{'train/accuracy': 0.9955579042434692, 'train/loss': 0.013908610679209232, 'train/mean_average_precision': 0.7868474332606503, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.28650039973884384, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.2759996771960736, 'test/num_examples': 43793, 'score': 50919.51976323128, 'total_duration': 76732.0926129818, 'accumulated_submission_time': 50919.51976323128, 'accumulated_eval_time': 25799.029140233994, 'accumulated_logging_time': 9.29334807395935}
I0312 00:33:36.310037 139621870597888 logging_writer.py:48] [161709] accumulated_eval_time=25799.029140, accumulated_logging_time=9.293348, accumulated_submission_time=50919.519763, global_step=161709, preemption_count=0, score=50919.519763, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.276000, test/num_examples=43793, total_duration=76732.092613, train/accuracy=0.995558, train/loss=0.013909, train/mean_average_precision=0.786847, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286500, validation/num_examples=43793
I0312 00:34:04.689307 139628670904064 logging_writer.py:48] [161800] global_step=161800, grad_norm=0.1493871957063675, loss=0.01754736714065075
I0312 00:34:35.817316 139621870597888 logging_writer.py:48] [161900] global_step=161900, grad_norm=0.1582646369934082, loss=0.018248550593852997
I0312 00:35:07.107342 139628670904064 logging_writer.py:48] [162000] global_step=162000, grad_norm=0.12101965397596359, loss=0.014318189583718777
I0312 00:35:38.094321 139621870597888 logging_writer.py:48] [162100] global_step=162100, grad_norm=0.15876097977161407, loss=0.01995113305747509
I0312 00:36:09.441713 139628670904064 logging_writer.py:48] [162200] global_step=162200, grad_norm=0.1525328904390335, loss=0.01998298056423664
I0312 00:36:40.488373 139621870597888 logging_writer.py:48] [162300] global_step=162300, grad_norm=0.1304360181093216, loss=0.01625276543200016
I0312 00:37:11.507881 139628670904064 logging_writer.py:48] [162400] global_step=162400, grad_norm=0.1374916136264801, loss=0.01818806678056717
I0312 00:37:36.288912 139789500200768 spec.py:321] Evaluating on the training split.
I0312 00:39:17.317808 139789500200768 spec.py:333] Evaluating on the validation split.
I0312 00:39:20.385553 139789500200768 spec.py:349] Evaluating on the test split.
I0312 00:39:23.389522 139789500200768 submission_runner.py:411] Time since start: 77079.22s, 	Step: 162480, 	{'train/accuracy': 0.9955121874809265, 'train/loss': 0.013980649411678314, 'train/mean_average_precision': 0.7751245426310285, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.2865003863132244, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.27607435670064906, 'test/num_examples': 43793, 'score': 51159.46822762489, 'total_duration': 77079.21677541733, 'accumulated_submission_time': 51159.46822762489, 'accumulated_eval_time': 25906.129714250565, 'accumulated_logging_time': 9.348551273345947}
I0312 00:39:23.435717 139622180505344 logging_writer.py:48] [162480] accumulated_eval_time=25906.129714, accumulated_logging_time=9.348551, accumulated_submission_time=51159.468228, global_step=162480, preemption_count=0, score=51159.468228, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.276074, test/num_examples=43793, total_duration=77079.216775, train/accuracy=0.995512, train/loss=0.013981, train/mean_average_precision=0.775125, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286500, validation/num_examples=43793
I0312 00:39:30.026344 139628679296768 logging_writer.py:48] [162500] global_step=162500, grad_norm=0.16883708536624908, loss=0.01754095032811165
I0312 00:40:01.171660 139622180505344 logging_writer.py:48] [162600] global_step=162600, grad_norm=0.13864567875862122, loss=0.01716131530702114
I0312 00:40:32.241570 139628679296768 logging_writer.py:48] [162700] global_step=162700, grad_norm=0.14014305174350739, loss=0.016935598105192184
I0312 00:41:03.244643 139622180505344 logging_writer.py:48] [162800] global_step=162800, grad_norm=0.13049198687076569, loss=0.015220674686133862
I0312 00:41:34.261911 139628679296768 logging_writer.py:48] [162900] global_step=162900, grad_norm=0.12347237765789032, loss=0.014561481773853302
I0312 00:42:05.572012 139622180505344 logging_writer.py:48] [163000] global_step=163000, grad_norm=0.13966399431228638, loss=0.017199944704771042
I0312 00:42:36.787152 139628679296768 logging_writer.py:48] [163100] global_step=163100, grad_norm=0.1420404314994812, loss=0.019151262938976288
I0312 00:43:08.024326 139622180505344 logging_writer.py:48] [163200] global_step=163200, grad_norm=0.1462152600288391, loss=0.019057823345065117
I0312 00:43:23.636867 139789500200768 spec.py:321] Evaluating on the training split.
I0312 00:45:02.759834 139789500200768 spec.py:333] Evaluating on the validation split.
I0312 00:45:05.907422 139789500200768 spec.py:349] Evaluating on the test split.
I0312 00:45:08.881415 139789500200768 submission_runner.py:411] Time since start: 77424.71s, 	Step: 163251, 	{'train/accuracy': 0.9955761432647705, 'train/loss': 0.013812215067446232, 'train/mean_average_precision': 0.7789223056384773, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.2865489837939391, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.2759357398817103, 'test/num_examples': 43793, 'score': 51399.63913941383, 'total_duration': 77424.70866513252, 'accumulated_submission_time': 51399.63913941383, 'accumulated_eval_time': 26011.374217510223, 'accumulated_logging_time': 9.40532898902893}
I0312 00:45:08.928751 139622172112640 logging_writer.py:48] [163251] accumulated_eval_time=26011.374218, accumulated_logging_time=9.405329, accumulated_submission_time=51399.639139, global_step=163251, preemption_count=0, score=51399.639139, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.275936, test/num_examples=43793, total_duration=77424.708665, train/accuracy=0.995576, train/loss=0.013812, train/mean_average_precision=0.778922, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286549, validation/num_examples=43793
I0312 00:45:24.517123 139628670904064 logging_writer.py:48] [163300] global_step=163300, grad_norm=0.1404908150434494, loss=0.018563367426395416
I0312 00:45:55.814427 139622172112640 logging_writer.py:48] [163400] global_step=163400, grad_norm=0.1441672146320343, loss=0.01579730398952961
I0312 00:46:26.875625 139628670904064 logging_writer.py:48] [163500] global_step=163500, grad_norm=0.15453164279460907, loss=0.020640764385461807
I0312 00:46:57.882333 139622172112640 logging_writer.py:48] [163600] global_step=163600, grad_norm=0.1352955549955368, loss=0.016607310622930527
I0312 00:47:29.311780 139628670904064 logging_writer.py:48] [163700] global_step=163700, grad_norm=0.13812004029750824, loss=0.0172476414591074
I0312 00:48:00.436064 139622172112640 logging_writer.py:48] [163800] global_step=163800, grad_norm=0.1466590315103531, loss=0.019010907039046288
I0312 00:48:31.664567 139628670904064 logging_writer.py:48] [163900] global_step=163900, grad_norm=0.1477283090353012, loss=0.01596151851117611
I0312 00:49:03.096770 139622172112640 logging_writer.py:48] [164000] global_step=164000, grad_norm=0.14630502462387085, loss=0.01689300313591957
I0312 00:49:08.979883 139789500200768 spec.py:321] Evaluating on the training split.
I0312 00:50:55.553846 139789500200768 spec.py:333] Evaluating on the validation split.
I0312 00:50:58.610268 139789500200768 spec.py:349] Evaluating on the test split.
I0312 00:51:01.640584 139789500200768 submission_runner.py:411] Time since start: 77777.47s, 	Step: 164020, 	{'train/accuracy': 0.9955022931098938, 'train/loss': 0.014085328206419945, 'train/mean_average_precision': 0.7771181152126543, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.28652771914015324, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.27597761160933704, 'test/num_examples': 43793, 'score': 51639.66033124924, 'total_duration': 77777.46783638, 'accumulated_submission_time': 51639.66033124924, 'accumulated_eval_time': 26124.034873485565, 'accumulated_logging_time': 9.462925434112549}
I0312 00:51:01.686254 139621870597888 logging_writer.py:48] [164020] accumulated_eval_time=26124.034873, accumulated_logging_time=9.462925, accumulated_submission_time=51639.660331, global_step=164020, preemption_count=0, score=51639.660331, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.275978, test/num_examples=43793, total_duration=77777.467836, train/accuracy=0.995502, train/loss=0.014085, train/mean_average_precision=0.777118, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286528, validation/num_examples=43793
I0312 00:51:26.875128 139628679296768 logging_writer.py:48] [164100] global_step=164100, grad_norm=0.1297045648097992, loss=0.01701083779335022
I0312 00:51:57.743676 139621870597888 logging_writer.py:48] [164200] global_step=164200, grad_norm=0.13702213764190674, loss=0.019360164180397987
I0312 00:52:28.996751 139628679296768 logging_writer.py:48] [164300] global_step=164300, grad_norm=0.13813713192939758, loss=0.017604893073439598
I0312 00:53:00.109936 139621870597888 logging_writer.py:48] [164400] global_step=164400, grad_norm=0.13723570108413696, loss=0.018309298902750015
I0312 00:53:31.275162 139628679296768 logging_writer.py:48] [164500] global_step=164500, grad_norm=0.1335940808057785, loss=0.01596977561712265
I0312 00:54:02.513030 139621870597888 logging_writer.py:48] [164600] global_step=164600, grad_norm=0.14796563982963562, loss=0.01833505369722843
I0312 00:54:33.321894 139628679296768 logging_writer.py:48] [164700] global_step=164700, grad_norm=0.14133766293525696, loss=0.01621897704899311
I0312 00:55:01.855103 139789500200768 spec.py:321] Evaluating on the training split.
I0312 00:56:42.242615 139789500200768 spec.py:333] Evaluating on the validation split.
I0312 00:56:45.289200 139789500200768 spec.py:349] Evaluating on the test split.
I0312 00:56:48.301783 139789500200768 submission_runner.py:411] Time since start: 78124.13s, 	Step: 164793, 	{'train/accuracy': 0.995532214641571, 'train/loss': 0.01391457486897707, 'train/mean_average_precision': 0.7748441575197567, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.28651042231846335, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.27602661766552516, 'test/num_examples': 43793, 'score': 51879.79873919487, 'total_duration': 78124.12903475761, 'accumulated_submission_time': 51879.79873919487, 'accumulated_eval_time': 26230.481513261795, 'accumulated_logging_time': 9.519116163253784}
I0312 00:56:48.349123 139622172112640 logging_writer.py:48] [164793] accumulated_eval_time=26230.481513, accumulated_logging_time=9.519116, accumulated_submission_time=51879.798739, global_step=164793, preemption_count=0, score=51879.798739, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.276027, test/num_examples=43793, total_duration=78124.129035, train/accuracy=0.995532, train/loss=0.013915, train/mean_average_precision=0.774844, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286510, validation/num_examples=43793
I0312 00:56:50.899662 139622180505344 logging_writer.py:48] [164800] global_step=164800, grad_norm=0.14525510370731354, loss=0.016192486509680748
I0312 00:57:22.153879 139622172112640 logging_writer.py:48] [164900] global_step=164900, grad_norm=0.12934207916259766, loss=0.015713028609752655
I0312 00:57:53.673935 139622180505344 logging_writer.py:48] [165000] global_step=165000, grad_norm=0.12996788322925568, loss=0.016668522730469704
I0312 00:58:24.991991 139622172112640 logging_writer.py:48] [165100] global_step=165100, grad_norm=0.13973966240882874, loss=0.019053425639867783
I0312 00:58:56.500536 139622180505344 logging_writer.py:48] [165200] global_step=165200, grad_norm=0.14536820352077484, loss=0.017296044155955315
I0312 00:59:28.282952 139622172112640 logging_writer.py:48] [165300] global_step=165300, grad_norm=0.14422917366027832, loss=0.017131801694631577
I0312 00:59:59.808487 139622180505344 logging_writer.py:48] [165400] global_step=165400, grad_norm=0.16479790210723877, loss=0.018738001585006714
I0312 01:00:31.718376 139622172112640 logging_writer.py:48] [165500] global_step=165500, grad_norm=0.13442204892635345, loss=0.01687142252922058
I0312 01:00:48.335663 139789500200768 spec.py:321] Evaluating on the training split.
I0312 01:02:30.455144 139789500200768 spec.py:333] Evaluating on the validation split.
I0312 01:02:33.488064 139789500200768 spec.py:349] Evaluating on the test split.
I0312 01:02:36.518730 139789500200768 submission_runner.py:411] Time since start: 78472.35s, 	Step: 165553, 	{'train/accuracy': 0.9955798387527466, 'train/loss': 0.01384301483631134, 'train/mean_average_precision': 0.7861792908162337, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.28653968998343554, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.27596000830745854, 'test/num_examples': 43793, 'score': 52119.75490355492, 'total_duration': 78472.34597706795, 'accumulated_submission_time': 52119.75490355492, 'accumulated_eval_time': 26338.664535284042, 'accumulated_logging_time': 9.57723355293274}
I0312 01:02:36.565061 139621870597888 logging_writer.py:48] [165553] accumulated_eval_time=26338.664535, accumulated_logging_time=9.577234, accumulated_submission_time=52119.754904, global_step=165553, preemption_count=0, score=52119.754904, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.275960, test/num_examples=43793, total_duration=78472.345977, train/accuracy=0.995580, train/loss=0.013843, train/mean_average_precision=0.786179, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286540, validation/num_examples=43793
I0312 01:02:51.665509 139628670904064 logging_writer.py:48] [165600] global_step=165600, grad_norm=0.1454736888408661, loss=0.017303846776485443
I0312 01:03:22.770925 139621870597888 logging_writer.py:48] [165700] global_step=165700, grad_norm=0.13941572606563568, loss=0.01643366739153862
I0312 01:03:54.237511 139628670904064 logging_writer.py:48] [165800] global_step=165800, grad_norm=0.14908231794834137, loss=0.01633816957473755
I0312 01:04:25.445233 139621870597888 logging_writer.py:48] [165900] global_step=165900, grad_norm=0.15695682168006897, loss=0.01748219132423401
I0312 01:04:56.642461 139628670904064 logging_writer.py:48] [166000] global_step=166000, grad_norm=0.14580217003822327, loss=0.01624431274831295
I0312 01:05:27.891656 139621870597888 logging_writer.py:48] [166100] global_step=166100, grad_norm=0.15370695292949677, loss=0.016738789156079292
I0312 01:05:59.424468 139628670904064 logging_writer.py:48] [166200] global_step=166200, grad_norm=0.14083939790725708, loss=0.017063278704881668
I0312 01:06:30.228559 139621870597888 logging_writer.py:48] [166300] global_step=166300, grad_norm=0.15445765852928162, loss=0.019160885363817215
I0312 01:06:36.772583 139789500200768 spec.py:321] Evaluating on the training split.
I0312 01:08:19.382802 139789500200768 spec.py:333] Evaluating on the validation split.
I0312 01:08:22.427353 139789500200768 spec.py:349] Evaluating on the test split.
I0312 01:08:25.440064 139789500200768 submission_runner.py:411] Time since start: 78821.27s, 	Step: 166322, 	{'train/accuracy': 0.9955323338508606, 'train/loss': 0.01395755261182785, 'train/mean_average_precision': 0.7757177991067279, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.28665719599885714, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.27601530374683786, 'test/num_examples': 43793, 'score': 52359.93154716492, 'total_duration': 78821.26730561256, 'accumulated_submission_time': 52359.93154716492, 'accumulated_eval_time': 26447.33196592331, 'accumulated_logging_time': 9.634422063827515}
I0312 01:08:25.580491 139622172112640 logging_writer.py:48] [166322] accumulated_eval_time=26447.331966, accumulated_logging_time=9.634422, accumulated_submission_time=52359.931547, global_step=166322, preemption_count=0, score=52359.931547, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.276015, test/num_examples=43793, total_duration=78821.267306, train/accuracy=0.995532, train/loss=0.013958, train/mean_average_precision=0.775718, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286657, validation/num_examples=43793
I0312 01:08:50.303316 139622180505344 logging_writer.py:48] [166400] global_step=166400, grad_norm=0.1375156044960022, loss=0.018346887081861496
I0312 01:09:21.154385 139622172112640 logging_writer.py:48] [166500] global_step=166500, grad_norm=0.1571667343378067, loss=0.01830279268324375
I0312 01:09:52.130919 139622180505344 logging_writer.py:48] [166600] global_step=166600, grad_norm=0.14110565185546875, loss=0.017633147537708282
I0312 01:10:23.418459 139622172112640 logging_writer.py:48] [166700] global_step=166700, grad_norm=0.1359119564294815, loss=0.016817688941955566
I0312 01:10:54.121401 139622180505344 logging_writer.py:48] [166800] global_step=166800, grad_norm=0.13330821692943573, loss=0.015370627865195274
I0312 01:11:25.219258 139622172112640 logging_writer.py:48] [166900] global_step=166900, grad_norm=0.12902216613292694, loss=0.0171679574996233
I0312 01:11:56.293372 139622180505344 logging_writer.py:48] [167000] global_step=167000, grad_norm=0.1481601446866989, loss=0.018542904406785965
I0312 01:12:25.698233 139789500200768 spec.py:321] Evaluating on the training split.
I0312 01:14:07.534940 139789500200768 spec.py:333] Evaluating on the validation split.
I0312 01:14:10.587429 139789500200768 spec.py:349] Evaluating on the test split.
I0312 01:14:13.630013 139789500200768 submission_runner.py:411] Time since start: 79169.46s, 	Step: 167095, 	{'train/accuracy': 0.995536208152771, 'train/loss': 0.013926392421126366, 'train/mean_average_precision': 0.7815679243915041, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.28677079381925547, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.27595214360075565, 'test/num_examples': 43793, 'score': 52600.018852472305, 'total_duration': 79169.45726418495, 'accumulated_submission_time': 52600.018852472305, 'accumulated_eval_time': 26555.263708114624, 'accumulated_logging_time': 9.785484075546265}
I0312 01:14:13.677252 139621870597888 logging_writer.py:48] [167095] accumulated_eval_time=26555.263708, accumulated_logging_time=9.785484, accumulated_submission_time=52600.018852, global_step=167095, preemption_count=0, score=52600.018852, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.275952, test/num_examples=43793, total_duration=79169.457264, train/accuracy=0.995536, train/loss=0.013926, train/mean_average_precision=0.781568, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286771, validation/num_examples=43793
I0312 01:14:15.617919 139628670904064 logging_writer.py:48] [167100] global_step=167100, grad_norm=0.14975403249263763, loss=0.018733398988842964
I0312 01:14:46.530089 139621870597888 logging_writer.py:48] [167200] global_step=167200, grad_norm=0.15823642909526825, loss=0.01774255745112896
I0312 01:15:17.821714 139628670904064 logging_writer.py:48] [167300] global_step=167300, grad_norm=0.14876271784305573, loss=0.018490144982933998
I0312 01:15:48.851240 139621870597888 logging_writer.py:48] [167400] global_step=167400, grad_norm=0.1577102690935135, loss=0.017571937292814255
I0312 01:16:19.731178 139628670904064 logging_writer.py:48] [167500] global_step=167500, grad_norm=0.12871643900871277, loss=0.01733807846903801
I0312 01:16:50.398247 139621870597888 logging_writer.py:48] [167600] global_step=167600, grad_norm=0.14639867842197418, loss=0.0173488762229681
I0312 01:17:21.046782 139628670904064 logging_writer.py:48] [167700] global_step=167700, grad_norm=0.15782515704631805, loss=0.018313003703951836
I0312 01:17:51.533790 139621870597888 logging_writer.py:48] [167800] global_step=167800, grad_norm=0.13141533732414246, loss=0.016736727207899094
I0312 01:18:13.896136 139789500200768 spec.py:321] Evaluating on the training split.
I0312 01:19:51.544888 139789500200768 spec.py:333] Evaluating on the validation split.
I0312 01:19:54.547990 139789500200768 spec.py:349] Evaluating on the test split.
I0312 01:19:57.607086 139789500200768 submission_runner.py:411] Time since start: 79513.43s, 	Step: 167874, 	{'train/accuracy': 0.9955568313598633, 'train/loss': 0.01395535096526146, 'train/mean_average_precision': 0.7682549175407976, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.2865932665129089, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.27595583346304425, 'test/num_examples': 43793, 'score': 52840.2066552639, 'total_duration': 79513.43434143066, 'accumulated_submission_time': 52840.2066552639, 'accumulated_eval_time': 26658.97461915016, 'accumulated_logging_time': 9.84364652633667}
I0312 01:19:57.652843 139622172112640 logging_writer.py:48] [167874] accumulated_eval_time=26658.974619, accumulated_logging_time=9.843647, accumulated_submission_time=52840.206655, global_step=167874, preemption_count=0, score=52840.206655, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.275956, test/num_examples=43793, total_duration=79513.434341, train/accuracy=0.995557, train/loss=0.013955, train/mean_average_precision=0.768255, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286593, validation/num_examples=43793
I0312 01:20:06.065606 139622180505344 logging_writer.py:48] [167900] global_step=167900, grad_norm=0.1492195874452591, loss=0.017170412465929985
I0312 01:20:36.999156 139622172112640 logging_writer.py:48] [168000] global_step=168000, grad_norm=0.12805548310279846, loss=0.017659617587924004
I0312 01:21:08.595821 139622180505344 logging_writer.py:48] [168100] global_step=168100, grad_norm=0.1376180350780487, loss=0.01749514974653721
I0312 01:21:42.915952 139622172112640 logging_writer.py:48] [168200] global_step=168200, grad_norm=0.1349378526210785, loss=0.01682927832007408
I0312 01:22:15.657976 139622180505344 logging_writer.py:48] [168300] global_step=168300, grad_norm=0.18039916455745697, loss=0.019729018211364746
I0312 01:22:48.477638 139622172112640 logging_writer.py:48] [168400] global_step=168400, grad_norm=0.1389865279197693, loss=0.017148224636912346
I0312 01:23:21.277202 139622180505344 logging_writer.py:48] [168500] global_step=168500, grad_norm=0.15521414577960968, loss=0.018568305298686028
I0312 01:23:53.179772 139622172112640 logging_writer.py:48] [168600] global_step=168600, grad_norm=0.14138102531433105, loss=0.01697259023785591
I0312 01:23:57.891002 139789500200768 spec.py:321] Evaluating on the training split.
I0312 01:25:37.689050 139789500200768 spec.py:333] Evaluating on the validation split.
I0312 01:25:42.725116 139789500200768 spec.py:349] Evaluating on the test split.
I0312 01:25:45.775843 139789500200768 submission_runner.py:411] Time since start: 79861.60s, 	Step: 168616, 	{'train/accuracy': 0.9955222606658936, 'train/loss': 0.01396720390766859, 'train/mean_average_precision': 0.7815628210311292, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.28654424720000155, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.27600317048976813, 'test/num_examples': 43793, 'score': 53080.41065263748, 'total_duration': 79861.60309934616, 'accumulated_submission_time': 53080.41065263748, 'accumulated_eval_time': 26766.8594186306, 'accumulated_logging_time': 9.901641130447388}
I0312 01:25:45.822824 139621870597888 logging_writer.py:48] [168616] accumulated_eval_time=26766.859419, accumulated_logging_time=9.901641, accumulated_submission_time=53080.410653, global_step=168616, preemption_count=0, score=53080.410653, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.276003, test/num_examples=43793, total_duration=79861.603099, train/accuracy=0.995522, train/loss=0.013967, train/mean_average_precision=0.781563, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286544, validation/num_examples=43793
I0312 01:26:12.858746 139628670904064 logging_writer.py:48] [168700] global_step=168700, grad_norm=0.1549166589975357, loss=0.01726405695080757
I0312 01:26:44.247547 139621870597888 logging_writer.py:48] [168800] global_step=168800, grad_norm=0.15369580686092377, loss=0.016505926847457886
I0312 01:27:15.250269 139628670904064 logging_writer.py:48] [168900] global_step=168900, grad_norm=0.13656111061573029, loss=0.016527963802218437
I0312 01:27:46.543131 139621870597888 logging_writer.py:48] [169000] global_step=169000, grad_norm=0.13069643080234528, loss=0.018554287031292915
I0312 01:28:17.518267 139628670904064 logging_writer.py:48] [169100] global_step=169100, grad_norm=0.14184688031673431, loss=0.016359170898795128
I0312 01:28:49.003636 139621870597888 logging_writer.py:48] [169200] global_step=169200, grad_norm=0.13012848794460297, loss=0.017490513622760773
I0312 01:29:20.027173 139628670904064 logging_writer.py:48] [169300] global_step=169300, grad_norm=0.14454121887683868, loss=0.018217645585536957
I0312 01:29:45.969339 139789500200768 spec.py:321] Evaluating on the training split.
I0312 01:31:26.563579 139789500200768 spec.py:333] Evaluating on the validation split.
I0312 01:31:29.562929 139789500200768 spec.py:349] Evaluating on the test split.
I0312 01:31:32.545510 139789500200768 submission_runner.py:411] Time since start: 80208.37s, 	Step: 169385, 	{'train/accuracy': 0.9955328702926636, 'train/loss': 0.013932731002569199, 'train/mean_average_precision': 0.784643210829564, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.2866681688143653, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.2759793808137554, 'test/num_examples': 43793, 'score': 53320.526663541794, 'total_duration': 80208.37276148796, 'accumulated_submission_time': 53320.526663541794, 'accumulated_eval_time': 26873.435554027557, 'accumulated_logging_time': 9.959112644195557}
I0312 01:31:32.592232 139622180505344 logging_writer.py:48] [169385] accumulated_eval_time=26873.435554, accumulated_logging_time=9.959113, accumulated_submission_time=53320.526664, global_step=169385, preemption_count=0, score=53320.526664, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.275979, test/num_examples=43793, total_duration=80208.372761, train/accuracy=0.995533, train/loss=0.013933, train/mean_average_precision=0.784643, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286668, validation/num_examples=43793
I0312 01:31:37.623479 139628679296768 logging_writer.py:48] [169400] global_step=169400, grad_norm=0.14498178660869598, loss=0.015121104195713997
I0312 01:32:08.548196 139622180505344 logging_writer.py:48] [169500] global_step=169500, grad_norm=0.14195185899734497, loss=0.017888275906443596
I0312 01:32:39.550371 139628679296768 logging_writer.py:48] [169600] global_step=169600, grad_norm=0.14420117437839508, loss=0.016364580020308495
I0312 01:33:10.461410 139622180505344 logging_writer.py:48] [169700] global_step=169700, grad_norm=0.14507260918617249, loss=0.01750117912888527
I0312 01:33:41.602350 139628679296768 logging_writer.py:48] [169800] global_step=169800, grad_norm=0.15660370886325836, loss=0.018729763105511665
I0312 01:34:12.733252 139622180505344 logging_writer.py:48] [169900] global_step=169900, grad_norm=0.14715029299259186, loss=0.020091675221920013
I0312 01:34:43.777086 139628679296768 logging_writer.py:48] [170000] global_step=170000, grad_norm=0.1445537507534027, loss=0.018369007855653763
I0312 01:35:15.525974 139622180505344 logging_writer.py:48] [170100] global_step=170100, grad_norm=0.1366460919380188, loss=0.018319562077522278
I0312 01:35:32.764975 139789500200768 spec.py:321] Evaluating on the training split.
I0312 01:37:12.304559 139789500200768 spec.py:333] Evaluating on the validation split.
I0312 01:37:15.414047 139789500200768 spec.py:349] Evaluating on the test split.
I0312 01:37:18.434180 139789500200768 submission_runner.py:411] Time since start: 80554.26s, 	Step: 170154, 	{'train/accuracy': 0.9955675005912781, 'train/loss': 0.01385553739964962, 'train/mean_average_precision': 0.7803541515077737, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.2865558987791517, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426059290766716, 'test/mean_average_precision': 0.27600072195706543, 'test/num_examples': 43793, 'score': 53560.668197631836, 'total_duration': 80554.26143503189, 'accumulated_submission_time': 53560.668197631836, 'accumulated_eval_time': 26979.104724645615, 'accumulated_logging_time': 10.017580509185791}
I0312 01:37:18.481366 139622172112640 logging_writer.py:48] [170154] accumulated_eval_time=26979.104725, accumulated_logging_time=10.017581, accumulated_submission_time=53560.668198, global_step=170154, preemption_count=0, score=53560.668198, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.276001, test/num_examples=43793, total_duration=80554.261435, train/accuracy=0.995568, train/loss=0.013856, train/mean_average_precision=0.780354, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286556, validation/num_examples=43793
I0312 01:37:33.364966 139628670904064 logging_writer.py:48] [170200] global_step=170200, grad_norm=0.16163066029548645, loss=0.018047671765089035
I0312 01:38:04.635067 139622172112640 logging_writer.py:48] [170300] global_step=170300, grad_norm=0.13185688853263855, loss=0.017087342217564583
I0312 01:38:35.914481 139628670904064 logging_writer.py:48] [170400] global_step=170400, grad_norm=0.1399850845336914, loss=0.016895461827516556
I0312 01:39:07.086750 139622172112640 logging_writer.py:48] [170500] global_step=170500, grad_norm=0.16696195304393768, loss=0.020842978730797768
I0312 01:39:38.652132 139628670904064 logging_writer.py:48] [170600] global_step=170600, grad_norm=0.15150807797908783, loss=0.017744485288858414
I0312 01:40:10.350577 139622172112640 logging_writer.py:48] [170700] global_step=170700, grad_norm=0.13432592153549194, loss=0.017249925062060356
I0312 01:40:41.552373 139628670904064 logging_writer.py:48] [170800] global_step=170800, grad_norm=0.14088541269302368, loss=0.018368611112236977
I0312 01:41:12.994631 139622172112640 logging_writer.py:48] [170900] global_step=170900, grad_norm=0.14667846262454987, loss=0.01716776005923748
I0312 01:41:18.561571 139789500200768 spec.py:321] Evaluating on the training split.
I0312 01:43:00.313962 139789500200768 spec.py:333] Evaluating on the validation split.
I0312 01:43:03.333181 139789500200768 spec.py:349] Evaluating on the test split.
I0312 01:43:06.285269 139789500200768 submission_runner.py:411] Time since start: 80902.11s, 	Step: 170918, 	{'train/accuracy': 0.9955355525016785, 'train/loss': 0.013914600014686584, 'train/mean_average_precision': 0.7782003795864987, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.28676479251732717, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.2759980739721762, 'test/num_examples': 43793, 'score': 53800.717715740204, 'total_duration': 80902.11252045631, 'accumulated_submission_time': 53800.717715740204, 'accumulated_eval_time': 27086.828375577927, 'accumulated_logging_time': 10.076150894165039}
I0312 01:43:06.332258 139621870597888 logging_writer.py:48] [170918] accumulated_eval_time=27086.828376, accumulated_logging_time=10.076151, accumulated_submission_time=53800.717716, global_step=170918, preemption_count=0, score=53800.717716, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.275998, test/num_examples=43793, total_duration=80902.112520, train/accuracy=0.995536, train/loss=0.013915, train/mean_average_precision=0.778200, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286765, validation/num_examples=43793
I0312 01:43:33.028149 139622180505344 logging_writer.py:48] [171000] global_step=171000, grad_norm=0.14241188764572144, loss=0.018740035593509674
I0312 01:44:04.615742 139621870597888 logging_writer.py:48] [171100] global_step=171100, grad_norm=0.15470242500305176, loss=0.01990336738526821
I0312 01:44:36.311692 139622180505344 logging_writer.py:48] [171200] global_step=171200, grad_norm=0.1393905133008957, loss=0.018617795780301094
I0312 01:45:08.290127 139621870597888 logging_writer.py:48] [171300] global_step=171300, grad_norm=0.1550041288137436, loss=0.017336731776595116
I0312 01:45:39.078072 139622180505344 logging_writer.py:48] [171400] global_step=171400, grad_norm=0.14716856181621552, loss=0.01813570037484169
I0312 01:46:09.809998 139621870597888 logging_writer.py:48] [171500] global_step=171500, grad_norm=0.1480029970407486, loss=0.01779770851135254
I0312 01:46:40.710750 139622180505344 logging_writer.py:48] [171600] global_step=171600, grad_norm=0.1512080579996109, loss=0.018379518762230873
I0312 01:47:06.405329 139789500200768 spec.py:321] Evaluating on the training split.
I0312 01:48:50.756946 139789500200768 spec.py:333] Evaluating on the validation split.
I0312 01:48:53.768132 139789500200768 spec.py:349] Evaluating on the test split.
I0312 01:48:56.757179 139789500200768 submission_runner.py:411] Time since start: 81252.58s, 	Step: 171685, 	{'train/accuracy': 0.9955580830574036, 'train/loss': 0.013860491104424, 'train/mean_average_precision': 0.7741929507380779, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.286599359546828, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.2760119254960862, 'test/num_examples': 43793, 'score': 54040.759315013885, 'total_duration': 81252.58443045616, 'accumulated_submission_time': 54040.759315013885, 'accumulated_eval_time': 27197.180181980133, 'accumulated_logging_time': 10.135051727294922}
I0312 01:48:56.805562 139622172112640 logging_writer.py:48] [171685] accumulated_eval_time=27197.180182, accumulated_logging_time=10.135052, accumulated_submission_time=54040.759315, global_step=171685, preemption_count=0, score=54040.759315, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.276012, test/num_examples=43793, total_duration=81252.584430, train/accuracy=0.995558, train/loss=0.013860, train/mean_average_precision=0.774193, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286599, validation/num_examples=43793
I0312 01:49:01.905585 139628679296768 logging_writer.py:48] [171700] global_step=171700, grad_norm=0.1398940235376358, loss=0.01690746285021305
I0312 01:49:32.771083 139622172112640 logging_writer.py:48] [171800] global_step=171800, grad_norm=0.15636950731277466, loss=0.01798500120639801
I0312 01:50:03.328714 139628679296768 logging_writer.py:48] [171900] global_step=171900, grad_norm=0.1676395982503891, loss=0.019929831847548485
I0312 01:50:33.890385 139622172112640 logging_writer.py:48] [172000] global_step=172000, grad_norm=0.14829576015472412, loss=0.01842568814754486
I0312 01:51:04.489588 139628679296768 logging_writer.py:48] [172100] global_step=172100, grad_norm=0.15369080007076263, loss=0.01679031364619732
I0312 01:51:35.086338 139622172112640 logging_writer.py:48] [172200] global_step=172200, grad_norm=0.16045482456684113, loss=0.017370233312249184
I0312 01:52:06.086219 139628679296768 logging_writer.py:48] [172300] global_step=172300, grad_norm=0.12399313598871231, loss=0.014823858626186848
I0312 01:52:36.945170 139622172112640 logging_writer.py:48] [172400] global_step=172400, grad_norm=0.13408507406711578, loss=0.014525000005960464
I0312 01:52:56.852772 139789500200768 spec.py:321] Evaluating on the training split.
I0312 01:54:39.366057 139789500200768 spec.py:333] Evaluating on the validation split.
I0312 01:54:42.402234 139789500200768 spec.py:349] Evaluating on the test split.
I0312 01:54:45.381321 139789500200768 submission_runner.py:411] Time since start: 81601.21s, 	Step: 172466, 	{'train/accuracy': 0.9955554008483887, 'train/loss': 0.013990029692649841, 'train/mean_average_precision': 0.7805614202828458, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.2864266172910606, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.2759691651199342, 'test/num_examples': 43793, 'score': 54280.77589941025, 'total_duration': 81601.20845675468, 'accumulated_submission_time': 54280.77589941025, 'accumulated_eval_time': 27305.70858001709, 'accumulated_logging_time': 10.194327116012573}
I0312 01:54:45.428336 139622180505344 logging_writer.py:48] [172466] accumulated_eval_time=27305.708580, accumulated_logging_time=10.194327, accumulated_submission_time=54280.775899, global_step=172466, preemption_count=0, score=54280.775899, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.275969, test/num_examples=43793, total_duration=81601.208457, train/accuracy=0.995555, train/loss=0.013990, train/mean_average_precision=0.780561, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286427, validation/num_examples=43793
I0312 01:54:56.247703 139628670904064 logging_writer.py:48] [172500] global_step=172500, grad_norm=0.1355406492948532, loss=0.0166338998824358
I0312 01:55:27.173283 139622180505344 logging_writer.py:48] [172600] global_step=172600, grad_norm=0.1666475385427475, loss=0.01919258013367653
I0312 01:55:57.740299 139628670904064 logging_writer.py:48] [172700] global_step=172700, grad_norm=0.16034811735153198, loss=0.018498698249459267
I0312 01:56:28.549712 139622180505344 logging_writer.py:48] [172800] global_step=172800, grad_norm=0.1471463292837143, loss=0.017221443355083466
I0312 01:56:59.175258 139628670904064 logging_writer.py:48] [172900] global_step=172900, grad_norm=0.14807100594043732, loss=0.01760045625269413
I0312 01:57:30.023045 139622180505344 logging_writer.py:48] [173000] global_step=173000, grad_norm=0.14408399164676666, loss=0.018505236133933067
I0312 01:58:00.767555 139628670904064 logging_writer.py:48] [173100] global_step=173100, grad_norm=0.14999665319919586, loss=0.018503135070204735
I0312 01:58:31.673290 139622180505344 logging_writer.py:48] [173200] global_step=173200, grad_norm=0.14191506803035736, loss=0.01696310006082058
I0312 01:58:45.504215 139789500200768 spec.py:321] Evaluating on the training split.
I0312 02:00:28.275826 139789500200768 spec.py:333] Evaluating on the validation split.
I0312 02:00:31.289938 139789500200768 spec.py:349] Evaluating on the test split.
I0312 02:00:34.292264 139789500200768 submission_runner.py:411] Time since start: 81950.12s, 	Step: 173246, 	{'train/accuracy': 0.99552983045578, 'train/loss': 0.013909823261201382, 'train/mean_average_precision': 0.7827256491404169, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.28644728594931773, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.27606934583939, 'test/num_examples': 43793, 'score': 54520.82110333443, 'total_duration': 81950.11951947212, 'accumulated_submission_time': 54520.82110333443, 'accumulated_eval_time': 27414.49658846855, 'accumulated_logging_time': 10.251972436904907}
I0312 02:00:34.339132 139622172112640 logging_writer.py:48] [173246] accumulated_eval_time=27414.496588, accumulated_logging_time=10.251972, accumulated_submission_time=54520.821103, global_step=173246, preemption_count=0, score=54520.821103, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.276069, test/num_examples=43793, total_duration=81950.119519, train/accuracy=0.995530, train/loss=0.013910, train/mean_average_precision=0.782726, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286447, validation/num_examples=43793
I0312 02:00:51.184026 139628679296768 logging_writer.py:48] [173300] global_step=173300, grad_norm=0.13707801699638367, loss=0.01647007092833519
I0312 02:01:21.918046 139622172112640 logging_writer.py:48] [173400] global_step=173400, grad_norm=0.13314542174339294, loss=0.01774318516254425
I0312 02:01:52.538183 139628679296768 logging_writer.py:48] [173500] global_step=173500, grad_norm=0.13530603051185608, loss=0.01756184548139572
I0312 02:02:23.575103 139622172112640 logging_writer.py:48] [173600] global_step=173600, grad_norm=0.14177395403385162, loss=0.01640370301902294
I0312 02:02:54.615193 139628679296768 logging_writer.py:48] [173700] global_step=173700, grad_norm=0.12520883977413177, loss=0.014774816110730171
I0312 02:03:25.671308 139622172112640 logging_writer.py:48] [173800] global_step=173800, grad_norm=0.12391243875026703, loss=0.016310179606080055
I0312 02:03:56.481858 139628679296768 logging_writer.py:48] [173900] global_step=173900, grad_norm=0.14258906245231628, loss=0.01887824572622776
I0312 02:04:27.111150 139622172112640 logging_writer.py:48] [174000] global_step=174000, grad_norm=0.13373231887817383, loss=0.016696471720933914
I0312 02:04:34.423615 139789500200768 spec.py:321] Evaluating on the training split.
I0312 02:06:18.379153 139789500200768 spec.py:333] Evaluating on the validation split.
I0312 02:06:21.469089 139789500200768 spec.py:349] Evaluating on the test split.
I0312 02:06:24.438063 139789500200768 submission_runner.py:411] Time since start: 82300.27s, 	Step: 174025, 	{'train/accuracy': 0.9955813884735107, 'train/loss': 0.013837228529155254, 'train/mean_average_precision': 0.7825517451429103, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.2865332651294395, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.2760301578149513, 'test/num_examples': 43793, 'score': 54760.8744161129, 'total_duration': 82300.26519680023, 'accumulated_submission_time': 54760.8744161129, 'accumulated_eval_time': 27524.51087450981, 'accumulated_logging_time': 10.31071424484253}
I0312 02:06:24.485520 139621870597888 logging_writer.py:48] [174025] accumulated_eval_time=27524.510875, accumulated_logging_time=10.310714, accumulated_submission_time=54760.874416, global_step=174025, preemption_count=0, score=54760.874416, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.276030, test/num_examples=43793, total_duration=82300.265197, train/accuracy=0.995581, train/loss=0.013837, train/mean_average_precision=0.782552, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286533, validation/num_examples=43793
I0312 02:06:47.998025 139622180505344 logging_writer.py:48] [174100] global_step=174100, grad_norm=0.14481627941131592, loss=0.017771048471331596
I0312 02:07:18.712684 139621870597888 logging_writer.py:48] [174200] global_step=174200, grad_norm=0.13720989227294922, loss=0.018106380477547646
I0312 02:07:49.602773 139622180505344 logging_writer.py:48] [174300] global_step=174300, grad_norm=0.16672685742378235, loss=0.018629584461450577
I0312 02:08:20.636266 139621870597888 logging_writer.py:48] [174400] global_step=174400, grad_norm=0.14562635123729706, loss=0.01923719421029091
I0312 02:08:51.289374 139622180505344 logging_writer.py:48] [174500] global_step=174500, grad_norm=0.1394428163766861, loss=0.01700618863105774
I0312 02:09:22.354280 139621870597888 logging_writer.py:48] [174600] global_step=174600, grad_norm=0.1401246339082718, loss=0.018127990886569023
I0312 02:09:53.369539 139622180505344 logging_writer.py:48] [174700] global_step=174700, grad_norm=0.14822469651699066, loss=0.017968613654375076
I0312 02:10:24.120453 139621870597888 logging_writer.py:48] [174800] global_step=174800, grad_norm=0.13828691840171814, loss=0.015988271683454514
I0312 02:10:24.740071 139789500200768 spec.py:321] Evaluating on the training split.
I0312 02:12:06.323231 139789500200768 spec.py:333] Evaluating on the validation split.
I0312 02:12:09.302849 139789500200768 spec.py:349] Evaluating on the test split.
I0312 02:12:12.242094 139789500200768 submission_runner.py:411] Time since start: 82648.07s, 	Step: 174803, 	{'train/accuracy': 0.9954863786697388, 'train/loss': 0.014019540511071682, 'train/mean_average_precision': 0.7766700627017158, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.28659379483746394, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.275994238581464, 'test/num_examples': 43793, 'score': 55001.098537921906, 'total_duration': 82648.0693461895, 'accumulated_submission_time': 55001.098537921906, 'accumulated_eval_time': 27632.01285123825, 'accumulated_logging_time': 10.36878228187561}
I0312 02:12:12.290337 139622172112640 logging_writer.py:48] [174803] accumulated_eval_time=27632.012851, accumulated_logging_time=10.368782, accumulated_submission_time=55001.098538, global_step=174803, preemption_count=0, score=55001.098538, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.275994, test/num_examples=43793, total_duration=82648.069346, train/accuracy=0.995486, train/loss=0.014020, train/mean_average_precision=0.776670, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286594, validation/num_examples=43793
I0312 02:12:42.515468 139628670904064 logging_writer.py:48] [174900] global_step=174900, grad_norm=0.14702525734901428, loss=0.01858551613986492
I0312 02:13:13.450257 139622172112640 logging_writer.py:48] [175000] global_step=175000, grad_norm=0.14558783173561096, loss=0.016178298741579056
I0312 02:13:44.549301 139628670904064 logging_writer.py:48] [175100] global_step=175100, grad_norm=0.12809644639492035, loss=0.016657302156090736
I0312 02:14:15.690711 139622172112640 logging_writer.py:48] [175200] global_step=175200, grad_norm=0.14268067479133606, loss=0.01680702529847622
I0312 02:14:46.968332 139628670904064 logging_writer.py:48] [175300] global_step=175300, grad_norm=0.15259972214698792, loss=0.018239492550492287
I0312 02:15:18.336108 139622172112640 logging_writer.py:48] [175400] global_step=175400, grad_norm=0.14465615153312683, loss=0.016558272764086723
I0312 02:15:49.519895 139628670904064 logging_writer.py:48] [175500] global_step=175500, grad_norm=0.12933504581451416, loss=0.015926267951726913
I0312 02:16:12.281702 139789500200768 spec.py:321] Evaluating on the training split.
I0312 02:17:55.834908 139789500200768 spec.py:333] Evaluating on the validation split.
I0312 02:17:58.859572 139789500200768 spec.py:349] Evaluating on the test split.
I0312 02:18:01.923456 139789500200768 submission_runner.py:411] Time since start: 82997.75s, 	Step: 175574, 	{'train/accuracy': 0.9955763220787048, 'train/loss': 0.013856913894414902, 'train/mean_average_precision': 0.7794461300147126, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050587158650159836, 'validation/mean_average_precision': 0.28655074676841663, 'validation/num_examples': 43793, 'test/accuracy': 0.9861329793930054, 'test/loss': 0.05426058918237686, 'test/mean_average_precision': 0.276069050592469, 'test/num_examples': 43793, 'score': 55241.06003189087, 'total_duration': 82997.75070357323, 'accumulated_submission_time': 55241.06003189087, 'accumulated_eval_time': 27741.654556512833, 'accumulated_logging_time': 10.427810430526733}
I0312 02:18:01.971416 139621870597888 logging_writer.py:48] [175574] accumulated_eval_time=27741.654557, accumulated_logging_time=10.427810, accumulated_submission_time=55241.060032, global_step=175574, preemption_count=0, score=55241.060032, test/accuracy=0.986133, test/loss=0.054261, test/mean_average_precision=0.276069, test/num_examples=43793, total_duration=82997.750704, train/accuracy=0.995576, train/loss=0.013857, train/mean_average_precision=0.779446, validation/accuracy=0.986956, validation/loss=0.050587, validation/mean_average_precision=0.286551, validation/num_examples=43793
I0312 02:18:10.507307 139622180505344 logging_writer.py:48] [175600] global_step=175600, grad_norm=0.17663484811782837, loss=0.020162532106041908
I0312 02:18:42.004640 139621870597888 logging_writer.py:48] [175700] global_step=175700, grad_norm=0.14505326747894287, loss=0.01811087317764759
I0312 02:19:13.197844 139622180505344 logging_writer.py:48] [175800] global_step=175800, grad_norm=0.13993608951568604, loss=0.01678050309419632
I0312 02:19:44.156177 139621870597888 logging_writer.py:48] [175900] global_step=175900, grad_norm=0.16183394193649292, loss=0.021063629537820816
I0312 02:20:15.590588 139622180505344 logging_writer.py:48] [176000] global_step=176000, grad_norm=0.13572512567043304, loss=0.018036004155874252
I0312 02:20:46.837496 139621870597888 logging_writer.py:48] [176100] global_step=176100, grad_norm=0.14135624468326569, loss=0.01629064418375492
I0312 02:21:12.155699 139622180505344 logging_writer.py:48] [176183] global_step=176183, preemption_count=0, score=55431.182583
I0312 02:21:12.208495 139789500200768 checkpoints.py:490] Saving checkpoint at step: 176183
I0312 02:21:12.332739 139789500200768 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification_self_tuning/study_4/ogbg_jax/trial_1/checkpoint_176183
I0312 02:21:12.334156 139789500200768 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification_self_tuning/study_4/ogbg_jax/trial_1/checkpoint_176183.
I0312 02:21:12.531188 139789500200768 submission_runner.py:676] Final ogbg score: 55431.18258333206
Dataset ogbg_molpcba downloaded and prepared to /root/data/ogbg_molpcba/0.1.3. Subsequent calls will reuse this data.
