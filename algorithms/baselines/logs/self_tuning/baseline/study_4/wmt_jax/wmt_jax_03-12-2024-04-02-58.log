python3 submission_runner.py --framework=jax --workload=wmt --submission_path=prize_qualification_baselines/self_tuning/jax_nadamw_full_budget.py --data_dir=/data/wmt --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=prize_qualification_self_tuning/study_4 --overwrite=true --save_checkpoints=false --rng_seed=1192756799 --max_global_steps=399999 --tuning_ruleset=self 2>&1 | tee -a /logs/wmt_jax_03-12-2024-04-02-58.log
I0312 04:03:20.529092 140573192058688 logger_utils.py:61] Removing existing experiment directory /experiment_runs/prize_qualification_self_tuning/study_4/wmt_jax because --overwrite was set.
I0312 04:03:20.531874 140573192058688 logger_utils.py:76] Creating experiment directory at /experiment_runs/prize_qualification_self_tuning/study_4/wmt_jax.
I0312 04:03:21.581542 140573192058688 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I0312 04:03:21.582381 140573192058688 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0312 04:03:21.582525 140573192058688 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0312 04:03:22.447645 140573192058688 submission_runner.py:612] Creating directory at /experiment_runs/prize_qualification_self_tuning/study_4/wmt_jax/trial_1.
I0312 04:03:22.652627 140573192058688 submission_runner.py:209] Initializing dataset.
I0312 04:03:22.664151 140573192058688 dataset_info.py:578] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0312 04:03:22.668973 140573192058688 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0312 04:03:22.825116 140573192058688 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0312 04:03:24.793689 140573192058688 submission_runner.py:220] Initializing model.
I0312 04:03:33.880153 140573192058688 submission_runner.py:262] Initializing optimizer.
I0312 04:03:34.957022 140573192058688 submission_runner.py:269] Initializing metrics bundle.
I0312 04:03:34.957224 140573192058688 submission_runner.py:287] Initializing checkpoint and logger.
I0312 04:03:34.958048 140573192058688 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification_self_tuning/study_4/wmt_jax/trial_1 with prefix checkpoint_
I0312 04:03:34.958194 140573192058688 submission_runner.py:307] Saving meta data to /experiment_runs/prize_qualification_self_tuning/study_4/wmt_jax/trial_1/meta_data_0.json.
I0312 04:03:34.958399 140573192058688 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0312 04:03:34.958464 140573192058688 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0312 04:03:35.296546 140573192058688 logger_utils.py:220] Unable to record git information. Continuing without it.
I0312 04:03:35.601014 140573192058688 submission_runner.py:311] Saving flags to /experiment_runs/prize_qualification_self_tuning/study_4/wmt_jax/trial_1/flags_0.json.
I0312 04:03:35.611984 140573192058688 submission_runner.py:321] Starting training loop.
I0312 04:04:15.247543 140407514068736 logging_writer.py:48] [0] global_step=0, grad_norm=4.916303634643555, loss=11.113301277160645
I0312 04:04:15.265649 140573192058688 spec.py:321] Evaluating on the training split.
I0312 04:04:15.270084 140573192058688 dataset_info.py:578] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0312 04:04:15.273547 140573192058688 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0312 04:04:15.312571 140573192058688 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0312 04:04:23.197860 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 04:09:18.207874 140573192058688 spec.py:333] Evaluating on the validation split.
I0312 04:09:18.212397 140573192058688 dataset_info.py:578] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0312 04:09:18.217114 140573192058688 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0312 04:09:18.264324 140573192058688 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split validation, from /data/wmt/wmt14_translate/de-en/1.0.0
I0312 04:09:25.214608 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 04:14:09.307280 140573192058688 spec.py:349] Evaluating on the test split.
I0312 04:14:09.310237 140573192058688 dataset_info.py:578] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0312 04:14:09.313634 140573192058688 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0312 04:14:09.351395 140573192058688 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split test, from /data/wmt/wmt14_translate/de-en/1.0.0
I0312 04:14:12.182932 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 04:18:56.576918 140573192058688 submission_runner.py:420] Time since start: 920.96s, 	Step: 1, 	{'train/accuracy': 0.0005556122632697225, 'train/loss': 11.132158279418945, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.097217559814453, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.113809585571289, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 39.65361142158508, 'total_duration': 920.964860200882, 'accumulated_submission_time': 39.65361142158508, 'accumulated_eval_time': 881.3112032413483, 'accumulated_logging_time': 0}
I0312 04:18:56.595340 140403528402688 logging_writer.py:48] [1] accumulated_eval_time=881.311203, accumulated_logging_time=0, accumulated_submission_time=39.653611, global_step=1, preemption_count=0, score=39.653611, test/accuracy=0.000709, test/bleu=0.000000, test/loss=11.113810, test/num_examples=3003, total_duration=920.964860, train/accuracy=0.000556, train/bleu=0.000000, train/loss=11.132158, validation/accuracy=0.000484, validation/bleu=0.000000, validation/loss=11.097218, validation/num_examples=3000
I0312 04:19:31.966247 140403520009984 logging_writer.py:48] [100] global_step=100, grad_norm=0.17965346574783325, loss=8.275825500488281
I0312 04:20:07.384092 140403528402688 logging_writer.py:48] [200] global_step=200, grad_norm=0.3968870937824249, loss=7.532678604125977
I0312 04:20:42.775139 140403520009984 logging_writer.py:48] [300] global_step=300, grad_norm=0.3976347744464874, loss=6.891668319702148
I0312 04:21:18.167231 140403528402688 logging_writer.py:48] [400] global_step=400, grad_norm=0.702122151851654, loss=6.3449625968933105
I0312 04:21:53.558006 140403520009984 logging_writer.py:48] [500] global_step=500, grad_norm=0.6832918524742126, loss=5.910689830780029
I0312 04:22:28.935783 140403528402688 logging_writer.py:48] [600] global_step=600, grad_norm=0.6546725034713745, loss=5.600922584533691
I0312 04:23:04.315014 140403520009984 logging_writer.py:48] [700] global_step=700, grad_norm=0.42331600189208984, loss=5.3312578201293945
I0312 04:23:39.676987 140403528402688 logging_writer.py:48] [800] global_step=800, grad_norm=0.44307973980903625, loss=4.92747163772583
I0312 04:24:15.146166 140403520009984 logging_writer.py:48] [900] global_step=900, grad_norm=0.4006887376308441, loss=4.779465675354004
I0312 04:24:50.515023 140403528402688 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.4635019302368164, loss=4.618990898132324
I0312 04:25:25.876149 140403520009984 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.5980187058448792, loss=4.316272735595703
I0312 04:26:01.219499 140403528402688 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.663327693939209, loss=4.043760776519775
I0312 04:26:36.564900 140403520009984 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.5281280279159546, loss=3.9795923233032227
I0312 04:27:11.907312 140403528402688 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.5050395131111145, loss=3.712700366973877
I0312 04:27:47.256233 140403520009984 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.47411394119262695, loss=3.621483087539673
I0312 04:28:22.605619 140403528402688 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.48000165820121765, loss=3.449032783508301
I0312 04:28:57.966992 140403520009984 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.4654015898704529, loss=3.4220035076141357
I0312 04:29:33.291040 140403528402688 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.7058538198471069, loss=3.3374972343444824
I0312 04:30:08.670346 140403520009984 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.4269782602787018, loss=3.293104887008667
I0312 04:30:44.066343 140403528402688 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.3923499286174774, loss=3.1055185794830322
I0312 04:31:19.454853 140403520009984 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.395376592874527, loss=3.091108560562134
I0312 04:31:54.785309 140403528402688 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.3985433280467987, loss=3.1487419605255127
I0312 04:32:30.129490 140403520009984 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.4939732551574707, loss=3.060004472732544
I0312 04:32:56.697864 140573192058688 spec.py:321] Evaluating on the training split.
I0312 04:32:59.659518 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 04:35:45.629472 140573192058688 spec.py:333] Evaluating on the validation split.
I0312 04:35:48.311811 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 04:38:35.208708 140573192058688 spec.py:349] Evaluating on the test split.
I0312 04:38:37.880372 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 04:41:18.251613 140573192058688 submission_runner.py:420] Time since start: 2262.64s, 	Step: 2377, 	{'train/accuracy': 0.5125483274459839, 'train/loss': 2.858908176422119, 'train/bleu': 22.492414050302205, 'validation/accuracy': 0.5142651796340942, 'validation/loss': 2.836130142211914, 'validation/bleu': 18.307205325611214, 'validation/num_examples': 3000, 'test/accuracy': 0.5126023888587952, 'test/loss': 2.8845648765563965, 'test/bleu': 16.82979069214398, 'test/num_examples': 3003, 'score': 879.6641981601715, 'total_duration': 2262.6394739151, 'accumulated_submission_time': 879.6641981601715, 'accumulated_eval_time': 1382.864819765091, 'accumulated_logging_time': 0.030211448669433594}
I0312 04:41:18.270511 140403528402688 logging_writer.py:48] [2377] accumulated_eval_time=1382.864820, accumulated_logging_time=0.030211, accumulated_submission_time=879.664198, global_step=2377, preemption_count=0, score=879.664198, test/accuracy=0.512602, test/bleu=16.829791, test/loss=2.884565, test/num_examples=3003, total_duration=2262.639474, train/accuracy=0.512548, train/bleu=22.492414, train/loss=2.858908, validation/accuracy=0.514265, validation/bleu=18.307205, validation/loss=2.836130, validation/num_examples=3000
I0312 04:41:26.784138 140403520009984 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.4233686625957489, loss=3.0287206172943115
I0312 04:42:02.106955 140403528402688 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.37632861733436584, loss=2.9406306743621826
I0312 04:42:37.476173 140403520009984 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.29973384737968445, loss=2.7745542526245117
I0312 04:43:12.796612 140403528402688 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.30261924862861633, loss=2.7724075317382812
I0312 04:43:48.104086 140403520009984 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.2933328449726105, loss=2.725480318069458
I0312 04:44:23.433037 140403528402688 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.29776546359062195, loss=2.7437024116516113
I0312 04:44:58.799357 140403520009984 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.2928522825241089, loss=2.68489146232605
I0312 04:45:34.192483 140403528402688 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.2479572296142578, loss=2.6954855918884277
I0312 04:46:09.528546 140403520009984 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.278402715921402, loss=2.690488576889038
I0312 04:46:44.858097 140403528402688 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.23832997679710388, loss=2.5668840408325195
I0312 04:47:20.204172 140403520009984 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.21723805367946625, loss=2.552031993865967
I0312 04:47:55.547936 140403528402688 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.23595991730690002, loss=2.429157018661499
I0312 04:48:30.961336 140403520009984 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.21308499574661255, loss=2.5208187103271484
I0312 04:49:06.372349 140403528402688 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.21269868314266205, loss=2.4989824295043945
I0312 04:49:41.711697 140403520009984 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.24176090955734253, loss=2.5993499755859375
I0312 04:50:17.026648 140403528402688 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.19847573339939117, loss=2.469247579574585
I0312 04:50:52.362056 140403520009984 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.18280787765979767, loss=2.4913032054901123
I0312 04:51:27.734456 140403528402688 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.22072964906692505, loss=2.3681633472442627
I0312 04:52:03.065085 140403520009984 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.17782501876354218, loss=2.4696433544158936
I0312 04:52:38.404930 140403528402688 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.19968648254871368, loss=2.4412829875946045
I0312 04:53:13.776596 140403520009984 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.2556613087654114, loss=2.3621346950531006
I0312 04:53:49.112534 140403528402688 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.19508112967014313, loss=2.3701932430267334
I0312 04:54:24.459288 140403520009984 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.1951535940170288, loss=2.3953943252563477
I0312 04:54:59.816927 140403528402688 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.1736079603433609, loss=2.2715611457824707
I0312 04:55:18.277510 140573192058688 spec.py:321] Evaluating on the training split.
I0312 04:55:21.241514 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 04:59:22.707427 140573192058688 spec.py:333] Evaluating on the validation split.
I0312 04:59:25.368496 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 05:02:01.711973 140573192058688 spec.py:349] Evaluating on the test split.
I0312 05:02:04.386633 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 05:04:40.332922 140573192058688 submission_runner.py:420] Time since start: 3664.72s, 	Step: 4754, 	{'train/accuracy': 0.5762366056442261, 'train/loss': 2.2599639892578125, 'train/bleu': 26.972132326701903, 'validation/accuracy': 0.5911024212837219, 'validation/loss': 2.15031099319458, 'validation/bleu': 23.643337175562927, 'validation/num_examples': 3000, 'test/accuracy': 0.594375729560852, 'test/loss': 2.114021062850952, 'test/bleu': 22.238124830388227, 'test/num_examples': 3003, 'score': 1719.578450679779, 'total_duration': 3664.720867872238, 'accumulated_submission_time': 1719.578450679779, 'accumulated_eval_time': 1944.9201946258545, 'accumulated_logging_time': 0.06104111671447754}
I0312 05:04:40.348247 140403520009984 logging_writer.py:48] [4754] accumulated_eval_time=1944.920195, accumulated_logging_time=0.061041, accumulated_submission_time=1719.578451, global_step=4754, preemption_count=0, score=1719.578451, test/accuracy=0.594376, test/bleu=22.238125, test/loss=2.114021, test/num_examples=3003, total_duration=3664.720868, train/accuracy=0.576237, train/bleu=26.972132, train/loss=2.259964, validation/accuracy=0.591102, validation/bleu=23.643337, validation/loss=2.150311, validation/num_examples=3000
I0312 05:04:56.954603 140403528402688 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.20115435123443604, loss=2.2771244049072266
I0312 05:05:32.268755 140403520009984 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.1748669594526291, loss=2.325315237045288
I0312 05:06:07.625827 140403528402688 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.19457557797431946, loss=2.2393975257873535
I0312 05:06:42.946010 140403520009984 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.17048534750938416, loss=2.3810856342315674
I0312 05:07:18.287451 140403528402688 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.174536794424057, loss=2.253004789352417
I0312 05:07:53.644648 140403520009984 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.15426218509674072, loss=2.272981643676758
I0312 05:08:28.977930 140403528402688 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.1585860699415207, loss=2.256643772125244
I0312 05:09:04.354849 140403520009984 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.16898585855960846, loss=2.2833709716796875
I0312 05:09:39.699132 140403528402688 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.16452142596244812, loss=2.3038270473480225
I0312 05:10:15.040081 140403520009984 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.17649893462657928, loss=2.240736246109009
I0312 05:10:50.389528 140403528402688 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.15169693529605865, loss=2.1971399784088135
I0312 05:11:25.711818 140403520009984 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.15706908702850342, loss=2.1737046241760254
I0312 05:12:01.033833 140403528402688 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.16281504929065704, loss=2.1814663410186768
I0312 05:12:36.357893 140403520009984 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.17640472948551178, loss=2.2131171226501465
I0312 05:13:11.658386 140403528402688 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.1540966033935547, loss=2.1812551021575928
I0312 05:13:46.993448 140403520009984 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.1467115879058838, loss=2.127197027206421
I0312 05:14:22.320546 140403528402688 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.16412639617919922, loss=2.210446834564209
I0312 05:14:57.707784 140403520009984 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.1507602334022522, loss=2.162355422973633
I0312 05:15:33.093605 140403528402688 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.16796916723251343, loss=2.1500766277313232
I0312 05:16:08.487146 140403520009984 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.15809215605258942, loss=2.060380458831787
I0312 05:16:43.851779 140403528402688 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.1558806598186493, loss=2.1756694316864014
I0312 05:17:19.219893 140403520009984 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.19952869415283203, loss=2.174255132675171
I0312 05:17:54.609844 140403528402688 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.1617332547903061, loss=2.1843957901000977
I0312 05:18:29.956345 140403520009984 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.21095582842826843, loss=2.2107021808624268
I0312 05:18:40.626979 140573192058688 spec.py:321] Evaluating on the training split.
I0312 05:18:43.589174 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 05:21:18.509685 140573192058688 spec.py:333] Evaluating on the validation split.
I0312 05:21:21.178761 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 05:23:41.792162 140573192058688 spec.py:349] Evaluating on the test split.
I0312 05:23:44.491517 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 05:25:51.192983 140573192058688 submission_runner.py:420] Time since start: 4935.58s, 	Step: 7132, 	{'train/accuracy': 0.6054196953773499, 'train/loss': 2.010335922241211, 'train/bleu': 28.578043861643884, 'validation/accuracy': 0.6175868511199951, 'validation/loss': 1.9254359006881714, 'validation/bleu': 25.03961870233312, 'validation/num_examples': 3000, 'test/accuracy': 0.6212770938873291, 'test/loss': 1.8826719522476196, 'test/bleu': 24.043971003125698, 'test/num_examples': 3003, 'score': 2559.766467809677, 'total_duration': 4935.580909729004, 'accumulated_submission_time': 2559.766467809677, 'accumulated_eval_time': 2375.4861319065094, 'accumulated_logging_time': 0.08647036552429199}
I0312 05:25:51.208475 140403528402688 logging_writer.py:48] [7132] accumulated_eval_time=2375.486132, accumulated_logging_time=0.086470, accumulated_submission_time=2559.766468, global_step=7132, preemption_count=0, score=2559.766468, test/accuracy=0.621277, test/bleu=24.043971, test/loss=1.882672, test/num_examples=3003, total_duration=4935.580910, train/accuracy=0.605420, train/bleu=28.578044, train/loss=2.010336, validation/accuracy=0.617587, validation/bleu=25.039619, validation/loss=1.925436, validation/num_examples=3000
I0312 05:26:15.608054 140403520009984 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.20671339333057404, loss=2.227065086364746
I0312 05:26:50.918116 140403528402688 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.1923438459634781, loss=2.113316774368286
I0312 05:27:26.231913 140403520009984 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.14878322184085846, loss=2.14622163772583
I0312 05:28:01.599925 140403528402688 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.15277449786663055, loss=2.0589206218719482
I0312 05:28:36.922593 140403520009984 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.15217439830303192, loss=2.1606831550598145
I0312 05:29:12.284100 140403528402688 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.15311996638774872, loss=2.019172191619873
I0312 05:29:47.652969 140403520009984 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.14266124367713928, loss=2.143106460571289
I0312 05:30:23.022881 140403528402688 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.15911871194839478, loss=2.0830023288726807
I0312 05:30:58.379771 140403520009984 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.1616118848323822, loss=2.145585536956787
I0312 05:31:33.689098 140403528402688 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.15954793989658356, loss=2.174609899520874
I0312 05:32:09.015840 140403520009984 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.1610211730003357, loss=2.0362069606781006
I0312 05:32:44.367280 140403528402688 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.19273662567138672, loss=2.086068868637085
I0312 05:33:19.677806 140403520009984 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.16497546434402466, loss=2.1546518802642822
I0312 05:33:55.002973 140403528402688 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.16011923551559448, loss=2.0817933082580566
I0312 05:34:30.316443 140403520009984 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.16682489216327667, loss=2.0298562049865723
I0312 05:35:05.637773 140403528402688 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.17936773598194122, loss=2.0869102478027344
I0312 05:35:40.940021 140403520009984 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.15702258050441742, loss=2.050283432006836
I0312 05:36:16.293026 140403528402688 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.1687467396259308, loss=2.036503314971924
I0312 05:36:51.659289 140403520009984 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.3351140320301056, loss=2.1007983684539795
I0312 05:37:26.993955 140403528402688 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.17496317625045776, loss=2.0875587463378906
I0312 05:38:02.365394 140403520009984 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.20750834047794342, loss=2.084167003631592
I0312 05:38:37.686276 140403528402688 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.3625270426273346, loss=2.094000816345215
I0312 05:39:13.025899 140403520009984 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.2645270824432373, loss=2.0320186614990234
I0312 05:39:48.426164 140403528402688 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.15665586292743683, loss=2.006335973739624
I0312 05:39:51.344681 140573192058688 spec.py:321] Evaluating on the training split.
I0312 05:39:54.313125 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 05:42:42.492695 140573192058688 spec.py:333] Evaluating on the validation split.
I0312 05:42:45.187658 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 05:45:15.034740 140573192058688 spec.py:349] Evaluating on the test split.
I0312 05:45:17.705670 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 05:47:29.219269 140573192058688 submission_runner.py:420] Time since start: 6233.61s, 	Step: 9510, 	{'train/accuracy': 0.6144471764564514, 'train/loss': 1.9414385557174683, 'train/bleu': 29.758036174075343, 'validation/accuracy': 0.6304943561553955, 'validation/loss': 1.8233507871627808, 'validation/bleu': 26.07328233983416, 'validation/num_examples': 3000, 'test/accuracy': 0.6384173035621643, 'test/loss': 1.761852741241455, 'test/bleu': 25.147387980827414, 'test/num_examples': 3003, 'score': 3399.8166558742523, 'total_duration': 6233.60720205307, 'accumulated_submission_time': 3399.8166558742523, 'accumulated_eval_time': 2833.3606865406036, 'accumulated_logging_time': 0.11213397979736328}
I0312 05:47:29.234927 140403520009984 logging_writer.py:48] [9510] accumulated_eval_time=2833.360687, accumulated_logging_time=0.112134, accumulated_submission_time=3399.816656, global_step=9510, preemption_count=0, score=3399.816656, test/accuracy=0.638417, test/bleu=25.147388, test/loss=1.761853, test/num_examples=3003, total_duration=6233.607202, train/accuracy=0.614447, train/bleu=29.758036, train/loss=1.941439, validation/accuracy=0.630494, validation/bleu=26.073282, validation/loss=1.823351, validation/num_examples=3000
I0312 05:48:01.371064 140403528402688 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.23633989691734314, loss=2.0790767669677734
I0312 05:48:36.690783 140403520009984 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.22187483310699463, loss=2.0257275104522705
I0312 05:49:12.020659 140403528402688 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.23038895428180695, loss=1.99825119972229
I0312 05:49:47.338950 140403520009984 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.201930433511734, loss=1.98917555809021
I0312 05:50:22.682282 140403528402688 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.21402369439601898, loss=2.045032501220703
I0312 05:50:57.990649 140403520009984 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.16880649328231812, loss=2.020921468734741
I0312 05:51:33.298266 140403528402688 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.15611259639263153, loss=2.0356040000915527
I0312 05:52:08.617887 140403520009984 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.1560574173927307, loss=1.9867054224014282
I0312 05:52:43.936317 140403528402688 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.17753596603870392, loss=1.9916691780090332
I0312 05:53:19.277854 140403520009984 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.16977447271347046, loss=2.001164436340332
I0312 05:53:54.604864 140403528402688 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.15228256583213806, loss=1.939833164215088
I0312 05:54:29.945781 140403520009984 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.1621560901403427, loss=2.099889039993286
I0312 05:55:05.282142 140403528402688 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.18867385387420654, loss=2.010820150375366
I0312 05:55:40.595548 140403520009984 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.16044434905052185, loss=2.0139577388763428
I0312 05:56:15.947454 140403528402688 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.19339464604854584, loss=2.01033878326416
I0312 05:56:51.341464 140403520009984 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.23805052042007446, loss=1.9684271812438965
I0312 05:57:26.693689 140403528402688 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.1875736564397812, loss=1.990415334701538
I0312 05:58:02.045581 140403520009984 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.3371283710002899, loss=2.058777332305908
I0312 05:58:37.363170 140403528402688 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.15465950965881348, loss=1.9317911863327026
I0312 05:59:12.695758 140403520009984 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.25116923451423645, loss=2.020291566848755
I0312 05:59:48.053358 140403528402688 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.1591600775718689, loss=1.9969654083251953
I0312 06:00:23.368690 140403520009984 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.23007608950138092, loss=2.0069210529327393
I0312 06:00:58.687879 140403528402688 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.19268842041492462, loss=2.016103506088257
I0312 06:01:29.566695 140573192058688 spec.py:321] Evaluating on the training split.
I0312 06:01:32.526565 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 06:04:01.213281 140573192058688 spec.py:333] Evaluating on the validation split.
I0312 06:04:03.881612 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 06:06:24.152968 140573192058688 spec.py:349] Evaluating on the test split.
I0312 06:06:26.830948 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 06:08:35.131613 140573192058688 submission_runner.py:420] Time since start: 7499.52s, 	Step: 11889, 	{'train/accuracy': 0.6189337968826294, 'train/loss': 1.889572024345398, 'train/bleu': 29.89805366521182, 'validation/accuracy': 0.6390745043754578, 'validation/loss': 1.7538105249404907, 'validation/bleu': 26.64441501614611, 'validation/num_examples': 3000, 'test/accuracy': 0.6477136611938477, 'test/loss': 1.6896036863327026, 'test/bleu': 25.624177133264116, 'test/num_examples': 3003, 'score': 4240.059615850449, 'total_duration': 7499.519518136978, 'accumulated_submission_time': 4240.059615850449, 'accumulated_eval_time': 3258.925538778305, 'accumulated_logging_time': 0.13827753067016602}
I0312 06:08:35.147291 140403520009984 logging_writer.py:48] [11889] accumulated_eval_time=3258.925539, accumulated_logging_time=0.138278, accumulated_submission_time=4240.059616, global_step=11889, preemption_count=0, score=4240.059616, test/accuracy=0.647714, test/bleu=25.624177, test/loss=1.689604, test/num_examples=3003, total_duration=7499.519518, train/accuracy=0.618934, train/bleu=29.898054, train/loss=1.889572, validation/accuracy=0.639075, validation/bleu=26.644415, validation/loss=1.753811, validation/num_examples=3000
I0312 06:08:39.405200 140403528402688 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.23760829865932465, loss=2.0126590728759766
I0312 06:09:14.746185 140403520009984 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.21225908398628235, loss=2.033931255340576
I0312 06:09:50.081603 140403528402688 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.15858204662799835, loss=1.9279614686965942
I0312 06:10:25.384498 140403520009984 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.14732271432876587, loss=1.9319599866867065
I0312 06:11:00.693015 140403528402688 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.1688161939382553, loss=2.003209114074707
I0312 06:11:36.008133 140403520009984 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.1721886843442917, loss=1.877724051475525
I0312 06:12:11.329677 140403528402688 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.1713353842496872, loss=1.9450329542160034
I0312 06:12:46.641573 140403520009984 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.2932836711406708, loss=2.0085368156433105
I0312 06:13:21.963874 140403528402688 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.18710526823997498, loss=1.8790289163589478
I0312 06:13:57.284282 140403520009984 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.16706931591033936, loss=1.9774510860443115
I0312 06:14:32.603021 140403528402688 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.19261357188224792, loss=1.8679131269454956
I0312 06:15:07.946996 140403520009984 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.18499182164669037, loss=1.9652018547058105
I0312 06:15:43.289842 140403528402688 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.324779212474823, loss=2.032254695892334
I0312 06:16:18.616596 140403520009984 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.18755070865154266, loss=1.9851247072219849
I0312 06:16:53.948019 140403528402688 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.15927860140800476, loss=1.8702744245529175
I0312 06:17:29.269195 140403520009984 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.19966062903404236, loss=1.9989203214645386
I0312 06:18:04.609442 140403528402688 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.2155303955078125, loss=1.9842158555984497
I0312 06:18:39.942115 140403520009984 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.24417491257190704, loss=2.0293593406677246
I0312 06:19:15.272211 140403528402688 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.2007257044315338, loss=1.9730417728424072
I0312 06:19:50.575516 140403520009984 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.2359265834093094, loss=2.006728410720825
I0312 06:20:25.918284 140403528402688 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.319705992937088, loss=1.9662907123565674
I0312 06:21:01.266322 140403520009984 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.18278411030769348, loss=1.9524379968643188
I0312 06:21:36.599516 140403528402688 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.16406001150608063, loss=1.9155120849609375
I0312 06:22:11.935335 140403520009984 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.19666573405265808, loss=1.890326976776123
I0312 06:22:35.315520 140573192058688 spec.py:321] Evaluating on the training split.
I0312 06:22:38.278251 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 06:25:51.509274 140573192058688 spec.py:333] Evaluating on the validation split.
I0312 06:25:54.177307 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 06:28:10.258641 140573192058688 spec.py:349] Evaluating on the test split.
I0312 06:28:12.934877 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 06:30:16.886257 140573192058688 submission_runner.py:420] Time since start: 8801.27s, 	Step: 14268, 	{'train/accuracy': 0.624485433101654, 'train/loss': 1.8488229513168335, 'train/bleu': 30.31258298412598, 'validation/accuracy': 0.6445549130439758, 'validation/loss': 1.7065232992172241, 'validation/bleu': 26.959945983848797, 'validation/num_examples': 3000, 'test/accuracy': 0.6522108316421509, 'test/loss': 1.6444939374923706, 'test/bleu': 25.721743297987793, 'test/num_examples': 3003, 'score': 5080.140574455261, 'total_duration': 8801.274183273315, 'accumulated_submission_time': 5080.140574455261, 'accumulated_eval_time': 3720.4962162971497, 'accumulated_logging_time': 0.16579031944274902}
I0312 06:30:16.902393 140403528402688 logging_writer.py:48] [14268] accumulated_eval_time=3720.496216, accumulated_logging_time=0.165790, accumulated_submission_time=5080.140574, global_step=14268, preemption_count=0, score=5080.140574, test/accuracy=0.652211, test/bleu=25.721743, test/loss=1.644494, test/num_examples=3003, total_duration=8801.274183, train/accuracy=0.624485, train/bleu=30.312583, train/loss=1.848823, validation/accuracy=0.644555, validation/bleu=26.959946, validation/loss=1.706523, validation/num_examples=3000
I0312 06:30:28.578264 140403520009984 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.2579096853733063, loss=1.923928141593933
I0312 06:31:03.897716 140403528402688 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.17331358790397644, loss=1.9749000072479248
I0312 06:31:39.182831 140403520009984 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.18893560767173767, loss=1.9974849224090576
I0312 06:32:14.602317 140403528402688 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.20140127837657928, loss=1.8956712484359741
I0312 06:32:49.913824 140403520009984 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.17851299047470093, loss=1.97145676612854
I0312 06:33:25.220020 140403528402688 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.24337007105350494, loss=1.8489806652069092
I0312 06:34:00.571057 140403520009984 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.1853950470685959, loss=1.880993366241455
I0312 06:34:35.915823 140403528402688 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.169161856174469, loss=2.0387508869171143
I0312 06:35:11.244678 140403520009984 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.19658201932907104, loss=1.892038345336914
I0312 06:35:46.577116 140403528402688 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.18980956077575684, loss=1.9399006366729736
I0312 06:36:21.918239 140403520009984 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.18412336707115173, loss=2.032146692276001
I0312 06:36:57.247024 140403528402688 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.18243229389190674, loss=1.935538411140442
I0312 06:37:32.582506 140403520009984 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.16516733169555664, loss=1.8593430519104004
I0312 06:38:07.922537 140403528402688 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.22059112787246704, loss=2.0187184810638428
I0312 06:38:43.231478 140403520009984 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.193151593208313, loss=1.9370211362838745
I0312 06:39:18.593719 140403528402688 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.18309782445430756, loss=1.820532202720642
I0312 06:39:53.933645 140403520009984 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.16522888839244843, loss=1.8159219026565552
I0312 06:40:29.318274 140403528402688 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.20757709443569183, loss=1.9425524473190308
I0312 06:41:04.715452 140403520009984 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.2673366665840149, loss=1.9295635223388672
I0312 06:41:40.046245 140403528402688 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.22975210845470428, loss=1.835445523262024
I0312 06:42:15.354894 140403520009984 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.18326129019260406, loss=1.9010045528411865
I0312 06:42:50.730169 140403528402688 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.232207253575325, loss=1.8933900594711304
I0312 06:43:26.043799 140403520009984 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.16768941283226013, loss=1.9636611938476562
I0312 06:44:01.391385 140403528402688 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.17640137672424316, loss=1.9146966934204102
I0312 06:44:17.011548 140573192058688 spec.py:321] Evaluating on the training split.
I0312 06:44:19.977032 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 06:48:12.320349 140573192058688 spec.py:333] Evaluating on the validation split.
I0312 06:48:14.991748 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 06:50:39.012028 140573192058688 spec.py:349] Evaluating on the test split.
I0312 06:50:41.701590 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 06:52:54.455571 140573192058688 submission_runner.py:420] Time since start: 10158.84s, 	Step: 16646, 	{'train/accuracy': 0.6264616847038269, 'train/loss': 1.830745816230774, 'train/bleu': 30.340130103612914, 'validation/accuracy': 0.6479646563529968, 'validation/loss': 1.6873875856399536, 'validation/bleu': 27.2750023954531, 'validation/num_examples': 3000, 'test/accuracy': 0.6577886343002319, 'test/loss': 1.6086273193359375, 'test/bleu': 26.537255448332026, 'test/num_examples': 3003, 'score': 5920.160865306854, 'total_duration': 10158.843495845795, 'accumulated_submission_time': 5920.160865306854, 'accumulated_eval_time': 4237.940180301666, 'accumulated_logging_time': 0.19264459609985352}
I0312 06:52:54.473357 140403520009984 logging_writer.py:48] [16646] accumulated_eval_time=4237.940180, accumulated_logging_time=0.192645, accumulated_submission_time=5920.160865, global_step=16646, preemption_count=0, score=5920.160865, test/accuracy=0.657789, test/bleu=26.537255, test/loss=1.608627, test/num_examples=3003, total_duration=10158.843496, train/accuracy=0.626462, train/bleu=30.340130, train/loss=1.830746, validation/accuracy=0.647965, validation/bleu=27.275002, validation/loss=1.687388, validation/num_examples=3000
I0312 06:53:13.895239 140403528402688 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.3300226628780365, loss=1.950652003288269
I0312 06:53:49.203555 140403520009984 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.21831341087818146, loss=1.872438669204712
I0312 06:54:24.544990 140403528402688 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.2772100567817688, loss=1.9143036603927612
I0312 06:54:59.874336 140403520009984 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.3325475752353668, loss=1.978351354598999
I0312 06:55:35.172563 140403528402688 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.2174045741558075, loss=1.9303007125854492
I0312 06:56:10.484234 140403520009984 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.20032300055027008, loss=1.8767729997634888
I0312 06:56:45.804098 140403528402688 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.2744099795818329, loss=1.8549538850784302
I0312 06:57:21.136781 140403520009984 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.27923303842544556, loss=1.838276743888855
I0312 06:57:56.465500 140403528402688 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.18662868440151215, loss=1.9363453388214111
I0312 06:58:31.798277 140403520009984 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.22074897587299347, loss=1.9055249691009521
I0312 06:59:07.115710 140403528402688 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.18323537707328796, loss=1.807065725326538
I0312 06:59:42.472451 140403520009984 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.19494400918483734, loss=1.8844746351242065
I0312 07:00:17.843783 140403528402688 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.19873309135437012, loss=1.84597909450531
I0312 07:00:53.213808 140403520009984 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.25958025455474854, loss=1.8238749504089355
I0312 07:01:28.601083 140403528402688 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.19451622664928436, loss=1.8741371631622314
I0312 07:02:03.919204 140403520009984 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.20713843405246735, loss=1.881150245666504
I0312 07:02:39.244484 140403528402688 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.19030924141407013, loss=1.882881760597229
I0312 07:03:14.581554 140403520009984 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.22881987690925598, loss=1.9182008504867554
I0312 07:03:49.928555 140403528402688 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.2539500594139099, loss=1.9318859577178955
I0312 07:04:25.245704 140403520009984 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.1965712159872055, loss=1.8191267251968384
I0312 07:05:00.579125 140403528402688 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.17231325805187225, loss=1.8817758560180664
I0312 07:05:35.961532 140403520009984 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.18250451982021332, loss=1.8629664182662964
I0312 07:06:11.310517 140403528402688 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.214229017496109, loss=1.8775231838226318
I0312 07:06:46.645005 140403520009984 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.20731890201568604, loss=1.8637036085128784
I0312 07:06:54.498049 140573192058688 spec.py:321] Evaluating on the training split.
I0312 07:06:57.457911 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 07:10:51.511305 140573192058688 spec.py:333] Evaluating on the validation split.
I0312 07:10:54.176681 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 07:13:45.253489 140573192058688 spec.py:349] Evaluating on the test split.
I0312 07:13:47.922093 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 07:16:16.704017 140573192058688 submission_runner.py:420] Time since start: 11561.09s, 	Step: 19024, 	{'train/accuracy': 0.6484805345535278, 'train/loss': 1.662880301475525, 'train/bleu': 32.06108394190534, 'validation/accuracy': 0.6506428718566895, 'validation/loss': 1.6595262289047241, 'validation/bleu': 27.19620966158643, 'validation/num_examples': 3000, 'test/accuracy': 0.6595317125320435, 'test/loss': 1.5889374017715454, 'test/bleu': 26.477100905505882, 'test/num_examples': 3003, 'score': 6760.097064495087, 'total_duration': 11561.091912984848, 'accumulated_submission_time': 6760.097064495087, 'accumulated_eval_time': 4800.146046638489, 'accumulated_logging_time': 0.22060847282409668}
I0312 07:16:16.721283 140403528402688 logging_writer.py:48] [19024] accumulated_eval_time=4800.146047, accumulated_logging_time=0.220608, accumulated_submission_time=6760.097064, global_step=19024, preemption_count=0, score=6760.097064, test/accuracy=0.659532, test/bleu=26.477101, test/loss=1.588937, test/num_examples=3003, total_duration=11561.091913, train/accuracy=0.648481, train/bleu=32.061084, train/loss=1.662880, validation/accuracy=0.650643, validation/bleu=27.196210, validation/loss=1.659526, validation/num_examples=3000
I0312 07:16:43.921210 140403520009984 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.18897949159145355, loss=1.9531892538070679
I0312 07:17:19.223407 140403528402688 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.2375270575284958, loss=1.9143985509872437
I0312 07:17:54.514424 140403520009984 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.205569788813591, loss=1.8589156866073608
I0312 07:18:29.845724 140403528402688 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.21939250826835632, loss=1.9472014904022217
I0312 07:19:05.176184 140403520009984 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.19428123533725739, loss=1.8682924509048462
I0312 07:19:40.505223 140403528402688 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.17415642738342285, loss=1.8765965700149536
I0312 07:20:15.841260 140403520009984 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.24722793698310852, loss=1.8144841194152832
I0312 07:20:51.174591 140403528402688 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.2058945894241333, loss=1.9766640663146973
I0312 07:21:26.483733 140403520009984 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.1928599625825882, loss=1.8328107595443726
I0312 07:22:01.821542 140403528402688 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.19214694201946259, loss=1.8896301984786987
I0312 07:22:37.147785 140403520009984 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.28099897503852844, loss=1.8662968873977661
I0312 07:23:12.453373 140403528402688 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.20630396902561188, loss=1.8383982181549072
I0312 07:23:47.815586 140403520009984 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.16706815361976624, loss=1.885483741760254
I0312 07:24:23.164157 140403528402688 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.1829751878976822, loss=1.870640754699707
I0312 07:24:58.563811 140403520009984 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.16736292839050293, loss=1.921081304550171
I0312 07:25:34.059585 140403528402688 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.1916774958372116, loss=1.9090931415557861
I0312 07:26:09.397881 140403520009984 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.27540701627731323, loss=1.8390588760375977
I0312 07:26:44.711249 140403528402688 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.17422720789909363, loss=1.8966264724731445
I0312 07:27:20.030027 140403520009984 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.19789063930511475, loss=1.7811775207519531
I0312 07:27:55.382793 140403528402688 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.19171972572803497, loss=1.8655856847763062
I0312 07:28:30.732614 140403520009984 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.20400260388851166, loss=1.8170088529586792
I0312 07:29:06.058788 140403528402688 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.2192005217075348, loss=1.8584810495376587
I0312 07:29:41.399995 140403520009984 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.19553416967391968, loss=1.8599971532821655
I0312 07:30:16.742661 140403528402688 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.33960041403770447, loss=1.9177653789520264
I0312 07:30:16.749724 140573192058688 spec.py:321] Evaluating on the training split.
I0312 07:30:19.436469 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 07:34:11.969556 140573192058688 spec.py:333] Evaluating on the validation split.
I0312 07:34:14.659566 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 07:37:00.498179 140573192058688 spec.py:349] Evaluating on the test split.
I0312 07:37:03.203060 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 07:39:35.662955 140573192058688 submission_runner.py:420] Time since start: 12960.05s, 	Step: 21401, 	{'train/accuracy': 0.6325982213020325, 'train/loss': 1.7833012342453003, 'train/bleu': 30.659170469932914, 'validation/accuracy': 0.652019202709198, 'validation/loss': 1.656695008277893, 'validation/bleu': 27.546996197827287, 'validation/num_examples': 3000, 'test/accuracy': 0.6625297665596008, 'test/loss': 1.5776896476745605, 'test/bleu': 27.053601061051705, 'test/num_examples': 3003, 'score': 7600.03609752655, 'total_duration': 12960.050861597061, 'accumulated_submission_time': 7600.03609752655, 'accumulated_eval_time': 5359.059168815613, 'accumulated_logging_time': 0.24940872192382812}
I0312 07:39:35.680108 140403520009984 logging_writer.py:48] [21401] accumulated_eval_time=5359.059169, accumulated_logging_time=0.249409, accumulated_submission_time=7600.036098, global_step=21401, preemption_count=0, score=7600.036098, test/accuracy=0.662530, test/bleu=27.053601, test/loss=1.577690, test/num_examples=3003, total_duration=12960.050862, train/accuracy=0.632598, train/bleu=30.659170, train/loss=1.783301, validation/accuracy=0.652019, validation/bleu=27.546996, validation/loss=1.656695, validation/num_examples=3000
I0312 07:40:10.981089 140403528402688 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.19829143583774567, loss=1.9691636562347412
I0312 07:40:46.337316 140403520009984 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.17483001947402954, loss=1.7600131034851074
I0312 07:41:21.650851 140403528402688 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.22104290127754211, loss=1.8010352849960327
I0312 07:41:56.993618 140403520009984 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.1970699280500412, loss=1.8372844457626343
I0312 07:42:32.310351 140403528402688 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.20813103020191193, loss=1.9432241916656494
I0312 07:43:07.647799 140403520009984 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.2657540440559387, loss=2.016352891921997
I0312 07:43:42.981247 140403528402688 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.2096886932849884, loss=1.9080322980880737
I0312 07:44:18.291702 140403520009984 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.1840030699968338, loss=1.8726119995117188
I0312 07:44:53.604256 140403528402688 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.25124579668045044, loss=1.8227300643920898
I0312 07:45:28.990048 140403520009984 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.16261108219623566, loss=1.8366899490356445
I0312 07:46:04.359902 140403528402688 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.19531920552253723, loss=1.882751226425171
I0312 07:46:39.849318 140403520009984 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.18864110112190247, loss=1.7397210597991943
I0312 07:47:15.168809 140403528402688 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.19397909939289093, loss=1.9051430225372314
I0312 07:47:50.488984 140403520009984 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.2150733321905136, loss=1.8472222089767456
I0312 07:48:25.804608 140403528402688 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.21687358617782593, loss=1.843683123588562
I0312 07:49:01.125236 140403520009984 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.1718130111694336, loss=1.9192798137664795
I0312 07:49:36.458602 140403528402688 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.2059895098209381, loss=1.8718435764312744
I0312 07:50:11.792354 140403520009984 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.18688808381557465, loss=1.850732445716858
I0312 07:50:47.157171 140403528402688 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.18687884509563446, loss=1.82059907913208
I0312 07:51:22.484198 140403520009984 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.21424931287765503, loss=1.8583191633224487
I0312 07:51:57.842346 140403528402688 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.22859197854995728, loss=1.8469644784927368
I0312 07:52:33.184472 140403520009984 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.1822507232427597, loss=1.8554186820983887
I0312 07:53:08.543919 140403528402688 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.20536425709724426, loss=1.7984954118728638
I0312 07:53:35.797692 140573192058688 spec.py:321] Evaluating on the training split.
I0312 07:53:38.754692 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 07:57:16.730878 140573192058688 spec.py:333] Evaluating on the validation split.
I0312 07:57:19.403852 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 07:59:57.814420 140573192058688 spec.py:349] Evaluating on the test split.
I0312 08:00:00.490259 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 08:02:16.487470 140573192058688 submission_runner.py:420] Time since start: 14320.88s, 	Step: 23779, 	{'train/accuracy': 0.6351043581962585, 'train/loss': 1.777759313583374, 'train/bleu': 30.41944310720691, 'validation/accuracy': 0.6547470092773438, 'validation/loss': 1.6266250610351562, 'validation/bleu': 27.474100074458494, 'validation/num_examples': 3000, 'test/accuracy': 0.6655743718147278, 'test/loss': 1.5527108907699585, 'test/bleu': 26.95498017227819, 'test/num_examples': 3003, 'score': 8440.063966989517, 'total_duration': 14320.875417470932, 'accumulated_submission_time': 8440.063966989517, 'accumulated_eval_time': 5879.748903989792, 'accumulated_logging_time': 0.27770471572875977}
I0312 08:02:16.505758 140403520009984 logging_writer.py:48] [23779] accumulated_eval_time=5879.748904, accumulated_logging_time=0.277705, accumulated_submission_time=8440.063967, global_step=23779, preemption_count=0, score=8440.063967, test/accuracy=0.665574, test/bleu=26.954980, test/loss=1.552711, test/num_examples=3003, total_duration=14320.875417, train/accuracy=0.635104, train/bleu=30.419443, train/loss=1.777759, validation/accuracy=0.654747, validation/bleu=27.474100, validation/loss=1.626625, validation/num_examples=3000
I0312 08:02:24.293416 140403528402688 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.20501455664634705, loss=1.84233558177948
I0312 08:02:59.653330 140403520009984 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.19857367873191833, loss=1.9090341329574585
I0312 08:03:35.006031 140403528402688 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.21929310262203217, loss=1.853908658027649
I0312 08:04:10.317241 140403520009984 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.26413705945014954, loss=1.8666446208953857
I0312 08:04:45.618073 140403528402688 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.2283598780632019, loss=1.8090358972549438
I0312 08:05:20.959937 140403520009984 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.20896056294441223, loss=1.8457825183868408
I0312 08:05:56.279604 140403528402688 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.2211688905954361, loss=1.901693344116211
I0312 08:06:31.610495 140403520009984 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.1930752992630005, loss=1.8774203062057495
I0312 08:07:06.956625 140403528402688 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.19400063157081604, loss=1.8703639507293701
I0312 08:07:42.320608 140403520009984 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.22482870519161224, loss=1.8195379972457886
I0312 08:08:17.646565 140403528402688 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.2139970064163208, loss=1.8459157943725586
I0312 08:08:52.999245 140403520009984 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.24145838618278503, loss=1.8765267133712769
I0312 08:09:28.318095 140403528402688 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.18738947808742523, loss=1.7511712312698364
I0312 08:10:03.621771 140403520009984 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.18851397931575775, loss=1.764762282371521
I0312 08:10:38.953342 140403528402688 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.1940876692533493, loss=1.8232972621917725
I0312 08:11:14.252742 140403520009984 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.1889055073261261, loss=1.8550423383712769
I0312 08:11:49.629868 140403528402688 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.19386377930641174, loss=1.7565597295761108
I0312 08:12:24.949012 140403520009984 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.18731313943862915, loss=1.7399473190307617
I0312 08:13:00.300698 140403528402688 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.2036229968070984, loss=1.7964047193527222
I0312 08:13:35.652668 140403520009984 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.1843048632144928, loss=1.822133183479309
I0312 08:14:10.967777 140403528402688 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.2257012277841568, loss=1.9144386053085327
I0312 08:14:46.290932 140403520009984 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.189663365483284, loss=1.7784197330474854
I0312 08:15:21.596774 140403528402688 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.21156281232833862, loss=1.851399302482605
I0312 08:15:56.934303 140403520009984 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.1811358481645584, loss=1.7997132539749146
I0312 08:16:16.799488 140573192058688 spec.py:321] Evaluating on the training split.
I0312 08:16:19.754861 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 08:20:21.071133 140573192058688 spec.py:333] Evaluating on the validation split.
I0312 08:20:23.735199 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 08:22:51.823292 140573192058688 spec.py:349] Evaluating on the test split.
I0312 08:22:54.507130 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 08:25:03.176747 140573192058688 submission_runner.py:420] Time since start: 15687.56s, 	Step: 26158, 	{'train/accuracy': 0.6418652534484863, 'train/loss': 1.7051342725753784, 'train/bleu': 31.214415856423287, 'validation/accuracy': 0.6578715443611145, 'validation/loss': 1.6139212846755981, 'validation/bleu': 28.109127770348575, 'validation/num_examples': 3000, 'test/accuracy': 0.6680960059165955, 'test/loss': 1.5441603660583496, 'test/bleu': 27.350112428891073, 'test/num_examples': 3003, 'score': 9280.269245147705, 'total_duration': 15687.564682245255, 'accumulated_submission_time': 9280.269245147705, 'accumulated_eval_time': 6406.126123189926, 'accumulated_logging_time': 0.30628037452697754}
I0312 08:25:03.194854 140403528402688 logging_writer.py:48] [26158] accumulated_eval_time=6406.126123, accumulated_logging_time=0.306280, accumulated_submission_time=9280.269245, global_step=26158, preemption_count=0, score=9280.269245, test/accuracy=0.668096, test/bleu=27.350112, test/loss=1.544160, test/num_examples=3003, total_duration=15687.564682, train/accuracy=0.641865, train/bleu=31.214416, train/loss=1.705134, validation/accuracy=0.657872, validation/bleu=28.109128, validation/loss=1.613921, validation/num_examples=3000
I0312 08:25:18.377618 140403520009984 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.2106035351753235, loss=1.8856651782989502
I0312 08:25:53.758420 140403528402688 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.21717898547649384, loss=1.847429633140564
I0312 08:26:29.096199 140403520009984 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.17452272772789001, loss=1.809969425201416
I0312 08:27:04.427407 140403528402688 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.19611629843711853, loss=1.853832483291626
I0312 08:27:39.762250 140403520009984 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.19006140530109406, loss=1.7116256952285767
I0312 08:28:15.148221 140403528402688 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.25657302141189575, loss=1.7592490911483765
I0312 08:28:50.471955 140403520009984 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.20096102356910706, loss=1.9128401279449463
I0312 08:29:25.843602 140403528402688 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.19510067999362946, loss=1.8226574659347534
I0312 08:30:01.173818 140403520009984 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.3921489119529724, loss=1.9052517414093018
I0312 08:30:36.498379 140403528402688 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.19999191164970398, loss=1.8199589252471924
I0312 08:31:11.824984 140403520009984 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.21685412526130676, loss=1.8347128629684448
I0312 08:31:47.141588 140403528402688 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.2161283642053604, loss=1.8391209840774536
I0312 08:32:22.481028 140403520009984 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.1973543018102646, loss=1.803017258644104
I0312 08:32:57.805813 140403528402688 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.23354993760585785, loss=1.918038010597229
I0312 08:33:33.131106 140403520009984 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.19096693396568298, loss=1.8127201795578003
I0312 08:34:08.468190 140403528402688 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.1775989681482315, loss=1.809205174446106
I0312 08:34:43.803660 140403520009984 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.17434656620025635, loss=1.8128297328948975
I0312 08:35:19.150895 140403528402688 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.19726718962192535, loss=1.802523136138916
I0312 08:35:54.479962 140403520009984 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.205938458442688, loss=1.885948657989502
I0312 08:36:29.827626 140403528402688 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.18162040412425995, loss=1.8192001581192017
I0312 08:37:05.178462 140403520009984 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.21236370503902435, loss=1.8266762495040894
I0312 08:37:40.538484 140403528402688 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.19719025492668152, loss=1.799054741859436
I0312 08:38:15.871279 140403520009984 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.19991248846054077, loss=1.8564001321792603
I0312 08:38:51.198292 140403528402688 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.20866604149341583, loss=1.9065232276916504
I0312 08:39:03.298484 140573192058688 spec.py:321] Evaluating on the training split.
I0312 08:39:06.264060 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 08:42:56.266203 140573192058688 spec.py:333] Evaluating on the validation split.
I0312 08:42:58.953976 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 08:46:06.309887 140573192058688 spec.py:349] Evaluating on the test split.
I0312 08:46:08.977062 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 08:49:06.447056 140573192058688 submission_runner.py:420] Time since start: 17130.83s, 	Step: 28536, 	{'train/accuracy': 0.6390178799629211, 'train/loss': 1.7468416690826416, 'train/bleu': 31.29680946133006, 'validation/accuracy': 0.6594090461730957, 'validation/loss': 1.6080009937286377, 'validation/bleu': 28.322223092791035, 'validation/num_examples': 3000, 'test/accuracy': 0.6701993346214294, 'test/loss': 1.5299732685089111, 'test/bleu': 27.660913346503644, 'test/num_examples': 3003, 'score': 10120.285497665405, 'total_duration': 17130.83499264717, 'accumulated_submission_time': 10120.285497665405, 'accumulated_eval_time': 7009.274637699127, 'accumulated_logging_time': 0.33443307876586914}
I0312 08:49:06.465399 140403520009984 logging_writer.py:48] [28536] accumulated_eval_time=7009.274638, accumulated_logging_time=0.334433, accumulated_submission_time=10120.285498, global_step=28536, preemption_count=0, score=10120.285498, test/accuracy=0.670199, test/bleu=27.660913, test/loss=1.529973, test/num_examples=3003, total_duration=17130.834993, train/accuracy=0.639018, train/bleu=31.296809, train/loss=1.746842, validation/accuracy=0.659409, validation/bleu=28.322223, validation/loss=1.608001, validation/num_examples=3000
I0312 08:49:29.440285 140403528402688 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.23103421926498413, loss=1.8596785068511963
I0312 08:50:04.760857 140403520009984 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.165297269821167, loss=1.8176689147949219
I0312 08:50:40.040221 140403528402688 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.28666937351226807, loss=1.8614383935928345
I0312 08:51:15.356345 140403520009984 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.27328062057495117, loss=1.797933578491211
I0312 08:51:50.660921 140403528402688 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.2470930814743042, loss=1.776620864868164
I0312 08:52:25.962693 140403520009984 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.19894690811634064, loss=1.8675775527954102
I0312 08:53:01.277331 140403528402688 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.22435162961483002, loss=1.771559476852417
I0312 08:53:36.632893 140403520009984 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.21564504504203796, loss=1.8436042070388794
I0312 08:54:11.926097 140403528402688 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.21709378063678741, loss=1.8153941631317139
I0312 08:54:47.246615 140403520009984 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.1941792517900467, loss=1.8612756729125977
I0312 08:55:22.590845 140403528402688 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.20897313952445984, loss=1.797946572303772
I0312 08:55:57.934563 140403520009984 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.2111831158399582, loss=1.8215653896331787
I0312 08:56:33.267998 140403528402688 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.21512508392333984, loss=1.8776476383209229
I0312 08:57:08.650324 140403520009984 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.19698642194271088, loss=1.8478373289108276
I0312 08:57:44.055554 140403528402688 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.21070191264152527, loss=1.8195979595184326
I0312 08:58:19.390639 140403520009984 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.21374255418777466, loss=1.8346599340438843
I0312 08:58:54.729215 140403528402688 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.21711525321006775, loss=1.7362060546875
I0312 08:59:30.063469 140403520009984 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.27982088923454285, loss=1.8034210205078125
I0312 09:00:05.384740 140403528402688 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.2639892101287842, loss=1.868388056755066
I0312 09:00:40.704832 140403520009984 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.2422334849834442, loss=1.8381335735321045
I0312 09:01:16.054809 140403528402688 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.20955350995063782, loss=1.7349447011947632
I0312 09:01:51.358108 140403520009984 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.22848282754421234, loss=1.7463730573654175
I0312 09:02:26.708009 140403528402688 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.25246551632881165, loss=1.808303952217102
I0312 09:03:02.030777 140403520009984 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.34510648250579834, loss=1.793256163597107
I0312 09:03:06.701933 140573192058688 spec.py:321] Evaluating on the training split.
I0312 09:03:09.678519 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 09:06:46.092314 140573192058688 spec.py:333] Evaluating on the validation split.
I0312 09:06:48.780745 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 09:09:48.783897 140573192058688 spec.py:349] Evaluating on the test split.
I0312 09:09:51.458371 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 09:12:10.820168 140573192058688 submission_runner.py:420] Time since start: 18515.21s, 	Step: 30915, 	{'train/accuracy': 0.6399180293083191, 'train/loss': 1.7383826971054077, 'train/bleu': 31.006162959312263, 'validation/accuracy': 0.659421443939209, 'validation/loss': 1.5983082056045532, 'validation/bleu': 27.883103402956422, 'validation/num_examples': 3000, 'test/accuracy': 0.669839084148407, 'test/loss': 1.5221806764602661, 'test/bleu': 27.341967471219807, 'test/num_examples': 3003, 'score': 10960.433304071426, 'total_duration': 18515.208089351654, 'accumulated_submission_time': 10960.433304071426, 'accumulated_eval_time': 7553.392800569534, 'accumulated_logging_time': 0.36414217948913574}
I0312 09:12:10.841512 140403528402688 logging_writer.py:48] [30915] accumulated_eval_time=7553.392801, accumulated_logging_time=0.364142, accumulated_submission_time=10960.433304, global_step=30915, preemption_count=0, score=10960.433304, test/accuracy=0.669839, test/bleu=27.341967, test/loss=1.522181, test/num_examples=3003, total_duration=18515.208089, train/accuracy=0.639918, train/bleu=31.006163, train/loss=1.738383, validation/accuracy=0.659421, validation/bleu=27.883103, validation/loss=1.598308, validation/num_examples=3000
I0312 09:12:41.215119 140403520009984 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.22003889083862305, loss=1.7769346237182617
I0312 09:13:16.543890 140403528402688 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.2654254138469696, loss=1.8789912462234497
I0312 09:13:51.859888 140403520009984 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.2068568766117096, loss=1.7780747413635254
I0312 09:14:27.176002 140403528402688 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.2118227779865265, loss=1.8799176216125488
I0312 09:15:02.481950 140403520009984 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.2402203232049942, loss=1.7696517705917358
I0312 09:15:37.802669 140403528402688 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.2434820681810379, loss=1.7645225524902344
I0312 09:16:13.119008 140403520009984 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.20168428122997284, loss=1.815633773803711
I0312 09:16:48.461513 140403528402688 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.1994384378194809, loss=1.8661803007125854
I0312 09:17:23.772768 140403520009984 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.20148634910583496, loss=1.7570130825042725
I0312 09:17:59.090354 140403528402688 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.19075503945350647, loss=1.8663280010223389
I0312 09:18:34.471013 140403520009984 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.21175986528396606, loss=1.7724225521087646
I0312 09:19:09.853795 140403528402688 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.20236408710479736, loss=1.8044788837432861
I0312 09:19:45.235639 140403520009984 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.307686448097229, loss=1.8364289999008179
I0312 09:20:20.558879 140403528402688 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.22809426486492157, loss=1.7108354568481445
I0312 09:20:55.928922 140403520009984 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.1831117570400238, loss=1.7585760354995728
I0312 09:21:31.253800 140403528402688 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.19197161495685577, loss=1.7342277765274048
I0312 09:22:06.580519 140403520009984 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.20174990594387054, loss=1.766748070716858
I0312 09:22:41.957015 140403528402688 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.22610419988632202, loss=1.8520325422286987
I0312 09:23:17.353924 140403520009984 logging_writer.py:48] [32800] global_step=32800, grad_norm=1.846073031425476, loss=1.8732678890228271
I0312 09:23:52.743049 140403528402688 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.19353945553302765, loss=1.8182183504104614
I0312 09:24:28.087476 140403520009984 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.27240487933158875, loss=1.8286738395690918
I0312 09:25:03.420415 140403528402688 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.23020711541175842, loss=1.8362348079681396
I0312 09:25:38.729252 140403520009984 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.21169224381446838, loss=1.8183952569961548
I0312 09:26:10.961236 140573192058688 spec.py:321] Evaluating on the training split.
I0312 09:26:13.913738 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 09:30:02.460744 140573192058688 spec.py:333] Evaluating on the validation split.
I0312 09:30:05.129682 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 09:32:30.349038 140573192058688 spec.py:349] Evaluating on the test split.
I0312 09:32:33.027711 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 09:34:54.769024 140573192058688 submission_runner.py:420] Time since start: 19879.16s, 	Step: 33293, 	{'train/accuracy': 0.6442225575447083, 'train/loss': 1.6994339227676392, 'train/bleu': 31.78765129289712, 'validation/accuracy': 0.6610333323478699, 'validation/loss': 1.5850367546081543, 'validation/bleu': 28.362076457447394, 'validation/num_examples': 3000, 'test/accuracy': 0.6725698709487915, 'test/loss': 1.5092467069625854, 'test/bleu': 27.681689309528668, 'test/num_examples': 3003, 'score': 11800.462206363678, 'total_duration': 19879.156966924667, 'accumulated_submission_time': 11800.462206363678, 'accumulated_eval_time': 8077.200543165207, 'accumulated_logging_time': 0.3959531784057617}
I0312 09:34:54.788274 140403528402688 logging_writer.py:48] [33293] accumulated_eval_time=8077.200543, accumulated_logging_time=0.395953, accumulated_submission_time=11800.462206, global_step=33293, preemption_count=0, score=11800.462206, test/accuracy=0.672570, test/bleu=27.681689, test/loss=1.509247, test/num_examples=3003, total_duration=19879.156967, train/accuracy=0.644223, train/bleu=31.787651, train/loss=1.699434, validation/accuracy=0.661033, validation/bleu=28.362076, validation/loss=1.585037, validation/num_examples=3000
I0312 09:34:57.632639 140403520009984 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.1794992983341217, loss=1.7902287244796753
I0312 09:35:32.966043 140403528402688 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.22221171855926514, loss=1.7507091760635376
I0312 09:36:08.360321 140403520009984 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.17492234706878662, loss=1.8223885297775269
I0312 09:36:43.750942 140403528402688 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.17973262071609497, loss=1.75346040725708
I0312 09:37:19.080705 140403520009984 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.18118803203105927, loss=1.8385676145553589
I0312 09:37:54.434803 140403528402688 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.2453184276819229, loss=1.7913295030593872
I0312 09:38:29.765071 140403520009984 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.19409500062465668, loss=1.8863282203674316
I0312 09:39:05.105833 140403528402688 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.2177700400352478, loss=1.8173253536224365
I0312 09:39:40.452647 140403520009984 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.18985827267169952, loss=1.7320998907089233
I0312 09:40:15.804512 140403528402688 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.20535814762115479, loss=1.8161234855651855
I0312 09:40:51.122725 140403520009984 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.2313372790813446, loss=1.811503529548645
I0312 09:41:26.477544 140403528402688 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.20799493789672852, loss=1.7598392963409424
I0312 09:42:01.850836 140403520009984 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.1944705694913864, loss=1.790631890296936
I0312 09:42:37.277970 140403528402688 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.20283620059490204, loss=1.7784295082092285
I0312 09:43:12.668807 140403520009984 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.2987361252307892, loss=1.794273018836975
I0312 09:43:48.053469 140403528402688 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.17637303471565247, loss=1.7427756786346436
I0312 09:44:23.354941 140403520009984 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.1953355371952057, loss=1.8418314456939697
I0312 09:44:58.694967 140403528402688 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.20743699371814728, loss=1.7543537616729736
I0312 09:45:34.034385 140403520009984 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.2202753722667694, loss=1.8164360523223877
I0312 09:46:09.349184 140403528402688 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.18386149406433105, loss=1.7886074781417847
I0312 09:46:44.693690 140403520009984 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.23913367092609406, loss=1.7565460205078125
I0312 09:47:20.026820 140403528402688 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.19295121729373932, loss=1.7992349863052368
I0312 09:47:55.344529 140403520009984 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.19808390736579895, loss=1.8288520574569702
I0312 09:48:30.680881 140403528402688 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.22010545432567596, loss=1.8099039793014526
I0312 09:48:54.777398 140573192058688 spec.py:321] Evaluating on the training split.
I0312 09:48:57.731587 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 09:53:06.131586 140573192058688 spec.py:333] Evaluating on the validation split.
I0312 09:53:08.793776 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 09:55:36.004452 140573192058688 spec.py:349] Evaluating on the test split.
I0312 09:55:38.674337 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 09:57:51.011413 140573192058688 submission_runner.py:420] Time since start: 21255.40s, 	Step: 35670, 	{'train/accuracy': 0.6402103304862976, 'train/loss': 1.7327693700790405, 'train/bleu': 31.204288012321065, 'validation/accuracy': 0.6622112393379211, 'validation/loss': 1.5775601863861084, 'validation/bleu': 28.52652591972902, 'validation/num_examples': 3000, 'test/accuracy': 0.6746150851249695, 'test/loss': 1.4966024160385132, 'test/bleu': 27.680088726518168, 'test/num_examples': 3003, 'score': 12640.359733104706, 'total_duration': 21255.39935207367, 'accumulated_submission_time': 12640.359733104706, 'accumulated_eval_time': 8613.434501886368, 'accumulated_logging_time': 0.4250335693359375}
I0312 09:57:51.030440 140403520009984 logging_writer.py:48] [35670] accumulated_eval_time=8613.434502, accumulated_logging_time=0.425034, accumulated_submission_time=12640.359733, global_step=35670, preemption_count=0, score=12640.359733, test/accuracy=0.674615, test/bleu=27.680089, test/loss=1.496602, test/num_examples=3003, total_duration=21255.399352, train/accuracy=0.640210, train/bleu=31.204288, train/loss=1.732769, validation/accuracy=0.662211, validation/bleu=28.526526, validation/loss=1.577560, validation/num_examples=3000
I0312 09:58:01.989725 140403528402688 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.191332146525383, loss=1.750232219696045
I0312 09:58:37.314375 140403520009984 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.20579786598682404, loss=1.751079797744751
I0312 09:59:12.663473 140403528402688 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.20854076743125916, loss=1.8489458560943604
I0312 09:59:47.994979 140403520009984 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.20242886245250702, loss=1.7571722269058228
I0312 10:00:23.324279 140403528402688 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.2013765573501587, loss=1.7177448272705078
I0312 10:00:58.665616 140403520009984 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.20038968324661255, loss=1.8684242963790894
I0312 10:01:33.985280 140403528402688 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.2074846774339676, loss=1.7101079225540161
I0312 10:02:09.298557 140403520009984 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.1851421743631363, loss=1.8396772146224976
I0312 10:02:44.631181 140403528402688 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.21108505129814148, loss=1.749607801437378
I0312 10:03:19.944024 140403520009984 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.2626011371612549, loss=1.7784072160720825
I0312 10:03:55.265684 140403528402688 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.18819008767604828, loss=1.7149394750595093
I0312 10:04:30.567434 140403520009984 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.21740025281906128, loss=1.762817144393921
I0312 10:05:05.899195 140403528402688 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.20189495384693146, loss=1.725232481956482
I0312 10:05:41.197163 140403520009984 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.20916499197483063, loss=1.7754120826721191
I0312 10:06:16.508863 140403528402688 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.20536163449287415, loss=1.6718072891235352
I0312 10:06:51.853806 140403520009984 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.19460883736610413, loss=1.7560402154922485
I0312 10:07:27.179015 140403528402688 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.2854492664337158, loss=1.8295632600784302
I0312 10:08:02.526274 140403520009984 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.198074072599411, loss=1.838830828666687
I0312 10:08:37.854820 140403528402688 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.18231995403766632, loss=1.8206586837768555
I0312 10:09:13.186677 140403520009984 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.22445668280124664, loss=1.7175281047821045
I0312 10:09:48.511099 140403528402688 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.23499023914337158, loss=1.8325920104980469
I0312 10:10:23.859239 140403520009984 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.2359260618686676, loss=1.7291358709335327
I0312 10:10:59.193356 140403528402688 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.18724167346954346, loss=1.7278473377227783
I0312 10:11:34.518288 140403520009984 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.17833077907562256, loss=1.769726276397705
I0312 10:11:51.212862 140573192058688 spec.py:321] Evaluating on the training split.
I0312 10:11:54.175023 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 10:15:12.368258 140573192058688 spec.py:333] Evaluating on the validation split.
I0312 10:15:15.037932 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 10:17:49.547215 140573192058688 spec.py:349] Evaluating on the test split.
I0312 10:17:52.221973 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 10:20:07.729742 140573192058688 submission_runner.py:420] Time since start: 22592.12s, 	Step: 38049, 	{'train/accuracy': 0.6570454835891724, 'train/loss': 1.6043413877487183, 'train/bleu': 32.339918945597454, 'validation/accuracy': 0.6630792021751404, 'validation/loss': 1.5772230625152588, 'validation/bleu': 28.517361145543024, 'validation/num_examples': 3000, 'test/accuracy': 0.6761025190353394, 'test/loss': 1.4913530349731445, 'test/bleu': 28.023246782720474, 'test/num_examples': 3003, 'score': 13480.456578493118, 'total_duration': 22592.117666721344, 'accumulated_submission_time': 13480.456578493118, 'accumulated_eval_time': 9109.951312541962, 'accumulated_logging_time': 0.45396995544433594}
I0312 10:20:07.749459 140403528402688 logging_writer.py:48] [38049] accumulated_eval_time=9109.951313, accumulated_logging_time=0.453970, accumulated_submission_time=13480.456578, global_step=38049, preemption_count=0, score=13480.456578, test/accuracy=0.676103, test/bleu=28.023247, test/loss=1.491353, test/num_examples=3003, total_duration=22592.117667, train/accuracy=0.657045, train/bleu=32.339919, train/loss=1.604341, validation/accuracy=0.663079, validation/bleu=28.517361, validation/loss=1.577223, validation/num_examples=3000
I0312 10:20:26.127512 140403520009984 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.22427161037921906, loss=1.7511212825775146
I0312 10:21:01.468114 140403528402688 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.1983948051929474, loss=1.8024253845214844
I0312 10:21:36.801848 140403520009984 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.20117107033729553, loss=1.7366321086883545
I0312 10:22:12.140238 140403528402688 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.22576554119586945, loss=1.7947847843170166
I0312 10:22:47.506578 140403520009984 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.2026997208595276, loss=1.7395225763320923
I0312 10:23:22.863698 140403528402688 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.2104998230934143, loss=1.841454267501831
I0312 10:23:58.278601 140403520009984 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.21373654901981354, loss=1.804062008857727
I0312 10:24:33.596253 140403528402688 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.17228944599628448, loss=1.7362622022628784
I0312 10:25:08.927572 140403520009984 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.268807590007782, loss=1.7914177179336548
I0312 10:25:44.241156 140403528402688 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.19788013398647308, loss=1.7478471994400024
I0312 10:26:19.561079 140403520009984 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.21300984919071198, loss=1.784515142440796
I0312 10:26:54.901947 140403528402688 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.20035424828529358, loss=1.8559074401855469
I0312 10:27:30.223716 140403520009984 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.21587760746479034, loss=1.8408595323562622
I0312 10:28:05.536017 140403528402688 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.19085285067558289, loss=1.753939151763916
I0312 10:28:40.848499 140403520009984 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.2004728764295578, loss=1.7464821338653564
I0312 10:29:16.152095 140403528402688 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.22277098894119263, loss=1.7424982786178589
I0312 10:29:51.474040 140403520009984 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.2028813362121582, loss=1.746497631072998
I0312 10:30:26.797446 140403528402688 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.19324806332588196, loss=1.8044507503509521
I0312 10:31:02.109662 140403520009984 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.18353216350078583, loss=1.6957809925079346
I0312 10:31:37.443030 140403528402688 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.22171255946159363, loss=1.7006409168243408
I0312 10:32:12.766797 140403520009984 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.21346595883369446, loss=1.7163283824920654
I0312 10:32:48.082123 140403528402688 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.2017185091972351, loss=1.7899564504623413
I0312 10:33:23.541721 140403520009984 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.2027355283498764, loss=1.8036108016967773
I0312 10:33:58.872217 140403528402688 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.20101600885391235, loss=1.8315739631652832
I0312 10:34:07.774276 140573192058688 spec.py:321] Evaluating on the training split.
I0312 10:34:10.736862 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 10:38:38.964504 140573192058688 spec.py:333] Evaluating on the validation split.
I0312 10:38:41.633231 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 10:41:36.167823 140573192058688 spec.py:349] Evaluating on the test split.
I0312 10:41:38.847911 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 10:44:51.072344 140573192058688 submission_runner.py:420] Time since start: 24075.46s, 	Step: 40427, 	{'train/accuracy': 0.6448788642883301, 'train/loss': 1.695878028869629, 'train/bleu': 31.994719373499642, 'validation/accuracy': 0.6641083359718323, 'validation/loss': 1.5695807933807373, 'validation/bleu': 28.419235351973153, 'validation/num_examples': 3000, 'test/accuracy': 0.6763232946395874, 'test/loss': 1.482987880706787, 'test/bleu': 27.918226356713774, 'test/num_examples': 3003, 'score': 14320.392687559128, 'total_duration': 24075.460280895233, 'accumulated_submission_time': 14320.392687559128, 'accumulated_eval_time': 9753.249324083328, 'accumulated_logging_time': 0.4837007522583008}
I0312 10:44:51.092136 140403520009984 logging_writer.py:48] [40427] accumulated_eval_time=9753.249324, accumulated_logging_time=0.483701, accumulated_submission_time=14320.392688, global_step=40427, preemption_count=0, score=14320.392688, test/accuracy=0.676323, test/bleu=27.918226, test/loss=1.482988, test/num_examples=3003, total_duration=24075.460281, train/accuracy=0.644879, train/bleu=31.994719, train/loss=1.695878, validation/accuracy=0.664108, validation/bleu=28.419235, validation/loss=1.569581, validation/num_examples=3000
I0312 10:45:17.185650 140403528402688 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.20915411412715912, loss=1.7519052028656006
I0312 10:45:52.496935 140403520009984 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.20548303425312042, loss=1.7544125318527222
I0312 10:46:27.804637 140403528402688 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.24296888709068298, loss=1.7765718698501587
I0312 10:47:03.144868 140403520009984 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.19822509586811066, loss=1.8023606538772583
I0312 10:47:38.472835 140403528402688 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.22304129600524902, loss=1.820554494857788
I0312 10:48:13.849735 140403520009984 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.2136443555355072, loss=1.7151410579681396
I0312 10:48:49.254522 140403528402688 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.24923016130924225, loss=1.790756344795227
I0312 10:49:24.671325 140403520009984 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.1843366175889969, loss=1.7346558570861816
I0312 10:50:00.005856 140403528402688 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.19136831164360046, loss=1.8284214735031128
I0312 10:50:35.346048 140403520009984 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.21934874355793, loss=1.789471983909607
I0312 10:51:10.655015 140403528402688 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.19032669067382812, loss=1.689281940460205
I0312 10:51:45.976593 140403520009984 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.31149032711982727, loss=1.7993143796920776
I0312 10:52:21.287611 140403528402688 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.20225325226783752, loss=1.7809494733810425
I0312 10:52:56.630807 140403520009984 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.20981839299201965, loss=1.7407374382019043
I0312 10:53:31.938415 140403528402688 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.23737135529518127, loss=1.733054518699646
I0312 10:54:07.284449 140403520009984 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.2156866192817688, loss=1.7374964952468872
I0312 10:54:42.611304 140403528402688 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.2272786796092987, loss=1.7780219316482544
I0312 10:55:17.929898 140403520009984 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.20071792602539062, loss=1.7717853784561157
I0312 10:55:53.248501 140403528402688 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.20094582438468933, loss=1.7802900075912476
I0312 10:56:28.569437 140403520009984 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.20540615916252136, loss=1.8245532512664795
I0312 10:57:03.859417 140403528402688 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.23241998255252838, loss=1.7266663312911987
I0312 10:57:39.214695 140403520009984 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.20025309920310974, loss=1.7343229055404663
I0312 10:58:14.515548 140403528402688 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.19471359252929688, loss=1.7605302333831787
I0312 10:58:49.851312 140403520009984 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.20642228424549103, loss=1.6869746446609497
I0312 10:58:51.346042 140573192058688 spec.py:321] Evaluating on the training split.
I0312 10:58:54.316999 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 11:02:28.562065 140573192058688 spec.py:333] Evaluating on the validation split.
I0312 11:02:31.213741 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 11:05:22.973985 140573192058688 spec.py:349] Evaluating on the test split.
I0312 11:05:25.645861 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 11:07:47.308294 140573192058688 submission_runner.py:420] Time since start: 25451.70s, 	Step: 42806, 	{'train/accuracy': 0.6462165117263794, 'train/loss': 1.6971834897994995, 'train/bleu': 31.77797547770024, 'validation/accuracy': 0.6654722094535828, 'validation/loss': 1.557735800743103, 'validation/bleu': 28.557748479244324, 'validation/num_examples': 3000, 'test/accuracy': 0.6781709790229797, 'test/loss': 1.4798240661621094, 'test/bleu': 27.79226369399139, 'test/num_examples': 3003, 'score': 15160.558741569519, 'total_duration': 25451.696177721024, 'accumulated_submission_time': 15160.558741569519, 'accumulated_eval_time': 10289.211467266083, 'accumulated_logging_time': 0.5132253170013428}
I0312 11:07:47.332246 140403528402688 logging_writer.py:48] [42806] accumulated_eval_time=10289.211467, accumulated_logging_time=0.513225, accumulated_submission_time=15160.558742, global_step=42806, preemption_count=0, score=15160.558742, test/accuracy=0.678171, test/bleu=27.792264, test/loss=1.479824, test/num_examples=3003, total_duration=25451.696178, train/accuracy=0.646217, train/bleu=31.777975, train/loss=1.697183, validation/accuracy=0.665472, validation/bleu=28.557748, validation/loss=1.557736, validation/num_examples=3000
I0312 11:08:20.932498 140403520009984 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.19751566648483276, loss=1.8385409116744995
I0312 11:08:56.315379 140403528402688 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.19891604781150818, loss=1.7700598239898682
I0312 11:09:31.675895 140403520009984 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.18624958395957947, loss=1.7420257329940796
I0312 11:10:07.151861 140403528402688 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.1738993525505066, loss=1.7144896984100342
I0312 11:10:42.546514 140403520009984 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.18326281011104584, loss=1.699224591255188
I0312 11:11:17.921172 140403528402688 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.19849444925785065, loss=1.7756189107894897
I0312 11:11:53.253205 140403520009984 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.1733768731355667, loss=1.736383080482483
I0312 11:12:28.562896 140403528402688 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.17602293193340302, loss=1.7491376399993896
I0312 11:13:03.886041 140403520009984 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.2295391857624054, loss=1.794264316558838
I0312 11:13:39.218336 140403528402688 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.2337961047887802, loss=1.784883975982666
I0312 11:14:14.542021 140403520009984 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.19582514464855194, loss=1.779133677482605
I0312 11:14:49.854343 140403528402688 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.1996542364358902, loss=1.7110241651535034
I0312 11:15:25.185916 140403520009984 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.1874607801437378, loss=1.8007360696792603
I0312 11:16:00.531671 140403528402688 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.25514817237854004, loss=1.736544132232666
I0312 11:16:35.851277 140403520009984 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.21434910595417023, loss=1.8096791505813599
I0312 11:17:11.171015 140403528402688 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.20788870751857758, loss=1.7186187505722046
I0312 11:17:46.483224 140403520009984 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.271307110786438, loss=1.8252196311950684
I0312 11:18:21.793097 140403528402688 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.19185754656791687, loss=1.7398152351379395
I0312 11:18:57.123018 140403520009984 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.20629437267780304, loss=1.7618306875228882
I0312 11:19:32.470036 140403528402688 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.20155124366283417, loss=1.7384566068649292
I0312 11:20:07.835934 140403520009984 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.2223610132932663, loss=1.7321546077728271
I0312 11:20:43.144529 140403528402688 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.25361010432243347, loss=1.7990081310272217
I0312 11:21:18.465430 140403520009984 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.2535609304904938, loss=1.7982957363128662
I0312 11:21:47.492754 140573192058688 spec.py:321] Evaluating on the training split.
I0312 11:21:50.446836 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 11:25:40.619624 140573192058688 spec.py:333] Evaluating on the validation split.
I0312 11:25:43.287562 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 11:28:09.586398 140573192058688 spec.py:349] Evaluating on the test split.
I0312 11:28:12.270146 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 11:30:28.175992 140573192058688 submission_runner.py:420] Time since start: 26812.56s, 	Step: 45184, 	{'train/accuracy': 0.6523397564888, 'train/loss': 1.6415144205093384, 'train/bleu': 32.163557741677224, 'validation/accuracy': 0.6671088933944702, 'validation/loss': 1.5524981021881104, 'validation/bleu': 29.014261662014693, 'validation/num_examples': 3000, 'test/accuracy': 0.6798791885375977, 'test/loss': 1.4696717262268066, 'test/bleu': 28.49927497697626, 'test/num_examples': 3003, 'score': 16000.626110076904, 'total_duration': 26812.56391477585, 'accumulated_submission_time': 16000.626110076904, 'accumulated_eval_time': 10809.894648313522, 'accumulated_logging_time': 0.5482079982757568}
I0312 11:30:28.197720 140403528402688 logging_writer.py:48] [45184] accumulated_eval_time=10809.894648, accumulated_logging_time=0.548208, accumulated_submission_time=16000.626110, global_step=45184, preemption_count=0, score=16000.626110, test/accuracy=0.679879, test/bleu=28.499275, test/loss=1.469672, test/num_examples=3003, total_duration=26812.563915, train/accuracy=0.652340, train/bleu=32.163558, train/loss=1.641514, validation/accuracy=0.667109, validation/bleu=29.014262, validation/loss=1.552498, validation/num_examples=3000
I0312 11:30:34.214385 140403520009984 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.1899312287569046, loss=1.7270243167877197
I0312 11:31:09.530682 140403528402688 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.2095915824174881, loss=1.724963903427124
I0312 11:31:44.880491 140403520009984 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.2221209853887558, loss=1.7720317840576172
I0312 11:32:20.253011 140403528402688 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.27382543683052063, loss=1.777569055557251
I0312 11:32:55.591193 140403520009984 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.21997185051441193, loss=1.7771538496017456
I0312 11:33:30.921470 140403528402688 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.18717867136001587, loss=1.7332707643508911
I0312 11:34:06.297759 140403520009984 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.21225666999816895, loss=1.6406919956207275
I0312 11:34:41.665520 140403528402688 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.1819882094860077, loss=1.7706702947616577
I0312 11:35:16.976354 140403520009984 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.2096073031425476, loss=1.7261253595352173
I0312 11:35:52.277895 140403528402688 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.20897504687309265, loss=1.8017410039901733
I0312 11:36:27.578248 140403520009984 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.2275863140821457, loss=1.7329999208450317
I0312 11:37:02.921449 140403528402688 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.2077784240245819, loss=1.7495243549346924
I0312 11:37:38.248219 140403520009984 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.20242293179035187, loss=1.7953335046768188
I0312 11:38:13.571597 140403528402688 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.19203460216522217, loss=1.7747185230255127
I0312 11:38:48.884130 140403520009984 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.1900375336408615, loss=1.6320513486862183
I0312 11:39:24.218879 140403528402688 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.1880183070898056, loss=1.8283615112304688
I0312 11:39:59.558124 140403520009984 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.18982429802417755, loss=1.7942160367965698
I0312 11:40:34.916496 140403528402688 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.20157641172409058, loss=1.7791343927383423
I0312 11:41:10.308877 140403520009984 logging_writer.py:48] [47000] global_step=47000, grad_norm=1.4255248308181763, loss=1.8102790117263794
I0312 11:41:45.739566 140403528402688 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.3825380802154541, loss=1.8104833364486694
I0312 11:42:21.073613 140403520009984 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.1793077439069748, loss=1.7763197422027588
I0312 11:42:56.392992 140403528402688 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.18945662677288055, loss=1.7001605033874512
I0312 11:43:31.699790 140403520009984 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.20649243891239166, loss=1.7632620334625244
I0312 11:44:07.001346 140403528402688 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.19027750194072723, loss=1.711283564567566
I0312 11:44:28.263879 140573192058688 spec.py:321] Evaluating on the training split.
I0312 11:44:31.217333 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 11:48:44.866284 140573192058688 spec.py:333] Evaluating on the validation split.
I0312 11:48:47.541305 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 11:52:36.995586 140573192058688 spec.py:349] Evaluating on the test split.
I0312 11:52:39.685826 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 11:56:36.646997 140573192058688 submission_runner.py:420] Time since start: 28381.03s, 	Step: 47562, 	{'train/accuracy': 0.6457970142364502, 'train/loss': 1.698169231414795, 'train/bleu': 31.71187306106621, 'validation/accuracy': 0.6662285327911377, 'validation/loss': 1.5489721298217773, 'validation/bleu': 28.7745931280484, 'validation/num_examples': 3000, 'test/accuracy': 0.679332971572876, 'test/loss': 1.4617844820022583, 'test/bleu': 28.274578857046443, 'test/num_examples': 3003, 'score': 16840.60457134247, 'total_duration': 28381.03490614891, 'accumulated_submission_time': 16840.60457134247, 'accumulated_eval_time': 11538.277686834335, 'accumulated_logging_time': 0.579641580581665}
I0312 11:56:36.672936 140403520009984 logging_writer.py:48] [47562] accumulated_eval_time=11538.277687, accumulated_logging_time=0.579642, accumulated_submission_time=16840.604571, global_step=47562, preemption_count=0, score=16840.604571, test/accuracy=0.679333, test/bleu=28.274579, test/loss=1.461784, test/num_examples=3003, total_duration=28381.034906, train/accuracy=0.645797, train/bleu=31.711873, train/loss=1.698169, validation/accuracy=0.666229, validation/bleu=28.774593, validation/loss=1.548972, validation/num_examples=3000
I0312 11:56:50.449838 140403528402688 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.22240208089351654, loss=1.7946820259094238
I0312 11:57:25.744668 140403520009984 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.20891284942626953, loss=1.683358907699585
I0312 11:58:01.068563 140403528402688 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.18668200075626373, loss=1.7158753871917725
I0312 11:58:36.366065 140403520009984 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.1968073695898056, loss=1.798906683921814
I0312 11:59:11.661580 140403528402688 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.2250833660364151, loss=1.786376714706421
I0312 11:59:47.007742 140403520009984 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.2088502049446106, loss=1.7769556045532227
I0312 12:00:22.312838 140403528402688 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.21740081906318665, loss=1.7077258825302124
I0312 12:00:57.588038 140403520009984 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.22481054067611694, loss=1.7547216415405273
I0312 12:01:32.900376 140403528402688 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.21036629378795624, loss=1.7479727268218994
I0312 12:02:08.215007 140403520009984 logging_writer.py:48] [48500] global_step=48500, grad_norm=1.0198140144348145, loss=1.7253851890563965
I0312 12:02:43.529623 140403528402688 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.22217312455177307, loss=1.8235913515090942
I0312 12:03:18.821070 140403520009984 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.19795306026935577, loss=1.771188497543335
I0312 12:03:54.126699 140403528402688 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.23856812715530396, loss=1.7619292736053467
I0312 12:04:29.456193 140403520009984 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.20441795885562897, loss=1.8179343938827515
I0312 12:05:04.769633 140403528402688 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.19296135008335114, loss=1.7843289375305176
I0312 12:05:40.066540 140403520009984 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.19354893267154694, loss=1.7265326976776123
I0312 12:06:15.391652 140403528402688 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.2089325189590454, loss=1.711898922920227
I0312 12:06:50.721091 140403520009984 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.22129970788955688, loss=1.7126495838165283
I0312 12:07:26.049998 140403528402688 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.22520937025547028, loss=1.7568743228912354
I0312 12:08:01.361314 140403520009984 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.19163742661476135, loss=1.8058555126190186
I0312 12:08:36.678251 140403528402688 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.23577967286109924, loss=1.8711597919464111
I0312 12:09:11.972670 140403520009984 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.2198130488395691, loss=1.7579809427261353
I0312 12:09:47.306317 140403528402688 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.1919269561767578, loss=1.7195477485656738
I0312 12:10:22.624297 140403520009984 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.1880481094121933, loss=1.7119156122207642
I0312 12:10:36.828575 140573192058688 spec.py:321] Evaluating on the training split.
I0312 12:10:39.783048 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 12:14:19.421924 140573192058688 spec.py:333] Evaluating on the validation split.
I0312 12:14:22.091635 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 12:17:04.827289 140573192058688 spec.py:349] Evaluating on the test split.
I0312 12:17:07.505244 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 12:19:56.973670 140573192058688 submission_runner.py:420] Time since start: 29781.36s, 	Step: 49942, 	{'train/accuracy': 0.6485532522201538, 'train/loss': 1.6859617233276367, 'train/bleu': 31.9474758616229, 'validation/accuracy': 0.6677164435386658, 'validation/loss': 1.5421111583709717, 'validation/bleu': 28.515111507698936, 'validation/num_examples': 3000, 'test/accuracy': 0.6789495348930359, 'test/loss': 1.4625924825668335, 'test/bleu': 28.027025139918145, 'test/num_examples': 3003, 'score': 17680.672848939896, 'total_duration': 29781.36154270172, 'accumulated_submission_time': 17680.672848939896, 'accumulated_eval_time': 12098.422665834427, 'accumulated_logging_time': 0.6169235706329346}
I0312 12:19:56.998268 140403528402688 logging_writer.py:48] [49942] accumulated_eval_time=12098.422666, accumulated_logging_time=0.616924, accumulated_submission_time=17680.672849, global_step=49942, preemption_count=0, score=17680.672849, test/accuracy=0.678950, test/bleu=28.027025, test/loss=1.462592, test/num_examples=3003, total_duration=29781.361543, train/accuracy=0.648553, train/bleu=31.947476, train/loss=1.685962, validation/accuracy=0.667716, validation/bleu=28.515112, validation/loss=1.542111, validation/num_examples=3000
I0312 12:20:17.820283 140403520009984 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.1941434144973755, loss=1.6624270677566528
I0312 12:20:53.156966 140403528402688 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.21162459254264832, loss=1.708065390586853
I0312 12:21:28.448280 140403520009984 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.1982036530971527, loss=1.6353569030761719
I0312 12:22:03.769315 140403528402688 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.18769097328186035, loss=1.769601583480835
I0312 12:22:39.084430 140403520009984 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.20938341319561005, loss=1.753985047340393
I0312 12:23:14.387681 140403528402688 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.18485227227210999, loss=1.7139148712158203
I0312 12:23:49.724578 140403520009984 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.3145763576030731, loss=1.7534763813018799
I0312 12:24:25.046377 140403528402688 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.2203798145055771, loss=1.7912250757217407
I0312 12:25:00.370120 140403520009984 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.21091127395629883, loss=1.7266182899475098
I0312 12:25:35.702829 140403528402688 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.20735524594783783, loss=1.7608674764633179
I0312 12:26:11.031541 140403520009984 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.22013190388679504, loss=1.745133638381958
I0312 12:26:46.331248 140403528402688 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.21029120683670044, loss=1.7191823720932007
I0312 12:27:21.661010 140403520009984 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.2054966390132904, loss=1.75266695022583
I0312 12:27:56.998196 140403528402688 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.22989051043987274, loss=1.768846869468689
I0312 12:28:32.339215 140403520009984 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.20520809292793274, loss=1.7320698499679565
I0312 12:29:07.650620 140403528402688 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.20099954307079315, loss=1.6987643241882324
I0312 12:29:42.995278 140403520009984 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.20809073746204376, loss=1.7147358655929565
I0312 12:30:18.311032 140403528402688 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.18847471475601196, loss=1.7468013763427734
I0312 12:30:53.681259 140403520009984 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.20160667598247528, loss=1.8500415086746216
I0312 12:31:29.015368 140403528402688 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.19874735176563263, loss=1.763730764389038
I0312 12:32:04.356542 140403520009984 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.20381838083267212, loss=1.7469021081924438
I0312 12:32:39.679492 140403528402688 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.19109027087688446, loss=1.8251744508743286
I0312 12:33:15.017633 140403520009984 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.20267142355442047, loss=1.6595176458358765
I0312 12:33:50.325257 140403528402688 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.20025815069675446, loss=1.7257189750671387
I0312 12:33:57.117774 140573192058688 spec.py:321] Evaluating on the training split.
I0312 12:34:00.076165 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 12:38:18.644079 140573192058688 spec.py:333] Evaluating on the validation split.
I0312 12:38:21.310597 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 12:41:05.499644 140573192058688 spec.py:349] Evaluating on the test split.
I0312 12:41:08.176852 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 12:43:32.490662 140573192058688 submission_runner.py:420] Time since start: 31196.88s, 	Step: 52321, 	{'train/accuracy': 0.6484258770942688, 'train/loss': 1.6598361730575562, 'train/bleu': 31.918248314161886, 'validation/accuracy': 0.6704814434051514, 'validation/loss': 1.5352940559387207, 'validation/bleu': 28.953870464578415, 'validation/num_examples': 3000, 'test/accuracy': 0.6807622909545898, 'test/loss': 1.4502663612365723, 'test/bleu': 28.344669175234554, 'test/num_examples': 3003, 'score': 18520.70462822914, 'total_duration': 31196.878527402878, 'accumulated_submission_time': 18520.70462822914, 'accumulated_eval_time': 12673.795425891876, 'accumulated_logging_time': 0.6527571678161621}
I0312 12:43:32.516486 140403520009984 logging_writer.py:48] [52321] accumulated_eval_time=12673.795426, accumulated_logging_time=0.652757, accumulated_submission_time=18520.704628, global_step=52321, preemption_count=0, score=18520.704628, test/accuracy=0.680762, test/bleu=28.344669, test/loss=1.450266, test/num_examples=3003, total_duration=31196.878527, train/accuracy=0.648426, train/bleu=31.918248, train/loss=1.659836, validation/accuracy=0.670481, validation/bleu=28.953870, validation/loss=1.535294, validation/num_examples=3000
I0312 12:44:00.809324 140403528402688 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.18616953492164612, loss=1.7763172388076782
I0312 12:44:36.089774 140403520009984 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.20246616005897522, loss=1.740991234779358
I0312 12:45:11.357960 140403528402688 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.2064688801765442, loss=1.7478182315826416
I0312 12:45:46.668394 140403520009984 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.1947455108165741, loss=1.6449916362762451
I0312 12:46:21.988205 140403528402688 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.18407787382602692, loss=1.7694092988967896
I0312 12:46:57.291219 140403520009984 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.19631290435791016, loss=1.7337615489959717
I0312 12:47:32.614151 140403528402688 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.2243606448173523, loss=1.6890199184417725
I0312 12:48:07.921938 140403520009984 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.18627583980560303, loss=1.7053643465042114
I0312 12:48:43.235427 140403528402688 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.22839084267616272, loss=1.8635772466659546
I0312 12:49:18.552738 140403520009984 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.21902649104595184, loss=1.675431489944458
I0312 12:49:53.867887 140403528402688 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.20103634893894196, loss=1.8067132234573364
I0312 12:50:29.165397 140403520009984 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.20245622098445892, loss=1.7058922052383423
I0312 12:51:04.479641 140403528402688 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.1951710730791092, loss=1.7861552238464355
I0312 12:51:39.818631 140403520009984 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.23679593205451965, loss=1.6733890771865845
I0312 12:52:15.182411 140403528402688 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.19744108617305756, loss=1.755049228668213
I0312 12:52:50.492027 140403520009984 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.23462586104869843, loss=1.6920326948165894
I0312 12:53:25.802105 140403528402688 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.18624943494796753, loss=1.7835434675216675
I0312 12:54:01.122052 140403520009984 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.20002678036689758, loss=1.7794605493545532
I0312 12:54:36.424495 140403528402688 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.18520699441432953, loss=1.7157164812088013
I0312 12:55:11.748585 140403520009984 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.19163189828395844, loss=1.7761832475662231
I0312 12:55:47.060506 140403528402688 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.20079247653484344, loss=1.77825129032135
I0312 12:56:22.352866 140403520009984 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.20846781134605408, loss=1.6790999174118042
I0312 12:56:57.733339 140403528402688 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.18422552943229675, loss=1.753259539604187
I0312 12:57:33.116475 140403520009984 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.20532788336277008, loss=1.6775528192520142
I0312 12:57:33.122525 140573192058688 spec.py:321] Evaluating on the training split.
I0312 12:57:35.803459 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 13:01:18.049326 140573192058688 spec.py:333] Evaluating on the validation split.
I0312 13:01:20.722089 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 13:03:55.021331 140573192058688 spec.py:349] Evaluating on the test split.
I0312 13:03:57.700267 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 13:06:26.549277 140573192058688 submission_runner.py:420] Time since start: 32570.94s, 	Step: 54701, 	{'train/accuracy': 0.6495399475097656, 'train/loss': 1.6735833883285522, 'train/bleu': 31.643514298402373, 'validation/accuracy': 0.6713989973068237, 'validation/loss': 1.5312122106552124, 'validation/bleu': 29.22028575150289, 'validation/num_examples': 3000, 'test/accuracy': 0.6822962164878845, 'test/loss': 1.4418914318084717, 'test/bleu': 28.36710670375829, 'test/num_examples': 3003, 'score': 19361.222714662552, 'total_duration': 32570.93722343445, 'accumulated_submission_time': 19361.222714662552, 'accumulated_eval_time': 13207.22210764885, 'accumulated_logging_time': 0.6896021366119385}
I0312 13:06:26.571131 140403528402688 logging_writer.py:48] [54701] accumulated_eval_time=13207.222108, accumulated_logging_time=0.689602, accumulated_submission_time=19361.222715, global_step=54701, preemption_count=0, score=19361.222715, test/accuracy=0.682296, test/bleu=28.367107, test/loss=1.441891, test/num_examples=3003, total_duration=32570.937223, train/accuracy=0.649540, train/bleu=31.643514, train/loss=1.673583, validation/accuracy=0.671399, validation/bleu=29.220286, validation/loss=1.531212, validation/num_examples=3000
I0312 13:07:01.898168 140403520009984 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.23569735884666443, loss=1.7965142726898193
I0312 13:07:37.271209 140403528402688 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.1880439668893814, loss=1.6375420093536377
I0312 13:08:12.635730 140403520009984 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.1953766644001007, loss=1.7251932621002197
I0312 13:08:47.981865 140403528402688 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.18529640138149261, loss=1.7356541156768799
I0312 13:09:23.337045 140403520009984 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.18237967789173126, loss=1.6935657262802124
I0312 13:09:58.728914 140403528402688 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.18781262636184692, loss=1.7290613651275635
I0312 13:10:34.076653 140403520009984 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.6815997362136841, loss=1.9216426610946655
I0312 13:11:09.426254 140403528402688 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.23150327801704407, loss=1.7741599082946777
I0312 13:11:44.768683 140403520009984 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.23775352537631989, loss=1.6784534454345703
I0312 13:12:20.151011 140403528402688 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.23650141060352325, loss=1.8278251886367798
I0312 13:12:55.455340 140403520009984 logging_writer.py:48] [55800] global_step=55800, grad_norm=1.0030059814453125, loss=1.7280499935150146
I0312 13:13:30.751674 140403528402688 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.20914910733699799, loss=1.7958346605300903
I0312 13:14:06.076953 140403520009984 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.26478907465934753, loss=1.7725627422332764
I0312 13:14:41.393245 140403528402688 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.1895652413368225, loss=1.7266860008239746
I0312 13:15:16.712990 140403520009984 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.2298325151205063, loss=1.8233332633972168
I0312 13:15:52.090601 140403528402688 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.20464998483657837, loss=1.7213220596313477
I0312 13:16:27.423508 140403520009984 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.21801352500915527, loss=1.6859920024871826
I0312 13:17:02.740289 140403528402688 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.23662850260734558, loss=1.7447724342346191
I0312 13:17:38.097899 140403520009984 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.21988284587860107, loss=1.828003168106079
I0312 13:18:13.435543 140403528402688 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.19045937061309814, loss=1.821292757987976
I0312 13:18:48.798479 140403520009984 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.18216995894908905, loss=1.7921888828277588
I0312 13:19:24.133784 140403528402688 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.21221297979354858, loss=1.720529317855835
I0312 13:19:59.481530 140403520009984 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.21219082176685333, loss=1.7265450954437256
I0312 13:20:26.759220 140573192058688 spec.py:321] Evaluating on the training split.
I0312 13:20:29.713940 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 13:23:45.336229 140573192058688 spec.py:333] Evaluating on the validation split.
I0312 13:23:48.006858 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 13:26:22.118596 140573192058688 spec.py:349] Evaluating on the test split.
I0312 13:26:24.794034 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 13:28:33.428013 140573192058688 submission_runner.py:420] Time since start: 33897.82s, 	Step: 57079, 	{'train/accuracy': 0.6603872179985046, 'train/loss': 1.5857213735580444, 'train/bleu': 32.61815973523987, 'validation/accuracy': 0.670741856098175, 'validation/loss': 1.5168012380599976, 'validation/bleu': 29.064819889773123, 'validation/num_examples': 3000, 'test/accuracy': 0.6844227910041809, 'test/loss': 1.4322528839111328, 'test/bleu': 28.452016178586515, 'test/num_examples': 3003, 'score': 20201.32160258293, 'total_duration': 33897.81595611572, 'accumulated_submission_time': 20201.32160258293, 'accumulated_eval_time': 13693.890861272812, 'accumulated_logging_time': 0.7218029499053955}
I0312 13:28:33.450067 140403528402688 logging_writer.py:48] [57079] accumulated_eval_time=13693.890861, accumulated_logging_time=0.721803, accumulated_submission_time=20201.321603, global_step=57079, preemption_count=0, score=20201.321603, test/accuracy=0.684423, test/bleu=28.452016, test/loss=1.432253, test/num_examples=3003, total_duration=33897.815956, train/accuracy=0.660387, train/bleu=32.618160, train/loss=1.585721, validation/accuracy=0.670742, validation/bleu=29.064820, validation/loss=1.516801, validation/num_examples=3000
I0312 13:28:41.250491 140403520009984 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.18912646174430847, loss=1.6368962526321411
I0312 13:29:16.652128 140403528402688 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.24181029200553894, loss=1.7942417860031128
I0312 13:29:51.957893 140403520009984 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.1994142383337021, loss=1.7296441793441772
I0312 13:30:27.276256 140403528402688 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.19968634843826294, loss=1.696609616279602
I0312 13:31:02.606000 140403520009984 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.20595021545886993, loss=1.752711296081543
I0312 13:31:37.923510 140403528402688 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.19476863741874695, loss=1.709721326828003
I0312 13:32:13.306151 140403520009984 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.197891965508461, loss=1.665794849395752
I0312 13:32:48.695137 140403528402688 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.20305410027503967, loss=1.6591707468032837
I0312 13:33:24.057323 140403520009984 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.5978357791900635, loss=1.6930599212646484
I0312 13:33:59.365993 140403528402688 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.19206437468528748, loss=1.6630630493164062
I0312 13:34:34.701549 140403520009984 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.17957349121570587, loss=1.6604745388031006
I0312 13:35:10.042025 140403528402688 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.19661986827850342, loss=1.723059892654419
I0312 13:35:45.355407 140403520009984 logging_writer.py:48] [58300] global_step=58300, grad_norm=0.19476264715194702, loss=1.6363468170166016
I0312 13:36:20.718865 140403528402688 logging_writer.py:48] [58400] global_step=58400, grad_norm=0.1994909644126892, loss=1.6917047500610352
I0312 13:36:56.087579 140403520009984 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.190362349152565, loss=1.654439926147461
I0312 13:37:31.464594 140403528402688 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.21580883860588074, loss=1.7334210872650146
I0312 13:38:06.808130 140403520009984 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.2326381355524063, loss=1.6646060943603516
I0312 13:38:42.121616 140403528402688 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.1941346526145935, loss=1.664278507232666
I0312 13:39:17.489453 140403520009984 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.22824089229106903, loss=1.7812169790267944
I0312 13:39:52.958415 140403528402688 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.21263106167316437, loss=1.6718816757202148
I0312 13:40:28.313465 140403520009984 logging_writer.py:48] [59100] global_step=59100, grad_norm=0.19748729467391968, loss=1.695649266242981
I0312 13:41:03.629011 140403528402688 logging_writer.py:48] [59200] global_step=59200, grad_norm=0.21812450885772705, loss=1.7311372756958008
I0312 13:41:38.977615 140403520009984 logging_writer.py:48] [59300] global_step=59300, grad_norm=0.18174193799495697, loss=1.7119299173355103
I0312 13:42:14.311505 140403528402688 logging_writer.py:48] [59400] global_step=59400, grad_norm=0.21282999217510223, loss=1.6428862810134888
I0312 13:42:33.436700 140573192058688 spec.py:321] Evaluating on the training split.
I0312 13:42:36.387963 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 13:46:19.056149 140573192058688 spec.py:333] Evaluating on the validation split.
I0312 13:46:21.721924 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 13:50:02.107116 140573192058688 spec.py:349] Evaluating on the test split.
I0312 13:50:04.795698 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 13:54:03.829050 140573192058688 submission_runner.py:420] Time since start: 35428.22s, 	Step: 59456, 	{'train/accuracy': 0.6552941799163818, 'train/loss': 1.624788522720337, 'train/bleu': 32.59673470601075, 'validation/accuracy': 0.6723288893699646, 'validation/loss': 1.5095142126083374, 'validation/bleu': 29.290384592689776, 'validation/num_examples': 3000, 'test/accuracy': 0.6850734949111938, 'test/loss': 1.4224475622177124, 'test/bleu': 28.683178688512474, 'test/num_examples': 3003, 'score': 21041.213926553726, 'total_duration': 35428.21695756912, 'accumulated_submission_time': 21041.213926553726, 'accumulated_eval_time': 14384.28313088417, 'accumulated_logging_time': 0.7561678886413574}
I0312 13:54:03.855564 140403520009984 logging_writer.py:48] [59456] accumulated_eval_time=14384.283131, accumulated_logging_time=0.756168, accumulated_submission_time=21041.213927, global_step=59456, preemption_count=0, score=21041.213927, test/accuracy=0.685073, test/bleu=28.683179, test/loss=1.422448, test/num_examples=3003, total_duration=35428.216958, train/accuracy=0.655294, train/bleu=32.596735, train/loss=1.624789, validation/accuracy=0.672329, validation/bleu=29.290385, validation/loss=1.509514, validation/num_examples=3000
I0312 13:54:19.762865 140403528402688 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.20650367438793182, loss=1.6761969327926636
I0312 13:54:55.119705 140403520009984 logging_writer.py:48] [59600] global_step=59600, grad_norm=0.2269359827041626, loss=1.7564303874969482
I0312 13:55:30.424028 140403528402688 logging_writer.py:48] [59700] global_step=59700, grad_norm=0.24719133973121643, loss=1.699230670928955
I0312 13:56:05.739981 140403520009984 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.1902487874031067, loss=1.6892588138580322
I0312 13:56:41.054198 140403528402688 logging_writer.py:48] [59900] global_step=59900, grad_norm=0.24611663818359375, loss=1.6582735776901245
I0312 13:57:16.394138 140403520009984 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.21066386997699738, loss=1.7346209287643433
I0312 13:57:51.691178 140403528402688 logging_writer.py:48] [60100] global_step=60100, grad_norm=0.1899716705083847, loss=1.7613309621810913
I0312 13:58:26.992568 140403520009984 logging_writer.py:48] [60200] global_step=60200, grad_norm=0.2118827849626541, loss=1.7997323274612427
I0312 13:59:02.294569 140403528402688 logging_writer.py:48] [60300] global_step=60300, grad_norm=0.21966823935508728, loss=1.6790578365325928
I0312 13:59:37.644845 140403520009984 logging_writer.py:48] [60400] global_step=60400, grad_norm=0.2099715769290924, loss=1.7289024591445923
I0312 14:00:12.993827 140403528402688 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.21178825199604034, loss=1.6831389665603638
I0312 14:00:48.314556 140403520009984 logging_writer.py:48] [60600] global_step=60600, grad_norm=0.20468513667583466, loss=1.7123873233795166
I0312 14:01:23.648960 140403528402688 logging_writer.py:48] [60700] global_step=60700, grad_norm=0.20682121813297272, loss=1.7418867349624634
I0312 14:01:58.975542 140403520009984 logging_writer.py:48] [60800] global_step=60800, grad_norm=0.2209678739309311, loss=1.6803950071334839
I0312 14:02:34.313700 140403528402688 logging_writer.py:48] [60900] global_step=60900, grad_norm=0.192889004945755, loss=1.7670584917068481
I0312 14:03:09.624321 140403520009984 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.19079768657684326, loss=1.745755910873413
I0312 14:03:44.961381 140403528402688 logging_writer.py:48] [61100] global_step=61100, grad_norm=0.22585739195346832, loss=1.7552016973495483
I0312 14:04:20.317979 140403520009984 logging_writer.py:48] [61200] global_step=61200, grad_norm=0.18135042488574982, loss=1.707433819770813
I0312 14:04:55.644615 140403528402688 logging_writer.py:48] [61300] global_step=61300, grad_norm=0.2096501886844635, loss=1.693042516708374
I0312 14:05:30.975934 140403520009984 logging_writer.py:48] [61400] global_step=61400, grad_norm=0.20418846607208252, loss=1.6801592111587524
I0312 14:06:06.293207 140403528402688 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.20694924890995026, loss=1.7465977668762207
I0312 14:06:41.665199 140403520009984 logging_writer.py:48] [61600] global_step=61600, grad_norm=0.18966375291347504, loss=1.7721562385559082
I0312 14:07:17.008227 140403528402688 logging_writer.py:48] [61700] global_step=61700, grad_norm=0.20831350982189178, loss=1.7421202659606934
I0312 14:07:52.334882 140403520009984 logging_writer.py:48] [61800] global_step=61800, grad_norm=0.19704517722129822, loss=1.6731171607971191
I0312 14:08:04.075798 140573192058688 spec.py:321] Evaluating on the training split.
I0312 14:08:07.037442 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 14:12:02.458093 140573192058688 spec.py:333] Evaluating on the validation split.
I0312 14:12:05.118912 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 14:14:40.916720 140573192058688 spec.py:349] Evaluating on the test split.
I0312 14:14:43.590252 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 14:17:07.315189 140573192058688 submission_runner.py:420] Time since start: 36811.70s, 	Step: 61835, 	{'train/accuracy': 0.6523348689079285, 'train/loss': 1.6461620330810547, 'train/bleu': 32.40031350898231, 'validation/accuracy': 0.6736432313919067, 'validation/loss': 1.504443645477295, 'validation/bleu': 29.267761153599153, 'validation/num_examples': 3000, 'test/accuracy': 0.6883040070533752, 'test/loss': 1.4157791137695312, 'test/bleu': 28.97549138929193, 'test/num_examples': 3003, 'score': 21881.345286130905, 'total_duration': 36811.70310878754, 'accumulated_submission_time': 21881.345286130905, 'accumulated_eval_time': 14927.522450685501, 'accumulated_logging_time': 0.7940044403076172}
I0312 14:17:07.337944 140403528402688 logging_writer.py:48] [61835] accumulated_eval_time=14927.522451, accumulated_logging_time=0.794004, accumulated_submission_time=21881.345286, global_step=61835, preemption_count=0, score=21881.345286, test/accuracy=0.688304, test/bleu=28.975491, test/loss=1.415779, test/num_examples=3003, total_duration=36811.703109, train/accuracy=0.652335, train/bleu=32.400314, train/loss=1.646162, validation/accuracy=0.673643, validation/bleu=29.267761, validation/loss=1.504444, validation/num_examples=3000
I0312 14:17:30.693480 140403520009984 logging_writer.py:48] [61900] global_step=61900, grad_norm=0.19340232014656067, loss=1.6823426485061646
I0312 14:18:06.027070 140403528402688 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.1839563399553299, loss=1.7621190547943115
I0312 14:18:41.342631 140403520009984 logging_writer.py:48] [62100] global_step=62100, grad_norm=0.18801064789295197, loss=1.6696562767028809
I0312 14:19:16.672070 140403528402688 logging_writer.py:48] [62200] global_step=62200, grad_norm=0.18262514472007751, loss=1.739542841911316
I0312 14:19:51.964931 140403520009984 logging_writer.py:48] [62300] global_step=62300, grad_norm=0.18540817499160767, loss=1.6239135265350342
I0312 14:20:27.291763 140403528402688 logging_writer.py:48] [62400] global_step=62400, grad_norm=0.21085099875926971, loss=1.6589640378952026
I0312 14:21:02.592266 140403520009984 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.19961397349834442, loss=1.7391694784164429
I0312 14:21:37.900436 140403528402688 logging_writer.py:48] [62600] global_step=62600, grad_norm=0.21082662045955658, loss=1.6926385164260864
I0312 14:22:13.264338 140403520009984 logging_writer.py:48] [62700] global_step=62700, grad_norm=0.19178566336631775, loss=1.7657485008239746
I0312 14:22:48.599253 140403528402688 logging_writer.py:48] [62800] global_step=62800, grad_norm=0.20428931713104248, loss=1.7602105140686035
I0312 14:23:23.930109 140403520009984 logging_writer.py:48] [62900] global_step=62900, grad_norm=0.19589237868785858, loss=1.7878063917160034
I0312 14:23:59.250044 140403528402688 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.19196948409080505, loss=1.7164243459701538
I0312 14:24:34.582581 140403520009984 logging_writer.py:48] [63100] global_step=63100, grad_norm=0.2185981720685959, loss=1.7130217552185059
I0312 14:25:09.907126 140403528402688 logging_writer.py:48] [63200] global_step=63200, grad_norm=0.19033043086528778, loss=1.7049421072006226
I0312 14:25:45.254199 140403520009984 logging_writer.py:48] [63300] global_step=63300, grad_norm=0.19769304990768433, loss=1.6764140129089355
I0312 14:26:20.615546 140403528402688 logging_writer.py:48] [63400] global_step=63400, grad_norm=0.18305838108062744, loss=1.700717806816101
I0312 14:26:55.929236 140403520009984 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.19790217280387878, loss=1.7253600358963013
I0312 14:27:31.248530 140403528402688 logging_writer.py:48] [63600] global_step=63600, grad_norm=0.2195960134267807, loss=1.750372290611267
I0312 14:28:06.591421 140403520009984 logging_writer.py:48] [63700] global_step=63700, grad_norm=0.19173920154571533, loss=1.6828601360321045
I0312 14:28:41.926891 140403528402688 logging_writer.py:48] [63800] global_step=63800, grad_norm=0.20296929776668549, loss=1.7415035963058472
I0312 14:29:17.238754 140403520009984 logging_writer.py:48] [63900] global_step=63900, grad_norm=0.2006017565727234, loss=1.6241788864135742
I0312 14:29:52.562584 140403528402688 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.23852665722370148, loss=1.6198902130126953
I0312 14:30:27.897694 140403520009984 logging_writer.py:48] [64100] global_step=64100, grad_norm=0.2084973007440567, loss=1.6883656978607178
I0312 14:31:03.237878 140403528402688 logging_writer.py:48] [64200] global_step=64200, grad_norm=0.22815737128257751, loss=1.7537785768508911
I0312 14:31:07.559168 140573192058688 spec.py:321] Evaluating on the training split.
I0312 14:31:10.517704 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 14:34:48.940660 140573192058688 spec.py:333] Evaluating on the validation split.
I0312 14:34:51.620412 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 14:37:36.035124 140573192058688 spec.py:349] Evaluating on the test split.
I0312 14:37:38.714226 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 14:40:04.581509 140573192058688 submission_runner.py:420] Time since start: 38188.97s, 	Step: 64214, 	{'train/accuracy': 0.6573935151100159, 'train/loss': 1.6122422218322754, 'train/bleu': 32.14257821076103, 'validation/accuracy': 0.6687331795692444, 'validation/loss': 1.5286437273025513, 'validation/bleu': 28.582508268054738, 'validation/num_examples': 3000, 'test/accuracy': 0.6827958822250366, 'test/loss': 1.4405723810195923, 'test/bleu': 28.18834542843649, 'test/num_examples': 3003, 'score': 22721.477643489838, 'total_duration': 38188.96942901611, 'accumulated_submission_time': 22721.477643489838, 'accumulated_eval_time': 15464.544717550278, 'accumulated_logging_time': 0.8278894424438477}
I0312 14:40:04.605182 140403520009984 logging_writer.py:48] [64214] accumulated_eval_time=15464.544718, accumulated_logging_time=0.827889, accumulated_submission_time=22721.477643, global_step=64214, preemption_count=0, score=22721.477643, test/accuracy=0.682796, test/bleu=28.188345, test/loss=1.440572, test/num_examples=3003, total_duration=38188.969429, train/accuracy=0.657394, train/bleu=32.142578, train/loss=1.612242, validation/accuracy=0.668733, validation/bleu=28.582508, validation/loss=1.528644, validation/num_examples=3000
I0312 14:40:35.361050 140403528402688 logging_writer.py:48] [64300] global_step=64300, grad_norm=0.19503837823867798, loss=1.7383651733398438
I0312 14:41:10.695748 140403520009984 logging_writer.py:48] [64400] global_step=64400, grad_norm=0.19225168228149414, loss=1.7739932537078857
I0312 14:41:46.028060 140403528402688 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.19753001630306244, loss=1.7416703701019287
I0312 14:42:21.387814 140403520009984 logging_writer.py:48] [64600] global_step=64600, grad_norm=0.19920001924037933, loss=1.763354778289795
I0312 14:42:56.716896 140403528402688 logging_writer.py:48] [64700] global_step=64700, grad_norm=0.19379423558712006, loss=1.7118247747421265
I0312 14:43:32.046629 140403520009984 logging_writer.py:48] [64800] global_step=64800, grad_norm=0.1938813030719757, loss=1.7621607780456543
I0312 14:44:07.392304 140403528402688 logging_writer.py:48] [64900] global_step=64900, grad_norm=0.1921803206205368, loss=1.6363223791122437
I0312 14:44:42.734208 140403520009984 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.19357889890670776, loss=1.6656635999679565
I0312 14:45:18.093935 140403528402688 logging_writer.py:48] [65100] global_step=65100, grad_norm=0.2211606502532959, loss=1.6565563678741455
I0312 14:45:53.429067 140403520009984 logging_writer.py:48] [65200] global_step=65200, grad_norm=0.19459261000156403, loss=1.7191401720046997
I0312 14:46:28.745086 140403528402688 logging_writer.py:48] [65300] global_step=65300, grad_norm=0.19403661787509918, loss=1.6567026376724243
I0312 14:47:04.046811 140403520009984 logging_writer.py:48] [65400] global_step=65400, grad_norm=0.18893814086914062, loss=1.6886461973190308
I0312 14:47:39.372896 140403528402688 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.21162059903144836, loss=1.6275233030319214
I0312 14:48:14.711887 140403520009984 logging_writer.py:48] [65600] global_step=65600, grad_norm=0.26599445939064026, loss=1.6740665435791016
I0312 14:48:50.044480 140403528402688 logging_writer.py:48] [65700] global_step=65700, grad_norm=0.1842222660779953, loss=1.600447177886963
I0312 14:49:25.373370 140403520009984 logging_writer.py:48] [65800] global_step=65800, grad_norm=0.20086835324764252, loss=1.7187334299087524
I0312 14:50:00.804012 140403528402688 logging_writer.py:48] [65900] global_step=65900, grad_norm=0.18229015171527863, loss=1.6563892364501953
I0312 14:50:36.169881 140403520009984 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.18433880805969238, loss=1.721588134765625
I0312 14:51:11.556788 140403528402688 logging_writer.py:48] [66100] global_step=66100, grad_norm=0.19410845637321472, loss=1.703324317932129
I0312 14:51:46.982453 140403520009984 logging_writer.py:48] [66200] global_step=66200, grad_norm=0.20380425453186035, loss=1.633711814880371
I0312 14:52:22.337442 140403528402688 logging_writer.py:48] [66300] global_step=66300, grad_norm=0.2091442197561264, loss=1.6744133234024048
I0312 14:52:57.687087 140403520009984 logging_writer.py:48] [66400] global_step=66400, grad_norm=0.19504252076148987, loss=1.6497560739517212
I0312 14:53:32.996650 140403528402688 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.20709656178951263, loss=1.6531987190246582
I0312 14:54:04.882256 140573192058688 spec.py:321] Evaluating on the training split.
I0312 14:54:07.855341 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 14:58:19.255056 140573192058688 spec.py:333] Evaluating on the validation split.
I0312 14:58:21.930778 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 15:01:38.764233 140573192058688 spec.py:349] Evaluating on the test split.
I0312 15:01:41.428323 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 15:04:47.498713 140573192058688 submission_runner.py:420] Time since start: 39671.89s, 	Step: 66592, 	{'train/accuracy': 0.6559733152389526, 'train/loss': 1.6213107109069824, 'train/bleu': 32.54262723353351, 'validation/accuracy': 0.6742507815361023, 'validation/loss': 1.4933558702468872, 'validation/bleu': 29.09013429115508, 'validation/num_examples': 3000, 'test/accuracy': 0.6875951886177063, 'test/loss': 1.4001637697219849, 'test/bleu': 28.568736458659583, 'test/num_examples': 3003, 'score': 23561.66490626335, 'total_duration': 39671.88665890694, 'accumulated_submission_time': 23561.66490626335, 'accumulated_eval_time': 16107.16114974022, 'accumulated_logging_time': 0.8622546195983887}
I0312 15:04:47.522166 140403520009984 logging_writer.py:48] [66592] accumulated_eval_time=16107.161150, accumulated_logging_time=0.862255, accumulated_submission_time=23561.664906, global_step=66592, preemption_count=0, score=23561.664906, test/accuracy=0.687595, test/bleu=28.568736, test/loss=1.400164, test/num_examples=3003, total_duration=39671.886659, train/accuracy=0.655973, train/bleu=32.542627, train/loss=1.621311, validation/accuracy=0.674251, validation/bleu=29.090134, validation/loss=1.493356, validation/num_examples=3000
I0312 15:04:50.717664 140403528402688 logging_writer.py:48] [66600] global_step=66600, grad_norm=0.1880260705947876, loss=1.6693339347839355
I0312 15:05:26.041401 140403520009984 logging_writer.py:48] [66700] global_step=66700, grad_norm=0.19820842146873474, loss=1.753900408744812
I0312 15:06:01.366039 140403528402688 logging_writer.py:48] [66800] global_step=66800, grad_norm=0.21489030122756958, loss=1.6763361692428589
I0312 15:06:36.810993 140403520009984 logging_writer.py:48] [66900] global_step=66900, grad_norm=0.21005745232105255, loss=1.6316547393798828
I0312 15:07:12.189715 140403528402688 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.19732338190078735, loss=1.729620337486267
I0312 15:07:47.526401 140403520009984 logging_writer.py:48] [67100] global_step=67100, grad_norm=0.1950172781944275, loss=1.6691361665725708
I0312 15:08:22.852589 140403528402688 logging_writer.py:48] [67200] global_step=67200, grad_norm=0.21991479396820068, loss=1.6699769496917725
I0312 15:08:58.180098 140403520009984 logging_writer.py:48] [67300] global_step=67300, grad_norm=0.19323308765888214, loss=1.6927779912948608
I0312 15:09:33.482973 140403528402688 logging_writer.py:48] [67400] global_step=67400, grad_norm=0.21224577724933624, loss=1.6381878852844238
I0312 15:10:08.804080 140403520009984 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.20779481530189514, loss=1.615007758140564
I0312 15:10:44.192546 140403528402688 logging_writer.py:48] [67600] global_step=67600, grad_norm=0.20997709035873413, loss=1.623848795890808
I0312 15:11:19.547267 140403520009984 logging_writer.py:48] [67700] global_step=67700, grad_norm=0.1980392187833786, loss=1.6966187953948975
I0312 15:11:54.872005 140403528402688 logging_writer.py:48] [67800] global_step=67800, grad_norm=0.1946392059326172, loss=1.704630732536316
I0312 15:12:30.193235 140403520009984 logging_writer.py:48] [67900] global_step=67900, grad_norm=0.19431202113628387, loss=1.675606369972229
I0312 15:13:05.536879 140403528402688 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.1872880458831787, loss=1.6181540489196777
I0312 15:13:40.910408 140403520009984 logging_writer.py:48] [68100] global_step=68100, grad_norm=0.2005096822977066, loss=1.6344095468521118
I0312 15:14:16.222007 140403528402688 logging_writer.py:48] [68200] global_step=68200, grad_norm=0.20174755156040192, loss=1.6262000799179077
I0312 15:14:51.508469 140403520009984 logging_writer.py:48] [68300] global_step=68300, grad_norm=0.24752263724803925, loss=1.7284395694732666
I0312 15:15:26.839210 140403528402688 logging_writer.py:48] [68400] global_step=68400, grad_norm=0.2275557965040207, loss=1.6437846422195435
I0312 15:16:02.158504 140403520009984 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.20700283348560333, loss=1.6906236410140991
I0312 15:16:37.529814 140403528402688 logging_writer.py:48] [68600] global_step=68600, grad_norm=0.20346452295780182, loss=1.6627998352050781
I0312 15:17:12.906819 140403520009984 logging_writer.py:48] [68700] global_step=68700, grad_norm=0.21645988523960114, loss=1.6762936115264893
I0312 15:17:48.271809 140403528402688 logging_writer.py:48] [68800] global_step=68800, grad_norm=0.20660334825515747, loss=1.702451229095459
I0312 15:18:23.602422 140403520009984 logging_writer.py:48] [68900] global_step=68900, grad_norm=0.1894027441740036, loss=1.7436559200286865
I0312 15:18:47.690239 140573192058688 spec.py:321] Evaluating on the training split.
I0312 15:18:50.647442 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 15:22:28.876998 140573192058688 spec.py:333] Evaluating on the validation split.
I0312 15:22:31.549139 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 15:25:01.025963 140573192058688 spec.py:349] Evaluating on the test split.
I0312 15:25:03.713188 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 15:27:17.569435 140573192058688 submission_runner.py:420] Time since start: 41021.96s, 	Step: 68970, 	{'train/accuracy': 0.6814596652984619, 'train/loss': 1.4550007581710815, 'train/bleu': 34.3580861992117, 'validation/accuracy': 0.6773753762245178, 'validation/loss': 1.4872820377349854, 'validation/bleu': 29.381252753878382, 'validation/num_examples': 3000, 'test/accuracy': 0.6907442808151245, 'test/loss': 1.3908847570419312, 'test/bleu': 29.05332517746374, 'test/num_examples': 3003, 'score': 24401.740617513657, 'total_duration': 41021.95736527443, 'accumulated_submission_time': 24401.740617513657, 'accumulated_eval_time': 16617.040287017822, 'accumulated_logging_time': 0.8978571891784668}
I0312 15:27:17.593548 140403528402688 logging_writer.py:48] [68970] accumulated_eval_time=16617.040287, accumulated_logging_time=0.897857, accumulated_submission_time=24401.740618, global_step=68970, preemption_count=0, score=24401.740618, test/accuracy=0.690744, test/bleu=29.053325, test/loss=1.390885, test/num_examples=3003, total_duration=41021.957365, train/accuracy=0.681460, train/bleu=34.358086, train/loss=1.455001, validation/accuracy=0.677375, validation/bleu=29.381253, validation/loss=1.487282, validation/num_examples=3000
I0312 15:27:28.576052 140403520009984 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.2117900550365448, loss=1.704504370689392
I0312 15:28:03.969275 140403528402688 logging_writer.py:48] [69100] global_step=69100, grad_norm=0.20647095143795013, loss=1.730370283126831
I0312 15:28:39.326941 140403520009984 logging_writer.py:48] [69200] global_step=69200, grad_norm=0.20914189517498016, loss=1.66783607006073
I0312 15:29:14.663112 140403528402688 logging_writer.py:48] [69300] global_step=69300, grad_norm=0.21000726521015167, loss=1.5948225259780884
I0312 15:29:50.007019 140403520009984 logging_writer.py:48] [69400] global_step=69400, grad_norm=0.19769522547721863, loss=1.6419556140899658
I0312 15:30:25.333854 140403528402688 logging_writer.py:48] [69500] global_step=69500, grad_norm=0.19674985110759735, loss=1.7012900114059448
I0312 15:31:00.644333 140403520009984 logging_writer.py:48] [69600] global_step=69600, grad_norm=0.2081180363893509, loss=1.6532255411148071
I0312 15:31:35.972955 140403528402688 logging_writer.py:48] [69700] global_step=69700, grad_norm=0.19732847809791565, loss=1.6289920806884766
I0312 15:32:11.334319 140403520009984 logging_writer.py:48] [69800] global_step=69800, grad_norm=0.18801631033420563, loss=1.6749213933944702
I0312 15:32:46.637055 140403528402688 logging_writer.py:48] [69900] global_step=69900, grad_norm=0.19289354979991913, loss=1.6651510000228882
I0312 15:33:21.998046 140403520009984 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.20476195216178894, loss=1.6423290967941284
I0312 15:33:57.303429 140403528402688 logging_writer.py:48] [70100] global_step=70100, grad_norm=0.2025183141231537, loss=1.6989582777023315
I0312 15:34:32.666375 140403520009984 logging_writer.py:48] [70200] global_step=70200, grad_norm=0.23271788656711578, loss=1.6425632238388062
I0312 15:35:08.048494 140403528402688 logging_writer.py:48] [70300] global_step=70300, grad_norm=0.20975829660892487, loss=1.638488531112671
I0312 15:35:43.360878 140403520009984 logging_writer.py:48] [70400] global_step=70400, grad_norm=0.2073977291584015, loss=1.7051714658737183
I0312 15:36:18.740856 140403528402688 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.18444709479808807, loss=1.577905535697937
I0312 15:36:54.101603 140403520009984 logging_writer.py:48] [70600] global_step=70600, grad_norm=0.1917889565229416, loss=1.7060272693634033
I0312 15:37:29.419419 140403528402688 logging_writer.py:48] [70700] global_step=70700, grad_norm=0.21192562580108643, loss=1.6770645380020142
I0312 15:38:04.739749 140403520009984 logging_writer.py:48] [70800] global_step=70800, grad_norm=0.18986773490905762, loss=1.656380295753479
I0312 15:38:40.075913 140403528402688 logging_writer.py:48] [70900] global_step=70900, grad_norm=0.19448944926261902, loss=1.6036362648010254
I0312 15:39:15.378781 140403520009984 logging_writer.py:48] [71000] global_step=71000, grad_norm=0.19447796046733856, loss=1.7170552015304565
I0312 15:39:50.712560 140403528402688 logging_writer.py:48] [71100] global_step=71100, grad_norm=0.2066781222820282, loss=1.671905755996704
I0312 15:40:26.028048 140403520009984 logging_writer.py:48] [71200] global_step=71200, grad_norm=0.22445440292358398, loss=1.6705756187438965
I0312 15:41:01.330588 140403528402688 logging_writer.py:48] [71300] global_step=71300, grad_norm=0.1973961442708969, loss=1.7425177097320557
I0312 15:41:17.666012 140573192058688 spec.py:321] Evaluating on the training split.
I0312 15:41:20.634970 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 15:46:02.269539 140573192058688 spec.py:333] Evaluating on the validation split.
I0312 15:46:04.933301 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 15:50:40.918877 140573192058688 spec.py:349] Evaluating on the test split.
I0312 15:50:43.593354 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 15:55:24.794866 140573192058688 submission_runner.py:420] Time since start: 42709.18s, 	Step: 71348, 	{'train/accuracy': 0.663874626159668, 'train/loss': 1.576033115386963, 'train/bleu': 32.56455290267204, 'validation/accuracy': 0.6783424615859985, 'validation/loss': 1.4791110754013062, 'validation/bleu': 29.55113498636966, 'validation/num_examples': 3000, 'test/accuracy': 0.6910116076469421, 'test/loss': 1.389503002166748, 'test/bleu': 29.21504352470452, 'test/num_examples': 3003, 'score': 25241.72496700287, 'total_duration': 42709.18280673027, 'accumulated_submission_time': 25241.72496700287, 'accumulated_eval_time': 17464.169107675552, 'accumulated_logging_time': 0.9319391250610352}
I0312 15:55:24.819766 140403520009984 logging_writer.py:48] [71348] accumulated_eval_time=17464.169108, accumulated_logging_time=0.931939, accumulated_submission_time=25241.724967, global_step=71348, preemption_count=0, score=25241.724967, test/accuracy=0.691012, test/bleu=29.215044, test/loss=1.389503, test/num_examples=3003, total_duration=42709.182807, train/accuracy=0.663875, train/bleu=32.564553, train/loss=1.576033, validation/accuracy=0.678342, validation/bleu=29.551135, validation/loss=1.479111, validation/num_examples=3000
I0312 15:55:43.528012 140403528402688 logging_writer.py:48] [71400] global_step=71400, grad_norm=0.2040732055902481, loss=1.6649386882781982
I0312 15:56:18.824203 140403520009984 logging_writer.py:48] [71500] global_step=71500, grad_norm=0.195962056517601, loss=1.6238594055175781
I0312 15:56:54.145300 140403528402688 logging_writer.py:48] [71600] global_step=71600, grad_norm=0.19215916097164154, loss=1.6471368074417114
I0312 15:57:29.466775 140403520009984 logging_writer.py:48] [71700] global_step=71700, grad_norm=0.20846658945083618, loss=1.6536474227905273
I0312 15:58:04.786136 140403528402688 logging_writer.py:48] [71800] global_step=71800, grad_norm=0.19848036766052246, loss=1.6572035551071167
I0312 15:58:40.127334 140403520009984 logging_writer.py:48] [71900] global_step=71900, grad_norm=0.21131248772144318, loss=1.6851518154144287
I0312 15:59:15.427637 140403528402688 logging_writer.py:48] [72000] global_step=72000, grad_norm=0.20471645891666412, loss=1.6490309238433838
I0312 15:59:50.740649 140403520009984 logging_writer.py:48] [72100] global_step=72100, grad_norm=0.20111039280891418, loss=1.6790931224822998
I0312 16:00:26.061659 140403528402688 logging_writer.py:48] [72200] global_step=72200, grad_norm=0.215345561504364, loss=1.747448444366455
I0312 16:01:01.360964 140403520009984 logging_writer.py:48] [72300] global_step=72300, grad_norm=0.20216628909111023, loss=1.6062281131744385
I0312 16:01:36.742095 140403528402688 logging_writer.py:48] [72400] global_step=72400, grad_norm=0.1940414011478424, loss=1.6609158515930176
I0312 16:02:12.070976 140403520009984 logging_writer.py:48] [72500] global_step=72500, grad_norm=0.21180349588394165, loss=1.7096103429794312
I0312 16:02:47.401531 140403528402688 logging_writer.py:48] [72600] global_step=72600, grad_norm=0.19331106543540955, loss=1.6208170652389526
I0312 16:03:22.721691 140403520009984 logging_writer.py:48] [72700] global_step=72700, grad_norm=0.23294180631637573, loss=1.6428102254867554
I0312 16:03:58.028600 140403528402688 logging_writer.py:48] [72800] global_step=72800, grad_norm=0.19723036885261536, loss=1.7081714868545532
I0312 16:04:33.376189 140403520009984 logging_writer.py:48] [72900] global_step=72900, grad_norm=0.19948282837867737, loss=1.6381537914276123
I0312 16:05:08.705158 140403528402688 logging_writer.py:48] [73000] global_step=73000, grad_norm=0.20591279864311218, loss=1.7178324460983276
I0312 16:05:44.073153 140403520009984 logging_writer.py:48] [73100] global_step=73100, grad_norm=0.21086466312408447, loss=1.6210932731628418
I0312 16:06:19.412212 140403528402688 logging_writer.py:48] [73200] global_step=73200, grad_norm=0.20636513829231262, loss=1.6059107780456543
I0312 16:06:54.753743 140403520009984 logging_writer.py:48] [73300] global_step=73300, grad_norm=0.2211243361234665, loss=1.6071451902389526
I0312 16:07:30.066719 140403528402688 logging_writer.py:48] [73400] global_step=73400, grad_norm=0.2226460576057434, loss=1.6324862241744995
I0312 16:08:05.398020 140403520009984 logging_writer.py:48] [73500] global_step=73500, grad_norm=0.18681207299232483, loss=1.6235605478286743
I0312 16:08:40.712582 140403528402688 logging_writer.py:48] [73600] global_step=73600, grad_norm=0.2122819572687149, loss=1.5928184986114502
I0312 16:09:16.031570 140403520009984 logging_writer.py:48] [73700] global_step=73700, grad_norm=0.20719124376773834, loss=1.6252989768981934
I0312 16:09:24.945670 140573192058688 spec.py:321] Evaluating on the training split.
I0312 16:09:27.898737 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 16:13:51.902204 140573192058688 spec.py:333] Evaluating on the validation split.
I0312 16:13:54.566058 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 16:16:37.194202 140573192058688 spec.py:349] Evaluating on the test split.
I0312 16:16:39.868642 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 16:18:58.200384 140573192058688 submission_runner.py:420] Time since start: 44122.59s, 	Step: 73727, 	{'train/accuracy': 0.6613771915435791, 'train/loss': 1.5929934978485107, 'train/bleu': 32.78422420721934, 'validation/accuracy': 0.6800411343574524, 'validation/loss': 1.4665690660476685, 'validation/bleu': 29.65939306819033, 'validation/num_examples': 3000, 'test/accuracy': 0.6914764046669006, 'test/loss': 1.378501296043396, 'test/bleu': 29.265794145234867, 'test/num_examples': 3003, 'score': 26081.76236653328, 'total_duration': 44122.5883204937, 'accumulated_submission_time': 26081.76236653328, 'accumulated_eval_time': 18037.423763990402, 'accumulated_logging_time': 0.9692091941833496}
I0312 16:18:58.225068 140403528402688 logging_writer.py:48] [73727] accumulated_eval_time=18037.423764, accumulated_logging_time=0.969209, accumulated_submission_time=26081.762367, global_step=73727, preemption_count=0, score=26081.762367, test/accuracy=0.691476, test/bleu=29.265794, test/loss=1.378501, test/num_examples=3003, total_duration=44122.588320, train/accuracy=0.661377, train/bleu=32.784224, train/loss=1.592993, validation/accuracy=0.680041, validation/bleu=29.659393, validation/loss=1.466569, validation/num_examples=3000
I0312 16:19:24.349210 140403520009984 logging_writer.py:48] [73800] global_step=73800, grad_norm=0.1944001466035843, loss=1.6312739849090576
I0312 16:19:59.655749 140403528402688 logging_writer.py:48] [73900] global_step=73900, grad_norm=0.21544833481311798, loss=1.6291204690933228
I0312 16:20:34.957728 140403520009984 logging_writer.py:48] [74000] global_step=74000, grad_norm=0.20187391340732574, loss=1.5969562530517578
I0312 16:21:10.310375 140403528402688 logging_writer.py:48] [74100] global_step=74100, grad_norm=0.20689213275909424, loss=1.6109267473220825
I0312 16:21:45.656192 140403520009984 logging_writer.py:48] [74200] global_step=74200, grad_norm=0.193165585398674, loss=1.63565194606781
I0312 16:22:20.998972 140403528402688 logging_writer.py:48] [74300] global_step=74300, grad_norm=0.20308689773082733, loss=1.673092246055603
I0312 16:22:56.319517 140403520009984 logging_writer.py:48] [74400] global_step=74400, grad_norm=0.20241700112819672, loss=1.6351758241653442
I0312 16:23:31.618297 140403528402688 logging_writer.py:48] [74500] global_step=74500, grad_norm=0.20678581297397614, loss=1.795690655708313
I0312 16:24:06.968539 140403520009984 logging_writer.py:48] [74600] global_step=74600, grad_norm=0.21778368949890137, loss=1.6162196397781372
I0312 16:24:42.286482 140403528402688 logging_writer.py:48] [74700] global_step=74700, grad_norm=0.22538726031780243, loss=1.664953589439392
I0312 16:25:17.606698 140403520009984 logging_writer.py:48] [74800] global_step=74800, grad_norm=0.20463821291923523, loss=1.6582790613174438
I0312 16:25:52.914970 140403528402688 logging_writer.py:48] [74900] global_step=74900, grad_norm=0.1887558251619339, loss=1.6042039394378662
I0312 16:26:28.258726 140403520009984 logging_writer.py:48] [75000] global_step=75000, grad_norm=0.19103756546974182, loss=1.6514965295791626
I0312 16:27:03.568237 140403528402688 logging_writer.py:48] [75100] global_step=75100, grad_norm=0.22238881886005402, loss=1.7350008487701416
I0312 16:27:38.880047 140403520009984 logging_writer.py:48] [75200] global_step=75200, grad_norm=0.20774126052856445, loss=1.6923877000808716
I0312 16:28:14.208627 140403528402688 logging_writer.py:48] [75300] global_step=75300, grad_norm=0.21126846969127655, loss=1.672503113746643
I0312 16:28:49.517392 140403520009984 logging_writer.py:48] [75400] global_step=75400, grad_norm=0.2053699791431427, loss=1.5821503400802612
I0312 16:29:24.852362 140403528402688 logging_writer.py:48] [75500] global_step=75500, grad_norm=0.18542300164699554, loss=1.6354038715362549
I0312 16:30:00.177109 140403520009984 logging_writer.py:48] [75600] global_step=75600, grad_norm=0.20203065872192383, loss=1.6328312158584595
I0312 16:30:35.496631 140403528402688 logging_writer.py:48] [75700] global_step=75700, grad_norm=0.1981518566608429, loss=1.7391053438186646
I0312 16:31:10.791076 140403520009984 logging_writer.py:48] [75800] global_step=75800, grad_norm=0.2125670164823532, loss=1.7089351415634155
I0312 16:31:46.106470 140403528402688 logging_writer.py:48] [75900] global_step=75900, grad_norm=0.21686944365501404, loss=1.6055090427398682
I0312 16:32:21.467349 140403520009984 logging_writer.py:48] [76000] global_step=76000, grad_norm=0.20643506944179535, loss=1.776330590248108
I0312 16:32:56.825607 140403528402688 logging_writer.py:48] [76100] global_step=76100, grad_norm=0.20497553050518036, loss=1.6236966848373413
I0312 16:32:58.319124 140573192058688 spec.py:321] Evaluating on the training split.
I0312 16:33:01.285274 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 16:37:17.150027 140573192058688 spec.py:333] Evaluating on the validation split.
I0312 16:37:19.831035 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 16:40:11.701258 140573192058688 spec.py:349] Evaluating on the test split.
I0312 16:40:14.372224 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 16:42:52.534497 140573192058688 submission_runner.py:420] Time since start: 45556.92s, 	Step: 76106, 	{'train/accuracy': 0.6700928211212158, 'train/loss': 1.5198198556900024, 'train/bleu': 33.644985715093746, 'validation/accuracy': 0.6783796548843384, 'validation/loss': 1.4667606353759766, 'validation/bleu': 29.574116723083673, 'validation/num_examples': 3000, 'test/accuracy': 0.6944977045059204, 'test/loss': 1.370276689529419, 'test/bleu': 29.374789377540463, 'test/num_examples': 3003, 'score': 26921.769094228745, 'total_duration': 45556.92244243622, 'accumulated_submission_time': 26921.769094228745, 'accumulated_eval_time': 18631.639093637466, 'accumulated_logging_time': 1.0040538311004639}
I0312 16:42:52.559707 140403520009984 logging_writer.py:48] [76106] accumulated_eval_time=18631.639094, accumulated_logging_time=1.004054, accumulated_submission_time=26921.769094, global_step=76106, preemption_count=0, score=26921.769094, test/accuracy=0.694498, test/bleu=29.374789, test/loss=1.370277, test/num_examples=3003, total_duration=45556.922442, train/accuracy=0.670093, train/bleu=33.644986, train/loss=1.519820, validation/accuracy=0.678380, validation/bleu=29.574117, validation/loss=1.466761, validation/num_examples=3000
I0312 16:43:26.111702 140403528402688 logging_writer.py:48] [76200] global_step=76200, grad_norm=0.19491535425186157, loss=1.652811050415039
I0312 16:44:01.445715 140403520009984 logging_writer.py:48] [76300] global_step=76300, grad_norm=0.1934599131345749, loss=1.547939658164978
I0312 16:44:36.777975 140403528402688 logging_writer.py:48] [76400] global_step=76400, grad_norm=0.2070320099592209, loss=1.6598361730575562
I0312 16:45:12.112807 140403520009984 logging_writer.py:48] [76500] global_step=76500, grad_norm=0.19215601682662964, loss=1.6313544511795044
I0312 16:45:47.422164 140403528402688 logging_writer.py:48] [76600] global_step=76600, grad_norm=0.21076719462871552, loss=1.6151388883590698
I0312 16:46:22.734529 140403520009984 logging_writer.py:48] [76700] global_step=76700, grad_norm=0.20867672562599182, loss=1.6281085014343262
I0312 16:46:58.046710 140403528402688 logging_writer.py:48] [76800] global_step=76800, grad_norm=0.2114524096250534, loss=1.7168432474136353
I0312 16:47:33.408671 140403520009984 logging_writer.py:48] [76900] global_step=76900, grad_norm=0.18618322908878326, loss=1.5890228748321533
I0312 16:48:08.731925 140403528402688 logging_writer.py:48] [77000] global_step=77000, grad_norm=0.19560235738754272, loss=1.6428741216659546
I0312 16:48:44.057060 140403520009984 logging_writer.py:48] [77100] global_step=77100, grad_norm=0.20667245984077454, loss=1.6501199007034302
I0312 16:49:19.388003 140403528402688 logging_writer.py:48] [77200] global_step=77200, grad_norm=0.21010538935661316, loss=1.5829979181289673
I0312 16:49:54.751238 140403520009984 logging_writer.py:48] [77300] global_step=77300, grad_norm=0.21574856340885162, loss=1.679031491279602
I0312 16:50:30.131496 140403528402688 logging_writer.py:48] [77400] global_step=77400, grad_norm=0.21133802831172943, loss=1.6509681940078735
I0312 16:51:05.446758 140403520009984 logging_writer.py:48] [77500] global_step=77500, grad_norm=0.20580372214317322, loss=1.6265535354614258
I0312 16:51:40.784135 140403528402688 logging_writer.py:48] [77600] global_step=77600, grad_norm=0.20643723011016846, loss=1.6105549335479736
I0312 16:52:16.125660 140403520009984 logging_writer.py:48] [77700] global_step=77700, grad_norm=0.28986239433288574, loss=1.6089287996292114
I0312 16:52:51.420450 140403528402688 logging_writer.py:48] [77800] global_step=77800, grad_norm=0.20200736820697784, loss=1.651659369468689
I0312 16:53:26.733587 140403520009984 logging_writer.py:48] [77900] global_step=77900, grad_norm=0.19737456738948822, loss=1.6351178884506226
I0312 16:54:02.072691 140403528402688 logging_writer.py:48] [78000] global_step=78000, grad_norm=0.21472924947738647, loss=1.7009848356246948
I0312 16:54:37.404210 140403520009984 logging_writer.py:48] [78100] global_step=78100, grad_norm=0.19190481305122375, loss=1.566990852355957
I0312 16:55:12.725457 140403528402688 logging_writer.py:48] [78200] global_step=78200, grad_norm=0.19485971331596375, loss=1.6404026746749878
I0312 16:55:48.066518 140403520009984 logging_writer.py:48] [78300] global_step=78300, grad_norm=0.19850468635559082, loss=1.6815979480743408
I0312 16:56:23.379124 140403528402688 logging_writer.py:48] [78400] global_step=78400, grad_norm=0.20000462234020233, loss=1.5928704738616943
I0312 16:56:52.773523 140573192058688 spec.py:321] Evaluating on the training split.
I0312 16:56:55.731210 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 17:00:24.031428 140573192058688 spec.py:333] Evaluating on the validation split.
I0312 17:00:26.723192 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 17:02:53.518958 140573192058688 spec.py:349] Evaluating on the test split.
I0312 17:02:56.204424 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 17:05:13.327810 140573192058688 submission_runner.py:420] Time since start: 46897.72s, 	Step: 78485, 	{'train/accuracy': 0.6622284650802612, 'train/loss': 1.5754069089889526, 'train/bleu': 32.57791200673433, 'validation/accuracy': 0.6812066435813904, 'validation/loss': 1.4563653469085693, 'validation/bleu': 29.760721638682718, 'validation/num_examples': 3000, 'test/accuracy': 0.6956830024719238, 'test/loss': 1.360418438911438, 'test/bleu': 29.425967056293665, 'test/num_examples': 3003, 'score': 27761.8942797184, 'total_duration': 46897.715759277344, 'accumulated_submission_time': 27761.8942797184, 'accumulated_eval_time': 19132.193336486816, 'accumulated_logging_time': 1.0392353534698486}
I0312 17:05:13.353129 140403520009984 logging_writer.py:48] [78485] accumulated_eval_time=19132.193336, accumulated_logging_time=1.039235, accumulated_submission_time=27761.894280, global_step=78485, preemption_count=0, score=27761.894280, test/accuracy=0.695683, test/bleu=29.425967, test/loss=1.360418, test/num_examples=3003, total_duration=46897.715759, train/accuracy=0.662228, train/bleu=32.577912, train/loss=1.575407, validation/accuracy=0.681207, validation/bleu=29.760722, validation/loss=1.456365, validation/num_examples=3000
I0312 17:05:19.019812 140403528402688 logging_writer.py:48] [78500] global_step=78500, grad_norm=0.19338586926460266, loss=1.5796725749969482
I0312 17:05:54.360728 140403520009984 logging_writer.py:48] [78600] global_step=78600, grad_norm=0.20122331380844116, loss=1.6386947631835938
I0312 17:06:29.690433 140403528402688 logging_writer.py:48] [78700] global_step=78700, grad_norm=0.2035476565361023, loss=1.606674075126648
I0312 17:07:04.997965 140403520009984 logging_writer.py:48] [78800] global_step=78800, grad_norm=0.2128933221101761, loss=1.6149744987487793
I0312 17:07:40.360133 140403528402688 logging_writer.py:48] [78900] global_step=78900, grad_norm=0.19219174981117249, loss=1.5786241292953491
I0312 17:08:15.667687 140403520009984 logging_writer.py:48] [79000] global_step=79000, grad_norm=0.21639801561832428, loss=1.6822996139526367
I0312 17:08:50.990943 140403528402688 logging_writer.py:48] [79100] global_step=79100, grad_norm=0.19939756393432617, loss=1.590712547302246
I0312 17:09:26.332282 140403520009984 logging_writer.py:48] [79200] global_step=79200, grad_norm=0.19628699123859406, loss=1.5714069604873657
I0312 17:10:01.683041 140403528402688 logging_writer.py:48] [79300] global_step=79300, grad_norm=0.20895352959632874, loss=1.6133244037628174
I0312 17:10:37.028887 140403520009984 logging_writer.py:48] [79400] global_step=79400, grad_norm=0.20217770338058472, loss=1.6692698001861572
I0312 17:11:12.354251 140403528402688 logging_writer.py:48] [79500] global_step=79500, grad_norm=0.2541336119174957, loss=1.6197270154953003
I0312 17:11:47.693148 140403520009984 logging_writer.py:48] [79600] global_step=79600, grad_norm=0.206467404961586, loss=1.6633563041687012
I0312 17:12:23.020782 140403528402688 logging_writer.py:48] [79700] global_step=79700, grad_norm=0.2066645622253418, loss=1.683439016342163
I0312 17:12:58.370106 140403520009984 logging_writer.py:48] [79800] global_step=79800, grad_norm=0.19989930093288422, loss=1.6489661931991577
I0312 17:13:33.703533 140403528402688 logging_writer.py:48] [79900] global_step=79900, grad_norm=0.2076176106929779, loss=1.596105694770813
I0312 17:14:09.025665 140403520009984 logging_writer.py:48] [80000] global_step=80000, grad_norm=0.20855644345283508, loss=1.6070713996887207
I0312 17:14:44.404900 140403528402688 logging_writer.py:48] [80100] global_step=80100, grad_norm=0.2058965116739273, loss=1.673994779586792
I0312 17:15:19.840893 140403520009984 logging_writer.py:48] [80200] global_step=80200, grad_norm=0.19369740784168243, loss=1.5396770238876343
I0312 17:15:55.202755 140403528402688 logging_writer.py:48] [80300] global_step=80300, grad_norm=0.20511017739772797, loss=1.6128960847854614
I0312 17:16:30.515007 140403520009984 logging_writer.py:48] [80400] global_step=80400, grad_norm=0.21463657915592194, loss=1.6970458030700684
I0312 17:17:05.831777 140403528402688 logging_writer.py:48] [80500] global_step=80500, grad_norm=0.202818363904953, loss=1.6085675954818726
I0312 17:17:41.142418 140403520009984 logging_writer.py:48] [80600] global_step=80600, grad_norm=0.2038641721010208, loss=1.6008167266845703
I0312 17:18:16.483269 140403528402688 logging_writer.py:48] [80700] global_step=80700, grad_norm=0.22495298087596893, loss=1.6517188549041748
I0312 17:18:51.809920 140403520009984 logging_writer.py:48] [80800] global_step=80800, grad_norm=0.23133054375648499, loss=1.676308274269104
I0312 17:19:13.420150 140573192058688 spec.py:321] Evaluating on the training split.
I0312 17:19:16.373698 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 17:23:01.774851 140573192058688 spec.py:333] Evaluating on the validation split.
I0312 17:23:04.439687 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 17:25:45.835583 140573192058688 spec.py:349] Evaluating on the test split.
I0312 17:25:48.516346 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 17:28:12.814655 140573192058688 submission_runner.py:420] Time since start: 48277.20s, 	Step: 80863, 	{'train/accuracy': 0.6636092662811279, 'train/loss': 1.5703322887420654, 'train/bleu': 32.70319836498964, 'validation/accuracy': 0.6823350191116333, 'validation/loss': 1.4525598287582397, 'validation/bleu': 30.035218304926424, 'validation/num_examples': 3000, 'test/accuracy': 0.6951949596405029, 'test/loss': 1.3603163957595825, 'test/bleu': 29.63272634036333, 'test/num_examples': 3003, 'score': 28601.873636484146, 'total_duration': 48277.20259356499, 'accumulated_submission_time': 28601.873636484146, 'accumulated_eval_time': 19671.587784528732, 'accumulated_logging_time': 1.0748109817504883}
I0312 17:28:12.840917 140403528402688 logging_writer.py:48] [80863] accumulated_eval_time=19671.587785, accumulated_logging_time=1.074811, accumulated_submission_time=28601.873636, global_step=80863, preemption_count=0, score=28601.873636, test/accuracy=0.695195, test/bleu=29.632726, test/loss=1.360316, test/num_examples=3003, total_duration=48277.202594, train/accuracy=0.663609, train/bleu=32.703198, train/loss=1.570332, validation/accuracy=0.682335, validation/bleu=30.035218, validation/loss=1.452560, validation/num_examples=3000
I0312 17:28:26.252298 140403520009984 logging_writer.py:48] [80900] global_step=80900, grad_norm=0.20308436453342438, loss=1.7389111518859863
I0312 17:29:01.564819 140403528402688 logging_writer.py:48] [81000] global_step=81000, grad_norm=0.18703113496303558, loss=1.565323829650879
I0312 17:29:36.915826 140403520009984 logging_writer.py:48] [81100] global_step=81100, grad_norm=0.1986842155456543, loss=1.6722859144210815
I0312 17:30:12.227451 140403528402688 logging_writer.py:48] [81200] global_step=81200, grad_norm=0.19128292798995972, loss=1.567326307296753
I0312 17:30:47.568991 140403520009984 logging_writer.py:48] [81300] global_step=81300, grad_norm=0.20981666445732117, loss=1.6311875581741333
I0312 17:31:22.931907 140403528402688 logging_writer.py:48] [81400] global_step=81400, grad_norm=0.21951575577259064, loss=1.6252397298812866
I0312 17:31:58.266718 140403520009984 logging_writer.py:48] [81500] global_step=81500, grad_norm=0.5650149583816528, loss=1.5782285928726196
I0312 17:32:33.599832 140403528402688 logging_writer.py:48] [81600] global_step=81600, grad_norm=0.3940597474575043, loss=1.6486542224884033
I0312 17:33:08.922066 140403520009984 logging_writer.py:48] [81700] global_step=81700, grad_norm=0.21869952976703644, loss=1.5891780853271484
I0312 17:33:44.215722 140403528402688 logging_writer.py:48] [81800] global_step=81800, grad_norm=0.2167893648147583, loss=1.6558253765106201
I0312 17:34:19.516183 140403520009984 logging_writer.py:48] [81900] global_step=81900, grad_norm=0.21836838126182556, loss=1.6426867246627808
I0312 17:34:54.839948 140403528402688 logging_writer.py:48] [82000] global_step=82000, grad_norm=0.2243005782365799, loss=1.6200377941131592
I0312 17:35:30.164962 140403520009984 logging_writer.py:48] [82100] global_step=82100, grad_norm=0.2137022614479065, loss=1.6587704420089722
I0312 17:36:05.484699 140403528402688 logging_writer.py:48] [82200] global_step=82200, grad_norm=0.23667283356189728, loss=1.693068265914917
I0312 17:36:40.830306 140403520009984 logging_writer.py:48] [82300] global_step=82300, grad_norm=0.21717572212219238, loss=1.7015317678451538
I0312 17:37:16.148849 140403528402688 logging_writer.py:48] [82400] global_step=82400, grad_norm=0.20801624655723572, loss=1.6009987592697144
I0312 17:37:51.482775 140403520009984 logging_writer.py:48] [82500] global_step=82500, grad_norm=0.21397177875041962, loss=1.6123453378677368
I0312 17:38:26.817619 140403528402688 logging_writer.py:48] [82600] global_step=82600, grad_norm=0.20393244922161102, loss=1.6571273803710938
I0312 17:39:02.154237 140403520009984 logging_writer.py:48] [82700] global_step=82700, grad_norm=0.23355825245380402, loss=1.6997524499893188
I0312 17:39:37.472860 140403528402688 logging_writer.py:48] [82800] global_step=82800, grad_norm=0.19959887862205505, loss=1.578843116760254
I0312 17:40:12.795721 140403520009984 logging_writer.py:48] [82900] global_step=82900, grad_norm=0.27896830439567566, loss=1.5408806800842285
I0312 17:40:48.138464 140403528402688 logging_writer.py:48] [83000] global_step=83000, grad_norm=0.21663600206375122, loss=1.5967681407928467
I0312 17:41:23.514850 140403520009984 logging_writer.py:48] [83100] global_step=83100, grad_norm=0.19838696718215942, loss=1.620079517364502
I0312 17:41:58.864555 140403528402688 logging_writer.py:48] [83200] global_step=83200, grad_norm=0.20169031620025635, loss=1.6214022636413574
I0312 17:42:13.107007 140573192058688 spec.py:321] Evaluating on the training split.
I0312 17:42:16.083157 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 17:46:42.505825 140573192058688 spec.py:333] Evaluating on the validation split.
I0312 17:46:45.176939 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 17:49:47.863985 140573192058688 spec.py:349] Evaluating on the test split.
I0312 17:49:50.521977 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 17:52:41.826290 140573192058688 submission_runner.py:420] Time since start: 49746.21s, 	Step: 83242, 	{'train/accuracy': 0.6721797585487366, 'train/loss': 1.513619065284729, 'train/bleu': 33.680118352812386, 'validation/accuracy': 0.6828929781913757, 'validation/loss': 1.4463530778884888, 'validation/bleu': 29.927711880365525, 'validation/num_examples': 3000, 'test/accuracy': 0.6981233358383179, 'test/loss': 1.3538438081741333, 'test/bleu': 29.69593409300139, 'test/num_examples': 3003, 'score': 29442.05039691925, 'total_duration': 49746.21423172951, 'accumulated_submission_time': 29442.05039691925, 'accumulated_eval_time': 20300.30703139305, 'accumulated_logging_time': 1.112574815750122}
I0312 17:52:41.853473 140403520009984 logging_writer.py:48] [83242] accumulated_eval_time=20300.307031, accumulated_logging_time=1.112575, accumulated_submission_time=29442.050397, global_step=83242, preemption_count=0, score=29442.050397, test/accuracy=0.698123, test/bleu=29.695934, test/loss=1.353844, test/num_examples=3003, total_duration=49746.214232, train/accuracy=0.672180, train/bleu=33.680118, train/loss=1.513619, validation/accuracy=0.682893, validation/bleu=29.927712, validation/loss=1.446353, validation/num_examples=3000
I0312 17:53:02.669085 140403528402688 logging_writer.py:48] [83300] global_step=83300, grad_norm=0.21125629544258118, loss=1.6415520906448364
I0312 17:53:37.946553 140403520009984 logging_writer.py:48] [83400] global_step=83400, grad_norm=0.21090422570705414, loss=1.584718108177185
I0312 17:54:13.261713 140403528402688 logging_writer.py:48] [83500] global_step=83500, grad_norm=0.1939311921596527, loss=1.5501961708068848
I0312 17:54:48.577472 140403520009984 logging_writer.py:48] [83600] global_step=83600, grad_norm=0.2180468738079071, loss=1.5990822315216064
I0312 17:55:23.926301 140403528402688 logging_writer.py:48] [83700] global_step=83700, grad_norm=0.21000893414020538, loss=1.629138708114624
I0312 17:55:59.304282 140403520009984 logging_writer.py:48] [83800] global_step=83800, grad_norm=0.21364271640777588, loss=1.7005412578582764
I0312 17:56:34.660497 140403528402688 logging_writer.py:48] [83900] global_step=83900, grad_norm=0.20047491788864136, loss=1.5678410530090332
I0312 17:57:09.992944 140403520009984 logging_writer.py:48] [84000] global_step=84000, grad_norm=0.19116024672985077, loss=1.68723726272583
I0312 17:57:45.330291 140403528402688 logging_writer.py:48] [84100] global_step=84100, grad_norm=0.21498748660087585, loss=1.6815017461776733
I0312 17:58:20.645080 140403520009984 logging_writer.py:48] [84200] global_step=84200, grad_norm=0.2358970195055008, loss=1.6309565305709839
I0312 17:58:55.962550 140403528402688 logging_writer.py:48] [84300] global_step=84300, grad_norm=0.2003050446510315, loss=1.6139624118804932
I0312 17:59:31.283198 140403520009984 logging_writer.py:48] [84400] global_step=84400, grad_norm=0.2018509954214096, loss=1.6644366979599
I0312 18:00:06.609230 140403528402688 logging_writer.py:48] [84500] global_step=84500, grad_norm=0.2080015391111374, loss=1.700567364692688
I0312 18:00:41.921099 140403520009984 logging_writer.py:48] [84600] global_step=84600, grad_norm=0.18430764973163605, loss=1.6060010194778442
I0312 18:01:17.291474 140403528402688 logging_writer.py:48] [84700] global_step=84700, grad_norm=0.20735964179039001, loss=1.561337947845459
I0312 18:01:52.608093 140403520009984 logging_writer.py:48] [84800] global_step=84800, grad_norm=0.1987241953611374, loss=1.6540125608444214
I0312 18:02:27.943726 140403528402688 logging_writer.py:48] [84900] global_step=84900, grad_norm=0.20133249461650848, loss=1.687700867652893
I0312 18:03:03.285093 140403520009984 logging_writer.py:48] [85000] global_step=85000, grad_norm=0.2073628306388855, loss=1.553581714630127
I0312 18:03:38.654683 140403528402688 logging_writer.py:48] [85100] global_step=85100, grad_norm=0.19082196056842804, loss=1.6266533136367798
I0312 18:04:13.970005 140403520009984 logging_writer.py:48] [85200] global_step=85200, grad_norm=0.2251109629869461, loss=1.6360318660736084
I0312 18:04:49.271936 140403528402688 logging_writer.py:48] [85300] global_step=85300, grad_norm=0.23325154185295105, loss=1.6503045558929443
I0312 18:05:24.579561 140403520009984 logging_writer.py:48] [85400] global_step=85400, grad_norm=0.18949422240257263, loss=1.5521588325500488
I0312 18:05:59.905177 140403528402688 logging_writer.py:48] [85500] global_step=85500, grad_norm=0.19958454370498657, loss=1.6724233627319336
I0312 18:06:35.232978 140403520009984 logging_writer.py:48] [85600] global_step=85600, grad_norm=0.2011733055114746, loss=1.6048414707183838
I0312 18:06:42.024329 140573192058688 spec.py:321] Evaluating on the training split.
I0312 18:06:44.981187 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 18:10:40.103040 140573192058688 spec.py:333] Evaluating on the validation split.
I0312 18:10:42.776878 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 18:13:07.596755 140573192058688 spec.py:349] Evaluating on the test split.
I0312 18:13:10.269759 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 18:15:39.907200 140573192058688 submission_runner.py:420] Time since start: 51124.30s, 	Step: 85621, 	{'train/accuracy': 0.6682446002960205, 'train/loss': 1.540069341659546, 'train/bleu': 33.75045946079044, 'validation/accuracy': 0.6835377216339111, 'validation/loss': 1.439979076385498, 'validation/bleu': 29.889512828415363, 'validation/num_examples': 3000, 'test/accuracy': 0.6989948749542236, 'test/loss': 1.3409332036972046, 'test/bleu': 30.012700040809833, 'test/num_examples': 3003, 'score': 30282.133563756943, 'total_duration': 51124.29514193535, 'accumulated_submission_time': 30282.133563756943, 'accumulated_eval_time': 20838.189848661423, 'accumulated_logging_time': 1.1500585079193115}
I0312 18:15:39.933479 140403528402688 logging_writer.py:48] [85621] accumulated_eval_time=20838.189849, accumulated_logging_time=1.150059, accumulated_submission_time=30282.133564, global_step=85621, preemption_count=0, score=30282.133564, test/accuracy=0.698995, test/bleu=30.012700, test/loss=1.340933, test/num_examples=3003, total_duration=51124.295142, train/accuracy=0.668245, train/bleu=33.750459, train/loss=1.540069, validation/accuracy=0.683538, validation/bleu=29.889513, validation/loss=1.439979, validation/num_examples=3000
I0312 18:16:08.174968 140403520009984 logging_writer.py:48] [85700] global_step=85700, grad_norm=0.2025294154882431, loss=1.5803806781768799
I0312 18:16:43.518113 140403528402688 logging_writer.py:48] [85800] global_step=85800, grad_norm=0.2189151495695114, loss=1.7165950536727905
I0312 18:17:18.854436 140403520009984 logging_writer.py:48] [85900] global_step=85900, grad_norm=0.19495809078216553, loss=1.5900548696517944
I0312 18:17:54.162936 140403528402688 logging_writer.py:48] [86000] global_step=86000, grad_norm=0.20389515161514282, loss=1.679876446723938
I0312 18:18:29.516819 140403520009984 logging_writer.py:48] [86100] global_step=86100, grad_norm=0.21768176555633545, loss=1.6059569120407104
I0312 18:19:04.854197 140403528402688 logging_writer.py:48] [86200] global_step=86200, grad_norm=0.1998445838689804, loss=1.6135075092315674
I0312 18:19:40.181945 140403520009984 logging_writer.py:48] [86300] global_step=86300, grad_norm=0.21819622814655304, loss=1.6058961153030396
I0312 18:20:15.524642 140403528402688 logging_writer.py:48] [86400] global_step=86400, grad_norm=0.20163005590438843, loss=1.6026270389556885
I0312 18:20:50.832204 140403520009984 logging_writer.py:48] [86500] global_step=86500, grad_norm=0.2125118374824524, loss=1.632543683052063
I0312 18:21:26.152606 140403528402688 logging_writer.py:48] [86600] global_step=86600, grad_norm=0.32647937536239624, loss=1.6256930828094482
I0312 18:22:01.465241 140403520009984 logging_writer.py:48] [86700] global_step=86700, grad_norm=0.2190578281879425, loss=1.5697588920593262
I0312 18:22:36.798401 140403528402688 logging_writer.py:48] [86800] global_step=86800, grad_norm=0.21368449926376343, loss=1.651440143585205
I0312 18:23:12.147575 140403520009984 logging_writer.py:48] [86900] global_step=86900, grad_norm=0.20472651720046997, loss=1.5790435075759888
I0312 18:23:47.493064 140403528402688 logging_writer.py:48] [87000] global_step=87000, grad_norm=0.20654405653476715, loss=1.6153972148895264
I0312 18:24:22.826984 140403520009984 logging_writer.py:48] [87100] global_step=87100, grad_norm=0.21399278938770294, loss=1.6087205410003662
I0312 18:24:58.170013 140403528402688 logging_writer.py:48] [87200] global_step=87200, grad_norm=0.2165035903453827, loss=1.7321064472198486
I0312 18:25:33.518738 140403520009984 logging_writer.py:48] [87300] global_step=87300, grad_norm=0.20429061353206635, loss=1.5638768672943115
I0312 18:26:08.923994 140403528402688 logging_writer.py:48] [87400] global_step=87400, grad_norm=0.20838364958763123, loss=1.5979493856430054
I0312 18:26:44.283434 140403520009984 logging_writer.py:48] [87500] global_step=87500, grad_norm=0.2173416167497635, loss=1.5665276050567627
I0312 18:27:19.741814 140403528402688 logging_writer.py:48] [87600] global_step=87600, grad_norm=0.2041500359773636, loss=1.6292918920516968
I0312 18:27:55.112411 140403520009984 logging_writer.py:48] [87700] global_step=87700, grad_norm=0.19874632358551025, loss=1.5736418962478638
I0312 18:28:30.498360 140403528402688 logging_writer.py:48] [87800] global_step=87800, grad_norm=0.21204368770122528, loss=1.5665135383605957
I0312 18:29:05.826337 140403520009984 logging_writer.py:48] [87900] global_step=87900, grad_norm=0.20844243466854095, loss=1.5225733518600464
I0312 18:29:40.171198 140573192058688 spec.py:321] Evaluating on the training split.
I0312 18:29:43.129979 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 18:34:00.739866 140573192058688 spec.py:333] Evaluating on the validation split.
I0312 18:34:03.428648 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 18:37:41.880229 140573192058688 spec.py:349] Evaluating on the test split.
I0312 18:37:44.559577 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 18:41:27.319447 140573192058688 submission_runner.py:420] Time since start: 52671.71s, 	Step: 87999, 	{'train/accuracy': 0.6825253367424011, 'train/loss': 1.4487786293029785, 'train/bleu': 34.338008319013625, 'validation/accuracy': 0.6857943534851074, 'validation/loss': 1.4298676252365112, 'validation/bleu': 30.0903180949495, 'validation/num_examples': 3000, 'test/accuracy': 0.7000523209571838, 'test/loss': 1.3362531661987305, 'test/bleu': 29.869542744298638, 'test/num_examples': 3003, 'score': 31122.28181028366, 'total_duration': 52671.707394599915, 'accumulated_submission_time': 31122.28181028366, 'accumulated_eval_time': 21545.33805847168, 'accumulated_logging_time': 1.1864256858825684}
I0312 18:41:27.346428 140403528402688 logging_writer.py:48] [87999] accumulated_eval_time=21545.338058, accumulated_logging_time=1.186426, accumulated_submission_time=31122.281810, global_step=87999, preemption_count=0, score=31122.281810, test/accuracy=0.700052, test/bleu=29.869543, test/loss=1.336253, test/num_examples=3003, total_duration=52671.707395, train/accuracy=0.682525, train/bleu=34.338008, train/loss=1.448779, validation/accuracy=0.685794, validation/bleu=30.090318, validation/loss=1.429868, validation/num_examples=3000
I0312 18:41:28.073067 140403520009984 logging_writer.py:48] [88000] global_step=88000, grad_norm=0.217472642660141, loss=1.6079212427139282
I0312 18:42:03.318173 140403528402688 logging_writer.py:48] [88100] global_step=88100, grad_norm=0.22490926086902618, loss=1.5387961864471436
I0312 18:42:38.598787 140403520009984 logging_writer.py:48] [88200] global_step=88200, grad_norm=0.20667412877082825, loss=1.565733551979065
I0312 18:43:13.932508 140403528402688 logging_writer.py:48] [88300] global_step=88300, grad_norm=0.22856125235557556, loss=1.6452574729919434
I0312 18:43:49.306833 140403520009984 logging_writer.py:48] [88400] global_step=88400, grad_norm=0.20354421436786652, loss=1.6490859985351562
I0312 18:44:24.782484 140403528402688 logging_writer.py:48] [88500] global_step=88500, grad_norm=0.21593862771987915, loss=1.6255534887313843
I0312 18:45:00.162984 140403520009984 logging_writer.py:48] [88600] global_step=88600, grad_norm=0.19829878211021423, loss=1.585789442062378
I0312 18:45:35.543257 140403528402688 logging_writer.py:48] [88700] global_step=88700, grad_norm=0.21698535978794098, loss=1.615602970123291
I0312 18:46:10.925986 140403520009984 logging_writer.py:48] [88800] global_step=88800, grad_norm=0.22003087401390076, loss=1.7160779237747192
I0312 18:46:46.302897 140403528402688 logging_writer.py:48] [88900] global_step=88900, grad_norm=0.20473803579807281, loss=1.5729275941848755
I0312 18:47:21.676836 140403520009984 logging_writer.py:48] [89000] global_step=89000, grad_norm=0.21500656008720398, loss=1.5544795989990234
I0312 18:47:57.016089 140403528402688 logging_writer.py:48] [89100] global_step=89100, grad_norm=0.21075332164764404, loss=1.5525797605514526
I0312 18:48:32.346133 140403520009984 logging_writer.py:48] [89200] global_step=89200, grad_norm=0.21102742850780487, loss=1.6213891506195068
I0312 18:49:07.664561 140403528402688 logging_writer.py:48] [89300] global_step=89300, grad_norm=0.19465284049510956, loss=1.6036831140518188
I0312 18:49:42.978982 140403520009984 logging_writer.py:48] [89400] global_step=89400, grad_norm=0.37223386764526367, loss=1.5988930463790894
I0312 18:50:18.310860 140403528402688 logging_writer.py:48] [89500] global_step=89500, grad_norm=0.2007787674665451, loss=1.6167596578598022
I0312 18:50:53.630910 140403520009984 logging_writer.py:48] [89600] global_step=89600, grad_norm=0.19897232949733734, loss=1.6130921840667725
I0312 18:51:28.999830 140403528402688 logging_writer.py:48] [89700] global_step=89700, grad_norm=0.1958460807800293, loss=1.5145784616470337
I0312 18:52:04.347273 140403520009984 logging_writer.py:48] [89800] global_step=89800, grad_norm=0.1973126083612442, loss=1.5583566427230835
I0312 18:52:39.641869 140403528402688 logging_writer.py:48] [89900] global_step=89900, grad_norm=0.21532508730888367, loss=1.5612361431121826
I0312 18:53:14.937470 140403520009984 logging_writer.py:48] [90000] global_step=90000, grad_norm=0.21491995453834534, loss=1.646134376525879
I0312 18:53:50.276848 140403528402688 logging_writer.py:48] [90100] global_step=90100, grad_norm=0.23713386058807373, loss=1.639506220817566
I0312 18:54:25.633860 140403520009984 logging_writer.py:48] [90200] global_step=90200, grad_norm=0.21715866029262543, loss=1.5585111379623413
I0312 18:55:00.949861 140403528402688 logging_writer.py:48] [90300] global_step=90300, grad_norm=0.21420793235301971, loss=1.621580958366394
I0312 18:55:27.527724 140573192058688 spec.py:321] Evaluating on the training split.
I0312 18:55:30.482873 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 18:59:48.135070 140573192058688 spec.py:333] Evaluating on the validation split.
I0312 18:59:50.800947 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 19:02:53.898134 140573192058688 spec.py:349] Evaluating on the test split.
I0312 19:02:56.575309 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 19:06:26.569655 140573192058688 submission_runner.py:420] Time since start: 54170.96s, 	Step: 90377, 	{'train/accuracy': 0.6723057627677917, 'train/loss': 1.5078905820846558, 'train/bleu': 33.55991972595787, 'validation/accuracy': 0.6858935356140137, 'validation/loss': 1.4237116575241089, 'validation/bleu': 30.049164222142057, 'validation/num_examples': 3000, 'test/accuracy': 0.7006449699401855, 'test/loss': 1.3293626308441162, 'test/bleu': 29.98744025410253, 'test/num_examples': 3003, 'score': 31962.3699696064, 'total_duration': 54170.957562446594, 'accumulated_submission_time': 31962.3699696064, 'accumulated_eval_time': 22204.37990808487, 'accumulated_logging_time': 1.2239248752593994}
I0312 19:06:26.602448 140403520009984 logging_writer.py:48] [90377] accumulated_eval_time=22204.379908, accumulated_logging_time=1.223925, accumulated_submission_time=31962.369970, global_step=90377, preemption_count=0, score=31962.369970, test/accuracy=0.700645, test/bleu=29.987440, test/loss=1.329363, test/num_examples=3003, total_duration=54170.957562, train/accuracy=0.672306, train/bleu=33.559920, train/loss=1.507891, validation/accuracy=0.685894, validation/bleu=30.049164, validation/loss=1.423712, validation/num_examples=3000
I0312 19:06:35.077330 140403528402688 logging_writer.py:48] [90400] global_step=90400, grad_norm=0.2073306441307068, loss=1.571252703666687
I0312 19:07:10.335073 140403520009984 logging_writer.py:48] [90500] global_step=90500, grad_norm=0.21090345084667206, loss=1.5889924764633179
I0312 19:07:45.649227 140403528402688 logging_writer.py:48] [90600] global_step=90600, grad_norm=0.21099312603473663, loss=1.5798356533050537
I0312 19:08:20.943192 140403520009984 logging_writer.py:48] [90700] global_step=90700, grad_norm=0.20110662281513214, loss=1.5329307317733765
I0312 19:08:56.258824 140403528402688 logging_writer.py:48] [90800] global_step=90800, grad_norm=0.22549322247505188, loss=1.6041736602783203
I0312 19:09:31.616529 140403520009984 logging_writer.py:48] [90900] global_step=90900, grad_norm=0.2067880630493164, loss=1.5436736345291138
I0312 19:10:06.974194 140403528402688 logging_writer.py:48] [91000] global_step=91000, grad_norm=0.21602265536785126, loss=1.6070951223373413
I0312 19:10:42.306949 140403520009984 logging_writer.py:48] [91100] global_step=91100, grad_norm=0.20392540097236633, loss=1.5962531566619873
I0312 19:11:17.621569 140403528402688 logging_writer.py:48] [91200] global_step=91200, grad_norm=0.2122405469417572, loss=1.6817240715026855
I0312 19:11:52.960647 140403520009984 logging_writer.py:48] [91300] global_step=91300, grad_norm=0.22497840225696564, loss=1.5967029333114624
I0312 19:12:28.277489 140403528402688 logging_writer.py:48] [91400] global_step=91400, grad_norm=0.2039792686700821, loss=1.5894734859466553
I0312 19:13:03.593798 140403520009984 logging_writer.py:48] [91500] global_step=91500, grad_norm=0.20397984981536865, loss=1.5608009099960327
I0312 19:13:38.903220 140403528402688 logging_writer.py:48] [91600] global_step=91600, grad_norm=0.22214068472385406, loss=1.6340289115905762
I0312 19:14:14.260079 140403520009984 logging_writer.py:48] [91700] global_step=91700, grad_norm=0.20356568694114685, loss=1.5645045042037964
I0312 19:14:49.605724 140403528402688 logging_writer.py:48] [91800] global_step=91800, grad_norm=0.21076156198978424, loss=1.6427363157272339
I0312 19:15:24.914842 140403520009984 logging_writer.py:48] [91900] global_step=91900, grad_norm=0.22173911333084106, loss=1.5842368602752686
I0312 19:16:00.249675 140403528402688 logging_writer.py:48] [92000] global_step=92000, grad_norm=0.2192695140838623, loss=1.5833498239517212
I0312 19:16:35.597998 140403520009984 logging_writer.py:48] [92100] global_step=92100, grad_norm=0.2338564395904541, loss=1.562207579612732
I0312 19:17:10.937693 140403528402688 logging_writer.py:48] [92200] global_step=92200, grad_norm=0.22461125254631042, loss=1.6428426504135132
I0312 19:17:46.279330 140403520009984 logging_writer.py:48] [92300] global_step=92300, grad_norm=0.19756506383419037, loss=1.519800066947937
I0312 19:18:21.635310 140403528402688 logging_writer.py:48] [92400] global_step=92400, grad_norm=0.23888787627220154, loss=1.631859302520752
I0312 19:18:56.949463 140403520009984 logging_writer.py:48] [92500] global_step=92500, grad_norm=0.20751318335533142, loss=1.620811939239502
I0312 19:19:32.290743 140403528402688 logging_writer.py:48] [92600] global_step=92600, grad_norm=0.20384536683559418, loss=1.5514347553253174
I0312 19:20:07.646742 140403520009984 logging_writer.py:48] [92700] global_step=92700, grad_norm=0.2070687711238861, loss=1.5628161430358887
I0312 19:20:26.795784 140573192058688 spec.py:321] Evaluating on the training split.
I0312 19:20:29.758745 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 19:24:14.181752 140573192058688 spec.py:333] Evaluating on the validation split.
I0312 19:24:16.859787 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 19:26:54.680494 140573192058688 spec.py:349] Evaluating on the test split.
I0312 19:26:57.369642 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 19:29:56.468217 140573192058688 submission_runner.py:420] Time since start: 55580.86s, 	Step: 92756, 	{'train/accuracy': 0.6719854474067688, 'train/loss': 1.514437198638916, 'train/bleu': 33.860206279249766, 'validation/accuracy': 0.6873442530632019, 'validation/loss': 1.416701078414917, 'validation/bleu': 30.474739154210475, 'validation/num_examples': 3000, 'test/accuracy': 0.7013421654701233, 'test/loss': 1.322433352470398, 'test/bleu': 29.937114050955184, 'test/num_examples': 3003, 'score': 32802.475167512894, 'total_duration': 55580.856061935425, 'accumulated_submission_time': 32802.475167512894, 'accumulated_eval_time': 22774.052196502686, 'accumulated_logging_time': 1.2687523365020752}
I0312 19:29:56.501150 140403528402688 logging_writer.py:48] [92756] accumulated_eval_time=22774.052197, accumulated_logging_time=1.268752, accumulated_submission_time=32802.475168, global_step=92756, preemption_count=0, score=32802.475168, test/accuracy=0.701342, test/bleu=29.937114, test/loss=1.322433, test/num_examples=3003, total_duration=55580.856062, train/accuracy=0.671985, train/bleu=33.860206, train/loss=1.514437, validation/accuracy=0.687344, validation/bleu=30.474739, validation/loss=1.416701, validation/num_examples=3000
I0312 19:30:12.390212 140403520009984 logging_writer.py:48] [92800] global_step=92800, grad_norm=0.21037542819976807, loss=1.5659846067428589
I0312 19:30:47.734796 140403528402688 logging_writer.py:48] [92900] global_step=92900, grad_norm=0.22010928392410278, loss=1.5951849222183228
I0312 19:31:23.106404 140403520009984 logging_writer.py:48] [93000] global_step=93000, grad_norm=0.2338641881942749, loss=1.6013585329055786
I0312 19:31:58.459861 140403528402688 logging_writer.py:48] [93100] global_step=93100, grad_norm=0.2142765372991562, loss=1.538194179534912
I0312 19:32:33.779351 140403520009984 logging_writer.py:48] [93200] global_step=93200, grad_norm=0.2126913219690323, loss=1.556915044784546
I0312 19:33:09.148352 140403528402688 logging_writer.py:48] [93300] global_step=93300, grad_norm=0.22559019923210144, loss=1.5257234573364258
I0312 19:33:44.531080 140403520009984 logging_writer.py:48] [93400] global_step=93400, grad_norm=0.21658968925476074, loss=1.592214822769165
I0312 19:34:19.949311 140403528402688 logging_writer.py:48] [93500] global_step=93500, grad_norm=0.21252675354480743, loss=1.6209582090377808
I0312 19:34:55.281527 140403520009984 logging_writer.py:48] [93600] global_step=93600, grad_norm=0.2150517702102661, loss=1.5265041589736938
I0312 19:35:30.628655 140403528402688 logging_writer.py:48] [93700] global_step=93700, grad_norm=0.23736165463924408, loss=1.640378475189209
I0312 19:36:05.940704 140403520009984 logging_writer.py:48] [93800] global_step=93800, grad_norm=0.2223956286907196, loss=1.5727088451385498
I0312 19:36:41.291864 140403528402688 logging_writer.py:48] [93900] global_step=93900, grad_norm=0.20560979843139648, loss=1.6114773750305176
I0312 19:37:16.663454 140403520009984 logging_writer.py:48] [94000] global_step=94000, grad_norm=0.21691572666168213, loss=1.545903205871582
I0312 19:37:51.963558 140403528402688 logging_writer.py:48] [94100] global_step=94100, grad_norm=0.20803053677082062, loss=1.6286197900772095
I0312 19:38:27.298875 140403520009984 logging_writer.py:48] [94200] global_step=94200, grad_norm=0.20827628672122955, loss=1.5963538885116577
I0312 19:39:02.593945 140403528402688 logging_writer.py:48] [94300] global_step=94300, grad_norm=0.37328407168388367, loss=1.591666579246521
I0312 19:39:37.914983 140403520009984 logging_writer.py:48] [94400] global_step=94400, grad_norm=0.21930821239948273, loss=1.6617525815963745
I0312 19:40:13.275834 140403528402688 logging_writer.py:48] [94500] global_step=94500, grad_norm=0.22244270145893097, loss=1.5897865295410156
I0312 19:40:48.632764 140403520009984 logging_writer.py:48] [94600] global_step=94600, grad_norm=0.22453363239765167, loss=1.5817451477050781
I0312 19:41:23.945680 140403528402688 logging_writer.py:48] [94700] global_step=94700, grad_norm=0.2097730189561844, loss=1.5559563636779785
I0312 19:41:59.277320 140403520009984 logging_writer.py:48] [94800] global_step=94800, grad_norm=0.3994311988353729, loss=1.561792016029358
I0312 19:42:34.605925 140403528402688 logging_writer.py:48] [94900] global_step=94900, grad_norm=0.21229474246501923, loss=1.5277193784713745
I0312 19:43:09.924333 140403520009984 logging_writer.py:48] [95000] global_step=95000, grad_norm=0.23392103612422943, loss=1.5437895059585571
I0312 19:43:45.253295 140403528402688 logging_writer.py:48] [95100] global_step=95100, grad_norm=0.22000788152217865, loss=1.509529709815979
I0312 19:43:56.634874 140573192058688 spec.py:321] Evaluating on the training split.
I0312 19:43:59.592615 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 19:48:21.408570 140573192058688 spec.py:333] Evaluating on the validation split.
I0312 19:48:24.080012 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 19:51:06.383831 140573192058688 spec.py:349] Evaluating on the test split.
I0312 19:51:09.059928 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 19:53:40.623833 140573192058688 submission_runner.py:420] Time since start: 57005.01s, 	Step: 95134, 	{'train/accuracy': 0.6826393604278564, 'train/loss': 1.4538395404815674, 'train/bleu': 34.60986378597462, 'validation/accuracy': 0.6881625652313232, 'validation/loss': 1.4122296571731567, 'validation/bleu': 30.472603703054315, 'validation/num_examples': 3000, 'test/accuracy': 0.7038289904594421, 'test/loss': 1.3092671632766724, 'test/bleu': 30.08935415843265, 'test/num_examples': 3003, 'score': 33642.51808953285, 'total_duration': 57005.01177072525, 'accumulated_submission_time': 33642.51808953285, 'accumulated_eval_time': 23358.041103363037, 'accumulated_logging_time': 1.312809705734253}
I0312 19:53:40.651762 140403520009984 logging_writer.py:48] [95134] accumulated_eval_time=23358.041103, accumulated_logging_time=1.312810, accumulated_submission_time=33642.518090, global_step=95134, preemption_count=0, score=33642.518090, test/accuracy=0.703829, test/bleu=30.089354, test/loss=1.309267, test/num_examples=3003, total_duration=57005.011771, train/accuracy=0.682639, train/bleu=34.609864, train/loss=1.453840, validation/accuracy=0.688163, validation/bleu=30.472604, validation/loss=1.412230, validation/num_examples=3000
I0312 19:54:04.338600 140403528402688 logging_writer.py:48] [95200] global_step=95200, grad_norm=0.2133234441280365, loss=1.5425434112548828
I0312 19:54:39.623969 140403520009984 logging_writer.py:48] [95300] global_step=95300, grad_norm=0.21045227348804474, loss=1.5559691190719604
I0312 19:55:14.956844 140403528402688 logging_writer.py:48] [95400] global_step=95400, grad_norm=0.20424942672252655, loss=1.5632141828536987
I0312 19:55:50.314151 140403520009984 logging_writer.py:48] [95500] global_step=95500, grad_norm=0.21759402751922607, loss=1.5045557022094727
I0312 19:56:25.640101 140403528402688 logging_writer.py:48] [95600] global_step=95600, grad_norm=0.2224639505147934, loss=1.622755527496338
I0312 19:57:00.955291 140403520009984 logging_writer.py:48] [95700] global_step=95700, grad_norm=0.22045288980007172, loss=1.501947045326233
I0312 19:57:36.303988 140403528402688 logging_writer.py:48] [95800] global_step=95800, grad_norm=0.21675892174243927, loss=1.5837173461914062
I0312 19:58:11.604744 140403520009984 logging_writer.py:48] [95900] global_step=95900, grad_norm=0.2215895801782608, loss=1.5740694999694824
I0312 19:58:46.946068 140403528402688 logging_writer.py:48] [96000] global_step=96000, grad_norm=0.2361251711845398, loss=1.5381863117218018
I0312 19:59:22.306576 140403520009984 logging_writer.py:48] [96100] global_step=96100, grad_norm=0.20734156668186188, loss=1.611619472503662
I0312 19:59:57.623175 140403528402688 logging_writer.py:48] [96200] global_step=96200, grad_norm=0.20354437828063965, loss=1.5793713331222534
I0312 20:00:32.924133 140403520009984 logging_writer.py:48] [96300] global_step=96300, grad_norm=0.22507837414741516, loss=1.61061429977417
I0312 20:01:08.245426 140403528402688 logging_writer.py:48] [96400] global_step=96400, grad_norm=0.21928946673870087, loss=1.5223332643508911
I0312 20:01:43.589997 140403520009984 logging_writer.py:48] [96500] global_step=96500, grad_norm=0.21515001356601715, loss=1.536081075668335
I0312 20:02:18.953957 140403528402688 logging_writer.py:48] [96600] global_step=96600, grad_norm=0.20907831192016602, loss=1.5503308773040771
I0312 20:02:54.337628 140403520009984 logging_writer.py:48] [96700] global_step=96700, grad_norm=0.2207188606262207, loss=1.5931034088134766
I0312 20:03:29.712256 140403528402688 logging_writer.py:48] [96800] global_step=96800, grad_norm=0.23441080749034882, loss=1.6240874528884888
I0312 20:04:05.042448 140403520009984 logging_writer.py:48] [96900] global_step=96900, grad_norm=0.21563436090946198, loss=1.5415695905685425
I0312 20:04:40.361673 140403528402688 logging_writer.py:48] [97000] global_step=97000, grad_norm=0.20771709084510803, loss=1.5446478128433228
I0312 20:05:15.661752 140403520009984 logging_writer.py:48] [97100] global_step=97100, grad_norm=0.2130999118089676, loss=1.5456761121749878
I0312 20:05:50.978535 140403528402688 logging_writer.py:48] [97200] global_step=97200, grad_norm=0.22087927162647247, loss=1.5499509572982788
I0312 20:06:26.308428 140403520009984 logging_writer.py:48] [97300] global_step=97300, grad_norm=0.2188231348991394, loss=1.490706443786621
I0312 20:07:01.641036 140403528402688 logging_writer.py:48] [97400] global_step=97400, grad_norm=0.20981070399284363, loss=1.5189255475997925
I0312 20:07:37.008272 140403520009984 logging_writer.py:48] [97500] global_step=97500, grad_norm=0.20960932970046997, loss=1.5391360521316528
I0312 20:07:40.973867 140573192058688 spec.py:321] Evaluating on the training split.
I0312 20:07:43.934365 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 20:11:54.917941 140573192058688 spec.py:333] Evaluating on the validation split.
I0312 20:11:57.599427 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 20:15:27.090423 140573192058688 spec.py:349] Evaluating on the test split.
I0312 20:15:29.759230 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 20:19:14.844789 140573192058688 submission_runner.py:420] Time since start: 58539.23s, 	Step: 97513, 	{'train/accuracy': 0.6825652122497559, 'train/loss': 1.4482752084732056, 'train/bleu': 34.105125485857464, 'validation/accuracy': 0.6888568997383118, 'validation/loss': 1.4053305387496948, 'validation/bleu': 30.3004675653798, 'validation/num_examples': 3000, 'test/accuracy': 0.7047934532165527, 'test/loss': 1.3054507970809937, 'test/bleu': 29.9236223901206, 'test/num_examples': 3003, 'score': 34482.75514650345, 'total_duration': 58539.23271560669, 'accumulated_submission_time': 34482.75514650345, 'accumulated_eval_time': 24051.911954164505, 'accumulated_logging_time': 1.350311040878296}
I0312 20:19:14.873199 140403528402688 logging_writer.py:48] [97513] accumulated_eval_time=24051.911954, accumulated_logging_time=1.350311, accumulated_submission_time=34482.755147, global_step=97513, preemption_count=0, score=34482.755147, test/accuracy=0.704793, test/bleu=29.923622, test/loss=1.305451, test/num_examples=3003, total_duration=58539.232716, train/accuracy=0.682565, train/bleu=34.105125, train/loss=1.448275, validation/accuracy=0.688857, validation/bleu=30.300468, validation/loss=1.405331, validation/num_examples=3000
I0312 20:19:45.896770 140403520009984 logging_writer.py:48] [97600] global_step=97600, grad_norm=0.19711868464946747, loss=1.4855133295059204
I0312 20:20:21.184659 140403528402688 logging_writer.py:48] [97700] global_step=97700, grad_norm=0.21034881472587585, loss=1.567715048789978
I0312 20:20:56.493477 140403520009984 logging_writer.py:48] [97800] global_step=97800, grad_norm=0.2193392515182495, loss=1.5393011569976807
I0312 20:21:31.807886 140403528402688 logging_writer.py:48] [97900] global_step=97900, grad_norm=0.2206011861562729, loss=1.587672472000122
I0312 20:22:07.135841 140403520009984 logging_writer.py:48] [98000] global_step=98000, grad_norm=0.21092721819877625, loss=1.6177916526794434
I0312 20:22:42.453264 140403528402688 logging_writer.py:48] [98100] global_step=98100, grad_norm=0.21848957240581512, loss=1.574385404586792
I0312 20:23:17.840824 140403520009984 logging_writer.py:48] [98200] global_step=98200, grad_norm=0.2168300747871399, loss=1.5690213441848755
I0312 20:23:53.155480 140403528402688 logging_writer.py:48] [98300] global_step=98300, grad_norm=0.21968823671340942, loss=1.5577675104141235
I0312 20:24:28.486369 140403520009984 logging_writer.py:48] [98400] global_step=98400, grad_norm=0.21153579652309418, loss=1.4987150430679321
I0312 20:25:03.835614 140403528402688 logging_writer.py:48] [98500] global_step=98500, grad_norm=0.2157759964466095, loss=1.5214766263961792
I0312 20:25:39.190051 140403520009984 logging_writer.py:48] [98600] global_step=98600, grad_norm=0.21086132526397705, loss=1.5417327880859375
I0312 20:26:14.520463 140403528402688 logging_writer.py:48] [98700] global_step=98700, grad_norm=0.20952296257019043, loss=1.4551916122436523
I0312 20:26:49.867579 140403520009984 logging_writer.py:48] [98800] global_step=98800, grad_norm=0.225594624876976, loss=1.507667064666748
I0312 20:27:25.270845 140403528402688 logging_writer.py:48] [98900] global_step=98900, grad_norm=0.23563310503959656, loss=1.5784921646118164
I0312 20:28:00.619196 140403520009984 logging_writer.py:48] [99000] global_step=99000, grad_norm=0.20404188334941864, loss=1.4911466836929321
I0312 20:28:35.946070 140403528402688 logging_writer.py:48] [99100] global_step=99100, grad_norm=0.23139850795269012, loss=1.5883970260620117
I0312 20:29:11.269487 140403520009984 logging_writer.py:48] [99200] global_step=99200, grad_norm=0.20854274928569794, loss=1.4909477233886719
I0312 20:29:46.587953 140403528402688 logging_writer.py:48] [99300] global_step=99300, grad_norm=0.21796663105487823, loss=1.5859397649765015
I0312 20:30:21.908974 140403520009984 logging_writer.py:48] [99400] global_step=99400, grad_norm=0.2100338488817215, loss=1.5993762016296387
I0312 20:30:57.225582 140403528402688 logging_writer.py:48] [99500] global_step=99500, grad_norm=0.2103920876979828, loss=1.5485097169876099
I0312 20:31:32.538253 140403520009984 logging_writer.py:48] [99600] global_step=99600, grad_norm=0.215205579996109, loss=1.52437424659729
I0312 20:32:07.880563 140403528402688 logging_writer.py:48] [99700] global_step=99700, grad_norm=0.20310325920581818, loss=1.526861548423767
I0312 20:32:43.227332 140403520009984 logging_writer.py:48] [99800] global_step=99800, grad_norm=0.2100902497768402, loss=1.5290906429290771
I0312 20:33:15.115315 140573192058688 spec.py:321] Evaluating on the training split.
I0312 20:33:18.076883 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 20:37:34.364004 140573192058688 spec.py:333] Evaluating on the validation split.
I0312 20:37:37.029627 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 20:40:49.377670 140573192058688 spec.py:349] Evaluating on the test split.
I0312 20:40:52.039553 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 20:43:57.300270 140573192058688 submission_runner.py:420] Time since start: 60021.69s, 	Step: 99892, 	{'train/accuracy': 0.678978443145752, 'train/loss': 1.479453206062317, 'train/bleu': 34.306114082887525, 'validation/accuracy': 0.6903820037841797, 'validation/loss': 1.4001511335372925, 'validation/bleu': 30.426522454410275, 'validation/num_examples': 3000, 'test/accuracy': 0.7065016627311707, 'test/loss': 1.2986218929290771, 'test/bleu': 30.29928668877095, 'test/num_examples': 3003, 'score': 35322.91156697273, 'total_duration': 60021.6882147789, 'accumulated_submission_time': 35322.91156697273, 'accumulated_eval_time': 24694.09686756134, 'accumulated_logging_time': 1.3887178897857666}
I0312 20:43:57.327884 140403528402688 logging_writer.py:48] [99892] accumulated_eval_time=24694.096868, accumulated_logging_time=1.388718, accumulated_submission_time=35322.911567, global_step=99892, preemption_count=0, score=35322.911567, test/accuracy=0.706502, test/bleu=30.299287, test/loss=1.298622, test/num_examples=3003, total_duration=60021.688215, train/accuracy=0.678978, train/bleu=34.306114, train/loss=1.479453, validation/accuracy=0.690382, validation/bleu=30.426522, validation/loss=1.400151, validation/num_examples=3000
I0312 20:44:00.509719 140403520009984 logging_writer.py:48] [99900] global_step=99900, grad_norm=0.22022360563278198, loss=1.5847105979919434
I0312 20:44:35.771320 140403528402688 logging_writer.py:48] [100000] global_step=100000, grad_norm=0.2568674683570862, loss=1.4679417610168457
I0312 20:45:11.042521 140403520009984 logging_writer.py:48] [100100] global_step=100100, grad_norm=0.22684265673160553, loss=1.5344209671020508
I0312 20:45:46.411745 140403528402688 logging_writer.py:48] [100200] global_step=100200, grad_norm=0.21976643800735474, loss=1.5307683944702148
I0312 20:46:21.783704 140403520009984 logging_writer.py:48] [100300] global_step=100300, grad_norm=0.22075943648815155, loss=1.5475610494613647
I0312 20:46:57.231563 140403528402688 logging_writer.py:48] [100400] global_step=100400, grad_norm=0.21771004796028137, loss=1.5356063842773438
I0312 20:47:32.617501 140403520009984 logging_writer.py:48] [100500] global_step=100500, grad_norm=0.2317800372838974, loss=1.5709089040756226
I0312 20:48:07.995226 140403528402688 logging_writer.py:48] [100600] global_step=100600, grad_norm=0.21171249449253082, loss=1.5439643859863281
I0312 20:48:43.316196 140403520009984 logging_writer.py:48] [100700] global_step=100700, grad_norm=0.21573998034000397, loss=1.535581111907959
I0312 20:49:18.612848 140403528402688 logging_writer.py:48] [100800] global_step=100800, grad_norm=0.22540512681007385, loss=1.4954161643981934
I0312 20:49:53.923281 140403520009984 logging_writer.py:48] [100900] global_step=100900, grad_norm=0.23331734538078308, loss=1.5316905975341797
I0312 20:50:29.259216 140403528402688 logging_writer.py:48] [101000] global_step=101000, grad_norm=0.20930804312229156, loss=1.5465337038040161
I0312 20:51:04.618727 140403520009984 logging_writer.py:48] [101100] global_step=101100, grad_norm=0.2219153791666031, loss=1.5040967464447021
I0312 20:51:39.941614 140403528402688 logging_writer.py:48] [101200] global_step=101200, grad_norm=0.235083669424057, loss=1.5290989875793457
I0312 20:52:15.288147 140403520009984 logging_writer.py:48] [101300] global_step=101300, grad_norm=0.2076822817325592, loss=1.5013562440872192
I0312 20:52:50.631362 140403528402688 logging_writer.py:48] [101400] global_step=101400, grad_norm=0.22871805727481842, loss=1.5552704334259033
I0312 20:53:25.950434 140403520009984 logging_writer.py:48] [101500] global_step=101500, grad_norm=0.20528772473335266, loss=1.463621973991394
I0312 20:54:01.281448 140403528402688 logging_writer.py:48] [101600] global_step=101600, grad_norm=0.2207535058259964, loss=1.5577787160873413
I0312 20:54:36.621361 140403520009984 logging_writer.py:48] [101700] global_step=101700, grad_norm=0.22052213549613953, loss=1.5706756114959717
I0312 20:55:11.916952 140403528402688 logging_writer.py:48] [101800] global_step=101800, grad_norm=0.21562305092811584, loss=1.5037500858306885
I0312 20:55:47.253131 140403520009984 logging_writer.py:48] [101900] global_step=101900, grad_norm=0.21390148997306824, loss=1.4974079132080078
I0312 20:56:22.568114 140403528402688 logging_writer.py:48] [102000] global_step=102000, grad_norm=0.23603539168834686, loss=1.5881164073944092
I0312 20:56:57.888254 140403520009984 logging_writer.py:48] [102100] global_step=102100, grad_norm=0.21397367119789124, loss=1.5129843950271606
I0312 20:57:33.212124 140403528402688 logging_writer.py:48] [102200] global_step=102200, grad_norm=0.22522962093353271, loss=1.535230278968811
I0312 20:57:57.322034 140573192058688 spec.py:321] Evaluating on the training split.
I0312 20:58:00.283895 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 21:02:05.234426 140573192058688 spec.py:333] Evaluating on the validation split.
I0312 21:02:07.908140 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 21:04:52.328290 140573192058688 spec.py:349] Evaluating on the test split.
I0312 21:04:55.007816 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 21:07:57.390073 140573192058688 submission_runner.py:420] Time since start: 61461.78s, 	Step: 102270, 	{'train/accuracy': 0.6835367679595947, 'train/loss': 1.4356738328933716, 'train/bleu': 34.774789100055045, 'validation/accuracy': 0.6904935836791992, 'validation/loss': 1.3909913301467896, 'validation/bleu': 30.641976355192796, 'validation/num_examples': 3000, 'test/accuracy': 0.7074313163757324, 'test/loss': 1.2908309698104858, 'test/bleu': 30.136253694643354, 'test/num_examples': 3003, 'score': 36162.81519198418, 'total_duration': 61461.7780213356, 'accumulated_submission_time': 36162.81519198418, 'accumulated_eval_time': 25294.164860248566, 'accumulated_logging_time': 1.4278733730316162}
I0312 21:07:57.417808 140403520009984 logging_writer.py:48] [102270] accumulated_eval_time=25294.164860, accumulated_logging_time=1.427873, accumulated_submission_time=36162.815192, global_step=102270, preemption_count=0, score=36162.815192, test/accuracy=0.707431, test/bleu=30.136254, test/loss=1.290831, test/num_examples=3003, total_duration=61461.778021, train/accuracy=0.683537, train/bleu=34.774789, train/loss=1.435674, validation/accuracy=0.690494, validation/bleu=30.641976, validation/loss=1.390991, validation/num_examples=3000
I0312 21:08:08.394098 140403528402688 logging_writer.py:48] [102300] global_step=102300, grad_norm=0.22856010496616364, loss=1.541149616241455
I0312 21:08:43.710506 140403520009984 logging_writer.py:48] [102400] global_step=102400, grad_norm=0.21956631541252136, loss=1.4648308753967285
I0312 21:09:19.009837 140403528402688 logging_writer.py:48] [102500] global_step=102500, grad_norm=0.22212731838226318, loss=1.5145903825759888
I0312 21:09:54.337829 140403520009984 logging_writer.py:48] [102600] global_step=102600, grad_norm=0.2228245884180069, loss=1.502689242362976
I0312 21:10:29.689908 140403528402688 logging_writer.py:48] [102700] global_step=102700, grad_norm=0.23338712751865387, loss=1.6039471626281738
I0312 21:11:05.004127 140403520009984 logging_writer.py:48] [102800] global_step=102800, grad_norm=0.21273308992385864, loss=1.5349661111831665
I0312 21:11:40.326721 140403528402688 logging_writer.py:48] [102900] global_step=102900, grad_norm=0.20092055201530457, loss=1.4900609254837036
I0312 21:12:15.652967 140403520009984 logging_writer.py:48] [103000] global_step=103000, grad_norm=0.23047424852848053, loss=1.558162808418274
I0312 21:12:50.988144 140403528402688 logging_writer.py:48] [103100] global_step=103100, grad_norm=0.24719612300395966, loss=1.5641478300094604
I0312 21:13:26.320241 140403520009984 logging_writer.py:48] [103200] global_step=103200, grad_norm=0.2300448715686798, loss=1.5143895149230957
I0312 21:14:01.679851 140403528402688 logging_writer.py:48] [103300] global_step=103300, grad_norm=0.23149192333221436, loss=1.6119163036346436
I0312 21:14:36.990371 140403520009984 logging_writer.py:48] [103400] global_step=103400, grad_norm=0.2268698811531067, loss=1.5083231925964355
I0312 21:15:12.341998 140403528402688 logging_writer.py:48] [103500] global_step=103500, grad_norm=0.21499468386173248, loss=1.5380295515060425
I0312 21:15:47.669344 140403520009984 logging_writer.py:48] [103600] global_step=103600, grad_norm=0.21712468564510345, loss=1.466444969177246
I0312 21:16:22.971125 140403528402688 logging_writer.py:48] [103700] global_step=103700, grad_norm=0.22165004909038544, loss=1.5200201272964478
I0312 21:16:58.272633 140403520009984 logging_writer.py:48] [103800] global_step=103800, grad_norm=0.21114668250083923, loss=1.4828147888183594
I0312 21:17:33.604816 140403528402688 logging_writer.py:48] [103900] global_step=103900, grad_norm=0.21676848828792572, loss=1.5074132680892944
I0312 21:18:08.942210 140403520009984 logging_writer.py:48] [104000] global_step=104000, grad_norm=0.22767065465450287, loss=1.567967176437378
I0312 21:18:44.261673 140403528402688 logging_writer.py:48] [104100] global_step=104100, grad_norm=0.22688961029052734, loss=1.5986509323120117
I0312 21:19:19.583684 140403520009984 logging_writer.py:48] [104200] global_step=104200, grad_norm=0.22044266760349274, loss=1.5158723592758179
I0312 21:19:54.912128 140403528402688 logging_writer.py:48] [104300] global_step=104300, grad_norm=0.22697870433330536, loss=1.593011736869812
I0312 21:20:30.250844 140403520009984 logging_writer.py:48] [104400] global_step=104400, grad_norm=0.23227006196975708, loss=1.5434731245040894
I0312 21:21:05.577503 140403528402688 logging_writer.py:48] [104500] global_step=104500, grad_norm=0.21198099851608276, loss=1.5630985498428345
I0312 21:21:40.897786 140403520009984 logging_writer.py:48] [104600] global_step=104600, grad_norm=0.22430866956710815, loss=1.430123209953308
I0312 21:21:57.602188 140573192058688 spec.py:321] Evaluating on the training split.
I0312 21:22:00.579964 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 21:26:12.129074 140573192058688 spec.py:333] Evaluating on the validation split.
I0312 21:26:14.794426 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 21:29:20.296006 140573192058688 spec.py:349] Evaluating on the test split.
I0312 21:29:22.974887 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 21:32:18.670349 140573192058688 submission_runner.py:420] Time since start: 62923.06s, 	Step: 104649, 	{'train/accuracy': 0.677696704864502, 'train/loss': 1.4766273498535156, 'train/bleu': 34.298339599665034, 'validation/accuracy': 0.6918699145317078, 'validation/loss': 1.387955665588379, 'validation/bleu': 30.743370190512355, 'validation/num_examples': 3000, 'test/accuracy': 0.708453893661499, 'test/loss': 1.2837616205215454, 'test/bleu': 30.46981512380905, 'test/num_examples': 3003, 'score': 37002.91240334511, 'total_duration': 62923.05829691887, 'accumulated_submission_time': 37002.91240334511, 'accumulated_eval_time': 25915.232989549637, 'accumulated_logging_time': 1.467015027999878}
I0312 21:32:18.699347 140403528402688 logging_writer.py:48] [104649] accumulated_eval_time=25915.232990, accumulated_logging_time=1.467015, accumulated_submission_time=37002.912403, global_step=104649, preemption_count=0, score=37002.912403, test/accuracy=0.708454, test/bleu=30.469815, test/loss=1.283762, test/num_examples=3003, total_duration=62923.058297, train/accuracy=0.677697, train/bleu=34.298340, train/loss=1.476627, validation/accuracy=0.691870, validation/bleu=30.743370, validation/loss=1.387956, validation/num_examples=3000
I0312 21:32:37.062497 140403520009984 logging_writer.py:48] [104700] global_step=104700, grad_norm=0.21741846203804016, loss=1.4845037460327148
I0312 21:33:12.382432 140403528402688 logging_writer.py:48] [104800] global_step=104800, grad_norm=0.22102877497673035, loss=1.470397710800171
I0312 21:33:47.685267 140403520009984 logging_writer.py:48] [104900] global_step=104900, grad_norm=0.21710805594921112, loss=1.523747444152832
I0312 21:34:23.000695 140403528402688 logging_writer.py:48] [105000] global_step=105000, grad_norm=0.21922190487384796, loss=1.479883074760437
I0312 21:34:58.331193 140403520009984 logging_writer.py:48] [105100] global_step=105100, grad_norm=0.23401130735874176, loss=1.493456482887268
I0312 21:35:33.638995 140403528402688 logging_writer.py:48] [105200] global_step=105200, grad_norm=0.21890397369861603, loss=1.5821019411087036
I0312 21:36:08.981926 140403520009984 logging_writer.py:48] [105300] global_step=105300, grad_norm=0.22529691457748413, loss=1.551138162612915
I0312 21:36:44.290163 140403528402688 logging_writer.py:48] [105400] global_step=105400, grad_norm=0.23183690011501312, loss=1.5544596910476685
I0312 21:37:19.612308 140403520009984 logging_writer.py:48] [105500] global_step=105500, grad_norm=3.01535964012146, loss=1.6176323890686035
I0312 21:37:54.932572 140403528402688 logging_writer.py:48] [105600] global_step=105600, grad_norm=0.22216609120368958, loss=1.5644559860229492
I0312 21:38:30.285954 140403520009984 logging_writer.py:48] [105700] global_step=105700, grad_norm=0.21944989264011383, loss=1.512039303779602
I0312 21:39:05.624926 140403528402688 logging_writer.py:48] [105800] global_step=105800, grad_norm=0.21817393600940704, loss=1.4902422428131104
I0312 21:39:40.939618 140403520009984 logging_writer.py:48] [105900] global_step=105900, grad_norm=0.21953816711902618, loss=1.5581531524658203
I0312 21:40:16.285159 140403528402688 logging_writer.py:48] [106000] global_step=106000, grad_norm=0.22038021683692932, loss=1.5249320268630981
I0312 21:40:51.610678 140403520009984 logging_writer.py:48] [106100] global_step=106100, grad_norm=0.23224441707134247, loss=1.5664551258087158
I0312 21:41:26.922916 140403528402688 logging_writer.py:48] [106200] global_step=106200, grad_norm=0.21327464282512665, loss=1.4762235879898071
I0312 21:42:02.258996 140403520009984 logging_writer.py:48] [106300] global_step=106300, grad_norm=0.23313599824905396, loss=1.585951805114746
I0312 21:42:37.600835 140403528402688 logging_writer.py:48] [106400] global_step=106400, grad_norm=0.2228800505399704, loss=1.5543546676635742
I0312 21:43:12.938160 140403520009984 logging_writer.py:48] [106500] global_step=106500, grad_norm=0.2288198471069336, loss=1.4925649166107178
I0312 21:43:48.250486 140403528402688 logging_writer.py:48] [106600] global_step=106600, grad_norm=0.22478187084197998, loss=1.4929072856903076
I0312 21:44:23.569706 140403520009984 logging_writer.py:48] [106700] global_step=106700, grad_norm=0.2341269850730896, loss=1.5422650575637817
I0312 21:44:58.944797 140403528402688 logging_writer.py:48] [106800] global_step=106800, grad_norm=0.22267159819602966, loss=1.5118165016174316
I0312 21:45:34.326263 140403520009984 logging_writer.py:48] [106900] global_step=106900, grad_norm=0.22815150022506714, loss=1.4324291944503784
I0312 21:46:09.709632 140403528402688 logging_writer.py:48] [107000] global_step=107000, grad_norm=0.23030154407024384, loss=1.6124672889709473
I0312 21:46:18.970890 140573192058688 spec.py:321] Evaluating on the training split.
I0312 21:46:21.936086 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 21:50:13.530955 140573192058688 spec.py:333] Evaluating on the validation split.
I0312 21:50:16.200897 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 21:53:12.891419 140573192058688 spec.py:349] Evaluating on the test split.
I0312 21:53:15.566160 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 21:57:04.500855 140573192058688 submission_runner.py:420] Time since start: 64408.89s, 	Step: 107028, 	{'train/accuracy': 0.6928110718727112, 'train/loss': 1.384670376777649, 'train/bleu': 35.15910011581935, 'validation/accuracy': 0.6928246021270752, 'validation/loss': 1.384840965270996, 'validation/bleu': 30.72062201906474, 'validation/num_examples': 3000, 'test/accuracy': 0.7085817456245422, 'test/loss': 1.2812225818634033, 'test/bleu': 30.52008339596013, 'test/num_examples': 3003, 'score': 37843.09711742401, 'total_duration': 64408.88877725601, 'accumulated_submission_time': 37843.09711742401, 'accumulated_eval_time': 26560.762880563736, 'accumulated_logging_time': 1.5060055255889893}
I0312 21:57:04.529894 140403520009984 logging_writer.py:48] [107028] accumulated_eval_time=26560.762881, accumulated_logging_time=1.506006, accumulated_submission_time=37843.097117, global_step=107028, preemption_count=0, score=37843.097117, test/accuracy=0.708582, test/bleu=30.520083, test/loss=1.281223, test/num_examples=3003, total_duration=64408.888777, train/accuracy=0.692811, train/bleu=35.159100, train/loss=1.384670, validation/accuracy=0.692825, validation/bleu=30.720622, validation/loss=1.384841, validation/num_examples=3000
I0312 21:57:30.247239 140403528402688 logging_writer.py:48] [107100] global_step=107100, grad_norm=0.22260116040706635, loss=1.5735403299331665
I0312 21:58:05.537142 140403520009984 logging_writer.py:48] [107200] global_step=107200, grad_norm=0.225714772939682, loss=1.5616592168807983
I0312 21:58:40.813362 140403528402688 logging_writer.py:48] [107300] global_step=107300, grad_norm=0.22991953790187836, loss=1.4585773944854736
I0312 21:59:16.137958 140403520009984 logging_writer.py:48] [107400] global_step=107400, grad_norm=0.23193815350532532, loss=1.5244925022125244
I0312 21:59:51.464717 140403528402688 logging_writer.py:48] [107500] global_step=107500, grad_norm=0.21852360665798187, loss=1.4747322797775269
I0312 22:00:26.837661 140403520009984 logging_writer.py:48] [107600] global_step=107600, grad_norm=0.2223094403743744, loss=1.5083447694778442
I0312 22:01:02.151357 140403528402688 logging_writer.py:48] [107700] global_step=107700, grad_norm=0.21426935493946075, loss=1.4574233293533325
I0312 22:01:37.494044 140403520009984 logging_writer.py:48] [107800] global_step=107800, grad_norm=0.22752083837985992, loss=1.4355379343032837
I0312 22:02:12.818123 140403528402688 logging_writer.py:48] [107900] global_step=107900, grad_norm=0.22132690250873566, loss=1.427314043045044
I0312 22:02:48.122686 140403520009984 logging_writer.py:48] [108000] global_step=108000, grad_norm=0.2231748402118683, loss=1.5001442432403564
I0312 22:03:23.477681 140403528402688 logging_writer.py:48] [108100] global_step=108100, grad_norm=0.23210403323173523, loss=1.5905903577804565
I0312 22:03:58.806482 140403520009984 logging_writer.py:48] [108200] global_step=108200, grad_norm=0.23417556285858154, loss=1.4780454635620117
I0312 22:04:34.122600 140403528402688 logging_writer.py:48] [108300] global_step=108300, grad_norm=0.23488610982894897, loss=1.4920209646224976
I0312 22:05:09.490645 140403520009984 logging_writer.py:48] [108400] global_step=108400, grad_norm=0.2362365424633026, loss=1.5406947135925293
I0312 22:05:44.813176 140403528402688 logging_writer.py:48] [108500] global_step=108500, grad_norm=0.22218455374240875, loss=1.4401191473007202
I0312 22:06:20.133113 140403520009984 logging_writer.py:48] [108600] global_step=108600, grad_norm=0.217739537358284, loss=1.4571001529693604
I0312 22:06:55.475214 140403528402688 logging_writer.py:48] [108700] global_step=108700, grad_norm=0.21961267292499542, loss=1.5289379358291626
I0312 22:07:30.809513 140403520009984 logging_writer.py:48] [108800] global_step=108800, grad_norm=0.2262994796037674, loss=1.4560297727584839
I0312 22:08:06.153789 140403528402688 logging_writer.py:48] [108900] global_step=108900, grad_norm=0.21676217019557953, loss=1.4683072566986084
I0312 22:08:41.538083 140403520009984 logging_writer.py:48] [109000] global_step=109000, grad_norm=0.25744426250457764, loss=1.602600336074829
I0312 22:09:16.905819 140403528402688 logging_writer.py:48] [109100] global_step=109100, grad_norm=0.2320958971977234, loss=1.5375107526779175
I0312 22:09:52.233889 140403520009984 logging_writer.py:48] [109200] global_step=109200, grad_norm=0.22732774913311005, loss=1.501712441444397
I0312 22:10:27.593315 140403528402688 logging_writer.py:48] [109300] global_step=109300, grad_norm=0.22645731270313263, loss=1.481057047843933
I0312 22:11:02.951568 140403520009984 logging_writer.py:48] [109400] global_step=109400, grad_norm=0.2244434505701065, loss=1.477668285369873
I0312 22:11:04.795610 140573192058688 spec.py:321] Evaluating on the training split.
I0312 22:11:07.760758 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 22:15:07.010723 140573192058688 spec.py:333] Evaluating on the validation split.
I0312 22:15:09.687909 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 22:18:09.207624 140573192058688 spec.py:349] Evaluating on the test split.
I0312 22:18:11.875604 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 22:21:22.527076 140573192058688 submission_runner.py:420] Time since start: 65866.92s, 	Step: 109407, 	{'train/accuracy': 0.6901442408561707, 'train/loss': 1.4055242538452148, 'train/bleu': 35.2604371709747, 'validation/accuracy': 0.6927750110626221, 'validation/loss': 1.3797883987426758, 'validation/bleu': 30.812101810739794, 'validation/num_examples': 3000, 'test/accuracy': 0.7109174728393555, 'test/loss': 1.2740641832351685, 'test/bleu': 30.94646817655173, 'test/num_examples': 3003, 'score': 38683.2755715847, 'total_duration': 65866.91502094269, 'accumulated_submission_time': 38683.2755715847, 'accumulated_eval_time': 27178.494292497635, 'accumulated_logging_time': 1.5452158451080322}
I0312 22:21:22.556502 140403528402688 logging_writer.py:48] [109407] accumulated_eval_time=27178.494292, accumulated_logging_time=1.545216, accumulated_submission_time=38683.275572, global_step=109407, preemption_count=0, score=38683.275572, test/accuracy=0.710917, test/bleu=30.946468, test/loss=1.274064, test/num_examples=3003, total_duration=65866.915021, train/accuracy=0.690144, train/bleu=35.260437, train/loss=1.405524, validation/accuracy=0.692775, validation/bleu=30.812102, validation/loss=1.379788, validation/num_examples=3000
I0312 22:21:55.707021 140403520009984 logging_writer.py:48] [109500] global_step=109500, grad_norm=0.23080600798130035, loss=1.4872714281082153
I0312 22:22:30.995481 140403528402688 logging_writer.py:48] [109600] global_step=109600, grad_norm=0.2396637350320816, loss=1.5233206748962402
I0312 22:23:06.300067 140403520009984 logging_writer.py:48] [109700] global_step=109700, grad_norm=0.2257680594921112, loss=1.47674560546875
I0312 22:23:41.642752 140403528402688 logging_writer.py:48] [109800] global_step=109800, grad_norm=0.23528872430324554, loss=1.5249392986297607
I0312 22:24:16.984931 140403520009984 logging_writer.py:48] [109900] global_step=109900, grad_norm=0.24201586842536926, loss=1.517964482307434
I0312 22:24:52.319682 140403528402688 logging_writer.py:48] [110000] global_step=110000, grad_norm=0.2554253041744232, loss=1.485581636428833
I0312 22:25:27.694438 140403520009984 logging_writer.py:48] [110100] global_step=110100, grad_norm=0.22880521416664124, loss=1.48609459400177
I0312 22:26:03.151424 140403528402688 logging_writer.py:48] [110200] global_step=110200, grad_norm=0.2642827033996582, loss=1.5507752895355225
I0312 22:26:38.536112 140403520009984 logging_writer.py:48] [110300] global_step=110300, grad_norm=0.21768738329410553, loss=1.4224190711975098
I0312 22:27:13.921619 140403528402688 logging_writer.py:48] [110400] global_step=110400, grad_norm=0.22693009674549103, loss=1.521286964416504
I0312 22:27:49.351885 140403520009984 logging_writer.py:48] [110500] global_step=110500, grad_norm=0.2871091663837433, loss=1.5072637796401978
I0312 22:28:24.739586 140403528402688 logging_writer.py:48] [110600] global_step=110600, grad_norm=0.2391834259033203, loss=1.4910705089569092
I0312 22:29:00.089518 140403520009984 logging_writer.py:48] [110700] global_step=110700, grad_norm=0.23351986706256866, loss=1.502812147140503
I0312 22:29:35.426779 140403528402688 logging_writer.py:48] [110800] global_step=110800, grad_norm=0.2374640554189682, loss=1.485984206199646
I0312 22:30:10.807403 140403520009984 logging_writer.py:48] [110900] global_step=110900, grad_norm=0.22994819283485413, loss=1.5178916454315186
I0312 22:30:46.129843 140403528402688 logging_writer.py:48] [111000] global_step=111000, grad_norm=0.23363621532917023, loss=1.4981046915054321
I0312 22:31:21.459662 140403520009984 logging_writer.py:48] [111100] global_step=111100, grad_norm=0.22566746175289154, loss=1.472312331199646
I0312 22:31:56.828382 140403528402688 logging_writer.py:48] [111200] global_step=111200, grad_norm=0.23255811631679535, loss=1.4557451009750366
I0312 22:32:32.154470 140403520009984 logging_writer.py:48] [111300] global_step=111300, grad_norm=0.21893195807933807, loss=1.4880664348602295
I0312 22:33:07.513410 140403528402688 logging_writer.py:48] [111400] global_step=111400, grad_norm=0.22501206398010254, loss=1.4556505680084229
I0312 22:33:42.860077 140403520009984 logging_writer.py:48] [111500] global_step=111500, grad_norm=0.21939793229103088, loss=1.5314871072769165
I0312 22:34:18.207477 140403528402688 logging_writer.py:48] [111600] global_step=111600, grad_norm=0.23279663920402527, loss=1.4445704221725464
I0312 22:34:53.583741 140403520009984 logging_writer.py:48] [111700] global_step=111700, grad_norm=0.21996521949768066, loss=1.4789254665374756
I0312 22:35:22.642234 140573192058688 spec.py:321] Evaluating on the training split.
I0312 22:35:25.605015 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 22:39:34.755989 140573192058688 spec.py:333] Evaluating on the validation split.
I0312 22:39:37.433459 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 22:42:09.225230 140573192058688 spec.py:349] Evaluating on the test split.
I0312 22:42:11.907249 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 22:44:31.030918 140573192058688 submission_runner.py:420] Time since start: 67255.42s, 	Step: 111784, 	{'train/accuracy': 0.6872426271438599, 'train/loss': 1.4228895902633667, 'train/bleu': 34.660629651300326, 'validation/accuracy': 0.6934818029403687, 'validation/loss': 1.3747117519378662, 'validation/bleu': 30.673267151118715, 'validation/num_examples': 3000, 'test/accuracy': 0.7106966376304626, 'test/loss': 1.268842339515686, 'test/bleu': 30.744305941224404, 'test/num_examples': 3003, 'score': 39523.270413160324, 'total_duration': 67255.41884446144, 'accumulated_submission_time': 39523.270413160324, 'accumulated_eval_time': 27726.88291144371, 'accumulated_logging_time': 1.5854876041412354}
I0312 22:44:31.061587 140403528402688 logging_writer.py:48] [111784] accumulated_eval_time=27726.882911, accumulated_logging_time=1.585488, accumulated_submission_time=39523.270413, global_step=111784, preemption_count=0, score=39523.270413, test/accuracy=0.710697, test/bleu=30.744306, test/loss=1.268842, test/num_examples=3003, total_duration=67255.418844, train/accuracy=0.687243, train/bleu=34.660630, train/loss=1.422890, validation/accuracy=0.693482, validation/bleu=30.673267, validation/loss=1.374712, validation/num_examples=3000
I0312 22:44:37.080551 140403520009984 logging_writer.py:48] [111800] global_step=111800, grad_norm=0.22170650959014893, loss=1.4516156911849976
I0312 22:45:12.375566 140403528402688 logging_writer.py:48] [111900] global_step=111900, grad_norm=0.23901796340942383, loss=1.5524600744247437
I0312 22:45:47.711786 140403520009984 logging_writer.py:48] [112000] global_step=112000, grad_norm=0.24100318551063538, loss=1.495146632194519
I0312 22:46:23.031828 140403528402688 logging_writer.py:48] [112100] global_step=112100, grad_norm=0.2403496354818344, loss=1.4590924978256226
I0312 22:46:58.346743 140403520009984 logging_writer.py:48] [112200] global_step=112200, grad_norm=0.22861436009407043, loss=1.524674654006958
I0312 22:47:33.681930 140403528402688 logging_writer.py:48] [112300] global_step=112300, grad_norm=0.23821088671684265, loss=1.4977061748504639
I0312 22:48:09.062476 140403520009984 logging_writer.py:48] [112400] global_step=112400, grad_norm=0.2329198271036148, loss=1.5419687032699585
I0312 22:48:44.462538 140403528402688 logging_writer.py:48] [112500] global_step=112500, grad_norm=0.21804775297641754, loss=1.5027399063110352
I0312 22:49:19.825587 140403520009984 logging_writer.py:48] [112600] global_step=112600, grad_norm=0.23062260448932648, loss=1.4972953796386719
I0312 22:49:55.166410 140403528402688 logging_writer.py:48] [112700] global_step=112700, grad_norm=0.22312642633914948, loss=1.4199903011322021
I0312 22:50:30.503897 140403520009984 logging_writer.py:48] [112800] global_step=112800, grad_norm=0.22323541343212128, loss=1.4310168027877808
I0312 22:51:05.824643 140403528402688 logging_writer.py:48] [112900] global_step=112900, grad_norm=0.2345578819513321, loss=1.457453966140747
I0312 22:51:41.179716 140403520009984 logging_writer.py:48] [113000] global_step=113000, grad_norm=0.23760367929935455, loss=1.4965331554412842
I0312 22:52:16.489760 140403528402688 logging_writer.py:48] [113100] global_step=113100, grad_norm=0.22657403349876404, loss=1.4455511569976807
I0312 22:52:51.835499 140403520009984 logging_writer.py:48] [113200] global_step=113200, grad_norm=0.2330474555492401, loss=1.477034091949463
I0312 22:53:27.161303 140403528402688 logging_writer.py:48] [113300] global_step=113300, grad_norm=0.2502419948577881, loss=1.5205481052398682
I0312 22:54:02.493808 140403520009984 logging_writer.py:48] [113400] global_step=113400, grad_norm=0.24103468656539917, loss=1.5075125694274902
I0312 22:54:37.878596 140403528402688 logging_writer.py:48] [113500] global_step=113500, grad_norm=0.23939065635204315, loss=1.524539828300476
I0312 22:55:13.228301 140403520009984 logging_writer.py:48] [113600] global_step=113600, grad_norm=0.22140979766845703, loss=1.464656949043274
I0312 22:55:48.526717 140403528402688 logging_writer.py:48] [113700] global_step=113700, grad_norm=0.22812066972255707, loss=1.4854007959365845
I0312 22:56:23.845408 140403520009984 logging_writer.py:48] [113800] global_step=113800, grad_norm=0.2553870379924774, loss=1.4989995956420898
I0312 22:56:59.181416 140403528402688 logging_writer.py:48] [113900] global_step=113900, grad_norm=0.23188230395317078, loss=1.494876503944397
I0312 22:57:34.516491 140403520009984 logging_writer.py:48] [114000] global_step=114000, grad_norm=0.2375996708869934, loss=1.472411870956421
I0312 22:58:09.866072 140403528402688 logging_writer.py:48] [114100] global_step=114100, grad_norm=0.25109270215034485, loss=1.4167759418487549
I0312 22:58:31.127770 140573192058688 spec.py:321] Evaluating on the training split.
I0312 22:58:34.097485 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 23:02:35.545296 140573192058688 spec.py:333] Evaluating on the validation split.
I0312 23:02:38.231448 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 23:05:39.598978 140573192058688 spec.py:349] Evaluating on the test split.
I0312 23:05:42.277037 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 23:08:49.175547 140573192058688 submission_runner.py:420] Time since start: 68713.56s, 	Step: 114162, 	{'train/accuracy': 0.6885464191436768, 'train/loss': 1.4088932275772095, 'train/bleu': 35.04633107417319, 'validation/accuracy': 0.6940521597862244, 'validation/loss': 1.3701497316360474, 'validation/bleu': 30.744583577862368, 'validation/num_examples': 3000, 'test/accuracy': 0.7109755277633667, 'test/loss': 1.2654063701629639, 'test/bleu': 30.69732003049124, 'test/num_examples': 3003, 'score': 40363.24680304527, 'total_duration': 68713.56347870827, 'accumulated_submission_time': 40363.24680304527, 'accumulated_eval_time': 28344.93062567711, 'accumulated_logging_time': 1.6279840469360352}
I0312 23:08:49.205057 140403520009984 logging_writer.py:48] [114162] accumulated_eval_time=28344.930626, accumulated_logging_time=1.627984, accumulated_submission_time=40363.246803, global_step=114162, preemption_count=0, score=40363.246803, test/accuracy=0.710976, test/bleu=30.697320, test/loss=1.265406, test/num_examples=3003, total_duration=68713.563479, train/accuracy=0.688546, train/bleu=35.046331, train/loss=1.408893, validation/accuracy=0.694052, validation/bleu=30.744584, validation/loss=1.370150, validation/num_examples=3000
I0312 23:09:02.982580 140403528402688 logging_writer.py:48] [114200] global_step=114200, grad_norm=0.23022142052650452, loss=1.480623483657837
I0312 23:09:38.289450 140403520009984 logging_writer.py:48] [114300] global_step=114300, grad_norm=0.2336254119873047, loss=1.549118161201477
I0312 23:10:13.631539 140403528402688 logging_writer.py:48] [114400] global_step=114400, grad_norm=0.22885823249816895, loss=1.5273666381835938
I0312 23:10:48.954845 140403520009984 logging_writer.py:48] [114500] global_step=114500, grad_norm=0.23374409973621368, loss=1.4889812469482422
I0312 23:11:24.295870 140403528402688 logging_writer.py:48] [114600] global_step=114600, grad_norm=0.2299690991640091, loss=1.5441811084747314
I0312 23:11:59.594820 140403520009984 logging_writer.py:48] [114700] global_step=114700, grad_norm=0.23941364884376526, loss=1.5563160181045532
I0312 23:12:34.912950 140403528402688 logging_writer.py:48] [114800] global_step=114800, grad_norm=0.2289806604385376, loss=1.4785722494125366
I0312 23:13:10.215087 140403520009984 logging_writer.py:48] [114900] global_step=114900, grad_norm=0.22462542355060577, loss=1.50570809841156
I0312 23:13:45.532526 140403528402688 logging_writer.py:48] [115000] global_step=115000, grad_norm=0.2368350327014923, loss=1.4499824047088623
I0312 23:14:20.862112 140403520009984 logging_writer.py:48] [115100] global_step=115100, grad_norm=0.23298510909080505, loss=1.4241517782211304
I0312 23:14:56.173803 140403528402688 logging_writer.py:48] [115200] global_step=115200, grad_norm=0.2277691662311554, loss=1.428682565689087
I0312 23:15:31.506034 140403520009984 logging_writer.py:48] [115300] global_step=115300, grad_norm=0.23507945239543915, loss=1.4600902795791626
I0312 23:16:06.821565 140403528402688 logging_writer.py:48] [115400] global_step=115400, grad_norm=0.23949646949768066, loss=1.4884272813796997
I0312 23:16:42.152267 140403520009984 logging_writer.py:48] [115500] global_step=115500, grad_norm=0.22276411950588226, loss=1.4911842346191406
I0312 23:17:17.495301 140403528402688 logging_writer.py:48] [115600] global_step=115600, grad_norm=0.2437221258878708, loss=1.4540120363235474
I0312 23:17:52.829067 140403520009984 logging_writer.py:48] [115700] global_step=115700, grad_norm=0.2428514063358307, loss=1.4824879169464111
I0312 23:18:28.147982 140403528402688 logging_writer.py:48] [115800] global_step=115800, grad_norm=0.22712577879428864, loss=1.4759681224822998
I0312 23:19:03.481002 140403520009984 logging_writer.py:48] [115900] global_step=115900, grad_norm=0.22871877253055573, loss=1.4267407655715942
I0312 23:19:38.813766 140403528402688 logging_writer.py:48] [116000] global_step=116000, grad_norm=0.22739917039871216, loss=1.5169867277145386
I0312 23:20:14.140541 140403520009984 logging_writer.py:48] [116100] global_step=116100, grad_norm=0.23511479794979095, loss=1.4610759019851685
I0312 23:20:49.612303 140403528402688 logging_writer.py:48] [116200] global_step=116200, grad_norm=0.22643543779850006, loss=1.4675874710083008
I0312 23:21:25.008808 140403520009984 logging_writer.py:48] [116300] global_step=116300, grad_norm=0.228971466422081, loss=1.460559368133545
I0312 23:22:00.331154 140403528402688 logging_writer.py:48] [116400] global_step=116400, grad_norm=0.22806254029273987, loss=1.3583637475967407
I0312 23:22:35.666404 140403520009984 logging_writer.py:48] [116500] global_step=116500, grad_norm=0.22973990440368652, loss=1.540124773979187
I0312 23:22:49.513000 140573192058688 spec.py:321] Evaluating on the training split.
I0312 23:22:52.473069 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 23:26:59.091141 140573192058688 spec.py:333] Evaluating on the validation split.
I0312 23:27:01.765743 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 23:30:05.420907 140573192058688 spec.py:349] Evaluating on the test split.
I0312 23:30:08.094816 140573192058688 workload.py:181] Translating evaluation dataset.
I0312 23:33:48.839229 140573192058688 submission_runner.py:420] Time since start: 70213.23s, 	Step: 116541, 	{'train/accuracy': 0.6922299861907959, 'train/loss': 1.387744426727295, 'train/bleu': 34.91732924260557, 'validation/accuracy': 0.6947712898254395, 'validation/loss': 1.366743564605713, 'validation/bleu': 30.894717875592825, 'validation/num_examples': 3000, 'test/accuracy': 0.7128115892410278, 'test/loss': 1.2598994970321655, 'test/bleu': 31.03030929029123, 'test/num_examples': 3003, 'score': 41203.46653318405, 'total_duration': 70213.22715878487, 'accumulated_submission_time': 41203.46653318405, 'accumulated_eval_time': 29004.25678896904, 'accumulated_logging_time': 1.6690990924835205}
I0312 23:33:48.871072 140403528402688 logging_writer.py:48] [116541] accumulated_eval_time=29004.256789, accumulated_logging_time=1.669099, accumulated_submission_time=41203.466533, global_step=116541, preemption_count=0, score=41203.466533, test/accuracy=0.712812, test/bleu=31.030309, test/loss=1.259899, test/num_examples=3003, total_duration=70213.227159, train/accuracy=0.692230, train/bleu=34.917329, train/loss=1.387744, validation/accuracy=0.694771, validation/bleu=30.894718, validation/loss=1.366744, validation/num_examples=3000
I0312 23:33:48.902351 140403520009984 logging_writer.py:48] [116541] global_step=116541, preemption_count=0, score=41203.466533
I0312 23:33:50.097238 140573192058688 checkpoints.py:490] Saving checkpoint at step: 116541
I0312 23:33:54.099908 140573192058688 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification_self_tuning/study_4/wmt_jax/trial_1/checkpoint_116541
I0312 23:33:54.105144 140573192058688 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification_self_tuning/study_4/wmt_jax/trial_1/checkpoint_116541.
I0312 23:33:54.154445 140573192058688 submission_runner.py:683] Final wmt score: 41203.46653318405
