python3 submission_runner.py --framework=jax --workload=wmt --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --data_dir=/data/wmt --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=prize_qualification/study_1 --overwrite=true --save_checkpoints=false --num_tuning_trials=5 --rng_seed=529981238 --max_global_steps=133333 2>&1 | tee -a /logs/wmt_jax_02-06-2024-11-39-02.log
I0206 11:39:25.038551 140699726837568 logger_utils.py:76] Creating experiment directory at /experiment_runs/prize_qualification/study_1/wmt_jax.
I0206 11:39:26.082554 140699726837568 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Host Interpreter CUDA
I0206 11:39:26.083357 140699726837568 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0206 11:39:26.083487 140699726837568 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0206 11:39:26.085813 140699726837568 submission_runner.py:542] Using RNG seed 529981238
I0206 11:39:27.242938 140699726837568 submission_runner.py:551] --- Tuning run 1/5 ---
I0206 11:39:27.243144 140699726837568 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_1/wmt_jax/trial_1.
I0206 11:39:27.243548 140699726837568 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_1/wmt_jax/trial_1/hparams.json.
I0206 11:39:27.430761 140699726837568 submission_runner.py:206] Initializing dataset.
I0206 11:39:27.442626 140699726837568 dataset_info.py:578] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0206 11:39:27.446850 140699726837568 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0206 11:39:27.610670 140699726837568 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0206 11:39:29.631136 140699726837568 submission_runner.py:213] Initializing model.
I0206 11:39:38.928910 140699726837568 submission_runner.py:255] Initializing optimizer.
I0206 11:39:40.083173 140699726837568 submission_runner.py:262] Initializing metrics bundle.
I0206 11:39:40.083388 140699726837568 submission_runner.py:280] Initializing checkpoint and logger.
I0206 11:39:40.084635 140699726837568 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_1/wmt_jax/trial_1 with prefix checkpoint_
I0206 11:39:40.084776 140699726837568 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_1/wmt_jax/trial_1/meta_data_0.json.
I0206 11:39:40.084980 140699726837568 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0206 11:39:40.085040 140699726837568 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0206 11:39:40.458447 140699726837568 logger_utils.py:220] Unable to record git information. Continuing without it.
I0206 11:39:40.792588 140699726837568 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_1/wmt_jax/trial_1/flags_0.json.
I0206 11:39:40.803766 140699726837568 submission_runner.py:314] Starting training loop.
I0206 11:40:17.824622 140534601471744 logging_writer.py:48] [0] global_step=0, grad_norm=5.61525297164917, loss=11.164706230163574
I0206 11:40:17.842366 140699726837568 spec.py:321] Evaluating on the training split.
I0206 11:40:17.845831 140699726837568 dataset_info.py:578] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0206 11:40:17.848920 140699726837568 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0206 11:40:17.887555 140699726837568 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0206 11:40:25.553551 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 11:45:24.807584 140699726837568 spec.py:333] Evaluating on the validation split.
I0206 11:45:24.813890 140699726837568 dataset_info.py:578] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0206 11:45:24.817785 140699726837568 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0206 11:45:24.858047 140699726837568 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split validation, from /data/wmt/wmt14_translate/de-en/1.0.0
I0206 11:45:31.707409 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 11:50:20.835656 140699726837568 spec.py:349] Evaluating on the test split.
I0206 11:50:20.838562 140699726837568 dataset_info.py:578] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0206 11:50:20.841707 140699726837568 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0206 11:50:20.880408 140699726837568 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split test, from /data/wmt/wmt14_translate/de-en/1.0.0
I0206 11:50:23.753902 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 11:55:12.817510 140699726837568 submission_runner.py:408] Time since start: 932.01s, 	Step: 1, 	{'train/accuracy': 0.0006529284291900694, 'train/loss': 11.176665306091309, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.208685874938965, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.19086742401123, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 37.038549184799194, 'total_duration': 932.0136518478394, 'accumulated_submission_time': 37.038549184799194, 'accumulated_eval_time': 894.9750609397888, 'accumulated_logging_time': 0}
I0206 11:55:12.838196 140530045396736 logging_writer.py:48] [1] accumulated_eval_time=894.975061, accumulated_logging_time=0, accumulated_submission_time=37.038549, global_step=1, preemption_count=0, score=37.038549, test/accuracy=0.000709, test/bleu=0.000000, test/loss=11.190867, test/num_examples=3003, total_duration=932.013652, train/accuracy=0.000653, train/bleu=0.000000, train/loss=11.176665, validation/accuracy=0.000484, validation/bleu=0.000000, validation/loss=11.208686, validation/num_examples=3000
I0206 11:55:48.025104 140530037004032 logging_writer.py:48] [100] global_step=100, grad_norm=0.43962424993515015, loss=8.90355110168457
I0206 11:56:23.232789 140530045396736 logging_writer.py:48] [200] global_step=200, grad_norm=0.15697377920150757, loss=8.547919273376465
I0206 11:56:58.463038 140530037004032 logging_writer.py:48] [300] global_step=300, grad_norm=0.16404852271080017, loss=8.325197219848633
I0206 11:57:33.706731 140530045396736 logging_writer.py:48] [400] global_step=400, grad_norm=0.288122296333313, loss=7.973024368286133
I0206 11:58:08.963767 140530037004032 logging_writer.py:48] [500] global_step=500, grad_norm=0.3593648374080658, loss=7.633212566375732
I0206 11:58:44.234342 140530045396736 logging_writer.py:48] [600] global_step=600, grad_norm=0.7561393976211548, loss=7.398614406585693
I0206 11:59:19.531333 140530037004032 logging_writer.py:48] [700] global_step=700, grad_norm=0.6924850344657898, loss=7.203282356262207
I0206 11:59:54.810769 140530045396736 logging_writer.py:48] [800] global_step=800, grad_norm=0.5590024590492249, loss=6.931511402130127
I0206 12:00:30.080741 140530037004032 logging_writer.py:48] [900] global_step=900, grad_norm=0.5186825394630432, loss=6.815753936767578
I0206 12:01:05.369699 140530045396736 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.4649958312511444, loss=6.560572624206543
I0206 12:01:40.762047 140530037004032 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.5652977228164673, loss=6.358702659606934
I0206 12:02:16.094035 140530045396736 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.7269953489303589, loss=6.225107192993164
I0206 12:02:51.380153 140530037004032 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.5980017185211182, loss=6.0884294509887695
I0206 12:03:26.680213 140530045396736 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.5780620574951172, loss=5.932805061340332
I0206 12:04:02.013089 140530037004032 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.5672272443771362, loss=5.790924549102783
I0206 12:04:37.301940 140530045396736 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.5938378572463989, loss=5.659497261047363
I0206 12:05:12.622108 140530037004032 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.603783905506134, loss=5.509642124176025
I0206 12:05:47.938782 140530045396736 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.8971288800239563, loss=5.469431400299072
I0206 12:06:23.257066 140530037004032 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.8999788165092468, loss=5.3523149490356445
I0206 12:06:58.565549 140530045396736 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.8161042928695679, loss=5.334662437438965
I0206 12:07:33.931786 140530037004032 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.8748909831047058, loss=5.164454460144043
I0206 12:08:09.293333 140530045396736 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.6426464915275574, loss=5.055200576782227
I0206 12:08:44.648874 140530037004032 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.8082967400550842, loss=4.97743558883667
I0206 12:09:13.001867 140699726837568 spec.py:321] Evaluating on the training split.
I0206 12:09:16.027427 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 12:13:32.491111 140699726837568 spec.py:333] Evaluating on the validation split.
I0206 12:13:35.224083 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 12:17:45.306617 140699726837568 spec.py:349] Evaluating on the test split.
I0206 12:17:48.027487 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 12:22:01.138643 140699726837568 submission_runner.py:408] Time since start: 2540.33s, 	Step: 2382, 	{'train/accuracy': 0.4112793505191803, 'train/loss': 4.001132965087891, 'train/bleu': 14.314785399828414, 'validation/accuracy': 0.39581653475761414, 'validation/loss': 4.1393537521362305, 'validation/bleu': 9.577254281286269, 'validation/num_examples': 3000, 'test/accuracy': 0.378374308347702, 'test/loss': 4.347209453582764, 'test/bleu': 7.77435242429996, 'test/num_examples': 3003, 'score': 877.1155626773834, 'total_duration': 2540.3347775936127, 'accumulated_submission_time': 877.1155626773834, 'accumulated_eval_time': 1663.1117770671844, 'accumulated_logging_time': 0.03058791160583496}
I0206 12:22:01.153711 140530045396736 logging_writer.py:48] [2382] accumulated_eval_time=1663.111777, accumulated_logging_time=0.030588, accumulated_submission_time=877.115563, global_step=2382, preemption_count=0, score=877.115563, test/accuracy=0.378374, test/bleu=7.774352, test/loss=4.347209, test/num_examples=3003, total_duration=2540.334778, train/accuracy=0.411279, train/bleu=14.314785, train/loss=4.001133, validation/accuracy=0.395817, validation/bleu=9.577254, validation/loss=4.139354, validation/num_examples=3000
I0206 12:22:07.847622 140530037004032 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.7200735211372375, loss=4.913783073425293
I0206 12:22:43.060207 140530045396736 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.66403728723526, loss=4.827714920043945
I0206 12:23:18.322755 140530037004032 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.7373752593994141, loss=4.647209167480469
I0206 12:23:53.658659 140530045396736 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.8753206729888916, loss=4.636376857757568
I0206 12:24:28.975308 140530037004032 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.7944011688232422, loss=4.554044723510742
I0206 12:25:04.279547 140530045396736 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.9081502556800842, loss=4.489220142364502
I0206 12:25:39.591436 140530037004032 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.0712976455688477, loss=4.490762710571289
I0206 12:26:14.918211 140530045396736 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.6287800073623657, loss=4.3538312911987305
I0206 12:26:50.206481 140530037004032 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.9690577983856201, loss=4.298713207244873
I0206 12:27:25.501821 140530045396736 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.9087149500846863, loss=4.282699108123779
I0206 12:28:00.847829 140530037004032 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.6334270238876343, loss=4.245983600616455
I0206 12:28:36.196552 140530045396736 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.6817268133163452, loss=4.170690536499023
I0206 12:29:11.504870 140530037004032 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.6867973208427429, loss=4.155292987823486
I0206 12:29:46.816058 140530045396736 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.6364837288856506, loss=4.054623603820801
I0206 12:30:22.132420 140530037004032 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.7742856740951538, loss=4.106556415557861
I0206 12:30:57.441463 140530045396736 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.8382261395454407, loss=4.055741310119629
I0206 12:31:32.764014 140530037004032 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.6543673276901245, loss=4.04228401184082
I0206 12:32:08.080704 140530045396736 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.6121101379394531, loss=3.9271352291107178
I0206 12:32:43.374734 140530037004032 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.6067943572998047, loss=3.9423420429229736
I0206 12:33:18.682395 140530045396736 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.539582371711731, loss=3.8672022819519043
I0206 12:33:53.986844 140530037004032 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.6368459463119507, loss=3.8975820541381836
I0206 12:34:29.305234 140530045396736 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.5915032029151917, loss=3.81266450881958
I0206 12:35:04.620673 140530037004032 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.5651856660842896, loss=3.8520350456237793
I0206 12:35:39.954110 140530045396736 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.5189154744148254, loss=3.682642698287964
I0206 12:36:01.194591 140699726837568 spec.py:321] Evaluating on the training split.
I0206 12:36:04.234406 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 12:38:51.032699 140699726837568 spec.py:333] Evaluating on the validation split.
I0206 12:38:53.765448 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 12:41:36.126034 140699726837568 spec.py:349] Evaluating on the test split.
I0206 12:41:38.880716 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 12:44:08.345412 140699726837568 submission_runner.py:408] Time since start: 3867.54s, 	Step: 4762, 	{'train/accuracy': 0.5386283993721008, 'train/loss': 2.7876904010772705, 'train/bleu': 24.47574218709137, 'validation/accuracy': 0.5418035387992859, 'validation/loss': 2.7314565181732178, 'validation/bleu': 20.31710448845741, 'validation/num_examples': 3000, 'test/accuracy': 0.5421649217605591, 'test/loss': 2.766301155090332, 'test/bleu': 18.93392274615353, 'test/num_examples': 3003, 'score': 1717.0735466480255, 'total_duration': 3867.541526556015, 'accumulated_submission_time': 1717.0735466480255, 'accumulated_eval_time': 2150.2625029087067, 'accumulated_logging_time': 0.05559253692626953}
I0206 12:44:08.361840 140530037004032 logging_writer.py:48] [4762] accumulated_eval_time=2150.262503, accumulated_logging_time=0.055593, accumulated_submission_time=1717.073547, global_step=4762, preemption_count=0, score=1717.073547, test/accuracy=0.542165, test/bleu=18.933923, test/loss=2.766301, test/num_examples=3003, total_duration=3867.541527, train/accuracy=0.538628, train/bleu=24.475742, train/loss=2.787690, validation/accuracy=0.541804, validation/bleu=20.317104, validation/loss=2.731457, validation/num_examples=3000
I0206 12:44:22.093075 140530045396736 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.6126542687416077, loss=3.7677507400512695
I0206 12:44:57.355191 140530037004032 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.5005627274513245, loss=3.7654216289520264
I0206 12:45:32.665645 140530045396736 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.6044248938560486, loss=3.7479686737060547
I0206 12:46:07.972147 140530037004032 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.7157527804374695, loss=3.7529547214508057
I0206 12:46:43.286425 140530045396736 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.5633400678634644, loss=3.7140796184539795
I0206 12:47:18.589151 140530037004032 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.48610758781433105, loss=3.6535606384277344
I0206 12:47:53.891437 140530045396736 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.5785999894142151, loss=3.659987449645996
I0206 12:48:29.187544 140530037004032 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.47269484400749207, loss=3.578615427017212
I0206 12:49:04.470964 140530045396736 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.562244713306427, loss=3.6600775718688965
I0206 12:49:39.748782 140530037004032 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.48843854665756226, loss=3.6111605167388916
I0206 12:50:15.035996 140530045396736 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.4943360984325409, loss=3.6684603691101074
I0206 12:50:50.328373 140530037004032 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.4754076898097992, loss=3.6294422149658203
I0206 12:51:25.653973 140530045396736 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.48069217801094055, loss=3.5548324584960938
I0206 12:52:00.982947 140530037004032 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.4794752895832062, loss=3.566842555999756
I0206 12:52:36.305359 140530045396736 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.5369073748588562, loss=3.609464168548584
I0206 12:53:11.602922 140530037004032 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.476475328207016, loss=3.5750646591186523
I0206 12:53:46.896543 140530045396736 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.456153005361557, loss=3.5971381664276123
I0206 12:54:22.196181 140530037004032 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.48070892691612244, loss=3.5512733459472656
I0206 12:54:57.469333 140530045396736 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.42656776309013367, loss=3.455396890640259
I0206 12:55:32.753310 140530037004032 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.44946497678756714, loss=3.4567136764526367
I0206 12:56:08.042018 140530045396736 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.4927053153514862, loss=3.5364625453948975
I0206 12:56:43.364467 140530037004032 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.5001598000526428, loss=3.460155487060547
I0206 12:57:18.793233 140530045396736 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.41544628143310547, loss=3.5212478637695312
I0206 12:57:54.125383 140530037004032 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.40347525477409363, loss=3.553232192993164
I0206 12:58:08.662417 140699726837568 spec.py:321] Evaluating on the training split.
I0206 12:58:11.698286 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 13:00:52.059178 140699726837568 spec.py:333] Evaluating on the validation split.
I0206 13:00:54.802973 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 13:03:31.981353 140699726837568 spec.py:349] Evaluating on the test split.
I0206 13:03:34.725111 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 13:06:01.793396 140699726837568 submission_runner.py:408] Time since start: 5180.99s, 	Step: 7143, 	{'train/accuracy': 0.5830803513526917, 'train/loss': 2.3459420204162598, 'train/bleu': 27.624996169924223, 'validation/accuracy': 0.5854235887527466, 'validation/loss': 2.320286512374878, 'validation/bleu': 23.295886495274086, 'validation/num_examples': 3000, 'test/accuracy': 0.588565468788147, 'test/loss': 2.322375535964966, 'test/bleu': 21.92302229850635, 'test/num_examples': 3003, 'score': 2557.290193080902, 'total_duration': 5180.989482164383, 'accumulated_submission_time': 2557.290193080902, 'accumulated_eval_time': 2623.3933651447296, 'accumulated_logging_time': 0.08187294006347656}
I0206 13:06:01.822655 140530045396736 logging_writer.py:48] [7143] accumulated_eval_time=2623.393365, accumulated_logging_time=0.081873, accumulated_submission_time=2557.290193, global_step=7143, preemption_count=0, score=2557.290193, test/accuracy=0.588565, test/bleu=21.923022, test/loss=2.322376, test/num_examples=3003, total_duration=5180.989482, train/accuracy=0.583080, train/bleu=27.624996, train/loss=2.345942, validation/accuracy=0.585424, validation/bleu=23.295886, validation/loss=2.320287, validation/num_examples=3000
I0206 13:06:22.227433 140530037004032 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.3904182016849518, loss=3.4755373001098633
I0206 13:06:57.455456 140530045396736 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.3966487944126129, loss=3.41941499710083
I0206 13:07:32.733786 140530037004032 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.46487587690353394, loss=3.4643020629882812
I0206 13:08:08.017364 140530045396736 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.4032898545265198, loss=3.4444713592529297
I0206 13:08:43.266967 140530037004032 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.4016972482204437, loss=3.5080060958862305
I0206 13:09:18.537145 140530045396736 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.48781827092170715, loss=3.3937792778015137
I0206 13:09:53.781986 140530037004032 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.4109492897987366, loss=3.3183865547180176
I0206 13:10:29.058224 140530045396736 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.37303248047828674, loss=3.4546713829040527
I0206 13:11:04.393720 140530037004032 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.39241859316825867, loss=3.393397092819214
I0206 13:11:39.697706 140530045396736 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.3264436721801758, loss=3.3318376541137695
I0206 13:12:14.984620 140530037004032 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.4311843812465668, loss=3.320119619369507
I0206 13:12:50.247748 140530045396736 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.4109684228897095, loss=3.457531452178955
I0206 13:13:25.557265 140530037004032 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.3665447235107422, loss=3.393843173980713
I0206 13:14:00.813458 140530045396736 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.3580620586872101, loss=3.352966070175171
I0206 13:14:36.078942 140530037004032 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.5137873888015747, loss=3.4066696166992188
I0206 13:15:11.345957 140530045396736 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.3479882478713989, loss=3.3530335426330566
I0206 13:15:46.619771 140530037004032 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.36293908953666687, loss=3.3226799964904785
I0206 13:16:21.879716 140530045396736 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.36497050523757935, loss=3.3465511798858643
I0206 13:16:57.147572 140530037004032 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.3630129396915436, loss=3.399303913116455
I0206 13:17:32.410300 140530045396736 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.34425410628318787, loss=3.3565049171447754
I0206 13:18:07.651533 140530037004032 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.362349271774292, loss=3.349674940109253
I0206 13:18:42.899122 140530045396736 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.3140905499458313, loss=3.2829973697662354
I0206 13:19:18.215010 140530037004032 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.289376437664032, loss=3.3253777027130127
I0206 13:19:53.488985 140530045396736 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.3116551339626312, loss=3.296175956726074
I0206 13:20:02.031112 140699726837568 spec.py:321] Evaluating on the training split.
I0206 13:20:05.060881 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 13:22:48.892601 140699726837568 spec.py:333] Evaluating on the validation split.
I0206 13:22:51.614272 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 13:25:22.897216 140699726837568 spec.py:349] Evaluating on the test split.
I0206 13:25:25.631934 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 13:27:51.976970 140699726837568 submission_runner.py:408] Time since start: 6491.17s, 	Step: 9526, 	{'train/accuracy': 0.5969531536102295, 'train/loss': 2.222651481628418, 'train/bleu': 28.03945898046316, 'validation/accuracy': 0.6077544093132019, 'validation/loss': 2.134187936782837, 'validation/bleu': 24.906629328826106, 'validation/num_examples': 3000, 'test/accuracy': 0.6118645071983337, 'test/loss': 2.1073811054229736, 'test/bleu': 23.48735964868124, 'test/num_examples': 3003, 'score': 3397.4100873470306, 'total_duration': 6491.173132181168, 'accumulated_submission_time': 3397.4100873470306, 'accumulated_eval_time': 3093.3391761779785, 'accumulated_logging_time': 0.12326359748840332}
I0206 13:27:51.995899 140530037004032 logging_writer.py:48] [9526] accumulated_eval_time=3093.339176, accumulated_logging_time=0.123264, accumulated_submission_time=3397.410087, global_step=9526, preemption_count=0, score=3397.410087, test/accuracy=0.611865, test/bleu=23.487360, test/loss=2.107381, test/num_examples=3003, total_duration=6491.173132, train/accuracy=0.596953, train/bleu=28.039459, train/loss=2.222651, validation/accuracy=0.607754, validation/bleu=24.906629, validation/loss=2.134188, validation/num_examples=3000
I0206 13:28:18.382766 140530045396736 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.3201828896999359, loss=3.2531349658966064
I0206 13:28:53.579058 140530037004032 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.3157981038093567, loss=3.2381081581115723
I0206 13:29:28.810452 140530045396736 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.30727991461753845, loss=3.2692129611968994
I0206 13:30:04.039714 140530037004032 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.2848215699195862, loss=3.3776731491088867
I0206 13:30:39.308319 140530045396736 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.31163889169692993, loss=3.3014276027679443
I0206 13:31:14.569053 140530037004032 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.27995046973228455, loss=3.2609128952026367
I0206 13:31:49.884118 140530045396736 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.27917876839637756, loss=3.2536544799804688
I0206 13:32:25.196779 140530037004032 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.27647560834884644, loss=3.2700035572052
I0206 13:33:00.528378 140530045396736 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.32421669363975525, loss=3.25555157661438
I0206 13:33:35.780534 140530037004032 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.2838292419910431, loss=3.213202714920044
I0206 13:34:11.013697 140530045396736 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.28647008538246155, loss=3.3121278285980225
I0206 13:34:46.242236 140530037004032 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.29090872406959534, loss=3.2527546882629395
I0206 13:35:21.481903 140530045396736 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.2691494822502136, loss=3.213343620300293
I0206 13:35:56.737982 140530037004032 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.36416932940483093, loss=3.3426260948181152
I0206 13:36:31.996746 140530045396736 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.27612024545669556, loss=3.3044369220733643
I0206 13:37:07.281323 140530037004032 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.2965450584888458, loss=3.2576560974121094
I0206 13:37:42.529749 140530045396736 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.29500940442085266, loss=3.168048143386841
I0206 13:38:17.792399 140530037004032 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.2769753336906433, loss=3.3143105506896973
I0206 13:38:53.032301 140530045396736 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.26173195242881775, loss=3.169797658920288
I0206 13:39:28.292892 140530037004032 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.2632492482662201, loss=3.325124740600586
I0206 13:40:03.553373 140530045396736 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.24885252118110657, loss=3.2233896255493164
I0206 13:40:38.865593 140530037004032 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.2436894029378891, loss=3.1846415996551514
I0206 13:41:14.174489 140530045396736 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.24030661582946777, loss=3.237229108810425
I0206 13:41:49.570800 140530037004032 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.26261571049690247, loss=3.229592800140381
I0206 13:41:52.113430 140699726837568 spec.py:321] Evaluating on the training split.
I0206 13:41:55.162989 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 13:44:28.703963 140699726837568 spec.py:333] Evaluating on the validation split.
I0206 13:44:31.457452 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 13:46:58.844134 140699726837568 spec.py:349] Evaluating on the test split.
I0206 13:47:01.615153 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 13:49:17.516030 140699726837568 submission_runner.py:408] Time since start: 7776.71s, 	Step: 11909, 	{'train/accuracy': 0.6016323566436768, 'train/loss': 2.1511049270629883, 'train/bleu': 28.773179468343937, 'validation/accuracy': 0.621133029460907, 'validation/loss': 2.0083134174346924, 'validation/bleu': 25.848073621681074, 'validation/num_examples': 3000, 'test/accuracy': 0.6260995864868164, 'test/loss': 1.976151943206787, 'test/bleu': 24.403812250497207, 'test/num_examples': 3003, 'score': 4237.438687324524, 'total_duration': 7776.712178230286, 'accumulated_submission_time': 4237.438687324524, 'accumulated_eval_time': 3538.7417256832123, 'accumulated_logging_time': 0.15330171585083008}
I0206 13:49:17.532481 140530045396736 logging_writer.py:48] [11909] accumulated_eval_time=3538.741726, accumulated_logging_time=0.153302, accumulated_submission_time=4237.438687, global_step=11909, preemption_count=0, score=4237.438687, test/accuracy=0.626100, test/bleu=24.403812, test/loss=1.976152, test/num_examples=3003, total_duration=7776.712178, train/accuracy=0.601632, train/bleu=28.773179, train/loss=2.151105, validation/accuracy=0.621133, validation/bleu=25.848074, validation/loss=2.008313, validation/num_examples=3000
I0206 13:49:49.907256 140530037004032 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.29474568367004395, loss=3.347916841506958
I0206 13:50:25.096155 140530045396736 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.28152039647102356, loss=3.1662917137145996
I0206 13:51:00.303117 140530037004032 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.2526077330112457, loss=3.131561756134033
I0206 13:51:35.545882 140530045396736 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.22466982901096344, loss=3.2245185375213623
I0206 13:52:10.798790 140530037004032 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.23433098196983337, loss=3.217784881591797
I0206 13:52:46.056496 140530045396736 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.2645937204360962, loss=3.156466007232666
I0206 13:53:21.319333 140530037004032 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.2586957514286041, loss=3.1878583431243896
I0206 13:53:56.535058 140530045396736 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.23580370843410492, loss=3.230319023132324
I0206 13:54:31.776570 140530037004032 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.23004725575447083, loss=3.2306761741638184
I0206 13:55:07.089091 140530045396736 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.21852973103523254, loss=3.193085193634033
I0206 13:55:42.291539 140530037004032 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.2305431067943573, loss=3.192587375640869
I0206 13:56:17.578515 140530045396736 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.2622794508934021, loss=3.192821741104126
I0206 13:56:52.839703 140530037004032 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.27685847878456116, loss=3.206820011138916
I0206 13:57:28.057276 140530045396736 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.2544097304344177, loss=3.210832118988037
I0206 13:58:03.312211 140530037004032 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.28527599573135376, loss=3.189746379852295
I0206 13:58:38.557344 140530045396736 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.2369382530450821, loss=3.18603253364563
I0206 13:59:13.871184 140530037004032 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.2709355354309082, loss=3.1309425830841064
I0206 13:59:49.157509 140530045396736 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.29563868045806885, loss=3.236905813217163
I0206 14:00:24.414808 140530037004032 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.25903818011283875, loss=3.2215328216552734
I0206 14:00:59.664072 140530045396736 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.25759226083755493, loss=3.1282291412353516
I0206 14:01:34.890472 140530037004032 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.3129357099533081, loss=3.1826305389404297
I0206 14:02:10.097877 140530045396736 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.23682840168476105, loss=3.1476502418518066
I0206 14:02:45.325751 140530037004032 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.28324300050735474, loss=3.1184699535369873
I0206 14:03:17.815243 140699726837568 spec.py:321] Evaluating on the training split.
I0206 14:03:20.835221 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 14:06:11.621873 140699726837568 spec.py:333] Evaluating on the validation split.
I0206 14:06:14.354077 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 14:08:44.963069 140699726837568 spec.py:349] Evaluating on the test split.
I0206 14:08:47.698693 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 14:11:18.355769 140699726837568 submission_runner.py:408] Time since start: 9097.55s, 	Step: 14294, 	{'train/accuracy': 0.6140919923782349, 'train/loss': 2.048624277114868, 'train/bleu': 29.75643167533389, 'validation/accuracy': 0.6318830251693726, 'validation/loss': 1.9232735633850098, 'validation/bleu': 26.592895269365595, 'validation/num_examples': 3000, 'test/accuracy': 0.6379757523536682, 'test/loss': 1.8753560781478882, 'test/bleu': 25.565374635606645, 'test/num_examples': 3003, 'score': 5077.634784460068, 'total_duration': 9097.55193066597, 'accumulated_submission_time': 5077.634784460068, 'accumulated_eval_time': 4019.282205581665, 'accumulated_logging_time': 0.18139338493347168}
I0206 14:11:18.372227 140530045396736 logging_writer.py:48] [14294] accumulated_eval_time=4019.282206, accumulated_logging_time=0.181393, accumulated_submission_time=5077.634784, global_step=14294, preemption_count=0, score=5077.634784, test/accuracy=0.637976, test/bleu=25.565375, test/loss=1.875356, test/num_examples=3003, total_duration=9097.551931, train/accuracy=0.614092, train/bleu=29.756432, train/loss=2.048624, validation/accuracy=0.631883, validation/bleu=26.592895, validation/loss=1.923274, validation/num_examples=3000
I0206 14:11:20.845549 140530037004032 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.2322092056274414, loss=3.1899964809417725
I0206 14:11:55.998662 140530045396736 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.2708751857280731, loss=3.107969045639038
I0206 14:12:31.186812 140530037004032 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.25997254252433777, loss=3.089475631713867
I0206 14:13:06.463020 140530045396736 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.2817271947860718, loss=3.167635440826416
I0206 14:13:41.691284 140530037004032 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.30205902457237244, loss=3.159785747528076
I0206 14:14:16.947469 140530045396736 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.28035664558410645, loss=3.118185520172119
I0206 14:14:52.198899 140530037004032 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.26448431611061096, loss=3.220829963684082
I0206 14:15:27.425259 140530045396736 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.26727041602134705, loss=3.138925790786743
I0206 14:16:02.692209 140530037004032 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.25909194350242615, loss=3.1525723934173584
I0206 14:16:37.956221 140530045396736 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.23611843585968018, loss=3.0794007778167725
I0206 14:17:13.232454 140530037004032 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.2542767822742462, loss=3.1745333671569824
I0206 14:17:48.515396 140530045396736 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.2684261202812195, loss=3.1804118156433105
I0206 14:18:23.739663 140530037004032 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.26136425137519836, loss=3.1345036029815674
I0206 14:18:58.976603 140530045396736 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.24567528069019318, loss=3.1148929595947266
I0206 14:19:34.228218 140530037004032 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.2788020670413971, loss=3.1450202465057373
I0206 14:20:09.487422 140530045396736 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.330074667930603, loss=3.0748205184936523
I0206 14:20:44.769948 140530037004032 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.25828272104263306, loss=3.0722036361694336
I0206 14:21:20.009530 140530045396736 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.25774112343788147, loss=3.107179641723633
I0206 14:21:55.259750 140530037004032 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.30202704668045044, loss=3.0791120529174805
I0206 14:22:30.503249 140530045396736 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.28144192695617676, loss=3.110780954360962
I0206 14:23:05.769692 140530037004032 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.2656702697277069, loss=3.073157787322998
I0206 14:23:41.031647 140530045396736 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.33252570033073425, loss=3.0902485847473145
I0206 14:24:16.253417 140530037004032 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.2826189398765564, loss=3.1422204971313477
I0206 14:24:51.490140 140530045396736 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.274467408657074, loss=3.0981192588806152
I0206 14:25:18.690288 140699726837568 spec.py:321] Evaluating on the training split.
I0206 14:25:21.715389 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 14:28:01.876616 140699726837568 spec.py:333] Evaluating on the validation split.
I0206 14:28:04.618355 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 14:30:31.421341 140699726837568 spec.py:349] Evaluating on the test split.
I0206 14:30:34.149981 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 14:32:58.070689 140699726837568 submission_runner.py:408] Time since start: 10397.27s, 	Step: 16679, 	{'train/accuracy': 0.6197834014892578, 'train/loss': 2.003021001815796, 'train/bleu': 29.712456080288373, 'validation/accuracy': 0.6387149691581726, 'validation/loss': 1.8588805198669434, 'validation/bleu': 26.800316120447345, 'validation/num_examples': 3000, 'test/accuracy': 0.6479925513267517, 'test/loss': 1.8052959442138672, 'test/bleu': 25.933641625620712, 'test/num_examples': 3003, 'score': 5917.868166685104, 'total_duration': 10397.26683807373, 'accumulated_submission_time': 5917.868166685104, 'accumulated_eval_time': 4478.662546873093, 'accumulated_logging_time': 0.20989251136779785}
I0206 14:32:58.087199 140530037004032 logging_writer.py:48] [16679] accumulated_eval_time=4478.662547, accumulated_logging_time=0.209893, accumulated_submission_time=5917.868167, global_step=16679, preemption_count=0, score=5917.868167, test/accuracy=0.647993, test/bleu=25.933642, test/loss=1.805296, test/num_examples=3003, total_duration=10397.266838, train/accuracy=0.619783, train/bleu=29.712456, train/loss=2.003021, validation/accuracy=0.638715, validation/bleu=26.800316, validation/loss=1.858881, validation/num_examples=3000
I0206 14:33:05.830074 140530045396736 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.2888931632041931, loss=3.050753116607666
I0206 14:33:40.986589 140530037004032 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.2749777138233185, loss=3.1069746017456055
I0206 14:34:16.204277 140530045396736 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.3153204023838043, loss=3.086127519607544
I0206 14:34:51.446563 140530037004032 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.26215890049934387, loss=3.0846478939056396
I0206 14:35:26.667572 140530045396736 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.2608760893344879, loss=3.0877835750579834
I0206 14:36:01.881357 140530037004032 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.38158664107322693, loss=3.1546082496643066
I0206 14:36:37.107358 140530045396736 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.28574877977371216, loss=3.0708751678466797
I0206 14:37:12.353332 140530037004032 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.2934042513370514, loss=3.061901330947876
I0206 14:37:47.613954 140530045396736 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.362882137298584, loss=3.173222064971924
I0206 14:38:22.871002 140530037004032 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.33567115664482117, loss=3.085005760192871
I0206 14:38:58.103306 140530045396736 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.2683674395084381, loss=3.0868396759033203
I0206 14:39:33.346387 140530037004032 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.30853071808815, loss=3.0854246616363525
I0206 14:40:08.607677 140530045396736 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.30090010166168213, loss=3.057750701904297
I0206 14:40:43.849487 140530037004032 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.2755746841430664, loss=2.9637722969055176
I0206 14:41:19.107635 140530045396736 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.3064644932746887, loss=3.088937282562256
I0206 14:41:54.356826 140530037004032 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.280497282743454, loss=3.1207468509674072
I0206 14:42:29.592377 140530045396736 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.3560371994972229, loss=3.036036252975464
I0206 14:43:04.820003 140530037004032 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.34708738327026367, loss=3.08880615234375
I0206 14:43:40.109302 140530045396736 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.28910118341445923, loss=3.057945728302002
I0206 14:44:15.370461 140530037004032 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.2798194885253906, loss=3.048734664916992
I0206 14:44:50.591104 140530045396736 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.3917860686779022, loss=3.049938678741455
I0206 14:45:25.929883 140530037004032 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.31411412358283997, loss=3.102541208267212
I0206 14:46:01.145865 140530045396736 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.2981763780117035, loss=3.0545883178710938
I0206 14:46:36.381272 140530037004032 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.3119215667247772, loss=3.046959161758423
I0206 14:46:58.284808 140699726837568 spec.py:321] Evaluating on the training split.
I0206 14:47:01.318650 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 14:51:34.202649 140699726837568 spec.py:333] Evaluating on the validation split.
I0206 14:51:36.928533 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 14:55:50.105345 140699726837568 spec.py:349] Evaluating on the test split.
I0206 14:55:52.832513 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 15:00:03.359036 140699726837568 submission_runner.py:408] Time since start: 12022.56s, 	Step: 19064, 	{'train/accuracy': 0.6391572952270508, 'train/loss': 1.8542532920837402, 'train/bleu': 31.17832988950659, 'validation/accuracy': 0.6441581845283508, 'validation/loss': 1.8158868551254272, 'validation/bleu': 27.1275590231522, 'validation/num_examples': 3000, 'test/accuracy': 0.652710497379303, 'test/loss': 1.761230707168579, 'test/bleu': 26.45541676211269, 'test/num_examples': 3003, 'score': 6757.979300022125, 'total_duration': 12022.555188179016, 'accumulated_submission_time': 6757.979300022125, 'accumulated_eval_time': 5263.736715555191, 'accumulated_logging_time': 0.23669672012329102}
I0206 15:00:03.376268 140530045396736 logging_writer.py:48] [19064] accumulated_eval_time=5263.736716, accumulated_logging_time=0.236697, accumulated_submission_time=6757.979300, global_step=19064, preemption_count=0, score=6757.979300, test/accuracy=0.652710, test/bleu=26.455417, test/loss=1.761231, test/num_examples=3003, total_duration=12022.555188, train/accuracy=0.639157, train/bleu=31.178330, train/loss=1.854253, validation/accuracy=0.644158, validation/bleu=27.127559, validation/loss=1.815887, validation/num_examples=3000
I0206 15:00:16.351314 140530037004032 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.35315337777137756, loss=3.1066455841064453
I0206 15:00:51.467775 140530045396736 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.3057476282119751, loss=3.0781056880950928
I0206 15:01:26.624455 140530037004032 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.31463974714279175, loss=3.0373053550720215
I0206 15:02:01.823618 140530045396736 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.3130340278148651, loss=3.111006498336792
I0206 15:02:37.050180 140530037004032 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.30843716859817505, loss=3.0227749347686768
I0206 15:03:12.265359 140530045396736 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.3308359384536743, loss=3.0976758003234863
I0206 15:03:47.488305 140530037004032 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.3440912365913391, loss=3.0934953689575195
I0206 15:04:22.737688 140530045396736 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.3333207368850708, loss=3.088021993637085
I0206 15:04:57.948356 140530037004032 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.3366925120353699, loss=3.016575813293457
I0206 15:05:33.195191 140530045396736 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.3148375451564789, loss=3.0588014125823975
I0206 15:06:08.432435 140530037004032 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.3024396300315857, loss=3.113312005996704
I0206 15:06:43.720667 140530045396736 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.3101504147052765, loss=2.976959466934204
I0206 15:07:18.998810 140530037004032 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.4300287067890167, loss=2.982455015182495
I0206 15:07:54.245491 140530045396736 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.33212852478027344, loss=3.0876107215881348
I0206 15:08:29.458540 140530037004032 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.3801664412021637, loss=3.0269789695739746
I0206 15:09:04.690439 140530045396736 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.3313574492931366, loss=2.938300371170044
I0206 15:09:39.905185 140530037004032 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.33878108859062195, loss=3.074281930923462
I0206 15:10:15.218155 140530045396736 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.349540114402771, loss=3.036240816116333
I0206 15:10:50.431036 140530037004032 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.3765331506729126, loss=3.0598971843719482
I0206 15:11:25.670930 140530045396736 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.43764838576316833, loss=2.997619867324829
I0206 15:12:00.903334 140530037004032 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.45227622985839844, loss=2.9881906509399414
I0206 15:12:36.166598 140530045396736 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.38100266456604004, loss=3.060641288757324
I0206 15:13:11.395515 140530037004032 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.3555727005004883, loss=3.0639467239379883
I0206 15:13:46.642431 140530045396736 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.3855621814727783, loss=3.0641658306121826
I0206 15:14:03.657279 140699726837568 spec.py:321] Evaluating on the training split.
I0206 15:14:06.701173 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 15:17:41.013304 140699726837568 spec.py:333] Evaluating on the validation split.
I0206 15:17:43.733390 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 15:20:17.109377 140699726837568 spec.py:349] Evaluating on the test split.
I0206 15:20:19.832603 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 15:22:38.787109 140699726837568 submission_runner.py:408] Time since start: 13377.98s, 	Step: 21450, 	{'train/accuracy': 0.6265982985496521, 'train/loss': 1.9437370300292969, 'train/bleu': 30.379573470875417, 'validation/accuracy': 0.6476795077323914, 'validation/loss': 1.7847602367401123, 'validation/bleu': 27.428115773451797, 'validation/num_examples': 3000, 'test/accuracy': 0.6565220355987549, 'test/loss': 1.7380309104919434, 'test/bleu': 26.60739545980313, 'test/num_examples': 3003, 'score': 7598.178004980087, 'total_duration': 13377.983260631561, 'accumulated_submission_time': 7598.178004980087, 'accumulated_eval_time': 5778.866499423981, 'accumulated_logging_time': 0.2639615535736084}
I0206 15:22:38.804834 140530037004032 logging_writer.py:48] [21450] accumulated_eval_time=5778.866499, accumulated_logging_time=0.263962, accumulated_submission_time=7598.178005, global_step=21450, preemption_count=0, score=7598.178005, test/accuracy=0.656522, test/bleu=26.607395, test/loss=1.738031, test/num_examples=3003, total_duration=13377.983261, train/accuracy=0.626598, train/bleu=30.379573, train/loss=1.943737, validation/accuracy=0.647680, validation/bleu=27.428116, validation/loss=1.784760, validation/num_examples=3000
I0206 15:22:56.707200 140530045396736 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.3717944622039795, loss=3.064845323562622
I0206 15:23:31.869963 140530037004032 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.42557594180107117, loss=3.077219009399414
I0206 15:24:07.093277 140530045396736 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.35815006494522095, loss=3.0294575691223145
I0206 15:24:42.292195 140530037004032 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.27671337127685547, loss=2.994588613510132
I0206 15:25:17.492355 140530045396736 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.34195050597190857, loss=2.9848926067352295
I0206 15:25:52.695809 140530037004032 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.3927302956581116, loss=3.077578544616699
I0206 15:26:27.944197 140530045396736 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.44038039445877075, loss=2.967595338821411
I0206 15:27:03.195790 140530037004032 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.33082127571105957, loss=3.009199857711792
I0206 15:27:38.405925 140530045396736 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.36169537901878357, loss=3.018928289413452
I0206 15:28:13.614531 140530037004032 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.377748966217041, loss=3.0195038318634033
I0206 15:28:48.823404 140530045396736 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.3122536242008209, loss=2.9824886322021484
I0206 15:29:24.038199 140530037004032 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.41020044684410095, loss=3.047315835952759
I0206 15:29:59.268458 140530045396736 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.40999481081962585, loss=2.992467164993286
I0206 15:30:34.502201 140530037004032 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.3852939009666443, loss=2.973644256591797
I0206 15:31:09.736233 140530045396736 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.3585546016693115, loss=3.029787540435791
I0206 15:31:44.974143 140530037004032 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.33851054310798645, loss=2.998307228088379
I0206 15:32:20.192736 140530045396736 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.3812313675880432, loss=2.9847564697265625
I0206 15:32:55.455418 140530037004032 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.42833372950553894, loss=3.0551929473876953
I0206 15:33:30.704335 140530045396736 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.3780945837497711, loss=3.0847747325897217
I0206 15:34:05.946074 140530037004032 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.3272053003311157, loss=3.0699095726013184
I0206 15:34:41.148937 140530045396736 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.33536297082901, loss=2.9503965377807617
I0206 15:35:16.369086 140530037004032 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.39982491731643677, loss=3.006531238555908
I0206 15:35:51.602428 140530045396736 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.33580976724624634, loss=2.9440505504608154
I0206 15:36:26.870552 140530037004032 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.40023142099380493, loss=2.9925894737243652
I0206 15:36:38.939772 140699726837568 spec.py:321] Evaluating on the training split.
I0206 15:36:41.978234 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 15:40:03.591636 140699726837568 spec.py:333] Evaluating on the validation split.
I0206 15:40:06.322345 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 15:42:38.714875 140699726837568 spec.py:349] Evaluating on the test split.
I0206 15:42:41.443917 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 15:45:03.789093 140699726837568 submission_runner.py:408] Time since start: 14722.99s, 	Step: 23836, 	{'train/accuracy': 0.6263420581817627, 'train/loss': 1.9479836225509644, 'train/bleu': 30.605509374050456, 'validation/accuracy': 0.6493533849716187, 'validation/loss': 1.7742805480957031, 'validation/bleu': 27.501190326452022, 'validation/num_examples': 3000, 'test/accuracy': 0.6588460803031921, 'test/loss': 1.7223812341690063, 'test/bleu': 26.958053620503918, 'test/num_examples': 3003, 'score': 8438.228722810745, 'total_duration': 14722.985248565674, 'accumulated_submission_time': 8438.228722810745, 'accumulated_eval_time': 6283.715788841248, 'accumulated_logging_time': 0.291827917098999}
I0206 15:45:03.808660 140530045396736 logging_writer.py:48] [23836] accumulated_eval_time=6283.715789, accumulated_logging_time=0.291828, accumulated_submission_time=8438.228723, global_step=23836, preemption_count=0, score=8438.228723, test/accuracy=0.658846, test/bleu=26.958054, test/loss=1.722381, test/num_examples=3003, total_duration=14722.985249, train/accuracy=0.626342, train/bleu=30.605509, train/loss=1.947984, validation/accuracy=0.649353, validation/bleu=27.501190, validation/loss=1.774281, validation/num_examples=3000
I0206 15:45:26.649276 140530037004032 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.3240285813808441, loss=3.0378432273864746
I0206 15:46:01.789927 140530045396736 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.3233148455619812, loss=2.936466932296753
I0206 15:46:36.966576 140530037004032 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.32722458243370056, loss=2.9843688011169434
I0206 15:47:12.201427 140530045396736 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.3325059711933136, loss=3.046513080596924
I0206 15:47:47.397461 140530037004032 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.4051372706890106, loss=3.0645864009857178
I0206 15:48:22.629791 140530045396736 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.35592666268348694, loss=3.022026300430298
I0206 15:48:57.848030 140530037004032 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.4083302319049835, loss=3.0220797061920166
I0206 15:49:33.078358 140530045396736 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.4400894343852997, loss=3.0316498279571533
I0206 15:50:08.286067 140530037004032 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.37046483159065247, loss=3.0550990104675293
I0206 15:50:43.553083 140530045396736 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.3827803432941437, loss=3.0460073947906494
I0206 15:51:18.839873 140530037004032 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.3369603157043457, loss=3.0206737518310547
I0206 15:51:54.102701 140530045396736 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.35426127910614014, loss=2.9651501178741455
I0206 15:52:29.349561 140530037004032 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.3341224491596222, loss=3.0678515434265137
I0206 15:53:04.618001 140530045396736 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.38567814230918884, loss=3.005232334136963
I0206 15:53:39.853999 140530037004032 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.3805120885372162, loss=2.9775846004486084
I0206 15:54:15.118125 140530045396736 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.4165617823600769, loss=3.013960123062134
I0206 15:54:50.364979 140530037004032 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.3403455913066864, loss=3.0283896923065186
I0206 15:55:25.604962 140530045396736 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.33894675970077515, loss=3.001138687133789
I0206 15:56:00.866914 140530037004032 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.3565841019153595, loss=3.0788321495056152
I0206 15:56:36.157762 140530045396736 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.329195111989975, loss=3.0318963527679443
I0206 15:57:11.444618 140530037004032 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.34727737307548523, loss=3.000882148742676
I0206 15:57:46.685247 140530045396736 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.38669297099113464, loss=3.014031171798706
I0206 15:58:21.912418 140530037004032 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.41834133863449097, loss=3.0316145420074463
I0206 15:58:57.143001 140530045396736 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.40925151109695435, loss=3.0484535694122314
I0206 15:59:03.909416 140699726837568 spec.py:321] Evaluating on the training split.
I0206 15:59:06.930663 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 16:01:55.184927 140699726837568 spec.py:333] Evaluating on the validation split.
I0206 16:01:57.903848 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 16:04:27.172888 140699726837568 spec.py:349] Evaluating on the test split.
I0206 16:04:29.898026 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 16:06:52.880530 140699726837568 submission_runner.py:408] Time since start: 16032.08s, 	Step: 26221, 	{'train/accuracy': 0.6345329880714417, 'train/loss': 1.8784587383270264, 'train/bleu': 31.028171095176663, 'validation/accuracy': 0.65301114320755, 'validation/loss': 1.7509132623672485, 'validation/bleu': 27.968816101277366, 'validation/num_examples': 3000, 'test/accuracy': 0.665864884853363, 'test/loss': 1.6981230974197388, 'test/bleu': 27.718216191957513, 'test/num_examples': 3003, 'score': 9278.243922948837, 'total_duration': 16032.076689481735, 'accumulated_submission_time': 9278.243922948837, 'accumulated_eval_time': 6752.686856031418, 'accumulated_logging_time': 0.32169675827026367}
I0206 16:06:52.899191 140530037004032 logging_writer.py:48] [26221] accumulated_eval_time=6752.686856, accumulated_logging_time=0.321697, accumulated_submission_time=9278.243923, global_step=26221, preemption_count=0, score=9278.243923, test/accuracy=0.665865, test/bleu=27.718216, test/loss=1.698123, test/num_examples=3003, total_duration=16032.076689, train/accuracy=0.634533, train/bleu=31.028171, train/loss=1.878459, validation/accuracy=0.653011, validation/bleu=27.968816, validation/loss=1.750913, validation/num_examples=3000
I0206 16:07:21.012680 140530045396736 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.38228151202201843, loss=3.016949415206909
I0206 16:07:56.168561 140530037004032 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.3621242344379425, loss=2.949141502380371
I0206 16:08:31.349556 140530045396736 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.3427478075027466, loss=2.9629015922546387
I0206 16:09:06.539129 140530037004032 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.3435697555541992, loss=2.945889711380005
I0206 16:09:41.728374 140530045396736 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.3988801836967468, loss=2.979123592376709
I0206 16:10:16.941655 140530037004032 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.35193225741386414, loss=2.946737766265869
I0206 16:10:52.171881 140530045396736 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.36080706119537354, loss=2.9697844982147217
I0206 16:11:27.391351 140530037004032 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.38443905115127563, loss=3.0084853172302246
I0206 16:12:02.621925 140530045396736 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.39885327219963074, loss=2.9900779724121094
I0206 16:12:37.813180 140530037004032 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.3324306607246399, loss=2.9502103328704834
I0206 16:13:13.034575 140530045396736 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.31935203075408936, loss=2.987672805786133
I0206 16:13:48.247571 140530037004032 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.3783167600631714, loss=2.97727108001709
I0206 16:14:23.458301 140530045396736 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.3610716760158539, loss=3.0225071907043457
I0206 16:14:58.708267 140530037004032 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.412600576877594, loss=2.9598944187164307
I0206 16:15:33.970791 140530045396736 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.3813192844390869, loss=3.0001230239868164
I0206 16:16:09.268840 140530037004032 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.35422244668006897, loss=2.964261054992676
I0206 16:16:44.488817 140530045396736 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.4213303327560425, loss=3.0193283557891846
I0206 16:17:19.745871 140530037004032 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.4505731463432312, loss=3.0338757038116455
I0206 16:17:55.002529 140530045396736 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.3838483393192291, loss=3.0126781463623047
I0206 16:18:30.242765 140530037004032 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.4110446870326996, loss=3.0363681316375732
I0206 16:19:05.464780 140530045396736 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.33938509225845337, loss=2.9911789894104004
I0206 16:19:40.696057 140530037004032 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.4026889503002167, loss=2.998410940170288
I0206 16:20:15.915489 140530045396736 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.3845576345920563, loss=2.9753427505493164
I0206 16:20:51.098479 140530037004032 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.4082239866256714, loss=2.983940601348877
I0206 16:20:52.928828 140699726837568 spec.py:321] Evaluating on the training split.
I0206 16:20:55.964565 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 16:23:41.980837 140699726837568 spec.py:333] Evaluating on the validation split.
I0206 16:23:44.705052 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 16:26:13.456861 140699726837568 spec.py:349] Evaluating on the test split.
I0206 16:26:16.188174 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 16:28:34.305847 140699726837568 submission_runner.py:408] Time since start: 17333.50s, 	Step: 28607, 	{'train/accuracy': 0.6294631361961365, 'train/loss': 1.9184149503707886, 'train/bleu': 30.843768750457446, 'validation/accuracy': 0.6532467007637024, 'validation/loss': 1.749211311340332, 'validation/bleu': 27.845418699552212, 'validation/num_examples': 3000, 'test/accuracy': 0.6636918187141418, 'test/loss': 1.6925480365753174, 'test/bleu': 26.978583127780233, 'test/num_examples': 3003, 'score': 10118.190644741058, 'total_duration': 17333.501986265182, 'accumulated_submission_time': 10118.190644741058, 'accumulated_eval_time': 7214.063798904419, 'accumulated_logging_time': 0.3504965305328369}
I0206 16:28:34.324439 140530045396736 logging_writer.py:48] [28607] accumulated_eval_time=7214.063799, accumulated_logging_time=0.350497, accumulated_submission_time=10118.190645, global_step=28607, preemption_count=0, score=10118.190645, test/accuracy=0.663692, test/bleu=26.978583, test/loss=1.692548, test/num_examples=3003, total_duration=17333.501986, train/accuracy=0.629463, train/bleu=30.843769, train/loss=1.918415, validation/accuracy=0.653247, validation/bleu=27.845419, validation/loss=1.749211, validation/num_examples=3000
I0206 16:29:07.362414 140530037004032 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.46262505650520325, loss=2.937596082687378
I0206 16:29:42.487630 140530045396736 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.3400551378726959, loss=3.0252177715301514
I0206 16:30:17.654407 140530037004032 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.4489661753177643, loss=2.939940929412842
I0206 16:30:52.841722 140530045396736 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.3325673043727875, loss=3.077343463897705
I0206 16:31:28.041797 140530037004032 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.37524834275245667, loss=2.9218759536743164
I0206 16:32:03.264019 140530045396736 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.31646788120269775, loss=2.9791791439056396
I0206 16:32:38.503252 140530037004032 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.3463689088821411, loss=2.992337703704834
I0206 16:33:13.692931 140530045396736 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.3507763743400574, loss=3.0821568965911865
I0206 16:33:48.909485 140530037004032 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.39910969138145447, loss=3.0604043006896973
I0206 16:34:24.134986 140530045396736 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.4187154471874237, loss=2.9941248893737793
I0206 16:34:59.384478 140530037004032 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.3614519238471985, loss=2.9802815914154053
I0206 16:35:34.634989 140530045396736 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.34006837010383606, loss=2.962465286254883
I0206 16:36:09.880057 140530037004032 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.5141260027885437, loss=2.9741909503936768
I0206 16:36:45.119804 140530045396736 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.3638051152229309, loss=2.93992018699646
I0206 16:37:20.381773 140530037004032 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.3857731819152832, loss=2.9739267826080322
I0206 16:37:55.636085 140530045396736 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.3643966019153595, loss=3.0167956352233887
I0206 16:38:30.858915 140530037004032 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.38822659850120544, loss=2.9976930618286133
I0206 16:39:06.088218 140530045396736 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.34662601351737976, loss=2.963780164718628
I0206 16:39:41.327890 140530037004032 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.3993666470050812, loss=2.9735066890716553
I0206 16:40:16.530815 140530045396736 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.426110178232193, loss=3.015388011932373
I0206 16:40:51.742849 140530037004032 logging_writer.py:48] [30700] global_step=30700, grad_norm=1.1853070259094238, loss=3.040112257003784
I0206 16:41:26.954965 140530045396736 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.38290655612945557, loss=3.0089523792266846
I0206 16:42:02.159350 140530037004032 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.3729764223098755, loss=2.893477439880371
I0206 16:42:34.599216 140699726837568 spec.py:321] Evaluating on the training split.
I0206 16:42:37.618865 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 16:46:15.881565 140699726837568 spec.py:333] Evaluating on the validation split.
I0206 16:46:18.606611 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 16:49:07.722584 140699726837568 spec.py:349] Evaluating on the test split.
I0206 16:49:10.451505 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 16:51:39.616246 140699726837568 submission_runner.py:408] Time since start: 18718.81s, 	Step: 30994, 	{'train/accuracy': 0.6348527669906616, 'train/loss': 1.892822265625, 'train/bleu': 30.792602667408772, 'validation/accuracy': 0.6570532321929932, 'validation/loss': 1.7295809984207153, 'validation/bleu': 28.399133611509708, 'validation/num_examples': 3000, 'test/accuracy': 0.6665040254592896, 'test/loss': 1.6737943887710571, 'test/bleu': 27.640295046729694, 'test/num_examples': 3003, 'score': 10958.382172107697, 'total_duration': 18718.81236767769, 'accumulated_submission_time': 10958.382172107697, 'accumulated_eval_time': 7759.0807383060455, 'accumulated_logging_time': 0.3789188861846924}
I0206 16:51:39.634405 140530045396736 logging_writer.py:48] [30994] accumulated_eval_time=7759.080738, accumulated_logging_time=0.378919, accumulated_submission_time=10958.382172, global_step=30994, preemption_count=0, score=10958.382172, test/accuracy=0.666504, test/bleu=27.640295, test/loss=1.673794, test/num_examples=3003, total_duration=18718.812368, train/accuracy=0.634853, train/bleu=30.792603, train/loss=1.892822, validation/accuracy=0.657053, validation/bleu=28.399134, validation/loss=1.729581, validation/num_examples=3000
I0206 16:51:42.112389 140530037004032 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.3738856017589569, loss=3.0211760997772217
I0206 16:52:17.243531 140530045396736 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.3604183793067932, loss=2.949774742126465
I0206 16:52:52.474563 140530037004032 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.3529212176799774, loss=2.970322370529175
I0206 16:53:27.719284 140530045396736 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.36308780312538147, loss=3.039294481277466
I0206 16:54:02.913866 140530037004032 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.34678053855895996, loss=3.011904001235962
I0206 16:54:38.149591 140530045396736 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.33887267112731934, loss=2.980872631072998
I0206 16:55:13.351556 140530037004032 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.33169302344322205, loss=2.9424338340759277
I0206 16:55:48.530880 140530045396736 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.3805426359176636, loss=2.993964672088623
I0206 16:56:23.791849 140530037004032 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.36819490790367126, loss=2.8566458225250244
I0206 16:56:58.982763 140530045396736 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.5040315985679626, loss=2.9834694862365723
I0206 16:57:34.183760 140530037004032 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.4582483470439911, loss=3.0003578662872314
I0206 16:58:09.386873 140530045396736 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.32252034544944763, loss=2.953456163406372
I0206 16:58:44.605858 140530037004032 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.38331472873687744, loss=2.9898953437805176
I0206 16:59:19.773800 140530045396736 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.343351811170578, loss=3.0152525901794434
I0206 16:59:55.004584 140530037004032 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.33496350049972534, loss=2.9474129676818848
I0206 17:00:30.189044 140530045396736 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.35575035214424133, loss=2.947321891784668
I0206 17:01:05.408112 140530037004032 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.3449782431125641, loss=3.009913444519043
I0206 17:01:40.612332 140530045396736 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.36078882217407227, loss=3.001473903656006
I0206 17:02:15.825345 140530037004032 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.35153284668922424, loss=3.0428671836853027
I0206 17:02:51.016446 140530045396736 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.364912211894989, loss=2.956976890563965
I0206 17:03:26.242582 140530037004032 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.49019110202789307, loss=2.9539432525634766
I0206 17:04:01.438784 140530045396736 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.33308833837509155, loss=3.004055976867676
I0206 17:04:36.664189 140530037004032 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.387900173664093, loss=2.9683001041412354
I0206 17:05:11.899683 140530045396736 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.4032388925552368, loss=2.9436867237091064
I0206 17:05:39.781824 140699726837568 spec.py:321] Evaluating on the training split.
I0206 17:05:42.800756 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 17:09:22.273045 140699726837568 spec.py:333] Evaluating on the validation split.
I0206 17:09:25.023933 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 17:12:58.081312 140699726837568 spec.py:349] Evaluating on the test split.
I0206 17:13:00.831235 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 17:16:31.196758 140699726837568 submission_runner.py:408] Time since start: 20210.39s, 	Step: 33381, 	{'train/accuracy': 0.6381608843803406, 'train/loss': 1.8649847507476807, 'train/bleu': 30.97019323236983, 'validation/accuracy': 0.6565820574760437, 'validation/loss': 1.7204769849777222, 'validation/bleu': 28.203732298008507, 'validation/num_examples': 3000, 'test/accuracy': 0.667026937007904, 'test/loss': 1.6664468050003052, 'test/bleu': 27.455205127358838, 'test/num_examples': 3003, 'score': 11798.445326805115, 'total_duration': 20210.39287185669, 'accumulated_submission_time': 11798.445326805115, 'accumulated_eval_time': 8410.495576143265, 'accumulated_logging_time': 0.40795230865478516}
I0206 17:16:31.219582 140530037004032 logging_writer.py:48] [33381] accumulated_eval_time=8410.495576, accumulated_logging_time=0.407952, accumulated_submission_time=11798.445327, global_step=33381, preemption_count=0, score=11798.445327, test/accuracy=0.667027, test/bleu=27.455205, test/loss=1.666447, test/num_examples=3003, total_duration=20210.392872, train/accuracy=0.638161, train/bleu=30.970193, train/loss=1.864985, validation/accuracy=0.656582, validation/bleu=28.203732, validation/loss=1.720477, validation/num_examples=3000
I0206 17:16:38.267026 140530045396736 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.3615666329860687, loss=2.9876699447631836
I0206 17:17:13.388322 140530037004032 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.3469388782978058, loss=2.9795961380004883
I0206 17:17:48.570628 140530045396736 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.4140552878379822, loss=2.9384710788726807
I0206 17:18:23.789255 140530037004032 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.35215944051742554, loss=2.8618016242980957
I0206 17:18:58.987118 140530045396736 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.3604355454444885, loss=2.8833861351013184
I0206 17:19:34.185867 140530037004032 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.43334463238716125, loss=2.9603896141052246
I0206 17:20:09.407259 140530045396736 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.3648384213447571, loss=2.965240716934204
I0206 17:20:44.624171 140530037004032 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.36333465576171875, loss=2.933635711669922
I0206 17:21:19.866684 140530045396736 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.4186663031578064, loss=2.90307354927063
I0206 17:21:55.129397 140530037004032 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.3363495171070099, loss=2.9861855506896973
I0206 17:22:30.374882 140530045396736 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.37209752202033997, loss=2.97206449508667
I0206 17:23:05.601306 140530037004032 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.36710068583488464, loss=2.9673330783843994
I0206 17:23:40.835968 140530045396736 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.3488706052303314, loss=2.9064886569976807
I0206 17:24:16.100255 140530037004032 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.4092797040939331, loss=3.0240671634674072
I0206 17:24:51.321353 140530045396736 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.33739936351776123, loss=2.953866720199585
I0206 17:25:26.544921 140530037004032 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.3503780961036682, loss=2.9190921783447266
I0206 17:26:01.781770 140530045396736 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.39711275696754456, loss=3.025543689727783
I0206 17:26:37.016702 140530037004032 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.3863316476345062, loss=2.994438648223877
I0206 17:27:12.235450 140530045396736 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.3803557753562927, loss=2.9670937061309814
I0206 17:27:47.450055 140530037004032 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.3474772274494171, loss=2.9260284900665283
I0206 17:28:22.673156 140530045396736 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.41484537720680237, loss=2.9808509349823
I0206 17:28:57.888874 140530037004032 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.3592708706855774, loss=3.002542495727539
I0206 17:29:33.107368 140530045396736 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.338176965713501, loss=2.9463539123535156
I0206 17:30:08.324964 140530037004032 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.3899349868297577, loss=2.949014663696289
I0206 17:30:31.295564 140699726837568 spec.py:321] Evaluating on the training split.
I0206 17:30:34.323007 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 17:33:40.537844 140699726837568 spec.py:333] Evaluating on the validation split.
I0206 17:33:43.268045 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 17:36:12.002279 140699726837568 spec.py:349] Evaluating on the test split.
I0206 17:36:14.731592 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 17:38:44.672816 140699726837568 submission_runner.py:408] Time since start: 21543.87s, 	Step: 35767, 	{'train/accuracy': 0.6377241015434265, 'train/loss': 1.875410556793213, 'train/bleu': 31.085512843035314, 'validation/accuracy': 0.6565572619438171, 'validation/loss': 1.71941077709198, 'validation/bleu': 28.019113346418383, 'validation/num_examples': 3000, 'test/accuracy': 0.6706873774528503, 'test/loss': 1.6476588249206543, 'test/bleu': 27.893507040265977, 'test/num_examples': 3003, 'score': 12638.43499302864, 'total_duration': 21543.868954896927, 'accumulated_submission_time': 12638.43499302864, 'accumulated_eval_time': 8903.872762203217, 'accumulated_logging_time': 0.4422931671142578}
I0206 17:38:44.691867 140530045396736 logging_writer.py:48] [35767] accumulated_eval_time=8903.872762, accumulated_logging_time=0.442293, accumulated_submission_time=12638.434993, global_step=35767, preemption_count=0, score=12638.434993, test/accuracy=0.670687, test/bleu=27.893507, test/loss=1.647659, test/num_examples=3003, total_duration=21543.868955, train/accuracy=0.637724, train/bleu=31.085513, train/loss=1.875411, validation/accuracy=0.656557, validation/bleu=28.019113, validation/loss=1.719411, validation/num_examples=3000
I0206 17:38:56.628132 140530037004032 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.39604640007019043, loss=3.0463998317718506
I0206 17:39:31.754791 140530045396736 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.40838366746902466, loss=2.975433111190796
I0206 17:40:06.960857 140530037004032 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.35036107897758484, loss=2.9313604831695557
I0206 17:40:42.130718 140530045396736 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.36041587591171265, loss=3.0021231174468994
I0206 17:41:17.376483 140530037004032 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.4910983741283417, loss=2.9828264713287354
I0206 17:41:52.609012 140530045396736 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.3710384964942932, loss=3.0326826572418213
I0206 17:42:27.893720 140530037004032 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.4216648042201996, loss=2.9772870540618896
I0206 17:43:03.217207 140530045396736 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.36329880356788635, loss=2.9014029502868652
I0206 17:43:38.422562 140530037004032 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.43299150466918945, loss=2.924138307571411
I0206 17:44:13.622628 140530045396736 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.34604623913764954, loss=2.9194018840789795
I0206 17:44:48.854631 140530037004032 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.3414171040058136, loss=2.989499092102051
I0206 17:45:24.076628 140530045396736 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.37297844886779785, loss=3.025190591812134
I0206 17:45:59.290919 140530037004032 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.37824326753616333, loss=2.9548630714416504
I0206 17:46:34.506445 140530045396736 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.4137009382247925, loss=3.002737283706665
I0206 17:47:09.743525 140530037004032 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.5514240860939026, loss=3.0248522758483887
I0206 17:47:45.004932 140530045396736 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.37855809926986694, loss=2.9665918350219727
I0206 17:48:20.291648 140530037004032 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.3393697440624237, loss=2.9136879444122314
I0206 17:48:55.606934 140530045396736 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.45274171233177185, loss=2.9708354473114014
I0206 17:49:30.867084 140530037004032 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.3854721188545227, loss=2.9427857398986816
I0206 17:50:06.077821 140530045396736 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.3776819407939911, loss=2.860739231109619
I0206 17:50:41.309829 140530037004032 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.46277254819869995, loss=2.9454588890075684
I0206 17:51:16.521158 140530045396736 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.35556596517562866, loss=2.936511754989624
I0206 17:51:51.778692 140530037004032 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.3924344778060913, loss=2.9494025707244873
I0206 17:52:27.016084 140530045396736 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.37406331300735474, loss=2.9782862663269043
I0206 17:52:44.679896 140699726837568 spec.py:321] Evaluating on the training split.
I0206 17:52:47.706109 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 17:55:30.481217 140699726837568 spec.py:333] Evaluating on the validation split.
I0206 17:55:33.211011 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 17:58:05.827902 140699726837568 spec.py:349] Evaluating on the test split.
I0206 17:58:08.553708 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 18:00:42.402920 140699726837568 submission_runner.py:408] Time since start: 22861.60s, 	Step: 38152, 	{'train/accuracy': 0.6488719582557678, 'train/loss': 1.7934083938598633, 'train/bleu': 31.613892582308633, 'validation/accuracy': 0.6610581278800964, 'validation/loss': 1.70951247215271, 'validation/bleu': 28.280349091822146, 'validation/num_examples': 3000, 'test/accuracy': 0.6716054081916809, 'test/loss': 1.6476448774337769, 'test/bleu': 27.802578131818603, 'test/num_examples': 3003, 'score': 13478.334066152573, 'total_duration': 22861.5990588665, 'accumulated_submission_time': 13478.334066152573, 'accumulated_eval_time': 9381.595716238022, 'accumulated_logging_time': 0.47262072563171387}
I0206 18:00:42.422789 140530037004032 logging_writer.py:48] [38152] accumulated_eval_time=9381.595716, accumulated_logging_time=0.472621, accumulated_submission_time=13478.334066, global_step=38152, preemption_count=0, score=13478.334066, test/accuracy=0.671605, test/bleu=27.802578, test/loss=1.647645, test/num_examples=3003, total_duration=22861.599059, train/accuracy=0.648872, train/bleu=31.613893, train/loss=1.793408, validation/accuracy=0.661058, validation/bleu=28.280349, validation/loss=1.709512, validation/num_examples=3000
I0206 18:00:59.606289 140530045396736 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.387020081281662, loss=2.9557175636291504
I0206 18:01:34.737629 140530037004032 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.37222492694854736, loss=2.8703324794769287
I0206 18:02:09.939841 140530045396736 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.41328123211860657, loss=2.978090524673462
I0206 18:02:45.156838 140530037004032 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.377490758895874, loss=2.906973361968994
I0206 18:03:20.369425 140530045396736 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.3537676930427551, loss=2.9040138721466064
I0206 18:03:55.573373 140530037004032 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.3454461991786957, loss=2.885774612426758
I0206 18:04:30.800365 140530045396736 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.3642938435077667, loss=2.97343111038208
I0206 18:05:06.009558 140530037004032 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.38877806067466736, loss=2.904618978500366
I0206 18:05:41.218551 140530045396736 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.36349955201148987, loss=3.0392560958862305
I0206 18:06:16.473618 140530037004032 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.3145042359828949, loss=2.959873914718628
I0206 18:06:51.670136 140530045396736 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.3839808702468872, loss=2.9525039196014404
I0206 18:07:26.921636 140530037004032 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.3675934672355652, loss=2.9668850898742676
I0206 18:08:02.137627 140530045396736 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.3453979790210724, loss=3.0221192836761475
I0206 18:08:37.369183 140530037004032 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.38061508536338806, loss=2.963484287261963
I0206 18:09:12.611117 140530045396736 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.3552843928337097, loss=2.9341158866882324
I0206 18:09:47.902130 140530037004032 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.4001023471355438, loss=2.917858839035034
I0206 18:10:23.150418 140530045396736 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.3487037122249603, loss=2.9561798572540283
I0206 18:10:58.388881 140530037004032 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.3567887246608734, loss=2.9462873935699463
I0206 18:11:33.671093 140530045396736 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.38695961236953735, loss=3.015929937362671
I0206 18:12:08.883364 140530037004032 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.3688919246196747, loss=2.949618101119995
I0206 18:12:44.115136 140530045396736 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.3910024166107178, loss=2.9268205165863037
I0206 18:13:19.334871 140530037004032 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.37881937623023987, loss=2.937417507171631
I0206 18:13:54.558166 140530045396736 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.4000703692436218, loss=2.9477407932281494
I0206 18:14:29.769641 140530037004032 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.36111804842948914, loss=2.961560010910034
I0206 18:14:42.515124 140699726837568 spec.py:321] Evaluating on the training split.
I0206 18:14:45.538213 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 18:17:55.358373 140699726837568 spec.py:333] Evaluating on the validation split.
I0206 18:17:58.105867 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 18:20:36.172072 140699726837568 spec.py:349] Evaluating on the test split.
I0206 18:20:38.918168 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 18:23:23.055166 140699726837568 submission_runner.py:408] Time since start: 24222.25s, 	Step: 40538, 	{'train/accuracy': 0.6424659490585327, 'train/loss': 1.8437772989273071, 'train/bleu': 31.08091692572428, 'validation/accuracy': 0.6593222618103027, 'validation/loss': 1.7040035724639893, 'validation/bleu': 28.3158290660357, 'validation/num_examples': 3000, 'test/accuracy': 0.6716402173042297, 'test/loss': 1.6384211778640747, 'test/bleu': 27.640285594202233, 'test/num_examples': 3003, 'score': 14318.342139482498, 'total_duration': 24222.251285791397, 'accumulated_submission_time': 14318.342139482498, 'accumulated_eval_time': 9902.135681152344, 'accumulated_logging_time': 0.5022103786468506}
I0206 18:23:23.074706 140530045396736 logging_writer.py:48] [40538] accumulated_eval_time=9902.135681, accumulated_logging_time=0.502210, accumulated_submission_time=14318.342139, global_step=40538, preemption_count=0, score=14318.342139, test/accuracy=0.671640, test/bleu=27.640286, test/loss=1.638421, test/num_examples=3003, total_duration=24222.251286, train/accuracy=0.642466, train/bleu=31.080917, train/loss=1.843777, validation/accuracy=0.659322, validation/bleu=28.315829, validation/loss=1.704004, validation/num_examples=3000
I0206 18:23:45.170473 140530037004032 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.3533485233783722, loss=2.9649417400360107
I0206 18:24:20.302890 140530045396736 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.3881036043167114, loss=2.9354822635650635
I0206 18:24:55.495843 140530037004032 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.3449164032936096, loss=2.903507947921753
I0206 18:25:30.712171 140530045396736 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.31441783905029297, loss=2.8788230419158936
I0206 18:26:05.906735 140530037004032 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.35541942715644836, loss=2.9338417053222656
I0206 18:26:41.099460 140530045396736 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.4186297059059143, loss=2.997532367706299
I0206 18:27:16.312251 140530037004032 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.4033982753753662, loss=2.9667809009552
I0206 18:27:51.506745 140530045396736 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.45222440361976624, loss=2.9511826038360596
I0206 18:28:26.696295 140530037004032 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.35048210620880127, loss=2.897122859954834
I0206 18:29:01.903742 140530045396736 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.3519432544708252, loss=2.9103026390075684
I0206 18:29:37.145695 140530037004032 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.5406109690666199, loss=2.9984283447265625
I0206 18:30:12.381257 140530045396736 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.38850653171539307, loss=2.989741563796997
I0206 18:30:47.626440 140530037004032 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.38326016068458557, loss=2.997729539871216
I0206 18:31:22.842977 140530045396736 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.34581372141838074, loss=2.972012758255005
I0206 18:31:58.096591 140530037004032 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.3196631968021393, loss=2.9427576065063477
I0206 18:32:33.321204 140530045396736 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.3605499565601349, loss=2.935102939605713
I0206 18:33:08.529334 140530037004032 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.3642827868461609, loss=2.9469776153564453
I0206 18:33:43.731390 140530045396736 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.3255096673965454, loss=2.916499376296997
I0206 18:34:18.937306 140530037004032 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.3622388541698456, loss=2.9273734092712402
I0206 18:34:54.139581 140530045396736 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.3407961428165436, loss=2.9096834659576416
I0206 18:35:29.365966 140530037004032 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.3918089270591736, loss=2.908825635910034
I0206 18:36:04.609680 140530045396736 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.3343947231769562, loss=2.92289400100708
I0206 18:36:39.814280 140530037004032 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.3771265745162964, loss=2.933680772781372
I0206 18:37:15.028989 140530045396736 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.36954039335250854, loss=2.895996332168579
I0206 18:37:23.194425 140699726837568 spec.py:321] Evaluating on the training split.
I0206 18:37:26.227203 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 18:40:05.827018 140699726837568 spec.py:333] Evaluating on the validation split.
I0206 18:40:08.542247 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 18:42:49.377807 140699726837568 spec.py:349] Evaluating on the test split.
I0206 18:42:52.099139 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 18:45:24.869065 140699726837568 submission_runner.py:408] Time since start: 25544.07s, 	Step: 42925, 	{'train/accuracy': 0.6389986276626587, 'train/loss': 1.856657862663269, 'train/bleu': 31.192170592865967, 'validation/accuracy': 0.6593842506408691, 'validation/loss': 1.7011802196502686, 'validation/bleu': 28.261132238555522, 'validation/num_examples': 3000, 'test/accuracy': 0.6723374724388123, 'test/loss': 1.6351051330566406, 'test/bleu': 28.000836489908377, 'test/num_examples': 3003, 'score': 15158.377641677856, 'total_duration': 25544.065200567245, 'accumulated_submission_time': 15158.377641677856, 'accumulated_eval_time': 10383.810242652893, 'accumulated_logging_time': 0.5315215587615967}
I0206 18:45:24.890030 140530037004032 logging_writer.py:48] [42925] accumulated_eval_time=10383.810243, accumulated_logging_time=0.531522, accumulated_submission_time=15158.377642, global_step=42925, preemption_count=0, score=15158.377642, test/accuracy=0.672337, test/bleu=28.000836, test/loss=1.635105, test/num_examples=3003, total_duration=25544.065201, train/accuracy=0.638999, train/bleu=31.192171, train/loss=1.856658, validation/accuracy=0.659384, validation/bleu=28.261132, validation/loss=1.701180, validation/num_examples=3000
I0206 18:45:51.569199 140530045396736 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.33866554498672485, loss=2.9432499408721924
I0206 18:46:26.691384 140530037004032 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.36591657996177673, loss=2.9763894081115723
I0206 18:47:01.841694 140530045396736 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.35403549671173096, loss=2.950385570526123
I0206 18:47:37.031717 140530037004032 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.47926589846611023, loss=2.9087629318237305
I0206 18:48:12.219485 140530045396736 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.35301673412323, loss=2.9754459857940674
I0206 18:48:47.439743 140530037004032 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.3936265707015991, loss=2.936579465866089
I0206 18:49:22.639901 140530045396736 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.4047274589538574, loss=2.915290117263794
I0206 18:49:57.817790 140530037004032 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.3533381223678589, loss=2.9622600078582764
I0206 18:50:33.063836 140530045396736 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.3457687497138977, loss=2.9406769275665283
I0206 18:51:08.295962 140530037004032 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.35160037875175476, loss=2.918430805206299
I0206 18:51:43.556818 140530045396736 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.3554517328739166, loss=2.917367935180664
I0206 18:52:18.813322 140530037004032 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.37071892619132996, loss=2.9076027870178223
I0206 18:52:54.016437 140530045396736 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.336073100566864, loss=2.9998459815979004
I0206 18:53:29.233771 140530037004032 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.3791367709636688, loss=2.9245457649230957
I0206 18:54:04.454131 140530045396736 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.3448096513748169, loss=2.9553916454315186
I0206 18:54:39.699376 140530037004032 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.4324696958065033, loss=2.9663517475128174
I0206 18:55:14.912659 140530045396736 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.34383609890937805, loss=2.9522042274475098
I0206 18:55:50.120639 140530037004032 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.3240286111831665, loss=2.854836940765381
I0206 18:56:25.334966 140530045396736 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.35647302865982056, loss=2.894144058227539
I0206 18:57:00.544961 140530037004032 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.377066969871521, loss=3.082622528076172
I0206 18:57:35.750599 140530045396736 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.3305882513523102, loss=2.884404420852661
I0206 18:58:10.964131 140530037004032 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.39938393235206604, loss=2.914611339569092
I0206 18:58:46.182344 140530045396736 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.3727947175502777, loss=2.9778881072998047
I0206 18:59:21.423651 140530037004032 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.36432531476020813, loss=2.9255053997039795
I0206 18:59:25.018802 140699726837568 spec.py:321] Evaluating on the training split.
I0206 18:59:28.039892 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 19:02:13.448818 140699726837568 spec.py:333] Evaluating on the validation split.
I0206 19:02:16.174675 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 19:04:49.338679 140699726837568 spec.py:349] Evaluating on the test split.
I0206 19:04:52.073292 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 19:07:14.125074 140699726837568 submission_runner.py:408] Time since start: 26853.32s, 	Step: 45312, 	{'train/accuracy': 0.6493979692459106, 'train/loss': 1.7883561849594116, 'train/bleu': 31.863872415061568, 'validation/accuracy': 0.6628807783126831, 'validation/loss': 1.6845715045928955, 'validation/bleu': 28.467148185414068, 'validation/num_examples': 3000, 'test/accuracy': 0.6726396083831787, 'test/loss': 1.62099027633667, 'test/bleu': 27.586332945086863, 'test/num_examples': 3003, 'score': 15998.422271966934, 'total_duration': 26853.321212291718, 'accumulated_submission_time': 15998.422271966934, 'accumulated_eval_time': 10852.916440725327, 'accumulated_logging_time': 0.5626571178436279}
I0206 19:07:14.145706 140530045396736 logging_writer.py:48] [45312] accumulated_eval_time=10852.916441, accumulated_logging_time=0.562657, accumulated_submission_time=15998.422272, global_step=45312, preemption_count=0, score=15998.422272, test/accuracy=0.672640, test/bleu=27.586333, test/loss=1.620990, test/num_examples=3003, total_duration=26853.321212, train/accuracy=0.649398, train/bleu=31.863872, train/loss=1.788356, validation/accuracy=0.662881, validation/bleu=28.467148, validation/loss=1.684572, validation/num_examples=3000
I0206 19:07:45.374718 140530037004032 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.41142523288726807, loss=2.9663965702056885
I0206 19:08:20.502476 140530045396736 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.39554673433303833, loss=3.020784378051758
I0206 19:08:55.677654 140530037004032 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.4016974866390228, loss=2.9584832191467285
I0206 19:09:30.879048 140530045396736 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.332527756690979, loss=2.9416303634643555
I0206 19:10:06.065662 140530037004032 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.31227222084999084, loss=2.9280319213867188
I0206 19:10:41.266530 140530045396736 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.40654435753822327, loss=3.0086886882781982
I0206 19:11:16.467559 140530037004032 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.3633648455142975, loss=2.9020352363586426
I0206 19:11:51.696400 140530045396736 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.39141881465911865, loss=2.9960286617279053
I0206 19:12:26.916817 140530037004032 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.3698458671569824, loss=2.9397590160369873
I0206 19:13:02.163966 140530045396736 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.40128499269485474, loss=2.9301564693450928
I0206 19:13:37.346894 140530037004032 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.3281193971633911, loss=2.9130632877349854
I0206 19:14:12.557411 140530045396736 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.39038270711898804, loss=2.878838062286377
I0206 19:14:47.780733 140530037004032 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.3502800464630127, loss=2.89970064163208
I0206 19:15:22.981629 140530045396736 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.3742089569568634, loss=2.9300084114074707
I0206 19:15:58.220166 140530037004032 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.35655006766319275, loss=2.938744068145752
I0206 19:16:33.421552 140530045396736 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.36794647574424744, loss=2.9293782711029053
I0206 19:17:08.626819 140530037004032 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.3400237560272217, loss=3.019744634628296
I0206 19:17:43.831912 140530045396736 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.41161686182022095, loss=3.0126640796661377
I0206 19:18:19.039246 140530037004032 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.37631747126579285, loss=3.004507303237915
I0206 19:18:54.230691 140530045396736 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.3672250807285309, loss=2.8700413703918457
I0206 19:19:29.448504 140530037004032 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.34151583909988403, loss=2.9032301902770996
I0206 19:20:04.668502 140530045396736 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.3813181519508362, loss=2.9538910388946533
I0206 19:20:39.866941 140530037004032 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.3449835181236267, loss=2.879673480987549
I0206 19:21:14.446246 140699726837568 spec.py:321] Evaluating on the training split.
I0206 19:21:17.473682 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 19:23:55.348559 140699726837568 spec.py:333] Evaluating on the validation split.
I0206 19:23:58.086743 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 19:26:28.729348 140699726837568 spec.py:349] Evaluating on the test split.
I0206 19:26:31.472673 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 19:28:57.678789 140699726837568 submission_runner.py:408] Time since start: 28156.87s, 	Step: 47700, 	{'train/accuracy': 0.641980767250061, 'train/loss': 1.8360681533813477, 'train/bleu': 31.306511234966308, 'validation/accuracy': 0.6625956296920776, 'validation/loss': 1.6862725019454956, 'validation/bleu': 28.622753251573556, 'validation/num_examples': 3000, 'test/accuracy': 0.6753239631652832, 'test/loss': 1.6231544017791748, 'test/bleu': 28.22375180587874, 'test/num_examples': 3003, 'score': 16838.640387535095, 'total_duration': 28156.87495303154, 'accumulated_submission_time': 16838.640387535095, 'accumulated_eval_time': 11316.14895439148, 'accumulated_logging_time': 0.5931167602539062}
I0206 19:28:57.699714 140530045396736 logging_writer.py:48] [47700] accumulated_eval_time=11316.148954, accumulated_logging_time=0.593117, accumulated_submission_time=16838.640388, global_step=47700, preemption_count=0, score=16838.640388, test/accuracy=0.675324, test/bleu=28.223752, test/loss=1.623154, test/num_examples=3003, total_duration=28156.874953, train/accuracy=0.641981, train/bleu=31.306511, train/loss=1.836068, validation/accuracy=0.662596, validation/bleu=28.622753, validation/loss=1.686273, validation/num_examples=3000
I0206 19:28:58.074227 140530037004032 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.37001141905784607, loss=2.9824979305267334
I0206 19:29:33.157891 140530045396736 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.4323004484176636, loss=2.9274656772613525
I0206 19:30:08.288227 140530037004032 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.3674554228782654, loss=2.92927885055542
I0206 19:30:43.441211 140530045396736 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.3747880458831787, loss=2.9073147773742676
I0206 19:31:18.673548 140530037004032 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.41459518671035767, loss=2.850365400314331
I0206 19:31:53.862528 140530045396736 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.3602059781551361, loss=2.935817003250122
I0206 19:32:29.045111 140530037004032 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.35048338770866394, loss=2.95613169670105
I0206 19:33:04.234989 140530045396736 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.4165104031562805, loss=2.955753803253174
I0206 19:33:39.421206 140530037004032 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.35834091901779175, loss=2.9222116470336914
I0206 19:34:14.656750 140530045396736 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.3317585289478302, loss=2.937288522720337
I0206 19:34:49.908981 140530037004032 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.38459858298301697, loss=2.9636518955230713
I0206 19:35:25.095901 140530045396736 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.31699204444885254, loss=2.8853399753570557
I0206 19:36:00.326695 140530037004032 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.3594795763492584, loss=2.9884836673736572
I0206 19:36:35.518878 140530045396736 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.3641429841518402, loss=2.9326038360595703
I0206 19:37:10.719093 140530037004032 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.3605644404888153, loss=2.9080145359039307
I0206 19:37:45.920920 140530045396736 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.32052239775657654, loss=2.9013986587524414
I0206 19:38:21.158502 140530037004032 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.37751081585884094, loss=2.9314446449279785
I0206 19:38:56.375746 140530045396736 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.36502912640571594, loss=2.9655165672302246
I0206 19:39:31.566147 140530037004032 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.36514812707901, loss=2.877936363220215
I0206 19:40:06.790459 140530045396736 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.35325437784194946, loss=2.927258253097534
I0206 19:40:42.045935 140530037004032 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.3762548863887787, loss=2.976909637451172
I0206 19:41:17.300710 140530045396736 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.38018327951431274, loss=3.0068342685699463
I0206 19:41:52.539502 140530037004032 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.3943648636341095, loss=3.0070290565490723
I0206 19:42:27.737910 140530045396736 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.33275991678237915, loss=2.9261324405670166
I0206 19:42:57.734201 140699726837568 spec.py:321] Evaluating on the training split.
I0206 19:43:00.759534 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 19:46:08.288379 140699726837568 spec.py:333] Evaluating on the validation split.
I0206 19:46:11.009557 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 19:48:35.083292 140699726837568 spec.py:349] Evaluating on the test split.
I0206 19:48:37.836734 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 19:50:57.997828 140699726837568 submission_runner.py:408] Time since start: 29477.19s, 	Step: 50087, 	{'train/accuracy': 0.691525399684906, 'train/loss': 1.5449894666671753, 'train/bleu': 35.324233093710475, 'validation/accuracy': 0.6646042466163635, 'validation/loss': 1.6788150072097778, 'validation/bleu': 28.461571849251246, 'validation/num_examples': 3000, 'test/accuracy': 0.6756957769393921, 'test/loss': 1.6147881746292114, 'test/bleu': 28.095035425482386, 'test/num_examples': 3003, 'score': 17678.58997631073, 'total_duration': 29477.193959236145, 'accumulated_submission_time': 17678.58997631073, 'accumulated_eval_time': 11796.412520170212, 'accumulated_logging_time': 0.6238071918487549}
I0206 19:50:58.023921 140530037004032 logging_writer.py:48] [50087] accumulated_eval_time=11796.412520, accumulated_logging_time=0.623807, accumulated_submission_time=17678.589976, global_step=50087, preemption_count=0, score=17678.589976, test/accuracy=0.675696, test/bleu=28.095035, test/loss=1.614788, test/num_examples=3003, total_duration=29477.193959, train/accuracy=0.691525, train/bleu=35.324233, train/loss=1.544989, validation/accuracy=0.664604, validation/bleu=28.461572, validation/loss=1.678815, validation/num_examples=3000
I0206 19:51:02.954715 140530045396736 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.37836626172065735, loss=2.9295225143432617
I0206 19:51:38.103554 140530037004032 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.32229897379875183, loss=2.925792932510376
I0206 19:52:13.256402 140530045396736 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.40662333369255066, loss=2.928640604019165
I0206 19:52:48.453527 140530037004032 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.3401690423488617, loss=2.934885025024414
I0206 19:53:23.665461 140530045396736 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.39101192355155945, loss=2.897827625274658
I0206 19:53:58.858564 140530037004032 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.3792704939842224, loss=2.9312632083892822
I0206 19:54:34.059442 140530045396736 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.48447057604789734, loss=2.9138259887695312
I0206 19:55:09.265217 140530037004032 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.3734644055366516, loss=2.977912187576294
I0206 19:55:44.444599 140530045396736 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.40323173999786377, loss=2.9567246437072754
I0206 19:56:19.665461 140530037004032 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.36630553007125854, loss=2.905029058456421
I0206 19:56:54.855104 140530045396736 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.3333621025085449, loss=2.872535467147827
I0206 19:57:30.055985 140530037004032 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.34661027789115906, loss=2.8984744548797607
I0206 19:58:05.246546 140530045396736 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.37534475326538086, loss=2.951035976409912
I0206 19:58:40.470640 140530037004032 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.390544056892395, loss=2.8564980030059814
I0206 19:59:15.669449 140530045396736 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.3212900161743164, loss=2.882488250732422
I0206 19:59:50.873138 140530037004032 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.3458419442176819, loss=2.9746174812316895
I0206 20:00:26.102297 140530045396736 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.3413102626800537, loss=2.929913282394409
I0206 20:01:01.314901 140530037004032 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.3612081706523895, loss=2.987645149230957
I0206 20:01:36.564122 140530045396736 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.39425548911094666, loss=2.894174575805664
I0206 20:02:11.776577 140530037004032 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.38886508345603943, loss=2.9025888442993164
I0206 20:02:47.000660 140530045396736 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.3459882438182831, loss=2.93758487701416
I0206 20:03:22.193783 140530037004032 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.35608088970184326, loss=2.9700734615325928
I0206 20:03:57.423770 140530045396736 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.3430183529853821, loss=2.910987615585327
I0206 20:04:32.613556 140530037004032 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.40279945731163025, loss=2.9828033447265625
I0206 20:04:58.040599 140699726837568 spec.py:321] Evaluating on the training split.
I0206 20:05:01.081411 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 20:08:03.376455 140699726837568 spec.py:333] Evaluating on the validation split.
I0206 20:08:06.119195 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 20:10:32.463861 140699726837568 spec.py:349] Evaluating on the test split.
I0206 20:10:35.191562 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 20:12:54.569683 140699726837568 submission_runner.py:408] Time since start: 30793.77s, 	Step: 52474, 	{'train/accuracy': 0.648737370967865, 'train/loss': 1.7925587892532349, 'train/bleu': 31.28400718143166, 'validation/accuracy': 0.6651870608329773, 'validation/loss': 1.667005181312561, 'validation/bleu': 28.830817072279498, 'validation/num_examples': 3000, 'test/accuracy': 0.6764511466026306, 'test/loss': 1.6009951829910278, 'test/bleu': 28.30420204851027, 'test/num_examples': 3003, 'score': 18518.520733118057, 'total_duration': 30793.765823841095, 'accumulated_submission_time': 18518.520733118057, 'accumulated_eval_time': 12272.941549777985, 'accumulated_logging_time': 0.6620500087738037}
I0206 20:12:54.593926 140530045396736 logging_writer.py:48] [52474] accumulated_eval_time=12272.941550, accumulated_logging_time=0.662050, accumulated_submission_time=18518.520733, global_step=52474, preemption_count=0, score=18518.520733, test/accuracy=0.676451, test/bleu=28.304202, test/loss=1.600995, test/num_examples=3003, total_duration=30793.765824, train/accuracy=0.648737, train/bleu=31.284007, train/loss=1.792559, validation/accuracy=0.665187, validation/bleu=28.830817, validation/loss=1.667005, validation/num_examples=3000
I0206 20:13:04.081831 140530037004032 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.36368343234062195, loss=2.929612874984741
I0206 20:13:39.223979 140530045396736 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.406229704618454, loss=2.9414315223693848
I0206 20:14:14.372023 140530037004032 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.361537903547287, loss=2.873775005340576
I0206 20:14:49.564071 140530045396736 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.3461999297142029, loss=2.922234535217285
I0206 20:15:24.766644 140530037004032 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.3721849024295807, loss=2.904576063156128
I0206 20:15:59.992699 140530045396736 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.3449884355068207, loss=2.9050137996673584
I0206 20:16:35.191567 140530037004032 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.35439926385879517, loss=2.8563718795776367
I0206 20:17:10.411503 140530045396736 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.35475149750709534, loss=2.8775672912597656
I0206 20:17:45.643804 140530037004032 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.34948039054870605, loss=2.8378217220306396
I0206 20:18:20.870141 140530045396736 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.36221885681152344, loss=2.924243927001953
I0206 20:18:56.093264 140530037004032 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.3315573036670685, loss=2.911285161972046
I0206 20:19:31.366805 140530045396736 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.441883385181427, loss=2.885368824005127
I0206 20:20:06.630805 140530037004032 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.3934842646121979, loss=2.9679172039031982
I0206 20:20:41.887884 140530045396736 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.34190067648887634, loss=2.9055027961730957
I0206 20:21:17.130049 140530037004032 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.36282822489738464, loss=2.8985660076141357
I0206 20:21:52.369480 140530045396736 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.34659767150878906, loss=2.8889687061309814
I0206 20:22:27.583476 140530037004032 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.363378643989563, loss=3.0223352909088135
I0206 20:23:02.765461 140530045396736 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.36693236231803894, loss=2.9460289478302
I0206 20:23:37.965304 140530037004032 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.3733175992965698, loss=2.92425799369812
I0206 20:24:13.179213 140530045396736 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.3328293561935425, loss=2.9109997749328613
I0206 20:24:48.396637 140530037004032 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.38161933422088623, loss=2.984194755554199
I0206 20:25:23.608140 140530045396736 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.3255883455276489, loss=2.9276599884033203
I0206 20:25:58.829786 140530037004032 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.3730931878089905, loss=2.9515771865844727
I0206 20:26:34.105612 140530045396736 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.3826758563518524, loss=2.871406078338623
I0206 20:26:54.635720 140699726837568 spec.py:321] Evaluating on the training split.
I0206 20:26:57.668594 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 20:29:46.537127 140699726837568 spec.py:333] Evaluating on the validation split.
I0206 20:29:49.289137 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 20:32:15.725528 140699726837568 spec.py:349] Evaluating on the test split.
I0206 20:32:18.455766 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 20:34:36.785491 140699726837568 submission_runner.py:408] Time since start: 32095.98s, 	Step: 54860, 	{'train/accuracy': 0.6473584175109863, 'train/loss': 1.8072962760925293, 'train/bleu': 31.921184654644467, 'validation/accuracy': 0.6671088933944702, 'validation/loss': 1.6631172895431519, 'validation/bleu': 29.0922268005727, 'validation/num_examples': 3000, 'test/accuracy': 0.6795421838760376, 'test/loss': 1.5950560569763184, 'test/bleu': 28.502942846220005, 'test/num_examples': 3003, 'score': 19358.474761724472, 'total_duration': 32095.981645822525, 'accumulated_submission_time': 19358.474761724472, 'accumulated_eval_time': 12735.091276407242, 'accumulated_logging_time': 0.6979632377624512}
I0206 20:34:36.807928 140530037004032 logging_writer.py:48] [54860] accumulated_eval_time=12735.091276, accumulated_logging_time=0.697963, accumulated_submission_time=19358.474762, global_step=54860, preemption_count=0, score=19358.474762, test/accuracy=0.679542, test/bleu=28.502943, test/loss=1.595056, test/num_examples=3003, total_duration=32095.981646, train/accuracy=0.647358, train/bleu=31.921185, train/loss=1.807296, validation/accuracy=0.667109, validation/bleu=29.092227, validation/loss=1.663117, validation/num_examples=3000
I0206 20:34:51.207934 140530045396736 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.40760284662246704, loss=2.936924695968628
I0206 20:35:26.312321 140530037004032 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.4143402874469757, loss=2.9566636085510254
I0206 20:36:01.477059 140530045396736 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.36245909333229065, loss=2.8984081745147705
I0206 20:36:36.681947 140530037004032 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.34697458148002625, loss=2.8763086795806885
I0206 20:37:11.894625 140530045396736 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.34946689009666443, loss=2.868347406387329
I0206 20:37:47.098584 140530037004032 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.34790775179862976, loss=2.912717819213867
I0206 20:38:22.291041 140530045396736 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.4185272753238678, loss=2.9715793132781982
I0206 20:38:57.551786 140530037004032 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.3832203149795532, loss=2.981687068939209
I0206 20:39:32.766161 140530045396736 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.35518917441368103, loss=2.812432289123535
I0206 20:40:07.989713 140530037004032 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.3633628189563751, loss=2.9110991954803467
I0206 20:40:43.209858 140530045396736 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.35128405690193176, loss=2.8934245109558105
I0206 20:41:18.425730 140530037004032 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.3793317973613739, loss=2.928317070007324
I0206 20:41:53.671992 140530045396736 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.3804973363876343, loss=2.9138739109039307
I0206 20:42:28.882530 140530037004032 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.3608461022377014, loss=2.968172311782837
I0206 20:43:04.087113 140530045396736 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.40436089038848877, loss=2.943502187728882
I0206 20:43:39.338897 140530037004032 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.36785250902175903, loss=2.8982858657836914
I0206 20:44:14.570328 140530045396736 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.3242962658405304, loss=2.947185754776001
I0206 20:44:49.806668 140530037004032 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.3476687967777252, loss=2.9268860816955566
I0206 20:45:25.047310 140530045396736 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.34880468249320984, loss=2.831247329711914
I0206 20:46:00.253598 140530037004032 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.31401073932647705, loss=2.882140874862671
I0206 20:46:35.476052 140530045396736 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.31804409623146057, loss=2.8367254734039307
I0206 20:47:10.687360 140530037004032 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.34624090790748596, loss=2.7893848419189453
I0206 20:47:45.914174 140530045396736 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.37645697593688965, loss=2.972827434539795
I0206 20:48:21.139618 140530037004032 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.34219852089881897, loss=2.9441561698913574
I0206 20:48:37.063358 140699726837568 spec.py:321] Evaluating on the training split.
I0206 20:48:40.096205 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 20:51:15.568297 140699726837568 spec.py:333] Evaluating on the validation split.
I0206 20:51:18.296665 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 20:53:44.587827 140699726837568 spec.py:349] Evaluating on the test split.
I0206 20:53:47.317092 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 20:56:24.056020 140699726837568 submission_runner.py:408] Time since start: 33403.25s, 	Step: 57247, 	{'train/accuracy': 0.6565492749214172, 'train/loss': 1.7392457723617554, 'train/bleu': 32.6825347018119, 'validation/accuracy': 0.6668609380722046, 'validation/loss': 1.654740810394287, 'validation/bleu': 29.001221127806534, 'validation/num_examples': 3000, 'test/accuracy': 0.6824705004692078, 'test/loss': 1.5814528465270996, 'test/bleu': 28.750735001154936, 'test/num_examples': 3003, 'score': 20198.645827054977, 'total_duration': 33403.252178907394, 'accumulated_submission_time': 20198.645827054977, 'accumulated_eval_time': 13202.083883523941, 'accumulated_logging_time': 0.7302701473236084}
I0206 20:56:24.078385 140530045396736 logging_writer.py:48] [57247] accumulated_eval_time=13202.083884, accumulated_logging_time=0.730270, accumulated_submission_time=20198.645827, global_step=57247, preemption_count=0, score=20198.645827, test/accuracy=0.682471, test/bleu=28.750735, test/loss=1.581453, test/num_examples=3003, total_duration=33403.252179, train/accuracy=0.656549, train/bleu=32.682535, train/loss=1.739246, validation/accuracy=0.666861, validation/bleu=29.001221, validation/loss=1.654741, validation/num_examples=3000
I0206 20:56:43.070181 140530037004032 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.37833839654922485, loss=2.92474365234375
I0206 20:57:18.185591 140530045396736 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.37362998723983765, loss=2.881272315979004
I0206 20:57:53.355926 140530037004032 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.36109742522239685, loss=2.8715665340423584
I0206 20:58:28.542404 140530045396736 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.3948872685432434, loss=2.8479835987091064
I0206 20:59:03.743446 140530037004032 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.36626654863357544, loss=2.8818321228027344
I0206 20:59:38.939959 140530045396736 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.36264002323150635, loss=2.8483078479766846
I0206 21:00:14.134677 140530037004032 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.3610997498035431, loss=2.9434804916381836
I0206 21:00:49.360698 140530045396736 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.35448282957077026, loss=2.967452049255371
I0206 21:01:24.602592 140530037004032 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.367296040058136, loss=2.909740924835205
I0206 21:01:59.815740 140530045396736 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.31803810596466064, loss=2.885915517807007
I0206 21:02:35.006862 140530037004032 logging_writer.py:48] [58300] global_step=58300, grad_norm=0.3631495237350464, loss=2.8497438430786133
I0206 21:03:10.226581 140530045396736 logging_writer.py:48] [58400] global_step=58400, grad_norm=0.34220650792121887, loss=2.9131433963775635
I0206 21:03:45.439236 140530037004032 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.3607480227947235, loss=2.9068751335144043
I0206 21:04:20.659543 140530045396736 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.37000149488449097, loss=2.870917320251465
I0206 21:04:55.873211 140530037004032 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.3467975854873657, loss=2.886329412460327
I0206 21:05:31.090767 140530045396736 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.35801011323928833, loss=2.8792152404785156
I0206 21:06:06.328022 140530037004032 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.3546901345252991, loss=2.9931437969207764
I0206 21:06:41.520681 140530045396736 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.45061784982681274, loss=2.898383378982544
I0206 21:07:16.770653 140530037004032 logging_writer.py:48] [59100] global_step=59100, grad_norm=0.403852641582489, loss=2.8632848262786865
I0206 21:07:52.105128 140530045396736 logging_writer.py:48] [59200] global_step=59200, grad_norm=0.4724060595035553, loss=2.975961685180664
I0206 21:08:27.338535 140530037004032 logging_writer.py:48] [59300] global_step=59300, grad_norm=0.4343612790107727, loss=2.905076503753662
I0206 21:09:02.530761 140530045396736 logging_writer.py:48] [59400] global_step=59400, grad_norm=0.3891749083995819, loss=2.90632963180542
I0206 21:09:37.719306 140530037004032 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.364301472902298, loss=2.8704371452331543
I0206 21:10:12.932631 140530045396736 logging_writer.py:48] [59600] global_step=59600, grad_norm=0.3884420692920685, loss=2.9477853775024414
I0206 21:10:24.257572 140699726837568 spec.py:321] Evaluating on the training split.
I0206 21:10:27.276409 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 21:13:20.829352 140699726837568 spec.py:333] Evaluating on the validation split.
I0206 21:13:23.553346 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 21:15:53.686022 140699726837568 spec.py:349] Evaluating on the test split.
I0206 21:15:56.413432 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 21:18:21.010490 140699726837568 submission_runner.py:408] Time since start: 34720.21s, 	Step: 59634, 	{'train/accuracy': 0.6492637395858765, 'train/loss': 1.7962538003921509, 'train/bleu': 31.7395194006414, 'validation/accuracy': 0.6694399118423462, 'validation/loss': 1.6482961177825928, 'validation/bleu': 28.93402112722786, 'validation/num_examples': 3000, 'test/accuracy': 0.6804021000862122, 'test/loss': 1.5773099660873413, 'test/bleu': 28.77173907460039, 'test/num_examples': 3003, 'score': 21038.74125480652, 'total_duration': 34720.2066423893, 'accumulated_submission_time': 21038.74125480652, 'accumulated_eval_time': 13678.836746692657, 'accumulated_logging_time': 0.7625217437744141}
I0206 21:18:21.033925 140530037004032 logging_writer.py:48] [59634] accumulated_eval_time=13678.836747, accumulated_logging_time=0.762522, accumulated_submission_time=21038.741255, global_step=59634, preemption_count=0, score=21038.741255, test/accuracy=0.680402, test/bleu=28.771739, test/loss=1.577310, test/num_examples=3003, total_duration=34720.206642, train/accuracy=0.649264, train/bleu=31.739519, train/loss=1.796254, validation/accuracy=0.669440, validation/bleu=28.934021, validation/loss=1.648296, validation/num_examples=3000
I0206 21:18:44.563809 140530045396736 logging_writer.py:48] [59700] global_step=59700, grad_norm=0.34333184361457825, loss=2.8567702770233154
I0206 21:19:19.688057 140530037004032 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.33789268136024475, loss=2.8458309173583984
I0206 21:19:54.869554 140530045396736 logging_writer.py:48] [59900] global_step=59900, grad_norm=0.33995646238327026, loss=2.8510830402374268
I0206 21:20:30.058174 140530037004032 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.35741204023361206, loss=2.9439611434936523
I0206 21:21:05.298234 140530045396736 logging_writer.py:48] [60100] global_step=60100, grad_norm=0.34618303179740906, loss=2.8528177738189697
I0206 21:21:40.510940 140530037004032 logging_writer.py:48] [60200] global_step=60200, grad_norm=0.408413827419281, loss=2.923285722732544
I0206 21:22:15.727648 140530045396736 logging_writer.py:48] [60300] global_step=60300, grad_norm=0.3759478032588959, loss=2.8951737880706787
I0206 21:22:50.922925 140530037004032 logging_writer.py:48] [60400] global_step=60400, grad_norm=0.33384183049201965, loss=2.8771250247955322
I0206 21:23:26.108832 140530045396736 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.36360567808151245, loss=2.8950326442718506
I0206 21:24:01.308395 140530037004032 logging_writer.py:48] [60600] global_step=60600, grad_norm=0.35580480098724365, loss=2.8806402683258057
I0206 21:24:36.531214 140530045396736 logging_writer.py:48] [60700] global_step=60700, grad_norm=0.3923873007297516, loss=2.8629770278930664
I0206 21:25:11.717510 140530037004032 logging_writer.py:48] [60800] global_step=60800, grad_norm=0.3347488045692444, loss=2.9092750549316406
I0206 21:25:46.969204 140530045396736 logging_writer.py:48] [60900] global_step=60900, grad_norm=0.45674508810043335, loss=2.8526437282562256
I0206 21:26:22.194505 140530037004032 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.328311949968338, loss=2.8690476417541504
I0206 21:26:57.400459 140530045396736 logging_writer.py:48] [61100] global_step=61100, grad_norm=0.37187182903289795, loss=2.9208757877349854
I0206 21:27:32.587378 140530037004032 logging_writer.py:48] [61200] global_step=61200, grad_norm=0.37455448508262634, loss=2.9334776401519775
I0206 21:28:07.803588 140530045396736 logging_writer.py:48] [61300] global_step=61300, grad_norm=0.36141684651374817, loss=2.8683345317840576
I0206 21:28:43.014737 140530037004032 logging_writer.py:48] [61400] global_step=61400, grad_norm=0.3389289975166321, loss=2.8713254928588867
I0206 21:29:18.205196 140530045396736 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.36014655232429504, loss=2.8559327125549316
I0206 21:29:53.398633 140530037004032 logging_writer.py:48] [61600] global_step=61600, grad_norm=0.40800464153289795, loss=2.8233675956726074
I0206 21:30:28.604746 140530045396736 logging_writer.py:48] [61700] global_step=61700, grad_norm=0.34171372652053833, loss=2.892385959625244
I0206 21:31:03.840509 140530037004032 logging_writer.py:48] [61800] global_step=61800, grad_norm=0.34051817655563354, loss=2.8832287788391113
I0206 21:31:39.057482 140530045396736 logging_writer.py:48] [61900] global_step=61900, grad_norm=0.35152044892311096, loss=2.932211399078369
I0206 21:32:14.287465 140530037004032 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.3662835359573364, loss=2.947385549545288
I0206 21:32:21.060568 140699726837568 spec.py:321] Evaluating on the training split.
I0206 21:32:24.097821 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 21:35:28.022269 140699726837568 spec.py:333] Evaluating on the validation split.
I0206 21:35:30.757233 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 21:38:04.246720 140699726837568 spec.py:349] Evaluating on the test split.
I0206 21:38:06.967650 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 21:40:27.925789 140699726837568 submission_runner.py:408] Time since start: 36047.12s, 	Step: 62021, 	{'train/accuracy': 0.6487945318222046, 'train/loss': 1.79489004611969, 'train/bleu': 32.09413766752779, 'validation/accuracy': 0.6691547632217407, 'validation/loss': 1.6473053693771362, 'validation/bleu': 28.967600319487286, 'validation/num_examples': 3000, 'test/accuracy': 0.6833536624908447, 'test/loss': 1.5698957443237305, 'test/bleu': 28.951198785785074, 'test/num_examples': 3003, 'score': 21878.683776140213, 'total_duration': 36047.12191772461, 'accumulated_submission_time': 21878.683776140213, 'accumulated_eval_time': 14165.701899528503, 'accumulated_logging_time': 0.7960004806518555}
I0206 21:40:27.949044 140530045396736 logging_writer.py:48] [62021] accumulated_eval_time=14165.701900, accumulated_logging_time=0.796000, accumulated_submission_time=21878.683776, global_step=62021, preemption_count=0, score=21878.683776, test/accuracy=0.683354, test/bleu=28.951199, test/loss=1.569896, test/num_examples=3003, total_duration=36047.121918, train/accuracy=0.648795, train/bleu=32.094138, train/loss=1.794890, validation/accuracy=0.669155, validation/bleu=28.967600, validation/loss=1.647305, validation/num_examples=3000
I0206 21:40:56.035876 140530037004032 logging_writer.py:48] [62100] global_step=62100, grad_norm=0.3928448557853699, loss=2.8731651306152344
I0206 21:41:31.154467 140530045396736 logging_writer.py:48] [62200] global_step=62200, grad_norm=0.340719074010849, loss=2.9105477333068848
I0206 21:42:06.373932 140530037004032 logging_writer.py:48] [62300] global_step=62300, grad_norm=0.3623959422111511, loss=2.830333948135376
I0206 21:42:41.614024 140530045396736 logging_writer.py:48] [62400] global_step=62400, grad_norm=0.36662134528160095, loss=2.89897084236145
I0206 21:43:16.836338 140530037004032 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.38731279969215393, loss=2.9344801902770996
I0206 21:43:52.143669 140530045396736 logging_writer.py:48] [62600] global_step=62600, grad_norm=0.3933289051055908, loss=2.943084716796875
I0206 21:44:27.366482 140530037004032 logging_writer.py:48] [62700] global_step=62700, grad_norm=0.39616069197654724, loss=2.9439027309417725
I0206 21:45:02.588689 140530045396736 logging_writer.py:48] [62800] global_step=62800, grad_norm=0.32780203223228455, loss=2.873152732849121
I0206 21:45:37.794936 140530037004032 logging_writer.py:48] [62900] global_step=62900, grad_norm=0.34730303287506104, loss=2.9413716793060303
I0206 21:46:13.019876 140530045396736 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.36732017993927, loss=2.914574146270752
I0206 21:46:48.236414 140530037004032 logging_writer.py:48] [63100] global_step=63100, grad_norm=0.35974863171577454, loss=2.9008774757385254
I0206 21:47:23.447739 140530045396736 logging_writer.py:48] [63200] global_step=63200, grad_norm=0.37593895196914673, loss=2.911888837814331
I0206 21:47:58.629481 140530037004032 logging_writer.py:48] [63300] global_step=63300, grad_norm=0.40278488397598267, loss=2.914747714996338
I0206 21:48:33.864060 140530045396736 logging_writer.py:48] [63400] global_step=63400, grad_norm=0.3620114326477051, loss=2.838299512863159
I0206 21:49:09.103081 140530037004032 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.36260274052619934, loss=2.827242612838745
I0206 21:49:44.309082 140530045396736 logging_writer.py:48] [63600] global_step=63600, grad_norm=0.3577426075935364, loss=2.8500170707702637
I0206 21:50:19.493419 140530037004032 logging_writer.py:48] [63700] global_step=63700, grad_norm=0.3637620806694031, loss=2.889516592025757
I0206 21:50:54.709581 140530045396736 logging_writer.py:48] [63800] global_step=63800, grad_norm=0.39544984698295593, loss=2.9250741004943848
I0206 21:51:29.929704 140530037004032 logging_writer.py:48] [63900] global_step=63900, grad_norm=0.40840065479278564, loss=2.9348766803741455
I0206 21:52:05.147012 140530045396736 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.32681724429130554, loss=2.8198046684265137
I0206 21:52:40.352350 140530037004032 logging_writer.py:48] [64100] global_step=64100, grad_norm=0.3995206952095032, loss=2.8807501792907715
I0206 21:53:15.559406 140530045396736 logging_writer.py:48] [64200] global_step=64200, grad_norm=0.3461494743824005, loss=2.952941417694092
I0206 21:53:50.743778 140530037004032 logging_writer.py:48] [64300] global_step=64300, grad_norm=0.3517209589481354, loss=2.8765830993652344
I0206 21:54:25.954478 140530045396736 logging_writer.py:48] [64400] global_step=64400, grad_norm=0.3576385974884033, loss=2.866058826446533
I0206 21:54:28.137205 140699726837568 spec.py:321] Evaluating on the training split.
I0206 21:54:31.170576 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 21:57:25.041542 140699726837568 spec.py:333] Evaluating on the validation split.
I0206 21:57:27.774276 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 21:59:59.654749 140699726837568 spec.py:349] Evaluating on the test split.
I0206 22:00:02.387190 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 22:02:27.220329 140699726837568 submission_runner.py:408] Time since start: 37366.42s, 	Step: 64408, 	{'train/accuracy': 0.6550009250640869, 'train/loss': 1.7537881135940552, 'train/bleu': 32.452760515426576, 'validation/accuracy': 0.6708534359931946, 'validation/loss': 1.635170578956604, 'validation/bleu': 28.91921608404348, 'validation/num_examples': 3000, 'test/accuracy': 0.6835861206054688, 'test/loss': 1.5647251605987549, 'test/bleu': 29.056131315412504, 'test/num_examples': 3003, 'score': 22718.78689146042, 'total_duration': 37366.41644477844, 'accumulated_submission_time': 22718.78689146042, 'accumulated_eval_time': 14644.784934043884, 'accumulated_logging_time': 0.8291144371032715}
I0206 22:02:27.243890 140530037004032 logging_writer.py:48] [64408] accumulated_eval_time=14644.784934, accumulated_logging_time=0.829114, accumulated_submission_time=22718.786891, global_step=64408, preemption_count=0, score=22718.786891, test/accuracy=0.683586, test/bleu=29.056131, test/loss=1.564725, test/num_examples=3003, total_duration=37366.416445, train/accuracy=0.655001, train/bleu=32.452761, train/loss=1.753788, validation/accuracy=0.670853, validation/bleu=28.919216, validation/loss=1.635171, validation/num_examples=3000
I0206 22:02:59.874441 140530045396736 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.39188268780708313, loss=2.9497554302215576
I0206 22:03:35.001199 140530037004032 logging_writer.py:48] [64600] global_step=64600, grad_norm=0.34876927733421326, loss=2.858518600463867
I0206 22:04:10.171256 140530045396736 logging_writer.py:48] [64700] global_step=64700, grad_norm=0.3341274857521057, loss=2.833040952682495
I0206 22:04:45.377523 140530037004032 logging_writer.py:48] [64800] global_step=64800, grad_norm=0.3326399326324463, loss=2.8874893188476562
I0206 22:05:20.585559 140530045396736 logging_writer.py:48] [64900] global_step=64900, grad_norm=0.3585915267467499, loss=2.9570765495300293
I0206 22:05:55.777547 140530037004032 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.36282655596733093, loss=2.8447279930114746
I0206 22:06:30.963095 140530045396736 logging_writer.py:48] [65100] global_step=65100, grad_norm=0.37669625878334045, loss=2.8547768592834473
I0206 22:07:06.168186 140530037004032 logging_writer.py:48] [65200] global_step=65200, grad_norm=0.34139811992645264, loss=2.8851232528686523
I0206 22:07:41.385103 140530045396736 logging_writer.py:48] [65300] global_step=65300, grad_norm=0.36268898844718933, loss=2.9653265476226807
I0206 22:08:16.589555 140530037004032 logging_writer.py:48] [65400] global_step=65400, grad_norm=0.3423500955104828, loss=2.8432281017303467
I0206 22:08:51.805509 140530045396736 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.39455196261405945, loss=2.86130690574646
I0206 22:09:27.004839 140530037004032 logging_writer.py:48] [65600] global_step=65600, grad_norm=0.3994724452495575, loss=2.8246095180511475
I0206 22:10:02.242169 140530045396736 logging_writer.py:48] [65700] global_step=65700, grad_norm=0.34293535351753235, loss=2.8987042903900146
I0206 22:10:37.481135 140530037004032 logging_writer.py:48] [65800] global_step=65800, grad_norm=0.3914862871170044, loss=2.845471143722534
I0206 22:11:12.723584 140530045396736 logging_writer.py:48] [65900] global_step=65900, grad_norm=0.3887566030025482, loss=2.9268767833709717
I0206 22:11:47.984510 140530037004032 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.38151225447654724, loss=2.9045040607452393
I0206 22:12:23.206784 140530045396736 logging_writer.py:48] [66100] global_step=66100, grad_norm=0.34627899527549744, loss=2.8831403255462646
I0206 22:12:58.484388 140530037004032 logging_writer.py:48] [66200] global_step=66200, grad_norm=0.4366670548915863, loss=2.8114430904388428
I0206 22:13:33.693490 140530045396736 logging_writer.py:48] [66300] global_step=66300, grad_norm=0.35278624296188354, loss=2.811960220336914
I0206 22:14:08.920540 140530037004032 logging_writer.py:48] [66400] global_step=66400, grad_norm=0.39980390667915344, loss=2.8299577236175537
I0206 22:14:44.136816 140530045396736 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.3495052456855774, loss=2.9330146312713623
I0206 22:15:19.391632 140530037004032 logging_writer.py:48] [66600] global_step=66600, grad_norm=0.36018410325050354, loss=2.938004493713379
I0206 22:15:54.605199 140530045396736 logging_writer.py:48] [66700] global_step=66700, grad_norm=0.3633224368095398, loss=2.8515985012054443
I0206 22:16:27.423818 140699726837568 spec.py:321] Evaluating on the training split.
I0206 22:16:30.453805 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 22:19:17.913439 140699726837568 spec.py:333] Evaluating on the validation split.
I0206 22:19:20.636966 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 22:21:53.933235 140699726837568 spec.py:349] Evaluating on the test split.
I0206 22:21:56.655807 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 22:24:27.573421 140699726837568 submission_runner.py:408] Time since start: 38686.77s, 	Step: 66795, 	{'train/accuracy': 0.6545759439468384, 'train/loss': 1.7578319311141968, 'train/bleu': 32.203787090257514, 'validation/accuracy': 0.6715105772018433, 'validation/loss': 1.635469675064087, 'validation/bleu': 29.412971115276957, 'validation/num_examples': 3000, 'test/accuracy': 0.6852013468742371, 'test/loss': 1.5587241649627686, 'test/bleu': 29.178349712756706, 'test/num_examples': 3003, 'score': 23558.88369011879, 'total_duration': 38686.76952624321, 'accumulated_submission_time': 23558.88369011879, 'accumulated_eval_time': 15124.934435129166, 'accumulated_logging_time': 0.8623538017272949}
I0206 22:24:27.602136 140530037004032 logging_writer.py:48] [66795] accumulated_eval_time=15124.934435, accumulated_logging_time=0.862354, accumulated_submission_time=23558.883690, global_step=66795, preemption_count=0, score=23558.883690, test/accuracy=0.685201, test/bleu=29.178350, test/loss=1.558724, test/num_examples=3003, total_duration=38686.769526, train/accuracy=0.654576, train/bleu=32.203787, train/loss=1.757832, validation/accuracy=0.671511, validation/bleu=29.412971, validation/loss=1.635470, validation/num_examples=3000
I0206 22:24:29.734377 140530045396736 logging_writer.py:48] [66800] global_step=66800, grad_norm=0.34122011065483093, loss=2.8950798511505127
I0206 22:25:04.869109 140530037004032 logging_writer.py:48] [66900] global_step=66900, grad_norm=0.36211371421813965, loss=2.838221788406372
I0206 22:25:40.109310 140530045396736 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.3697883188724518, loss=2.8648626804351807
I0206 22:26:15.278187 140530037004032 logging_writer.py:48] [67100] global_step=67100, grad_norm=0.37595048546791077, loss=2.8910651206970215
I0206 22:26:50.482954 140530045396736 logging_writer.py:48] [67200] global_step=67200, grad_norm=0.3824196755886078, loss=2.9647858142852783
I0206 22:27:25.735202 140530037004032 logging_writer.py:48] [67300] global_step=67300, grad_norm=0.3315863609313965, loss=2.85823392868042
I0206 22:28:00.970171 140530045396736 logging_writer.py:48] [67400] global_step=67400, grad_norm=0.3498844802379608, loss=2.8348615169525146
I0206 22:28:36.223735 140530037004032 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.34817269444465637, loss=2.867210626602173
I0206 22:29:11.466954 140530045396736 logging_writer.py:48] [67600] global_step=67600, grad_norm=0.35495537519454956, loss=2.8853654861450195
I0206 22:29:46.677432 140530037004032 logging_writer.py:48] [67700] global_step=67700, grad_norm=0.3354519307613373, loss=2.902855157852173
I0206 22:30:21.905350 140530045396736 logging_writer.py:48] [67800] global_step=67800, grad_norm=0.35163745284080505, loss=2.881037950515747
I0206 22:30:57.146996 140530037004032 logging_writer.py:48] [67900] global_step=67900, grad_norm=0.36074888706207275, loss=2.8039937019348145
I0206 22:31:32.369306 140530045396736 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.38677293062210083, loss=2.948593854904175
I0206 22:32:07.654134 140530037004032 logging_writer.py:48] [68100] global_step=68100, grad_norm=0.3658229410648346, loss=2.8530566692352295
I0206 22:32:42.885163 140530045396736 logging_writer.py:48] [68200] global_step=68200, grad_norm=0.3849464952945709, loss=2.87180757522583
I0206 22:33:18.117800 140530037004032 logging_writer.py:48] [68300] global_step=68300, grad_norm=0.36501604318618774, loss=2.9136834144592285
I0206 22:33:53.350900 140530045396736 logging_writer.py:48] [68400] global_step=68400, grad_norm=0.37544214725494385, loss=2.9752519130706787
I0206 22:34:28.589764 140530037004032 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.3606986403465271, loss=2.8698179721832275
I0206 22:35:03.792661 140530045396736 logging_writer.py:48] [68600] global_step=68600, grad_norm=0.38180968165397644, loss=2.805454969406128
I0206 22:35:38.993636 140530037004032 logging_writer.py:48] [68700] global_step=68700, grad_norm=0.35728737711906433, loss=2.826723575592041
I0206 22:36:14.240323 140530045396736 logging_writer.py:48] [68800] global_step=68800, grad_norm=0.39392733573913574, loss=2.93666672706604
I0206 22:36:49.453120 140530037004032 logging_writer.py:48] [68900] global_step=68900, grad_norm=0.3718971908092499, loss=2.907332420349121
I0206 22:37:24.679529 140530045396736 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.37462347745895386, loss=2.873185873031616
I0206 22:37:59.888156 140530037004032 logging_writer.py:48] [69100] global_step=69100, grad_norm=0.3817545473575592, loss=2.9081037044525146
I0206 22:38:27.789344 140699726837568 spec.py:321] Evaluating on the training split.
I0206 22:38:30.825947 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 22:41:48.126505 140699726837568 spec.py:333] Evaluating on the validation split.
I0206 22:41:50.852655 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 22:44:39.910160 140699726837568 spec.py:349] Evaluating on the test split.
I0206 22:44:42.628273 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 22:47:23.554240 140699726837568 submission_runner.py:408] Time since start: 40062.75s, 	Step: 69181, 	{'train/accuracy': 0.6702680587768555, 'train/loss': 1.6583515405654907, 'train/bleu': 33.066980321510584, 'validation/accuracy': 0.6727132797241211, 'validation/loss': 1.62405264377594, 'validation/bleu': 29.20222921314499, 'validation/num_examples': 3000, 'test/accuracy': 0.6837836503982544, 'test/loss': 1.5548111200332642, 'test/bleu': 28.954724806981417, 'test/num_examples': 3003, 'score': 24398.984651088715, 'total_duration': 40062.75036764145, 'accumulated_submission_time': 24398.984651088715, 'accumulated_eval_time': 15660.699274778366, 'accumulated_logging_time': 0.9025025367736816}
I0206 22:47:23.578248 140530045396736 logging_writer.py:48] [69181] accumulated_eval_time=15660.699275, accumulated_logging_time=0.902503, accumulated_submission_time=24398.984651, global_step=69181, preemption_count=0, score=24398.984651, test/accuracy=0.683784, test/bleu=28.954725, test/loss=1.554811, test/num_examples=3003, total_duration=40062.750368, train/accuracy=0.670268, train/bleu=33.066980, train/loss=1.658352, validation/accuracy=0.672713, validation/bleu=29.202229, validation/loss=1.624053, validation/num_examples=3000
I0206 22:47:30.603875 140530037004032 logging_writer.py:48] [69200] global_step=69200, grad_norm=0.35511136054992676, loss=2.891606092453003
I0206 22:48:05.695636 140530045396736 logging_writer.py:48] [69300] global_step=69300, grad_norm=0.3677343726158142, loss=2.940161943435669
I0206 22:48:40.843050 140530037004032 logging_writer.py:48] [69400] global_step=69400, grad_norm=0.34276971220970154, loss=2.824462652206421
I0206 22:49:16.051123 140530045396736 logging_writer.py:48] [69500] global_step=69500, grad_norm=0.3759562075138092, loss=2.8509838581085205
I0206 22:49:51.269890 140530037004032 logging_writer.py:48] [69600] global_step=69600, grad_norm=0.34358125925064087, loss=2.86096453666687
I0206 22:50:26.468771 140530045396736 logging_writer.py:48] [69700] global_step=69700, grad_norm=0.40329596400260925, loss=2.866576910018921
I0206 22:51:01.686043 140530037004032 logging_writer.py:48] [69800] global_step=69800, grad_norm=0.3820646405220032, loss=2.845102548599243
I0206 22:51:36.926282 140530045396736 logging_writer.py:48] [69900] global_step=69900, grad_norm=0.3765411376953125, loss=2.845020055770874
I0206 22:52:12.151687 140530037004032 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.3283601403236389, loss=2.8650734424591064
I0206 22:52:47.347480 140530045396736 logging_writer.py:48] [70100] global_step=70100, grad_norm=0.34633639454841614, loss=2.8682773113250732
I0206 22:53:22.567008 140530037004032 logging_writer.py:48] [70200] global_step=70200, grad_norm=0.3751990497112274, loss=2.821322441101074
I0206 22:53:57.767867 140530045396736 logging_writer.py:48] [70300] global_step=70300, grad_norm=0.3270202875137329, loss=2.8274242877960205
I0206 22:54:32.967466 140530037004032 logging_writer.py:48] [70400] global_step=70400, grad_norm=0.39972299337387085, loss=2.866745948791504
I0206 22:55:08.181004 140530045396736 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.39633241295814514, loss=2.835941791534424
I0206 22:55:43.405457 140530037004032 logging_writer.py:48] [70600] global_step=70600, grad_norm=0.31676703691482544, loss=2.878105640411377
I0206 22:56:18.621151 140530045396736 logging_writer.py:48] [70700] global_step=70700, grad_norm=0.3574771583080292, loss=2.8545379638671875
I0206 22:56:53.835653 140530037004032 logging_writer.py:48] [70800] global_step=70800, grad_norm=0.3628198206424713, loss=2.831594944000244
I0206 22:57:29.094863 140530045396736 logging_writer.py:48] [70900] global_step=70900, grad_norm=0.3784921169281006, loss=2.840860605239868
I0206 22:58:04.313447 140530037004032 logging_writer.py:48] [71000] global_step=71000, grad_norm=0.36736470460891724, loss=2.819143056869507
I0206 22:58:39.552155 140530045396736 logging_writer.py:48] [71100] global_step=71100, grad_norm=0.37970298528671265, loss=2.8449156284332275
I0206 22:59:14.833899 140530037004032 logging_writer.py:48] [71200] global_step=71200, grad_norm=0.3503243327140808, loss=2.8540897369384766
I0206 22:59:50.026613 140530045396736 logging_writer.py:48] [71300] global_step=71300, grad_norm=0.34793373942375183, loss=2.8614072799682617
I0206 23:00:25.239347 140530037004032 logging_writer.py:48] [71400] global_step=71400, grad_norm=0.36845824122428894, loss=2.8806822299957275
I0206 23:01:00.435330 140530045396736 logging_writer.py:48] [71500] global_step=71500, grad_norm=0.3818696439266205, loss=2.8688933849334717
I0206 23:01:23.766428 140699726837568 spec.py:321] Evaluating on the training split.
I0206 23:01:26.789538 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 23:05:05.248966 140699726837568 spec.py:333] Evaluating on the validation split.
I0206 23:05:07.977861 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 23:07:41.005259 140699726837568 spec.py:349] Evaluating on the test split.
I0206 23:07:43.747601 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 23:10:13.882642 140699726837568 submission_runner.py:408] Time since start: 41433.08s, 	Step: 71568, 	{'train/accuracy': 0.655456006526947, 'train/loss': 1.7491358518600464, 'train/bleu': 32.35903194112188, 'validation/accuracy': 0.6749451160430908, 'validation/loss': 1.6112587451934814, 'validation/bleu': 29.356844389961918, 'validation/num_examples': 3000, 'test/accuracy': 0.6880018711090088, 'test/loss': 1.5388880968093872, 'test/bleu': 28.961093312146044, 'test/num_examples': 3003, 'score': 25239.088314056396, 'total_duration': 41433.078765153885, 'accumulated_submission_time': 25239.088314056396, 'accumulated_eval_time': 16190.815400600433, 'accumulated_logging_time': 0.9364674091339111}
I0206 23:10:13.911608 140530037004032 logging_writer.py:48] [71568] accumulated_eval_time=16190.815401, accumulated_logging_time=0.936467, accumulated_submission_time=25239.088314, global_step=71568, preemption_count=0, score=25239.088314, test/accuracy=0.688002, test/bleu=28.961093, test/loss=1.538888, test/num_examples=3003, total_duration=41433.078765, train/accuracy=0.655456, train/bleu=32.359032, train/loss=1.749136, validation/accuracy=0.674945, validation/bleu=29.356844, validation/loss=1.611259, validation/num_examples=3000
I0206 23:10:25.521259 140530045396736 logging_writer.py:48] [71600] global_step=71600, grad_norm=0.45423269271850586, loss=2.8106932640075684
I0206 23:11:00.696829 140530037004032 logging_writer.py:48] [71700] global_step=71700, grad_norm=0.3706144094467163, loss=2.9095382690429688
I0206 23:11:35.872142 140530045396736 logging_writer.py:48] [71800] global_step=71800, grad_norm=0.3358650207519531, loss=2.82551646232605
I0206 23:12:11.077490 140530037004032 logging_writer.py:48] [71900] global_step=71900, grad_norm=0.3891952335834503, loss=2.8708906173706055
I0206 23:12:46.250628 140530045396736 logging_writer.py:48] [72000] global_step=72000, grad_norm=0.37372907996177673, loss=2.7945704460144043
I0206 23:13:21.448713 140530037004032 logging_writer.py:48] [72100] global_step=72100, grad_norm=0.36664554476737976, loss=2.82620906829834
I0206 23:13:56.633367 140530045396736 logging_writer.py:48] [72200] global_step=72200, grad_norm=0.3623621165752411, loss=2.836501121520996
I0206 23:14:31.841157 140530037004032 logging_writer.py:48] [72300] global_step=72300, grad_norm=0.3734101951122284, loss=2.829801321029663
I0206 23:15:07.050590 140530045396736 logging_writer.py:48] [72400] global_step=72400, grad_norm=0.3535504639148712, loss=2.8962740898132324
I0206 23:15:42.247418 140530037004032 logging_writer.py:48] [72500] global_step=72500, grad_norm=0.3664051294326782, loss=2.896146535873413
I0206 23:16:17.453962 140530045396736 logging_writer.py:48] [72600] global_step=72600, grad_norm=0.3685894310474396, loss=2.835675001144409
I0206 23:16:52.687047 140530037004032 logging_writer.py:48] [72700] global_step=72700, grad_norm=0.3421987295150757, loss=2.792158603668213
I0206 23:17:27.914603 140530045396736 logging_writer.py:48] [72800] global_step=72800, grad_norm=0.36187446117401123, loss=2.898179531097412
I0206 23:18:03.168853 140530037004032 logging_writer.py:48] [72900] global_step=72900, grad_norm=0.3596983850002289, loss=2.9319090843200684
I0206 23:18:38.377731 140530045396736 logging_writer.py:48] [73000] global_step=73000, grad_norm=0.3477342128753662, loss=2.8735358715057373
I0206 23:19:13.574508 140530037004032 logging_writer.py:48] [73100] global_step=73100, grad_norm=0.35312971472740173, loss=2.8567850589752197
I0206 23:19:48.776014 140530045396736 logging_writer.py:48] [73200] global_step=73200, grad_norm=0.36559778451919556, loss=2.857525110244751
I0206 23:20:24.008688 140530037004032 logging_writer.py:48] [73300] global_step=73300, grad_norm=0.360480397939682, loss=2.8481156826019287
I0206 23:20:59.264383 140530045396736 logging_writer.py:48] [73400] global_step=73400, grad_norm=0.3512066900730133, loss=2.8021936416625977
I0206 23:21:34.507738 140530037004032 logging_writer.py:48] [73500] global_step=73500, grad_norm=0.36239710450172424, loss=2.8883485794067383
I0206 23:22:09.716804 140530045396736 logging_writer.py:48] [73600] global_step=73600, grad_norm=0.40312784910202026, loss=2.7536303997039795
I0206 23:22:44.923073 140530037004032 logging_writer.py:48] [73700] global_step=73700, grad_norm=0.3761012554168701, loss=2.930846691131592
I0206 23:23:20.154678 140530045396736 logging_writer.py:48] [73800] global_step=73800, grad_norm=0.3937123417854309, loss=2.8459882736206055
I0206 23:23:55.351743 140530037004032 logging_writer.py:48] [73900] global_step=73900, grad_norm=0.41414874792099, loss=2.867757558822632
I0206 23:24:14.081592 140699726837568 spec.py:321] Evaluating on the training split.
I0206 23:24:17.109889 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 23:27:49.746849 140699726837568 spec.py:333] Evaluating on the validation split.
I0206 23:27:52.475276 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 23:31:45.470099 140699726837568 spec.py:349] Evaluating on the test split.
I0206 23:31:48.203773 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 23:34:50.223977 140699726837568 submission_runner.py:408] Time since start: 42909.42s, 	Step: 73955, 	{'train/accuracy': 0.6537445783615112, 'train/loss': 1.7614141702651978, 'train/bleu': 32.23235903214701, 'validation/accuracy': 0.6735440492630005, 'validation/loss': 1.6155986785888672, 'validation/bleu': 28.445815162064275, 'validation/num_examples': 3000, 'test/accuracy': 0.6869211792945862, 'test/loss': 1.5449918508529663, 'test/bleu': 29.291123433349274, 'test/num_examples': 3003, 'score': 26079.173448324203, 'total_duration': 42909.42013859749, 'accumulated_submission_time': 26079.173448324203, 'accumulated_eval_time': 16826.95787167549, 'accumulated_logging_time': 0.9769396781921387}
I0206 23:34:50.249245 140530045396736 logging_writer.py:48] [73955] accumulated_eval_time=16826.957872, accumulated_logging_time=0.976940, accumulated_submission_time=26079.173448, global_step=73955, preemption_count=0, score=26079.173448, test/accuracy=0.686921, test/bleu=29.291123, test/loss=1.544992, test/num_examples=3003, total_duration=42909.420139, train/accuracy=0.653745, train/bleu=32.232359, train/loss=1.761414, validation/accuracy=0.673544, validation/bleu=28.445815, validation/loss=1.615599, validation/num_examples=3000
I0206 23:35:06.384228 140530037004032 logging_writer.py:48] [74000] global_step=74000, grad_norm=0.3851659595966339, loss=2.921738386154175
I0206 23:35:41.523231 140530045396736 logging_writer.py:48] [74100] global_step=74100, grad_norm=0.37560051679611206, loss=2.8822593688964844
I0206 23:36:16.740787 140530037004032 logging_writer.py:48] [74200] global_step=74200, grad_norm=0.3841444253921509, loss=2.809567928314209
I0206 23:36:51.943993 140530045396736 logging_writer.py:48] [74300] global_step=74300, grad_norm=0.36877861618995667, loss=2.866971969604492
I0206 23:37:27.212683 140530037004032 logging_writer.py:48] [74400] global_step=74400, grad_norm=0.3534524142742157, loss=2.8305230140686035
I0206 23:38:02.456360 140530045396736 logging_writer.py:48] [74500] global_step=74500, grad_norm=0.35842886567115784, loss=2.828610420227051
I0206 23:38:37.733278 140530037004032 logging_writer.py:48] [74600] global_step=74600, grad_norm=0.39802640676498413, loss=2.8340790271759033
I0206 23:39:12.976469 140530045396736 logging_writer.py:48] [74700] global_step=74700, grad_norm=0.37703847885131836, loss=2.8738958835601807
I0206 23:39:48.187640 140530037004032 logging_writer.py:48] [74800] global_step=74800, grad_norm=0.38360923528671265, loss=2.8820929527282715
I0206 23:40:23.557475 140530045396736 logging_writer.py:48] [74900] global_step=74900, grad_norm=0.34960511326789856, loss=2.8403756618499756
I0206 23:40:58.851620 140530037004032 logging_writer.py:48] [75000] global_step=75000, grad_norm=0.361722856760025, loss=2.847581624984741
I0206 23:41:34.098106 140530045396736 logging_writer.py:48] [75100] global_step=75100, grad_norm=0.3771395981311798, loss=2.857546329498291
I0206 23:42:09.340716 140530037004032 logging_writer.py:48] [75200] global_step=75200, grad_norm=0.3753611743450165, loss=2.8422865867614746
I0206 23:42:44.606095 140530045396736 logging_writer.py:48] [75300] global_step=75300, grad_norm=0.37155917286872864, loss=2.8120124340057373
I0206 23:43:19.854687 140530037004032 logging_writer.py:48] [75400] global_step=75400, grad_norm=0.3688506782054901, loss=2.9275405406951904
I0206 23:43:55.065084 140530045396736 logging_writer.py:48] [75500] global_step=75500, grad_norm=0.3490544855594635, loss=2.8609108924865723
I0206 23:44:30.283848 140530037004032 logging_writer.py:48] [75600] global_step=75600, grad_norm=0.3678499460220337, loss=2.7685773372650146
I0206 23:45:05.531579 140530045396736 logging_writer.py:48] [75700] global_step=75700, grad_norm=0.3322781026363373, loss=2.8687551021575928
I0206 23:45:40.797904 140530037004032 logging_writer.py:48] [75800] global_step=75800, grad_norm=0.36422568559646606, loss=2.816094398498535
I0206 23:46:16.089880 140530045396736 logging_writer.py:48] [75900] global_step=75900, grad_norm=0.3624705672264099, loss=2.946409225463867
I0206 23:46:51.299034 140530037004032 logging_writer.py:48] [76000] global_step=76000, grad_norm=0.376630961894989, loss=2.8046717643737793
I0206 23:47:26.519490 140530045396736 logging_writer.py:48] [76100] global_step=76100, grad_norm=0.3664804995059967, loss=2.817645788192749
I0206 23:48:01.745493 140530037004032 logging_writer.py:48] [76200] global_step=76200, grad_norm=0.38867101073265076, loss=2.8556647300720215
I0206 23:48:36.979302 140530045396736 logging_writer.py:48] [76300] global_step=76300, grad_norm=0.3533168137073517, loss=2.870551109313965
I0206 23:48:50.431007 140699726837568 spec.py:321] Evaluating on the training split.
I0206 23:48:53.457894 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 23:51:49.773340 140699726837568 spec.py:333] Evaluating on the validation split.
I0206 23:51:52.511362 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 23:54:27.310610 140699726837568 spec.py:349] Evaluating on the test split.
I0206 23:54:30.038479 140699726837568 workload.py:181] Translating evaluation dataset.
I0206 23:56:51.538649 140699726837568 submission_runner.py:408] Time since start: 44230.73s, 	Step: 76340, 	{'train/accuracy': 0.6645991206169128, 'train/loss': 1.6866929531097412, 'train/bleu': 32.924616682714294, 'validation/accuracy': 0.67549067735672, 'validation/loss': 1.6071454286575317, 'validation/bleu': 29.642798192788497, 'validation/num_examples': 3000, 'test/accuracy': 0.6900935769081116, 'test/loss': 1.5276169776916504, 'test/bleu': 29.379224504547373, 'test/num_examples': 3003, 'score': 26919.26244521141, 'total_duration': 44230.73479604721, 'accumulated_submission_time': 26919.26244521141, 'accumulated_eval_time': 17308.0654463768, 'accumulated_logging_time': 1.0143301486968994}
I0206 23:56:51.565143 140530037004032 logging_writer.py:48] [76340] accumulated_eval_time=17308.065446, accumulated_logging_time=1.014330, accumulated_submission_time=26919.262445, global_step=76340, preemption_count=0, score=26919.262445, test/accuracy=0.690094, test/bleu=29.379225, test/loss=1.527617, test/num_examples=3003, total_duration=44230.734796, train/accuracy=0.664599, train/bleu=32.924617, train/loss=1.686693, validation/accuracy=0.675491, validation/bleu=29.642798, validation/loss=1.607145, validation/num_examples=3000
I0206 23:57:12.995620 140530045396736 logging_writer.py:48] [76400] global_step=76400, grad_norm=0.37419044971466064, loss=2.8016021251678467
I0206 23:57:48.103682 140530037004032 logging_writer.py:48] [76500] global_step=76500, grad_norm=0.362906277179718, loss=2.7945499420166016
I0206 23:58:23.280316 140530045396736 logging_writer.py:48] [76600] global_step=76600, grad_norm=0.3942860960960388, loss=2.831523895263672
I0206 23:58:58.482224 140530037004032 logging_writer.py:48] [76700] global_step=76700, grad_norm=0.36914879083633423, loss=2.881667137145996
I0206 23:59:33.701354 140530045396736 logging_writer.py:48] [76800] global_step=76800, grad_norm=0.37475740909576416, loss=2.7707152366638184
I0207 00:00:08.917046 140530037004032 logging_writer.py:48] [76900] global_step=76900, grad_norm=0.3592257797718048, loss=2.923074245452881
I0207 00:00:44.116253 140530045396736 logging_writer.py:48] [77000] global_step=77000, grad_norm=0.36856386065483093, loss=2.8405539989471436
I0207 00:01:19.312010 140530037004032 logging_writer.py:48] [77100] global_step=77100, grad_norm=0.33250299096107483, loss=2.826770067214966
I0207 00:01:54.551187 140530045396736 logging_writer.py:48] [77200] global_step=77200, grad_norm=0.4208783209323883, loss=2.8402442932128906
I0207 00:02:29.802263 140530037004032 logging_writer.py:48] [77300] global_step=77300, grad_norm=0.38405168056488037, loss=2.856414794921875
I0207 00:03:05.052181 140530045396736 logging_writer.py:48] [77400] global_step=77400, grad_norm=0.39665552973747253, loss=2.830338716506958
I0207 00:03:40.276598 140530037004032 logging_writer.py:48] [77500] global_step=77500, grad_norm=0.378763347864151, loss=2.8221936225891113
I0207 00:04:15.478929 140530045396736 logging_writer.py:48] [77600] global_step=77600, grad_norm=0.3832310736179352, loss=2.8503785133361816
I0207 00:04:50.729947 140530037004032 logging_writer.py:48] [77700] global_step=77700, grad_norm=0.3630354702472687, loss=2.8667514324188232
I0207 00:05:25.961516 140530045396736 logging_writer.py:48] [77800] global_step=77800, grad_norm=0.3770390748977661, loss=2.8183231353759766
I0207 00:06:01.159802 140530037004032 logging_writer.py:48] [77900] global_step=77900, grad_norm=0.400863915681839, loss=2.8993632793426514
I0207 00:06:36.386981 140530045396736 logging_writer.py:48] [78000] global_step=78000, grad_norm=0.37578344345092773, loss=2.8265793323516846
I0207 00:07:11.606860 140530037004032 logging_writer.py:48] [78100] global_step=78100, grad_norm=0.33830294013023376, loss=2.823631525039673
I0207 00:07:46.849375 140530045396736 logging_writer.py:48] [78200] global_step=78200, grad_norm=0.4203367531299591, loss=2.8933730125427246
I0207 00:08:22.093202 140530037004032 logging_writer.py:48] [78300] global_step=78300, grad_norm=0.3751031458377838, loss=2.8718035221099854
I0207 00:08:57.292554 140530045396736 logging_writer.py:48] [78400] global_step=78400, grad_norm=0.47391393780708313, loss=2.866971254348755
I0207 00:09:32.499308 140530037004032 logging_writer.py:48] [78500] global_step=78500, grad_norm=0.3687596023082733, loss=2.803926944732666
I0207 00:10:07.730363 140530045396736 logging_writer.py:48] [78600] global_step=78600, grad_norm=0.36850136518478394, loss=2.74214243888855
I0207 00:10:42.936450 140530037004032 logging_writer.py:48] [78700] global_step=78700, grad_norm=0.38774263858795166, loss=2.831676959991455
I0207 00:10:51.809824 140699726837568 spec.py:321] Evaluating on the training split.
I0207 00:10:54.846757 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 00:13:44.913161 140699726837568 spec.py:333] Evaluating on the validation split.
I0207 00:13:47.648650 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 00:16:24.863511 140699726837568 spec.py:349] Evaluating on the test split.
I0207 00:16:27.589451 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 00:19:02.128822 140699726837568 submission_runner.py:408] Time since start: 45561.32s, 	Step: 78727, 	{'train/accuracy': 0.6578866243362427, 'train/loss': 1.7336337566375732, 'train/bleu': 33.02379867229866, 'validation/accuracy': 0.6777225136756897, 'validation/loss': 1.597773551940918, 'validation/bleu': 29.78239122588757, 'validation/num_examples': 3000, 'test/accuracy': 0.6900470852851868, 'test/loss': 1.5211902856826782, 'test/bleu': 29.493238645573108, 'test/num_examples': 3003, 'score': 27759.420594215393, 'total_duration': 45561.32496523857, 'accumulated_submission_time': 27759.420594215393, 'accumulated_eval_time': 17798.384374141693, 'accumulated_logging_time': 1.0526585578918457}
I0207 00:19:02.154759 140530045396736 logging_writer.py:48] [78727] accumulated_eval_time=17798.384374, accumulated_logging_time=1.052659, accumulated_submission_time=27759.420594, global_step=78727, preemption_count=0, score=27759.420594, test/accuracy=0.690047, test/bleu=29.493239, test/loss=1.521190, test/num_examples=3003, total_duration=45561.324965, train/accuracy=0.657887, train/bleu=33.023799, train/loss=1.733634, validation/accuracy=0.677723, validation/bleu=29.782391, validation/loss=1.597774, validation/num_examples=3000
I0207 00:19:28.109786 140530037004032 logging_writer.py:48] [78800] global_step=78800, grad_norm=0.3726893961429596, loss=2.859133005142212
I0207 00:20:03.244164 140530045396736 logging_writer.py:48] [78900] global_step=78900, grad_norm=0.37289950251579285, loss=2.835371494293213
I0207 00:20:38.432933 140530037004032 logging_writer.py:48] [79000] global_step=79000, grad_norm=0.39049020409584045, loss=2.824564218521118
I0207 00:21:13.631823 140530045396736 logging_writer.py:48] [79100] global_step=79100, grad_norm=0.39014557003974915, loss=2.7687203884124756
I0207 00:21:48.855830 140530037004032 logging_writer.py:48] [79200] global_step=79200, grad_norm=0.42854997515678406, loss=2.877026081085205
I0207 00:22:24.071455 140530045396736 logging_writer.py:48] [79300] global_step=79300, grad_norm=0.36777085065841675, loss=2.8149847984313965
I0207 00:22:59.294518 140530037004032 logging_writer.py:48] [79400] global_step=79400, grad_norm=0.34273654222488403, loss=2.8170549869537354
I0207 00:23:34.487669 140530045396736 logging_writer.py:48] [79500] global_step=79500, grad_norm=0.37531936168670654, loss=2.78926420211792
I0207 00:24:09.737125 140530037004032 logging_writer.py:48] [79600] global_step=79600, grad_norm=0.38742348551750183, loss=2.87296462059021
I0207 00:24:44.963482 140530045396736 logging_writer.py:48] [79700] global_step=79700, grad_norm=0.39252057671546936, loss=2.8321337699890137
I0207 00:25:20.166869 140530037004032 logging_writer.py:48] [79800] global_step=79800, grad_norm=0.36991846561431885, loss=2.8202412128448486
I0207 00:25:55.412373 140530045396736 logging_writer.py:48] [79900] global_step=79900, grad_norm=0.4076937735080719, loss=2.847790002822876
I0207 00:26:30.624830 140530037004032 logging_writer.py:48] [80000] global_step=80000, grad_norm=0.38193202018737793, loss=2.8651480674743652
I0207 00:27:05.882823 140530045396736 logging_writer.py:48] [80100] global_step=80100, grad_norm=0.3762117922306061, loss=2.8016390800476074
I0207 00:27:41.096942 140530037004032 logging_writer.py:48] [80200] global_step=80200, grad_norm=0.3843240439891815, loss=2.85774302482605
I0207 00:28:16.308375 140530045396736 logging_writer.py:48] [80300] global_step=80300, grad_norm=0.36887994408607483, loss=2.858884334564209
I0207 00:28:51.510051 140530037004032 logging_writer.py:48] [80400] global_step=80400, grad_norm=2.8823180198669434, loss=2.791376829147339
I0207 00:29:26.728515 140530045396736 logging_writer.py:48] [80500] global_step=80500, grad_norm=0.387501060962677, loss=2.9328665733337402
I0207 00:30:01.949404 140530037004032 logging_writer.py:48] [80600] global_step=80600, grad_norm=0.3885255455970764, loss=2.80704402923584
I0207 00:30:37.175363 140530045396736 logging_writer.py:48] [80700] global_step=80700, grad_norm=0.37386971712112427, loss=2.839573621749878
I0207 00:31:12.377634 140530037004032 logging_writer.py:48] [80800] global_step=80800, grad_norm=0.37893590331077576, loss=2.812645435333252
I0207 00:31:47.612156 140530045396736 logging_writer.py:48] [80900] global_step=80900, grad_norm=0.3566288352012634, loss=2.749119758605957
I0207 00:32:22.863240 140530037004032 logging_writer.py:48] [81000] global_step=81000, grad_norm=0.3663560450077057, loss=2.889660120010376
I0207 00:32:58.116557 140530045396736 logging_writer.py:48] [81100] global_step=81100, grad_norm=0.3555458188056946, loss=2.849935293197632
I0207 00:33:02.429766 140699726837568 spec.py:321] Evaluating on the training split.
I0207 00:33:05.464465 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 00:36:14.642246 140699726837568 spec.py:333] Evaluating on the validation split.
I0207 00:36:17.366849 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 00:38:43.060756 140699726837568 spec.py:349] Evaluating on the test split.
I0207 00:38:45.790595 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 00:40:57.147043 140699726837568 submission_runner.py:408] Time since start: 46876.34s, 	Step: 81114, 	{'train/accuracy': 0.6640418171882629, 'train/loss': 1.6978520154953003, 'train/bleu': 32.88214843273441, 'validation/accuracy': 0.6789872050285339, 'validation/loss': 1.593400001525879, 'validation/bleu': 29.676987911310327, 'validation/num_examples': 3000, 'test/accuracy': 0.6944047808647156, 'test/loss': 1.5073984861373901, 'test/bleu': 29.825063306519596, 'test/num_examples': 3003, 'score': 28599.610867261887, 'total_duration': 46876.34319114685, 'accumulated_submission_time': 28599.610867261887, 'accumulated_eval_time': 18273.101594686508, 'accumulated_logging_time': 1.0890939235687256}
I0207 00:40:57.172868 140530037004032 logging_writer.py:48] [81114] accumulated_eval_time=18273.101595, accumulated_logging_time=1.089094, accumulated_submission_time=28599.610867, global_step=81114, preemption_count=0, score=28599.610867, test/accuracy=0.694405, test/bleu=29.825063, test/loss=1.507398, test/num_examples=3003, total_duration=46876.343191, train/accuracy=0.664042, train/bleu=32.882148, train/loss=1.697852, validation/accuracy=0.678987, validation/bleu=29.676988, validation/loss=1.593400, validation/num_examples=3000
I0207 00:41:27.681778 140530045396736 logging_writer.py:48] [81200] global_step=81200, grad_norm=0.38638627529144287, loss=2.8486928939819336
I0207 00:42:02.840507 140530037004032 logging_writer.py:48] [81300] global_step=81300, grad_norm=0.36750149726867676, loss=2.8284311294555664
I0207 00:42:38.052941 140530045396736 logging_writer.py:48] [81400] global_step=81400, grad_norm=0.3810979127883911, loss=2.8188905715942383
I0207 00:43:13.287797 140530037004032 logging_writer.py:48] [81500] global_step=81500, grad_norm=0.37958824634552, loss=2.84537935256958
I0207 00:43:48.515372 140530045396736 logging_writer.py:48] [81600] global_step=81600, grad_norm=0.3767576813697815, loss=2.8022780418395996
I0207 00:44:23.709365 140530037004032 logging_writer.py:48] [81700] global_step=81700, grad_norm=0.3835674822330475, loss=2.829432487487793
I0207 00:44:58.914515 140530045396736 logging_writer.py:48] [81800] global_step=81800, grad_norm=0.3619565963745117, loss=2.805004119873047
I0207 00:45:34.137920 140530037004032 logging_writer.py:48] [81900] global_step=81900, grad_norm=0.40129920840263367, loss=2.7911384105682373
I0207 00:46:09.358995 140530045396736 logging_writer.py:48] [82000] global_step=82000, grad_norm=0.3994360566139221, loss=2.874098539352417
I0207 00:46:44.602932 140530037004032 logging_writer.py:48] [82100] global_step=82100, grad_norm=0.36769816279411316, loss=2.7431719303131104
I0207 00:47:19.825118 140530045396736 logging_writer.py:48] [82200] global_step=82200, grad_norm=0.40570250153541565, loss=2.8190221786499023
I0207 00:47:55.044194 140530037004032 logging_writer.py:48] [82300] global_step=82300, grad_norm=0.3647511601448059, loss=2.795536994934082
I0207 00:48:30.273636 140530045396736 logging_writer.py:48] [82400] global_step=82400, grad_norm=0.39079272747039795, loss=2.8410534858703613
I0207 00:49:05.509327 140530037004032 logging_writer.py:48] [82500] global_step=82500, grad_norm=0.3769088685512543, loss=2.826653003692627
I0207 00:49:40.727294 140530045396736 logging_writer.py:48] [82600] global_step=82600, grad_norm=0.38719722628593445, loss=2.883946418762207
I0207 00:50:15.941902 140530037004032 logging_writer.py:48] [82700] global_step=82700, grad_norm=0.3822825253009796, loss=2.837562084197998
I0207 00:50:51.148967 140530045396736 logging_writer.py:48] [82800] global_step=82800, grad_norm=0.37704941630363464, loss=2.870246171951294
I0207 00:51:26.376237 140530037004032 logging_writer.py:48] [82900] global_step=82900, grad_norm=0.39351028203964233, loss=2.81378173828125
I0207 00:52:01.611579 140530045396736 logging_writer.py:48] [83000] global_step=83000, grad_norm=0.4206318259239197, loss=2.828958749771118
I0207 00:52:36.816212 140530037004032 logging_writer.py:48] [83100] global_step=83100, grad_norm=0.3704788088798523, loss=2.809562921524048
I0207 00:53:12.038349 140530045396736 logging_writer.py:48] [83200] global_step=83200, grad_norm=0.37738868594169617, loss=2.8092048168182373
I0207 00:53:47.252473 140530037004032 logging_writer.py:48] [83300] global_step=83300, grad_norm=0.370113343000412, loss=2.800328493118286
I0207 00:54:22.455831 140530045396736 logging_writer.py:48] [83400] global_step=83400, grad_norm=0.3589899241924286, loss=2.817772388458252
I0207 00:54:57.678626 140530037004032 logging_writer.py:48] [83500] global_step=83500, grad_norm=0.37911900877952576, loss=2.751983404159546
I0207 00:54:57.685358 140699726837568 spec.py:321] Evaluating on the training split.
I0207 00:55:00.425412 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 00:58:15.476316 140699726837568 spec.py:333] Evaluating on the validation split.
I0207 00:58:18.203722 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 01:00:44.000913 140699726837568 spec.py:349] Evaluating on the test split.
I0207 01:00:46.739528 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 01:03:06.372224 140699726837568 submission_runner.py:408] Time since start: 48205.57s, 	Step: 83501, 	{'train/accuracy': 0.6630203723907471, 'train/loss': 1.7007750272750854, 'train/bleu': 32.74125649796642, 'validation/accuracy': 0.6791360378265381, 'validation/loss': 1.5790903568267822, 'validation/bleu': 29.654650380293756, 'validation/num_examples': 3000, 'test/accuracy': 0.6937307715415955, 'test/loss': 1.5014517307281494, 'test/bleu': 29.500032983740162, 'test/num_examples': 3003, 'score': 29440.040162086487, 'total_duration': 48205.56838059425, 'accumulated_submission_time': 29440.040162086487, 'accumulated_eval_time': 18761.78837966919, 'accumulated_logging_time': 1.1252844333648682}
I0207 01:03:06.397826 140530045396736 logging_writer.py:48] [83501] accumulated_eval_time=18761.788380, accumulated_logging_time=1.125284, accumulated_submission_time=29440.040162, global_step=83501, preemption_count=0, score=29440.040162, test/accuracy=0.693731, test/bleu=29.500033, test/loss=1.501452, test/num_examples=3003, total_duration=48205.568381, train/accuracy=0.663020, train/bleu=32.741256, train/loss=1.700775, validation/accuracy=0.679136, validation/bleu=29.654650, validation/loss=1.579090, validation/num_examples=3000
I0207 01:03:41.517825 140530037004032 logging_writer.py:48] [83600] global_step=83600, grad_norm=0.38400232791900635, loss=2.7946462631225586
I0207 01:04:16.700785 140530045396736 logging_writer.py:48] [83700] global_step=83700, grad_norm=0.3910435736179352, loss=2.891244411468506
I0207 01:04:51.879168 140530037004032 logging_writer.py:48] [83800] global_step=83800, grad_norm=0.37010064721107483, loss=2.8171799182891846
I0207 01:05:27.073167 140530045396736 logging_writer.py:48] [83900] global_step=83900, grad_norm=0.3874102234840393, loss=2.8170900344848633
I0207 01:06:02.275580 140530037004032 logging_writer.py:48] [84000] global_step=84000, grad_norm=0.4091809093952179, loss=2.8428866863250732
I0207 01:06:37.477595 140530045396736 logging_writer.py:48] [84100] global_step=84100, grad_norm=0.35123440623283386, loss=2.8260114192962646
I0207 01:07:12.697325 140530037004032 logging_writer.py:48] [84200] global_step=84200, grad_norm=0.4380379319190979, loss=2.8489906787872314
I0207 01:07:47.940621 140530045396736 logging_writer.py:48] [84300] global_step=84300, grad_norm=0.4070689082145691, loss=2.760920286178589
I0207 01:08:23.165537 140530037004032 logging_writer.py:48] [84400] global_step=84400, grad_norm=0.3669622838497162, loss=2.838505268096924
I0207 01:08:58.367080 140530045396736 logging_writer.py:48] [84500] global_step=84500, grad_norm=0.3816729187965393, loss=2.7820067405700684
I0207 01:09:33.592892 140530037004032 logging_writer.py:48] [84600] global_step=84600, grad_norm=0.3712996542453766, loss=2.7835774421691895
I0207 01:10:08.816025 140530045396736 logging_writer.py:48] [84700] global_step=84700, grad_norm=0.4034501612186432, loss=2.823627233505249
I0207 01:10:44.024392 140530037004032 logging_writer.py:48] [84800] global_step=84800, grad_norm=0.46693843603134155, loss=2.814950942993164
I0207 01:11:19.260578 140530045396736 logging_writer.py:48] [84900] global_step=84900, grad_norm=0.3711211383342743, loss=2.838009834289551
I0207 01:11:54.475951 140530037004032 logging_writer.py:48] [85000] global_step=85000, grad_norm=0.38906899094581604, loss=2.7506415843963623
I0207 01:12:29.707355 140530045396736 logging_writer.py:48] [85100] global_step=85100, grad_norm=0.4016658365726471, loss=2.796182632446289
I0207 01:13:04.964589 140530037004032 logging_writer.py:48] [85200] global_step=85200, grad_norm=0.41282111406326294, loss=2.8117244243621826
I0207 01:13:40.243171 140530045396736 logging_writer.py:48] [85300] global_step=85300, grad_norm=0.4199362099170685, loss=2.7558515071868896
I0207 01:14:15.507003 140530037004032 logging_writer.py:48] [85400] global_step=85400, grad_norm=0.38738635182380676, loss=2.8579087257385254
I0207 01:14:50.725952 140530045396736 logging_writer.py:48] [85500] global_step=85500, grad_norm=0.42634668946266174, loss=2.762941360473633
I0207 01:15:25.922419 140530037004032 logging_writer.py:48] [85600] global_step=85600, grad_norm=0.38537362217903137, loss=2.7897677421569824
I0207 01:16:01.148055 140530045396736 logging_writer.py:48] [85700] global_step=85700, grad_norm=0.4129825234413147, loss=2.860822916030884
I0207 01:16:36.361979 140530037004032 logging_writer.py:48] [85800] global_step=85800, grad_norm=0.37764570116996765, loss=2.7887048721313477
I0207 01:17:06.708950 140699726837568 spec.py:321] Evaluating on the training split.
I0207 01:17:09.750310 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 01:20:16.222744 140699726837568 spec.py:333] Evaluating on the validation split.
I0207 01:20:18.954839 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 01:22:50.649669 140699726837568 spec.py:349] Evaluating on the test split.
I0207 01:22:53.375650 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 01:25:11.513928 140699726837568 submission_runner.py:408] Time since start: 49530.71s, 	Step: 85888, 	{'train/accuracy': 0.6633317470550537, 'train/loss': 1.6965419054031372, 'train/bleu': 32.76356242688841, 'validation/accuracy': 0.6802891492843628, 'validation/loss': 1.5811126232147217, 'validation/bleu': 30.043155582785047, 'validation/num_examples': 3000, 'test/accuracy': 0.6941258907318115, 'test/loss': 1.5003278255462646, 'test/bleu': 29.87476830317458, 'test/num_examples': 3003, 'score': 30280.26576089859, 'total_duration': 49530.71009230614, 'accumulated_submission_time': 30280.26576089859, 'accumulated_eval_time': 19246.59332036972, 'accumulated_logging_time': 1.1620018482208252}
I0207 01:25:11.540521 140530045396736 logging_writer.py:48] [85888] accumulated_eval_time=19246.593320, accumulated_logging_time=1.162002, accumulated_submission_time=30280.265761, global_step=85888, preemption_count=0, score=30280.265761, test/accuracy=0.694126, test/bleu=29.874768, test/loss=1.500328, test/num_examples=3003, total_duration=49530.710092, train/accuracy=0.663332, train/bleu=32.763562, train/loss=1.696542, validation/accuracy=0.680289, validation/bleu=30.043156, validation/loss=1.581113, validation/num_examples=3000
I0207 01:25:16.124177 140530037004032 logging_writer.py:48] [85900] global_step=85900, grad_norm=0.384809285402298, loss=2.7550175189971924
I0207 01:25:51.263052 140530045396736 logging_writer.py:48] [86000] global_step=86000, grad_norm=0.521854043006897, loss=2.8074774742126465
I0207 01:26:26.417108 140530037004032 logging_writer.py:48] [86100] global_step=86100, grad_norm=0.35337671637535095, loss=2.75496506690979
I0207 01:27:01.626783 140530045396736 logging_writer.py:48] [86200] global_step=86200, grad_norm=0.3773189187049866, loss=2.7793662548065186
I0207 01:27:36.833003 140530037004032 logging_writer.py:48] [86300] global_step=86300, grad_norm=0.35972997546195984, loss=2.7927024364471436
I0207 01:28:12.101600 140530045396736 logging_writer.py:48] [86400] global_step=86400, grad_norm=0.374019980430603, loss=2.8193039894104004
I0207 01:28:47.309798 140530037004032 logging_writer.py:48] [86500] global_step=86500, grad_norm=0.3840412497520447, loss=2.8431556224823
I0207 01:29:22.550727 140530045396736 logging_writer.py:48] [86600] global_step=86600, grad_norm=0.4041275084018707, loss=2.8306355476379395
I0207 01:29:57.803644 140530037004032 logging_writer.py:48] [86700] global_step=86700, grad_norm=0.3905255198478699, loss=2.7901437282562256
I0207 01:30:33.013577 140530045396736 logging_writer.py:48] [86800] global_step=86800, grad_norm=0.3834366500377655, loss=2.758664131164551
I0207 01:31:08.287772 140530037004032 logging_writer.py:48] [86900] global_step=86900, grad_norm=0.37603047490119934, loss=2.8096115589141846
I0207 01:31:43.569795 140530045396736 logging_writer.py:48] [87000] global_step=87000, grad_norm=0.38460204005241394, loss=2.7179832458496094
I0207 01:32:18.792803 140530037004032 logging_writer.py:48] [87100] global_step=87100, grad_norm=0.384017676115036, loss=2.6929733753204346
I0207 01:32:53.978170 140530045396736 logging_writer.py:48] [87200] global_step=87200, grad_norm=0.3943274915218353, loss=2.7662856578826904
I0207 01:33:29.169212 140530037004032 logging_writer.py:48] [87300] global_step=87300, grad_norm=0.4009411931037903, loss=2.7528507709503174
I0207 01:34:04.369972 140530045396736 logging_writer.py:48] [87400] global_step=87400, grad_norm=0.3794991672039032, loss=2.8375282287597656
I0207 01:34:39.583486 140530037004032 logging_writer.py:48] [87500] global_step=87500, grad_norm=0.40872785449028015, loss=2.7341556549072266
I0207 01:35:14.794605 140530045396736 logging_writer.py:48] [87600] global_step=87600, grad_norm=0.4020349681377411, loss=2.7545878887176514
I0207 01:35:50.022383 140530037004032 logging_writer.py:48] [87700] global_step=87700, grad_norm=0.3897659480571747, loss=2.7521538734436035
I0207 01:36:25.249753 140530045396736 logging_writer.py:48] [87800] global_step=87800, grad_norm=0.40144574642181396, loss=2.799334764480591
I0207 01:37:00.468946 140530037004032 logging_writer.py:48] [87900] global_step=87900, grad_norm=0.3856121301651001, loss=2.783383846282959
I0207 01:37:35.689119 140530045396736 logging_writer.py:48] [88000] global_step=88000, grad_norm=0.38682466745376587, loss=2.760004758834839
I0207 01:38:10.900010 140530037004032 logging_writer.py:48] [88100] global_step=88100, grad_norm=0.38675424456596375, loss=2.7657997608184814
I0207 01:38:46.125726 140530045396736 logging_writer.py:48] [88200] global_step=88200, grad_norm=0.4091495871543884, loss=2.8510005474090576
I0207 01:39:11.528966 140699726837568 spec.py:321] Evaluating on the training split.
I0207 01:39:14.560228 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 01:42:13.090452 140699726837568 spec.py:333] Evaluating on the validation split.
I0207 01:42:15.824203 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 01:44:55.685921 140699726837568 spec.py:349] Evaluating on the test split.
I0207 01:44:58.415661 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 01:47:29.637430 140699726837568 submission_runner.py:408] Time since start: 50868.83s, 	Step: 88274, 	{'train/accuracy': 0.682263195514679, 'train/loss': 1.5941760540008545, 'train/bleu': 34.32433497901006, 'validation/accuracy': 0.6819010376930237, 'validation/loss': 1.5699979066848755, 'validation/bleu': 29.823944589013955, 'validation/num_examples': 3000, 'test/accuracy': 0.6963802576065063, 'test/loss': 1.4937410354614258, 'test/bleu': 29.658199063669386, 'test/num_examples': 3003, 'score': 31120.169873714447, 'total_duration': 50868.833562374115, 'accumulated_submission_time': 31120.169873714447, 'accumulated_eval_time': 19744.70170378685, 'accumulated_logging_time': 1.199810266494751}
I0207 01:47:29.663865 140530037004032 logging_writer.py:48] [88274] accumulated_eval_time=19744.701704, accumulated_logging_time=1.199810, accumulated_submission_time=31120.169874, global_step=88274, preemption_count=0, score=31120.169874, test/accuracy=0.696380, test/bleu=29.658199, test/loss=1.493741, test/num_examples=3003, total_duration=50868.833562, train/accuracy=0.682263, train/bleu=34.324335, train/loss=1.594176, validation/accuracy=0.681901, validation/bleu=29.823945, validation/loss=1.569998, validation/num_examples=3000
I0207 01:47:39.153776 140530045396736 logging_writer.py:48] [88300] global_step=88300, grad_norm=0.38501814007759094, loss=2.859665870666504
I0207 01:48:14.326066 140530037004032 logging_writer.py:48] [88400] global_step=88400, grad_norm=0.4085993766784668, loss=2.8037447929382324
I0207 01:48:49.497958 140530045396736 logging_writer.py:48] [88500] global_step=88500, grad_norm=0.4036760926246643, loss=2.755265951156616
I0207 01:49:24.725682 140530037004032 logging_writer.py:48] [88600] global_step=88600, grad_norm=0.39942196011543274, loss=2.80192232131958
I0207 01:50:00.023705 140530045396736 logging_writer.py:48] [88700] global_step=88700, grad_norm=0.4297631084918976, loss=2.8093032836914062
I0207 01:50:35.250474 140530037004032 logging_writer.py:48] [88800] global_step=88800, grad_norm=0.4016679525375366, loss=2.8301236629486084
I0207 01:51:10.467138 140530045396736 logging_writer.py:48] [88900] global_step=88900, grad_norm=0.40872326493263245, loss=2.8046786785125732
I0207 01:51:45.701924 140530037004032 logging_writer.py:48] [89000] global_step=89000, grad_norm=0.4191541373729706, loss=2.7957823276519775
I0207 01:52:20.976051 140530045396736 logging_writer.py:48] [89100] global_step=89100, grad_norm=0.3812217712402344, loss=2.784040689468384
I0207 01:52:56.208755 140530037004032 logging_writer.py:48] [89200] global_step=89200, grad_norm=0.4074018895626068, loss=2.797661066055298
I0207 01:53:31.428125 140530045396736 logging_writer.py:48] [89300] global_step=89300, grad_norm=0.37354204058647156, loss=2.7356839179992676
I0207 01:54:06.661956 140530037004032 logging_writer.py:48] [89400] global_step=89400, grad_norm=0.39732393622398376, loss=2.8398489952087402
I0207 01:54:41.887520 140530045396736 logging_writer.py:48] [89500] global_step=89500, grad_norm=0.382988303899765, loss=2.7555181980133057
I0207 01:55:17.092998 140530037004032 logging_writer.py:48] [89600] global_step=89600, grad_norm=0.403325617313385, loss=2.727800130844116
I0207 01:55:52.330337 140530045396736 logging_writer.py:48] [89700] global_step=89700, grad_norm=0.4059126079082489, loss=2.741814374923706
I0207 01:56:27.561641 140530037004032 logging_writer.py:48] [89800] global_step=89800, grad_norm=0.372067391872406, loss=2.7193844318389893
I0207 01:57:02.815603 140530045396736 logging_writer.py:48] [89900] global_step=89900, grad_norm=0.38573789596557617, loss=2.7327661514282227
I0207 01:57:38.064954 140530037004032 logging_writer.py:48] [90000] global_step=90000, grad_norm=0.40455445647239685, loss=2.7329537868499756
I0207 01:58:13.300284 140530045396736 logging_writer.py:48] [90100] global_step=90100, grad_norm=0.4073982536792755, loss=2.8086435794830322
I0207 01:58:48.530895 140530037004032 logging_writer.py:48] [90200] global_step=90200, grad_norm=0.44193950295448303, loss=2.820803165435791
I0207 01:59:23.766330 140530045396736 logging_writer.py:48] [90300] global_step=90300, grad_norm=0.42360609769821167, loss=2.765458583831787
I0207 01:59:58.968312 140530037004032 logging_writer.py:48] [90400] global_step=90400, grad_norm=0.42109036445617676, loss=2.738375425338745
I0207 02:00:34.173342 140530045396736 logging_writer.py:48] [90500] global_step=90500, grad_norm=0.40619349479675293, loss=2.781830310821533
I0207 02:01:09.392031 140530037004032 logging_writer.py:48] [90600] global_step=90600, grad_norm=0.40210872888565063, loss=2.7971291542053223
I0207 02:01:29.901573 140699726837568 spec.py:321] Evaluating on the training split.
I0207 02:01:32.970182 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 02:04:54.738515 140699726837568 spec.py:333] Evaluating on the validation split.
I0207 02:04:57.461364 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 02:07:21.856585 140699726837568 spec.py:349] Evaluating on the test split.
I0207 02:07:24.592507 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 02:09:41.572680 140699726837568 submission_runner.py:408] Time since start: 52200.77s, 	Step: 90660, 	{'train/accuracy': 0.6701759696006775, 'train/loss': 1.6549545526504517, 'train/bleu': 32.95552252211354, 'validation/accuracy': 0.6838228702545166, 'validation/loss': 1.5629757642745972, 'validation/bleu': 30.110572580155196, 'validation/num_examples': 3000, 'test/accuracy': 0.6983673572540283, 'test/loss': 1.4784048795700073, 'test/bleu': 30.366281457229306, 'test/num_examples': 3003, 'score': 31960.32098174095, 'total_duration': 52200.768839120865, 'accumulated_submission_time': 31960.32098174095, 'accumulated_eval_time': 20236.372787475586, 'accumulated_logging_time': 1.2370805740356445}
I0207 02:09:41.599280 140530045396736 logging_writer.py:48] [90660] accumulated_eval_time=20236.372787, accumulated_logging_time=1.237081, accumulated_submission_time=31960.320982, global_step=90660, preemption_count=0, score=31960.320982, test/accuracy=0.698367, test/bleu=30.366281, test/loss=1.478405, test/num_examples=3003, total_duration=52200.768839, train/accuracy=0.670176, train/bleu=32.955523, train/loss=1.654955, validation/accuracy=0.683823, validation/bleu=30.110573, validation/loss=1.562976, validation/num_examples=3000
I0207 02:09:55.988456 140530037004032 logging_writer.py:48] [90700] global_step=90700, grad_norm=0.403430312871933, loss=2.8219573497772217
I0207 02:10:31.102107 140530045396736 logging_writer.py:48] [90800] global_step=90800, grad_norm=0.4097534418106079, loss=2.84958815574646
I0207 02:11:06.249186 140530037004032 logging_writer.py:48] [90900] global_step=90900, grad_norm=0.40172359347343445, loss=2.7757534980773926
I0207 02:11:41.429212 140530045396736 logging_writer.py:48] [91000] global_step=91000, grad_norm=0.3851703405380249, loss=2.7484397888183594
I0207 02:12:16.665950 140530037004032 logging_writer.py:48] [91100] global_step=91100, grad_norm=0.41414785385131836, loss=2.78232479095459
I0207 02:12:51.931819 140530045396736 logging_writer.py:48] [91200] global_step=91200, grad_norm=0.40722891688346863, loss=2.720676898956299
I0207 02:13:27.143059 140530037004032 logging_writer.py:48] [91300] global_step=91300, grad_norm=0.39727410674095154, loss=2.7128641605377197
I0207 02:14:02.357801 140530045396736 logging_writer.py:48] [91400] global_step=91400, grad_norm=0.4116998016834259, loss=2.7963955402374268
I0207 02:14:37.579694 140530037004032 logging_writer.py:48] [91500] global_step=91500, grad_norm=0.4293581545352936, loss=2.7728512287139893
I0207 02:15:12.805475 140530045396736 logging_writer.py:48] [91600] global_step=91600, grad_norm=0.4133206307888031, loss=2.7698898315429688
I0207 02:15:47.989943 140530037004032 logging_writer.py:48] [91700] global_step=91700, grad_norm=0.4254826307296753, loss=2.7340316772460938
I0207 02:16:23.189159 140530045396736 logging_writer.py:48] [91800] global_step=91800, grad_norm=0.4050375521183014, loss=2.7372567653656006
I0207 02:16:58.398020 140530037004032 logging_writer.py:48] [91900] global_step=91900, grad_norm=0.4332086145877838, loss=2.7679333686828613
I0207 02:17:33.690388 140530045396736 logging_writer.py:48] [92000] global_step=92000, grad_norm=0.4275936484336853, loss=2.8114564418792725
I0207 02:18:08.927545 140530037004032 logging_writer.py:48] [92100] global_step=92100, grad_norm=0.4143628776073456, loss=2.8332085609436035
I0207 02:18:44.160481 140530045396736 logging_writer.py:48] [92200] global_step=92200, grad_norm=0.38085857033729553, loss=2.7258541584014893
I0207 02:19:19.379213 140530037004032 logging_writer.py:48] [92300] global_step=92300, grad_norm=0.4349393844604492, loss=2.753631830215454
I0207 02:19:54.574618 140530045396736 logging_writer.py:48] [92400] global_step=92400, grad_norm=0.3978211581707001, loss=2.7290985584259033
I0207 02:20:29.785804 140530037004032 logging_writer.py:48] [92500] global_step=92500, grad_norm=0.41191473603248596, loss=2.8137879371643066
I0207 02:21:04.996326 140530045396736 logging_writer.py:48] [92600] global_step=92600, grad_norm=0.42174163460731506, loss=2.8621561527252197
I0207 02:21:40.252990 140530037004032 logging_writer.py:48] [92700] global_step=92700, grad_norm=0.4096388518810272, loss=2.7301394939422607
I0207 02:22:15.492062 140530045396736 logging_writer.py:48] [92800] global_step=92800, grad_norm=0.4526127576828003, loss=2.7852182388305664
I0207 02:22:50.702387 140530037004032 logging_writer.py:48] [92900] global_step=92900, grad_norm=0.3883497714996338, loss=2.7494800090789795
I0207 02:23:25.903743 140530045396736 logging_writer.py:48] [93000] global_step=93000, grad_norm=0.4242866039276123, loss=2.7212839126586914
I0207 02:23:41.826164 140699726837568 spec.py:321] Evaluating on the training split.
I0207 02:23:44.860837 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 02:27:11.818083 140699726837568 spec.py:333] Evaluating on the validation split.
I0207 02:27:14.553115 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 02:29:42.137454 140699726837568 spec.py:349] Evaluating on the test split.
I0207 02:29:44.883608 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 02:32:11.596580 140699726837568 submission_runner.py:408] Time since start: 53550.79s, 	Step: 93047, 	{'train/accuracy': 0.6740824580192566, 'train/loss': 1.6410588026046753, 'train/bleu': 32.95555072276922, 'validation/accuracy': 0.6841452717781067, 'validation/loss': 1.5564584732055664, 'validation/bleu': 30.397595709889032, 'validation/num_examples': 3000, 'test/accuracy': 0.7005403637886047, 'test/loss': 1.4703547954559326, 'test/bleu': 30.228897107393603, 'test/num_examples': 3003, 'score': 32800.464728832245, 'total_duration': 53550.79271054268, 'accumulated_submission_time': 32800.464728832245, 'accumulated_eval_time': 20746.14312171936, 'accumulated_logging_time': 1.2735848426818848}
I0207 02:32:11.629325 140530037004032 logging_writer.py:48] [93047] accumulated_eval_time=20746.143122, accumulated_logging_time=1.273585, accumulated_submission_time=32800.464729, global_step=93047, preemption_count=0, score=32800.464729, test/accuracy=0.700540, test/bleu=30.228897, test/loss=1.470355, test/num_examples=3003, total_duration=53550.792711, train/accuracy=0.674082, train/bleu=32.955551, train/loss=1.641059, validation/accuracy=0.684145, validation/bleu=30.397596, validation/loss=1.556458, validation/num_examples=3000
I0207 02:32:30.615515 140530045396736 logging_writer.py:48] [93100] global_step=93100, grad_norm=0.39360809326171875, loss=2.695474863052368
I0207 02:33:05.787889 140530037004032 logging_writer.py:48] [93200] global_step=93200, grad_norm=0.4076124429702759, loss=2.781832456588745
I0207 02:33:41.036027 140530045396736 logging_writer.py:48] [93300] global_step=93300, grad_norm=0.4420461654663086, loss=2.7974941730499268
I0207 02:34:16.273816 140530037004032 logging_writer.py:48] [93400] global_step=93400, grad_norm=0.4198787212371826, loss=2.797318696975708
I0207 02:34:51.454723 140530045396736 logging_writer.py:48] [93500] global_step=93500, grad_norm=0.39965927600860596, loss=2.782924175262451
I0207 02:35:26.674381 140530037004032 logging_writer.py:48] [93600] global_step=93600, grad_norm=0.40515196323394775, loss=2.779414653778076
I0207 02:36:01.892810 140530045396736 logging_writer.py:48] [93700] global_step=93700, grad_norm=0.43519237637519836, loss=2.8426434993743896
I0207 02:36:37.084972 140530037004032 logging_writer.py:48] [93800] global_step=93800, grad_norm=0.44704344868659973, loss=2.858991861343384
I0207 02:37:12.301347 140530045396736 logging_writer.py:48] [93900] global_step=93900, grad_norm=0.4246552884578705, loss=2.722386360168457
I0207 02:37:47.568819 140530037004032 logging_writer.py:48] [94000] global_step=94000, grad_norm=0.46583783626556396, loss=2.779418468475342
I0207 02:38:22.797007 140530045396736 logging_writer.py:48] [94100] global_step=94100, grad_norm=0.4478449523448944, loss=2.8011088371276855
I0207 02:38:58.006823 140530037004032 logging_writer.py:48] [94200] global_step=94200, grad_norm=0.4152275323867798, loss=2.7314438819885254
I0207 02:39:33.213658 140530045396736 logging_writer.py:48] [94300] global_step=94300, grad_norm=0.42861607670783997, loss=2.7332117557525635
I0207 02:40:08.410358 140530037004032 logging_writer.py:48] [94400] global_step=94400, grad_norm=0.4314488470554352, loss=2.812736749649048
I0207 02:40:43.631518 140530045396736 logging_writer.py:48] [94500] global_step=94500, grad_norm=0.4444778859615326, loss=2.750270366668701
I0207 02:41:18.865028 140530037004032 logging_writer.py:48] [94600] global_step=94600, grad_norm=0.415176659822464, loss=2.750678539276123
I0207 02:41:54.092639 140530045396736 logging_writer.py:48] [94700] global_step=94700, grad_norm=0.39856231212615967, loss=2.8139967918395996
I0207 02:42:29.307835 140530037004032 logging_writer.py:48] [94800] global_step=94800, grad_norm=0.4275103211402893, loss=2.831592559814453
I0207 02:43:04.524933 140530045396736 logging_writer.py:48] [94900] global_step=94900, grad_norm=0.3922034800052643, loss=2.8041272163391113
I0207 02:43:39.713024 140530037004032 logging_writer.py:48] [95000] global_step=95000, grad_norm=0.42779821157455444, loss=2.750678777694702
I0207 02:44:14.909650 140530045396736 logging_writer.py:48] [95100] global_step=95100, grad_norm=0.4268898665904999, loss=2.668056011199951
I0207 02:44:50.119900 140530037004032 logging_writer.py:48] [95200] global_step=95200, grad_norm=0.42450979351997375, loss=2.8170053958892822
I0207 02:45:25.345645 140530045396736 logging_writer.py:48] [95300] global_step=95300, grad_norm=0.43596944212913513, loss=2.76951003074646
I0207 02:46:00.553707 140530037004032 logging_writer.py:48] [95400] global_step=95400, grad_norm=0.45503270626068115, loss=2.7698872089385986
I0207 02:46:11.919341 140699726837568 spec.py:321] Evaluating on the training split.
I0207 02:46:14.963057 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 02:49:22.191150 140699726837568 spec.py:333] Evaluating on the validation split.
I0207 02:49:24.928195 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 02:51:49.655263 140699726837568 spec.py:349] Evaluating on the test split.
I0207 02:51:52.391995 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 02:54:09.682698 140699726837568 submission_runner.py:408] Time since start: 54868.88s, 	Step: 95434, 	{'train/accuracy': 0.6752694249153137, 'train/loss': 1.6265360116958618, 'train/bleu': 34.1304166462366, 'validation/accuracy': 0.6851744055747986, 'validation/loss': 1.5516746044158936, 'validation/bleu': 30.227560754570124, 'validation/num_examples': 3000, 'test/accuracy': 0.7010632753372192, 'test/loss': 1.465505599975586, 'test/bleu': 30.193043914956924, 'test/num_examples': 3003, 'score': 33640.66740679741, 'total_duration': 54868.87886095047, 'accumulated_submission_time': 33640.66740679741, 'accumulated_eval_time': 21223.906438589096, 'accumulated_logging_time': 1.3174645900726318}
I0207 02:54:09.710164 140530045396736 logging_writer.py:48] [95434] accumulated_eval_time=21223.906439, accumulated_logging_time=1.317465, accumulated_submission_time=33640.667407, global_step=95434, preemption_count=0, score=33640.667407, test/accuracy=0.701063, test/bleu=30.193044, test/loss=1.465506, test/num_examples=3003, total_duration=54868.878861, train/accuracy=0.675269, train/bleu=34.130417, train/loss=1.626536, validation/accuracy=0.685174, validation/bleu=30.227561, validation/loss=1.551675, validation/num_examples=3000
I0207 02:54:33.237517 140530037004032 logging_writer.py:48] [95500] global_step=95500, grad_norm=0.4477917551994324, loss=2.767781972885132
I0207 02:55:08.385624 140530045396736 logging_writer.py:48] [95600] global_step=95600, grad_norm=0.4354872703552246, loss=2.70100474357605
I0207 02:55:43.607457 140530037004032 logging_writer.py:48] [95700] global_step=95700, grad_norm=0.41298410296440125, loss=2.7580342292785645
I0207 02:56:18.785172 140530045396736 logging_writer.py:48] [95800] global_step=95800, grad_norm=0.42503467202186584, loss=2.755218029022217
I0207 02:56:53.969010 140530037004032 logging_writer.py:48] [95900] global_step=95900, grad_norm=0.4191593527793884, loss=2.743783712387085
I0207 02:57:29.185739 140530045396736 logging_writer.py:48] [96000] global_step=96000, grad_norm=0.4370611906051636, loss=2.8085169792175293
I0207 02:58:04.420838 140530037004032 logging_writer.py:48] [96100] global_step=96100, grad_norm=0.45772457122802734, loss=2.7369744777679443
I0207 02:58:39.700310 140530045396736 logging_writer.py:48] [96200] global_step=96200, grad_norm=0.4564509987831116, loss=2.7748196125030518
I0207 02:59:14.979978 140530037004032 logging_writer.py:48] [96300] global_step=96300, grad_norm=0.4192289710044861, loss=2.7260124683380127
I0207 02:59:50.212781 140530045396736 logging_writer.py:48] [96400] global_step=96400, grad_norm=0.44400542974472046, loss=2.8177926540374756
I0207 03:00:25.449207 140530037004032 logging_writer.py:48] [96500] global_step=96500, grad_norm=0.4407883882522583, loss=2.7014458179473877
I0207 03:01:00.674804 140530045396736 logging_writer.py:48] [96600] global_step=96600, grad_norm=0.4531991183757782, loss=2.76678729057312
I0207 03:01:35.893784 140530037004032 logging_writer.py:48] [96700] global_step=96700, grad_norm=0.44147470593452454, loss=2.8178141117095947
I0207 03:02:11.087984 140530045396736 logging_writer.py:48] [96800] global_step=96800, grad_norm=0.4332335293292999, loss=2.7710132598876953
I0207 03:02:46.281491 140530037004032 logging_writer.py:48] [96900] global_step=96900, grad_norm=0.44005221128463745, loss=2.739039421081543
I0207 03:03:21.467743 140530045396736 logging_writer.py:48] [97000] global_step=97000, grad_norm=0.4375368654727936, loss=2.805283546447754
I0207 03:03:56.683815 140530037004032 logging_writer.py:48] [97100] global_step=97100, grad_norm=0.402276873588562, loss=2.714841842651367
I0207 03:04:31.917941 140530045396736 logging_writer.py:48] [97200] global_step=97200, grad_norm=0.4394482970237732, loss=2.7950804233551025
I0207 03:05:07.137349 140530037004032 logging_writer.py:48] [97300] global_step=97300, grad_norm=0.4534803628921509, loss=2.775195598602295
I0207 03:05:42.353653 140530045396736 logging_writer.py:48] [97400] global_step=97400, grad_norm=0.45951977372169495, loss=2.7145941257476807
I0207 03:06:17.550372 140530037004032 logging_writer.py:48] [97500] global_step=97500, grad_norm=0.4486832916736603, loss=2.693223476409912
I0207 03:06:52.774621 140530045396736 logging_writer.py:48] [97600] global_step=97600, grad_norm=0.4162592887878418, loss=2.721797466278076
I0207 03:07:27.988052 140530037004032 logging_writer.py:48] [97700] global_step=97700, grad_norm=0.46465224027633667, loss=2.8146209716796875
I0207 03:08:03.201823 140530045396736 logging_writer.py:48] [97800] global_step=97800, grad_norm=0.4248657524585724, loss=2.710223436355591
I0207 03:08:09.955565 140699726837568 spec.py:321] Evaluating on the training split.
I0207 03:08:12.978775 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 03:11:06.426800 140699726837568 spec.py:333] Evaluating on the validation split.
I0207 03:11:09.156439 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 03:13:36.665636 140699726837568 spec.py:349] Evaluating on the test split.
I0207 03:13:39.410663 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 03:16:00.979657 140699726837568 submission_runner.py:408] Time since start: 56180.18s, 	Step: 97821, 	{'train/accuracy': 0.6793409585952759, 'train/loss': 1.6154285669326782, 'train/bleu': 33.39148472321215, 'validation/accuracy': 0.6855835318565369, 'validation/loss': 1.5477550029754639, 'validation/bleu': 30.360630621623393, 'validation/num_examples': 3000, 'test/accuracy': 0.7020161747932434, 'test/loss': 1.464707612991333, 'test/bleu': 30.33091817118647, 'test/num_examples': 3003, 'score': 34480.82843494415, 'total_duration': 56180.17577815056, 'accumulated_submission_time': 34480.82843494415, 'accumulated_eval_time': 21694.930439949036, 'accumulated_logging_time': 1.3555314540863037}
I0207 03:16:01.012419 140530037004032 logging_writer.py:48] [97821] accumulated_eval_time=21694.930440, accumulated_logging_time=1.355531, accumulated_submission_time=34480.828435, global_step=97821, preemption_count=0, score=34480.828435, test/accuracy=0.702016, test/bleu=30.330918, test/loss=1.464708, test/num_examples=3003, total_duration=56180.175778, train/accuracy=0.679341, train/bleu=33.391485, train/loss=1.615429, validation/accuracy=0.685584, validation/bleu=30.360631, validation/loss=1.547755, validation/num_examples=3000
I0207 03:16:29.143054 140530045396736 logging_writer.py:48] [97900] global_step=97900, grad_norm=0.45822179317474365, loss=2.6909122467041016
I0207 03:17:04.283017 140530037004032 logging_writer.py:48] [98000] global_step=98000, grad_norm=0.4575693905353546, loss=2.7794692516326904
I0207 03:17:39.463499 140530045396736 logging_writer.py:48] [98100] global_step=98100, grad_norm=0.4665834605693817, loss=2.7717132568359375
I0207 03:18:14.676138 140530037004032 logging_writer.py:48] [98200] global_step=98200, grad_norm=0.43189433217048645, loss=2.6875510215759277
I0207 03:18:49.860398 140530045396736 logging_writer.py:48] [98300] global_step=98300, grad_norm=0.47664815187454224, loss=2.796095609664917
I0207 03:19:25.071362 140530037004032 logging_writer.py:48] [98400] global_step=98400, grad_norm=0.49776172637939453, loss=2.7979376316070557
I0207 03:20:00.285687 140530045396736 logging_writer.py:48] [98500] global_step=98500, grad_norm=0.438937783241272, loss=2.7405343055725098
I0207 03:20:35.522787 140530037004032 logging_writer.py:48] [98600] global_step=98600, grad_norm=0.4209286868572235, loss=2.660712242126465
I0207 03:21:10.777133 140530045396736 logging_writer.py:48] [98700] global_step=98700, grad_norm=0.4286997616291046, loss=2.710131883621216
I0207 03:21:46.055248 140530037004032 logging_writer.py:48] [98800] global_step=98800, grad_norm=0.45109885931015015, loss=2.779106378555298
I0207 03:22:21.287966 140530045396736 logging_writer.py:48] [98900] global_step=98900, grad_norm=0.43055325746536255, loss=2.7597126960754395
I0207 03:22:56.512674 140530037004032 logging_writer.py:48] [99000] global_step=99000, grad_norm=0.452740877866745, loss=2.742114782333374
I0207 03:23:31.712259 140530045396736 logging_writer.py:48] [99100] global_step=99100, grad_norm=0.45167893171310425, loss=2.7297000885009766
I0207 03:24:06.930945 140530037004032 logging_writer.py:48] [99200] global_step=99200, grad_norm=0.45408743619918823, loss=2.697354555130005
I0207 03:24:42.160066 140530045396736 logging_writer.py:48] [99300] global_step=99300, grad_norm=0.45219218730926514, loss=2.6984522342681885
I0207 03:25:17.401832 140530037004032 logging_writer.py:48] [99400] global_step=99400, grad_norm=0.4306188225746155, loss=2.7512595653533936
I0207 03:25:52.624652 140530045396736 logging_writer.py:48] [99500] global_step=99500, grad_norm=0.4752715528011322, loss=2.720045804977417
I0207 03:26:27.875377 140530037004032 logging_writer.py:48] [99600] global_step=99600, grad_norm=0.4736493229866028, loss=2.719449758529663
I0207 03:27:03.146512 140530045396736 logging_writer.py:48] [99700] global_step=99700, grad_norm=0.46535831689834595, loss=2.7485992908477783
I0207 03:27:38.358151 140530037004032 logging_writer.py:48] [99800] global_step=99800, grad_norm=0.45514488220214844, loss=2.699129343032837
I0207 03:28:13.613709 140530045396736 logging_writer.py:48] [99900] global_step=99900, grad_norm=0.4614075720310211, loss=2.7092723846435547
I0207 03:28:48.872756 140530037004032 logging_writer.py:48] [100000] global_step=100000, grad_norm=0.4640581011772156, loss=2.7340123653411865
I0207 03:29:24.088901 140530045396736 logging_writer.py:48] [100100] global_step=100100, grad_norm=0.4437412619590759, loss=2.7589306831359863
I0207 03:29:59.341737 140530037004032 logging_writer.py:48] [100200] global_step=100200, grad_norm=0.44659632444381714, loss=2.7222211360931396
I0207 03:30:01.175575 140699726837568 spec.py:321] Evaluating on the training split.
I0207 03:30:04.220238 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 03:32:58.393201 140699726837568 spec.py:333] Evaluating on the validation split.
I0207 03:33:01.126133 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 03:35:25.615144 140699726837568 spec.py:349] Evaluating on the test split.
I0207 03:35:28.349870 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 03:37:43.399987 140699726837568 submission_runner.py:408] Time since start: 57482.60s, 	Step: 100207, 	{'train/accuracy': 0.7089230418205261, 'train/loss': 1.443196415901184, 'train/bleu': 36.04611483927222, 'validation/accuracy': 0.6880261898040771, 'validation/loss': 1.534866452217102, 'validation/bleu': 30.359806178761417, 'validation/num_examples': 3000, 'test/accuracy': 0.7030271291732788, 'test/loss': 1.4542731046676636, 'test/bleu': 30.087657005563646, 'test/num_examples': 3003, 'score': 35320.90512704849, 'total_duration': 57482.59611940384, 'accumulated_submission_time': 35320.90512704849, 'accumulated_eval_time': 22157.154767990112, 'accumulated_logging_time': 1.3990330696105957}
I0207 03:37:43.433863 140530045396736 logging_writer.py:48] [100207] accumulated_eval_time=22157.154768, accumulated_logging_time=1.399033, accumulated_submission_time=35320.905127, global_step=100207, preemption_count=0, score=35320.905127, test/accuracy=0.703027, test/bleu=30.087657, test/loss=1.454273, test/num_examples=3003, total_duration=57482.596119, train/accuracy=0.708923, train/bleu=36.046115, train/loss=1.443196, validation/accuracy=0.688026, validation/bleu=30.359806, validation/loss=1.534866, validation/num_examples=3000
I0207 03:38:16.451874 140530037004032 logging_writer.py:48] [100300] global_step=100300, grad_norm=0.4369664490222931, loss=2.649878740310669
I0207 03:38:51.592207 140530045396736 logging_writer.py:48] [100400] global_step=100400, grad_norm=0.42661991715431213, loss=2.7828733921051025
I0207 03:39:26.784022 140530037004032 logging_writer.py:48] [100500] global_step=100500, grad_norm=0.4456053376197815, loss=2.7470715045928955
I0207 03:40:02.007458 140530045396736 logging_writer.py:48] [100600] global_step=100600, grad_norm=0.44522860646247864, loss=2.7350199222564697
I0207 03:40:37.246495 140530037004032 logging_writer.py:48] [100700] global_step=100700, grad_norm=0.4661126136779785, loss=2.6756880283355713
I0207 03:41:12.449557 140530045396736 logging_writer.py:48] [100800] global_step=100800, grad_norm=0.4635127782821655, loss=2.6696107387542725
I0207 03:41:47.682654 140530037004032 logging_writer.py:48] [100900] global_step=100900, grad_norm=0.4689720571041107, loss=2.7982254028320312
I0207 03:42:22.888530 140530045396736 logging_writer.py:48] [101000] global_step=101000, grad_norm=0.5099987983703613, loss=2.7297608852386475
I0207 03:42:58.089590 140530037004032 logging_writer.py:48] [101100] global_step=101100, grad_norm=0.43423089385032654, loss=2.679194688796997
I0207 03:43:33.294785 140530045396736 logging_writer.py:48] [101200] global_step=101200, grad_norm=0.4883027672767639, loss=2.720391035079956
I0207 03:44:08.506288 140530037004032 logging_writer.py:48] [101300] global_step=101300, grad_norm=0.4651471972465515, loss=2.703270196914673
I0207 03:44:43.701982 140530045396736 logging_writer.py:48] [101400] global_step=101400, grad_norm=0.46526968479156494, loss=2.7177600860595703
I0207 03:45:18.924850 140530037004032 logging_writer.py:48] [101500] global_step=101500, grad_norm=0.4779301583766937, loss=2.7646241188049316
I0207 03:45:54.144933 140530045396736 logging_writer.py:48] [101600] global_step=101600, grad_norm=0.46366825699806213, loss=2.7967817783355713
I0207 03:46:29.384986 140530037004032 logging_writer.py:48] [101700] global_step=101700, grad_norm=0.4738088846206665, loss=2.771688222885132
I0207 03:47:04.612092 140530045396736 logging_writer.py:48] [101800] global_step=101800, grad_norm=0.49114349484443665, loss=2.7802774906158447
I0207 03:47:39.839316 140530037004032 logging_writer.py:48] [101900] global_step=101900, grad_norm=0.47832947969436646, loss=2.7706515789031982
I0207 03:48:15.146275 140530045396736 logging_writer.py:48] [102000] global_step=102000, grad_norm=0.4852955639362335, loss=2.7490735054016113
I0207 03:48:50.454282 140530037004032 logging_writer.py:48] [102100] global_step=102100, grad_norm=0.46336108446121216, loss=2.6476194858551025
I0207 03:49:25.750900 140530045396736 logging_writer.py:48] [102200] global_step=102200, grad_norm=0.4983702003955841, loss=2.735548496246338
I0207 03:50:00.991229 140530037004032 logging_writer.py:48] [102300] global_step=102300, grad_norm=0.4556758403778076, loss=2.7154765129089355
I0207 03:50:36.234070 140530045396736 logging_writer.py:48] [102400] global_step=102400, grad_norm=0.47027042508125305, loss=2.7036447525024414
I0207 03:51:11.458641 140530037004032 logging_writer.py:48] [102500] global_step=102500, grad_norm=0.47634634375572205, loss=2.7089176177978516
I0207 03:51:43.610325 140699726837568 spec.py:321] Evaluating on the training split.
I0207 03:51:46.642018 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 03:54:45.943530 140699726837568 spec.py:333] Evaluating on the validation split.
I0207 03:54:48.667438 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 03:57:16.452561 140699726837568 spec.py:349] Evaluating on the test split.
I0207 03:57:19.192354 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 03:59:42.385078 140699726837568 submission_runner.py:408] Time since start: 58801.58s, 	Step: 102593, 	{'train/accuracy': 0.6842008233070374, 'train/loss': 1.5798007249832153, 'train/bleu': 34.35210443372337, 'validation/accuracy': 0.6877781748771667, 'validation/loss': 1.531459927558899, 'validation/bleu': 30.397316037761975, 'validation/num_examples': 3000, 'test/accuracy': 0.7057347297668457, 'test/loss': 1.445566177368164, 'test/bleu': 30.57403197903927, 'test/num_examples': 3003, 'score': 36160.994537353516, 'total_duration': 58801.58122611046, 'accumulated_submission_time': 36160.994537353516, 'accumulated_eval_time': 22635.92946076393, 'accumulated_logging_time': 1.444124460220337}
I0207 03:59:42.413463 140530045396736 logging_writer.py:48] [102593] accumulated_eval_time=22635.929461, accumulated_logging_time=1.444124, accumulated_submission_time=36160.994537, global_step=102593, preemption_count=0, score=36160.994537, test/accuracy=0.705735, test/bleu=30.574032, test/loss=1.445566, test/num_examples=3003, total_duration=58801.581226, train/accuracy=0.684201, train/bleu=34.352104, train/loss=1.579801, validation/accuracy=0.687778, validation/bleu=30.397316, validation/loss=1.531460, validation/num_examples=3000
I0207 03:59:45.235743 140530037004032 logging_writer.py:48] [102600] global_step=102600, grad_norm=0.49723830819129944, loss=2.7429006099700928
I0207 04:00:20.307996 140530045396736 logging_writer.py:48] [102700] global_step=102700, grad_norm=0.48257216811180115, loss=2.807147741317749
I0207 04:00:55.455617 140530037004032 logging_writer.py:48] [102800] global_step=102800, grad_norm=0.48092150688171387, loss=2.700960159301758
I0207 04:01:30.646643 140530045396736 logging_writer.py:48] [102900] global_step=102900, grad_norm=0.47480854392051697, loss=2.6861684322357178
I0207 04:02:05.887188 140530037004032 logging_writer.py:48] [103000] global_step=103000, grad_norm=0.49837931990623474, loss=2.6965227127075195
I0207 04:02:41.100592 140530045396736 logging_writer.py:48] [103100] global_step=103100, grad_norm=0.4838065505027771, loss=2.720344305038452
I0207 04:03:16.320671 140530037004032 logging_writer.py:48] [103200] global_step=103200, grad_norm=0.5105603933334351, loss=2.705690622329712
I0207 04:03:51.534932 140530045396736 logging_writer.py:48] [103300] global_step=103300, grad_norm=0.5027171969413757, loss=2.7147321701049805
I0207 04:04:26.753405 140530037004032 logging_writer.py:48] [103400] global_step=103400, grad_norm=0.49685803055763245, loss=2.7247424125671387
I0207 04:05:01.965291 140530045396736 logging_writer.py:48] [103500] global_step=103500, grad_norm=0.4927545189857483, loss=2.659611225128174
I0207 04:05:37.208418 140530037004032 logging_writer.py:48] [103600] global_step=103600, grad_norm=0.4739729166030884, loss=2.6710903644561768
I0207 04:06:12.429429 140530045396736 logging_writer.py:48] [103700] global_step=103700, grad_norm=0.4868953227996826, loss=2.7356390953063965
I0207 04:06:47.628134 140530037004032 logging_writer.py:48] [103800] global_step=103800, grad_norm=0.48837241530418396, loss=2.7025437355041504
I0207 04:07:22.860828 140530045396736 logging_writer.py:48] [103900] global_step=103900, grad_norm=0.4966295063495636, loss=2.6712493896484375
I0207 04:07:58.057444 140530037004032 logging_writer.py:48] [104000] global_step=104000, grad_norm=0.4968098998069763, loss=2.684837818145752
I0207 04:08:33.314570 140530045396736 logging_writer.py:48] [104100] global_step=104100, grad_norm=0.4956563413143158, loss=2.7013232707977295
I0207 04:09:08.552382 140530037004032 logging_writer.py:48] [104200] global_step=104200, grad_norm=0.4987882375717163, loss=2.6918044090270996
I0207 04:09:43.780443 140530045396736 logging_writer.py:48] [104300] global_step=104300, grad_norm=0.4832136034965515, loss=2.6873738765716553
I0207 04:10:19.075948 140530037004032 logging_writer.py:48] [104400] global_step=104400, grad_norm=0.5499326586723328, loss=2.7165746688842773
I0207 04:10:54.349355 140530045396736 logging_writer.py:48] [104500] global_step=104500, grad_norm=0.49941250681877136, loss=2.7420947551727295
I0207 04:11:29.638117 140530037004032 logging_writer.py:48] [104600] global_step=104600, grad_norm=0.5047265887260437, loss=2.7211358547210693
I0207 04:12:04.914114 140530045396736 logging_writer.py:48] [104700] global_step=104700, grad_norm=0.503602147102356, loss=2.7024028301239014
I0207 04:12:40.173807 140530037004032 logging_writer.py:48] [104800] global_step=104800, grad_norm=0.4879687428474426, loss=2.658966541290283
I0207 04:13:15.417282 140530045396736 logging_writer.py:48] [104900] global_step=104900, grad_norm=0.52310711145401, loss=2.7193708419799805
I0207 04:13:42.598656 140699726837568 spec.py:321] Evaluating on the training split.
I0207 04:13:45.632863 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 04:17:13.806984 140699726837568 spec.py:333] Evaluating on the validation split.
I0207 04:17:16.549344 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 04:19:48.766360 140699726837568 spec.py:349] Evaluating on the test split.
I0207 04:19:51.503730 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 04:22:14.566653 140699726837568 submission_runner.py:408] Time since start: 60153.76s, 	Step: 104979, 	{'train/accuracy': 0.683157205581665, 'train/loss': 1.5824335813522339, 'train/bleu': 34.28504809514936, 'validation/accuracy': 0.6898860335350037, 'validation/loss': 1.5270402431488037, 'validation/bleu': 30.47028869134486, 'validation/num_examples': 3000, 'test/accuracy': 0.7062576413154602, 'test/loss': 1.4383666515350342, 'test/bleu': 30.793022631949142, 'test/num_examples': 3003, 'score': 37001.09319806099, 'total_duration': 60153.76279973984, 'accumulated_submission_time': 37001.09319806099, 'accumulated_eval_time': 23147.897393465042, 'accumulated_logging_time': 1.483485221862793}
I0207 04:22:14.595983 140530037004032 logging_writer.py:48] [104979] accumulated_eval_time=23147.897393, accumulated_logging_time=1.483485, accumulated_submission_time=37001.093198, global_step=104979, preemption_count=0, score=37001.093198, test/accuracy=0.706258, test/bleu=30.793023, test/loss=1.438367, test/num_examples=3003, total_duration=60153.762800, train/accuracy=0.683157, train/bleu=34.285048, train/loss=1.582434, validation/accuracy=0.689886, validation/bleu=30.470289, validation/loss=1.527040, validation/num_examples=3000
I0207 04:22:22.343859 140530045396736 logging_writer.py:48] [105000] global_step=105000, grad_norm=0.47083210945129395, loss=2.6834142208099365
I0207 04:22:57.499290 140530037004032 logging_writer.py:48] [105100] global_step=105100, grad_norm=0.4815581440925598, loss=2.6460583209991455
I0207 04:23:32.690823 140530045396736 logging_writer.py:48] [105200] global_step=105200, grad_norm=0.4919799268245697, loss=2.747727394104004
I0207 04:24:07.914588 140530037004032 logging_writer.py:48] [105300] global_step=105300, grad_norm=0.5177642703056335, loss=2.703197479248047
I0207 04:24:43.173817 140530045396736 logging_writer.py:48] [105400] global_step=105400, grad_norm=0.5046480298042297, loss=2.6910269260406494
I0207 04:25:18.391236 140530037004032 logging_writer.py:48] [105500] global_step=105500, grad_norm=0.4920119047164917, loss=2.646111249923706
I0207 04:25:53.613430 140530045396736 logging_writer.py:48] [105600] global_step=105600, grad_norm=0.5177573561668396, loss=2.6733546257019043
I0207 04:26:28.846711 140530037004032 logging_writer.py:48] [105700] global_step=105700, grad_norm=0.5220239162445068, loss=2.6934115886688232
I0207 04:27:04.052965 140530045396736 logging_writer.py:48] [105800] global_step=105800, grad_norm=0.5031309127807617, loss=2.73172926902771
I0207 04:27:39.308056 140530037004032 logging_writer.py:48] [105900] global_step=105900, grad_norm=0.5648206472396851, loss=2.734691619873047
I0207 04:28:14.525827 140530045396736 logging_writer.py:48] [106000] global_step=106000, grad_norm=0.4921439290046692, loss=2.701934576034546
I0207 04:28:49.744060 140530037004032 logging_writer.py:48] [106100] global_step=106100, grad_norm=0.5039709210395813, loss=2.70232892036438
I0207 04:29:24.979452 140530045396736 logging_writer.py:48] [106200] global_step=106200, grad_norm=0.49550074338912964, loss=2.602738380432129
I0207 04:30:00.240536 140530037004032 logging_writer.py:48] [106300] global_step=106300, grad_norm=0.5159406661987305, loss=2.6862504482269287
I0207 04:30:35.455977 140530045396736 logging_writer.py:48] [106400] global_step=106400, grad_norm=0.5088138580322266, loss=2.6524739265441895
I0207 04:31:10.671984 140530037004032 logging_writer.py:48] [106500] global_step=106500, grad_norm=0.5487058162689209, loss=2.724499225616455
I0207 04:31:45.900724 140530045396736 logging_writer.py:48] [106600] global_step=106600, grad_norm=0.5274122953414917, loss=2.753338098526001
I0207 04:32:21.128876 140530037004032 logging_writer.py:48] [106700] global_step=106700, grad_norm=0.5152894258499146, loss=2.6527280807495117
I0207 04:32:56.347869 140530045396736 logging_writer.py:48] [106800] global_step=106800, grad_norm=0.5233719348907471, loss=2.7001917362213135
I0207 04:33:31.545999 140530037004032 logging_writer.py:48] [106900] global_step=106900, grad_norm=0.49649906158447266, loss=2.65090274810791
I0207 04:34:06.736708 140530045396736 logging_writer.py:48] [107000] global_step=107000, grad_norm=0.5308513045310974, loss=2.7050137519836426
I0207 04:34:41.953116 140530037004032 logging_writer.py:48] [107100] global_step=107100, grad_norm=0.521817684173584, loss=2.6946542263031006
I0207 04:35:17.167392 140530045396736 logging_writer.py:48] [107200] global_step=107200, grad_norm=0.5256702899932861, loss=2.6851966381073
I0207 04:35:52.387359 140530037004032 logging_writer.py:48] [107300] global_step=107300, grad_norm=0.520534873008728, loss=2.7042911052703857
I0207 04:36:14.641071 140699726837568 spec.py:321] Evaluating on the training split.
I0207 04:36:17.671016 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 04:39:19.837322 140699726837568 spec.py:333] Evaluating on the validation split.
I0207 04:39:22.561260 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 04:41:49.917007 140699726837568 spec.py:349] Evaluating on the test split.
I0207 04:41:52.680589 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 04:44:22.175834 140699726837568 submission_runner.py:408] Time since start: 61481.37s, 	Step: 107365, 	{'train/accuracy': 0.6957497596740723, 'train/loss': 1.5065596103668213, 'train/bleu': 35.563739134784505, 'validation/accuracy': 0.6896504759788513, 'validation/loss': 1.5250399112701416, 'validation/bleu': 30.502875640198855, 'validation/num_examples': 3000, 'test/accuracy': 0.7057463526725769, 'test/loss': 1.435275673866272, 'test/bleu': 30.804335144952017, 'test/num_examples': 3003, 'score': 37841.05610227585, 'total_duration': 61481.371970653534, 'accumulated_submission_time': 37841.05610227585, 'accumulated_eval_time': 23635.432076931, 'accumulated_logging_time': 1.522993564605713}
I0207 04:44:22.209273 140530045396736 logging_writer.py:48] [107365] accumulated_eval_time=23635.432077, accumulated_logging_time=1.522994, accumulated_submission_time=37841.056102, global_step=107365, preemption_count=0, score=37841.056102, test/accuracy=0.705746, test/bleu=30.804335, test/loss=1.435276, test/num_examples=3003, total_duration=61481.371971, train/accuracy=0.695750, train/bleu=35.563739, train/loss=1.506560, validation/accuracy=0.689650, validation/bleu=30.502876, validation/loss=1.525040, validation/num_examples=3000
I0207 04:44:34.851472 140530037004032 logging_writer.py:48] [107400] global_step=107400, grad_norm=0.5214780569076538, loss=2.7555816173553467
I0207 04:45:09.946815 140530045396736 logging_writer.py:48] [107500] global_step=107500, grad_norm=0.563066303730011, loss=2.6553966999053955
I0207 04:45:45.148746 140530037004032 logging_writer.py:48] [107600] global_step=107600, grad_norm=0.5143290162086487, loss=2.625033140182495
I0207 04:46:20.333101 140530045396736 logging_writer.py:48] [107700] global_step=107700, grad_norm=0.5289281606674194, loss=2.700411319732666
I0207 04:46:55.543644 140530037004032 logging_writer.py:48] [107800] global_step=107800, grad_norm=0.5596178770065308, loss=2.757375955581665
I0207 04:47:30.767785 140530045396736 logging_writer.py:48] [107900] global_step=107900, grad_norm=0.5173555016517639, loss=2.679886817932129
I0207 04:48:06.010389 140530037004032 logging_writer.py:48] [108000] global_step=108000, grad_norm=0.5472360253334045, loss=2.694293737411499
I0207 04:48:41.222669 140530045396736 logging_writer.py:48] [108100] global_step=108100, grad_norm=0.5155372023582458, loss=2.6672234535217285
I0207 04:49:16.434426 140530037004032 logging_writer.py:48] [108200] global_step=108200, grad_norm=0.5311217904090881, loss=2.7036101818084717
I0207 04:49:51.717955 140530045396736 logging_writer.py:48] [108300] global_step=108300, grad_norm=0.5106530785560608, loss=2.7250492572784424
I0207 04:50:27.046374 140530037004032 logging_writer.py:48] [108400] global_step=108400, grad_norm=0.5165536999702454, loss=2.713953733444214
I0207 04:51:02.283451 140530045396736 logging_writer.py:48] [108500] global_step=108500, grad_norm=0.5306740999221802, loss=2.6754555702209473
I0207 04:51:37.525360 140530037004032 logging_writer.py:48] [108600] global_step=108600, grad_norm=0.5461031198501587, loss=2.7300381660461426
I0207 04:52:12.749812 140530045396736 logging_writer.py:48] [108700] global_step=108700, grad_norm=0.5283876657485962, loss=2.671544313430786
I0207 04:52:47.972918 140530037004032 logging_writer.py:48] [108800] global_step=108800, grad_norm=0.5662861466407776, loss=2.700791597366333
I0207 04:53:23.200597 140530045396736 logging_writer.py:48] [108900] global_step=108900, grad_norm=0.5496600866317749, loss=2.633897542953491
I0207 04:53:58.393373 140530037004032 logging_writer.py:48] [109000] global_step=109000, grad_norm=0.5497539639472961, loss=2.673135280609131
I0207 04:54:33.649085 140530045396736 logging_writer.py:48] [109100] global_step=109100, grad_norm=0.521458625793457, loss=2.6862783432006836
I0207 04:55:08.888070 140530037004032 logging_writer.py:48] [109200] global_step=109200, grad_norm=0.5502921938896179, loss=2.6659300327301025
I0207 04:55:44.126466 140530045396736 logging_writer.py:48] [109300] global_step=109300, grad_norm=0.5515850782394409, loss=2.692375659942627
I0207 04:56:19.376013 140530037004032 logging_writer.py:48] [109400] global_step=109400, grad_norm=0.5582844018936157, loss=2.72420597076416
I0207 04:56:54.608153 140530045396736 logging_writer.py:48] [109500] global_step=109500, grad_norm=0.5675017833709717, loss=2.6642656326293945
I0207 04:57:29.850057 140530037004032 logging_writer.py:48] [109600] global_step=109600, grad_norm=0.5306915640830994, loss=2.6675329208374023
I0207 04:58:05.074683 140530045396736 logging_writer.py:48] [109700] global_step=109700, grad_norm=0.5790786147117615, loss=2.7118709087371826
I0207 04:58:22.395177 140699726837568 spec.py:321] Evaluating on the training split.
I0207 04:58:25.430552 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 05:02:07.720817 140699726837568 spec.py:333] Evaluating on the validation split.
I0207 05:02:10.454679 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 05:04:40.940693 140699726837568 spec.py:349] Evaluating on the test split.
I0207 05:04:43.670813 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 05:07:25.005743 140699726837568 submission_runner.py:408] Time since start: 62864.20s, 	Step: 109751, 	{'train/accuracy': 0.6920896172523499, 'train/loss': 1.5295428037643433, 'train/bleu': 35.06577869851787, 'validation/accuracy': 0.6902828216552734, 'validation/loss': 1.5207116603851318, 'validation/bleu': 30.492036028114327, 'validation/num_examples': 3000, 'test/accuracy': 0.7073732018470764, 'test/loss': 1.4307950735092163, 'test/bleu': 30.37974883692844, 'test/num_examples': 3003, 'score': 38681.15629196167, 'total_duration': 62864.20189833641, 'accumulated_submission_time': 38681.15629196167, 'accumulated_eval_time': 24178.042583703995, 'accumulated_logging_time': 1.5672264099121094}
I0207 05:07:25.035170 140530037004032 logging_writer.py:48] [109751] accumulated_eval_time=24178.042584, accumulated_logging_time=1.567226, accumulated_submission_time=38681.156292, global_step=109751, preemption_count=0, score=38681.156292, test/accuracy=0.707373, test/bleu=30.379749, test/loss=1.430795, test/num_examples=3003, total_duration=62864.201898, train/accuracy=0.692090, train/bleu=35.065779, train/loss=1.529543, validation/accuracy=0.690283, validation/bleu=30.492036, validation/loss=1.520712, validation/num_examples=3000
I0207 05:07:42.587578 140530045396736 logging_writer.py:48] [109800] global_step=109800, grad_norm=0.5385587811470032, loss=2.673224687576294
I0207 05:08:17.681434 140530037004032 logging_writer.py:48] [109900] global_step=109900, grad_norm=0.5465652942657471, loss=2.710043430328369
I0207 05:08:52.874753 140530045396736 logging_writer.py:48] [110000] global_step=110000, grad_norm=0.5402959585189819, loss=2.6783690452575684
I0207 05:09:28.071100 140530037004032 logging_writer.py:48] [110100] global_step=110100, grad_norm=0.5587586164474487, loss=2.6845076084136963
I0207 05:10:03.283476 140530045396736 logging_writer.py:48] [110200] global_step=110200, grad_norm=0.5634320378303528, loss=2.6964848041534424
I0207 05:10:38.510387 140530037004032 logging_writer.py:48] [110300] global_step=110300, grad_norm=0.5774980783462524, loss=2.714491367340088
I0207 05:11:13.854094 140530045396736 logging_writer.py:48] [110400] global_step=110400, grad_norm=0.5559030771255493, loss=2.6516647338867188
I0207 05:11:49.076043 140530037004032 logging_writer.py:48] [110500] global_step=110500, grad_norm=0.5746293067932129, loss=2.6699631214141846
I0207 05:12:24.289973 140530045396736 logging_writer.py:48] [110600] global_step=110600, grad_norm=0.5389323234558105, loss=2.672330141067505
I0207 05:12:59.553922 140530037004032 logging_writer.py:48] [110700] global_step=110700, grad_norm=0.5549944043159485, loss=2.6593666076660156
I0207 05:13:34.771269 140530045396736 logging_writer.py:48] [110800] global_step=110800, grad_norm=0.5899600386619568, loss=2.6666390895843506
I0207 05:14:10.000446 140530037004032 logging_writer.py:48] [110900] global_step=110900, grad_norm=0.5594438910484314, loss=2.738938093185425
I0207 05:14:45.236671 140530045396736 logging_writer.py:48] [111000] global_step=111000, grad_norm=0.5405294895172119, loss=2.670994997024536
I0207 05:15:20.479119 140530037004032 logging_writer.py:48] [111100] global_step=111100, grad_norm=0.535260021686554, loss=2.6514267921447754
I0207 05:15:55.707863 140530045396736 logging_writer.py:48] [111200] global_step=111200, grad_norm=0.5449639558792114, loss=2.6232728958129883
I0207 05:16:30.942686 140530037004032 logging_writer.py:48] [111300] global_step=111300, grad_norm=0.5825533866882324, loss=2.6707816123962402
I0207 05:17:06.188839 140530045396736 logging_writer.py:48] [111400] global_step=111400, grad_norm=0.5656852722167969, loss=2.661815643310547
I0207 05:17:41.412116 140530037004032 logging_writer.py:48] [111500] global_step=111500, grad_norm=0.5724069476127625, loss=2.6837689876556396
I0207 05:18:16.638137 140530045396736 logging_writer.py:48] [111600] global_step=111600, grad_norm=0.5658455491065979, loss=2.677716016769409
I0207 05:18:51.873816 140530037004032 logging_writer.py:48] [111700] global_step=111700, grad_norm=0.5683456063270569, loss=2.6381728649139404
I0207 05:19:27.102875 140530045396736 logging_writer.py:48] [111800] global_step=111800, grad_norm=0.5651491284370422, loss=2.5941781997680664
I0207 05:20:02.324104 140530037004032 logging_writer.py:48] [111900] global_step=111900, grad_norm=0.5608957409858704, loss=2.659377098083496
I0207 05:20:37.545575 140530045396736 logging_writer.py:48] [112000] global_step=112000, grad_norm=0.586237907409668, loss=2.6343979835510254
I0207 05:21:12.757261 140530037004032 logging_writer.py:48] [112100] global_step=112100, grad_norm=0.5636808276176453, loss=2.6606533527374268
I0207 05:21:25.162107 140699726837568 spec.py:321] Evaluating on the training split.
I0207 05:21:28.198866 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 05:24:24.026619 140699726837568 spec.py:333] Evaluating on the validation split.
I0207 05:24:26.767295 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 05:26:53.262126 140699726837568 spec.py:349] Evaluating on the test split.
I0207 05:26:55.997588 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 05:29:07.692332 140699726837568 submission_runner.py:408] Time since start: 64166.89s, 	Step: 112137, 	{'train/accuracy': 0.6917627453804016, 'train/loss': 1.534698724746704, 'train/bleu': 34.744883823713636, 'validation/accuracy': 0.6911011338233948, 'validation/loss': 1.5178720951080322, 'validation/bleu': 30.914413893236805, 'validation/num_examples': 3000, 'test/accuracy': 0.7082563638687134, 'test/loss': 1.4285781383514404, 'test/bleu': 30.58984000784102, 'test/num_examples': 3003, 'score': 39521.200323581696, 'total_duration': 64166.888488292694, 'accumulated_submission_time': 39521.200323581696, 'accumulated_eval_time': 24640.57275032997, 'accumulated_logging_time': 1.606520652770996}
I0207 05:29:07.721719 140530045396736 logging_writer.py:48] [112137] accumulated_eval_time=24640.572750, accumulated_logging_time=1.606521, accumulated_submission_time=39521.200324, global_step=112137, preemption_count=0, score=39521.200324, test/accuracy=0.708256, test/bleu=30.589840, test/loss=1.428578, test/num_examples=3003, total_duration=64166.888488, train/accuracy=0.691763, train/bleu=34.744884, train/loss=1.534699, validation/accuracy=0.691101, validation/bleu=30.914414, validation/loss=1.517872, validation/num_examples=3000
I0207 05:29:30.196176 140530037004032 logging_writer.py:48] [112200] global_step=112200, grad_norm=0.5861617922782898, loss=2.6223039627075195
I0207 05:30:05.346746 140530045396736 logging_writer.py:48] [112300] global_step=112300, grad_norm=0.5574142932891846, loss=2.6606574058532715
I0207 05:30:40.544078 140530037004032 logging_writer.py:48] [112400] global_step=112400, grad_norm=0.5768072605133057, loss=2.6538546085357666
I0207 05:31:15.736163 140530045396736 logging_writer.py:48] [112500] global_step=112500, grad_norm=0.5742762088775635, loss=2.6411519050598145
I0207 05:31:50.946641 140530037004032 logging_writer.py:48] [112600] global_step=112600, grad_norm=0.5980756878852844, loss=2.6515471935272217
I0207 05:32:26.151744 140530045396736 logging_writer.py:48] [112700] global_step=112700, grad_norm=0.5916985273361206, loss=2.689497947692871
I0207 05:33:01.453745 140530037004032 logging_writer.py:48] [112800] global_step=112800, grad_norm=0.551144003868103, loss=2.6983296871185303
I0207 05:33:36.705422 140530045396736 logging_writer.py:48] [112900] global_step=112900, grad_norm=0.5823510885238647, loss=2.6052112579345703
I0207 05:34:11.916637 140530037004032 logging_writer.py:48] [113000] global_step=113000, grad_norm=0.598046600818634, loss=2.6761865615844727
I0207 05:34:47.138343 140530045396736 logging_writer.py:48] [113100] global_step=113100, grad_norm=0.6173965334892273, loss=2.690747022628784
I0207 05:35:22.385593 140530037004032 logging_writer.py:48] [113200] global_step=113200, grad_norm=0.606072187423706, loss=2.6999988555908203
I0207 05:35:57.632017 140530045396736 logging_writer.py:48] [113300] global_step=113300, grad_norm=1.4534415006637573, loss=2.7039496898651123
I0207 05:36:32.889323 140530037004032 logging_writer.py:48] [113400] global_step=113400, grad_norm=0.5861207246780396, loss=2.666067361831665
I0207 05:37:08.128689 140530045396736 logging_writer.py:48] [113500] global_step=113500, grad_norm=0.5796956419944763, loss=2.596975088119507
I0207 05:37:43.339489 140530037004032 logging_writer.py:48] [113600] global_step=113600, grad_norm=0.5941401124000549, loss=2.6408631801605225
I0207 05:38:18.602569 140530045396736 logging_writer.py:48] [113700] global_step=113700, grad_norm=0.6048960089683533, loss=2.6210312843322754
I0207 05:38:53.797475 140530037004032 logging_writer.py:48] [113800] global_step=113800, grad_norm=0.5644243359565735, loss=2.626159191131592
I0207 05:39:29.015769 140530045396736 logging_writer.py:48] [113900] global_step=113900, grad_norm=0.6006602644920349, loss=2.660787343978882
I0207 05:40:04.249737 140530037004032 logging_writer.py:48] [114000] global_step=114000, grad_norm=0.5739015340805054, loss=2.5909790992736816
I0207 05:40:39.530959 140530045396736 logging_writer.py:48] [114100] global_step=114100, grad_norm=0.5686833262443542, loss=2.649225950241089
I0207 05:41:14.742137 140530037004032 logging_writer.py:48] [114200] global_step=114200, grad_norm=0.5980058908462524, loss=2.5901060104370117
I0207 05:41:49.954038 140530045396736 logging_writer.py:48] [114300] global_step=114300, grad_norm=0.5952591300010681, loss=2.57621693611145
I0207 05:42:25.197474 140530037004032 logging_writer.py:48] [114400] global_step=114400, grad_norm=0.5995311737060547, loss=2.627986192703247
I0207 05:43:00.392691 140530045396736 logging_writer.py:48] [114500] global_step=114500, grad_norm=0.5726450681686401, loss=2.6484248638153076
I0207 05:43:07.861317 140699726837568 spec.py:321] Evaluating on the training split.
I0207 05:43:10.896327 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 05:46:31.501068 140699726837568 spec.py:333] Evaluating on the validation split.
I0207 05:46:34.225313 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 05:49:10.828922 140699726837568 spec.py:349] Evaluating on the test split.
I0207 05:49:13.577605 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 05:51:32.642056 140699726837568 submission_runner.py:408] Time since start: 65511.84s, 	Step: 114523, 	{'train/accuracy': 0.698196291923523, 'train/loss': 1.4954743385314941, 'train/bleu': 35.77462367787801, 'validation/accuracy': 0.6925270557403564, 'validation/loss': 1.5115963220596313, 'validation/bleu': 30.57723185326211, 'validation/num_examples': 3000, 'test/accuracy': 0.708628237247467, 'test/loss': 1.4227757453918457, 'test/bleu': 30.869338740033143, 'test/num_examples': 3003, 'score': 40361.25352883339, 'total_duration': 65511.83819484711, 'accumulated_submission_time': 40361.25352883339, 'accumulated_eval_time': 25145.353414535522, 'accumulated_logging_time': 1.647085428237915}
I0207 05:51:32.673945 140530037004032 logging_writer.py:48] [114523] accumulated_eval_time=25145.353415, accumulated_logging_time=1.647085, accumulated_submission_time=40361.253529, global_step=114523, preemption_count=0, score=40361.253529, test/accuracy=0.708628, test/bleu=30.869339, test/loss=1.422776, test/num_examples=3003, total_duration=65511.838195, train/accuracy=0.698196, train/bleu=35.774624, train/loss=1.495474, validation/accuracy=0.692527, validation/bleu=30.577232, validation/loss=1.511596, validation/num_examples=3000
I0207 05:52:00.052392 140530045396736 logging_writer.py:48] [114600] global_step=114600, grad_norm=0.6189863681793213, loss=2.6935179233551025
I0207 05:52:35.213496 140530037004032 logging_writer.py:48] [114700] global_step=114700, grad_norm=0.6199577450752258, loss=2.642577648162842
I0207 05:53:10.402618 140530045396736 logging_writer.py:48] [114800] global_step=114800, grad_norm=0.6034339070320129, loss=2.6202383041381836
I0207 05:53:45.666388 140530037004032 logging_writer.py:48] [114900] global_step=114900, grad_norm=0.589285671710968, loss=2.712822914123535
I0207 05:54:20.883781 140530045396736 logging_writer.py:48] [115000] global_step=115000, grad_norm=0.6365604400634766, loss=2.6361289024353027
I0207 05:54:56.091391 140530037004032 logging_writer.py:48] [115100] global_step=115100, grad_norm=0.6049979329109192, loss=2.6238765716552734
I0207 05:55:31.351207 140530045396736 logging_writer.py:48] [115200] global_step=115200, grad_norm=0.660882294178009, loss=2.679046154022217
I0207 05:56:06.713019 140530037004032 logging_writer.py:48] [115300] global_step=115300, grad_norm=0.6037489175796509, loss=2.6334166526794434
I0207 05:56:41.968746 140530045396736 logging_writer.py:48] [115400] global_step=115400, grad_norm=0.6034464240074158, loss=2.661428928375244
I0207 05:57:17.180233 140530037004032 logging_writer.py:48] [115500] global_step=115500, grad_norm=0.6005269289016724, loss=2.6234991550445557
I0207 05:57:52.387010 140530045396736 logging_writer.py:48] [115600] global_step=115600, grad_norm=0.6199247241020203, loss=2.6324687004089355
I0207 05:58:27.627176 140530037004032 logging_writer.py:48] [115700] global_step=115700, grad_norm=0.5976554155349731, loss=2.581789493560791
I0207 05:59:02.824979 140530045396736 logging_writer.py:48] [115800] global_step=115800, grad_norm=0.6204734444618225, loss=2.667185068130493
I0207 05:59:38.028331 140530037004032 logging_writer.py:48] [115900] global_step=115900, grad_norm=0.5915802717208862, loss=2.6290104389190674
I0207 06:00:13.279384 140530045396736 logging_writer.py:48] [116000] global_step=116000, grad_norm=0.598426342010498, loss=2.631143569946289
I0207 06:00:48.508566 140530037004032 logging_writer.py:48] [116100] global_step=116100, grad_norm=0.6212242245674133, loss=2.6377644538879395
I0207 06:01:23.741478 140530045396736 logging_writer.py:48] [116200] global_step=116200, grad_norm=0.6218866109848022, loss=2.6921744346618652
I0207 06:01:59.001558 140530037004032 logging_writer.py:48] [116300] global_step=116300, grad_norm=0.6130866408348083, loss=2.5479490756988525
I0207 06:02:34.248787 140530045396736 logging_writer.py:48] [116400] global_step=116400, grad_norm=0.6147698163986206, loss=2.6324756145477295
I0207 06:03:09.495077 140530037004032 logging_writer.py:48] [116500] global_step=116500, grad_norm=0.6127257347106934, loss=2.601595640182495
I0207 06:03:44.738955 140530045396736 logging_writer.py:48] [116600] global_step=116600, grad_norm=0.610378086566925, loss=2.580406904220581
I0207 06:04:19.963595 140530037004032 logging_writer.py:48] [116700] global_step=116700, grad_norm=0.6183230876922607, loss=2.65726637840271
I0207 06:04:55.248454 140530045396736 logging_writer.py:48] [116800] global_step=116800, grad_norm=0.6140527129173279, loss=2.6625404357910156
I0207 06:05:30.544790 140530037004032 logging_writer.py:48] [116900] global_step=116900, grad_norm=0.6320235729217529, loss=2.658604383468628
I0207 06:05:32.737638 140699726837568 spec.py:321] Evaluating on the training split.
I0207 06:05:35.764921 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 06:08:38.730814 140699726837568 spec.py:333] Evaluating on the validation split.
I0207 06:08:41.460832 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 06:11:07.549777 140699726837568 spec.py:349] Evaluating on the test split.
I0207 06:11:10.291704 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 06:13:34.623454 140699726837568 submission_runner.py:408] Time since start: 66833.82s, 	Step: 116908, 	{'train/accuracy': 0.6974208354949951, 'train/loss': 1.5098053216934204, 'train/bleu': 35.78149866685925, 'validation/accuracy': 0.693990170955658, 'validation/loss': 1.5112836360931396, 'validation/bleu': 30.96133738228591, 'validation/num_examples': 3000, 'test/accuracy': 0.7094300389289856, 'test/loss': 1.4189343452453613, 'test/bleu': 30.701844657815702, 'test/num_examples': 3003, 'score': 41201.23071146011, 'total_duration': 66833.81958413124, 'accumulated_submission_time': 41201.23071146011, 'accumulated_eval_time': 25627.239145994186, 'accumulated_logging_time': 1.689962387084961}
I0207 06:13:34.654745 140530045396736 logging_writer.py:48] [116908] accumulated_eval_time=25627.239146, accumulated_logging_time=1.689962, accumulated_submission_time=41201.230711, global_step=116908, preemption_count=0, score=41201.230711, test/accuracy=0.709430, test/bleu=30.701845, test/loss=1.418934, test/num_examples=3003, total_duration=66833.819584, train/accuracy=0.697421, train/bleu=35.781499, train/loss=1.509805, validation/accuracy=0.693990, validation/bleu=30.961337, validation/loss=1.511284, validation/num_examples=3000
I0207 06:14:07.317581 140530037004032 logging_writer.py:48] [117000] global_step=117000, grad_norm=0.6045905947685242, loss=2.571577548980713
I0207 06:14:42.497117 140530045396736 logging_writer.py:48] [117100] global_step=117100, grad_norm=0.6154137253761292, loss=2.6387314796447754
I0207 06:15:17.675759 140530037004032 logging_writer.py:48] [117200] global_step=117200, grad_norm=0.6355865001678467, loss=2.635653495788574
I0207 06:15:52.888126 140530045396736 logging_writer.py:48] [117300] global_step=117300, grad_norm=0.6550002694129944, loss=2.6939632892608643
I0207 06:16:28.121809 140530037004032 logging_writer.py:48] [117400] global_step=117400, grad_norm=0.6496509909629822, loss=2.6438939571380615
I0207 06:17:03.330260 140530045396736 logging_writer.py:48] [117500] global_step=117500, grad_norm=0.6307247877120972, loss=2.6131155490875244
I0207 06:17:38.532970 140530037004032 logging_writer.py:48] [117600] global_step=117600, grad_norm=0.6278760433197021, loss=2.616608142852783
I0207 06:18:13.751360 140530045396736 logging_writer.py:48] [117700] global_step=117700, grad_norm=0.6076589226722717, loss=2.6055359840393066
I0207 06:18:48.968373 140530037004032 logging_writer.py:48] [117800] global_step=117800, grad_norm=0.6275452971458435, loss=2.5998342037200928
I0207 06:19:24.224129 140530045396736 logging_writer.py:48] [117900] global_step=117900, grad_norm=0.63582843542099, loss=2.5935466289520264
I0207 06:19:59.468604 140530037004032 logging_writer.py:48] [118000] global_step=118000, grad_norm=0.607154905796051, loss=2.5349411964416504
I0207 06:20:34.708399 140530045396736 logging_writer.py:48] [118100] global_step=118100, grad_norm=0.6301587820053101, loss=2.5634257793426514
I0207 06:21:09.932251 140530037004032 logging_writer.py:48] [118200] global_step=118200, grad_norm=0.6426218748092651, loss=2.6483254432678223
I0207 06:21:45.165345 140530045396736 logging_writer.py:48] [118300] global_step=118300, grad_norm=0.6376914381980896, loss=2.6254031658172607
I0207 06:22:20.366007 140530037004032 logging_writer.py:48] [118400] global_step=118400, grad_norm=0.6344724893569946, loss=2.6065502166748047
I0207 06:22:55.570032 140530045396736 logging_writer.py:48] [118500] global_step=118500, grad_norm=0.6553537249565125, loss=2.7104413509368896
I0207 06:23:30.771425 140530037004032 logging_writer.py:48] [118600] global_step=118600, grad_norm=0.6592902541160583, loss=2.756736993789673
I0207 06:24:06.002585 140530045396736 logging_writer.py:48] [118700] global_step=118700, grad_norm=0.6475625038146973, loss=2.675208568572998
I0207 06:24:41.183759 140530037004032 logging_writer.py:48] [118800] global_step=118800, grad_norm=0.6435709595680237, loss=2.551853895187378
I0207 06:25:16.421368 140530045396736 logging_writer.py:48] [118900] global_step=118900, grad_norm=0.639374315738678, loss=2.6363632678985596
I0207 06:25:51.619807 140530037004032 logging_writer.py:48] [119000] global_step=119000, grad_norm=0.6594158411026001, loss=2.603557586669922
I0207 06:26:26.831843 140530045396736 logging_writer.py:48] [119100] global_step=119100, grad_norm=0.6300196051597595, loss=2.567538261413574
I0207 06:27:02.044366 140530037004032 logging_writer.py:48] [119200] global_step=119200, grad_norm=0.6414104104042053, loss=2.6424269676208496
I0207 06:27:34.867891 140699726837568 spec.py:321] Evaluating on the training split.
I0207 06:27:37.918208 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 06:30:41.455080 140699726837568 spec.py:333] Evaluating on the validation split.
I0207 06:30:44.190532 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 06:33:15.655233 140699726837568 spec.py:349] Evaluating on the test split.
I0207 06:33:18.388948 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 06:35:49.255887 140699726837568 submission_runner.py:408] Time since start: 68168.45s, 	Step: 119295, 	{'train/accuracy': 0.710716187953949, 'train/loss': 1.4390244483947754, 'train/bleu': 36.5941331650276, 'validation/accuracy': 0.6939281225204468, 'validation/loss': 1.5062028169631958, 'validation/bleu': 30.915858804496704, 'validation/num_examples': 3000, 'test/accuracy': 0.7100808024406433, 'test/loss': 1.416991114616394, 'test/bleu': 30.658603829809408, 'test/num_examples': 3003, 'score': 42041.36020541191, 'total_duration': 68168.45203638077, 'accumulated_submission_time': 42041.36020541191, 'accumulated_eval_time': 26121.627092838287, 'accumulated_logging_time': 1.7309155464172363}
I0207 06:35:49.286821 140530045396736 logging_writer.py:48] [119295] accumulated_eval_time=26121.627093, accumulated_logging_time=1.730916, accumulated_submission_time=42041.360205, global_step=119295, preemption_count=0, score=42041.360205, test/accuracy=0.710081, test/bleu=30.658604, test/loss=1.416991, test/num_examples=3003, total_duration=68168.452036, train/accuracy=0.710716, train/bleu=36.594133, train/loss=1.439024, validation/accuracy=0.693928, validation/bleu=30.915859, validation/loss=1.506203, validation/num_examples=3000
I0207 06:35:51.403534 140530037004032 logging_writer.py:48] [119300] global_step=119300, grad_norm=0.6491058468818665, loss=2.601090908050537
I0207 06:36:26.532835 140530045396736 logging_writer.py:48] [119400] global_step=119400, grad_norm=0.6750158667564392, loss=2.628713607788086
I0207 06:37:01.676661 140530037004032 logging_writer.py:48] [119500] global_step=119500, grad_norm=0.6363393068313599, loss=2.6070640087127686
I0207 06:37:36.870895 140530045396736 logging_writer.py:48] [119600] global_step=119600, grad_norm=0.6324438452720642, loss=2.6133244037628174
I0207 06:38:12.107736 140530037004032 logging_writer.py:48] [119700] global_step=119700, grad_norm=0.6515270471572876, loss=2.546907901763916
I0207 06:38:47.313515 140530045396736 logging_writer.py:48] [119800] global_step=119800, grad_norm=0.6318665146827698, loss=2.622851610183716
I0207 06:39:22.544885 140530037004032 logging_writer.py:48] [119900] global_step=119900, grad_norm=0.6687166690826416, loss=2.614969491958618
I0207 06:39:57.821475 140530045396736 logging_writer.py:48] [120000] global_step=120000, grad_norm=0.6644924879074097, loss=2.6060495376586914
I0207 06:40:33.083906 140530037004032 logging_writer.py:48] [120100] global_step=120100, grad_norm=0.6555792689323425, loss=2.5862836837768555
I0207 06:41:08.303064 140530045396736 logging_writer.py:48] [120200] global_step=120200, grad_norm=0.6412917375564575, loss=2.620415449142456
I0207 06:41:43.528419 140530037004032 logging_writer.py:48] [120300] global_step=120300, grad_norm=0.6361967325210571, loss=2.620309829711914
I0207 06:42:18.790664 140530045396736 logging_writer.py:48] [120400] global_step=120400, grad_norm=0.6644842624664307, loss=2.6475491523742676
I0207 06:42:54.025721 140530037004032 logging_writer.py:48] [120500] global_step=120500, grad_norm=0.6484217047691345, loss=2.5452301502227783
I0207 06:43:29.264700 140530045396736 logging_writer.py:48] [120600] global_step=120600, grad_norm=0.6449601650238037, loss=2.6391308307647705
I0207 06:44:04.484903 140530037004032 logging_writer.py:48] [120700] global_step=120700, grad_norm=0.6850163340568542, loss=2.606205701828003
I0207 06:44:39.734524 140530045396736 logging_writer.py:48] [120800] global_step=120800, grad_norm=0.6429656744003296, loss=2.6382813453674316
I0207 06:45:14.967601 140530037004032 logging_writer.py:48] [120900] global_step=120900, grad_norm=0.6495331525802612, loss=2.5654513835906982
I0207 06:45:50.221025 140530045396736 logging_writer.py:48] [121000] global_step=121000, grad_norm=0.6551544666290283, loss=2.5749456882476807
I0207 06:46:25.426794 140530037004032 logging_writer.py:48] [121100] global_step=121100, grad_norm=0.6452826857566833, loss=2.6352505683898926
I0207 06:47:00.643947 140530045396736 logging_writer.py:48] [121200] global_step=121200, grad_norm=0.6423240303993225, loss=2.560656785964966
I0207 06:47:35.857260 140530037004032 logging_writer.py:48] [121300] global_step=121300, grad_norm=0.6563095450401306, loss=2.5502688884735107
I0207 06:48:11.092749 140530045396736 logging_writer.py:48] [121400] global_step=121400, grad_norm=0.667904257774353, loss=2.594757318496704
I0207 06:48:46.378689 140530037004032 logging_writer.py:48] [121500] global_step=121500, grad_norm=0.6326739192008972, loss=2.6921653747558594
I0207 06:49:21.612732 140530045396736 logging_writer.py:48] [121600] global_step=121600, grad_norm=0.7112914323806763, loss=2.564948081970215
I0207 06:49:49.513700 140699726837568 spec.py:321] Evaluating on the training split.
I0207 06:49:52.539693 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 06:53:12.356930 140699726837568 spec.py:333] Evaluating on the validation split.
I0207 06:53:15.105053 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 06:55:46.925236 140699726837568 spec.py:349] Evaluating on the test split.
I0207 06:55:49.683906 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 06:58:05.537441 140699726837568 submission_runner.py:408] Time since start: 69504.73s, 	Step: 121681, 	{'train/accuracy': 0.7038121819496155, 'train/loss': 1.4720726013183594, 'train/bleu': 36.19937840630523, 'validation/accuracy': 0.6947588920593262, 'validation/loss': 1.5022433996200562, 'validation/bleu': 30.821739257184117, 'validation/num_examples': 3000, 'test/accuracy': 0.7108593583106995, 'test/loss': 1.4113267660140991, 'test/bleu': 30.843223404698186, 'test/num_examples': 3003, 'score': 42881.50228333473, 'total_duration': 69504.73358535767, 'accumulated_submission_time': 42881.50228333473, 'accumulated_eval_time': 26617.650767326355, 'accumulated_logging_time': 1.7730438709259033}
I0207 06:58:05.569541 140530037004032 logging_writer.py:48] [121681] accumulated_eval_time=26617.650767, accumulated_logging_time=1.773044, accumulated_submission_time=42881.502283, global_step=121681, preemption_count=0, score=42881.502283, test/accuracy=0.710859, test/bleu=30.843223, test/loss=1.411327, test/num_examples=3003, total_duration=69504.733585, train/accuracy=0.703812, train/bleu=36.199378, train/loss=1.472073, validation/accuracy=0.694759, validation/bleu=30.821739, validation/loss=1.502243, validation/num_examples=3000
I0207 06:58:12.603608 140530045396736 logging_writer.py:48] [121700] global_step=121700, grad_norm=0.6410665512084961, loss=2.5771453380584717
I0207 06:58:47.732834 140530037004032 logging_writer.py:48] [121800] global_step=121800, grad_norm=0.6578423380851746, loss=2.642564296722412
I0207 06:59:22.955956 140530045396736 logging_writer.py:48] [121900] global_step=121900, grad_norm=0.6497177481651306, loss=2.594447612762451
I0207 06:59:58.192988 140530037004032 logging_writer.py:48] [122000] global_step=122000, grad_norm=0.637431800365448, loss=2.5655033588409424
I0207 07:00:33.421133 140530045396736 logging_writer.py:48] [122100] global_step=122100, grad_norm=0.6752067804336548, loss=2.634852886199951
I0207 07:01:08.626929 140530037004032 logging_writer.py:48] [122200] global_step=122200, grad_norm=0.6639730334281921, loss=2.6000304222106934
I0207 07:01:43.847404 140530045396736 logging_writer.py:48] [122300] global_step=122300, grad_norm=0.6512523889541626, loss=2.576993227005005
I0207 07:02:19.084547 140530037004032 logging_writer.py:48] [122400] global_step=122400, grad_norm=0.6828648447990417, loss=2.686717987060547
I0207 07:02:54.337271 140530045396736 logging_writer.py:48] [122500] global_step=122500, grad_norm=0.7217521071434021, loss=2.578977584838867
I0207 07:03:29.567851 140530037004032 logging_writer.py:48] [122600] global_step=122600, grad_norm=0.6381602883338928, loss=2.5793521404266357
I0207 07:04:04.780317 140530045396736 logging_writer.py:48] [122700] global_step=122700, grad_norm=0.6846452355384827, loss=2.662668466567993
I0207 07:04:39.995794 140530037004032 logging_writer.py:48] [122800] global_step=122800, grad_norm=0.6725598573684692, loss=2.5451886653900146
I0207 07:05:15.259765 140530045396736 logging_writer.py:48] [122900] global_step=122900, grad_norm=0.6783908009529114, loss=2.6037240028381348
I0207 07:05:50.535937 140530037004032 logging_writer.py:48] [123000] global_step=123000, grad_norm=0.6583549976348877, loss=2.602994918823242
I0207 07:06:25.814896 140530045396736 logging_writer.py:48] [123100] global_step=123100, grad_norm=0.6606525182723999, loss=2.5836689472198486
I0207 07:07:01.169453 140530037004032 logging_writer.py:48] [123200] global_step=123200, grad_norm=0.6512393951416016, loss=2.622011661529541
I0207 07:07:36.419209 140530045396736 logging_writer.py:48] [123300] global_step=123300, grad_norm=0.6613729596138, loss=2.5805585384368896
I0207 07:08:11.662162 140530037004032 logging_writer.py:48] [123400] global_step=123400, grad_norm=0.670708179473877, loss=2.548963785171509
I0207 07:08:46.888135 140530045396736 logging_writer.py:48] [123500] global_step=123500, grad_norm=0.6655172109603882, loss=2.6220245361328125
I0207 07:09:22.097223 140530037004032 logging_writer.py:48] [123600] global_step=123600, grad_norm=0.6955039501190186, loss=2.586371660232544
I0207 07:09:57.293541 140530045396736 logging_writer.py:48] [123700] global_step=123700, grad_norm=0.6887686848640442, loss=2.610954761505127
I0207 07:10:32.503342 140530037004032 logging_writer.py:48] [123800] global_step=123800, grad_norm=0.657666802406311, loss=2.5995254516601562
I0207 07:11:07.742159 140530045396736 logging_writer.py:48] [123900] global_step=123900, grad_norm=0.6524142026901245, loss=2.6290979385375977
I0207 07:11:42.971334 140530037004032 logging_writer.py:48] [124000] global_step=124000, grad_norm=0.6628754138946533, loss=2.5774128437042236
I0207 07:12:05.600686 140699726837568 spec.py:321] Evaluating on the training split.
I0207 07:12:08.640495 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 07:15:30.094705 140699726837568 spec.py:333] Evaluating on the validation split.
I0207 07:15:32.825654 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 07:17:55.792034 140699726837568 spec.py:349] Evaluating on the test split.
I0207 07:17:58.545012 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 07:20:19.149891 140699726837568 submission_runner.py:408] Time since start: 70838.35s, 	Step: 124066, 	{'train/accuracy': 0.7046647667884827, 'train/loss': 1.4676135778427124, 'train/bleu': 36.53354128224039, 'validation/accuracy': 0.6953912377357483, 'validation/loss': 1.5036792755126953, 'validation/bleu': 30.746032259348308, 'validation/num_examples': 3000, 'test/accuracy': 0.7117890119552612, 'test/loss': 1.4099397659301758, 'test/bleu': 30.892537332352088, 'test/num_examples': 3003, 'score': 43721.44666552544, 'total_duration': 70838.34599137306, 'accumulated_submission_time': 43721.44666552544, 'accumulated_eval_time': 27111.19986152649, 'accumulated_logging_time': 1.8156933784484863}
I0207 07:20:19.188895 140530045396736 logging_writer.py:48] [124066] accumulated_eval_time=27111.199862, accumulated_logging_time=1.815693, accumulated_submission_time=43721.446666, global_step=124066, preemption_count=0, score=43721.446666, test/accuracy=0.711789, test/bleu=30.892537, test/loss=1.409940, test/num_examples=3003, total_duration=70838.345991, train/accuracy=0.704665, train/bleu=36.533541, train/loss=1.467614, validation/accuracy=0.695391, validation/bleu=30.746032, validation/loss=1.503679, validation/num_examples=3000
I0207 07:20:31.580113 140530037004032 logging_writer.py:48] [124100] global_step=124100, grad_norm=0.671094536781311, loss=2.582162857055664
I0207 07:21:06.736207 140530045396736 logging_writer.py:48] [124200] global_step=124200, grad_norm=0.6821013689041138, loss=2.617905616760254
I0207 07:21:41.956717 140530037004032 logging_writer.py:48] [124300] global_step=124300, grad_norm=0.6991729736328125, loss=2.62058162689209
I0207 07:22:17.242835 140530045396736 logging_writer.py:48] [124400] global_step=124400, grad_norm=0.6918462514877319, loss=2.6284334659576416
I0207 07:22:52.463686 140530037004032 logging_writer.py:48] [124500] global_step=124500, grad_norm=0.6737911701202393, loss=2.6496689319610596
I0207 07:23:27.691593 140530045396736 logging_writer.py:48] [124600] global_step=124600, grad_norm=0.670366108417511, loss=2.6172680854797363
I0207 07:24:02.905842 140530037004032 logging_writer.py:48] [124700] global_step=124700, grad_norm=0.6692795157432556, loss=2.628413677215576
I0207 07:24:38.130983 140530045396736 logging_writer.py:48] [124800] global_step=124800, grad_norm=0.6710323095321655, loss=2.530878782272339
I0207 07:25:13.349793 140530037004032 logging_writer.py:48] [124900] global_step=124900, grad_norm=0.6912602782249451, loss=2.585730791091919
I0207 07:25:48.580608 140530045396736 logging_writer.py:48] [125000] global_step=125000, grad_norm=0.6704617738723755, loss=2.5107128620147705
I0207 07:26:23.827084 140530037004032 logging_writer.py:48] [125100] global_step=125100, grad_norm=0.679897129535675, loss=2.642286777496338
I0207 07:26:59.100762 140530045396736 logging_writer.py:48] [125200] global_step=125200, grad_norm=0.6666680574417114, loss=2.6085751056671143
I0207 07:27:34.301796 140530037004032 logging_writer.py:48] [125300] global_step=125300, grad_norm=0.684055745601654, loss=2.602762460708618
I0207 07:28:09.513753 140530045396736 logging_writer.py:48] [125400] global_step=125400, grad_norm=0.6669812202453613, loss=2.619619607925415
I0207 07:28:44.725364 140530037004032 logging_writer.py:48] [125500] global_step=125500, grad_norm=0.6530331373214722, loss=2.6031715869903564
I0207 07:29:19.968384 140530045396736 logging_writer.py:48] [125600] global_step=125600, grad_norm=0.6727694869041443, loss=2.5949413776397705
I0207 07:29:55.236550 140530037004032 logging_writer.py:48] [125700] global_step=125700, grad_norm=0.66096031665802, loss=2.5775043964385986
I0207 07:30:30.544372 140530045396736 logging_writer.py:48] [125800] global_step=125800, grad_norm=0.7109084129333496, loss=2.6257405281066895
I0207 07:31:05.798407 140530037004032 logging_writer.py:48] [125900] global_step=125900, grad_norm=0.6379230618476868, loss=2.5389859676361084
I0207 07:31:41.032808 140530045396736 logging_writer.py:48] [126000] global_step=126000, grad_norm=0.7058674097061157, loss=2.60974383354187
I0207 07:32:16.261510 140530037004032 logging_writer.py:48] [126100] global_step=126100, grad_norm=0.6892963647842407, loss=2.5991992950439453
I0207 07:32:51.509300 140530045396736 logging_writer.py:48] [126200] global_step=126200, grad_norm=0.6879692673683167, loss=2.6258177757263184
I0207 07:33:26.761612 140530037004032 logging_writer.py:48] [126300] global_step=126300, grad_norm=0.6879780888557434, loss=2.6125824451446533
I0207 07:34:01.990464 140530045396736 logging_writer.py:48] [126400] global_step=126400, grad_norm=0.6706161499023438, loss=2.6395559310913086
I0207 07:34:19.337697 140699726837568 spec.py:321] Evaluating on the training split.
I0207 07:34:22.362300 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 07:37:17.323193 140699726837568 spec.py:333] Evaluating on the validation split.
I0207 07:37:20.052878 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 07:39:46.197976 140699726837568 spec.py:349] Evaluating on the test split.
I0207 07:39:48.932121 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 07:42:10.354635 140699726837568 submission_runner.py:408] Time since start: 72149.55s, 	Step: 126451, 	{'train/accuracy': 0.7076228857040405, 'train/loss': 1.4430257081985474, 'train/bleu': 36.56570523268232, 'validation/accuracy': 0.6953416466712952, 'validation/loss': 1.5003718137741089, 'validation/bleu': 30.9751714367643, 'validation/num_examples': 3000, 'test/accuracy': 0.7123816609382629, 'test/loss': 1.4068301916122437, 'test/bleu': 31.215212632401542, 'test/num_examples': 3003, 'score': 44561.50462055206, 'total_duration': 72149.5507850647, 'accumulated_submission_time': 44561.50462055206, 'accumulated_eval_time': 27582.21673822403, 'accumulated_logging_time': 1.8660566806793213}
I0207 07:42:10.386950 140530037004032 logging_writer.py:48] [126451] accumulated_eval_time=27582.216738, accumulated_logging_time=1.866057, accumulated_submission_time=44561.504621, global_step=126451, preemption_count=0, score=44561.504621, test/accuracy=0.712382, test/bleu=31.215213, test/loss=1.406830, test/num_examples=3003, total_duration=72149.550785, train/accuracy=0.707623, train/bleu=36.565705, train/loss=1.443026, validation/accuracy=0.695342, validation/bleu=30.975171, validation/loss=1.500372, validation/num_examples=3000
I0207 07:42:27.960253 140530045396736 logging_writer.py:48] [126500] global_step=126500, grad_norm=0.6996570825576782, loss=2.547074794769287
I0207 07:43:03.079841 140530037004032 logging_writer.py:48] [126600] global_step=126600, grad_norm=0.6831347942352295, loss=2.6295812129974365
I0207 07:43:38.265610 140530045396736 logging_writer.py:48] [126700] global_step=126700, grad_norm=0.6848371624946594, loss=2.5945003032684326
I0207 07:44:13.482768 140530037004032 logging_writer.py:48] [126800] global_step=126800, grad_norm=0.6657604575157166, loss=2.5709590911865234
I0207 07:44:48.681276 140530045396736 logging_writer.py:48] [126900] global_step=126900, grad_norm=0.7050039768218994, loss=2.6520440578460693
I0207 07:45:23.890813 140530037004032 logging_writer.py:48] [127000] global_step=127000, grad_norm=0.66351717710495, loss=2.540492534637451
I0207 07:45:59.119679 140530045396736 logging_writer.py:48] [127100] global_step=127100, grad_norm=0.6867626309394836, loss=2.569178581237793
I0207 07:46:34.360078 140530037004032 logging_writer.py:48] [127200] global_step=127200, grad_norm=0.6940981149673462, loss=2.611220359802246
I0207 07:47:09.598659 140530045396736 logging_writer.py:48] [127300] global_step=127300, grad_norm=0.667186975479126, loss=2.6151747703552246
I0207 07:47:44.853368 140530037004032 logging_writer.py:48] [127400] global_step=127400, grad_norm=0.708421528339386, loss=2.7298364639282227
I0207 07:48:20.103789 140530045396736 logging_writer.py:48] [127500] global_step=127500, grad_norm=0.6858280301094055, loss=2.5656657218933105
I0207 07:48:55.303470 140530037004032 logging_writer.py:48] [127600] global_step=127600, grad_norm=0.7043501138687134, loss=2.638320207595825
I0207 07:49:30.546245 140530045396736 logging_writer.py:48] [127700] global_step=127700, grad_norm=0.6780804395675659, loss=2.600477933883667
I0207 07:50:05.755465 140530037004032 logging_writer.py:48] [127800] global_step=127800, grad_norm=0.6506776809692383, loss=2.6131629943847656
I0207 07:50:40.983884 140530045396736 logging_writer.py:48] [127900] global_step=127900, grad_norm=0.6879684925079346, loss=2.55708909034729
I0207 07:51:16.185057 140530037004032 logging_writer.py:48] [128000] global_step=128000, grad_norm=0.6809099912643433, loss=2.6090219020843506
I0207 07:51:51.553254 140530045396736 logging_writer.py:48] [128100] global_step=128100, grad_norm=0.6876742243766785, loss=2.5601754188537598
I0207 07:52:26.771092 140530037004032 logging_writer.py:48] [128200] global_step=128200, grad_norm=0.6781259775161743, loss=2.533149480819702
I0207 07:53:01.986280 140530045396736 logging_writer.py:48] [128300] global_step=128300, grad_norm=0.6881393194198608, loss=2.538541793823242
I0207 07:53:37.216753 140530037004032 logging_writer.py:48] [128400] global_step=128400, grad_norm=0.6662889122962952, loss=2.578291654586792
I0207 07:54:12.451038 140530045396736 logging_writer.py:48] [128500] global_step=128500, grad_norm=0.7061917781829834, loss=2.5267345905303955
I0207 07:54:47.672350 140530037004032 logging_writer.py:48] [128600] global_step=128600, grad_norm=0.6864907741546631, loss=2.5956437587738037
I0207 07:55:22.888275 140530045396736 logging_writer.py:48] [128700] global_step=128700, grad_norm=0.6821494102478027, loss=2.6137807369232178
I0207 07:55:58.091154 140530037004032 logging_writer.py:48] [128800] global_step=128800, grad_norm=0.6612831950187683, loss=2.5412724018096924
I0207 07:56:10.479256 140699726837568 spec.py:321] Evaluating on the training split.
I0207 07:56:13.505383 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 07:59:31.273898 140699726837568 spec.py:333] Evaluating on the validation split.
I0207 07:59:34.011555 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 08:01:59.224512 140699726837568 spec.py:349] Evaluating on the test split.
I0207 08:02:01.965280 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 08:04:21.779711 140699726837568 submission_runner.py:408] Time since start: 73480.98s, 	Step: 128837, 	{'train/accuracy': 0.7105525732040405, 'train/loss': 1.4332529306411743, 'train/bleu': 36.75074571548151, 'validation/accuracy': 0.6956764459609985, 'validation/loss': 1.4999107122421265, 'validation/bleu': 30.823767338628773, 'validation/num_examples': 3000, 'test/accuracy': 0.7122421860694885, 'test/loss': 1.4065791368484497, 'test/bleu': 30.932261071712002, 'test/num_examples': 3003, 'score': 45401.51370239258, 'total_duration': 73480.97584033012, 'accumulated_submission_time': 45401.51370239258, 'accumulated_eval_time': 28073.51711320877, 'accumulated_logging_time': 1.9084973335266113}
I0207 08:04:21.816849 140530045396736 logging_writer.py:48] [128837] accumulated_eval_time=28073.517113, accumulated_logging_time=1.908497, accumulated_submission_time=45401.513702, global_step=128837, preemption_count=0, score=45401.513702, test/accuracy=0.712242, test/bleu=30.932261, test/loss=1.406579, test/num_examples=3003, total_duration=73480.975840, train/accuracy=0.710553, train/bleu=36.750746, train/loss=1.433253, validation/accuracy=0.695676, validation/bleu=30.823767, validation/loss=1.499911, validation/num_examples=3000
I0207 08:04:44.308946 140530037004032 logging_writer.py:48] [128900] global_step=128900, grad_norm=0.6867172122001648, loss=2.6147375106811523
I0207 08:05:19.454089 140530045396736 logging_writer.py:48] [129000] global_step=129000, grad_norm=0.6763448715209961, loss=2.6353437900543213
I0207 08:05:54.710259 140530037004032 logging_writer.py:48] [129100] global_step=129100, grad_norm=0.666469931602478, loss=2.572113037109375
I0207 08:06:29.912916 140530045396736 logging_writer.py:48] [129200] global_step=129200, grad_norm=0.677191972732544, loss=2.5919029712677
I0207 08:07:05.130628 140530037004032 logging_writer.py:48] [129300] global_step=129300, grad_norm=0.6934003829956055, loss=2.615313768386841
I0207 08:07:40.383674 140530045396736 logging_writer.py:48] [129400] global_step=129400, grad_norm=0.7184320092201233, loss=2.5884554386138916
I0207 08:08:15.627410 140530037004032 logging_writer.py:48] [129500] global_step=129500, grad_norm=0.6764519810676575, loss=2.545267343521118
I0207 08:08:50.869821 140530045396736 logging_writer.py:48] [129600] global_step=129600, grad_norm=0.6806018352508545, loss=2.570589542388916
I0207 08:09:26.086662 140530037004032 logging_writer.py:48] [129700] global_step=129700, grad_norm=0.6622287631034851, loss=2.560328483581543
I0207 08:10:01.317962 140530045396736 logging_writer.py:48] [129800] global_step=129800, grad_norm=0.6764295697212219, loss=2.548586130142212
I0207 08:10:36.537823 140530037004032 logging_writer.py:48] [129900] global_step=129900, grad_norm=0.7081660628318787, loss=2.6185827255249023
I0207 08:11:11.771486 140530045396736 logging_writer.py:48] [130000] global_step=130000, grad_norm=0.6639648675918579, loss=2.5286455154418945
I0207 08:11:47.005551 140530037004032 logging_writer.py:48] [130100] global_step=130100, grad_norm=0.6758043169975281, loss=2.5753872394561768
I0207 08:12:22.248178 140530045396736 logging_writer.py:48] [130200] global_step=130200, grad_norm=0.6996040344238281, loss=2.611182451248169
I0207 08:12:57.499010 140530037004032 logging_writer.py:48] [130300] global_step=130300, grad_norm=0.6880938410758972, loss=2.5872018337249756
I0207 08:13:32.717655 140530045396736 logging_writer.py:48] [130400] global_step=130400, grad_norm=0.6684684753417969, loss=2.6156609058380127
I0207 08:14:07.921028 140530037004032 logging_writer.py:48] [130500] global_step=130500, grad_norm=0.670982301235199, loss=2.5592923164367676
I0207 08:14:43.145397 140530045396736 logging_writer.py:48] [130600] global_step=130600, grad_norm=0.6805674433708191, loss=2.594315767288208
I0207 08:15:18.376568 140530037004032 logging_writer.py:48] [130700] global_step=130700, grad_norm=0.6787098050117493, loss=2.5560519695281982
I0207 08:15:53.641431 140530045396736 logging_writer.py:48] [130800] global_step=130800, grad_norm=0.6731488704681396, loss=2.604731798171997
I0207 08:16:28.896610 140530037004032 logging_writer.py:48] [130900] global_step=130900, grad_norm=0.6936777830123901, loss=2.6159510612487793
I0207 08:17:04.118140 140530045396736 logging_writer.py:48] [131000] global_step=131000, grad_norm=0.6703475713729858, loss=2.598212957382202
I0207 08:17:39.368303 140530037004032 logging_writer.py:48] [131100] global_step=131100, grad_norm=0.6435897350311279, loss=2.6222023963928223
I0207 08:18:14.595600 140530045396736 logging_writer.py:48] [131200] global_step=131200, grad_norm=0.6866218447685242, loss=2.604555130004883
I0207 08:18:22.081843 140699726837568 spec.py:321] Evaluating on the training split.
I0207 08:18:25.120435 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 08:21:40.836935 140699726837568 spec.py:333] Evaluating on the validation split.
I0207 08:21:43.568807 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 08:24:08.178867 140699726837568 spec.py:349] Evaluating on the test split.
I0207 08:24:10.909663 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 08:26:33.925549 140699726837568 submission_runner.py:408] Time since start: 74813.12s, 	Step: 131223, 	{'train/accuracy': 0.7070929408073425, 'train/loss': 1.4555907249450684, 'train/bleu': 36.070049531934, 'validation/accuracy': 0.695874810218811, 'validation/loss': 1.499701976776123, 'validation/bleu': 30.95694516258592, 'validation/num_examples': 3000, 'test/accuracy': 0.7126489281654358, 'test/loss': 1.4054698944091797, 'test/bleu': 31.097917544741282, 'test/num_examples': 3003, 'score': 46241.69057679176, 'total_duration': 74813.12171435356, 'accumulated_submission_time': 46241.69057679176, 'accumulated_eval_time': 28565.360773801804, 'accumulated_logging_time': 1.958054780960083}
I0207 08:26:33.958671 140530037004032 logging_writer.py:48] [131223] accumulated_eval_time=28565.360774, accumulated_logging_time=1.958055, accumulated_submission_time=46241.690577, global_step=131223, preemption_count=0, score=46241.690577, test/accuracy=0.712649, test/bleu=31.097918, test/loss=1.405470, test/num_examples=3003, total_duration=74813.121714, train/accuracy=0.707093, train/bleu=36.070050, train/loss=1.455591, validation/accuracy=0.695875, validation/bleu=30.956945, validation/loss=1.499702, validation/num_examples=3000
I0207 08:27:01.328944 140530045396736 logging_writer.py:48] [131300] global_step=131300, grad_norm=0.6732051372528076, loss=2.594163179397583
I0207 08:27:36.472203 140530037004032 logging_writer.py:48] [131400] global_step=131400, grad_norm=0.6644043922424316, loss=2.5777711868286133
I0207 08:28:11.674491 140530045396736 logging_writer.py:48] [131500] global_step=131500, grad_norm=0.6794072389602661, loss=2.534071922302246
I0207 08:28:46.861625 140530037004032 logging_writer.py:48] [131600] global_step=131600, grad_norm=0.6843348145484924, loss=2.6405293941497803
I0207 08:29:22.084407 140530045396736 logging_writer.py:48] [131700] global_step=131700, grad_norm=0.6828975081443787, loss=2.6339616775512695
I0207 08:29:57.367487 140530037004032 logging_writer.py:48] [131800] global_step=131800, grad_norm=0.6653447151184082, loss=2.591132164001465
I0207 08:30:32.634366 140530045396736 logging_writer.py:48] [131900] global_step=131900, grad_norm=0.6538019180297852, loss=2.507713794708252
I0207 08:31:07.892982 140530037004032 logging_writer.py:48] [132000] global_step=132000, grad_norm=0.6853585243225098, loss=2.6527585983276367
I0207 08:31:43.156108 140530045396736 logging_writer.py:48] [132100] global_step=132100, grad_norm=0.6856680512428284, loss=2.566354751586914
I0207 08:32:18.390354 140530037004032 logging_writer.py:48] [132200] global_step=132200, grad_norm=0.6715724468231201, loss=2.586655378341675
I0207 08:32:53.641195 140530045396736 logging_writer.py:48] [132300] global_step=132300, grad_norm=0.6868577599525452, loss=2.6220626831054688
I0207 08:33:28.887931 140530037004032 logging_writer.py:48] [132400] global_step=132400, grad_norm=0.6573507189750671, loss=2.5458617210388184
I0207 08:34:04.170459 140530045396736 logging_writer.py:48] [132500] global_step=132500, grad_norm=0.68232262134552, loss=2.58730149269104
I0207 08:34:39.420233 140530037004032 logging_writer.py:48] [132600] global_step=132600, grad_norm=0.6839989423751831, loss=2.615238666534424
I0207 08:35:14.698845 140530045396736 logging_writer.py:48] [132700] global_step=132700, grad_norm=0.6769290566444397, loss=2.578460216522217
I0207 08:35:49.943636 140530037004032 logging_writer.py:48] [132800] global_step=132800, grad_norm=0.688271701335907, loss=2.6270368099212646
I0207 08:36:25.180094 140530045396736 logging_writer.py:48] [132900] global_step=132900, grad_norm=0.6788177490234375, loss=2.5906221866607666
I0207 08:37:00.412060 140530037004032 logging_writer.py:48] [133000] global_step=133000, grad_norm=0.6667265892028809, loss=2.5364110469818115
I0207 08:37:35.658451 140530045396736 logging_writer.py:48] [133100] global_step=133100, grad_norm=0.6934143900871277, loss=2.6330623626708984
I0207 08:38:10.887518 140530037004032 logging_writer.py:48] [133200] global_step=133200, grad_norm=0.661836564540863, loss=2.598670244216919
I0207 08:38:46.150901 140530045396736 logging_writer.py:48] [133300] global_step=133300, grad_norm=0.6428279280662537, loss=2.533169984817505
I0207 08:38:57.130227 140699726837568 spec.py:321] Evaluating on the training split.
I0207 08:39:00.157998 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 08:42:12.432333 140699726837568 spec.py:333] Evaluating on the validation split.
I0207 08:42:15.155806 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 08:44:43.483909 140699726837568 spec.py:349] Evaluating on the test split.
I0207 08:44:46.223844 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 08:47:08.168802 140699726837568 submission_runner.py:408] Time since start: 76047.36s, 	Step: 133333, 	{'train/accuracy': 0.7128967642784119, 'train/loss': 1.4201592206954956, 'train/bleu': 36.50041035697179, 'validation/accuracy': 0.6956764459609985, 'validation/loss': 1.4999949932098389, 'validation/bleu': 30.901218707876914, 'validation/num_examples': 3000, 'test/accuracy': 0.7125210762023926, 'test/loss': 1.4059910774230957, 'test/bleu': 31.041513578042657, 'test/num_examples': 3003, 'score': 46984.7851524353, 'total_duration': 76047.3649597168, 'accumulated_submission_time': 46984.7851524353, 'accumulated_eval_time': 29056.399307250977, 'accumulated_logging_time': 2.001035213470459}
I0207 08:47:08.203228 140530037004032 logging_writer.py:48] [133333] accumulated_eval_time=29056.399307, accumulated_logging_time=2.001035, accumulated_submission_time=46984.785152, global_step=133333, preemption_count=0, score=46984.785152, test/accuracy=0.712521, test/bleu=31.041514, test/loss=1.405991, test/num_examples=3003, total_duration=76047.364960, train/accuracy=0.712897, train/bleu=36.500410, train/loss=1.420159, validation/accuracy=0.695676, validation/bleu=30.901219, validation/loss=1.499995, validation/num_examples=3000
I0207 08:47:08.236621 140530045396736 logging_writer.py:48] [133333] global_step=133333, preemption_count=0, score=46984.785152
I0207 08:47:09.460171 140699726837568 checkpoints.py:490] Saving checkpoint at step: 133333
I0207 08:47:13.521232 140699726837568 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_1/wmt_jax/trial_1/checkpoint_133333
I0207 08:47:13.525992 140699726837568 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_1/wmt_jax/trial_1/checkpoint_133333.
I0207 08:47:13.570802 140699726837568 submission_runner.py:583] Tuning trial 1/5
I0207 08:47:13.570967 140699726837568 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.1, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0207 08:47:13.575493 140699726837568 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0006529284291900694, 'train/loss': 11.176665306091309, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.208685874938965, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.19086742401123, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 37.038549184799194, 'total_duration': 932.0136518478394, 'accumulated_submission_time': 37.038549184799194, 'accumulated_eval_time': 894.9750609397888, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (2382, {'train/accuracy': 0.4112793505191803, 'train/loss': 4.001132965087891, 'train/bleu': 14.314785399828414, 'validation/accuracy': 0.39581653475761414, 'validation/loss': 4.1393537521362305, 'validation/bleu': 9.577254281286269, 'validation/num_examples': 3000, 'test/accuracy': 0.378374308347702, 'test/loss': 4.347209453582764, 'test/bleu': 7.77435242429996, 'test/num_examples': 3003, 'score': 877.1155626773834, 'total_duration': 2540.3347775936127, 'accumulated_submission_time': 877.1155626773834, 'accumulated_eval_time': 1663.1117770671844, 'accumulated_logging_time': 0.03058791160583496, 'global_step': 2382, 'preemption_count': 0}), (4762, {'train/accuracy': 0.5386283993721008, 'train/loss': 2.7876904010772705, 'train/bleu': 24.47574218709137, 'validation/accuracy': 0.5418035387992859, 'validation/loss': 2.7314565181732178, 'validation/bleu': 20.31710448845741, 'validation/num_examples': 3000, 'test/accuracy': 0.5421649217605591, 'test/loss': 2.766301155090332, 'test/bleu': 18.93392274615353, 'test/num_examples': 3003, 'score': 1717.0735466480255, 'total_duration': 3867.541526556015, 'accumulated_submission_time': 1717.0735466480255, 'accumulated_eval_time': 2150.2625029087067, 'accumulated_logging_time': 0.05559253692626953, 'global_step': 4762, 'preemption_count': 0}), (7143, {'train/accuracy': 0.5830803513526917, 'train/loss': 2.3459420204162598, 'train/bleu': 27.624996169924223, 'validation/accuracy': 0.5854235887527466, 'validation/loss': 2.320286512374878, 'validation/bleu': 23.295886495274086, 'validation/num_examples': 3000, 'test/accuracy': 0.588565468788147, 'test/loss': 2.322375535964966, 'test/bleu': 21.92302229850635, 'test/num_examples': 3003, 'score': 2557.290193080902, 'total_duration': 5180.989482164383, 'accumulated_submission_time': 2557.290193080902, 'accumulated_eval_time': 2623.3933651447296, 'accumulated_logging_time': 0.08187294006347656, 'global_step': 7143, 'preemption_count': 0}), (9526, {'train/accuracy': 0.5969531536102295, 'train/loss': 2.222651481628418, 'train/bleu': 28.03945898046316, 'validation/accuracy': 0.6077544093132019, 'validation/loss': 2.134187936782837, 'validation/bleu': 24.906629328826106, 'validation/num_examples': 3000, 'test/accuracy': 0.6118645071983337, 'test/loss': 2.1073811054229736, 'test/bleu': 23.48735964868124, 'test/num_examples': 3003, 'score': 3397.4100873470306, 'total_duration': 6491.173132181168, 'accumulated_submission_time': 3397.4100873470306, 'accumulated_eval_time': 3093.3391761779785, 'accumulated_logging_time': 0.12326359748840332, 'global_step': 9526, 'preemption_count': 0}), (11909, {'train/accuracy': 0.6016323566436768, 'train/loss': 2.1511049270629883, 'train/bleu': 28.773179468343937, 'validation/accuracy': 0.621133029460907, 'validation/loss': 2.0083134174346924, 'validation/bleu': 25.848073621681074, 'validation/num_examples': 3000, 'test/accuracy': 0.6260995864868164, 'test/loss': 1.976151943206787, 'test/bleu': 24.403812250497207, 'test/num_examples': 3003, 'score': 4237.438687324524, 'total_duration': 7776.712178230286, 'accumulated_submission_time': 4237.438687324524, 'accumulated_eval_time': 3538.7417256832123, 'accumulated_logging_time': 0.15330171585083008, 'global_step': 11909, 'preemption_count': 0}), (14294, {'train/accuracy': 0.6140919923782349, 'train/loss': 2.048624277114868, 'train/bleu': 29.75643167533389, 'validation/accuracy': 0.6318830251693726, 'validation/loss': 1.9232735633850098, 'validation/bleu': 26.592895269365595, 'validation/num_examples': 3000, 'test/accuracy': 0.6379757523536682, 'test/loss': 1.8753560781478882, 'test/bleu': 25.565374635606645, 'test/num_examples': 3003, 'score': 5077.634784460068, 'total_duration': 9097.55193066597, 'accumulated_submission_time': 5077.634784460068, 'accumulated_eval_time': 4019.282205581665, 'accumulated_logging_time': 0.18139338493347168, 'global_step': 14294, 'preemption_count': 0}), (16679, {'train/accuracy': 0.6197834014892578, 'train/loss': 2.003021001815796, 'train/bleu': 29.712456080288373, 'validation/accuracy': 0.6387149691581726, 'validation/loss': 1.8588805198669434, 'validation/bleu': 26.800316120447345, 'validation/num_examples': 3000, 'test/accuracy': 0.6479925513267517, 'test/loss': 1.8052959442138672, 'test/bleu': 25.933641625620712, 'test/num_examples': 3003, 'score': 5917.868166685104, 'total_duration': 10397.26683807373, 'accumulated_submission_time': 5917.868166685104, 'accumulated_eval_time': 4478.662546873093, 'accumulated_logging_time': 0.20989251136779785, 'global_step': 16679, 'preemption_count': 0}), (19064, {'train/accuracy': 0.6391572952270508, 'train/loss': 1.8542532920837402, 'train/bleu': 31.17832988950659, 'validation/accuracy': 0.6441581845283508, 'validation/loss': 1.8158868551254272, 'validation/bleu': 27.1275590231522, 'validation/num_examples': 3000, 'test/accuracy': 0.652710497379303, 'test/loss': 1.761230707168579, 'test/bleu': 26.45541676211269, 'test/num_examples': 3003, 'score': 6757.979300022125, 'total_duration': 12022.555188179016, 'accumulated_submission_time': 6757.979300022125, 'accumulated_eval_time': 5263.736715555191, 'accumulated_logging_time': 0.23669672012329102, 'global_step': 19064, 'preemption_count': 0}), (21450, {'train/accuracy': 0.6265982985496521, 'train/loss': 1.9437370300292969, 'train/bleu': 30.379573470875417, 'validation/accuracy': 0.6476795077323914, 'validation/loss': 1.7847602367401123, 'validation/bleu': 27.428115773451797, 'validation/num_examples': 3000, 'test/accuracy': 0.6565220355987549, 'test/loss': 1.7380309104919434, 'test/bleu': 26.60739545980313, 'test/num_examples': 3003, 'score': 7598.178004980087, 'total_duration': 13377.983260631561, 'accumulated_submission_time': 7598.178004980087, 'accumulated_eval_time': 5778.866499423981, 'accumulated_logging_time': 0.2639615535736084, 'global_step': 21450, 'preemption_count': 0}), (23836, {'train/accuracy': 0.6263420581817627, 'train/loss': 1.9479836225509644, 'train/bleu': 30.605509374050456, 'validation/accuracy': 0.6493533849716187, 'validation/loss': 1.7742805480957031, 'validation/bleu': 27.501190326452022, 'validation/num_examples': 3000, 'test/accuracy': 0.6588460803031921, 'test/loss': 1.7223812341690063, 'test/bleu': 26.958053620503918, 'test/num_examples': 3003, 'score': 8438.228722810745, 'total_duration': 14722.985248565674, 'accumulated_submission_time': 8438.228722810745, 'accumulated_eval_time': 6283.715788841248, 'accumulated_logging_time': 0.291827917098999, 'global_step': 23836, 'preemption_count': 0}), (26221, {'train/accuracy': 0.6345329880714417, 'train/loss': 1.8784587383270264, 'train/bleu': 31.028171095176663, 'validation/accuracy': 0.65301114320755, 'validation/loss': 1.7509132623672485, 'validation/bleu': 27.968816101277366, 'validation/num_examples': 3000, 'test/accuracy': 0.665864884853363, 'test/loss': 1.6981230974197388, 'test/bleu': 27.718216191957513, 'test/num_examples': 3003, 'score': 9278.243922948837, 'total_duration': 16032.076689481735, 'accumulated_submission_time': 9278.243922948837, 'accumulated_eval_time': 6752.686856031418, 'accumulated_logging_time': 0.32169675827026367, 'global_step': 26221, 'preemption_count': 0}), (28607, {'train/accuracy': 0.6294631361961365, 'train/loss': 1.9184149503707886, 'train/bleu': 30.843768750457446, 'validation/accuracy': 0.6532467007637024, 'validation/loss': 1.749211311340332, 'validation/bleu': 27.845418699552212, 'validation/num_examples': 3000, 'test/accuracy': 0.6636918187141418, 'test/loss': 1.6925480365753174, 'test/bleu': 26.978583127780233, 'test/num_examples': 3003, 'score': 10118.190644741058, 'total_duration': 17333.501986265182, 'accumulated_submission_time': 10118.190644741058, 'accumulated_eval_time': 7214.063798904419, 'accumulated_logging_time': 0.3504965305328369, 'global_step': 28607, 'preemption_count': 0}), (30994, {'train/accuracy': 0.6348527669906616, 'train/loss': 1.892822265625, 'train/bleu': 30.792602667408772, 'validation/accuracy': 0.6570532321929932, 'validation/loss': 1.7295809984207153, 'validation/bleu': 28.399133611509708, 'validation/num_examples': 3000, 'test/accuracy': 0.6665040254592896, 'test/loss': 1.6737943887710571, 'test/bleu': 27.640295046729694, 'test/num_examples': 3003, 'score': 10958.382172107697, 'total_duration': 18718.81236767769, 'accumulated_submission_time': 10958.382172107697, 'accumulated_eval_time': 7759.0807383060455, 'accumulated_logging_time': 0.3789188861846924, 'global_step': 30994, 'preemption_count': 0}), (33381, {'train/accuracy': 0.6381608843803406, 'train/loss': 1.8649847507476807, 'train/bleu': 30.97019323236983, 'validation/accuracy': 0.6565820574760437, 'validation/loss': 1.7204769849777222, 'validation/bleu': 28.203732298008507, 'validation/num_examples': 3000, 'test/accuracy': 0.667026937007904, 'test/loss': 1.6664468050003052, 'test/bleu': 27.455205127358838, 'test/num_examples': 3003, 'score': 11798.445326805115, 'total_duration': 20210.39287185669, 'accumulated_submission_time': 11798.445326805115, 'accumulated_eval_time': 8410.495576143265, 'accumulated_logging_time': 0.40795230865478516, 'global_step': 33381, 'preemption_count': 0}), (35767, {'train/accuracy': 0.6377241015434265, 'train/loss': 1.875410556793213, 'train/bleu': 31.085512843035314, 'validation/accuracy': 0.6565572619438171, 'validation/loss': 1.71941077709198, 'validation/bleu': 28.019113346418383, 'validation/num_examples': 3000, 'test/accuracy': 0.6706873774528503, 'test/loss': 1.6476588249206543, 'test/bleu': 27.893507040265977, 'test/num_examples': 3003, 'score': 12638.43499302864, 'total_duration': 21543.868954896927, 'accumulated_submission_time': 12638.43499302864, 'accumulated_eval_time': 8903.872762203217, 'accumulated_logging_time': 0.4422931671142578, 'global_step': 35767, 'preemption_count': 0}), (38152, {'train/accuracy': 0.6488719582557678, 'train/loss': 1.7934083938598633, 'train/bleu': 31.613892582308633, 'validation/accuracy': 0.6610581278800964, 'validation/loss': 1.70951247215271, 'validation/bleu': 28.280349091822146, 'validation/num_examples': 3000, 'test/accuracy': 0.6716054081916809, 'test/loss': 1.6476448774337769, 'test/bleu': 27.802578131818603, 'test/num_examples': 3003, 'score': 13478.334066152573, 'total_duration': 22861.5990588665, 'accumulated_submission_time': 13478.334066152573, 'accumulated_eval_time': 9381.595716238022, 'accumulated_logging_time': 0.47262072563171387, 'global_step': 38152, 'preemption_count': 0}), (40538, {'train/accuracy': 0.6424659490585327, 'train/loss': 1.8437772989273071, 'train/bleu': 31.08091692572428, 'validation/accuracy': 0.6593222618103027, 'validation/loss': 1.7040035724639893, 'validation/bleu': 28.3158290660357, 'validation/num_examples': 3000, 'test/accuracy': 0.6716402173042297, 'test/loss': 1.6384211778640747, 'test/bleu': 27.640285594202233, 'test/num_examples': 3003, 'score': 14318.342139482498, 'total_duration': 24222.251285791397, 'accumulated_submission_time': 14318.342139482498, 'accumulated_eval_time': 9902.135681152344, 'accumulated_logging_time': 0.5022103786468506, 'global_step': 40538, 'preemption_count': 0}), (42925, {'train/accuracy': 0.6389986276626587, 'train/loss': 1.856657862663269, 'train/bleu': 31.192170592865967, 'validation/accuracy': 0.6593842506408691, 'validation/loss': 1.7011802196502686, 'validation/bleu': 28.261132238555522, 'validation/num_examples': 3000, 'test/accuracy': 0.6723374724388123, 'test/loss': 1.6351051330566406, 'test/bleu': 28.000836489908377, 'test/num_examples': 3003, 'score': 15158.377641677856, 'total_duration': 25544.065200567245, 'accumulated_submission_time': 15158.377641677856, 'accumulated_eval_time': 10383.810242652893, 'accumulated_logging_time': 0.5315215587615967, 'global_step': 42925, 'preemption_count': 0}), (45312, {'train/accuracy': 0.6493979692459106, 'train/loss': 1.7883561849594116, 'train/bleu': 31.863872415061568, 'validation/accuracy': 0.6628807783126831, 'validation/loss': 1.6845715045928955, 'validation/bleu': 28.467148185414068, 'validation/num_examples': 3000, 'test/accuracy': 0.6726396083831787, 'test/loss': 1.62099027633667, 'test/bleu': 27.586332945086863, 'test/num_examples': 3003, 'score': 15998.422271966934, 'total_duration': 26853.321212291718, 'accumulated_submission_time': 15998.422271966934, 'accumulated_eval_time': 10852.916440725327, 'accumulated_logging_time': 0.5626571178436279, 'global_step': 45312, 'preemption_count': 0}), (47700, {'train/accuracy': 0.641980767250061, 'train/loss': 1.8360681533813477, 'train/bleu': 31.306511234966308, 'validation/accuracy': 0.6625956296920776, 'validation/loss': 1.6862725019454956, 'validation/bleu': 28.622753251573556, 'validation/num_examples': 3000, 'test/accuracy': 0.6753239631652832, 'test/loss': 1.6231544017791748, 'test/bleu': 28.22375180587874, 'test/num_examples': 3003, 'score': 16838.640387535095, 'total_duration': 28156.87495303154, 'accumulated_submission_time': 16838.640387535095, 'accumulated_eval_time': 11316.14895439148, 'accumulated_logging_time': 0.5931167602539062, 'global_step': 47700, 'preemption_count': 0}), (50087, {'train/accuracy': 0.691525399684906, 'train/loss': 1.5449894666671753, 'train/bleu': 35.324233093710475, 'validation/accuracy': 0.6646042466163635, 'validation/loss': 1.6788150072097778, 'validation/bleu': 28.461571849251246, 'validation/num_examples': 3000, 'test/accuracy': 0.6756957769393921, 'test/loss': 1.6147881746292114, 'test/bleu': 28.095035425482386, 'test/num_examples': 3003, 'score': 17678.58997631073, 'total_duration': 29477.193959236145, 'accumulated_submission_time': 17678.58997631073, 'accumulated_eval_time': 11796.412520170212, 'accumulated_logging_time': 0.6238071918487549, 'global_step': 50087, 'preemption_count': 0}), (52474, {'train/accuracy': 0.648737370967865, 'train/loss': 1.7925587892532349, 'train/bleu': 31.28400718143166, 'validation/accuracy': 0.6651870608329773, 'validation/loss': 1.667005181312561, 'validation/bleu': 28.830817072279498, 'validation/num_examples': 3000, 'test/accuracy': 0.6764511466026306, 'test/loss': 1.6009951829910278, 'test/bleu': 28.30420204851027, 'test/num_examples': 3003, 'score': 18518.520733118057, 'total_duration': 30793.765823841095, 'accumulated_submission_time': 18518.520733118057, 'accumulated_eval_time': 12272.941549777985, 'accumulated_logging_time': 0.6620500087738037, 'global_step': 52474, 'preemption_count': 0}), (54860, {'train/accuracy': 0.6473584175109863, 'train/loss': 1.8072962760925293, 'train/bleu': 31.921184654644467, 'validation/accuracy': 0.6671088933944702, 'validation/loss': 1.6631172895431519, 'validation/bleu': 29.0922268005727, 'validation/num_examples': 3000, 'test/accuracy': 0.6795421838760376, 'test/loss': 1.5950560569763184, 'test/bleu': 28.502942846220005, 'test/num_examples': 3003, 'score': 19358.474761724472, 'total_duration': 32095.981645822525, 'accumulated_submission_time': 19358.474761724472, 'accumulated_eval_time': 12735.091276407242, 'accumulated_logging_time': 0.6979632377624512, 'global_step': 54860, 'preemption_count': 0}), (57247, {'train/accuracy': 0.6565492749214172, 'train/loss': 1.7392457723617554, 'train/bleu': 32.6825347018119, 'validation/accuracy': 0.6668609380722046, 'validation/loss': 1.654740810394287, 'validation/bleu': 29.001221127806534, 'validation/num_examples': 3000, 'test/accuracy': 0.6824705004692078, 'test/loss': 1.5814528465270996, 'test/bleu': 28.750735001154936, 'test/num_examples': 3003, 'score': 20198.645827054977, 'total_duration': 33403.252178907394, 'accumulated_submission_time': 20198.645827054977, 'accumulated_eval_time': 13202.083883523941, 'accumulated_logging_time': 0.7302701473236084, 'global_step': 57247, 'preemption_count': 0}), (59634, {'train/accuracy': 0.6492637395858765, 'train/loss': 1.7962538003921509, 'train/bleu': 31.7395194006414, 'validation/accuracy': 0.6694399118423462, 'validation/loss': 1.6482961177825928, 'validation/bleu': 28.93402112722786, 'validation/num_examples': 3000, 'test/accuracy': 0.6804021000862122, 'test/loss': 1.5773099660873413, 'test/bleu': 28.77173907460039, 'test/num_examples': 3003, 'score': 21038.74125480652, 'total_duration': 34720.2066423893, 'accumulated_submission_time': 21038.74125480652, 'accumulated_eval_time': 13678.836746692657, 'accumulated_logging_time': 0.7625217437744141, 'global_step': 59634, 'preemption_count': 0}), (62021, {'train/accuracy': 0.6487945318222046, 'train/loss': 1.79489004611969, 'train/bleu': 32.09413766752779, 'validation/accuracy': 0.6691547632217407, 'validation/loss': 1.6473053693771362, 'validation/bleu': 28.967600319487286, 'validation/num_examples': 3000, 'test/accuracy': 0.6833536624908447, 'test/loss': 1.5698957443237305, 'test/bleu': 28.951198785785074, 'test/num_examples': 3003, 'score': 21878.683776140213, 'total_duration': 36047.12191772461, 'accumulated_submission_time': 21878.683776140213, 'accumulated_eval_time': 14165.701899528503, 'accumulated_logging_time': 0.7960004806518555, 'global_step': 62021, 'preemption_count': 0}), (64408, {'train/accuracy': 0.6550009250640869, 'train/loss': 1.7537881135940552, 'train/bleu': 32.452760515426576, 'validation/accuracy': 0.6708534359931946, 'validation/loss': 1.635170578956604, 'validation/bleu': 28.91921608404348, 'validation/num_examples': 3000, 'test/accuracy': 0.6835861206054688, 'test/loss': 1.5647251605987549, 'test/bleu': 29.056131315412504, 'test/num_examples': 3003, 'score': 22718.78689146042, 'total_duration': 37366.41644477844, 'accumulated_submission_time': 22718.78689146042, 'accumulated_eval_time': 14644.784934043884, 'accumulated_logging_time': 0.8291144371032715, 'global_step': 64408, 'preemption_count': 0}), (66795, {'train/accuracy': 0.6545759439468384, 'train/loss': 1.7578319311141968, 'train/bleu': 32.203787090257514, 'validation/accuracy': 0.6715105772018433, 'validation/loss': 1.635469675064087, 'validation/bleu': 29.412971115276957, 'validation/num_examples': 3000, 'test/accuracy': 0.6852013468742371, 'test/loss': 1.5587241649627686, 'test/bleu': 29.178349712756706, 'test/num_examples': 3003, 'score': 23558.88369011879, 'total_duration': 38686.76952624321, 'accumulated_submission_time': 23558.88369011879, 'accumulated_eval_time': 15124.934435129166, 'accumulated_logging_time': 0.8623538017272949, 'global_step': 66795, 'preemption_count': 0}), (69181, {'train/accuracy': 0.6702680587768555, 'train/loss': 1.6583515405654907, 'train/bleu': 33.066980321510584, 'validation/accuracy': 0.6727132797241211, 'validation/loss': 1.62405264377594, 'validation/bleu': 29.20222921314499, 'validation/num_examples': 3000, 'test/accuracy': 0.6837836503982544, 'test/loss': 1.5548111200332642, 'test/bleu': 28.954724806981417, 'test/num_examples': 3003, 'score': 24398.984651088715, 'total_duration': 40062.75036764145, 'accumulated_submission_time': 24398.984651088715, 'accumulated_eval_time': 15660.699274778366, 'accumulated_logging_time': 0.9025025367736816, 'global_step': 69181, 'preemption_count': 0}), (71568, {'train/accuracy': 0.655456006526947, 'train/loss': 1.7491358518600464, 'train/bleu': 32.35903194112188, 'validation/accuracy': 0.6749451160430908, 'validation/loss': 1.6112587451934814, 'validation/bleu': 29.356844389961918, 'validation/num_examples': 3000, 'test/accuracy': 0.6880018711090088, 'test/loss': 1.5388880968093872, 'test/bleu': 28.961093312146044, 'test/num_examples': 3003, 'score': 25239.088314056396, 'total_duration': 41433.078765153885, 'accumulated_submission_time': 25239.088314056396, 'accumulated_eval_time': 16190.815400600433, 'accumulated_logging_time': 0.9364674091339111, 'global_step': 71568, 'preemption_count': 0}), (73955, {'train/accuracy': 0.6537445783615112, 'train/loss': 1.7614141702651978, 'train/bleu': 32.23235903214701, 'validation/accuracy': 0.6735440492630005, 'validation/loss': 1.6155986785888672, 'validation/bleu': 28.445815162064275, 'validation/num_examples': 3000, 'test/accuracy': 0.6869211792945862, 'test/loss': 1.5449918508529663, 'test/bleu': 29.291123433349274, 'test/num_examples': 3003, 'score': 26079.173448324203, 'total_duration': 42909.42013859749, 'accumulated_submission_time': 26079.173448324203, 'accumulated_eval_time': 16826.95787167549, 'accumulated_logging_time': 0.9769396781921387, 'global_step': 73955, 'preemption_count': 0}), (76340, {'train/accuracy': 0.6645991206169128, 'train/loss': 1.6866929531097412, 'train/bleu': 32.924616682714294, 'validation/accuracy': 0.67549067735672, 'validation/loss': 1.6071454286575317, 'validation/bleu': 29.642798192788497, 'validation/num_examples': 3000, 'test/accuracy': 0.6900935769081116, 'test/loss': 1.5276169776916504, 'test/bleu': 29.379224504547373, 'test/num_examples': 3003, 'score': 26919.26244521141, 'total_duration': 44230.73479604721, 'accumulated_submission_time': 26919.26244521141, 'accumulated_eval_time': 17308.0654463768, 'accumulated_logging_time': 1.0143301486968994, 'global_step': 76340, 'preemption_count': 0}), (78727, {'train/accuracy': 0.6578866243362427, 'train/loss': 1.7336337566375732, 'train/bleu': 33.02379867229866, 'validation/accuracy': 0.6777225136756897, 'validation/loss': 1.597773551940918, 'validation/bleu': 29.78239122588757, 'validation/num_examples': 3000, 'test/accuracy': 0.6900470852851868, 'test/loss': 1.5211902856826782, 'test/bleu': 29.493238645573108, 'test/num_examples': 3003, 'score': 27759.420594215393, 'total_duration': 45561.32496523857, 'accumulated_submission_time': 27759.420594215393, 'accumulated_eval_time': 17798.384374141693, 'accumulated_logging_time': 1.0526585578918457, 'global_step': 78727, 'preemption_count': 0}), (81114, {'train/accuracy': 0.6640418171882629, 'train/loss': 1.6978520154953003, 'train/bleu': 32.88214843273441, 'validation/accuracy': 0.6789872050285339, 'validation/loss': 1.593400001525879, 'validation/bleu': 29.676987911310327, 'validation/num_examples': 3000, 'test/accuracy': 0.6944047808647156, 'test/loss': 1.5073984861373901, 'test/bleu': 29.825063306519596, 'test/num_examples': 3003, 'score': 28599.610867261887, 'total_duration': 46876.34319114685, 'accumulated_submission_time': 28599.610867261887, 'accumulated_eval_time': 18273.101594686508, 'accumulated_logging_time': 1.0890939235687256, 'global_step': 81114, 'preemption_count': 0}), (83501, {'train/accuracy': 0.6630203723907471, 'train/loss': 1.7007750272750854, 'train/bleu': 32.74125649796642, 'validation/accuracy': 0.6791360378265381, 'validation/loss': 1.5790903568267822, 'validation/bleu': 29.654650380293756, 'validation/num_examples': 3000, 'test/accuracy': 0.6937307715415955, 'test/loss': 1.5014517307281494, 'test/bleu': 29.500032983740162, 'test/num_examples': 3003, 'score': 29440.040162086487, 'total_duration': 48205.56838059425, 'accumulated_submission_time': 29440.040162086487, 'accumulated_eval_time': 18761.78837966919, 'accumulated_logging_time': 1.1252844333648682, 'global_step': 83501, 'preemption_count': 0}), (85888, {'train/accuracy': 0.6633317470550537, 'train/loss': 1.6965419054031372, 'train/bleu': 32.76356242688841, 'validation/accuracy': 0.6802891492843628, 'validation/loss': 1.5811126232147217, 'validation/bleu': 30.043155582785047, 'validation/num_examples': 3000, 'test/accuracy': 0.6941258907318115, 'test/loss': 1.5003278255462646, 'test/bleu': 29.87476830317458, 'test/num_examples': 3003, 'score': 30280.26576089859, 'total_duration': 49530.71009230614, 'accumulated_submission_time': 30280.26576089859, 'accumulated_eval_time': 19246.59332036972, 'accumulated_logging_time': 1.1620018482208252, 'global_step': 85888, 'preemption_count': 0}), (88274, {'train/accuracy': 0.682263195514679, 'train/loss': 1.5941760540008545, 'train/bleu': 34.32433497901006, 'validation/accuracy': 0.6819010376930237, 'validation/loss': 1.5699979066848755, 'validation/bleu': 29.823944589013955, 'validation/num_examples': 3000, 'test/accuracy': 0.6963802576065063, 'test/loss': 1.4937410354614258, 'test/bleu': 29.658199063669386, 'test/num_examples': 3003, 'score': 31120.169873714447, 'total_duration': 50868.833562374115, 'accumulated_submission_time': 31120.169873714447, 'accumulated_eval_time': 19744.70170378685, 'accumulated_logging_time': 1.199810266494751, 'global_step': 88274, 'preemption_count': 0}), (90660, {'train/accuracy': 0.6701759696006775, 'train/loss': 1.6549545526504517, 'train/bleu': 32.95552252211354, 'validation/accuracy': 0.6838228702545166, 'validation/loss': 1.5629757642745972, 'validation/bleu': 30.110572580155196, 'validation/num_examples': 3000, 'test/accuracy': 0.6983673572540283, 'test/loss': 1.4784048795700073, 'test/bleu': 30.366281457229306, 'test/num_examples': 3003, 'score': 31960.32098174095, 'total_duration': 52200.768839120865, 'accumulated_submission_time': 31960.32098174095, 'accumulated_eval_time': 20236.372787475586, 'accumulated_logging_time': 1.2370805740356445, 'global_step': 90660, 'preemption_count': 0}), (93047, {'train/accuracy': 0.6740824580192566, 'train/loss': 1.6410588026046753, 'train/bleu': 32.95555072276922, 'validation/accuracy': 0.6841452717781067, 'validation/loss': 1.5564584732055664, 'validation/bleu': 30.397595709889032, 'validation/num_examples': 3000, 'test/accuracy': 0.7005403637886047, 'test/loss': 1.4703547954559326, 'test/bleu': 30.228897107393603, 'test/num_examples': 3003, 'score': 32800.464728832245, 'total_duration': 53550.79271054268, 'accumulated_submission_time': 32800.464728832245, 'accumulated_eval_time': 20746.14312171936, 'accumulated_logging_time': 1.2735848426818848, 'global_step': 93047, 'preemption_count': 0}), (95434, {'train/accuracy': 0.6752694249153137, 'train/loss': 1.6265360116958618, 'train/bleu': 34.1304166462366, 'validation/accuracy': 0.6851744055747986, 'validation/loss': 1.5516746044158936, 'validation/bleu': 30.227560754570124, 'validation/num_examples': 3000, 'test/accuracy': 0.7010632753372192, 'test/loss': 1.465505599975586, 'test/bleu': 30.193043914956924, 'test/num_examples': 3003, 'score': 33640.66740679741, 'total_duration': 54868.87886095047, 'accumulated_submission_time': 33640.66740679741, 'accumulated_eval_time': 21223.906438589096, 'accumulated_logging_time': 1.3174645900726318, 'global_step': 95434, 'preemption_count': 0}), (97821, {'train/accuracy': 0.6793409585952759, 'train/loss': 1.6154285669326782, 'train/bleu': 33.39148472321215, 'validation/accuracy': 0.6855835318565369, 'validation/loss': 1.5477550029754639, 'validation/bleu': 30.360630621623393, 'validation/num_examples': 3000, 'test/accuracy': 0.7020161747932434, 'test/loss': 1.464707612991333, 'test/bleu': 30.33091817118647, 'test/num_examples': 3003, 'score': 34480.82843494415, 'total_duration': 56180.17577815056, 'accumulated_submission_time': 34480.82843494415, 'accumulated_eval_time': 21694.930439949036, 'accumulated_logging_time': 1.3555314540863037, 'global_step': 97821, 'preemption_count': 0}), (100207, {'train/accuracy': 0.7089230418205261, 'train/loss': 1.443196415901184, 'train/bleu': 36.04611483927222, 'validation/accuracy': 0.6880261898040771, 'validation/loss': 1.534866452217102, 'validation/bleu': 30.359806178761417, 'validation/num_examples': 3000, 'test/accuracy': 0.7030271291732788, 'test/loss': 1.4542731046676636, 'test/bleu': 30.087657005563646, 'test/num_examples': 3003, 'score': 35320.90512704849, 'total_duration': 57482.59611940384, 'accumulated_submission_time': 35320.90512704849, 'accumulated_eval_time': 22157.154767990112, 'accumulated_logging_time': 1.3990330696105957, 'global_step': 100207, 'preemption_count': 0}), (102593, {'train/accuracy': 0.6842008233070374, 'train/loss': 1.5798007249832153, 'train/bleu': 34.35210443372337, 'validation/accuracy': 0.6877781748771667, 'validation/loss': 1.531459927558899, 'validation/bleu': 30.397316037761975, 'validation/num_examples': 3000, 'test/accuracy': 0.7057347297668457, 'test/loss': 1.445566177368164, 'test/bleu': 30.57403197903927, 'test/num_examples': 3003, 'score': 36160.994537353516, 'total_duration': 58801.58122611046, 'accumulated_submission_time': 36160.994537353516, 'accumulated_eval_time': 22635.92946076393, 'accumulated_logging_time': 1.444124460220337, 'global_step': 102593, 'preemption_count': 0}), (104979, {'train/accuracy': 0.683157205581665, 'train/loss': 1.5824335813522339, 'train/bleu': 34.28504809514936, 'validation/accuracy': 0.6898860335350037, 'validation/loss': 1.5270402431488037, 'validation/bleu': 30.47028869134486, 'validation/num_examples': 3000, 'test/accuracy': 0.7062576413154602, 'test/loss': 1.4383666515350342, 'test/bleu': 30.793022631949142, 'test/num_examples': 3003, 'score': 37001.09319806099, 'total_duration': 60153.76279973984, 'accumulated_submission_time': 37001.09319806099, 'accumulated_eval_time': 23147.897393465042, 'accumulated_logging_time': 1.483485221862793, 'global_step': 104979, 'preemption_count': 0}), (107365, {'train/accuracy': 0.6957497596740723, 'train/loss': 1.5065596103668213, 'train/bleu': 35.563739134784505, 'validation/accuracy': 0.6896504759788513, 'validation/loss': 1.5250399112701416, 'validation/bleu': 30.502875640198855, 'validation/num_examples': 3000, 'test/accuracy': 0.7057463526725769, 'test/loss': 1.435275673866272, 'test/bleu': 30.804335144952017, 'test/num_examples': 3003, 'score': 37841.05610227585, 'total_duration': 61481.371970653534, 'accumulated_submission_time': 37841.05610227585, 'accumulated_eval_time': 23635.432076931, 'accumulated_logging_time': 1.522993564605713, 'global_step': 107365, 'preemption_count': 0}), (109751, {'train/accuracy': 0.6920896172523499, 'train/loss': 1.5295428037643433, 'train/bleu': 35.06577869851787, 'validation/accuracy': 0.6902828216552734, 'validation/loss': 1.5207116603851318, 'validation/bleu': 30.492036028114327, 'validation/num_examples': 3000, 'test/accuracy': 0.7073732018470764, 'test/loss': 1.4307950735092163, 'test/bleu': 30.37974883692844, 'test/num_examples': 3003, 'score': 38681.15629196167, 'total_duration': 62864.20189833641, 'accumulated_submission_time': 38681.15629196167, 'accumulated_eval_time': 24178.042583703995, 'accumulated_logging_time': 1.5672264099121094, 'global_step': 109751, 'preemption_count': 0}), (112137, {'train/accuracy': 0.6917627453804016, 'train/loss': 1.534698724746704, 'train/bleu': 34.744883823713636, 'validation/accuracy': 0.6911011338233948, 'validation/loss': 1.5178720951080322, 'validation/bleu': 30.914413893236805, 'validation/num_examples': 3000, 'test/accuracy': 0.7082563638687134, 'test/loss': 1.4285781383514404, 'test/bleu': 30.58984000784102, 'test/num_examples': 3003, 'score': 39521.200323581696, 'total_duration': 64166.888488292694, 'accumulated_submission_time': 39521.200323581696, 'accumulated_eval_time': 24640.57275032997, 'accumulated_logging_time': 1.606520652770996, 'global_step': 112137, 'preemption_count': 0}), (114523, {'train/accuracy': 0.698196291923523, 'train/loss': 1.4954743385314941, 'train/bleu': 35.77462367787801, 'validation/accuracy': 0.6925270557403564, 'validation/loss': 1.5115963220596313, 'validation/bleu': 30.57723185326211, 'validation/num_examples': 3000, 'test/accuracy': 0.708628237247467, 'test/loss': 1.4227757453918457, 'test/bleu': 30.869338740033143, 'test/num_examples': 3003, 'score': 40361.25352883339, 'total_duration': 65511.83819484711, 'accumulated_submission_time': 40361.25352883339, 'accumulated_eval_time': 25145.353414535522, 'accumulated_logging_time': 1.647085428237915, 'global_step': 114523, 'preemption_count': 0}), (116908, {'train/accuracy': 0.6974208354949951, 'train/loss': 1.5098053216934204, 'train/bleu': 35.78149866685925, 'validation/accuracy': 0.693990170955658, 'validation/loss': 1.5112836360931396, 'validation/bleu': 30.96133738228591, 'validation/num_examples': 3000, 'test/accuracy': 0.7094300389289856, 'test/loss': 1.4189343452453613, 'test/bleu': 30.701844657815702, 'test/num_examples': 3003, 'score': 41201.23071146011, 'total_duration': 66833.81958413124, 'accumulated_submission_time': 41201.23071146011, 'accumulated_eval_time': 25627.239145994186, 'accumulated_logging_time': 1.689962387084961, 'global_step': 116908, 'preemption_count': 0}), (119295, {'train/accuracy': 0.710716187953949, 'train/loss': 1.4390244483947754, 'train/bleu': 36.5941331650276, 'validation/accuracy': 0.6939281225204468, 'validation/loss': 1.5062028169631958, 'validation/bleu': 30.915858804496704, 'validation/num_examples': 3000, 'test/accuracy': 0.7100808024406433, 'test/loss': 1.416991114616394, 'test/bleu': 30.658603829809408, 'test/num_examples': 3003, 'score': 42041.36020541191, 'total_duration': 68168.45203638077, 'accumulated_submission_time': 42041.36020541191, 'accumulated_eval_time': 26121.627092838287, 'accumulated_logging_time': 1.7309155464172363, 'global_step': 119295, 'preemption_count': 0}), (121681, {'train/accuracy': 0.7038121819496155, 'train/loss': 1.4720726013183594, 'train/bleu': 36.19937840630523, 'validation/accuracy': 0.6947588920593262, 'validation/loss': 1.5022433996200562, 'validation/bleu': 30.821739257184117, 'validation/num_examples': 3000, 'test/accuracy': 0.7108593583106995, 'test/loss': 1.4113267660140991, 'test/bleu': 30.843223404698186, 'test/num_examples': 3003, 'score': 42881.50228333473, 'total_duration': 69504.73358535767, 'accumulated_submission_time': 42881.50228333473, 'accumulated_eval_time': 26617.650767326355, 'accumulated_logging_time': 1.7730438709259033, 'global_step': 121681, 'preemption_count': 0}), (124066, {'train/accuracy': 0.7046647667884827, 'train/loss': 1.4676135778427124, 'train/bleu': 36.53354128224039, 'validation/accuracy': 0.6953912377357483, 'validation/loss': 1.5036792755126953, 'validation/bleu': 30.746032259348308, 'validation/num_examples': 3000, 'test/accuracy': 0.7117890119552612, 'test/loss': 1.4099397659301758, 'test/bleu': 30.892537332352088, 'test/num_examples': 3003, 'score': 43721.44666552544, 'total_duration': 70838.34599137306, 'accumulated_submission_time': 43721.44666552544, 'accumulated_eval_time': 27111.19986152649, 'accumulated_logging_time': 1.8156933784484863, 'global_step': 124066, 'preemption_count': 0}), (126451, {'train/accuracy': 0.7076228857040405, 'train/loss': 1.4430257081985474, 'train/bleu': 36.56570523268232, 'validation/accuracy': 0.6953416466712952, 'validation/loss': 1.5003718137741089, 'validation/bleu': 30.9751714367643, 'validation/num_examples': 3000, 'test/accuracy': 0.7123816609382629, 'test/loss': 1.4068301916122437, 'test/bleu': 31.215212632401542, 'test/num_examples': 3003, 'score': 44561.50462055206, 'total_duration': 72149.5507850647, 'accumulated_submission_time': 44561.50462055206, 'accumulated_eval_time': 27582.21673822403, 'accumulated_logging_time': 1.8660566806793213, 'global_step': 126451, 'preemption_count': 0}), (128837, {'train/accuracy': 0.7105525732040405, 'train/loss': 1.4332529306411743, 'train/bleu': 36.75074571548151, 'validation/accuracy': 0.6956764459609985, 'validation/loss': 1.4999107122421265, 'validation/bleu': 30.823767338628773, 'validation/num_examples': 3000, 'test/accuracy': 0.7122421860694885, 'test/loss': 1.4065791368484497, 'test/bleu': 30.932261071712002, 'test/num_examples': 3003, 'score': 45401.51370239258, 'total_duration': 73480.97584033012, 'accumulated_submission_time': 45401.51370239258, 'accumulated_eval_time': 28073.51711320877, 'accumulated_logging_time': 1.9084973335266113, 'global_step': 128837, 'preemption_count': 0}), (131223, {'train/accuracy': 0.7070929408073425, 'train/loss': 1.4555907249450684, 'train/bleu': 36.070049531934, 'validation/accuracy': 0.695874810218811, 'validation/loss': 1.499701976776123, 'validation/bleu': 30.95694516258592, 'validation/num_examples': 3000, 'test/accuracy': 0.7126489281654358, 'test/loss': 1.4054698944091797, 'test/bleu': 31.097917544741282, 'test/num_examples': 3003, 'score': 46241.69057679176, 'total_duration': 74813.12171435356, 'accumulated_submission_time': 46241.69057679176, 'accumulated_eval_time': 28565.360773801804, 'accumulated_logging_time': 1.958054780960083, 'global_step': 131223, 'preemption_count': 0}), (133333, {'train/accuracy': 0.7128967642784119, 'train/loss': 1.4201592206954956, 'train/bleu': 36.50041035697179, 'validation/accuracy': 0.6956764459609985, 'validation/loss': 1.4999949932098389, 'validation/bleu': 30.901218707876914, 'validation/num_examples': 3000, 'test/accuracy': 0.7125210762023926, 'test/loss': 1.4059910774230957, 'test/bleu': 31.041513578042657, 'test/num_examples': 3003, 'score': 46984.7851524353, 'total_duration': 76047.3649597168, 'accumulated_submission_time': 46984.7851524353, 'accumulated_eval_time': 29056.399307250977, 'accumulated_logging_time': 2.001035213470459, 'global_step': 133333, 'preemption_count': 0})], 'global_step': 133333}
I0207 08:47:13.575727 140699726837568 submission_runner.py:586] Timing: 46984.7851524353
I0207 08:47:13.575791 140699726837568 submission_runner.py:588] Total number of evals: 57
I0207 08:47:13.575841 140699726837568 submission_runner.py:589] ====================
I0207 08:47:13.575893 140699726837568 submission_runner.py:542] Using RNG seed 529981238
I0207 08:47:13.577404 140699726837568 submission_runner.py:551] --- Tuning run 2/5 ---
I0207 08:47:13.577507 140699726837568 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_1/wmt_jax/trial_2.
I0207 08:47:13.577948 140699726837568 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_1/wmt_jax/trial_2/hparams.json.
I0207 08:47:13.578823 140699726837568 submission_runner.py:206] Initializing dataset.
I0207 08:47:13.581261 140699726837568 dataset_info.py:578] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0207 08:47:13.584310 140699726837568 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0207 08:47:13.620838 140699726837568 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0207 08:47:14.200234 140699726837568 submission_runner.py:213] Initializing model.
I0207 08:47:20.588712 140699726837568 submission_runner.py:255] Initializing optimizer.
I0207 08:47:21.401985 140699726837568 submission_runner.py:262] Initializing metrics bundle.
I0207 08:47:21.402148 140699726837568 submission_runner.py:280] Initializing checkpoint and logger.
I0207 08:47:21.402892 140699726837568 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_1/wmt_jax/trial_2 with prefix checkpoint_
I0207 08:47:21.403017 140699726837568 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_1/wmt_jax/trial_2/meta_data_0.json.
I0207 08:47:21.403224 140699726837568 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0207 08:47:21.403286 140699726837568 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0207 08:47:21.913373 140699726837568 logger_utils.py:220] Unable to record git information. Continuing without it.
I0207 08:47:22.393218 140699726837568 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_1/wmt_jax/trial_2/flags_0.json.
I0207 08:47:22.400178 140699726837568 submission_runner.py:314] Starting training loop.
I0207 08:47:49.199746 140529919506176 logging_writer.py:48] [0] global_step=0, grad_norm=5.0944061279296875, loss=11.156318664550781
I0207 08:47:49.209099 140699726837568 spec.py:321] Evaluating on the training split.
I0207 08:47:51.938066 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 08:52:41.289154 140699726837568 spec.py:333] Evaluating on the validation split.
I0207 08:52:44.016555 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 08:57:32.716460 140699726837568 spec.py:349] Evaluating on the test split.
I0207 08:57:35.451494 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 09:02:24.224511 140699726837568 submission_runner.py:408] Time since start: 901.82s, 	Step: 1, 	{'train/accuracy': 0.0005899437237530947, 'train/loss': 11.17578411102295, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.208685874938965, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.19086742401123, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 26.80888557434082, 'total_duration': 901.8242671489716, 'accumulated_submission_time': 26.80888557434082, 'accumulated_eval_time': 875.0153396129608, 'accumulated_logging_time': 0}
I0207 09:02:24.233486 140529927898880 logging_writer.py:48] [1] accumulated_eval_time=875.015340, accumulated_logging_time=0, accumulated_submission_time=26.808886, global_step=1, preemption_count=0, score=26.808886, test/accuracy=0.000709, test/bleu=0.000000, test/loss=11.190867, test/num_examples=3003, total_duration=901.824267, train/accuracy=0.000590, train/bleu=0.000000, train/loss=11.175784, validation/accuracy=0.000484, validation/bleu=0.000000, validation/loss=11.208686, validation/num_examples=3000
I0207 09:02:59.478687 140529919506176 logging_writer.py:48] [100] global_step=100, grad_norm=0.2789818048477173, loss=9.017483711242676
I0207 09:03:34.708901 140529927898880 logging_writer.py:48] [200] global_step=200, grad_norm=0.2225375920534134, loss=8.678911209106445
I0207 09:04:09.982954 140529919506176 logging_writer.py:48] [300] global_step=300, grad_norm=0.8212966918945312, loss=8.304845809936523
I0207 09:04:45.312191 140529927898880 logging_writer.py:48] [400] global_step=400, grad_norm=1.0460213422775269, loss=8.050576210021973
I0207 09:05:20.620927 140529919506176 logging_writer.py:48] [500] global_step=500, grad_norm=0.411268413066864, loss=7.7990641593933105
I0207 09:05:55.913791 140529927898880 logging_writer.py:48] [600] global_step=600, grad_norm=0.8178564310073853, loss=7.636787414550781
I0207 09:06:31.228412 140529919506176 logging_writer.py:48] [700] global_step=700, grad_norm=0.8437115550041199, loss=7.495419502258301
I0207 09:07:06.565894 140529927898880 logging_writer.py:48] [800] global_step=800, grad_norm=0.5832977890968323, loss=7.242551326751709
I0207 09:07:41.886693 140529919506176 logging_writer.py:48] [900] global_step=900, grad_norm=0.6592276096343994, loss=7.181116580963135
I0207 09:08:17.259493 140529927898880 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.8815780282020569, loss=6.993342876434326
I0207 09:08:52.595309 140529919506176 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.6199831366539001, loss=6.804590225219727
I0207 09:09:27.964906 140529927898880 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.6867193579673767, loss=6.730329990386963
I0207 09:10:03.306094 140529919506176 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.7108011841773987, loss=6.630814552307129
I0207 09:10:38.666648 140529927898880 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.46918192505836487, loss=6.493544578552246
I0207 09:11:14.016943 140529919506176 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.49586278200149536, loss=6.383498191833496
I0207 09:11:49.363142 140529927898880 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.6141541004180908, loss=6.289968013763428
I0207 09:12:24.693179 140529919506176 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.774557888507843, loss=6.182978630065918
I0207 09:13:00.018826 140529927898880 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.6876537203788757, loss=6.121119976043701
I0207 09:13:35.363757 140529919506176 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.7547600865364075, loss=6.041655540466309
I0207 09:14:10.714102 140529927898880 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.700738787651062, loss=6.011113166809082
I0207 09:14:46.054794 140529919506176 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.7867403626441956, loss=5.861031532287598
I0207 09:15:21.409103 140529927898880 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.7150696516036987, loss=5.742377758026123
I0207 09:15:56.754837 140529919506176 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.5899730920791626, loss=5.6556196212768555
I0207 09:16:24.418997 140699726837568 spec.py:321] Evaluating on the training split.
I0207 09:16:27.458506 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 09:20:16.133984 140699726837568 spec.py:333] Evaluating on the validation split.
I0207 09:20:18.868176 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 09:24:27.681442 140699726837568 spec.py:349] Evaluating on the test split.
I0207 09:24:30.419821 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 09:28:26.531493 140699726837568 submission_runner.py:408] Time since start: 2464.13s, 	Step: 2380, 	{'train/accuracy': 0.4236276149749756, 'train/loss': 3.9450714588165283, 'train/bleu': 15.317106956985512, 'validation/accuracy': 0.4104350805282593, 'validation/loss': 4.064540863037109, 'validation/bleu': 10.871939591746955, 'validation/num_examples': 3000, 'test/accuracy': 0.4006739854812622, 'test/loss': 4.220223426818848, 'test/bleu': 9.119701587852655, 'test/num_examples': 3003, 'score': 866.9106457233429, 'total_duration': 2464.131246328354, 'accumulated_submission_time': 866.9106457233429, 'accumulated_eval_time': 1597.1277873516083, 'accumulated_logging_time': 0.01984238624572754}
I0207 09:28:26.546314 140529927898880 logging_writer.py:48] [2380] accumulated_eval_time=1597.127787, accumulated_logging_time=0.019842, accumulated_submission_time=866.910646, global_step=2380, preemption_count=0, score=866.910646, test/accuracy=0.400674, test/bleu=9.119702, test/loss=4.220223, test/num_examples=3003, total_duration=2464.131246, train/accuracy=0.423628, train/bleu=15.317107, train/loss=3.945071, validation/accuracy=0.410435, validation/bleu=10.871940, validation/loss=4.064541, validation/num_examples=3000
I0207 09:28:33.958938 140529919506176 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.7224026918411255, loss=5.615198135375977
I0207 09:29:09.209460 140529927898880 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.7253808975219727, loss=5.550820827484131
I0207 09:29:44.487421 140529919506176 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.5150991678237915, loss=5.38171911239624
I0207 09:30:19.792397 140529927898880 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.5492712259292603, loss=5.382962703704834
I0207 09:30:55.096239 140529919506176 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.5237642526626587, loss=5.308654308319092
I0207 09:31:30.402573 140529927898880 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.5224447250366211, loss=5.259487628936768
I0207 09:32:05.723412 140529919506176 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.756096601486206, loss=5.3161163330078125
I0207 09:32:41.056343 140529927898880 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.4734176993370056, loss=5.153346538543701
I0207 09:33:16.376631 140529919506176 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.49483412504196167, loss=5.112443447113037
I0207 09:33:51.723363 140529927898880 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.5443504452705383, loss=5.111471176147461
I0207 09:34:27.069855 140529919506176 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.49037665128707886, loss=5.08458948135376
I0207 09:35:02.391284 140529927898880 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.5616363883018494, loss=5.0355095863342285
I0207 09:35:37.731458 140529919506176 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.5781604647636414, loss=5.029183864593506
I0207 09:36:13.058704 140529927898880 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.4421978294849396, loss=4.917858123779297
I0207 09:36:48.375473 140529919506176 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.47300270199775696, loss=4.9765944480896
I0207 09:37:23.703802 140529927898880 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.4372673034667969, loss=4.9390339851379395
I0207 09:37:59.025398 140529919506176 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.3959238529205322, loss=4.917660236358643
I0207 09:38:34.355957 140529927898880 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.4528868496417999, loss=4.834805011749268
I0207 09:39:09.662051 140529919506176 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.4530060887336731, loss=4.847229957580566
I0207 09:39:44.960016 140529927898880 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.42027005553245544, loss=4.795882701873779
I0207 09:40:20.281737 140529919506176 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.44481271505355835, loss=4.820410251617432
I0207 09:40:55.606601 140529927898880 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.3723212480545044, loss=4.73201322555542
I0207 09:41:30.942276 140529919506176 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.43444496393203735, loss=4.787936210632324
I0207 09:42:06.252713 140529927898880 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.354602187871933, loss=4.62974739074707
I0207 09:42:26.839788 140699726837568 spec.py:321] Evaluating on the training split.
I0207 09:42:29.892393 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 09:45:20.043363 140699726837568 spec.py:333] Evaluating on the validation split.
I0207 09:45:22.791965 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 09:48:07.510388 140699726837568 spec.py:349] Evaluating on the test split.
I0207 09:48:10.250612 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 09:50:40.600420 140699726837568 submission_runner.py:408] Time since start: 3798.20s, 	Step: 4760, 	{'train/accuracy': 0.5412089824676514, 'train/loss': 2.8526296615600586, 'train/bleu': 24.40067253420964, 'validation/accuracy': 0.5490322709083557, 'validation/loss': 2.781830072402954, 'validation/bleu': 20.54890937518002, 'validation/num_examples': 3000, 'test/accuracy': 0.5474405884742737, 'test/loss': 2.8263823986053467, 'test/bleu': 19.069473249226416, 'test/num_examples': 3003, 'score': 1707.1207020282745, 'total_duration': 3798.2001650333405, 'accumulated_submission_time': 1707.1207020282745, 'accumulated_eval_time': 2090.888371706009, 'accumulated_logging_time': 0.045859575271606445}
I0207 09:50:40.615818 140529919506176 logging_writer.py:48] [4760] accumulated_eval_time=2090.888372, accumulated_logging_time=0.045860, accumulated_submission_time=1707.120702, global_step=4760, preemption_count=0, score=1707.120702, test/accuracy=0.547441, test/bleu=19.069473, test/loss=2.826382, test/num_examples=3003, total_duration=3798.200165, train/accuracy=0.541209, train/bleu=24.400673, train/loss=2.852630, validation/accuracy=0.549032, validation/bleu=20.548909, validation/loss=2.781830, validation/num_examples=3000
I0207 09:50:55.056534 140529927898880 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.3960743546485901, loss=4.716006278991699
I0207 09:51:30.283587 140529919506176 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.3811534643173218, loss=4.708996295928955
I0207 09:52:05.529445 140529927898880 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.33485856652259827, loss=4.690217971801758
I0207 09:52:40.796680 140529919506176 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.3630900979042053, loss=4.691866874694824
I0207 09:53:16.076875 140529927898880 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.4884815216064453, loss=4.701406955718994
I0207 09:53:51.361268 140529919506176 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.3147009611129761, loss=4.6037187576293945
I0207 09:54:26.665496 140529927898880 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.31621676683425903, loss=4.611832618713379
I0207 09:55:01.985397 140529919506176 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.341628760099411, loss=4.552410125732422
I0207 09:55:37.278007 140529927898880 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.321212500333786, loss=4.610877513885498
I0207 09:56:12.565789 140529919506176 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.30856525897979736, loss=4.57234001159668
I0207 09:56:47.877636 140529927898880 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.30408135056495667, loss=4.629738807678223
I0207 09:57:23.168582 140529919506176 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.2998805046081543, loss=4.594737529754639
I0207 09:57:58.446518 140529927898880 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.30565157532691956, loss=4.527968883514404
I0207 09:58:33.726952 140529919506176 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.2822459638118744, loss=4.530093193054199
I0207 09:59:09.001715 140529927898880 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.2790236473083496, loss=4.567281246185303
I0207 09:59:44.293923 140529919506176 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.2835935950279236, loss=4.539339065551758
I0207 10:00:19.591503 140529927898880 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.309039831161499, loss=4.553555965423584
I0207 10:00:54.872061 140529919506176 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.34630534052848816, loss=4.531299591064453
I0207 10:01:30.181531 140529927898880 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.29495781660079956, loss=4.447339057922363
I0207 10:02:05.453646 140529919506176 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.2645580768585205, loss=4.439747333526611
I0207 10:02:40.729335 140529927898880 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.27039846777915955, loss=4.512269020080566
I0207 10:03:16.014457 140529919506176 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.24541844427585602, loss=4.429045677185059
I0207 10:03:51.321062 140529927898880 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.257257342338562, loss=4.495728015899658
I0207 10:04:26.604426 140529919506176 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.2270684838294983, loss=4.512598037719727
I0207 10:04:40.771802 140699726837568 spec.py:321] Evaluating on the training split.
I0207 10:04:43.799270 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 10:07:33.224388 140699726837568 spec.py:333] Evaluating on the validation split.
I0207 10:07:35.948907 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 10:10:17.664558 140699726837568 spec.py:349] Evaluating on the test split.
I0207 10:10:20.387603 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 10:12:50.608501 140699726837568 submission_runner.py:408] Time since start: 5128.21s, 	Step: 7142, 	{'train/accuracy': 0.5862109065055847, 'train/loss': 2.43117094039917, 'train/bleu': 26.95864129778303, 'validation/accuracy': 0.5920323133468628, 'validation/loss': 2.3843495845794678, 'validation/bleu': 23.53881679992252, 'validation/num_examples': 3000, 'test/accuracy': 0.5926558971405029, 'test/loss': 2.3840017318725586, 'test/bleu': 22.39646133848995, 'test/num_examples': 3003, 'score': 2547.1937956809998, 'total_duration': 5128.208259344101, 'accumulated_submission_time': 2547.1937956809998, 'accumulated_eval_time': 2580.725028514862, 'accumulated_logging_time': 0.07153177261352539}
I0207 10:12:50.623475 140529927898880 logging_writer.py:48] [7142] accumulated_eval_time=2580.725029, accumulated_logging_time=0.071532, accumulated_submission_time=2547.193796, global_step=7142, preemption_count=0, score=2547.193796, test/accuracy=0.592656, test/bleu=22.396461, test/loss=2.384002, test/num_examples=3003, total_duration=5128.208259, train/accuracy=0.586211, train/bleu=26.958641, train/loss=2.431171, validation/accuracy=0.592032, validation/bleu=23.538817, validation/loss=2.384350, validation/num_examples=3000
I0207 10:13:11.371726 140529919506176 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.23086772859096527, loss=4.440915584564209
I0207 10:13:46.564443 140529927898880 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.2487049251794815, loss=4.392422676086426
I0207 10:14:21.822074 140529919506176 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.27274301648139954, loss=4.433617115020752
I0207 10:14:57.108759 140529927898880 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.23900797963142395, loss=4.413307189941406
I0207 10:15:32.417783 140529919506176 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.2801811993122101, loss=4.475717544555664
I0207 10:16:07.746692 140529927898880 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.2511084973812103, loss=4.358883857727051
I0207 10:16:43.027712 140529919506176 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.22099614143371582, loss=4.297680377960205
I0207 10:17:18.309408 140529927898880 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.20380347967147827, loss=4.414114952087402
I0207 10:17:53.586966 140529919506176 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.23798474669456482, loss=4.364501476287842
I0207 10:18:28.868190 140529927898880 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.20899444818496704, loss=4.3063836097717285
I0207 10:19:04.140969 140529919506176 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.23387768864631653, loss=4.291159629821777
I0207 10:19:39.425624 140529927898880 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.2244729995727539, loss=4.413812160491943
I0207 10:20:14.715124 140529919506176 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.20932908356189728, loss=4.353079319000244
I0207 10:20:50.015073 140529927898880 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.20192621648311615, loss=4.320307731628418
I0207 10:21:25.299534 140529919506176 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.21450403332710266, loss=4.360709190368652
I0207 10:22:00.640861 140529927898880 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.21427717804908752, loss=4.320144176483154
I0207 10:22:35.897206 140529919506176 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.20958448946475983, loss=4.297898292541504
I0207 10:23:11.166074 140529927898880 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.20612719655036926, loss=4.310949325561523
I0207 10:23:46.427336 140529919506176 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.22617092728614807, loss=4.36052131652832
I0207 10:24:21.711615 140529927898880 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.2344885766506195, loss=4.32531213760376
I0207 10:24:56.983695 140529919506176 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.2059859186410904, loss=4.318805694580078
I0207 10:25:32.258971 140529927898880 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.19901464879512787, loss=4.263723850250244
I0207 10:26:07.515230 140529919506176 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.198204904794693, loss=4.292295455932617
I0207 10:26:42.811760 140529927898880 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.19187843799591064, loss=4.259172439575195
I0207 10:26:50.658629 140699726837568 spec.py:321] Evaluating on the training split.
I0207 10:26:53.704809 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 10:29:37.258381 140699726837568 spec.py:333] Evaluating on the validation split.
I0207 10:29:40.020730 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 10:32:17.761853 140699726837568 spec.py:349] Evaluating on the test split.
I0207 10:32:20.496982 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 10:34:50.322029 140699726837568 submission_runner.py:408] Time since start: 6447.92s, 	Step: 9524, 	{'train/accuracy': 0.5987629294395447, 'train/loss': 2.312525987625122, 'train/bleu': 28.450225323180657, 'validation/accuracy': 0.6168057322502136, 'validation/loss': 2.1870875358581543, 'validation/bleu': 25.084379421999714, 'validation/num_examples': 3000, 'test/accuracy': 0.6223113536834717, 'test/loss': 2.157191276550293, 'test/bleu': 24.093482603258604, 'test/num_examples': 3003, 'score': 3387.143469810486, 'total_duration': 6447.921786308289, 'accumulated_submission_time': 3387.143469810486, 'accumulated_eval_time': 3060.3883907794952, 'accumulated_logging_time': 0.0963890552520752}
I0207 10:34:50.338013 140529919506176 logging_writer.py:48] [9524] accumulated_eval_time=3060.388391, accumulated_logging_time=0.096389, accumulated_submission_time=3387.143470, global_step=9524, preemption_count=0, score=3387.143470, test/accuracy=0.622311, test/bleu=24.093483, test/loss=2.157191, test/num_examples=3003, total_duration=6447.921786, train/accuracy=0.598763, train/bleu=28.450225, train/loss=2.312526, validation/accuracy=0.616806, validation/bleu=25.084379, validation/loss=2.187088, validation/num_examples=3000
I0207 10:35:17.414686 140529927898880 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.17839166522026062, loss=4.2217912673950195
I0207 10:35:52.598388 140529919506176 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.19078843295574188, loss=4.204836845397949
I0207 10:36:27.850131 140529927898880 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.17823700606822968, loss=4.243494033813477
I0207 10:37:03.082423 140529919506176 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.17688217759132385, loss=4.337530136108398
I0207 10:37:38.315335 140529927898880 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.19539766013622284, loss=4.268057823181152
I0207 10:38:13.553298 140529919506176 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.17200912535190582, loss=4.2296223640441895
I0207 10:38:48.855215 140529927898880 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.17575593292713165, loss=4.223507881164551
I0207 10:39:24.111295 140529919506176 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.17639502882957458, loss=4.235000133514404
I0207 10:39:59.380971 140529927898880 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.18295623362064362, loss=4.2275543212890625
I0207 10:40:34.661659 140529919506176 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.17506763339042664, loss=4.188692092895508
I0207 10:41:09.928645 140529927898880 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.1784953624010086, loss=4.26631498336792
I0207 10:41:45.169411 140529919506176 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.17576809227466583, loss=4.218778133392334
I0207 10:42:20.427825 140529927898880 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.18099746108055115, loss=4.189378261566162
I0207 10:42:55.687611 140529919506176 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.2620849907398224, loss=4.303890228271484
I0207 10:43:30.970879 140529927898880 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.18974529206752777, loss=4.2595438957214355
I0207 10:44:06.247447 140529919506176 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.19824793934822083, loss=4.224435329437256
I0207 10:44:41.554033 140529927898880 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.19399122893810272, loss=4.14976692199707
I0207 10:45:16.834502 140529919506176 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.21235060691833496, loss=4.276063919067383
I0207 10:45:52.090588 140529927898880 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.17377407848834991, loss=4.1524434089660645
I0207 10:46:27.353064 140529919506176 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.16900530457496643, loss=4.284698486328125
I0207 10:47:02.629242 140529927898880 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.16044870018959045, loss=4.193580627441406
I0207 10:47:37.910395 140529919506176 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.1778600513935089, loss=4.158576011657715
I0207 10:48:13.210743 140529927898880 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.17749793827533722, loss=4.20036506652832
I0207 10:48:48.469598 140529919506176 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.16521887481212616, loss=4.1924567222595215
I0207 10:48:50.655783 140699726837568 spec.py:321] Evaluating on the training split.
I0207 10:48:53.682339 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 10:51:41.766624 140699726837568 spec.py:333] Evaluating on the validation split.
I0207 10:51:44.502892 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 10:54:21.658881 140699726837568 spec.py:349] Evaluating on the test split.
I0207 10:54:24.385125 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 10:56:45.546321 140699726837568 submission_runner.py:408] Time since start: 7763.15s, 	Step: 11908, 	{'train/accuracy': 0.6138762831687927, 'train/loss': 2.1910603046417236, 'train/bleu': 29.359989662943025, 'validation/accuracy': 0.632627010345459, 'validation/loss': 2.045173168182373, 'validation/bleu': 26.211492834655424, 'validation/num_examples': 3000, 'test/accuracy': 0.6385219097137451, 'test/loss': 2.0061533451080322, 'test/bleu': 25.16989331342719, 'test/num_examples': 3003, 'score': 4227.378788471222, 'total_duration': 7763.146024465561, 'accumulated_submission_time': 4227.378788471222, 'accumulated_eval_time': 3535.278825521469, 'accumulated_logging_time': 0.12252569198608398}
I0207 10:56:45.562725 140529927898880 logging_writer.py:48] [11908] accumulated_eval_time=3535.278826, accumulated_logging_time=0.122526, accumulated_submission_time=4227.378788, global_step=11908, preemption_count=0, score=4227.378788, test/accuracy=0.638522, test/bleu=25.169893, test/loss=2.006153, test/num_examples=3003, total_duration=7763.146024, train/accuracy=0.613876, train/bleu=29.359990, train/loss=2.191060, validation/accuracy=0.632627, validation/bleu=26.211493, validation/loss=2.045173, validation/num_examples=3000
I0207 10:57:18.289451 140529919506176 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.16553489863872528, loss=4.2896223068237305
I0207 10:57:53.491182 140529927898880 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.21316756308078766, loss=4.146100044250488
I0207 10:58:28.723521 140529919506176 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.18620514869689941, loss=4.118473052978516
I0207 10:59:03.958950 140529927898880 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.1624157428741455, loss=4.190264701843262
I0207 10:59:39.174206 140529919506176 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.15681324899196625, loss=4.186794757843018
I0207 11:00:14.444958 140529927898880 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.17113877832889557, loss=4.128138542175293
I0207 11:00:49.716695 140529919506176 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.20011216402053833, loss=4.154877185821533
I0207 11:01:25.011684 140529927898880 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.1621384471654892, loss=4.197837829589844
I0207 11:02:00.304921 140529919506176 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.15124407410621643, loss=4.195383548736572
I0207 11:02:35.573117 140529927898880 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.15322400629520416, loss=4.164526462554932
I0207 11:03:10.822258 140529919506176 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.15507148206233978, loss=4.157406806945801
I0207 11:03:46.088057 140529927898880 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.16112199425697327, loss=4.155795097351074
I0207 11:04:21.508486 140529919506176 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.17660990357398987, loss=4.177252769470215
I0207 11:04:56.794228 140529927898880 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.15741321444511414, loss=4.174193859100342
I0207 11:05:32.077657 140529919506176 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.17785502970218658, loss=4.158333778381348
I0207 11:06:07.375561 140529927898880 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.1693974733352661, loss=4.154317378997803
I0207 11:06:42.715968 140529919506176 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.23623616993427277, loss=4.105915069580078
I0207 11:07:18.050221 140529927898880 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.19096839427947998, loss=4.202484130859375
I0207 11:07:53.380118 140529919506176 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.15236952900886536, loss=4.187981128692627
I0207 11:08:28.772204 140529927898880 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.16209249198436737, loss=4.1025710105896
I0207 11:09:04.076121 140529919506176 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.17936460673809052, loss=4.157179355621338
I0207 11:09:39.325716 140529927898880 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.1594821959733963, loss=4.127880573272705
I0207 11:10:14.578759 140529919506176 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.15612293779850006, loss=4.097623825073242
I0207 11:10:45.640828 140699726837568 spec.py:321] Evaluating on the training split.
I0207 11:10:48.672265 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 11:13:34.293332 140699726837568 spec.py:333] Evaluating on the validation split.
I0207 11:13:37.055959 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 11:16:14.384967 140699726837568 spec.py:349] Evaluating on the test split.
I0207 11:16:17.136255 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 11:18:35.789972 140699726837568 submission_runner.py:408] Time since start: 9073.39s, 	Step: 14290, 	{'train/accuracy': 0.6261821985244751, 'train/loss': 2.0904488563537598, 'train/bleu': 30.646065698482577, 'validation/accuracy': 0.6428066492080688, 'validation/loss': 1.982466459274292, 'validation/bleu': 27.19086719876297, 'validation/num_examples': 3000, 'test/accuracy': 0.6499332189559937, 'test/loss': 1.9334949254989624, 'test/bleu': 26.31243160118559, 'test/num_examples': 3003, 'score': 5067.369838953018, 'total_duration': 9073.389715909958, 'accumulated_submission_time': 5067.369838953018, 'accumulated_eval_time': 4005.4279165267944, 'accumulated_logging_time': 0.14914679527282715}
I0207 11:18:35.806416 140529927898880 logging_writer.py:48] [14290] accumulated_eval_time=4005.427917, accumulated_logging_time=0.149147, accumulated_submission_time=5067.369839, global_step=14290, preemption_count=0, score=5067.369839, test/accuracy=0.649933, test/bleu=26.312432, test/loss=1.933495, test/num_examples=3003, total_duration=9073.389716, train/accuracy=0.626182, train/bleu=30.646066, train/loss=2.090449, validation/accuracy=0.642807, validation/bleu=27.190867, validation/loss=1.982466, validation/num_examples=3000
I0207 11:18:39.694390 140529919506176 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.1747111678123474, loss=4.166967868804932
I0207 11:19:14.900618 140529927898880 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.20423735678195953, loss=4.094552516937256
I0207 11:19:50.090939 140529919506176 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.16263023018836975, loss=4.079585075378418
I0207 11:20:25.340840 140529927898880 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.1751328408718109, loss=4.1404924392700195
I0207 11:21:00.612094 140529919506176 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.1672804206609726, loss=4.1392903327941895
I0207 11:21:35.911739 140529927898880 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.1737421602010727, loss=4.102216720581055
I0207 11:22:11.237500 140529919506176 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.16185082495212555, loss=4.188584804534912
I0207 11:22:46.502374 140529927898880 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.16394665837287903, loss=4.117015361785889
I0207 11:23:21.832073 140529919506176 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.1548328697681427, loss=4.130446434020996
I0207 11:23:57.092477 140529927898880 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.16136030852794647, loss=4.066877365112305
I0207 11:24:32.347375 140529919506176 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.15709646046161652, loss=4.1495466232299805
I0207 11:25:07.632709 140529927898880 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.1566590964794159, loss=4.157062530517578
I0207 11:25:42.880517 140529919506176 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.19430309534072876, loss=4.117182731628418
I0207 11:26:18.157291 140529927898880 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.15090994536876678, loss=4.094519138336182
I0207 11:26:53.466610 140529919506176 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.15726803243160248, loss=4.123547077178955
I0207 11:27:28.780714 140529927898880 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.17503412067890167, loss=4.068240642547607
I0207 11:28:04.063681 140529919506176 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.1505260467529297, loss=4.064368724822998
I0207 11:28:39.336377 140529927898880 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.19969050586223602, loss=4.103728771209717
I0207 11:29:14.637029 140529919506176 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.16194795072078705, loss=4.069750785827637
I0207 11:29:49.920910 140529927898880 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.17590025067329407, loss=4.098689079284668
I0207 11:30:25.166938 140529919506176 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.1539413034915924, loss=4.059682369232178
I0207 11:31:00.475714 140529927898880 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.17847801744937897, loss=4.0792012214660645
I0207 11:31:35.763861 140529919506176 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.1665307730436325, loss=4.12639045715332
I0207 11:32:11.012477 140529927898880 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.165785014629364, loss=4.086057186126709
I0207 11:32:36.130868 140699726837568 spec.py:321] Evaluating on the training split.
I0207 11:32:39.167279 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 11:35:21.946719 140699726837568 spec.py:333] Evaluating on the validation split.
I0207 11:35:24.678631 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 11:38:07.082457 140699726837568 spec.py:349] Evaluating on the test split.
I0207 11:38:09.843823 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 11:40:51.135281 140699726837568 submission_runner.py:408] Time since start: 10408.74s, 	Step: 16673, 	{'train/accuracy': 0.6261575818061829, 'train/loss': 2.085031509399414, 'train/bleu': 30.482522261779774, 'validation/accuracy': 0.648510217666626, 'validation/loss': 1.9287440776824951, 'validation/bleu': 27.83188217261842, 'validation/num_examples': 3000, 'test/accuracy': 0.6567311882972717, 'test/loss': 1.875162959098816, 'test/bleu': 26.530187275714983, 'test/num_examples': 3003, 'score': 5907.608487606049, 'total_duration': 10408.735023498535, 'accumulated_submission_time': 5907.608487606049, 'accumulated_eval_time': 4500.4322681427, 'accumulated_logging_time': 0.177018404006958}
I0207 11:40:51.151968 140529919506176 logging_writer.py:48] [16673] accumulated_eval_time=4500.432268, accumulated_logging_time=0.177018, accumulated_submission_time=5907.608488, global_step=16673, preemption_count=0, score=5907.608488, test/accuracy=0.656731, test/bleu=26.530187, test/loss=1.875163, test/num_examples=3003, total_duration=10408.735023, train/accuracy=0.626158, train/bleu=30.482522, train/loss=2.085032, validation/accuracy=0.648510, validation/bleu=27.831882, validation/loss=1.928744, validation/num_examples=3000
I0207 11:41:01.017284 140529927898880 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.1657591611146927, loss=4.042547225952148
I0207 11:41:36.203978 140529919506176 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.16460588574409485, loss=4.093700408935547
I0207 11:42:11.436431 140529927898880 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.15777507424354553, loss=4.0800323486328125
I0207 11:42:46.691678 140529919506176 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.2064443826675415, loss=4.080103874206543
I0207 11:43:21.941632 140529927898880 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.1618661731481552, loss=4.078487873077393
I0207 11:43:57.200448 140529919506176 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.20247310400009155, loss=4.139273643493652
I0207 11:44:32.453702 140529927898880 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.1517072319984436, loss=4.0606584548950195
I0207 11:45:07.767671 140529919506176 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.15667839348316193, loss=4.056077003479004
I0207 11:45:43.008304 140529927898880 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.1639391928911209, loss=4.143413543701172
I0207 11:46:18.273967 140529919506176 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.16044645011425018, loss=4.064615726470947
I0207 11:46:53.525627 140529927898880 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.1560354381799698, loss=4.072052001953125
I0207 11:47:28.854756 140529919506176 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.19705043733119965, loss=4.082765579223633
I0207 11:48:04.125255 140529927898880 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.15653575956821442, loss=4.048994064331055
I0207 11:48:39.399066 140529919506176 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.15347665548324585, loss=3.9728987216949463
I0207 11:49:14.697025 140529927898880 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.16919156908988953, loss=4.076956272125244
I0207 11:49:49.991212 140529919506176 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.1707710325717926, loss=4.109730243682861
I0207 11:50:25.294326 140529927898880 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.16886650025844574, loss=4.028417110443115
I0207 11:51:00.542650 140529919506176 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.181128591299057, loss=4.0766801834106445
I0207 11:51:35.843235 140529927898880 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.1589518040418625, loss=4.045351982116699
I0207 11:52:11.138608 140529919506176 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.14589884877204895, loss=4.039432525634766
I0207 11:52:46.412086 140529927898880 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.17406359314918518, loss=4.040088176727295
I0207 11:53:21.656042 140529919506176 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.15336176753044128, loss=4.0816168785095215
I0207 11:53:56.990553 140529927898880 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.16922038793563843, loss=4.044366836547852
I0207 11:54:32.254994 140529919506176 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.1636706292629242, loss=4.033415794372559
I0207 11:54:51.357763 140699726837568 spec.py:321] Evaluating on the training split.
I0207 11:54:54.388042 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 11:57:56.177882 140699726837568 spec.py:333] Evaluating on the validation split.
I0207 11:57:58.914574 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 12:00:32.908305 140699726837568 spec.py:349] Evaluating on the test split.
I0207 12:00:35.637914 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 12:03:00.625498 140699726837568 submission_runner.py:408] Time since start: 11738.23s, 	Step: 19056, 	{'train/accuracy': 0.6523534059524536, 'train/loss': 1.8958368301391602, 'train/bleu': 32.224112132764745, 'validation/accuracy': 0.6549577713012695, 'validation/loss': 1.867025375366211, 'validation/bleu': 28.023329453083598, 'validation/num_examples': 3000, 'test/accuracy': 0.6621579527854919, 'test/loss': 1.8095208406448364, 'test/bleu': 27.09557119536779, 'test/num_examples': 3003, 'score': 6747.7297422885895, 'total_duration': 11738.225252628326, 'accumulated_submission_time': 6747.7297422885895, 'accumulated_eval_time': 4989.699973106384, 'accumulated_logging_time': 0.20397615432739258}
I0207 12:03:00.643305 140529927898880 logging_writer.py:48] [19056] accumulated_eval_time=4989.699973, accumulated_logging_time=0.203976, accumulated_submission_time=6747.729742, global_step=19056, preemption_count=0, score=6747.729742, test/accuracy=0.662158, test/bleu=27.095571, test/loss=1.809521, test/num_examples=3003, total_duration=11738.225253, train/accuracy=0.652353, train/bleu=32.224112, train/loss=1.895837, validation/accuracy=0.654958, validation/bleu=28.023329, validation/loss=1.867025, validation/num_examples=3000
I0207 12:03:16.484321 140529919506176 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.16988928616046906, loss=4.090117454528809
I0207 12:03:51.632649 140529927898880 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.19807252287864685, loss=4.064966678619385
I0207 12:04:26.846726 140529919506176 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.14692695438861847, loss=4.021884441375732
I0207 12:05:02.115481 140529927898880 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.19552309811115265, loss=4.097844123840332
I0207 12:05:37.364981 140529919506176 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.15423732995986938, loss=4.006717681884766
I0207 12:06:12.639154 140529927898880 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.16117790341377258, loss=4.079404830932617
I0207 12:06:47.902583 140529919506176 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.21111392974853516, loss=4.083847999572754
I0207 12:07:23.163309 140529927898880 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.17873850464820862, loss=4.072492599487305
I0207 12:07:58.587024 140529919506176 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.16092997789382935, loss=4.005908012390137
I0207 12:08:33.914990 140529927898880 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.17142178118228912, loss=4.048114776611328
I0207 12:09:09.182950 140529919506176 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.16637380421161652, loss=4.088156223297119
I0207 12:09:44.492742 140529927898880 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.14976710081100464, loss=3.973024606704712
I0207 12:10:19.768350 140529919506176 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.19526027143001556, loss=3.9791078567504883
I0207 12:10:55.044070 140529927898880 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.16904594004154205, loss=4.071549892425537
I0207 12:11:30.351730 140529919506176 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.20331725478172302, loss=4.008333683013916
I0207 12:12:05.652254 140529927898880 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.18005873262882233, loss=3.9359893798828125
I0207 12:12:40.960646 140529919506176 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.15710924565792084, loss=4.0533976554870605
I0207 12:13:16.229841 140529927898880 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.1622629165649414, loss=4.020939350128174
I0207 12:13:51.512593 140529919506176 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.1567854881286621, loss=4.039459705352783
I0207 12:14:26.768629 140529927898880 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.2966335415840149, loss=3.991041898727417
I0207 12:15:02.053644 140529919506176 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.16245195269584656, loss=3.9782605171203613
I0207 12:15:37.287896 140529927898880 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.19586150348186493, loss=4.035459518432617
I0207 12:16:12.542731 140529919506176 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.17139416933059692, loss=4.050297260284424
I0207 12:16:47.788930 140529927898880 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.18279702961444855, loss=4.0418171882629395
I0207 12:17:00.908582 140699726837568 spec.py:321] Evaluating on the training split.
I0207 12:17:03.949307 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 12:19:48.854129 140699726837568 spec.py:333] Evaluating on the validation split.
I0207 12:19:51.586366 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 12:22:25.948083 140699726837568 spec.py:349] Evaluating on the test split.
I0207 12:22:28.677845 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 12:24:55.096136 140699726837568 submission_runner.py:408] Time since start: 13052.70s, 	Step: 21439, 	{'train/accuracy': 0.6392760276794434, 'train/loss': 1.9621353149414062, 'train/bleu': 31.365681946344402, 'validation/accuracy': 0.6572020053863525, 'validation/loss': 1.832504153251648, 'validation/bleu': 28.291012927380795, 'validation/num_examples': 3000, 'test/accuracy': 0.667910099029541, 'test/loss': 1.771620273590088, 'test/bleu': 27.43810439700653, 'test/num_examples': 3003, 'score': 7587.909989356995, 'total_duration': 13052.695889472961, 'accumulated_submission_time': 7587.909989356995, 'accumulated_eval_time': 5463.887475013733, 'accumulated_logging_time': 0.2319626808166504}
I0207 12:24:55.113288 140529919506176 logging_writer.py:48] [21439] accumulated_eval_time=5463.887475, accumulated_logging_time=0.231963, accumulated_submission_time=7587.909989, global_step=21439, preemption_count=0, score=7587.909989, test/accuracy=0.667910, test/bleu=27.438104, test/loss=1.771620, test/num_examples=3003, total_duration=13052.695889, train/accuracy=0.639276, train/bleu=31.365682, train/loss=1.962135, validation/accuracy=0.657202, validation/bleu=28.291013, validation/loss=1.832504, validation/num_examples=3000
I0207 12:25:16.935561 140529927898880 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.17182131111621857, loss=4.044775485992432
I0207 12:25:52.152595 140529919506176 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.16481080651283264, loss=4.059805393218994
I0207 12:26:27.434136 140529927898880 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.15949083864688873, loss=4.0170183181762695
I0207 12:27:02.756629 140529919506176 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.14682196080684662, loss=3.9832918643951416
I0207 12:27:37.995639 140529927898880 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.17295192182064056, loss=3.9776315689086914
I0207 12:28:13.283294 140529919506176 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.19020001590251923, loss=4.048797607421875
I0207 12:28:48.555954 140529927898880 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.1757376343011856, loss=3.9554026126861572
I0207 12:29:23.799925 140529919506176 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.18894124031066895, loss=3.993326425552368
I0207 12:29:59.042620 140529927898880 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.18926388025283813, loss=4.006588935852051
I0207 12:30:34.308698 140529919506176 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.18104790151119232, loss=4.000056266784668
I0207 12:31:09.556277 140529927898880 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.16939517855644226, loss=3.9683828353881836
I0207 12:31:44.811966 140529919506176 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.1654926836490631, loss=4.0212721824646
I0207 12:32:20.118751 140529927898880 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.27216798067092896, loss=3.9816765785217285
I0207 12:32:55.370131 140529919506176 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.17631439864635468, loss=3.959507942199707
I0207 12:33:30.668160 140529927898880 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.15335950255393982, loss=4.005562782287598
I0207 12:34:06.004001 140529919506176 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.19970615208148956, loss=3.9811806678771973
I0207 12:34:41.251589 140529927898880 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.17875199019908905, loss=3.9685471057891846
I0207 12:35:16.516006 140529919506176 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.19244495034217834, loss=4.023632526397705
I0207 12:35:51.769632 140529927898880 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.1720317006111145, loss=4.060031890869141
I0207 12:36:27.028671 140529919506176 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.16277571022510529, loss=4.041502952575684
I0207 12:37:02.254722 140529927898880 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.17384353280067444, loss=3.935790777206421
I0207 12:37:37.519525 140529919506176 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.16752740740776062, loss=3.9879891872406006
I0207 12:38:12.794758 140529927898880 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.17186732590198517, loss=3.936607837677002
I0207 12:38:48.032532 140529919506176 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.19381198287010193, loss=3.969273567199707
I0207 12:38:55.161593 140699726837568 spec.py:321] Evaluating on the training split.
I0207 12:38:58.195541 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 12:41:40.092717 140699726837568 spec.py:333] Evaluating on the validation split.
I0207 12:41:42.827102 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 12:44:16.182716 140699726837568 spec.py:349] Evaluating on the test split.
I0207 12:44:18.955852 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 12:46:45.963733 140699726837568 submission_runner.py:408] Time since start: 14363.56s, 	Step: 23822, 	{'train/accuracy': 0.643540620803833, 'train/loss': 1.939845085144043, 'train/bleu': 31.37433547834663, 'validation/accuracy': 0.6603513956069946, 'validation/loss': 1.8111510276794434, 'validation/bleu': 28.123673946788806, 'validation/num_examples': 3000, 'test/accuracy': 0.6710127592086792, 'test/loss': 1.7506834268569946, 'test/bleu': 27.898585149515966, 'test/num_examples': 3003, 'score': 8427.87146282196, 'total_duration': 14363.563488960266, 'accumulated_submission_time': 8427.87146282196, 'accumulated_eval_time': 5934.689563751221, 'accumulated_logging_time': 0.2605419158935547}
I0207 12:46:45.982898 140529927898880 logging_writer.py:48] [23822] accumulated_eval_time=5934.689564, accumulated_logging_time=0.260542, accumulated_submission_time=8427.871463, global_step=23822, preemption_count=0, score=8427.871463, test/accuracy=0.671013, test/bleu=27.898585, test/loss=1.750683, test/num_examples=3003, total_duration=14363.563489, train/accuracy=0.643541, train/bleu=31.374335, train/loss=1.939845, validation/accuracy=0.660351, validation/bleu=28.123674, validation/loss=1.811151, validation/num_examples=3000
I0207 12:47:13.731028 140529919506176 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.18764132261276245, loss=4.017829418182373
I0207 12:47:48.945536 140529927898880 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.16287533938884735, loss=3.9203426837921143
I0207 12:48:24.187551 140529919506176 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.16819888353347778, loss=3.9616620540618896
I0207 12:48:59.492928 140529927898880 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.1746167093515396, loss=4.026670455932617
I0207 12:49:34.798013 140529919506176 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.192093625664711, loss=4.030848503112793
I0207 12:50:10.066724 140529927898880 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.19812102615833282, loss=3.9938125610351562
I0207 12:50:45.321266 140529919506176 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.1764557808637619, loss=3.9976913928985596
I0207 12:51:20.586882 140529927898880 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.19821080565452576, loss=3.9880189895629883
I0207 12:51:55.830922 140529919506176 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.21004880964756012, loss=4.018054008483887
I0207 12:52:31.117575 140529927898880 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.20400260388851166, loss=4.014822006225586
I0207 12:53:06.379176 140529919506176 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.17328563332557678, loss=3.9848155975341797
I0207 12:53:41.670922 140529927898880 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.17060866951942444, loss=3.9479870796203613
I0207 12:54:16.918601 140529919506176 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.26018983125686646, loss=4.037556171417236
I0207 12:54:52.190757 140529927898880 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.1784970760345459, loss=3.9769418239593506
I0207 12:55:27.437752 140529919506176 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.17268675565719604, loss=3.9558584690093994
I0207 12:56:02.692227 140529927898880 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.18810085952281952, loss=3.9795708656311035
I0207 12:56:37.947136 140529919506176 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.18105384707450867, loss=3.9960291385650635
I0207 12:57:13.229523 140529927898880 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.17638903856277466, loss=3.9796926975250244
I0207 12:57:48.483541 140529919506176 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.18436545133590698, loss=4.041125774383545
I0207 12:58:23.723731 140529927898880 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.1880277544260025, loss=3.9957785606384277
I0207 12:58:58.991833 140529919506176 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.22400103509426117, loss=3.966338872909546
I0207 12:59:34.215560 140529927898880 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.2961288094520569, loss=3.9898712635040283
I0207 13:00:09.448313 140529919506176 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.1809719204902649, loss=4.003506183624268
I0207 13:00:44.704085 140529927898880 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.1835383027791977, loss=4.011199951171875
I0207 13:00:46.194941 140699726837568 spec.py:321] Evaluating on the training split.
I0207 13:00:49.240396 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 13:04:11.749735 140699726837568 spec.py:333] Evaluating on the validation split.
I0207 13:04:14.483127 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 13:07:00.674644 140699726837568 spec.py:349] Evaluating on the test split.
I0207 13:07:03.425641 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 13:09:36.732175 140699726837568 submission_runner.py:408] Time since start: 15734.33s, 	Step: 26206, 	{'train/accuracy': 0.653814435005188, 'train/loss': 1.8643923997879028, 'train/bleu': 32.415930823155556, 'validation/accuracy': 0.6627939939498901, 'validation/loss': 1.7915291786193848, 'validation/bleu': 28.888755920109194, 'validation/num_examples': 3000, 'test/accuracy': 0.6742897033691406, 'test/loss': 1.722639799118042, 'test/bleu': 28.15439954133418, 'test/num_examples': 3003, 'score': 9268.00053191185, 'total_duration': 15734.331931114197, 'accumulated_submission_time': 9268.00053191185, 'accumulated_eval_time': 6465.22674870491, 'accumulated_logging_time': 0.2896280288696289}
I0207 13:09:36.750637 140529919506176 logging_writer.py:48] [26206] accumulated_eval_time=6465.226749, accumulated_logging_time=0.289628, accumulated_submission_time=9268.000532, global_step=26206, preemption_count=0, score=9268.000532, test/accuracy=0.674290, test/bleu=28.154400, test/loss=1.722640, test/num_examples=3003, total_duration=15734.331931, train/accuracy=0.653814, train/bleu=32.415931, train/loss=1.864392, validation/accuracy=0.662794, validation/bleu=28.888756, validation/loss=1.791529, validation/num_examples=3000
I0207 13:10:10.147607 140529927898880 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.2209787517786026, loss=3.9861361980438232
I0207 13:10:45.334194 140529919506176 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.1808783859014511, loss=3.9241228103637695
I0207 13:11:20.588263 140529927898880 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.18169493973255157, loss=3.9413247108459473
I0207 13:11:55.865148 140529919506176 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.19219788908958435, loss=3.9209728240966797
I0207 13:12:31.194474 140529927898880 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.18591471016407013, loss=3.955681324005127
I0207 13:13:06.429246 140529919506176 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.22602874040603638, loss=6.177408218383789
I0207 13:13:41.631841 140529927898880 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.22640016674995422, loss=5.675031661987305
I0207 13:14:16.816603 140529919506176 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.21961890161037445, loss=5.572152137756348
I0207 13:14:52.063790 140529927898880 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.22532640397548676, loss=5.5760416984558105
I0207 13:15:27.276724 140529919506176 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.23266346752643585, loss=5.495283603668213
I0207 13:16:02.483495 140529927898880 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.5714705586433411, loss=5.5077619552612305
I0207 13:16:37.692989 140529919506176 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.6407503485679626, loss=5.491149425506592
I0207 13:17:12.895901 140529927898880 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.9783444404602051, loss=5.4387593269348145
I0207 13:17:48.142533 140529919506176 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.20043911039829254, loss=4.011094570159912
I0207 13:18:23.449036 140529927898880 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.20015443861484528, loss=3.9939048290252686
I0207 13:18:58.737497 140529919506176 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.19473643600940704, loss=3.9585444927215576
I0207 13:19:33.989047 140529927898880 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.1881643533706665, loss=3.9920010566711426
I0207 13:20:09.251559 140529919506176 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.18675976991653442, loss=4.001220226287842
I0207 13:20:44.508910 140529927898880 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.21085864305496216, loss=3.9844441413879395
I0207 13:21:19.782440 140529919506176 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.18389247357845306, loss=4.000640869140625
I0207 13:21:55.029428 140529927898880 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.1946961134672165, loss=3.9624977111816406
I0207 13:22:30.320970 140529919506176 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.2367411106824875, loss=3.9713938236236572
I0207 13:23:05.569527 140529927898880 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.17424215376377106, loss=3.9420855045318604
I0207 13:23:37.005645 140699726837568 spec.py:321] Evaluating on the training split.
I0207 13:23:40.038552 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 13:26:27.602156 140699726837568 spec.py:333] Evaluating on the validation split.
I0207 13:26:30.338383 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 13:29:16.836336 140699726837568 spec.py:349] Evaluating on the test split.
I0207 13:29:19.568785 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 13:31:54.390287 140699726837568 submission_runner.py:408] Time since start: 17071.99s, 	Step: 28591, 	{'train/accuracy': 0.6467625498771667, 'train/loss': 1.921960711479187, 'train/bleu': 31.52587145991113, 'validation/accuracy': 0.6663525700569153, 'validation/loss': 1.796502947807312, 'validation/bleu': 28.858447484971762, 'validation/num_examples': 3000, 'test/accuracy': 0.6772529482841492, 'test/loss': 1.7303847074508667, 'test/bleu': 28.447757417888468, 'test/num_examples': 3003, 'score': 10108.170245409012, 'total_duration': 17071.990015506744, 'accumulated_submission_time': 10108.170245409012, 'accumulated_eval_time': 6962.611318826675, 'accumulated_logging_time': 0.31827235221862793}
I0207 13:31:54.408876 140529919506176 logging_writer.py:48] [28591] accumulated_eval_time=6962.611319, accumulated_logging_time=0.318272, accumulated_submission_time=10108.170245, global_step=28591, preemption_count=0, score=10108.170245, test/accuracy=0.677253, test/bleu=28.447757, test/loss=1.730385, test/num_examples=3003, total_duration=17071.990016, train/accuracy=0.646763, train/bleu=31.525871, train/loss=1.921961, validation/accuracy=0.666353, validation/bleu=28.858447, validation/loss=1.796503, validation/num_examples=3000
I0207 13:31:57.936195 140529927898880 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.24884919822216034, loss=3.957373857498169
I0207 13:32:33.067124 140529919506176 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.26186180114746094, loss=3.9139020442962646
I0207 13:33:08.255121 140529927898880 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.2071789652109146, loss=3.9914753437042236
I0207 13:33:43.507129 140529919506176 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.24586328864097595, loss=3.918015480041504
I0207 13:34:18.802662 140529927898880 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.19511014223098755, loss=4.035495758056641
I0207 13:34:54.053108 140529919506176 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.240111842751503, loss=3.902606964111328
I0207 13:35:29.295277 140529927898880 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.27084189653396606, loss=3.9585962295532227
I0207 13:36:04.575862 140529919506176 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.18438221514225006, loss=3.9599246978759766
I0207 13:36:39.835832 140529927898880 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.18491120636463165, loss=4.0432939529418945
I0207 13:37:15.087507 140529919506176 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.1826331913471222, loss=4.019321441650391
I0207 13:37:50.364359 140529927898880 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.21997207403182983, loss=3.9666199684143066
I0207 13:38:25.656806 140529919506176 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.2728211581707001, loss=3.9547386169433594
I0207 13:39:00.982426 140529927898880 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.18760906159877777, loss=3.938157558441162
I0207 13:39:36.296683 140529919506176 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.2841077744960785, loss=3.937530040740967
I0207 13:40:11.664105 140529927898880 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.18659557402133942, loss=3.911259412765503
I0207 13:40:46.909344 140529919506176 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.19810058176517487, loss=3.9375405311584473
I0207 13:41:22.214668 140529927898880 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.1958846151828766, loss=3.9812891483306885
I0207 13:41:57.479348 140529919506176 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.192037433385849, loss=3.9591989517211914
I0207 13:42:32.748558 140529927898880 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.245771124958992, loss=3.939803123474121
I0207 13:43:07.989984 140529919506176 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.20590434968471527, loss=3.942223310470581
I0207 13:43:43.263165 140529927898880 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.24264302849769592, loss=3.9798789024353027
I0207 13:44:18.542043 140529919506176 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.25789719820022583, loss=3.9964888095855713
I0207 13:44:53.820960 140529927898880 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.1857910007238388, loss=3.966254949569702
I0207 13:45:29.074606 140529919506176 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.19645121693611145, loss=3.864366054534912
I0207 13:45:54.532922 140699726837568 spec.py:321] Evaluating on the training split.
I0207 13:45:57.566420 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 13:49:03.256057 140699726837568 spec.py:333] Evaluating on the validation split.
I0207 13:49:05.999694 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 13:51:48.716533 140699726837568 spec.py:349] Evaluating on the test split.
I0207 13:51:51.458264 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 13:54:23.209672 140699726837568 submission_runner.py:408] Time since start: 18420.81s, 	Step: 30974, 	{'train/accuracy': 0.6443696022033691, 'train/loss': 1.9311410188674927, 'train/bleu': 31.986524329560304, 'validation/accuracy': 0.6660549640655518, 'validation/loss': 1.7905811071395874, 'validation/bleu': 28.60632817214931, 'validation/num_examples': 3000, 'test/accuracy': 0.6762768030166626, 'test/loss': 1.725675344467163, 'test/bleu': 27.87737284091013, 'test/num_examples': 3003, 'score': 10948.20868062973, 'total_duration': 18420.80942606926, 'accumulated_submission_time': 10948.20868062973, 'accumulated_eval_time': 7471.288013458252, 'accumulated_logging_time': 0.3465893268585205}
I0207 13:54:23.228703 140529927898880 logging_writer.py:48] [30974] accumulated_eval_time=7471.288013, accumulated_logging_time=0.346589, accumulated_submission_time=10948.208681, global_step=30974, preemption_count=0, score=10948.208681, test/accuracy=0.676277, test/bleu=27.877373, test/loss=1.725675, test/num_examples=3003, total_duration=18420.809426, train/accuracy=0.644370, train/bleu=31.986524, train/loss=1.931141, validation/accuracy=0.666055, validation/bleu=28.606328, validation/loss=1.790581, validation/num_examples=3000
I0207 13:54:32.737357 140529919506176 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.25260937213897705, loss=3.9917006492614746
I0207 13:55:07.934820 140529927898880 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.19003325700759888, loss=3.9223904609680176
I0207 13:55:43.151904 140529919506176 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.2310144454240799, loss=3.9437308311462402
I0207 13:56:18.401015 140529927898880 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.19946329295635223, loss=4.000044345855713
I0207 13:56:53.615211 140529919506176 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.2021765559911728, loss=3.968122959136963
I0207 13:57:28.863194 140529927898880 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.20490700006484985, loss=3.9466912746429443
I0207 13:58:04.131165 140529919506176 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.19438208639621735, loss=3.912966012954712
I0207 13:58:39.376519 140529927898880 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.21778836846351624, loss=3.9545514583587646
I0207 13:59:14.646759 140529919506176 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.2538154125213623, loss=3.845075845718384
I0207 13:59:49.921027 140529927898880 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.3087797164916992, loss=3.9504470825195312
I0207 14:00:25.173025 140529919506176 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.20622466504573822, loss=3.959275245666504
I0207 14:01:00.417429 140529927898880 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.18890449404716492, loss=3.919802665710449
I0207 14:01:35.700466 140529919506176 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.22104118764400482, loss=3.94830584526062
I0207 14:02:10.984411 140529927898880 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.1996709406375885, loss=3.9710612297058105
I0207 14:02:46.239362 140529919506176 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.21789602935314178, loss=3.915515899658203
I0207 14:03:21.526931 140529927898880 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.21401144564151764, loss=3.9162044525146484
I0207 14:03:56.793141 140529919506176 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.19826646149158478, loss=3.9734036922454834
I0207 14:04:32.069615 140529927898880 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.2103588730096817, loss=3.959388256072998
I0207 14:05:07.317998 140529919506176 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.20398645102977753, loss=3.9877400398254395
I0207 14:05:42.587434 140529927898880 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.21832579374313354, loss=3.922398567199707
I0207 14:06:17.958948 140529919506176 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.21960803866386414, loss=3.9116296768188477
I0207 14:06:53.230223 140529927898880 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.19125108420848846, loss=3.9798989295959473
I0207 14:07:28.495265 140529919506176 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.268888384103775, loss=3.9446887969970703
I0207 14:08:03.739572 140529927898880 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.19961358606815338, loss=3.924851655960083
I0207 14:08:23.213129 140699726837568 spec.py:321] Evaluating on the training split.
I0207 14:08:26.263545 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 14:13:12.536063 140699726837568 spec.py:333] Evaluating on the validation split.
I0207 14:13:15.287447 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 14:16:36.162725 140699726837568 spec.py:349] Evaluating on the test split.
I0207 14:16:38.916545 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 14:19:20.342340 140699726837568 submission_runner.py:408] Time since start: 19917.94s, 	Step: 33357, 	{'train/accuracy': 0.6579074263572693, 'train/loss': 1.8461037874221802, 'train/bleu': 32.069965372459464, 'validation/accuracy': 0.6691919565200806, 'validation/loss': 1.755408525466919, 'validation/bleu': 29.00561299490477, 'validation/num_examples': 3000, 'test/accuracy': 0.680727481842041, 'test/loss': 1.6848061084747314, 'test/bleu': 28.342088839250426, 'test/num_examples': 3003, 'score': 11788.108996391296, 'total_duration': 19917.942067861557, 'accumulated_submission_time': 11788.108996391296, 'accumulated_eval_time': 8128.417171955109, 'accumulated_logging_time': 0.37586474418640137}
I0207 14:19:20.361716 140529919506176 logging_writer.py:48] [33357] accumulated_eval_time=8128.417172, accumulated_logging_time=0.375865, accumulated_submission_time=11788.108996, global_step=33357, preemption_count=0, score=11788.108996, test/accuracy=0.680727, test/bleu=28.342089, test/loss=1.684806, test/num_examples=3003, total_duration=19917.942068, train/accuracy=0.657907, train/bleu=32.069965, train/loss=1.846104, validation/accuracy=0.669192, validation/bleu=29.005613, validation/loss=1.755409, validation/num_examples=3000
I0207 14:19:35.835306 140529927898880 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.20930767059326172, loss=3.958062171936035
I0207 14:20:10.990245 140529919506176 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.1921158730983734, loss=3.9461724758148193
I0207 14:20:46.248291 140529927898880 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.23739878833293915, loss=3.923314094543457
I0207 14:21:21.477010 140529919506176 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.18414433300495148, loss=3.8513407707214355
I0207 14:21:56.765968 140529927898880 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.20870070159435272, loss=3.865983247756958
I0207 14:22:32.028266 140529919506176 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.3244733214378357, loss=3.9255759716033936
I0207 14:23:07.335805 140529927898880 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.23777133226394653, loss=3.927684783935547
I0207 14:23:42.643526 140529919506176 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.2273164838552475, loss=3.9028866291046143
I0207 14:24:17.941781 140529927898880 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.2023845762014389, loss=3.8761379718780518
I0207 14:24:53.197114 140529919506176 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.2233595997095108, loss=3.946669101715088
I0207 14:25:28.450649 140529927898880 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.20091846585273743, loss=3.928251266479492
I0207 14:26:03.705526 140529919506176 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.19553124904632568, loss=3.9261231422424316
I0207 14:26:38.987902 140529927898880 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.2961352467536926, loss=3.8769521713256836
I0207 14:27:14.269357 140529919506176 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.2064240276813507, loss=3.9759891033172607
I0207 14:27:49.590484 140529927898880 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.19064246118068695, loss=3.9145634174346924
I0207 14:28:24.881986 140529919506176 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.1922483891248703, loss=3.88187313079834
I0207 14:29:00.141834 140529927898880 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.24262821674346924, loss=3.9682726860046387
I0207 14:29:35.402148 140529919506176 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.2224215269088745, loss=3.9419896602630615
I0207 14:30:10.680444 140529927898880 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.2325049191713333, loss=3.923173427581787
I0207 14:30:45.934822 140529919506176 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.19168467819690704, loss=3.8911094665527344
I0207 14:31:21.193309 140529927898880 logging_writer.py:48] [35400] global_step=35400, grad_norm=2.504080295562744, loss=5.064643859863281
I0207 14:31:56.453321 140529919506176 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.22802774608135223, loss=4.053130626678467
I0207 14:32:31.696784 140529927898880 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.22008368372917175, loss=3.9498589038848877
I0207 14:33:06.970965 140529919506176 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.20354393124580383, loss=3.9340267181396484
I0207 14:33:20.468062 140699726837568 spec.py:321] Evaluating on the training split.
I0207 14:33:23.507667 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 14:36:10.091900 140699726837568 spec.py:333] Evaluating on the validation split.
I0207 14:36:12.850856 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 14:38:52.724821 140699726837568 spec.py:349] Evaluating on the test split.
I0207 14:38:55.474918 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 14:41:17.225641 140699726837568 submission_runner.py:408] Time since start: 21234.83s, 	Step: 35740, 	{'train/accuracy': 0.6511590480804443, 'train/loss': 1.8901034593582153, 'train/bleu': 32.00241740281816, 'validation/accuracy': 0.6663649678230286, 'validation/loss': 1.756929874420166, 'validation/bleu': 29.13012192945443, 'validation/num_examples': 3000, 'test/accuracy': 0.6793214082717896, 'test/loss': 1.6866551637649536, 'test/bleu': 28.543354366147703, 'test/num_examples': 3003, 'score': 12628.127183675766, 'total_duration': 21234.825357437134, 'accumulated_submission_time': 12628.127183675766, 'accumulated_eval_time': 8605.174662351608, 'accumulated_logging_time': 0.40660762786865234}
I0207 14:41:17.249776 140529927898880 logging_writer.py:48] [35740] accumulated_eval_time=8605.174662, accumulated_logging_time=0.406608, accumulated_submission_time=12628.127184, global_step=35740, preemption_count=0, score=12628.127184, test/accuracy=0.679321, test/bleu=28.543354, test/loss=1.686655, test/num_examples=3003, total_duration=21234.825357, train/accuracy=0.651159, train/bleu=32.002417, train/loss=1.890103, validation/accuracy=0.666365, validation/bleu=29.130122, validation/loss=1.756930, validation/num_examples=3000
I0207 14:41:38.731466 140529919506176 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.21498586237430573, loss=4.002870082855225
I0207 14:42:13.932910 140529927898880 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.2098783254623413, loss=3.9382522106170654
I0207 14:42:49.187708 140529919506176 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.21228288114070892, loss=3.893049955368042
I0207 14:43:24.446280 140529927898880 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.21780407428741455, loss=3.9523186683654785
I0207 14:43:59.660873 140529919506176 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.25843918323516846, loss=3.9285686016082764
I0207 14:44:34.909152 140529927898880 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.19779233634471893, loss=3.9766640663146973
I0207 14:45:10.151518 140529919506176 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.20446966588497162, loss=3.922551393508911
I0207 14:45:45.437151 140529927898880 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.19834376871585846, loss=3.8563172817230225
I0207 14:46:20.780377 140529919506176 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.18898051977157593, loss=3.8800578117370605
I0207 14:46:56.096256 140529927898880 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.18764393031597137, loss=3.877229690551758
I0207 14:47:31.417127 140529919506176 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.2318938821554184, loss=3.935534954071045
I0207 14:48:06.701741 140529927898880 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.20349855720996857, loss=3.9666378498077393
I0207 14:48:41.976688 140529919506176 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.19810380041599274, loss=3.904095411300659
I0207 14:49:17.278465 140529927898880 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.2694421410560608, loss=3.947822093963623
I0207 14:49:52.581495 140529919506176 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.2909083068370819, loss=3.9571449756622314
I0207 14:50:27.873906 140529927898880 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.2655613422393799, loss=3.914137363433838
I0207 14:51:03.181044 140529919506176 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.21031233668327332, loss=3.865668296813965
I0207 14:51:38.441530 140529927898880 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.3042711913585663, loss=3.9170315265655518
I0207 14:52:13.710449 140529919506176 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.22094205021858215, loss=3.895341634750366
I0207 14:52:48.951384 140529927898880 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.25615164637565613, loss=3.8217146396636963
I0207 14:53:24.223673 140529919506176 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.30636849999427795, loss=3.896185874938965
I0207 14:53:59.465965 140529927898880 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.2678067684173584, loss=3.893033742904663
I0207 14:54:34.714683 140529919506176 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.20671752095222473, loss=3.9002790451049805
I0207 14:55:09.993063 140529927898880 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.2193109095096588, loss=3.9298996925354004
I0207 14:55:17.479171 140699726837568 spec.py:321] Evaluating on the training split.
I0207 14:55:20.528496 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 14:59:28.093902 140699726837568 spec.py:333] Evaluating on the validation split.
I0207 14:59:30.842998 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 15:02:23.697772 140699726837568 spec.py:349] Evaluating on the test split.
I0207 15:02:26.432673 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 15:04:44.792049 140699726837568 submission_runner.py:408] Time since start: 22642.39s, 	Step: 38123, 	{'train/accuracy': 0.6650872230529785, 'train/loss': 1.7879432439804077, 'train/bleu': 33.173089200434895, 'validation/accuracy': 0.6712377667427063, 'validation/loss': 1.7390223741531372, 'validation/bleu': 29.3184200998274, 'validation/num_examples': 3000, 'test/accuracy': 0.6831677556037903, 'test/loss': 1.668337106704712, 'test/bleu': 28.572843907960156, 'test/num_examples': 3003, 'score': 13468.265769004822, 'total_duration': 22642.391790390015, 'accumulated_submission_time': 13468.265769004822, 'accumulated_eval_time': 9172.487488031387, 'accumulated_logging_time': 0.4418942928314209}
I0207 15:04:44.814632 140529919506176 logging_writer.py:48] [38123] accumulated_eval_time=9172.487488, accumulated_logging_time=0.441894, accumulated_submission_time=13468.265769, global_step=38123, preemption_count=0, score=13468.265769, test/accuracy=0.683168, test/bleu=28.572844, test/loss=1.668337, test/num_examples=3003, total_duration=22642.391790, train/accuracy=0.665087, train/bleu=33.173089, train/loss=1.787943, validation/accuracy=0.671238, validation/bleu=29.318420, validation/loss=1.739022, validation/num_examples=3000
I0207 15:05:12.266910 140529927898880 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.2185012847185135, loss=3.907482624053955
I0207 15:05:47.436692 140529919506176 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.2194414734840393, loss=3.8298211097717285
I0207 15:06:22.667557 140529927898880 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.25912708044052124, loss=3.9258553981781006
I0207 15:06:57.934928 140529919506176 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.24778752028942108, loss=3.868039131164551
I0207 15:07:33.179786 140529927898880 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.19824685156345367, loss=3.8689374923706055
I0207 15:08:08.406309 140529919506176 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.23179323971271515, loss=3.847465753555298
I0207 15:08:43.666645 140529927898880 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.22082825005054474, loss=3.920022487640381
I0207 15:09:18.957571 140529919506176 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.2440020740032196, loss=3.8689677715301514
I0207 15:09:54.225764 140529927898880 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.22380231320858002, loss=3.9810779094696045
I0207 15:10:29.499145 140529919506176 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.22554157674312592, loss=3.9177629947662354
I0207 15:11:04.825025 140529927898880 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.22527766227722168, loss=3.90803599357605
I0207 15:11:40.091203 140529919506176 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.22859834134578705, loss=3.9263603687286377
I0207 15:12:15.345762 140529927898880 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.2268044501543045, loss=3.971601963043213
I0207 15:12:50.609963 140529919506176 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.21654687821865082, loss=3.917405366897583
I0207 15:13:25.898002 140529927898880 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.25557398796081543, loss=3.895102024078369
I0207 15:14:01.168270 140529919506176 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.2849665582180023, loss=3.876605272293091
I0207 15:14:36.464068 140529927898880 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.255403995513916, loss=3.9153239727020264
I0207 15:15:11.743458 140529919506176 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.23389682173728943, loss=3.9041147232055664
I0207 15:15:46.989476 140529927898880 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.22041048109531403, loss=3.963947057723999
I0207 15:16:22.247317 140529919506176 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.2384209781885147, loss=3.9063022136688232
I0207 15:16:57.489108 140529927898880 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.2219385951757431, loss=3.8851332664489746
I0207 15:17:32.763117 140529919506176 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.24358941614627838, loss=3.892977714538574
I0207 15:18:08.057460 140529927898880 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.24692302942276, loss=3.9068281650543213
I0207 15:18:43.326215 140529919506176 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.22588029503822327, loss=3.913761615753174
I0207 15:18:44.810952 140699726837568 spec.py:321] Evaluating on the training split.
I0207 15:18:47.849978 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 15:22:01.501065 140699726837568 spec.py:333] Evaluating on the validation split.
I0207 15:22:04.253260 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 15:24:49.182449 140699726837568 spec.py:349] Evaluating on the test split.
I0207 15:24:51.935101 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 15:27:22.203746 140699726837568 submission_runner.py:408] Time since start: 23999.80s, 	Step: 40506, 	{'train/accuracy': 0.6558361649513245, 'train/loss': 1.8276304006576538, 'train/bleu': 32.338058333076155, 'validation/accuracy': 0.6708658337593079, 'validation/loss': 1.7183750867843628, 'validation/bleu': 29.019836053244, 'validation/num_examples': 3000, 'test/accuracy': 0.6837139129638672, 'test/loss': 1.6359859704971313, 'test/bleu': 28.80755072318091, 'test/num_examples': 3003, 'score': 14308.177471637726, 'total_duration': 23999.80347084999, 'accumulated_submission_time': 14308.177471637726, 'accumulated_eval_time': 9689.880198001862, 'accumulated_logging_time': 0.47484350204467773}
I0207 15:27:22.228448 140529927898880 logging_writer.py:48] [40506] accumulated_eval_time=9689.880198, accumulated_logging_time=0.474844, accumulated_submission_time=14308.177472, global_step=40506, preemption_count=0, score=14308.177472, test/accuracy=0.683714, test/bleu=28.807551, test/loss=1.635986, test/num_examples=3003, total_duration=23999.803471, train/accuracy=0.655836, train/bleu=32.338058, train/loss=1.827630, validation/accuracy=0.670866, validation/bleu=29.019836, validation/loss=1.718375, validation/num_examples=3000
I0207 15:27:55.654449 140529919506176 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.23445747792720795, loss=3.9135122299194336
I0207 15:28:30.842205 140529927898880 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.23307284712791443, loss=3.888289213180542
I0207 15:29:06.099081 140529919506176 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.3148048222064972, loss=3.8762283325195312
I0207 15:29:41.368053 140529927898880 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.22872664034366608, loss=3.8416213989257812
I0207 15:30:16.646585 140529919506176 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.21160297095775604, loss=3.892515182495117
I0207 15:30:51.947259 140529927898880 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.2408962994813919, loss=3.9443788528442383
I0207 15:31:27.250495 140529919506176 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.2565087080001831, loss=3.918600082397461
I0207 15:32:02.520212 140529927898880 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.2327292114496231, loss=3.895399570465088
I0207 15:32:37.782748 140529919506176 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.22760599851608276, loss=3.8557116985321045
I0207 15:33:13.019976 140529927898880 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.21453391015529633, loss=3.8716769218444824
I0207 15:33:48.309518 140529919506176 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.2628017067909241, loss=3.9080588817596436
I0207 15:34:23.576817 140529927898880 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.37801501154899597, loss=3.947840690612793
I0207 15:34:58.870367 140529919506176 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.23084890842437744, loss=3.945342540740967
I0207 15:35:34.146800 140529927898880 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.22319437563419342, loss=3.92467999458313
I0207 15:36:09.447819 140529919506176 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.2468128651380539, loss=3.904242515563965
I0207 15:36:44.710113 140529927898880 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.2280302196741104, loss=3.8977725505828857
I0207 15:37:20.011158 140529919506176 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.22667334973812103, loss=3.906285047531128
I0207 15:37:55.256793 140529927898880 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.21659421920776367, loss=3.882683277130127
I0207 15:38:30.515797 140529919506176 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.2362753301858902, loss=3.8842544555664062
I0207 15:39:05.771036 140529927898880 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.219096839427948, loss=3.869647264480591
I0207 15:39:41.048778 140529919506176 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.24628134071826935, loss=3.8612232208251953
I0207 15:40:16.327701 140529927898880 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.22814124822616577, loss=3.889486312866211
I0207 15:40:51.573584 140529919506176 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.24994711577892303, loss=3.888315200805664
I0207 15:41:22.322268 140699726837568 spec.py:321] Evaluating on the training split.
I0207 15:41:25.349724 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 15:45:24.727800 140699726837568 spec.py:333] Evaluating on the validation split.
I0207 15:45:27.454832 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 15:49:11.855199 140699726837568 spec.py:349] Evaluating on the test split.
I0207 15:49:14.594746 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 15:53:02.171035 140699726837568 submission_runner.py:408] Time since start: 25539.77s, 	Step: 42889, 	{'train/accuracy': 0.32560446858406067, 'train/loss': 4.233160018920898, 'train/bleu': 0.1383137865844718, 'validation/accuracy': 0.3041127920150757, 'validation/loss': 4.484804153442383, 'validation/bleu': 0.06474557629339085, 'validation/num_examples': 3000, 'test/accuracy': 0.3018534779548645, 'test/loss': 4.571069240570068, 'test/bleu': 0.033897742842188085, 'test/num_examples': 3003, 'score': 15148.184902191162, 'total_duration': 25539.770740509033, 'accumulated_submission_time': 15148.184902191162, 'accumulated_eval_time': 10389.728868246078, 'accumulated_logging_time': 0.51078200340271}
I0207 15:53:02.196770 140529927898880 logging_writer.py:48] [42889] accumulated_eval_time=10389.728868, accumulated_logging_time=0.510782, accumulated_submission_time=15148.184902, global_step=42889, preemption_count=0, score=15148.184902, test/accuracy=0.301853, test/bleu=0.033898, test/loss=4.571069, test/num_examples=3003, total_duration=25539.770741, train/accuracy=0.325604, train/bleu=0.138314, train/loss=4.233160, validation/accuracy=0.304113, validation/bleu=0.064746, validation/loss=4.484804, validation/num_examples=3000
I0207 15:53:06.422658 140529919506176 logging_writer.py:48] [42900] global_step=42900, grad_norm=9.599933624267578, loss=9.042530059814453
I0207 15:53:41.522477 140529927898880 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.33919310569763184, loss=5.574984550476074
I0207 15:54:16.802503 140529919506176 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.6890671849250793, loss=5.480607032775879
I0207 15:54:52.046413 140529927898880 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.26027148962020874, loss=5.46267557144165
I0207 15:55:27.305018 140529919506176 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.30914169549942017, loss=5.4719319343566895
I0207 15:56:02.560584 140529927898880 logging_writer.py:48] [43400] global_step=43400, grad_norm=1.891618013381958, loss=5.385476112365723
I0207 15:56:37.795110 140529919506176 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.9063915610313416, loss=4.06251335144043
I0207 15:57:13.111860 140529927898880 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.23844583332538605, loss=3.8934457302093506
I0207 15:57:48.433037 140529919506176 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.27480876445770264, loss=3.9244935512542725
I0207 15:58:23.706967 140529927898880 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.24581201374530792, loss=3.9054079055786133
I0207 15:58:58.955540 140529919506176 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.20962031185626984, loss=3.877131223678589
I0207 15:59:34.213297 140529927898880 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.21973896026611328, loss=3.869231939315796
I0207 16:00:09.506633 140529919506176 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.22922050952911377, loss=3.862785816192627
I0207 16:00:44.818398 140529927898880 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.2285395860671997, loss=3.940335273742676
I0207 16:01:20.082923 140529919506176 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.21746879816055298, loss=3.8695311546325684
I0207 16:01:55.358708 140529927898880 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.2020895779132843, loss=3.895258903503418
I0207 16:02:30.628299 140529919506176 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.21107158064842224, loss=3.9051613807678223
I0207 16:03:05.916895 140529927898880 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.22033585608005524, loss=3.8965542316436768
I0207 16:03:41.248879 140529919506176 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.21355745196342468, loss=3.812527894973755
I0207 16:04:16.544724 140529927898880 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.2201162576675415, loss=3.8519446849823
I0207 16:04:51.786186 140529919506176 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.23369848728179932, loss=4.001601696014404
I0207 16:05:27.054551 140529927898880 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.2961784601211548, loss=3.8425967693328857
I0207 16:06:02.334875 140529919506176 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.24828560650348663, loss=3.8691885471343994
I0207 16:06:37.606895 140529927898880 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.2318066507577896, loss=3.915309190750122
I0207 16:07:02.377409 140699726837568 spec.py:321] Evaluating on the training split.
I0207 16:07:05.407966 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 16:09:50.920773 140699726837568 spec.py:333] Evaluating on the validation split.
I0207 16:09:53.656388 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 16:12:28.311534 140699726837568 spec.py:349] Evaluating on the test split.
I0207 16:12:31.064854 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 16:14:50.000987 140699726837568 submission_runner.py:408] Time since start: 26847.60s, 	Step: 45272, 	{'train/accuracy': 0.6611015796661377, 'train/loss': 1.7925008535385132, 'train/bleu': 32.56452484565516, 'validation/accuracy': 0.6752551198005676, 'validation/loss': 1.6979889869689941, 'validation/bleu': 29.620418156833455, 'validation/num_examples': 3000, 'test/accuracy': 0.6868398189544678, 'test/loss': 1.6206845045089722, 'test/bleu': 28.890927730804112, 'test/num_examples': 3003, 'score': 15988.277658700943, 'total_duration': 26847.600742578506, 'accumulated_submission_time': 15988.277658700943, 'accumulated_eval_time': 10857.352401733398, 'accumulated_logging_time': 0.5476312637329102}
I0207 16:14:50.022119 140529919506176 logging_writer.py:48] [45272] accumulated_eval_time=10857.352402, accumulated_logging_time=0.547631, accumulated_submission_time=15988.277659, global_step=45272, preemption_count=0, score=15988.277659, test/accuracy=0.686840, test/bleu=28.890928, test/loss=1.620685, test/num_examples=3003, total_duration=26847.600743, train/accuracy=0.661102, train/bleu=32.564525, train/loss=1.792501, validation/accuracy=0.675255, validation/bleu=29.620418, validation/loss=1.697989, validation/num_examples=3000
I0207 16:15:00.235362 140529927898880 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.2838478982448578, loss=3.8735642433166504
I0207 16:15:35.402819 140529919506176 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.24916121363639832, loss=3.908597230911255
I0207 16:16:10.632665 140529927898880 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.27054986357688904, loss=3.9560179710388184
I0207 16:16:45.856067 140529919506176 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.26024821400642395, loss=3.908777952194214
I0207 16:17:21.099031 140529927898880 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.20943808555603027, loss=3.9006636142730713
I0207 16:17:56.365362 140529919506176 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.22103211283683777, loss=3.8880276679992676
I0207 16:18:31.607990 140529927898880 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.2384217083454132, loss=3.9469351768493652
I0207 16:19:06.889796 140529919506176 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.26611384749412537, loss=3.857452154159546
I0207 16:19:42.161521 140529927898880 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.22548405826091766, loss=3.9437975883483887
I0207 16:20:17.437741 140529919506176 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.2251175194978714, loss=3.889744997024536
I0207 16:20:52.749438 140529927898880 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.22609825432300568, loss=3.8872363567352295
I0207 16:21:27.996580 140529919506176 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.21734704077243805, loss=3.8659448623657227
I0207 16:22:03.267973 140529927898880 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.21654865145683289, loss=3.839164972305298
I0207 16:22:38.548033 140529919506176 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.38347506523132324, loss=3.870060682296753
I0207 16:23:13.805873 140529927898880 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.3207213878631592, loss=3.8827028274536133
I0207 16:23:49.080561 140529919506176 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.24754014611244202, loss=3.888252019882202
I0207 16:24:24.315029 140529927898880 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.233446404337883, loss=3.876668691635132
I0207 16:24:59.572722 140529919506176 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.24414992332458496, loss=3.9555110931396484
I0207 16:25:34.841797 140529927898880 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.24140873551368713, loss=3.950962543487549
I0207 16:26:10.103797 140529919506176 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.23129956424236298, loss=3.934171438217163
I0207 16:26:45.354419 140529927898880 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.2344890981912613, loss=3.8233590126037598
I0207 16:27:20.647312 140529919506176 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.23595789074897766, loss=3.855912446975708
I0207 16:27:55.946658 140529927898880 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.26648658514022827, loss=3.9059712886810303
I0207 16:28:31.232769 140529919506176 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.23217688500881195, loss=3.8337714672088623
I0207 16:28:50.338602 140699726837568 spec.py:321] Evaluating on the training split.
I0207 16:28:53.373500 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 16:32:14.219300 140699726837568 spec.py:333] Evaluating on the validation split.
I0207 16:32:16.946353 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 16:34:56.676744 140699726837568 spec.py:349] Evaluating on the test split.
I0207 16:34:59.415282 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 16:37:20.951079 140699726837568 submission_runner.py:408] Time since start: 28198.55s, 	Step: 47656, 	{'train/accuracy': 0.6571767330169678, 'train/loss': 1.834312081336975, 'train/bleu': 32.09281506764354, 'validation/accuracy': 0.6745359301567078, 'validation/loss': 1.7079156637191772, 'validation/bleu': 29.54546456908649, 'validation/num_examples': 3000, 'test/accuracy': 0.6875022053718567, 'test/loss': 1.6302539110183716, 'test/bleu': 29.12670142648821, 'test/num_examples': 3003, 'score': 16828.508974790573, 'total_duration': 28198.55082011223, 'accumulated_submission_time': 16828.508974790573, 'accumulated_eval_time': 11367.964815616608, 'accumulated_logging_time': 0.5800197124481201}
I0207 16:37:20.972330 140529927898880 logging_writer.py:48] [47656] accumulated_eval_time=11367.964816, accumulated_logging_time=0.580020, accumulated_submission_time=16828.508975, global_step=47656, preemption_count=0, score=16828.508975, test/accuracy=0.687502, test/bleu=29.126701, test/loss=1.630254, test/num_examples=3003, total_duration=28198.550820, train/accuracy=0.657177, train/bleu=32.092815, train/loss=1.834312, validation/accuracy=0.674536, validation/bleu=29.545465, validation/loss=1.707916, validation/num_examples=3000
I0207 16:37:36.793375 140529919506176 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.291838675737381, loss=3.930251121520996
I0207 16:38:12.015953 140529927898880 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.27568668127059937, loss=3.873382806777954
I0207 16:38:47.208508 140529919506176 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.2706969380378723, loss=3.8964316844940186
I0207 16:39:22.427814 140529927898880 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.26958590745925903, loss=3.8635735511779785
I0207 16:39:57.685348 140529919506176 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.2557090222835541, loss=3.819749116897583
I0207 16:40:32.985998 140529927898880 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.29135656356811523, loss=3.8897743225097656
I0207 16:41:08.237223 140529919506176 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.23542650043964386, loss=3.905189037322998
I0207 16:41:43.527483 140529927898880 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.25135180354118347, loss=3.900895595550537
I0207 16:42:18.774506 140529919506176 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.27644363045692444, loss=3.876142740249634
I0207 16:42:54.083476 140529927898880 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.23618009686470032, loss=3.8852906227111816
I0207 16:43:29.341608 140529919506176 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.29416099190711975, loss=3.9113361835479736
I0207 16:44:04.640322 140529927898880 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.23020613193511963, loss=3.8388819694519043
I0207 16:44:39.931051 140529919506176 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.24692432582378387, loss=3.926175594329834
I0207 16:45:15.235564 140529927898880 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.22905752062797546, loss=3.8770864009857178
I0207 16:45:50.506406 140529919506176 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.22322620451450348, loss=3.869833469390869
I0207 16:46:25.802735 140529927898880 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.22583413124084473, loss=3.8694756031036377
I0207 16:47:01.056768 140529919506176 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.24745315313339233, loss=3.9026153087615967
I0207 16:47:36.329578 140529927898880 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.25235262513160706, loss=3.923799514770508
I0207 16:48:11.599373 140529919506176 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.2490827888250351, loss=3.849708080291748
I0207 16:48:46.850527 140529927898880 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.23864628374576569, loss=3.8929123878479004
I0207 16:49:22.102855 140529919506176 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.2468746304512024, loss=3.9278879165649414
I0207 16:49:57.405486 140529927898880 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.25636184215545654, loss=3.9444046020507812
I0207 16:50:32.641982 140529919506176 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.2532683312892914, loss=3.9505865573883057
I0207 16:51:07.938125 140529927898880 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.22121493518352509, loss=3.883208990097046
I0207 16:51:21.078364 140699726837568 spec.py:321] Evaluating on the training split.
I0207 16:51:24.110913 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 16:55:40.809923 140699726837568 spec.py:333] Evaluating on the validation split.
I0207 16:55:43.557743 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 16:58:17.745090 140699726837568 spec.py:349] Evaluating on the test split.
I0207 16:58:20.486646 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 17:00:56.526649 140699726837568 submission_runner.py:408] Time since start: 29614.13s, 	Step: 50039, 	{'train/accuracy': 0.70138019323349, 'train/loss': 1.5984097719192505, 'train/bleu': 35.59088268760344, 'validation/accuracy': 0.6770529747009277, 'validation/loss': 1.7046748399734497, 'validation/bleu': 29.584613023929922, 'validation/num_examples': 3000, 'test/accuracy': 0.6908721327781677, 'test/loss': 1.6230562925338745, 'test/bleu': 29.203659503289817, 'test/num_examples': 3003, 'score': 17668.53111076355, 'total_duration': 29614.12636780739, 'accumulated_submission_time': 17668.53111076355, 'accumulated_eval_time': 11943.41303062439, 'accumulated_logging_time': 0.6114578247070312}
I0207 17:00:56.553503 140529919506176 logging_writer.py:48] [50039] accumulated_eval_time=11943.413031, accumulated_logging_time=0.611458, accumulated_submission_time=17668.531111, global_step=50039, preemption_count=0, score=17668.531111, test/accuracy=0.690872, test/bleu=29.203660, test/loss=1.623056, test/num_examples=3003, total_duration=29614.126368, train/accuracy=0.701380, train/bleu=35.590883, train/loss=1.598410, validation/accuracy=0.677053, validation/bleu=29.584613, validation/loss=1.704675, validation/num_examples=3000
I0207 17:01:18.380181 140529927898880 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.28519192337989807, loss=3.8834712505340576
I0207 17:01:53.611414 140529919506176 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.24605704843997955, loss=3.881535053253174
I0207 17:02:28.841668 140529927898880 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.2594224512577057, loss=3.8799238204956055
I0207 17:03:04.115511 140529919506176 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.2731304466724396, loss=3.8814539909362793
I0207 17:03:39.378472 140529927898880 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.23635289072990417, loss=3.846060037612915
I0207 17:04:14.667615 140529919506176 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.23941203951835632, loss=3.871112823486328
I0207 17:04:49.949568 140529927898880 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.22206974029541016, loss=3.8698770999908447
I0207 17:05:25.264094 140529919506176 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.2438778281211853, loss=3.9165267944335938
I0207 17:06:00.610070 140529927898880 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.24986153841018677, loss=3.8979332447052
I0207 17:06:35.889826 140529919506176 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.2919051945209503, loss=3.84798526763916
I0207 17:07:11.180039 140529927898880 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.24284912645816803, loss=3.8276944160461426
I0207 17:07:46.493567 140529919506176 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.23975743353366852, loss=3.84543776512146
I0207 17:08:21.819582 140529927898880 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.2818586528301239, loss=3.8967154026031494
I0207 17:08:57.131588 140529919506176 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.25060513615608215, loss=3.811446189880371
I0207 17:09:32.381542 140529927898880 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.2551342844963074, loss=3.8388819694519043
I0207 17:10:07.691921 140529919506176 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.24708835780620575, loss=3.9132025241851807
I0207 17:10:42.963613 140529927898880 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.23968994617462158, loss=3.878535032272339
I0207 17:11:18.264127 140529919506176 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.2558342218399048, loss=3.932307720184326
I0207 17:11:53.541067 140529927898880 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.2485998272895813, loss=3.8463945388793945
I0207 17:12:28.801266 140529919506176 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.32063403725624084, loss=3.849893808364868
I0207 17:13:04.116924 140529927898880 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.23146623373031616, loss=3.8806800842285156
I0207 17:13:39.386237 140529919506176 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.25347334146499634, loss=3.9050772190093994
I0207 17:14:14.641047 140529927898880 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.25735393166542053, loss=3.8510642051696777
I0207 17:14:49.912287 140529919506176 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.25066202878952026, loss=3.9199438095092773
I0207 17:14:56.682842 140699726837568 spec.py:321] Evaluating on the training split.
I0207 17:14:59.712098 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 17:18:05.847948 140699726837568 spec.py:333] Evaluating on the validation split.
I0207 17:18:08.593658 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 17:20:55.912963 140699726837568 spec.py:349] Evaluating on the test split.
I0207 17:20:58.643603 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 17:23:24.764623 140699726837568 submission_runner.py:408] Time since start: 30962.36s, 	Step: 52421, 	{'train/accuracy': 0.6642529368400574, 'train/loss': 1.7819786071777344, 'train/bleu': 32.94824790704417, 'validation/accuracy': 0.6771645545959473, 'validation/loss': 1.690804123878479, 'validation/bleu': 29.778122229123387, 'validation/num_examples': 3000, 'test/accuracy': 0.6908256411552429, 'test/loss': 1.6125904321670532, 'test/bleu': 29.400656364664417, 'test/num_examples': 3003, 'score': 18508.571378707886, 'total_duration': 30962.36435317993, 'accumulated_submission_time': 18508.571378707886, 'accumulated_eval_time': 12451.49473619461, 'accumulated_logging_time': 0.6497724056243896}
I0207 17:23:24.786742 140529927898880 logging_writer.py:48] [52421] accumulated_eval_time=12451.494736, accumulated_logging_time=0.649772, accumulated_submission_time=18508.571379, global_step=52421, preemption_count=0, score=18508.571379, test/accuracy=0.690826, test/bleu=29.400656, test/loss=1.612590, test/num_examples=3003, total_duration=30962.364353, train/accuracy=0.664253, train/bleu=32.948248, train/loss=1.781979, validation/accuracy=0.677165, validation/bleu=29.778122, validation/loss=1.690804, validation/num_examples=3000
I0207 17:23:52.869547 140529919506176 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.24475228786468506, loss=3.8752493858337402
I0207 17:24:28.034577 140529927898880 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.2618703544139862, loss=3.886758327484131
I0207 17:25:03.244358 140529919506176 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.24664199352264404, loss=3.8262581825256348
I0207 17:25:38.471685 140529927898880 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.24566474556922913, loss=3.8606553077697754
I0207 17:26:13.756094 140529919506176 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.2570299804210663, loss=3.8564743995666504
I0207 17:26:49.017575 140529927898880 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.25905606150627136, loss=3.8564529418945312
I0207 17:27:24.279152 140529919506176 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.2512224614620209, loss=3.8030054569244385
I0207 17:27:59.547580 140529927898880 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.24215447902679443, loss=3.8255107402801514
I0207 17:28:34.850948 140529919506176 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.2592375874519348, loss=3.794982433319092
I0207 17:29:10.142651 140529927898880 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.24250870943069458, loss=3.8656389713287354
I0207 17:29:45.415992 140529919506176 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.27091583609580994, loss=3.8577001094818115
I0207 17:30:20.684955 140529927898880 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.27994292974472046, loss=3.8302600383758545
I0207 17:30:55.946404 140529919506176 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.28028440475463867, loss=3.8974251747131348
I0207 17:31:31.233146 140529927898880 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.24800263345241547, loss=3.839033603668213
I0207 17:32:06.503024 140529919506176 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.2537045180797577, loss=3.851576805114746
I0207 17:32:41.808625 140529927898880 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.4128692150115967, loss=3.8436405658721924
I0207 17:33:17.107025 140529919506176 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.2651941180229187, loss=3.942608118057251
I0207 17:33:52.380569 140529927898880 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.26803791522979736, loss=3.8845510482788086
I0207 17:34:27.687281 140529919506176 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.28138989210128784, loss=3.866581439971924
I0207 17:35:03.032089 140529927898880 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.3001832365989685, loss=3.8536417484283447
I0207 17:35:38.302447 140529919506176 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.26885390281677246, loss=3.9198787212371826
I0207 17:36:13.561688 140529927898880 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.23584367334842682, loss=3.8681187629699707
I0207 17:36:48.903046 140529919506176 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.2435973733663559, loss=3.888481378555298
I0207 17:37:24.192716 140529927898880 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.31115585565567017, loss=3.8136465549468994
I0207 17:37:24.973204 140699726837568 spec.py:321] Evaluating on the training split.
I0207 17:37:28.017760 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 17:40:22.921334 140699726837568 spec.py:333] Evaluating on the validation split.
I0207 17:40:25.656803 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 17:42:58.392345 140699726837568 spec.py:349] Evaluating on the test split.
I0207 17:43:01.133404 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 17:45:29.876814 140699726837568 submission_runner.py:408] Time since start: 32287.48s, 	Step: 54804, 	{'train/accuracy': 0.6630371809005737, 'train/loss': 1.7826213836669922, 'train/bleu': 32.97631954929356, 'validation/accuracy': 0.6774993538856506, 'validation/loss': 1.6878796815872192, 'validation/bleu': 29.82505230190574, 'validation/num_examples': 3000, 'test/accuracy': 0.6921155452728271, 'test/loss': 1.6047054529190063, 'test/bleu': 29.701937568525977, 'test/num_examples': 3003, 'score': 19348.670289993286, 'total_duration': 32287.476562976837, 'accumulated_submission_time': 19348.670289993286, 'accumulated_eval_time': 12936.39828658104, 'accumulated_logging_time': 0.6831979751586914}
I0207 17:45:29.899442 140529919506176 logging_writer.py:48] [54804] accumulated_eval_time=12936.398287, accumulated_logging_time=0.683198, accumulated_submission_time=19348.670290, global_step=54804, preemption_count=0, score=19348.670290, test/accuracy=0.692116, test/bleu=29.701938, test/loss=1.604705, test/num_examples=3003, total_duration=32287.476563, train/accuracy=0.663037, train/bleu=32.976320, train/loss=1.782621, validation/accuracy=0.677499, validation/bleu=29.825052, validation/loss=1.687880, validation/num_examples=3000
I0207 17:46:03.993708 140529927898880 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.2661997377872467, loss=3.882918357849121
I0207 17:46:39.165348 140529919506176 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.31062406301498413, loss=3.8887717723846436
I0207 17:47:14.388909 140529927898880 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.2600650191307068, loss=3.8492603302001953
I0207 17:47:49.639673 140529919506176 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.38310086727142334, loss=3.831146001815796
I0207 17:48:24.894080 140529927898880 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.2525169253349304, loss=3.8222720623016357
I0207 17:49:00.147208 140529919506176 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.23165690898895264, loss=3.863203525543213
I0207 17:49:35.405639 140529927898880 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.3217496871948242, loss=3.905306816101074
I0207 17:50:10.695497 140529919506176 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.2848094701766968, loss=3.915658712387085
I0207 17:50:45.976553 140529927898880 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.2627914845943451, loss=3.780927896499634
I0207 17:51:21.278443 140529919506176 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.2501910924911499, loss=3.8627707958221436
I0207 17:51:56.537886 140529927898880 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.2394997626543045, loss=3.84501314163208
I0207 17:52:31.797414 140529919506176 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.2457437515258789, loss=3.8719067573547363
I0207 17:53:07.067257 140529927898880 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.25322291254997253, loss=3.8583786487579346
I0207 17:53:42.445374 140529919506176 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.2862374782562256, loss=3.901554584503174
I0207 17:54:17.752440 140529927898880 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.2610156238079071, loss=3.8930108547210693
I0207 17:54:53.017471 140529919506176 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.2392396330833435, loss=3.8440561294555664
I0207 17:55:28.281388 140529927898880 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.2409096211194992, loss=3.8838860988616943
I0207 17:56:03.540535 140529919506176 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.2599707841873169, loss=3.8640871047973633
I0207 17:56:38.835554 140529927898880 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.2938520610332489, loss=3.786316156387329
I0207 17:57:14.102134 140529919506176 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.24899357557296753, loss=3.838602066040039
I0207 17:57:49.387334 140529927898880 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.23584507405757904, loss=3.788921594619751
I0207 17:58:24.663144 140529919506176 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.24018968641757965, loss=3.752681016921997
I0207 17:58:59.909300 140529927898880 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.2517158091068268, loss=3.9046671390533447
I0207 17:59:29.989276 140699726837568 spec.py:321] Evaluating on the training split.
I0207 17:59:33.026361 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 18:02:34.076956 140699726837568 spec.py:333] Evaluating on the validation split.
I0207 18:02:36.818850 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 18:05:09.113062 140699726837568 spec.py:349] Evaluating on the test split.
I0207 18:05:11.855439 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 18:07:31.393740 140699726837568 submission_runner.py:408] Time since start: 33608.99s, 	Step: 57187, 	{'train/accuracy': 0.677360475063324, 'train/loss': 1.7102564573287964, 'train/bleu': 33.62485481934153, 'validation/accuracy': 0.6791856288909912, 'validation/loss': 1.6819928884506226, 'validation/bleu': 29.688366128750683, 'validation/num_examples': 3000, 'test/accuracy': 0.6951368451118469, 'test/loss': 1.5948320627212524, 'test/bleu': 29.96182087882542, 'test/num_examples': 3003, 'score': 20188.675621509552, 'total_duration': 33608.99349451065, 'accumulated_submission_time': 20188.675621509552, 'accumulated_eval_time': 13417.802710533142, 'accumulated_logging_time': 0.7159018516540527}
I0207 18:07:31.416628 140529919506176 logging_writer.py:48] [57187] accumulated_eval_time=13417.802711, accumulated_logging_time=0.715902, accumulated_submission_time=20188.675622, global_step=57187, preemption_count=0, score=20188.675622, test/accuracy=0.695137, test/bleu=29.961821, test/loss=1.594832, test/num_examples=3003, total_duration=33608.993495, train/accuracy=0.677360, train/bleu=33.624855, train/loss=1.710256, validation/accuracy=0.679186, validation/bleu=29.688366, validation/loss=1.681993, validation/num_examples=3000
I0207 18:07:36.350166 140529927898880 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.273939847946167, loss=3.8816769123077393
I0207 18:08:11.490359 140529919506176 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.26838067173957825, loss=3.863859176635742
I0207 18:08:46.707529 140529927898880 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.252165824174881, loss=3.8299551010131836
I0207 18:09:21.950873 140529919506176 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.35179418325424194, loss=3.826603651046753
I0207 18:09:57.205123 140529927898880 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.25778713822364807, loss=3.7996015548706055
I0207 18:10:32.488323 140529919506176 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.2551356256008148, loss=3.8316025733947754
I0207 18:11:07.740183 140529927898880 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.24663053452968597, loss=3.7960658073425293
I0207 18:11:42.992678 140529919506176 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.26050126552581787, loss=3.8805530071258545
I0207 18:12:18.257140 140529927898880 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.26290345191955566, loss=3.8909692764282227
I0207 18:12:53.541725 140529919506176 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.231153666973114, loss=3.8530893325805664
I0207 18:13:28.795832 140529927898880 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.24620024859905243, loss=3.8284220695495605
I0207 18:14:04.094962 140529919506176 logging_writer.py:48] [58300] global_step=58300, grad_norm=0.26380184292793274, loss=3.7969472408294678
I0207 18:14:39.349075 140529927898880 logging_writer.py:48] [58400] global_step=58400, grad_norm=0.2530089318752289, loss=3.8619296550750732
I0207 18:15:14.590327 140529919506176 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.2509126365184784, loss=3.845421552658081
I0207 18:15:49.843336 140529927898880 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.2626829445362091, loss=3.822392463684082
I0207 18:16:25.098380 140529919506176 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.2620806396007538, loss=3.8333323001861572
I0207 18:17:00.348531 140529927898880 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.2761823534965515, loss=3.8255386352539062
I0207 18:17:35.622859 140529919506176 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.3424016237258911, loss=3.9153833389282227
I0207 18:18:10.911737 140529927898880 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.32112252712249756, loss=3.8380277156829834
I0207 18:18:46.193332 140529919506176 logging_writer.py:48] [59100] global_step=59100, grad_norm=0.2716059386730194, loss=3.795599937438965
I0207 18:19:21.437223 140529927898880 logging_writer.py:48] [59200] global_step=59200, grad_norm=0.39969050884246826, loss=3.8995184898376465
I0207 18:19:56.679042 140529919506176 logging_writer.py:48] [59300] global_step=59300, grad_norm=0.2707473933696747, loss=3.8374361991882324
I0207 18:20:31.972968 140529927898880 logging_writer.py:48] [59400] global_step=59400, grad_norm=0.2476322501897812, loss=3.842925548553467
I0207 18:21:07.244109 140529919506176 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.2635064721107483, loss=3.814894914627075
I0207 18:21:31.664537 140699726837568 spec.py:321] Evaluating on the training split.
I0207 18:21:34.716119 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 18:24:46.477854 140699726837568 spec.py:333] Evaluating on the validation split.
I0207 18:24:49.211139 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 18:28:07.371086 140699726837568 spec.py:349] Evaluating on the test split.
I0207 18:28:10.113324 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 18:31:12.013634 140699726837568 submission_runner.py:408] Time since start: 35029.61s, 	Step: 59571, 	{'train/accuracy': 0.665337324142456, 'train/loss': 1.7709975242614746, 'train/bleu': 32.771778877290295, 'validation/accuracy': 0.6795576214790344, 'validation/loss': 1.667377233505249, 'validation/bleu': 29.518673328869195, 'validation/num_examples': 3000, 'test/accuracy': 0.6934635043144226, 'test/loss': 1.5827269554138184, 'test/bleu': 29.45679735869189, 'test/num_examples': 3003, 'score': 21028.83904337883, 'total_duration': 35029.61338472366, 'accumulated_submission_time': 21028.83904337883, 'accumulated_eval_time': 13998.151756286621, 'accumulated_logging_time': 0.7499048709869385}
I0207 18:31:12.036720 140529927898880 logging_writer.py:48] [59571] accumulated_eval_time=13998.151756, accumulated_logging_time=0.749905, accumulated_submission_time=21028.839043, global_step=59571, preemption_count=0, score=21028.839043, test/accuracy=0.693464, test/bleu=29.456797, test/loss=1.582727, test/num_examples=3003, total_duration=35029.613385, train/accuracy=0.665337, train/bleu=32.771779, train/loss=1.770998, validation/accuracy=0.679558, validation/bleu=29.518673, validation/loss=1.667377, validation/num_examples=3000
I0207 18:31:22.595259 140529919506176 logging_writer.py:48] [59600] global_step=59600, grad_norm=0.29546675086021423, loss=3.8804690837860107
I0207 18:31:57.764458 140529927898880 logging_writer.py:48] [59700] global_step=59700, grad_norm=0.23331935703754425, loss=3.8004660606384277
I0207 18:32:32.957302 140529919506176 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.30238571763038635, loss=3.792515277862549
I0207 18:33:08.240080 140529927898880 logging_writer.py:48] [59900] global_step=59900, grad_norm=0.2540614902973175, loss=3.8012678623199463
I0207 18:33:43.479411 140529919506176 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.2650015950202942, loss=3.870999813079834
I0207 18:34:18.721667 140529927898880 logging_writer.py:48] [60100] global_step=60100, grad_norm=0.3174273669719696, loss=3.7910311222076416
I0207 18:34:53.972690 140529919506176 logging_writer.py:48] [60200] global_step=60200, grad_norm=0.2746073305606842, loss=3.8547956943511963
I0207 18:35:29.213361 140529927898880 logging_writer.py:48] [60300] global_step=60300, grad_norm=0.2632245123386383, loss=3.8350989818573
I0207 18:36:04.483864 140529919506176 logging_writer.py:48] [60400] global_step=60400, grad_norm=0.25400716066360474, loss=3.82883358001709
I0207 18:36:39.744022 140529927898880 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.26466915011405945, loss=3.8336644172668457
I0207 18:37:15.002758 140529919506176 logging_writer.py:48] [60600] global_step=60600, grad_norm=0.2659604251384735, loss=3.8288466930389404
I0207 18:37:50.242799 140529927898880 logging_writer.py:48] [60700] global_step=60700, grad_norm=0.25696900486946106, loss=3.801208019256592
I0207 18:38:25.503623 140529919506176 logging_writer.py:48] [60800] global_step=60800, grad_norm=0.2598858177661896, loss=3.845480442047119
I0207 18:39:00.782073 140529927898880 logging_writer.py:48] [60900] global_step=60900, grad_norm=0.2616053521633148, loss=3.8003249168395996
I0207 18:39:36.106141 140529919506176 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.2761680781841278, loss=3.816680431365967
I0207 18:40:11.398611 140529927898880 logging_writer.py:48] [61100] global_step=61100, grad_norm=0.2757621705532074, loss=3.85314679145813
I0207 18:40:46.662485 140529919506176 logging_writer.py:48] [61200] global_step=61200, grad_norm=0.25662532448768616, loss=3.875568151473999
I0207 18:41:21.927889 140529927898880 logging_writer.py:48] [61300] global_step=61300, grad_norm=0.2514261305332184, loss=3.8148155212402344
I0207 18:41:57.220352 140529919506176 logging_writer.py:48] [61400] global_step=61400, grad_norm=0.27441343665122986, loss=3.8098232746124268
I0207 18:42:32.465494 140529927898880 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.25243812799453735, loss=3.7977447509765625
I0207 18:43:07.755724 140529919506176 logging_writer.py:48] [61600] global_step=61600, grad_norm=0.2704390287399292, loss=3.7742679119110107
I0207 18:43:43.049147 140529927898880 logging_writer.py:48] [61700] global_step=61700, grad_norm=0.29121848940849304, loss=3.8361144065856934
I0207 18:44:18.340325 140529919506176 logging_writer.py:48] [61800] global_step=61800, grad_norm=0.2443746030330658, loss=3.8328444957733154
I0207 18:44:53.587299 140529927898880 logging_writer.py:48] [61900] global_step=61900, grad_norm=0.26993829011917114, loss=3.8792166709899902
I0207 18:45:12.366147 140699726837568 spec.py:321] Evaluating on the training split.
I0207 18:45:15.402615 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 18:48:30.638401 140699726837568 spec.py:333] Evaluating on the validation split.
I0207 18:48:33.377577 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 18:51:11.314837 140699726837568 spec.py:349] Evaluating on the test split.
I0207 18:51:14.082609 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 18:53:41.961818 140699726837568 submission_runner.py:408] Time since start: 36379.56s, 	Step: 61955, 	{'train/accuracy': 0.6696307063102722, 'train/loss': 1.765308141708374, 'train/bleu': 33.5078406006622, 'validation/accuracy': 0.6800783276557922, 'validation/loss': 1.670266032218933, 'validation/bleu': 29.881170707042948, 'validation/num_examples': 3000, 'test/accuracy': 0.6972982287406921, 'test/loss': 1.5744260549545288, 'test/bleu': 30.065476502372746, 'test/num_examples': 3003, 'score': 21869.08454298973, 'total_duration': 36379.561566352844, 'accumulated_submission_time': 21869.08454298973, 'accumulated_eval_time': 14507.747369527817, 'accumulated_logging_time': 0.7831311225891113}
I0207 18:53:41.985995 140529919506176 logging_writer.py:48] [61955] accumulated_eval_time=14507.747370, accumulated_logging_time=0.783131, accumulated_submission_time=21869.084543, global_step=61955, preemption_count=0, score=21869.084543, test/accuracy=0.697298, test/bleu=30.065477, test/loss=1.574426, test/num_examples=3003, total_duration=36379.561566, train/accuracy=0.669631, train/bleu=33.507841, train/loss=1.765308, validation/accuracy=0.680078, validation/bleu=29.881171, validation/loss=1.670266, validation/num_examples=3000
I0207 18:53:58.205442 140529927898880 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.27459627389907837, loss=3.871701955795288
I0207 18:54:33.374963 140529919506176 logging_writer.py:48] [62100] global_step=62100, grad_norm=0.30189502239227295, loss=3.823312520980835
I0207 18:55:08.597857 140529927898880 logging_writer.py:48] [62200] global_step=62200, grad_norm=0.2588459849357605, loss=3.849773406982422
I0207 18:55:43.833296 140529919506176 logging_writer.py:48] [62300] global_step=62300, grad_norm=0.26234936714172363, loss=3.783601760864258
I0207 18:56:19.120826 140529927898880 logging_writer.py:48] [62400] global_step=62400, grad_norm=0.26527780294418335, loss=3.83241868019104
I0207 18:56:54.394970 140529919506176 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.2659042775630951, loss=3.8647329807281494
I0207 18:57:29.655766 140529927898880 logging_writer.py:48] [62600] global_step=62600, grad_norm=0.2990792691707611, loss=3.8768296241760254
I0207 18:58:04.929874 140529919506176 logging_writer.py:48] [62700] global_step=62700, grad_norm=0.27339082956314087, loss=3.8764264583587646
I0207 18:58:40.205234 140529927898880 logging_writer.py:48] [62800] global_step=62800, grad_norm=0.2341669499874115, loss=3.8169121742248535
I0207 18:59:15.458342 140529919506176 logging_writer.py:48] [62900] global_step=62900, grad_norm=0.2749563753604889, loss=3.87038254737854
I0207 18:59:50.721992 140529927898880 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.25853976607322693, loss=3.851109504699707
I0207 19:00:25.984676 140529919506176 logging_writer.py:48] [63100] global_step=63100, grad_norm=0.2822456657886505, loss=3.840384006500244
I0207 19:01:01.253637 140529927898880 logging_writer.py:48] [63200] global_step=63200, grad_norm=0.2701351046562195, loss=3.84607195854187
I0207 19:01:36.532902 140529919506176 logging_writer.py:48] [63300] global_step=63300, grad_norm=0.27987566590309143, loss=3.8537819385528564
I0207 19:02:11.813021 140529927898880 logging_writer.py:48] [63400] global_step=63400, grad_norm=0.28186672925949097, loss=3.7900631427764893
I0207 19:02:47.113029 140529919506176 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.27552637457847595, loss=3.771328926086426
I0207 19:03:22.386818 140529927898880 logging_writer.py:48] [63600] global_step=63600, grad_norm=0.2741779088973999, loss=3.803004741668701
I0207 19:03:57.622924 140529919506176 logging_writer.py:48] [63700] global_step=63700, grad_norm=0.27040576934814453, loss=3.823246955871582
I0207 19:04:32.894377 140529927898880 logging_writer.py:48] [63800] global_step=63800, grad_norm=0.3270307779312134, loss=3.8525278568267822
I0207 19:05:08.120347 140529919506176 logging_writer.py:48] [63900] global_step=63900, grad_norm=0.2589813470840454, loss=3.8688881397247314
I0207 19:05:43.382429 140529927898880 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.2686842978000641, loss=3.767500638961792
I0207 19:06:18.692864 140529919506176 logging_writer.py:48] [64100] global_step=64100, grad_norm=0.2809930145740509, loss=3.817396402359009
I0207 19:06:53.960509 140529927898880 logging_writer.py:48] [64200] global_step=64200, grad_norm=0.2535444498062134, loss=3.868872880935669
I0207 19:07:29.220019 140529919506176 logging_writer.py:48] [64300] global_step=64300, grad_norm=0.2572745084762573, loss=3.8266971111297607
I0207 19:07:41.986621 140699726837568 spec.py:321] Evaluating on the training split.
I0207 19:07:45.020484 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 19:10:40.881141 140699726837568 spec.py:333] Evaluating on the validation split.
I0207 19:10:43.619150 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 19:13:12.689885 140699726837568 spec.py:349] Evaluating on the test split.
I0207 19:13:15.434506 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 19:15:35.679830 140699726837568 submission_runner.py:408] Time since start: 37693.28s, 	Step: 64338, 	{'train/accuracy': 0.6764718890190125, 'train/loss': 1.7071046829223633, 'train/bleu': 33.57440543575773, 'validation/accuracy': 0.680251955986023, 'validation/loss': 1.6660431623458862, 'validation/bleu': 29.700324861556748, 'validation/num_examples': 3000, 'test/accuracy': 0.6957643628120422, 'test/loss': 1.577563762664795, 'test/bleu': 29.864767151801697, 'test/num_examples': 3003, 'score': 22709.00170826912, 'total_duration': 37693.27958345413, 'accumulated_submission_time': 22709.00170826912, 'accumulated_eval_time': 14981.440528154373, 'accumulated_logging_time': 0.8170902729034424}
I0207 19:15:35.704236 140529927898880 logging_writer.py:48] [64338] accumulated_eval_time=14981.440528, accumulated_logging_time=0.817090, accumulated_submission_time=22709.001708, global_step=64338, preemption_count=0, score=22709.001708, test/accuracy=0.695764, test/bleu=29.864767, test/loss=1.577564, test/num_examples=3003, total_duration=37693.279583, train/accuracy=0.676472, train/bleu=33.574405, train/loss=1.707105, validation/accuracy=0.680252, validation/bleu=29.700325, validation/loss=1.666043, validation/num_examples=3000
I0207 19:15:57.835526 140529919506176 logging_writer.py:48] [64400] global_step=64400, grad_norm=0.2638912498950958, loss=3.8184285163879395
I0207 19:16:33.028183 140529927898880 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.27757173776626587, loss=3.8790318965911865
I0207 19:17:08.295866 140529919506176 logging_writer.py:48] [64600] global_step=64600, grad_norm=0.2569581866264343, loss=3.802630662918091
I0207 19:17:43.545397 140529927898880 logging_writer.py:48] [64700] global_step=64700, grad_norm=0.3466595411300659, loss=3.764894723892212
I0207 19:18:18.828454 140529919506176 logging_writer.py:48] [64800] global_step=64800, grad_norm=0.2511973977088928, loss=3.825831890106201
I0207 19:18:54.115249 140529927898880 logging_writer.py:48] [64900] global_step=64900, grad_norm=0.2776312232017517, loss=3.886523485183716
I0207 19:19:29.360563 140529919506176 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.27487489581108093, loss=3.787909746170044
I0207 19:20:04.647047 140529927898880 logging_writer.py:48] [65100] global_step=65100, grad_norm=0.27286866307258606, loss=3.7932770252227783
I0207 19:20:39.904129 140529919506176 logging_writer.py:48] [65200] global_step=65200, grad_norm=0.2583407163619995, loss=3.821337938308716
I0207 19:21:15.184493 140529927898880 logging_writer.py:48] [65300] global_step=65300, grad_norm=0.305376797914505, loss=3.8900046348571777
I0207 19:21:50.434608 140529919506176 logging_writer.py:48] [65400] global_step=65400, grad_norm=0.27585482597351074, loss=3.7901508808135986
I0207 19:22:25.739438 140529927898880 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.2561964988708496, loss=3.802760124206543
I0207 19:23:01.049094 140529919506176 logging_writer.py:48] [65600] global_step=65600, grad_norm=0.3067303001880646, loss=3.7623684406280518
I0207 19:23:36.305586 140529927898880 logging_writer.py:48] [65700] global_step=65700, grad_norm=0.2796037793159485, loss=3.8211476802825928
I0207 19:24:11.590309 140529919506176 logging_writer.py:48] [65800] global_step=65800, grad_norm=0.28227028250694275, loss=3.791569948196411
I0207 19:24:46.905014 140529927898880 logging_writer.py:48] [65900] global_step=65900, grad_norm=0.2804780602455139, loss=3.8606655597686768
I0207 19:25:22.172091 140529919506176 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.32435718178749084, loss=3.834031581878662
I0207 19:25:57.444603 140529927898880 logging_writer.py:48] [66100] global_step=66100, grad_norm=0.2536807656288147, loss=3.8210721015930176
I0207 19:26:32.702374 140529919506176 logging_writer.py:48] [66200] global_step=66200, grad_norm=0.29830294847488403, loss=3.758086919784546
I0207 19:27:07.967522 140529927898880 logging_writer.py:48] [66300] global_step=66300, grad_norm=0.25133126974105835, loss=3.761554002761841
I0207 19:27:43.236416 140529919506176 logging_writer.py:48] [66400] global_step=66400, grad_norm=0.25278177857398987, loss=3.781391143798828
I0207 19:28:18.522449 140529927898880 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.26885512471199036, loss=3.8565165996551514
I0207 19:28:53.785831 140529919506176 logging_writer.py:48] [66600] global_step=66600, grad_norm=0.2700580060482025, loss=3.875767469406128
I0207 19:29:29.139052 140529927898880 logging_writer.py:48] [66700] global_step=66700, grad_norm=0.2692776918411255, loss=3.8032948970794678
I0207 19:29:35.915299 140699726837568 spec.py:321] Evaluating on the training split.
I0207 19:29:38.961983 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 19:32:35.028033 140699726837568 spec.py:333] Evaluating on the validation split.
I0207 19:32:37.800998 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 19:35:31.006276 140699726837568 spec.py:349] Evaluating on the test split.
I0207 19:35:33.759088 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 19:38:03.617278 140699726837568 submission_runner.py:408] Time since start: 39041.22s, 	Step: 66721, 	{'train/accuracy': 0.6704260110855103, 'train/loss': 1.7423919439315796, 'train/bleu': 33.43165042153991, 'validation/accuracy': 0.6816902160644531, 'validation/loss': 1.6634081602096558, 'validation/bleu': 29.84663557839197, 'validation/num_examples': 3000, 'test/accuracy': 0.6970425844192505, 'test/loss': 1.5691356658935547, 'test/bleu': 30.122066124861863, 'test/num_examples': 3003, 'score': 23549.12446165085, 'total_duration': 39041.21701860428, 'accumulated_submission_time': 23549.12446165085, 'accumulated_eval_time': 15489.14245057106, 'accumulated_logging_time': 0.854212760925293}
I0207 19:38:03.642541 140529919506176 logging_writer.py:48] [66721] accumulated_eval_time=15489.142451, accumulated_logging_time=0.854213, accumulated_submission_time=23549.124462, global_step=66721, preemption_count=0, score=23549.124462, test/accuracy=0.697043, test/bleu=30.122066, test/loss=1.569136, test/num_examples=3003, total_duration=39041.217019, train/accuracy=0.670426, train/bleu=33.431650, train/loss=1.742392, validation/accuracy=0.681690, validation/bleu=29.846636, validation/loss=1.663408, validation/num_examples=3000
I0207 19:38:31.751652 140529927898880 logging_writer.py:48] [66800] global_step=66800, grad_norm=0.2695217728614807, loss=3.8351285457611084
I0207 19:39:06.934705 140529919506176 logging_writer.py:48] [66900] global_step=66900, grad_norm=0.2791876792907715, loss=3.7835311889648438
I0207 19:39:42.165870 140529927898880 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.2734912931919098, loss=3.8037428855895996
I0207 19:40:17.428961 140529919506176 logging_writer.py:48] [67100] global_step=67100, grad_norm=0.2706645131111145, loss=3.828946828842163
I0207 19:40:52.688503 140529927898880 logging_writer.py:48] [67200] global_step=67200, grad_norm=0.26790833473205566, loss=3.882559299468994
I0207 19:41:27.961965 140529919506176 logging_writer.py:48] [67300] global_step=67300, grad_norm=0.27988019585609436, loss=3.8087949752807617
I0207 19:42:03.273306 140529927898880 logging_writer.py:48] [67400] global_step=67400, grad_norm=0.25382184982299805, loss=3.784809112548828
I0207 19:42:38.528379 140529919506176 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.29751288890838623, loss=3.80379581451416
I0207 19:43:13.794704 140529927898880 logging_writer.py:48] [67600] global_step=67600, grad_norm=0.2727600932121277, loss=3.826354503631592
I0207 19:43:49.048392 140529919506176 logging_writer.py:48] [67700] global_step=67700, grad_norm=0.28274649381637573, loss=3.8263700008392334
I0207 19:44:24.308105 140529927898880 logging_writer.py:48] [67800] global_step=67800, grad_norm=0.2571914792060852, loss=3.8241233825683594
I0207 19:44:59.572319 140529919506176 logging_writer.py:48] [67900] global_step=67900, grad_norm=0.29522475600242615, loss=3.7561562061309814
I0207 19:45:34.808705 140529927898880 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.2874905467033386, loss=3.8732614517211914
I0207 19:46:10.093499 140529919506176 logging_writer.py:48] [68100] global_step=68100, grad_norm=0.28217941522598267, loss=3.797494649887085
I0207 19:46:45.415256 140529927898880 logging_writer.py:48] [68200] global_step=68200, grad_norm=0.2819107472896576, loss=3.8143317699432373
I0207 19:47:20.807905 140529919506176 logging_writer.py:48] [68300] global_step=68300, grad_norm=0.30472204089164734, loss=3.847659111022949
I0207 19:47:56.109461 140529927898880 logging_writer.py:48] [68400] global_step=68400, grad_norm=0.2667286694049835, loss=3.8925788402557373
I0207 19:48:31.409079 140529919506176 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.25693970918655396, loss=3.812865972518921
I0207 19:49:06.707574 140529927898880 logging_writer.py:48] [68600] global_step=68600, grad_norm=0.27561235427856445, loss=3.7523655891418457
I0207 19:49:42.005702 140529919506176 logging_writer.py:48] [68700] global_step=68700, grad_norm=0.27203938364982605, loss=3.781738519668579
I0207 19:50:17.310033 140529927898880 logging_writer.py:48] [68800] global_step=68800, grad_norm=0.2792932093143463, loss=3.868436574935913
I0207 19:50:52.578263 140529919506176 logging_writer.py:48] [68900] global_step=68900, grad_norm=0.300193190574646, loss=3.8361434936523438
I0207 19:51:27.863085 140529927898880 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.26457905769348145, loss=3.8106813430786133
I0207 19:52:03.133836 140529919506176 logging_writer.py:48] [69100] global_step=69100, grad_norm=0.2788393497467041, loss=3.8508002758026123
I0207 19:52:03.915668 140699726837568 spec.py:321] Evaluating on the training split.
I0207 19:52:06.954190 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 19:55:19.126804 140699726837568 spec.py:333] Evaluating on the validation split.
I0207 19:55:21.866496 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 19:57:58.657445 140699726837568 spec.py:349] Evaluating on the test split.
I0207 19:58:01.394242 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 20:00:27.650829 140699726837568 submission_runner.py:408] Time since start: 40385.25s, 	Step: 69104, 	{'train/accuracy': 0.6957202553749084, 'train/loss': 1.5890588760375977, 'train/bleu': 35.166386173569414, 'validation/accuracy': 0.682223379611969, 'validation/loss': 1.6552072763442993, 'validation/bleu': 29.95974849835134, 'validation/num_examples': 3000, 'test/accuracy': 0.6975655555725098, 'test/loss': 1.562856674194336, 'test/bleu': 29.880227745785337, 'test/num_examples': 3003, 'score': 24389.310983896255, 'total_duration': 40385.25058054924, 'accumulated_submission_time': 24389.310983896255, 'accumulated_eval_time': 15992.877562999725, 'accumulated_logging_time': 0.8907725811004639}
I0207 20:00:27.675178 140529927898880 logging_writer.py:48] [69104] accumulated_eval_time=15992.877563, accumulated_logging_time=0.890773, accumulated_submission_time=24389.310984, global_step=69104, preemption_count=0, score=24389.310984, test/accuracy=0.697566, test/bleu=29.880228, test/loss=1.562857, test/num_examples=3003, total_duration=40385.250581, train/accuracy=0.695720, train/bleu=35.166386, train/loss=1.589059, validation/accuracy=0.682223, validation/bleu=29.959748, validation/loss=1.655207, validation/num_examples=3000
I0207 20:01:01.729733 140529919506176 logging_writer.py:48] [69200] global_step=69200, grad_norm=0.2653237581253052, loss=3.834130048751831
I0207 20:01:36.961379 140529927898880 logging_writer.py:48] [69300] global_step=69300, grad_norm=0.28878748416900635, loss=3.870197057723999
I0207 20:02:12.212745 140529919506176 logging_writer.py:48] [69400] global_step=69400, grad_norm=0.2552824914455414, loss=3.7684085369110107
I0207 20:02:47.489691 140529927898880 logging_writer.py:48] [69500] global_step=69500, grad_norm=0.25428688526153564, loss=3.800049066543579
I0207 20:03:22.752749 140529919506176 logging_writer.py:48] [69600] global_step=69600, grad_norm=0.2585969567298889, loss=3.8115525245666504
I0207 20:03:57.999621 140529927898880 logging_writer.py:48] [69700] global_step=69700, grad_norm=0.2576763331890106, loss=3.8087120056152344
I0207 20:04:33.260465 140529919506176 logging_writer.py:48] [69800] global_step=69800, grad_norm=0.27068427205085754, loss=3.7918918132781982
I0207 20:05:08.552795 140529927898880 logging_writer.py:48] [69900] global_step=69900, grad_norm=0.2732177674770355, loss=3.782109022140503
I0207 20:05:43.799284 140529919506176 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.26747655868530273, loss=3.8040385246276855
I0207 20:06:19.085189 140529927898880 logging_writer.py:48] [70100] global_step=70100, grad_norm=0.3009372353553772, loss=3.803250312805176
I0207 20:06:54.328999 140529919506176 logging_writer.py:48] [70200] global_step=70200, grad_norm=0.2839282155036926, loss=3.7668590545654297
I0207 20:07:29.579825 140529927898880 logging_writer.py:48] [70300] global_step=70300, grad_norm=0.30153176188468933, loss=3.771017074584961
I0207 20:08:04.834248 140529919506176 logging_writer.py:48] [70400] global_step=70400, grad_norm=0.2840757369995117, loss=3.806751251220703
I0207 20:08:40.127696 140529927898880 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.3021598160266876, loss=3.783071517944336
I0207 20:09:15.408479 140529919506176 logging_writer.py:48] [70600] global_step=70600, grad_norm=0.2739919424057007, loss=3.821320056915283
I0207 20:09:50.682705 140529927898880 logging_writer.py:48] [70700] global_step=70700, grad_norm=0.26474714279174805, loss=3.7993292808532715
I0207 20:10:25.968696 140529919506176 logging_writer.py:48] [70800] global_step=70800, grad_norm=0.28157830238342285, loss=3.780106782913208
I0207 20:11:01.249719 140529927898880 logging_writer.py:48] [70900] global_step=70900, grad_norm=0.24501082301139832, loss=3.781564235687256
I0207 20:11:36.505667 140529919506176 logging_writer.py:48] [71000] global_step=71000, grad_norm=0.2505760192871094, loss=3.7639031410217285
I0207 20:12:11.816383 140529927898880 logging_writer.py:48] [71100] global_step=71100, grad_norm=0.3134569525718689, loss=3.7869691848754883
I0207 20:12:47.074062 140529919506176 logging_writer.py:48] [71200] global_step=71200, grad_norm=0.26450175046920776, loss=3.7947795391082764
I0207 20:13:22.447377 140529927898880 logging_writer.py:48] [71300] global_step=71300, grad_norm=0.2788090705871582, loss=3.8025529384613037
I0207 20:13:57.769472 140529919506176 logging_writer.py:48] [71400] global_step=71400, grad_norm=0.266014039516449, loss=3.812204122543335
I0207 20:14:27.800366 140699726837568 spec.py:321] Evaluating on the training split.
I0207 20:14:30.840716 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 20:17:46.371945 140699726837568 spec.py:333] Evaluating on the validation split.
I0207 20:17:49.120609 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 20:20:30.502675 140699726837568 spec.py:349] Evaluating on the test split.
I0207 20:20:33.248062 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 20:23:02.480093 140699726837568 submission_runner.py:408] Time since start: 41740.08s, 	Step: 71487, 	{'train/accuracy': 0.676636815071106, 'train/loss': 1.7049415111541748, 'train/bleu': 34.26073413751061, 'validation/accuracy': 0.6833145022392273, 'validation/loss': 1.646494746208191, 'validation/bleu': 30.023471272179897, 'validation/num_examples': 3000, 'test/accuracy': 0.6996223330497742, 'test/loss': 1.5564168691635132, 'test/bleu': 30.269282466335337, 'test/num_examples': 3003, 'score': 25229.351717948914, 'total_duration': 41740.079825639725, 'accumulated_submission_time': 25229.351717948914, 'accumulated_eval_time': 16507.55722308159, 'accumulated_logging_time': 0.925260066986084}
I0207 20:23:02.505324 140529927898880 logging_writer.py:48] [71487] accumulated_eval_time=16507.557223, accumulated_logging_time=0.925260, accumulated_submission_time=25229.351718, global_step=71487, preemption_count=0, score=25229.351718, test/accuracy=0.699622, test/bleu=30.269282, test/loss=1.556417, test/num_examples=3003, total_duration=41740.079826, train/accuracy=0.676637, train/bleu=34.260734, train/loss=1.704942, validation/accuracy=0.683315, validation/bleu=30.023471, validation/loss=1.646495, validation/num_examples=3000
I0207 20:23:07.439866 140529919506176 logging_writer.py:48] [71500] global_step=71500, grad_norm=0.2589806318283081, loss=3.8098597526550293
I0207 20:23:42.598313 140529927898880 logging_writer.py:48] [71600] global_step=71600, grad_norm=0.4036353528499603, loss=3.753096580505371
I0207 20:24:17.824275 140529919506176 logging_writer.py:48] [71700] global_step=71700, grad_norm=0.3221510648727417, loss=3.8441715240478516
I0207 20:24:53.063176 140529927898880 logging_writer.py:48] [71800] global_step=71800, grad_norm=0.2714749276638031, loss=3.7736220359802246
I0207 20:25:28.336001 140529919506176 logging_writer.py:48] [71900] global_step=71900, grad_norm=0.2683430016040802, loss=3.7930688858032227
I0207 20:26:03.586355 140529927898880 logging_writer.py:48] [72000] global_step=72000, grad_norm=0.2832016050815582, loss=3.741520881652832
I0207 20:26:38.828557 140529919506176 logging_writer.py:48] [72100] global_step=72100, grad_norm=0.27436432242393494, loss=3.772284984588623
I0207 20:27:14.109470 140529927898880 logging_writer.py:48] [72200] global_step=72200, grad_norm=0.2685447931289673, loss=3.7788379192352295
I0207 20:27:49.369385 140529919506176 logging_writer.py:48] [72300] global_step=72300, grad_norm=0.30032092332839966, loss=3.7664339542388916
I0207 20:28:24.644869 140529927898880 logging_writer.py:48] [72400] global_step=72400, grad_norm=0.28532007336616516, loss=3.8227176666259766
I0207 20:28:59.926372 140529919506176 logging_writer.py:48] [72500] global_step=72500, grad_norm=0.2873668074607849, loss=3.8350400924682617
I0207 20:29:35.203439 140529927898880 logging_writer.py:48] [72600] global_step=72600, grad_norm=0.2771056294441223, loss=3.770482063293457
I0207 20:30:10.444188 140529919506176 logging_writer.py:48] [72700] global_step=72700, grad_norm=0.25301456451416016, loss=3.7439801692962646
I0207 20:30:45.748108 140529927898880 logging_writer.py:48] [72800] global_step=72800, grad_norm=0.2811843454837799, loss=3.822610378265381
I0207 20:31:21.028841 140529919506176 logging_writer.py:48] [72900] global_step=72900, grad_norm=0.3550035357475281, loss=3.862657308578491
I0207 20:31:56.276872 140529927898880 logging_writer.py:48] [73000] global_step=73000, grad_norm=0.275290310382843, loss=3.7996652126312256
I0207 20:32:31.527055 140529919506176 logging_writer.py:48] [73100] global_step=73100, grad_norm=0.2815961539745331, loss=3.7876482009887695
I0207 20:33:06.772111 140529927898880 logging_writer.py:48] [73200] global_step=73200, grad_norm=0.260137140750885, loss=3.804410219192505
I0207 20:33:42.000430 140529919506176 logging_writer.py:48] [73300] global_step=73300, grad_norm=0.27052947878837585, loss=3.7835464477539062
I0207 20:34:17.253561 140529927898880 logging_writer.py:48] [73400] global_step=73400, grad_norm=0.26694074273109436, loss=3.749605178833008
I0207 20:34:52.509107 140529919506176 logging_writer.py:48] [73500] global_step=73500, grad_norm=0.2851419150829315, loss=3.8248841762542725
I0207 20:35:27.786676 140529927898880 logging_writer.py:48] [73600] global_step=73600, grad_norm=0.27770182490348816, loss=3.7017743587493896
I0207 20:36:03.023042 140529919506176 logging_writer.py:48] [73700] global_step=73700, grad_norm=0.29508647322654724, loss=3.8559157848358154
I0207 20:36:38.317755 140529927898880 logging_writer.py:48] [73800] global_step=73800, grad_norm=0.28614503145217896, loss=3.7880938053131104
I0207 20:37:02.723863 140699726837568 spec.py:321] Evaluating on the training split.
I0207 20:37:05.759253 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 20:41:04.733557 140699726837568 spec.py:333] Evaluating on the validation split.
I0207 20:41:07.459980 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 20:43:54.110361 140699726837568 spec.py:349] Evaluating on the test split.
I0207 20:43:56.856566 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 20:46:39.825418 140699726837568 submission_runner.py:408] Time since start: 43157.43s, 	Step: 73871, 	{'train/accuracy': 0.6750386357307434, 'train/loss': 1.7208690643310547, 'train/bleu': 34.451335996141516, 'validation/accuracy': 0.6839592456817627, 'validation/loss': 1.6430405378341675, 'validation/bleu': 30.166010042652832, 'validation/num_examples': 3000, 'test/accuracy': 0.6999593377113342, 'test/loss': 1.5452179908752441, 'test/bleu': 30.42530986159958, 'test/num_examples': 3003, 'score': 26069.48642897606, 'total_duration': 43157.425143003464, 'accumulated_submission_time': 26069.48642897606, 'accumulated_eval_time': 17084.658698558807, 'accumulated_logging_time': 0.9607217311859131}
I0207 20:46:39.855454 140529919506176 logging_writer.py:48] [73871] accumulated_eval_time=17084.658699, accumulated_logging_time=0.960722, accumulated_submission_time=26069.486429, global_step=73871, preemption_count=0, score=26069.486429, test/accuracy=0.699959, test/bleu=30.425310, test/loss=1.545218, test/num_examples=3003, total_duration=43157.425143, train/accuracy=0.675039, train/bleu=34.451336, train/loss=1.720869, validation/accuracy=0.683959, validation/bleu=30.166010, validation/loss=1.643041, validation/num_examples=3000
I0207 20:46:50.415879 140529927898880 logging_writer.py:48] [73900] global_step=73900, grad_norm=0.27015846967697144, loss=3.8033831119537354
I0207 20:47:25.592412 140529919506176 logging_writer.py:48] [74000] global_step=74000, grad_norm=0.3386894762516022, loss=3.851424217224121
I0207 20:48:00.800246 140529927898880 logging_writer.py:48] [74100] global_step=74100, grad_norm=0.2772398293018341, loss=3.8233556747436523
I0207 20:48:36.035865 140529919506176 logging_writer.py:48] [74200] global_step=74200, grad_norm=0.2623402178287506, loss=3.757622003555298
I0207 20:49:11.283398 140529927898880 logging_writer.py:48] [74300] global_step=74300, grad_norm=0.27048808336257935, loss=3.805837631225586
I0207 20:49:46.514920 140529919506176 logging_writer.py:48] [74400] global_step=74400, grad_norm=0.28305670619010925, loss=3.774761915206909
I0207 20:50:21.762972 140529927898880 logging_writer.py:48] [74500] global_step=74500, grad_norm=0.263213574886322, loss=3.780679225921631
I0207 20:50:57.045422 140529919506176 logging_writer.py:48] [74600] global_step=74600, grad_norm=0.27974581718444824, loss=3.7766807079315186
I0207 20:51:32.273533 140529927898880 logging_writer.py:48] [74700] global_step=74700, grad_norm=0.2631400227546692, loss=3.805206060409546
I0207 20:52:07.557328 140529919506176 logging_writer.py:48] [74800] global_step=74800, grad_norm=0.2971270978450775, loss=3.8092877864837646
I0207 20:52:42.785534 140529927898880 logging_writer.py:48] [74900] global_step=74900, grad_norm=0.2587706744670868, loss=3.7778048515319824
I0207 20:53:18.070990 140529919506176 logging_writer.py:48] [75000] global_step=75000, grad_norm=0.2942011058330536, loss=3.796384334564209
I0207 20:53:53.409960 140529927898880 logging_writer.py:48] [75100] global_step=75100, grad_norm=0.27423617243766785, loss=3.7900550365448
I0207 20:54:28.707335 140529919506176 logging_writer.py:48] [75200] global_step=75200, grad_norm=0.2820093035697937, loss=3.7830305099487305
I0207 20:55:04.025429 140529927898880 logging_writer.py:48] [75300] global_step=75300, grad_norm=0.29309096932411194, loss=3.763915538787842
I0207 20:55:39.295066 140529919506176 logging_writer.py:48] [75400] global_step=75400, grad_norm=0.27379873394966125, loss=3.843184232711792
I0207 20:56:14.570778 140529927898880 logging_writer.py:48] [75500] global_step=75500, grad_norm=0.27897539734840393, loss=3.797133445739746
I0207 20:56:49.869582 140529919506176 logging_writer.py:48] [75600] global_step=75600, grad_norm=0.26583805680274963, loss=3.723858594894409
I0207 20:57:25.128766 140529927898880 logging_writer.py:48] [75700] global_step=75700, grad_norm=0.28223490715026855, loss=3.8113512992858887
I0207 20:58:00.474202 140529919506176 logging_writer.py:48] [75800] global_step=75800, grad_norm=0.29099324345588684, loss=3.7684128284454346
I0207 20:58:35.749500 140529927898880 logging_writer.py:48] [75900] global_step=75900, grad_norm=0.2850426435470581, loss=3.8668525218963623
I0207 20:59:11.003702 140529919506176 logging_writer.py:48] [76000] global_step=76000, grad_norm=0.26714766025543213, loss=3.7527287006378174
I0207 20:59:46.257327 140529927898880 logging_writer.py:48] [76100] global_step=76100, grad_norm=0.2846203148365021, loss=3.7603812217712402
I0207 21:00:21.514340 140529919506176 logging_writer.py:48] [76200] global_step=76200, grad_norm=0.2754805088043213, loss=3.793180227279663
I0207 21:00:39.923758 140699726837568 spec.py:321] Evaluating on the training split.
I0207 21:00:42.964706 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 21:04:17.272689 140699726837568 spec.py:333] Evaluating on the validation split.
I0207 21:04:20.016004 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 21:07:20.484780 140699726837568 spec.py:349] Evaluating on the test split.
I0207 21:07:23.218139 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 21:10:13.350827 140699726837568 submission_runner.py:408] Time since start: 44570.95s, 	Step: 76254, 	{'train/accuracy': 0.6888977885246277, 'train/loss': 1.6275510787963867, 'train/bleu': 35.07580034747715, 'validation/accuracy': 0.6844180226325989, 'validation/loss': 1.6386370658874512, 'validation/bleu': 29.928076607129135, 'validation/num_examples': 3000, 'test/accuracy': 0.701655924320221, 'test/loss': 1.541452407836914, 'test/bleu': 30.24334076025487, 'test/num_examples': 3003, 'score': 26909.4674077034, 'total_duration': 44570.950585365295, 'accumulated_submission_time': 26909.4674077034, 'accumulated_eval_time': 17658.08572268486, 'accumulated_logging_time': 1.0019049644470215}
I0207 21:10:13.376661 140529927898880 logging_writer.py:48] [76254] accumulated_eval_time=17658.085723, accumulated_logging_time=1.001905, accumulated_submission_time=26909.467408, global_step=76254, preemption_count=0, score=26909.467408, test/accuracy=0.701656, test/bleu=30.243341, test/loss=1.541452, test/num_examples=3003, total_duration=44570.950585, train/accuracy=0.688898, train/bleu=35.075800, train/loss=1.627551, validation/accuracy=0.684418, validation/bleu=29.928077, validation/loss=1.638637, validation/num_examples=3000
I0207 21:10:29.874419 140529919506176 logging_writer.py:48] [76300] global_step=76300, grad_norm=0.2833620309829712, loss=3.808410882949829
I0207 21:11:05.020252 140529927898880 logging_writer.py:48] [76400] global_step=76400, grad_norm=0.2874290943145752, loss=3.7442877292633057
I0207 21:11:40.253646 140529919506176 logging_writer.py:48] [76500] global_step=76500, grad_norm=0.2726146876811981, loss=3.7514755725860596
I0207 21:12:15.559016 140529927898880 logging_writer.py:48] [76600] global_step=76600, grad_norm=0.2758207619190216, loss=3.7709972858428955
I0207 21:12:50.848063 140529919506176 logging_writer.py:48] [76700] global_step=76700, grad_norm=0.2931475043296814, loss=3.8082194328308105
I0207 21:13:26.106512 140529927898880 logging_writer.py:48] [76800] global_step=76800, grad_norm=0.2777557373046875, loss=3.7220218181610107
I0207 21:14:01.340967 140529919506176 logging_writer.py:48] [76900] global_step=76900, grad_norm=0.26186007261276245, loss=3.843862295150757
I0207 21:14:36.634877 140529927898880 logging_writer.py:48] [77000] global_step=77000, grad_norm=0.2913214862346649, loss=3.7769594192504883
I0207 21:15:11.902638 140529919506176 logging_writer.py:48] [77100] global_step=77100, grad_norm=0.2577379643917084, loss=3.7728488445281982
I0207 21:15:47.171644 140529927898880 logging_writer.py:48] [77200] global_step=77200, grad_norm=0.29485616087913513, loss=3.7836954593658447
I0207 21:16:22.432448 140529919506176 logging_writer.py:48] [77300] global_step=77300, grad_norm=0.28409191966056824, loss=3.791896343231201
I0207 21:16:57.684559 140529927898880 logging_writer.py:48] [77400] global_step=77400, grad_norm=0.28740280866622925, loss=3.7798802852630615
I0207 21:17:32.949881 140529919506176 logging_writer.py:48] [77500] global_step=77500, grad_norm=0.2840695083141327, loss=3.756072998046875
I0207 21:18:08.203052 140529927898880 logging_writer.py:48] [77600] global_step=77600, grad_norm=0.27712565660476685, loss=3.7881886959075928
I0207 21:18:43.466786 140529919506176 logging_writer.py:48] [77700] global_step=77700, grad_norm=0.2700548768043518, loss=3.8046963214874268
I0207 21:19:18.726592 140529927898880 logging_writer.py:48] [77800] global_step=77800, grad_norm=0.27038484811782837, loss=3.764195203781128
I0207 21:19:53.976782 140529919506176 logging_writer.py:48] [77900] global_step=77900, grad_norm=0.29904699325561523, loss=3.817391872406006
I0207 21:20:29.268667 140529927898880 logging_writer.py:48] [78000] global_step=78000, grad_norm=0.2974614202976227, loss=3.7621965408325195
I0207 21:21:04.572960 140529919506176 logging_writer.py:48] [78100] global_step=78100, grad_norm=0.26585379242897034, loss=3.7706656455993652
I0207 21:21:39.887834 140529927898880 logging_writer.py:48] [78200] global_step=78200, grad_norm=0.3061676025390625, loss=3.8147175312042236
I0207 21:22:15.213075 140529919506176 logging_writer.py:48] [78300] global_step=78300, grad_norm=0.28019729256629944, loss=3.8070554733276367
I0207 21:22:50.476550 140529927898880 logging_writer.py:48] [78400] global_step=78400, grad_norm=0.2857000231742859, loss=3.794311046600342
I0207 21:23:25.745862 140529919506176 logging_writer.py:48] [78500] global_step=78500, grad_norm=0.2723581790924072, loss=3.755058765411377
I0207 21:24:01.006884 140529927898880 logging_writer.py:48] [78600] global_step=78600, grad_norm=0.26666632294654846, loss=3.706686496734619
I0207 21:24:13.406106 140699726837568 spec.py:321] Evaluating on the training split.
I0207 21:24:16.456049 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 21:27:20.587261 140699726837568 spec.py:333] Evaluating on the validation split.
I0207 21:27:23.339260 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 21:30:04.137858 140699726837568 spec.py:349] Evaluating on the test split.
I0207 21:30:06.887381 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 21:32:38.406962 140699726837568 submission_runner.py:408] Time since start: 45916.01s, 	Step: 78637, 	{'train/accuracy': 0.6790190935134888, 'train/loss': 1.6930485963821411, 'train/bleu': 34.606128151781625, 'validation/accuracy': 0.6853355765342712, 'validation/loss': 1.6412664651870728, 'validation/bleu': 30.084906994619733, 'validation/num_examples': 3000, 'test/accuracy': 0.7021439671516418, 'test/loss': 1.543318271636963, 'test/bleu': 30.256775334023622, 'test/num_examples': 3003, 'score': 27749.411451101303, 'total_duration': 45916.00671863556, 'accumulated_submission_time': 27749.411451101303, 'accumulated_eval_time': 18163.086530447006, 'accumulated_logging_time': 1.0375986099243164}
I0207 21:32:38.434331 140529919506176 logging_writer.py:48] [78637] accumulated_eval_time=18163.086530, accumulated_logging_time=1.037599, accumulated_submission_time=27749.411451, global_step=78637, preemption_count=0, score=27749.411451, test/accuracy=0.702144, test/bleu=30.256775, test/loss=1.543318, test/num_examples=3003, total_duration=45916.006719, train/accuracy=0.679019, train/bleu=34.606128, train/loss=1.693049, validation/accuracy=0.685336, validation/bleu=30.084907, validation/loss=1.641266, validation/num_examples=3000
I0207 21:33:00.919653 140529927898880 logging_writer.py:48] [78700] global_step=78700, grad_norm=0.2851482927799225, loss=3.782489538192749
I0207 21:33:36.090045 140529919506176 logging_writer.py:48] [78800] global_step=78800, grad_norm=0.30135753750801086, loss=3.8042709827423096
I0207 21:34:11.290181 140529927898880 logging_writer.py:48] [78900] global_step=78900, grad_norm=0.28771737217903137, loss=3.771083354949951
I0207 21:34:46.551223 140529919506176 logging_writer.py:48] [79000] global_step=79000, grad_norm=0.27163320779800415, loss=3.7557358741760254
I0207 21:35:21.801243 140529927898880 logging_writer.py:48] [79100] global_step=79100, grad_norm=0.29313337802886963, loss=3.721111536026001
I0207 21:35:57.067372 140529919506176 logging_writer.py:48] [79200] global_step=79200, grad_norm=0.29131466150283813, loss=3.80356502532959
I0207 21:36:32.319955 140529927898880 logging_writer.py:48] [79300] global_step=79300, grad_norm=0.2868538200855255, loss=3.762601375579834
I0207 21:37:07.583578 140529919506176 logging_writer.py:48] [79400] global_step=79400, grad_norm=0.3231106400489807, loss=3.7674129009246826
I0207 21:37:42.809771 140529927898880 logging_writer.py:48] [79500] global_step=79500, grad_norm=0.27604588866233826, loss=3.7411415576934814
I0207 21:38:18.105499 140529919506176 logging_writer.py:48] [79600] global_step=79600, grad_norm=0.28500068187713623, loss=3.808310031890869
I0207 21:38:53.382260 140529927898880 logging_writer.py:48] [79700] global_step=79700, grad_norm=0.31096726655960083, loss=3.7747459411621094
I0207 21:39:28.654801 140529919506176 logging_writer.py:48] [79800] global_step=79800, grad_norm=0.2926483154296875, loss=3.768171548843384
I0207 21:40:03.916435 140529927898880 logging_writer.py:48] [79900] global_step=79900, grad_norm=0.32099682092666626, loss=3.792100191116333
I0207 21:40:39.207436 140529919506176 logging_writer.py:48] [80000] global_step=80000, grad_norm=0.2883526384830475, loss=3.8019022941589355
I0207 21:41:14.477906 140529927898880 logging_writer.py:48] [80100] global_step=80100, grad_norm=0.30818837881088257, loss=3.748067617416382
I0207 21:41:49.706017 140529919506176 logging_writer.py:48] [80200] global_step=80200, grad_norm=0.278583824634552, loss=3.797856092453003
I0207 21:42:24.996453 140529927898880 logging_writer.py:48] [80300] global_step=80300, grad_norm=0.2744154632091522, loss=3.8003885746002197
I0207 21:43:00.303758 140529919506176 logging_writer.py:48] [80400] global_step=80400, grad_norm=0.2835211753845215, loss=3.7416200637817383
I0207 21:43:35.583530 140529927898880 logging_writer.py:48] [80500] global_step=80500, grad_norm=0.3021153509616852, loss=3.8630871772766113
I0207 21:44:10.842103 140529919506176 logging_writer.py:48] [80600] global_step=80600, grad_norm=0.29379037022590637, loss=3.749497652053833
I0207 21:44:46.140009 140529927898880 logging_writer.py:48] [80700] global_step=80700, grad_norm=0.2780782878398895, loss=3.7849197387695312
I0207 21:45:21.437903 140529919506176 logging_writer.py:48] [80800] global_step=80800, grad_norm=0.28585153818130493, loss=3.7629733085632324
I0207 21:45:56.701581 140529927898880 logging_writer.py:48] [80900] global_step=80900, grad_norm=0.281645804643631, loss=3.7097601890563965
I0207 21:46:31.983090 140529919506176 logging_writer.py:48] [81000] global_step=81000, grad_norm=0.2785704731941223, loss=3.823786497116089
I0207 21:46:38.749442 140699726837568 spec.py:321] Evaluating on the training split.
I0207 21:46:41.788024 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 21:49:40.224219 140699726837568 spec.py:333] Evaluating on the validation split.
I0207 21:49:42.971512 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 21:52:13.923367 140699726837568 spec.py:349] Evaluating on the test split.
I0207 21:52:16.667245 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 21:54:34.775523 140699726837568 submission_runner.py:408] Time since start: 47232.38s, 	Step: 81021, 	{'train/accuracy': 0.68350750207901, 'train/loss': 1.6653733253479004, 'train/bleu': 33.94700163998571, 'validation/accuracy': 0.6868730783462524, 'validation/loss': 1.628083348274231, 'validation/bleu': 30.37649273654434, 'validation/num_examples': 3000, 'test/accuracy': 0.7027831077575684, 'test/loss': 1.532280445098877, 'test/bleu': 30.454600655162768, 'test/num_examples': 3003, 'score': 28589.642166614532, 'total_duration': 47232.37526059151, 'accumulated_submission_time': 28589.642166614532, 'accumulated_eval_time': 18639.112541913986, 'accumulated_logging_time': 1.0760555267333984}
I0207 21:54:34.802722 140529927898880 logging_writer.py:48] [81021] accumulated_eval_time=18639.112542, accumulated_logging_time=1.076056, accumulated_submission_time=28589.642167, global_step=81021, preemption_count=0, score=28589.642167, test/accuracy=0.702783, test/bleu=30.454601, test/loss=1.532280, test/num_examples=3003, total_duration=47232.375261, train/accuracy=0.683508, train/bleu=33.947002, train/loss=1.665373, validation/accuracy=0.686873, validation/bleu=30.376493, validation/loss=1.628083, validation/num_examples=3000
I0207 21:55:02.932664 140529919506176 logging_writer.py:48] [81100] global_step=81100, grad_norm=0.28421205282211304, loss=3.785895824432373
I0207 21:55:38.163399 140529927898880 logging_writer.py:48] [81200] global_step=81200, grad_norm=0.313787043094635, loss=3.7815847396850586
I0207 21:56:13.410373 140529919506176 logging_writer.py:48] [81300] global_step=81300, grad_norm=0.2902326285839081, loss=3.774533748626709
I0207 21:56:48.680698 140529927898880 logging_writer.py:48] [81400] global_step=81400, grad_norm=0.29354172945022583, loss=3.7713098526000977
I0207 21:57:23.957228 140529919506176 logging_writer.py:48] [81500] global_step=81500, grad_norm=0.2909094989299774, loss=3.7865359783172607
I0207 21:57:59.214694 140529927898880 logging_writer.py:48] [81600] global_step=81600, grad_norm=0.29652804136276245, loss=3.7511918544769287
I0207 21:58:34.495742 140529919506176 logging_writer.py:48] [81700] global_step=81700, grad_norm=0.2843526005744934, loss=3.7605338096618652
I0207 21:59:09.752249 140529927898880 logging_writer.py:48] [81800] global_step=81800, grad_norm=0.273443341255188, loss=3.7501230239868164
I0207 21:59:45.039542 140529919506176 logging_writer.py:48] [81900] global_step=81900, grad_norm=0.2952328324317932, loss=3.7377610206604004
I0207 22:00:20.301837 140529927898880 logging_writer.py:48] [82000] global_step=82000, grad_norm=0.3018477261066437, loss=3.806541681289673
I0207 22:00:55.541187 140529919506176 logging_writer.py:48] [82100] global_step=82100, grad_norm=0.2847824692726135, loss=3.691113233566284
I0207 22:01:30.816101 140529927898880 logging_writer.py:48] [82200] global_step=82200, grad_norm=0.2756144404411316, loss=3.761136293411255
I0207 22:02:06.086940 140529919506176 logging_writer.py:48] [82300] global_step=82300, grad_norm=0.32097378373146057, loss=3.7367355823516846
I0207 22:02:41.346374 140529927898880 logging_writer.py:48] [82400] global_step=82400, grad_norm=0.2952714264392853, loss=3.7750775814056396
I0207 22:03:16.605135 140529919506176 logging_writer.py:48] [82500] global_step=82500, grad_norm=0.30396464467048645, loss=3.7716522216796875
I0207 22:03:51.874556 140529927898880 logging_writer.py:48] [82600] global_step=82600, grad_norm=0.328683465719223, loss=3.8161814212799072
I0207 22:04:27.135397 140529919506176 logging_writer.py:48] [82700] global_step=82700, grad_norm=0.3024704158306122, loss=3.775400400161743
I0207 22:05:02.409777 140529927898880 logging_writer.py:48] [82800] global_step=82800, grad_norm=0.31521567702293396, loss=3.80844783782959
I0207 22:05:37.730458 140529919506176 logging_writer.py:48] [82900] global_step=82900, grad_norm=0.30748245120048523, loss=3.7537412643432617
I0207 22:06:13.068142 140529927898880 logging_writer.py:48] [83000] global_step=83000, grad_norm=0.29076430201530457, loss=3.7706429958343506
I0207 22:06:48.296640 140529919506176 logging_writer.py:48] [83100] global_step=83100, grad_norm=0.2954343855381012, loss=3.7531821727752686
I0207 22:07:23.607006 140529927898880 logging_writer.py:48] [83200] global_step=83200, grad_norm=0.3008708357810974, loss=3.7572579383850098
I0207 22:07:58.917514 140529919506176 logging_writer.py:48] [83300] global_step=83300, grad_norm=0.29470834136009216, loss=3.7471694946289062
I0207 22:08:34.218238 140529927898880 logging_writer.py:48] [83400] global_step=83400, grad_norm=0.29303789138793945, loss=3.748115062713623
I0207 22:08:35.001992 140699726837568 spec.py:321] Evaluating on the training split.
I0207 22:08:38.036891 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 22:11:48.239917 140699726837568 spec.py:333] Evaluating on the validation split.
I0207 22:11:50.985522 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 22:14:25.171903 140699726837568 spec.py:349] Evaluating on the test split.
I0207 22:14:27.909352 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 22:16:51.831079 140699726837568 submission_runner.py:408] Time since start: 48569.43s, 	Step: 83404, 	{'train/accuracy': 0.689186692237854, 'train/loss': 1.628559947013855, 'train/bleu': 34.95898421051862, 'validation/accuracy': 0.6865258812904358, 'validation/loss': 1.6255191564559937, 'validation/bleu': 29.923109223085948, 'validation/num_examples': 3000, 'test/accuracy': 0.7037476301193237, 'test/loss': 1.527652382850647, 'test/bleu': 30.497443854719528, 'test/num_examples': 3003, 'score': 29429.755152463913, 'total_duration': 48569.43083524704, 'accumulated_submission_time': 29429.755152463913, 'accumulated_eval_time': 19135.941576480865, 'accumulated_logging_time': 1.1130588054656982}
I0207 22:16:51.857339 140529919506176 logging_writer.py:48] [83404] accumulated_eval_time=19135.941576, accumulated_logging_time=1.113059, accumulated_submission_time=29429.755152, global_step=83404, preemption_count=0, score=29429.755152, test/accuracy=0.703748, test/bleu=30.497444, test/loss=1.527652, test/num_examples=3003, total_duration=48569.430835, train/accuracy=0.689187, train/bleu=34.958984, train/loss=1.628560, validation/accuracy=0.686526, validation/bleu=29.923109, validation/loss=1.625519, validation/num_examples=3000
I0207 22:17:25.965741 140529927898880 logging_writer.py:48] [83500] global_step=83500, grad_norm=0.281499981880188, loss=3.7109289169311523
I0207 22:18:01.197241 140529919506176 logging_writer.py:48] [83600] global_step=83600, grad_norm=0.29842910170555115, loss=3.7413711547851562
I0207 22:18:36.417942 140529927898880 logging_writer.py:48] [83700] global_step=83700, grad_norm=0.30728694796562195, loss=3.820404529571533
I0207 22:19:11.637993 140529919506176 logging_writer.py:48] [83800] global_step=83800, grad_norm=0.2832493782043457, loss=3.7622413635253906
I0207 22:19:46.900629 140529927898880 logging_writer.py:48] [83900] global_step=83900, grad_norm=0.31397441029548645, loss=3.7629098892211914
I0207 22:20:22.148945 140529919506176 logging_writer.py:48] [84000] global_step=84000, grad_norm=0.2853223383426666, loss=3.783303737640381
I0207 22:20:57.405872 140529927898880 logging_writer.py:48] [84100] global_step=84100, grad_norm=0.2878260016441345, loss=3.7724359035491943
I0207 22:21:32.658472 140529919506176 logging_writer.py:48] [84200] global_step=84200, grad_norm=0.309304803609848, loss=3.781769275665283
I0207 22:22:07.913095 140529927898880 logging_writer.py:48] [84300] global_step=84300, grad_norm=0.27909865975379944, loss=3.7040038108825684
I0207 22:22:43.169373 140529919506176 logging_writer.py:48] [84400] global_step=84400, grad_norm=0.2805023193359375, loss=3.7752926349639893
I0207 22:23:18.432415 140529927898880 logging_writer.py:48] [84500] global_step=84500, grad_norm=0.29555246233940125, loss=3.729051113128662
I0207 22:23:53.697242 140529919506176 logging_writer.py:48] [84600] global_step=84600, grad_norm=0.2769680917263031, loss=3.7343084812164307
I0207 22:24:28.936499 140529927898880 logging_writer.py:48] [84700] global_step=84700, grad_norm=0.29930421710014343, loss=3.763838768005371
I0207 22:25:04.211246 140529919506176 logging_writer.py:48] [84800] global_step=84800, grad_norm=0.2904413938522339, loss=3.7528250217437744
I0207 22:25:39.481902 140529927898880 logging_writer.py:48] [84900] global_step=84900, grad_norm=0.28944873809814453, loss=3.779283285140991
I0207 22:26:14.734172 140529919506176 logging_writer.py:48] [85000] global_step=85000, grad_norm=0.27624574303627014, loss=3.7059519290924072
I0207 22:26:49.963229 140529927898880 logging_writer.py:48] [85100] global_step=85100, grad_norm=0.32059207558631897, loss=3.7355902194976807
I0207 22:27:25.237703 140529919506176 logging_writer.py:48] [85200] global_step=85200, grad_norm=0.3132266402244568, loss=3.7480623722076416
I0207 22:28:00.583483 140529927898880 logging_writer.py:48] [85300] global_step=85300, grad_norm=0.2814166247844696, loss=3.706392526626587
I0207 22:28:36.022416 140529919506176 logging_writer.py:48] [85400] global_step=85400, grad_norm=0.2903796136379242, loss=3.797086238861084
I0207 22:29:11.343804 140529927898880 logging_writer.py:48] [85500] global_step=85500, grad_norm=0.3042834997177124, loss=3.7156100273132324
I0207 22:29:46.604724 140529919506176 logging_writer.py:48] [85600] global_step=85600, grad_norm=0.2844514548778534, loss=3.7309908866882324
I0207 22:30:21.857353 140529927898880 logging_writer.py:48] [85700] global_step=85700, grad_norm=0.3012870252132416, loss=3.796750068664551
I0207 22:30:51.886227 140699726837568 spec.py:321] Evaluating on the training split.
I0207 22:30:54.934051 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 22:35:12.852422 140699726837568 spec.py:333] Evaluating on the validation split.
I0207 22:35:15.596221 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 22:38:37.998864 140699726837568 spec.py:349] Evaluating on the test split.
I0207 22:38:40.744199 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 22:41:37.888712 140699726837568 submission_runner.py:408] Time since start: 50055.49s, 	Step: 85787, 	{'train/accuracy': 0.6846340894699097, 'train/loss': 1.6518151760101318, 'train/bleu': 34.29854509964452, 'validation/accuracy': 0.6845916509628296, 'validation/loss': 1.627881407737732, 'validation/bleu': 29.913907501321336, 'validation/num_examples': 3000, 'test/accuracy': 0.7035732865333557, 'test/loss': 1.526257038116455, 'test/bleu': 30.163936574095636, 'test/num_examples': 3003, 'score': 30269.69832634926, 'total_duration': 50055.488450050354, 'accumulated_submission_time': 30269.69832634926, 'accumulated_eval_time': 19781.944012403488, 'accumulated_logging_time': 1.1491875648498535}
I0207 22:41:37.916009 140529919506176 logging_writer.py:48] [85787] accumulated_eval_time=19781.944012, accumulated_logging_time=1.149188, accumulated_submission_time=30269.698326, global_step=85787, preemption_count=0, score=30269.698326, test/accuracy=0.703573, test/bleu=30.163937, test/loss=1.526257, test/num_examples=3003, total_duration=50055.488450, train/accuracy=0.684634, train/bleu=34.298545, train/loss=1.651815, validation/accuracy=0.684592, validation/bleu=29.913908, validation/loss=1.627881, validation/num_examples=3000
I0207 22:41:42.852039 140529927898880 logging_writer.py:48] [85800] global_step=85800, grad_norm=0.3369380235671997, loss=3.737271308898926
I0207 22:42:17.967856 140529919506176 logging_writer.py:48] [85900] global_step=85900, grad_norm=0.30240610241889954, loss=3.7108154296875
I0207 22:42:53.132117 140529927898880 logging_writer.py:48] [86000] global_step=86000, grad_norm=0.29848021268844604, loss=3.752509117126465
I0207 22:43:28.350570 140529919506176 logging_writer.py:48] [86100] global_step=86100, grad_norm=0.2878609597682953, loss=3.7029812335968018
I0207 22:44:03.612139 140529927898880 logging_writer.py:48] [86200] global_step=86200, grad_norm=0.2878824472427368, loss=3.733008623123169
I0207 22:44:38.861684 140529919506176 logging_writer.py:48] [86300] global_step=86300, grad_norm=0.27878519892692566, loss=3.7421555519104004
I0207 22:45:14.135153 140529927898880 logging_writer.py:48] [86400] global_step=86400, grad_norm=0.3049050271511078, loss=3.762495279312134
I0207 22:45:49.397113 140529919506176 logging_writer.py:48] [86500] global_step=86500, grad_norm=0.3166138231754303, loss=3.7859621047973633
I0207 22:46:24.679637 140529927898880 logging_writer.py:48] [86600] global_step=86600, grad_norm=0.31669938564300537, loss=3.7775959968566895
I0207 22:46:59.933398 140529919506176 logging_writer.py:48] [86700] global_step=86700, grad_norm=0.29446345567703247, loss=3.7416749000549316
I0207 22:47:35.211494 140529927898880 logging_writer.py:48] [86800] global_step=86800, grad_norm=0.3003898561000824, loss=3.7144274711608887
I0207 22:48:10.477256 140529919506176 logging_writer.py:48] [86900] global_step=86900, grad_norm=0.2901047170162201, loss=3.7535400390625
I0207 22:48:45.785714 140529927898880 logging_writer.py:48] [87000] global_step=87000, grad_norm=0.28228071331977844, loss=3.678919553756714
I0207 22:49:21.064944 140529919506176 logging_writer.py:48] [87100] global_step=87100, grad_norm=0.29486319422721863, loss=3.6553759574890137
I0207 22:49:56.404797 140529927898880 logging_writer.py:48] [87200] global_step=87200, grad_norm=0.28632864356040955, loss=3.7171378135681152
I0207 22:50:31.683761 140529919506176 logging_writer.py:48] [87300] global_step=87300, grad_norm=0.3068377673625946, loss=3.707749605178833
I0207 22:51:06.968165 140529927898880 logging_writer.py:48] [87400] global_step=87400, grad_norm=0.28630155324935913, loss=3.7834579944610596
I0207 22:51:42.248616 140529919506176 logging_writer.py:48] [87500] global_step=87500, grad_norm=0.3180239498615265, loss=3.6966617107391357
I0207 22:52:17.519776 140529927898880 logging_writer.py:48] [87600] global_step=87600, grad_norm=0.2926349639892578, loss=3.710832357406616
I0207 22:52:52.823411 140529919506176 logging_writer.py:48] [87700] global_step=87700, grad_norm=0.29504644870758057, loss=3.7112832069396973
I0207 22:53:28.082001 140529927898880 logging_writer.py:48] [87800] global_step=87800, grad_norm=0.3294126093387604, loss=3.742176055908203
I0207 22:54:03.360808 140529919506176 logging_writer.py:48] [87900] global_step=87900, grad_norm=0.3042157292366028, loss=3.726299285888672
I0207 22:54:38.710199 140529927898880 logging_writer.py:48] [88000] global_step=88000, grad_norm=0.2878745496273041, loss=3.7150018215179443
I0207 22:55:14.030362 140529919506176 logging_writer.py:48] [88100] global_step=88100, grad_norm=0.3196661174297333, loss=3.712024211883545
I0207 22:55:38.104344 140699726837568 spec.py:321] Evaluating on the training split.
I0207 22:55:41.164483 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 22:59:31.327762 140699726837568 spec.py:333] Evaluating on the validation split.
I0207 22:59:34.075965 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 23:03:27.739363 140699726837568 spec.py:349] Evaluating on the test split.
I0207 23:03:30.473735 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 23:06:29.478356 140699726837568 submission_runner.py:408] Time since start: 51547.08s, 	Step: 88170, 	{'train/accuracy': 0.7073625326156616, 'train/loss': 1.5367120504379272, 'train/bleu': 36.36983182542021, 'validation/accuracy': 0.6875426173210144, 'validation/loss': 1.6262383460998535, 'validation/bleu': 30.037688410889466, 'validation/num_examples': 3000, 'test/accuracy': 0.7039335370063782, 'test/loss': 1.526638388633728, 'test/bleu': 30.32476310275814, 'test/num_examples': 3003, 'score': 31109.80075287819, 'total_duration': 51547.07810497284, 'accumulated_submission_time': 31109.80075287819, 'accumulated_eval_time': 20433.317983865738, 'accumulated_logging_time': 1.1875977516174316}
I0207 23:06:29.506142 140529927898880 logging_writer.py:48] [88170] accumulated_eval_time=20433.317984, accumulated_logging_time=1.187598, accumulated_submission_time=31109.800753, global_step=88170, preemption_count=0, score=31109.800753, test/accuracy=0.703934, test/bleu=30.324763, test/loss=1.526638, test/num_examples=3003, total_duration=51547.078105, train/accuracy=0.707363, train/bleu=36.369832, train/loss=1.536712, validation/accuracy=0.687543, validation/bleu=30.037688, validation/loss=1.626238, validation/num_examples=3000
I0207 23:06:40.396356 140529919506176 logging_writer.py:48] [88200] global_step=88200, grad_norm=0.3140818476676941, loss=3.785524606704712
I0207 23:07:15.520995 140529927898880 logging_writer.py:48] [88300] global_step=88300, grad_norm=0.3045656085014343, loss=3.7964084148406982
I0207 23:07:50.715081 140529919506176 logging_writer.py:48] [88400] global_step=88400, grad_norm=0.3150157630443573, loss=3.7464993000030518
I0207 23:08:25.943618 140529927898880 logging_writer.py:48] [88500] global_step=88500, grad_norm=0.3170919418334961, loss=3.704944610595703
I0207 23:09:01.202952 140529919506176 logging_writer.py:48] [88600] global_step=88600, grad_norm=0.3108120262622833, loss=3.748530864715576
I0207 23:09:36.432092 140529927898880 logging_writer.py:48] [88700] global_step=88700, grad_norm=0.30808892846107483, loss=3.7570226192474365
I0207 23:10:11.729234 140529919506176 logging_writer.py:48] [88800] global_step=88800, grad_norm=0.3001081347465515, loss=3.7716245651245117
I0207 23:10:46.986741 140529927898880 logging_writer.py:48] [88900] global_step=88900, grad_norm=0.3133423626422882, loss=3.747236967086792
I0207 23:11:22.249030 140529919506176 logging_writer.py:48] [89000] global_step=89000, grad_norm=0.3203834295272827, loss=3.743976593017578
I0207 23:11:57.536794 140529927898880 logging_writer.py:48] [89100] global_step=89100, grad_norm=0.30221933126449585, loss=3.7305514812469482
I0207 23:12:32.804854 140529919506176 logging_writer.py:48] [89200] global_step=89200, grad_norm=0.2935863435268402, loss=3.7428903579711914
I0207 23:13:08.081207 140529927898880 logging_writer.py:48] [89300] global_step=89300, grad_norm=0.28913968801498413, loss=3.6889278888702393
I0207 23:13:43.386013 140529919506176 logging_writer.py:48] [89400] global_step=89400, grad_norm=0.31940019130706787, loss=3.7776525020599365
I0207 23:14:18.668155 140529927898880 logging_writer.py:48] [89500] global_step=89500, grad_norm=0.31303471326828003, loss=3.706014633178711
I0207 23:14:53.944222 140529919506176 logging_writer.py:48] [89600] global_step=89600, grad_norm=0.31435030698776245, loss=3.6786296367645264
I0207 23:15:29.205596 140529927898880 logging_writer.py:48] [89700] global_step=89700, grad_norm=0.2886626720428467, loss=3.695754289627075
I0207 23:16:04.460612 140529919506176 logging_writer.py:48] [89800] global_step=89800, grad_norm=0.3059439957141876, loss=3.6804733276367188
I0207 23:16:39.723855 140529927898880 logging_writer.py:48] [89900] global_step=89900, grad_norm=0.288597971200943, loss=3.690330982208252
I0207 23:17:15.021034 140529919506176 logging_writer.py:48] [90000] global_step=90000, grad_norm=0.2938903272151947, loss=3.6863820552825928
I0207 23:17:50.276656 140529927898880 logging_writer.py:48] [90100] global_step=90100, grad_norm=0.3076516389846802, loss=3.7480437755584717
I0207 23:18:25.582432 140529919506176 logging_writer.py:48] [90200] global_step=90200, grad_norm=0.34901610016822815, loss=3.7545180320739746
I0207 23:19:00.881349 140529927898880 logging_writer.py:48] [90300] global_step=90300, grad_norm=0.3064228892326355, loss=3.7214274406433105
I0207 23:19:36.181191 140529919506176 logging_writer.py:48] [90400] global_step=90400, grad_norm=0.3184438943862915, loss=3.6895384788513184
I0207 23:20:11.460561 140529927898880 logging_writer.py:48] [90500] global_step=90500, grad_norm=0.3083992898464203, loss=3.7290780544281006
I0207 23:20:29.517977 140699726837568 spec.py:321] Evaluating on the training split.
I0207 23:20:32.555992 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 23:24:33.101435 140699726837568 spec.py:333] Evaluating on the validation split.
I0207 23:24:35.830544 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 23:27:59.938559 140699726837568 spec.py:349] Evaluating on the test split.
I0207 23:28:02.696199 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 23:31:01.305264 140699726837568 submission_runner.py:408] Time since start: 53018.91s, 	Step: 90553, 	{'train/accuracy': 0.6886942386627197, 'train/loss': 1.6172972917556763, 'train/bleu': 34.94074502185821, 'validation/accuracy': 0.6868730783462524, 'validation/loss': 1.6169028282165527, 'validation/bleu': 29.63990232421176, 'validation/num_examples': 3000, 'test/accuracy': 0.7055139541625977, 'test/loss': 1.5144983530044556, 'test/bleu': 30.369156672036247, 'test/num_examples': 3003, 'score': 31949.730099201202, 'total_duration': 53018.90501999855, 'accumulated_submission_time': 31949.730099201202, 'accumulated_eval_time': 21065.105221271515, 'accumulated_logging_time': 1.225377082824707}
I0207 23:31:01.333213 140529919506176 logging_writer.py:48] [90553] accumulated_eval_time=21065.105221, accumulated_logging_time=1.225377, accumulated_submission_time=31949.730099, global_step=90553, preemption_count=0, score=31949.730099, test/accuracy=0.705514, test/bleu=30.369157, test/loss=1.514498, test/num_examples=3003, total_duration=53018.905020, train/accuracy=0.688694, train/bleu=34.940745, train/loss=1.617297, validation/accuracy=0.686873, validation/bleu=29.639902, validation/loss=1.616903, validation/num_examples=3000
I0207 23:31:18.211018 140529927898880 logging_writer.py:48] [90600] global_step=90600, grad_norm=0.3088034987449646, loss=3.740828037261963
I0207 23:31:53.334013 140529919506176 logging_writer.py:48] [90700] global_step=90700, grad_norm=0.30445581674575806, loss=3.7613744735717773
I0207 23:32:28.520906 140529927898880 logging_writer.py:48] [90800] global_step=90800, grad_norm=0.33086535334587097, loss=3.7858476638793945
I0207 23:33:03.768515 140529919506176 logging_writer.py:48] [90900] global_step=90900, grad_norm=0.30982041358947754, loss=3.721054792404175
I0207 23:33:39.034528 140529927898880 logging_writer.py:48] [91000] global_step=91000, grad_norm=0.30400145053863525, loss=3.7028865814208984
I0207 23:34:14.359897 140529919506176 logging_writer.py:48] [91100] global_step=91100, grad_norm=0.33833685517311096, loss=3.7301084995269775
I0207 23:34:49.653519 140529927898880 logging_writer.py:48] [91200] global_step=91200, grad_norm=0.30963507294654846, loss=3.68200421333313
I0207 23:35:24.928122 140529919506176 logging_writer.py:48] [91300] global_step=91300, grad_norm=0.28198346495628357, loss=3.66886568069458
I0207 23:36:00.195958 140529927898880 logging_writer.py:48] [91400] global_step=91400, grad_norm=0.3266221284866333, loss=3.7390170097351074
I0207 23:36:35.487175 140529919506176 logging_writer.py:48] [91500] global_step=91500, grad_norm=0.30434054136276245, loss=3.7187142372131348
I0207 23:37:10.777893 140529927898880 logging_writer.py:48] [91600] global_step=91600, grad_norm=0.293782114982605, loss=3.7208287715911865
I0207 23:37:46.086801 140529919506176 logging_writer.py:48] [91700] global_step=91700, grad_norm=0.29977527260780334, loss=3.6868839263916016
I0207 23:38:21.363218 140529927898880 logging_writer.py:48] [91800] global_step=91800, grad_norm=0.29460179805755615, loss=3.6994662284851074
I0207 23:38:56.639036 140529919506176 logging_writer.py:48] [91900] global_step=91900, grad_norm=0.3287184536457062, loss=3.7177672386169434
I0207 23:39:31.917562 140529927898880 logging_writer.py:48] [92000] global_step=92000, grad_norm=0.3557654321193695, loss=3.749720573425293
I0207 23:40:07.168526 140529919506176 logging_writer.py:48] [92100] global_step=92100, grad_norm=0.3078887164592743, loss=3.7754271030426025
I0207 23:40:42.414141 140529927898880 logging_writer.py:48] [92200] global_step=92200, grad_norm=0.2905748188495636, loss=3.684523820877075
I0207 23:41:17.711074 140529919506176 logging_writer.py:48] [92300] global_step=92300, grad_norm=0.3142370283603668, loss=3.7025399208068848
I0207 23:41:52.988426 140529927898880 logging_writer.py:48] [92400] global_step=92400, grad_norm=0.30418989062309265, loss=3.688598155975342
I0207 23:42:28.259634 140529919506176 logging_writer.py:48] [92500] global_step=92500, grad_norm=0.32392820715904236, loss=3.7614552974700928
I0207 23:43:03.494426 140529927898880 logging_writer.py:48] [92600] global_step=92600, grad_norm=0.31829696893692017, loss=3.7922189235687256
I0207 23:43:38.749099 140529919506176 logging_writer.py:48] [92700] global_step=92700, grad_norm=0.29598698019981384, loss=3.6852080821990967
I0207 23:44:14.010505 140529927898880 logging_writer.py:48] [92800] global_step=92800, grad_norm=0.3151833713054657, loss=3.7265844345092773
I0207 23:44:49.272770 140529919506176 logging_writer.py:48] [92900] global_step=92900, grad_norm=0.3161076605319977, loss=3.7033345699310303
I0207 23:45:01.328885 140699726837568 spec.py:321] Evaluating on the training split.
I0207 23:45:04.370131 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 23:48:02.146500 140699726837568 spec.py:333] Evaluating on the validation split.
I0207 23:48:04.881946 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 23:50:42.042743 140699726837568 spec.py:349] Evaluating on the test split.
I0207 23:50:44.794059 140699726837568 workload.py:181] Translating evaluation dataset.
I0207 23:53:09.343114 140699726837568 submission_runner.py:408] Time since start: 54346.94s, 	Step: 92936, 	{'train/accuracy': 0.6906367540359497, 'train/loss': 1.6220505237579346, 'train/bleu': 35.15019654432984, 'validation/accuracy': 0.6880013942718506, 'validation/loss': 1.6124529838562012, 'validation/bleu': 30.125960524935937, 'validation/num_examples': 3000, 'test/accuracy': 0.7075939774513245, 'test/loss': 1.5099539756774902, 'test/bleu': 30.907346055474644, 'test/num_examples': 3003, 'score': 32789.64198088646, 'total_duration': 54346.94282460213, 'accumulated_submission_time': 32789.64198088646, 'accumulated_eval_time': 21553.11935710907, 'accumulated_logging_time': 1.2633757591247559}
I0207 23:53:09.376631 140529927898880 logging_writer.py:48] [92936] accumulated_eval_time=21553.119357, accumulated_logging_time=1.263376, accumulated_submission_time=32789.641981, global_step=92936, preemption_count=0, score=32789.641981, test/accuracy=0.707594, test/bleu=30.907346, test/loss=1.509954, test/num_examples=3003, total_duration=54346.942825, train/accuracy=0.690637, train/bleu=35.150197, train/loss=1.622051, validation/accuracy=0.688001, validation/bleu=30.125961, validation/loss=1.612453, validation/num_examples=3000
I0207 23:53:32.249382 140529919506176 logging_writer.py:48] [93000] global_step=93000, grad_norm=0.30945178866386414, loss=3.68464732170105
I0207 23:54:07.452878 140529927898880 logging_writer.py:48] [93100] global_step=93100, grad_norm=0.3061622083187103, loss=3.664700746536255
I0207 23:54:42.689804 140529919506176 logging_writer.py:48] [93200] global_step=93200, grad_norm=0.32843026518821716, loss=3.7341508865356445
I0207 23:55:17.932884 140529927898880 logging_writer.py:48] [93300] global_step=93300, grad_norm=0.31996095180511475, loss=3.746520519256592
I0207 23:55:53.169702 140529919506176 logging_writer.py:48] [93400] global_step=93400, grad_norm=0.3177040219306946, loss=3.745314598083496
I0207 23:56:28.422340 140529927898880 logging_writer.py:48] [93500] global_step=93500, grad_norm=0.30897819995880127, loss=3.725181818008423
I0207 23:57:03.718034 140529919506176 logging_writer.py:48] [93600] global_step=93600, grad_norm=0.3110201060771942, loss=3.72829008102417
I0207 23:57:38.977735 140529927898880 logging_writer.py:48] [93700] global_step=93700, grad_norm=0.3553377389907837, loss=3.7762367725372314
I0207 23:58:14.247555 140529919506176 logging_writer.py:48] [93800] global_step=93800, grad_norm=0.3486559987068176, loss=3.7972724437713623
I0207 23:58:49.513159 140529927898880 logging_writer.py:48] [93900] global_step=93900, grad_norm=0.31699225306510925, loss=3.685241937637329
I0207 23:59:24.813293 140529919506176 logging_writer.py:48] [94000] global_step=94000, grad_norm=0.3122538626194, loss=3.730666399002075
I0208 00:00:00.142483 140529927898880 logging_writer.py:48] [94100] global_step=94100, grad_norm=0.313021719455719, loss=3.7455806732177734
I0208 00:00:35.399468 140529919506176 logging_writer.py:48] [94200] global_step=94200, grad_norm=0.2986871302127838, loss=3.6910266876220703
I0208 00:01:10.669548 140529927898880 logging_writer.py:48] [94300] global_step=94300, grad_norm=0.3130381107330322, loss=3.6911442279815674
I0208 00:01:45.964277 140529919506176 logging_writer.py:48] [94400] global_step=94400, grad_norm=0.3176700174808502, loss=3.7588109970092773
I0208 00:02:21.267572 140529927898880 logging_writer.py:48] [94500] global_step=94500, grad_norm=0.30245959758758545, loss=3.704331874847412
I0208 00:02:56.586839 140529919506176 logging_writer.py:48] [94600] global_step=94600, grad_norm=0.3138597905635834, loss=3.7071146965026855
I0208 00:03:31.903534 140529927898880 logging_writer.py:48] [94700] global_step=94700, grad_norm=0.33021020889282227, loss=3.759371042251587
I0208 00:04:07.153558 140529919506176 logging_writer.py:48] [94800] global_step=94800, grad_norm=0.32646405696868896, loss=3.7735912799835205
I0208 00:04:42.427371 140529927898880 logging_writer.py:48] [94900] global_step=94900, grad_norm=0.29908642172813416, loss=3.749044179916382
I0208 00:05:17.702374 140529919506176 logging_writer.py:48] [95000] global_step=95000, grad_norm=0.3304430842399597, loss=3.7054195404052734
I0208 00:05:52.980823 140529927898880 logging_writer.py:48] [95100] global_step=95100, grad_norm=0.3038466274738312, loss=3.632652759552002
I0208 00:06:28.244202 140529919506176 logging_writer.py:48] [95200] global_step=95200, grad_norm=0.3195585310459137, loss=3.7572391033172607
I0208 00:07:03.527207 140529927898880 logging_writer.py:48] [95300] global_step=95300, grad_norm=0.34337982535362244, loss=3.720900774002075
I0208 00:07:09.603342 140699726837568 spec.py:321] Evaluating on the training split.
I0208 00:07:12.640172 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 00:10:25.323917 140699726837568 spec.py:333] Evaluating on the validation split.
I0208 00:10:28.042415 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 00:13:05.904015 140699726837568 spec.py:349] Evaluating on the test split.
I0208 00:13:08.640772 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 00:15:48.352699 140699726837568 submission_runner.py:408] Time since start: 55705.95s, 	Step: 95319, 	{'train/accuracy': 0.7033336758613586, 'train/loss': 1.5496033430099487, 'train/bleu': 35.7991515308961, 'validation/accuracy': 0.6894024610519409, 'validation/loss': 1.6060791015625, 'validation/bleu': 30.590930186553226, 'validation/num_examples': 3000, 'test/accuracy': 0.7068386673927307, 'test/loss': 1.5075197219848633, 'test/bleu': 30.556849668080478, 'test/num_examples': 3003, 'score': 33629.78120470047, 'total_duration': 55705.952450037, 'accumulated_submission_time': 33629.78120470047, 'accumulated_eval_time': 22071.868659973145, 'accumulated_logging_time': 1.3095154762268066}
I0208 00:15:48.380921 140529919506176 logging_writer.py:48] [95319] accumulated_eval_time=22071.868660, accumulated_logging_time=1.309515, accumulated_submission_time=33629.781205, global_step=95319, preemption_count=0, score=33629.781205, test/accuracy=0.706839, test/bleu=30.556850, test/loss=1.507520, test/num_examples=3003, total_duration=55705.952450, train/accuracy=0.703334, train/bleu=35.799152, train/loss=1.549603, validation/accuracy=0.689402, validation/bleu=30.590930, validation/loss=1.606079, validation/num_examples=3000
I0208 00:16:17.196324 140529927898880 logging_writer.py:48] [95400] global_step=95400, grad_norm=0.3190636932849884, loss=3.7166225910186768
I0208 00:16:52.391867 140529919506176 logging_writer.py:48] [95500] global_step=95500, grad_norm=0.31375426054000854, loss=3.715794563293457
I0208 00:17:27.672498 140529927898880 logging_writer.py:48] [95600] global_step=95600, grad_norm=0.3190035820007324, loss=3.6709938049316406
I0208 00:18:02.916321 140529919506176 logging_writer.py:48] [95700] global_step=95700, grad_norm=0.3288188874721527, loss=3.7093520164489746
I0208 00:18:38.175790 140529927898880 logging_writer.py:48] [95800] global_step=95800, grad_norm=0.3095267117023468, loss=3.708266019821167
I0208 00:19:13.473406 140529919506176 logging_writer.py:48] [95900] global_step=95900, grad_norm=0.324106901884079, loss=3.6972434520721436
I0208 00:19:48.754523 140529927898880 logging_writer.py:48] [96000] global_step=96000, grad_norm=0.3188852071762085, loss=3.757909059524536
I0208 00:20:24.035046 140529919506176 logging_writer.py:48] [96100] global_step=96100, grad_norm=0.32208091020584106, loss=3.6936111450195312
I0208 00:20:59.309542 140529927898880 logging_writer.py:48] [96200] global_step=96200, grad_norm=0.32991787791252136, loss=3.719703197479248
I0208 00:21:34.595600 140529919506176 logging_writer.py:48] [96300] global_step=96300, grad_norm=0.31960123777389526, loss=3.6838510036468506
I0208 00:22:09.898887 140529927898880 logging_writer.py:48] [96400] global_step=96400, grad_norm=0.36495792865753174, loss=3.75839900970459
I0208 00:22:45.184165 140529919506176 logging_writer.py:48] [96500] global_step=96500, grad_norm=0.3141196072101593, loss=3.6639835834503174
I0208 00:23:20.455188 140529927898880 logging_writer.py:48] [96600] global_step=96600, grad_norm=0.35633790493011475, loss=3.7222542762756348
I0208 00:23:55.700274 140529919506176 logging_writer.py:48] [96700] global_step=96700, grad_norm=0.3181166350841522, loss=3.758419990539551
I0208 00:24:30.973668 140529927898880 logging_writer.py:48] [96800] global_step=96800, grad_norm=0.32610222697257996, loss=3.715686321258545
I0208 00:25:06.232803 140529919506176 logging_writer.py:48] [96900] global_step=96900, grad_norm=0.33634647727012634, loss=3.690361261367798
I0208 00:25:41.479503 140529927898880 logging_writer.py:48] [97000] global_step=97000, grad_norm=0.3245576024055481, loss=3.7396416664123535
I0208 00:26:16.779983 140529919506176 logging_writer.py:48] [97100] global_step=97100, grad_norm=0.31412509083747864, loss=3.6726791858673096
I0208 00:26:52.040732 140529927898880 logging_writer.py:48] [97200] global_step=97200, grad_norm=0.3358171880245209, loss=3.7456412315368652
I0208 00:27:27.298403 140529919506176 logging_writer.py:48] [97300] global_step=97300, grad_norm=0.3260703682899475, loss=3.7239651679992676
I0208 00:28:02.568733 140529927898880 logging_writer.py:48] [97400] global_step=97400, grad_norm=0.3233586251735687, loss=3.666086435317993
I0208 00:28:37.819581 140529919506176 logging_writer.py:48] [97500] global_step=97500, grad_norm=0.3199683427810669, loss=3.6574349403381348
I0208 00:29:13.069933 140529927898880 logging_writer.py:48] [97600] global_step=97600, grad_norm=0.32495662569999695, loss=3.673525094985962
I0208 00:29:48.340335 140529919506176 logging_writer.py:48] [97700] global_step=97700, grad_norm=0.3310019373893738, loss=3.74963116645813
I0208 00:29:48.417610 140699726837568 spec.py:321] Evaluating on the training split.
I0208 00:29:51.456604 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 00:32:43.535531 140699726837568 spec.py:333] Evaluating on the validation split.
I0208 00:32:46.278903 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 00:35:25.898618 140699726837568 spec.py:349] Evaluating on the test split.
I0208 00:35:28.654071 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 00:37:50.894572 140699726837568 submission_runner.py:408] Time since start: 57028.49s, 	Step: 97702, 	{'train/accuracy': 0.6969262957572937, 'train/loss': 1.5804659128189087, 'train/bleu': 35.152333339015506, 'validation/accuracy': 0.6876417994499207, 'validation/loss': 1.6106321811676025, 'validation/bleu': 30.332051794275422, 'validation/num_examples': 3000, 'test/accuracy': 0.7058508992195129, 'test/loss': 1.5085893869400024, 'test/bleu': 30.394577530924465, 'test/num_examples': 3003, 'score': 34469.73361515999, 'total_duration': 57028.49427843094, 'accumulated_submission_time': 34469.73361515999, 'accumulated_eval_time': 22554.345523118973, 'accumulated_logging_time': 1.3491921424865723}
I0208 00:37:50.929253 140529927898880 logging_writer.py:48] [97702] accumulated_eval_time=22554.345523, accumulated_logging_time=1.349192, accumulated_submission_time=34469.733615, global_step=97702, preemption_count=0, score=34469.733615, test/accuracy=0.705851, test/bleu=30.394578, test/loss=1.508589, test/num_examples=3003, total_duration=57028.494278, train/accuracy=0.696926, train/bleu=35.152333, train/loss=1.580466, validation/accuracy=0.687642, validation/bleu=30.332052, validation/loss=1.610632, validation/num_examples=3000
I0208 00:38:25.766232 140529919506176 logging_writer.py:48] [97800] global_step=97800, grad_norm=0.33649662137031555, loss=3.6709988117218018
I0208 00:39:00.985302 140529927898880 logging_writer.py:48] [97900] global_step=97900, grad_norm=0.3194127380847931, loss=3.653078556060791
I0208 00:39:36.223870 140529919506176 logging_writer.py:48] [98000] global_step=98000, grad_norm=0.3340449333190918, loss=3.7278707027435303
I0208 00:40:11.451356 140529927898880 logging_writer.py:48] [98100] global_step=98100, grad_norm=0.3345312476158142, loss=3.7215099334716797
I0208 00:40:46.724639 140529919506176 logging_writer.py:48] [98200] global_step=98200, grad_norm=0.30557140707969666, loss=3.6517269611358643
I0208 00:41:22.001666 140529927898880 logging_writer.py:48] [98300] global_step=98300, grad_norm=0.33908313512802124, loss=3.740359306335449
I0208 00:41:57.276764 140529919506176 logging_writer.py:48] [98400] global_step=98400, grad_norm=0.3423878848552704, loss=3.741126537322998
I0208 00:42:32.532693 140529927898880 logging_writer.py:48] [98500] global_step=98500, grad_norm=0.3259284496307373, loss=3.6954851150512695
I0208 00:43:07.855094 140529919506176 logging_writer.py:48] [98600] global_step=98600, grad_norm=0.3357715308666229, loss=3.6278445720672607
I0208 00:43:43.169252 140529927898880 logging_writer.py:48] [98700] global_step=98700, grad_norm=0.33553892374038696, loss=3.6695923805236816
I0208 00:44:18.450762 140529919506176 logging_writer.py:48] [98800] global_step=98800, grad_norm=0.3356510102748871, loss=3.726287841796875
I0208 00:44:53.693051 140529927898880 logging_writer.py:48] [98900] global_step=98900, grad_norm=0.31989404559135437, loss=3.7168588638305664
I0208 00:45:28.992807 140529919506176 logging_writer.py:48] [99000] global_step=99000, grad_norm=0.32149553298950195, loss=3.6851541996002197
I0208 00:46:04.267959 140529927898880 logging_writer.py:48] [99100] global_step=99100, grad_norm=0.32256048917770386, loss=3.6874351501464844
I0208 00:46:39.548251 140529919506176 logging_writer.py:48] [99200] global_step=99200, grad_norm=0.31900763511657715, loss=3.6632325649261475
I0208 00:47:14.821631 140529927898880 logging_writer.py:48] [99300] global_step=99300, grad_norm=0.319969117641449, loss=3.6651952266693115
I0208 00:47:50.138830 140529919506176 logging_writer.py:48] [99400] global_step=99400, grad_norm=0.3196481764316559, loss=3.712293863296509
I0208 00:48:25.441081 140529927898880 logging_writer.py:48] [99500] global_step=99500, grad_norm=0.3256777822971344, loss=3.6803157329559326
I0208 00:49:00.845192 140529919506176 logging_writer.py:48] [99600] global_step=99600, grad_norm=0.3290179371833801, loss=3.678584098815918
I0208 00:49:36.198573 140529927898880 logging_writer.py:48] [99700] global_step=99700, grad_norm=0.3345421254634857, loss=3.702409505844116
I0208 00:50:11.553093 140529919506176 logging_writer.py:48] [99800] global_step=99800, grad_norm=0.33168914914131165, loss=3.6646640300750732
I0208 00:50:46.890049 140529927898880 logging_writer.py:48] [99900] global_step=99900, grad_norm=0.3475656807422638, loss=3.673590898513794
I0208 00:51:22.135489 140529919506176 logging_writer.py:48] [100000] global_step=100000, grad_norm=0.3266213834285736, loss=3.687026023864746
I0208 00:51:51.128879 140699726837568 spec.py:321] Evaluating on the training split.
I0208 00:51:54.168210 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 00:54:49.274399 140699726837568 spec.py:333] Evaluating on the validation split.
I0208 00:54:52.018506 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 00:57:36.038240 140699726837568 spec.py:349] Evaluating on the test split.
I0208 00:57:38.774196 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 01:00:03.973984 140699726837568 submission_runner.py:408] Time since start: 58361.57s, 	Step: 100084, 	{'train/accuracy': 0.7314824461936951, 'train/loss': 1.4231904745101929, 'train/bleu': 38.338865093238454, 'validation/accuracy': 0.6890305280685425, 'validation/loss': 1.6068017482757568, 'validation/bleu': 30.711764528611326, 'validation/num_examples': 3000, 'test/accuracy': 0.7073267102241516, 'test/loss': 1.5063436031341553, 'test/bleu': 30.535233603110655, 'test/num_examples': 3003, 'score': 35309.84344100952, 'total_duration': 58361.5737016201, 'accumulated_submission_time': 35309.84344100952, 'accumulated_eval_time': 23047.190549373627, 'accumulated_logging_time': 1.3953723907470703}
I0208 01:00:04.008882 140529927898880 logging_writer.py:48] [100084] accumulated_eval_time=23047.190549, accumulated_logging_time=1.395372, accumulated_submission_time=35309.843441, global_step=100084, preemption_count=0, score=35309.843441, test/accuracy=0.707327, test/bleu=30.535234, test/loss=1.506344, test/num_examples=3003, total_duration=58361.573702, train/accuracy=0.731482, train/bleu=38.338865, train/loss=1.423190, validation/accuracy=0.689031, validation/bleu=30.711765, validation/loss=1.606802, validation/num_examples=3000
I0208 01:00:10.019875 140529919506176 logging_writer.py:48] [100100] global_step=100100, grad_norm=0.33956602215766907, loss=3.7057323455810547
I0208 01:00:45.237831 140529927898880 logging_writer.py:48] [100200] global_step=100200, grad_norm=0.3271429240703583, loss=3.68103289604187
I0208 01:01:20.468398 140529919506176 logging_writer.py:48] [100300] global_step=100300, grad_norm=0.3262671232223511, loss=3.6271607875823975
I0208 01:01:55.709962 140529927898880 logging_writer.py:48] [100400] global_step=100400, grad_norm=0.3293426036834717, loss=3.7394626140594482
I0208 01:02:30.972206 140529919506176 logging_writer.py:48] [100500] global_step=100500, grad_norm=0.32637307047843933, loss=3.7015631198883057
I0208 01:03:06.250293 140529927898880 logging_writer.py:48] [100600] global_step=100600, grad_norm=0.3310527801513672, loss=3.6906938552856445
I0208 01:03:41.517182 140529919506176 logging_writer.py:48] [100700] global_step=100700, grad_norm=0.3312622606754303, loss=3.6417465209960938
I0208 01:04:16.829775 140529927898880 logging_writer.py:48] [100800] global_step=100800, grad_norm=0.3175649046897888, loss=3.6330068111419678
I0208 01:04:52.139442 140529919506176 logging_writer.py:48] [100900] global_step=100900, grad_norm=0.3336939513683319, loss=3.7481024265289307
I0208 01:05:27.426718 140529927898880 logging_writer.py:48] [101000] global_step=101000, grad_norm=0.3581497073173523, loss=3.6834981441497803
I0208 01:06:02.675296 140529919506176 logging_writer.py:48] [101100] global_step=101100, grad_norm=0.3236801326274872, loss=3.6539783477783203
I0208 01:06:37.982980 140529927898880 logging_writer.py:48] [101200] global_step=101200, grad_norm=0.336948961019516, loss=3.6842920780181885
I0208 01:07:13.278172 140529919506176 logging_writer.py:48] [101300] global_step=101300, grad_norm=0.3301291763782501, loss=3.663679838180542
I0208 01:07:48.587295 140529927898880 logging_writer.py:48] [101400] global_step=101400, grad_norm=0.34437453746795654, loss=3.6779305934906006
I0208 01:08:23.907799 140529919506176 logging_writer.py:48] [101500] global_step=101500, grad_norm=0.3483147919178009, loss=3.7176923751831055
I0208 01:08:59.272769 140529927898880 logging_writer.py:48] [101600] global_step=101600, grad_norm=0.3476514220237732, loss=3.7431910037994385
I0208 01:09:34.531125 140529919506176 logging_writer.py:48] [101700] global_step=101700, grad_norm=0.34440305829048157, loss=3.724280834197998
I0208 01:10:09.815207 140529927898880 logging_writer.py:48] [101800] global_step=101800, grad_norm=0.3488667607307434, loss=3.733612060546875
I0208 01:10:45.102250 140529919506176 logging_writer.py:48] [101900] global_step=101900, grad_norm=0.3461616039276123, loss=3.7301549911499023
I0208 01:11:20.405944 140529927898880 logging_writer.py:48] [102000] global_step=102000, grad_norm=0.3401790261268616, loss=3.705085515975952
I0208 01:11:55.703516 140529919506176 logging_writer.py:48] [102100] global_step=102100, grad_norm=0.33304545283317566, loss=3.6225407123565674
I0208 01:12:30.983432 140529927898880 logging_writer.py:48] [102200] global_step=102200, grad_norm=0.34637486934661865, loss=3.6970605850219727
I0208 01:13:06.263222 140529919506176 logging_writer.py:48] [102300] global_step=102300, grad_norm=0.32757845520973206, loss=3.681898355484009
I0208 01:13:41.548217 140529927898880 logging_writer.py:48] [102400] global_step=102400, grad_norm=0.32790815830230713, loss=3.6728131771087646
I0208 01:14:04.181668 140699726837568 spec.py:321] Evaluating on the training split.
I0208 01:14:07.225415 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 01:17:19.342769 140699726837568 spec.py:333] Evaluating on the validation split.
I0208 01:17:22.077761 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 01:20:00.129946 140699726837568 spec.py:349] Evaluating on the test split.
I0208 01:20:02.870798 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 01:22:34.159857 140699726837568 submission_runner.py:408] Time since start: 59711.76s, 	Step: 102466, 	{'train/accuracy': 0.7023757696151733, 'train/loss': 1.553908348083496, 'train/bleu': 36.134819561514, 'validation/accuracy': 0.6896008849143982, 'validation/loss': 1.6050572395324707, 'validation/bleu': 30.46585574558103, 'validation/num_examples': 3000, 'test/accuracy': 0.7066527605056763, 'test/loss': 1.5068280696868896, 'test/bleu': 30.829963453250123, 'test/num_examples': 3003, 'score': 36149.92675304413, 'total_duration': 59711.75960898399, 'accumulated_submission_time': 36149.92675304413, 'accumulated_eval_time': 23557.168686389923, 'accumulated_logging_time': 1.442002773284912}
I0208 01:22:34.189050 140529919506176 logging_writer.py:48] [102466] accumulated_eval_time=23557.168686, accumulated_logging_time=1.442003, accumulated_submission_time=36149.926753, global_step=102466, preemption_count=0, score=36149.926753, test/accuracy=0.706653, test/bleu=30.829963, test/loss=1.506828, test/num_examples=3003, total_duration=59711.759609, train/accuracy=0.702376, train/bleu=36.134820, train/loss=1.553908, validation/accuracy=0.689601, validation/bleu=30.465856, validation/loss=1.605057, validation/num_examples=3000
I0208 01:22:46.501168 140529927898880 logging_writer.py:48] [102500] global_step=102500, grad_norm=0.3315480351448059, loss=3.67681622505188
I0208 01:23:21.691429 140529919506176 logging_writer.py:48] [102600] global_step=102600, grad_norm=0.3657817244529724, loss=3.7027130126953125
I0208 01:23:56.876908 140529927898880 logging_writer.py:48] [102700] global_step=102700, grad_norm=0.3461342453956604, loss=3.7487058639526367
I0208 01:24:32.145076 140529919506176 logging_writer.py:48] [102800] global_step=102800, grad_norm=0.34537407755851746, loss=3.6568243503570557
I0208 01:25:07.395613 140529927898880 logging_writer.py:48] [102900] global_step=102900, grad_norm=0.3302510976791382, loss=3.6566176414489746
I0208 01:25:42.639471 140529919506176 logging_writer.py:48] [103000] global_step=103000, grad_norm=0.3455933928489685, loss=3.6616170406341553
I0208 01:26:17.896453 140529927898880 logging_writer.py:48] [103100] global_step=103100, grad_norm=0.3378903567790985, loss=3.679996967315674
I0208 01:26:53.141700 140529919506176 logging_writer.py:48] [103200] global_step=103200, grad_norm=0.35349327325820923, loss=3.6652956008911133
I0208 01:27:28.409911 140529927898880 logging_writer.py:48] [103300] global_step=103300, grad_norm=0.33505120873451233, loss=3.6792471408843994
I0208 01:28:03.710201 140529919506176 logging_writer.py:48] [103400] global_step=103400, grad_norm=0.3519573211669922, loss=3.6815805435180664
I0208 01:28:38.968412 140529927898880 logging_writer.py:48] [103500] global_step=103500, grad_norm=0.33955928683280945, loss=3.635995626449585
I0208 01:29:14.253174 140529919506176 logging_writer.py:48] [103600] global_step=103600, grad_norm=0.3553757071495056, loss=3.6389901638031006
I0208 01:29:49.493883 140529927898880 logging_writer.py:48] [103700] global_step=103700, grad_norm=0.3386627435684204, loss=3.6936757564544678
I0208 01:30:24.759172 140529919506176 logging_writer.py:48] [103800] global_step=103800, grad_norm=0.33498767018318176, loss=3.6683831214904785
I0208 01:31:00.027946 140529927898880 logging_writer.py:48] [103900] global_step=103900, grad_norm=0.34650859236717224, loss=3.643533706665039
I0208 01:31:35.321876 140529919506176 logging_writer.py:48] [104000] global_step=104000, grad_norm=0.33468514680862427, loss=3.6537625789642334
I0208 01:32:10.611349 140529927898880 logging_writer.py:48] [104100] global_step=104100, grad_norm=0.33842551708221436, loss=3.656693935394287
I0208 01:32:45.930118 140529919506176 logging_writer.py:48] [104200] global_step=104200, grad_norm=0.3549104928970337, loss=3.6615428924560547
I0208 01:33:21.238504 140529927898880 logging_writer.py:48] [104300] global_step=104300, grad_norm=0.3367454707622528, loss=3.6557607650756836
I0208 01:33:56.490992 140529919506176 logging_writer.py:48] [104400] global_step=104400, grad_norm=0.3477838933467865, loss=3.6796276569366455
I0208 01:34:31.755314 140529927898880 logging_writer.py:48] [104500] global_step=104500, grad_norm=0.36280307173728943, loss=3.6953818798065186
I0208 01:35:07.063954 140529919506176 logging_writer.py:48] [104600] global_step=104600, grad_norm=0.3378155529499054, loss=3.681041955947876
I0208 01:35:42.338090 140529927898880 logging_writer.py:48] [104700] global_step=104700, grad_norm=0.35308825969696045, loss=3.662482500076294
I0208 01:36:17.607494 140529919506176 logging_writer.py:48] [104800] global_step=104800, grad_norm=0.33683210611343384, loss=3.634031057357788
I0208 01:36:34.242617 140699726837568 spec.py:321] Evaluating on the training split.
I0208 01:36:37.279150 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 01:39:44.211266 140699726837568 spec.py:333] Evaluating on the validation split.
I0208 01:39:46.944087 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 01:42:20.299472 140699726837568 spec.py:349] Evaluating on the test split.
I0208 01:42:23.050959 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 01:44:40.040217 140699726837568 submission_runner.py:408] Time since start: 61037.64s, 	Step: 104849, 	{'train/accuracy': 0.6993989944458008, 'train/loss': 1.5596814155578613, 'train/bleu': 35.754191956860694, 'validation/accuracy': 0.6894644498825073, 'validation/loss': 1.6042819023132324, 'validation/bleu': 30.594905174890503, 'validation/num_examples': 3000, 'test/accuracy': 0.7081517577171326, 'test/loss': 1.4993467330932617, 'test/bleu': 30.92962167513607, 'test/num_examples': 3003, 'score': 36989.896939754486, 'total_duration': 61037.63996696472, 'accumulated_submission_time': 36989.896939754486, 'accumulated_eval_time': 24042.966230154037, 'accumulated_logging_time': 1.4812438488006592}
I0208 01:44:40.071483 140529927898880 logging_writer.py:48] [104849] accumulated_eval_time=24042.966230, accumulated_logging_time=1.481244, accumulated_submission_time=36989.896940, global_step=104849, preemption_count=0, score=36989.896940, test/accuracy=0.708152, test/bleu=30.929622, test/loss=1.499347, test/num_examples=3003, total_duration=61037.639967, train/accuracy=0.699399, train/bleu=35.754192, train/loss=1.559681, validation/accuracy=0.689464, validation/bleu=30.594905, validation/loss=1.604282, validation/num_examples=3000
I0208 01:44:58.363903 140529919506176 logging_writer.py:48] [104900] global_step=104900, grad_norm=0.35719963908195496, loss=3.6839468479156494
I0208 01:45:33.559083 140529927898880 logging_writer.py:48] [105000] global_step=105000, grad_norm=0.34107711911201477, loss=3.651854991912842
I0208 01:46:08.769640 140529919506176 logging_writer.py:48] [105100] global_step=105100, grad_norm=0.33796095848083496, loss=3.625462770462036
I0208 01:46:43.987056 140529927898880 logging_writer.py:48] [105200] global_step=105200, grad_norm=0.3438926339149475, loss=3.6980488300323486
I0208 01:47:19.247529 140529919506176 logging_writer.py:48] [105300] global_step=105300, grad_norm=0.3379976153373718, loss=3.6633424758911133
I0208 01:47:54.497721 140529927898880 logging_writer.py:48] [105400] global_step=105400, grad_norm=0.3450342118740082, loss=3.6633667945861816
I0208 01:48:29.790235 140529919506176 logging_writer.py:48] [105500] global_step=105500, grad_norm=0.34649473428726196, loss=3.629668712615967
I0208 01:49:05.023547 140529927898880 logging_writer.py:48] [105600] global_step=105600, grad_norm=0.38019421696662903, loss=3.645228147506714
I0208 01:49:40.263787 140529919506176 logging_writer.py:48] [105700] global_step=105700, grad_norm=0.3364301025867462, loss=3.6593823432922363
I0208 01:50:15.540559 140529927898880 logging_writer.py:48] [105800] global_step=105800, grad_norm=0.359761118888855, loss=3.699115753173828
I0208 01:50:50.816997 140529919506176 logging_writer.py:48] [105900] global_step=105900, grad_norm=0.3452564477920532, loss=3.7003469467163086
I0208 01:51:26.066820 140529927898880 logging_writer.py:48] [106000] global_step=106000, grad_norm=0.34543392062187195, loss=3.67266845703125
I0208 01:52:01.357241 140529919506176 logging_writer.py:48] [106100] global_step=106100, grad_norm=0.3879301846027374, loss=3.6702582836151123
I0208 01:52:36.632499 140529927898880 logging_writer.py:48] [106200] global_step=106200, grad_norm=0.3493010699748993, loss=3.5852713584899902
I0208 01:53:11.889348 140529919506176 logging_writer.py:48] [106300] global_step=106300, grad_norm=0.35591137409210205, loss=3.652981996536255
I0208 01:53:47.188038 140529927898880 logging_writer.py:48] [106400] global_step=106400, grad_norm=0.3567400574684143, loss=3.6323628425598145
I0208 01:54:22.445949 140529919506176 logging_writer.py:48] [106500] global_step=106500, grad_norm=0.36788228154182434, loss=3.684378147125244
I0208 01:54:57.704926 140529927898880 logging_writer.py:48] [106600] global_step=106600, grad_norm=0.35386016964912415, loss=3.7198128700256348
I0208 01:55:32.980949 140529919506176 logging_writer.py:48] [106700] global_step=106700, grad_norm=0.3722640573978424, loss=3.6275277137756348
I0208 01:56:08.299129 140529927898880 logging_writer.py:48] [106800] global_step=106800, grad_norm=0.35137608647346497, loss=3.667476177215576
I0208 01:56:43.561051 140529919506176 logging_writer.py:48] [106900] global_step=106900, grad_norm=0.3417789936065674, loss=3.621026039123535
I0208 01:57:18.825370 140529927898880 logging_writer.py:48] [107000] global_step=107000, grad_norm=0.3486602306365967, loss=3.67065691947937
I0208 01:57:54.106291 140529919506176 logging_writer.py:48] [107100] global_step=107100, grad_norm=0.35491839051246643, loss=3.6663873195648193
I0208 01:58:29.371877 140529927898880 logging_writer.py:48] [107200] global_step=107200, grad_norm=0.3618914783000946, loss=3.652170181274414
I0208 01:58:40.376283 140699726837568 spec.py:321] Evaluating on the training split.
I0208 01:58:43.408610 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 02:01:41.361610 140699726837568 spec.py:333] Evaluating on the validation split.
I0208 02:01:44.100052 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 02:04:22.644597 140699726837568 spec.py:349] Evaluating on the test split.
I0208 02:04:25.379532 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 02:06:45.260766 140699726837568 submission_runner.py:408] Time since start: 62362.86s, 	Step: 107233, 	{'train/accuracy': 0.7179781794548035, 'train/loss': 1.4720852375030518, 'train/bleu': 37.12911174176097, 'validation/accuracy': 0.6896132826805115, 'validation/loss': 1.6041953563690186, 'validation/bleu': 30.57522490142288, 'validation/num_examples': 3000, 'test/accuracy': 0.7072221636772156, 'test/loss': 1.5013386011123657, 'test/bleu': 30.69685450178556, 'test/num_examples': 3003, 'score': 37830.11774921417, 'total_duration': 62362.86051940918, 'accumulated_submission_time': 37830.11774921417, 'accumulated_eval_time': 24527.8506731987, 'accumulated_logging_time': 1.5237393379211426}
I0208 02:06:45.290809 140529919506176 logging_writer.py:48] [107233] accumulated_eval_time=24527.850673, accumulated_logging_time=1.523739, accumulated_submission_time=37830.117749, global_step=107233, preemption_count=0, score=37830.117749, test/accuracy=0.707222, test/bleu=30.696855, test/loss=1.501339, test/num_examples=3003, total_duration=62362.860519, train/accuracy=0.717978, train/bleu=37.129112, train/loss=1.472085, validation/accuracy=0.689613, validation/bleu=30.575225, validation/loss=1.604195, validation/num_examples=3000
I0208 02:07:09.205670 140529927898880 logging_writer.py:48] [107300] global_step=107300, grad_norm=0.3361426591873169, loss=3.6700022220611572
I0208 02:07:44.386050 140529919506176 logging_writer.py:48] [107400] global_step=107400, grad_norm=0.35707488656044006, loss=3.7163217067718506
I0208 02:08:19.625418 140529927898880 logging_writer.py:48] [107500] global_step=107500, grad_norm=0.35634326934814453, loss=3.6324639320373535
I0208 02:08:54.899255 140529919506176 logging_writer.py:48] [107600] global_step=107600, grad_norm=0.35999932885169983, loss=3.604402780532837
I0208 02:09:30.158365 140529927898880 logging_writer.py:48] [107700] global_step=107700, grad_norm=0.35560348629951477, loss=3.670332670211792
I0208 02:10:05.383695 140529919506176 logging_writer.py:48] [107800] global_step=107800, grad_norm=0.37808021903038025, loss=3.7157609462738037
I0208 02:10:40.675521 140529927898880 logging_writer.py:48] [107900] global_step=107900, grad_norm=0.3587265908718109, loss=3.6581077575683594
I0208 02:11:15.980059 140529919506176 logging_writer.py:48] [108000] global_step=108000, grad_norm=0.36932748556137085, loss=3.6647307872772217
I0208 02:11:51.251361 140529927898880 logging_writer.py:48] [108100] global_step=108100, grad_norm=0.3466218411922455, loss=3.641798257827759
I0208 02:12:26.501319 140529919506176 logging_writer.py:48] [108200] global_step=108200, grad_norm=0.3641802668571472, loss=3.6717002391815186
I0208 02:13:01.777604 140529927898880 logging_writer.py:48] [108300] global_step=108300, grad_norm=0.3517773747444153, loss=3.6932766437530518
I0208 02:13:37.053784 140529919506176 logging_writer.py:48] [108400] global_step=108400, grad_norm=0.34754443168640137, loss=3.6824235916137695
I0208 02:14:12.368684 140529927898880 logging_writer.py:48] [108500] global_step=108500, grad_norm=0.3567149341106415, loss=3.6552438735961914
I0208 02:14:47.614039 140529919506176 logging_writer.py:48] [108600] global_step=108600, grad_norm=0.3619692921638489, loss=3.693587064743042
I0208 02:15:22.882046 140529927898880 logging_writer.py:48] [108700] global_step=108700, grad_norm=0.3622841536998749, loss=3.642678737640381
I0208 02:15:58.197930 140529919506176 logging_writer.py:48] [108800] global_step=108800, grad_norm=0.36206474900245667, loss=3.663381814956665
I0208 02:16:33.554498 140529927898880 logging_writer.py:48] [108900] global_step=108900, grad_norm=0.34483590722084045, loss=3.615217447280884
I0208 02:17:08.867803 140529919506176 logging_writer.py:48] [109000] global_step=109000, grad_norm=0.3617236316204071, loss=3.6466422080993652
I0208 02:17:44.140527 140529927898880 logging_writer.py:48] [109100] global_step=109100, grad_norm=0.33627164363861084, loss=3.655937910079956
I0208 02:18:19.433499 140529919506176 logging_writer.py:48] [109200] global_step=109200, grad_norm=0.37242698669433594, loss=3.6392834186553955
I0208 02:18:54.686314 140529927898880 logging_writer.py:48] [109300] global_step=109300, grad_norm=0.3665589690208435, loss=3.661738634109497
I0208 02:19:29.955141 140529919506176 logging_writer.py:48] [109400] global_step=109400, grad_norm=0.35841965675354004, loss=3.693955183029175
I0208 02:20:05.251682 140529927898880 logging_writer.py:48] [109500] global_step=109500, grad_norm=0.36430492997169495, loss=3.636439323425293
I0208 02:20:40.685024 140529919506176 logging_writer.py:48] [109600] global_step=109600, grad_norm=0.3506239056587219, loss=3.640911102294922
I0208 02:20:45.356735 140699726837568 spec.py:321] Evaluating on the training split.
I0208 02:20:48.413284 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 02:23:53.173748 140699726837568 spec.py:333] Evaluating on the validation split.
I0208 02:23:55.916083 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 02:26:48.978127 140699726837568 spec.py:349] Evaluating on the test split.
I0208 02:26:51.720503 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 02:29:15.421510 140699726837568 submission_runner.py:408] Time since start: 63713.02s, 	Step: 109615, 	{'train/accuracy': 0.710374653339386, 'train/loss': 1.5148351192474365, 'train/bleu': 36.08658871891254, 'validation/accuracy': 0.690220832824707, 'validation/loss': 1.601181983947754, 'validation/bleu': 30.252796426395733, 'validation/num_examples': 3000, 'test/accuracy': 0.7097089290618896, 'test/loss': 1.493978500366211, 'test/bleu': 30.87070303183882, 'test/num_examples': 3003, 'score': 38670.0972571373, 'total_duration': 63713.02125096321, 'accumulated_submission_time': 38670.0972571373, 'accumulated_eval_time': 25037.91539287567, 'accumulated_logging_time': 1.563863754272461}
I0208 02:29:15.452431 140529927898880 logging_writer.py:48] [109615] accumulated_eval_time=25037.915393, accumulated_logging_time=1.563864, accumulated_submission_time=38670.097257, global_step=109615, preemption_count=0, score=38670.097257, test/accuracy=0.709709, test/bleu=30.870703, test/loss=1.493979, test/num_examples=3003, total_duration=63713.021251, train/accuracy=0.710375, train/bleu=36.086589, train/loss=1.514835, validation/accuracy=0.690221, validation/bleu=30.252796, validation/loss=1.601182, validation/num_examples=3000
I0208 02:29:45.728207 140529919506176 logging_writer.py:48] [109700] global_step=109700, grad_norm=0.3664356768131256, loss=3.680515766143799
I0208 02:30:20.953883 140529927898880 logging_writer.py:48] [109800] global_step=109800, grad_norm=0.35629719495773315, loss=3.6480767726898193
I0208 02:30:56.215643 140529919506176 logging_writer.py:48] [109900] global_step=109900, grad_norm=0.3532257378101349, loss=3.6778366565704346
I0208 02:31:31.456971 140529927898880 logging_writer.py:48] [110000] global_step=110000, grad_norm=0.36016103625297546, loss=3.6556413173675537
I0208 02:32:06.735305 140529919506176 logging_writer.py:48] [110100] global_step=110100, grad_norm=0.35301917791366577, loss=3.6540093421936035
I0208 02:32:42.000850 140529927898880 logging_writer.py:48] [110200] global_step=110200, grad_norm=0.3735330402851105, loss=3.6757118701934814
I0208 02:33:17.316321 140529919506176 logging_writer.py:48] [110300] global_step=110300, grad_norm=0.36065709590911865, loss=3.6855344772338867
I0208 02:33:52.624819 140529927898880 logging_writer.py:48] [110400] global_step=110400, grad_norm=0.3685629069805145, loss=3.6259846687316895
I0208 02:34:27.910461 140529919506176 logging_writer.py:48] [110500] global_step=110500, grad_norm=0.36356690526008606, loss=3.647895336151123
I0208 02:35:03.197727 140529927898880 logging_writer.py:48] [110600] global_step=110600, grad_norm=0.350976824760437, loss=3.6489977836608887
I0208 02:35:38.461794 140529919506176 logging_writer.py:48] [110700] global_step=110700, grad_norm=0.35156479477882385, loss=3.6410000324249268
I0208 02:36:13.760993 140529927898880 logging_writer.py:48] [110800] global_step=110800, grad_norm=0.3700874149799347, loss=3.6439669132232666
I0208 02:36:49.086769 140529919506176 logging_writer.py:48] [110900] global_step=110900, grad_norm=0.36141565442085266, loss=3.7067203521728516
I0208 02:37:24.392564 140529927898880 logging_writer.py:48] [111000] global_step=111000, grad_norm=0.34366175532341003, loss=3.6481332778930664
I0208 02:37:59.675228 140529919506176 logging_writer.py:48] [111100] global_step=111100, grad_norm=0.3581412136554718, loss=3.6406123638153076
I0208 02:38:34.998733 140529927898880 logging_writer.py:48] [111200] global_step=111200, grad_norm=0.3711833357810974, loss=3.6117658615112305
I0208 02:39:10.277966 140529919506176 logging_writer.py:48] [111300] global_step=111300, grad_norm=0.381313294172287, loss=3.6397881507873535
I0208 02:39:45.622462 140529927898880 logging_writer.py:48] [111400] global_step=111400, grad_norm=0.37010815739631653, loss=3.640852212905884
I0208 02:40:20.933249 140529919506176 logging_writer.py:48] [111500] global_step=111500, grad_norm=0.39018529653549194, loss=3.6556859016418457
I0208 02:40:56.227908 140529927898880 logging_writer.py:48] [111600] global_step=111600, grad_norm=0.3616301715373993, loss=3.653059482574463
I0208 02:41:31.515757 140529919506176 logging_writer.py:48] [111700] global_step=111700, grad_norm=0.3726026117801666, loss=3.625575542449951
I0208 02:42:06.844752 140529927898880 logging_writer.py:48] [111800] global_step=111800, grad_norm=0.3802357017993927, loss=3.5911126136779785
I0208 02:42:42.117890 140529919506176 logging_writer.py:48] [111900] global_step=111900, grad_norm=0.37815994024276733, loss=3.6402578353881836
I0208 02:43:15.679695 140699726837568 spec.py:321] Evaluating on the training split.
I0208 02:43:18.722985 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 02:46:15.727515 140699726837568 spec.py:333] Evaluating on the validation split.
I0208 02:46:18.460391 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 02:48:54.660255 140699726837568 spec.py:349] Evaluating on the test split.
I0208 02:48:57.392667 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 02:51:19.537481 140699726837568 submission_runner.py:408] Time since start: 65037.14s, 	Step: 111997, 	{'train/accuracy': 0.7104139924049377, 'train/loss': 1.5093110799789429, 'train/bleu': 36.71417614009322, 'validation/accuracy': 0.6904935836791992, 'validation/loss': 1.6021604537963867, 'validation/bleu': 30.746368302220958, 'validation/num_examples': 3000, 'test/accuracy': 0.7084655165672302, 'test/loss': 1.4979416131973267, 'test/bleu': 30.64387931009616, 'test/num_examples': 3003, 'score': 39510.233573913574, 'total_duration': 65037.137236356735, 'accumulated_submission_time': 39510.233573913574, 'accumulated_eval_time': 25521.773129224777, 'accumulated_logging_time': 1.6069166660308838}
I0208 02:51:19.568594 140529927898880 logging_writer.py:48] [111997] accumulated_eval_time=25521.773129, accumulated_logging_time=1.606917, accumulated_submission_time=39510.233574, global_step=111997, preemption_count=0, score=39510.233574, test/accuracy=0.708466, test/bleu=30.643879, test/loss=1.497942, test/num_examples=3003, total_duration=65037.137236, train/accuracy=0.710414, train/bleu=36.714176, train/loss=1.509311, validation/accuracy=0.690494, validation/bleu=30.746368, validation/loss=1.602160, validation/num_examples=3000
I0208 02:51:20.994234 140529919506176 logging_writer.py:48] [112000] global_step=112000, grad_norm=0.3671090602874756, loss=3.625897169113159
I0208 02:51:56.198456 140529927898880 logging_writer.py:48] [112100] global_step=112100, grad_norm=0.35847893357276917, loss=3.6519150733947754
I0208 02:52:31.445647 140529919506176 logging_writer.py:48] [112200] global_step=112200, grad_norm=0.3586921989917755, loss=3.605431079864502
I0208 02:53:06.718163 140529927898880 logging_writer.py:48] [112300] global_step=112300, grad_norm=0.36144429445266724, loss=3.650266647338867
I0208 02:53:41.978209 140529919506176 logging_writer.py:48] [112400] global_step=112400, grad_norm=0.39072391390800476, loss=3.6411337852478027
I0208 02:54:17.231252 140529927898880 logging_writer.py:48] [112500] global_step=112500, grad_norm=0.37546980381011963, loss=3.628610849380493
I0208 02:54:52.485656 140529919506176 logging_writer.py:48] [112600] global_step=112600, grad_norm=0.38260936737060547, loss=3.6363601684570312
I0208 02:55:27.767342 140529927898880 logging_writer.py:48] [112700] global_step=112700, grad_norm=0.36188948154449463, loss=3.6759207248687744
I0208 02:56:03.041650 140529919506176 logging_writer.py:48] [112800] global_step=112800, grad_norm=0.3702814280986786, loss=3.674142837524414
I0208 02:56:38.330645 140529927898880 logging_writer.py:48] [112900] global_step=112900, grad_norm=0.3556046783924103, loss=3.5969338417053223
I0208 02:57:13.596787 140529919506176 logging_writer.py:48] [113000] global_step=113000, grad_norm=0.3735075294971466, loss=3.6624083518981934
I0208 02:57:48.893321 140529927898880 logging_writer.py:48] [113100] global_step=113100, grad_norm=0.38283583521842957, loss=3.66780686378479
I0208 02:58:24.131686 140529919506176 logging_writer.py:48] [113200] global_step=113200, grad_norm=0.384219229221344, loss=3.6753132343292236
I0208 02:58:59.382658 140529927898880 logging_writer.py:48] [113300] global_step=113300, grad_norm=0.37111955881118774, loss=3.6777849197387695
I0208 02:59:34.631439 140529919506176 logging_writer.py:48] [113400] global_step=113400, grad_norm=0.3716023564338684, loss=3.647047758102417
I0208 03:00:09.992929 140529927898880 logging_writer.py:48] [113500] global_step=113500, grad_norm=0.3763209581375122, loss=3.5840582847595215
I0208 03:00:45.270606 140529919506176 logging_writer.py:48] [113600] global_step=113600, grad_norm=0.372726708650589, loss=3.625413417816162
I0208 03:01:20.580622 140529927898880 logging_writer.py:48] [113700] global_step=113700, grad_norm=0.36658334732055664, loss=3.6071417331695557
I0208 03:01:55.842724 140529919506176 logging_writer.py:48] [113800] global_step=113800, grad_norm=0.3688047230243683, loss=3.612088441848755
I0208 03:02:31.115230 140529927898880 logging_writer.py:48] [113900] global_step=113900, grad_norm=0.37878715991973877, loss=3.6454293727874756
I0208 03:03:06.376743 140529919506176 logging_writer.py:48] [114000] global_step=114000, grad_norm=0.3830986022949219, loss=3.5860517024993896
I0208 03:03:41.646140 140529927898880 logging_writer.py:48] [114100] global_step=114100, grad_norm=0.3625865876674652, loss=3.6298410892486572
I0208 03:04:16.932209 140529919506176 logging_writer.py:48] [114200] global_step=114200, grad_norm=0.3572918176651001, loss=3.58805775642395
I0208 03:04:52.198335 140529927898880 logging_writer.py:48] [114300] global_step=114300, grad_norm=0.3846454620361328, loss=3.5675220489501953
I0208 03:05:19.801059 140699726837568 spec.py:321] Evaluating on the training split.
I0208 03:05:22.870756 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 03:08:32.047967 140699726837568 spec.py:333] Evaluating on the validation split.
I0208 03:08:34.796788 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 03:11:05.584425 140699726837568 spec.py:349] Evaluating on the test split.
I0208 03:11:08.334045 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 03:13:25.365956 140699726837568 submission_runner.py:408] Time since start: 66362.97s, 	Step: 114380, 	{'train/accuracy': 0.7173652052879333, 'train/loss': 1.4794282913208008, 'train/bleu': 37.131145115956905, 'validation/accuracy': 0.6896628737449646, 'validation/loss': 1.6034998893737793, 'validation/bleu': 30.397301082654415, 'validation/num_examples': 3000, 'test/accuracy': 0.7098019123077393, 'test/loss': 1.4975818395614624, 'test/bleu': 30.778936015094025, 'test/num_examples': 3003, 'score': 40350.37732386589, 'total_duration': 66362.9657073021, 'accumulated_submission_time': 40350.37732386589, 'accumulated_eval_time': 26007.337995052338, 'accumulated_logging_time': 1.6503000259399414}
I0208 03:13:25.396825 140529919506176 logging_writer.py:48] [114380] accumulated_eval_time=26007.337995, accumulated_logging_time=1.650300, accumulated_submission_time=40350.377324, global_step=114380, preemption_count=0, score=40350.377324, test/accuracy=0.709802, test/bleu=30.778936, test/loss=1.497582, test/num_examples=3003, total_duration=66362.965707, train/accuracy=0.717365, train/bleu=37.131145, train/loss=1.479428, validation/accuracy=0.689663, validation/bleu=30.397301, validation/loss=1.603500, validation/num_examples=3000
I0208 03:13:32.803117 140529927898880 logging_writer.py:48] [114400] global_step=114400, grad_norm=0.36716923117637634, loss=3.618206262588501
I0208 03:14:07.966583 140529919506176 logging_writer.py:48] [114500] global_step=114500, grad_norm=0.37081193923950195, loss=3.633431911468506
I0208 03:14:43.169430 140529927898880 logging_writer.py:48] [114600] global_step=114600, grad_norm=0.38909515738487244, loss=3.676659107208252
I0208 03:15:18.407427 140529919506176 logging_writer.py:48] [114700] global_step=114700, grad_norm=0.3693511486053467, loss=3.6313812732696533
I0208 03:15:53.647176 140529927898880 logging_writer.py:48] [114800] global_step=114800, grad_norm=0.380124032497406, loss=3.612506628036499
I0208 03:16:28.892066 140529919506176 logging_writer.py:48] [114900] global_step=114900, grad_norm=0.3723270297050476, loss=3.6841225624084473
I0208 03:17:04.138298 140529927898880 logging_writer.py:48] [115000] global_step=115000, grad_norm=0.3688533902168274, loss=3.6195108890533447
I0208 03:17:39.416630 140529919506176 logging_writer.py:48] [115100] global_step=115100, grad_norm=0.37043073773384094, loss=3.6111433506011963
I0208 03:18:14.718581 140529927898880 logging_writer.py:48] [115200] global_step=115200, grad_norm=0.3855328857898712, loss=3.6634321212768555
I0208 03:18:49.992525 140529919506176 logging_writer.py:48] [115300] global_step=115300, grad_norm=0.37375468015670776, loss=3.625574827194214
I0208 03:19:25.242660 140529927898880 logging_writer.py:48] [115400] global_step=115400, grad_norm=0.3723489046096802, loss=3.6397829055786133
I0208 03:20:00.498424 140529919506176 logging_writer.py:48] [115500] global_step=115500, grad_norm=0.35952243208885193, loss=3.6100857257843018
I0208 03:20:35.732041 140529927898880 logging_writer.py:48] [115600] global_step=115600, grad_norm=0.3693743646144867, loss=3.6176395416259766
I0208 03:21:10.980610 140529919506176 logging_writer.py:48] [115700] global_step=115700, grad_norm=0.3503178060054779, loss=3.5789055824279785
I0208 03:21:46.239696 140529927898880 logging_writer.py:48] [115800] global_step=115800, grad_norm=0.37728187441825867, loss=3.6519782543182373
I0208 03:22:21.502235 140529919506176 logging_writer.py:48] [115900] global_step=115900, grad_norm=0.37143537402153015, loss=3.6172428131103516
I0208 03:22:56.797122 140529927898880 logging_writer.py:48] [116000] global_step=116000, grad_norm=0.38527345657348633, loss=3.6171834468841553
I0208 03:23:32.073077 140529919506176 logging_writer.py:48] [116100] global_step=116100, grad_norm=0.37285104393959045, loss=3.6232264041900635
I0208 03:24:07.322424 140529927898880 logging_writer.py:48] [116200] global_step=116200, grad_norm=0.3792738616466522, loss=3.6761722564697266
I0208 03:24:42.586138 140529919506176 logging_writer.py:48] [116300] global_step=116300, grad_norm=0.3718545138835907, loss=3.545816659927368
I0208 03:25:17.853585 140529927898880 logging_writer.py:48] [116400] global_step=116400, grad_norm=0.37194982171058655, loss=3.6259381771087646
I0208 03:25:53.106783 140529919506176 logging_writer.py:48] [116500] global_step=116500, grad_norm=0.35954609513282776, loss=3.5921554565429688
I0208 03:26:28.358867 140529927898880 logging_writer.py:48] [116600] global_step=116600, grad_norm=0.3770078122615814, loss=3.580674886703491
I0208 03:27:03.631459 140529919506176 logging_writer.py:48] [116700] global_step=116700, grad_norm=0.38287752866744995, loss=3.6518428325653076
I0208 03:27:25.596145 140699726837568 spec.py:321] Evaluating on the training split.
I0208 03:27:28.645004 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 03:30:24.182664 140699726837568 spec.py:333] Evaluating on the validation split.
I0208 03:30:26.920482 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 03:33:02.765592 140699726837568 spec.py:349] Evaluating on the test split.
I0208 03:33:05.505542 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 03:35:32.375603 140699726837568 submission_runner.py:408] Time since start: 67689.98s, 	Step: 116764, 	{'train/accuracy': 0.7147499918937683, 'train/loss': 1.4900606870651245, 'train/bleu': 36.79285558146597, 'validation/accuracy': 0.6898736357688904, 'validation/loss': 1.601643681526184, 'validation/bleu': 30.654046685474967, 'validation/num_examples': 3000, 'test/accuracy': 0.7097205519676208, 'test/loss': 1.494398832321167, 'test/bleu': 30.84845509744524, 'test/num_examples': 3003, 'score': 41190.49334859848, 'total_duration': 67689.97535419464, 'accumulated_submission_time': 41190.49334859848, 'accumulated_eval_time': 26494.117411851883, 'accumulated_logging_time': 1.6911771297454834}
I0208 03:35:32.406805 140529927898880 logging_writer.py:48] [116764] accumulated_eval_time=26494.117412, accumulated_logging_time=1.691177, accumulated_submission_time=41190.493349, global_step=116764, preemption_count=0, score=41190.493349, test/accuracy=0.709721, test/bleu=30.848455, test/loss=1.494399, test/num_examples=3003, total_duration=67689.975354, train/accuracy=0.714750, train/bleu=36.792856, train/loss=1.490061, validation/accuracy=0.689874, validation/bleu=30.654047, validation/loss=1.601644, validation/num_examples=3000
I0208 03:35:45.418363 140529919506176 logging_writer.py:48] [116800] global_step=116800, grad_norm=0.3875492811203003, loss=3.6471850872039795
I0208 03:36:20.613370 140529927898880 logging_writer.py:48] [116900] global_step=116900, grad_norm=0.37206345796585083, loss=3.6441240310668945
I0208 03:36:55.813916 140529919506176 logging_writer.py:48] [117000] global_step=117000, grad_norm=0.3579331934452057, loss=3.574476718902588
I0208 03:37:31.039006 140529927898880 logging_writer.py:48] [117100] global_step=117100, grad_norm=0.3762214779853821, loss=3.6295342445373535
I0208 03:38:06.263036 140529919506176 logging_writer.py:48] [117200] global_step=117200, grad_norm=0.3891276717185974, loss=3.629462480545044
I0208 03:38:41.554877 140529927898880 logging_writer.py:48] [117300] global_step=117300, grad_norm=0.4137292802333832, loss=3.671488046646118
I0208 03:39:16.866344 140529919506176 logging_writer.py:48] [117400] global_step=117400, grad_norm=0.3850395381450653, loss=3.628117322921753
I0208 03:39:52.121620 140529927898880 logging_writer.py:48] [117500] global_step=117500, grad_norm=0.37167781591415405, loss=3.606276512145996
I0208 03:40:27.423185 140529919506176 logging_writer.py:48] [117600] global_step=117600, grad_norm=0.3873077630996704, loss=3.609243392944336
I0208 03:41:02.786036 140529927898880 logging_writer.py:48] [117700] global_step=117700, grad_norm=0.3706798851490021, loss=3.598419189453125
I0208 03:41:38.101116 140529919506176 logging_writer.py:48] [117800] global_step=117800, grad_norm=0.380543977022171, loss=3.600994825363159
I0208 03:42:13.400188 140529927898880 logging_writer.py:48] [117900] global_step=117900, grad_norm=0.36906495690345764, loss=3.592780590057373
I0208 03:42:48.670542 140529919506176 logging_writer.py:48] [118000] global_step=118000, grad_norm=0.38780468702316284, loss=3.5472214221954346
I0208 03:43:23.939733 140529927898880 logging_writer.py:48] [118100] global_step=118100, grad_norm=0.3671293258666992, loss=3.573345899581909
I0208 03:43:59.217422 140529919506176 logging_writer.py:48] [118200] global_step=118200, grad_norm=0.38974201679229736, loss=3.6453466415405273
I0208 03:44:34.498681 140529927898880 logging_writer.py:48] [118300] global_step=118300, grad_norm=0.3750450015068054, loss=3.623997211456299
I0208 03:45:09.765857 140529919506176 logging_writer.py:48] [118400] global_step=118400, grad_norm=0.3736916482448578, loss=3.6102404594421387
I0208 03:45:45.037271 140529927898880 logging_writer.py:48] [118500] global_step=118500, grad_norm=0.4070640504360199, loss=3.6951375007629395
I0208 03:46:20.313828 140529919506176 logging_writer.py:48] [118600] global_step=118600, grad_norm=0.4057721495628357, loss=3.7260210514068604
I0208 03:46:55.634145 140529927898880 logging_writer.py:48] [118700] global_step=118700, grad_norm=0.3947755694389343, loss=3.6626551151275635
I0208 03:47:30.902695 140529919506176 logging_writer.py:48] [118800] global_step=118800, grad_norm=0.36346516013145447, loss=3.5614752769470215
I0208 03:48:06.180789 140529927898880 logging_writer.py:48] [118900] global_step=118900, grad_norm=0.3748288154602051, loss=3.6341183185577393
I0208 03:48:41.451868 140529919506176 logging_writer.py:48] [119000] global_step=119000, grad_norm=0.3767741620540619, loss=3.6044416427612305
I0208 03:49:16.730783 140529927898880 logging_writer.py:48] [119100] global_step=119100, grad_norm=0.38760852813720703, loss=3.578258752822876
I0208 03:49:32.679024 140699726837568 spec.py:321] Evaluating on the training split.
I0208 03:49:35.721376 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 03:52:24.386896 140699726837568 spec.py:333] Evaluating on the validation split.
I0208 03:52:27.136862 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 03:54:58.857119 140699726837568 spec.py:349] Evaluating on the test split.
I0208 03:55:01.612379 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 03:57:24.068121 140699726837568 submission_runner.py:408] Time since start: 69001.67s, 	Step: 119147, 	{'train/accuracy': 0.7253099083900452, 'train/loss': 1.4400979280471802, 'train/bleu': 38.21547448718535, 'validation/accuracy': 0.6901092529296875, 'validation/loss': 1.599493384361267, 'validation/bleu': 30.736349330402373, 'validation/num_examples': 3000, 'test/accuracy': 0.709674060344696, 'test/loss': 1.493552803993225, 'test/bleu': 30.740337151689708, 'test/num_examples': 3003, 'score': 42030.68079471588, 'total_duration': 69001.66783833504, 'accumulated_submission_time': 42030.68079471588, 'accumulated_eval_time': 26965.506422758102, 'accumulated_logging_time': 1.7330989837646484}
I0208 03:57:24.105914 140529919506176 logging_writer.py:48] [119147] accumulated_eval_time=26965.506423, accumulated_logging_time=1.733099, accumulated_submission_time=42030.680795, global_step=119147, preemption_count=0, score=42030.680795, test/accuracy=0.709674, test/bleu=30.740337, test/loss=1.493553, test/num_examples=3003, total_duration=69001.667838, train/accuracy=0.725310, train/bleu=38.215474, train/loss=1.440098, validation/accuracy=0.690109, validation/bleu=30.736349, validation/loss=1.599493, validation/num_examples=3000
I0208 03:57:43.099149 140529927898880 logging_writer.py:48] [119200] global_step=119200, grad_norm=0.3859889507293701, loss=3.6366872787475586
I0208 03:58:18.280310 140529919506176 logging_writer.py:48] [119300] global_step=119300, grad_norm=0.38172098994255066, loss=3.6018259525299072
I0208 03:58:53.521507 140529927898880 logging_writer.py:48] [119400] global_step=119400, grad_norm=0.38037651777267456, loss=3.6229817867279053
I0208 03:59:28.757122 140529919506176 logging_writer.py:48] [119500] global_step=119500, grad_norm=0.40040817856788635, loss=3.605619192123413
I0208 04:00:04.086844 140529927898880 logging_writer.py:48] [119600] global_step=119600, grad_norm=0.3869488835334778, loss=3.6163787841796875
I0208 04:00:39.370770 140529919506176 logging_writer.py:48] [119700] global_step=119700, grad_norm=0.3663588762283325, loss=3.5531179904937744
I0208 04:01:14.628210 140529927898880 logging_writer.py:48] [119800] global_step=119800, grad_norm=0.37411415576934814, loss=3.6233866214752197
I0208 04:01:49.924514 140529919506176 logging_writer.py:48] [119900] global_step=119900, grad_norm=0.386699914932251, loss=3.6133148670196533
I0208 04:02:25.196854 140529927898880 logging_writer.py:48] [120000] global_step=120000, grad_norm=0.38642361760139465, loss=3.6066887378692627
I0208 04:03:00.508854 140529919506176 logging_writer.py:48] [120100] global_step=120100, grad_norm=0.3698575794696808, loss=3.590679407119751
I0208 04:03:35.784021 140529927898880 logging_writer.py:48] [120200] global_step=120200, grad_norm=0.3754178285598755, loss=3.62082839012146
I0208 04:04:11.059805 140529919506176 logging_writer.py:48] [120300] global_step=120300, grad_norm=0.399373859167099, loss=3.6210896968841553
I0208 04:04:46.317883 140529927898880 logging_writer.py:48] [120400] global_step=120400, grad_norm=0.3869195580482483, loss=3.647378921508789
I0208 04:05:21.605945 140529919506176 logging_writer.py:48] [120500] global_step=120500, grad_norm=0.3745303750038147, loss=3.560619831085205
I0208 04:05:56.889095 140529927898880 logging_writer.py:48] [120600] global_step=120600, grad_norm=0.3790711760520935, loss=3.6339986324310303
I0208 04:06:32.277021 140529919506176 logging_writer.py:48] [120700] global_step=120700, grad_norm=0.4139479696750641, loss=3.6017959117889404
I0208 04:07:07.573642 140529927898880 logging_writer.py:48] [120800] global_step=120800, grad_norm=0.37573060393333435, loss=3.635500192642212
I0208 04:07:42.845525 140529919506176 logging_writer.py:48] [120900] global_step=120900, grad_norm=0.3715642988681793, loss=3.5728607177734375
I0208 04:08:18.146971 140529927898880 logging_writer.py:48] [121000] global_step=121000, grad_norm=0.37264198064804077, loss=3.58476185798645
I0208 04:08:53.440041 140529919506176 logging_writer.py:48] [121100] global_step=121100, grad_norm=0.37417465448379517, loss=3.6361396312713623
I0208 04:09:28.725148 140529927898880 logging_writer.py:48] [121200] global_step=121200, grad_norm=0.3720298111438751, loss=3.5690531730651855
I0208 04:10:03.986853 140529919506176 logging_writer.py:48] [121300] global_step=121300, grad_norm=0.37241798639297485, loss=3.561767101287842
I0208 04:10:39.280990 140529927898880 logging_writer.py:48] [121400] global_step=121400, grad_norm=0.39049068093299866, loss=3.600969076156616
I0208 04:11:14.527475 140529919506176 logging_writer.py:48] [121500] global_step=121500, grad_norm=0.3869892656803131, loss=3.6826632022857666
I0208 04:11:24.105031 140699726837568 spec.py:321] Evaluating on the training split.
I0208 04:11:27.143327 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 04:14:48.266544 140699726837568 spec.py:333] Evaluating on the validation split.
I0208 04:14:51.006281 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 04:17:23.942686 140699726837568 spec.py:349] Evaluating on the test split.
I0208 04:17:26.680050 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 04:20:06.299572 140699726837568 submission_runner.py:408] Time since start: 70363.90s, 	Step: 121529, 	{'train/accuracy': 0.7211779356002808, 'train/loss': 1.4619131088256836, 'train/bleu': 37.837886173614265, 'validation/accuracy': 0.6910391449928284, 'validation/loss': 1.5997225046157837, 'validation/bleu': 30.736317737646832, 'validation/num_examples': 3000, 'test/accuracy': 0.7105339765548706, 'test/loss': 1.4927688837051392, 'test/bleu': 30.733517509111195, 'test/num_examples': 3003, 'score': 42870.594207286835, 'total_duration': 70363.89932894707, 'accumulated_submission_time': 42870.594207286835, 'accumulated_eval_time': 27487.700913906097, 'accumulated_logging_time': 1.7821390628814697}
I0208 04:20:06.332686 140529927898880 logging_writer.py:48] [121529] accumulated_eval_time=27487.700914, accumulated_logging_time=1.782139, accumulated_submission_time=42870.594207, global_step=121529, preemption_count=0, score=42870.594207, test/accuracy=0.710534, test/bleu=30.733518, test/loss=1.492769, test/num_examples=3003, total_duration=70363.899329, train/accuracy=0.721178, train/bleu=37.837886, train/loss=1.461913, validation/accuracy=0.691039, validation/bleu=30.736318, validation/loss=1.599723, validation/num_examples=3000
I0208 04:20:31.620476 140529919506176 logging_writer.py:48] [121600] global_step=121600, grad_norm=0.3663479685783386, loss=3.568187713623047
I0208 04:21:06.812684 140529927898880 logging_writer.py:48] [121700] global_step=121700, grad_norm=0.3864704370498657, loss=3.5807037353515625
I0208 04:21:42.036006 140529919506176 logging_writer.py:48] [121800] global_step=121800, grad_norm=0.38190746307373047, loss=3.6401054859161377
I0208 04:22:17.304116 140529927898880 logging_writer.py:48] [121900] global_step=121900, grad_norm=0.37758055329322815, loss=3.5997016429901123
I0208 04:22:52.650485 140529919506176 logging_writer.py:48] [122000] global_step=122000, grad_norm=0.3615587651729584, loss=3.5746891498565674
I0208 04:23:27.931101 140529927898880 logging_writer.py:48] [122100] global_step=122100, grad_norm=0.4011581242084503, loss=3.6345109939575195
I0208 04:24:03.259384 140529919506176 logging_writer.py:48] [122200] global_step=122200, grad_norm=0.3856014609336853, loss=3.601572036743164
I0208 04:24:38.568089 140529927898880 logging_writer.py:48] [122300] global_step=122300, grad_norm=0.3757108151912689, loss=3.5867905616760254
I0208 04:25:13.887340 140529919506176 logging_writer.py:48] [122400] global_step=122400, grad_norm=0.41644713282585144, loss=3.6753053665161133
I0208 04:25:49.167166 140529927898880 logging_writer.py:48] [122500] global_step=122500, grad_norm=0.3671163320541382, loss=3.5923566818237305
I0208 04:26:24.479642 140529919506176 logging_writer.py:48] [122600] global_step=122600, grad_norm=0.36685776710510254, loss=3.588106393814087
I0208 04:26:59.758326 140529927898880 logging_writer.py:48] [122700] global_step=122700, grad_norm=0.4081191420555115, loss=3.6583375930786133
I0208 04:27:35.020491 140529919506176 logging_writer.py:48] [122800] global_step=122800, grad_norm=0.371535062789917, loss=3.5550107955932617
I0208 04:28:10.327960 140529927898880 logging_writer.py:48] [122900] global_step=122900, grad_norm=0.3871758282184601, loss=3.61004900932312
I0208 04:28:45.602043 140529919506176 logging_writer.py:48] [123000] global_step=123000, grad_norm=0.3942376971244812, loss=3.607018232345581
I0208 04:29:20.845157 140529927898880 logging_writer.py:48] [123100] global_step=123100, grad_norm=0.37213730812072754, loss=3.595590353012085
I0208 04:29:56.106305 140529919506176 logging_writer.py:48] [123200] global_step=123200, grad_norm=0.38622552156448364, loss=3.6316583156585693
I0208 04:30:31.354754 140529927898880 logging_writer.py:48] [123300] global_step=123300, grad_norm=0.38559144735336304, loss=3.5875837802886963
I0208 04:31:06.631595 140529919506176 logging_writer.py:48] [123400] global_step=123400, grad_norm=0.38450509309768677, loss=3.565372943878174
I0208 04:31:41.915864 140529927898880 logging_writer.py:48] [123500] global_step=123500, grad_norm=0.3931137025356293, loss=3.6298816204071045
I0208 04:32:17.212854 140529919506176 logging_writer.py:48] [123600] global_step=123600, grad_norm=0.38446950912475586, loss=3.5977163314819336
I0208 04:32:52.476539 140529927898880 logging_writer.py:48] [123700] global_step=123700, grad_norm=0.3863331079483032, loss=3.620755195617676
I0208 04:33:27.732137 140529919506176 logging_writer.py:48] [123800] global_step=123800, grad_norm=0.36848852038383484, loss=3.6108927726745605
I0208 04:34:03.011478 140529927898880 logging_writer.py:48] [123900] global_step=123900, grad_norm=0.3879847526550293, loss=3.6266608238220215
I0208 04:34:06.610581 140699726837568 spec.py:321] Evaluating on the training split.
I0208 04:34:09.650624 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 04:37:07.578002 140699726837568 spec.py:333] Evaluating on the validation split.
I0208 04:37:10.317972 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 04:39:42.432880 140699726837568 spec.py:349] Evaluating on the test split.
I0208 04:39:45.178674 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 04:42:23.168706 140699726837568 submission_runner.py:408] Time since start: 71700.77s, 	Step: 123912, 	{'train/accuracy': 0.7188615202903748, 'train/loss': 1.4707727432250977, 'train/bleu': 37.37068803202791, 'validation/accuracy': 0.6907415986061096, 'validation/loss': 1.5994369983673096, 'validation/bleu': 30.730796295851167, 'validation/num_examples': 3000, 'test/accuracy': 0.7107663750648499, 'test/loss': 1.4905011653900146, 'test/bleu': 30.858893171024814, 'test/num_examples': 3003, 'score': 43710.785326480865, 'total_duration': 71700.7684583664, 'accumulated_submission_time': 43710.785326480865, 'accumulated_eval_time': 27984.25898528099, 'accumulated_logging_time': 1.8253540992736816}
I0208 04:42:23.200816 140529919506176 logging_writer.py:48] [123912] accumulated_eval_time=27984.258985, accumulated_logging_time=1.825354, accumulated_submission_time=43710.785326, global_step=123912, preemption_count=0, score=43710.785326, test/accuracy=0.710766, test/bleu=30.858893, test/loss=1.490501, test/num_examples=3003, total_duration=71700.768458, train/accuracy=0.718862, train/bleu=37.370688, train/loss=1.470773, validation/accuracy=0.690742, validation/bleu=30.730796, validation/loss=1.599437, validation/num_examples=3000
I0208 04:42:54.463963 140529927898880 logging_writer.py:48] [124000] global_step=124000, grad_norm=0.379625141620636, loss=3.5901405811309814
I0208 04:43:29.619847 140529919506176 logging_writer.py:48] [124100] global_step=124100, grad_norm=0.3945188522338867, loss=3.5998895168304443
I0208 04:44:04.882961 140529927898880 logging_writer.py:48] [124200] global_step=124200, grad_norm=0.40051349997520447, loss=3.6262569427490234
I0208 04:44:40.132765 140529919506176 logging_writer.py:48] [124300] global_step=124300, grad_norm=0.39216259121894836, loss=3.6295559406280518
I0208 04:45:15.372186 140529927898880 logging_writer.py:48] [124400] global_step=124400, grad_norm=0.3833988308906555, loss=3.6316475868225098
I0208 04:45:50.624260 140529919506176 logging_writer.py:48] [124500] global_step=124500, grad_norm=0.40879327058792114, loss=3.649587631225586
I0208 04:46:25.926756 140529927898880 logging_writer.py:48] [124600] global_step=124600, grad_norm=0.37885916233062744, loss=3.62447190284729
I0208 04:47:01.200088 140529919506176 logging_writer.py:48] [124700] global_step=124700, grad_norm=0.3952205181121826, loss=3.632824420928955
I0208 04:47:36.466533 140529927898880 logging_writer.py:48] [124800] global_step=124800, grad_norm=0.37867623567581177, loss=3.5540943145751953
I0208 04:48:11.741297 140529919506176 logging_writer.py:48] [124900] global_step=124900, grad_norm=0.4040949046611786, loss=3.6026058197021484
I0208 04:48:47.051221 140529927898880 logging_writer.py:48] [125000] global_step=125000, grad_norm=0.3837567865848541, loss=3.536397933959961
I0208 04:49:22.328787 140529919506176 logging_writer.py:48] [125100] global_step=125100, grad_norm=0.4042482078075409, loss=3.6487877368927
I0208 04:49:57.636247 140529927898880 logging_writer.py:48] [125200] global_step=125200, grad_norm=0.3928908109664917, loss=3.6241865158081055
I0208 04:50:32.982682 140529919506176 logging_writer.py:48] [125300] global_step=125300, grad_norm=0.3833111822605133, loss=3.6158270835876465
I0208 04:51:08.277275 140529927898880 logging_writer.py:48] [125400] global_step=125400, grad_norm=0.3783796727657318, loss=3.623638391494751
I0208 04:51:43.554258 140529919506176 logging_writer.py:48] [125500] global_step=125500, grad_norm=0.37733888626098633, loss=3.610163927078247
I0208 04:52:18.825591 140529927898880 logging_writer.py:48] [125600] global_step=125600, grad_norm=0.3763503432273865, loss=3.6099510192871094
I0208 04:52:54.089735 140529919506176 logging_writer.py:48] [125700] global_step=125700, grad_norm=0.37935206294059753, loss=3.597334623336792
I0208 04:53:29.401123 140529927898880 logging_writer.py:48] [125800] global_step=125800, grad_norm=0.3967336118221283, loss=3.632622241973877
I0208 04:54:04.715813 140529919506176 logging_writer.py:48] [125900] global_step=125900, grad_norm=0.38172829151153564, loss=3.5600476264953613
I0208 04:54:40.032358 140529927898880 logging_writer.py:48] [126000] global_step=126000, grad_norm=0.39855846762657166, loss=3.611637830734253
I0208 04:55:15.340089 140529919506176 logging_writer.py:48] [126100] global_step=126100, grad_norm=0.38920658826828003, loss=3.6100828647613525
I0208 04:55:50.611551 140529927898880 logging_writer.py:48] [126200] global_step=126200, grad_norm=0.39520585536956787, loss=3.628960371017456
I0208 04:56:23.495921 140699726837568 spec.py:321] Evaluating on the training split.
I0208 04:56:26.528449 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 04:59:28.267574 140699726837568 spec.py:333] Evaluating on the validation split.
I0208 04:59:31.018263 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 05:02:17.219192 140699726837568 spec.py:349] Evaluating on the test split.
I0208 05:02:19.965355 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 05:04:53.455551 140699726837568 submission_runner.py:408] Time since start: 73051.06s, 	Step: 126295, 	{'train/accuracy': 0.7226437330245972, 'train/loss': 1.4515371322631836, 'train/bleu': 38.29304533888482, 'validation/accuracy': 0.6909151673316956, 'validation/loss': 1.5993297100067139, 'validation/bleu': 30.735822581848513, 'validation/num_examples': 3000, 'test/accuracy': 0.7114520072937012, 'test/loss': 1.4903055429458618, 'test/bleu': 30.760997036011428, 'test/num_examples': 3003, 'score': 44550.9940226078, 'total_duration': 73051.0552790165, 'accumulated_submission_time': 44550.9940226078, 'accumulated_eval_time': 28494.218544483185, 'accumulated_logging_time': 1.8677458763122559}
I0208 05:04:53.495946 140529919506176 logging_writer.py:48] [126295] accumulated_eval_time=28494.218544, accumulated_logging_time=1.867746, accumulated_submission_time=44550.994023, global_step=126295, preemption_count=0, score=44550.994023, test/accuracy=0.711452, test/bleu=30.760997, test/loss=1.490306, test/num_examples=3003, total_duration=73051.055279, train/accuracy=0.722644, train/bleu=38.293045, train/loss=1.451537, validation/accuracy=0.690915, validation/bleu=30.735823, validation/loss=1.599330, validation/num_examples=3000
I0208 05:04:55.624317 140529927898880 logging_writer.py:48] [126300] global_step=126300, grad_norm=0.3809795677661896, loss=3.629013776779175
I0208 05:05:30.742276 140529919506176 logging_writer.py:48] [126400] global_step=126400, grad_norm=0.3800152838230133, loss=3.6402809619903564
I0208 05:06:05.951877 140529927898880 logging_writer.py:48] [126500] global_step=126500, grad_norm=0.3724873960018158, loss=3.5681252479553223
I0208 05:06:41.219600 140529919506176 logging_writer.py:48] [126600] global_step=126600, grad_norm=0.38145118951797485, loss=3.6367757320404053
I0208 05:07:16.510734 140529927898880 logging_writer.py:48] [126700] global_step=126700, grad_norm=0.37218230962753296, loss=3.607133150100708
I0208 05:07:51.857064 140529919506176 logging_writer.py:48] [126800] global_step=126800, grad_norm=0.37393718957901, loss=3.5813400745391846
I0208 05:08:27.098411 140529927898880 logging_writer.py:48] [126900] global_step=126900, grad_norm=0.4006076455116272, loss=3.651207447052002
I0208 05:09:02.365551 140529919506176 logging_writer.py:48] [127000] global_step=127000, grad_norm=0.382292240858078, loss=3.5659635066986084
I0208 05:09:37.618271 140529927898880 logging_writer.py:48] [127100] global_step=127100, grad_norm=0.3936612904071808, loss=3.593733549118042
I0208 05:10:12.909306 140529919506176 logging_writer.py:48] [127200] global_step=127200, grad_norm=0.384174108505249, loss=3.6244351863861084
I0208 05:10:48.233848 140529927898880 logging_writer.py:48] [127300] global_step=127300, grad_norm=0.3779478073120117, loss=3.6284701824188232
I0208 05:11:23.544366 140529919506176 logging_writer.py:48] [127400] global_step=127400, grad_norm=0.41087040305137634, loss=3.7192201614379883
I0208 05:11:58.818997 140529927898880 logging_writer.py:48] [127500] global_step=127500, grad_norm=0.39875608682632446, loss=3.5876240730285645
I0208 05:12:34.113434 140529919506176 logging_writer.py:48] [127600] global_step=127600, grad_norm=0.4024302661418915, loss=3.6474995613098145
I0208 05:13:09.395283 140529927898880 logging_writer.py:48] [127700] global_step=127700, grad_norm=0.39704397320747375, loss=3.61374568939209
I0208 05:13:44.702382 140529919506176 logging_writer.py:48] [127800] global_step=127800, grad_norm=0.3854808807373047, loss=3.6241817474365234
I0208 05:14:19.992991 140529927898880 logging_writer.py:48] [127900] global_step=127900, grad_norm=0.3827081620693207, loss=3.5765175819396973
I0208 05:14:55.267488 140529919506176 logging_writer.py:48] [128000] global_step=128000, grad_norm=0.4121849536895752, loss=3.6145966053009033
I0208 05:15:30.587066 140529927898880 logging_writer.py:48] [128100] global_step=128100, grad_norm=0.38092899322509766, loss=3.5814008712768555
I0208 05:16:05.874477 140529919506176 logging_writer.py:48] [128200] global_step=128200, grad_norm=0.3865775465965271, loss=3.5565438270568848
I0208 05:16:41.183707 140529927898880 logging_writer.py:48] [128300] global_step=128300, grad_norm=0.3827587962150574, loss=3.568572759628296
I0208 05:17:16.494525 140529919506176 logging_writer.py:48] [128400] global_step=128400, grad_norm=0.38101398944854736, loss=3.592151165008545
I0208 05:17:51.833152 140529927898880 logging_writer.py:48] [128500] global_step=128500, grad_norm=0.3783487379550934, loss=3.5515589714050293
I0208 05:18:27.125895 140529919506176 logging_writer.py:48] [128600] global_step=128600, grad_norm=0.37736350297927856, loss=3.612778902053833
I0208 05:18:53.685077 140699726837568 spec.py:321] Evaluating on the training split.
I0208 05:18:56.720237 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 05:22:05.091995 140699726837568 spec.py:333] Evaluating on the validation split.
I0208 05:22:07.838700 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 05:24:44.386748 140699726837568 spec.py:349] Evaluating on the test split.
I0208 05:24:47.138851 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 05:27:29.936671 140699726837568 submission_runner.py:408] Time since start: 74407.54s, 	Step: 128677, 	{'train/accuracy': 0.7253398895263672, 'train/loss': 1.437965750694275, 'train/bleu': 38.15674927738666, 'validation/accuracy': 0.6909027695655823, 'validation/loss': 1.5986698865890503, 'validation/bleu': 30.539796950932832, 'validation/num_examples': 3000, 'test/accuracy': 0.7110103964805603, 'test/loss': 1.4899711608886719, 'test/bleu': 30.711279312995767, 'test/num_examples': 3003, 'score': 45391.09645104408, 'total_duration': 74407.53640341759, 'accumulated_submission_time': 45391.09645104408, 'accumulated_eval_time': 29010.47008228302, 'accumulated_logging_time': 1.9190173149108887}
I0208 05:27:29.971369 140529927898880 logging_writer.py:48] [128677] accumulated_eval_time=29010.470082, accumulated_logging_time=1.919017, accumulated_submission_time=45391.096451, global_step=128677, preemption_count=0, score=45391.096451, test/accuracy=0.711010, test/bleu=30.711279, test/loss=1.489971, test/num_examples=3003, total_duration=74407.536403, train/accuracy=0.725340, train/bleu=38.156749, train/loss=1.437966, validation/accuracy=0.690903, validation/bleu=30.539797, validation/loss=1.598670, validation/num_examples=3000
I0208 05:27:38.411269 140529919506176 logging_writer.py:48] [128700] global_step=128700, grad_norm=0.3907149136066437, loss=3.6168742179870605
I0208 05:28:13.576269 140529927898880 logging_writer.py:48] [128800] global_step=128800, grad_norm=0.3717895448207855, loss=3.568389654159546
I0208 05:28:48.793204 140529919506176 logging_writer.py:48] [128900] global_step=128900, grad_norm=0.4003297984600067, loss=3.6276659965515137
I0208 05:29:24.027969 140529927898880 logging_writer.py:48] [129000] global_step=129000, grad_norm=0.3891497850418091, loss=3.646582841873169
I0208 05:29:59.308876 140529919506176 logging_writer.py:48] [129100] global_step=129100, grad_norm=0.37749210000038147, loss=3.5964274406433105
I0208 05:30:34.608099 140529927898880 logging_writer.py:48] [129200] global_step=129200, grad_norm=0.399582177400589, loss=3.6087646484375
I0208 05:31:09.900655 140529919506176 logging_writer.py:48] [129300] global_step=129300, grad_norm=0.3982172906398773, loss=3.6267151832580566
I0208 05:31:45.176809 140529927898880 logging_writer.py:48] [129400] global_step=129400, grad_norm=0.38502931594848633, loss=3.604759693145752
I0208 05:32:20.450881 140529919506176 logging_writer.py:48] [129500] global_step=129500, grad_norm=0.37726882100105286, loss=3.5690391063690186
I0208 05:32:55.749604 140529927898880 logging_writer.py:48] [129600] global_step=129600, grad_norm=0.3968753516674042, loss=3.594541311264038
I0208 05:33:31.076212 140529919506176 logging_writer.py:48] [129700] global_step=129700, grad_norm=0.3875545263290405, loss=3.5726158618927
I0208 05:34:06.374137 140529927898880 logging_writer.py:48] [129800] global_step=129800, grad_norm=0.3830409348011017, loss=3.5725860595703125
I0208 05:34:41.654372 140529919506176 logging_writer.py:48] [129900] global_step=129900, grad_norm=0.4127650558948517, loss=3.6281776428222656
I0208 05:35:16.934634 140529927898880 logging_writer.py:48] [130000] global_step=130000, grad_norm=0.3680645525455475, loss=3.557783365249634
I0208 05:35:52.206741 140529919506176 logging_writer.py:48] [130100] global_step=130100, grad_norm=0.38367193937301636, loss=3.5935006141662598
I0208 05:36:27.470786 140529927898880 logging_writer.py:48] [130200] global_step=130200, grad_norm=0.3931625783443451, loss=3.6242141723632812
I0208 05:37:02.759472 140529919506176 logging_writer.py:48] [130300] global_step=130300, grad_norm=0.3855339288711548, loss=3.6047511100769043
I0208 05:37:38.029860 140529927898880 logging_writer.py:48] [130400] global_step=130400, grad_norm=0.3866891860961914, loss=3.624965190887451
I0208 05:38:13.331387 140529919506176 logging_writer.py:48] [130500] global_step=130500, grad_norm=0.38092678785324097, loss=3.583261013031006
I0208 05:38:48.650111 140529927898880 logging_writer.py:48] [130600] global_step=130600, grad_norm=0.3811498284339905, loss=3.61391019821167
I0208 05:39:23.986913 140529919506176 logging_writer.py:48] [130700] global_step=130700, grad_norm=0.3694365620613098, loss=3.5753068923950195
I0208 05:39:59.416066 140529927898880 logging_writer.py:48] [130800] global_step=130800, grad_norm=0.3809957504272461, loss=3.6172707080841064
I0208 05:40:34.725492 140529919506176 logging_writer.py:48] [130900] global_step=130900, grad_norm=0.39394980669021606, loss=3.6334147453308105
I0208 05:41:09.997728 140529927898880 logging_writer.py:48] [131000] global_step=131000, grad_norm=0.38328710198402405, loss=3.6162469387054443
I0208 05:41:30.182831 140699726837568 spec.py:321] Evaluating on the training split.
I0208 05:41:33.253379 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 05:44:41.564877 140699726837568 spec.py:333] Evaluating on the validation split.
I0208 05:44:44.304867 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 05:47:28.662908 140699726837568 spec.py:349] Evaluating on the test split.
I0208 05:47:31.410897 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 05:50:09.296446 140699726837568 submission_runner.py:408] Time since start: 75766.90s, 	Step: 131059, 	{'train/accuracy': 0.724202036857605, 'train/loss': 1.443503499031067, 'train/bleu': 38.04270301127323, 'validation/accuracy': 0.6908655762672424, 'validation/loss': 1.599054217338562, 'validation/bleu': 30.667733220414544, 'validation/num_examples': 3000, 'test/accuracy': 0.7107431292533875, 'test/loss': 1.490159273147583, 'test/bleu': 30.77321518935283, 'test/num_examples': 3003, 'score': 46231.218935251236, 'total_duration': 75766.89620161057, 'accumulated_submission_time': 46231.218935251236, 'accumulated_eval_time': 29529.583674669266, 'accumulated_logging_time': 1.9653499126434326}
I0208 05:50:09.330643 140529919506176 logging_writer.py:48] [131059] accumulated_eval_time=29529.583675, accumulated_logging_time=1.965350, accumulated_submission_time=46231.218935, global_step=131059, preemption_count=0, score=46231.218935, test/accuracy=0.710743, test/bleu=30.773215, test/loss=1.490159, test/num_examples=3003, total_duration=75766.896202, train/accuracy=0.724202, train/bleu=38.042703, train/loss=1.443503, validation/accuracy=0.690866, validation/bleu=30.667733, validation/loss=1.599054, validation/num_examples=3000
I0208 05:50:24.094940 140529927898880 logging_writer.py:48] [131100] global_step=131100, grad_norm=0.39208438992500305, loss=3.6367886066436768
I0208 05:50:59.240324 140529919506176 logging_writer.py:48] [131200] global_step=131200, grad_norm=0.38882529735565186, loss=3.624690532684326
I0208 05:51:34.505024 140529927898880 logging_writer.py:48] [131300] global_step=131300, grad_norm=0.3951231837272644, loss=3.6153838634490967
I0208 05:52:09.808486 140529919506176 logging_writer.py:48] [131400] global_step=131400, grad_norm=0.3797282576560974, loss=3.5915603637695312
I0208 05:52:45.067121 140529927898880 logging_writer.py:48] [131500] global_step=131500, grad_norm=0.36819183826446533, loss=3.5638718605041504
I0208 05:53:20.372321 140529919506176 logging_writer.py:48] [131600] global_step=131600, grad_norm=0.39313989877700806, loss=3.6467323303222656
I0208 05:53:55.717684 140529927898880 logging_writer.py:48] [131700] global_step=131700, grad_norm=0.389342725276947, loss=3.6481618881225586
I0208 05:54:31.126924 140529919506176 logging_writer.py:48] [131800] global_step=131800, grad_norm=0.37591052055358887, loss=3.609947443008423
I0208 05:55:06.474792 140529927898880 logging_writer.py:48] [131900] global_step=131900, grad_norm=0.3656066656112671, loss=3.538616418838501
I0208 05:55:41.748729 140529919506176 logging_writer.py:48] [132000] global_step=132000, grad_norm=0.39653480052948, loss=3.6553661823272705
I0208 05:56:17.025213 140529927898880 logging_writer.py:48] [132100] global_step=132100, grad_norm=0.39161786437034607, loss=3.5918312072753906
I0208 05:56:52.335821 140529919506176 logging_writer.py:48] [132200] global_step=132200, grad_norm=0.37798938155174255, loss=3.607531785964966
I0208 05:57:27.637757 140529927898880 logging_writer.py:48] [132300] global_step=132300, grad_norm=0.39134594798088074, loss=3.6302502155303955
I0208 05:58:02.895084 140529919506176 logging_writer.py:48] [132400] global_step=132400, grad_norm=0.3780762851238251, loss=3.5712361335754395
I0208 05:58:38.178432 140529927898880 logging_writer.py:48] [132500] global_step=132500, grad_norm=0.39103269577026367, loss=3.604245662689209
I0208 05:59:13.446564 140529919506176 logging_writer.py:48] [132600] global_step=132600, grad_norm=0.39897823333740234, loss=3.6271018981933594
I0208 05:59:48.748467 140529927898880 logging_writer.py:48] [132700] global_step=132700, grad_norm=0.38583797216415405, loss=3.5985116958618164
I0208 06:00:24.063673 140529919506176 logging_writer.py:48] [132800] global_step=132800, grad_norm=0.3998970687389374, loss=3.6403250694274902
I0208 06:00:59.362474 140529927898880 logging_writer.py:48] [132900] global_step=132900, grad_norm=0.3945519030094147, loss=3.6153359413146973
I0208 06:01:34.697487 140529919506176 logging_writer.py:48] [133000] global_step=133000, grad_norm=0.3785151541233063, loss=3.5627570152282715
I0208 06:02:10.012054 140529927898880 logging_writer.py:48] [133100] global_step=133100, grad_norm=0.39287301898002625, loss=3.639568328857422
I0208 06:02:45.315782 140529919506176 logging_writer.py:48] [133200] global_step=133200, grad_norm=0.3714645802974701, loss=3.615748882293701
I0208 06:03:20.582544 140529927898880 logging_writer.py:48] [133300] global_step=133300, grad_norm=0.37000134587287903, loss=3.561570882797241
I0208 06:03:31.589591 140699726837568 spec.py:321] Evaluating on the training split.
I0208 06:03:34.629061 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 06:06:41.871984 140699726837568 spec.py:333] Evaluating on the validation split.
I0208 06:06:44.618831 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 06:09:28.083493 140699726837568 spec.py:349] Evaluating on the test split.
I0208 06:09:30.820721 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 06:12:14.831175 140699726837568 submission_runner.py:408] Time since start: 77092.43s, 	Step: 133333, 	{'train/accuracy': 0.7267444133758545, 'train/loss': 1.427182912826538, 'train/bleu': 37.58713677051531, 'validation/accuracy': 0.6907787919044495, 'validation/loss': 1.5991333723068237, 'validation/bleu': 30.66343634971797, 'validation/num_examples': 3000, 'test/accuracy': 0.7107315063476562, 'test/loss': 1.4902734756469727, 'test/bleu': 30.821354274030078, 'test/num_examples': 3003, 'score': 47033.39378809929, 'total_duration': 77092.4309284687, 'accumulated_submission_time': 47033.39378809929, 'accumulated_eval_time': 30052.825205802917, 'accumulated_logging_time': 2.0094943046569824}
I0208 06:12:14.865964 140529919506176 logging_writer.py:48] [133333] accumulated_eval_time=30052.825206, accumulated_logging_time=2.009494, accumulated_submission_time=47033.393788, global_step=133333, preemption_count=0, score=47033.393788, test/accuracy=0.710732, test/bleu=30.821354, test/loss=1.490273, test/num_examples=3003, total_duration=77092.430928, train/accuracy=0.726744, train/bleu=37.587137, train/loss=1.427183, validation/accuracy=0.690779, validation/bleu=30.663436, validation/loss=1.599133, validation/num_examples=3000
I0208 06:12:14.899982 140529927898880 logging_writer.py:48] [133333] global_step=133333, preemption_count=0, score=47033.393788
I0208 06:12:16.114271 140699726837568 checkpoints.py:490] Saving checkpoint at step: 133333
I0208 06:12:20.093079 140699726837568 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_1/wmt_jax/trial_2/checkpoint_133333
I0208 06:12:20.097700 140699726837568 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_1/wmt_jax/trial_2/checkpoint_133333.
I0208 06:12:20.145222 140699726837568 submission_runner.py:583] Tuning trial 2/5
I0208 06:12:20.145369 140699726837568 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.2, learning_rate=0.0008445074561975979, one_minus_beta1=0.11042418465, beta2=0.9978504782314613, weight_decay=0.08135402759553023, warmup_factor=0.05)
I0208 06:12:20.150961 140699726837568 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0005899437237530947, 'train/loss': 11.17578411102295, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.208685874938965, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.19086742401123, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 26.80888557434082, 'total_duration': 901.8242671489716, 'accumulated_submission_time': 26.80888557434082, 'accumulated_eval_time': 875.0153396129608, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (2380, {'train/accuracy': 0.4236276149749756, 'train/loss': 3.9450714588165283, 'train/bleu': 15.317106956985512, 'validation/accuracy': 0.4104350805282593, 'validation/loss': 4.064540863037109, 'validation/bleu': 10.871939591746955, 'validation/num_examples': 3000, 'test/accuracy': 0.4006739854812622, 'test/loss': 4.220223426818848, 'test/bleu': 9.119701587852655, 'test/num_examples': 3003, 'score': 866.9106457233429, 'total_duration': 2464.131246328354, 'accumulated_submission_time': 866.9106457233429, 'accumulated_eval_time': 1597.1277873516083, 'accumulated_logging_time': 0.01984238624572754, 'global_step': 2380, 'preemption_count': 0}), (4760, {'train/accuracy': 0.5412089824676514, 'train/loss': 2.8526296615600586, 'train/bleu': 24.40067253420964, 'validation/accuracy': 0.5490322709083557, 'validation/loss': 2.781830072402954, 'validation/bleu': 20.54890937518002, 'validation/num_examples': 3000, 'test/accuracy': 0.5474405884742737, 'test/loss': 2.8263823986053467, 'test/bleu': 19.069473249226416, 'test/num_examples': 3003, 'score': 1707.1207020282745, 'total_duration': 3798.2001650333405, 'accumulated_submission_time': 1707.1207020282745, 'accumulated_eval_time': 2090.888371706009, 'accumulated_logging_time': 0.045859575271606445, 'global_step': 4760, 'preemption_count': 0}), (7142, {'train/accuracy': 0.5862109065055847, 'train/loss': 2.43117094039917, 'train/bleu': 26.95864129778303, 'validation/accuracy': 0.5920323133468628, 'validation/loss': 2.3843495845794678, 'validation/bleu': 23.53881679992252, 'validation/num_examples': 3000, 'test/accuracy': 0.5926558971405029, 'test/loss': 2.3840017318725586, 'test/bleu': 22.39646133848995, 'test/num_examples': 3003, 'score': 2547.1937956809998, 'total_duration': 5128.208259344101, 'accumulated_submission_time': 2547.1937956809998, 'accumulated_eval_time': 2580.725028514862, 'accumulated_logging_time': 0.07153177261352539, 'global_step': 7142, 'preemption_count': 0}), (9524, {'train/accuracy': 0.5987629294395447, 'train/loss': 2.312525987625122, 'train/bleu': 28.450225323180657, 'validation/accuracy': 0.6168057322502136, 'validation/loss': 2.1870875358581543, 'validation/bleu': 25.084379421999714, 'validation/num_examples': 3000, 'test/accuracy': 0.6223113536834717, 'test/loss': 2.157191276550293, 'test/bleu': 24.093482603258604, 'test/num_examples': 3003, 'score': 3387.143469810486, 'total_duration': 6447.921786308289, 'accumulated_submission_time': 3387.143469810486, 'accumulated_eval_time': 3060.3883907794952, 'accumulated_logging_time': 0.0963890552520752, 'global_step': 9524, 'preemption_count': 0}), (11908, {'train/accuracy': 0.6138762831687927, 'train/loss': 2.1910603046417236, 'train/bleu': 29.359989662943025, 'validation/accuracy': 0.632627010345459, 'validation/loss': 2.045173168182373, 'validation/bleu': 26.211492834655424, 'validation/num_examples': 3000, 'test/accuracy': 0.6385219097137451, 'test/loss': 2.0061533451080322, 'test/bleu': 25.16989331342719, 'test/num_examples': 3003, 'score': 4227.378788471222, 'total_duration': 7763.146024465561, 'accumulated_submission_time': 4227.378788471222, 'accumulated_eval_time': 3535.278825521469, 'accumulated_logging_time': 0.12252569198608398, 'global_step': 11908, 'preemption_count': 0}), (14290, {'train/accuracy': 0.6261821985244751, 'train/loss': 2.0904488563537598, 'train/bleu': 30.646065698482577, 'validation/accuracy': 0.6428066492080688, 'validation/loss': 1.982466459274292, 'validation/bleu': 27.19086719876297, 'validation/num_examples': 3000, 'test/accuracy': 0.6499332189559937, 'test/loss': 1.9334949254989624, 'test/bleu': 26.31243160118559, 'test/num_examples': 3003, 'score': 5067.369838953018, 'total_duration': 9073.389715909958, 'accumulated_submission_time': 5067.369838953018, 'accumulated_eval_time': 4005.4279165267944, 'accumulated_logging_time': 0.14914679527282715, 'global_step': 14290, 'preemption_count': 0}), (16673, {'train/accuracy': 0.6261575818061829, 'train/loss': 2.085031509399414, 'train/bleu': 30.482522261779774, 'validation/accuracy': 0.648510217666626, 'validation/loss': 1.9287440776824951, 'validation/bleu': 27.83188217261842, 'validation/num_examples': 3000, 'test/accuracy': 0.6567311882972717, 'test/loss': 1.875162959098816, 'test/bleu': 26.530187275714983, 'test/num_examples': 3003, 'score': 5907.608487606049, 'total_duration': 10408.735023498535, 'accumulated_submission_time': 5907.608487606049, 'accumulated_eval_time': 4500.4322681427, 'accumulated_logging_time': 0.177018404006958, 'global_step': 16673, 'preemption_count': 0}), (19056, {'train/accuracy': 0.6523534059524536, 'train/loss': 1.8958368301391602, 'train/bleu': 32.224112132764745, 'validation/accuracy': 0.6549577713012695, 'validation/loss': 1.867025375366211, 'validation/bleu': 28.023329453083598, 'validation/num_examples': 3000, 'test/accuracy': 0.6621579527854919, 'test/loss': 1.8095208406448364, 'test/bleu': 27.09557119536779, 'test/num_examples': 3003, 'score': 6747.7297422885895, 'total_duration': 11738.225252628326, 'accumulated_submission_time': 6747.7297422885895, 'accumulated_eval_time': 4989.699973106384, 'accumulated_logging_time': 0.20397615432739258, 'global_step': 19056, 'preemption_count': 0}), (21439, {'train/accuracy': 0.6392760276794434, 'train/loss': 1.9621353149414062, 'train/bleu': 31.365681946344402, 'validation/accuracy': 0.6572020053863525, 'validation/loss': 1.832504153251648, 'validation/bleu': 28.291012927380795, 'validation/num_examples': 3000, 'test/accuracy': 0.667910099029541, 'test/loss': 1.771620273590088, 'test/bleu': 27.43810439700653, 'test/num_examples': 3003, 'score': 7587.909989356995, 'total_duration': 13052.695889472961, 'accumulated_submission_time': 7587.909989356995, 'accumulated_eval_time': 5463.887475013733, 'accumulated_logging_time': 0.2319626808166504, 'global_step': 21439, 'preemption_count': 0}), (23822, {'train/accuracy': 0.643540620803833, 'train/loss': 1.939845085144043, 'train/bleu': 31.37433547834663, 'validation/accuracy': 0.6603513956069946, 'validation/loss': 1.8111510276794434, 'validation/bleu': 28.123673946788806, 'validation/num_examples': 3000, 'test/accuracy': 0.6710127592086792, 'test/loss': 1.7506834268569946, 'test/bleu': 27.898585149515966, 'test/num_examples': 3003, 'score': 8427.87146282196, 'total_duration': 14363.563488960266, 'accumulated_submission_time': 8427.87146282196, 'accumulated_eval_time': 5934.689563751221, 'accumulated_logging_time': 0.2605419158935547, 'global_step': 23822, 'preemption_count': 0}), (26206, {'train/accuracy': 0.653814435005188, 'train/loss': 1.8643923997879028, 'train/bleu': 32.415930823155556, 'validation/accuracy': 0.6627939939498901, 'validation/loss': 1.7915291786193848, 'validation/bleu': 28.888755920109194, 'validation/num_examples': 3000, 'test/accuracy': 0.6742897033691406, 'test/loss': 1.722639799118042, 'test/bleu': 28.15439954133418, 'test/num_examples': 3003, 'score': 9268.00053191185, 'total_duration': 15734.331931114197, 'accumulated_submission_time': 9268.00053191185, 'accumulated_eval_time': 6465.22674870491, 'accumulated_logging_time': 0.2896280288696289, 'global_step': 26206, 'preemption_count': 0}), (28591, {'train/accuracy': 0.6467625498771667, 'train/loss': 1.921960711479187, 'train/bleu': 31.52587145991113, 'validation/accuracy': 0.6663525700569153, 'validation/loss': 1.796502947807312, 'validation/bleu': 28.858447484971762, 'validation/num_examples': 3000, 'test/accuracy': 0.6772529482841492, 'test/loss': 1.7303847074508667, 'test/bleu': 28.447757417888468, 'test/num_examples': 3003, 'score': 10108.170245409012, 'total_duration': 17071.990015506744, 'accumulated_submission_time': 10108.170245409012, 'accumulated_eval_time': 6962.611318826675, 'accumulated_logging_time': 0.31827235221862793, 'global_step': 28591, 'preemption_count': 0}), (30974, {'train/accuracy': 0.6443696022033691, 'train/loss': 1.9311410188674927, 'train/bleu': 31.986524329560304, 'validation/accuracy': 0.6660549640655518, 'validation/loss': 1.7905811071395874, 'validation/bleu': 28.60632817214931, 'validation/num_examples': 3000, 'test/accuracy': 0.6762768030166626, 'test/loss': 1.725675344467163, 'test/bleu': 27.87737284091013, 'test/num_examples': 3003, 'score': 10948.20868062973, 'total_duration': 18420.80942606926, 'accumulated_submission_time': 10948.20868062973, 'accumulated_eval_time': 7471.288013458252, 'accumulated_logging_time': 0.3465893268585205, 'global_step': 30974, 'preemption_count': 0}), (33357, {'train/accuracy': 0.6579074263572693, 'train/loss': 1.8461037874221802, 'train/bleu': 32.069965372459464, 'validation/accuracy': 0.6691919565200806, 'validation/loss': 1.755408525466919, 'validation/bleu': 29.00561299490477, 'validation/num_examples': 3000, 'test/accuracy': 0.680727481842041, 'test/loss': 1.6848061084747314, 'test/bleu': 28.342088839250426, 'test/num_examples': 3003, 'score': 11788.108996391296, 'total_duration': 19917.942067861557, 'accumulated_submission_time': 11788.108996391296, 'accumulated_eval_time': 8128.417171955109, 'accumulated_logging_time': 0.37586474418640137, 'global_step': 33357, 'preemption_count': 0}), (35740, {'train/accuracy': 0.6511590480804443, 'train/loss': 1.8901034593582153, 'train/bleu': 32.00241740281816, 'validation/accuracy': 0.6663649678230286, 'validation/loss': 1.756929874420166, 'validation/bleu': 29.13012192945443, 'validation/num_examples': 3000, 'test/accuracy': 0.6793214082717896, 'test/loss': 1.6866551637649536, 'test/bleu': 28.543354366147703, 'test/num_examples': 3003, 'score': 12628.127183675766, 'total_duration': 21234.825357437134, 'accumulated_submission_time': 12628.127183675766, 'accumulated_eval_time': 8605.174662351608, 'accumulated_logging_time': 0.40660762786865234, 'global_step': 35740, 'preemption_count': 0}), (38123, {'train/accuracy': 0.6650872230529785, 'train/loss': 1.7879432439804077, 'train/bleu': 33.173089200434895, 'validation/accuracy': 0.6712377667427063, 'validation/loss': 1.7390223741531372, 'validation/bleu': 29.3184200998274, 'validation/num_examples': 3000, 'test/accuracy': 0.6831677556037903, 'test/loss': 1.668337106704712, 'test/bleu': 28.572843907960156, 'test/num_examples': 3003, 'score': 13468.265769004822, 'total_duration': 22642.391790390015, 'accumulated_submission_time': 13468.265769004822, 'accumulated_eval_time': 9172.487488031387, 'accumulated_logging_time': 0.4418942928314209, 'global_step': 38123, 'preemption_count': 0}), (40506, {'train/accuracy': 0.6558361649513245, 'train/loss': 1.8276304006576538, 'train/bleu': 32.338058333076155, 'validation/accuracy': 0.6708658337593079, 'validation/loss': 1.7183750867843628, 'validation/bleu': 29.019836053244, 'validation/num_examples': 3000, 'test/accuracy': 0.6837139129638672, 'test/loss': 1.6359859704971313, 'test/bleu': 28.80755072318091, 'test/num_examples': 3003, 'score': 14308.177471637726, 'total_duration': 23999.80347084999, 'accumulated_submission_time': 14308.177471637726, 'accumulated_eval_time': 9689.880198001862, 'accumulated_logging_time': 0.47484350204467773, 'global_step': 40506, 'preemption_count': 0}), (42889, {'train/accuracy': 0.32560446858406067, 'train/loss': 4.233160018920898, 'train/bleu': 0.1383137865844718, 'validation/accuracy': 0.3041127920150757, 'validation/loss': 4.484804153442383, 'validation/bleu': 0.06474557629339085, 'validation/num_examples': 3000, 'test/accuracy': 0.3018534779548645, 'test/loss': 4.571069240570068, 'test/bleu': 0.033897742842188085, 'test/num_examples': 3003, 'score': 15148.184902191162, 'total_duration': 25539.770740509033, 'accumulated_submission_time': 15148.184902191162, 'accumulated_eval_time': 10389.728868246078, 'accumulated_logging_time': 0.51078200340271, 'global_step': 42889, 'preemption_count': 0}), (45272, {'train/accuracy': 0.6611015796661377, 'train/loss': 1.7925008535385132, 'train/bleu': 32.56452484565516, 'validation/accuracy': 0.6752551198005676, 'validation/loss': 1.6979889869689941, 'validation/bleu': 29.620418156833455, 'validation/num_examples': 3000, 'test/accuracy': 0.6868398189544678, 'test/loss': 1.6206845045089722, 'test/bleu': 28.890927730804112, 'test/num_examples': 3003, 'score': 15988.277658700943, 'total_duration': 26847.600742578506, 'accumulated_submission_time': 15988.277658700943, 'accumulated_eval_time': 10857.352401733398, 'accumulated_logging_time': 0.5476312637329102, 'global_step': 45272, 'preemption_count': 0}), (47656, {'train/accuracy': 0.6571767330169678, 'train/loss': 1.834312081336975, 'train/bleu': 32.09281506764354, 'validation/accuracy': 0.6745359301567078, 'validation/loss': 1.7079156637191772, 'validation/bleu': 29.54546456908649, 'validation/num_examples': 3000, 'test/accuracy': 0.6875022053718567, 'test/loss': 1.6302539110183716, 'test/bleu': 29.12670142648821, 'test/num_examples': 3003, 'score': 16828.508974790573, 'total_duration': 28198.55082011223, 'accumulated_submission_time': 16828.508974790573, 'accumulated_eval_time': 11367.964815616608, 'accumulated_logging_time': 0.5800197124481201, 'global_step': 47656, 'preemption_count': 0}), (50039, {'train/accuracy': 0.70138019323349, 'train/loss': 1.5984097719192505, 'train/bleu': 35.59088268760344, 'validation/accuracy': 0.6770529747009277, 'validation/loss': 1.7046748399734497, 'validation/bleu': 29.584613023929922, 'validation/num_examples': 3000, 'test/accuracy': 0.6908721327781677, 'test/loss': 1.6230562925338745, 'test/bleu': 29.203659503289817, 'test/num_examples': 3003, 'score': 17668.53111076355, 'total_duration': 29614.12636780739, 'accumulated_submission_time': 17668.53111076355, 'accumulated_eval_time': 11943.41303062439, 'accumulated_logging_time': 0.6114578247070312, 'global_step': 50039, 'preemption_count': 0}), (52421, {'train/accuracy': 0.6642529368400574, 'train/loss': 1.7819786071777344, 'train/bleu': 32.94824790704417, 'validation/accuracy': 0.6771645545959473, 'validation/loss': 1.690804123878479, 'validation/bleu': 29.778122229123387, 'validation/num_examples': 3000, 'test/accuracy': 0.6908256411552429, 'test/loss': 1.6125904321670532, 'test/bleu': 29.400656364664417, 'test/num_examples': 3003, 'score': 18508.571378707886, 'total_duration': 30962.36435317993, 'accumulated_submission_time': 18508.571378707886, 'accumulated_eval_time': 12451.49473619461, 'accumulated_logging_time': 0.6497724056243896, 'global_step': 52421, 'preemption_count': 0}), (54804, {'train/accuracy': 0.6630371809005737, 'train/loss': 1.7826213836669922, 'train/bleu': 32.97631954929356, 'validation/accuracy': 0.6774993538856506, 'validation/loss': 1.6878796815872192, 'validation/bleu': 29.82505230190574, 'validation/num_examples': 3000, 'test/accuracy': 0.6921155452728271, 'test/loss': 1.6047054529190063, 'test/bleu': 29.701937568525977, 'test/num_examples': 3003, 'score': 19348.670289993286, 'total_duration': 32287.476562976837, 'accumulated_submission_time': 19348.670289993286, 'accumulated_eval_time': 12936.39828658104, 'accumulated_logging_time': 0.6831979751586914, 'global_step': 54804, 'preemption_count': 0}), (57187, {'train/accuracy': 0.677360475063324, 'train/loss': 1.7102564573287964, 'train/bleu': 33.62485481934153, 'validation/accuracy': 0.6791856288909912, 'validation/loss': 1.6819928884506226, 'validation/bleu': 29.688366128750683, 'validation/num_examples': 3000, 'test/accuracy': 0.6951368451118469, 'test/loss': 1.5948320627212524, 'test/bleu': 29.96182087882542, 'test/num_examples': 3003, 'score': 20188.675621509552, 'total_duration': 33608.99349451065, 'accumulated_submission_time': 20188.675621509552, 'accumulated_eval_time': 13417.802710533142, 'accumulated_logging_time': 0.7159018516540527, 'global_step': 57187, 'preemption_count': 0}), (59571, {'train/accuracy': 0.665337324142456, 'train/loss': 1.7709975242614746, 'train/bleu': 32.771778877290295, 'validation/accuracy': 0.6795576214790344, 'validation/loss': 1.667377233505249, 'validation/bleu': 29.518673328869195, 'validation/num_examples': 3000, 'test/accuracy': 0.6934635043144226, 'test/loss': 1.5827269554138184, 'test/bleu': 29.45679735869189, 'test/num_examples': 3003, 'score': 21028.83904337883, 'total_duration': 35029.61338472366, 'accumulated_submission_time': 21028.83904337883, 'accumulated_eval_time': 13998.151756286621, 'accumulated_logging_time': 0.7499048709869385, 'global_step': 59571, 'preemption_count': 0}), (61955, {'train/accuracy': 0.6696307063102722, 'train/loss': 1.765308141708374, 'train/bleu': 33.5078406006622, 'validation/accuracy': 0.6800783276557922, 'validation/loss': 1.670266032218933, 'validation/bleu': 29.881170707042948, 'validation/num_examples': 3000, 'test/accuracy': 0.6972982287406921, 'test/loss': 1.5744260549545288, 'test/bleu': 30.065476502372746, 'test/num_examples': 3003, 'score': 21869.08454298973, 'total_duration': 36379.561566352844, 'accumulated_submission_time': 21869.08454298973, 'accumulated_eval_time': 14507.747369527817, 'accumulated_logging_time': 0.7831311225891113, 'global_step': 61955, 'preemption_count': 0}), (64338, {'train/accuracy': 0.6764718890190125, 'train/loss': 1.7071046829223633, 'train/bleu': 33.57440543575773, 'validation/accuracy': 0.680251955986023, 'validation/loss': 1.6660431623458862, 'validation/bleu': 29.700324861556748, 'validation/num_examples': 3000, 'test/accuracy': 0.6957643628120422, 'test/loss': 1.577563762664795, 'test/bleu': 29.864767151801697, 'test/num_examples': 3003, 'score': 22709.00170826912, 'total_duration': 37693.27958345413, 'accumulated_submission_time': 22709.00170826912, 'accumulated_eval_time': 14981.440528154373, 'accumulated_logging_time': 0.8170902729034424, 'global_step': 64338, 'preemption_count': 0}), (66721, {'train/accuracy': 0.6704260110855103, 'train/loss': 1.7423919439315796, 'train/bleu': 33.43165042153991, 'validation/accuracy': 0.6816902160644531, 'validation/loss': 1.6634081602096558, 'validation/bleu': 29.84663557839197, 'validation/num_examples': 3000, 'test/accuracy': 0.6970425844192505, 'test/loss': 1.5691356658935547, 'test/bleu': 30.122066124861863, 'test/num_examples': 3003, 'score': 23549.12446165085, 'total_duration': 39041.21701860428, 'accumulated_submission_time': 23549.12446165085, 'accumulated_eval_time': 15489.14245057106, 'accumulated_logging_time': 0.854212760925293, 'global_step': 66721, 'preemption_count': 0}), (69104, {'train/accuracy': 0.6957202553749084, 'train/loss': 1.5890588760375977, 'train/bleu': 35.166386173569414, 'validation/accuracy': 0.682223379611969, 'validation/loss': 1.6552072763442993, 'validation/bleu': 29.95974849835134, 'validation/num_examples': 3000, 'test/accuracy': 0.6975655555725098, 'test/loss': 1.562856674194336, 'test/bleu': 29.880227745785337, 'test/num_examples': 3003, 'score': 24389.310983896255, 'total_duration': 40385.25058054924, 'accumulated_submission_time': 24389.310983896255, 'accumulated_eval_time': 15992.877562999725, 'accumulated_logging_time': 0.8907725811004639, 'global_step': 69104, 'preemption_count': 0}), (71487, {'train/accuracy': 0.676636815071106, 'train/loss': 1.7049415111541748, 'train/bleu': 34.26073413751061, 'validation/accuracy': 0.6833145022392273, 'validation/loss': 1.646494746208191, 'validation/bleu': 30.023471272179897, 'validation/num_examples': 3000, 'test/accuracy': 0.6996223330497742, 'test/loss': 1.5564168691635132, 'test/bleu': 30.269282466335337, 'test/num_examples': 3003, 'score': 25229.351717948914, 'total_duration': 41740.079825639725, 'accumulated_submission_time': 25229.351717948914, 'accumulated_eval_time': 16507.55722308159, 'accumulated_logging_time': 0.925260066986084, 'global_step': 71487, 'preemption_count': 0}), (73871, {'train/accuracy': 0.6750386357307434, 'train/loss': 1.7208690643310547, 'train/bleu': 34.451335996141516, 'validation/accuracy': 0.6839592456817627, 'validation/loss': 1.6430405378341675, 'validation/bleu': 30.166010042652832, 'validation/num_examples': 3000, 'test/accuracy': 0.6999593377113342, 'test/loss': 1.5452179908752441, 'test/bleu': 30.42530986159958, 'test/num_examples': 3003, 'score': 26069.48642897606, 'total_duration': 43157.425143003464, 'accumulated_submission_time': 26069.48642897606, 'accumulated_eval_time': 17084.658698558807, 'accumulated_logging_time': 0.9607217311859131, 'global_step': 73871, 'preemption_count': 0}), (76254, {'train/accuracy': 0.6888977885246277, 'train/loss': 1.6275510787963867, 'train/bleu': 35.07580034747715, 'validation/accuracy': 0.6844180226325989, 'validation/loss': 1.6386370658874512, 'validation/bleu': 29.928076607129135, 'validation/num_examples': 3000, 'test/accuracy': 0.701655924320221, 'test/loss': 1.541452407836914, 'test/bleu': 30.24334076025487, 'test/num_examples': 3003, 'score': 26909.4674077034, 'total_duration': 44570.950585365295, 'accumulated_submission_time': 26909.4674077034, 'accumulated_eval_time': 17658.08572268486, 'accumulated_logging_time': 1.0019049644470215, 'global_step': 76254, 'preemption_count': 0}), (78637, {'train/accuracy': 0.6790190935134888, 'train/loss': 1.6930485963821411, 'train/bleu': 34.606128151781625, 'validation/accuracy': 0.6853355765342712, 'validation/loss': 1.6412664651870728, 'validation/bleu': 30.084906994619733, 'validation/num_examples': 3000, 'test/accuracy': 0.7021439671516418, 'test/loss': 1.543318271636963, 'test/bleu': 30.256775334023622, 'test/num_examples': 3003, 'score': 27749.411451101303, 'total_duration': 45916.00671863556, 'accumulated_submission_time': 27749.411451101303, 'accumulated_eval_time': 18163.086530447006, 'accumulated_logging_time': 1.0375986099243164, 'global_step': 78637, 'preemption_count': 0}), (81021, {'train/accuracy': 0.68350750207901, 'train/loss': 1.6653733253479004, 'train/bleu': 33.94700163998571, 'validation/accuracy': 0.6868730783462524, 'validation/loss': 1.628083348274231, 'validation/bleu': 30.37649273654434, 'validation/num_examples': 3000, 'test/accuracy': 0.7027831077575684, 'test/loss': 1.532280445098877, 'test/bleu': 30.454600655162768, 'test/num_examples': 3003, 'score': 28589.642166614532, 'total_duration': 47232.37526059151, 'accumulated_submission_time': 28589.642166614532, 'accumulated_eval_time': 18639.112541913986, 'accumulated_logging_time': 1.0760555267333984, 'global_step': 81021, 'preemption_count': 0}), (83404, {'train/accuracy': 0.689186692237854, 'train/loss': 1.628559947013855, 'train/bleu': 34.95898421051862, 'validation/accuracy': 0.6865258812904358, 'validation/loss': 1.6255191564559937, 'validation/bleu': 29.923109223085948, 'validation/num_examples': 3000, 'test/accuracy': 0.7037476301193237, 'test/loss': 1.527652382850647, 'test/bleu': 30.497443854719528, 'test/num_examples': 3003, 'score': 29429.755152463913, 'total_duration': 48569.43083524704, 'accumulated_submission_time': 29429.755152463913, 'accumulated_eval_time': 19135.941576480865, 'accumulated_logging_time': 1.1130588054656982, 'global_step': 83404, 'preemption_count': 0}), (85787, {'train/accuracy': 0.6846340894699097, 'train/loss': 1.6518151760101318, 'train/bleu': 34.29854509964452, 'validation/accuracy': 0.6845916509628296, 'validation/loss': 1.627881407737732, 'validation/bleu': 29.913907501321336, 'validation/num_examples': 3000, 'test/accuracy': 0.7035732865333557, 'test/loss': 1.526257038116455, 'test/bleu': 30.163936574095636, 'test/num_examples': 3003, 'score': 30269.69832634926, 'total_duration': 50055.488450050354, 'accumulated_submission_time': 30269.69832634926, 'accumulated_eval_time': 19781.944012403488, 'accumulated_logging_time': 1.1491875648498535, 'global_step': 85787, 'preemption_count': 0}), (88170, {'train/accuracy': 0.7073625326156616, 'train/loss': 1.5367120504379272, 'train/bleu': 36.36983182542021, 'validation/accuracy': 0.6875426173210144, 'validation/loss': 1.6262383460998535, 'validation/bleu': 30.037688410889466, 'validation/num_examples': 3000, 'test/accuracy': 0.7039335370063782, 'test/loss': 1.526638388633728, 'test/bleu': 30.32476310275814, 'test/num_examples': 3003, 'score': 31109.80075287819, 'total_duration': 51547.07810497284, 'accumulated_submission_time': 31109.80075287819, 'accumulated_eval_time': 20433.317983865738, 'accumulated_logging_time': 1.1875977516174316, 'global_step': 88170, 'preemption_count': 0}), (90553, {'train/accuracy': 0.6886942386627197, 'train/loss': 1.6172972917556763, 'train/bleu': 34.94074502185821, 'validation/accuracy': 0.6868730783462524, 'validation/loss': 1.6169028282165527, 'validation/bleu': 29.63990232421176, 'validation/num_examples': 3000, 'test/accuracy': 0.7055139541625977, 'test/loss': 1.5144983530044556, 'test/bleu': 30.369156672036247, 'test/num_examples': 3003, 'score': 31949.730099201202, 'total_duration': 53018.90501999855, 'accumulated_submission_time': 31949.730099201202, 'accumulated_eval_time': 21065.105221271515, 'accumulated_logging_time': 1.225377082824707, 'global_step': 90553, 'preemption_count': 0}), (92936, {'train/accuracy': 0.6906367540359497, 'train/loss': 1.6220505237579346, 'train/bleu': 35.15019654432984, 'validation/accuracy': 0.6880013942718506, 'validation/loss': 1.6124529838562012, 'validation/bleu': 30.125960524935937, 'validation/num_examples': 3000, 'test/accuracy': 0.7075939774513245, 'test/loss': 1.5099539756774902, 'test/bleu': 30.907346055474644, 'test/num_examples': 3003, 'score': 32789.64198088646, 'total_duration': 54346.94282460213, 'accumulated_submission_time': 32789.64198088646, 'accumulated_eval_time': 21553.11935710907, 'accumulated_logging_time': 1.2633757591247559, 'global_step': 92936, 'preemption_count': 0}), (95319, {'train/accuracy': 0.7033336758613586, 'train/loss': 1.5496033430099487, 'train/bleu': 35.7991515308961, 'validation/accuracy': 0.6894024610519409, 'validation/loss': 1.6060791015625, 'validation/bleu': 30.590930186553226, 'validation/num_examples': 3000, 'test/accuracy': 0.7068386673927307, 'test/loss': 1.5075197219848633, 'test/bleu': 30.556849668080478, 'test/num_examples': 3003, 'score': 33629.78120470047, 'total_duration': 55705.952450037, 'accumulated_submission_time': 33629.78120470047, 'accumulated_eval_time': 22071.868659973145, 'accumulated_logging_time': 1.3095154762268066, 'global_step': 95319, 'preemption_count': 0}), (97702, {'train/accuracy': 0.6969262957572937, 'train/loss': 1.5804659128189087, 'train/bleu': 35.152333339015506, 'validation/accuracy': 0.6876417994499207, 'validation/loss': 1.6106321811676025, 'validation/bleu': 30.332051794275422, 'validation/num_examples': 3000, 'test/accuracy': 0.7058508992195129, 'test/loss': 1.5085893869400024, 'test/bleu': 30.394577530924465, 'test/num_examples': 3003, 'score': 34469.73361515999, 'total_duration': 57028.49427843094, 'accumulated_submission_time': 34469.73361515999, 'accumulated_eval_time': 22554.345523118973, 'accumulated_logging_time': 1.3491921424865723, 'global_step': 97702, 'preemption_count': 0}), (100084, {'train/accuracy': 0.7314824461936951, 'train/loss': 1.4231904745101929, 'train/bleu': 38.338865093238454, 'validation/accuracy': 0.6890305280685425, 'validation/loss': 1.6068017482757568, 'validation/bleu': 30.711764528611326, 'validation/num_examples': 3000, 'test/accuracy': 0.7073267102241516, 'test/loss': 1.5063436031341553, 'test/bleu': 30.535233603110655, 'test/num_examples': 3003, 'score': 35309.84344100952, 'total_duration': 58361.5737016201, 'accumulated_submission_time': 35309.84344100952, 'accumulated_eval_time': 23047.190549373627, 'accumulated_logging_time': 1.3953723907470703, 'global_step': 100084, 'preemption_count': 0}), (102466, {'train/accuracy': 0.7023757696151733, 'train/loss': 1.553908348083496, 'train/bleu': 36.134819561514, 'validation/accuracy': 0.6896008849143982, 'validation/loss': 1.6050572395324707, 'validation/bleu': 30.46585574558103, 'validation/num_examples': 3000, 'test/accuracy': 0.7066527605056763, 'test/loss': 1.5068280696868896, 'test/bleu': 30.829963453250123, 'test/num_examples': 3003, 'score': 36149.92675304413, 'total_duration': 59711.75960898399, 'accumulated_submission_time': 36149.92675304413, 'accumulated_eval_time': 23557.168686389923, 'accumulated_logging_time': 1.442002773284912, 'global_step': 102466, 'preemption_count': 0}), (104849, {'train/accuracy': 0.6993989944458008, 'train/loss': 1.5596814155578613, 'train/bleu': 35.754191956860694, 'validation/accuracy': 0.6894644498825073, 'validation/loss': 1.6042819023132324, 'validation/bleu': 30.594905174890503, 'validation/num_examples': 3000, 'test/accuracy': 0.7081517577171326, 'test/loss': 1.4993467330932617, 'test/bleu': 30.92962167513607, 'test/num_examples': 3003, 'score': 36989.896939754486, 'total_duration': 61037.63996696472, 'accumulated_submission_time': 36989.896939754486, 'accumulated_eval_time': 24042.966230154037, 'accumulated_logging_time': 1.4812438488006592, 'global_step': 104849, 'preemption_count': 0}), (107233, {'train/accuracy': 0.7179781794548035, 'train/loss': 1.4720852375030518, 'train/bleu': 37.12911174176097, 'validation/accuracy': 0.6896132826805115, 'validation/loss': 1.6041953563690186, 'validation/bleu': 30.57522490142288, 'validation/num_examples': 3000, 'test/accuracy': 0.7072221636772156, 'test/loss': 1.5013386011123657, 'test/bleu': 30.69685450178556, 'test/num_examples': 3003, 'score': 37830.11774921417, 'total_duration': 62362.86051940918, 'accumulated_submission_time': 37830.11774921417, 'accumulated_eval_time': 24527.8506731987, 'accumulated_logging_time': 1.5237393379211426, 'global_step': 107233, 'preemption_count': 0}), (109615, {'train/accuracy': 0.710374653339386, 'train/loss': 1.5148351192474365, 'train/bleu': 36.08658871891254, 'validation/accuracy': 0.690220832824707, 'validation/loss': 1.601181983947754, 'validation/bleu': 30.252796426395733, 'validation/num_examples': 3000, 'test/accuracy': 0.7097089290618896, 'test/loss': 1.493978500366211, 'test/bleu': 30.87070303183882, 'test/num_examples': 3003, 'score': 38670.0972571373, 'total_duration': 63713.02125096321, 'accumulated_submission_time': 38670.0972571373, 'accumulated_eval_time': 25037.91539287567, 'accumulated_logging_time': 1.563863754272461, 'global_step': 109615, 'preemption_count': 0}), (111997, {'train/accuracy': 0.7104139924049377, 'train/loss': 1.5093110799789429, 'train/bleu': 36.71417614009322, 'validation/accuracy': 0.6904935836791992, 'validation/loss': 1.6021604537963867, 'validation/bleu': 30.746368302220958, 'validation/num_examples': 3000, 'test/accuracy': 0.7084655165672302, 'test/loss': 1.4979416131973267, 'test/bleu': 30.64387931009616, 'test/num_examples': 3003, 'score': 39510.233573913574, 'total_duration': 65037.137236356735, 'accumulated_submission_time': 39510.233573913574, 'accumulated_eval_time': 25521.773129224777, 'accumulated_logging_time': 1.6069166660308838, 'global_step': 111997, 'preemption_count': 0}), (114380, {'train/accuracy': 0.7173652052879333, 'train/loss': 1.4794282913208008, 'train/bleu': 37.131145115956905, 'validation/accuracy': 0.6896628737449646, 'validation/loss': 1.6034998893737793, 'validation/bleu': 30.397301082654415, 'validation/num_examples': 3000, 'test/accuracy': 0.7098019123077393, 'test/loss': 1.4975818395614624, 'test/bleu': 30.778936015094025, 'test/num_examples': 3003, 'score': 40350.37732386589, 'total_duration': 66362.9657073021, 'accumulated_submission_time': 40350.37732386589, 'accumulated_eval_time': 26007.337995052338, 'accumulated_logging_time': 1.6503000259399414, 'global_step': 114380, 'preemption_count': 0}), (116764, {'train/accuracy': 0.7147499918937683, 'train/loss': 1.4900606870651245, 'train/bleu': 36.79285558146597, 'validation/accuracy': 0.6898736357688904, 'validation/loss': 1.601643681526184, 'validation/bleu': 30.654046685474967, 'validation/num_examples': 3000, 'test/accuracy': 0.7097205519676208, 'test/loss': 1.494398832321167, 'test/bleu': 30.84845509744524, 'test/num_examples': 3003, 'score': 41190.49334859848, 'total_duration': 67689.97535419464, 'accumulated_submission_time': 41190.49334859848, 'accumulated_eval_time': 26494.117411851883, 'accumulated_logging_time': 1.6911771297454834, 'global_step': 116764, 'preemption_count': 0}), (119147, {'train/accuracy': 0.7253099083900452, 'train/loss': 1.4400979280471802, 'train/bleu': 38.21547448718535, 'validation/accuracy': 0.6901092529296875, 'validation/loss': 1.599493384361267, 'validation/bleu': 30.736349330402373, 'validation/num_examples': 3000, 'test/accuracy': 0.709674060344696, 'test/loss': 1.493552803993225, 'test/bleu': 30.740337151689708, 'test/num_examples': 3003, 'score': 42030.68079471588, 'total_duration': 69001.66783833504, 'accumulated_submission_time': 42030.68079471588, 'accumulated_eval_time': 26965.506422758102, 'accumulated_logging_time': 1.7330989837646484, 'global_step': 119147, 'preemption_count': 0}), (121529, {'train/accuracy': 0.7211779356002808, 'train/loss': 1.4619131088256836, 'train/bleu': 37.837886173614265, 'validation/accuracy': 0.6910391449928284, 'validation/loss': 1.5997225046157837, 'validation/bleu': 30.736317737646832, 'validation/num_examples': 3000, 'test/accuracy': 0.7105339765548706, 'test/loss': 1.4927688837051392, 'test/bleu': 30.733517509111195, 'test/num_examples': 3003, 'score': 42870.594207286835, 'total_duration': 70363.89932894707, 'accumulated_submission_time': 42870.594207286835, 'accumulated_eval_time': 27487.700913906097, 'accumulated_logging_time': 1.7821390628814697, 'global_step': 121529, 'preemption_count': 0}), (123912, {'train/accuracy': 0.7188615202903748, 'train/loss': 1.4707727432250977, 'train/bleu': 37.37068803202791, 'validation/accuracy': 0.6907415986061096, 'validation/loss': 1.5994369983673096, 'validation/bleu': 30.730796295851167, 'validation/num_examples': 3000, 'test/accuracy': 0.7107663750648499, 'test/loss': 1.4905011653900146, 'test/bleu': 30.858893171024814, 'test/num_examples': 3003, 'score': 43710.785326480865, 'total_duration': 71700.7684583664, 'accumulated_submission_time': 43710.785326480865, 'accumulated_eval_time': 27984.25898528099, 'accumulated_logging_time': 1.8253540992736816, 'global_step': 123912, 'preemption_count': 0}), (126295, {'train/accuracy': 0.7226437330245972, 'train/loss': 1.4515371322631836, 'train/bleu': 38.29304533888482, 'validation/accuracy': 0.6909151673316956, 'validation/loss': 1.5993297100067139, 'validation/bleu': 30.735822581848513, 'validation/num_examples': 3000, 'test/accuracy': 0.7114520072937012, 'test/loss': 1.4903055429458618, 'test/bleu': 30.760997036011428, 'test/num_examples': 3003, 'score': 44550.9940226078, 'total_duration': 73051.0552790165, 'accumulated_submission_time': 44550.9940226078, 'accumulated_eval_time': 28494.218544483185, 'accumulated_logging_time': 1.8677458763122559, 'global_step': 126295, 'preemption_count': 0}), (128677, {'train/accuracy': 0.7253398895263672, 'train/loss': 1.437965750694275, 'train/bleu': 38.15674927738666, 'validation/accuracy': 0.6909027695655823, 'validation/loss': 1.5986698865890503, 'validation/bleu': 30.539796950932832, 'validation/num_examples': 3000, 'test/accuracy': 0.7110103964805603, 'test/loss': 1.4899711608886719, 'test/bleu': 30.711279312995767, 'test/num_examples': 3003, 'score': 45391.09645104408, 'total_duration': 74407.53640341759, 'accumulated_submission_time': 45391.09645104408, 'accumulated_eval_time': 29010.47008228302, 'accumulated_logging_time': 1.9190173149108887, 'global_step': 128677, 'preemption_count': 0}), (131059, {'train/accuracy': 0.724202036857605, 'train/loss': 1.443503499031067, 'train/bleu': 38.04270301127323, 'validation/accuracy': 0.6908655762672424, 'validation/loss': 1.599054217338562, 'validation/bleu': 30.667733220414544, 'validation/num_examples': 3000, 'test/accuracy': 0.7107431292533875, 'test/loss': 1.490159273147583, 'test/bleu': 30.77321518935283, 'test/num_examples': 3003, 'score': 46231.218935251236, 'total_duration': 75766.89620161057, 'accumulated_submission_time': 46231.218935251236, 'accumulated_eval_time': 29529.583674669266, 'accumulated_logging_time': 1.9653499126434326, 'global_step': 131059, 'preemption_count': 0}), (133333, {'train/accuracy': 0.7267444133758545, 'train/loss': 1.427182912826538, 'train/bleu': 37.58713677051531, 'validation/accuracy': 0.6907787919044495, 'validation/loss': 1.5991333723068237, 'validation/bleu': 30.66343634971797, 'validation/num_examples': 3000, 'test/accuracy': 0.7107315063476562, 'test/loss': 1.4902734756469727, 'test/bleu': 30.821354274030078, 'test/num_examples': 3003, 'score': 47033.39378809929, 'total_duration': 77092.4309284687, 'accumulated_submission_time': 47033.39378809929, 'accumulated_eval_time': 30052.825205802917, 'accumulated_logging_time': 2.0094943046569824, 'global_step': 133333, 'preemption_count': 0})], 'global_step': 133333}
I0208 06:12:20.151176 140699726837568 submission_runner.py:586] Timing: 47033.39378809929
I0208 06:12:20.151239 140699726837568 submission_runner.py:588] Total number of evals: 57
I0208 06:12:20.151289 140699726837568 submission_runner.py:589] ====================
I0208 06:12:20.151341 140699726837568 submission_runner.py:542] Using RNG seed 529981238
I0208 06:12:20.152877 140699726837568 submission_runner.py:551] --- Tuning run 3/5 ---
I0208 06:12:20.152976 140699726837568 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_1/wmt_jax/trial_3.
I0208 06:12:20.153408 140699726837568 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_1/wmt_jax/trial_3/hparams.json.
I0208 06:12:20.154201 140699726837568 submission_runner.py:206] Initializing dataset.
I0208 06:12:20.157073 140699726837568 dataset_info.py:578] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0208 06:12:20.160158 140699726837568 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0208 06:12:20.198151 140699726837568 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0208 06:12:20.833514 140699726837568 submission_runner.py:213] Initializing model.
I0208 06:12:27.461600 140699726837568 submission_runner.py:255] Initializing optimizer.
I0208 06:12:28.266707 140699726837568 submission_runner.py:262] Initializing metrics bundle.
I0208 06:12:28.266861 140699726837568 submission_runner.py:280] Initializing checkpoint and logger.
I0208 06:12:28.267709 140699726837568 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_1/wmt_jax/trial_3 with prefix checkpoint_
I0208 06:12:28.267827 140699726837568 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_1/wmt_jax/trial_3/meta_data_0.json.
I0208 06:12:28.268027 140699726837568 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0208 06:12:28.268087 140699726837568 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0208 06:12:28.811154 140699726837568 logger_utils.py:220] Unable to record git information. Continuing without it.
I0208 06:12:29.320844 140699726837568 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_1/wmt_jax/trial_3/flags_0.json.
I0208 06:12:29.324477 140699726837568 submission_runner.py:314] Starting training loop.
I0208 06:12:56.194317 140529910601472 logging_writer.py:48] [0] global_step=0, grad_norm=6.158485412597656, loss=11.173095703125
I0208 06:12:56.206517 140699726837568 spec.py:321] Evaluating on the training split.
I0208 06:12:58.932598 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 06:17:49.016928 140699726837568 spec.py:333] Evaluating on the validation split.
I0208 06:17:51.770848 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 06:22:42.542244 140699726837568 spec.py:349] Evaluating on the test split.
I0208 06:22:45.294656 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 06:27:34.577014 140699726837568 submission_runner.py:408] Time since start: 905.25s, 	Step: 1, 	{'train/accuracy': 0.0005962961004115641, 'train/loss': 11.175597190856934, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.208685874938965, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.19086742401123, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 26.882007360458374, 'total_duration': 905.2524693012238, 'accumulated_submission_time': 26.882007360458374, 'accumulated_eval_time': 878.3704223632812, 'accumulated_logging_time': 0}
I0208 06:27:34.586009 140529918994176 logging_writer.py:48] [1] accumulated_eval_time=878.370422, accumulated_logging_time=0, accumulated_submission_time=26.882007, global_step=1, preemption_count=0, score=26.882007, test/accuracy=0.000709, test/bleu=0.000000, test/loss=11.190867, test/num_examples=3003, total_duration=905.252469, train/accuracy=0.000596, train/bleu=0.000000, train/loss=11.175597, validation/accuracy=0.000484, validation/bleu=0.000000, validation/loss=11.208686, validation/num_examples=3000
I0208 06:28:09.812434 140529910601472 logging_writer.py:48] [100] global_step=100, grad_norm=0.48677241802215576, loss=8.652958869934082
I0208 06:28:45.053078 140529918994176 logging_writer.py:48] [200] global_step=200, grad_norm=0.17784808576107025, loss=8.26159381866455
I0208 06:29:20.342638 140529910601472 logging_writer.py:48] [300] global_step=300, grad_norm=0.1911654770374298, loss=8.004620552062988
I0208 06:29:55.703257 140529918994176 logging_writer.py:48] [400] global_step=400, grad_norm=0.32695454359054565, loss=7.603524208068848
I0208 06:30:31.009328 140529910601472 logging_writer.py:48] [500] global_step=500, grad_norm=0.429352343082428, loss=7.222700119018555
I0208 06:31:06.319521 140529918994176 logging_writer.py:48] [600] global_step=600, grad_norm=0.86744225025177, loss=6.959226131439209
I0208 06:31:41.641150 140529910601472 logging_writer.py:48] [700] global_step=700, grad_norm=0.7864968180656433, loss=6.734180450439453
I0208 06:32:17.010317 140529918994176 logging_writer.py:48] [800] global_step=800, grad_norm=0.5715757608413696, loss=6.4225172996521
I0208 06:32:52.387070 140529910601472 logging_writer.py:48] [900] global_step=900, grad_norm=0.5877580046653748, loss=6.28732967376709
I0208 06:33:27.725709 140529918994176 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.5786173939704895, loss=5.9914679527282715
I0208 06:34:03.066497 140529910601472 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.6468769311904907, loss=5.755326271057129
I0208 06:34:38.401507 140529918994176 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.8221134543418884, loss=5.595517158508301
I0208 06:35:13.740579 140529910601472 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.6952406764030457, loss=5.433115482330322
I0208 06:35:49.108432 140529918994176 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.7274879217147827, loss=5.247682571411133
I0208 06:36:24.463689 140529910601472 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.7806932330131531, loss=5.079120635986328
I0208 06:36:59.816448 140529918994176 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.6949336528778076, loss=4.917863845825195
I0208 06:37:35.163067 140529910601472 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.7181931734085083, loss=4.7420973777771
I0208 06:38:10.516028 140529918994176 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.8892420530319214, loss=4.680059432983398
I0208 06:38:45.902709 140529910601472 logging_writer.py:48] [1900] global_step=1900, grad_norm=1.1575703620910645, loss=4.5513386726379395
I0208 06:39:21.273948 140529918994176 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.8849506974220276, loss=4.524774551391602
I0208 06:39:56.607587 140529910601472 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.8796679377555847, loss=4.320693492889404
I0208 06:40:31.947744 140529918994176 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.6507337689399719, loss=4.187289237976074
I0208 06:41:07.301904 140529910601472 logging_writer.py:48] [2300] global_step=2300, grad_norm=1.7135796546936035, loss=4.123805522918701
I0208 06:41:34.626065 140699726837568 spec.py:321] Evaluating on the training split.
I0208 06:41:37.666300 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 06:45:23.793047 140699726837568 spec.py:333] Evaluating on the validation split.
I0208 06:45:26.530426 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 06:49:29.820540 140699726837568 spec.py:349] Evaluating on the test split.
I0208 06:49:32.576244 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 06:53:40.538213 140699726837568 submission_runner.py:408] Time since start: 2471.21s, 	Step: 2379, 	{'train/accuracy': 0.4058598577976227, 'train/loss': 3.9908957481384277, 'train/bleu': 14.225651382899393, 'validation/accuracy': 0.3933243155479431, 'validation/loss': 4.097593307495117, 'validation/bleu': 9.435176775149337, 'validation/num_examples': 3000, 'test/accuracy': 0.375143826007843, 'test/loss': 4.313692092895508, 'test/bleu': 7.492265560875259, 'test/num_examples': 3003, 'score': 866.837769985199, 'total_duration': 2471.213627576828, 'accumulated_submission_time': 866.837769985199, 'accumulated_eval_time': 1604.2824800014496, 'accumulated_logging_time': 0.01880478858947754}
I0208 06:53:40.555207 140529918994176 logging_writer.py:48] [2379] accumulated_eval_time=1604.282480, accumulated_logging_time=0.018805, accumulated_submission_time=866.837770, global_step=2379, preemption_count=0, score=866.837770, test/accuracy=0.375144, test/bleu=7.492266, test/loss=4.313692, test/num_examples=3003, total_duration=2471.213628, train/accuracy=0.405860, train/bleu=14.225651, train/loss=3.990896, validation/accuracy=0.393324, validation/bleu=9.435177, validation/loss=4.097593, validation/num_examples=3000
I0208 06:53:48.332289 140529910601472 logging_writer.py:48] [2400] global_step=2400, grad_norm=1.178652048110962, loss=4.029186725616455
I0208 06:54:23.601015 140529918994176 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.155170202255249, loss=3.927626132965088
I0208 06:54:58.898543 140529910601472 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.8621888160705566, loss=3.7073447704315186
I0208 06:55:34.215817 140529918994176 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.7915192246437073, loss=3.6848323345184326
I0208 06:56:09.547419 140529910601472 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.8885874152183533, loss=3.5938186645507812
I0208 06:56:44.879917 140529918994176 logging_writer.py:48] [2900] global_step=2900, grad_norm=1.071272850036621, loss=3.5212390422821045
I0208 06:57:20.234674 140529910601472 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.9892736673355103, loss=3.5092945098876953
I0208 06:57:55.620850 140529918994176 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.8866808414459229, loss=3.3548667430877686
I0208 06:58:30.972974 140529910601472 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.8889516592025757, loss=3.280054807662964
I0208 06:59:06.309468 140529918994176 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.8547276258468628, loss=3.2627742290496826
I0208 06:59:41.689094 140529910601472 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.8212223052978516, loss=3.2239744663238525
I0208 07:00:17.035608 140529918994176 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.8437419533729553, loss=3.1363706588745117
I0208 07:00:52.406744 140529910601472 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.6616776585578918, loss=3.114938974380493
I0208 07:01:27.760741 140529918994176 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.7704966068267822, loss=2.994901657104492
I0208 07:02:03.138212 140529910601472 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.8157850503921509, loss=3.050072431564331
I0208 07:02:38.486465 140529918994176 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.7381186485290527, loss=2.995993137359619
I0208 07:03:13.851965 140529910601472 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.6958251595497131, loss=2.98417067527771
I0208 07:03:49.238301 140529918994176 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.7295884490013123, loss=2.8491647243499756
I0208 07:04:24.613459 140529910601472 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.6900942325592041, loss=2.8629415035247803
I0208 07:04:59.942854 140529918994176 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.6079789996147156, loss=2.7843515872955322
I0208 07:05:35.301534 140529910601472 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.7000570297241211, loss=2.8010337352752686
I0208 07:06:10.690707 140529918994176 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.7986032366752625, loss=2.7178938388824463
I0208 07:06:46.023710 140529910601472 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.8457115888595581, loss=2.7669427394866943
I0208 07:07:21.366027 140529918994176 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.6663500666618347, loss=2.5736806392669678
I0208 07:07:40.877538 140699726837568 spec.py:321] Evaluating on the training split.
I0208 07:07:43.923922 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 07:10:37.289948 140699726837568 spec.py:333] Evaluating on the validation split.
I0208 07:10:40.023594 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 07:13:17.179616 140699726837568 spec.py:349] Evaluating on the test split.
I0208 07:13:19.918168 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 07:15:45.501715 140699726837568 submission_runner.py:408] Time since start: 3796.18s, 	Step: 4757, 	{'train/accuracy': 0.5393306016921997, 'train/loss': 2.7030396461486816, 'train/bleu': 24.873358742540944, 'validation/accuracy': 0.542708694934845, 'validation/loss': 2.6487877368927, 'validation/bleu': 20.43187495336819, 'validation/num_examples': 3000, 'test/accuracy': 0.5409215092658997, 'test/loss': 2.6869993209838867, 'test/bleu': 18.88999064105362, 'test/num_examples': 3003, 'score': 1707.074345588684, 'total_duration': 3796.1771688461304, 'accumulated_submission_time': 1707.074345588684, 'accumulated_eval_time': 2088.90660238266, 'accumulated_logging_time': 0.047066450119018555}
I0208 07:15:45.516801 140529910601472 logging_writer.py:48] [4757] accumulated_eval_time=2088.906602, accumulated_logging_time=0.047066, accumulated_submission_time=1707.074346, global_step=4757, preemption_count=0, score=1707.074346, test/accuracy=0.540922, test/bleu=18.889991, test/loss=2.686999, test/num_examples=3003, total_duration=3796.177169, train/accuracy=0.539331, train/bleu=24.873359, train/loss=2.703040, validation/accuracy=0.542709, validation/bleu=20.431875, validation/loss=2.648788, validation/num_examples=3000
I0208 07:16:00.997949 140529918994176 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.6437941789627075, loss=2.6650753021240234
I0208 07:16:36.223005 140529910601472 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.5973995327949524, loss=2.659315586090088
I0208 07:17:11.508399 140529918994176 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.5878398418426514, loss=2.6397483348846436
I0208 07:17:46.807330 140529910601472 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.6208502650260925, loss=2.633085012435913
I0208 07:18:22.172732 140529918994176 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.8216440081596375, loss=2.6048147678375244
I0208 07:18:57.504860 140529910601472 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.6035893559455872, loss=2.532747507095337
I0208 07:19:32.819324 140529918994176 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.553496241569519, loss=2.541158437728882
I0208 07:20:08.175107 140529910601472 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.6050530672073364, loss=2.435638904571533
I0208 07:20:43.503724 140529918994176 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.6798263192176819, loss=2.537964344024658
I0208 07:21:18.839176 140529910601472 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.797444224357605, loss=2.4849390983581543
I0208 07:21:54.180253 140529918994176 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.5671144127845764, loss=2.544529914855957
I0208 07:22:29.509007 140529910601472 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.6738497614860535, loss=2.5022270679473877
I0208 07:23:04.847904 140529918994176 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.6123607754707336, loss=2.414762020111084
I0208 07:23:40.207968 140529910601472 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.5678585171699524, loss=2.4216513633728027
I0208 07:24:15.552284 140529918994176 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.5692446231842041, loss=2.477660655975342
I0208 07:24:50.878277 140529910601472 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.5912674069404602, loss=2.435576915740967
I0208 07:25:26.196349 140529918994176 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.5026434659957886, loss=2.453854560852051
I0208 07:26:01.503034 140529910601472 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.5225314497947693, loss=2.4028306007385254
I0208 07:26:36.888327 140529918994176 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.5277751684188843, loss=2.3001410961151123
I0208 07:27:12.210269 140529910601472 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.5349353551864624, loss=2.3111867904663086
I0208 07:27:47.520433 140529918994176 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.4999614357948303, loss=2.3951947689056396
I0208 07:28:22.857239 140529910601472 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.46280524134635925, loss=2.30996036529541
I0208 07:28:58.212343 140529918994176 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.46996647119522095, loss=2.383474588394165
I0208 07:29:33.516046 140529910601472 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.5685875415802002, loss=2.419473648071289
I0208 07:29:45.601505 140699726837568 spec.py:321] Evaluating on the training split.
I0208 07:29:48.628993 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 07:32:32.502844 140699726837568 spec.py:333] Evaluating on the validation split.
I0208 07:32:35.241057 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 07:35:08.779892 140699726837568 spec.py:349] Evaluating on the test split.
I0208 07:35:11.518413 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 07:37:36.726544 140699726837568 submission_runner.py:408] Time since start: 5107.40s, 	Step: 7136, 	{'train/accuracy': 0.5833343267440796, 'train/loss': 2.2597906589508057, 'train/bleu': 27.53477110939534, 'validation/accuracy': 0.5829437971115112, 'validation/loss': 2.237121105194092, 'validation/bleu': 23.01984843027956, 'validation/num_examples': 3000, 'test/accuracy': 0.586462140083313, 'test/loss': 2.2365055084228516, 'test/bleu': 22.075670983383866, 'test/num_examples': 3003, 'score': 2547.0738015174866, 'total_duration': 5107.401966333389, 'accumulated_submission_time': 2547.0738015174866, 'accumulated_eval_time': 2560.031564474106, 'accumulated_logging_time': 0.07350635528564453}
I0208 07:37:36.745514 140529918994176 logging_writer.py:48] [7136] accumulated_eval_time=2560.031564, accumulated_logging_time=0.073506, accumulated_submission_time=2547.073802, global_step=7136, preemption_count=0, score=2547.073802, test/accuracy=0.586462, test/bleu=22.075671, test/loss=2.236506, test/num_examples=3003, total_duration=5107.401966, train/accuracy=0.583334, train/bleu=27.534771, train/loss=2.259791, validation/accuracy=0.582944, validation/bleu=23.019848, validation/loss=2.237121, validation/num_examples=3000
I0208 07:37:59.645626 140529910601472 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.5286664962768555, loss=2.3378992080688477
I0208 07:38:34.851459 140529918994176 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.4581371545791626, loss=2.263740062713623
I0208 07:39:10.096315 140529910601472 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.426275372505188, loss=2.3088459968566895
I0208 07:39:45.369567 140529918994176 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.49456241726875305, loss=2.288630247116089
I0208 07:40:20.697046 140529910601472 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.5207268595695496, loss=2.3696658611297607
I0208 07:40:56.046707 140529918994176 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.6029446125030518, loss=2.2284436225891113
I0208 07:41:31.478629 140529910601472 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.4294608235359192, loss=2.1422789096832275
I0208 07:42:06.852035 140529918994176 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.465108722448349, loss=2.310271739959717
I0208 07:42:42.147544 140529910601472 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.5359766483306885, loss=2.242438793182373
I0208 07:43:17.451593 140529918994176 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.4577481150627136, loss=2.164783477783203
I0208 07:43:52.780601 140529910601472 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.4868272840976715, loss=2.1537976264953613
I0208 07:44:28.126593 140529918994176 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.5003183484077454, loss=2.3070950508117676
I0208 07:45:03.519641 140529910601472 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.5181768536567688, loss=2.2334160804748535
I0208 07:45:38.820565 140529918994176 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.3800937235355377, loss=2.182737112045288
I0208 07:46:14.118425 140529910601472 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.5281150937080383, loss=2.2427563667297363
I0208 07:46:49.407295 140529918994176 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.38760560750961304, loss=2.187864065170288
I0208 07:47:24.714214 140529910601472 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.39042937755584717, loss=2.159526824951172
I0208 07:48:00.031545 140529918994176 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.42974600195884705, loss=2.179645299911499
I0208 07:48:35.347754 140529910601472 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.40556904673576355, loss=2.238765239715576
I0208 07:49:10.626733 140529918994176 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.374147891998291, loss=2.19494891166687
I0208 07:49:45.922209 140529910601472 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.36412709951400757, loss=2.1944966316223145
I0208 07:50:21.223891 140529918994176 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.37198927998542786, loss=2.1178815364837646
I0208 07:50:56.497522 140529910601472 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.36169829964637756, loss=2.1621623039245605
I0208 07:51:31.805651 140529918994176 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.35213297605514526, loss=2.1253395080566406
I0208 07:51:36.832336 140699726837568 spec.py:321] Evaluating on the training split.
I0208 07:51:39.869066 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 07:54:23.635771 140699726837568 spec.py:333] Evaluating on the validation split.
I0208 07:54:26.383102 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 07:56:58.183196 140699726837568 spec.py:349] Evaluating on the test split.
I0208 07:57:00.922541 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 07:59:20.402437 140699726837568 submission_runner.py:408] Time since start: 6411.08s, 	Step: 9516, 	{'train/accuracy': 0.5903454422950745, 'train/loss': 2.161125421524048, 'train/bleu': 28.423563496088704, 'validation/accuracy': 0.60672527551651, 'validation/loss': 2.036038398742676, 'validation/bleu': 24.557212452962293, 'validation/num_examples': 3000, 'test/accuracy': 0.6093777418136597, 'test/loss': 2.014265537261963, 'test/bleu': 23.438955386735778, 'test/num_examples': 3003, 'score': 3387.072328567505, 'total_duration': 6411.077891111374, 'accumulated_submission_time': 3387.072328567505, 'accumulated_eval_time': 3023.601635694504, 'accumulated_logging_time': 0.10449838638305664}
I0208 07:59:20.418751 140529910601472 logging_writer.py:48] [9516] accumulated_eval_time=3023.601636, accumulated_logging_time=0.104498, accumulated_submission_time=3387.072329, global_step=9516, preemption_count=0, score=3387.072329, test/accuracy=0.609378, test/bleu=23.438955, test/loss=2.014266, test/num_examples=3003, total_duration=6411.077891, train/accuracy=0.590345, train/bleu=28.423563, train/loss=2.161125, validation/accuracy=0.606725, validation/bleu=24.557212, validation/loss=2.036038, validation/num_examples=3000
I0208 07:59:50.278488 140529918994176 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.3659646511077881, loss=2.079575777053833
I0208 08:00:25.476904 140529910601472 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.35729140043258667, loss=2.055694103240967
I0208 08:01:00.707405 140529918994176 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.3725922405719757, loss=2.093914747238159
I0208 08:01:35.979084 140529910601472 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.3368024528026581, loss=2.2224109172821045
I0208 08:02:11.255211 140529918994176 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.32430338859558105, loss=2.1339774131774902
I0208 08:02:46.533522 140529910601472 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.3109045922756195, loss=2.0891757011413574
I0208 08:03:21.845764 140529918994176 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.32097128033638, loss=2.077869415283203
I0208 08:03:57.136619 140529910601472 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.31126895546913147, loss=2.1017801761627197
I0208 08:04:32.429534 140529918994176 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.41713249683380127, loss=2.0913283824920654
I0208 08:05:07.742586 140529910601472 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.3617938756942749, loss=2.0325093269348145
I0208 08:05:43.028268 140529918994176 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.3042699992656708, loss=2.1481521129608154
I0208 08:06:18.299166 140529910601472 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.3319815397262573, loss=2.0798799991607666
I0208 08:06:53.633018 140529918994176 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.312820702791214, loss=2.0358872413635254
I0208 08:07:29.038883 140529910601472 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.41248440742492676, loss=2.1835649013519287
I0208 08:08:04.373725 140529918994176 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.3090052604675293, loss=2.142720937728882
I0208 08:08:39.726983 140529910601472 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.3446655571460724, loss=2.0920848846435547
I0208 08:09:15.070911 140529918994176 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.3156428635120392, loss=1.9912993907928467
I0208 08:09:50.366153 140529910601472 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.38112983107566833, loss=2.164748430252075
I0208 08:10:25.703294 140529918994176 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.26760128140449524, loss=1.9877716302871704
I0208 08:11:01.026592 140529910601472 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.3235607445240021, loss=2.1704001426696777
I0208 08:11:36.326070 140529918994176 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.2856755554676056, loss=2.048137903213501
I0208 08:12:11.659946 140529910601472 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.3069586753845215, loss=2.0120766162872314
I0208 08:12:46.954678 140529918994176 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.3101881146430969, loss=2.0599608421325684
I0208 08:13:20.742112 140699726837568 spec.py:321] Evaluating on the training split.
I0208 08:13:23.780107 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 08:16:03.517037 140699726837568 spec.py:333] Evaluating on the validation split.
I0208 08:16:06.254291 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 08:18:36.786493 140699726837568 spec.py:349] Evaluating on the test split.
I0208 08:18:39.529148 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 08:20:51.880122 140699726837568 submission_runner.py:408] Time since start: 7702.56s, 	Step: 11897, 	{'train/accuracy': 0.6027729511260986, 'train/loss': 2.0559475421905518, 'train/bleu': 28.85630338895498, 'validation/accuracy': 0.6205874681472778, 'validation/loss': 1.9075512886047363, 'validation/bleu': 26.280029016395673, 'validation/num_examples': 3000, 'test/accuracy': 0.6275289058685303, 'test/loss': 1.8683427572250366, 'test/bleu': 24.511840943821287, 'test/num_examples': 3003, 'score': 4227.306416749954, 'total_duration': 7702.555572986603, 'accumulated_submission_time': 4227.306416749954, 'accumulated_eval_time': 3474.739589214325, 'accumulated_logging_time': 0.13073039054870605}
I0208 08:20:51.897278 140529910601472 logging_writer.py:48] [11897] accumulated_eval_time=3474.739589, accumulated_logging_time=0.130730, accumulated_submission_time=4227.306417, global_step=11897, preemption_count=0, score=4227.306417, test/accuracy=0.627529, test/bleu=24.511841, test/loss=1.868343, test/num_examples=3003, total_duration=7702.555573, train/accuracy=0.602773, train/bleu=28.856303, train/loss=2.055948, validation/accuracy=0.620587, validation/bleu=26.280029, validation/loss=1.907551, validation/num_examples=3000
I0208 08:20:53.320658 140529918994176 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.3010362684726715, loss=2.0525712966918945
I0208 08:21:28.493923 140529910601472 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.32699358463287354, loss=2.1920740604400635
I0208 08:22:03.728709 140529918994176 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.29394441843032837, loss=1.9849443435668945
I0208 08:22:38.964920 140529910601472 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.28921639919281006, loss=1.9589217901229858
I0208 08:23:14.225739 140529918994176 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.2738991379737854, loss=2.0570321083068848
I0208 08:23:49.532719 140529910601472 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.258045494556427, loss=2.050875663757324
I0208 08:24:24.842370 140529918994176 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.3108328580856323, loss=1.9835060834884644
I0208 08:25:00.116060 140529910601472 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.3958510756492615, loss=2.0297040939331055
I0208 08:25:35.378356 140529918994176 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.2739378809928894, loss=2.0695888996124268
I0208 08:26:10.682951 140529910601472 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.27489063143730164, loss=2.0730741024017334
I0208 08:26:45.959523 140529918994176 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.2564777731895447, loss=2.030538558959961
I0208 08:27:21.229587 140529910601472 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.25755345821380615, loss=2.018594264984131
I0208 08:27:56.492259 140529918994176 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.3037910461425781, loss=2.0131449699401855
I0208 08:28:31.754809 140529910601472 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.3306728005409241, loss=2.0360755920410156
I0208 08:29:07.030146 140529918994176 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.2753870487213135, loss=2.042208194732666
I0208 08:29:42.330135 140529910601472 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.34049949049949646, loss=2.017125129699707
I0208 08:30:17.581037 140529918994176 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.25335419178009033, loss=2.0105502605438232
I0208 08:30:52.868258 140529910601472 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.29181671142578125, loss=1.9498813152313232
I0208 08:31:28.183160 140529918994176 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.3075196444988251, loss=2.067145347595215
I0208 08:32:03.491265 140529910601472 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.2849339246749878, loss=2.0514564514160156
I0208 08:32:38.813628 140529918994176 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.2551860511302948, loss=1.9553680419921875
I0208 08:33:14.083809 140529910601472 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.2782311737537384, loss=2.0194530487060547
I0208 08:33:49.375491 140529918994176 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.2881680428981781, loss=1.9725865125656128
I0208 08:34:24.689464 140529910601472 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.29016348719596863, loss=1.940132737159729
I0208 08:34:51.929838 140699726837568 spec.py:321] Evaluating on the training split.
I0208 08:34:54.974250 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 08:38:32.157960 140699726837568 spec.py:333] Evaluating on the validation split.
I0208 08:38:34.902524 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 08:41:01.471583 140699726837568 spec.py:349] Evaluating on the test split.
I0208 08:41:04.222711 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 08:43:17.451646 140699726837568 submission_runner.py:408] Time since start: 9048.13s, 	Step: 14279, 	{'train/accuracy': 0.6161245703697205, 'train/loss': 1.941306471824646, 'train/bleu': 29.923540706085337, 'validation/accuracy': 0.6305191516876221, 'validation/loss': 1.827352523803711, 'validation/bleu': 26.47629077209134, 'validation/num_examples': 3000, 'test/accuracy': 0.6389285922050476, 'test/loss': 1.7839852571487427, 'test/bleu': 25.561584130812317, 'test/num_examples': 3003, 'score': 5067.252546310425, 'total_duration': 9048.127070188522, 'accumulated_submission_time': 5067.252546310425, 'accumulated_eval_time': 3980.2613196372986, 'accumulated_logging_time': 0.15803933143615723}
I0208 08:43:17.471818 140529918994176 logging_writer.py:48] [14279] accumulated_eval_time=3980.261320, accumulated_logging_time=0.158039, accumulated_submission_time=5067.252546, global_step=14279, preemption_count=0, score=5067.252546, test/accuracy=0.638929, test/bleu=25.561584, test/loss=1.783985, test/num_examples=3003, total_duration=9048.127070, train/accuracy=0.616125, train/bleu=29.923541, train/loss=1.941306, validation/accuracy=0.630519, validation/bleu=26.476291, validation/loss=1.827353, validation/num_examples=3000
I0208 08:43:25.224180 140529910601472 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.293920636177063, loss=2.0257537364959717
I0208 08:44:00.411020 140529918994176 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.32785698771476746, loss=1.9340107440948486
I0208 08:44:35.613303 140529910601472 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.34180474281311035, loss=1.9091585874557495
I0208 08:45:10.848623 140529918994176 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.34174859523773193, loss=1.9942814111709595
I0208 08:45:46.138671 140529910601472 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.35114866495132446, loss=2.002272129058838
I0208 08:46:21.457188 140529918994176 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.29626649618148804, loss=1.9410243034362793
I0208 08:46:56.705540 140529910601472 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.33826279640197754, loss=2.0609121322631836
I0208 08:47:32.004305 140529918994176 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.33939695358276367, loss=1.9647717475891113
I0208 08:48:07.266364 140529910601472 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.3207995891571045, loss=1.9900872707366943
I0208 08:48:42.573992 140529918994176 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.30829185247421265, loss=1.9004948139190674
I0208 08:49:17.851597 140529910601472 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.2668929398059845, loss=2.0018856525421143
I0208 08:49:53.179827 140529918994176 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.31002187728881836, loss=2.015472412109375
I0208 08:50:28.462233 140529910601472 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.3058101534843445, loss=1.9682725667953491
I0208 08:51:03.723947 140529918994176 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.2799646258354187, loss=1.935804009437561
I0208 08:51:39.021437 140529910601472 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.3030737638473511, loss=1.9811309576034546
I0208 08:52:14.305636 140529918994176 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.4030645191669464, loss=1.8985317945480347
I0208 08:52:49.573386 140529910601472 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.28207775950431824, loss=1.8889315128326416
I0208 08:53:24.848494 140529918994176 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.2847725749015808, loss=1.9404881000518799
I0208 08:54:00.103118 140529910601472 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.31189781427383423, loss=1.9062464237213135
I0208 08:54:35.360993 140529918994176 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.32547956705093384, loss=1.9338263273239136
I0208 08:55:10.619707 140529910601472 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.2960931658744812, loss=1.8851568698883057
I0208 08:55:45.882668 140529918994176 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.33479392528533936, loss=1.9176362752914429
I0208 08:56:21.125741 140529910601472 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.3607749938964844, loss=1.973630666732788
I0208 08:56:56.419362 140529918994176 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.30631881952285767, loss=1.9184465408325195
I0208 08:57:17.649937 140699726837568 spec.py:321] Evaluating on the training split.
I0208 08:57:20.677361 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 09:00:03.359850 140699726837568 spec.py:333] Evaluating on the validation split.
I0208 09:00:06.089026 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 09:02:36.257714 140699726837568 spec.py:349] Evaluating on the test split.
I0208 09:02:38.989651 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 09:04:51.378923 140699726837568 submission_runner.py:408] Time since start: 10342.05s, 	Step: 16662, 	{'train/accuracy': 0.6206269860267639, 'train/loss': 1.902586579322815, 'train/bleu': 29.945450713323183, 'validation/accuracy': 0.6397068500518799, 'validation/loss': 1.7634724378585815, 'validation/bleu': 27.15322636447869, 'validation/num_examples': 3000, 'test/accuracy': 0.6466794610023499, 'test/loss': 1.7082695960998535, 'test/bleu': 26.0778676097258, 'test/num_examples': 3003, 'score': 5907.344487428665, 'total_duration': 10342.05437874794, 'accumulated_submission_time': 5907.344487428665, 'accumulated_eval_time': 4433.9902555942535, 'accumulated_logging_time': 0.18922209739685059}
I0208 09:04:51.396506 140529910601472 logging_writer.py:48] [16662] accumulated_eval_time=4433.990256, accumulated_logging_time=0.189222, accumulated_submission_time=5907.344487, global_step=16662, preemption_count=0, score=5907.344487, test/accuracy=0.646679, test/bleu=26.077868, test/loss=1.708270, test/num_examples=3003, total_duration=10342.054379, train/accuracy=0.620627, train/bleu=29.945451, train/loss=1.902587, validation/accuracy=0.639707, validation/bleu=27.153226, validation/loss=1.763472, validation/num_examples=3000
I0208 09:05:05.119446 140529918994176 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.38678473234176636, loss=1.8685463666915894
I0208 09:05:40.283799 140529910601472 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.33045828342437744, loss=1.9315545558929443
I0208 09:06:15.550155 140529918994176 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.3735888600349426, loss=1.9144057035446167
I0208 09:06:50.768294 140529910601472 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.28952035307884216, loss=1.9108688831329346
I0208 09:07:26.011918 140529918994176 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.3002188205718994, loss=1.917618751525879
I0208 09:08:01.296190 140529910601472 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.618901252746582, loss=2.0065464973449707
I0208 09:08:36.566605 140529918994176 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.37876763939857483, loss=1.892065167427063
I0208 09:09:11.828200 140529910601472 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.29982084035873413, loss=1.8826040029525757
I0208 09:09:47.055941 140529918994176 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.30967000126838684, loss=2.001272678375244
I0208 09:10:22.343105 140529910601472 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.3248417377471924, loss=1.9107255935668945
I0208 09:10:57.587016 140529918994176 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.3566320538520813, loss=1.9076590538024902
I0208 09:11:32.864610 140529910601472 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.3431147038936615, loss=1.9162615537643433
I0208 09:12:08.138305 140529918994176 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.3259686827659607, loss=1.8779393434524536
I0208 09:12:43.420167 140529910601472 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.30398330092430115, loss=1.784264326095581
I0208 09:13:18.673590 140529918994176 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.4718451201915741, loss=1.9122518301010132
I0208 09:13:53.958138 140529910601472 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.3398105800151825, loss=1.9582772254943848
I0208 09:14:29.249390 140529918994176 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.35361188650131226, loss=1.855899691581726
I0208 09:15:04.536505 140529910601472 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.30618816614151, loss=1.9153201580047607
I0208 09:15:39.845895 140529918994176 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.3409941792488098, loss=1.8774164915084839
I0208 09:16:15.092674 140529910601472 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.31347501277923584, loss=1.8643748760223389
I0208 09:16:50.360173 140529918994176 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.3642302453517914, loss=1.8627324104309082
I0208 09:17:25.620411 140529910601472 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.4355785846710205, loss=1.92915940284729
I0208 09:18:00.870141 140529918994176 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.3590956926345825, loss=1.8852914571762085
I0208 09:18:36.126362 140529910601472 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.377849280834198, loss=1.8734146356582642
I0208 09:18:51.718918 140699726837568 spec.py:321] Evaluating on the training split.
I0208 09:18:54.754016 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 09:23:19.501673 140699726837568 spec.py:333] Evaluating on the validation split.
I0208 09:23:22.232451 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 09:27:44.381463 140699726837568 spec.py:349] Evaluating on the test split.
I0208 09:27:47.112548 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 09:31:37.266077 140699726837568 submission_runner.py:408] Time since start: 11947.94s, 	Step: 19046, 	{'train/accuracy': 0.641328752040863, 'train/loss': 1.730721354484558, 'train/bleu': 31.378610385408912, 'validation/accuracy': 0.6421619057655334, 'validation/loss': 1.719527244567871, 'validation/bleu': 26.5024963346979, 'validation/num_examples': 3000, 'test/accuracy': 0.6498286128044128, 'test/loss': 1.6713584661483765, 'test/bleu': 26.317142488955124, 'test/num_examples': 3003, 'score': 6747.582292556763, 'total_duration': 11947.941531896591, 'accumulated_submission_time': 6747.582292556763, 'accumulated_eval_time': 5199.53736281395, 'accumulated_logging_time': 0.21701884269714355}
I0208 09:31:37.283826 140529918994176 logging_writer.py:48] [19046] accumulated_eval_time=5199.537363, accumulated_logging_time=0.217019, accumulated_submission_time=6747.582293, global_step=19046, preemption_count=0, score=6747.582293, test/accuracy=0.649829, test/bleu=26.317142, test/loss=1.671358, test/num_examples=3003, total_duration=11947.941532, train/accuracy=0.641329, train/bleu=31.378610, train/loss=1.730721, validation/accuracy=0.642162, validation/bleu=26.502496, validation/loss=1.719527, validation/num_examples=3000
I0208 09:31:56.618515 140529910601472 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.36349889636039734, loss=1.9372413158416748
I0208 09:32:31.805303 140529918994176 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.4203842282295227, loss=1.9013680219650269
I0208 09:33:07.025594 140529910601472 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.3520548343658447, loss=1.8569356203079224
I0208 09:33:42.263195 140529918994176 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.39685678482055664, loss=1.9366904497146606
I0208 09:34:17.509201 140529910601472 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.32368960976600647, loss=1.8378751277923584
I0208 09:34:52.760688 140529918994176 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.35868266224861145, loss=1.9244015216827393
I0208 09:35:28.051558 140529910601472 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.608630359172821, loss=1.9334858655929565
I0208 09:36:03.320466 140529918994176 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.4044606387615204, loss=1.911548137664795
I0208 09:36:38.634011 140529910601472 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.35869526863098145, loss=1.8251581192016602
I0208 09:37:13.910359 140529918994176 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.35972800850868225, loss=1.8819352388381958
I0208 09:37:49.154803 140529910601472 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.38535982370376587, loss=1.9462774991989136
I0208 09:38:24.438060 140529918994176 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.3709016442298889, loss=1.7887896299362183
I0208 09:38:59.718834 140529910601472 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.41803333163261414, loss=1.7948156595230103
I0208 09:39:34.985543 140529918994176 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.4102577269077301, loss=1.9216176271438599
I0208 09:40:10.264099 140529910601472 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.3234112560749054, loss=1.828637719154358
I0208 09:40:45.559640 140529918994176 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.3271864950656891, loss=1.745375633239746
I0208 09:41:20.854552 140529910601472 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.35157206654548645, loss=1.8938519954681396
I0208 09:41:56.129702 140529918994176 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.372263640165329, loss=1.8531391620635986
I0208 09:42:31.397077 140529910601472 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.4151885509490967, loss=1.8887250423431396
I0208 09:43:06.663066 140529918994176 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.41523903608322144, loss=1.8107881546020508
I0208 09:43:41.921700 140529910601472 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.37490034103393555, loss=1.8030095100402832
I0208 09:44:17.196519 140529918994176 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.41520407795906067, loss=1.879299283027649
I0208 09:44:52.450612 140529910601472 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.34695422649383545, loss=1.8832308053970337
I0208 09:45:27.730953 140529918994176 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.3815257251262665, loss=1.8789944648742676
I0208 09:45:37.334547 140699726837568 spec.py:321] Evaluating on the training split.
I0208 09:45:40.355344 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 09:48:42.226787 140699726837568 spec.py:333] Evaluating on the validation split.
I0208 09:48:44.979280 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 09:51:11.934057 140699726837568 spec.py:349] Evaluating on the test split.
I0208 09:51:14.676568 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 09:53:38.910751 140699726837568 submission_runner.py:408] Time since start: 13269.59s, 	Step: 21429, 	{'train/accuracy': 0.6279956698417664, 'train/loss': 1.8332117795944214, 'train/bleu': 30.87111135681036, 'validation/accuracy': 0.6472703218460083, 'validation/loss': 1.690517783164978, 'validation/bleu': 27.717725941527142, 'validation/num_examples': 3000, 'test/accuracy': 0.6571030020713806, 'test/loss': 1.642043113708496, 'test/bleu': 26.552804953338676, 'test/num_examples': 3003, 'score': 7587.546016216278, 'total_duration': 13269.586208820343, 'accumulated_submission_time': 7587.546016216278, 'accumulated_eval_time': 5681.113517045975, 'accumulated_logging_time': 0.24637985229492188}
I0208 09:53:38.928594 140529910601472 logging_writer.py:48] [21429] accumulated_eval_time=5681.113517, accumulated_logging_time=0.246380, accumulated_submission_time=7587.546016, global_step=21429, preemption_count=0, score=7587.546016, test/accuracy=0.657103, test/bleu=26.552805, test/loss=1.642043, test/num_examples=3003, total_duration=13269.586209, train/accuracy=0.627996, train/bleu=30.871111, train/loss=1.833212, validation/accuracy=0.647270, validation/bleu=27.717726, validation/loss=1.690518, validation/num_examples=3000
I0208 09:54:04.213384 140529918994176 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.38289934396743774, loss=1.8902883529663086
I0208 09:54:39.370641 140529910601472 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.39208099246025085, loss=1.8999078273773193
I0208 09:55:14.605554 140529918994176 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.3748582899570465, loss=1.8520891666412354
I0208 09:55:49.836086 140529910601472 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.3891560435295105, loss=1.8140891790390015
I0208 09:56:25.076014 140529918994176 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.40741056203842163, loss=1.7983890771865845
I0208 09:57:00.323269 140529910601472 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.3960811495780945, loss=1.8998379707336426
I0208 09:57:35.570153 140529918994176 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.40784209966659546, loss=1.778509259223938
I0208 09:58:10.868671 140529910601472 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.36088573932647705, loss=1.8231934309005737
I0208 09:58:46.119983 140529918994176 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.36710691452026367, loss=1.8334912061691284
I0208 09:59:21.393354 140529910601472 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.39732223749160767, loss=1.8382792472839355
I0208 09:59:56.650321 140529918994176 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.3745269775390625, loss=1.7941583395004272
I0208 10:00:31.921514 140529910601472 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.3801024854183197, loss=1.8715994358062744
I0208 10:01:07.216383 140529918994176 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.4991031885147095, loss=1.817152738571167
I0208 10:01:42.482832 140529910601472 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.3519417345523834, loss=1.7859158515930176
I0208 10:02:17.776971 140529918994176 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.346211314201355, loss=1.8437665700912476
I0208 10:02:53.050609 140529910601472 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.35833507776260376, loss=1.8174278736114502
I0208 10:03:28.323796 140529918994176 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.4204697906970978, loss=1.7941524982452393
I0208 10:04:03.657917 140529910601472 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.47542744874954224, loss=1.8700124025344849
I0208 10:04:38.938486 140529918994176 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.3879012167453766, loss=1.9228465557098389
I0208 10:05:14.226336 140529910601472 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.4082644283771515, loss=1.8977333307266235
I0208 10:05:49.554332 140529918994176 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.41362056136131287, loss=1.757742166519165
I0208 10:06:24.821514 140529910601472 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.4421340525150299, loss=1.8295199871063232
I0208 10:07:00.121984 140529918994176 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.3911929428577423, loss=1.7486623525619507
I0208 10:07:35.378376 140529910601472 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.36655405163764954, loss=1.8100521564483643
I0208 10:07:38.978119 140699726837568 spec.py:321] Evaluating on the training split.
I0208 10:07:42.003527 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 10:10:48.383424 140699726837568 spec.py:333] Evaluating on the validation split.
I0208 10:10:51.117914 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 10:13:18.916400 140699726837568 spec.py:349] Evaluating on the test split.
I0208 10:13:21.650175 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 10:15:41.692356 140699726837568 submission_runner.py:408] Time since start: 14592.37s, 	Step: 23812, 	{'train/accuracy': 0.6292793154716492, 'train/loss': 1.8349435329437256, 'train/bleu': 31.07930974577942, 'validation/accuracy': 0.6511016488075256, 'validation/loss': 1.673044204711914, 'validation/bleu': 27.767557476735757, 'validation/num_examples': 3000, 'test/accuracy': 0.6596711277961731, 'test/loss': 1.6173677444458008, 'test/bleu': 27.04909577649694, 'test/num_examples': 3003, 'score': 8427.509428739548, 'total_duration': 14592.367801189423, 'accumulated_submission_time': 8427.509428739548, 'accumulated_eval_time': 6163.827691793442, 'accumulated_logging_time': 0.27437496185302734}
I0208 10:15:41.710353 140529918994176 logging_writer.py:48] [23812] accumulated_eval_time=6163.827692, accumulated_logging_time=0.274375, accumulated_submission_time=8427.509429, global_step=23812, preemption_count=0, score=8427.509429, test/accuracy=0.659671, test/bleu=27.049096, test/loss=1.617368, test/num_examples=3003, total_duration=14592.367801, train/accuracy=0.629279, train/bleu=31.079310, train/loss=1.834944, validation/accuracy=0.651102, validation/bleu=27.767557, validation/loss=1.673044, validation/num_examples=3000
I0208 10:16:13.030521 140529910601472 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.41771307587623596, loss=1.868048906326294
I0208 10:16:48.238224 140529918994176 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.39692461490631104, loss=1.7522720098495483
I0208 10:17:23.489893 140529910601472 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.37743738293647766, loss=1.7997782230377197
I0208 10:17:58.728970 140529918994176 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.3798668086528778, loss=1.871254801750183
I0208 10:18:33.991761 140529910601472 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.3830765187740326, loss=1.8912233114242554
I0208 10:19:09.220875 140529918994176 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.4313860237598419, loss=1.8446182012557983
I0208 10:19:44.516460 140529910601472 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.5037142038345337, loss=1.8491312265396118
I0208 10:20:19.790525 140529918994176 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.4308653473854065, loss=1.8441588878631592
I0208 10:20:55.091964 140529910601472 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.4327394962310791, loss=1.8708988428115845
I0208 10:21:30.412626 140529918994176 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.40055617690086365, loss=1.8713401556015015
I0208 10:22:05.696216 140529910601472 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.38162967562675476, loss=1.8271147012710571
I0208 10:22:40.973612 140529918994176 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.4030435085296631, loss=1.7815907001495361
I0208 10:23:16.218802 140529910601472 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.3737697899341583, loss=1.8863047361373901
I0208 10:23:51.451711 140529918994176 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.38883817195892334, loss=1.8145228624343872
I0208 10:24:26.692842 140529910601472 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.4271063804626465, loss=1.7910171747207642
I0208 10:25:01.939742 140529918994176 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.580636739730835, loss=1.835056185722351
I0208 10:25:37.168967 140529910601472 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.4288961589336395, loss=1.8482065200805664
I0208 10:26:12.500698 140529918994176 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.4009244740009308, loss=1.8248260021209717
I0208 10:26:47.795613 140529910601472 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.37000367045402527, loss=1.9044336080551147
I0208 10:27:23.070049 140529918994176 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.36338359117507935, loss=1.8537904024124146
I0208 10:27:58.300324 140529910601472 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.4046050012111664, loss=1.809274435043335
I0208 10:28:33.561488 140529918994176 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.6153030395507812, loss=1.8343547582626343
I0208 10:29:08.814750 140529910601472 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.4036269783973694, loss=1.8618324995040894
I0208 10:29:42.024305 140699726837568 spec.py:321] Evaluating on the training split.
I0208 10:29:45.074159 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 10:32:47.282037 140699726837568 spec.py:333] Evaluating on the validation split.
I0208 10:32:50.016669 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 10:35:27.557678 140699726837568 spec.py:349] Evaluating on the test split.
I0208 10:35:30.292057 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 10:38:05.235762 140699726837568 submission_runner.py:408] Time since start: 15935.91s, 	Step: 26196, 	{'train/accuracy': 0.636094868183136, 'train/loss': 1.7777622938156128, 'train/bleu': 31.102546323862676, 'validation/accuracy': 0.6532343029975891, 'validation/loss': 1.6610873937606812, 'validation/bleu': 27.915533557356824, 'validation/num_examples': 3000, 'test/accuracy': 0.662216067314148, 'test/loss': 1.600104570388794, 'test/bleu': 27.36525046340449, 'test/num_examples': 3003, 'score': 9267.73613333702, 'total_duration': 15935.911216020584, 'accumulated_submission_time': 9267.73613333702, 'accumulated_eval_time': 6667.039098739624, 'accumulated_logging_time': 0.3026127815246582}
I0208 10:38:05.254003 140529918994176 logging_writer.py:48] [26196] accumulated_eval_time=6667.039099, accumulated_logging_time=0.302613, accumulated_submission_time=9267.736133, global_step=26196, preemption_count=0, score=9267.736133, test/accuracy=0.662216, test/bleu=27.365250, test/loss=1.600105, test/num_examples=3003, total_duration=15935.911216, train/accuracy=0.636095, train/bleu=31.102546, train/loss=1.777762, validation/accuracy=0.653234, validation/bleu=27.915534, validation/loss=1.661087, validation/num_examples=3000
I0208 10:38:07.032863 140529910601472 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.5501434803009033, loss=1.8651052713394165
I0208 10:38:42.186758 140529918994176 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.4064883589744568, loss=1.8346718549728394
I0208 10:39:17.391955 140529910601472 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.3826982080936432, loss=1.7578374147415161
I0208 10:39:52.596765 140529918994176 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.40318599343299866, loss=1.780769944190979
I0208 10:40:27.828173 140529910601472 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.3966374695301056, loss=1.7556419372558594
I0208 10:41:03.076534 140529918994176 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.4016069769859314, loss=1.7998453378677368
I0208 10:41:38.335803 140529910601472 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.35834771394729614, loss=1.7632375955581665
I0208 10:42:13.609256 140529918994176 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.4196648597717285, loss=1.784908652305603
I0208 10:42:48.864959 140529910601472 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.4133986234664917, loss=1.8223412036895752
I0208 10:43:24.106327 140529918994176 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.4273134768009186, loss=1.8051724433898926
I0208 10:43:59.324832 140529910601472 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.35796231031417847, loss=1.7645471096038818
I0208 10:44:34.553661 140529918994176 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.3751429617404938, loss=1.804857611656189
I0208 10:45:09.778568 140529910601472 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.48583897948265076, loss=1.7926708459854126
I0208 10:45:45.057500 140529918994176 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.5219161510467529, loss=1.8502007722854614
I0208 10:46:20.330054 140529910601472 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.4342121183872223, loss=1.778559923171997
I0208 10:46:55.579558 140529918994176 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.38926219940185547, loss=1.8168308734893799
I0208 10:47:30.814396 140529910601472 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.4052921235561371, loss=1.7786474227905273
I0208 10:48:06.028840 140529918994176 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.43069055676460266, loss=1.8338426351547241
I0208 10:48:41.278691 140529910601472 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.4237176775932312, loss=1.8546191453933716
I0208 10:49:16.506428 140529918994176 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.43526139855384827, loss=1.837446928024292
I0208 10:49:51.746438 140529910601472 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.402712345123291, loss=1.8517580032348633
I0208 10:50:26.971575 140529918994176 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.5655062198638916, loss=1.8421635627746582
I0208 10:51:02.228616 140529910601472 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.41160881519317627, loss=1.8205362558364868
I0208 10:51:37.526835 140529918994176 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.6690250635147095, loss=1.8743637800216675
I0208 10:52:05.441466 140699726837568 spec.py:321] Evaluating on the training split.
I0208 10:52:08.461421 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 10:55:28.861580 140699726837568 spec.py:333] Evaluating on the validation split.
I0208 10:55:31.591130 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 10:58:06.577181 140699726837568 spec.py:349] Evaluating on the test split.
I0208 10:58:09.307427 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 11:00:49.091025 140699726837568 submission_runner.py:408] Time since start: 17299.77s, 	Step: 28581, 	{'train/accuracy': 0.6279000639915466, 'train/loss': 1.8329882621765137, 'train/bleu': 30.217147413299987, 'validation/accuracy': 0.650444507598877, 'validation/loss': 1.677872896194458, 'validation/bleu': 27.74585837118948, 'validation/num_examples': 3000, 'test/accuracy': 0.6586834192276001, 'test/loss': 1.6259721517562866, 'test/bleu': 26.974623928988517, 'test/num_examples': 3003, 'score': 10107.839243650436, 'total_duration': 17299.766478300095, 'accumulated_submission_time': 10107.839243650436, 'accumulated_eval_time': 7190.688607931137, 'accumulated_logging_time': 0.3311488628387451}
I0208 11:00:49.109613 140529910601472 logging_writer.py:48] [28581] accumulated_eval_time=7190.688608, accumulated_logging_time=0.331149, accumulated_submission_time=10107.839244, global_step=28581, preemption_count=0, score=10107.839244, test/accuracy=0.658683, test/bleu=26.974624, test/loss=1.625972, test/num_examples=3003, total_duration=17299.766478, train/accuracy=0.627900, train/bleu=30.217147, train/loss=1.832988, validation/accuracy=0.650445, validation/bleu=27.745858, validation/loss=1.677873, validation/num_examples=3000
I0208 11:00:56.148314 140529918994176 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.4573180079460144, loss=1.8222943544387817
I0208 11:01:31.329530 140529910601472 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.4003905653953552, loss=1.747164011001587
I0208 11:02:06.578306 140529918994176 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.4201366901397705, loss=1.839743971824646
I0208 11:02:41.834408 140529910601472 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.3925974667072296, loss=1.7415845394134521
I0208 11:03:17.105659 140529918994176 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.35714980959892273, loss=1.8933113813400269
I0208 11:03:52.381398 140529910601472 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.4185793399810791, loss=1.7249877452850342
I0208 11:04:27.685596 140529918994176 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.3663896918296814, loss=1.785881519317627
I0208 11:05:03.063039 140529910601472 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.3757193088531494, loss=1.7920502424240112
I0208 11:05:38.372436 140529918994176 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.3901199400424957, loss=1.9059960842132568
I0208 11:06:13.670211 140529910601472 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.3925890326499939, loss=1.8753398656845093
I0208 11:06:48.987509 140529918994176 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.39199888706207275, loss=1.8121531009674072
I0208 11:07:24.267440 140529910601472 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.401302695274353, loss=1.7950308322906494
I0208 11:07:59.532341 140529918994176 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.3378596007823944, loss=1.7697641849517822
I0208 11:08:34.770990 140529910601472 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.3900894522666931, loss=1.7745362520217896
I0208 11:09:10.014641 140529918994176 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.36566492915153503, loss=1.7441279888153076
I0208 11:09:45.236636 140529910601472 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.39109915494918823, loss=1.7890840768814087
I0208 11:10:20.449776 140529918994176 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.4466211497783661, loss=1.836763620376587
I0208 11:10:55.689712 140529910601472 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.40647685527801514, loss=1.810891032218933
I0208 11:11:30.954381 140529918994176 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.4113272428512573, loss=1.7839454412460327
I0208 11:12:06.198344 140529910601472 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.4203784465789795, loss=1.7854751348495483
I0208 11:12:41.480371 140529918994176 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.4067414104938507, loss=1.8352919816970825
I0208 11:13:16.737753 140529910601472 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.41654476523399353, loss=1.8538007736206055
I0208 11:13:51.964031 140529918994176 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.3972410261631012, loss=1.8277512788772583
I0208 11:14:27.212772 140529910601472 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.37085404992103577, loss=1.695380449295044
I0208 11:14:49.134268 140699726837568 spec.py:321] Evaluating on the training split.
I0208 11:14:52.166604 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 11:18:10.636070 140699726837568 spec.py:333] Evaluating on the validation split.
I0208 11:18:13.363041 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 11:20:46.464407 140699726837568 spec.py:349] Evaluating on the test split.
I0208 11:20:49.195183 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 11:23:22.137439 140699726837568 submission_runner.py:408] Time since start: 18652.81s, 	Step: 30964, 	{'train/accuracy': 0.6326751112937927, 'train/loss': 1.8045499324798584, 'train/bleu': 30.968724846774887, 'validation/accuracy': 0.6552057266235352, 'validation/loss': 1.6448639631271362, 'validation/bleu': 28.025201830906546, 'validation/num_examples': 3000, 'test/accuracy': 0.666748046875, 'test/loss': 1.587527871131897, 'test/bleu': 27.358672182603275, 'test/num_examples': 3003, 'score': 10947.769191265106, 'total_duration': 18652.812893152237, 'accumulated_submission_time': 10947.769191265106, 'accumulated_eval_time': 7703.6917362213135, 'accumulated_logging_time': 0.36203646659851074}
I0208 11:23:22.156730 140529918994176 logging_writer.py:48] [30964] accumulated_eval_time=7703.691736, accumulated_logging_time=0.362036, accumulated_submission_time=10947.769191, global_step=30964, preemption_count=0, score=10947.769191, test/accuracy=0.666748, test/bleu=27.358672, test/loss=1.587528, test/num_examples=3003, total_duration=18652.812893, train/accuracy=0.632675, train/bleu=30.968725, train/loss=1.804550, validation/accuracy=0.655206, validation/bleu=28.025202, validation/loss=1.644864, validation/num_examples=3000
I0208 11:23:35.164538 140529910601472 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.43320444226264954, loss=1.8477826118469238
I0208 11:24:10.256817 140529918994176 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.4119200110435486, loss=1.7705427408218384
I0208 11:24:45.481148 140529910601472 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.4148980677127838, loss=1.7874032258987427
I0208 11:25:20.762444 140529918994176 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.3710183799266815, loss=1.8705412149429321
I0208 11:25:55.987527 140529910601472 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.3893337845802307, loss=1.8352210521697998
I0208 11:26:31.233730 140529918994176 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.3927891254425049, loss=1.7965847253799438
I0208 11:27:06.478216 140529910601472 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.4245728552341461, loss=1.7546272277832031
I0208 11:27:41.720550 140529918994176 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.4131886661052704, loss=1.8212212324142456
I0208 11:28:16.998830 140529910601472 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.4463123679161072, loss=1.6648536920547485
I0208 11:28:52.228897 140529918994176 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.41859835386276245, loss=1.7988489866256714
I0208 11:29:27.482098 140529910601472 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.4036397933959961, loss=1.8180171251296997
I0208 11:30:02.732011 140529918994176 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.3944265842437744, loss=1.7724552154541016
I0208 11:30:37.977134 140529910601472 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.4804534614086151, loss=1.8136489391326904
I0208 11:31:13.205387 140529918994176 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.3671167194843292, loss=1.8396841287612915
I0208 11:31:48.466717 140529910601472 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.3763573467731476, loss=1.7575737237930298
I0208 11:32:23.753021 140529918994176 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.39374440908432007, loss=1.7584441900253296
I0208 11:32:59.030387 140529910601472 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.3839738667011261, loss=1.8400596380233765
I0208 11:33:34.342460 140529918994176 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.40512627363204956, loss=1.8187156915664673
I0208 11:34:09.650521 140529910601472 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.43719837069511414, loss=1.8663190603256226
I0208 11:34:44.902214 140529918994176 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.4119928181171417, loss=1.7827304601669312
I0208 11:35:20.168586 140529910601472 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.4206242859363556, loss=1.757117509841919
I0208 11:35:55.425304 140529918994176 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.4406619966030121, loss=1.8249199390411377
I0208 11:36:30.671146 140529910601472 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.4135906994342804, loss=1.7889037132263184
I0208 11:37:05.950894 140529918994176 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.4242120385169983, loss=1.7534817457199097
I0208 11:37:22.233603 140699726837568 spec.py:321] Evaluating on the training split.
I0208 11:37:25.268348 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 11:41:27.836972 140699726837568 spec.py:333] Evaluating on the validation split.
I0208 11:41:30.583920 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 11:44:00.687785 140699726837568 spec.py:349] Evaluating on the test split.
I0208 11:44:03.433845 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 11:46:26.393142 140699726837568 submission_runner.py:408] Time since start: 20037.07s, 	Step: 33348, 	{'train/accuracy': 0.6399797201156616, 'train/loss': 1.7495006322860718, 'train/bleu': 31.4189488530457, 'validation/accuracy': 0.6567928194999695, 'validation/loss': 1.6323739290237427, 'validation/bleu': 28.445252742902714, 'validation/num_examples': 3000, 'test/accuracy': 0.6678636074066162, 'test/loss': 1.5693237781524658, 'test/bleu': 27.742504843960525, 'test/num_examples': 3003, 'score': 11787.761863231659, 'total_duration': 20037.068594694138, 'accumulated_submission_time': 11787.761863231659, 'accumulated_eval_time': 8247.851219177246, 'accumulated_logging_time': 0.39116501808166504}
I0208 11:46:26.412554 140529910601472 logging_writer.py:48] [33348] accumulated_eval_time=8247.851219, accumulated_logging_time=0.391165, accumulated_submission_time=11787.761863, global_step=33348, preemption_count=0, score=11787.761863, test/accuracy=0.667864, test/bleu=27.742505, test/loss=1.569324, test/num_examples=3003, total_duration=20037.068595, train/accuracy=0.639980, train/bleu=31.418949, train/loss=1.749501, validation/accuracy=0.656793, validation/bleu=28.445253, validation/loss=1.632374, validation/num_examples=3000
I0208 11:46:45.024253 140529918994176 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.48278531432151794, loss=1.8100106716156006
I0208 11:47:20.129275 140529910601472 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.4001757502555847, loss=1.7903081178665161
I0208 11:47:55.344014 140529918994176 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.42632147669792175, loss=1.7478172779083252
I0208 11:48:30.565140 140529910601472 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.39160990715026855, loss=1.6643604040145874
I0208 11:49:05.818558 140529918994176 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.48825350403785706, loss=1.6937276124954224
I0208 11:49:41.067679 140529910601472 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.39735373854637146, loss=1.7748225927352905
I0208 11:50:16.313766 140529918994176 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.4198434352874756, loss=1.781625747680664
I0208 11:50:51.566172 140529910601472 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.38780200481414795, loss=1.7440229654312134
I0208 11:51:26.810830 140529918994176 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.42622512578964233, loss=1.7092342376708984
I0208 11:52:02.102209 140529910601472 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.4402734041213989, loss=1.8043348789215088
I0208 11:52:37.353818 140529918994176 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.3677138388156891, loss=1.7857683897018433
I0208 11:53:12.604073 140529910601472 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.39129364490509033, loss=1.779976725578308
I0208 11:53:47.860286 140529918994176 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.3979456126689911, loss=1.7177183628082275
I0208 11:54:23.093751 140529910601472 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.4574919641017914, loss=1.855528712272644
I0208 11:54:58.352977 140529918994176 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.4308861792087555, loss=1.7701387405395508
I0208 11:55:33.574357 140529910601472 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.37317219376564026, loss=1.7280912399291992
I0208 11:56:08.829438 140529918994176 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.390382319688797, loss=1.838904857635498
I0208 11:56:44.088575 140529910601472 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.42838847637176514, loss=1.8113020658493042
I0208 11:57:19.346116 140529918994176 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.40297064185142517, loss=1.778197169303894
I0208 11:57:54.654689 140529910601472 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.6172287464141846, loss=1.738505482673645
I0208 11:58:29.942083 140529918994176 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.4263313114643097, loss=1.7963539361953735
I0208 11:59:05.185696 140529910601472 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.3986007571220398, loss=1.827629804611206
I0208 11:59:40.440474 140529918994176 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.3854800760746002, loss=1.7537391185760498
I0208 12:00:15.679888 140529910601472 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.379328191280365, loss=1.7652604579925537
I0208 12:00:26.677472 140699726837568 spec.py:321] Evaluating on the training split.
I0208 12:00:29.704446 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 12:04:53.722871 140699726837568 spec.py:333] Evaluating on the validation split.
I0208 12:04:56.458521 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 12:07:36.794533 140699726837568 spec.py:349] Evaluating on the test split.
I0208 12:07:39.527284 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 12:10:00.981863 140699726837568 submission_runner.py:408] Time since start: 21451.66s, 	Step: 35733, 	{'train/accuracy': 0.636881947517395, 'train/loss': 1.7801685333251953, 'train/bleu': 31.0226532366646, 'validation/accuracy': 0.6587890982627869, 'validation/loss': 1.621066689491272, 'validation/bleu': 28.25602613523417, 'validation/num_examples': 3000, 'test/accuracy': 0.6670036911964417, 'test/loss': 1.5616695880889893, 'test/bleu': 27.611120742716544, 'test/num_examples': 3003, 'score': 12627.94023323059, 'total_duration': 21451.65731573105, 'accumulated_submission_time': 12627.94023323059, 'accumulated_eval_time': 8822.155555486679, 'accumulated_logging_time': 0.4218254089355469}
I0208 12:10:01.002008 140529918994176 logging_writer.py:48] [35733] accumulated_eval_time=8822.155555, accumulated_logging_time=0.421825, accumulated_submission_time=12627.940233, global_step=35733, preemption_count=0, score=12627.940233, test/accuracy=0.667004, test/bleu=27.611121, test/loss=1.561670, test/num_examples=3003, total_duration=21451.657316, train/accuracy=0.636882, train/bleu=31.022653, train/loss=1.780169, validation/accuracy=0.658789, validation/bleu=28.256026, validation/loss=1.621067, validation/num_examples=3000
I0208 12:10:24.914313 140529910601472 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.3771316707134247, loss=1.8686920404434204
I0208 12:11:00.071154 140529918994176 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.4452589750289917, loss=1.799959421157837
I0208 12:11:35.297528 140529910601472 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.3945976197719574, loss=1.7450505495071411
I0208 12:12:10.484470 140529918994176 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.4344305694103241, loss=1.8238037824630737
I0208 12:12:45.691218 140529910601472 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.4277164041996002, loss=1.8021055459976196
I0208 12:13:20.892364 140529918994176 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.38061797618865967, loss=1.8623871803283691
I0208 12:13:56.148418 140529910601472 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.42941486835479736, loss=1.7849905490875244
I0208 12:14:31.377178 140529918994176 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.39475035667419434, loss=1.701419711112976
I0208 12:15:06.605907 140529910601472 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.3800119459629059, loss=1.7334388494491577
I0208 12:15:41.831311 140529918994176 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.37494969367980957, loss=1.7267701625823975
I0208 12:16:17.093516 140529910601472 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.4632662832736969, loss=1.8058031797409058
I0208 12:16:52.339581 140529918994176 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.4205399453639984, loss=1.8514690399169922
I0208 12:17:27.607617 140529910601472 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.45168259739875793, loss=1.778321623802185
I0208 12:18:02.842943 140529918994176 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.42652779817581177, loss=1.8329558372497559
I0208 12:18:38.074119 140529910601472 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.46161898970603943, loss=1.8477137088775635
I0208 12:19:13.305802 140529918994176 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.4429282248020172, loss=1.7800952196121216
I0208 12:19:48.561932 140529910601472 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.44711247086524963, loss=1.7138323783874512
I0208 12:20:23.841800 140529918994176 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.4098626971244812, loss=1.7813072204589844
I0208 12:20:59.152897 140529910601472 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.44354894757270813, loss=1.748375415802002
I0208 12:21:34.453845 140529918994176 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.39416584372520447, loss=1.651689052581787
I0208 12:22:09.698569 140529910601472 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.4393593966960907, loss=1.7557132244110107
I0208 12:22:45.000793 140529918994176 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.43022897839546204, loss=1.7416117191314697
I0208 12:23:20.290209 140529910601472 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.4407444894313812, loss=1.7675901651382446
I0208 12:23:55.517943 140529918994176 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.42740949988365173, loss=1.792148232460022
I0208 12:24:01.237863 140699726837568 spec.py:321] Evaluating on the training split.
I0208 12:24:04.274382 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 12:27:06.592649 140699726837568 spec.py:333] Evaluating on the validation split.
I0208 12:27:09.329588 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 12:29:31.140269 140699726837568 spec.py:349] Evaluating on the test split.
I0208 12:29:33.882806 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 12:32:04.831712 140699726837568 submission_runner.py:408] Time since start: 22775.51s, 	Step: 38118, 	{'train/accuracy': 0.6518439054489136, 'train/loss': 1.66854727268219, 'train/bleu': 32.117936466755715, 'validation/accuracy': 0.6590867042541504, 'validation/loss': 1.615612506866455, 'validation/bleu': 28.395664950065505, 'validation/num_examples': 3000, 'test/accuracy': 0.6677822470664978, 'test/loss': 1.551979899406433, 'test/bleu': 27.36167683701787, 'test/num_examples': 3003, 'score': 13468.088101387024, 'total_duration': 22775.507161140442, 'accumulated_submission_time': 13468.088101387024, 'accumulated_eval_time': 9305.749348640442, 'accumulated_logging_time': 0.45316171646118164}
I0208 12:32:04.852102 140529910601472 logging_writer.py:48] [38118] accumulated_eval_time=9305.749349, accumulated_logging_time=0.453162, accumulated_submission_time=13468.088101, global_step=38118, preemption_count=0, score=13468.088101, test/accuracy=0.667782, test/bleu=27.361677, test/loss=1.551980, test/num_examples=3003, total_duration=22775.507161, train/accuracy=0.651844, train/bleu=32.117936, train/loss=1.668547, validation/accuracy=0.659087, validation/bleu=28.395665, validation/loss=1.615613, validation/num_examples=3000
I0208 12:32:34.002272 140529918994176 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.44681215286254883, loss=1.7619249820709229
I0208 12:33:09.162281 140529910601472 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.4238109886646271, loss=1.6664015054702759
I0208 12:33:44.387147 140529918994176 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.41642341017723083, loss=1.7861099243164062
I0208 12:34:19.615856 140529910601472 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.40708547830581665, loss=1.7225416898727417
I0208 12:34:54.850440 140529918994176 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.3714701235294342, loss=1.7082761526107788
I0208 12:35:30.088204 140529910601472 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.3694065809249878, loss=1.6901342868804932
I0208 12:36:05.327459 140529918994176 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.39725175499916077, loss=1.7928701639175415
I0208 12:36:40.587132 140529910601472 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.39818546175956726, loss=1.7091628313064575
I0208 12:37:15.864865 140529918994176 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.439590185880661, loss=1.8647359609603882
I0208 12:37:51.177850 140529910601472 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.3626512289047241, loss=1.7788006067276
I0208 12:38:26.434126 140529918994176 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.5046122670173645, loss=1.7646639347076416
I0208 12:39:01.679738 140529910601472 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.4048084616661072, loss=1.7814898490905762
I0208 12:39:36.943971 140529918994176 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.41806909441947937, loss=1.8500347137451172
I0208 12:40:12.322736 140529910601472 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.47947958111763, loss=1.778741717338562
I0208 12:40:47.556176 140529918994176 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.3918214440345764, loss=1.7447943687438965
I0208 12:41:22.771979 140529910601472 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.43595653772354126, loss=1.725295901298523
I0208 12:41:58.004798 140529918994176 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.4315360188484192, loss=1.7696667909622192
I0208 12:42:33.232130 140529910601472 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.3851746916770935, loss=1.7610474824905396
I0208 12:43:08.487600 140529918994176 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.4364948868751526, loss=1.8308695554733276
I0208 12:43:43.722379 140529910601472 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.38581371307373047, loss=1.7568004131317139
I0208 12:44:18.993206 140529918994176 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.40628132224082947, loss=1.7346410751342773
I0208 12:44:54.274651 140529910601472 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.6722109913825989, loss=1.7552928924560547
I0208 12:45:29.534233 140529918994176 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.4964759647846222, loss=1.764176368713379
I0208 12:46:04.815686 140529910601472 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.42855730652809143, loss=1.7688192129135132
I0208 12:46:04.896001 140699726837568 spec.py:321] Evaluating on the training split.
I0208 12:46:07.925760 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 12:50:11.074153 140699726837568 spec.py:333] Evaluating on the validation split.
I0208 12:50:13.793317 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 12:53:07.224284 140699726837568 spec.py:349] Evaluating on the test split.
I0208 12:53:09.950854 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 12:55:41.599554 140699726837568 submission_runner.py:408] Time since start: 24192.27s, 	Step: 40502, 	{'train/accuracy': 0.6441013216972351, 'train/loss': 1.7263169288635254, 'train/bleu': 31.448439318045764, 'validation/accuracy': 0.6599918007850647, 'validation/loss': 1.606161117553711, 'validation/bleu': 28.42181715596443, 'validation/num_examples': 3000, 'test/accuracy': 0.6714543104171753, 'test/loss': 1.5414283275604248, 'test/bleu': 28.16023044547092, 'test/num_examples': 3003, 'score': 14308.046524524689, 'total_duration': 24192.274977445602, 'accumulated_submission_time': 14308.046524524689, 'accumulated_eval_time': 9882.452818155289, 'accumulated_logging_time': 0.4833953380584717}
I0208 12:55:41.619662 140529918994176 logging_writer.py:48] [40502] accumulated_eval_time=9882.452818, accumulated_logging_time=0.483395, accumulated_submission_time=14308.046525, global_step=40502, preemption_count=0, score=14308.046525, test/accuracy=0.671454, test/bleu=28.160230, test/loss=1.541428, test/num_examples=3003, total_duration=24192.274977, train/accuracy=0.644101, train/bleu=31.448439, train/loss=1.726317, validation/accuracy=0.659992, validation/bleu=28.421817, validation/loss=1.606161, validation/num_examples=3000
I0208 12:56:16.417729 140529910601472 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.37228521704673767, loss=1.77836012840271
I0208 12:56:51.581100 140529918994176 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.40369272232055664, loss=1.7440768480300903
I0208 12:57:26.784617 140529910601472 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.3995135426521301, loss=1.7116124629974365
I0208 12:58:02.052988 140529918994176 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.3826029598712921, loss=1.6853102445602417
I0208 12:58:37.292191 140529910601472 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.40044084191322327, loss=1.7430624961853027
I0208 12:59:12.516954 140529918994176 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.44931429624557495, loss=1.8200383186340332
I0208 12:59:47.715925 140529910601472 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.4264160692691803, loss=1.7866358757019043
I0208 13:00:22.955472 140529918994176 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.48101574182510376, loss=1.7529836893081665
I0208 13:00:58.224406 140529910601472 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.4270954430103302, loss=1.7038109302520752
I0208 13:01:33.564730 140529918994176 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.4122183620929718, loss=1.7211308479309082
I0208 13:02:08.851411 140529910601472 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.45079338550567627, loss=1.78592050075531
I0208 13:02:44.084194 140529918994176 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.46069130301475525, loss=1.807938814163208
I0208 13:03:19.346822 140529910601472 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.41839444637298584, loss=1.8191732168197632
I0208 13:03:54.601243 140529918994176 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.366421639919281, loss=1.7806296348571777
I0208 13:04:29.863947 140529910601472 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.3583580553531647, loss=1.7562540769577026
I0208 13:05:05.155749 140529918994176 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.37663963437080383, loss=1.7498213052749634
I0208 13:05:40.390724 140529910601472 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.4454655945301056, loss=1.7552642822265625
I0208 13:06:15.623215 140529918994176 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.42454788088798523, loss=1.7290992736816406
I0208 13:06:50.876994 140529910601472 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.41957253217697144, loss=1.7329384088516235
I0208 13:07:26.153556 140529918994176 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.3750801980495453, loss=1.7142857313156128
I0208 13:08:01.378453 140529910601472 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.42583632469177246, loss=1.7113970518112183
I0208 13:08:36.638245 140529918994176 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.424177348613739, loss=1.7335026264190674
I0208 13:09:11.890369 140529910601472 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.3881358504295349, loss=1.74118173122406
I0208 13:09:41.935213 140699726837568 spec.py:321] Evaluating on the training split.
I0208 13:09:44.963178 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 13:13:05.195624 140699726837568 spec.py:333] Evaluating on the validation split.
I0208 13:13:07.939855 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 13:15:56.111503 140699726837568 spec.py:349] Evaluating on the test split.
I0208 13:15:58.841476 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 13:18:29.078679 140699726837568 submission_runner.py:408] Time since start: 25559.75s, 	Step: 42887, 	{'train/accuracy': 0.6383638978004456, 'train/loss': 1.7570757865905762, 'train/bleu': 31.426057838089996, 'validation/accuracy': 0.6582187414169312, 'validation/loss': 1.6062474250793457, 'validation/bleu': 28.302365767108448, 'validation/num_examples': 3000, 'test/accuracy': 0.6733600497245789, 'test/loss': 1.5363410711288452, 'test/bleu': 28.174196940148576, 'test/num_examples': 3003, 'score': 15148.275726556778, 'total_duration': 25559.754118919373, 'accumulated_submission_time': 15148.275726556778, 'accumulated_eval_time': 10409.5962266922, 'accumulated_logging_time': 0.5133147239685059}
I0208 13:18:29.099990 140529918994176 logging_writer.py:48] [42887] accumulated_eval_time=10409.596227, accumulated_logging_time=0.513315, accumulated_submission_time=15148.275727, global_step=42887, preemption_count=0, score=15148.275727, test/accuracy=0.673360, test/bleu=28.174197, test/loss=1.536341, test/num_examples=3003, total_duration=25559.754119, train/accuracy=0.638364, train/bleu=31.426058, train/loss=1.757076, validation/accuracy=0.658219, validation/bleu=28.302366, validation/loss=1.606247, validation/num_examples=3000
I0208 13:18:34.026336 140529910601472 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.393406480550766, loss=1.6962640285491943
I0208 13:19:09.214310 140529918994176 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.4190889298915863, loss=1.7506810426712036
I0208 13:19:44.376530 140529910601472 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.4004492163658142, loss=1.7885184288024902
I0208 13:20:19.589671 140529918994176 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.3704519271850586, loss=1.757416009902954
I0208 13:20:54.822511 140529910601472 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.376521497964859, loss=1.6970010995864868
I0208 13:21:30.074587 140529918994176 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.39259693026542664, loss=1.7846866846084595
I0208 13:22:05.327086 140529910601472 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.36743465065956116, loss=1.7433080673217773
I0208 13:22:40.614960 140529918994176 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.45179376006126404, loss=1.7146244049072266
I0208 13:23:15.868607 140529910601472 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.44176241755485535, loss=1.7722827196121216
I0208 13:23:51.137645 140529918994176 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.39465901255607605, loss=1.7504419088363647
I0208 13:24:26.408005 140529910601472 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.402290016412735, loss=1.7258423566818237
I0208 13:25:01.651083 140529918994176 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.38978224992752075, loss=1.7225691080093384
I0208 13:25:36.905689 140529910601472 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.3653315007686615, loss=1.712751030921936
I0208 13:26:12.157757 140529918994176 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.4719342589378357, loss=1.8213261365890503
I0208 13:26:47.434143 140529910601472 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.46853241324424744, loss=1.7314205169677734
I0208 13:27:22.672655 140529918994176 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.39485761523246765, loss=1.7631216049194336
I0208 13:27:57.924594 140529910601472 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.4504096806049347, loss=1.776930570602417
I0208 13:28:33.176279 140529918994176 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.3739483654499054, loss=1.7629092931747437
I0208 13:29:08.462962 140529910601472 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.3692120313644409, loss=1.6556097269058228
I0208 13:29:43.674133 140529918994176 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.41221189498901367, loss=1.7005947828292847
I0208 13:30:18.950014 140529910601472 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.4626394510269165, loss=1.9084007740020752
I0208 13:30:54.228071 140529918994176 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.43821200728416443, loss=1.6952828168869019
I0208 13:31:29.539170 140529910601472 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.39141011238098145, loss=1.7242532968521118
I0208 13:32:04.800604 140529918994176 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.4009775221347809, loss=1.7901450395584106
I0208 13:32:29.195930 140699726837568 spec.py:321] Evaluating on the training split.
I0208 13:32:32.223356 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 13:36:41.256108 140699726837568 spec.py:333] Evaluating on the validation split.
I0208 13:36:43.992142 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 13:39:31.520676 140699726837568 spec.py:349] Evaluating on the test split.
I0208 13:39:34.244130 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 13:42:19.705887 140699726837568 submission_runner.py:408] Time since start: 26990.38s, 	Step: 45271, 	{'train/accuracy': 0.6441451907157898, 'train/loss': 1.7160241603851318, 'train/bleu': 31.51585807077358, 'validation/accuracy': 0.6614177227020264, 'validation/loss': 1.5945888757705688, 'validation/bleu': 28.401748387926006, 'validation/num_examples': 3000, 'test/accuracy': 0.6741386651992798, 'test/loss': 1.5258550643920898, 'test/bleu': 27.84871434765751, 'test/num_examples': 3003, 'score': 15988.28655910492, 'total_duration': 26990.38134407997, 'accumulated_submission_time': 15988.28655910492, 'accumulated_eval_time': 11000.106136083603, 'accumulated_logging_time': 0.5449538230895996}
I0208 13:42:19.726838 140529910601472 logging_writer.py:48] [45271] accumulated_eval_time=11000.106136, accumulated_logging_time=0.544954, accumulated_submission_time=15988.286559, global_step=45271, preemption_count=0, score=15988.286559, test/accuracy=0.674139, test/bleu=27.848714, test/loss=1.525855, test/num_examples=3003, total_duration=26990.381344, train/accuracy=0.644145, train/bleu=31.515858, train/loss=1.716024, validation/accuracy=0.661418, validation/bleu=28.401748, validation/loss=1.594589, validation/num_examples=3000
I0208 13:42:30.263992 140529918994176 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.417766273021698, loss=1.737975835800171
I0208 13:43:05.375176 140529910601472 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.41671985387802124, loss=1.7887436151504517
I0208 13:43:40.600962 140529918994176 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.4409438371658325, loss=1.8390209674835205
I0208 13:44:15.919028 140529910601472 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.3898673355579376, loss=1.7680916786193848
I0208 13:44:51.170990 140529918994176 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.39564019441604614, loss=1.7593004703521729
I0208 13:45:26.400217 140529910601472 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.3632720708847046, loss=1.7406989336013794
I0208 13:46:01.634541 140529918994176 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.4425843060016632, loss=1.8260283470153809
I0208 13:46:36.923701 140529910601472 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.41926640272140503, loss=1.7072473764419556
I0208 13:47:12.193208 140529918994176 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.38415396213531494, loss=1.8177618980407715
I0208 13:47:47.414531 140529910601472 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.4225769639015198, loss=1.7460848093032837
I0208 13:48:22.660398 140529918994176 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.3878200352191925, loss=1.7372487783432007
I0208 13:48:57.898155 140529910601472 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.3798154890537262, loss=1.7163869142532349
I0208 13:49:33.159328 140529918994176 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.3949398100376129, loss=1.6711653470993042
I0208 13:50:08.447341 140529910601472 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.4262255132198334, loss=1.7026786804199219
I0208 13:50:43.665400 140529918994176 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.426258385181427, loss=1.7426824569702148
I0208 13:51:18.890519 140529910601472 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.38432663679122925, loss=1.7518491744995117
I0208 13:51:54.143839 140529918994176 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.43007519841194153, loss=1.7427444458007812
I0208 13:52:29.363755 140529910601472 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.408939927816391, loss=1.844126582145691
I0208 13:53:04.637955 140529918994176 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.39830756187438965, loss=1.8369648456573486
I0208 13:53:39.879738 140529910601472 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.3992293179035187, loss=1.819876790046692
I0208 13:54:15.185324 140529918994176 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.4353289008140564, loss=1.6716015338897705
I0208 13:54:50.447917 140529910601472 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.4286153018474579, loss=1.7116280794143677
I0208 13:55:25.683792 140529918994176 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.3745315670967102, loss=1.7690013647079468
I0208 13:56:00.907592 140529910601472 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.428976833820343, loss=1.6824337244033813
I0208 13:56:20.033571 140699726837568 spec.py:321] Evaluating on the training split.
I0208 13:56:23.095304 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 13:59:33.372961 140699726837568 spec.py:333] Evaluating on the validation split.
I0208 13:59:36.109340 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 14:02:48.815885 140699726837568 spec.py:349] Evaluating on the test split.
I0208 14:02:51.539119 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 14:05:50.513338 140699726837568 submission_runner.py:408] Time since start: 28401.19s, 	Step: 47656, 	{'train/accuracy': 0.6404057145118713, 'train/loss': 1.7511805295944214, 'train/bleu': 31.178070126696518, 'validation/accuracy': 0.6617772579193115, 'validation/loss': 1.587974190711975, 'validation/bleu': 28.050460138448788, 'validation/num_examples': 3000, 'test/accuracy': 0.674312949180603, 'test/loss': 1.5223636627197266, 'test/bleu': 28.13550040341368, 'test/num_examples': 3003, 'score': 16828.508977890015, 'total_duration': 28401.188784360886, 'accumulated_submission_time': 16828.508977890015, 'accumulated_eval_time': 11570.585859537125, 'accumulated_logging_time': 0.5759387016296387}
I0208 14:05:50.534351 140529918994176 logging_writer.py:48] [47656] accumulated_eval_time=11570.585860, accumulated_logging_time=0.575939, accumulated_submission_time=16828.508978, global_step=47656, preemption_count=0, score=16828.508978, test/accuracy=0.674313, test/bleu=28.135500, test/loss=1.522364, test/num_examples=3003, total_duration=28401.188784, train/accuracy=0.640406, train/bleu=31.178070, train/loss=1.751181, validation/accuracy=0.661777, validation/bleu=28.050460, validation/loss=1.587974, validation/num_examples=3000
I0208 14:06:06.350279 140529910601472 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.37249648571014404, loss=1.8084375858306885
I0208 14:06:41.478020 140529918994176 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.5809665322303772, loss=1.733080506324768
I0208 14:07:16.653063 140529910601472 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.4016157388687134, loss=1.7443970441818237
I0208 14:07:51.851481 140529918994176 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.4273681044578552, loss=1.7091909646987915
I0208 14:08:27.088557 140529910601472 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.5279874205589294, loss=1.651694416999817
I0208 14:09:02.334384 140529918994176 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.4429926872253418, loss=1.7442595958709717
I0208 14:09:37.573107 140529910601472 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.39547887444496155, loss=1.7747788429260254
I0208 14:10:12.799869 140529918994176 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.49257391691207886, loss=1.768334984779358
I0208 14:10:48.047914 140529910601472 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.4131106734275818, loss=1.728379726409912
I0208 14:11:23.315649 140529918994176 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.3660465180873871, loss=1.7453380823135376
I0208 14:11:58.558609 140529910601472 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.4409348964691162, loss=1.7735393047332764
I0208 14:12:33.796041 140529918994176 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.37394559383392334, loss=1.6899073123931885
I0208 14:13:09.053335 140529910601472 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.4248362183570862, loss=1.8034653663635254
I0208 14:13:44.280030 140529918994176 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.44365227222442627, loss=1.7368780374526978
I0208 14:14:19.518858 140529910601472 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.4209597706794739, loss=1.7238540649414062
I0208 14:14:54.819358 140529918994176 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.37237268686294556, loss=1.7003893852233887
I0208 14:15:30.049811 140529910601472 logging_writer.py:48] [49300] global_step=49300, grad_norm=1.0267579555511475, loss=1.7536437511444092
I0208 14:16:05.320258 140529918994176 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.43042638897895813, loss=1.778296709060669
I0208 14:16:40.614332 140529910601472 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.41903990507125854, loss=1.6824181079864502
I0208 14:17:15.861813 140529918994176 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.505193293094635, loss=1.732876181602478
I0208 14:17:51.249544 140529910601472 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.42024844884872437, loss=1.7878823280334473
I0208 14:18:26.546767 140529918994176 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.4549950063228607, loss=1.8233647346496582
I0208 14:19:01.855268 140529910601472 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.4157562255859375, loss=1.8221580982208252
I0208 14:19:37.174611 140529918994176 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.41047999262809753, loss=1.7381811141967773
I0208 14:19:50.654405 140699726837568 spec.py:321] Evaluating on the training split.
I0208 14:19:53.681454 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 14:23:22.527777 140699726837568 spec.py:333] Evaluating on the validation split.
I0208 14:23:25.260778 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 14:26:21.754624 140699726837568 spec.py:349] Evaluating on the test split.
I0208 14:26:24.493667 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 14:29:15.158995 140699726837568 submission_runner.py:408] Time since start: 29805.83s, 	Step: 50040, 	{'train/accuracy': 0.6673862934112549, 'train/loss': 1.5891749858856201, 'train/bleu': 33.58396302504942, 'validation/accuracy': 0.6645546555519104, 'validation/loss': 1.5788758993148804, 'validation/bleu': 28.75264872649214, 'validation/num_examples': 3000, 'test/accuracy': 0.6757422685623169, 'test/loss': 1.516922116279602, 'test/bleu': 27.995736464992472, 'test/num_examples': 3003, 'score': 17668.541675567627, 'total_duration': 29805.83444905281, 'accumulated_submission_time': 17668.541675567627, 'accumulated_eval_time': 12135.090401649475, 'accumulated_logging_time': 0.6083238124847412}
I0208 14:29:15.182064 140529910601472 logging_writer.py:48] [50040] accumulated_eval_time=12135.090402, accumulated_logging_time=0.608324, accumulated_submission_time=17668.541676, global_step=50040, preemption_count=0, score=17668.541676, test/accuracy=0.675742, test/bleu=27.995736, test/loss=1.516922, test/num_examples=3003, total_duration=29805.834449, train/accuracy=0.667386, train/bleu=33.583963, train/loss=1.589175, validation/accuracy=0.664555, validation/bleu=28.752649, validation/loss=1.578876, validation/num_examples=3000
I0208 14:29:36.656651 140529918994176 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.43170979619026184, loss=1.7431548833847046
I0208 14:30:11.818684 140529910601472 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.4281719923019409, loss=1.7379227876663208
I0208 14:30:47.048924 140529918994176 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.48033639788627625, loss=1.736167073249817
I0208 14:31:22.306598 140529910601472 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.4301226735115051, loss=1.7420289516448975
I0208 14:31:57.542161 140529918994176 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.4627680480480194, loss=1.701952338218689
I0208 14:32:32.765229 140529910601472 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.44170528650283813, loss=1.7242283821105957
I0208 14:33:08.051824 140529918994176 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.4142770767211914, loss=1.7290875911712646
I0208 14:33:43.300960 140529910601472 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.3816746473312378, loss=1.7972922325134277
I0208 14:34:18.544856 140529918994176 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.38846397399902344, loss=1.7719498872756958
I0208 14:34:53.801083 140529910601472 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.4347996413707733, loss=1.7051746845245361
I0208 14:35:29.031757 140529918994176 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.3899557888507843, loss=1.6766464710235596
I0208 14:36:04.297155 140529910601472 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.3850202262401581, loss=1.7015379667282104
I0208 14:36:39.532363 140529918994176 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.4463983476161957, loss=1.7623640298843384
I0208 14:37:14.754683 140529910601472 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.41232603788375854, loss=1.659636378288269
I0208 14:37:50.019880 140529918994176 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.36103999614715576, loss=1.6919885873794556
I0208 14:38:25.250005 140529910601472 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.41801097989082336, loss=1.7837611436843872
I0208 14:39:00.475059 140529918994176 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.42451584339141846, loss=1.740243673324585
I0208 14:39:35.686219 140529910601472 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.41799935698509216, loss=1.808992624282837
I0208 14:40:10.914282 140529918994176 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.39306333661079407, loss=1.6987576484680176
I0208 14:40:46.175405 140529910601472 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.4118114411830902, loss=1.6988365650177002
I0208 14:41:21.440082 140529918994176 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.4069559574127197, loss=1.7522773742675781
I0208 14:41:56.692986 140529910601472 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.3994017243385315, loss=1.7945255041122437
I0208 14:42:31.962877 140529918994176 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.39124536514282227, loss=1.7144123315811157
I0208 14:43:07.197799 140529910601472 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.41386452317237854, loss=1.792828917503357
I0208 14:43:15.376706 140699726837568 spec.py:321] Evaluating on the training split.
I0208 14:43:18.408783 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 14:46:49.017013 140699726837568 spec.py:333] Evaluating on the validation split.
I0208 14:46:51.763790 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 14:49:42.244340 140699726837568 spec.py:349] Evaluating on the test split.
I0208 14:49:44.981271 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 14:52:06.108454 140699726837568 submission_runner.py:408] Time since start: 31176.78s, 	Step: 52425, 	{'train/accuracy': 0.6453410387039185, 'train/loss': 1.7113089561462402, 'train/bleu': 31.580043661207323, 'validation/accuracy': 0.6636619567871094, 'validation/loss': 1.575170874595642, 'validation/bleu': 28.384619001282065, 'validation/num_examples': 3000, 'test/accuracy': 0.6774504780769348, 'test/loss': 1.5054657459259033, 'test/bleu': 27.875327607675807, 'test/num_examples': 3003, 'score': 18508.653678417206, 'total_duration': 31176.78390431404, 'accumulated_submission_time': 18508.653678417206, 'accumulated_eval_time': 12665.822091341019, 'accumulated_logging_time': 0.6415340900421143}
I0208 14:52:06.130687 140529918994176 logging_writer.py:48] [52425] accumulated_eval_time=12665.822091, accumulated_logging_time=0.641534, accumulated_submission_time=18508.653678, global_step=52425, preemption_count=0, score=18508.653678, test/accuracy=0.677450, test/bleu=27.875328, test/loss=1.505466, test/num_examples=3003, total_duration=31176.783904, train/accuracy=0.645341, train/bleu=31.580044, train/loss=1.711309, validation/accuracy=0.663662, validation/bleu=28.384619, validation/loss=1.575171, validation/num_examples=3000
I0208 14:52:32.843344 140529910601472 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.3743036687374115, loss=1.740526556968689
I0208 14:53:08.011144 140529918994176 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.4027675688266754, loss=1.7485418319702148
I0208 14:53:43.193104 140529910601472 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.45146703720092773, loss=1.6792263984680176
I0208 14:54:18.407412 140529918994176 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.42167341709136963, loss=1.7287440299987793
I0208 14:54:53.640456 140529910601472 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.4044329822063446, loss=1.712215781211853
I0208 14:55:28.899847 140529918994176 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.39962059259414673, loss=1.7104500532150269
I0208 14:56:04.127276 140529910601472 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.36751478910446167, loss=1.6555615663528442
I0208 14:56:39.382882 140529918994176 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.4303751289844513, loss=1.673811435699463
I0208 14:57:14.653194 140529910601472 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.42196494340896606, loss=1.6323273181915283
I0208 14:57:49.901241 140529918994176 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.435268372297287, loss=1.733027696609497
I0208 14:58:25.148597 140529910601472 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.3790070414543152, loss=1.719825267791748
I0208 14:59:00.365112 140529918994176 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.5038975477218628, loss=1.6846591234207153
I0208 14:59:35.585905 140529910601472 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.4655916392803192, loss=1.7866989374160767
I0208 15:00:10.856045 140529918994176 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.4602726697921753, loss=1.7044681310653687
I0208 15:00:46.086887 140529910601472 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.41006261110305786, loss=1.7139524221420288
I0208 15:01:21.334831 140529918994176 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.4225409924983978, loss=1.6969581842422485
I0208 15:01:56.603813 140529910601472 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.39731547236442566, loss=1.845971941947937
I0208 15:02:31.832758 140529918994176 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.48555535078048706, loss=1.7653213739395142
I0208 15:03:07.064291 140529910601472 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.4280833899974823, loss=1.7336673736572266
I0208 15:03:42.328170 140529918994176 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.4386439025402069, loss=1.7269277572631836
I0208 15:04:17.606775 140529910601472 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.44356974959373474, loss=1.8066437244415283
I0208 15:04:52.851037 140529918994176 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.38100653886795044, loss=1.737845540046692
I0208 15:05:28.102075 140529910601472 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.38012057542800903, loss=1.751261591911316
I0208 15:06:03.313501 140529918994176 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.3873940408229828, loss=1.6659119129180908
I0208 15:06:06.209865 140699726837568 spec.py:321] Evaluating on the training split.
I0208 15:06:09.243350 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 15:09:39.190004 140699726837568 spec.py:333] Evaluating on the validation split.
I0208 15:09:41.918809 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 15:12:04.522923 140699726837568 spec.py:349] Evaluating on the test split.
I0208 15:12:07.265623 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 15:14:40.185218 140699726837568 submission_runner.py:408] Time since start: 32530.86s, 	Step: 54810, 	{'train/accuracy': 0.6456965804100037, 'train/loss': 1.7131825685501099, 'train/bleu': 31.870851020784162, 'validation/accuracy': 0.6642695069313049, 'validation/loss': 1.5717233419418335, 'validation/bleu': 28.47791587474528, 'validation/num_examples': 3000, 'test/accuracy': 0.6790425181388855, 'test/loss': 1.4980599880218506, 'test/bleu': 28.441837741320292, 'test/num_examples': 3003, 'score': 19348.648066282272, 'total_duration': 32530.86065387726, 'accumulated_submission_time': 19348.648066282272, 'accumulated_eval_time': 13179.797394990921, 'accumulated_logging_time': 0.6752762794494629}
I0208 15:14:40.207892 140529910601472 logging_writer.py:48] [54810] accumulated_eval_time=13179.797395, accumulated_logging_time=0.675276, accumulated_submission_time=19348.648066, global_step=54810, preemption_count=0, score=19348.648066, test/accuracy=0.679043, test/bleu=28.441838, test/loss=1.498060, test/num_examples=3003, total_duration=32530.860654, train/accuracy=0.645697, train/bleu=31.870851, train/loss=1.713183, validation/accuracy=0.664270, validation/bleu=28.477916, validation/loss=1.571723, validation/num_examples=3000
I0208 15:15:12.131602 140529918994176 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.39255228638648987, loss=1.7455005645751953
I0208 15:15:47.297004 140529910601472 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.5389719009399414, loss=1.7637940645217896
I0208 15:16:22.497292 140529918994176 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.40844497084617615, loss=1.7102197408676147
I0208 15:16:57.737594 140529910601472 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.3882341682910919, loss=1.670163631439209
I0208 15:17:33.027648 140529918994176 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.4205666184425354, loss=1.6732439994812012
I0208 15:18:08.263435 140529910601472 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.4236143231391907, loss=1.7103606462478638
I0208 15:18:43.486607 140529918994176 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.39189398288726807, loss=1.775312900543213
I0208 15:19:18.730838 140529910601472 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.41924890875816345, loss=1.7942241430282593
I0208 15:19:53.982105 140529918994176 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.39214813709259033, loss=1.612373948097229
I0208 15:20:29.233727 140529910601472 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.3723297715187073, loss=1.7091764211654663
I0208 15:21:04.449769 140529918994176 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.3762369453907013, loss=1.6932523250579834
I0208 15:21:39.715223 140529910601472 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.39508962631225586, loss=1.730531096458435
I0208 15:22:14.949571 140529918994176 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.41077157855033875, loss=1.7191195487976074
I0208 15:22:50.178557 140529910601472 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.39105820655822754, loss=1.7795647382736206
I0208 15:23:25.433100 140529918994176 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.44367459416389465, loss=1.7491711378097534
I0208 15:24:00.672109 140529910601472 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.3759327530860901, loss=1.7027027606964111
I0208 15:24:35.901994 140529918994176 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.4165610074996948, loss=1.7571481466293335
I0208 15:25:11.180650 140529910601472 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.39991846680641174, loss=1.7411912679672241
I0208 15:25:46.435329 140529918994176 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.4591255187988281, loss=1.6386774778366089
I0208 15:26:21.672869 140529910601472 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.40330198407173157, loss=1.6892290115356445
I0208 15:26:56.923039 140529918994176 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.3601677417755127, loss=1.638179898262024
I0208 15:27:32.147181 140529910601472 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.39236244559288025, loss=1.584999918937683
I0208 15:28:07.385052 140529918994176 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.4155564606189728, loss=1.790682315826416
I0208 15:28:40.261137 140699726837568 spec.py:321] Evaluating on the training split.
I0208 15:28:43.307812 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 15:32:04.154378 140699726837568 spec.py:333] Evaluating on the validation split.
I0208 15:32:06.885195 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 15:34:34.253453 140699726837568 spec.py:349] Evaluating on the test split.
I0208 15:34:36.989409 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 15:37:01.520006 140699726837568 submission_runner.py:408] Time since start: 33872.20s, 	Step: 57195, 	{'train/accuracy': 0.6577078104019165, 'train/loss': 1.6320240497589111, 'train/bleu': 32.78105983753737, 'validation/accuracy': 0.667592465877533, 'validation/loss': 1.5603337287902832, 'validation/bleu': 29.03892667850046, 'validation/num_examples': 3000, 'test/accuracy': 0.6807158589363098, 'test/loss': 1.4926679134368896, 'test/bleu': 28.451057156751197, 'test/num_examples': 3003, 'score': 20188.616775512695, 'total_duration': 33872.19545149803, 'accumulated_submission_time': 20188.616775512695, 'accumulated_eval_time': 13681.056218147278, 'accumulated_logging_time': 0.7081527709960938}
I0208 15:37:01.542700 140529910601472 logging_writer.py:48] [57195] accumulated_eval_time=13681.056218, accumulated_logging_time=0.708153, accumulated_submission_time=20188.616776, global_step=57195, preemption_count=0, score=20188.616776, test/accuracy=0.680716, test/bleu=28.451057, test/loss=1.492668, test/num_examples=3003, total_duration=33872.195451, train/accuracy=0.657708, train/bleu=32.781060, train/loss=1.632024, validation/accuracy=0.667592, validation/bleu=29.038927, validation/loss=1.560334, validation/num_examples=3000
I0208 15:37:03.656203 140529918994176 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.4210152328014374, loss=1.7473680973052979
I0208 15:37:38.764321 140529910601472 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.3817167282104492, loss=1.7336030006408691
I0208 15:38:13.973222 140529918994176 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.3777616620063782, loss=1.681965708732605
I0208 15:38:49.190860 140529910601472 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.4006555378437042, loss=1.6857661008834839
I0208 15:39:24.398476 140529918994176 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.40377578139305115, loss=1.652121901512146
I0208 15:39:59.611225 140529910601472 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.400805801153183, loss=1.6839625835418701
I0208 15:40:34.848613 140529918994176 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.47830650210380554, loss=1.6473689079284668
I0208 15:41:10.077285 140529910601472 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.3763432800769806, loss=1.754425048828125
I0208 15:41:45.311142 140529918994176 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.39037543535232544, loss=1.7813886404037476
I0208 15:42:20.575500 140529910601472 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.3943808972835541, loss=1.7170709371566772
I0208 15:42:55.846848 140529918994176 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.39612361788749695, loss=1.685719609260559
I0208 15:43:31.092692 140529910601472 logging_writer.py:48] [58300] global_step=58300, grad_norm=0.43548959493637085, loss=1.6468790769577026
I0208 15:44:06.328618 140529918994176 logging_writer.py:48] [58400] global_step=58400, grad_norm=0.4067528247833252, loss=1.7244373559951782
I0208 15:44:41.572028 140529910601472 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.4008857309818268, loss=1.7109184265136719
I0208 15:45:16.793923 140529918994176 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.4339190721511841, loss=1.672890305519104
I0208 15:45:52.028677 140529910601472 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.39194485545158386, loss=1.694252848625183
I0208 15:46:27.372641 140529918994176 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.39340680837631226, loss=1.6813722848892212
I0208 15:47:02.666712 140529910601472 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.40849292278289795, loss=1.8083816766738892
I0208 15:47:37.885902 140529918994176 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.397958904504776, loss=1.689023733139038
I0208 15:48:13.122776 140529910601472 logging_writer.py:48] [59100] global_step=59100, grad_norm=0.36941584944725037, loss=1.6423540115356445
I0208 15:48:48.360887 140529918994176 logging_writer.py:48] [59200] global_step=59200, grad_norm=0.4308139979839325, loss=1.7820250988006592
I0208 15:49:23.649379 140529910601472 logging_writer.py:48] [59300] global_step=59300, grad_norm=0.4104001224040985, loss=1.7047970294952393
I0208 15:49:58.938311 140529918994176 logging_writer.py:48] [59400] global_step=59400, grad_norm=0.395256906747818, loss=1.712013840675354
I0208 15:50:34.214209 140529910601472 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.38630211353302, loss=1.6675554513931274
I0208 15:51:01.758830 140699726837568 spec.py:321] Evaluating on the training split.
I0208 15:51:04.789149 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 15:55:08.970345 140699726837568 spec.py:333] Evaluating on the validation split.
I0208 15:55:11.702494 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 15:58:06.123816 140699726837568 spec.py:349] Evaluating on the test split.
I0208 15:58:08.852000 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 16:00:45.441089 140699726837568 submission_runner.py:408] Time since start: 35296.12s, 	Step: 59580, 	{'train/accuracy': 0.6465297937393188, 'train/loss': 1.694580316543579, 'train/bleu': 32.05305332294858, 'validation/accuracy': 0.6670469045639038, 'validation/loss': 1.5555315017700195, 'validation/bleu': 28.62439054702784, 'validation/num_examples': 3000, 'test/accuracy': 0.6815292835235596, 'test/loss': 1.4816378355026245, 'test/bleu': 28.475886821190702, 'test/num_examples': 3003, 'score': 21028.747178077698, 'total_duration': 35296.116545677185, 'accumulated_submission_time': 21028.747178077698, 'accumulated_eval_time': 14264.738429784775, 'accumulated_logging_time': 0.74080491065979}
I0208 16:00:45.463826 140529918994176 logging_writer.py:48] [59580] accumulated_eval_time=14264.738430, accumulated_logging_time=0.740805, accumulated_submission_time=21028.747178, global_step=59580, preemption_count=0, score=21028.747178, test/accuracy=0.681529, test/bleu=28.475887, test/loss=1.481638, test/num_examples=3003, total_duration=35296.116546, train/accuracy=0.646530, train/bleu=32.053053, train/loss=1.694580, validation/accuracy=0.667047, validation/bleu=28.624391, validation/loss=1.555532, validation/num_examples=3000
I0208 16:00:52.832597 140529910601472 logging_writer.py:48] [59600] global_step=59600, grad_norm=0.46722444891929626, loss=1.7587064504623413
I0208 16:01:27.943683 140529918994176 logging_writer.py:48] [59700] global_step=59700, grad_norm=0.3817829489707947, loss=1.6651424169540405
I0208 16:02:03.132868 140529910601472 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.4004758298397064, loss=1.6430637836456299
I0208 16:02:38.388405 140529918994176 logging_writer.py:48] [59900] global_step=59900, grad_norm=0.40909940004348755, loss=1.6534805297851562
I0208 16:03:13.592305 140529910601472 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.41774821281433105, loss=1.7486165761947632
I0208 16:03:48.802044 140529918994176 logging_writer.py:48] [60100] global_step=60100, grad_norm=0.4082849323749542, loss=1.6485220193862915
I0208 16:04:24.034029 140529910601472 logging_writer.py:48] [60200] global_step=60200, grad_norm=0.4164596199989319, loss=1.735779047012329
I0208 16:04:59.269209 140529918994176 logging_writer.py:48] [60300] global_step=60300, grad_norm=0.910908579826355, loss=1.7031161785125732
I0208 16:05:34.513194 140529910601472 logging_writer.py:48] [60400] global_step=60400, grad_norm=0.37085795402526855, loss=1.6851726770401
I0208 16:06:09.804942 140529918994176 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.41794607043266296, loss=1.6980968713760376
I0208 16:06:45.104915 140529910601472 logging_writer.py:48] [60600] global_step=60600, grad_norm=0.41697052121162415, loss=1.691159963607788
I0208 16:07:20.357923 140529918994176 logging_writer.py:48] [60700] global_step=60700, grad_norm=0.39836299419403076, loss=1.6579948663711548
I0208 16:07:55.628600 140529910601472 logging_writer.py:48] [60800] global_step=60800, grad_norm=0.42972785234451294, loss=1.717299222946167
I0208 16:08:30.896902 140529918994176 logging_writer.py:48] [60900] global_step=60900, grad_norm=0.4484092891216278, loss=1.6419854164123535
I0208 16:09:06.159474 140529910601472 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.4260678291320801, loss=1.6665711402893066
I0208 16:09:41.403347 140529918994176 logging_writer.py:48] [61100] global_step=61100, grad_norm=0.4114052355289459, loss=1.7234063148498535
I0208 16:10:16.654119 140529910601472 logging_writer.py:48] [61200] global_step=61200, grad_norm=0.4962393045425415, loss=1.754760980606079
I0208 16:10:51.931582 140529918994176 logging_writer.py:48] [61300] global_step=61300, grad_norm=0.420726478099823, loss=1.670082688331604
I0208 16:11:27.159689 140529910601472 logging_writer.py:48] [61400] global_step=61400, grad_norm=0.40158137679100037, loss=1.6704761981964111
I0208 16:12:02.392646 140529918994176 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.41234058141708374, loss=1.6563091278076172
I0208 16:12:37.607007 140529910601472 logging_writer.py:48] [61600] global_step=61600, grad_norm=0.4409598708152771, loss=1.6234657764434814
I0208 16:13:12.840985 140529918994176 logging_writer.py:48] [61700] global_step=61700, grad_norm=0.38851985335350037, loss=1.6922744512557983
I0208 16:13:48.131278 140529910601472 logging_writer.py:48] [61800] global_step=61800, grad_norm=0.4081873595714569, loss=1.6906810998916626
I0208 16:14:23.379305 140529918994176 logging_writer.py:48] [61900] global_step=61900, grad_norm=0.40358614921569824, loss=1.744318962097168
I0208 16:14:45.625561 140699726837568 spec.py:321] Evaluating on the training split.
I0208 16:14:48.658308 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 16:18:00.817301 140699726837568 spec.py:333] Evaluating on the validation split.
I0208 16:18:03.561344 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 16:20:24.222559 140699726837568 spec.py:349] Evaluating on the test split.
I0208 16:20:26.948576 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 16:22:55.076396 140699726837568 submission_runner.py:408] Time since start: 36625.75s, 	Step: 61965, 	{'train/accuracy': 0.648226261138916, 'train/loss': 1.7013450860977173, 'train/bleu': 32.419402937612425, 'validation/accuracy': 0.6691423654556274, 'validation/loss': 1.5469614267349243, 'validation/bleu': 29.033741216883577, 'validation/num_examples': 3000, 'test/accuracy': 0.6838417649269104, 'test/loss': 1.4731414318084717, 'test/bleu': 28.84332825064846, 'test/num_examples': 3003, 'score': 21868.823503017426, 'total_duration': 36625.751838207245, 'accumulated_submission_time': 21868.823503017426, 'accumulated_eval_time': 14754.18920135498, 'accumulated_logging_time': 0.7733290195465088}
I0208 16:22:55.099914 140529910601472 logging_writer.py:48] [61965] accumulated_eval_time=14754.189201, accumulated_logging_time=0.773329, accumulated_submission_time=21868.823503, global_step=61965, preemption_count=0, score=21868.823503, test/accuracy=0.683842, test/bleu=28.843328, test/loss=1.473141, test/num_examples=3003, total_duration=36625.751838, train/accuracy=0.648226, train/bleu=32.419403, train/loss=1.701345, validation/accuracy=0.669142, validation/bleu=29.033741, validation/loss=1.546961, validation/num_examples=3000
I0208 16:23:07.743417 140529918994176 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.4238249361515045, loss=1.7532994747161865
I0208 16:23:42.925661 140529910601472 logging_writer.py:48] [62100] global_step=62100, grad_norm=0.43296018242836, loss=1.6724858283996582
I0208 16:24:18.151142 140529918994176 logging_writer.py:48] [62200] global_step=62200, grad_norm=0.39774343371391296, loss=1.715172529220581
I0208 16:24:53.364410 140529910601472 logging_writer.py:48] [62300] global_step=62300, grad_norm=0.37586504220962524, loss=1.623454213142395
I0208 16:25:28.600504 140529918994176 logging_writer.py:48] [62400] global_step=62400, grad_norm=0.37363627552986145, loss=1.6916855573654175
I0208 16:26:03.889871 140529910601472 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.40258923172950745, loss=1.7443511486053467
I0208 16:26:39.134101 140529918994176 logging_writer.py:48] [62600] global_step=62600, grad_norm=0.40606945753097534, loss=1.7611979246139526
I0208 16:27:14.361947 140529910601472 logging_writer.py:48] [62700] global_step=62700, grad_norm=0.45024701952934265, loss=1.7562236785888672
I0208 16:27:49.610325 140529918994176 logging_writer.py:48] [62800] global_step=62800, grad_norm=0.3977983891963959, loss=1.6767404079437256
I0208 16:28:24.968203 140529910601472 logging_writer.py:48] [62900] global_step=62900, grad_norm=0.42503300309181213, loss=1.747700810432434
I0208 16:29:00.284435 140529918994176 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.3906104564666748, loss=1.7260063886642456
I0208 16:29:35.537456 140529910601472 logging_writer.py:48] [63100] global_step=63100, grad_norm=0.3772525191307068, loss=1.7080315351486206
I0208 16:30:10.793299 140529918994176 logging_writer.py:48] [63200] global_step=63200, grad_norm=0.43060731887817383, loss=1.718674898147583
I0208 16:30:46.098901 140529910601472 logging_writer.py:48] [63300] global_step=63300, grad_norm=0.5071884989738464, loss=1.73133385181427
I0208 16:31:21.434503 140529918994176 logging_writer.py:48] [63400] global_step=63400, grad_norm=0.3879052698612213, loss=1.6360598802566528
I0208 16:31:56.698328 140529910601472 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.4072185754776001, loss=1.6202443838119507
I0208 16:32:31.910161 140529918994176 logging_writer.py:48] [63600] global_step=63600, grad_norm=0.3840607702732086, loss=1.6498125791549683
I0208 16:33:07.163271 140529910601472 logging_writer.py:48] [63700] global_step=63700, grad_norm=0.40774017572402954, loss=1.68599534034729
I0208 16:33:42.402844 140529918994176 logging_writer.py:48] [63800] global_step=63800, grad_norm=0.4085771441459656, loss=1.7256295680999756
I0208 16:34:17.664780 140529910601472 logging_writer.py:48] [63900] global_step=63900, grad_norm=0.3938925266265869, loss=1.7456393241882324
I0208 16:34:52.906290 140529918994176 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.3603692650794983, loss=1.609867811203003
I0208 16:35:28.144545 140529910601472 logging_writer.py:48] [64100] global_step=64100, grad_norm=0.4044848084449768, loss=1.7013742923736572
I0208 16:36:03.384799 140529918994176 logging_writer.py:48] [64200] global_step=64200, grad_norm=0.43843260407447815, loss=1.7597860097885132
I0208 16:36:38.622312 140529910601472 logging_writer.py:48] [64300] global_step=64300, grad_norm=0.40323248505592346, loss=1.6821990013122559
I0208 16:36:55.270378 140699726837568 spec.py:321] Evaluating on the training split.
I0208 16:36:58.298152 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 16:40:20.720646 140699726837568 spec.py:333] Evaluating on the validation split.
I0208 16:40:23.446816 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 16:42:52.882689 140699726837568 spec.py:349] Evaluating on the test split.
I0208 16:42:55.640972 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 16:45:25.458673 140699726837568 submission_runner.py:408] Time since start: 37976.13s, 	Step: 64349, 	{'train/accuracy': 0.6545460820198059, 'train/loss': 1.6462438106536865, 'train/bleu': 32.617605237832414, 'validation/accuracy': 0.6699978709220886, 'validation/loss': 1.5386850833892822, 'validation/bleu': 29.007868109997457, 'validation/num_examples': 3000, 'test/accuracy': 0.6819010972976685, 'test/loss': 1.4711955785751343, 'test/bleu': 28.584893070290796, 'test/num_examples': 3003, 'score': 22708.90673828125, 'total_duration': 37976.13408732414, 'accumulated_submission_time': 22708.90673828125, 'accumulated_eval_time': 15264.377411842346, 'accumulated_logging_time': 0.8070766925811768}
I0208 16:45:25.487808 140529918994176 logging_writer.py:48] [64349] accumulated_eval_time=15264.377412, accumulated_logging_time=0.807077, accumulated_submission_time=22708.906738, global_step=64349, preemption_count=0, score=22708.906738, test/accuracy=0.681901, test/bleu=28.584893, test/loss=1.471196, test/num_examples=3003, total_duration=37976.134087, train/accuracy=0.654546, train/bleu=32.617605, train/loss=1.646244, validation/accuracy=0.669998, validation/bleu=29.007868, validation/loss=1.538685, validation/num_examples=3000
I0208 16:45:43.769589 140529910601472 logging_writer.py:48] [64400] global_step=64400, grad_norm=0.3987776041030884, loss=1.6695961952209473
I0208 16:46:18.921312 140529918994176 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.4478307068347931, loss=1.7644292116165161
I0208 16:46:54.087550 140529910601472 logging_writer.py:48] [64600] global_step=64600, grad_norm=0.37377962470054626, loss=1.654516339302063
I0208 16:47:29.388381 140529918994176 logging_writer.py:48] [64700] global_step=64700, grad_norm=0.4188652038574219, loss=1.5980006456375122
I0208 16:48:04.626550 140529910601472 logging_writer.py:48] [64800] global_step=64800, grad_norm=0.39417773485183716, loss=1.6925207376480103
I0208 16:48:39.896657 140529918994176 logging_writer.py:48] [64900] global_step=64900, grad_norm=0.3883163034915924, loss=1.76975679397583
I0208 16:49:15.148219 140529910601472 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.44365161657333374, loss=1.6397749185562134
I0208 16:49:50.407964 140529918994176 logging_writer.py:48] [65100] global_step=65100, grad_norm=0.40598973631858826, loss=1.6542749404907227
I0208 16:50:25.652469 140529910601472 logging_writer.py:48] [65200] global_step=65200, grad_norm=0.40324121713638306, loss=1.6887831687927246
I0208 16:51:00.914301 140529918994176 logging_writer.py:48] [65300] global_step=65300, grad_norm=0.42257562279701233, loss=1.780354380607605
I0208 16:51:36.180664 140529910601472 logging_writer.py:48] [65400] global_step=65400, grad_norm=0.434876948595047, loss=1.6385343074798584
I0208 16:52:11.427948 140529918994176 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.4928438067436218, loss=1.6663016080856323
I0208 16:52:46.682687 140529910601472 logging_writer.py:48] [65600] global_step=65600, grad_norm=0.41058817505836487, loss=1.6071580648422241
I0208 16:53:21.982515 140529918994176 logging_writer.py:48] [65700] global_step=65700, grad_norm=0.40005579590797424, loss=1.6981186866760254
I0208 16:53:57.221506 140529910601472 logging_writer.py:48] [65800] global_step=65800, grad_norm=0.4430157542228699, loss=1.6423100233078003
I0208 16:54:32.465891 140529918994176 logging_writer.py:48] [65900] global_step=65900, grad_norm=0.4308457672595978, loss=1.7308152914047241
I0208 16:55:07.728773 140529910601472 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.4443458616733551, loss=1.7002606391906738
I0208 16:55:42.991608 140529918994176 logging_writer.py:48] [66100] global_step=66100, grad_norm=0.4138660132884979, loss=1.689122200012207
I0208 16:56:18.265501 140529910601472 logging_writer.py:48] [66200] global_step=66200, grad_norm=0.4290026128292084, loss=1.5969382524490356
I0208 16:56:53.512978 140529918994176 logging_writer.py:48] [66300] global_step=66300, grad_norm=0.42687535285949707, loss=1.616607666015625
I0208 16:57:28.778680 140529910601472 logging_writer.py:48] [66400] global_step=66400, grad_norm=0.3942018151283264, loss=1.6371935606002808
I0208 16:58:03.996037 140529918994176 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.4291599988937378, loss=1.7366448640823364
I0208 16:58:39.227718 140529910601472 logging_writer.py:48] [66600] global_step=66600, grad_norm=0.38764142990112305, loss=1.7461084127426147
I0208 16:59:14.508376 140529918994176 logging_writer.py:48] [66700] global_step=66700, grad_norm=0.3782773017883301, loss=1.6576274633407593
I0208 16:59:25.514569 140699726837568 spec.py:321] Evaluating on the training split.
I0208 16:59:28.546683 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 17:02:20.020895 140699726837568 spec.py:333] Evaluating on the validation split.
I0208 17:02:22.738285 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 17:04:55.440656 140699726837568 spec.py:349] Evaluating on the test split.
I0208 17:04:58.170570 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 17:07:19.647249 140699726837568 submission_runner.py:408] Time since start: 39290.32s, 	Step: 66733, 	{'train/accuracy': 0.6512013673782349, 'train/loss': 1.6751712560653687, 'train/bleu': 32.131119778833856, 'validation/accuracy': 0.6714361906051636, 'validation/loss': 1.5344096422195435, 'validation/bleu': 29.08472915961638, 'validation/num_examples': 3000, 'test/accuracy': 0.6844227910041809, 'test/loss': 1.462852954864502, 'test/bleu': 28.940553718862866, 'test/num_examples': 3003, 'score': 23548.84869503975, 'total_duration': 39290.32270479202, 'accumulated_submission_time': 23548.84869503975, 'accumulated_eval_time': 15738.510041713715, 'accumulated_logging_time': 0.8472380638122559}
I0208 17:07:19.671954 140529910601472 logging_writer.py:48] [66733] accumulated_eval_time=15738.510042, accumulated_logging_time=0.847238, accumulated_submission_time=23548.848695, global_step=66733, preemption_count=0, score=23548.848695, test/accuracy=0.684423, test/bleu=28.940554, test/loss=1.462853, test/num_examples=3003, total_duration=39290.322705, train/accuracy=0.651201, train/bleu=32.131120, train/loss=1.675171, validation/accuracy=0.671436, validation/bleu=29.084729, validation/loss=1.534410, validation/num_examples=3000
I0208 17:07:43.531694 140529918994176 logging_writer.py:48] [66800] global_step=66800, grad_norm=0.39769306778907776, loss=1.6995962858200073
I0208 17:08:18.705684 140529910601472 logging_writer.py:48] [66900] global_step=66900, grad_norm=0.3905305862426758, loss=1.6358914375305176
I0208 17:08:54.056945 140529918994176 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.37563249468803406, loss=1.663804054260254
I0208 17:09:29.298823 140529910601472 logging_writer.py:48] [67100] global_step=67100, grad_norm=0.386269211769104, loss=1.6955060958862305
I0208 17:10:04.530343 140529918994176 logging_writer.py:48] [67200] global_step=67200, grad_norm=0.42820945382118225, loss=1.7706044912338257
I0208 17:10:39.762490 140529910601472 logging_writer.py:48] [67300] global_step=67300, grad_norm=0.3891615569591522, loss=1.6630885601043701
I0208 17:11:15.012579 140529918994176 logging_writer.py:48] [67400] global_step=67400, grad_norm=0.38377994298934937, loss=1.6321766376495361
I0208 17:11:50.279599 140529910601472 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.3958103656768799, loss=1.6725094318389893
I0208 17:12:25.518150 140529918994176 logging_writer.py:48] [67600] global_step=67600, grad_norm=0.4216112196445465, loss=1.6892691850662231
I0208 17:13:00.735392 140529910601472 logging_writer.py:48] [67700] global_step=67700, grad_norm=0.40322285890579224, loss=1.688474416732788
I0208 17:13:36.013888 140529918994176 logging_writer.py:48] [67800] global_step=67800, grad_norm=0.3920643627643585, loss=1.6920915842056274
I0208 17:14:11.296770 140529910601472 logging_writer.py:48] [67900] global_step=67900, grad_norm=0.4000872075557709, loss=1.6005975008010864
I0208 17:14:46.657886 140529918994176 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.4061201214790344, loss=1.755308985710144
I0208 17:15:21.883350 140529910601472 logging_writer.py:48] [68100] global_step=68100, grad_norm=0.40887755155563354, loss=1.6514896154403687
I0208 17:15:57.176367 140529918994176 logging_writer.py:48] [68200] global_step=68200, grad_norm=0.42125120759010315, loss=1.6741913557052612
I0208 17:16:32.492424 140529910601472 logging_writer.py:48] [68300] global_step=68300, grad_norm=0.3936779499053955, loss=1.7149564027786255
I0208 17:17:07.718036 140529918994176 logging_writer.py:48] [68400] global_step=68400, grad_norm=0.4238375425338745, loss=1.7753245830535889
I0208 17:17:42.977204 140529910601472 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.38653627038002014, loss=1.672739028930664
I0208 17:18:18.280130 140529918994176 logging_writer.py:48] [68600] global_step=68600, grad_norm=0.40674421191215515, loss=1.5995252132415771
I0208 17:18:53.540056 140529910601472 logging_writer.py:48] [68700] global_step=68700, grad_norm=0.41079407930374146, loss=1.6256643533706665
I0208 17:19:28.768330 140529918994176 logging_writer.py:48] [68800] global_step=68800, grad_norm=0.415928453207016, loss=1.7434523105621338
I0208 17:20:04.003938 140529910601472 logging_writer.py:48] [68900] global_step=68900, grad_norm=0.42945441603660583, loss=1.7063336372375488
I0208 17:20:39.221614 140529918994176 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.4176614284515381, loss=1.6720541715621948
I0208 17:21:14.467732 140529910601472 logging_writer.py:48] [69100] global_step=69100, grad_norm=0.44280505180358887, loss=1.7253451347351074
I0208 17:21:19.827669 140699726837568 spec.py:321] Evaluating on the training split.
I0208 17:21:22.857670 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 17:24:51.952623 140699726837568 spec.py:333] Evaluating on the validation split.
I0208 17:24:54.682711 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 17:27:45.208690 140699726837568 spec.py:349] Evaluating on the test split.
I0208 17:27:47.950839 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 17:30:21.424178 140699726837568 submission_runner.py:408] Time since start: 40672.10s, 	Step: 69117, 	{'train/accuracy': 0.6723154783248901, 'train/loss': 1.5345184803009033, 'train/bleu': 33.717381365500145, 'validation/accuracy': 0.6729241013526917, 'validation/loss': 1.5261874198913574, 'validation/bleu': 29.173103041104703, 'validation/num_examples': 3000, 'test/accuracy': 0.6858404874801636, 'test/loss': 1.4534401893615723, 'test/bleu': 29.00716191550685, 'test/num_examples': 3003, 'score': 24388.915120363235, 'total_duration': 40672.09962892532, 'accumulated_submission_time': 24388.915120363235, 'accumulated_eval_time': 16280.106495141983, 'accumulated_logging_time': 0.8839507102966309}
I0208 17:30:21.449141 140529918994176 logging_writer.py:48] [69117] accumulated_eval_time=16280.106495, accumulated_logging_time=0.883951, accumulated_submission_time=24388.915120, global_step=69117, preemption_count=0, score=24388.915120, test/accuracy=0.685840, test/bleu=29.007162, test/loss=1.453440, test/num_examples=3003, total_duration=40672.099629, train/accuracy=0.672315, train/bleu=33.717381, train/loss=1.534518, validation/accuracy=0.672924, validation/bleu=29.173103, validation/loss=1.526187, validation/num_examples=3000
I0208 17:30:50.974555 140529910601472 logging_writer.py:48] [69200] global_step=69200, grad_norm=0.38564345240592957, loss=1.7016748189926147
I0208 17:31:26.150553 140529918994176 logging_writer.py:48] [69300] global_step=69300, grad_norm=0.3989812731742859, loss=1.7545305490493774
I0208 17:32:01.368126 140529910601472 logging_writer.py:48] [69400] global_step=69400, grad_norm=0.3977412283420563, loss=1.6158827543258667
I0208 17:32:36.611989 140529918994176 logging_writer.py:48] [69500] global_step=69500, grad_norm=0.4131472408771515, loss=1.6558589935302734
I0208 17:33:11.865750 140529910601472 logging_writer.py:48] [69600] global_step=69600, grad_norm=0.4290371537208557, loss=1.667789340019226
I0208 17:33:47.164142 140529918994176 logging_writer.py:48] [69700] global_step=69700, grad_norm=0.4114138185977936, loss=1.6663031578063965
I0208 17:34:22.405257 140529910601472 logging_writer.py:48] [69800] global_step=69800, grad_norm=0.41499125957489014, loss=1.652334451675415
I0208 17:34:57.629683 140529918994176 logging_writer.py:48] [69900] global_step=69900, grad_norm=0.38832181692123413, loss=1.6380696296691895
I0208 17:35:32.872301 140529910601472 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.3905948996543884, loss=1.665594458580017
I0208 17:36:08.106683 140529918994176 logging_writer.py:48] [70100] global_step=70100, grad_norm=0.399460107088089, loss=1.6672687530517578
I0208 17:36:43.373327 140529910601472 logging_writer.py:48] [70200] global_step=70200, grad_norm=0.4574301540851593, loss=1.6194922924041748
I0208 17:37:18.654973 140529918994176 logging_writer.py:48] [70300] global_step=70300, grad_norm=0.411014199256897, loss=1.6257703304290771
I0208 17:37:53.901851 140529910601472 logging_writer.py:48] [70400] global_step=70400, grad_norm=0.4098276197910309, loss=1.6720671653747559
I0208 17:38:29.146593 140529918994176 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.41276857256889343, loss=1.6285147666931152
I0208 17:39:04.387448 140529910601472 logging_writer.py:48] [70600] global_step=70600, grad_norm=0.36266660690307617, loss=1.6746083498001099
I0208 17:39:39.644182 140529918994176 logging_writer.py:48] [70700] global_step=70700, grad_norm=0.40317413210868835, loss=1.6558538675308228
I0208 17:40:14.876103 140529910601472 logging_writer.py:48] [70800] global_step=70800, grad_norm=0.41226983070373535, loss=1.6342360973358154
I0208 17:40:50.125843 140529918994176 logging_writer.py:48] [70900] global_step=70900, grad_norm=0.4082426130771637, loss=1.6387112140655518
I0208 17:41:25.384477 140529910601472 logging_writer.py:48] [71000] global_step=71000, grad_norm=0.4111190140247345, loss=1.6117520332336426
I0208 17:42:00.644940 140529918994176 logging_writer.py:48] [71100] global_step=71100, grad_norm=0.4411954879760742, loss=1.6441277265548706
I0208 17:42:35.879958 140529910601472 logging_writer.py:48] [71200] global_step=71200, grad_norm=0.43548446893692017, loss=1.6472654342651367
I0208 17:43:11.099504 140529918994176 logging_writer.py:48] [71300] global_step=71300, grad_norm=0.40452679991722107, loss=1.666492223739624
I0208 17:43:46.373498 140529910601472 logging_writer.py:48] [71400] global_step=71400, grad_norm=0.4202430248260498, loss=1.6844348907470703
I0208 17:44:21.608652 140529918994176 logging_writer.py:48] [71500] global_step=71500, grad_norm=0.3872433006763458, loss=1.6725107431411743
I0208 17:44:21.615696 140699726837568 spec.py:321] Evaluating on the training split.
I0208 17:44:24.355657 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 17:48:25.495300 140699726837568 spec.py:333] Evaluating on the validation split.
I0208 17:48:28.230332 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 17:51:00.497959 140699726837568 spec.py:349] Evaluating on the test split.
I0208 17:51:03.245104 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 17:53:27.881566 140699726837568 submission_runner.py:408] Time since start: 42058.56s, 	Step: 71501, 	{'train/accuracy': 0.65537029504776, 'train/loss': 1.6473228931427002, 'train/bleu': 32.375148555377464, 'validation/accuracy': 0.6737300157546997, 'validation/loss': 1.5176098346710205, 'validation/bleu': 29.287501929283852, 'validation/num_examples': 3000, 'test/accuracy': 0.685910165309906, 'test/loss': 1.4481548070907593, 'test/bleu': 28.593936374173712, 'test/num_examples': 3003, 'score': 25228.996363401413, 'total_duration': 42058.55702161789, 'accumulated_submission_time': 25228.996363401413, 'accumulated_eval_time': 16826.37229347229, 'accumulated_logging_time': 0.9204680919647217}
I0208 17:53:27.907203 140529910601472 logging_writer.py:48] [71501] accumulated_eval_time=16826.372293, accumulated_logging_time=0.920468, accumulated_submission_time=25228.996363, global_step=71501, preemption_count=0, score=25228.996363, test/accuracy=0.685910, test/bleu=28.593936, test/loss=1.448155, test/num_examples=3003, total_duration=42058.557022, train/accuracy=0.655370, train/bleu=32.375149, train/loss=1.647323, validation/accuracy=0.673730, validation/bleu=29.287502, validation/loss=1.517610, validation/num_examples=3000
I0208 17:54:02.971526 140529918994176 logging_writer.py:48] [71600] global_step=71600, grad_norm=0.41334089636802673, loss=1.6007106304168701
I0208 17:54:38.171227 140529910601472 logging_writer.py:48] [71700] global_step=71700, grad_norm=0.4136166572570801, loss=1.713381052017212
I0208 17:55:13.392241 140529918994176 logging_writer.py:48] [71800] global_step=71800, grad_norm=0.3804048001766205, loss=1.6195857524871826
I0208 17:55:48.614240 140529910601472 logging_writer.py:48] [71900] global_step=71900, grad_norm=0.4192469120025635, loss=1.650627851486206
I0208 17:56:23.829340 140529918994176 logging_writer.py:48] [72000] global_step=72000, grad_norm=0.4021003246307373, loss=1.591185450553894
I0208 17:56:59.065275 140529910601472 logging_writer.py:48] [72100] global_step=72100, grad_norm=0.40439191460609436, loss=1.619789958000183
I0208 17:57:34.302308 140529918994176 logging_writer.py:48] [72200] global_step=72200, grad_norm=0.40579962730407715, loss=1.6386690139770508
I0208 17:58:09.548851 140529910601472 logging_writer.py:48] [72300] global_step=72300, grad_norm=0.4178909659385681, loss=1.6214007139205933
I0208 17:58:44.778535 140529918994176 logging_writer.py:48] [72400] global_step=72400, grad_norm=0.42538970708847046, loss=1.6943224668502808
I0208 17:59:20.060366 140529910601472 logging_writer.py:48] [72500] global_step=72500, grad_norm=0.3948131501674652, loss=1.711002230644226
I0208 17:59:55.340414 140529918994176 logging_writer.py:48] [72600] global_step=72600, grad_norm=0.43058159947395325, loss=1.6290903091430664
I0208 18:00:30.586959 140529910601472 logging_writer.py:48] [72700] global_step=72700, grad_norm=0.4197085201740265, loss=1.5855896472930908
I0208 18:01:05.795892 140529918994176 logging_writer.py:48] [72800] global_step=72800, grad_norm=0.44031333923339844, loss=1.705471158027649
I0208 18:01:41.053850 140529910601472 logging_writer.py:48] [72900] global_step=72900, grad_norm=0.443937748670578, loss=1.7498902082443237
I0208 18:02:16.291918 140529918994176 logging_writer.py:48] [73000] global_step=73000, grad_norm=0.42240509390830994, loss=1.6696048974990845
I0208 18:02:51.541596 140529910601472 logging_writer.py:48] [73100] global_step=73100, grad_norm=0.4091813266277313, loss=1.6414952278137207
I0208 18:03:26.824413 140529918994176 logging_writer.py:48] [73200] global_step=73200, grad_norm=0.4161105453968048, loss=1.6694812774658203
I0208 18:04:02.092080 140529910601472 logging_writer.py:48] [73300] global_step=73300, grad_norm=0.41708821058273315, loss=1.6466012001037598
I0208 18:04:37.370586 140529918994176 logging_writer.py:48] [73400] global_step=73400, grad_norm=0.4226958155632019, loss=1.5999423265457153
I0208 18:05:12.602901 140529910601472 logging_writer.py:48] [73500] global_step=73500, grad_norm=0.38899222016334534, loss=1.6956425905227661
I0208 18:05:47.853871 140529918994176 logging_writer.py:48] [73600] global_step=73600, grad_norm=0.3997476100921631, loss=1.5285345315933228
I0208 18:06:23.125448 140529910601472 logging_writer.py:48] [73700] global_step=73700, grad_norm=0.40322452783584595, loss=1.7354419231414795
I0208 18:06:58.388342 140529918994176 logging_writer.py:48] [73800] global_step=73800, grad_norm=0.3979227840900421, loss=1.6505327224731445
I0208 18:07:28.106457 140699726837568 spec.py:321] Evaluating on the training split.
I0208 18:07:31.131643 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 18:11:14.618104 140699726837568 spec.py:333] Evaluating on the validation split.
I0208 18:11:17.352177 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 18:14:04.071976 140699726837568 spec.py:349] Evaluating on the test split.
I0208 18:14:06.799243 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 18:16:58.061646 140699726837568 submission_runner.py:408] Time since start: 43468.74s, 	Step: 73886, 	{'train/accuracy': 0.6524078845977783, 'train/loss': 1.6667128801345825, 'train/bleu': 32.701466533580295, 'validation/accuracy': 0.6737548112869263, 'validation/loss': 1.5123512744903564, 'validation/bleu': 29.320466587101414, 'validation/num_examples': 3000, 'test/accuracy': 0.6876416206359863, 'test/loss': 1.4385989904403687, 'test/bleu': 28.943381743475623, 'test/num_examples': 3003, 'score': 26069.110144138336, 'total_duration': 43468.737097263336, 'accumulated_submission_time': 26069.110144138336, 'accumulated_eval_time': 17396.327433347702, 'accumulated_logging_time': 0.9564275741577148}
I0208 18:16:58.087887 140529910601472 logging_writer.py:48] [73886] accumulated_eval_time=17396.327433, accumulated_logging_time=0.956428, accumulated_submission_time=26069.110144, global_step=73886, preemption_count=0, score=26069.110144, test/accuracy=0.687642, test/bleu=28.943382, test/loss=1.438599, test/num_examples=3003, total_duration=43468.737097, train/accuracy=0.652408, train/bleu=32.701467, train/loss=1.666713, validation/accuracy=0.673755, validation/bleu=29.320467, validation/loss=1.512351, validation/num_examples=3000
I0208 18:17:03.357701 140529918994176 logging_writer.py:48] [73900] global_step=73900, grad_norm=0.45469698309898376, loss=1.6718202829360962
I0208 18:17:38.463521 140529910601472 logging_writer.py:48] [74000] global_step=74000, grad_norm=0.4254942834377289, loss=1.7332079410552979
I0208 18:18:13.635633 140529918994176 logging_writer.py:48] [74100] global_step=74100, grad_norm=0.4156397879123688, loss=1.6940803527832031
I0208 18:18:49.009207 140529910601472 logging_writer.py:48] [74200] global_step=74200, grad_norm=0.4282184839248657, loss=1.6018198728561401
I0208 18:19:24.284403 140529918994176 logging_writer.py:48] [74300] global_step=74300, grad_norm=0.4170047640800476, loss=1.6751757860183716
I0208 18:19:59.591933 140529910601472 logging_writer.py:48] [74400] global_step=74400, grad_norm=0.409437358379364, loss=1.617661952972412
I0208 18:20:34.868020 140529918994176 logging_writer.py:48] [74500] global_step=74500, grad_norm=0.4411860704421997, loss=1.6316953897476196
I0208 18:21:10.164977 140529910601472 logging_writer.py:48] [74600] global_step=74600, grad_norm=0.4212237298488617, loss=1.62746262550354
I0208 18:21:45.428823 140529918994176 logging_writer.py:48] [74700] global_step=74700, grad_norm=0.3890705108642578, loss=1.670843243598938
I0208 18:22:20.695037 140529910601472 logging_writer.py:48] [74800] global_step=74800, grad_norm=0.42942288517951965, loss=1.6809566020965576
I0208 18:22:55.948188 140529918994176 logging_writer.py:48] [74900] global_step=74900, grad_norm=0.41814446449279785, loss=1.6311140060424805
I0208 18:23:31.177576 140529910601472 logging_writer.py:48] [75000] global_step=75000, grad_norm=0.4354463815689087, loss=1.649916410446167
I0208 18:24:06.447041 140529918994176 logging_writer.py:48] [75100] global_step=75100, grad_norm=0.4019404649734497, loss=1.647778868675232
I0208 18:24:41.717082 140529910601472 logging_writer.py:48] [75200] global_step=75200, grad_norm=0.41490283608436584, loss=1.6423828601837158
I0208 18:25:16.998388 140529918994176 logging_writer.py:48] [75300] global_step=75300, grad_norm=0.3724992871284485, loss=1.6093398332595825
I0208 18:25:52.306643 140529910601472 logging_writer.py:48] [75400] global_step=75400, grad_norm=0.44054746627807617, loss=1.7117406129837036
I0208 18:26:27.567941 140529918994176 logging_writer.py:48] [75500] global_step=75500, grad_norm=0.39612987637519836, loss=1.658895492553711
I0208 18:27:02.821411 140529910601472 logging_writer.py:48] [75600] global_step=75600, grad_norm=0.39392733573913574, loss=1.5673764944076538
I0208 18:27:38.071585 140529918994176 logging_writer.py:48] [75700] global_step=75700, grad_norm=0.392058789730072, loss=1.6730448007583618
I0208 18:28:13.324806 140529910601472 logging_writer.py:48] [75800] global_step=75800, grad_norm=0.3882685601711273, loss=1.609179139137268
I0208 18:28:48.653589 140529918994176 logging_writer.py:48] [75900] global_step=75900, grad_norm=0.4168086647987366, loss=1.7449114322662354
I0208 18:29:23.936857 140529910601472 logging_writer.py:48] [76000] global_step=76000, grad_norm=0.3978739082813263, loss=1.5970969200134277
I0208 18:29:59.176736 140529918994176 logging_writer.py:48] [76100] global_step=76100, grad_norm=0.4049578309059143, loss=1.6164631843566895
I0208 18:30:34.445099 140529910601472 logging_writer.py:48] [76200] global_step=76200, grad_norm=0.4495568573474884, loss=1.6637431383132935
I0208 18:30:58.154469 140699726837568 spec.py:321] Evaluating on the training split.
I0208 18:31:01.200036 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 18:33:55.775148 140699726837568 spec.py:333] Evaluating on the validation split.
I0208 18:33:58.531301 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 18:36:19.700904 140699726837568 spec.py:349] Evaluating on the test split.
I0208 18:36:22.435812 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 18:38:42.945845 140699726837568 submission_runner.py:408] Time since start: 44773.62s, 	Step: 76269, 	{'train/accuracy': 0.6652563810348511, 'train/loss': 1.5872588157653809, 'train/bleu': 33.20367223177718, 'validation/accuracy': 0.6766065955162048, 'validation/loss': 1.50298011302948, 'validation/bleu': 29.703277161865785, 'validation/num_examples': 3000, 'test/accuracy': 0.6899773478507996, 'test/loss': 1.4313315153121948, 'test/bleu': 29.44708155748432, 'test/num_examples': 3003, 'score': 26909.085456848145, 'total_duration': 44773.62126874924, 'accumulated_submission_time': 26909.085456848145, 'accumulated_eval_time': 17861.11874818802, 'accumulated_logging_time': 0.9940569400787354}
I0208 18:38:42.976616 140529918994176 logging_writer.py:48] [76269] accumulated_eval_time=17861.118748, accumulated_logging_time=0.994057, accumulated_submission_time=26909.085457, global_step=76269, preemption_count=0, score=26909.085457, test/accuracy=0.689977, test/bleu=29.447082, test/loss=1.431332, test/num_examples=3003, total_duration=44773.621269, train/accuracy=0.665256, train/bleu=33.203672, train/loss=1.587259, validation/accuracy=0.676607, validation/bleu=29.703277, validation/loss=1.502980, validation/num_examples=3000
I0208 18:38:54.229535 140529910601472 logging_writer.py:48] [76300] global_step=76300, grad_norm=0.4101998805999756, loss=1.6754577159881592
I0208 18:39:29.425513 140529918994176 logging_writer.py:48] [76400] global_step=76400, grad_norm=0.4079809784889221, loss=1.5923163890838623
I0208 18:40:04.640722 140529910601472 logging_writer.py:48] [76500] global_step=76500, grad_norm=0.41089218854904175, loss=1.599957823753357
I0208 18:40:39.846079 140529918994176 logging_writer.py:48] [76600] global_step=76600, grad_norm=0.4296973943710327, loss=1.622307538986206
I0208 18:41:15.089239 140529910601472 logging_writer.py:48] [76700] global_step=76700, grad_norm=0.40874508023262024, loss=1.6794325113296509
I0208 18:41:50.317322 140529918994176 logging_writer.py:48] [76800] global_step=76800, grad_norm=0.41736137866973877, loss=1.5632466077804565
I0208 18:42:25.580786 140529910601472 logging_writer.py:48] [76900] global_step=76900, grad_norm=0.4119093120098114, loss=1.7320460081100464
I0208 18:43:00.832160 140529918994176 logging_writer.py:48] [77000] global_step=77000, grad_norm=0.4295462369918823, loss=1.6296555995941162
I0208 18:43:36.080048 140529910601472 logging_writer.py:48] [77100] global_step=77100, grad_norm=0.41269221901893616, loss=1.6264606714248657
I0208 18:44:11.347614 140529918994176 logging_writer.py:48] [77200] global_step=77200, grad_norm=0.45912981033325195, loss=1.6433098316192627
I0208 18:44:46.597947 140529910601472 logging_writer.py:48] [77300] global_step=77300, grad_norm=0.44450145959854126, loss=1.6507763862609863
I0208 18:45:21.838372 140529918994176 logging_writer.py:48] [77400] global_step=77400, grad_norm=0.4293972849845886, loss=1.6344470977783203
I0208 18:45:57.055507 140529910601472 logging_writer.py:48] [77500] global_step=77500, grad_norm=0.42528602480888367, loss=1.6195398569107056
I0208 18:46:32.312727 140529918994176 logging_writer.py:48] [77600] global_step=77600, grad_norm=0.4293348789215088, loss=1.6478255987167358
I0208 18:47:07.526704 140529910601472 logging_writer.py:48] [77700] global_step=77700, grad_norm=0.4525962471961975, loss=1.6742383241653442
I0208 18:47:42.770926 140529918994176 logging_writer.py:48] [77800] global_step=77800, grad_norm=0.3943322002887726, loss=1.6113862991333008
I0208 18:48:18.012532 140529910601472 logging_writer.py:48] [77900] global_step=77900, grad_norm=0.44876429438591003, loss=1.697325587272644
I0208 18:48:53.257347 140529918994176 logging_writer.py:48] [78000] global_step=78000, grad_norm=0.43177494406700134, loss=1.6189873218536377
I0208 18:49:28.539791 140529910601472 logging_writer.py:48] [78100] global_step=78100, grad_norm=0.38688379526138306, loss=1.6291948556900024
I0208 18:50:03.822325 140529918994176 logging_writer.py:48] [78200] global_step=78200, grad_norm=0.43883228302001953, loss=1.6927392482757568
I0208 18:50:39.063116 140529910601472 logging_writer.py:48] [78300] global_step=78300, grad_norm=0.4312487244606018, loss=1.6846567392349243
I0208 18:51:14.322054 140529918994176 logging_writer.py:48] [78400] global_step=78400, grad_norm=0.40548422932624817, loss=1.662649393081665
I0208 18:51:49.602079 140529910601472 logging_writer.py:48] [78500] global_step=78500, grad_norm=0.3896785378456116, loss=1.6044811010360718
I0208 18:52:24.844350 140529918994176 logging_writer.py:48] [78600] global_step=78600, grad_norm=0.4209200441837311, loss=1.5329399108886719
I0208 18:52:43.253372 140699726837568 spec.py:321] Evaluating on the training split.
I0208 18:52:46.291384 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 18:56:21.430196 140699726837568 spec.py:333] Evaluating on the validation split.
I0208 18:56:24.167216 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 18:59:04.683990 140699726837568 spec.py:349] Evaluating on the test split.
I0208 18:59:07.415372 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 19:01:42.297960 140699726837568 submission_runner.py:408] Time since start: 46152.97s, 	Step: 78654, 	{'train/accuracy': 0.6580626368522644, 'train/loss': 1.6298712491989136, 'train/bleu': 32.67718189408843, 'validation/accuracy': 0.67764812707901, 'validation/loss': 1.4988480806350708, 'validation/bleu': 29.646954785860082, 'validation/num_examples': 3000, 'test/accuracy': 0.6922898292541504, 'test/loss': 1.4208958148956299, 'test/bleu': 29.830781468889445, 'test/num_examples': 3003, 'score': 27749.27845311165, 'total_duration': 46152.97341346741, 'accumulated_submission_time': 27749.27845311165, 'accumulated_eval_time': 18400.163288593292, 'accumulated_logging_time': 1.0356485843658447}
I0208 19:01:42.325731 140529910601472 logging_writer.py:48] [78654] accumulated_eval_time=18400.163289, accumulated_logging_time=1.035649, accumulated_submission_time=27749.278453, global_step=78654, preemption_count=0, score=27749.278453, test/accuracy=0.692290, test/bleu=29.830781, test/loss=1.420896, test/num_examples=3003, total_duration=46152.973413, train/accuracy=0.658063, train/bleu=32.677182, train/loss=1.629871, validation/accuracy=0.677648, validation/bleu=29.646955, validation/loss=1.498848, validation/num_examples=3000
I0208 19:01:58.837847 140529918994176 logging_writer.py:48] [78700] global_step=78700, grad_norm=0.4311155378818512, loss=1.627358317375183
I0208 19:02:33.942939 140529910601472 logging_writer.py:48] [78800] global_step=78800, grad_norm=0.42626065015792847, loss=1.6665995121002197
I0208 19:03:09.129544 140529918994176 logging_writer.py:48] [78900] global_step=78900, grad_norm=0.4026540219783783, loss=1.628494143486023
I0208 19:03:44.396960 140529910601472 logging_writer.py:48] [79000] global_step=79000, grad_norm=0.4836615324020386, loss=1.6211353540420532
I0208 19:04:19.639861 140529918994176 logging_writer.py:48] [79100] global_step=79100, grad_norm=0.43356209993362427, loss=1.5564594268798828
I0208 19:04:54.879981 140529910601472 logging_writer.py:48] [79200] global_step=79200, grad_norm=0.45285025238990784, loss=1.6722979545593262
I0208 19:05:30.125474 140529918994176 logging_writer.py:48] [79300] global_step=79300, grad_norm=0.4270734488964081, loss=1.6127936840057373
I0208 19:06:05.341269 140529910601472 logging_writer.py:48] [79400] global_step=79400, grad_norm=0.4566582441329956, loss=1.6173853874206543
I0208 19:06:40.583698 140529918994176 logging_writer.py:48] [79500] global_step=79500, grad_norm=0.4321177005767822, loss=1.591610312461853
I0208 19:07:15.810757 140529910601472 logging_writer.py:48] [79600] global_step=79600, grad_norm=0.4120740294456482, loss=1.675918459892273
I0208 19:07:51.046673 140529918994176 logging_writer.py:48] [79700] global_step=79700, grad_norm=0.429218590259552, loss=1.6354035139083862
I0208 19:08:26.266829 140529910601472 logging_writer.py:48] [79800] global_step=79800, grad_norm=0.432765394449234, loss=1.623164415359497
I0208 19:09:01.505225 140529918994176 logging_writer.py:48] [79900] global_step=79900, grad_norm=0.4286697506904602, loss=1.6574093103408813
I0208 19:09:36.750392 140529910601472 logging_writer.py:48] [80000] global_step=80000, grad_norm=0.4503777325153351, loss=1.668562650680542
I0208 19:10:12.025341 140529918994176 logging_writer.py:48] [80100] global_step=80100, grad_norm=0.42885616421699524, loss=1.5942729711532593
I0208 19:10:47.302772 140529910601472 logging_writer.py:48] [80200] global_step=80200, grad_norm=0.4331147372722626, loss=1.6590418815612793
I0208 19:11:22.540484 140529918994176 logging_writer.py:48] [80300] global_step=80300, grad_norm=0.4161902666091919, loss=1.6654739379882812
I0208 19:11:57.789359 140529910601472 logging_writer.py:48] [80400] global_step=80400, grad_norm=0.4209502041339874, loss=1.5898244380950928
I0208 19:12:33.013053 140529918994176 logging_writer.py:48] [80500] global_step=80500, grad_norm=0.43752434849739075, loss=1.7393001317977905
I0208 19:13:08.256275 140529910601472 logging_writer.py:48] [80600] global_step=80600, grad_norm=0.4493555724620819, loss=1.5997167825698853
I0208 19:13:43.515233 140529918994176 logging_writer.py:48] [80700] global_step=80700, grad_norm=0.4206463098526001, loss=1.635239601135254
I0208 19:14:18.756474 140529910601472 logging_writer.py:48] [80800] global_step=80800, grad_norm=0.4616546630859375, loss=1.6065644025802612
I0208 19:14:53.987409 140529918994176 logging_writer.py:48] [80900] global_step=80900, grad_norm=0.4264445900917053, loss=1.5368064641952515
I0208 19:15:29.231609 140529910601472 logging_writer.py:48] [81000] global_step=81000, grad_norm=0.42634010314941406, loss=1.7004016637802124
I0208 19:15:42.335099 140699726837568 spec.py:321] Evaluating on the training split.
I0208 19:15:45.360970 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 19:20:13.551316 140699726837568 spec.py:333] Evaluating on the validation split.
I0208 19:20:16.275868 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 19:23:38.617886 140699726837568 spec.py:349] Evaluating on the test split.
I0208 19:23:41.354587 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 19:26:54.200693 140699726837568 submission_runner.py:408] Time since start: 47664.88s, 	Step: 81039, 	{'train/accuracy': 0.6602153778076172, 'train/loss': 1.6250991821289062, 'train/bleu': 32.57357972662718, 'validation/accuracy': 0.6790988445281982, 'validation/loss': 1.4938795566558838, 'validation/bleu': 29.843115183079956, 'validation/num_examples': 3000, 'test/accuracy': 0.6907210946083069, 'test/loss': 1.4177829027175903, 'test/bleu': 29.211380677165277, 'test/num_examples': 3003, 'score': 28589.20459485054, 'total_duration': 47664.87612128258, 'accumulated_submission_time': 28589.20459485054, 'accumulated_eval_time': 19072.028824090958, 'accumulated_logging_time': 1.0736279487609863}
I0208 19:26:54.227402 140529918994176 logging_writer.py:48] [81039] accumulated_eval_time=19072.028824, accumulated_logging_time=1.073628, accumulated_submission_time=28589.204595, global_step=81039, preemption_count=0, score=28589.204595, test/accuracy=0.690721, test/bleu=29.211381, test/loss=1.417783, test/num_examples=3003, total_duration=47664.876121, train/accuracy=0.660215, train/bleu=32.573580, train/loss=1.625099, validation/accuracy=0.679099, validation/bleu=29.843115, validation/loss=1.493880, validation/num_examples=3000
I0208 19:27:15.991061 140529910601472 logging_writer.py:48] [81100] global_step=81100, grad_norm=0.42735153436660767, loss=1.6447659730911255
I0208 19:27:51.133218 140529918994176 logging_writer.py:48] [81200] global_step=81200, grad_norm=0.43354254961013794, loss=1.653631567955017
I0208 19:28:26.361612 140529910601472 logging_writer.py:48] [81300] global_step=81300, grad_norm=0.4261235296726227, loss=1.6276675462722778
I0208 19:29:01.578167 140529918994176 logging_writer.py:48] [81400] global_step=81400, grad_norm=0.43756186962127686, loss=1.6145497560501099
I0208 19:29:36.820626 140529910601472 logging_writer.py:48] [81500] global_step=81500, grad_norm=0.4579389989376068, loss=1.6476597785949707
I0208 19:30:12.068253 140529918994176 logging_writer.py:48] [81600] global_step=81600, grad_norm=0.4285220503807068, loss=1.5959372520446777
I0208 19:30:47.324622 140529910601472 logging_writer.py:48] [81700] global_step=81700, grad_norm=0.43692219257354736, loss=1.6127783060073853
I0208 19:31:22.595194 140529918994176 logging_writer.py:48] [81800] global_step=81800, grad_norm=0.43026453256607056, loss=1.5985661745071411
I0208 19:31:57.833217 140529910601472 logging_writer.py:48] [81900] global_step=81900, grad_norm=0.41362378001213074, loss=1.5836303234100342
I0208 19:32:33.056350 140529918994176 logging_writer.py:48] [82000] global_step=82000, grad_norm=0.4264717102050781, loss=1.6789944171905518
I0208 19:33:08.321950 140529910601472 logging_writer.py:48] [82100] global_step=82100, grad_norm=0.4238024652004242, loss=1.5245894193649292
I0208 19:33:43.634132 140529918994176 logging_writer.py:48] [82200] global_step=82200, grad_norm=0.42030248045921326, loss=1.621830940246582
I0208 19:34:19.011308 140529910601472 logging_writer.py:48] [82300] global_step=82300, grad_norm=0.43159782886505127, loss=1.5831936597824097
I0208 19:34:54.328231 140529918994176 logging_writer.py:48] [82400] global_step=82400, grad_norm=0.4398607313632965, loss=1.6396467685699463
I0208 19:35:29.602827 140529910601472 logging_writer.py:48] [82500] global_step=82500, grad_norm=0.4052289128303528, loss=1.6276333332061768
I0208 19:36:04.857543 140529918994176 logging_writer.py:48] [82600] global_step=82600, grad_norm=0.4712846875190735, loss=1.6960655450820923
I0208 19:36:40.072442 140529910601472 logging_writer.py:48] [82700] global_step=82700, grad_norm=0.41736558079719543, loss=1.6335127353668213
I0208 19:37:15.317148 140529918994176 logging_writer.py:48] [82800] global_step=82800, grad_norm=0.4647584557533264, loss=1.6795240640640259
I0208 19:37:50.541730 140529910601472 logging_writer.py:48] [82900] global_step=82900, grad_norm=0.45192262530326843, loss=1.60574471950531
I0208 19:38:25.836406 140529918994176 logging_writer.py:48] [83000] global_step=83000, grad_norm=0.47083044052124023, loss=1.636708378791809
I0208 19:39:01.146379 140529910601472 logging_writer.py:48] [83100] global_step=83100, grad_norm=0.4131961762905121, loss=1.6034257411956787
I0208 19:39:36.418334 140529918994176 logging_writer.py:48] [83200] global_step=83200, grad_norm=0.44254058599472046, loss=1.6146990060806274
I0208 19:40:11.692470 140529910601472 logging_writer.py:48] [83300] global_step=83300, grad_norm=0.42228975892066956, loss=1.5922229290008545
I0208 19:40:46.946444 140529918994176 logging_writer.py:48] [83400] global_step=83400, grad_norm=0.44026049971580505, loss=1.5988985300064087
I0208 19:40:54.437247 140699726837568 spec.py:321] Evaluating on the training split.
I0208 19:40:57.470063 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 19:44:40.718324 140699726837568 spec.py:333] Evaluating on the validation split.
I0208 19:44:43.452204 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 19:47:21.489058 140699726837568 spec.py:349] Evaluating on the test split.
I0208 19:47:24.227805 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 19:49:59.018630 140699726837568 submission_runner.py:408] Time since start: 49049.69s, 	Step: 83423, 	{'train/accuracy': 0.6681274175643921, 'train/loss': 1.57500159740448, 'train/bleu': 32.9723800626729, 'validation/accuracy': 0.6786648631095886, 'validation/loss': 1.4860962629318237, 'validation/bleu': 29.676601764832707, 'validation/num_examples': 3000, 'test/accuracy': 0.6939747929573059, 'test/loss': 1.405526041984558, 'test/bleu': 29.724564276034794, 'test/num_examples': 3003, 'score': 29429.326312065125, 'total_duration': 49049.69407105446, 'accumulated_submission_time': 29429.326312065125, 'accumulated_eval_time': 19616.61013817787, 'accumulated_logging_time': 1.1113903522491455}
I0208 19:49:59.045384 140529910601472 logging_writer.py:48] [83423] accumulated_eval_time=19616.610138, accumulated_logging_time=1.111390, accumulated_submission_time=29429.326312, global_step=83423, preemption_count=0, score=29429.326312, test/accuracy=0.693975, test/bleu=29.724564, test/loss=1.405526, test/num_examples=3003, total_duration=49049.694071, train/accuracy=0.668127, train/bleu=32.972380, train/loss=1.575002, validation/accuracy=0.678665, validation/bleu=29.676602, validation/loss=1.486096, validation/num_examples=3000
I0208 19:50:26.426365 140529918994176 logging_writer.py:48] [83500] global_step=83500, grad_norm=0.442426860332489, loss=1.5342594385147095
I0208 19:51:01.578702 140529910601472 logging_writer.py:48] [83600] global_step=83600, grad_norm=0.45315343141555786, loss=1.5830552577972412
I0208 19:51:36.852959 140529918994176 logging_writer.py:48] [83700] global_step=83700, grad_norm=0.42660385370254517, loss=1.700464129447937
I0208 19:52:12.078303 140529910601472 logging_writer.py:48] [83800] global_step=83800, grad_norm=0.4274038076400757, loss=1.623023271560669
I0208 19:52:47.304600 140529918994176 logging_writer.py:48] [83900] global_step=83900, grad_norm=0.5106295943260193, loss=1.6191604137420654
I0208 19:53:22.580155 140529910601472 logging_writer.py:48] [84000] global_step=84000, grad_norm=0.45822182297706604, loss=1.6464099884033203
I0208 19:53:57.858464 140529918994176 logging_writer.py:48] [84100] global_step=84100, grad_norm=0.40979984402656555, loss=1.6278818845748901
I0208 19:54:33.101146 140529910601472 logging_writer.py:48] [84200] global_step=84200, grad_norm=0.4891710579395294, loss=1.6375969648361206
I0208 19:55:08.369940 140529918994176 logging_writer.py:48] [84300] global_step=84300, grad_norm=0.41287511587142944, loss=1.540258765220642
I0208 19:55:43.613626 140529910601472 logging_writer.py:48] [84400] global_step=84400, grad_norm=0.43640071153640747, loss=1.6345529556274414
I0208 19:56:18.840884 140529918994176 logging_writer.py:48] [84500] global_step=84500, grad_norm=0.446298211812973, loss=1.5689266920089722
I0208 19:56:54.137385 140529910601472 logging_writer.py:48] [84600] global_step=84600, grad_norm=0.4308429956436157, loss=1.58575439453125
I0208 19:57:29.383035 140529918994176 logging_writer.py:48] [84700] global_step=84700, grad_norm=0.43738695979118347, loss=1.6158987283706665
I0208 19:58:04.604430 140529910601472 logging_writer.py:48] [84800] global_step=84800, grad_norm=0.44856369495391846, loss=1.6078696250915527
I0208 19:58:39.856343 140529918994176 logging_writer.py:48] [84900] global_step=84900, grad_norm=0.4285965859889984, loss=1.6346259117126465
I0208 19:59:15.081058 140529910601472 logging_writer.py:48] [85000] global_step=85000, grad_norm=0.4795916676521301, loss=1.5494204759597778
I0208 19:59:50.367347 140529918994176 logging_writer.py:48] [85100] global_step=85100, grad_norm=0.4384644627571106, loss=1.590513825416565
I0208 20:00:25.619411 140529910601472 logging_writer.py:48] [85200] global_step=85200, grad_norm=0.48141029477119446, loss=1.6093800067901611
I0208 20:01:00.864142 140529918994176 logging_writer.py:48] [85300] global_step=85300, grad_norm=0.4232019782066345, loss=1.5471142530441284
I0208 20:01:36.139809 140529910601472 logging_writer.py:48] [85400] global_step=85400, grad_norm=0.4112039804458618, loss=1.6634188890457153
I0208 20:02:11.384126 140529918994176 logging_writer.py:48] [85500] global_step=85500, grad_norm=0.4882291853427887, loss=1.5549745559692383
I0208 20:02:46.663546 140529910601472 logging_writer.py:48] [85600] global_step=85600, grad_norm=0.44694072008132935, loss=1.5792076587677002
I0208 20:03:21.905049 140529918994176 logging_writer.py:48] [85700] global_step=85700, grad_norm=0.464458703994751, loss=1.6625701189041138
I0208 20:03:57.139504 140529910601472 logging_writer.py:48] [85800] global_step=85800, grad_norm=0.43459513783454895, loss=1.5779556035995483
I0208 20:03:59.334569 140699726837568 spec.py:321] Evaluating on the training split.
I0208 20:04:02.371541 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 20:07:56.784399 140699726837568 spec.py:333] Evaluating on the validation split.
I0208 20:07:59.542495 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 20:10:43.456188 140699726837568 spec.py:349] Evaluating on the test split.
I0208 20:10:46.220538 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 20:13:19.485097 140699726837568 submission_runner.py:408] Time since start: 50450.16s, 	Step: 85808, 	{'train/accuracy': 0.6655429601669312, 'train/loss': 1.5882421731948853, 'train/bleu': 33.31576160365833, 'validation/accuracy': 0.6799171566963196, 'validation/loss': 1.4798542261123657, 'validation/bleu': 29.792160334268157, 'validation/num_examples': 3000, 'test/accuracy': 0.694195568561554, 'test/loss': 1.39683997631073, 'test/bleu': 29.681786035169242, 'test/num_examples': 3003, 'score': 30269.53056025505, 'total_duration': 50450.160507917404, 'accumulated_submission_time': 30269.53056025505, 'accumulated_eval_time': 20176.76056933403, 'accumulated_logging_time': 1.147826910018921}
I0208 20:13:19.517682 140529918994176 logging_writer.py:48] [85808] accumulated_eval_time=20176.760569, accumulated_logging_time=1.147827, accumulated_submission_time=30269.530560, global_step=85808, preemption_count=0, score=30269.530560, test/accuracy=0.694196, test/bleu=29.681786, test/loss=1.396840, test/num_examples=3003, total_duration=50450.160508, train/accuracy=0.665543, train/bleu=33.315762, train/loss=1.588242, validation/accuracy=0.679917, validation/bleu=29.792160, validation/loss=1.479854, validation/num_examples=3000
I0208 20:13:52.164512 140529910601472 logging_writer.py:48] [85900] global_step=85900, grad_norm=0.4441649317741394, loss=1.54447340965271
I0208 20:14:27.296666 140529918994176 logging_writer.py:48] [86000] global_step=86000, grad_norm=0.44504910707473755, loss=1.5985769033432007
I0208 20:15:02.545354 140529910601472 logging_writer.py:48] [86100] global_step=86100, grad_norm=0.44622692465782166, loss=1.5415270328521729
I0208 20:15:37.780126 140529918994176 logging_writer.py:48] [86200] global_step=86200, grad_norm=0.45143863558769226, loss=1.572394609451294
I0208 20:16:13.009273 140529910601472 logging_writer.py:48] [86300] global_step=86300, grad_norm=0.42513415217399597, loss=1.5900259017944336
I0208 20:16:48.225519 140529918994176 logging_writer.py:48] [86400] global_step=86400, grad_norm=0.4388713836669922, loss=1.6114544868469238
I0208 20:17:23.466265 140529910601472 logging_writer.py:48] [86500] global_step=86500, grad_norm=0.44203007221221924, loss=1.6416144371032715
I0208 20:17:58.704475 140529918994176 logging_writer.py:48] [86600] global_step=86600, grad_norm=0.4340129792690277, loss=1.6298396587371826
I0208 20:18:33.942935 140529910601472 logging_writer.py:48] [86700] global_step=86700, grad_norm=0.44590872526168823, loss=1.5786809921264648
I0208 20:19:09.216722 140529918994176 logging_writer.py:48] [86800] global_step=86800, grad_norm=0.4357478618621826, loss=1.5513465404510498
I0208 20:19:44.491334 140529910601472 logging_writer.py:48] [86900] global_step=86900, grad_norm=0.43255361914634705, loss=1.6025186777114868
I0208 20:20:19.781755 140529918994176 logging_writer.py:48] [87000] global_step=87000, grad_norm=0.4477432370185852, loss=1.5017849206924438
I0208 20:20:54.999133 140529910601472 logging_writer.py:48] [87100] global_step=87100, grad_norm=0.4596400558948517, loss=1.478434681892395
I0208 20:21:30.302801 140529918994176 logging_writer.py:48] [87200] global_step=87200, grad_norm=0.42769894003868103, loss=1.5602737665176392
I0208 20:22:05.609901 140529910601472 logging_writer.py:48] [87300] global_step=87300, grad_norm=0.47017306089401245, loss=1.549707293510437
I0208 20:22:40.855010 140529918994176 logging_writer.py:48] [87400] global_step=87400, grad_norm=0.4409976303577423, loss=1.6435389518737793
I0208 20:23:16.082747 140529910601472 logging_writer.py:48] [87500] global_step=87500, grad_norm=0.4814082980155945, loss=1.5222928524017334
I0208 20:23:51.346662 140529918994176 logging_writer.py:48] [87600] global_step=87600, grad_norm=0.48988765478134155, loss=1.5490387678146362
I0208 20:24:26.614563 140529910601472 logging_writer.py:48] [87700] global_step=87700, grad_norm=0.41371428966522217, loss=1.5413528680801392
I0208 20:25:01.873461 140529918994176 logging_writer.py:48] [87800] global_step=87800, grad_norm=0.44910380244255066, loss=1.5976721048355103
I0208 20:25:37.104315 140529910601472 logging_writer.py:48] [87900] global_step=87900, grad_norm=0.46287643909454346, loss=1.57267165184021
I0208 20:26:12.357790 140529918994176 logging_writer.py:48] [88000] global_step=88000, grad_norm=0.43872228264808655, loss=1.5532398223876953
I0208 20:26:47.589646 140529910601472 logging_writer.py:48] [88100] global_step=88100, grad_norm=0.44110289216041565, loss=1.557236671447754
I0208 20:27:19.729691 140699726837568 spec.py:321] Evaluating on the training split.
I0208 20:27:22.760121 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 20:30:22.292540 140699726837568 spec.py:333] Evaluating on the validation split.
I0208 20:30:25.025925 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 20:33:01.377446 140699726837568 spec.py:349] Evaluating on the test split.
I0208 20:33:04.113124 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 20:35:40.873514 140699726837568 submission_runner.py:408] Time since start: 51791.55s, 	Step: 88193, 	{'train/accuracy': 0.6824063062667847, 'train/loss': 1.4828976392745972, 'train/bleu': 34.05213368530559, 'validation/accuracy': 0.6825209856033325, 'validation/loss': 1.4713144302368164, 'validation/bleu': 30.038753265463793, 'validation/num_examples': 3000, 'test/accuracy': 0.6972517967224121, 'test/loss': 1.393563151359558, 'test/bleu': 30.091496010180137, 'test/num_examples': 3003, 'score': 31109.656126499176, 'total_duration': 51791.548968076706, 'accumulated_submission_time': 31109.656126499176, 'accumulated_eval_time': 20677.904341459274, 'accumulated_logging_time': 1.1917221546173096}
I0208 20:35:40.901619 140529918994176 logging_writer.py:48] [88193] accumulated_eval_time=20677.904341, accumulated_logging_time=1.191722, accumulated_submission_time=31109.656126, global_step=88193, preemption_count=0, score=31109.656126, test/accuracy=0.697252, test/bleu=30.091496, test/loss=1.393563, test/num_examples=3003, total_duration=51791.548968, train/accuracy=0.682406, train/bleu=34.052134, train/loss=1.482898, validation/accuracy=0.682521, validation/bleu=30.038753, validation/loss=1.471314, validation/num_examples=3000
I0208 20:35:43.723793 140529910601472 logging_writer.py:48] [88200] global_step=88200, grad_norm=3.101339340209961, loss=1.6561156511306763
I0208 20:36:18.856832 140529918994176 logging_writer.py:48] [88300] global_step=88300, grad_norm=0.44250574707984924, loss=1.6667454242706299
I0208 20:36:54.011104 140529910601472 logging_writer.py:48] [88400] global_step=88400, grad_norm=0.4493144452571869, loss=1.5994449853897095
I0208 20:37:29.210099 140529918994176 logging_writer.py:48] [88500] global_step=88500, grad_norm=0.45976707339286804, loss=1.541775107383728
I0208 20:38:04.426649 140529910601472 logging_writer.py:48] [88600] global_step=88600, grad_norm=0.45981457829475403, loss=1.5983514785766602
I0208 20:38:39.699453 140529918994176 logging_writer.py:48] [88700] global_step=88700, grad_norm=0.48429110646247864, loss=1.610302209854126
I0208 20:39:14.915777 140529910601472 logging_writer.py:48] [88800] global_step=88800, grad_norm=0.4407438039779663, loss=1.6295995712280273
I0208 20:39:50.138875 140529918994176 logging_writer.py:48] [88900] global_step=88900, grad_norm=0.4609355628490448, loss=1.5968210697174072
I0208 20:40:25.363753 140529910601472 logging_writer.py:48] [89000] global_step=89000, grad_norm=0.46990475058555603, loss=1.591463565826416
I0208 20:41:00.633213 140529918994176 logging_writer.py:48] [89100] global_step=89100, grad_norm=0.45069342851638794, loss=1.5751620531082153
I0208 20:41:35.890997 140529910601472 logging_writer.py:48] [89200] global_step=89200, grad_norm=0.45466119050979614, loss=1.5939382314682007
I0208 20:42:11.162929 140529918994176 logging_writer.py:48] [89300] global_step=89300, grad_norm=0.43157488107681274, loss=1.5213507413864136
I0208 20:42:46.403660 140529910601472 logging_writer.py:48] [89400] global_step=89400, grad_norm=0.4820730984210968, loss=1.6427254676818848
I0208 20:43:21.649007 140529918994176 logging_writer.py:48] [89500] global_step=89500, grad_norm=0.4762076437473297, loss=1.5437871217727661
I0208 20:43:56.878519 140529910601472 logging_writer.py:48] [89600] global_step=89600, grad_norm=0.4524228870868683, loss=1.5081804990768433
I0208 20:44:32.137799 140529918994176 logging_writer.py:48] [89700] global_step=89700, grad_norm=0.4742337763309479, loss=1.528574824333191
I0208 20:45:07.450947 140529910601472 logging_writer.py:48] [89800] global_step=89800, grad_norm=0.4445304274559021, loss=1.506278157234192
I0208 20:45:42.757073 140529918994176 logging_writer.py:48] [89900] global_step=89900, grad_norm=0.4437733292579651, loss=1.521331787109375
I0208 20:46:18.018947 140529910601472 logging_writer.py:48] [90000] global_step=90000, grad_norm=0.44654345512390137, loss=1.518521785736084
I0208 20:46:53.318841 140529918994176 logging_writer.py:48] [90100] global_step=90100, grad_norm=0.4591885507106781, loss=1.5997631549835205
I0208 20:47:28.567285 140529910601472 logging_writer.py:48] [90200] global_step=90200, grad_norm=0.47630611062049866, loss=1.6136857271194458
I0208 20:48:03.797486 140529918994176 logging_writer.py:48] [90300] global_step=90300, grad_norm=0.42995986342430115, loss=1.5563477277755737
I0208 20:48:39.059945 140529910601472 logging_writer.py:48] [90400] global_step=90400, grad_norm=0.4704454243183136, loss=1.5231590270996094
I0208 20:49:14.294725 140529918994176 logging_writer.py:48] [90500] global_step=90500, grad_norm=0.4691372811794281, loss=1.571505069732666
I0208 20:49:41.188429 140699726837568 spec.py:321] Evaluating on the training split.
I0208 20:49:44.223263 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 20:54:16.069122 140699726837568 spec.py:333] Evaluating on the validation split.
I0208 20:54:18.800049 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 20:57:15.225013 140699726837568 spec.py:349] Evaluating on the test split.
I0208 20:57:17.978551 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 20:59:58.608649 140699726837568 submission_runner.py:408] Time since start: 53249.28s, 	Step: 90578, 	{'train/accuracy': 0.6718730330467224, 'train/loss': 1.5490468740463257, 'train/bleu': 33.41231035188514, 'validation/accuracy': 0.6825333833694458, 'validation/loss': 1.4652026891708374, 'validation/bleu': 30.102713832189707, 'validation/num_examples': 3000, 'test/accuracy': 0.6974958181381226, 'test/loss': 1.3812869787216187, 'test/bleu': 29.89253457886926, 'test/num_examples': 3003, 'score': 31949.858066558838, 'total_duration': 53249.28406596184, 'accumulated_submission_time': 31949.858066558838, 'accumulated_eval_time': 21295.32447552681, 'accumulated_logging_time': 1.2300746440887451}
I0208 20:59:58.641313 140529910601472 logging_writer.py:48] [90578] accumulated_eval_time=21295.324476, accumulated_logging_time=1.230075, accumulated_submission_time=31949.858067, global_step=90578, preemption_count=0, score=31949.858067, test/accuracy=0.697496, test/bleu=29.892535, test/loss=1.381287, test/num_examples=3003, total_duration=53249.284066, train/accuracy=0.671873, train/bleu=33.412310, train/loss=1.549047, validation/accuracy=0.682533, validation/bleu=30.102714, validation/loss=1.465203, validation/num_examples=3000
I0208 21:00:06.731832 140529918994176 logging_writer.py:48] [90600] global_step=90600, grad_norm=0.4474131166934967, loss=1.5890474319458008
I0208 21:00:41.902548 140529910601472 logging_writer.py:48] [90700] global_step=90700, grad_norm=0.46869102120399475, loss=1.6178677082061768
I0208 21:01:17.089760 140529918994176 logging_writer.py:48] [90800] global_step=90800, grad_norm=0.4801390767097473, loss=1.655603289604187
I0208 21:01:52.294301 140529910601472 logging_writer.py:48] [90900] global_step=90900, grad_norm=0.4768073856830597, loss=1.566379427909851
I0208 21:02:27.488473 140529918994176 logging_writer.py:48] [91000] global_step=91000, grad_norm=0.4722236692905426, loss=1.5358684062957764
I0208 21:03:02.705101 140529910601472 logging_writer.py:48] [91100] global_step=91100, grad_norm=0.4666517972946167, loss=1.5747743844985962
I0208 21:03:37.984399 140529918994176 logging_writer.py:48] [91200] global_step=91200, grad_norm=0.4818926751613617, loss=1.5100606679916382
I0208 21:04:13.271612 140529910601472 logging_writer.py:48] [91300] global_step=91300, grad_norm=0.4333016574382782, loss=1.4996088743209839
I0208 21:04:48.511687 140529918994176 logging_writer.py:48] [91400] global_step=91400, grad_norm=0.45821163058280945, loss=1.5897796154022217
I0208 21:05:23.786571 140529910601472 logging_writer.py:48] [91500] global_step=91500, grad_norm=0.46310684084892273, loss=1.5639256238937378
I0208 21:05:59.061313 140529918994176 logging_writer.py:48] [91600] global_step=91600, grad_norm=0.4570727050304413, loss=1.5665092468261719
I0208 21:06:34.324325 140529910601472 logging_writer.py:48] [91700] global_step=91700, grad_norm=0.46350911259651184, loss=1.5263128280639648
I0208 21:07:09.568235 140529918994176 logging_writer.py:48] [91800] global_step=91800, grad_norm=0.4755154848098755, loss=1.526219129562378
I0208 21:07:44.841041 140529910601472 logging_writer.py:48] [91900] global_step=91900, grad_norm=0.47917240858078003, loss=1.555654525756836
I0208 21:08:20.094715 140529918994176 logging_writer.py:48] [92000] global_step=92000, grad_norm=0.4824978709220886, loss=1.6115460395812988
I0208 21:08:55.345062 140529910601472 logging_writer.py:48] [92100] global_step=92100, grad_norm=0.471035361289978, loss=1.6322139501571655
I0208 21:09:30.606332 140529918994176 logging_writer.py:48] [92200] global_step=92200, grad_norm=0.47127074003219604, loss=1.519245982170105
I0208 21:10:05.876832 140529910601472 logging_writer.py:48] [92300] global_step=92300, grad_norm=0.4604073762893677, loss=1.5351320505142212
I0208 21:10:41.095087 140529918994176 logging_writer.py:48] [92400] global_step=92400, grad_norm=0.4688132107257843, loss=1.51832115650177
I0208 21:11:16.327961 140529910601472 logging_writer.py:48] [92500] global_step=92500, grad_norm=0.4902845621109009, loss=1.6128894090652466
I0208 21:11:51.576194 140529918994176 logging_writer.py:48] [92600] global_step=92600, grad_norm=0.4696081280708313, loss=1.668349027633667
I0208 21:12:26.827036 140529910601472 logging_writer.py:48] [92700] global_step=92700, grad_norm=0.47100114822387695, loss=1.5148155689239502
I0208 21:13:02.094251 140529918994176 logging_writer.py:48] [92800] global_step=92800, grad_norm=0.4955612123012543, loss=1.5780713558197021
I0208 21:13:37.336472 140529910601472 logging_writer.py:48] [92900] global_step=92900, grad_norm=0.46737194061279297, loss=1.541414499282837
I0208 21:13:58.893644 140699726837568 spec.py:321] Evaluating on the training split.
I0208 21:14:01.939777 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 21:17:27.840240 140699726837568 spec.py:333] Evaluating on the validation split.
I0208 21:17:30.577374 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 21:20:00.003820 140699726837568 spec.py:349] Evaluating on the test split.
I0208 21:20:02.754766 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 21:22:28.666038 140699726837568 submission_runner.py:408] Time since start: 54599.34s, 	Step: 92963, 	{'train/accuracy': 0.6706978678703308, 'train/loss': 1.5593184232711792, 'train/bleu': 33.93765018553232, 'validation/accuracy': 0.6834261417388916, 'validation/loss': 1.4623475074768066, 'validation/bleu': 29.940012422735734, 'validation/num_examples': 3000, 'test/accuracy': 0.6990064382553101, 'test/loss': 1.3774524927139282, 'test/bleu': 30.2127779651062, 'test/num_examples': 3003, 'score': 32790.02381038666, 'total_duration': 54599.341495513916, 'accumulated_submission_time': 32790.02381038666, 'accumulated_eval_time': 21805.096822977066, 'accumulated_logging_time': 1.2744011878967285}
I0208 21:22:28.694039 140529918994176 logging_writer.py:48] [92963] accumulated_eval_time=21805.096823, accumulated_logging_time=1.274401, accumulated_submission_time=32790.023810, global_step=92963, preemption_count=0, score=32790.023810, test/accuracy=0.699006, test/bleu=30.212778, test/loss=1.377452, test/num_examples=3003, total_duration=54599.341496, train/accuracy=0.670698, train/bleu=33.937650, train/loss=1.559318, validation/accuracy=0.683426, validation/bleu=29.940012, validation/loss=1.462348, validation/num_examples=3000
I0208 21:22:42.059721 140529910601472 logging_writer.py:48] [93000] global_step=93000, grad_norm=0.4651843309402466, loss=1.5046491622924805
I0208 21:23:17.213409 140529918994176 logging_writer.py:48] [93100] global_step=93100, grad_norm=0.4527113735675812, loss=1.4807292222976685
I0208 21:23:52.379204 140529910601472 logging_writer.py:48] [93200] global_step=93200, grad_norm=0.4926396310329437, loss=1.5747965574264526
I0208 21:24:27.579236 140529918994176 logging_writer.py:48] [93300] global_step=93300, grad_norm=0.539074182510376, loss=1.5909101963043213
I0208 21:25:02.837125 140529910601472 logging_writer.py:48] [93400] global_step=93400, grad_norm=0.4540330171585083, loss=1.5873818397521973
I0208 21:25:38.226911 140529918994176 logging_writer.py:48] [93500] global_step=93500, grad_norm=0.46294569969177246, loss=1.5782486200332642
I0208 21:26:13.491880 140529910601472 logging_writer.py:48] [93600] global_step=93600, grad_norm=0.4827571511268616, loss=1.566053867340088
I0208 21:26:48.729990 140529918994176 logging_writer.py:48] [93700] global_step=93700, grad_norm=0.49400952458381653, loss=1.6506272554397583
I0208 21:27:23.942562 140529910601472 logging_writer.py:48] [93800] global_step=93800, grad_norm=0.5381146669387817, loss=1.666428565979004
I0208 21:27:59.249600 140529918994176 logging_writer.py:48] [93900] global_step=93900, grad_norm=0.4653016924858093, loss=1.5128748416900635
I0208 21:28:34.482340 140529910601472 logging_writer.py:48] [94000] global_step=94000, grad_norm=0.4745088219642639, loss=1.573179006576538
I0208 21:29:09.730620 140529918994176 logging_writer.py:48] [94100] global_step=94100, grad_norm=0.5012798309326172, loss=1.5995928049087524
I0208 21:29:44.957323 140529910601472 logging_writer.py:48] [94200] global_step=94200, grad_norm=0.4938470125198364, loss=1.523291826248169
I0208 21:30:20.253044 140529918994176 logging_writer.py:48] [94300] global_step=94300, grad_norm=0.4892251789569855, loss=1.5166603326797485
I0208 21:30:55.503826 140529910601472 logging_writer.py:48] [94400] global_step=94400, grad_norm=0.516650378704071, loss=1.622880458831787
I0208 21:31:30.760715 140529918994176 logging_writer.py:48] [94500] global_step=94500, grad_norm=0.49781152606010437, loss=1.5383199453353882
I0208 21:32:06.063399 140529910601472 logging_writer.py:48] [94600] global_step=94600, grad_norm=0.48907673358917236, loss=1.5425206422805786
I0208 21:32:41.380817 140529918994176 logging_writer.py:48] [94700] global_step=94700, grad_norm=0.4625682830810547, loss=1.6106245517730713
I0208 21:33:16.622808 140529910601472 logging_writer.py:48] [94800] global_step=94800, grad_norm=0.4921730160713196, loss=1.6377556324005127
I0208 21:33:51.872109 140529918994176 logging_writer.py:48] [94900] global_step=94900, grad_norm=0.5039531588554382, loss=1.600966453552246
I0208 21:34:27.135560 140529910601472 logging_writer.py:48] [95000] global_step=95000, grad_norm=0.48934102058410645, loss=1.5418869256973267
I0208 21:35:02.417111 140529918994176 logging_writer.py:48] [95100] global_step=95100, grad_norm=0.49591344594955444, loss=1.4418553113937378
I0208 21:35:37.665073 140529910601472 logging_writer.py:48] [95200] global_step=95200, grad_norm=0.5111995339393616, loss=1.616931676864624
I0208 21:36:12.953352 140529918994176 logging_writer.py:48] [95300] global_step=95300, grad_norm=0.5197475552558899, loss=1.5702883005142212
I0208 21:36:28.885149 140699726837568 spec.py:321] Evaluating on the training split.
I0208 21:36:31.916669 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 21:40:11.865042 140699726837568 spec.py:333] Evaluating on the validation split.
I0208 21:40:14.605645 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 21:42:48.985560 140699726837568 spec.py:349] Evaluating on the test split.
I0208 21:42:51.726030 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 21:45:21.029110 140699726837568 submission_runner.py:408] Time since start: 55971.70s, 	Step: 95347, 	{'train/accuracy': 0.6808968186378479, 'train/loss': 1.4978055953979492, 'train/bleu': 34.68351336887508, 'validation/accuracy': 0.6850131750106812, 'validation/loss': 1.4518790245056152, 'validation/bleu': 30.293797765390387, 'validation/num_examples': 3000, 'test/accuracy': 0.6996804475784302, 'test/loss': 1.3698073625564575, 'test/bleu': 29.981582976534877, 'test/num_examples': 3003, 'score': 33630.12921476364, 'total_duration': 55971.70452427864, 'accumulated_submission_time': 33630.12921476364, 'accumulated_eval_time': 22337.240693807602, 'accumulated_logging_time': 1.3123936653137207}
I0208 21:45:21.064456 140529910601472 logging_writer.py:48] [95347] accumulated_eval_time=22337.240694, accumulated_logging_time=1.312394, accumulated_submission_time=33630.129215, global_step=95347, preemption_count=0, score=33630.129215, test/accuracy=0.699680, test/bleu=29.981583, test/loss=1.369807, test/num_examples=3003, total_duration=55971.704524, train/accuracy=0.680897, train/bleu=34.683513, train/loss=1.497806, validation/accuracy=0.685013, validation/bleu=30.293798, validation/loss=1.451879, validation/num_examples=3000
I0208 21:45:40.074707 140529918994176 logging_writer.py:48] [95400] global_step=95400, grad_norm=0.5178599953651428, loss=1.5619440078735352
I0208 21:46:15.255710 140529910601472 logging_writer.py:48] [95500] global_step=95500, grad_norm=0.48516470193862915, loss=1.5538573265075684
I0208 21:46:50.564171 140529918994176 logging_writer.py:48] [95600] global_step=95600, grad_norm=0.47101274132728577, loss=1.4823920726776123
I0208 21:47:25.763803 140529910601472 logging_writer.py:48] [95700] global_step=95700, grad_norm=0.5376611351966858, loss=1.555336833000183
I0208 21:48:00.984386 140529918994176 logging_writer.py:48] [95800] global_step=95800, grad_norm=0.4936688542366028, loss=1.5506389141082764
I0208 21:48:36.192526 140529910601472 logging_writer.py:48] [95900] global_step=95900, grad_norm=0.5155805349349976, loss=1.5334398746490479
I0208 21:49:11.431575 140529918994176 logging_writer.py:48] [96000] global_step=96000, grad_norm=0.5267244577407837, loss=1.606918454170227
I0208 21:49:46.679847 140529910601472 logging_writer.py:48] [96100] global_step=96100, grad_norm=0.5281376838684082, loss=1.5226051807403564
I0208 21:50:21.916917 140529918994176 logging_writer.py:48] [96200] global_step=96200, grad_norm=0.5244911313056946, loss=1.566063404083252
I0208 21:50:57.136446 140529910601472 logging_writer.py:48] [96300] global_step=96300, grad_norm=0.5170149207115173, loss=1.505898356437683
I0208 21:51:32.380428 140529918994176 logging_writer.py:48] [96400] global_step=96400, grad_norm=0.5103263854980469, loss=1.6139073371887207
I0208 21:52:07.658945 140529910601472 logging_writer.py:48] [96500] global_step=96500, grad_norm=0.5268301367759705, loss=1.4867393970489502
I0208 21:52:42.917821 140529918994176 logging_writer.py:48] [96600] global_step=96600, grad_norm=0.5249602794647217, loss=1.5622005462646484
I0208 21:53:18.187390 140529910601472 logging_writer.py:48] [96700] global_step=96700, grad_norm=0.532207190990448, loss=1.6176084280014038
I0208 21:53:53.475294 140529918994176 logging_writer.py:48] [96800] global_step=96800, grad_norm=0.5193103551864624, loss=1.5594030618667603
I0208 21:54:28.775474 140529910601472 logging_writer.py:48] [96900] global_step=96900, grad_norm=0.5590648651123047, loss=1.5316226482391357
I0208 21:55:04.064602 140529918994176 logging_writer.py:48] [97000] global_step=97000, grad_norm=0.5157563090324402, loss=1.6027143001556396
I0208 21:55:39.316471 140529910601472 logging_writer.py:48] [97100] global_step=97100, grad_norm=0.4884932041168213, loss=1.500430703163147
I0208 21:56:14.545359 140529918994176 logging_writer.py:48] [97200] global_step=97200, grad_norm=0.4908076226711273, loss=1.5913727283477783
I0208 21:56:49.765710 140529910601472 logging_writer.py:48] [97300] global_step=97300, grad_norm=0.5152206420898438, loss=1.5655794143676758
I0208 21:57:25.065092 140529918994176 logging_writer.py:48] [97400] global_step=97400, grad_norm=0.5287914276123047, loss=1.49518620967865
I0208 21:58:00.293579 140529910601472 logging_writer.py:48] [97500] global_step=97500, grad_norm=0.5188261866569519, loss=1.4732316732406616
I0208 21:58:35.680666 140529918994176 logging_writer.py:48] [97600] global_step=97600, grad_norm=0.5216128826141357, loss=1.5048657655715942
I0208 21:59:10.983644 140529910601472 logging_writer.py:48] [97700] global_step=97700, grad_norm=0.5332826375961304, loss=1.6046544313430786
I0208 21:59:21.306492 140699726837568 spec.py:321] Evaluating on the training split.
I0208 21:59:24.355118 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 22:02:30.852007 140699726837568 spec.py:333] Evaluating on the validation split.
I0208 22:02:33.587698 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 22:05:03.206836 140699726837568 spec.py:349] Evaluating on the test split.
I0208 22:05:05.945162 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 22:07:35.351889 140699726837568 submission_runner.py:408] Time since start: 57306.03s, 	Step: 97731, 	{'train/accuracy': 0.6694570183753967, 'train/loss': 1.563796043395996, 'train/bleu': 33.762738030108665, 'validation/accuracy': 0.6860547065734863, 'validation/loss': 1.4455714225769043, 'validation/bleu': 30.13158593552824, 'validation/num_examples': 3000, 'test/accuracy': 0.7033641338348389, 'test/loss': 1.3601025342941284, 'test/bleu': 30.140830859153333, 'test/num_examples': 3003, 'score': 34470.28120446205, 'total_duration': 57306.02733922005, 'accumulated_submission_time': 34470.28120446205, 'accumulated_eval_time': 22831.28604865074, 'accumulated_logging_time': 1.3595654964447021}
I0208 22:07:35.381212 140529918994176 logging_writer.py:48] [97731] accumulated_eval_time=22831.286049, accumulated_logging_time=1.359565, accumulated_submission_time=34470.281204, global_step=97731, preemption_count=0, score=34470.281204, test/accuracy=0.703364, test/bleu=30.140831, test/loss=1.360103, test/num_examples=3003, total_duration=57306.027339, train/accuracy=0.669457, train/bleu=33.762738, train/loss=1.563796, validation/accuracy=0.686055, validation/bleu=30.131586, validation/loss=1.445571, validation/num_examples=3000
I0208 22:07:59.959277 140529910601472 logging_writer.py:48] [97800] global_step=97800, grad_norm=0.4955008327960968, loss=1.4931750297546387
I0208 22:08:35.136703 140529918994176 logging_writer.py:48] [97900] global_step=97900, grad_norm=0.4996999502182007, loss=1.4755878448486328
I0208 22:09:10.338284 140529910601472 logging_writer.py:48] [98000] global_step=98000, grad_norm=0.5287623405456543, loss=1.570029854774475
I0208 22:09:45.539658 140529918994176 logging_writer.py:48] [98100] global_step=98100, grad_norm=0.5557528138160706, loss=1.5654107332229614
I0208 22:10:20.785141 140529910601472 logging_writer.py:48] [98200] global_step=98200, grad_norm=0.5448801517486572, loss=1.470109462738037
I0208 22:10:56.006763 140529918994176 logging_writer.py:48] [98300] global_step=98300, grad_norm=0.5278573632240295, loss=1.5837702751159668
I0208 22:11:31.280444 140529910601472 logging_writer.py:48] [98400] global_step=98400, grad_norm=0.5285608172416687, loss=1.5896008014678955
I0208 22:12:06.561381 140529918994176 logging_writer.py:48] [98500] global_step=98500, grad_norm=0.5327602624893188, loss=1.5289418697357178
I0208 22:12:41.832936 140529910601472 logging_writer.py:48] [98600] global_step=98600, grad_norm=0.5053835511207581, loss=1.436303973197937
I0208 22:13:17.103092 140529918994176 logging_writer.py:48] [98700] global_step=98700, grad_norm=0.5249489545822144, loss=1.4956976175308228
I0208 22:13:52.377296 140529910601472 logging_writer.py:48] [98800] global_step=98800, grad_norm=0.5142670273780823, loss=1.5709304809570312
I0208 22:14:27.643437 140529918994176 logging_writer.py:48] [98900] global_step=98900, grad_norm=0.5502153635025024, loss=1.5545848608016968
I0208 22:15:02.913223 140529910601472 logging_writer.py:48] [99000] global_step=99000, grad_norm=0.5102725625038147, loss=1.524608850479126
I0208 22:15:38.170683 140529918994176 logging_writer.py:48] [99100] global_step=99100, grad_norm=0.5208520889282227, loss=1.511397361755371
I0208 22:16:13.402223 140529910601472 logging_writer.py:48] [99200] global_step=99200, grad_norm=0.5308716893196106, loss=1.4801652431488037
I0208 22:16:48.641690 140529918994176 logging_writer.py:48] [99300] global_step=99300, grad_norm=0.5354751944541931, loss=1.4889891147613525
I0208 22:17:23.892736 140529910601472 logging_writer.py:48] [99400] global_step=99400, grad_norm=0.5319019556045532, loss=1.5506083965301514
I0208 22:17:59.106866 140529918994176 logging_writer.py:48] [99500] global_step=99500, grad_norm=0.5412725210189819, loss=1.5084782838821411
I0208 22:18:34.357535 140529910601472 logging_writer.py:48] [99600] global_step=99600, grad_norm=0.5413439273834229, loss=1.5000730752944946
I0208 22:19:09.626309 140529918994176 logging_writer.py:48] [99700] global_step=99700, grad_norm=0.5353094935417175, loss=1.533239722251892
I0208 22:19:44.918049 140529910601472 logging_writer.py:48] [99800] global_step=99800, grad_norm=0.5265910029411316, loss=1.4846887588500977
I0208 22:20:20.154892 140529918994176 logging_writer.py:48] [99900] global_step=99900, grad_norm=0.5433148145675659, loss=1.4850021600723267
I0208 22:20:55.388648 140529910601472 logging_writer.py:48] [100000] global_step=100000, grad_norm=0.5352668166160583, loss=1.5193811655044556
I0208 22:21:30.662026 140529918994176 logging_writer.py:48] [100100] global_step=100100, grad_norm=0.5426322221755981, loss=1.547541618347168
I0208 22:21:35.683575 140699726837568 spec.py:321] Evaluating on the training split.
I0208 22:21:38.724297 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 22:25:11.864410 140699726837568 spec.py:333] Evaluating on the validation split.
I0208 22:25:14.606142 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 22:27:46.102331 140699726837568 spec.py:349] Evaluating on the test split.
I0208 22:27:48.828522 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 22:30:10.476008 140699726837568 submission_runner.py:408] Time since start: 58661.15s, 	Step: 100116, 	{'train/accuracy': 0.709169864654541, 'train/loss': 1.3537744283676147, 'train/bleu': 36.85112230721641, 'validation/accuracy': 0.6860795021057129, 'validation/loss': 1.4437220096588135, 'validation/bleu': 30.376215081158747, 'validation/num_examples': 3000, 'test/accuracy': 0.7017953991889954, 'test/loss': 1.3589884042739868, 'test/bleu': 30.204320024066764, 'test/num_examples': 3003, 'score': 35310.500405311584, 'total_duration': 58661.151460170746, 'accumulated_submission_time': 35310.500405311584, 'accumulated_eval_time': 23346.078429937363, 'accumulated_logging_time': 1.3988215923309326}
I0208 22:30:10.505081 140529910601472 logging_writer.py:48] [100116] accumulated_eval_time=23346.078430, accumulated_logging_time=1.398822, accumulated_submission_time=35310.500405, global_step=100116, preemption_count=0, score=35310.500405, test/accuracy=0.701795, test/bleu=30.204320, test/loss=1.358988, test/num_examples=3003, total_duration=58661.151460, train/accuracy=0.709170, train/bleu=36.851122, train/loss=1.353774, validation/accuracy=0.686080, validation/bleu=30.376215, validation/loss=1.443722, validation/num_examples=3000
I0208 22:30:40.337304 140529918994176 logging_writer.py:48] [100200] global_step=100200, grad_norm=0.5284073948860168, loss=1.5064094066619873
I0208 22:31:15.505037 140529910601472 logging_writer.py:48] [100300] global_step=100300, grad_norm=0.5418500304222107, loss=1.4349128007888794
I0208 22:31:50.700906 140529918994176 logging_writer.py:48] [100400] global_step=100400, grad_norm=0.5173298716545105, loss=1.5786492824554443
I0208 22:32:25.908682 140529910601472 logging_writer.py:48] [100500] global_step=100500, grad_norm=0.5323149561882019, loss=1.5406819581985474
I0208 22:33:01.150401 140529918994176 logging_writer.py:48] [100600] global_step=100600, grad_norm=0.562078595161438, loss=1.5255311727523804
I0208 22:33:36.386345 140529910601472 logging_writer.py:48] [100700] global_step=100700, grad_norm=0.5397807955741882, loss=1.4610404968261719
I0208 22:34:11.631618 140529918994176 logging_writer.py:48] [100800] global_step=100800, grad_norm=0.5565124750137329, loss=1.4455246925354004
I0208 22:34:46.893995 140529910601472 logging_writer.py:48] [100900] global_step=100900, grad_norm=0.5629106163978577, loss=1.591456413269043
I0208 22:35:22.167682 140529918994176 logging_writer.py:48] [101000] global_step=101000, grad_norm=0.5854601860046387, loss=1.514862060546875
I0208 22:35:57.418603 140529910601472 logging_writer.py:48] [101100] global_step=101100, grad_norm=0.5448043942451477, loss=1.4681717157363892
I0208 22:36:32.760165 140529918994176 logging_writer.py:48] [101200] global_step=101200, grad_norm=0.5433939099311829, loss=1.510758399963379
I0208 22:37:08.119572 140529910601472 logging_writer.py:48] [101300] global_step=101300, grad_norm=0.5937589406967163, loss=1.4875935316085815
I0208 22:37:43.370890 140529918994176 logging_writer.py:48] [101400] global_step=101400, grad_norm=0.5461406111717224, loss=1.50491464138031
I0208 22:38:18.645727 140529910601472 logging_writer.py:48] [101500] global_step=101500, grad_norm=0.584349513053894, loss=1.5585243701934814
I0208 22:38:53.886520 140529918994176 logging_writer.py:48] [101600] global_step=101600, grad_norm=0.5533162951469421, loss=1.5962185859680176
I0208 22:39:29.141594 140529910601472 logging_writer.py:48] [101700] global_step=101700, grad_norm=0.5630834698677063, loss=1.5684864521026611
I0208 22:40:04.413217 140529918994176 logging_writer.py:48] [101800] global_step=101800, grad_norm=0.5490636229515076, loss=1.576945424079895
I0208 22:40:39.647137 140529910601472 logging_writer.py:48] [101900] global_step=101900, grad_norm=0.557137668132782, loss=1.5641380548477173
I0208 22:41:14.900110 140529918994176 logging_writer.py:48] [102000] global_step=102000, grad_norm=0.5659480690956116, loss=1.5369964838027954
I0208 22:41:50.157024 140529910601472 logging_writer.py:48] [102100] global_step=102100, grad_norm=0.5583497285842896, loss=1.4306946992874146
I0208 22:42:25.385015 140529918994176 logging_writer.py:48] [102200] global_step=102200, grad_norm=0.5526637434959412, loss=1.5240168571472168
I0208 22:43:00.675344 140529910601472 logging_writer.py:48] [102300] global_step=102300, grad_norm=0.5542019605636597, loss=1.5061206817626953
I0208 22:43:35.939734 140529918994176 logging_writer.py:48] [102400] global_step=102400, grad_norm=0.5485405921936035, loss=1.4978413581848145
I0208 22:44:10.547587 140699726837568 spec.py:321] Evaluating on the training split.
I0208 22:44:13.587625 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 22:47:52.053978 140699726837568 spec.py:333] Evaluating on the validation split.
I0208 22:47:54.790516 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 22:50:27.833485 140699726837568 spec.py:349] Evaluating on the test split.
I0208 22:50:30.571053 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 22:52:53.508404 140699726837568 submission_runner.py:408] Time since start: 60024.18s, 	Step: 102500, 	{'train/accuracy': 0.6830866932868958, 'train/loss': 1.4718409776687622, 'train/bleu': 34.33374070683871, 'validation/accuracy': 0.6886585354804993, 'validation/loss': 1.4326103925704956, 'validation/bleu': 30.327454897877665, 'validation/num_examples': 3000, 'test/accuracy': 0.7032363414764404, 'test/loss': 1.3476167917251587, 'test/bleu': 30.557944063580372, 'test/num_examples': 3003, 'score': 36150.45610380173, 'total_duration': 60024.18383717537, 'accumulated_submission_time': 36150.45610380173, 'accumulated_eval_time': 23869.03919649124, 'accumulated_logging_time': 1.439997673034668}
I0208 22:52:53.537804 140529910601472 logging_writer.py:48] [102500] accumulated_eval_time=23869.039196, accumulated_logging_time=1.439998, accumulated_submission_time=36150.456104, global_step=102500, preemption_count=0, score=36150.456104, test/accuracy=0.703236, test/bleu=30.557944, test/loss=1.347617, test/num_examples=3003, total_duration=60024.183837, train/accuracy=0.683087, train/bleu=34.333741, train/loss=1.471841, validation/accuracy=0.688659, validation/bleu=30.327455, validation/loss=1.432610, validation/num_examples=3000
I0208 22:52:53.917231 140529918994176 logging_writer.py:48] [102500] global_step=102500, grad_norm=0.5687465667724609, loss=1.5025712251663208
I0208 22:53:29.041474 140529910601472 logging_writer.py:48] [102600] global_step=102600, grad_norm=0.5779675841331482, loss=1.538970947265625
I0208 22:54:04.211679 140529918994176 logging_writer.py:48] [102700] global_step=102700, grad_norm=0.5376562476158142, loss=1.5980162620544434
I0208 22:54:39.433325 140529910601472 logging_writer.py:48] [102800] global_step=102800, grad_norm=0.5554069876670837, loss=1.4829413890838623
I0208 22:55:14.657067 140529918994176 logging_writer.py:48] [102900] global_step=102900, grad_norm=0.5588745474815369, loss=1.4670807123184204
I0208 22:55:49.886962 140529910601472 logging_writer.py:48] [103000] global_step=103000, grad_norm=0.594272255897522, loss=1.4810569286346436
I0208 22:56:25.135558 140529918994176 logging_writer.py:48] [103100] global_step=103100, grad_norm=0.6051865816116333, loss=1.503334879875183
I0208 22:57:00.404379 140529910601472 logging_writer.py:48] [103200] global_step=103200, grad_norm=0.5856672525405884, loss=1.489773154258728
I0208 22:57:35.731281 140529918994176 logging_writer.py:48] [103300] global_step=103300, grad_norm=0.5751528739929199, loss=1.5090892314910889
I0208 22:58:11.016544 140529910601472 logging_writer.py:48] [103400] global_step=103400, grad_norm=0.5686782598495483, loss=1.515987515449524
I0208 22:58:46.266885 140529918994176 logging_writer.py:48] [103500] global_step=103500, grad_norm=0.57862389087677, loss=1.4417418241500854
I0208 22:59:21.513424 140529910601472 logging_writer.py:48] [103600] global_step=103600, grad_norm=0.591475248336792, loss=1.4551149606704712
I0208 22:59:56.794540 140529918994176 logging_writer.py:48] [103700] global_step=103700, grad_norm=0.590878963470459, loss=1.521506428718567
I0208 23:00:32.080940 140529910601472 logging_writer.py:48] [103800] global_step=103800, grad_norm=0.5596411824226379, loss=1.482949137687683
I0208 23:01:07.311008 140529918994176 logging_writer.py:48] [103900] global_step=103900, grad_norm=0.582726240158081, loss=1.4551188945770264
I0208 23:01:42.559138 140529910601472 logging_writer.py:48] [104000] global_step=104000, grad_norm=0.5708945989608765, loss=1.4720802307128906
I0208 23:02:17.853493 140529918994176 logging_writer.py:48] [104100] global_step=104100, grad_norm=0.5666601061820984, loss=1.4815216064453125
I0208 23:02:53.118461 140529910601472 logging_writer.py:48] [104200] global_step=104200, grad_norm=0.6129772663116455, loss=1.4705289602279663
I0208 23:03:28.395504 140529918994176 logging_writer.py:48] [104300] global_step=104300, grad_norm=0.574455201625824, loss=1.4649451971054077
I0208 23:04:03.665182 140529910601472 logging_writer.py:48] [104400] global_step=104400, grad_norm=0.5913099050521851, loss=1.500867486000061
I0208 23:04:38.923138 140529918994176 logging_writer.py:48] [104500] global_step=104500, grad_norm=0.5853357315063477, loss=1.531620740890503
I0208 23:05:14.194416 140529910601472 logging_writer.py:48] [104600] global_step=104600, grad_norm=0.571702778339386, loss=1.5076113939285278
I0208 23:05:49.410087 140529918994176 logging_writer.py:48] [104700] global_step=104700, grad_norm=0.6064510941505432, loss=1.4881242513656616
I0208 23:06:24.716420 140529910601472 logging_writer.py:48] [104800] global_step=104800, grad_norm=0.566228449344635, loss=1.436629295349121
I0208 23:06:53.701072 140699726837568 spec.py:321] Evaluating on the training split.
I0208 23:06:56.736899 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 23:10:48.499269 140699726837568 spec.py:333] Evaluating on the validation split.
I0208 23:10:51.225905 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 23:13:15.827518 140699726837568 spec.py:349] Evaluating on the test split.
I0208 23:13:18.566466 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 23:15:39.824232 140699726837568 submission_runner.py:408] Time since start: 61390.50s, 	Step: 104884, 	{'train/accuracy': 0.6831173896789551, 'train/loss': 1.4802350997924805, 'train/bleu': 34.37978318584616, 'validation/accuracy': 0.6883485317230225, 'validation/loss': 1.4367409944534302, 'validation/bleu': 30.31289615139876, 'validation/num_examples': 3000, 'test/accuracy': 0.7036778926849365, 'test/loss': 1.347154140472412, 'test/bleu': 30.484548709173747, 'test/num_examples': 3003, 'score': 36990.53392624855, 'total_duration': 61390.499673843384, 'accumulated_submission_time': 36990.53392624855, 'accumulated_eval_time': 24395.162294387817, 'accumulated_logging_time': 1.479119062423706}
I0208 23:15:39.855025 140529918994176 logging_writer.py:48] [104884] accumulated_eval_time=24395.162294, accumulated_logging_time=1.479119, accumulated_submission_time=36990.533926, global_step=104884, preemption_count=0, score=36990.533926, test/accuracy=0.703678, test/bleu=30.484549, test/loss=1.347154, test/num_examples=3003, total_duration=61390.499674, train/accuracy=0.683117, train/bleu=34.379783, train/loss=1.480235, validation/accuracy=0.688349, validation/bleu=30.312896, validation/loss=1.436741, validation/num_examples=3000
I0208 23:15:45.817189 140529910601472 logging_writer.py:48] [104900] global_step=104900, grad_norm=0.5759774446487427, loss=1.5018733739852905
I0208 23:16:20.910330 140529918994176 logging_writer.py:48] [105000] global_step=105000, grad_norm=0.5836503505706787, loss=1.4625906944274902
I0208 23:16:56.094312 140529910601472 logging_writer.py:48] [105100] global_step=105100, grad_norm=0.5645471215248108, loss=1.4260791540145874
I0208 23:17:31.308728 140529918994176 logging_writer.py:48] [105200] global_step=105200, grad_norm=0.5842636227607727, loss=1.5374014377593994
I0208 23:18:06.535195 140529910601472 logging_writer.py:48] [105300] global_step=105300, grad_norm=0.6001340746879578, loss=1.4846652746200562
I0208 23:18:41.749828 140529918994176 logging_writer.py:48] [105400] global_step=105400, grad_norm=0.6084954738616943, loss=1.478668451309204
I0208 23:19:16.998666 140529910601472 logging_writer.py:48] [105500] global_step=105500, grad_norm=0.6134508848190308, loss=1.4281460046768188
I0208 23:19:52.227312 140529918994176 logging_writer.py:48] [105600] global_step=105600, grad_norm=0.6002069711685181, loss=1.462116003036499
I0208 23:20:27.483217 140529910601472 logging_writer.py:48] [105700] global_step=105700, grad_norm=0.6186848282814026, loss=1.4683047533035278
I0208 23:21:02.732585 140529918994176 logging_writer.py:48] [105800] global_step=105800, grad_norm=0.5763856172561646, loss=1.522489309310913
I0208 23:21:38.019790 140529910601472 logging_writer.py:48] [105900] global_step=105900, grad_norm=0.629539966583252, loss=1.5253220796585083
I0208 23:22:13.277298 140529918994176 logging_writer.py:48] [106000] global_step=106000, grad_norm=0.5753509402275085, loss=1.4860447645187378
I0208 23:22:48.501245 140529910601472 logging_writer.py:48] [106100] global_step=106100, grad_norm=0.6250342726707458, loss=1.488860011100769
I0208 23:23:23.755908 140529918994176 logging_writer.py:48] [106200] global_step=106200, grad_norm=0.607240617275238, loss=1.3698078393936157
I0208 23:23:58.993180 140529910601472 logging_writer.py:48] [106300] global_step=106300, grad_norm=0.5971328616142273, loss=1.4687950611114502
I0208 23:24:34.257107 140529918994176 logging_writer.py:48] [106400] global_step=106400, grad_norm=0.5963306427001953, loss=1.4347898960113525
I0208 23:25:09.506601 140529910601472 logging_writer.py:48] [106500] global_step=106500, grad_norm=0.6175041198730469, loss=1.5130006074905396
I0208 23:25:44.757243 140529918994176 logging_writer.py:48] [106600] global_step=106600, grad_norm=0.6170439124107361, loss=1.5476874113082886
I0208 23:26:20.008934 140529910601472 logging_writer.py:48] [106700] global_step=106700, grad_norm=0.60821533203125, loss=1.4274568557739258
I0208 23:26:55.269973 140529918994176 logging_writer.py:48] [106800] global_step=106800, grad_norm=0.6306112408638, loss=1.4879099130630493
I0208 23:27:30.503959 140529910601472 logging_writer.py:48] [106900] global_step=106900, grad_norm=0.5942548513412476, loss=1.428855061531067
I0208 23:28:05.767043 140529918994176 logging_writer.py:48] [107000] global_step=107000, grad_norm=0.6116222739219666, loss=1.4877253770828247
I0208 23:28:41.018579 140529910601472 logging_writer.py:48] [107100] global_step=107100, grad_norm=0.5841610431671143, loss=1.4822344779968262
I0208 23:29:16.273110 140529918994176 logging_writer.py:48] [107200] global_step=107200, grad_norm=0.6092724204063416, loss=1.468549370765686
I0208 23:29:39.953136 140699726837568 spec.py:321] Evaluating on the training split.
I0208 23:29:42.984823 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 23:32:59.255872 140699726837568 spec.py:333] Evaluating on the validation split.
I0208 23:33:02.005281 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 23:35:31.429853 140699726837568 spec.py:349] Evaluating on the test split.
I0208 23:35:34.167747 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 23:38:14.819374 140699726837568 submission_runner.py:408] Time since start: 62745.49s, 	Step: 107269, 	{'train/accuracy': 0.6964491009712219, 'train/loss': 1.404707908630371, 'train/bleu': 35.48056756531287, 'validation/accuracy': 0.6896876692771912, 'validation/loss': 1.4275447130203247, 'validation/bleu': 30.40983792345602, 'validation/num_examples': 3000, 'test/accuracy': 0.7058392763137817, 'test/loss': 1.339435338973999, 'test/bleu': 30.684268005675964, 'test/num_examples': 3003, 'score': 37830.54970383644, 'total_duration': 62745.49480700493, 'accumulated_submission_time': 37830.54970383644, 'accumulated_eval_time': 24910.0284614563, 'accumulated_logging_time': 1.5198464393615723}
I0208 23:38:14.849252 140529910601472 logging_writer.py:48] [107269] accumulated_eval_time=24910.028461, accumulated_logging_time=1.519846, accumulated_submission_time=37830.549704, global_step=107269, preemption_count=0, score=37830.549704, test/accuracy=0.705839, test/bleu=30.684268, test/loss=1.339435, test/num_examples=3003, total_duration=62745.494807, train/accuracy=0.696449, train/bleu=35.480568, train/loss=1.404708, validation/accuracy=0.689688, validation/bleu=30.409838, validation/loss=1.427545, validation/num_examples=3000
I0208 23:38:26.081639 140529918994176 logging_writer.py:48] [107300] global_step=107300, grad_norm=0.6201688647270203, loss=1.489587426185608
I0208 23:39:01.179235 140529910601472 logging_writer.py:48] [107400] global_step=107400, grad_norm=0.6047633290290833, loss=1.5476789474487305
I0208 23:39:36.361796 140529918994176 logging_writer.py:48] [107500] global_step=107500, grad_norm=0.6314848065376282, loss=1.4387176036834717
I0208 23:40:11.570882 140529910601472 logging_writer.py:48] [107600] global_step=107600, grad_norm=0.6344593167304993, loss=1.3999735116958618
I0208 23:40:46.808671 140529918994176 logging_writer.py:48] [107700] global_step=107700, grad_norm=0.6317504048347473, loss=1.4817261695861816
I0208 23:41:22.054823 140529910601472 logging_writer.py:48] [107800] global_step=107800, grad_norm=0.6370121836662292, loss=1.5548404455184937
I0208 23:41:57.286925 140529918994176 logging_writer.py:48] [107900] global_step=107900, grad_norm=0.623703122138977, loss=1.4646002054214478
I0208 23:42:32.537874 140529910601472 logging_writer.py:48] [108000] global_step=108000, grad_norm=0.6433955430984497, loss=1.4805656671524048
I0208 23:43:07.837419 140529918994176 logging_writer.py:48] [108100] global_step=108100, grad_norm=0.6205877065658569, loss=1.444148063659668
I0208 23:43:43.123641 140529910601472 logging_writer.py:48] [108200] global_step=108200, grad_norm=0.6256240606307983, loss=1.4929335117340088
I0208 23:44:18.403808 140529918994176 logging_writer.py:48] [108300] global_step=108300, grad_norm=0.6064851880073547, loss=1.5123578310012817
I0208 23:44:53.656820 140529910601472 logging_writer.py:48] [108400] global_step=108400, grad_norm=0.6308994293212891, loss=1.5044381618499756
I0208 23:45:28.901998 140529918994176 logging_writer.py:48] [108500] global_step=108500, grad_norm=0.6079970598220825, loss=1.461881160736084
I0208 23:46:04.175805 140529910601472 logging_writer.py:48] [108600] global_step=108600, grad_norm=0.6360176205635071, loss=1.5139703750610352
I0208 23:46:39.428712 140529918994176 logging_writer.py:48] [108700] global_step=108700, grad_norm=0.6364672183990479, loss=1.4534683227539062
I0208 23:47:14.679667 140529910601472 logging_writer.py:48] [108800] global_step=108800, grad_norm=0.6476687788963318, loss=1.4818410873413086
I0208 23:47:49.975274 140529918994176 logging_writer.py:48] [108900] global_step=108900, grad_norm=0.6406987905502319, loss=1.4105144739151
I0208 23:48:25.287670 140529910601472 logging_writer.py:48] [109000] global_step=109000, grad_norm=0.6577670574188232, loss=1.4557721614837646
I0208 23:49:00.518481 140529918994176 logging_writer.py:48] [109100] global_step=109100, grad_norm=0.6137456297874451, loss=1.4654321670532227
I0208 23:49:35.763723 140529910601472 logging_writer.py:48] [109200] global_step=109200, grad_norm=0.6475667357444763, loss=1.4433125257492065
I0208 23:50:11.012979 140529918994176 logging_writer.py:48] [109300] global_step=109300, grad_norm=0.6586117148399353, loss=1.4783799648284912
I0208 23:50:46.283450 140529910601472 logging_writer.py:48] [109400] global_step=109400, grad_norm=0.6686910390853882, loss=1.5114479064941406
I0208 23:51:21.567923 140529918994176 logging_writer.py:48] [109500] global_step=109500, grad_norm=0.6869491338729858, loss=1.4436410665512085
I0208 23:51:56.858320 140529910601472 logging_writer.py:48] [109600] global_step=109600, grad_norm=0.6384851336479187, loss=1.4473035335540771
I0208 23:52:14.935952 140699726837568 spec.py:321] Evaluating on the training split.
I0208 23:52:17.969502 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 23:56:08.992191 140699726837568 spec.py:333] Evaluating on the validation split.
I0208 23:56:11.733131 140699726837568 workload.py:181] Translating evaluation dataset.
I0208 23:58:50.048389 140699726837568 spec.py:349] Evaluating on the test split.
I0208 23:58:52.787936 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 00:01:23.084802 140699726837568 submission_runner.py:408] Time since start: 64133.76s, 	Step: 109653, 	{'train/accuracy': 0.6924313902854919, 'train/loss': 1.4321004152297974, 'train/bleu': 35.2917509143731, 'validation/accuracy': 0.6913739442825317, 'validation/loss': 1.4228721857070923, 'validation/bleu': 30.570859095014146, 'validation/num_examples': 3000, 'test/accuracy': 0.7071059346199036, 'test/loss': 1.3327090740203857, 'test/bleu': 30.789080482976654, 'test/num_examples': 3003, 'score': 38670.55089187622, 'total_duration': 64133.76025557518, 'accumulated_submission_time': 38670.55089187622, 'accumulated_eval_time': 25458.177261590958, 'accumulated_logging_time': 1.561103343963623}
I0209 00:01:23.115244 140529918994176 logging_writer.py:48] [109653] accumulated_eval_time=25458.177262, accumulated_logging_time=1.561103, accumulated_submission_time=38670.550892, global_step=109653, preemption_count=0, score=38670.550892, test/accuracy=0.707106, test/bleu=30.789080, test/loss=1.332709, test/num_examples=3003, total_duration=64133.760256, train/accuracy=0.692431, train/bleu=35.291751, train/loss=1.432100, validation/accuracy=0.691374, validation/bleu=30.570859, validation/loss=1.422872, validation/num_examples=3000
I0209 00:01:39.998242 140529910601472 logging_writer.py:48] [109700] global_step=109700, grad_norm=0.649189829826355, loss=1.494958758354187
I0209 00:02:15.121561 140529918994176 logging_writer.py:48] [109800] global_step=109800, grad_norm=0.6437075138092041, loss=1.4476245641708374
I0209 00:02:50.346913 140529910601472 logging_writer.py:48] [109900] global_step=109900, grad_norm=0.6524620652198792, loss=1.492841124534607
I0209 00:03:25.585373 140529918994176 logging_writer.py:48] [110000] global_step=110000, grad_norm=0.6460413336753845, loss=1.4579068422317505
I0209 00:04:00.827521 140529910601472 logging_writer.py:48] [110100] global_step=110100, grad_norm=0.6626095175743103, loss=1.4639781713485718
I0209 00:04:36.074844 140529918994176 logging_writer.py:48] [110200] global_step=110200, grad_norm=0.6373838186264038, loss=1.4865565299987793
I0209 00:05:11.338568 140529910601472 logging_writer.py:48] [110300] global_step=110300, grad_norm=0.6686964631080627, loss=1.502389669418335
I0209 00:05:46.614220 140529918994176 logging_writer.py:48] [110400] global_step=110400, grad_norm=0.6513093113899231, loss=1.4289463758468628
I0209 00:06:21.855167 140529910601472 logging_writer.py:48] [110500] global_step=110500, grad_norm=0.6558654308319092, loss=1.4460256099700928
I0209 00:06:57.146553 140529918994176 logging_writer.py:48] [110600] global_step=110600, grad_norm=0.6722368597984314, loss=1.4522700309753418
I0209 00:07:32.402098 140529910601472 logging_writer.py:48] [110700] global_step=110700, grad_norm=0.6605068445205688, loss=1.436967134475708
I0209 00:08:07.649926 140529918994176 logging_writer.py:48] [110800] global_step=110800, grad_norm=0.6726211309432983, loss=1.4551219940185547
I0209 00:08:42.909151 140529910601472 logging_writer.py:48] [110900] global_step=110900, grad_norm=0.6513950824737549, loss=1.5378212928771973
I0209 00:09:18.163863 140529918994176 logging_writer.py:48] [111000] global_step=111000, grad_norm=0.660967230796814, loss=1.4562203884124756
I0209 00:09:53.401191 140529910601472 logging_writer.py:48] [111100] global_step=111100, grad_norm=0.6518327593803406, loss=1.4355782270431519
I0209 00:10:28.678087 140529918994176 logging_writer.py:48] [111200] global_step=111200, grad_norm=0.6512274742126465, loss=1.3992706537246704
I0209 00:11:03.930608 140529910601472 logging_writer.py:48] [111300] global_step=111300, grad_norm=0.7032759189605713, loss=1.4415857791900635
I0209 00:11:39.226408 140529918994176 logging_writer.py:48] [111400] global_step=111400, grad_norm=0.7000047564506531, loss=1.4435129165649414
I0209 00:12:14.512578 140529910601472 logging_writer.py:48] [111500] global_step=111500, grad_norm=0.6721638441085815, loss=1.4658595323562622
I0209 00:12:49.755803 140529918994176 logging_writer.py:48] [111600] global_step=111600, grad_norm=0.6693128943443298, loss=1.4593374729156494
I0209 00:13:25.027425 140529910601472 logging_writer.py:48] [111700] global_step=111700, grad_norm=0.6551130414009094, loss=1.4144374132156372
I0209 00:14:00.267940 140529918994176 logging_writer.py:48] [111800] global_step=111800, grad_norm=0.679551362991333, loss=1.3673816919326782
I0209 00:14:35.518840 140529910601472 logging_writer.py:48] [111900] global_step=111900, grad_norm=0.6630921363830566, loss=1.4371262788772583
I0209 00:15:10.800847 140529918994176 logging_writer.py:48] [112000] global_step=112000, grad_norm=0.7141026854515076, loss=1.4172780513763428
I0209 00:15:23.203370 140699726837568 spec.py:321] Evaluating on the training split.
I0209 00:15:26.246939 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 00:18:55.256521 140699726837568 spec.py:333] Evaluating on the validation split.
I0209 00:18:57.990782 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 00:21:22.105828 140699726837568 spec.py:349] Evaluating on the test split.
I0209 00:21:24.845963 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 00:23:46.354933 140699726837568 submission_runner.py:408] Time since start: 65477.03s, 	Step: 112037, 	{'train/accuracy': 0.6871734857559204, 'train/loss': 1.4625294208526611, 'train/bleu': 35.0512106849693, 'validation/accuracy': 0.6905679702758789, 'validation/loss': 1.4227373600006104, 'validation/bleu': 30.782762669814545, 'validation/num_examples': 3000, 'test/accuracy': 0.7082796096801758, 'test/loss': 1.3301644325256348, 'test/bleu': 30.976652739571417, 'test/num_examples': 3003, 'score': 39510.55522322655, 'total_duration': 65477.030385017395, 'accumulated_submission_time': 39510.55522322655, 'accumulated_eval_time': 25961.32877779007, 'accumulated_logging_time': 1.60233473777771}
I0209 00:23:46.386159 140529910601472 logging_writer.py:48] [112037] accumulated_eval_time=25961.328778, accumulated_logging_time=1.602335, accumulated_submission_time=39510.555223, global_step=112037, preemption_count=0, score=39510.555223, test/accuracy=0.708280, test/bleu=30.976653, test/loss=1.330164, test/num_examples=3003, total_duration=65477.030385, train/accuracy=0.687173, train/bleu=35.051211, train/loss=1.462529, validation/accuracy=0.690568, validation/bleu=30.782763, validation/loss=1.422737, validation/num_examples=3000
I0209 00:24:08.860011 140529918994176 logging_writer.py:48] [112100] global_step=112100, grad_norm=0.6661553978919983, loss=1.4414558410644531
I0209 00:24:43.998701 140529910601472 logging_writer.py:48] [112200] global_step=112200, grad_norm=0.660334587097168, loss=1.3898985385894775
I0209 00:25:19.229559 140529918994176 logging_writer.py:48] [112300] global_step=112300, grad_norm=0.6758601069450378, loss=1.4403005838394165
I0209 00:25:54.479346 140529910601472 logging_writer.py:48] [112400] global_step=112400, grad_norm=0.6813685297966003, loss=1.4417835474014282
I0209 00:26:29.738140 140529918994176 logging_writer.py:48] [112500] global_step=112500, grad_norm=0.686801552772522, loss=1.417860507965088
I0209 00:27:04.973102 140529910601472 logging_writer.py:48] [112600] global_step=112600, grad_norm=0.7044010758399963, loss=1.4258519411087036
I0209 00:27:40.230092 140529918994176 logging_writer.py:48] [112700] global_step=112700, grad_norm=0.7367507219314575, loss=1.474279761314392
I0209 00:28:15.509500 140529910601472 logging_writer.py:48] [112800] global_step=112800, grad_norm=0.6687355041503906, loss=1.4852513074874878
I0209 00:28:50.775216 140529918994176 logging_writer.py:48] [112900] global_step=112900, grad_norm=0.6752381920814514, loss=1.3846861124038696
I0209 00:29:26.032631 140529910601472 logging_writer.py:48] [113000] global_step=113000, grad_norm=0.6776174902915955, loss=1.4610116481781006
I0209 00:30:01.339071 140529918994176 logging_writer.py:48] [113100] global_step=113100, grad_norm=0.7282339930534363, loss=1.4754616022109985
I0209 00:30:36.644225 140529910601472 logging_writer.py:48] [113200] global_step=113200, grad_norm=0.765583336353302, loss=1.4863910675048828
I0209 00:31:11.900863 140529918994176 logging_writer.py:48] [113300] global_step=113300, grad_norm=0.7171374559402466, loss=1.483795166015625
I0209 00:31:47.150346 140529910601472 logging_writer.py:48] [113400] global_step=113400, grad_norm=0.689609706401825, loss=1.4489682912826538
I0209 00:32:22.432006 140529918994176 logging_writer.py:48] [113500] global_step=113500, grad_norm=0.6858049035072327, loss=1.3701056241989136
I0209 00:32:57.695967 140529910601472 logging_writer.py:48] [113600] global_step=113600, grad_norm=0.6699166297912598, loss=1.4164987802505493
I0209 00:33:32.941440 140529918994176 logging_writer.py:48] [113700] global_step=113700, grad_norm=0.7317565083503723, loss=1.3971002101898193
I0209 00:34:08.174640 140529910601472 logging_writer.py:48] [113800] global_step=113800, grad_norm=0.6817509531974792, loss=1.4068360328674316
I0209 00:34:43.436457 140529918994176 logging_writer.py:48] [113900] global_step=113900, grad_norm=0.6919864416122437, loss=1.4380755424499512
I0209 00:35:18.687211 140529910601472 logging_writer.py:48] [114000] global_step=114000, grad_norm=0.6880345940589905, loss=1.36100435256958
I0209 00:35:53.978671 140529918994176 logging_writer.py:48] [114100] global_step=114100, grad_norm=0.6930317878723145, loss=1.4283089637756348
I0209 00:36:29.245336 140529910601472 logging_writer.py:48] [114200] global_step=114200, grad_norm=0.6675451993942261, loss=1.3562250137329102
I0209 00:37:04.528804 140529918994176 logging_writer.py:48] [114300] global_step=114300, grad_norm=0.7044551372528076, loss=1.3452585935592651
I0209 00:37:39.794768 140529910601472 logging_writer.py:48] [114400] global_step=114400, grad_norm=0.6963503360748291, loss=1.4028713703155518
I0209 00:37:46.565535 140699726837568 spec.py:321] Evaluating on the training split.
I0209 00:37:49.594892 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 00:41:36.347526 140699726837568 spec.py:333] Evaluating on the validation split.
I0209 00:41:39.078271 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 00:44:03.800350 140699726837568 spec.py:349] Evaluating on the test split.
I0209 00:44:06.532227 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 00:46:18.881465 140699726837568 submission_runner.py:408] Time since start: 66829.56s, 	Step: 114421, 	{'train/accuracy': 0.699521541595459, 'train/loss': 1.3912333250045776, 'train/bleu': 36.07696290784867, 'validation/accuracy': 0.6913739442825317, 'validation/loss': 1.4179686307907104, 'validation/bleu': 30.715100454032235, 'validation/num_examples': 3000, 'test/accuracy': 0.7078379988670349, 'test/loss': 1.3299546241760254, 'test/bleu': 30.80652754405374, 'test/num_examples': 3003, 'score': 40350.648983716965, 'total_duration': 66829.55691623688, 'accumulated_submission_time': 40350.648983716965, 'accumulated_eval_time': 26473.644669294357, 'accumulated_logging_time': 1.644965410232544}
I0209 00:46:18.912906 140529918994176 logging_writer.py:48] [114421] accumulated_eval_time=26473.644669, accumulated_logging_time=1.644965, accumulated_submission_time=40350.648984, global_step=114421, preemption_count=0, score=40350.648984, test/accuracy=0.707838, test/bleu=30.806528, test/loss=1.329955, test/num_examples=3003, total_duration=66829.556916, train/accuracy=0.699522, train/bleu=36.076963, train/loss=1.391233, validation/accuracy=0.691374, validation/bleu=30.715100, validation/loss=1.417969, validation/num_examples=3000
I0209 00:46:47.052629 140529910601472 logging_writer.py:48] [114500] global_step=114500, grad_norm=0.7005687355995178, loss=1.4236059188842773
I0209 00:47:22.219418 140529918994176 logging_writer.py:48] [114600] global_step=114600, grad_norm=0.7414712309837341, loss=1.484803557395935
I0209 00:47:57.425016 140529910601472 logging_writer.py:48] [114700] global_step=114700, grad_norm=0.7158695459365845, loss=1.4194867610931396
I0209 00:48:32.676598 140529918994176 logging_writer.py:48] [114800] global_step=114800, grad_norm=0.7283304333686829, loss=1.3923836946487427
I0209 00:49:07.936888 140529910601472 logging_writer.py:48] [114900] global_step=114900, grad_norm=0.7200178503990173, loss=1.5003087520599365
I0209 00:49:43.350446 140529918994176 logging_writer.py:48] [115000] global_step=115000, grad_norm=0.717168390750885, loss=1.4066990613937378
I0209 00:50:18.612977 140529910601472 logging_writer.py:48] [115100] global_step=115100, grad_norm=0.7243977785110474, loss=1.3959569931030273
I0209 00:50:53.895995 140529918994176 logging_writer.py:48] [115200] global_step=115200, grad_norm=0.752703070640564, loss=1.4690685272216797
I0209 00:51:29.196126 140529910601472 logging_writer.py:48] [115300] global_step=115300, grad_norm=0.7165025472640991, loss=1.411342978477478
I0209 00:52:04.526404 140529918994176 logging_writer.py:48] [115400] global_step=115400, grad_norm=0.7407573461532593, loss=1.4377270936965942
I0209 00:52:39.758986 140529910601472 logging_writer.py:48] [115500] global_step=115500, grad_norm=0.7150065302848816, loss=1.39682137966156
I0209 00:53:15.014217 140529918994176 logging_writer.py:48] [115600] global_step=115600, grad_norm=0.7363201975822449, loss=1.408116340637207
I0209 00:53:50.249587 140529910601472 logging_writer.py:48] [115700] global_step=115700, grad_norm=0.7198201417922974, loss=1.3523609638214111
I0209 00:54:25.499659 140529918994176 logging_writer.py:48] [115800] global_step=115800, grad_norm=0.7267414927482605, loss=1.4506754875183105
I0209 00:55:00.774643 140529910601472 logging_writer.py:48] [115900] global_step=115900, grad_norm=0.682725727558136, loss=1.405344009399414
I0209 00:55:36.067378 140529918994176 logging_writer.py:48] [116000] global_step=116000, grad_norm=0.7094354033470154, loss=1.4092738628387451
I0209 00:56:11.330374 140529910601472 logging_writer.py:48] [116100] global_step=116100, grad_norm=0.7257174849510193, loss=1.40984308719635
I0209 00:56:46.605430 140529918994176 logging_writer.py:48] [116200] global_step=116200, grad_norm=0.7220974564552307, loss=1.4749598503112793
I0209 00:57:21.861499 140529910601472 logging_writer.py:48] [116300] global_step=116300, grad_norm=0.7097063660621643, loss=1.3088929653167725
I0209 00:57:57.118724 140529918994176 logging_writer.py:48] [116400] global_step=116400, grad_norm=0.7139131426811218, loss=1.4111944437026978
I0209 00:58:32.434139 140529910601472 logging_writer.py:48] [116500] global_step=116500, grad_norm=0.7124196887016296, loss=1.3744739294052124
I0209 00:59:07.746938 140529918994176 logging_writer.py:48] [116600] global_step=116600, grad_norm=0.7672661542892456, loss=1.354798674583435
I0209 00:59:43.062274 140529910601472 logging_writer.py:48] [116700] global_step=116700, grad_norm=0.7379276752471924, loss=1.4405834674835205
I0209 01:00:18.341742 140529918994176 logging_writer.py:48] [116800] global_step=116800, grad_norm=0.7510129809379578, loss=1.4403622150421143
I0209 01:00:19.124647 140699726837568 spec.py:321] Evaluating on the training split.
I0209 01:00:22.159044 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 01:04:12.330045 140699726837568 spec.py:333] Evaluating on the validation split.
I0209 01:04:15.058711 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 01:06:47.932169 140699726837568 spec.py:349] Evaluating on the test split.
I0209 01:06:50.670609 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 01:09:09.638792 140699726837568 submission_runner.py:408] Time since start: 68200.31s, 	Step: 116804, 	{'train/accuracy': 0.6963258385658264, 'train/loss': 1.4070066213607788, 'train/bleu': 35.96184061155457, 'validation/accuracy': 0.6927750110626221, 'validation/loss': 1.4155522584915161, 'validation/bleu': 30.68112671416496, 'validation/num_examples': 3000, 'test/accuracy': 0.7087211608886719, 'test/loss': 1.3230842351913452, 'test/bleu': 30.802705175820623, 'test/num_examples': 3003, 'score': 41190.77096199989, 'total_duration': 68200.31424498558, 'accumulated_submission_time': 41190.77096199989, 'accumulated_eval_time': 27004.1587600708, 'accumulated_logging_time': 1.687828779220581}
I0209 01:09:09.669793 140529910601472 logging_writer.py:48] [116804] accumulated_eval_time=27004.158760, accumulated_logging_time=1.687829, accumulated_submission_time=41190.770962, global_step=116804, preemption_count=0, score=41190.770962, test/accuracy=0.708721, test/bleu=30.802705, test/loss=1.323084, test/num_examples=3003, total_duration=68200.314245, train/accuracy=0.696326, train/bleu=35.961841, train/loss=1.407007, validation/accuracy=0.692775, validation/bleu=30.681127, validation/loss=1.415552, validation/num_examples=3000
I0209 01:09:43.753031 140529918994176 logging_writer.py:48] [116900] global_step=116900, grad_norm=0.768600344657898, loss=1.432192325592041
I0209 01:10:18.911867 140529910601472 logging_writer.py:48] [117000] global_step=117000, grad_norm=0.7633979320526123, loss=1.345786452293396
I0209 01:10:54.124484 140529918994176 logging_writer.py:48] [117100] global_step=117100, grad_norm=0.7219032645225525, loss=1.4143662452697754
I0209 01:11:29.337182 140529910601472 logging_writer.py:48] [117200] global_step=117200, grad_norm=0.7790014743804932, loss=1.4208909273147583
I0209 01:12:04.604526 140529918994176 logging_writer.py:48] [117300] global_step=117300, grad_norm=0.7899764180183411, loss=1.4737355709075928
I0209 01:12:39.843896 140529910601472 logging_writer.py:48] [117400] global_step=117400, grad_norm=0.7526220083236694, loss=1.4254839420318604
I0209 01:13:15.114454 140529918994176 logging_writer.py:48] [117500] global_step=117500, grad_norm=0.7617241740226746, loss=1.3862138986587524
I0209 01:13:50.331060 140529910601472 logging_writer.py:48] [117600] global_step=117600, grad_norm=0.751390814781189, loss=1.3880425691604614
I0209 01:14:25.577826 140529918994176 logging_writer.py:48] [117700] global_step=117700, grad_norm=0.7303867340087891, loss=1.3771024942398071
I0209 01:15:00.840441 140529910601472 logging_writer.py:48] [117800] global_step=117800, grad_norm=0.7443812489509583, loss=1.376402735710144
I0209 01:15:36.131518 140529918994176 logging_writer.py:48] [117900] global_step=117900, grad_norm=0.7656463384628296, loss=1.3696755170822144
I0209 01:16:11.393558 140529910601472 logging_writer.py:48] [118000] global_step=118000, grad_norm=0.734832763671875, loss=1.2930099964141846
I0209 01:16:46.675435 140529918994176 logging_writer.py:48] [118100] global_step=118100, grad_norm=0.7547255754470825, loss=1.338365077972412
I0209 01:17:22.065865 140529910601472 logging_writer.py:48] [118200] global_step=118200, grad_norm=0.7532283663749695, loss=1.429328203201294
I0209 01:17:57.329800 140529918994176 logging_writer.py:48] [118300] global_step=118300, grad_norm=0.7431226372718811, loss=1.3938322067260742
I0209 01:18:32.574777 140529910601472 logging_writer.py:48] [118400] global_step=118400, grad_norm=0.766334593296051, loss=1.3812658786773682
I0209 01:19:07.837152 140529918994176 logging_writer.py:48] [118500] global_step=118500, grad_norm=0.7515804171562195, loss=1.5013498067855835
I0209 01:19:43.115870 140529910601472 logging_writer.py:48] [118600] global_step=118600, grad_norm=0.7973830699920654, loss=1.5502736568450928
I0209 01:20:18.423383 140529918994176 logging_writer.py:48] [118700] global_step=118700, grad_norm=0.7540745139122009, loss=1.4533047676086426
I0209 01:20:53.731868 140529910601472 logging_writer.py:48] [118800] global_step=118800, grad_norm=0.7763493657112122, loss=1.3186755180358887
I0209 01:21:29.001480 140529918994176 logging_writer.py:48] [118900] global_step=118900, grad_norm=0.7817978858947754, loss=1.4106767177581787
I0209 01:22:04.282791 140529910601472 logging_writer.py:48] [119000] global_step=119000, grad_norm=0.7748585343360901, loss=1.3756297826766968
I0209 01:22:39.563684 140529918994176 logging_writer.py:48] [119100] global_step=119100, grad_norm=0.7646253705024719, loss=1.3403288125991821
I0209 01:23:09.944103 140699726837568 spec.py:321] Evaluating on the training split.
I0209 01:23:12.973315 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 01:27:03.921909 140699726837568 spec.py:333] Evaluating on the validation split.
I0209 01:27:06.652193 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 01:29:32.188123 140699726837568 spec.py:349] Evaluating on the test split.
I0209 01:29:34.926228 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 01:32:03.675099 140699726837568 submission_runner.py:408] Time since start: 69574.35s, 	Step: 119188, 	{'train/accuracy': 0.7110795378684998, 'train/loss': 1.3276680707931519, 'train/bleu': 37.20115594615644, 'validation/accuracy': 0.6929734349250793, 'validation/loss': 1.4138745069503784, 'validation/bleu': 30.86193975348537, 'validation/num_examples': 3000, 'test/accuracy': 0.7092092633247375, 'test/loss': 1.3204371929168701, 'test/bleu': 30.994883238838508, 'test/num_examples': 3003, 'score': 42030.960902929306, 'total_duration': 69574.35052037239, 'accumulated_submission_time': 42030.960902929306, 'accumulated_eval_time': 27537.889677286148, 'accumulated_logging_time': 1.7288134098052979}
I0209 01:32:03.707343 140529910601472 logging_writer.py:48] [119188] accumulated_eval_time=27537.889677, accumulated_logging_time=1.728813, accumulated_submission_time=42030.960903, global_step=119188, preemption_count=0, score=42030.960903, test/accuracy=0.709209, test/bleu=30.994883, test/loss=1.320437, test/num_examples=3003, total_duration=69574.350520, train/accuracy=0.711080, train/bleu=37.201156, train/loss=1.327668, validation/accuracy=0.692973, validation/bleu=30.861940, validation/loss=1.413875, validation/num_examples=3000
I0209 01:32:08.280243 140529918994176 logging_writer.py:48] [119200] global_step=119200, grad_norm=0.7432214617729187, loss=1.4118329286575317
I0209 01:32:43.361428 140529910601472 logging_writer.py:48] [119300] global_step=119300, grad_norm=0.7870858311653137, loss=1.3714747428894043
I0209 01:33:18.555006 140529918994176 logging_writer.py:48] [119400] global_step=119400, grad_norm=0.7761268615722656, loss=1.4029686450958252
I0209 01:33:53.761191 140529910601472 logging_writer.py:48] [119500] global_step=119500, grad_norm=0.7696208357810974, loss=1.3797013759613037
I0209 01:34:28.991022 140529918994176 logging_writer.py:48] [119600] global_step=119600, grad_norm=0.7627725005149841, loss=1.3904194831848145
I0209 01:35:04.291531 140529910601472 logging_writer.py:48] [119700] global_step=119700, grad_norm=0.7291572690010071, loss=1.314212679862976
I0209 01:35:39.559322 140529918994176 logging_writer.py:48] [119800] global_step=119800, grad_norm=0.7685909271240234, loss=1.3978979587554932
I0209 01:36:14.795639 140529910601472 logging_writer.py:48] [119900] global_step=119900, grad_norm=0.7596997618675232, loss=1.3852862119674683
I0209 01:36:50.069277 140529918994176 logging_writer.py:48] [120000] global_step=120000, grad_norm=0.7726266384124756, loss=1.378046989440918
I0209 01:37:25.326813 140529910601472 logging_writer.py:48] [120100] global_step=120100, grad_norm=0.7545254230499268, loss=1.3596346378326416
I0209 01:38:00.570253 140529918994176 logging_writer.py:48] [120200] global_step=120200, grad_norm=0.774141252040863, loss=1.3990302085876465
I0209 01:38:35.856136 140529910601472 logging_writer.py:48] [120300] global_step=120300, grad_norm=0.7693971395492554, loss=1.3960915803909302
I0209 01:39:11.090138 140529918994176 logging_writer.py:48] [120400] global_step=120400, grad_norm=0.8031572103500366, loss=1.4312540292739868
I0209 01:39:46.355251 140529910601472 logging_writer.py:48] [120500] global_step=120500, grad_norm=0.7683780789375305, loss=1.3172242641448975
I0209 01:40:21.624622 140529918994176 logging_writer.py:48] [120600] global_step=120600, grad_norm=0.813779354095459, loss=1.4140756130218506
I0209 01:40:56.879915 140529910601472 logging_writer.py:48] [120700] global_step=120700, grad_norm=0.8160536289215088, loss=1.3759994506835938
I0209 01:41:32.158702 140529918994176 logging_writer.py:48] [120800] global_step=120800, grad_norm=0.7667296528816223, loss=1.4089648723602295
I0209 01:42:07.418509 140529910601472 logging_writer.py:48] [120900] global_step=120900, grad_norm=0.7951602339744568, loss=1.3335362672805786
I0209 01:42:42.678460 140529918994176 logging_writer.py:48] [121000] global_step=121000, grad_norm=0.7602905035018921, loss=1.344031572341919
I0209 01:43:17.954408 140529910601472 logging_writer.py:48] [121100] global_step=121100, grad_norm=0.7576947212219238, loss=1.4180554151535034
I0209 01:43:53.215613 140529918994176 logging_writer.py:48] [121200] global_step=121200, grad_norm=0.7674745917320251, loss=1.3282766342163086
I0209 01:44:28.462290 140529910601472 logging_writer.py:48] [121300] global_step=121300, grad_norm=0.7474697232246399, loss=1.314890742301941
I0209 01:45:03.755375 140529918994176 logging_writer.py:48] [121400] global_step=121400, grad_norm=0.7941836714744568, loss=1.3631044626235962
I0209 01:45:39.004171 140529910601472 logging_writer.py:48] [121500] global_step=121500, grad_norm=0.7555128931999207, loss=1.4717490673065186
I0209 01:46:03.747380 140699726837568 spec.py:321] Evaluating on the training split.
I0209 01:46:06.786363 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 01:50:13.989044 140699726837568 spec.py:333] Evaluating on the validation split.
I0209 01:50:16.721297 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 01:52:46.149396 140699726837568 spec.py:349] Evaluating on the test split.
I0209 01:52:48.881419 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 01:55:16.116999 140699726837568 submission_runner.py:408] Time since start: 70966.79s, 	Step: 121572, 	{'train/accuracy': 0.7089225053787231, 'train/loss': 1.3409868478775024, 'train/bleu': 36.699779532755656, 'validation/accuracy': 0.6929982304573059, 'validation/loss': 1.4128586053848267, 'validation/bleu': 30.743967260596456, 'validation/num_examples': 3000, 'test/accuracy': 0.7098135352134705, 'test/loss': 1.3198758363723755, 'test/bleu': 30.807151339414787, 'test/num_examples': 3003, 'score': 42870.91543865204, 'total_duration': 70966.79245257378, 'accumulated_submission_time': 42870.91543865204, 'accumulated_eval_time': 28090.259249210358, 'accumulated_logging_time': 1.7727289199829102}
I0209 01:55:16.148852 140529918994176 logging_writer.py:48] [121572] accumulated_eval_time=28090.259249, accumulated_logging_time=1.772729, accumulated_submission_time=42870.915439, global_step=121572, preemption_count=0, score=42870.915439, test/accuracy=0.709814, test/bleu=30.807151, test/loss=1.319876, test/num_examples=3003, total_duration=70966.792453, train/accuracy=0.708923, train/bleu=36.699780, train/loss=1.340987, validation/accuracy=0.692998, validation/bleu=30.743967, validation/loss=1.412859, validation/num_examples=3000
I0209 01:55:26.349972 140529910601472 logging_writer.py:48] [121600] global_step=121600, grad_norm=0.772527813911438, loss=1.326179027557373
I0209 01:56:01.492929 140529918994176 logging_writer.py:48] [121700] global_step=121700, grad_norm=0.8010279536247253, loss=1.3474457263946533
I0209 01:56:36.711211 140529910601472 logging_writer.py:48] [121800] global_step=121800, grad_norm=0.7973639369010925, loss=1.4177219867706299
I0209 01:57:11.949972 140529918994176 logging_writer.py:48] [121900] global_step=121900, grad_norm=0.7973574995994568, loss=1.3561598062515259
I0209 01:57:47.208652 140529910601472 logging_writer.py:48] [122000] global_step=122000, grad_norm=0.7826694250106812, loss=1.3312159776687622
I0209 01:58:22.507827 140529918994176 logging_writer.py:48] [122100] global_step=122100, grad_norm=0.7841602563858032, loss=1.4132232666015625
I0209 01:58:57.799258 140529910601472 logging_writer.py:48] [122200] global_step=122200, grad_norm=0.8218790292739868, loss=1.3686925172805786
I0209 01:59:33.026380 140529918994176 logging_writer.py:48] [122300] global_step=122300, grad_norm=0.7567391395568848, loss=1.344728708267212
I0209 02:00:08.267787 140529910601472 logging_writer.py:48] [122400] global_step=122400, grad_norm=0.8035229444503784, loss=1.4685890674591064
I0209 02:00:43.496342 140529918994176 logging_writer.py:48] [122500] global_step=122500, grad_norm=0.7932454943656921, loss=1.3538156747817993
I0209 02:01:18.755035 140529910601472 logging_writer.py:48] [122600] global_step=122600, grad_norm=0.7752338647842407, loss=1.3472411632537842
I0209 02:01:54.010373 140529918994176 logging_writer.py:48] [122700] global_step=122700, grad_norm=0.7948441505432129, loss=1.4425032138824463
I0209 02:02:29.275272 140529910601472 logging_writer.py:48] [122800] global_step=122800, grad_norm=0.7922513484954834, loss=1.3081860542297363
I0209 02:03:04.522042 140529918994176 logging_writer.py:48] [122900] global_step=122900, grad_norm=0.7997875809669495, loss=1.3788340091705322
I0209 02:03:39.772219 140529910601472 logging_writer.py:48] [123000] global_step=123000, grad_norm=0.7842966914176941, loss=1.3745253086090088
I0209 02:04:15.035160 140529918994176 logging_writer.py:48] [123100] global_step=123100, grad_norm=0.7991388440132141, loss=1.352323055267334
I0209 02:04:50.290654 140529910601472 logging_writer.py:48] [123200] global_step=123200, grad_norm=0.7803295850753784, loss=1.4017764329910278
I0209 02:05:25.529422 140529918994176 logging_writer.py:48] [123300] global_step=123300, grad_norm=0.8243646025657654, loss=1.349077582359314
I0209 02:06:00.788817 140529910601472 logging_writer.py:48] [123400] global_step=123400, grad_norm=0.789161741733551, loss=1.3126370906829834
I0209 02:06:36.011575 140529918994176 logging_writer.py:48] [123500] global_step=123500, grad_norm=0.7903180718421936, loss=1.4007619619369507
I0209 02:07:11.295278 140529910601472 logging_writer.py:48] [123600] global_step=123600, grad_norm=0.8251141309738159, loss=1.3564717769622803
I0209 02:07:46.532172 140529918994176 logging_writer.py:48] [123700] global_step=123700, grad_norm=0.8038732409477234, loss=1.3834776878356934
I0209 02:08:21.813337 140529910601472 logging_writer.py:48] [123800] global_step=123800, grad_norm=0.800452709197998, loss=1.3734848499298096
I0209 02:08:57.056059 140529918994176 logging_writer.py:48] [123900] global_step=123900, grad_norm=0.7999146580696106, loss=1.4002634286880493
I0209 02:09:16.201423 140699726837568 spec.py:321] Evaluating on the training split.
I0209 02:09:19.257079 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 02:13:13.070497 140699726837568 spec.py:333] Evaluating on the validation split.
I0209 02:13:15.816844 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 02:15:40.928209 140699726837568 spec.py:349] Evaluating on the test split.
I0209 02:15:43.666521 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 02:18:01.672492 140699726837568 submission_runner.py:408] Time since start: 72332.35s, 	Step: 123956, 	{'train/accuracy': 0.7044199109077454, 'train/loss': 1.3619587421417236, 'train/bleu': 36.63037870395407, 'validation/accuracy': 0.6941761374473572, 'validation/loss': 1.409651756286621, 'validation/bleu': 30.92822877713484, 'validation/num_examples': 3000, 'test/accuracy': 0.7102085947990417, 'test/loss': 1.3139033317565918, 'test/bleu': 31.06983647732182, 'test/num_examples': 3003, 'score': 43710.88386774063, 'total_duration': 72332.34792470932, 'accumulated_submission_time': 43710.88386774063, 'accumulated_eval_time': 28615.730256080627, 'accumulated_logging_time': 1.814807415008545}
I0209 02:18:01.705621 140529910601472 logging_writer.py:48] [123956] accumulated_eval_time=28615.730256, accumulated_logging_time=1.814807, accumulated_submission_time=43710.883868, global_step=123956, preemption_count=0, score=43710.883868, test/accuracy=0.710209, test/bleu=31.069836, test/loss=1.313903, test/num_examples=3003, total_duration=72332.347925, train/accuracy=0.704420, train/bleu=36.630379, train/loss=1.361959, validation/accuracy=0.694176, validation/bleu=30.928229, validation/loss=1.409652, validation/num_examples=3000
I0209 02:18:17.505437 140529918994176 logging_writer.py:48] [124000] global_step=124000, grad_norm=0.7842223644256592, loss=1.345159888267517
I0209 02:18:52.653141 140529910601472 logging_writer.py:48] [124100] global_step=124100, grad_norm=0.799050509929657, loss=1.357886791229248
I0209 02:19:27.937285 140529918994176 logging_writer.py:48] [124200] global_step=124200, grad_norm=0.8092688918113708, loss=1.3884917497634888
I0209 02:20:03.152596 140529910601472 logging_writer.py:48] [124300] global_step=124300, grad_norm=0.813293993473053, loss=1.4006749391555786
I0209 02:20:38.396531 140529918994176 logging_writer.py:48] [124400] global_step=124400, grad_norm=0.8229495882987976, loss=1.409874677658081
I0209 02:21:13.638012 140529910601472 logging_writer.py:48] [124500] global_step=124500, grad_norm=0.8014558553695679, loss=1.4313961267471313
I0209 02:21:48.904416 140529918994176 logging_writer.py:48] [124600] global_step=124600, grad_norm=0.8155122995376587, loss=1.3869996070861816
I0209 02:22:24.168462 140529910601472 logging_writer.py:48] [124700] global_step=124700, grad_norm=0.7669123411178589, loss=1.4037373065948486
I0209 02:22:59.420125 140529918994176 logging_writer.py:48] [124800] global_step=124800, grad_norm=0.7863633036613464, loss=1.2931212186813354
I0209 02:23:34.729871 140529910601472 logging_writer.py:48] [124900] global_step=124900, grad_norm=0.8374265432357788, loss=1.360366702079773
I0209 02:24:10.028807 140529918994176 logging_writer.py:48] [125000] global_step=125000, grad_norm=0.8228392004966736, loss=1.272214651107788
I0209 02:24:45.362578 140529910601472 logging_writer.py:48] [125100] global_step=125100, grad_norm=0.828068196773529, loss=1.4159756898880005
I0209 02:25:20.682509 140529918994176 logging_writer.py:48] [125200] global_step=125200, grad_norm=0.7702728509902954, loss=1.3826658725738525
I0209 02:25:55.973993 140529910601472 logging_writer.py:48] [125300] global_step=125300, grad_norm=0.8206331133842468, loss=1.382788896560669
I0209 02:26:31.269700 140529918994176 logging_writer.py:48] [125400] global_step=125400, grad_norm=0.7801946401596069, loss=1.3915472030639648
I0209 02:27:06.571604 140529910601472 logging_writer.py:48] [125500] global_step=125500, grad_norm=0.8006535172462463, loss=1.379196047782898
I0209 02:27:41.844010 140529918994176 logging_writer.py:48] [125600] global_step=125600, grad_norm=0.8439491391181946, loss=1.3651264905929565
I0209 02:28:17.135121 140529910601472 logging_writer.py:48] [125700] global_step=125700, grad_norm=0.8106063008308411, loss=1.351562261581421
I0209 02:28:52.418367 140529918994176 logging_writer.py:48] [125800] global_step=125800, grad_norm=0.8308253884315491, loss=1.3999483585357666
I0209 02:29:27.671502 140529910601472 logging_writer.py:48] [125900] global_step=125900, grad_norm=0.7929587960243225, loss=1.3121293783187866
I0209 02:30:02.936001 140529918994176 logging_writer.py:48] [126000] global_step=126000, grad_norm=0.8409401178359985, loss=1.3785094022750854
I0209 02:30:38.180371 140529910601472 logging_writer.py:48] [126100] global_step=126100, grad_norm=0.816755473613739, loss=1.3730894327163696
I0209 02:31:13.417505 140529918994176 logging_writer.py:48] [126200] global_step=126200, grad_norm=0.8291469216346741, loss=1.4033091068267822
I0209 02:31:48.703784 140529910601472 logging_writer.py:48] [126300] global_step=126300, grad_norm=0.8031661510467529, loss=1.3830009698867798
I0209 02:32:01.826025 140699726837568 spec.py:321] Evaluating on the training split.
I0209 02:32:04.855317 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 02:35:36.705296 140699726837568 spec.py:333] Evaluating on the validation split.
I0209 02:35:39.436880 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 02:38:13.375562 140699726837568 spec.py:349] Evaluating on the test split.
I0209 02:38:16.116127 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 02:40:43.676384 140699726837568 submission_runner.py:408] Time since start: 73694.35s, 	Step: 126339, 	{'train/accuracy': 0.7150582075119019, 'train/loss': 1.3104259967803955, 'train/bleu': 36.93187865217039, 'validation/accuracy': 0.6944612860679626, 'validation/loss': 1.411051869392395, 'validation/bleu': 30.66054748234449, 'validation/num_examples': 3000, 'test/accuracy': 0.7101853489875793, 'test/loss': 1.3157200813293457, 'test/bleu': 30.995070613341326, 'test/num_examples': 3003, 'score': 44550.9148273468, 'total_duration': 73694.35182857513, 'accumulated_submission_time': 44550.9148273468, 'accumulated_eval_time': 29137.580555915833, 'accumulated_logging_time': 1.8581435680389404}
I0209 02:40:43.709188 140529918994176 logging_writer.py:48] [126339] accumulated_eval_time=29137.580556, accumulated_logging_time=1.858144, accumulated_submission_time=44550.914827, global_step=126339, preemption_count=0, score=44550.914827, test/accuracy=0.710185, test/bleu=30.995071, test/loss=1.315720, test/num_examples=3003, total_duration=73694.351829, train/accuracy=0.715058, train/bleu=36.931879, train/loss=1.310426, validation/accuracy=0.694461, validation/bleu=30.660547, validation/loss=1.411052, validation/num_examples=3000
I0209 02:41:05.502929 140529910601472 logging_writer.py:48] [126400] global_step=126400, grad_norm=0.8401383757591248, loss=1.4107410907745361
I0209 02:41:40.675142 140529918994176 logging_writer.py:48] [126500] global_step=126500, grad_norm=0.8155239820480347, loss=1.315369963645935
I0209 02:42:15.965872 140529910601472 logging_writer.py:48] [126600] global_step=126600, grad_norm=0.8381421566009521, loss=1.4140805006027222
I0209 02:42:51.179845 140529918994176 logging_writer.py:48] [126700] global_step=126700, grad_norm=0.8153952956199646, loss=1.3667536973953247
I0209 02:43:26.433002 140529910601472 logging_writer.py:48] [126800] global_step=126800, grad_norm=0.7899538278579712, loss=1.3369157314300537
I0209 02:44:01.678680 140529918994176 logging_writer.py:48] [126900] global_step=126900, grad_norm=0.8092709183692932, loss=1.4287694692611694
I0209 02:44:36.934534 140529910601472 logging_writer.py:48] [127000] global_step=127000, grad_norm=0.7909437417984009, loss=1.3024696111679077
I0209 02:45:12.215387 140529918994176 logging_writer.py:48] [127100] global_step=127100, grad_norm=0.8392898440361023, loss=1.342936396598816
I0209 02:45:47.439654 140529910601472 logging_writer.py:48] [127200] global_step=127200, grad_norm=0.8204769492149353, loss=1.3781365156173706
I0209 02:46:22.770400 140529918994176 logging_writer.py:48] [127300] global_step=127300, grad_norm=0.7995368242263794, loss=1.3919756412506104
I0209 02:46:58.074498 140529910601472 logging_writer.py:48] [127400] global_step=127400, grad_norm=0.819096028804779, loss=1.5191166400909424
I0209 02:47:33.331921 140529918994176 logging_writer.py:48] [127500] global_step=127500, grad_norm=0.8059178590774536, loss=1.3320544958114624
I0209 02:48:08.575267 140529910601472 logging_writer.py:48] [127600] global_step=127600, grad_norm=0.842178463935852, loss=1.4159762859344482
I0209 02:48:43.861452 140529918994176 logging_writer.py:48] [127700] global_step=127700, grad_norm=0.837921679019928, loss=1.366958737373352
I0209 02:49:19.143052 140529910601472 logging_writer.py:48] [127800] global_step=127800, grad_norm=0.8255257606506348, loss=1.3916257619857788
I0209 02:49:54.387600 140529918994176 logging_writer.py:48] [127900] global_step=127900, grad_norm=0.8164989948272705, loss=1.3219259977340698
I0209 02:50:29.601105 140529910601472 logging_writer.py:48] [128000] global_step=128000, grad_norm=0.8340127468109131, loss=1.379095435142517
I0209 02:51:04.853353 140529918994176 logging_writer.py:48] [128100] global_step=128100, grad_norm=0.8340175747871399, loss=1.3321831226348877
I0209 02:51:40.085020 140529910601472 logging_writer.py:48] [128200] global_step=128200, grad_norm=0.817227840423584, loss=1.2958707809448242
I0209 02:52:15.349926 140529918994176 logging_writer.py:48] [128300] global_step=128300, grad_norm=0.7806622385978699, loss=1.3013490438461304
I0209 02:52:50.626327 140529910601472 logging_writer.py:48] [128400] global_step=128400, grad_norm=0.8117228746414185, loss=1.3441654443740845
I0209 02:53:25.938478 140529918994176 logging_writer.py:48] [128500] global_step=128500, grad_norm=0.8270730376243591, loss=1.2946586608886719
I0209 02:54:01.236687 140529910601472 logging_writer.py:48] [128600] global_step=128600, grad_norm=0.828377902507782, loss=1.3731298446655273
I0209 02:54:36.528421 140529918994176 logging_writer.py:48] [128700] global_step=128700, grad_norm=0.8186079263687134, loss=1.3912230730056763
I0209 02:54:44.016477 140699726837568 spec.py:321] Evaluating on the training split.
I0209 02:54:47.048234 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 02:58:53.200694 140699726837568 spec.py:333] Evaluating on the validation split.
I0209 02:58:55.934658 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 03:01:26.260507 140699726837568 spec.py:349] Evaluating on the test split.
I0209 03:01:29.005770 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 03:03:52.838985 140699726837568 submission_runner.py:408] Time since start: 75083.51s, 	Step: 128723, 	{'train/accuracy': 0.7099118232727051, 'train/loss': 1.337271809577942, 'train/bleu': 36.93021401210776, 'validation/accuracy': 0.6949200630187988, 'validation/loss': 1.4093292951583862, 'validation/bleu': 30.931063555305773, 'validation/num_examples': 3000, 'test/accuracy': 0.7109407186508179, 'test/loss': 1.3132773637771606, 'test/bleu': 31.110231469925782, 'test/num_examples': 3003, 'score': 45391.13405776024, 'total_duration': 75083.51441955566, 'accumulated_submission_time': 45391.13405776024, 'accumulated_eval_time': 29686.40299320221, 'accumulated_logging_time': 1.9021804332733154}
I0209 03:03:52.872336 140529910601472 logging_writer.py:48] [128723] accumulated_eval_time=29686.402993, accumulated_logging_time=1.902180, accumulated_submission_time=45391.134058, global_step=128723, preemption_count=0, score=45391.134058, test/accuracy=0.710941, test/bleu=31.110231, test/loss=1.313277, test/num_examples=3003, total_duration=75083.514420, train/accuracy=0.709912, train/bleu=36.930214, train/loss=1.337272, validation/accuracy=0.694920, validation/bleu=30.931064, validation/loss=1.409329, validation/num_examples=3000
I0209 03:04:20.224814 140529918994176 logging_writer.py:48] [128800] global_step=128800, grad_norm=0.8133362531661987, loss=1.3048454523086548
I0209 03:04:55.388194 140529910601472 logging_writer.py:48] [128900] global_step=128900, grad_norm=0.810298502445221, loss=1.3817566633224487
I0209 03:05:30.616863 140529918994176 logging_writer.py:48] [129000] global_step=129000, grad_norm=0.8314341306686401, loss=1.4166170358657837
I0209 03:06:05.852235 140529910601472 logging_writer.py:48] [129100] global_step=129100, grad_norm=0.7830329537391663, loss=1.347990870475769
I0209 03:06:41.099178 140529918994176 logging_writer.py:48] [129200] global_step=129200, grad_norm=0.8199688196182251, loss=1.3640515804290771
I0209 03:07:16.337489 140529910601472 logging_writer.py:48] [129300] global_step=129300, grad_norm=0.8309575319290161, loss=1.3884459733963013
I0209 03:07:51.568409 140529918994176 logging_writer.py:48] [129400] global_step=129400, grad_norm=0.8660395741462708, loss=1.3570281267166138
I0209 03:08:26.836433 140529910601472 logging_writer.py:48] [129500] global_step=129500, grad_norm=0.8005992770195007, loss=1.316678762435913
I0209 03:09:02.095302 140529918994176 logging_writer.py:48] [129600] global_step=129600, grad_norm=0.8072996735572815, loss=1.344342589378357
I0209 03:09:37.397619 140529910601472 logging_writer.py:48] [129700] global_step=129700, grad_norm=0.8424718379974365, loss=1.322837471961975
I0209 03:10:12.659916 140529918994176 logging_writer.py:48] [129800] global_step=129800, grad_norm=0.8169662356376648, loss=1.3125427961349487
I0209 03:10:47.920755 140529910601472 logging_writer.py:48] [129900] global_step=129900, grad_norm=0.8293819427490234, loss=1.3891996145248413
I0209 03:11:23.211616 140529918994176 logging_writer.py:48] [130000] global_step=130000, grad_norm=0.8041863441467285, loss=1.2979692220687866
I0209 03:11:58.539510 140529910601472 logging_writer.py:48] [130100] global_step=130100, grad_norm=0.7973589897155762, loss=1.338191032409668
I0209 03:12:33.868638 140529918994176 logging_writer.py:48] [130200] global_step=130200, grad_norm=0.828765869140625, loss=1.3839747905731201
I0209 03:13:09.175384 140529910601472 logging_writer.py:48] [130300] global_step=130300, grad_norm=0.8372169733047485, loss=1.3549220561981201
I0209 03:13:44.457374 140529918994176 logging_writer.py:48] [130400] global_step=130400, grad_norm=0.7838866710662842, loss=1.3822294473648071
I0209 03:14:19.722774 140529910601472 logging_writer.py:48] [130500] global_step=130500, grad_norm=0.836202085018158, loss=1.326959490776062
I0209 03:14:54.999548 140529918994176 logging_writer.py:48] [130600] global_step=130600, grad_norm=0.7991824150085449, loss=1.3670563697814941
I0209 03:15:30.283896 140529910601472 logging_writer.py:48] [130700] global_step=130700, grad_norm=0.7912654876708984, loss=1.3174233436584473
I0209 03:16:05.529533 140529918994176 logging_writer.py:48] [130800] global_step=130800, grad_norm=0.8158257007598877, loss=1.371178150177002
I0209 03:16:40.808324 140529910601472 logging_writer.py:48] [130900] global_step=130900, grad_norm=0.803996741771698, loss=1.3919042348861694
I0209 03:17:16.087483 140529918994176 logging_writer.py:48] [131000] global_step=131000, grad_norm=0.7838881611824036, loss=1.373133897781372
I0209 03:17:51.354294 140529910601472 logging_writer.py:48] [131100] global_step=131100, grad_norm=0.810544490814209, loss=1.3922512531280518
I0209 03:17:52.843803 140699726837568 spec.py:321] Evaluating on the training split.
I0209 03:17:55.894726 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 03:21:56.363626 140699726837568 spec.py:333] Evaluating on the validation split.
I0209 03:21:59.101046 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 03:24:28.504829 140699726837568 spec.py:349] Evaluating on the test split.
I0209 03:24:31.262172 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 03:26:58.996605 140699726837568 submission_runner.py:408] Time since start: 76469.67s, 	Step: 131106, 	{'train/accuracy': 0.7094533443450928, 'train/loss': 1.3412723541259766, 'train/bleu': 36.79860717293863, 'validation/accuracy': 0.6949820518493652, 'validation/loss': 1.4096693992614746, 'validation/bleu': 30.92847868651178, 'validation/num_examples': 3000, 'test/accuracy': 0.710777997970581, 'test/loss': 1.3123667240142822, 'test/bleu': 31.17334827670532, 'test/num_examples': 3003, 'score': 46231.019748449326, 'total_duration': 76469.67206168175, 'accumulated_submission_time': 46231.019748449326, 'accumulated_eval_time': 30232.55574464798, 'accumulated_logging_time': 1.9454841613769531}
I0209 03:26:59.030292 140529918994176 logging_writer.py:48] [131106] accumulated_eval_time=30232.555745, accumulated_logging_time=1.945484, accumulated_submission_time=46231.019748, global_step=131106, preemption_count=0, score=46231.019748, test/accuracy=0.710778, test/bleu=31.173348, test/loss=1.312367, test/num_examples=3003, total_duration=76469.672062, train/accuracy=0.709453, train/bleu=36.798607, train/loss=1.341272, validation/accuracy=0.694982, validation/bleu=30.928479, validation/loss=1.409669, validation/num_examples=3000
I0209 03:27:32.391869 140529910601472 logging_writer.py:48] [131200] global_step=131200, grad_norm=0.8074378371238708, loss=1.377737283706665
I0209 03:28:07.551143 140529918994176 logging_writer.py:48] [131300] global_step=131300, grad_norm=0.7945690751075745, loss=1.3627187013626099
I0209 03:28:42.766845 140529910601472 logging_writer.py:48] [131400] global_step=131400, grad_norm=0.8205682039260864, loss=1.3449604511260986
I0209 03:29:17.998695 140529918994176 logging_writer.py:48] [131500] global_step=131500, grad_norm=0.8055133819580078, loss=1.2996026277542114
I0209 03:29:53.247440 140529910601472 logging_writer.py:48] [131600] global_step=131600, grad_norm=0.827226459980011, loss=1.4148571491241455
I0209 03:30:28.524666 140529918994176 logging_writer.py:48] [131700] global_step=131700, grad_norm=0.8316850662231445, loss=1.4104061126708984
I0209 03:31:03.828181 140529910601472 logging_writer.py:48] [131800] global_step=131800, grad_norm=0.7784299850463867, loss=1.3605585098266602
I0209 03:31:39.148891 140529918994176 logging_writer.py:48] [131900] global_step=131900, grad_norm=0.793113112449646, loss=1.2712851762771606
I0209 03:32:14.451536 140529910601472 logging_writer.py:48] [132000] global_step=132000, grad_norm=0.8114083409309387, loss=1.432102918624878
I0209 03:32:49.705488 140529918994176 logging_writer.py:48] [132100] global_step=132100, grad_norm=0.8185827136039734, loss=1.3371919393539429
I0209 03:33:24.981439 140529910601472 logging_writer.py:48] [132200] global_step=132200, grad_norm=0.8120209574699402, loss=1.3572118282318115
I0209 03:34:00.228584 140529918994176 logging_writer.py:48] [132300] global_step=132300, grad_norm=0.8278681635856628, loss=1.3933624029159546
I0209 03:34:35.509450 140529910601472 logging_writer.py:48] [132400] global_step=132400, grad_norm=0.7921393513679504, loss=1.3128248453140259
I0209 03:35:10.748058 140529918994176 logging_writer.py:48] [132500] global_step=132500, grad_norm=0.8087012767791748, loss=1.357430100440979
I0209 03:35:46.019300 140529910601472 logging_writer.py:48] [132600] global_step=132600, grad_norm=0.8134094476699829, loss=1.397083044052124
I0209 03:36:21.297071 140529918994176 logging_writer.py:48] [132700] global_step=132700, grad_norm=0.7951187491416931, loss=1.3443448543548584
I0209 03:36:56.558443 140529910601472 logging_writer.py:48] [132800] global_step=132800, grad_norm=0.8066088557243347, loss=1.394078254699707
I0209 03:37:31.811240 140529918994176 logging_writer.py:48] [132900] global_step=132900, grad_norm=0.803987979888916, loss=1.3693217039108276
I0209 03:38:07.068980 140529910601472 logging_writer.py:48] [133000] global_step=133000, grad_norm=0.8064680099487305, loss=1.3027664422988892
I0209 03:38:42.328955 140529918994176 logging_writer.py:48] [133100] global_step=133100, grad_norm=0.8287739753723145, loss=1.4064862728118896
I0209 03:39:17.599421 140529910601472 logging_writer.py:48] [133200] global_step=133200, grad_norm=0.8186248540878296, loss=1.3754576444625854
I0209 03:39:52.848555 140529918994176 logging_writer.py:48] [133300] global_step=133300, grad_norm=0.7776787281036377, loss=1.302178144454956
I0209 03:40:03.861520 140699726837568 spec.py:321] Evaluating on the training split.
I0209 03:40:06.896550 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 03:43:59.906331 140699726837568 spec.py:333] Evaluating on the validation split.
I0209 03:44:02.652758 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 03:46:25.649105 140699726837568 spec.py:349] Evaluating on the test split.
I0209 03:46:28.401923 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 03:48:57.813122 140699726837568 submission_runner.py:408] Time since start: 77788.49s, 	Step: 133333, 	{'train/accuracy': 0.7125383019447327, 'train/loss': 1.3173706531524658, 'train/bleu': 36.68598863876687, 'validation/accuracy': 0.6947216987609863, 'validation/loss': 1.4095044136047363, 'validation/bleu': 30.934123880652553, 'validation/num_examples': 3000, 'test/accuracy': 0.7105455994606018, 'test/loss': 1.312601923942566, 'test/bleu': 31.11336078741953, 'test/num_examples': 3003, 'score': 47015.76848649979, 'total_duration': 77788.48857069016, 'accumulated_submission_time': 47015.76848649979, 'accumulated_eval_time': 30766.507289409637, 'accumulated_logging_time': 1.9903368949890137}
I0209 03:48:57.848084 140529910601472 logging_writer.py:48] [133333] accumulated_eval_time=30766.507289, accumulated_logging_time=1.990337, accumulated_submission_time=47015.768486, global_step=133333, preemption_count=0, score=47015.768486, test/accuracy=0.710546, test/bleu=31.113361, test/loss=1.312602, test/num_examples=3003, total_duration=77788.488571, train/accuracy=0.712538, train/bleu=36.685989, train/loss=1.317371, validation/accuracy=0.694722, validation/bleu=30.934124, validation/loss=1.409504, validation/num_examples=3000
I0209 03:48:57.882888 140529918994176 logging_writer.py:48] [133333] global_step=133333, preemption_count=0, score=47015.768486
I0209 03:48:59.150281 140699726837568 checkpoints.py:490] Saving checkpoint at step: 133333
I0209 03:49:03.256712 140699726837568 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_1/wmt_jax/trial_3/checkpoint_133333
I0209 03:49:03.261377 140699726837568 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_1/wmt_jax/trial_3/checkpoint_133333.
I0209 03:49:03.322430 140699726837568 submission_runner.py:583] Tuning trial 3/5
I0209 03:49:03.322584 140699726837568 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0209 03:49:03.329132 140699726837568 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0005962961004115641, 'train/loss': 11.175597190856934, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.208685874938965, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.19086742401123, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 26.882007360458374, 'total_duration': 905.2524693012238, 'accumulated_submission_time': 26.882007360458374, 'accumulated_eval_time': 878.3704223632812, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (2379, {'train/accuracy': 0.4058598577976227, 'train/loss': 3.9908957481384277, 'train/bleu': 14.225651382899393, 'validation/accuracy': 0.3933243155479431, 'validation/loss': 4.097593307495117, 'validation/bleu': 9.435176775149337, 'validation/num_examples': 3000, 'test/accuracy': 0.375143826007843, 'test/loss': 4.313692092895508, 'test/bleu': 7.492265560875259, 'test/num_examples': 3003, 'score': 866.837769985199, 'total_duration': 2471.213627576828, 'accumulated_submission_time': 866.837769985199, 'accumulated_eval_time': 1604.2824800014496, 'accumulated_logging_time': 0.01880478858947754, 'global_step': 2379, 'preemption_count': 0}), (4757, {'train/accuracy': 0.5393306016921997, 'train/loss': 2.7030396461486816, 'train/bleu': 24.873358742540944, 'validation/accuracy': 0.542708694934845, 'validation/loss': 2.6487877368927, 'validation/bleu': 20.43187495336819, 'validation/num_examples': 3000, 'test/accuracy': 0.5409215092658997, 'test/loss': 2.6869993209838867, 'test/bleu': 18.88999064105362, 'test/num_examples': 3003, 'score': 1707.074345588684, 'total_duration': 3796.1771688461304, 'accumulated_submission_time': 1707.074345588684, 'accumulated_eval_time': 2088.90660238266, 'accumulated_logging_time': 0.047066450119018555, 'global_step': 4757, 'preemption_count': 0}), (7136, {'train/accuracy': 0.5833343267440796, 'train/loss': 2.2597906589508057, 'train/bleu': 27.53477110939534, 'validation/accuracy': 0.5829437971115112, 'validation/loss': 2.237121105194092, 'validation/bleu': 23.01984843027956, 'validation/num_examples': 3000, 'test/accuracy': 0.586462140083313, 'test/loss': 2.2365055084228516, 'test/bleu': 22.075670983383866, 'test/num_examples': 3003, 'score': 2547.0738015174866, 'total_duration': 5107.401966333389, 'accumulated_submission_time': 2547.0738015174866, 'accumulated_eval_time': 2560.031564474106, 'accumulated_logging_time': 0.07350635528564453, 'global_step': 7136, 'preemption_count': 0}), (9516, {'train/accuracy': 0.5903454422950745, 'train/loss': 2.161125421524048, 'train/bleu': 28.423563496088704, 'validation/accuracy': 0.60672527551651, 'validation/loss': 2.036038398742676, 'validation/bleu': 24.557212452962293, 'validation/num_examples': 3000, 'test/accuracy': 0.6093777418136597, 'test/loss': 2.014265537261963, 'test/bleu': 23.438955386735778, 'test/num_examples': 3003, 'score': 3387.072328567505, 'total_duration': 6411.077891111374, 'accumulated_submission_time': 3387.072328567505, 'accumulated_eval_time': 3023.601635694504, 'accumulated_logging_time': 0.10449838638305664, 'global_step': 9516, 'preemption_count': 0}), (11897, {'train/accuracy': 0.6027729511260986, 'train/loss': 2.0559475421905518, 'train/bleu': 28.85630338895498, 'validation/accuracy': 0.6205874681472778, 'validation/loss': 1.9075512886047363, 'validation/bleu': 26.280029016395673, 'validation/num_examples': 3000, 'test/accuracy': 0.6275289058685303, 'test/loss': 1.8683427572250366, 'test/bleu': 24.511840943821287, 'test/num_examples': 3003, 'score': 4227.306416749954, 'total_duration': 7702.555572986603, 'accumulated_submission_time': 4227.306416749954, 'accumulated_eval_time': 3474.739589214325, 'accumulated_logging_time': 0.13073039054870605, 'global_step': 11897, 'preemption_count': 0}), (14279, {'train/accuracy': 0.6161245703697205, 'train/loss': 1.941306471824646, 'train/bleu': 29.923540706085337, 'validation/accuracy': 0.6305191516876221, 'validation/loss': 1.827352523803711, 'validation/bleu': 26.47629077209134, 'validation/num_examples': 3000, 'test/accuracy': 0.6389285922050476, 'test/loss': 1.7839852571487427, 'test/bleu': 25.561584130812317, 'test/num_examples': 3003, 'score': 5067.252546310425, 'total_duration': 9048.127070188522, 'accumulated_submission_time': 5067.252546310425, 'accumulated_eval_time': 3980.2613196372986, 'accumulated_logging_time': 0.15803933143615723, 'global_step': 14279, 'preemption_count': 0}), (16662, {'train/accuracy': 0.6206269860267639, 'train/loss': 1.902586579322815, 'train/bleu': 29.945450713323183, 'validation/accuracy': 0.6397068500518799, 'validation/loss': 1.7634724378585815, 'validation/bleu': 27.15322636447869, 'validation/num_examples': 3000, 'test/accuracy': 0.6466794610023499, 'test/loss': 1.7082695960998535, 'test/bleu': 26.0778676097258, 'test/num_examples': 3003, 'score': 5907.344487428665, 'total_duration': 10342.05437874794, 'accumulated_submission_time': 5907.344487428665, 'accumulated_eval_time': 4433.9902555942535, 'accumulated_logging_time': 0.18922209739685059, 'global_step': 16662, 'preemption_count': 0}), (19046, {'train/accuracy': 0.641328752040863, 'train/loss': 1.730721354484558, 'train/bleu': 31.378610385408912, 'validation/accuracy': 0.6421619057655334, 'validation/loss': 1.719527244567871, 'validation/bleu': 26.5024963346979, 'validation/num_examples': 3000, 'test/accuracy': 0.6498286128044128, 'test/loss': 1.6713584661483765, 'test/bleu': 26.317142488955124, 'test/num_examples': 3003, 'score': 6747.582292556763, 'total_duration': 11947.941531896591, 'accumulated_submission_time': 6747.582292556763, 'accumulated_eval_time': 5199.53736281395, 'accumulated_logging_time': 0.21701884269714355, 'global_step': 19046, 'preemption_count': 0}), (21429, {'train/accuracy': 0.6279956698417664, 'train/loss': 1.8332117795944214, 'train/bleu': 30.87111135681036, 'validation/accuracy': 0.6472703218460083, 'validation/loss': 1.690517783164978, 'validation/bleu': 27.717725941527142, 'validation/num_examples': 3000, 'test/accuracy': 0.6571030020713806, 'test/loss': 1.642043113708496, 'test/bleu': 26.552804953338676, 'test/num_examples': 3003, 'score': 7587.546016216278, 'total_duration': 13269.586208820343, 'accumulated_submission_time': 7587.546016216278, 'accumulated_eval_time': 5681.113517045975, 'accumulated_logging_time': 0.24637985229492188, 'global_step': 21429, 'preemption_count': 0}), (23812, {'train/accuracy': 0.6292793154716492, 'train/loss': 1.8349435329437256, 'train/bleu': 31.07930974577942, 'validation/accuracy': 0.6511016488075256, 'validation/loss': 1.673044204711914, 'validation/bleu': 27.767557476735757, 'validation/num_examples': 3000, 'test/accuracy': 0.6596711277961731, 'test/loss': 1.6173677444458008, 'test/bleu': 27.04909577649694, 'test/num_examples': 3003, 'score': 8427.509428739548, 'total_duration': 14592.367801189423, 'accumulated_submission_time': 8427.509428739548, 'accumulated_eval_time': 6163.827691793442, 'accumulated_logging_time': 0.27437496185302734, 'global_step': 23812, 'preemption_count': 0}), (26196, {'train/accuracy': 0.636094868183136, 'train/loss': 1.7777622938156128, 'train/bleu': 31.102546323862676, 'validation/accuracy': 0.6532343029975891, 'validation/loss': 1.6610873937606812, 'validation/bleu': 27.915533557356824, 'validation/num_examples': 3000, 'test/accuracy': 0.662216067314148, 'test/loss': 1.600104570388794, 'test/bleu': 27.36525046340449, 'test/num_examples': 3003, 'score': 9267.73613333702, 'total_duration': 15935.911216020584, 'accumulated_submission_time': 9267.73613333702, 'accumulated_eval_time': 6667.039098739624, 'accumulated_logging_time': 0.3026127815246582, 'global_step': 26196, 'preemption_count': 0}), (28581, {'train/accuracy': 0.6279000639915466, 'train/loss': 1.8329882621765137, 'train/bleu': 30.217147413299987, 'validation/accuracy': 0.650444507598877, 'validation/loss': 1.677872896194458, 'validation/bleu': 27.74585837118948, 'validation/num_examples': 3000, 'test/accuracy': 0.6586834192276001, 'test/loss': 1.6259721517562866, 'test/bleu': 26.974623928988517, 'test/num_examples': 3003, 'score': 10107.839243650436, 'total_duration': 17299.766478300095, 'accumulated_submission_time': 10107.839243650436, 'accumulated_eval_time': 7190.688607931137, 'accumulated_logging_time': 0.3311488628387451, 'global_step': 28581, 'preemption_count': 0}), (30964, {'train/accuracy': 0.6326751112937927, 'train/loss': 1.8045499324798584, 'train/bleu': 30.968724846774887, 'validation/accuracy': 0.6552057266235352, 'validation/loss': 1.6448639631271362, 'validation/bleu': 28.025201830906546, 'validation/num_examples': 3000, 'test/accuracy': 0.666748046875, 'test/loss': 1.587527871131897, 'test/bleu': 27.358672182603275, 'test/num_examples': 3003, 'score': 10947.769191265106, 'total_duration': 18652.812893152237, 'accumulated_submission_time': 10947.769191265106, 'accumulated_eval_time': 7703.6917362213135, 'accumulated_logging_time': 0.36203646659851074, 'global_step': 30964, 'preemption_count': 0}), (33348, {'train/accuracy': 0.6399797201156616, 'train/loss': 1.7495006322860718, 'train/bleu': 31.4189488530457, 'validation/accuracy': 0.6567928194999695, 'validation/loss': 1.6323739290237427, 'validation/bleu': 28.445252742902714, 'validation/num_examples': 3000, 'test/accuracy': 0.6678636074066162, 'test/loss': 1.5693237781524658, 'test/bleu': 27.742504843960525, 'test/num_examples': 3003, 'score': 11787.761863231659, 'total_duration': 20037.068594694138, 'accumulated_submission_time': 11787.761863231659, 'accumulated_eval_time': 8247.851219177246, 'accumulated_logging_time': 0.39116501808166504, 'global_step': 33348, 'preemption_count': 0}), (35733, {'train/accuracy': 0.636881947517395, 'train/loss': 1.7801685333251953, 'train/bleu': 31.0226532366646, 'validation/accuracy': 0.6587890982627869, 'validation/loss': 1.621066689491272, 'validation/bleu': 28.25602613523417, 'validation/num_examples': 3000, 'test/accuracy': 0.6670036911964417, 'test/loss': 1.5616695880889893, 'test/bleu': 27.611120742716544, 'test/num_examples': 3003, 'score': 12627.94023323059, 'total_duration': 21451.65731573105, 'accumulated_submission_time': 12627.94023323059, 'accumulated_eval_time': 8822.155555486679, 'accumulated_logging_time': 0.4218254089355469, 'global_step': 35733, 'preemption_count': 0}), (38118, {'train/accuracy': 0.6518439054489136, 'train/loss': 1.66854727268219, 'train/bleu': 32.117936466755715, 'validation/accuracy': 0.6590867042541504, 'validation/loss': 1.615612506866455, 'validation/bleu': 28.395664950065505, 'validation/num_examples': 3000, 'test/accuracy': 0.6677822470664978, 'test/loss': 1.551979899406433, 'test/bleu': 27.36167683701787, 'test/num_examples': 3003, 'score': 13468.088101387024, 'total_duration': 22775.507161140442, 'accumulated_submission_time': 13468.088101387024, 'accumulated_eval_time': 9305.749348640442, 'accumulated_logging_time': 0.45316171646118164, 'global_step': 38118, 'preemption_count': 0}), (40502, {'train/accuracy': 0.6441013216972351, 'train/loss': 1.7263169288635254, 'train/bleu': 31.448439318045764, 'validation/accuracy': 0.6599918007850647, 'validation/loss': 1.606161117553711, 'validation/bleu': 28.42181715596443, 'validation/num_examples': 3000, 'test/accuracy': 0.6714543104171753, 'test/loss': 1.5414283275604248, 'test/bleu': 28.16023044547092, 'test/num_examples': 3003, 'score': 14308.046524524689, 'total_duration': 24192.274977445602, 'accumulated_submission_time': 14308.046524524689, 'accumulated_eval_time': 9882.452818155289, 'accumulated_logging_time': 0.4833953380584717, 'global_step': 40502, 'preemption_count': 0}), (42887, {'train/accuracy': 0.6383638978004456, 'train/loss': 1.7570757865905762, 'train/bleu': 31.426057838089996, 'validation/accuracy': 0.6582187414169312, 'validation/loss': 1.6062474250793457, 'validation/bleu': 28.302365767108448, 'validation/num_examples': 3000, 'test/accuracy': 0.6733600497245789, 'test/loss': 1.5363410711288452, 'test/bleu': 28.174196940148576, 'test/num_examples': 3003, 'score': 15148.275726556778, 'total_duration': 25559.754118919373, 'accumulated_submission_time': 15148.275726556778, 'accumulated_eval_time': 10409.5962266922, 'accumulated_logging_time': 0.5133147239685059, 'global_step': 42887, 'preemption_count': 0}), (45271, {'train/accuracy': 0.6441451907157898, 'train/loss': 1.7160241603851318, 'train/bleu': 31.51585807077358, 'validation/accuracy': 0.6614177227020264, 'validation/loss': 1.5945888757705688, 'validation/bleu': 28.401748387926006, 'validation/num_examples': 3000, 'test/accuracy': 0.6741386651992798, 'test/loss': 1.5258550643920898, 'test/bleu': 27.84871434765751, 'test/num_examples': 3003, 'score': 15988.28655910492, 'total_duration': 26990.38134407997, 'accumulated_submission_time': 15988.28655910492, 'accumulated_eval_time': 11000.106136083603, 'accumulated_logging_time': 0.5449538230895996, 'global_step': 45271, 'preemption_count': 0}), (47656, {'train/accuracy': 0.6404057145118713, 'train/loss': 1.7511805295944214, 'train/bleu': 31.178070126696518, 'validation/accuracy': 0.6617772579193115, 'validation/loss': 1.587974190711975, 'validation/bleu': 28.050460138448788, 'validation/num_examples': 3000, 'test/accuracy': 0.674312949180603, 'test/loss': 1.5223636627197266, 'test/bleu': 28.13550040341368, 'test/num_examples': 3003, 'score': 16828.508977890015, 'total_duration': 28401.188784360886, 'accumulated_submission_time': 16828.508977890015, 'accumulated_eval_time': 11570.585859537125, 'accumulated_logging_time': 0.5759387016296387, 'global_step': 47656, 'preemption_count': 0}), (50040, {'train/accuracy': 0.6673862934112549, 'train/loss': 1.5891749858856201, 'train/bleu': 33.58396302504942, 'validation/accuracy': 0.6645546555519104, 'validation/loss': 1.5788758993148804, 'validation/bleu': 28.75264872649214, 'validation/num_examples': 3000, 'test/accuracy': 0.6757422685623169, 'test/loss': 1.516922116279602, 'test/bleu': 27.995736464992472, 'test/num_examples': 3003, 'score': 17668.541675567627, 'total_duration': 29805.83444905281, 'accumulated_submission_time': 17668.541675567627, 'accumulated_eval_time': 12135.090401649475, 'accumulated_logging_time': 0.6083238124847412, 'global_step': 50040, 'preemption_count': 0}), (52425, {'train/accuracy': 0.6453410387039185, 'train/loss': 1.7113089561462402, 'train/bleu': 31.580043661207323, 'validation/accuracy': 0.6636619567871094, 'validation/loss': 1.575170874595642, 'validation/bleu': 28.384619001282065, 'validation/num_examples': 3000, 'test/accuracy': 0.6774504780769348, 'test/loss': 1.5054657459259033, 'test/bleu': 27.875327607675807, 'test/num_examples': 3003, 'score': 18508.653678417206, 'total_duration': 31176.78390431404, 'accumulated_submission_time': 18508.653678417206, 'accumulated_eval_time': 12665.822091341019, 'accumulated_logging_time': 0.6415340900421143, 'global_step': 52425, 'preemption_count': 0}), (54810, {'train/accuracy': 0.6456965804100037, 'train/loss': 1.7131825685501099, 'train/bleu': 31.870851020784162, 'validation/accuracy': 0.6642695069313049, 'validation/loss': 1.5717233419418335, 'validation/bleu': 28.47791587474528, 'validation/num_examples': 3000, 'test/accuracy': 0.6790425181388855, 'test/loss': 1.4980599880218506, 'test/bleu': 28.441837741320292, 'test/num_examples': 3003, 'score': 19348.648066282272, 'total_duration': 32530.86065387726, 'accumulated_submission_time': 19348.648066282272, 'accumulated_eval_time': 13179.797394990921, 'accumulated_logging_time': 0.6752762794494629, 'global_step': 54810, 'preemption_count': 0}), (57195, {'train/accuracy': 0.6577078104019165, 'train/loss': 1.6320240497589111, 'train/bleu': 32.78105983753737, 'validation/accuracy': 0.667592465877533, 'validation/loss': 1.5603337287902832, 'validation/bleu': 29.03892667850046, 'validation/num_examples': 3000, 'test/accuracy': 0.6807158589363098, 'test/loss': 1.4926679134368896, 'test/bleu': 28.451057156751197, 'test/num_examples': 3003, 'score': 20188.616775512695, 'total_duration': 33872.19545149803, 'accumulated_submission_time': 20188.616775512695, 'accumulated_eval_time': 13681.056218147278, 'accumulated_logging_time': 0.7081527709960938, 'global_step': 57195, 'preemption_count': 0}), (59580, {'train/accuracy': 0.6465297937393188, 'train/loss': 1.694580316543579, 'train/bleu': 32.05305332294858, 'validation/accuracy': 0.6670469045639038, 'validation/loss': 1.5555315017700195, 'validation/bleu': 28.62439054702784, 'validation/num_examples': 3000, 'test/accuracy': 0.6815292835235596, 'test/loss': 1.4816378355026245, 'test/bleu': 28.475886821190702, 'test/num_examples': 3003, 'score': 21028.747178077698, 'total_duration': 35296.116545677185, 'accumulated_submission_time': 21028.747178077698, 'accumulated_eval_time': 14264.738429784775, 'accumulated_logging_time': 0.74080491065979, 'global_step': 59580, 'preemption_count': 0}), (61965, {'train/accuracy': 0.648226261138916, 'train/loss': 1.7013450860977173, 'train/bleu': 32.419402937612425, 'validation/accuracy': 0.6691423654556274, 'validation/loss': 1.5469614267349243, 'validation/bleu': 29.033741216883577, 'validation/num_examples': 3000, 'test/accuracy': 0.6838417649269104, 'test/loss': 1.4731414318084717, 'test/bleu': 28.84332825064846, 'test/num_examples': 3003, 'score': 21868.823503017426, 'total_duration': 36625.751838207245, 'accumulated_submission_time': 21868.823503017426, 'accumulated_eval_time': 14754.18920135498, 'accumulated_logging_time': 0.7733290195465088, 'global_step': 61965, 'preemption_count': 0}), (64349, {'train/accuracy': 0.6545460820198059, 'train/loss': 1.6462438106536865, 'train/bleu': 32.617605237832414, 'validation/accuracy': 0.6699978709220886, 'validation/loss': 1.5386850833892822, 'validation/bleu': 29.007868109997457, 'validation/num_examples': 3000, 'test/accuracy': 0.6819010972976685, 'test/loss': 1.4711955785751343, 'test/bleu': 28.584893070290796, 'test/num_examples': 3003, 'score': 22708.90673828125, 'total_duration': 37976.13408732414, 'accumulated_submission_time': 22708.90673828125, 'accumulated_eval_time': 15264.377411842346, 'accumulated_logging_time': 0.8070766925811768, 'global_step': 64349, 'preemption_count': 0}), (66733, {'train/accuracy': 0.6512013673782349, 'train/loss': 1.6751712560653687, 'train/bleu': 32.131119778833856, 'validation/accuracy': 0.6714361906051636, 'validation/loss': 1.5344096422195435, 'validation/bleu': 29.08472915961638, 'validation/num_examples': 3000, 'test/accuracy': 0.6844227910041809, 'test/loss': 1.462852954864502, 'test/bleu': 28.940553718862866, 'test/num_examples': 3003, 'score': 23548.84869503975, 'total_duration': 39290.32270479202, 'accumulated_submission_time': 23548.84869503975, 'accumulated_eval_time': 15738.510041713715, 'accumulated_logging_time': 0.8472380638122559, 'global_step': 66733, 'preemption_count': 0}), (69117, {'train/accuracy': 0.6723154783248901, 'train/loss': 1.5345184803009033, 'train/bleu': 33.717381365500145, 'validation/accuracy': 0.6729241013526917, 'validation/loss': 1.5261874198913574, 'validation/bleu': 29.173103041104703, 'validation/num_examples': 3000, 'test/accuracy': 0.6858404874801636, 'test/loss': 1.4534401893615723, 'test/bleu': 29.00716191550685, 'test/num_examples': 3003, 'score': 24388.915120363235, 'total_duration': 40672.09962892532, 'accumulated_submission_time': 24388.915120363235, 'accumulated_eval_time': 16280.106495141983, 'accumulated_logging_time': 0.8839507102966309, 'global_step': 69117, 'preemption_count': 0}), (71501, {'train/accuracy': 0.65537029504776, 'train/loss': 1.6473228931427002, 'train/bleu': 32.375148555377464, 'validation/accuracy': 0.6737300157546997, 'validation/loss': 1.5176098346710205, 'validation/bleu': 29.287501929283852, 'validation/num_examples': 3000, 'test/accuracy': 0.685910165309906, 'test/loss': 1.4481548070907593, 'test/bleu': 28.593936374173712, 'test/num_examples': 3003, 'score': 25228.996363401413, 'total_duration': 42058.55702161789, 'accumulated_submission_time': 25228.996363401413, 'accumulated_eval_time': 16826.37229347229, 'accumulated_logging_time': 0.9204680919647217, 'global_step': 71501, 'preemption_count': 0}), (73886, {'train/accuracy': 0.6524078845977783, 'train/loss': 1.6667128801345825, 'train/bleu': 32.701466533580295, 'validation/accuracy': 0.6737548112869263, 'validation/loss': 1.5123512744903564, 'validation/bleu': 29.320466587101414, 'validation/num_examples': 3000, 'test/accuracy': 0.6876416206359863, 'test/loss': 1.4385989904403687, 'test/bleu': 28.943381743475623, 'test/num_examples': 3003, 'score': 26069.110144138336, 'total_duration': 43468.737097263336, 'accumulated_submission_time': 26069.110144138336, 'accumulated_eval_time': 17396.327433347702, 'accumulated_logging_time': 0.9564275741577148, 'global_step': 73886, 'preemption_count': 0}), (76269, {'train/accuracy': 0.6652563810348511, 'train/loss': 1.5872588157653809, 'train/bleu': 33.20367223177718, 'validation/accuracy': 0.6766065955162048, 'validation/loss': 1.50298011302948, 'validation/bleu': 29.703277161865785, 'validation/num_examples': 3000, 'test/accuracy': 0.6899773478507996, 'test/loss': 1.4313315153121948, 'test/bleu': 29.44708155748432, 'test/num_examples': 3003, 'score': 26909.085456848145, 'total_duration': 44773.62126874924, 'accumulated_submission_time': 26909.085456848145, 'accumulated_eval_time': 17861.11874818802, 'accumulated_logging_time': 0.9940569400787354, 'global_step': 76269, 'preemption_count': 0}), (78654, {'train/accuracy': 0.6580626368522644, 'train/loss': 1.6298712491989136, 'train/bleu': 32.67718189408843, 'validation/accuracy': 0.67764812707901, 'validation/loss': 1.4988480806350708, 'validation/bleu': 29.646954785860082, 'validation/num_examples': 3000, 'test/accuracy': 0.6922898292541504, 'test/loss': 1.4208958148956299, 'test/bleu': 29.830781468889445, 'test/num_examples': 3003, 'score': 27749.27845311165, 'total_duration': 46152.97341346741, 'accumulated_submission_time': 27749.27845311165, 'accumulated_eval_time': 18400.163288593292, 'accumulated_logging_time': 1.0356485843658447, 'global_step': 78654, 'preemption_count': 0}), (81039, {'train/accuracy': 0.6602153778076172, 'train/loss': 1.6250991821289062, 'train/bleu': 32.57357972662718, 'validation/accuracy': 0.6790988445281982, 'validation/loss': 1.4938795566558838, 'validation/bleu': 29.843115183079956, 'validation/num_examples': 3000, 'test/accuracy': 0.6907210946083069, 'test/loss': 1.4177829027175903, 'test/bleu': 29.211380677165277, 'test/num_examples': 3003, 'score': 28589.20459485054, 'total_duration': 47664.87612128258, 'accumulated_submission_time': 28589.20459485054, 'accumulated_eval_time': 19072.028824090958, 'accumulated_logging_time': 1.0736279487609863, 'global_step': 81039, 'preemption_count': 0}), (83423, {'train/accuracy': 0.6681274175643921, 'train/loss': 1.57500159740448, 'train/bleu': 32.9723800626729, 'validation/accuracy': 0.6786648631095886, 'validation/loss': 1.4860962629318237, 'validation/bleu': 29.676601764832707, 'validation/num_examples': 3000, 'test/accuracy': 0.6939747929573059, 'test/loss': 1.405526041984558, 'test/bleu': 29.724564276034794, 'test/num_examples': 3003, 'score': 29429.326312065125, 'total_duration': 49049.69407105446, 'accumulated_submission_time': 29429.326312065125, 'accumulated_eval_time': 19616.61013817787, 'accumulated_logging_time': 1.1113903522491455, 'global_step': 83423, 'preemption_count': 0}), (85808, {'train/accuracy': 0.6655429601669312, 'train/loss': 1.5882421731948853, 'train/bleu': 33.31576160365833, 'validation/accuracy': 0.6799171566963196, 'validation/loss': 1.4798542261123657, 'validation/bleu': 29.792160334268157, 'validation/num_examples': 3000, 'test/accuracy': 0.694195568561554, 'test/loss': 1.39683997631073, 'test/bleu': 29.681786035169242, 'test/num_examples': 3003, 'score': 30269.53056025505, 'total_duration': 50450.160507917404, 'accumulated_submission_time': 30269.53056025505, 'accumulated_eval_time': 20176.76056933403, 'accumulated_logging_time': 1.147826910018921, 'global_step': 85808, 'preemption_count': 0}), (88193, {'train/accuracy': 0.6824063062667847, 'train/loss': 1.4828976392745972, 'train/bleu': 34.05213368530559, 'validation/accuracy': 0.6825209856033325, 'validation/loss': 1.4713144302368164, 'validation/bleu': 30.038753265463793, 'validation/num_examples': 3000, 'test/accuracy': 0.6972517967224121, 'test/loss': 1.393563151359558, 'test/bleu': 30.091496010180137, 'test/num_examples': 3003, 'score': 31109.656126499176, 'total_duration': 51791.548968076706, 'accumulated_submission_time': 31109.656126499176, 'accumulated_eval_time': 20677.904341459274, 'accumulated_logging_time': 1.1917221546173096, 'global_step': 88193, 'preemption_count': 0}), (90578, {'train/accuracy': 0.6718730330467224, 'train/loss': 1.5490468740463257, 'train/bleu': 33.41231035188514, 'validation/accuracy': 0.6825333833694458, 'validation/loss': 1.4652026891708374, 'validation/bleu': 30.102713832189707, 'validation/num_examples': 3000, 'test/accuracy': 0.6974958181381226, 'test/loss': 1.3812869787216187, 'test/bleu': 29.89253457886926, 'test/num_examples': 3003, 'score': 31949.858066558838, 'total_duration': 53249.28406596184, 'accumulated_submission_time': 31949.858066558838, 'accumulated_eval_time': 21295.32447552681, 'accumulated_logging_time': 1.2300746440887451, 'global_step': 90578, 'preemption_count': 0}), (92963, {'train/accuracy': 0.6706978678703308, 'train/loss': 1.5593184232711792, 'train/bleu': 33.93765018553232, 'validation/accuracy': 0.6834261417388916, 'validation/loss': 1.4623475074768066, 'validation/bleu': 29.940012422735734, 'validation/num_examples': 3000, 'test/accuracy': 0.6990064382553101, 'test/loss': 1.3774524927139282, 'test/bleu': 30.2127779651062, 'test/num_examples': 3003, 'score': 32790.02381038666, 'total_duration': 54599.341495513916, 'accumulated_submission_time': 32790.02381038666, 'accumulated_eval_time': 21805.096822977066, 'accumulated_logging_time': 1.2744011878967285, 'global_step': 92963, 'preemption_count': 0}), (95347, {'train/accuracy': 0.6808968186378479, 'train/loss': 1.4978055953979492, 'train/bleu': 34.68351336887508, 'validation/accuracy': 0.6850131750106812, 'validation/loss': 1.4518790245056152, 'validation/bleu': 30.293797765390387, 'validation/num_examples': 3000, 'test/accuracy': 0.6996804475784302, 'test/loss': 1.3698073625564575, 'test/bleu': 29.981582976534877, 'test/num_examples': 3003, 'score': 33630.12921476364, 'total_duration': 55971.70452427864, 'accumulated_submission_time': 33630.12921476364, 'accumulated_eval_time': 22337.240693807602, 'accumulated_logging_time': 1.3123936653137207, 'global_step': 95347, 'preemption_count': 0}), (97731, {'train/accuracy': 0.6694570183753967, 'train/loss': 1.563796043395996, 'train/bleu': 33.762738030108665, 'validation/accuracy': 0.6860547065734863, 'validation/loss': 1.4455714225769043, 'validation/bleu': 30.13158593552824, 'validation/num_examples': 3000, 'test/accuracy': 0.7033641338348389, 'test/loss': 1.3601025342941284, 'test/bleu': 30.140830859153333, 'test/num_examples': 3003, 'score': 34470.28120446205, 'total_duration': 57306.02733922005, 'accumulated_submission_time': 34470.28120446205, 'accumulated_eval_time': 22831.28604865074, 'accumulated_logging_time': 1.3595654964447021, 'global_step': 97731, 'preemption_count': 0}), (100116, {'train/accuracy': 0.709169864654541, 'train/loss': 1.3537744283676147, 'train/bleu': 36.85112230721641, 'validation/accuracy': 0.6860795021057129, 'validation/loss': 1.4437220096588135, 'validation/bleu': 30.376215081158747, 'validation/num_examples': 3000, 'test/accuracy': 0.7017953991889954, 'test/loss': 1.3589884042739868, 'test/bleu': 30.204320024066764, 'test/num_examples': 3003, 'score': 35310.500405311584, 'total_duration': 58661.151460170746, 'accumulated_submission_time': 35310.500405311584, 'accumulated_eval_time': 23346.078429937363, 'accumulated_logging_time': 1.3988215923309326, 'global_step': 100116, 'preemption_count': 0}), (102500, {'train/accuracy': 0.6830866932868958, 'train/loss': 1.4718409776687622, 'train/bleu': 34.33374070683871, 'validation/accuracy': 0.6886585354804993, 'validation/loss': 1.4326103925704956, 'validation/bleu': 30.327454897877665, 'validation/num_examples': 3000, 'test/accuracy': 0.7032363414764404, 'test/loss': 1.3476167917251587, 'test/bleu': 30.557944063580372, 'test/num_examples': 3003, 'score': 36150.45610380173, 'total_duration': 60024.18383717537, 'accumulated_submission_time': 36150.45610380173, 'accumulated_eval_time': 23869.03919649124, 'accumulated_logging_time': 1.439997673034668, 'global_step': 102500, 'preemption_count': 0}), (104884, {'train/accuracy': 0.6831173896789551, 'train/loss': 1.4802350997924805, 'train/bleu': 34.37978318584616, 'validation/accuracy': 0.6883485317230225, 'validation/loss': 1.4367409944534302, 'validation/bleu': 30.31289615139876, 'validation/num_examples': 3000, 'test/accuracy': 0.7036778926849365, 'test/loss': 1.347154140472412, 'test/bleu': 30.484548709173747, 'test/num_examples': 3003, 'score': 36990.53392624855, 'total_duration': 61390.499673843384, 'accumulated_submission_time': 36990.53392624855, 'accumulated_eval_time': 24395.162294387817, 'accumulated_logging_time': 1.479119062423706, 'global_step': 104884, 'preemption_count': 0}), (107269, {'train/accuracy': 0.6964491009712219, 'train/loss': 1.404707908630371, 'train/bleu': 35.48056756531287, 'validation/accuracy': 0.6896876692771912, 'validation/loss': 1.4275447130203247, 'validation/bleu': 30.40983792345602, 'validation/num_examples': 3000, 'test/accuracy': 0.7058392763137817, 'test/loss': 1.339435338973999, 'test/bleu': 30.684268005675964, 'test/num_examples': 3003, 'score': 37830.54970383644, 'total_duration': 62745.49480700493, 'accumulated_submission_time': 37830.54970383644, 'accumulated_eval_time': 24910.0284614563, 'accumulated_logging_time': 1.5198464393615723, 'global_step': 107269, 'preemption_count': 0}), (109653, {'train/accuracy': 0.6924313902854919, 'train/loss': 1.4321004152297974, 'train/bleu': 35.2917509143731, 'validation/accuracy': 0.6913739442825317, 'validation/loss': 1.4228721857070923, 'validation/bleu': 30.570859095014146, 'validation/num_examples': 3000, 'test/accuracy': 0.7071059346199036, 'test/loss': 1.3327090740203857, 'test/bleu': 30.789080482976654, 'test/num_examples': 3003, 'score': 38670.55089187622, 'total_duration': 64133.76025557518, 'accumulated_submission_time': 38670.55089187622, 'accumulated_eval_time': 25458.177261590958, 'accumulated_logging_time': 1.561103343963623, 'global_step': 109653, 'preemption_count': 0}), (112037, {'train/accuracy': 0.6871734857559204, 'train/loss': 1.4625294208526611, 'train/bleu': 35.0512106849693, 'validation/accuracy': 0.6905679702758789, 'validation/loss': 1.4227373600006104, 'validation/bleu': 30.782762669814545, 'validation/num_examples': 3000, 'test/accuracy': 0.7082796096801758, 'test/loss': 1.3301644325256348, 'test/bleu': 30.976652739571417, 'test/num_examples': 3003, 'score': 39510.55522322655, 'total_duration': 65477.030385017395, 'accumulated_submission_time': 39510.55522322655, 'accumulated_eval_time': 25961.32877779007, 'accumulated_logging_time': 1.60233473777771, 'global_step': 112037, 'preemption_count': 0}), (114421, {'train/accuracy': 0.699521541595459, 'train/loss': 1.3912333250045776, 'train/bleu': 36.07696290784867, 'validation/accuracy': 0.6913739442825317, 'validation/loss': 1.4179686307907104, 'validation/bleu': 30.715100454032235, 'validation/num_examples': 3000, 'test/accuracy': 0.7078379988670349, 'test/loss': 1.3299546241760254, 'test/bleu': 30.80652754405374, 'test/num_examples': 3003, 'score': 40350.648983716965, 'total_duration': 66829.55691623688, 'accumulated_submission_time': 40350.648983716965, 'accumulated_eval_time': 26473.644669294357, 'accumulated_logging_time': 1.644965410232544, 'global_step': 114421, 'preemption_count': 0}), (116804, {'train/accuracy': 0.6963258385658264, 'train/loss': 1.4070066213607788, 'train/bleu': 35.96184061155457, 'validation/accuracy': 0.6927750110626221, 'validation/loss': 1.4155522584915161, 'validation/bleu': 30.68112671416496, 'validation/num_examples': 3000, 'test/accuracy': 0.7087211608886719, 'test/loss': 1.3230842351913452, 'test/bleu': 30.802705175820623, 'test/num_examples': 3003, 'score': 41190.77096199989, 'total_duration': 68200.31424498558, 'accumulated_submission_time': 41190.77096199989, 'accumulated_eval_time': 27004.1587600708, 'accumulated_logging_time': 1.687828779220581, 'global_step': 116804, 'preemption_count': 0}), (119188, {'train/accuracy': 0.7110795378684998, 'train/loss': 1.3276680707931519, 'train/bleu': 37.20115594615644, 'validation/accuracy': 0.6929734349250793, 'validation/loss': 1.4138745069503784, 'validation/bleu': 30.86193975348537, 'validation/num_examples': 3000, 'test/accuracy': 0.7092092633247375, 'test/loss': 1.3204371929168701, 'test/bleu': 30.994883238838508, 'test/num_examples': 3003, 'score': 42030.960902929306, 'total_duration': 69574.35052037239, 'accumulated_submission_time': 42030.960902929306, 'accumulated_eval_time': 27537.889677286148, 'accumulated_logging_time': 1.7288134098052979, 'global_step': 119188, 'preemption_count': 0}), (121572, {'train/accuracy': 0.7089225053787231, 'train/loss': 1.3409868478775024, 'train/bleu': 36.699779532755656, 'validation/accuracy': 0.6929982304573059, 'validation/loss': 1.4128586053848267, 'validation/bleu': 30.743967260596456, 'validation/num_examples': 3000, 'test/accuracy': 0.7098135352134705, 'test/loss': 1.3198758363723755, 'test/bleu': 30.807151339414787, 'test/num_examples': 3003, 'score': 42870.91543865204, 'total_duration': 70966.79245257378, 'accumulated_submission_time': 42870.91543865204, 'accumulated_eval_time': 28090.259249210358, 'accumulated_logging_time': 1.7727289199829102, 'global_step': 121572, 'preemption_count': 0}), (123956, {'train/accuracy': 0.7044199109077454, 'train/loss': 1.3619587421417236, 'train/bleu': 36.63037870395407, 'validation/accuracy': 0.6941761374473572, 'validation/loss': 1.409651756286621, 'validation/bleu': 30.92822877713484, 'validation/num_examples': 3000, 'test/accuracy': 0.7102085947990417, 'test/loss': 1.3139033317565918, 'test/bleu': 31.06983647732182, 'test/num_examples': 3003, 'score': 43710.88386774063, 'total_duration': 72332.34792470932, 'accumulated_submission_time': 43710.88386774063, 'accumulated_eval_time': 28615.730256080627, 'accumulated_logging_time': 1.814807415008545, 'global_step': 123956, 'preemption_count': 0}), (126339, {'train/accuracy': 0.7150582075119019, 'train/loss': 1.3104259967803955, 'train/bleu': 36.93187865217039, 'validation/accuracy': 0.6944612860679626, 'validation/loss': 1.411051869392395, 'validation/bleu': 30.66054748234449, 'validation/num_examples': 3000, 'test/accuracy': 0.7101853489875793, 'test/loss': 1.3157200813293457, 'test/bleu': 30.995070613341326, 'test/num_examples': 3003, 'score': 44550.9148273468, 'total_duration': 73694.35182857513, 'accumulated_submission_time': 44550.9148273468, 'accumulated_eval_time': 29137.580555915833, 'accumulated_logging_time': 1.8581435680389404, 'global_step': 126339, 'preemption_count': 0}), (128723, {'train/accuracy': 0.7099118232727051, 'train/loss': 1.337271809577942, 'train/bleu': 36.93021401210776, 'validation/accuracy': 0.6949200630187988, 'validation/loss': 1.4093292951583862, 'validation/bleu': 30.931063555305773, 'validation/num_examples': 3000, 'test/accuracy': 0.7109407186508179, 'test/loss': 1.3132773637771606, 'test/bleu': 31.110231469925782, 'test/num_examples': 3003, 'score': 45391.13405776024, 'total_duration': 75083.51441955566, 'accumulated_submission_time': 45391.13405776024, 'accumulated_eval_time': 29686.40299320221, 'accumulated_logging_time': 1.9021804332733154, 'global_step': 128723, 'preemption_count': 0}), (131106, {'train/accuracy': 0.7094533443450928, 'train/loss': 1.3412723541259766, 'train/bleu': 36.79860717293863, 'validation/accuracy': 0.6949820518493652, 'validation/loss': 1.4096693992614746, 'validation/bleu': 30.92847868651178, 'validation/num_examples': 3000, 'test/accuracy': 0.710777997970581, 'test/loss': 1.3123667240142822, 'test/bleu': 31.17334827670532, 'test/num_examples': 3003, 'score': 46231.019748449326, 'total_duration': 76469.67206168175, 'accumulated_submission_time': 46231.019748449326, 'accumulated_eval_time': 30232.55574464798, 'accumulated_logging_time': 1.9454841613769531, 'global_step': 131106, 'preemption_count': 0}), (133333, {'train/accuracy': 0.7125383019447327, 'train/loss': 1.3173706531524658, 'train/bleu': 36.68598863876687, 'validation/accuracy': 0.6947216987609863, 'validation/loss': 1.4095044136047363, 'validation/bleu': 30.934123880652553, 'validation/num_examples': 3000, 'test/accuracy': 0.7105455994606018, 'test/loss': 1.312601923942566, 'test/bleu': 31.11336078741953, 'test/num_examples': 3003, 'score': 47015.76848649979, 'total_duration': 77788.48857069016, 'accumulated_submission_time': 47015.76848649979, 'accumulated_eval_time': 30766.507289409637, 'accumulated_logging_time': 1.9903368949890137, 'global_step': 133333, 'preemption_count': 0})], 'global_step': 133333}
I0209 03:49:03.329354 140699726837568 submission_runner.py:586] Timing: 47015.76848649979
I0209 03:49:03.329406 140699726837568 submission_runner.py:588] Total number of evals: 57
I0209 03:49:03.329447 140699726837568 submission_runner.py:589] ====================
I0209 03:49:03.329506 140699726837568 submission_runner.py:542] Using RNG seed 529981238
I0209 03:49:03.330986 140699726837568 submission_runner.py:551] --- Tuning run 4/5 ---
I0209 03:49:03.331090 140699726837568 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_1/wmt_jax/trial_4.
I0209 03:49:03.331526 140699726837568 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_1/wmt_jax/trial_4/hparams.json.
I0209 03:49:03.332319 140699726837568 submission_runner.py:206] Initializing dataset.
I0209 03:49:03.335065 140699726837568 dataset_info.py:578] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0209 03:49:03.338704 140699726837568 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0209 03:49:03.378260 140699726837568 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0209 03:49:04.090990 140699726837568 submission_runner.py:213] Initializing model.
I0209 03:49:11.145011 140699726837568 submission_runner.py:255] Initializing optimizer.
I0209 03:49:11.974546 140699726837568 submission_runner.py:262] Initializing metrics bundle.
I0209 03:49:11.974790 140699726837568 submission_runner.py:280] Initializing checkpoint and logger.
I0209 03:49:11.976255 140699726837568 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_1/wmt_jax/trial_4 with prefix checkpoint_
I0209 03:49:11.976390 140699726837568 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_1/wmt_jax/trial_4/meta_data_0.json.
I0209 03:49:11.976762 140699726837568 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0209 03:49:11.976907 140699726837568 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0209 03:49:12.514828 140699726837568 logger_utils.py:220] Unable to record git information. Continuing without it.
I0209 03:49:13.031168 140699726837568 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_1/wmt_jax/trial_4/flags_0.json.
I0209 03:49:13.034997 140699726837568 submission_runner.py:314] Starting training loop.
I0209 03:49:43.921664 140529887049472 logging_writer.py:48] [0] global_step=0, grad_norm=6.158485412597656, loss=11.173095703125
I0209 03:49:43.934971 140699726837568 spec.py:321] Evaluating on the training split.
I0209 03:49:46.658240 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 03:54:36.569249 140699726837568 spec.py:333] Evaluating on the validation split.
I0209 03:54:39.311652 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 03:59:28.308897 140699726837568 spec.py:349] Evaluating on the test split.
I0209 03:59:31.055153 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 04:04:21.469855 140699726837568 submission_runner.py:408] Time since start: 908.43s, 	Step: 1, 	{'train/accuracy': 0.0006454141112044454, 'train/loss': 11.175609588623047, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.208685874938965, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.19086742401123, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 30.8999125957489, 'total_duration': 908.4347548484802, 'accumulated_submission_time': 30.8999125957489, 'accumulated_eval_time': 877.5347895622253, 'accumulated_logging_time': 0}
I0209 04:04:21.480941 140529895442176 logging_writer.py:48] [1] accumulated_eval_time=877.534790, accumulated_logging_time=0, accumulated_submission_time=30.899913, global_step=1, preemption_count=0, score=30.899913, test/accuracy=0.000709, test/bleu=0.000000, test/loss=11.190867, test/num_examples=3003, total_duration=908.434755, train/accuracy=0.000645, train/bleu=0.000000, train/loss=11.175610, validation/accuracy=0.000484, validation/bleu=0.000000, validation/loss=11.208686, validation/num_examples=3000
I0209 04:04:56.761430 140529887049472 logging_writer.py:48] [100] global_step=100, grad_norm=0.9120364189147949, loss=7.576748847961426
I0209 04:05:32.040238 140529895442176 logging_writer.py:48] [200] global_step=200, grad_norm=0.6719930768013, loss=6.599908351898193
I0209 04:06:07.330288 140529887049472 logging_writer.py:48] [300] global_step=300, grad_norm=0.5261837244033813, loss=5.804872035980225
I0209 04:06:42.640583 140529895442176 logging_writer.py:48] [400] global_step=400, grad_norm=0.46051791310310364, loss=5.349123954772949
I0209 04:07:17.968559 140529887049472 logging_writer.py:48] [500] global_step=500, grad_norm=0.6017723083496094, loss=5.063741683959961
I0209 04:07:53.268680 140529895442176 logging_writer.py:48] [600] global_step=600, grad_norm=0.5340909957885742, loss=4.792167663574219
I0209 04:08:28.575322 140529887049472 logging_writer.py:48] [700] global_step=700, grad_norm=0.503132164478302, loss=4.575741291046143
I0209 04:09:03.902545 140529895442176 logging_writer.py:48] [800] global_step=800, grad_norm=0.4641398787498474, loss=4.175586223602295
I0209 04:09:39.185422 140529887049472 logging_writer.py:48] [900] global_step=900, grad_norm=0.4426079988479614, loss=4.074409008026123
I0209 04:10:14.467468 140529895442176 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.42510727047920227, loss=3.7905826568603516
I0209 04:10:49.729774 140529887049472 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.3690357208251953, loss=3.5921950340270996
I0209 04:11:25.002896 140529895442176 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.5717382431030273, loss=3.6164309978485107
I0209 04:12:00.292394 140529887049472 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.37413421273231506, loss=3.488593578338623
I0209 04:12:35.609468 140529895442176 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.3712480962276459, loss=3.3654961585998535
I0209 04:13:10.949100 140529887049472 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.27076098322868347, loss=3.227691650390625
I0209 04:13:46.305588 140529895442176 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.2654857337474823, loss=3.0955731868743896
I0209 04:14:21.621153 140529887049472 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.26452380418777466, loss=2.9984190464019775
I0209 04:14:56.909054 140529895442176 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.23360653221607208, loss=2.916961431503296
I0209 04:15:32.166763 140529887049472 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.2862197160720825, loss=2.9298195838928223
I0209 04:16:07.496194 140529895442176 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.23085306584835052, loss=2.9039711952209473
I0209 04:16:42.764118 140529887049472 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.2871246039867401, loss=2.787496328353882
I0209 04:17:18.036764 140529895442176 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.23148399591445923, loss=2.7511894702911377
I0209 04:17:53.297290 140529887049472 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.24632079899311066, loss=2.7155520915985107
I0209 04:18:21.558489 140699726837568 spec.py:321] Evaluating on the training split.
I0209 04:18:24.586816 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 04:21:24.127847 140699726837568 spec.py:333] Evaluating on the validation split.
I0209 04:21:26.867221 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 04:24:18.592844 140699726837568 spec.py:349] Evaluating on the test split.
I0209 04:24:21.333946 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 04:27:07.282777 140699726837568 submission_runner.py:408] Time since start: 2274.25s, 	Step: 2382, 	{'train/accuracy': 0.5327548980712891, 'train/loss': 2.6247782707214355, 'train/bleu': 24.206353735215085, 'validation/accuracy': 0.537860631942749, 'validation/loss': 2.5653810501098633, 'validation/bleu': 20.049483860443015, 'validation/num_examples': 3000, 'test/accuracy': 0.5347859263420105, 'test/loss': 2.591620922088623, 'test/bleu': 18.522566174985347, 'test/num_examples': 3003, 'score': 870.8914546966553, 'total_duration': 2274.2477111816406, 'accumulated_submission_time': 870.8914546966553, 'accumulated_eval_time': 1403.2590272426605, 'accumulated_logging_time': 0.02254319190979004}
I0209 04:27:07.297625 140529895442176 logging_writer.py:48] [2382] accumulated_eval_time=1403.259027, accumulated_logging_time=0.022543, accumulated_submission_time=870.891455, global_step=2382, preemption_count=0, score=870.891455, test/accuracy=0.534786, test/bleu=18.522566, test/loss=2.591621, test/num_examples=3003, total_duration=2274.247711, train/accuracy=0.532755, train/bleu=24.206354, train/loss=2.624778, validation/accuracy=0.537861, validation/bleu=20.049484, validation/loss=2.565381, validation/num_examples=3000
I0209 04:27:13.972936 140529887049472 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.2232026606798172, loss=2.6450233459472656
I0209 04:27:49.115539 140529895442176 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.2702448070049286, loss=2.6548361778259277
I0209 04:28:24.339295 140529887049472 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.20373524725437164, loss=2.4771440029144287
I0209 04:28:59.627193 140529895442176 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.30140772461891174, loss=2.510805130004883
I0209 04:29:34.849366 140529887049472 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.20846423506736755, loss=2.4710326194763184
I0209 04:30:10.088303 140529895442176 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.2156989872455597, loss=2.4159162044525146
I0209 04:30:45.349364 140529887049472 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.19579476118087769, loss=2.481846570968628
I0209 04:31:20.586657 140529895442176 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.23698456585407257, loss=2.392766237258911
I0209 04:31:55.848016 140529887049472 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.5397160649299622, loss=2.376211166381836
I0209 04:32:31.163875 140529895442176 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.3681143820285797, loss=2.3664236068725586
I0209 04:33:06.509536 140529887049472 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.3686753511428833, loss=2.393944025039673
I0209 04:33:41.788555 140529895442176 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.3793374300003052, loss=2.3231348991394043
I0209 04:34:17.051187 140529887049472 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.21555902063846588, loss=2.338658332824707
I0209 04:34:52.316870 140529895442176 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.3844996690750122, loss=2.258017063140869
I0209 04:35:27.564506 140529887049472 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.40978747606277466, loss=2.3632214069366455
I0209 04:36:02.789947 140529895442176 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.24523931741714478, loss=2.299762487411499
I0209 04:36:38.042053 140529887049472 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.30346548557281494, loss=2.307015895843506
I0209 04:37:13.291462 140529895442176 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.4386795461177826, loss=2.2307114601135254
I0209 04:37:48.545654 140529887049472 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.9533926844596863, loss=2.2503409385681152
I0209 04:38:23.825474 140529895442176 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.4094236195087433, loss=2.229104518890381
I0209 04:38:59.061956 140529887049472 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.2954091429710388, loss=2.2568323612213135
I0209 04:39:34.382671 140529895442176 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.7420628070831299, loss=2.1899449825286865
I0209 04:40:09.682314 140529887049472 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.22798173129558563, loss=2.2295684814453125
I0209 04:40:44.929385 140529895442176 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.26891887187957764, loss=2.098451614379883
I0209 04:41:07.540724 140699726837568 spec.py:321] Evaluating on the training split.
I0209 04:41:10.563343 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 04:44:15.228964 140699726837568 spec.py:333] Evaluating on the validation split.
I0209 04:44:17.981776 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 04:47:08.201133 140699726837568 spec.py:349] Evaluating on the test split.
I0209 04:47:10.945008 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 04:49:39.466782 140699726837568 submission_runner.py:408] Time since start: 3626.43s, 	Step: 4766, 	{'train/accuracy': 0.5796718597412109, 'train/loss': 2.213855266571045, 'train/bleu': 26.36870746881244, 'validation/accuracy': 0.5924167037010193, 'validation/loss': 2.0987660884857178, 'validation/bleu': 23.469450249820703, 'validation/num_examples': 3000, 'test/accuracy': 0.5945848822593689, 'test/loss': 2.0823488235473633, 'test/bleu': 21.990632388952925, 'test/num_examples': 3003, 'score': 1711.0473158359528, 'total_duration': 3626.4316835403442, 'accumulated_submission_time': 1711.0473158359528, 'accumulated_eval_time': 1915.1850049495697, 'accumulated_logging_time': 0.047638893127441406}
I0209 04:49:39.484994 140529887049472 logging_writer.py:48] [4766] accumulated_eval_time=1915.185005, accumulated_logging_time=0.047639, accumulated_submission_time=1711.047316, global_step=4766, preemption_count=0, score=1711.047316, test/accuracy=0.594585, test/bleu=21.990632, test/loss=2.082349, test/num_examples=3003, total_duration=3626.431684, train/accuracy=0.579672, train/bleu=26.368707, train/loss=2.213855, validation/accuracy=0.592417, validation/bleu=23.469450, validation/loss=2.098766, validation/num_examples=3000
I0209 04:49:51.782965 140529895442176 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.3454453945159912, loss=2.1811282634735107
I0209 04:50:26.943414 140529887049472 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.4182499051094055, loss=2.2185144424438477
I0209 04:51:02.183204 140529895442176 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.55948805809021, loss=2.2389206886291504
I0209 04:51:37.457262 140529887049472 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.5500540733337402, loss=2.2270498275756836
I0209 04:52:12.716293 140529895442176 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.6456129550933838, loss=2.2233684062957764
I0209 04:52:47.930426 140529887049472 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.9003364443778992, loss=2.174274206161499
I0209 04:53:23.196445 140529895442176 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.4571833312511444, loss=2.1769514083862305
I0209 04:53:58.480075 140529887049472 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.5643281936645508, loss=2.12143611907959
I0209 04:54:33.730229 140529895442176 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.5155787467956543, loss=2.194338083267212
I0209 04:55:08.968617 140529887049472 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.568447470664978, loss=2.1490862369537354
I0209 04:55:44.203871 140529895442176 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.3123956620693207, loss=2.241067886352539
I0209 04:56:19.457446 140529887049472 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.4615786671638489, loss=2.200986862182617
I0209 04:56:54.724995 140529895442176 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.31899386644363403, loss=2.139103412628174
I0209 04:57:29.993450 140529887049472 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.26976102590560913, loss=2.147892951965332
I0209 04:58:05.259922 140529895442176 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.6110588908195496, loss=2.2187459468841553
I0209 04:58:40.532331 140529887049472 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.5518816113471985, loss=2.1927270889282227
I0209 04:59:15.801154 140529895442176 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.41723379492759705, loss=2.207754135131836
I0209 04:59:51.037090 140529887049472 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.32801535725593567, loss=2.190567970275879
I0209 05:00:26.299088 140529895442176 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.6550275683403015, loss=2.0760185718536377
I0209 05:01:01.562606 140529887049472 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.27480819821357727, loss=2.0937271118164062
I0209 05:01:36.847743 140529895442176 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.3251537084579468, loss=2.195331573486328
I0209 05:02:12.148793 140529887049472 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.3208230435848236, loss=2.1058311462402344
I0209 05:02:47.415215 140529895442176 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.38863158226013184, loss=2.1900269985198975
I0209 05:03:22.689045 140529887049472 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.4344276487827301, loss=2.2363665103912354
I0209 05:03:39.659127 140699726837568 spec.py:321] Evaluating on the training split.
I0209 05:03:42.676052 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 05:06:42.169180 140699726837568 spec.py:333] Evaluating on the validation split.
I0209 05:06:44.899704 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 05:09:20.165711 140699726837568 spec.py:349] Evaluating on the test split.
I0209 05:09:22.883462 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 05:11:52.126672 140699726837568 submission_runner.py:408] Time since start: 4959.09s, 	Step: 7150, 	{'train/accuracy': 0.5893054604530334, 'train/loss': 2.118350028991699, 'train/bleu': 27.176336003425604, 'validation/accuracy': 0.6037743091583252, 'validation/loss': 2.024528980255127, 'validation/bleu': 24.357176752244964, 'validation/num_examples': 3000, 'test/accuracy': 0.6053221821784973, 'test/loss': 2.0033843517303467, 'test/bleu': 22.893728461005857, 'test/num_examples': 3003, 'score': 2551.1354496479034, 'total_duration': 4959.091591835022, 'accumulated_submission_time': 2551.1354496479034, 'accumulated_eval_time': 2407.65248298645, 'accumulated_logging_time': 0.07714414596557617}
I0209 05:11:52.141969 140529895442176 logging_writer.py:48] [7150] accumulated_eval_time=2407.652483, accumulated_logging_time=0.077144, accumulated_submission_time=2551.135450, global_step=7150, preemption_count=0, score=2551.135450, test/accuracy=0.605322, test/bleu=22.893728, test/loss=2.003384, test/num_examples=3003, total_duration=4959.091592, train/accuracy=0.589305, train/bleu=27.176336, train/loss=2.118350, validation/accuracy=0.603774, validation/bleu=24.357177, validation/loss=2.024529, validation/num_examples=3000
I0209 05:12:10.031323 140529887049472 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.5497874021530151, loss=2.159986972808838
I0209 05:12:45.166865 140529895442176 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.5342136025428772, loss=2.0906894207000732
I0209 05:13:20.378151 140529887049472 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.3760766088962555, loss=2.159052610397339
I0209 05:13:55.579039 140529895442176 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.24422521889209747, loss=2.1416380405426025
I0209 05:14:30.819824 140529887049472 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.45036423206329346, loss=2.223539113998413
I0209 05:15:06.079158 140529895442176 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.7464891076087952, loss=2.100529909133911
I0209 05:15:41.332776 140529887049472 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.39127403497695923, loss=2.0225136280059814
I0209 05:16:16.647493 140529895442176 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.3924727737903595, loss=2.1780247688293457
I0209 05:16:51.922773 140529887049472 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.5346444845199585, loss=2.116790294647217
I0209 05:17:27.177413 140529895442176 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.2685670256614685, loss=2.051574230194092
I0209 05:18:02.424783 140529887049472 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.3663554787635803, loss=2.059230089187622
I0209 05:18:37.672837 140529895442176 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.4401479661464691, loss=2.2029714584350586
I0209 05:19:12.926439 140529887049472 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.5292083024978638, loss=2.1326770782470703
I0209 05:19:48.177765 140529895442176 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.5194340944290161, loss=2.1112687587738037
I0209 05:20:23.433335 140529887049472 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.2857939898967743, loss=2.1524477005004883
I0209 05:20:58.695413 140529895442176 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.40245893597602844, loss=2.114053726196289
I0209 05:21:33.943464 140529887049472 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.5036338567733765, loss=2.097679853439331
I0209 05:22:09.200404 140529895442176 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.8825774192810059, loss=2.1168947219848633
I0209 05:22:44.470887 140529887049472 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.2681143581867218, loss=2.1848886013031006
I0209 05:23:19.727714 140529895442176 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.4931619167327881, loss=2.134366273880005
I0209 05:23:54.980583 140529887049472 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.8091169595718384, loss=2.132240056991577
I0209 05:24:30.257927 140529895442176 logging_writer.py:48] [9300] global_step=9300, grad_norm=1.0083019733428955, loss=2.0991828441619873
I0209 05:25:05.483990 140529887049472 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.8418443202972412, loss=2.1200921535491943
I0209 05:25:40.731259 140529895442176 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.33083733916282654, loss=2.086951494216919
I0209 05:25:52.435227 140699726837568 spec.py:321] Evaluating on the training split.
I0209 05:25:55.456238 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 05:29:02.605565 140699726837568 spec.py:333] Evaluating on the validation split.
I0209 05:29:05.330252 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 05:32:15.456386 140699726837568 spec.py:349] Evaluating on the test split.
I0209 05:32:18.167360 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 05:35:27.242942 140699726837568 submission_runner.py:408] Time since start: 6374.21s, 	Step: 9535, 	{'train/accuracy': 0.5954850912094116, 'train/loss': 2.119326114654541, 'train/bleu': 27.711826590662206, 'validation/accuracy': 0.6125404238700867, 'validation/loss': 1.9642142057418823, 'validation/bleu': 24.82005795946175, 'validation/num_examples': 3000, 'test/accuracy': 0.6162105798721313, 'test/loss': 1.9303609132766724, 'test/bleu': 23.417765370115543, 'test/num_examples': 3003, 'score': 3391.3415517807007, 'total_duration': 6374.207869529724, 'accumulated_submission_time': 3391.3415517807007, 'accumulated_eval_time': 2982.4601430892944, 'accumulated_logging_time': 0.1054227352142334}
I0209 05:35:27.259520 140529887049472 logging_writer.py:48] [9535] accumulated_eval_time=2982.460143, accumulated_logging_time=0.105423, accumulated_submission_time=3391.341552, global_step=9535, preemption_count=0, score=3391.341552, test/accuracy=0.616211, test/bleu=23.417765, test/loss=1.930361, test/num_examples=3003, total_duration=6374.207870, train/accuracy=0.595485, train/bleu=27.711827, train/loss=2.119326, validation/accuracy=0.612540, validation/bleu=24.820058, validation/loss=1.964214, validation/num_examples=3000
I0209 05:35:50.400720 140529895442176 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.4093102216720581, loss=2.047074317932129
I0209 05:36:25.518919 140529887049472 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.5133670568466187, loss=2.030257225036621
I0209 05:37:00.714275 140529895442176 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.4794061481952667, loss=2.079698324203491
I0209 05:37:36.136827 140529887049472 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.9756810069084167, loss=2.207864284515381
I0209 05:38:11.382611 140529895442176 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.35595211386680603, loss=2.115501880645752
I0209 05:38:46.622151 140529887049472 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.2573586404323578, loss=2.0813536643981934
I0209 05:39:21.874132 140529895442176 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.41529667377471924, loss=2.08010196685791
I0209 05:39:57.122380 140529887049472 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.6205227971076965, loss=2.0903055667877197
I0209 05:40:32.411202 140529895442176 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.33171361684799194, loss=2.0813307762145996
I0209 05:41:07.730692 140529887049472 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.2998173236846924, loss=2.0381784439086914
I0209 05:41:43.032631 140529895442176 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.2902231812477112, loss=2.1531543731689453
I0209 05:42:18.340770 140529887049472 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.4562700092792511, loss=2.0942277908325195
I0209 05:42:53.643098 140529895442176 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.38336384296417236, loss=2.051541328430176
I0209 05:43:28.964541 140529887049472 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.9718933701515198, loss=2.1931381225585938
I0209 05:44:04.266532 140529895442176 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.6397928595542908, loss=2.172999858856201
I0209 05:44:39.522877 140529887049472 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.3209971785545349, loss=2.1122019290924072
I0209 05:45:14.818475 140529895442176 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.41622450947761536, loss=2.010409116744995
I0209 05:45:50.117391 140529887049472 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.43276405334472656, loss=2.1775219440460205
I0209 05:46:25.547765 140529895442176 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.49525687098503113, loss=2.027494192123413
I0209 05:47:00.870852 140529887049472 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.716440737247467, loss=2.2081258296966553
I0209 05:47:36.197599 140529895442176 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.301653116941452, loss=2.0858066082000732
I0209 05:48:11.492507 140529887049472 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.5030221343040466, loss=2.0472593307495117
I0209 05:48:46.743023 140529895442176 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.618263304233551, loss=2.107921838760376
I0209 05:49:22.017352 140529887049472 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.2917768061161041, loss=2.0966336727142334
I0209 05:49:27.383923 140699726837568 spec.py:321] Evaluating on the training split.
I0209 05:49:30.403864 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 05:52:30.652289 140699726837568 spec.py:333] Evaluating on the validation split.
I0209 05:52:33.368865 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 05:55:10.445592 140699726837568 spec.py:349] Evaluating on the test split.
I0209 05:55:13.167639 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 05:57:53.207250 140699726837568 submission_runner.py:408] Time since start: 7720.17s, 	Step: 11917, 	{'train/accuracy': 0.5949356555938721, 'train/loss': 2.093808650970459, 'train/bleu': 27.32630178120262, 'validation/accuracy': 0.6108169555664062, 'validation/loss': 1.9556013345718384, 'validation/bleu': 23.938898942260515, 'validation/num_examples': 3000, 'test/accuracy': 0.6186973452568054, 'test/loss': 1.9178035259246826, 'test/bleu': 22.844513408768293, 'test/num_examples': 3003, 'score': 4231.374727487564, 'total_duration': 7720.172181844711, 'accumulated_submission_time': 4231.374727487564, 'accumulated_eval_time': 3488.283415555954, 'accumulated_logging_time': 0.13197827339172363}
I0209 05:57:53.223856 140529895442176 logging_writer.py:48] [11917] accumulated_eval_time=3488.283416, accumulated_logging_time=0.131978, accumulated_submission_time=4231.374727, global_step=11917, preemption_count=0, score=4231.374727, test/accuracy=0.618697, test/bleu=22.844513, test/loss=1.917804, test/num_examples=3003, total_duration=7720.172182, train/accuracy=0.594936, train/bleu=27.326302, train/loss=2.093809, validation/accuracy=0.610817, validation/bleu=23.938899, validation/loss=1.955601, validation/num_examples=3000
I0209 05:58:22.705086 140529887049472 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.39049386978149414, loss=2.228219509124756
I0209 05:58:57.851116 140529895442176 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.5503755211830139, loss=2.0346546173095703
I0209 05:59:33.056717 140529887049472 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.8111923336982727, loss=2.018568515777588
I0209 06:00:08.329694 140529895442176 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.3076919615268707, loss=2.107365846633911
I0209 06:00:43.576266 140529887049472 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.47910571098327637, loss=2.097388982772827
I0209 06:01:18.866445 140529895442176 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.3432019054889679, loss=2.037806749343872
I0209 06:01:54.204504 140529887049472 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.5757583975791931, loss=2.076573133468628
I0209 06:02:29.511675 140529895442176 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.26034897565841675, loss=2.119816303253174
I0209 06:03:04.788986 140529887049472 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.3342582881450653, loss=2.134661912918091
I0209 06:03:40.061603 140529895442176 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.6730784177780151, loss=2.0944271087646484
I0209 06:04:15.337213 140529887049472 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.6646520495414734, loss=2.0958876609802246
I0209 06:04:50.657875 140529895442176 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.44567832350730896, loss=2.090268135070801
I0209 06:05:25.901177 140529887049472 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.2975904047489166, loss=2.1281790733337402
I0209 06:06:01.166284 140529895442176 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.506523072719574, loss=2.124541997909546
I0209 06:06:36.434108 140529887049472 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.43998315930366516, loss=2.091805934906006
I0209 06:07:11.701863 140529895442176 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.39988553524017334, loss=2.099433422088623
I0209 06:07:46.967954 140529887049472 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.5344710946083069, loss=2.035510778427124
I0209 06:08:22.222545 140529895442176 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.6413904428482056, loss=2.1600375175476074
I0209 06:08:57.463117 140529887049472 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.5757675766944885, loss=2.153257369995117
I0209 06:09:32.744838 140529895442176 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.6554133296012878, loss=2.0430703163146973
I0209 06:10:08.044544 140529887049472 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.24247246980667114, loss=2.121258497238159
I0209 06:10:43.323040 140529895442176 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.26150110363960266, loss=2.081338405609131
I0209 06:11:18.569025 140529887049472 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.2870650291442871, loss=2.02182674407959
I0209 06:11:53.834167 140529895442176 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.8033952116966248, loss=2.133822441101074
I0209 06:11:53.841227 140699726837568 spec.py:321] Evaluating on the training split.
I0209 06:11:56.581458 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 06:15:02.562315 140699726837568 spec.py:333] Evaluating on the validation split.
I0209 06:15:05.309861 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 06:17:36.144406 140699726837568 spec.py:349] Evaluating on the test split.
I0209 06:17:38.880697 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 06:20:02.900338 140699726837568 submission_runner.py:408] Time since start: 9049.87s, 	Step: 14301, 	{'train/accuracy': 0.597704291343689, 'train/loss': 2.0802969932556152, 'train/bleu': 28.308626342993755, 'validation/accuracy': 0.6167685389518738, 'validation/loss': 1.930830478668213, 'validation/bleu': 25.063348149842113, 'validation/num_examples': 3000, 'test/accuracy': 0.621997594833374, 'test/loss': 1.8840601444244385, 'test/bleu': 23.81654558217464, 'test/num_examples': 3003, 'score': 5071.9056560993195, 'total_duration': 9049.865262746811, 'accumulated_submission_time': 5071.9056560993195, 'accumulated_eval_time': 3977.3424422740936, 'accumulated_logging_time': 0.15987753868103027}
I0209 06:20:02.916714 140529887049472 logging_writer.py:48] [14301] accumulated_eval_time=3977.342442, accumulated_logging_time=0.159878, accumulated_submission_time=5071.905656, global_step=14301, preemption_count=0, score=5071.905656, test/accuracy=0.621998, test/bleu=23.816546, test/loss=1.884060, test/num_examples=3003, total_duration=9049.865263, train/accuracy=0.597704, train/bleu=28.308626, train/loss=2.080297, validation/accuracy=0.616769, validation/bleu=25.063348, validation/loss=1.930830, validation/num_examples=3000
I0209 06:20:38.059031 140529895442176 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.25685515999794006, loss=2.0323784351348877
I0209 06:21:13.266854 140529887049472 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.6959827542304993, loss=2.023879051208496
I0209 06:21:48.571879 140529895442176 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.34198683500289917, loss=2.108325958251953
I0209 06:22:23.862042 140529887049472 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.48550546169281006, loss=2.115478277206421
I0209 06:22:59.105971 140529895442176 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.7787996530532837, loss=2.072751522064209
I0209 06:23:34.393352 140529887049472 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.7410219311714172, loss=2.183868408203125
I0209 06:24:09.678646 140529895442176 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.513190746307373, loss=2.089047431945801
I0209 06:24:44.947648 140529887049472 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.24721065163612366, loss=2.083026647567749
I0209 06:25:20.242239 140529895442176 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.5536363124847412, loss=2.03043270111084
I0209 06:25:55.502754 140529887049472 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.35676151514053345, loss=2.1398119926452637
I0209 06:26:30.760982 140529895442176 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.6958997845649719, loss=2.1604483127593994
I0209 06:27:05.992262 140529887049472 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.6179297566413879, loss=2.0867669582366943
I0209 06:27:41.247869 140529895442176 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.5812570452690125, loss=2.063534736633301
I0209 06:28:16.566004 140529887049472 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.47537511587142944, loss=2.109529733657837
I0209 06:28:51.810084 140529895442176 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.320132851600647, loss=2.0356791019439697
I0209 06:29:27.073261 140529887049472 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.47834983468055725, loss=2.027372360229492
I0209 06:30:02.347153 140529895442176 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.6054669618606567, loss=2.0780718326568604
I0209 06:30:37.621726 140529887049472 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.6522519588470459, loss=2.0565924644470215
I0209 06:31:12.853421 140529895442176 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.4713754951953888, loss=2.1164603233337402
I0209 06:31:48.095900 140529887049472 logging_writer.py:48] [16300] global_step=16300, grad_norm=1.0738261938095093, loss=2.052367925643921
I0209 06:32:23.370480 140529895442176 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.27698442339897156, loss=2.074410915374756
I0209 06:32:58.652664 140529887049472 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.6366991996765137, loss=2.1264169216156006
I0209 06:33:33.960576 140529895442176 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.5279014110565186, loss=2.092399835586548
I0209 06:34:02.931228 140699726837568 spec.py:321] Evaluating on the training split.
I0209 06:34:05.953660 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 06:37:09.174534 140699726837568 spec.py:333] Evaluating on the validation split.
I0209 06:37:11.892604 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 06:39:40.062173 140699726837568 spec.py:349] Evaluating on the test split.
I0209 06:39:42.805677 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 06:42:09.586461 140699726837568 submission_runner.py:408] Time since start: 10376.55s, 	Step: 16684, 	{'train/accuracy': 0.5992457866668701, 'train/loss': 2.0773322582244873, 'train/bleu': 28.287729139742407, 'validation/accuracy': 0.6202650666236877, 'validation/loss': 1.911595344543457, 'validation/bleu': 25.16760638686699, 'validation/num_examples': 3000, 'test/accuracy': 0.6254836916923523, 'test/loss': 1.8665118217468262, 'test/bleu': 23.955626051084025, 'test/num_examples': 3003, 'score': 5911.835096359253, 'total_duration': 10376.551353693008, 'accumulated_submission_time': 5911.835096359253, 'accumulated_eval_time': 4463.997585058212, 'accumulated_logging_time': 0.1864314079284668}
I0209 06:42:09.607406 140529887049472 logging_writer.py:48] [16684] accumulated_eval_time=4463.997585, accumulated_logging_time=0.186431, accumulated_submission_time=5911.835096, global_step=16684, preemption_count=0, score=5911.835096, test/accuracy=0.625484, test/bleu=23.955626, test/loss=1.866512, test/num_examples=3003, total_duration=10376.551354, train/accuracy=0.599246, train/bleu=28.287729, train/loss=2.077332, validation/accuracy=0.620265, validation/bleu=25.167606, validation/loss=1.911595, validation/num_examples=3000
I0209 06:42:15.591699 140529895442176 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.4986914098262787, loss=2.020298719406128
I0209 06:42:50.698997 140529887049472 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.47080883383750916, loss=2.091581106185913
I0209 06:43:25.904283 140529895442176 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.7126113772392273, loss=2.0578315258026123
I0209 06:44:01.150923 140529887049472 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.5125259757041931, loss=2.0756542682647705
I0209 06:44:36.380380 140529895442176 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.39639925956726074, loss=2.0658233165740967
I0209 06:45:11.636546 140529887049472 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.3422810733318329, loss=2.161831855773926
I0209 06:45:46.882108 140529895442176 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.37878182530403137, loss=2.0670008659362793
I0209 06:46:22.119992 140529887049472 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.39176154136657715, loss=2.0438551902770996
I0209 06:46:57.339456 140529895442176 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.30768874287605286, loss=2.184948444366455
I0209 06:47:32.744648 140529887049472 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.34650495648384094, loss=2.082000732421875
I0209 06:48:08.030355 140529895442176 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.8436616659164429, loss=2.0774106979370117
I0209 06:48:43.372019 140529887049472 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.35032978653907776, loss=2.0852088928222656
I0209 06:49:18.645626 140529895442176 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.5743376016616821, loss=2.064274311065674
I0209 06:49:53.871259 140529887049472 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.521072506904602, loss=1.945039987564087
I0209 06:50:29.126456 140529895442176 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.7439881563186646, loss=2.089376449584961
I0209 06:51:04.397804 140529887049472 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.7169232368469238, loss=2.1321616172790527
I0209 06:51:39.638929 140529895442176 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.6285515427589417, loss=2.0386877059936523
I0209 06:52:14.883230 140529887049472 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.30080553889274597, loss=2.0924525260925293
I0209 06:52:50.139866 140529895442176 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.5946322083473206, loss=2.03474497795105
I0209 06:53:25.394800 140529887049472 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.3059503138065338, loss=2.039595365524292
I0209 06:54:00.699167 140529895442176 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.6225287318229675, loss=2.0523018836975098
I0209 06:54:35.931616 140529887049472 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.36011189222335815, loss=2.1033663749694824
I0209 06:55:11.188954 140529895442176 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.4274263083934784, loss=2.0569286346435547
I0209 06:55:46.486301 140529887049472 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.6820303201675415, loss=2.0395429134368896
I0209 06:56:09.848838 140699726837568 spec.py:321] Evaluating on the training split.
I0209 06:56:12.872243 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 06:59:19.365486 140699726837568 spec.py:333] Evaluating on the validation split.
I0209 06:59:22.087098 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 07:02:00.154415 140699726837568 spec.py:349] Evaluating on the test split.
I0209 07:02:02.910680 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 07:04:37.968217 140699726837568 submission_runner.py:408] Time since start: 11724.93s, 	Step: 19068, 	{'train/accuracy': 0.6137471199035645, 'train/loss': 1.926009178161621, 'train/bleu': 29.14411802678976, 'validation/accuracy': 0.6178720593452454, 'validation/loss': 1.9107155799865723, 'validation/bleu': 25.27364721195347, 'validation/num_examples': 3000, 'test/accuracy': 0.6246935129165649, 'test/loss': 1.8686076402664185, 'test/bleu': 24.06795003200436, 'test/num_examples': 3003, 'score': 6751.987172842026, 'total_duration': 11724.933151721954, 'accumulated_submission_time': 6751.987172842026, 'accumulated_eval_time': 4972.11691904068, 'accumulated_logging_time': 0.21974945068359375}
I0209 07:04:37.986620 140529895442176 logging_writer.py:48] [19068] accumulated_eval_time=4972.116919, accumulated_logging_time=0.219749, accumulated_submission_time=6751.987173, global_step=19068, preemption_count=0, score=6751.987173, test/accuracy=0.624694, test/bleu=24.067950, test/loss=1.868608, test/num_examples=3003, total_duration=11724.933152, train/accuracy=0.613747, train/bleu=29.144118, train/loss=1.926009, validation/accuracy=0.617872, validation/bleu=25.273647, validation/loss=1.910716, validation/num_examples=3000
I0209 07:04:49.573853 140529887049472 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.752046525478363, loss=2.110832929611206
I0209 07:05:24.671156 140529895442176 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.4065708816051483, loss=2.079061985015869
I0209 07:05:59.834693 140529887049472 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.7206102609634399, loss=2.0445432662963867
I0209 07:06:35.078116 140529895442176 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.34148669242858887, loss=2.1201422214508057
I0209 07:07:10.298441 140529887049472 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.4117177724838257, loss=2.018846035003662
I0209 07:07:45.562479 140529895442176 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.2922194302082062, loss=2.120004415512085
I0209 07:08:20.799136 140529887049472 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.7322719693183899, loss=2.11548113822937
I0209 07:08:56.085112 140529895442176 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.514866292476654, loss=2.092703104019165
I0209 07:09:31.331677 140529887049472 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.5122926831245422, loss=2.03116774559021
I0209 07:10:06.601389 140529895442176 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.5819007158279419, loss=2.081273078918457
I0209 07:10:41.821729 140529887049472 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.6429081559181213, loss=2.1381194591522217
I0209 07:11:17.077144 140529895442176 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.49870818853378296, loss=1.9768935441970825
I0209 07:11:52.366904 140529887049472 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.3063957691192627, loss=1.977639079093933
I0209 07:12:27.647617 140529895442176 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.7396650314331055, loss=2.103663921356201
I0209 07:13:02.905095 140529887049472 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.5561065077781677, loss=2.0250442028045654
I0209 07:13:38.214674 140529895442176 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.535724401473999, loss=1.9417436122894287
I0209 07:14:13.475822 140529887049472 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.24414528906345367, loss=2.09112548828125
I0209 07:14:48.754925 140529895442176 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.4698120951652527, loss=2.0642499923706055
I0209 07:15:23.995774 140529887049472 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.30878156423568726, loss=2.0864429473876953
I0209 07:15:59.237784 140529895442176 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.44854462146759033, loss=2.0119972229003906
I0209 07:16:34.515414 140529887049472 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.7415333390235901, loss=2.011284112930298
I0209 07:17:09.733839 140529895442176 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.4943399131298065, loss=2.087202310562134
I0209 07:17:44.975432 140529887049472 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.25880369544029236, loss=2.0933852195739746
I0209 07:18:20.266013 140529895442176 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.34836912155151367, loss=2.084749221801758
I0209 07:18:37.991778 140699726837568 spec.py:321] Evaluating on the training split.
I0209 07:18:41.038200 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 07:21:57.379807 140699726837568 spec.py:333] Evaluating on the validation split.
I0209 07:22:00.118001 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 07:25:02.089467 140699726837568 spec.py:349] Evaluating on the test split.
I0209 07:25:04.824571 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 07:27:50.902669 140699726837568 submission_runner.py:408] Time since start: 13117.87s, 	Step: 21452, 	{'train/accuracy': 0.6039170026779175, 'train/loss': 2.04196834564209, 'train/bleu': 28.499650206528244, 'validation/accuracy': 0.6211702227592468, 'validation/loss': 1.8938138484954834, 'validation/bleu': 25.326143244982028, 'validation/num_examples': 3000, 'test/accuracy': 0.6274592280387878, 'test/loss': 1.8478097915649414, 'test/bleu': 24.41130121879812, 'test/num_examples': 3003, 'score': 7591.907975435257, 'total_duration': 13117.867561101913, 'accumulated_submission_time': 7591.907975435257, 'accumulated_eval_time': 5525.0277309417725, 'accumulated_logging_time': 0.24841761589050293}
I0209 07:27:50.920627 140529887049472 logging_writer.py:48] [21452] accumulated_eval_time=5525.027731, accumulated_logging_time=0.248418, accumulated_submission_time=7591.907975, global_step=21452, preemption_count=0, score=7591.907975, test/accuracy=0.627459, test/bleu=24.411301, test/loss=1.847810, test/num_examples=3003, total_duration=13117.867561, train/accuracy=0.603917, train/bleu=28.499650, train/loss=2.041968, validation/accuracy=0.621170, validation/bleu=25.326143, validation/loss=1.893814, validation/num_examples=3000
I0209 07:28:08.105397 140529895442176 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.8939631581306458, loss=2.100393772125244
I0209 07:28:43.226518 140529887049472 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.43218398094177246, loss=2.111184597015381
I0209 07:29:18.418394 140529895442176 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.5196031928062439, loss=2.037787437438965
I0209 07:29:53.645909 140529887049472 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.5316230654716492, loss=2.0154268741607666
I0209 07:30:28.899984 140529895442176 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.3705880343914032, loss=1.9904662370681763
I0209 07:31:04.179602 140529887049472 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.4196934700012207, loss=2.1125028133392334
I0209 07:31:39.488137 140529895442176 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.3728482127189636, loss=1.9654054641723633
I0209 07:32:14.758672 140529887049472 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.5661710500717163, loss=2.0364432334899902
I0209 07:32:50.030555 140529895442176 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.513104259967804, loss=2.042431116104126
I0209 07:33:25.310389 140529887049472 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.5775495171546936, loss=2.0331180095672607
I0209 07:34:00.536495 140529895442176 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.5059870481491089, loss=2.002819061279297
I0209 07:34:35.796939 140529887049472 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.7102239727973938, loss=2.0748934745788574
I0209 07:35:11.044945 140529895442176 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.70027095079422, loss=2.020577907562256
I0209 07:35:46.285368 140529887049472 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.28601181507110596, loss=1.9828366041183472
I0209 07:36:21.517082 140529895442176 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.5448545813560486, loss=2.063871383666992
I0209 07:36:56.770026 140529887049472 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.5684875845909119, loss=2.0138940811157227
I0209 07:37:31.997724 140529895442176 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.2561730444431305, loss=1.9850958585739136
I0209 07:38:07.269557 140529887049472 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.812161386013031, loss=2.1092467308044434
I0209 07:38:42.553187 140529895442176 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.5711292028427124, loss=2.126112222671509
I0209 07:39:17.791232 140529887049472 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.5915049910545349, loss=2.111604928970337
I0209 07:39:53.016247 140529895442176 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.6063782572746277, loss=1.9613457918167114
I0209 07:40:28.259174 140529887049472 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.3872262239456177, loss=2.0305252075195312
I0209 07:41:03.505118 140529895442176 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.4116094708442688, loss=1.9651374816894531
I0209 07:41:38.818432 140529887049472 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.4004746079444885, loss=2.019970417022705
I0209 07:41:51.240273 140699726837568 spec.py:321] Evaluating on the training split.
I0209 07:41:54.276019 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 07:44:50.376893 140699726837568 spec.py:333] Evaluating on the validation split.
I0209 07:44:53.110271 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 07:47:31.622764 140699726837568 spec.py:349] Evaluating on the test split.
I0209 07:47:34.356068 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 07:50:09.577562 140699726837568 submission_runner.py:408] Time since start: 14456.54s, 	Step: 23837, 	{'train/accuracy': 0.6014580130577087, 'train/loss': 2.0570669174194336, 'train/bleu': 28.452707209093468, 'validation/accuracy': 0.6210214495658875, 'validation/loss': 1.8938277959823608, 'validation/bleu': 25.541055572678147, 'validation/num_examples': 3000, 'test/accuracy': 0.6271222233772278, 'test/loss': 1.8466172218322754, 'test/bleu': 24.527137407140724, 'test/num_examples': 3003, 'score': 8432.142497062683, 'total_duration': 14456.54249548912, 'accumulated_submission_time': 8432.142497062683, 'accumulated_eval_time': 6023.364975690842, 'accumulated_logging_time': 0.2760303020477295}
I0209 07:50:09.595721 140529895442176 logging_writer.py:48] [23837] accumulated_eval_time=6023.364976, accumulated_logging_time=0.276030, accumulated_submission_time=8432.142497, global_step=23837, preemption_count=0, score=8432.142497, test/accuracy=0.627122, test/bleu=24.527137, test/loss=1.846617, test/num_examples=3003, total_duration=14456.542495, train/accuracy=0.601458, train/bleu=28.452707, train/loss=2.057067, validation/accuracy=0.621021, validation/bleu=25.541056, validation/loss=1.893828, validation/num_examples=3000
I0209 07:50:32.058095 140529887049472 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.44310262799263, loss=2.0717175006866455
I0209 07:51:07.215547 140529895442176 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.32360270619392395, loss=1.9552056789398193
I0209 07:51:42.428267 140529887049472 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.28530585765838623, loss=2.0060410499572754
I0209 07:52:17.718829 140529895442176 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.6721706390380859, loss=2.0932650566101074
I0209 07:52:53.025141 140529887049472 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.3471919298171997, loss=2.097724199295044
I0209 07:53:28.274319 140529895442176 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.5585205554962158, loss=2.0506229400634766
I0209 07:54:03.518212 140529887049472 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.2683384120464325, loss=2.04203462600708
I0209 07:54:38.764102 140529895442176 logging_writer.py:48] [24600] global_step=24600, grad_norm=1.1018179655075073, loss=2.0726521015167236
I0209 07:55:14.033132 140529887049472 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.29764994978904724, loss=2.0759668350219727
I0209 07:55:49.264184 140529895442176 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.4029127359390259, loss=2.0815846920013428
I0209 07:56:24.515445 140529887049472 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.3865073025226593, loss=2.0517144203186035
I0209 07:56:59.741594 140529895442176 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.3827033042907715, loss=1.9875733852386475
I0209 07:57:35.027729 140529887049472 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.3480159640312195, loss=2.119642734527588
I0209 07:58:10.295707 140529895442176 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.3452461063861847, loss=2.0172669887542725
I0209 07:58:45.521246 140529887049472 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.2742406129837036, loss=1.9945549964904785
I0209 07:59:20.779700 140529895442176 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.3701384663581848, loss=2.0497043132781982
I0209 07:59:56.026118 140529887049472 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.2632862329483032, loss=2.05981183052063
I0209 08:00:31.291896 140529895442176 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.554477334022522, loss=2.034156560897827
I0209 08:01:06.592176 140529887049472 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.5187610983848572, loss=2.136249303817749
I0209 08:01:41.937355 140529895442176 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.7936696410179138, loss=2.0705578327178955
I0209 08:02:17.340659 140529887049472 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.45817628502845764, loss=2.044375419616699
I0209 08:02:52.648681 140529895442176 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.8059812188148499, loss=2.0403831005096436
I0209 08:03:27.941485 140529887049472 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.3738013207912445, loss=2.074206829071045
I0209 08:04:03.206118 140529895442176 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.6028130054473877, loss=2.0830538272857666
I0209 08:04:09.627112 140699726837568 spec.py:321] Evaluating on the training split.
I0209 08:04:12.644990 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 08:08:29.155618 140699726837568 spec.py:333] Evaluating on the validation split.
I0209 08:08:31.871788 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 08:11:23.719616 140699726837568 spec.py:349] Evaluating on the test split.
I0209 08:11:26.442233 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 08:14:02.680132 140699726837568 submission_runner.py:408] Time since start: 15889.65s, 	Step: 26220, 	{'train/accuracy': 0.6070371866226196, 'train/loss': 2.0028188228607178, 'train/bleu': 28.626233950948446, 'validation/accuracy': 0.6237616539001465, 'validation/loss': 1.874969720840454, 'validation/bleu': 25.473166487207465, 'validation/num_examples': 3000, 'test/accuracy': 0.6297716498374939, 'test/loss': 1.828667402267456, 'test/bleu': 24.472345680734758, 'test/num_examples': 3003, 'score': 9272.08500289917, 'total_duration': 15889.645047426224, 'accumulated_submission_time': 9272.08500289917, 'accumulated_eval_time': 6616.417924404144, 'accumulated_logging_time': 0.30393409729003906}
I0209 08:14:02.699108 140529887049472 logging_writer.py:48] [26220] accumulated_eval_time=6616.417924, accumulated_logging_time=0.303934, accumulated_submission_time=9272.085003, global_step=26220, preemption_count=0, score=9272.085003, test/accuracy=0.629772, test/bleu=24.472346, test/loss=1.828667, test/num_examples=3003, total_duration=15889.645047, train/accuracy=0.607037, train/bleu=28.626234, train/loss=2.002819, validation/accuracy=0.623762, validation/bleu=25.473166, validation/loss=1.874970, validation/num_examples=3000
I0209 08:14:31.105804 140529895442176 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.4883357286453247, loss=2.0541129112243652
I0209 08:15:06.249470 140529887049472 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.2929627299308777, loss=1.977089762687683
I0209 08:15:41.468209 140529895442176 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.6387417316436768, loss=1.9819658994674683
I0209 08:16:16.725552 140529887049472 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.44147029519081116, loss=1.9559030532836914
I0209 08:16:52.001015 140529895442176 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.3705541789531708, loss=2.0036420822143555
I0209 08:17:27.241439 140529887049472 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.3119368851184845, loss=1.9625762701034546
I0209 08:18:02.513550 140529895442176 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.7293059825897217, loss=2.013584852218628
I0209 08:18:37.753491 140529887049472 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.2745884656906128, loss=2.04508900642395
I0209 08:19:13.005819 140529895442176 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.344546377658844, loss=2.014411211013794
I0209 08:19:48.234447 140529887049472 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.3177640438079834, loss=1.983425259590149
I0209 08:20:23.500996 140529895442176 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.38566866517066956, loss=2.010998249053955
I0209 08:20:58.747122 140529887049472 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.5503976345062256, loss=2.0093424320220947
I0209 08:21:34.044464 140529895442176 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.495511531829834, loss=2.063278913497925
I0209 08:22:09.325478 140529887049472 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.4930163323879242, loss=1.980540156364441
I0209 08:22:44.575545 140529895442176 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.4297274649143219, loss=2.0264267921447754
I0209 08:23:19.821034 140529887049472 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.2979348301887512, loss=1.9845917224884033
I0209 08:23:55.063247 140529895442176 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.3338041603565216, loss=2.0497219562530518
I0209 08:24:30.343790 140529887049472 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.4987200200557709, loss=2.060243606567383
I0209 08:25:05.588267 140529895442176 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.46164682507514954, loss=2.059187173843384
I0209 08:25:40.863473 140529887049472 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.3301714062690735, loss=2.0853707790374756
I0209 08:26:16.145491 140529895442176 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.525438129901886, loss=2.0277016162872314
I0209 08:26:51.415801 140529887049472 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.4166311025619507, loss=2.0391688346862793
I0209 08:27:26.691051 140529895442176 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.5262694954872131, loss=2.0046727657318115
I0209 08:28:01.950709 140529887049472 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.277068167924881, loss=2.015594005584717
I0209 08:28:02.733872 140699726837568 spec.py:321] Evaluating on the training split.
I0209 08:28:05.757583 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 08:31:13.870324 140699726837568 spec.py:333] Evaluating on the validation split.
I0209 08:31:16.599606 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 08:34:02.955058 140699726837568 spec.py:349] Evaluating on the test split.
I0209 08:34:05.683101 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 08:36:40.589942 140699726837568 submission_runner.py:408] Time since start: 17247.55s, 	Step: 28604, 	{'train/accuracy': 0.6041083335876465, 'train/loss': 2.042267322540283, 'train/bleu': 28.460018032548266, 'validation/accuracy': 0.6240839958190918, 'validation/loss': 1.880605697631836, 'validation/bleu': 25.687265643742247, 'validation/num_examples': 3000, 'test/accuracy': 0.6322584748268127, 'test/loss': 1.824683427810669, 'test/bleu': 24.058762358703923, 'test/num_examples': 3003, 'score': 10112.033434867859, 'total_duration': 17247.554879665375, 'accumulated_submission_time': 10112.033434867859, 'accumulated_eval_time': 7134.273945808411, 'accumulated_logging_time': 0.3334333896636963}
I0209 08:36:40.610417 140529895442176 logging_writer.py:48] [28604] accumulated_eval_time=7134.273946, accumulated_logging_time=0.333433, accumulated_submission_time=10112.033435, global_step=28604, preemption_count=0, score=10112.033435, test/accuracy=0.632258, test/bleu=24.058762, test/loss=1.824683, test/num_examples=3003, total_duration=17247.554880, train/accuracy=0.604108, train/bleu=28.460018, train/loss=2.042267, validation/accuracy=0.624084, validation/bleu=25.687266, validation/loss=1.880606, validation/num_examples=3000
I0209 08:37:14.681942 140529887049472 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.8042213916778564, loss=1.9662096500396729
I0209 08:37:49.910097 140529895442176 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.7947276830673218, loss=2.0843307971954346
I0209 08:38:25.166593 140529887049472 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.7587766051292419, loss=1.9697928428649902
I0209 08:39:00.402844 140529895442176 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.33442234992980957, loss=2.123311758041382
I0209 08:39:35.643553 140529887049472 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.5281761288642883, loss=1.9484046697616577
I0209 08:40:10.880455 140529895442176 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.5095912218093872, loss=2.024137496948242
I0209 08:40:46.126566 140529887049472 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.45314493775367737, loss=2.03029203414917
I0209 08:41:21.406814 140529895442176 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.41759929060935974, loss=2.147679328918457
I0209 08:41:56.669108 140529887049472 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.34160280227661133, loss=2.1128931045532227
I0209 08:42:31.929964 140529895442176 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.3025031089782715, loss=2.02593994140625
I0209 08:43:07.249870 140529887049472 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.4246565103530884, loss=2.0247833728790283
I0209 08:43:42.487749 140529895442176 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.5663065314292908, loss=2.0063726902008057
I0209 08:44:17.740422 140529887049472 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.573476254940033, loss=2.0139827728271484
I0209 08:44:52.993588 140529895442176 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.6108009815216064, loss=1.967637538909912
I0209 08:45:28.208521 140529887049472 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.2775765061378479, loss=2.0021839141845703
I0209 08:46:03.435742 140529895442176 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.4468531608581543, loss=2.0742545127868652
I0209 08:46:38.685313 140529887049472 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.3145768642425537, loss=2.0330090522766113
I0209 08:47:13.931648 140529895442176 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.5622381567955017, loss=2.0072295665740967
I0209 08:47:49.204433 140529887049472 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.8330740928649902, loss=2.006206512451172
I0209 08:48:24.454375 140529895442176 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.43997591733932495, loss=2.092188835144043
I0209 08:48:59.716634 140529887049472 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.48163145780563354, loss=2.103687047958374
I0209 08:49:34.958392 140529895442176 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.44849342107772827, loss=2.050300359725952
I0209 08:50:10.205193 140529887049472 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.5415059328079224, loss=1.9098384380340576
I0209 08:50:40.931928 140699726837568 spec.py:321] Evaluating on the training split.
I0209 08:50:43.966005 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 08:54:01.447710 140699726837568 spec.py:333] Evaluating on the validation split.
I0209 08:54:04.194159 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 08:56:49.999788 140699726837568 spec.py:349] Evaluating on the test split.
I0209 08:56:52.720840 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 08:59:14.237070 140699726837568 submission_runner.py:408] Time since start: 18601.20s, 	Step: 30989, 	{'train/accuracy': 0.6057415008544922, 'train/loss': 2.0303850173950195, 'train/bleu': 28.31511913979044, 'validation/accuracy': 0.6273201704025269, 'validation/loss': 1.8572407960891724, 'validation/bleu': 25.756574842278006, 'validation/num_examples': 3000, 'test/accuracy': 0.6323397755622864, 'test/loss': 1.8168457746505737, 'test/bleu': 24.526306536985867, 'test/num_examples': 3003, 'score': 10952.269091129303, 'total_duration': 18601.201991081238, 'accumulated_submission_time': 10952.269091129303, 'accumulated_eval_time': 7647.579026222229, 'accumulated_logging_time': 0.3638780117034912}
I0209 08:59:14.256108 140529895442176 logging_writer.py:48] [30989] accumulated_eval_time=7647.579026, accumulated_logging_time=0.363878, accumulated_submission_time=10952.269091, global_step=30989, preemption_count=0, score=10952.269091, test/accuracy=0.632340, test/bleu=24.526307, test/loss=1.816846, test/num_examples=3003, total_duration=18601.201991, train/accuracy=0.605742, train/bleu=28.315119, train/loss=2.030385, validation/accuracy=0.627320, validation/bleu=25.756575, validation/loss=1.857241, validation/num_examples=3000
I0209 08:59:18.482403 140529887049472 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.6501098871231079, loss=2.080470561981201
I0209 08:59:53.547961 140529895442176 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.4075855016708374, loss=1.9816874265670776
I0209 09:00:28.711502 140529887049472 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.31475895643234253, loss=2.00429105758667
I0209 09:01:03.913891 140529895442176 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.3658772110939026, loss=2.0946290493011475
I0209 09:01:39.148378 140529887049472 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.4069051146507263, loss=2.058245897293091
I0209 09:02:14.387062 140529895442176 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.7104345560073853, loss=2.012366771697998
I0209 09:02:49.598797 140529887049472 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.5925620198249817, loss=1.9664570093154907
I0209 09:03:24.847927 140529895442176 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.33494365215301514, loss=2.054318428039551
I0209 09:04:00.081185 140529887049472 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.45625045895576477, loss=1.8829667568206787
I0209 09:04:35.331872 140529895442176 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.37404865026474, loss=2.006843328475952
I0209 09:05:10.566815 140529887049472 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.4989962577819824, loss=2.042142152786255
I0209 09:05:45.803686 140529895442176 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.4133441150188446, loss=1.9983277320861816
I0209 09:06:21.063981 140529887049472 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.40197843313217163, loss=2.03485107421875
I0209 09:06:56.297360 140529895442176 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.3155117630958557, loss=2.069981575012207
I0209 09:07:31.557484 140529887049472 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.7588804364204407, loss=1.9961415529251099
I0209 09:08:06.789472 140529895442176 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.3282713294029236, loss=1.9788581132888794
I0209 09:08:42.071721 140529887049472 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.3298221528530121, loss=2.0584359169006348
I0209 09:09:17.328259 140529895442176 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.3356539309024811, loss=2.056853771209717
I0209 09:09:52.564357 140529887049472 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.4952169954776764, loss=2.096891164779663
I0209 09:10:27.812050 140529895442176 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.24907638132572174, loss=1.994185447692871
I0209 09:11:03.078536 140529887049472 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.5227126479148865, loss=1.988525390625
I0209 09:11:38.303164 140529895442176 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.2882414162158966, loss=2.060370922088623
I0209 09:12:13.536939 140529887049472 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.4101749360561371, loss=1.9964823722839355
I0209 09:12:48.804338 140529895442176 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.5256882905960083, loss=1.9679560661315918
I0209 09:13:14.245971 140699726837568 spec.py:321] Evaluating on the training split.
I0209 09:13:17.271620 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 09:17:03.123390 140699726837568 spec.py:333] Evaluating on the validation split.
I0209 09:17:05.845694 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 09:20:25.756491 140699726837568 spec.py:349] Evaluating on the test split.
I0209 09:20:28.494966 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 09:23:12.790278 140699726837568 submission_runner.py:408] Time since start: 20039.76s, 	Step: 33374, 	{'train/accuracy': 0.60844486951828, 'train/loss': 2.0008599758148193, 'train/bleu': 28.007482098976688, 'validation/accuracy': 0.6259438991546631, 'validation/loss': 1.856594443321228, 'validation/bleu': 25.505005927829227, 'validation/num_examples': 3000, 'test/accuracy': 0.6319795846939087, 'test/loss': 1.811674952507019, 'test/bleu': 24.446853717819966, 'test/num_examples': 3003, 'score': 11792.176321744919, 'total_duration': 20039.755168437958, 'accumulated_submission_time': 11792.176321744919, 'accumulated_eval_time': 8246.12324142456, 'accumulated_logging_time': 0.39251065254211426}
I0209 09:23:12.810407 140529887049472 logging_writer.py:48] [33374] accumulated_eval_time=8246.123241, accumulated_logging_time=0.392511, accumulated_submission_time=11792.176322, global_step=33374, preemption_count=0, score=11792.176322, test/accuracy=0.631980, test/bleu=24.446854, test/loss=1.811675, test/num_examples=3003, total_duration=20039.755168, train/accuracy=0.608445, train/bleu=28.007482, train/loss=2.000860, validation/accuracy=0.625944, validation/bleu=25.505006, validation/loss=1.856594, validation/num_examples=3000
I0209 09:23:22.298835 140529895442176 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.39901256561279297, loss=2.0256285667419434
I0209 09:23:57.407690 140529887049472 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.3211527168750763, loss=2.0114126205444336
I0209 09:24:32.585321 140529895442176 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.3685999810695648, loss=1.983723521232605
I0209 09:25:07.794382 140529887049472 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.34570011496543884, loss=1.8929563760757446
I0209 09:25:43.039529 140529895442176 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.5778370499610901, loss=1.915215253829956
I0209 09:26:18.284940 140529887049472 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.5832654237747192, loss=1.9949116706848145
I0209 09:26:53.499834 140529895442176 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.48938342928886414, loss=2.004214286804199
I0209 09:27:28.731468 140529887049472 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.5214271545410156, loss=1.9688376188278198
I0209 09:28:03.979882 140529895442176 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.35491666197776794, loss=1.9254075288772583
I0209 09:28:39.219459 140529887049472 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.49667835235595703, loss=2.02478289604187
I0209 09:29:14.502087 140529895442176 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.2991750240325928, loss=2.005682945251465
I0209 09:29:49.731383 140529887049472 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.32673266530036926, loss=1.9960997104644775
I0209 09:30:24.970898 140529895442176 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.2933575510978699, loss=1.9394264221191406
I0209 09:31:00.191303 140529887049472 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.4247864782810211, loss=2.075467348098755
I0209 09:31:35.443707 140529895442176 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.628215491771698, loss=1.9876872301101685
I0209 09:32:10.690051 140529887049472 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.35901138186454773, loss=1.9589108228683472
I0209 09:32:45.977161 140529895442176 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.3397400975227356, loss=2.0622968673706055
I0209 09:33:21.258508 140529887049472 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.28198182582855225, loss=2.0361838340759277
I0209 09:33:56.656743 140529895442176 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.43503451347351074, loss=2.0054218769073486
I0209 09:34:31.903369 140529887049472 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.2893884479999542, loss=1.9539352655410767
I0209 09:35:07.164726 140529895442176 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.5186894536018372, loss=2.021176815032959
I0209 09:35:42.414563 140529887049472 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.6463488340377808, loss=2.0468292236328125
I0209 09:36:17.711238 140529895442176 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.5167391300201416, loss=1.9816542863845825
I0209 09:36:52.969364 140529887049472 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.30799925327301025, loss=1.9944877624511719
I0209 09:37:12.802435 140699726837568 spec.py:321] Evaluating on the training split.
I0209 09:37:15.837266 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 09:40:12.762303 140699726837568 spec.py:333] Evaluating on the validation split.
I0209 09:40:15.488812 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 09:42:54.329737 140699726837568 spec.py:349] Evaluating on the test split.
I0209 09:42:57.075754 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 09:45:51.713391 140699726837568 submission_runner.py:408] Time since start: 21398.68s, 	Step: 35758, 	{'train/accuracy': 0.6062001585960388, 'train/loss': 2.0129001140594482, 'train/bleu': 28.496158366728405, 'validation/accuracy': 0.6244683861732483, 'validation/loss': 1.8566988706588745, 'validation/bleu': 25.335470426237546, 'validation/num_examples': 3000, 'test/accuracy': 0.6337923407554626, 'test/loss': 1.8009341955184937, 'test/bleu': 24.817481823924734, 'test/num_examples': 3003, 'score': 12632.082436800003, 'total_duration': 21398.67829966545, 'accumulated_submission_time': 12632.082436800003, 'accumulated_eval_time': 8765.0341360569, 'accumulated_logging_time': 0.4228677749633789}
I0209 09:45:51.736739 140529895442176 logging_writer.py:48] [35758] accumulated_eval_time=8765.034136, accumulated_logging_time=0.422868, accumulated_submission_time=12632.082437, global_step=35758, preemption_count=0, score=12632.082437, test/accuracy=0.633792, test/bleu=24.817482, test/loss=1.800934, test/num_examples=3003, total_duration=21398.678300, train/accuracy=0.606200, train/bleu=28.496158, train/loss=2.012900, validation/accuracy=0.624468, validation/bleu=25.335470, validation/loss=1.856699, validation/num_examples=3000
I0209 09:46:06.863532 140529887049472 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.3064976930618286, loss=2.0965075492858887
I0209 09:46:41.967941 140529895442176 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.4201681315898895, loss=2.010855197906494
I0209 09:47:17.151585 140529887049472 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.5323955416679382, loss=1.9771969318389893
I0209 09:47:52.344029 140529895442176 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.3786853849887848, loss=2.0579729080200195
I0209 09:48:27.593504 140529887049472 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.3103678524494171, loss=2.0288214683532715
I0209 09:49:02.847247 140529895442176 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.8376463651657104, loss=2.083674430847168
I0209 09:49:38.098224 140529887049472 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.49458423256874084, loss=2.0184996128082275
I0209 09:50:13.347804 140529895442176 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.4245873689651489, loss=1.9250543117523193
I0209 09:50:48.580826 140529887049472 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.49749383330345154, loss=1.956794261932373
I0209 09:51:23.832476 140529895442176 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.5755160450935364, loss=1.949960708618164
I0209 09:51:59.127053 140529887049472 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.6089989542961121, loss=2.0410075187683105
I0209 09:52:34.372911 140529895442176 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.5014280676841736, loss=2.0841927528381348
I0209 09:53:09.613106 140529887049472 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.3489174544811249, loss=2.0020415782928467
I0209 09:53:44.862323 140529895442176 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.37790554761886597, loss=2.0544722080230713
I0209 09:54:20.118033 140529887049472 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.3658687174320221, loss=2.087675094604492
I0209 09:54:55.373274 140529895442176 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.358798623085022, loss=2.0131285190582275
I0209 09:55:30.587226 140529887049472 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.4836905002593994, loss=1.9362261295318604
I0209 09:56:05.834746 140529895442176 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.5106209516525269, loss=1.9989360570907593
I0209 09:56:41.141143 140529887049472 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.4697708785533905, loss=1.9658489227294922
I0209 09:57:16.398095 140529895442176 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.26607999205589294, loss=1.8712692260742188
I0209 09:57:51.687349 140529887049472 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.42187798023223877, loss=1.98284113407135
I0209 09:58:26.921588 140529895442176 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.41052818298339844, loss=1.9819828271865845
I0209 09:59:02.168738 140529887049472 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.7622225880622864, loss=1.9888794422149658
I0209 09:59:37.418052 140529895442176 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.4872627258300781, loss=2.018277645111084
I0209 09:59:51.961159 140699726837568 spec.py:321] Evaluating on the training split.
I0209 09:59:54.994500 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 10:02:37.495896 140699726837568 spec.py:333] Evaluating on the validation split.
I0209 10:02:40.219860 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 10:05:10.274030 140699726837568 spec.py:349] Evaluating on the test split.
I0209 10:05:13.015759 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 10:07:37.853349 140699726837568 submission_runner.py:408] Time since start: 22704.82s, 	Step: 38143, 	{'train/accuracy': 0.6146321892738342, 'train/loss': 1.9567898511886597, 'train/bleu': 29.38206898813265, 'validation/accuracy': 0.628361701965332, 'validation/loss': 1.8436554670333862, 'validation/bleu': 25.92643316412883, 'validation/num_examples': 3000, 'test/accuracy': 0.6382081508636475, 'test/loss': 1.7820231914520264, 'test/bleu': 25.00556614291527, 'test/num_examples': 3003, 'score': 13472.220349311829, 'total_duration': 22704.81827187538, 'accumulated_submission_time': 13472.220349311829, 'accumulated_eval_time': 9230.926263809204, 'accumulated_logging_time': 0.4587104320526123}
I0209 10:07:37.873223 140529887049472 logging_writer.py:48] [38143] accumulated_eval_time=9230.926264, accumulated_logging_time=0.458710, accumulated_submission_time=13472.220349, global_step=38143, preemption_count=0, score=13472.220349, test/accuracy=0.638208, test/bleu=25.005566, test/loss=1.782023, test/num_examples=3003, total_duration=22704.818272, train/accuracy=0.614632, train/bleu=29.382069, train/loss=1.956790, validation/accuracy=0.628362, validation/bleu=25.926433, validation/loss=1.843655, validation/num_examples=3000
I0209 10:07:58.241440 140529895442176 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.3656438887119293, loss=1.9930872917175293
I0209 10:08:33.524569 140529887049472 logging_writer.py:48] [38300] global_step=38300, grad_norm=1.0586543083190918, loss=1.8971744775772095
I0209 10:09:08.756202 140529895442176 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.35704413056373596, loss=2.0066680908203125
I0209 10:09:44.023936 140529887049472 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.3280639946460724, loss=1.9327348470687866
I0209 10:10:19.307727 140529895442176 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.33995065093040466, loss=1.943339467048645
I0209 10:10:54.516445 140529887049472 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.6395540237426758, loss=1.9160505533218384
I0209 10:11:29.763739 140529895442176 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.29119738936424255, loss=2.0108981132507324
I0209 10:12:05.018790 140529887049472 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.3032970428466797, loss=1.9301609992980957
I0209 10:12:40.261801 140529895442176 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.45733386278152466, loss=2.0852127075195312
I0209 10:13:15.482077 140529887049472 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.4390939772129059, loss=2.0096960067749023
I0209 10:13:50.713192 140529895442176 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.6048509478569031, loss=1.9902393817901611
I0209 10:14:26.015939 140529887049472 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.2705354690551758, loss=1.9995659589767456
I0209 10:15:01.282589 140529895442176 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.3149782121181488, loss=2.0772411823272705
I0209 10:15:36.561999 140529887049472 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.3114941120147705, loss=2.0118649005889893
I0209 10:16:11.802638 140529895442176 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.44312918186187744, loss=1.9506980180740356
I0209 10:16:47.081833 140529887049472 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.23395563662052155, loss=1.9434741735458374
I0209 10:17:22.364277 140529895442176 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.903698205947876, loss=2.006720781326294
I0209 10:17:57.644649 140529887049472 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.3082440495491028, loss=1.9834082126617432
I0209 10:18:32.897589 140529895442176 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.8701765537261963, loss=2.070507526397705
I0209 10:19:08.167618 140529887049472 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.45470955967903137, loss=1.989586591720581
I0209 10:19:43.418969 140529895442176 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.5767928957939148, loss=1.9575155973434448
I0209 10:20:18.679084 140529887049472 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.29191699624061584, loss=1.9708333015441895
I0209 10:20:53.966155 140529895442176 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.6574049592018127, loss=1.9965276718139648
I0209 10:21:29.206293 140529887049472 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.6941720247268677, loss=2.0043113231658936
I0209 10:21:38.105235 140699726837568 spec.py:321] Evaluating on the training split.
I0209 10:21:41.132660 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 10:24:42.875228 140699726837568 spec.py:333] Evaluating on the validation split.
I0209 10:24:45.590218 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 10:27:19.639897 140699726837568 spec.py:349] Evaluating on the test split.
I0209 10:27:22.365493 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 10:29:49.096824 140699726837568 submission_runner.py:408] Time since start: 24036.06s, 	Step: 40527, 	{'train/accuracy': 0.611473023891449, 'train/loss': 1.9678258895874023, 'train/bleu': 28.94678678261578, 'validation/accuracy': 0.6278533339500427, 'validation/loss': 1.835180640220642, 'validation/bleu': 25.68769018292687, 'validation/num_examples': 3000, 'test/accuracy': 0.6370925903320312, 'test/loss': 1.7789210081100464, 'test/bleu': 24.969637842391354, 'test/num_examples': 3003, 'score': 14312.36628293991, 'total_duration': 24036.061758756638, 'accumulated_submission_time': 14312.36628293991, 'accumulated_eval_time': 9721.91780424118, 'accumulated_logging_time': 0.48862719535827637}
I0209 10:29:49.117460 140529895442176 logging_writer.py:48] [40527] accumulated_eval_time=9721.917804, accumulated_logging_time=0.488627, accumulated_submission_time=14312.366283, global_step=40527, preemption_count=0, score=14312.366283, test/accuracy=0.637093, test/bleu=24.969638, test/loss=1.778921, test/num_examples=3003, total_duration=24036.061759, train/accuracy=0.611473, train/bleu=28.946787, train/loss=1.967826, validation/accuracy=0.627853, validation/bleu=25.687690, validation/loss=1.835181, validation/num_examples=3000
I0209 10:30:15.080662 140529887049472 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.5011868476867676, loss=2.0010461807250977
I0209 10:30:50.205709 140529895442176 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.6966173052787781, loss=1.9670747518539429
I0209 10:31:25.409494 140529887049472 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.3144740164279938, loss=1.943967342376709
I0209 10:32:00.700241 140529895442176 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.3803867995738983, loss=1.914913535118103
I0209 10:32:35.934362 140529887049472 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.3045114576816559, loss=1.9734386205673218
I0209 10:33:11.176894 140529895442176 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.3690227270126343, loss=2.0440890789031982
I0209 10:33:46.416926 140529887049472 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.3073122799396515, loss=2.0204226970672607
I0209 10:34:21.712523 140529895442176 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.24781203269958496, loss=1.966483473777771
I0209 10:34:57.101259 140529887049472 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.2979785203933716, loss=1.9141113758087158
I0209 10:35:32.364682 140529895442176 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.3136386275291443, loss=1.9398705959320068
I0209 10:36:07.629994 140529887049472 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.561571478843689, loss=1.9999866485595703
I0209 10:36:42.882493 140529895442176 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.5053486824035645, loss=2.0327093601226807
I0209 10:37:18.123531 140529887049472 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.30126097798347473, loss=2.042640447616577
I0209 10:37:53.345701 140529895442176 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.3349912464618683, loss=2.004492998123169
I0209 10:38:28.604128 140529887049472 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.33980995416641235, loss=1.962533950805664
I0209 10:39:03.850461 140529895442176 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.38500434160232544, loss=1.9729825258255005
I0209 10:39:39.165200 140529887049472 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.3450108766555786, loss=1.9873645305633545
I0209 10:40:14.411942 140529895442176 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.26321423053741455, loss=1.9576823711395264
I0209 10:40:49.683494 140529887049472 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.36671826243400574, loss=1.9623684883117676
I0209 10:41:24.973089 140529895442176 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.3548959791660309, loss=1.9387236833572388
I0209 10:42:00.268053 140529887049472 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.29613232612609863, loss=1.944240689277649
I0209 10:42:35.526300 140529895442176 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.4112449586391449, loss=1.95703125
I0209 10:43:10.759153 140529887049472 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.8010250329971313, loss=1.9805163145065308
I0209 10:43:45.990108 140529895442176 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.44284912943840027, loss=1.9283150434494019
I0209 10:43:49.241280 140699726837568 spec.py:321] Evaluating on the training split.
I0209 10:43:52.265271 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 10:46:45.729529 140699726837568 spec.py:333] Evaluating on the validation split.
I0209 10:46:48.455041 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 10:49:42.209156 140699726837568 spec.py:349] Evaluating on the test split.
I0209 10:49:44.925396 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 10:52:41.299026 140699726837568 submission_runner.py:408] Time since start: 25408.26s, 	Step: 42911, 	{'train/accuracy': 0.6131593585014343, 'train/loss': 1.972790241241455, 'train/bleu': 29.089999188219597, 'validation/accuracy': 0.631461501121521, 'validation/loss': 1.8121095895767212, 'validation/bleu': 26.098148061288406, 'validation/num_examples': 3000, 'test/accuracy': 0.6399396061897278, 'test/loss': 1.767079472541809, 'test/bleu': 25.392114429594827, 'test/num_examples': 3003, 'score': 15152.403260946274, 'total_duration': 25408.2639605999, 'accumulated_submission_time': 15152.403260946274, 'accumulated_eval_time': 10253.975495576859, 'accumulated_logging_time': 0.5196661949157715}
I0209 10:52:41.319932 140529887049472 logging_writer.py:48] [42911] accumulated_eval_time=10253.975496, accumulated_logging_time=0.519666, accumulated_submission_time=15152.403261, global_step=42911, preemption_count=0, score=15152.403261, test/accuracy=0.639940, test/bleu=25.392114, test/loss=1.767079, test/num_examples=3003, total_duration=25408.263961, train/accuracy=0.613159, train/bleu=29.089999, train/loss=1.972790, validation/accuracy=0.631462, validation/bleu=26.098148, validation/loss=1.812110, validation/num_examples=3000
I0209 10:53:12.889661 140529895442176 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.32715848088264465, loss=1.975784420967102
I0209 10:53:48.033541 140529887049472 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.6732866168022156, loss=2.0150058269500732
I0209 10:54:23.208394 140529895442176 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.3321061134338379, loss=1.9791659116744995
I0209 10:54:58.427604 140529887049472 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.3949032723903656, loss=1.9097747802734375
I0209 10:55:33.638138 140529895442176 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.5628517270088196, loss=2.017396926879883
I0209 10:56:08.880592 140529887049472 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.5277054309844971, loss=1.9697662591934204
I0209 10:56:44.134539 140529895442176 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.6565453410148621, loss=1.946655511856079
I0209 10:57:19.390416 140529887049472 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.6826918721199036, loss=2.0004491806030273
I0209 10:57:54.625043 140529895442176 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.3266385495662689, loss=1.9798314571380615
I0209 10:58:29.878329 140529887049472 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.3883616328239441, loss=1.9375834465026855
I0209 10:59:05.111954 140529895442176 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.4129267930984497, loss=1.9390023946762085
I0209 10:59:40.418987 140529887049472 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.4561062455177307, loss=1.9473670721054077
I0209 11:00:15.667912 140529895442176 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.278249055147171, loss=2.0391855239868164
I0209 11:00:50.910655 140529887049472 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.2815784215927124, loss=1.9512746334075928
I0209 11:01:26.116381 140529895442176 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.2855951488018036, loss=1.9924893379211426
I0209 11:02:01.393484 140529887049472 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.31976157426834106, loss=1.997442364692688
I0209 11:02:36.628074 140529895442176 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.31617048382759094, loss=2.000237226486206
I0209 11:03:11.865565 140529887049472 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.4326161742210388, loss=1.8744587898254395
I0209 11:03:47.103841 140529895442176 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.3270919620990753, loss=1.9302551746368408
I0209 11:04:22.398193 140529887049472 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.45504748821258545, loss=2.1380395889282227
I0209 11:04:57.604052 140529895442176 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.383537232875824, loss=1.9036314487457275
I0209 11:05:32.847208 140529887049472 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.33255916833877563, loss=1.9459420442581177
I0209 11:06:08.102113 140529895442176 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.4120372533798218, loss=1.9995206594467163
I0209 11:06:41.339055 140699726837568 spec.py:321] Evaluating on the training split.
I0209 11:06:44.376700 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 11:10:03.981155 140699726837568 spec.py:333] Evaluating on the validation split.
I0209 11:10:06.695643 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 11:13:00.135544 140699726837568 spec.py:349] Evaluating on the test split.
I0209 11:13:02.876832 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 11:16:01.467170 140699726837568 submission_runner.py:408] Time since start: 26808.43s, 	Step: 45296, 	{'train/accuracy': 0.6127845048904419, 'train/loss': 1.955664873123169, 'train/bleu': 28.965295301006385, 'validation/accuracy': 0.6328253746032715, 'validation/loss': 1.8045439720153809, 'validation/bleu': 25.967067669048774, 'validation/num_examples': 3000, 'test/accuracy': 0.6396374702453613, 'test/loss': 1.7606006860733032, 'test/bleu': 24.73479737642498, 'test/num_examples': 3003, 'score': 15992.33811044693, 'total_duration': 26808.432076215744, 'accumulated_submission_time': 15992.33811044693, 'accumulated_eval_time': 10814.103539466858, 'accumulated_logging_time': 0.5507168769836426}
I0209 11:16:01.492449 140529887049472 logging_writer.py:48] [45296] accumulated_eval_time=10814.103539, accumulated_logging_time=0.550717, accumulated_submission_time=15992.338110, global_step=45296, preemption_count=0, score=15992.338110, test/accuracy=0.639637, test/bleu=24.734797, test/loss=1.760601, test/num_examples=3003, total_duration=26808.432076, train/accuracy=0.612785, train/bleu=28.965295, train/loss=1.955665, validation/accuracy=0.632825, validation/bleu=25.967068, validation/loss=1.804544, validation/num_examples=3000
I0209 11:16:03.271480 140529895442176 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.8606576919555664, loss=1.9570986032485962
I0209 11:16:38.443604 140529887049472 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.47257187962532043, loss=2.0068376064300537
I0209 11:17:13.623980 140529895442176 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.8030571341514587, loss=2.0642805099487305
I0209 11:17:48.843924 140529887049472 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.482246071100235, loss=1.991216778755188
I0209 11:18:24.064255 140529895442176 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.5294795036315918, loss=1.980681300163269
I0209 11:18:59.305172 140529887049472 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.3482007384300232, loss=1.9576483964920044
I0209 11:19:34.543073 140529895442176 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.38010838627815247, loss=2.0558576583862305
I0209 11:20:09.761491 140529887049472 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.4157841205596924, loss=1.924957036972046
I0209 11:20:44.999363 140529895442176 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.30368614196777344, loss=2.033444881439209
I0209 11:21:20.238504 140529887049472 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.3268420398235321, loss=1.9626623392105103
I0209 11:21:55.499825 140529895442176 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.49873989820480347, loss=1.969603180885315
I0209 11:22:30.748941 140529887049472 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.3056800663471222, loss=1.930964469909668
I0209 11:23:06.029170 140529895442176 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.4016299843788147, loss=1.8849965333938599
I0209 11:23:41.248867 140529887049472 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.315250962972641, loss=1.9234373569488525
I0209 11:24:16.494384 140529895442176 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.4533151090145111, loss=1.9602940082550049
I0209 11:24:51.750344 140529887049472 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.6432995200157166, loss=1.9747185707092285
I0209 11:25:27.010221 140529895442176 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.8542832136154175, loss=1.9768701791763306
I0209 11:26:02.276247 140529887049472 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.3510524332523346, loss=2.072251081466675
I0209 11:26:37.511563 140529895442176 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.4344334900379181, loss=2.0679244995117188
I0209 11:27:12.808331 140529887049472 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.24784786999225616, loss=2.042191743850708
I0209 11:27:48.125197 140529895442176 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.39884433150291443, loss=1.8859624862670898
I0209 11:28:23.370711 140529887049472 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.3872806131839752, loss=1.938246250152588
I0209 11:28:58.601749 140529895442176 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.5357151627540588, loss=2.002181053161621
I0209 11:29:33.821514 140529887049472 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.7695100903511047, loss=1.9060741662979126
I0209 11:30:01.731914 140699726837568 spec.py:321] Evaluating on the training split.
I0209 11:30:04.760399 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 11:33:22.329420 140699726837568 spec.py:333] Evaluating on the validation split.
I0209 11:33:25.045593 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 11:36:06.915387 140699726837568 spec.py:349] Evaluating on the test split.
I0209 11:36:09.658626 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 11:38:28.653963 140699726837568 submission_runner.py:408] Time since start: 28155.62s, 	Step: 47681, 	{'train/accuracy': 0.6143538355827332, 'train/loss': 1.9528555870056152, 'train/bleu': 29.107341526376448, 'validation/accuracy': 0.6314491033554077, 'validation/loss': 1.8058665990829468, 'validation/bleu': 25.943934302859418, 'validation/num_examples': 3000, 'test/accuracy': 0.6436465382575989, 'test/loss': 1.74813711643219, 'test/bleu': 25.42608692174256, 'test/num_examples': 3003, 'score': 16832.49195432663, 'total_duration': 28155.61889219284, 'accumulated_submission_time': 16832.49195432663, 'accumulated_eval_time': 11321.025541305542, 'accumulated_logging_time': 0.5871026515960693}
I0209 11:38:28.675297 140529895442176 logging_writer.py:48] [47681] accumulated_eval_time=11321.025541, accumulated_logging_time=0.587103, accumulated_submission_time=16832.491954, global_step=47681, preemption_count=0, score=16832.491954, test/accuracy=0.643647, test/bleu=25.426087, test/loss=1.748137, test/num_examples=3003, total_duration=28155.618892, train/accuracy=0.614354, train/bleu=29.107342, train/loss=1.952856, validation/accuracy=0.631449, validation/bleu=25.943934, validation/loss=1.805867, validation/num_examples=3000
I0209 11:38:35.713095 140529887049472 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.6776857376098633, loss=2.035614252090454
I0209 11:39:10.833446 140529895442176 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.3697294294834137, loss=1.954545259475708
I0209 11:39:46.026827 140529887049472 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.462833046913147, loss=1.9674891233444214
I0209 11:40:21.281398 140529895442176 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.307067334651947, loss=1.9242361783981323
I0209 11:40:56.513670 140529887049472 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.5483814477920532, loss=1.8722814321517944
I0209 11:41:31.768589 140529895442176 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.593149721622467, loss=1.9607632160186768
I0209 11:42:07.056244 140529887049472 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.32410579919815063, loss=2.002011775970459
I0209 11:42:42.335494 140529895442176 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.5029869079589844, loss=1.9948132038116455
I0209 11:43:17.634679 140529887049472 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.33334413170814514, loss=1.9454398155212402
I0209 11:43:52.922792 140529895442176 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.3097873032093048, loss=1.956234097480774
I0209 11:44:28.206473 140529887049472 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.3814128041267395, loss=2.0128610134124756
I0209 11:45:03.517050 140529895442176 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.3611474633216858, loss=1.913478970527649
I0209 11:45:38.820228 140529887049472 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.4515171945095062, loss=2.0301926136016846
I0209 11:46:14.111306 140529895442176 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.3137771785259247, loss=1.9576175212860107
I0209 11:46:49.381484 140529887049472 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.5553693175315857, loss=1.9383788108825684
I0209 11:47:24.639703 140529895442176 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.6998723745346069, loss=1.9313280582427979
I0209 11:47:59.909261 140529887049472 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.28842878341674805, loss=1.9655163288116455
I0209 11:48:35.190408 140529895442176 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.628287672996521, loss=2.0141918659210205
I0209 11:49:10.464832 140529887049472 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.5353689193725586, loss=1.905630111694336
I0209 11:49:45.760133 140529895442176 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.9464158415794373, loss=1.9686826467514038
I0209 11:50:21.137188 140529887049472 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.31158187985420227, loss=2.007568359375
I0209 11:50:56.374443 140529895442176 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.5013771653175354, loss=2.055877923965454
I0209 11:51:31.645450 140529887049472 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.5522580146789551, loss=2.0471794605255127
I0209 11:52:06.974681 140529895442176 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.3637748658657074, loss=1.9580047130584717
I0209 11:52:28.918437 140699726837568 spec.py:321] Evaluating on the training split.
I0209 11:52:31.942349 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 11:55:29.351902 140699726837568 spec.py:333] Evaluating on the validation split.
I0209 11:55:32.084485 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 11:58:04.555836 140699726837568 spec.py:349] Evaluating on the test split.
I0209 11:58:07.289424 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 12:00:29.640608 140699726837568 submission_runner.py:408] Time since start: 29476.61s, 	Step: 50064, 	{'train/accuracy': 0.6843953728675842, 'train/loss': 1.4730702638626099, 'train/bleu': 34.336430518788745, 'validation/accuracy': 0.635949969291687, 'validation/loss': 1.7930233478546143, 'validation/bleu': 26.441084450311465, 'validation/num_examples': 3000, 'test/accuracy': 0.6434489488601685, 'test/loss': 1.7467548847198486, 'test/bleu': 25.14636965746108, 'test/num_examples': 3003, 'score': 17672.648672819138, 'total_duration': 29476.605541706085, 'accumulated_submission_time': 17672.648672819138, 'accumulated_eval_time': 11801.747661113739, 'accumulated_logging_time': 0.6183607578277588}
I0209 12:00:29.662876 140529887049472 logging_writer.py:48] [50064] accumulated_eval_time=11801.747661, accumulated_logging_time=0.618361, accumulated_submission_time=17672.648673, global_step=50064, preemption_count=0, score=17672.648673, test/accuracy=0.643449, test/bleu=25.146370, test/loss=1.746755, test/num_examples=3003, total_duration=29476.605542, train/accuracy=0.684395, train/bleu=34.336431, train/loss=1.473070, validation/accuracy=0.635950, validation/bleu=26.441084, validation/loss=1.793023, validation/num_examples=3000
I0209 12:00:42.686155 140529895442176 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.3314121663570404, loss=1.9574187994003296
I0209 12:01:17.847163 140529887049472 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.45493820309638977, loss=1.9511452913284302
I0209 12:01:53.068546 140529895442176 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.32334935665130615, loss=1.955539584159851
I0209 12:02:28.351691 140529887049472 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.27706876397132874, loss=1.9599586725234985
I0209 12:03:03.598796 140529895442176 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.353596031665802, loss=1.9189895391464233
I0209 12:03:38.831428 140529887049472 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.37070927023887634, loss=1.9372092485427856
I0209 12:04:14.104427 140529895442176 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.27552053332328796, loss=1.9412394762039185
I0209 12:04:49.339137 140529887049472 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.8805006742477417, loss=2.0299103260040283
I0209 12:05:24.587414 140529895442176 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.411745548248291, loss=1.9957035779953003
I0209 12:05:59.827957 140529887049472 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.6811971068382263, loss=1.9458061456680298
I0209 12:06:35.053779 140529895442176 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.27912089228630066, loss=1.8938862085342407
I0209 12:07:10.311561 140529887049472 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.41398394107818604, loss=1.9157871007919312
I0209 12:07:45.551410 140529895442176 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.4283770024776459, loss=1.990884780883789
I0209 12:08:20.790441 140529887049472 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.6198921203613281, loss=1.8746966123580933
I0209 12:08:56.029267 140529895442176 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.29912346601486206, loss=1.9004671573638916
I0209 12:09:31.282956 140529887049472 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.3427862226963043, loss=2.0324084758758545
I0209 12:10:06.659641 140529895442176 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.2988712191581726, loss=1.959011435508728
I0209 12:10:41.976336 140529887049472 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.2835821807384491, loss=2.0425710678100586
I0209 12:11:17.279230 140529895442176 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.5867409110069275, loss=1.9069924354553223
I0209 12:11:52.613551 140529887049472 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.47596099972724915, loss=1.9158903360366821
I0209 12:12:27.917628 140529895442176 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.5224606990814209, loss=1.969806432723999
I0209 12:13:03.146562 140529887049472 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.3535178005695343, loss=2.0003573894500732
I0209 12:13:38.406142 140529895442176 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.5627590417861938, loss=1.9225654602050781
I0209 12:14:13.704931 140529887049472 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.35258474946022034, loss=2.0219483375549316
I0209 12:14:29.663190 140699726837568 spec.py:321] Evaluating on the training split.
I0209 12:14:32.707353 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 12:18:08.317861 140699726837568 spec.py:333] Evaluating on the validation split.
I0209 12:18:11.053380 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 12:21:18.977902 140699726837568 spec.py:349] Evaluating on the test split.
I0209 12:21:21.718462 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 12:24:05.558643 140699726837568 submission_runner.py:408] Time since start: 30892.52s, 	Step: 52447, 	{'train/accuracy': 0.6166776418685913, 'train/loss': 1.935380220413208, 'train/bleu': 28.009326838866563, 'validation/accuracy': 0.6369170546531677, 'validation/loss': 1.7796038389205933, 'validation/bleu': 26.137517861350776, 'validation/num_examples': 3000, 'test/accuracy': 0.6453315019607544, 'test/loss': 1.7311948537826538, 'test/bleu': 25.591061740837908, 'test/num_examples': 3003, 'score': 18512.559674978256, 'total_duration': 30892.52356314659, 'accumulated_submission_time': 18512.559674978256, 'accumulated_eval_time': 12377.643058538437, 'accumulated_logging_time': 0.6505851745605469}
I0209 12:24:05.581460 140529895442176 logging_writer.py:48] [52447] accumulated_eval_time=12377.643059, accumulated_logging_time=0.650585, accumulated_submission_time=18512.559675, global_step=52447, preemption_count=0, score=18512.559675, test/accuracy=0.645332, test/bleu=25.591062, test/loss=1.731195, test/num_examples=3003, total_duration=30892.523563, train/accuracy=0.616678, train/bleu=28.009327, train/loss=1.935380, validation/accuracy=0.636917, validation/bleu=26.137518, validation/loss=1.779604, validation/num_examples=3000
I0209 12:24:24.564800 140529887049472 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.433094322681427, loss=1.9598654508590698
I0209 12:24:59.743230 140529895442176 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.4222298562526703, loss=1.9615607261657715
I0209 12:25:34.957244 140529887049472 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.3795238137245178, loss=1.888547420501709
I0209 12:26:10.239078 140529895442176 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.5210483074188232, loss=1.9554047584533691
I0209 12:26:45.491494 140529887049472 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.3579849600791931, loss=1.926404356956482
I0209 12:27:20.761589 140529895442176 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.27302417159080505, loss=1.9238386154174805
I0209 12:27:56.014351 140529887049472 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.46088099479675293, loss=1.8743760585784912
I0209 12:28:31.242218 140529895442176 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.2615317404270172, loss=1.8901400566101074
I0209 12:29:06.474222 140529887049472 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.3272426426410675, loss=1.8564318418502808
I0209 12:29:41.727066 140529895442176 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.3427814543247223, loss=1.9526642560958862
I0209 12:30:17.002732 140529887049472 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.34091347455978394, loss=1.921507477760315
I0209 12:30:52.246311 140529895442176 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.595648467540741, loss=1.8969240188598633
I0209 12:31:27.472896 140529887049472 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.5175997614860535, loss=1.9918049573898315
I0209 12:32:02.728296 140529895442176 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.35512039065361023, loss=1.9118281602859497
I0209 12:32:38.011316 140529887049472 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.3289160132408142, loss=1.9279743432998657
I0209 12:33:13.304886 140529895442176 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.7647820711135864, loss=1.9121577739715576
I0209 12:33:48.567663 140529887049472 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.3150962293148041, loss=2.0765974521636963
I0209 12:34:23.806078 140529895442176 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.27184760570526123, loss=1.9702140092849731
I0209 12:34:59.067029 140529887049472 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.2776610255241394, loss=1.9451229572296143
I0209 12:35:34.314485 140529895442176 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.3041640520095825, loss=1.9325947761535645
I0209 12:36:09.557050 140529887049472 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.480118989944458, loss=2.0132579803466797
I0209 12:36:44.789160 140529895442176 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.30112284421920776, loss=1.9569072723388672
I0209 12:37:20.036673 140529887049472 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.3554876446723938, loss=1.9746934175491333
I0209 12:37:55.263824 140529895442176 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.3271166980266571, loss=1.897938847541809
I0209 12:38:05.562981 140699726837568 spec.py:321] Evaluating on the training split.
I0209 12:38:08.583071 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 12:41:52.903007 140699726837568 spec.py:333] Evaluating on the validation split.
I0209 12:41:55.626650 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 12:44:34.718279 140699726837568 spec.py:349] Evaluating on the test split.
I0209 12:44:37.452139 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 12:47:18.322208 140699726837568 submission_runner.py:408] Time since start: 32285.29s, 	Step: 54831, 	{'train/accuracy': 0.6168438792228699, 'train/loss': 1.9355334043502808, 'train/bleu': 29.60037342386599, 'validation/accuracy': 0.639086902141571, 'validation/loss': 1.7692811489105225, 'validation/bleu': 26.58346724934059, 'validation/num_examples': 3000, 'test/accuracy': 0.6477136611938477, 'test/loss': 1.711084008216858, 'test/bleu': 25.65318269635807, 'test/num_examples': 3003, 'score': 19352.45453119278, 'total_duration': 32285.287123441696, 'accumulated_submission_time': 19352.45453119278, 'accumulated_eval_time': 12930.402215003967, 'accumulated_logging_time': 0.6834166049957275}
I0209 12:47:18.345395 140529887049472 logging_writer.py:48] [54831] accumulated_eval_time=12930.402215, accumulated_logging_time=0.683417, accumulated_submission_time=19352.454531, global_step=54831, preemption_count=0, score=19352.454531, test/accuracy=0.647714, test/bleu=25.653183, test/loss=1.711084, test/num_examples=3003, total_duration=32285.287123, train/accuracy=0.616844, train/bleu=29.600373, train/loss=1.935533, validation/accuracy=0.639087, validation/bleu=26.583467, validation/loss=1.769281, validation/num_examples=3000
I0209 12:47:42.927297 140529895442176 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.5608727335929871, loss=1.9745267629623413
I0209 12:48:18.080208 140529887049472 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.29247263073921204, loss=1.975339651107788
I0209 12:48:53.292062 140529895442176 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.5083667635917664, loss=1.9303683042526245
I0209 12:49:28.572816 140529887049472 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.4396464228630066, loss=1.8842045068740845
I0209 12:50:03.860180 140529895442176 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.5090469121932983, loss=1.8807650804519653
I0209 12:50:39.099415 140529887049472 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.35527855157852173, loss=1.930050253868103
I0209 12:51:14.339555 140529895442176 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.5862269997596741, loss=1.9946118593215942
I0209 12:51:49.645115 140529887049472 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.33157989382743835, loss=2.0079967975616455
I0209 12:52:24.925705 140529895442176 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.7172244787216187, loss=1.819390058517456
I0209 12:53:00.170673 140529887049472 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.3120211660861969, loss=1.926133632659912
I0209 12:53:35.416355 140529895442176 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.30822139978408813, loss=1.9069932699203491
I0209 12:54:10.655447 140529887049472 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.31261172890663147, loss=1.950720191001892
I0209 12:54:45.906854 140529895442176 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.31664547324180603, loss=1.9353336095809937
I0209 12:55:21.187548 140529887049472 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.683463990688324, loss=2.001492500305176
I0209 12:55:56.477317 140529895442176 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.35339468717575073, loss=1.9936020374298096
I0209 12:56:31.721833 140529887049472 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.49914252758026123, loss=1.9080079793930054
I0209 12:57:06.995423 140529895442176 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.5057846903800964, loss=1.9802223443984985
I0209 12:57:42.264266 140529887049472 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.6632088422775269, loss=1.9547086954116821
I0209 12:58:17.498357 140529895442176 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.5688056945800781, loss=1.8412275314331055
I0209 12:58:52.724584 140529887049472 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.31935057044029236, loss=1.9056397676467896
I0209 12:59:27.968329 140529895442176 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.37057843804359436, loss=1.8455147743225098
I0209 13:00:03.177253 140529887049472 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.3239552974700928, loss=1.795527458190918
I0209 13:00:38.418307 140529895442176 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.37777841091156006, loss=2.0059077739715576
I0209 13:01:13.651655 140529887049472 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.2807908058166504, loss=1.9701792001724243
I0209 13:01:18.669087 140699726837568 spec.py:321] Evaluating on the training split.
I0209 13:01:21.696979 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 13:04:55.381354 140699726837568 spec.py:333] Evaluating on the validation split.
I0209 13:04:58.113791 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 13:08:03.053589 140699726837568 spec.py:349] Evaluating on the test split.
I0209 13:08:05.780802 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 13:11:16.643628 140699726837568 submission_runner.py:408] Time since start: 33723.61s, 	Step: 57216, 	{'train/accuracy': 0.6217406392097473, 'train/loss': 1.8886256217956543, 'train/bleu': 30.00592278859685, 'validation/accuracy': 0.6395828723907471, 'validation/loss': 1.764250636100769, 'validation/bleu': 26.488397964326836, 'validation/num_examples': 3000, 'test/accuracy': 0.6459241509437561, 'test/loss': 1.7121566534042358, 'test/bleu': 25.754728648283326, 'test/num_examples': 3003, 'score': 20192.690421819687, 'total_duration': 33723.60852479935, 'accumulated_submission_time': 20192.690421819687, 'accumulated_eval_time': 13528.37666606903, 'accumulated_logging_time': 0.7170102596282959}
I0209 13:11:16.672943 140529895442176 logging_writer.py:48] [57216] accumulated_eval_time=13528.376666, accumulated_logging_time=0.717010, accumulated_submission_time=20192.690422, global_step=57216, preemption_count=0, score=20192.690422, test/accuracy=0.645924, test/bleu=25.754729, test/loss=1.712157, test/num_examples=3003, total_duration=33723.608525, train/accuracy=0.621741, train/bleu=30.005923, train/loss=1.888626, validation/accuracy=0.639583, validation/bleu=26.488398, validation/loss=1.764251, validation/num_examples=3000
I0209 13:11:46.562540 140529887049472 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.3619784712791443, loss=1.9416837692260742
I0209 13:12:21.786786 140529895442176 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.38998496532440186, loss=1.8949573040008545
I0209 13:12:57.006640 140529887049472 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.5026671886444092, loss=1.8821390867233276
I0209 13:13:32.205417 140529895442176 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.6089392900466919, loss=1.8568556308746338
I0209 13:14:07.463808 140529887049472 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.25500237941741943, loss=1.8917560577392578
I0209 13:14:42.669282 140529895442176 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.3783726990222931, loss=1.8570055961608887
I0209 13:15:17.917389 140529887049472 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.4559992849826813, loss=1.9687716960906982
I0209 13:15:53.139157 140529895442176 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.3014056384563446, loss=1.9968520402908325
I0209 13:16:28.381567 140529887049472 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.27078142762184143, loss=1.928381323814392
I0209 13:17:03.625142 140529895442176 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.5146974325180054, loss=1.9009430408477783
I0209 13:17:38.861689 140529887049472 logging_writer.py:48] [58300] global_step=58300, grad_norm=0.46489161252975464, loss=1.851026177406311
I0209 13:18:14.142799 140529895442176 logging_writer.py:48] [58400] global_step=58400, grad_norm=0.478164404630661, loss=1.945796012878418
I0209 13:18:49.428141 140529887049472 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.29100826382637024, loss=1.918866515159607
I0209 13:19:24.692425 140529895442176 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.4148464500904083, loss=1.8899070024490356
I0209 13:19:59.937805 140529887049472 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.3594473600387573, loss=1.904005765914917
I0209 13:20:35.162373 140529895442176 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.39602720737457275, loss=1.891510248184204
I0209 13:21:10.401058 140529887049472 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.35082653164863586, loss=2.0252721309661865
I0209 13:21:45.673630 140529895442176 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.33574530482292175, loss=1.9112507104873657
I0209 13:22:20.915191 140529887049472 logging_writer.py:48] [59100] global_step=59100, grad_norm=0.6148487329483032, loss=1.8759994506835938
I0209 13:22:56.204814 140529895442176 logging_writer.py:48] [59200] global_step=59200, grad_norm=0.28894078731536865, loss=1.9965683221817017
I0209 13:23:31.476196 140529887049472 logging_writer.py:48] [59300] global_step=59300, grad_norm=0.3783886730670929, loss=1.9143236875534058
I0209 13:24:06.698738 140529895442176 logging_writer.py:48] [59400] global_step=59400, grad_norm=0.44496071338653564, loss=1.9187045097351074
I0209 13:24:41.944681 140529887049472 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.5933380722999573, loss=1.8837119340896606
I0209 13:25:17.178653 140529895442176 logging_writer.py:48] [59600] global_step=59600, grad_norm=0.576902449131012, loss=1.9773612022399902
I0209 13:25:17.185819 140699726837568 spec.py:321] Evaluating on the training split.
I0209 13:25:19.925885 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 13:28:59.502761 140699726837568 spec.py:333] Evaluating on the validation split.
I0209 13:29:02.232322 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 13:31:46.759769 140699726837568 spec.py:349] Evaluating on the test split.
I0209 13:31:49.491569 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 13:34:45.139114 140699726837568 submission_runner.py:408] Time since start: 35132.10s, 	Step: 59601, 	{'train/accuracy': 0.6205396056175232, 'train/loss': 1.9234768152236938, 'train/bleu': 29.974718274766825, 'validation/accuracy': 0.6435505747795105, 'validation/loss': 1.7510372400283813, 'validation/bleu': 26.64118576387656, 'validation/num_examples': 3000, 'test/accuracy': 0.6510720252990723, 'test/loss': 1.6877105236053467, 'test/bleu': 26.27622806513249, 'test/num_examples': 3003, 'score': 21033.113726854324, 'total_duration': 35132.104048252106, 'accumulated_submission_time': 21033.113726854324, 'accumulated_eval_time': 14096.32988357544, 'accumulated_logging_time': 0.7592217922210693}
I0209 13:34:45.163435 140529887049472 logging_writer.py:48] [59601] accumulated_eval_time=14096.329884, accumulated_logging_time=0.759222, accumulated_submission_time=21033.113727, global_step=59601, preemption_count=0, score=21033.113727, test/accuracy=0.651072, test/bleu=26.276228, test/loss=1.687711, test/num_examples=3003, total_duration=35132.104048, train/accuracy=0.620540, train/bleu=29.974718, train/loss=1.923477, validation/accuracy=0.643551, validation/bleu=26.641186, validation/loss=1.751037, validation/num_examples=3000
I0209 13:35:20.274034 140529895442176 logging_writer.py:48] [59700] global_step=59700, grad_norm=0.3623318672180176, loss=1.8657780885696411
I0209 13:35:55.455545 140529887049472 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.7492953538894653, loss=1.8552602529525757
I0209 13:36:30.666469 140529895442176 logging_writer.py:48] [59900] global_step=59900, grad_norm=0.2763724625110626, loss=1.856856107711792
I0209 13:37:05.878777 140529887049472 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.4419946074485779, loss=1.962192177772522
I0209 13:37:41.131734 140529895442176 logging_writer.py:48] [60100] global_step=60100, grad_norm=0.3596232831478119, loss=1.8614389896392822
I0209 13:38:16.350796 140529887049472 logging_writer.py:48] [60200] global_step=60200, grad_norm=0.3338298797607422, loss=1.941689133644104
I0209 13:38:51.616236 140529895442176 logging_writer.py:48] [60300] global_step=60300, grad_norm=0.4695548415184021, loss=1.9116922616958618
I0209 13:39:26.845783 140529887049472 logging_writer.py:48] [60400] global_step=60400, grad_norm=0.5831985473632812, loss=1.8960890769958496
I0209 13:40:02.110828 140529895442176 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.3005051612854004, loss=1.9103549718856812
I0209 13:40:37.338590 140529887049472 logging_writer.py:48] [60600] global_step=60600, grad_norm=0.2864006757736206, loss=1.892007827758789
I0209 13:41:12.583944 140529895442176 logging_writer.py:48] [60700] global_step=60700, grad_norm=0.31343868374824524, loss=1.8627771139144897
I0209 13:41:47.828324 140529887049472 logging_writer.py:48] [60800] global_step=60800, grad_norm=0.30529850721359253, loss=1.9224730730056763
I0209 13:42:23.090098 140529895442176 logging_writer.py:48] [60900] global_step=60900, grad_norm=0.3185183107852936, loss=1.85347318649292
I0209 13:42:58.326355 140529887049472 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.331965833902359, loss=1.8761744499206543
I0209 13:43:33.550858 140529895442176 logging_writer.py:48] [61100] global_step=61100, grad_norm=0.4912053048610687, loss=1.9433773756027222
I0209 13:44:08.789370 140529887049472 logging_writer.py:48] [61200] global_step=61200, grad_norm=0.4884633421897888, loss=1.960861086845398
I0209 13:44:44.054367 140529895442176 logging_writer.py:48] [61300] global_step=61300, grad_norm=0.31454312801361084, loss=1.8708416223526
I0209 13:45:19.291225 140529887049472 logging_writer.py:48] [61400] global_step=61400, grad_norm=0.34248900413513184, loss=1.88221275806427
I0209 13:45:54.521336 140529895442176 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.4631742238998413, loss=1.8690723180770874
I0209 13:46:29.758638 140529887049472 logging_writer.py:48] [61600] global_step=61600, grad_norm=0.2967418134212494, loss=1.8227750062942505
I0209 13:47:04.989628 140529895442176 logging_writer.py:48] [61700] global_step=61700, grad_norm=0.3214843273162842, loss=1.8920315504074097
I0209 13:47:40.241419 140529887049472 logging_writer.py:48] [61800] global_step=61800, grad_norm=0.4171585738658905, loss=1.9010648727416992
I0209 13:48:15.481176 140529895442176 logging_writer.py:48] [61900] global_step=61900, grad_norm=0.5512334108352661, loss=1.9472975730895996
I0209 13:48:45.176983 140699726837568 spec.py:321] Evaluating on the training split.
I0209 13:48:48.201829 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 13:53:28.822386 140699726837568 spec.py:333] Evaluating on the validation split.
I0209 13:53:31.533109 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 13:58:10.785370 140699726837568 spec.py:349] Evaluating on the test split.
I0209 13:58:13.501115 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 14:02:34.886644 140699726837568 submission_runner.py:408] Time since start: 36801.85s, 	Step: 61986, 	{'train/accuracy': 0.620170533657074, 'train/loss': 1.924217939376831, 'train/bleu': 29.704826058174017, 'validation/accuracy': 0.6417403221130371, 'validation/loss': 1.7392182350158691, 'validation/bleu': 24.938980730325106, 'validation/num_examples': 3000, 'test/accuracy': 0.6511649489402771, 'test/loss': 1.6913459300994873, 'test/bleu': 25.027113486174702, 'test/num_examples': 3003, 'score': 21873.043880462646, 'total_duration': 36801.8515625, 'accumulated_submission_time': 21873.043880462646, 'accumulated_eval_time': 14926.03948044777, 'accumulated_logging_time': 0.7936761379241943}
I0209 14:02:34.909857 140529887049472 logging_writer.py:48] [61986] accumulated_eval_time=14926.039480, accumulated_logging_time=0.793676, accumulated_submission_time=21873.043880, global_step=61986, preemption_count=0, score=21873.043880, test/accuracy=0.651165, test/bleu=25.027113, test/loss=1.691346, test/num_examples=3003, total_duration=36801.851562, train/accuracy=0.620171, train/bleu=29.704826, train/loss=1.924218, validation/accuracy=0.641740, validation/bleu=24.938981, validation/loss=1.739218, validation/num_examples=3000
I0209 14:02:40.176449 140529895442176 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.5473718643188477, loss=1.964005470275879
I0209 14:03:15.284170 140529887049472 logging_writer.py:48] [62100] global_step=62100, grad_norm=0.4286104440689087, loss=1.877010464668274
I0209 14:03:50.447307 140529895442176 logging_writer.py:48] [62200] global_step=62200, grad_norm=0.5779523849487305, loss=1.9241611957550049
I0209 14:04:25.644832 140529887049472 logging_writer.py:48] [62300] global_step=62300, grad_norm=0.3161523938179016, loss=1.8238481283187866
I0209 14:05:00.888632 140529895442176 logging_writer.py:48] [62400] global_step=62400, grad_norm=0.30387523770332336, loss=1.911904215812683
I0209 14:05:36.118133 140529887049472 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.46093544363975525, loss=1.9495998620986938
I0209 14:06:11.380833 140529895442176 logging_writer.py:48] [62600] global_step=62600, grad_norm=0.6646921634674072, loss=1.9700661897659302
I0209 14:06:46.615570 140529887049472 logging_writer.py:48] [62700] global_step=62700, grad_norm=0.38742926716804504, loss=1.9516242742538452
I0209 14:07:21.863946 140529895442176 logging_writer.py:48] [62800] global_step=62800, grad_norm=0.2724182903766632, loss=1.8677901029586792
I0209 14:07:57.156728 140529887049472 logging_writer.py:48] [62900] global_step=62900, grad_norm=0.4239886999130249, loss=1.9655977487564087
I0209 14:08:32.410225 140529895442176 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.4503563642501831, loss=1.9311177730560303
I0209 14:09:07.644168 140529887049472 logging_writer.py:48] [63100] global_step=63100, grad_norm=0.29312923550605774, loss=1.8979158401489258
I0209 14:09:42.911696 140529895442176 logging_writer.py:48] [63200] global_step=63200, grad_norm=0.43424317240715027, loss=1.9188785552978516
I0209 14:10:18.274993 140529887049472 logging_writer.py:48] [63300] global_step=63300, grad_norm=0.29303663969039917, loss=1.9328137636184692
I0209 14:10:53.509009 140529895442176 logging_writer.py:48] [63400] global_step=63400, grad_norm=0.392145574092865, loss=1.8440802097320557
I0209 14:11:28.764446 140529887049472 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.3901245892047882, loss=1.8262295722961426
I0209 14:12:03.997309 140529895442176 logging_writer.py:48] [63600] global_step=63600, grad_norm=0.35431209206581116, loss=1.855719804763794
I0209 14:12:39.235216 140529887049472 logging_writer.py:48] [63700] global_step=63700, grad_norm=0.2925161123275757, loss=1.8846135139465332
I0209 14:13:14.474170 140529895442176 logging_writer.py:48] [63800] global_step=63800, grad_norm=0.33313658833503723, loss=1.94969642162323
I0209 14:13:49.728095 140529887049472 logging_writer.py:48] [63900] global_step=63900, grad_norm=0.34932655096054077, loss=1.9436835050582886
I0209 14:14:24.979140 140529895442176 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.3161243796348572, loss=1.8128832578659058
I0209 14:15:00.376371 140529887049472 logging_writer.py:48] [64100] global_step=64100, grad_norm=0.48282918334007263, loss=1.885623812675476
I0209 14:15:35.683877 140529895442176 logging_writer.py:48] [64200] global_step=64200, grad_norm=0.3226862847805023, loss=1.964167594909668
I0209 14:16:10.983983 140529887049472 logging_writer.py:48] [64300] global_step=64300, grad_norm=0.6640391945838928, loss=1.8828768730163574
I0209 14:16:35.031033 140699726837568 spec.py:321] Evaluating on the training split.
I0209 14:16:38.059479 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 14:20:52.945469 140699726837568 spec.py:333] Evaluating on the validation split.
I0209 14:20:55.669283 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 14:23:29.747276 140699726837568 spec.py:349] Evaluating on the test split.
I0209 14:23:32.478312 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 14:26:18.648670 140699726837568 submission_runner.py:408] Time since start: 38225.61s, 	Step: 64370, 	{'train/accuracy': 0.6249828934669495, 'train/loss': 1.8825191259384155, 'train/bleu': 30.120804428595495, 'validation/accuracy': 0.6434885859489441, 'validation/loss': 1.73857581615448, 'validation/bleu': 27.00111898618707, 'validation/num_examples': 3000, 'test/accuracy': 0.6523153781890869, 'test/loss': 1.6770296096801758, 'test/bleu': 26.33216731822289, 'test/num_examples': 3003, 'score': 22713.07751083374, 'total_duration': 38225.61360192299, 'accumulated_submission_time': 22713.07751083374, 'accumulated_eval_time': 15509.657069206238, 'accumulated_logging_time': 0.826836109161377}
I0209 14:26:18.673390 140529895442176 logging_writer.py:48] [64370] accumulated_eval_time=15509.657069, accumulated_logging_time=0.826836, accumulated_submission_time=22713.077511, global_step=64370, preemption_count=0, score=22713.077511, test/accuracy=0.652315, test/bleu=26.332167, test/loss=1.677030, test/num_examples=3003, total_duration=38225.613602, train/accuracy=0.624983, train/bleu=30.120804, train/loss=1.882519, validation/accuracy=0.643489, validation/bleu=27.001119, validation/loss=1.738576, validation/num_examples=3000
I0209 14:26:29.585595 140529887049472 logging_writer.py:48] [64400] global_step=64400, grad_norm=0.41865962743759155, loss=1.864925742149353
I0209 14:27:04.720057 140529895442176 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.5050293803215027, loss=1.9667644500732422
I0209 14:27:39.898358 140529887049472 logging_writer.py:48] [64600] global_step=64600, grad_norm=0.29885151982307434, loss=1.8446192741394043
I0209 14:28:15.127800 140529895442176 logging_writer.py:48] [64700] global_step=64700, grad_norm=0.30179068446159363, loss=1.7923434972763062
I0209 14:28:50.377504 140529887049472 logging_writer.py:48] [64800] global_step=64800, grad_norm=0.5630226731300354, loss=1.9035216569900513
I0209 14:29:25.621895 140529895442176 logging_writer.py:48] [64900] global_step=64900, grad_norm=0.48210451006889343, loss=1.9803729057312012
I0209 14:30:00.852940 140529887049472 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.4589269161224365, loss=1.846598744392395
I0209 14:30:36.086193 140529895442176 logging_writer.py:48] [65100] global_step=65100, grad_norm=0.2723768949508667, loss=1.8461734056472778
I0209 14:31:11.395544 140529887049472 logging_writer.py:48] [65200] global_step=65200, grad_norm=0.5490655899047852, loss=1.8810851573944092
I0209 14:31:46.679517 140529895442176 logging_writer.py:48] [65300] global_step=65300, grad_norm=0.6277667284011841, loss=1.9946075677871704
I0209 14:32:21.938701 140529887049472 logging_writer.py:48] [65400] global_step=65400, grad_norm=0.30905255675315857, loss=1.840100884437561
I0209 14:32:57.174381 140529895442176 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.3099730312824249, loss=1.8603500127792358
I0209 14:33:32.407769 140529887049472 logging_writer.py:48] [65600] global_step=65600, grad_norm=0.3804723620414734, loss=1.7995498180389404
I0209 14:34:07.648625 140529895442176 logging_writer.py:48] [65700] global_step=65700, grad_norm=0.45811641216278076, loss=1.8978493213653564
I0209 14:34:42.899990 140529887049472 logging_writer.py:48] [65800] global_step=65800, grad_norm=0.49388760328292847, loss=1.841735601425171
I0209 14:35:18.134393 140529895442176 logging_writer.py:48] [65900] global_step=65900, grad_norm=0.3459579348564148, loss=1.9311267137527466
I0209 14:35:53.416356 140529887049472 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.42141616344451904, loss=1.9016274213790894
I0209 14:36:28.690952 140529895442176 logging_writer.py:48] [66100] global_step=66100, grad_norm=0.2579602897167206, loss=1.8920032978057861
I0209 14:37:03.940607 140529887049472 logging_writer.py:48] [66200] global_step=66200, grad_norm=0.37314721941947937, loss=1.7974822521209717
I0209 14:37:39.170276 140529895442176 logging_writer.py:48] [66300] global_step=66300, grad_norm=0.2927164137363434, loss=1.8005739450454712
I0209 14:38:14.399024 140529887049472 logging_writer.py:48] [66400] global_step=66400, grad_norm=0.3259425759315491, loss=1.8332242965698242
I0209 14:38:49.644170 140529895442176 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.3074513375759125, loss=1.9485111236572266
I0209 14:39:24.919986 140529887049472 logging_writer.py:48] [66600] global_step=66600, grad_norm=0.35693293809890747, loss=1.9456592798233032
I0209 14:40:00.201583 140529895442176 logging_writer.py:48] [66700] global_step=66700, grad_norm=0.32264217734336853, loss=1.8572628498077393
I0209 14:40:18.976183 140699726837568 spec.py:321] Evaluating on the training split.
I0209 14:40:22.005838 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 14:43:25.741458 140699726837568 spec.py:333] Evaluating on the validation split.
I0209 14:43:28.469110 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 14:46:08.793108 140699726837568 spec.py:349] Evaluating on the test split.
I0209 14:46:11.529932 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 14:48:50.501393 140699726837568 submission_runner.py:408] Time since start: 39577.47s, 	Step: 66755, 	{'train/accuracy': 0.6223474144935608, 'train/loss': 1.897873878479004, 'train/bleu': 29.606168008502756, 'validation/accuracy': 0.6448525190353394, 'validation/loss': 1.712667465209961, 'validation/bleu': 26.909955268306703, 'validation/num_examples': 3000, 'test/accuracy': 0.6545000672340393, 'test/loss': 1.6544857025146484, 'test/bleu': 26.066548963880994, 'test/num_examples': 3003, 'score': 23553.293513298035, 'total_duration': 39577.4663040638, 'accumulated_submission_time': 23553.293513298035, 'accumulated_eval_time': 16021.182207584381, 'accumulated_logging_time': 0.8614444732666016}
I0209 14:48:50.526089 140529887049472 logging_writer.py:48] [66755] accumulated_eval_time=16021.182208, accumulated_logging_time=0.861444, accumulated_submission_time=23553.293513, global_step=66755, preemption_count=0, score=23553.293513, test/accuracy=0.654500, test/bleu=26.066549, test/loss=1.654486, test/num_examples=3003, total_duration=39577.466304, train/accuracy=0.622347, train/bleu=29.606168, train/loss=1.897874, validation/accuracy=0.644853, validation/bleu=26.909955, validation/loss=1.712667, validation/num_examples=3000
I0209 14:49:06.669998 140529895442176 logging_writer.py:48] [66800] global_step=66800, grad_norm=0.2899334728717804, loss=1.8935649394989014
I0209 14:49:41.823792 140529887049472 logging_writer.py:48] [66900] global_step=66900, grad_norm=0.35174667835235596, loss=1.8394945859909058
I0209 14:50:17.021375 140529895442176 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.5152329206466675, loss=1.8611793518066406
I0209 14:50:52.244500 140529887049472 logging_writer.py:48] [67100] global_step=67100, grad_norm=0.40270012617111206, loss=1.9090030193328857
I0209 14:51:27.450776 140529895442176 logging_writer.py:48] [67200] global_step=67200, grad_norm=0.27027952671051025, loss=1.9787784814834595
I0209 14:52:02.693552 140529887049472 logging_writer.py:48] [67300] global_step=67300, grad_norm=0.4911242425441742, loss=1.8535524606704712
I0209 14:52:37.903584 140529895442176 logging_writer.py:48] [67400] global_step=67400, grad_norm=0.3999360501766205, loss=1.8232512474060059
I0209 14:53:13.159091 140529887049472 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.40077367424964905, loss=1.8682101964950562
I0209 14:53:48.395476 140529895442176 logging_writer.py:48] [67600] global_step=67600, grad_norm=0.35969600081443787, loss=1.8884618282318115
I0209 14:54:23.628947 140529887049472 logging_writer.py:48] [67700] global_step=67700, grad_norm=0.5468928217887878, loss=1.885475993156433
I0209 14:54:58.917909 140529895442176 logging_writer.py:48] [67800] global_step=67800, grad_norm=0.3477345407009125, loss=1.8923126459121704
I0209 14:55:34.162383 140529887049472 logging_writer.py:48] [67900] global_step=67900, grad_norm=0.5384383797645569, loss=1.787351131439209
I0209 14:56:09.417443 140529895442176 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.3698699474334717, loss=1.9589347839355469
I0209 14:56:44.639628 140529887049472 logging_writer.py:48] [68100] global_step=68100, grad_norm=0.3607040047645569, loss=1.8507963418960571
I0209 14:57:19.880219 140529895442176 logging_writer.py:48] [68200] global_step=68200, grad_norm=0.27332472801208496, loss=1.8715296983718872
I0209 14:57:55.149189 140529887049472 logging_writer.py:48] [68300] global_step=68300, grad_norm=0.2908354103565216, loss=1.9082615375518799
I0209 14:58:30.379727 140529895442176 logging_writer.py:48] [68400] global_step=68400, grad_norm=0.33881378173828125, loss=1.9840792417526245
I0209 14:59:05.593657 140529887049472 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.30541396141052246, loss=1.8677890300750732
I0209 14:59:40.816496 140529895442176 logging_writer.py:48] [68600] global_step=68600, grad_norm=0.347064733505249, loss=1.7986361980438232
I0209 15:00:16.083762 140529887049472 logging_writer.py:48] [68700] global_step=68700, grad_norm=0.41403090953826904, loss=1.8254148960113525
I0209 15:00:51.353198 140529895442176 logging_writer.py:48] [68800] global_step=68800, grad_norm=0.3344280421733856, loss=1.9714910984039307
I0209 15:01:26.580242 140529887049472 logging_writer.py:48] [68900] global_step=68900, grad_norm=0.27871525287628174, loss=1.901612401008606
I0209 15:02:01.837936 140529895442176 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.3199612498283386, loss=1.8779841661453247
I0209 15:02:37.084943 140529887049472 logging_writer.py:48] [69100] global_step=69100, grad_norm=0.5179702043533325, loss=1.9205315113067627
I0209 15:02:50.553567 140699726837568 spec.py:321] Evaluating on the training split.
I0209 15:02:53.571482 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 15:06:22.149775 140699726837568 spec.py:333] Evaluating on the validation split.
I0209 15:06:24.875734 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 15:09:10.679848 140699726837568 spec.py:349] Evaluating on the test split.
I0209 15:09:13.411394 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 15:12:27.172446 140699726837568 submission_runner.py:408] Time since start: 40994.14s, 	Step: 69140, 	{'train/accuracy': 0.6475690007209778, 'train/loss': 1.7080274820327759, 'train/bleu': 31.347088927075134, 'validation/accuracy': 0.6484978199005127, 'validation/loss': 1.7034063339233398, 'validation/bleu': 27.044254202767398, 'validation/num_examples': 3000, 'test/accuracy': 0.6583929061889648, 'test/loss': 1.6351416110992432, 'test/bleu': 26.70894910112787, 'test/num_examples': 3003, 'score': 24393.235889673233, 'total_duration': 40994.1373796463, 'accumulated_submission_time': 24393.235889673233, 'accumulated_eval_time': 16597.801033496857, 'accumulated_logging_time': 0.8966538906097412}
I0209 15:12:27.197379 140529895442176 logging_writer.py:48] [69140] accumulated_eval_time=16597.801033, accumulated_logging_time=0.896654, accumulated_submission_time=24393.235890, global_step=69140, preemption_count=0, score=24393.235890, test/accuracy=0.658393, test/bleu=26.708949, test/loss=1.635142, test/num_examples=3003, total_duration=40994.137380, train/accuracy=0.647569, train/bleu=31.347089, train/loss=1.708027, validation/accuracy=0.648498, validation/bleu=27.044254, validation/loss=1.703406, validation/num_examples=3000
I0209 15:12:48.617831 140529887049472 logging_writer.py:48] [69200] global_step=69200, grad_norm=0.3696676194667816, loss=1.8928136825561523
I0209 15:13:23.777156 140529895442176 logging_writer.py:48] [69300] global_step=69300, grad_norm=0.2980407476425171, loss=1.960863471031189
I0209 15:13:59.012179 140529887049472 logging_writer.py:48] [69400] global_step=69400, grad_norm=0.36708611249923706, loss=1.8016983270645142
I0209 15:14:34.221138 140529895442176 logging_writer.py:48] [69500] global_step=69500, grad_norm=0.2957698702812195, loss=1.8634439706802368
I0209 15:15:09.440924 140529887049472 logging_writer.py:48] [69600] global_step=69600, grad_norm=0.7019677758216858, loss=1.8656902313232422
I0209 15:15:44.684413 140529895442176 logging_writer.py:48] [69700] global_step=69700, grad_norm=0.3335821032524109, loss=1.8691970109939575
I0209 15:16:19.959973 140529887049472 logging_writer.py:48] [69800] global_step=69800, grad_norm=0.30807048082351685, loss=1.8411369323730469
I0209 15:16:55.258948 140529895442176 logging_writer.py:48] [69900] global_step=69900, grad_norm=0.49886345863342285, loss=1.825166940689087
I0209 15:17:30.495503 140529887049472 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.29549458622932434, loss=1.8683429956436157
I0209 15:18:05.758703 140529895442176 logging_writer.py:48] [70100] global_step=70100, grad_norm=0.736649751663208, loss=1.8745132684707642
I0209 15:18:40.996230 140529887049472 logging_writer.py:48] [70200] global_step=70200, grad_norm=0.28701865673065186, loss=1.8003255128860474
I0209 15:19:16.207368 140529895442176 logging_writer.py:48] [70300] global_step=70300, grad_norm=0.3841732442378998, loss=1.8175878524780273
I0209 15:19:51.423238 140529887049472 logging_writer.py:48] [70400] global_step=70400, grad_norm=0.3356742262840271, loss=1.8713809251785278
I0209 15:20:26.659554 140529895442176 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.5471579432487488, loss=1.8295270204544067
I0209 15:21:01.906583 140529887049472 logging_writer.py:48] [70600] global_step=70600, grad_norm=0.3273644745349884, loss=1.880318284034729
I0209 15:21:37.127305 140529895442176 logging_writer.py:48] [70700] global_step=70700, grad_norm=0.2936572730541229, loss=1.835975170135498
I0209 15:22:12.358862 140529887049472 logging_writer.py:48] [70800] global_step=70800, grad_norm=0.28942838311195374, loss=1.827166199684143
I0209 15:22:47.605657 140529895442176 logging_writer.py:48] [70900] global_step=70900, grad_norm=0.3027137517929077, loss=1.8398349285125732
I0209 15:23:22.853402 140529887049472 logging_writer.py:48] [71000] global_step=71000, grad_norm=0.3052751123905182, loss=1.8178983926773071
I0209 15:23:58.172521 140529895442176 logging_writer.py:48] [71100] global_step=71100, grad_norm=0.297911137342453, loss=1.846228837966919
I0209 15:24:33.450364 140529887049472 logging_writer.py:48] [71200] global_step=71200, grad_norm=0.2979012429714203, loss=1.849066138267517
I0209 15:25:08.741193 140529895442176 logging_writer.py:48] [71300] global_step=71300, grad_norm=0.266594797372818, loss=1.865514874458313
I0209 15:25:44.013930 140529887049472 logging_writer.py:48] [71400] global_step=71400, grad_norm=0.38311371207237244, loss=1.8860633373260498
I0209 15:26:19.268290 140529895442176 logging_writer.py:48] [71500] global_step=71500, grad_norm=0.2765728235244751, loss=1.8644171953201294
I0209 15:26:27.446769 140699726837568 spec.py:321] Evaluating on the training split.
I0209 15:26:30.478096 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 15:29:42.950107 140699726837568 spec.py:333] Evaluating on the validation split.
I0209 15:29:45.676964 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 15:32:33.790286 140699726837568 spec.py:349] Evaluating on the test split.
I0209 15:32:36.528106 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 15:35:21.269642 140699726837568 submission_runner.py:408] Time since start: 42368.23s, 	Step: 71525, 	{'train/accuracy': 0.6270906329154968, 'train/loss': 1.8595634698867798, 'train/bleu': 30.490072894391158, 'validation/accuracy': 0.6483118534088135, 'validation/loss': 1.6941295862197876, 'validation/bleu': 27.250290188229883, 'validation/num_examples': 3000, 'test/accuracy': 0.6592528223991394, 'test/loss': 1.626090168952942, 'test/bleu': 26.910900978321823, 'test/num_examples': 3003, 'score': 25233.399693965912, 'total_duration': 42368.234537124634, 'accumulated_submission_time': 25233.399693965912, 'accumulated_eval_time': 17131.623816490173, 'accumulated_logging_time': 0.9319117069244385}
I0209 15:35:21.299394 140529887049472 logging_writer.py:48] [71525] accumulated_eval_time=17131.623816, accumulated_logging_time=0.931912, accumulated_submission_time=25233.399694, global_step=71525, preemption_count=0, score=25233.399694, test/accuracy=0.659253, test/bleu=26.910901, test/loss=1.626090, test/num_examples=3003, total_duration=42368.234537, train/accuracy=0.627091, train/bleu=30.490073, train/loss=1.859563, validation/accuracy=0.648312, validation/bleu=27.250290, validation/loss=1.694130, validation/num_examples=3000
I0209 15:35:47.995820 140529895442176 logging_writer.py:48] [71600] global_step=71600, grad_norm=0.5631497502326965, loss=1.7929961681365967
I0209 15:36:23.188397 140529887049472 logging_writer.py:48] [71700] global_step=71700, grad_norm=0.32045990228652954, loss=1.9119411706924438
I0209 15:36:58.425912 140529895442176 logging_writer.py:48] [71800] global_step=71800, grad_norm=0.5848026871681213, loss=1.818739652633667
I0209 15:37:33.643295 140529887049472 logging_writer.py:48] [71900] global_step=71900, grad_norm=0.38867077231407166, loss=1.8509324789047241
I0209 15:38:08.870485 140529895442176 logging_writer.py:48] [72000] global_step=72000, grad_norm=0.5064506530761719, loss=1.774457573890686
I0209 15:38:44.121145 140529887049472 logging_writer.py:48] [72100] global_step=72100, grad_norm=0.5310904383659363, loss=1.8156064748764038
I0209 15:39:19.378793 140529895442176 logging_writer.py:48] [72200] global_step=72200, grad_norm=0.5356475710868835, loss=1.8351703882217407
I0209 15:39:54.640517 140529887049472 logging_writer.py:48] [72300] global_step=72300, grad_norm=0.3572150468826294, loss=1.8078824281692505
I0209 15:40:29.909739 140529895442176 logging_writer.py:48] [72400] global_step=72400, grad_norm=0.39487412571907043, loss=1.8888092041015625
I0209 15:41:05.135251 140529887049472 logging_writer.py:48] [72500] global_step=72500, grad_norm=0.40398305654525757, loss=1.9104797840118408
I0209 15:41:40.398622 140529895442176 logging_writer.py:48] [72600] global_step=72600, grad_norm=0.372835248708725, loss=1.8177194595336914
I0209 15:42:15.658424 140529887049472 logging_writer.py:48] [72700] global_step=72700, grad_norm=0.25985807180404663, loss=1.765938639640808
I0209 15:42:50.876576 140529895442176 logging_writer.py:48] [72800] global_step=72800, grad_norm=0.32722023129463196, loss=1.8883453607559204
I0209 15:43:26.122733 140529887049472 logging_writer.py:48] [72900] global_step=72900, grad_norm=0.4461108446121216, loss=1.9518831968307495
I0209 15:44:01.395622 140529895442176 logging_writer.py:48] [73000] global_step=73000, grad_norm=0.2896554172039032, loss=1.855721354484558
I0209 15:44:36.618480 140529887049472 logging_writer.py:48] [73100] global_step=73100, grad_norm=0.25342023372650146, loss=1.8286632299423218
I0209 15:45:11.852957 140529895442176 logging_writer.py:48] [73200] global_step=73200, grad_norm=0.39060550928115845, loss=1.8604600429534912
I0209 15:45:47.103498 140529887049472 logging_writer.py:48] [73300] global_step=73300, grad_norm=0.36145949363708496, loss=1.8291915655136108
I0209 15:46:22.386407 140529895442176 logging_writer.py:48] [73400] global_step=73400, grad_norm=0.30232954025268555, loss=1.7827736139297485
I0209 15:46:57.640048 140529887049472 logging_writer.py:48] [73500] global_step=73500, grad_norm=0.2839661240577698, loss=1.8885983228683472
I0209 15:47:32.891074 140529895442176 logging_writer.py:48] [73600] global_step=73600, grad_norm=0.30099591612815857, loss=1.7200192213058472
I0209 15:48:08.147261 140529887049472 logging_writer.py:48] [73700] global_step=73700, grad_norm=0.6041138768196106, loss=1.929964303970337
I0209 15:48:43.432311 140529895442176 logging_writer.py:48] [73800] global_step=73800, grad_norm=0.3345085382461548, loss=1.8407416343688965
I0209 15:49:18.711368 140529887049472 logging_writer.py:48] [73900] global_step=73900, grad_norm=0.4811253547668457, loss=1.8645769357681274
I0209 15:49:21.610564 140699726837568 spec.py:321] Evaluating on the training split.
I0209 15:49:24.662388 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 15:52:34.603564 140699726837568 spec.py:333] Evaluating on the validation split.
I0209 15:52:37.336958 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 15:55:13.763998 140699726837568 spec.py:349] Evaluating on the test split.
I0209 15:55:16.499951 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 15:57:32.713408 140699726837568 submission_runner.py:408] Time since start: 43699.68s, 	Step: 73910, 	{'train/accuracy': 0.6320022940635681, 'train/loss': 1.8371987342834473, 'train/bleu': 30.474995803724568, 'validation/accuracy': 0.6498989462852478, 'validation/loss': 1.6905559301376343, 'validation/bleu': 27.39593869430475, 'validation/num_examples': 3000, 'test/accuracy': 0.658288300037384, 'test/loss': 1.6216771602630615, 'test/bleu': 26.540084120816232, 'test/num_examples': 3003, 'score': 26073.622700452805, 'total_duration': 43699.67828798294, 'accumulated_submission_time': 26073.622700452805, 'accumulated_eval_time': 17622.726563453674, 'accumulated_logging_time': 0.9732100963592529}
I0209 15:57:32.745153 140529895442176 logging_writer.py:48] [73910] accumulated_eval_time=17622.726563, accumulated_logging_time=0.973210, accumulated_submission_time=26073.622700, global_step=73910, preemption_count=0, score=26073.622700, test/accuracy=0.658288, test/bleu=26.540084, test/loss=1.621677, test/num_examples=3003, total_duration=43699.678288, train/accuracy=0.632002, train/bleu=30.474996, train/loss=1.837199, validation/accuracy=0.649899, validation/bleu=27.395939, validation/loss=1.690556, validation/num_examples=3000
I0209 15:58:04.751927 140529887049472 logging_writer.py:48] [74000] global_step=74000, grad_norm=0.6166512966156006, loss=1.9130052328109741
I0209 15:58:39.892048 140529895442176 logging_writer.py:48] [74100] global_step=74100, grad_norm=0.3051769733428955, loss=1.8847131729125977
I0209 15:59:15.113240 140529887049472 logging_writer.py:48] [74200] global_step=74200, grad_norm=0.49751758575439453, loss=1.7843512296676636
I0209 15:59:50.355394 140529895442176 logging_writer.py:48] [74300] global_step=74300, grad_norm=0.3108498156070709, loss=1.8687916994094849
I0209 16:00:25.580688 140529887049472 logging_writer.py:48] [74400] global_step=74400, grad_norm=0.5091328620910645, loss=1.8234269618988037
I0209 16:01:00.797547 140529895442176 logging_writer.py:48] [74500] global_step=74500, grad_norm=0.3115927278995514, loss=1.8152692317962646
I0209 16:01:36.074944 140529887049472 logging_writer.py:48] [74600] global_step=74600, grad_norm=0.27513715624809265, loss=1.8196916580200195
I0209 16:02:11.307809 140529895442176 logging_writer.py:48] [74700] global_step=74700, grad_norm=0.3765764832496643, loss=1.8457388877868652
I0209 16:02:46.586542 140529887049472 logging_writer.py:48] [74800] global_step=74800, grad_norm=0.3146425485610962, loss=1.871252179145813
I0209 16:03:21.811179 140529895442176 logging_writer.py:48] [74900] global_step=74900, grad_norm=0.7645925283432007, loss=1.8312842845916748
I0209 16:03:57.033045 140529887049472 logging_writer.py:48] [75000] global_step=75000, grad_norm=0.664810061454773, loss=1.844899296760559
I0209 16:04:32.261604 140529895442176 logging_writer.py:48] [75100] global_step=75100, grad_norm=0.2991059422492981, loss=1.8389979600906372
I0209 16:05:07.499114 140529887049472 logging_writer.py:48] [75200] global_step=75200, grad_norm=0.4974524974822998, loss=1.8367863893508911
I0209 16:05:42.743326 140529895442176 logging_writer.py:48] [75300] global_step=75300, grad_norm=0.41878753900527954, loss=1.796181321144104
I0209 16:06:17.989557 140529887049472 logging_writer.py:48] [75400] global_step=75400, grad_norm=0.35460224747657776, loss=1.894903302192688
I0209 16:06:53.297326 140529895442176 logging_writer.py:48] [75500] global_step=75500, grad_norm=0.3624088764190674, loss=1.8454020023345947
I0209 16:07:28.534703 140529887049472 logging_writer.py:48] [75600] global_step=75600, grad_norm=0.4943470358848572, loss=1.7497410774230957
I0209 16:08:03.761241 140529895442176 logging_writer.py:48] [75700] global_step=75700, grad_norm=0.4598065912723541, loss=1.8485479354858398
I0209 16:08:38.979269 140529887049472 logging_writer.py:48] [75800] global_step=75800, grad_norm=0.45946183800697327, loss=1.8025928735733032
I0209 16:09:14.249031 140529895442176 logging_writer.py:48] [75900] global_step=75900, grad_norm=0.42934128642082214, loss=1.936090350151062
I0209 16:09:49.456410 140529887049472 logging_writer.py:48] [76000] global_step=76000, grad_norm=0.4490737020969391, loss=1.7850916385650635
I0209 16:10:24.698334 140529895442176 logging_writer.py:48] [76100] global_step=76100, grad_norm=0.4299318194389343, loss=1.814259648323059
I0209 16:10:59.923864 140529887049472 logging_writer.py:48] [76200] global_step=76200, grad_norm=0.2922004163265228, loss=1.8444308042526245
I0209 16:11:32.767147 140699726837568 spec.py:321] Evaluating on the training split.
I0209 16:11:35.792743 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 16:15:46.880758 140699726837568 spec.py:333] Evaluating on the validation split.
I0209 16:15:49.604988 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 16:19:48.554212 140699726837568 spec.py:349] Evaluating on the test split.
I0209 16:19:51.281576 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 16:23:30.559532 140699726837568 submission_runner.py:408] Time since start: 45257.52s, 	Step: 76295, 	{'train/accuracy': 0.6367040872573853, 'train/loss': 1.7800288200378418, 'train/bleu': 31.028093772921565, 'validation/accuracy': 0.6544989943504333, 'validation/loss': 1.6648274660110474, 'validation/bleu': 27.578319945120132, 'validation/num_examples': 3000, 'test/accuracy': 0.6646563410758972, 'test/loss': 1.5929890871047974, 'test/bleu': 27.065361558941973, 'test/num_examples': 3003, 'score': 26913.55896472931, 'total_duration': 45257.52446436882, 'accumulated_submission_time': 26913.55896472931, 'accumulated_eval_time': 18340.518897771835, 'accumulated_logging_time': 1.016244649887085}
I0209 16:23:30.586067 140529895442176 logging_writer.py:48] [76295] accumulated_eval_time=18340.518898, accumulated_logging_time=1.016245, accumulated_submission_time=26913.558965, global_step=76295, preemption_count=0, score=26913.558965, test/accuracy=0.664656, test/bleu=27.065362, test/loss=1.592989, test/num_examples=3003, total_duration=45257.524464, train/accuracy=0.636704, train/bleu=31.028094, train/loss=1.780029, validation/accuracy=0.654499, validation/bleu=27.578320, validation/loss=1.664827, validation/num_examples=3000
I0209 16:23:32.717745 140529887049472 logging_writer.py:48] [76300] global_step=76300, grad_norm=0.6567798852920532, loss=1.8671255111694336
I0209 16:24:07.836699 140529895442176 logging_writer.py:48] [76400] global_step=76400, grad_norm=0.44317859411239624, loss=1.7842140197753906
I0209 16:24:43.040996 140529887049472 logging_writer.py:48] [76500] global_step=76500, grad_norm=0.4643254578113556, loss=1.7763994932174683
I0209 16:25:18.250293 140529895442176 logging_writer.py:48] [76600] global_step=76600, grad_norm=0.4131319224834442, loss=1.8132134675979614
I0209 16:25:53.472313 140529887049472 logging_writer.py:48] [76700] global_step=76700, grad_norm=0.39086323976516724, loss=1.8691129684448242
I0209 16:26:28.689429 140529895442176 logging_writer.py:48] [76800] global_step=76800, grad_norm=0.35474544763565063, loss=1.7452718019485474
I0209 16:27:03.905336 140529887049472 logging_writer.py:48] [76900] global_step=76900, grad_norm=0.5518086552619934, loss=1.928125262260437
I0209 16:27:39.133798 140529895442176 logging_writer.py:48] [77000] global_step=77000, grad_norm=0.3035484254360199, loss=1.8142646551132202
I0209 16:28:14.376039 140529887049472 logging_writer.py:48] [77100] global_step=77100, grad_norm=0.2597102224826813, loss=1.812119722366333
I0209 16:28:49.619114 140529895442176 logging_writer.py:48] [77200] global_step=77200, grad_norm=0.5928953289985657, loss=1.8311501741409302
I0209 16:29:24.871497 140529887049472 logging_writer.py:48] [77300] global_step=77300, grad_norm=0.4791257679462433, loss=1.8370521068572998
I0209 16:30:00.112168 140529895442176 logging_writer.py:48] [77400] global_step=77400, grad_norm=0.30363360047340393, loss=1.8101634979248047
I0209 16:30:35.341002 140529887049472 logging_writer.py:48] [77500] global_step=77500, grad_norm=0.43786513805389404, loss=1.8033030033111572
I0209 16:31:10.558835 140529895442176 logging_writer.py:48] [77600] global_step=77600, grad_norm=0.29622164368629456, loss=1.8411240577697754
I0209 16:31:45.814455 140529887049472 logging_writer.py:48] [77700] global_step=77700, grad_norm=0.37101680040359497, loss=1.8533213138580322
I0209 16:32:21.047175 140529895442176 logging_writer.py:48] [77800] global_step=77800, grad_norm=0.31177884340286255, loss=1.7945128679275513
I0209 16:32:56.292324 140529887049472 logging_writer.py:48] [77900] global_step=77900, grad_norm=0.34957483410835266, loss=1.877152919769287
I0209 16:33:31.530583 140529895442176 logging_writer.py:48] [78000] global_step=78000, grad_norm=0.3020710349082947, loss=1.8082178831100464
I0209 16:34:06.761131 140529887049472 logging_writer.py:48] [78100] global_step=78100, grad_norm=0.3317488729953766, loss=1.8065999746322632
I0209 16:34:41.977261 140529895442176 logging_writer.py:48] [78200] global_step=78200, grad_norm=0.29897287487983704, loss=1.8794435262680054
I0209 16:35:17.256091 140529887049472 logging_writer.py:48] [78300] global_step=78300, grad_norm=0.49190986156463623, loss=1.86497962474823
I0209 16:35:52.558483 140529895442176 logging_writer.py:48] [78400] global_step=78400, grad_norm=0.28938060998916626, loss=1.8603103160858154
I0209 16:36:27.868615 140529887049472 logging_writer.py:48] [78500] global_step=78500, grad_norm=0.3139530420303345, loss=1.7854942083358765
I0209 16:37:03.113500 140529895442176 logging_writer.py:48] [78600] global_step=78600, grad_norm=0.274793803691864, loss=1.7040009498596191
I0209 16:37:30.663692 140699726837568 spec.py:321] Evaluating on the training split.
I0209 16:37:33.688897 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 16:40:59.130518 140699726837568 spec.py:333] Evaluating on the validation split.
I0209 16:41:01.881432 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 16:43:55.630726 140699726837568 spec.py:349] Evaluating on the test split.
I0209 16:43:58.364136 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 16:46:50.304616 140699726837568 submission_runner.py:408] Time since start: 46657.27s, 	Step: 78680, 	{'train/accuracy': 0.6351809501647949, 'train/loss': 1.8063995838165283, 'train/bleu': 30.51627421938524, 'validation/accuracy': 0.6567060351371765, 'validation/loss': 1.6517740488052368, 'validation/bleu': 27.850444978291744, 'validation/num_examples': 3000, 'test/accuracy': 0.6647725701332092, 'test/loss': 1.5881857872009277, 'test/bleu': 27.320551583794046, 'test/num_examples': 3003, 'score': 27753.546751499176, 'total_duration': 46657.269523620605, 'accumulated_submission_time': 27753.546751499176, 'accumulated_eval_time': 18900.159744501114, 'accumulated_logging_time': 1.0552592277526855}
I0209 16:46:50.335506 140529887049472 logging_writer.py:48] [78680] accumulated_eval_time=18900.159745, accumulated_logging_time=1.055259, accumulated_submission_time=27753.546751, global_step=78680, preemption_count=0, score=27753.546751, test/accuracy=0.664773, test/bleu=27.320552, test/loss=1.588186, test/num_examples=3003, total_duration=46657.269524, train/accuracy=0.635181, train/bleu=30.516274, train/loss=1.806400, validation/accuracy=0.656706, validation/bleu=27.850445, validation/loss=1.651774, validation/num_examples=3000
I0209 16:46:57.720392 140529895442176 logging_writer.py:48] [78700] global_step=78700, grad_norm=0.3619782328605652, loss=1.8134841918945312
I0209 16:47:32.819338 140529887049472 logging_writer.py:48] [78800] global_step=78800, grad_norm=0.5095081925392151, loss=1.8556689023971558
I0209 16:48:07.980075 140529895442176 logging_writer.py:48] [78900] global_step=78900, grad_norm=0.3342166841030121, loss=1.8221588134765625
I0209 16:48:43.184104 140529887049472 logging_writer.py:48] [79000] global_step=79000, grad_norm=0.3199886083602905, loss=1.8105779886245728
I0209 16:49:18.412089 140529895442176 logging_writer.py:48] [79100] global_step=79100, grad_norm=0.2692870497703552, loss=1.7386261224746704
I0209 16:49:53.636841 140529887049472 logging_writer.py:48] [79200] global_step=79200, grad_norm=0.2994283139705658, loss=1.843508243560791
I0209 16:50:28.892738 140529895442176 logging_writer.py:48] [79300] global_step=79300, grad_norm=0.2741090953350067, loss=1.7923567295074463
I0209 16:51:04.127472 140529887049472 logging_writer.py:48] [79400] global_step=79400, grad_norm=0.5493618845939636, loss=1.8030247688293457
I0209 16:51:39.412065 140529895442176 logging_writer.py:48] [79500] global_step=79500, grad_norm=0.30784696340560913, loss=1.7648922204971313
I0209 16:52:14.742390 140529887049472 logging_writer.py:48] [79600] global_step=79600, grad_norm=0.46682676672935486, loss=1.8688288927078247
I0209 16:52:49.986224 140529895442176 logging_writer.py:48] [79700] global_step=79700, grad_norm=0.3030584454536438, loss=1.8020977973937988
I0209 16:53:25.203715 140529887049472 logging_writer.py:48] [79800] global_step=79800, grad_norm=0.2985605001449585, loss=1.809736967086792
I0209 16:54:00.444437 140529895442176 logging_writer.py:48] [79900] global_step=79900, grad_norm=0.5181358456611633, loss=1.8345061540603638
I0209 16:54:35.676738 140529887049472 logging_writer.py:48] [80000] global_step=80000, grad_norm=0.4245939254760742, loss=1.854588508605957
I0209 16:55:10.935225 140529895442176 logging_writer.py:48] [80100] global_step=80100, grad_norm=0.33266159892082214, loss=1.7723280191421509
I0209 16:55:46.178109 140529887049472 logging_writer.py:48] [80200] global_step=80200, grad_norm=0.3200911581516266, loss=1.8322131633758545
I0209 16:56:21.415866 140529895442176 logging_writer.py:48] [80300] global_step=80300, grad_norm=0.3011108934879303, loss=1.8534801006317139
I0209 16:56:56.648216 140529887049472 logging_writer.py:48] [80400] global_step=80400, grad_norm=0.2958466410636902, loss=1.7580947875976562
I0209 16:57:31.878514 140529895442176 logging_writer.py:48] [80500] global_step=80500, grad_norm=0.32938966155052185, loss=1.9283900260925293
I0209 16:58:07.116010 140529887049472 logging_writer.py:48] [80600] global_step=80600, grad_norm=0.309984028339386, loss=1.781509280204773
I0209 16:58:42.444663 140529895442176 logging_writer.py:48] [80700] global_step=80700, grad_norm=0.3161581754684448, loss=1.8166465759277344
I0209 16:59:17.751157 140529887049472 logging_writer.py:48] [80800] global_step=80800, grad_norm=0.31171542406082153, loss=1.7905371189117432
I0209 16:59:52.975923 140529895442176 logging_writer.py:48] [80900] global_step=80900, grad_norm=0.35047534108161926, loss=1.7160756587982178
I0209 17:00:28.232063 140529887049472 logging_writer.py:48] [81000] global_step=81000, grad_norm=0.4258972704410553, loss=1.8712679147720337
I0209 17:00:50.493869 140699726837568 spec.py:321] Evaluating on the training split.
I0209 17:00:53.518266 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 17:04:59.561390 140699726837568 spec.py:333] Evaluating on the validation split.
I0209 17:05:02.303427 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 17:07:44.600922 140699726837568 spec.py:349] Evaluating on the test split.
I0209 17:07:47.341037 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 17:10:36.042217 140699726837568 submission_runner.py:408] Time since start: 48083.01s, 	Step: 81065, 	{'train/accuracy': 0.6338257789611816, 'train/loss': 1.8154295682907104, 'train/bleu': 30.37254047659617, 'validation/accuracy': 0.657177209854126, 'validation/loss': 1.6409265995025635, 'validation/bleu': 27.84527507937404, 'validation/num_examples': 3000, 'test/accuracy': 0.6671895980834961, 'test/loss': 1.572178840637207, 'test/bleu': 27.59701393455958, 'test/num_examples': 3003, 'score': 28593.618614435196, 'total_duration': 48083.00713443756, 'accumulated_submission_time': 28593.618614435196, 'accumulated_eval_time': 19485.708025693893, 'accumulated_logging_time': 1.0978131294250488}
I0209 17:10:36.068574 140529895442176 logging_writer.py:48] [81065] accumulated_eval_time=19485.708026, accumulated_logging_time=1.097813, accumulated_submission_time=28593.618614, global_step=81065, preemption_count=0, score=28593.618614, test/accuracy=0.667190, test/bleu=27.597014, test/loss=1.572179, test/num_examples=3003, total_duration=48083.007134, train/accuracy=0.633826, train/bleu=30.372540, train/loss=1.815430, validation/accuracy=0.657177, validation/bleu=27.845275, validation/loss=1.640927, validation/num_examples=3000
I0209 17:10:48.710083 140529887049472 logging_writer.py:48] [81100] global_step=81100, grad_norm=0.32661309838294983, loss=1.829655647277832
I0209 17:11:23.861592 140529895442176 logging_writer.py:48] [81200] global_step=81200, grad_norm=0.6088317632675171, loss=1.8354789018630981
I0209 17:11:59.075637 140529887049472 logging_writer.py:48] [81300] global_step=81300, grad_norm=0.3475816249847412, loss=1.812747597694397
I0209 17:12:34.333132 140529895442176 logging_writer.py:48] [81400] global_step=81400, grad_norm=0.3050750494003296, loss=1.8003287315368652
I0209 17:13:09.574497 140529887049472 logging_writer.py:48] [81500] global_step=81500, grad_norm=0.357634961605072, loss=1.8231955766677856
I0209 17:13:44.823955 140529895442176 logging_writer.py:48] [81600] global_step=81600, grad_norm=0.3080119490623474, loss=1.7764272689819336
I0209 17:14:20.039963 140529887049472 logging_writer.py:48] [81700] global_step=81700, grad_norm=0.3403429090976715, loss=1.8055644035339355
I0209 17:14:55.282349 140529895442176 logging_writer.py:48] [81800] global_step=81800, grad_norm=0.5638217926025391, loss=1.7677100896835327
I0209 17:15:30.565424 140529887049472 logging_writer.py:48] [81900] global_step=81900, grad_norm=0.40823647379875183, loss=1.7605276107788086
I0209 17:16:05.827171 140529895442176 logging_writer.py:48] [82000] global_step=82000, grad_norm=0.4152366816997528, loss=1.8500510454177856
I0209 17:16:41.099678 140529887049472 logging_writer.py:48] [82100] global_step=82100, grad_norm=0.3615475594997406, loss=1.6966629028320312
I0209 17:17:16.374097 140529895442176 logging_writer.py:48] [82200] global_step=82200, grad_norm=0.2959851324558258, loss=1.7989234924316406
I0209 17:17:51.695490 140529887049472 logging_writer.py:48] [82300] global_step=82300, grad_norm=0.29458415508270264, loss=1.7681031227111816
I0209 17:18:26.937644 140529895442176 logging_writer.py:48] [82400] global_step=82400, grad_norm=0.35280537605285645, loss=1.8278512954711914
I0209 17:19:02.174365 140529887049472 logging_writer.py:48] [82500] global_step=82500, grad_norm=0.4144085943698883, loss=1.8068482875823975
I0209 17:19:37.417636 140529895442176 logging_writer.py:48] [82600] global_step=82600, grad_norm=0.36834850907325745, loss=1.87286376953125
I0209 17:20:12.656110 140529887049472 logging_writer.py:48] [82700] global_step=82700, grad_norm=0.29548200964927673, loss=1.8099234104156494
I0209 17:20:47.929064 140529895442176 logging_writer.py:48] [82800] global_step=82800, grad_norm=0.5379762053489685, loss=1.869783878326416
I0209 17:21:23.231053 140529887049472 logging_writer.py:48] [82900] global_step=82900, grad_norm=0.334127277135849, loss=1.7758293151855469
I0209 17:21:58.550727 140529895442176 logging_writer.py:48] [83000] global_step=83000, grad_norm=0.45786237716674805, loss=1.802518367767334
I0209 17:22:33.813380 140529887049472 logging_writer.py:48] [83100] global_step=83100, grad_norm=0.3664121925830841, loss=1.7761683464050293
I0209 17:23:09.059582 140529895442176 logging_writer.py:48] [83200] global_step=83200, grad_norm=0.28730419278144836, loss=1.7924896478652954
I0209 17:23:44.277563 140529887049472 logging_writer.py:48] [83300] global_step=83300, grad_norm=0.3353479504585266, loss=1.7711037397384644
I0209 17:24:19.540862 140529895442176 logging_writer.py:48] [83400] global_step=83400, grad_norm=0.2904755175113678, loss=1.762778401374817
I0209 17:24:36.216523 140699726837568 spec.py:321] Evaluating on the training split.
I0209 17:24:39.264991 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 17:29:10.072795 140699726837568 spec.py:333] Evaluating on the validation split.
I0209 17:29:12.803288 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 17:32:29.231284 140699726837568 spec.py:349] Evaluating on the test split.
I0209 17:32:31.980947 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 17:35:51.419983 140699726837568 submission_runner.py:408] Time since start: 49598.38s, 	Step: 83449, 	{'train/accuracy': 0.6420011520385742, 'train/loss': 1.7547723054885864, 'train/bleu': 31.104798039546385, 'validation/accuracy': 0.6573383808135986, 'validation/loss': 1.6285314559936523, 'validation/bleu': 28.031213845797343, 'validation/num_examples': 3000, 'test/accuracy': 0.6720237135887146, 'test/loss': 1.551444411277771, 'test/bleu': 28.043043785349038, 'test/num_examples': 3003, 'score': 29433.67586684227, 'total_duration': 49598.384884119034, 'accumulated_submission_time': 29433.67586684227, 'accumulated_eval_time': 20160.911412000656, 'accumulated_logging_time': 1.1356170177459717}
I0209 17:35:51.451157 140529887049472 logging_writer.py:48] [83449] accumulated_eval_time=20160.911412, accumulated_logging_time=1.135617, accumulated_submission_time=29433.675867, global_step=83449, preemption_count=0, score=29433.675867, test/accuracy=0.672024, test/bleu=28.043044, test/loss=1.551444, test/num_examples=3003, total_duration=49598.384884, train/accuracy=0.642001, train/bleu=31.104798, train/loss=1.754772, validation/accuracy=0.657338, validation/bleu=28.031214, validation/loss=1.628531, validation/num_examples=3000
I0209 17:36:09.732919 140529895442176 logging_writer.py:48] [83500] global_step=83500, grad_norm=0.6426147818565369, loss=1.710758924484253
I0209 17:36:44.913898 140529887049472 logging_writer.py:48] [83600] global_step=83600, grad_norm=0.3513908088207245, loss=1.7714768648147583
I0209 17:37:20.150447 140529895442176 logging_writer.py:48] [83700] global_step=83700, grad_norm=0.28578507900238037, loss=1.8853970766067505
I0209 17:37:55.404991 140529887049472 logging_writer.py:48] [83800] global_step=83800, grad_norm=0.2739125192165375, loss=1.7976081371307373
I0209 17:38:30.648303 140529895442176 logging_writer.py:48] [83900] global_step=83900, grad_norm=0.4103234112262726, loss=1.7886738777160645
I0209 17:39:05.871975 140529887049472 logging_writer.py:48] [84000] global_step=84000, grad_norm=0.441593199968338, loss=1.8226877450942993
I0209 17:39:41.154874 140529895442176 logging_writer.py:48] [84100] global_step=84100, grad_norm=0.4227847158908844, loss=1.7976287603378296
I0209 17:40:16.421036 140529887049472 logging_writer.py:48] [84200] global_step=84200, grad_norm=0.3714454472064972, loss=1.8112297058105469
I0209 17:40:51.689155 140529895442176 logging_writer.py:48] [84300] global_step=84300, grad_norm=0.2926136255264282, loss=1.709296703338623
I0209 17:41:26.982458 140529887049472 logging_writer.py:48] [84400] global_step=84400, grad_norm=0.3348725438117981, loss=1.8093385696411133
I0209 17:42:02.281778 140529895442176 logging_writer.py:48] [84500] global_step=84500, grad_norm=0.3412749469280243, loss=1.7386057376861572
I0209 17:42:37.497959 140529887049472 logging_writer.py:48] [84600] global_step=84600, grad_norm=0.5186025500297546, loss=1.7488518953323364
I0209 17:43:12.706034 140529895442176 logging_writer.py:48] [84700] global_step=84700, grad_norm=0.3141257166862488, loss=1.7929792404174805
I0209 17:43:47.943719 140529887049472 logging_writer.py:48] [84800] global_step=84800, grad_norm=0.3202342092990875, loss=1.7773752212524414
I0209 17:44:23.164299 140529895442176 logging_writer.py:48] [84900] global_step=84900, grad_norm=0.4691123366355896, loss=1.8216631412506104
I0209 17:44:58.423937 140529887049472 logging_writer.py:48] [85000] global_step=85000, grad_norm=0.3087327182292938, loss=1.719103455543518
I0209 17:45:33.669566 140529895442176 logging_writer.py:48] [85100] global_step=85100, grad_norm=0.367901086807251, loss=1.7695056200027466
I0209 17:46:08.924433 140529887049472 logging_writer.py:48] [85200] global_step=85200, grad_norm=0.319513201713562, loss=1.7729995250701904
I0209 17:46:44.160025 140529895442176 logging_writer.py:48] [85300] global_step=85300, grad_norm=0.29252517223358154, loss=1.7085965871810913
I0209 17:47:19.394598 140529887049472 logging_writer.py:48] [85400] global_step=85400, grad_norm=0.3874451816082001, loss=1.8450416326522827
I0209 17:47:54.616851 140529895442176 logging_writer.py:48] [85500] global_step=85500, grad_norm=0.31097325682640076, loss=1.7276594638824463
I0209 17:48:29.913786 140529887049472 logging_writer.py:48] [85600] global_step=85600, grad_norm=0.3075166642665863, loss=1.7477293014526367
I0209 17:49:05.140024 140529895442176 logging_writer.py:48] [85700] global_step=85700, grad_norm=0.39357051253318787, loss=1.8398631811141968
I0209 17:49:40.360699 140529887049472 logging_writer.py:48] [85800] global_step=85800, grad_norm=0.2831524610519409, loss=1.7522424459457397
I0209 17:49:51.706564 140699726837568 spec.py:321] Evaluating on the training split.
I0209 17:49:54.730968 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 17:54:09.160529 140699726837568 spec.py:333] Evaluating on the validation split.
I0209 17:54:11.894213 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 17:56:56.581271 140699726837568 spec.py:349] Evaluating on the test split.
I0209 17:56:59.316349 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 17:59:36.812512 140699726837568 submission_runner.py:408] Time since start: 51023.78s, 	Step: 85834, 	{'train/accuracy': 0.6379316449165344, 'train/loss': 1.7899552583694458, 'train/bleu': 31.381088323768665, 'validation/accuracy': 0.6602894067764282, 'validation/loss': 1.6165797710418701, 'validation/bleu': 27.88452353796739, 'validation/num_examples': 3000, 'test/accuracy': 0.673267126083374, 'test/loss': 1.5379685163497925, 'test/bleu': 28.295872994291173, 'test/num_examples': 3003, 'score': 30273.843912363052, 'total_duration': 51023.77744102478, 'accumulated_submission_time': 30273.843912363052, 'accumulated_eval_time': 20746.017300128937, 'accumulated_logging_time': 1.178145170211792}
I0209 17:59:36.841078 140529895442176 logging_writer.py:48] [85834] accumulated_eval_time=20746.017300, accumulated_logging_time=1.178145, accumulated_submission_time=30273.843912, global_step=85834, preemption_count=0, score=30273.843912, test/accuracy=0.673267, test/bleu=28.295873, test/loss=1.537969, test/num_examples=3003, total_duration=51023.777441, train/accuracy=0.637932, train/bleu=31.381088, train/loss=1.789955, validation/accuracy=0.660289, validation/bleu=27.884524, validation/loss=1.616580, validation/num_examples=3000
I0209 18:00:00.338598 140529887049472 logging_writer.py:48] [85900] global_step=85900, grad_norm=0.3285547196865082, loss=1.71403169631958
I0209 18:00:35.466160 140529895442176 logging_writer.py:48] [86000] global_step=86000, grad_norm=0.44930046796798706, loss=1.7774128913879395
I0209 18:01:10.667026 140529887049472 logging_writer.py:48] [86100] global_step=86100, grad_norm=0.373243123292923, loss=1.7018858194351196
I0209 18:01:45.878533 140529895442176 logging_writer.py:48] [86200] global_step=86200, grad_norm=0.37465691566467285, loss=1.7382094860076904
I0209 18:02:21.132590 140529887049472 logging_writer.py:48] [86300] global_step=86300, grad_norm=0.40831243991851807, loss=1.7673707008361816
I0209 18:02:56.332686 140529895442176 logging_writer.py:48] [86400] global_step=86400, grad_norm=0.4341474771499634, loss=1.7819693088531494
I0209 18:03:31.555459 140529887049472 logging_writer.py:48] [86500] global_step=86500, grad_norm=0.38486701250076294, loss=1.812482476234436
I0209 18:04:06.795554 140529895442176 logging_writer.py:48] [86600] global_step=86600, grad_norm=0.3064804673194885, loss=1.8001495599746704
I0209 18:04:42.044465 140529887049472 logging_writer.py:48] [86700] global_step=86700, grad_norm=0.318889856338501, loss=1.7536604404449463
I0209 18:05:17.304127 140529895442176 logging_writer.py:48] [86800] global_step=86800, grad_norm=0.3065125048160553, loss=1.7207753658294678
I0209 18:05:52.540364 140529887049472 logging_writer.py:48] [86900] global_step=86900, grad_norm=0.3093237280845642, loss=1.7686131000518799
I0209 18:06:27.787087 140529895442176 logging_writer.py:48] [87000] global_step=87000, grad_norm=0.3376319706439972, loss=1.6743470430374146
I0209 18:07:03.026836 140529887049472 logging_writer.py:48] [87100] global_step=87100, grad_norm=0.5443435907363892, loss=1.6347790956497192
I0209 18:07:38.258986 140529895442176 logging_writer.py:48] [87200] global_step=87200, grad_norm=0.38787052035331726, loss=1.7318400144577026
I0209 18:08:13.525547 140529887049472 logging_writer.py:48] [87300] global_step=87300, grad_norm=0.3565060496330261, loss=1.7053613662719727
I0209 18:08:48.761497 140529895442176 logging_writer.py:48] [87400] global_step=87400, grad_norm=0.5092810392379761, loss=1.8233017921447754
I0209 18:09:23.993101 140529887049472 logging_writer.py:48] [87500] global_step=87500, grad_norm=0.367257684469223, loss=1.6935136318206787
I0209 18:09:59.217038 140529895442176 logging_writer.py:48] [87600] global_step=87600, grad_norm=0.27041804790496826, loss=1.7143408060073853
I0209 18:10:34.450497 140529887049472 logging_writer.py:48] [87700] global_step=87700, grad_norm=0.44251304864883423, loss=1.7099709510803223
I0209 18:11:09.677817 140529895442176 logging_writer.py:48] [87800] global_step=87800, grad_norm=0.3439062237739563, loss=1.776426076889038
I0209 18:11:44.956537 140529887049472 logging_writer.py:48] [87900] global_step=87900, grad_norm=0.29516130685806274, loss=1.7402669191360474
I0209 18:12:20.210855 140529895442176 logging_writer.py:48] [88000] global_step=88000, grad_norm=0.36260145902633667, loss=1.7145661115646362
I0209 18:12:55.443872 140529887049472 logging_writer.py:48] [88100] global_step=88100, grad_norm=0.3740368187427521, loss=1.743901014328003
I0209 18:13:30.670130 140529895442176 logging_writer.py:48] [88200] global_step=88200, grad_norm=0.3390081822872162, loss=1.8340367078781128
I0209 18:13:37.100902 140699726837568 spec.py:321] Evaluating on the training split.
I0209 18:13:40.133293 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 18:16:37.962325 140699726837568 spec.py:333] Evaluating on the validation split.
I0209 18:16:40.691617 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 18:19:07.177413 140699726837568 spec.py:349] Evaluating on the test split.
I0209 18:19:09.921718 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 18:21:27.601366 140699726837568 submission_runner.py:408] Time since start: 52334.57s, 	Step: 88220, 	{'train/accuracy': 0.6552841663360596, 'train/loss': 1.6534067392349243, 'train/bleu': 32.29373394632669, 'validation/accuracy': 0.6635503768920898, 'validation/loss': 1.599921703338623, 'validation/bleu': 28.25207895536669, 'validation/num_examples': 3000, 'test/accuracy': 0.6726628541946411, 'test/loss': 1.5305873155593872, 'test/bleu': 28.065620186470788, 'test/num_examples': 3003, 'score': 31114.019277334213, 'total_duration': 52334.566292762756, 'accumulated_submission_time': 31114.019277334213, 'accumulated_eval_time': 21216.517703533173, 'accumulated_logging_time': 1.216465711593628}
I0209 18:21:27.628385 140529887049472 logging_writer.py:48] [88220] accumulated_eval_time=21216.517704, accumulated_logging_time=1.216466, accumulated_submission_time=31114.019277, global_step=88220, preemption_count=0, score=31114.019277, test/accuracy=0.672663, test/bleu=28.065620, test/loss=1.530587, test/num_examples=3003, total_duration=52334.566293, train/accuracy=0.655284, train/bleu=32.293734, train/loss=1.653407, validation/accuracy=0.663550, validation/bleu=28.252079, validation/loss=1.599922, validation/num_examples=3000
I0209 18:21:56.090525 140529895442176 logging_writer.py:48] [88300] global_step=88300, grad_norm=0.3447926938533783, loss=1.8341565132141113
I0209 18:22:31.253340 140529887049472 logging_writer.py:48] [88400] global_step=88400, grad_norm=0.312150239944458, loss=1.757694125175476
I0209 18:23:06.446297 140529895442176 logging_writer.py:48] [88500] global_step=88500, grad_norm=0.3039705455303192, loss=1.7036137580871582
I0209 18:23:41.652670 140529887049472 logging_writer.py:48] [88600] global_step=88600, grad_norm=0.3549247682094574, loss=1.7689495086669922
I0209 18:24:16.901154 140529895442176 logging_writer.py:48] [88700] global_step=88700, grad_norm=0.31012341380119324, loss=1.7719601392745972
I0209 18:24:52.154145 140529887049472 logging_writer.py:48] [88800] global_step=88800, grad_norm=0.3577345311641693, loss=1.7943401336669922
I0209 18:25:27.498633 140529895442176 logging_writer.py:48] [88900] global_step=88900, grad_norm=0.3144370913505554, loss=1.759752631187439
I0209 18:26:02.783408 140529887049472 logging_writer.py:48] [89000] global_step=89000, grad_norm=0.40818268060684204, loss=1.7655766010284424
I0209 18:26:38.120114 140529895442176 logging_writer.py:48] [89100] global_step=89100, grad_norm=0.3129724860191345, loss=1.7445203065872192
I0209 18:27:13.354300 140529887049472 logging_writer.py:48] [89200] global_step=89200, grad_norm=0.40495535731315613, loss=1.7578517198562622
I0209 18:27:48.623184 140529895442176 logging_writer.py:48] [89300] global_step=89300, grad_norm=0.29452618956565857, loss=1.685646891593933
I0209 18:28:23.943105 140529887049472 logging_writer.py:48] [89400] global_step=89400, grad_norm=0.3237181007862091, loss=1.8305442333221436
I0209 18:28:59.223537 140529895442176 logging_writer.py:48] [89500] global_step=89500, grad_norm=0.30804041028022766, loss=1.7096387147903442
I0209 18:29:34.500060 140529887049472 logging_writer.py:48] [89600] global_step=89600, grad_norm=0.31728067994117737, loss=1.6686804294586182
I0209 18:30:09.769349 140529895442176 logging_writer.py:48] [89700] global_step=89700, grad_norm=0.3495984673500061, loss=1.6908657550811768
I0209 18:30:45.002582 140529887049472 logging_writer.py:48] [89800] global_step=89800, grad_norm=0.392650842666626, loss=1.6619791984558105
I0209 18:31:20.245829 140529895442176 logging_writer.py:48] [89900] global_step=89900, grad_norm=0.4252196252346039, loss=1.6803513765335083
I0209 18:31:55.494009 140529887049472 logging_writer.py:48] [90000] global_step=90000, grad_norm=0.34576094150543213, loss=1.682006597518921
I0209 18:32:30.744928 140529895442176 logging_writer.py:48] [90100] global_step=90100, grad_norm=0.3316023051738739, loss=1.7576018571853638
I0209 18:33:06.040347 140529887049472 logging_writer.py:48] [90200] global_step=90200, grad_norm=0.31361520290374756, loss=1.7840853929519653
I0209 18:33:41.297382 140529895442176 logging_writer.py:48] [90300] global_step=90300, grad_norm=0.3673558235168457, loss=1.7196329832077026
I0209 18:34:16.560764 140529887049472 logging_writer.py:48] [90400] global_step=90400, grad_norm=0.33698996901512146, loss=1.698198676109314
I0209 18:34:51.877124 140529895442176 logging_writer.py:48] [90500] global_step=90500, grad_norm=0.3567381501197815, loss=1.7378990650177002
I0209 18:35:27.192706 140529887049472 logging_writer.py:48] [90600] global_step=90600, grad_norm=0.3210265636444092, loss=1.7525066137313843
I0209 18:35:27.628603 140699726837568 spec.py:321] Evaluating on the training split.
I0209 18:35:30.667163 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 18:39:07.312412 140699726837568 spec.py:333] Evaluating on the validation split.
I0209 18:39:10.053162 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 18:41:57.236290 140699726837568 spec.py:349] Evaluating on the test split.
I0209 18:41:59.992248 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 18:44:53.940514 140699726837568 submission_runner.py:408] Time since start: 53740.91s, 	Step: 90603, 	{'train/accuracy': 0.6459221243858337, 'train/loss': 1.7254306077957153, 'train/bleu': 31.6283829980775, 'validation/accuracy': 0.6639595031738281, 'validation/loss': 1.5904563665390015, 'validation/bleu': 28.325600095389117, 'validation/num_examples': 3000, 'test/accuracy': 0.6796583533287048, 'test/loss': 1.5105689764022827, 'test/bleu': 28.84191883449241, 'test/num_examples': 3003, 'score': 31953.927941083908, 'total_duration': 53740.905447244644, 'accumulated_submission_time': 31953.927941083908, 'accumulated_eval_time': 21782.829572200775, 'accumulated_logging_time': 1.2534148693084717}
I0209 18:44:53.968109 140529895442176 logging_writer.py:48] [90603] accumulated_eval_time=21782.829572, accumulated_logging_time=1.253415, accumulated_submission_time=31953.927941, global_step=90603, preemption_count=0, score=31953.927941, test/accuracy=0.679658, test/bleu=28.841919, test/loss=1.510569, test/num_examples=3003, total_duration=53740.905447, train/accuracy=0.645922, train/bleu=31.628383, train/loss=1.725431, validation/accuracy=0.663960, validation/bleu=28.325600, validation/loss=1.590456, validation/num_examples=3000
I0209 18:45:28.358992 140529887049472 logging_writer.py:48] [90700] global_step=90700, grad_norm=0.30086132884025574, loss=1.7774561643600464
I0209 18:46:03.490694 140529895442176 logging_writer.py:48] [90800] global_step=90800, grad_norm=0.35645928978919983, loss=1.8214119672775269
I0209 18:46:38.666688 140529887049472 logging_writer.py:48] [90900] global_step=90900, grad_norm=0.30670231580734253, loss=1.7301448583602905
I0209 18:47:13.850689 140529895442176 logging_writer.py:48] [91000] global_step=91000, grad_norm=0.4205164313316345, loss=1.6950467824935913
I0209 18:47:49.051285 140529887049472 logging_writer.py:48] [91100] global_step=91100, grad_norm=0.2918284833431244, loss=1.7419980764389038
I0209 18:48:24.300541 140529895442176 logging_writer.py:48] [91200] global_step=91200, grad_norm=0.31903138756752014, loss=1.67304527759552
I0209 18:48:59.510742 140529887049472 logging_writer.py:48] [91300] global_step=91300, grad_norm=0.27951955795288086, loss=1.650254487991333
I0209 18:49:34.775652 140529895442176 logging_writer.py:48] [91400] global_step=91400, grad_norm=0.29558616876602173, loss=1.758756160736084
I0209 18:50:10.010851 140529887049472 logging_writer.py:48] [91500] global_step=91500, grad_norm=0.7435519695281982, loss=1.7305324077606201
I0209 18:50:45.219630 140529895442176 logging_writer.py:48] [91600] global_step=91600, grad_norm=0.3023841679096222, loss=1.7227222919464111
I0209 18:51:20.455471 140529887049472 logging_writer.py:48] [91700] global_step=91700, grad_norm=0.5091461539268494, loss=1.6882532835006714
I0209 18:51:55.674789 140529895442176 logging_writer.py:48] [91800] global_step=91800, grad_norm=0.6432868242263794, loss=1.6797270774841309
I0209 18:52:30.903026 140529887049472 logging_writer.py:48] [91900] global_step=91900, grad_norm=0.3064807653427124, loss=1.7187598943710327
I0209 18:53:06.141058 140529895442176 logging_writer.py:48] [92000] global_step=92000, grad_norm=0.32199323177337646, loss=1.7848801612854004
I0209 18:53:41.400478 140529887049472 logging_writer.py:48] [92100] global_step=92100, grad_norm=0.32211753726005554, loss=1.7992664575576782
I0209 18:54:16.619769 140529895442176 logging_writer.py:48] [92200] global_step=92200, grad_norm=0.3077646493911743, loss=1.6702136993408203
I0209 18:54:51.895873 140529887049472 logging_writer.py:48] [92300] global_step=92300, grad_norm=0.4245440363883972, loss=1.7101860046386719
I0209 18:55:27.137470 140529895442176 logging_writer.py:48] [92400] global_step=92400, grad_norm=0.3182993531227112, loss=1.6755496263504028
I0209 18:56:02.380484 140529887049472 logging_writer.py:48] [92500] global_step=92500, grad_norm=0.416559636592865, loss=1.7702972888946533
I0209 18:56:37.602092 140529895442176 logging_writer.py:48] [92600] global_step=92600, grad_norm=0.2850291430950165, loss=1.83262300491333
I0209 18:57:12.878864 140529887049472 logging_writer.py:48] [92700] global_step=92700, grad_norm=0.46269863843917847, loss=1.6774089336395264
I0209 18:57:48.135466 140529895442176 logging_writer.py:48] [92800] global_step=92800, grad_norm=0.3434080481529236, loss=1.7445005178451538
I0209 18:58:23.408481 140529887049472 logging_writer.py:48] [92900] global_step=92900, grad_norm=0.3608061671257019, loss=1.6991384029388428
I0209 18:58:54.127962 140699726837568 spec.py:321] Evaluating on the training split.
I0209 18:58:57.160734 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 19:02:12.761367 140699726837568 spec.py:333] Evaluating on the validation split.
I0209 19:02:15.502244 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 19:04:51.933085 140699726837568 spec.py:349] Evaluating on the test split.
I0209 19:04:54.684535 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 19:07:27.359642 140699726837568 submission_runner.py:408] Time since start: 55094.32s, 	Step: 92989, 	{'train/accuracy': 0.6447259783744812, 'train/loss': 1.7305859327316284, 'train/bleu': 31.694070491390008, 'validation/accuracy': 0.6674808859825134, 'validation/loss': 1.5697031021118164, 'validation/bleu': 28.977459287447072, 'validation/num_examples': 3000, 'test/accuracy': 0.6814130544662476, 'test/loss': 1.4934310913085938, 'test/bleu': 28.439179032853332, 'test/num_examples': 3003, 'score': 32794.00373888016, 'total_duration': 55094.324538469315, 'accumulated_submission_time': 32794.00373888016, 'accumulated_eval_time': 22296.06116771698, 'accumulated_logging_time': 1.2910587787628174}
I0209 19:07:27.393848 140529895442176 logging_writer.py:48] [92989] accumulated_eval_time=22296.061168, accumulated_logging_time=1.291059, accumulated_submission_time=32794.003739, global_step=92989, preemption_count=0, score=32794.003739, test/accuracy=0.681413, test/bleu=28.439179, test/loss=1.493431, test/num_examples=3003, total_duration=55094.324538, train/accuracy=0.644726, train/bleu=31.694070, train/loss=1.730586, validation/accuracy=0.667481, validation/bleu=28.977459, validation/loss=1.569703, validation/num_examples=3000
I0209 19:07:31.632067 140529887049472 logging_writer.py:48] [93000] global_step=93000, grad_norm=0.3288654685020447, loss=1.6646008491516113
I0209 19:08:06.841976 140529895442176 logging_writer.py:48] [93100] global_step=93100, grad_norm=0.5031460523605347, loss=1.639038324356079
I0209 19:08:42.003739 140529887049472 logging_writer.py:48] [93200] global_step=93200, grad_norm=0.31398940086364746, loss=1.7395519018173218
I0209 19:09:17.232159 140529895442176 logging_writer.py:48] [93300] global_step=93300, grad_norm=0.34048718214035034, loss=1.7415759563446045
I0209 19:09:52.472444 140529887049472 logging_writer.py:48] [93400] global_step=93400, grad_norm=0.34672701358795166, loss=1.756416916847229
I0209 19:10:27.678036 140529895442176 logging_writer.py:48] [93500] global_step=93500, grad_norm=0.3080488443374634, loss=1.7310247421264648
I0209 19:11:02.882606 140529887049472 logging_writer.py:48] [93600] global_step=93600, grad_norm=0.3281400799751282, loss=1.7325191497802734
I0209 19:11:38.116184 140529895442176 logging_writer.py:48] [93700] global_step=93700, grad_norm=0.33968812227249146, loss=1.7998690605163574
I0209 19:12:13.336173 140529887049472 logging_writer.py:48] [93800] global_step=93800, grad_norm=0.32912370562553406, loss=1.8378758430480957
I0209 19:12:48.553831 140529895442176 logging_writer.py:48] [93900] global_step=93900, grad_norm=0.3076103925704956, loss=1.6645400524139404
I0209 19:13:23.777994 140529887049472 logging_writer.py:48] [94000] global_step=94000, grad_norm=0.34500226378440857, loss=1.734410285949707
I0209 19:13:59.073316 140529895442176 logging_writer.py:48] [94100] global_step=94100, grad_norm=0.2943527400493622, loss=1.763372778892517
I0209 19:14:34.339416 140529887049472 logging_writer.py:48] [94200] global_step=94200, grad_norm=0.3045404851436615, loss=1.6820396184921265
I0209 19:15:09.579113 140529895442176 logging_writer.py:48] [94300] global_step=94300, grad_norm=0.29792478680610657, loss=1.6649675369262695
I0209 19:15:44.851433 140529887049472 logging_writer.py:48] [94400] global_step=94400, grad_norm=0.4317088723182678, loss=1.7823643684387207
I0209 19:16:20.090295 140529895442176 logging_writer.py:48] [94500] global_step=94500, grad_norm=0.376645028591156, loss=1.695757508277893
I0209 19:16:55.314371 140529887049472 logging_writer.py:48] [94600] global_step=94600, grad_norm=0.3410907983779907, loss=1.6926082372665405
I0209 19:17:30.533339 140529895442176 logging_writer.py:48] [94700] global_step=94700, grad_norm=0.35266879200935364, loss=1.7705000638961792
I0209 19:18:05.733691 140529887049472 logging_writer.py:48] [94800] global_step=94800, grad_norm=0.3199600279331207, loss=1.790289282798767
I0209 19:18:40.959361 140529895442176 logging_writer.py:48] [94900] global_step=94900, grad_norm=0.3431749939918518, loss=1.755389928817749
I0209 19:19:16.183952 140529887049472 logging_writer.py:48] [95000] global_step=95000, grad_norm=0.318790465593338, loss=1.701716423034668
I0209 19:19:51.448966 140529895442176 logging_writer.py:48] [95100] global_step=95100, grad_norm=0.3373125195503235, loss=1.5919252634048462
I0209 19:20:26.679515 140529887049472 logging_writer.py:48] [95200] global_step=95200, grad_norm=0.3187846839427948, loss=1.7774791717529297
I0209 19:21:01.921334 140529895442176 logging_writer.py:48] [95300] global_step=95300, grad_norm=0.3359891474246979, loss=1.725489616394043
I0209 19:21:27.688992 140699726837568 spec.py:321] Evaluating on the training split.
I0209 19:21:30.740988 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 19:25:25.437486 140699726837568 spec.py:333] Evaluating on the validation split.
I0209 19:25:28.193410 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 19:28:21.051750 140699726837568 spec.py:349] Evaluating on the test split.
I0209 19:28:23.802826 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 19:31:13.865179 140699726837568 submission_runner.py:408] Time since start: 56520.83s, 	Step: 95375, 	{'train/accuracy': 0.6559216380119324, 'train/loss': 1.6479884386062622, 'train/bleu': 32.57261780221355, 'validation/accuracy': 0.6692167520523071, 'validation/loss': 1.5522282123565674, 'validation/bleu': 28.733379976232605, 'validation/num_examples': 3000, 'test/accuracy': 0.6810063719749451, 'test/loss': 1.4797627925872803, 'test/bleu': 28.512990721724428, 'test/num_examples': 3003, 'score': 33634.21266889572, 'total_duration': 56520.830107450485, 'accumulated_submission_time': 33634.21266889572, 'accumulated_eval_time': 22882.237318992615, 'accumulated_logging_time': 1.3375027179718018}
I0209 19:31:13.894826 140529887049472 logging_writer.py:48] [95375] accumulated_eval_time=22882.237319, accumulated_logging_time=1.337503, accumulated_submission_time=33634.212669, global_step=95375, preemption_count=0, score=33634.212669, test/accuracy=0.681006, test/bleu=28.512991, test/loss=1.479763, test/num_examples=3003, total_duration=56520.830107, train/accuracy=0.655922, train/bleu=32.572618, train/loss=1.647988, validation/accuracy=0.669217, validation/bleu=28.733380, validation/loss=1.552228, validation/num_examples=3000
I0209 19:31:23.028457 140529895442176 logging_writer.py:48] [95400] global_step=95400, grad_norm=0.32707491517066956, loss=1.7079386711120605
I0209 19:31:58.153404 140529887049472 logging_writer.py:48] [95500] global_step=95500, grad_norm=0.3463207185268402, loss=1.7198816537857056
I0209 19:32:33.302398 140529895442176 logging_writer.py:48] [95600] global_step=95600, grad_norm=0.2947438955307007, loss=1.6366565227508545
I0209 19:33:08.551671 140529887049472 logging_writer.py:48] [95700] global_step=95700, grad_norm=0.3606767952442169, loss=1.710160732269287
I0209 19:33:43.805623 140529895442176 logging_writer.py:48] [95800] global_step=95800, grad_norm=0.3609147369861603, loss=1.6963624954223633
I0209 19:34:19.061162 140529887049472 logging_writer.py:48] [95900] global_step=95900, grad_norm=0.3402165472507477, loss=1.6815403699874878
I0209 19:34:54.339220 140529895442176 logging_writer.py:48] [96000] global_step=96000, grad_norm=0.30524203181266785, loss=1.7555934190750122
I0209 19:35:29.564433 140529887049472 logging_writer.py:48] [96100] global_step=96100, grad_norm=0.3215268552303314, loss=1.6723133325576782
I0209 19:36:04.781606 140529895442176 logging_writer.py:48] [96200] global_step=96200, grad_norm=0.31554627418518066, loss=1.7179172039031982
I0209 19:36:40.038880 140529887049472 logging_writer.py:48] [96300] global_step=96300, grad_norm=0.36708733439445496, loss=1.6676537990570068
I0209 19:37:15.310214 140529895442176 logging_writer.py:48] [96400] global_step=96400, grad_norm=0.6544522643089294, loss=1.7703394889831543
I0209 19:37:50.540927 140529887049472 logging_writer.py:48] [96500] global_step=96500, grad_norm=0.30409443378448486, loss=1.6325291395187378
I0209 19:38:25.776397 140529895442176 logging_writer.py:48] [96600] global_step=96600, grad_norm=0.35649269819259644, loss=1.7143285274505615
I0209 19:39:01.006577 140529887049472 logging_writer.py:48] [96700] global_step=96700, grad_norm=0.3235386610031128, loss=1.7715988159179688
I0209 19:39:36.242849 140529895442176 logging_writer.py:48] [96800] global_step=96800, grad_norm=0.30670467019081116, loss=1.7126984596252441
I0209 19:40:11.464472 140529887049472 logging_writer.py:48] [96900] global_step=96900, grad_norm=0.3261908292770386, loss=1.6891627311706543
I0209 19:40:46.681829 140529895442176 logging_writer.py:48] [97000] global_step=97000, grad_norm=0.34622347354888916, loss=1.7554131746292114
I0209 19:41:21.915435 140529887049472 logging_writer.py:48] [97100] global_step=97100, grad_norm=0.28841346502304077, loss=1.6422837972640991
I0209 19:41:57.289819 140529895442176 logging_writer.py:48] [97200] global_step=97200, grad_norm=0.37041082978248596, loss=1.7438644170761108
I0209 19:42:32.541097 140529887049472 logging_writer.py:48] [97300] global_step=97300, grad_norm=0.2952364683151245, loss=1.7206510305404663
I0209 19:43:07.806009 140529895442176 logging_writer.py:48] [97400] global_step=97400, grad_norm=0.3576137125492096, loss=1.6499162912368774
I0209 19:43:43.024185 140529887049472 logging_writer.py:48] [97500] global_step=97500, grad_norm=0.32784339785575867, loss=1.6208807229995728
I0209 19:44:18.253823 140529895442176 logging_writer.py:48] [97600] global_step=97600, grad_norm=0.3628845810890198, loss=1.6484957933425903
I0209 19:44:53.458737 140529887049472 logging_writer.py:48] [97700] global_step=97700, grad_norm=0.44528624415397644, loss=1.7603076696395874
I0209 19:45:13.953542 140699726837568 spec.py:321] Evaluating on the training split.
I0209 19:45:16.987714 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 19:48:51.205936 140699726837568 spec.py:333] Evaluating on the validation split.
I0209 19:48:53.934438 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 19:51:41.912052 140699726837568 spec.py:349] Evaluating on the test split.
I0209 19:51:44.646136 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 19:54:22.095445 140699726837568 submission_runner.py:408] Time since start: 57909.06s, 	Step: 97760, 	{'train/accuracy': 0.6507320404052734, 'train/loss': 1.6990152597427368, 'train/bleu': 32.16376730049248, 'validation/accuracy': 0.6716469526290894, 'validation/loss': 1.5430229902267456, 'validation/bleu': 28.933022568504928, 'validation/num_examples': 3000, 'test/accuracy': 0.6845040917396545, 'test/loss': 1.4635298252105713, 'test/bleu': 28.750973036590516, 'test/num_examples': 3003, 'score': 34474.18307328224, 'total_duration': 57909.06037116051, 'accumulated_submission_time': 34474.18307328224, 'accumulated_eval_time': 23430.37916445732, 'accumulated_logging_time': 1.3787147998809814}
I0209 19:54:22.124427 140529895442176 logging_writer.py:48] [97760] accumulated_eval_time=23430.379164, accumulated_logging_time=1.378715, accumulated_submission_time=34474.183073, global_step=97760, preemption_count=0, score=34474.183073, test/accuracy=0.684504, test/bleu=28.750973, test/loss=1.463530, test/num_examples=3003, total_duration=57909.060371, train/accuracy=0.650732, train/bleu=32.163767, train/loss=1.699015, validation/accuracy=0.671647, validation/bleu=28.933023, validation/loss=1.543023, validation/num_examples=3000
I0209 19:54:36.517039 140529887049472 logging_writer.py:48] [97800] global_step=97800, grad_norm=0.3404366672039032, loss=1.6428898572921753
I0209 19:55:11.651697 140529895442176 logging_writer.py:48] [97900] global_step=97900, grad_norm=0.5111045241355896, loss=1.6191902160644531
I0209 19:55:46.834590 140529887049472 logging_writer.py:48] [98000] global_step=98000, grad_norm=0.33451107144355774, loss=1.7329691648483276
I0209 19:56:22.053819 140529895442176 logging_writer.py:48] [98100] global_step=98100, grad_norm=0.31005698442459106, loss=1.719022512435913
I0209 19:56:57.301126 140529887049472 logging_writer.py:48] [98200] global_step=98200, grad_norm=0.322270005941391, loss=1.6038053035736084
I0209 19:57:32.513112 140529895442176 logging_writer.py:48] [98300] global_step=98300, grad_norm=0.3238154947757721, loss=1.737251877784729
I0209 19:58:07.750455 140529887049472 logging_writer.py:48] [98400] global_step=98400, grad_norm=0.327857106924057, loss=1.740554928779602
I0209 19:58:42.996505 140529895442176 logging_writer.py:48] [98500] global_step=98500, grad_norm=0.3920472264289856, loss=1.674902081489563
I0209 19:59:18.242038 140529887049472 logging_writer.py:48] [98600] global_step=98600, grad_norm=0.32462990283966064, loss=1.5754610300064087
I0209 19:59:53.523030 140529895442176 logging_writer.py:48] [98700] global_step=98700, grad_norm=0.35338038206100464, loss=1.6483293771743774
I0209 20:00:28.760324 140529887049472 logging_writer.py:48] [98800] global_step=98800, grad_norm=0.3475855588912964, loss=1.723897933959961
I0209 20:01:03.993590 140529895442176 logging_writer.py:48] [98900] global_step=98900, grad_norm=0.3346443474292755, loss=1.7049949169158936
I0209 20:01:39.221196 140529887049472 logging_writer.py:48] [99000] global_step=99000, grad_norm=0.36273813247680664, loss=1.6727641820907593
I0209 20:02:14.440074 140529895442176 logging_writer.py:48] [99100] global_step=99100, grad_norm=0.31821781396865845, loss=1.6590086221694946
I0209 20:02:49.716316 140529887049472 logging_writer.py:48] [99200] global_step=99200, grad_norm=0.3273616135120392, loss=1.625598430633545
I0209 20:03:24.934087 140529895442176 logging_writer.py:48] [99300] global_step=99300, grad_norm=0.3783756196498871, loss=1.6285593509674072
I0209 20:04:00.153981 140529887049472 logging_writer.py:48] [99400] global_step=99400, grad_norm=0.3604319095611572, loss=1.6994807720184326
I0209 20:04:35.406415 140529895442176 logging_writer.py:48] [99500] global_step=99500, grad_norm=0.33510926365852356, loss=1.6509689092636108
I0209 20:05:10.656556 140529887049472 logging_writer.py:48] [99600] global_step=99600, grad_norm=0.3427344560623169, loss=1.642947793006897
I0209 20:05:45.887155 140529895442176 logging_writer.py:48] [99700] global_step=99700, grad_norm=0.35236725211143494, loss=1.6824915409088135
I0209 20:06:21.136466 140529887049472 logging_writer.py:48] [99800] global_step=99800, grad_norm=0.41606301069259644, loss=1.6247801780700684
I0209 20:06:56.419220 140529895442176 logging_writer.py:48] [99900] global_step=99900, grad_norm=0.35162922739982605, loss=1.6306507587432861
I0209 20:07:31.716597 140529887049472 logging_writer.py:48] [100000] global_step=100000, grad_norm=0.32218286395072937, loss=1.671653151512146
I0209 20:08:06.993720 140529895442176 logging_writer.py:48] [100100] global_step=100100, grad_norm=0.3884696662425995, loss=1.7084840536117554
I0209 20:08:22.215827 140699726837568 spec.py:321] Evaluating on the training split.
I0209 20:08:25.246606 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 20:12:31.473194 140699726837568 spec.py:333] Evaluating on the validation split.
I0209 20:12:34.202475 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 20:15:36.206367 140699726837568 spec.py:349] Evaluating on the test split.
I0209 20:15:38.944903 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 20:18:33.631176 140699726837568 submission_runner.py:408] Time since start: 59360.60s, 	Step: 100145, 	{'train/accuracy': 0.6949189305305481, 'train/loss': 1.418954849243164, 'train/bleu': 35.23068643277161, 'validation/accuracy': 0.673246443271637, 'validation/loss': 1.528011441230774, 'validation/bleu': 29.01353235327636, 'validation/num_examples': 3000, 'test/accuracy': 0.6881529688835144, 'test/loss': 1.4465465545654297, 'test/bleu': 29.32560119300237, 'test/num_examples': 3003, 'score': 35314.18997120857, 'total_duration': 59360.596106767654, 'accumulated_submission_time': 35314.18997120857, 'accumulated_eval_time': 24041.794478416443, 'accumulated_logging_time': 1.417504072189331}
I0209 20:18:33.660739 140529887049472 logging_writer.py:48] [100145] accumulated_eval_time=24041.794478, accumulated_logging_time=1.417504, accumulated_submission_time=35314.189971, global_step=100145, preemption_count=0, score=35314.189971, test/accuracy=0.688153, test/bleu=29.325601, test/loss=1.446547, test/num_examples=3003, total_duration=59360.596107, train/accuracy=0.694919, train/bleu=35.230686, train/loss=1.418955, validation/accuracy=0.673246, validation/bleu=29.013532, validation/loss=1.528011, validation/num_examples=3000
I0209 20:18:53.330381 140529895442176 logging_writer.py:48] [100200] global_step=100200, grad_norm=0.3212888538837433, loss=1.6542927026748657
I0209 20:19:28.607641 140529887049472 logging_writer.py:48] [100300] global_step=100300, grad_norm=0.31757062673568726, loss=1.5692353248596191
I0209 20:20:03.882791 140529895442176 logging_writer.py:48] [100400] global_step=100400, grad_norm=0.31235143542289734, loss=1.716062068939209
I0209 20:20:39.166812 140529887049472 logging_writer.py:48] [100500] global_step=100500, grad_norm=0.325528621673584, loss=1.6906613111495972
I0209 20:21:14.365407 140529895442176 logging_writer.py:48] [100600] global_step=100600, grad_norm=0.34178826212882996, loss=1.6698572635650635
I0209 20:21:49.596910 140529887049472 logging_writer.py:48] [100700] global_step=100700, grad_norm=0.3512211740016937, loss=1.5895771980285645
I0209 20:22:24.845602 140529895442176 logging_writer.py:48] [100800] global_step=100800, grad_norm=0.3077884912490845, loss=1.5917103290557861
I0209 20:23:00.068845 140529887049472 logging_writer.py:48] [100900] global_step=100900, grad_norm=0.3549792170524597, loss=1.7474817037582397
I0209 20:23:35.335252 140529895442176 logging_writer.py:48] [101000] global_step=101000, grad_norm=0.31795448064804077, loss=1.660457730293274
I0209 20:24:10.571539 140529887049472 logging_writer.py:48] [101100] global_step=101100, grad_norm=0.32115745544433594, loss=1.601914405822754
I0209 20:24:45.803738 140529895442176 logging_writer.py:48] [101200] global_step=101200, grad_norm=0.42199334502220154, loss=1.6575185060501099
I0209 20:25:21.076593 140529887049472 logging_writer.py:48] [101300] global_step=101300, grad_norm=0.3088263273239136, loss=1.6210463047027588
I0209 20:25:56.302144 140529895442176 logging_writer.py:48] [101400] global_step=101400, grad_norm=0.3310098350048065, loss=1.6467885971069336
I0209 20:26:31.521228 140529887049472 logging_writer.py:48] [101500] global_step=101500, grad_norm=0.30550307035446167, loss=1.7046408653259277
I0209 20:27:06.755055 140529895442176 logging_writer.py:48] [101600] global_step=101600, grad_norm=0.3455987274646759, loss=1.7377557754516602
I0209 20:27:41.987489 140529887049472 logging_writer.py:48] [101700] global_step=101700, grad_norm=0.3219315707683563, loss=1.7130663394927979
I0209 20:28:17.241118 140529895442176 logging_writer.py:48] [101800] global_step=101800, grad_norm=0.4012930989265442, loss=1.723273515701294
I0209 20:28:52.448252 140529887049472 logging_writer.py:48] [101900] global_step=101900, grad_norm=0.4353562593460083, loss=1.6992945671081543
I0209 20:29:27.731480 140529895442176 logging_writer.py:48] [102000] global_step=102000, grad_norm=0.3219180107116699, loss=1.6871702671051025
I0209 20:30:02.965127 140529887049472 logging_writer.py:48] [102100] global_step=102100, grad_norm=0.3503376543521881, loss=1.5698908567428589
I0209 20:30:38.193845 140529895442176 logging_writer.py:48] [102200] global_step=102200, grad_norm=0.3134711980819702, loss=1.6657474040985107
I0209 20:31:13.421982 140529887049472 logging_writer.py:48] [102300] global_step=102300, grad_norm=0.3707418441772461, loss=1.6505967378616333
I0209 20:31:48.675545 140529895442176 logging_writer.py:48] [102400] global_step=102400, grad_norm=0.3654935657978058, loss=1.6331636905670166
I0209 20:32:23.917634 140529887049472 logging_writer.py:48] [102500] global_step=102500, grad_norm=0.3621446192264557, loss=1.644010305404663
I0209 20:32:33.866673 140699726837568 spec.py:321] Evaluating on the training split.
I0209 20:32:36.902158 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 20:36:31.558682 140699726837568 spec.py:333] Evaluating on the validation split.
I0209 20:36:34.302262 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 20:39:02.442563 140699726837568 spec.py:349] Evaluating on the test split.
I0209 20:39:05.181444 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 20:42:02.346054 140699726837568 submission_runner.py:408] Time since start: 60769.31s, 	Step: 102530, 	{'train/accuracy': 0.6625471115112305, 'train/loss': 1.6055678129196167, 'train/bleu': 32.88659371950133, 'validation/accuracy': 0.6756394505500793, 'validation/loss': 1.5099588632583618, 'validation/bleu': 29.382846968354542, 'validation/num_examples': 3000, 'test/accuracy': 0.6910231709480286, 'test/loss': 1.4268368482589722, 'test/bleu': 29.189676258834265, 'test/num_examples': 3003, 'score': 36154.3091506958, 'total_duration': 60769.31097340584, 'accumulated_submission_time': 36154.3091506958, 'accumulated_eval_time': 24610.273792028427, 'accumulated_logging_time': 1.4581010341644287}
I0209 20:42:02.380638 140529895442176 logging_writer.py:48] [102530] accumulated_eval_time=24610.273792, accumulated_logging_time=1.458101, accumulated_submission_time=36154.309151, global_step=102530, preemption_count=0, score=36154.309151, test/accuracy=0.691023, test/bleu=29.189676, test/loss=1.426837, test/num_examples=3003, total_duration=60769.310973, train/accuracy=0.662547, train/bleu=32.886594, train/loss=1.605568, validation/accuracy=0.675639, validation/bleu=29.382847, validation/loss=1.509959, validation/num_examples=3000
I0209 20:42:27.302392 140529887049472 logging_writer.py:48] [102600] global_step=102600, grad_norm=0.38744986057281494, loss=1.6817699670791626
I0209 20:43:02.406285 140529895442176 logging_writer.py:48] [102700] global_step=102700, grad_norm=0.33944621682167053, loss=1.734889268875122
I0209 20:43:37.588260 140529887049472 logging_writer.py:48] [102800] global_step=102800, grad_norm=0.35356852412223816, loss=1.6165766716003418
I0209 20:44:12.780405 140529895442176 logging_writer.py:48] [102900] global_step=102900, grad_norm=0.5381776094436646, loss=1.6181776523590088
I0209 20:44:47.979419 140529887049472 logging_writer.py:48] [103000] global_step=103000, grad_norm=0.36720284819602966, loss=1.6162053346633911
I0209 20:45:23.227640 140529895442176 logging_writer.py:48] [103100] global_step=103100, grad_norm=0.3398902416229248, loss=1.6372101306915283
I0209 20:45:58.445227 140529887049472 logging_writer.py:48] [103200] global_step=103200, grad_norm=0.3089217245578766, loss=1.6285735368728638
I0209 20:46:33.685501 140529895442176 logging_writer.py:48] [103300] global_step=103300, grad_norm=0.32376980781555176, loss=1.6456279754638672
I0209 20:47:08.922218 140529887049472 logging_writer.py:48] [103400] global_step=103400, grad_norm=0.32093557715415955, loss=1.6472238302230835
I0209 20:47:44.145428 140529895442176 logging_writer.py:48] [103500] global_step=103500, grad_norm=0.37482401728630066, loss=1.5742884874343872
I0209 20:48:19.357361 140529887049472 logging_writer.py:48] [103600] global_step=103600, grad_norm=0.3798992931842804, loss=1.5953541994094849
I0209 20:48:54.582278 140529895442176 logging_writer.py:48] [103700] global_step=103700, grad_norm=0.3348226547241211, loss=1.6599514484405518
I0209 20:49:29.862414 140529887049472 logging_writer.py:48] [103800] global_step=103800, grad_norm=0.3405594229698181, loss=1.6166207790374756
I0209 20:50:05.062346 140529895442176 logging_writer.py:48] [103900] global_step=103900, grad_norm=0.3539029657840729, loss=1.5883382558822632
I0209 20:50:40.301591 140529887049472 logging_writer.py:48] [104000] global_step=104000, grad_norm=0.3245915174484253, loss=1.6106187105178833
I0209 20:51:15.553873 140529895442176 logging_writer.py:48] [104100] global_step=104100, grad_norm=0.31331437826156616, loss=1.6115083694458008
I0209 20:51:50.784442 140529887049472 logging_writer.py:48] [104200] global_step=104200, grad_norm=0.3446260392665863, loss=1.6065499782562256
I0209 20:52:26.014778 140529895442176 logging_writer.py:48] [104300] global_step=104300, grad_norm=0.33796143531799316, loss=1.6110329627990723
I0209 20:53:01.227944 140529887049472 logging_writer.py:48] [104400] global_step=104400, grad_norm=0.344638466835022, loss=1.6395460367202759
I0209 20:53:36.479695 140529895442176 logging_writer.py:48] [104500] global_step=104500, grad_norm=0.3362143039703369, loss=1.6688107252120972
I0209 20:54:11.708453 140529887049472 logging_writer.py:48] [104600] global_step=104600, grad_norm=0.34789150953292847, loss=1.6416430473327637
I0209 20:54:46.935377 140529895442176 logging_writer.py:48] [104700] global_step=104700, grad_norm=0.3983032703399658, loss=1.6121364831924438
I0209 20:55:22.163871 140529887049472 logging_writer.py:48] [104800] global_step=104800, grad_norm=0.32609179615974426, loss=1.5732829570770264
I0209 20:55:57.398005 140529895442176 logging_writer.py:48] [104900] global_step=104900, grad_norm=0.3280254304409027, loss=1.639086365699768
I0209 20:56:02.416060 140699726837568 spec.py:321] Evaluating on the training split.
I0209 20:56:05.469995 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 21:00:07.361537 140699726837568 spec.py:333] Evaluating on the validation split.
I0209 21:00:10.106306 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 21:02:58.309075 140699726837568 spec.py:349] Evaluating on the test split.
I0209 21:03:01.052590 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 21:05:33.854750 140699726837568 submission_runner.py:408] Time since start: 62180.82s, 	Step: 104916, 	{'train/accuracy': 0.6604390144348145, 'train/loss': 1.6284128427505493, 'train/bleu': 32.50623094459274, 'validation/accuracy': 0.6783796548843384, 'validation/loss': 1.500727891921997, 'validation/bleu': 29.517135048861, 'validation/num_examples': 3000, 'test/accuracy': 0.6924060583114624, 'test/loss': 1.420128583908081, 'test/bleu': 29.2301595025155, 'test/num_examples': 3003, 'score': 36994.25947546959, 'total_duration': 62180.81967806816, 'accumulated_submission_time': 36994.25947546959, 'accumulated_eval_time': 25181.712441921234, 'accumulated_logging_time': 1.5033392906188965}
I0209 21:05:33.885459 140529887049472 logging_writer.py:48] [104916] accumulated_eval_time=25181.712442, accumulated_logging_time=1.503339, accumulated_submission_time=36994.259475, global_step=104916, preemption_count=0, score=36994.259475, test/accuracy=0.692406, test/bleu=29.230160, test/loss=1.420129, test/num_examples=3003, total_duration=62180.819678, train/accuracy=0.660439, train/bleu=32.506231, train/loss=1.628413, validation/accuracy=0.678380, validation/bleu=29.517135, validation/loss=1.500728, validation/num_examples=3000
I0209 21:06:03.739301 140529895442176 logging_writer.py:48] [105000] global_step=105000, grad_norm=0.359602153301239, loss=1.6024322509765625
I0209 21:06:38.908130 140529887049472 logging_writer.py:48] [105100] global_step=105100, grad_norm=0.3779453635215759, loss=1.5454692840576172
I0209 21:07:14.121783 140529895442176 logging_writer.py:48] [105200] global_step=105200, grad_norm=0.3727763891220093, loss=1.6709561347961426
I0209 21:07:49.349727 140529887049472 logging_writer.py:48] [105300] global_step=105300, grad_norm=0.3205127418041229, loss=1.6213252544403076
I0209 21:08:24.617348 140529895442176 logging_writer.py:48] [105400] global_step=105400, grad_norm=0.3387099802494049, loss=1.6112052202224731
I0209 21:08:59.863930 140529887049472 logging_writer.py:48] [105500] global_step=105500, grad_norm=0.363061785697937, loss=1.5543965101242065
I0209 21:09:35.083524 140529895442176 logging_writer.py:48] [105600] global_step=105600, grad_norm=0.3647077977657318, loss=1.5809822082519531
I0209 21:10:10.292678 140529887049472 logging_writer.py:48] [105700] global_step=105700, grad_norm=0.39213886857032776, loss=1.6073663234710693
I0209 21:10:45.524880 140529895442176 logging_writer.py:48] [105800] global_step=105800, grad_norm=0.38962456583976746, loss=1.6512784957885742
I0209 21:11:20.772545 140529887049472 logging_writer.py:48] [105900] global_step=105900, grad_norm=0.41968387365341187, loss=1.6597446203231812
I0209 21:11:56.052032 140529895442176 logging_writer.py:48] [106000] global_step=106000, grad_norm=0.39265376329421997, loss=1.6202956438064575
I0209 21:12:31.353270 140529887049472 logging_writer.py:48] [106100] global_step=106100, grad_norm=0.4499683678150177, loss=1.6173523664474487
I0209 21:13:06.635419 140529895442176 logging_writer.py:48] [106200] global_step=106200, grad_norm=0.34666013717651367, loss=1.497482180595398
I0209 21:13:41.934961 140529887049472 logging_writer.py:48] [106300] global_step=106300, grad_norm=0.34685152769088745, loss=1.610868215560913
I0209 21:14:17.221436 140529895442176 logging_writer.py:48] [106400] global_step=106400, grad_norm=0.3414677679538727, loss=1.559455156326294
I0209 21:14:52.597977 140529887049472 logging_writer.py:48] [106500] global_step=106500, grad_norm=0.4036153256893158, loss=1.644188642501831
I0209 21:15:27.860682 140529895442176 logging_writer.py:48] [106600] global_step=106600, grad_norm=0.39864933490753174, loss=1.6817235946655273
I0209 21:16:03.091585 140529887049472 logging_writer.py:48] [106700] global_step=106700, grad_norm=0.3811793625354767, loss=1.5737265348434448
I0209 21:16:38.336067 140529895442176 logging_writer.py:48] [106800] global_step=106800, grad_norm=0.3389199674129486, loss=1.6205239295959473
I0209 21:17:13.572265 140529887049472 logging_writer.py:48] [106900] global_step=106900, grad_norm=0.3293737471103668, loss=1.5582289695739746
I0209 21:17:48.823350 140529895442176 logging_writer.py:48] [107000] global_step=107000, grad_norm=0.36254361271858215, loss=1.626690149307251
I0209 21:18:24.073332 140529887049472 logging_writer.py:48] [107100] global_step=107100, grad_norm=0.37703466415405273, loss=1.6116652488708496
I0209 21:18:59.333265 140529895442176 logging_writer.py:48] [107200] global_step=107200, grad_norm=0.3605193495750427, loss=1.594482660293579
I0209 21:19:33.923884 140699726837568 spec.py:321] Evaluating on the training split.
I0209 21:19:36.959036 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 21:23:34.906619 140699726837568 spec.py:333] Evaluating on the validation split.
I0209 21:23:37.632231 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 21:26:31.611850 140699726837568 spec.py:349] Evaluating on the test split.
I0209 21:26:34.342568 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 21:29:50.746615 140699726837568 submission_runner.py:408] Time since start: 63637.71s, 	Step: 107300, 	{'train/accuracy': 0.6785178780555725, 'train/loss': 1.5146962404251099, 'train/bleu': 33.844285592149795, 'validation/accuracy': 0.6805495023727417, 'validation/loss': 1.4880130290985107, 'validation/bleu': 29.774191434883186, 'validation/num_examples': 3000, 'test/accuracy': 0.693591296672821, 'test/loss': 1.4022724628448486, 'test/bleu': 29.437192795959454, 'test/num_examples': 3003, 'score': 37834.20816755295, 'total_duration': 63637.71153593063, 'accumulated_submission_time': 37834.20816755295, 'accumulated_eval_time': 25798.535117149353, 'accumulated_logging_time': 1.5452971458435059}
I0209 21:29:50.776342 140529887049472 logging_writer.py:48] [107300] accumulated_eval_time=25798.535117, accumulated_logging_time=1.545297, accumulated_submission_time=37834.208168, global_step=107300, preemption_count=0, score=37834.208168, test/accuracy=0.693591, test/bleu=29.437193, test/loss=1.402272, test/num_examples=3003, total_duration=63637.711536, train/accuracy=0.678518, train/bleu=33.844286, train/loss=1.514696, validation/accuracy=0.680550, validation/bleu=29.774191, validation/loss=1.488013, validation/num_examples=3000
I0209 21:29:51.150139 140529895442176 logging_writer.py:48] [107300] global_step=107300, grad_norm=0.3851829171180725, loss=1.6133594512939453
I0209 21:30:26.244213 140529887049472 logging_writer.py:48] [107400] global_step=107400, grad_norm=0.3430657982826233, loss=1.6787164211273193
I0209 21:31:01.496992 140529895442176 logging_writer.py:48] [107500] global_step=107500, grad_norm=0.3120667338371277, loss=1.5486679077148438
I0209 21:31:36.767848 140529887049472 logging_writer.py:48] [107600] global_step=107600, grad_norm=0.36333441734313965, loss=1.5239108800888062
I0209 21:32:12.015430 140529895442176 logging_writer.py:48] [107700] global_step=107700, grad_norm=0.3543718159198761, loss=1.614285945892334
I0209 21:32:47.292067 140529887049472 logging_writer.py:48] [107800] global_step=107800, grad_norm=0.4167279005050659, loss=1.6759499311447144
I0209 21:33:22.562416 140529895442176 logging_writer.py:48] [107900] global_step=107900, grad_norm=0.36005905270576477, loss=1.5988644361495972
I0209 21:33:57.827332 140529887049472 logging_writer.py:48] [108000] global_step=108000, grad_norm=0.33203402161598206, loss=1.6050963401794434
I0209 21:34:33.115492 140529895442176 logging_writer.py:48] [108100] global_step=108100, grad_norm=0.32826969027519226, loss=1.5762763023376465
I0209 21:35:08.364214 140529887049472 logging_writer.py:48] [108200] global_step=108200, grad_norm=0.3693842589855194, loss=1.6208025217056274
I0209 21:35:43.585822 140529895442176 logging_writer.py:48] [108300] global_step=108300, grad_norm=0.36779141426086426, loss=1.6461882591247559
I0209 21:36:18.823660 140529887049472 logging_writer.py:48] [108400] global_step=108400, grad_norm=0.3872149884700775, loss=1.6292223930358887
I0209 21:36:54.078012 140529895442176 logging_writer.py:48] [108500] global_step=108500, grad_norm=0.35293054580688477, loss=1.5894148349761963
I0209 21:37:29.303719 140529887049472 logging_writer.py:48] [108600] global_step=108600, grad_norm=0.37918752431869507, loss=1.6510984897613525
I0209 21:38:04.544727 140529895442176 logging_writer.py:48] [108700] global_step=108700, grad_norm=0.3461452126502991, loss=1.5755897760391235
I0209 21:38:39.772629 140529887049472 logging_writer.py:48] [108800] global_step=108800, grad_norm=0.3367031514644623, loss=1.6125181913375854
I0209 21:39:15.022259 140529895442176 logging_writer.py:48] [108900] global_step=108900, grad_norm=0.3594289720058441, loss=1.5308308601379395
I0209 21:39:50.292041 140529887049472 logging_writer.py:48] [109000] global_step=109000, grad_norm=0.38645386695861816, loss=1.5723451375961304
I0209 21:40:25.550050 140529895442176 logging_writer.py:48] [109100] global_step=109100, grad_norm=0.36737701296806335, loss=1.5902522802352905
I0209 21:41:00.773037 140529887049472 logging_writer.py:48] [109200] global_step=109200, grad_norm=0.3510116636753082, loss=1.5682272911071777
I0209 21:41:36.042694 140529895442176 logging_writer.py:48] [109300] global_step=109300, grad_norm=0.34988242387771606, loss=1.6087907552719116
I0209 21:42:11.332787 140529887049472 logging_writer.py:48] [109400] global_step=109400, grad_norm=0.34724661707878113, loss=1.6389968395233154
I0209 21:42:46.631246 140529895442176 logging_writer.py:48] [109500] global_step=109500, grad_norm=0.3658175468444824, loss=1.5629165172576904
I0209 21:43:21.993383 140529887049472 logging_writer.py:48] [109600] global_step=109600, grad_norm=0.3363775610923767, loss=1.5695487260818481
I0209 21:43:50.994168 140699726837568 spec.py:321] Evaluating on the training split.
I0209 21:43:54.050997 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 21:47:21.649436 140699726837568 spec.py:333] Evaluating on the validation split.
I0209 21:47:24.405893 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 21:50:06.596634 140699726837568 spec.py:349] Evaluating on the test split.
I0209 21:50:09.353610 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 21:52:46.530334 140699726837568 submission_runner.py:408] Time since start: 65013.50s, 	Step: 109684, 	{'train/accuracy': 0.6720339059829712, 'train/loss': 1.5570589303970337, 'train/bleu': 32.979392765729266, 'validation/accuracy': 0.6831905245780945, 'validation/loss': 1.4739441871643066, 'validation/bleu': 30.149647077355855, 'validation/num_examples': 3000, 'test/accuracy': 0.6969845294952393, 'test/loss': 1.3863232135772705, 'test/bleu': 29.99551478418762, 'test/num_examples': 3003, 'score': 38674.33937954903, 'total_duration': 65013.49522304535, 'accumulated_submission_time': 38674.33937954903, 'accumulated_eval_time': 26334.07120013237, 'accumulated_logging_time': 1.5848398208618164}
I0209 21:52:46.561291 140529895442176 logging_writer.py:48] [109684] accumulated_eval_time=26334.071200, accumulated_logging_time=1.584840, accumulated_submission_time=38674.339380, global_step=109684, preemption_count=0, score=38674.339380, test/accuracy=0.696985, test/bleu=29.995515, test/loss=1.386323, test/num_examples=3003, total_duration=65013.495223, train/accuracy=0.672034, train/bleu=32.979393, train/loss=1.557059, validation/accuracy=0.683191, validation/bleu=30.149647, validation/loss=1.473944, validation/num_examples=3000
I0209 21:52:52.546563 140529887049472 logging_writer.py:48] [109700] global_step=109700, grad_norm=0.36705806851387024, loss=1.6193238496780396
I0209 21:53:27.658737 140529895442176 logging_writer.py:48] [109800] global_step=109800, grad_norm=0.36340048909187317, loss=1.5728836059570312
I0209 21:54:02.787435 140529887049472 logging_writer.py:48] [109900] global_step=109900, grad_norm=0.37428849935531616, loss=1.6160894632339478
I0209 21:54:38.021575 140529895442176 logging_writer.py:48] [110000] global_step=110000, grad_norm=0.35912439227104187, loss=1.5850502252578735
I0209 21:55:13.301111 140529887049472 logging_writer.py:48] [110100] global_step=110100, grad_norm=0.351868212223053, loss=1.581761360168457
I0209 21:55:48.575735 140529895442176 logging_writer.py:48] [110200] global_step=110200, grad_norm=0.3632737398147583, loss=1.6058589220046997
I0209 21:56:23.806982 140529887049472 logging_writer.py:48] [110300] global_step=110300, grad_norm=0.3607386648654938, loss=1.6334182024002075
I0209 21:56:59.096994 140529895442176 logging_writer.py:48] [110400] global_step=110400, grad_norm=0.3417610824108124, loss=1.5514370203018188
I0209 21:57:34.319494 140529887049472 logging_writer.py:48] [110500] global_step=110500, grad_norm=0.3662826120853424, loss=1.5741403102874756
I0209 21:58:09.574960 140529895442176 logging_writer.py:48] [110600] global_step=110600, grad_norm=0.3505503833293915, loss=1.570056676864624
I0209 21:58:44.819295 140529887049472 logging_writer.py:48] [110700] global_step=110700, grad_norm=0.4200972318649292, loss=1.560336709022522
I0209 21:59:20.066418 140529895442176 logging_writer.py:48] [110800] global_step=110800, grad_norm=0.37708431482315063, loss=1.570172905921936
I0209 21:59:55.342531 140529887049472 logging_writer.py:48] [110900] global_step=110900, grad_norm=0.31723129749298096, loss=1.6577563285827637
I0209 22:00:30.620914 140529895442176 logging_writer.py:48] [111000] global_step=111000, grad_norm=0.33001065254211426, loss=1.5713119506835938
I0209 22:01:05.905728 140529887049472 logging_writer.py:48] [111100] global_step=111100, grad_norm=0.3404789865016937, loss=1.5543763637542725
I0209 22:01:41.223056 140529895442176 logging_writer.py:48] [111200] global_step=111200, grad_norm=0.34067997336387634, loss=1.5182219743728638
I0209 22:02:16.456796 140529887049472 logging_writer.py:48] [111300] global_step=111300, grad_norm=0.34588438272476196, loss=1.571000099182129
I0209 22:02:51.684604 140529895442176 logging_writer.py:48] [111400] global_step=111400, grad_norm=0.3901210427284241, loss=1.5619285106658936
I0209 22:03:26.952570 140529887049472 logging_writer.py:48] [111500] global_step=111500, grad_norm=0.3651452660560608, loss=1.5923442840576172
I0209 22:04:02.209091 140529895442176 logging_writer.py:48] [111600] global_step=111600, grad_norm=0.3585217595100403, loss=1.5745395421981812
I0209 22:04:37.466298 140529887049472 logging_writer.py:48] [111700] global_step=111700, grad_norm=0.38248929381370544, loss=1.5298230648040771
I0209 22:05:12.757783 140529895442176 logging_writer.py:48] [111800] global_step=111800, grad_norm=0.38763928413391113, loss=1.4815195798873901
I0209 22:05:48.054758 140529887049472 logging_writer.py:48] [111900] global_step=111900, grad_norm=0.34844106435775757, loss=1.5562667846679688
I0209 22:06:23.337237 140529895442176 logging_writer.py:48] [112000] global_step=112000, grad_norm=0.3322560787200928, loss=1.5143357515335083
I0209 22:06:46.656951 140699726837568 spec.py:321] Evaluating on the training split.
I0209 22:06:49.680894 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 22:10:12.462843 140699726837568 spec.py:333] Evaluating on the validation split.
I0209 22:10:15.200951 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 22:12:48.857456 140699726837568 spec.py:349] Evaluating on the test split.
I0209 22:12:51.574156 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 22:15:11.344999 140699726837568 submission_runner.py:408] Time since start: 66358.31s, 	Step: 112068, 	{'train/accuracy': 0.6718950271606445, 'train/loss': 1.5602552890777588, 'train/bleu': 33.424706492563324, 'validation/accuracy': 0.6845420598983765, 'validation/loss': 1.466409683227539, 'validation/bleu': 30.091905902654414, 'validation/num_examples': 3000, 'test/accuracy': 0.6990064382553101, 'test/loss': 1.3751919269561768, 'test/bleu': 29.9190960808925, 'test/num_examples': 3003, 'score': 39514.344621658325, 'total_duration': 66358.30993127823, 'accumulated_submission_time': 39514.344621658325, 'accumulated_eval_time': 26838.75919866562, 'accumulated_logging_time': 1.627079963684082}
I0209 22:15:11.376764 140529887049472 logging_writer.py:48] [112068] accumulated_eval_time=26838.759199, accumulated_logging_time=1.627080, accumulated_submission_time=39514.344622, global_step=112068, preemption_count=0, score=39514.344622, test/accuracy=0.699006, test/bleu=29.919096, test/loss=1.375192, test/num_examples=3003, total_duration=66358.309931, train/accuracy=0.671895, train/bleu=33.424706, train/loss=1.560255, validation/accuracy=0.684542, validation/bleu=30.091906, validation/loss=1.466410, validation/num_examples=3000
I0209 22:15:22.969841 140529895442176 logging_writer.py:48] [112100] global_step=112100, grad_norm=0.334504634141922, loss=1.5507413148880005
I0209 22:15:58.077422 140529887049472 logging_writer.py:48] [112200] global_step=112200, grad_norm=0.34713709354400635, loss=1.503602147102356
I0209 22:16:33.238052 140529895442176 logging_writer.py:48] [112300] global_step=112300, grad_norm=0.38729116320610046, loss=1.556885004043579
I0209 22:17:08.438441 140529887049472 logging_writer.py:48] [112400] global_step=112400, grad_norm=0.3481784760951996, loss=1.5530500411987305
I0209 22:17:43.649018 140529895442176 logging_writer.py:48] [112500] global_step=112500, grad_norm=0.3526375889778137, loss=1.5320885181427002
I0209 22:18:18.872825 140529887049472 logging_writer.py:48] [112600] global_step=112600, grad_norm=0.38442981243133545, loss=1.5507755279541016
I0209 22:18:54.080671 140529895442176 logging_writer.py:48] [112700] global_step=112700, grad_norm=0.36628255248069763, loss=1.5908539295196533
I0209 22:19:29.304148 140529887049472 logging_writer.py:48] [112800] global_step=112800, grad_norm=0.39453718066215515, loss=1.5993726253509521
I0209 22:20:04.606998 140529895442176 logging_writer.py:48] [112900] global_step=112900, grad_norm=0.3773861229419708, loss=1.4923893213272095
I0209 22:20:39.889256 140529887049472 logging_writer.py:48] [113000] global_step=113000, grad_norm=0.3606065511703491, loss=1.5730531215667725
I0209 22:21:15.153153 140529895442176 logging_writer.py:48] [113100] global_step=113100, grad_norm=0.38095822930336, loss=1.6016993522644043
I0209 22:21:50.382080 140529887049472 logging_writer.py:48] [113200] global_step=113200, grad_norm=0.40952783823013306, loss=1.6031084060668945
I0209 22:22:25.620539 140529895442176 logging_writer.py:48] [113300] global_step=113300, grad_norm=0.39075398445129395, loss=1.6002358198165894
I0209 22:23:00.840516 140529887049472 logging_writer.py:48] [113400] global_step=113400, grad_norm=0.3602617383003235, loss=1.560603141784668
I0209 22:23:36.091363 140529895442176 logging_writer.py:48] [113500] global_step=113500, grad_norm=0.3521748483181, loss=1.4685548543930054
I0209 22:24:11.308302 140529887049472 logging_writer.py:48] [113600] global_step=113600, grad_norm=0.3587736189365387, loss=1.524147629737854
I0209 22:24:46.562995 140529895442176 logging_writer.py:48] [113700] global_step=113700, grad_norm=0.35489407181739807, loss=1.5068438053131104
I0209 22:25:21.828964 140529887049472 logging_writer.py:48] [113800] global_step=113800, grad_norm=0.3463970720767975, loss=1.5113744735717773
I0209 22:25:57.083388 140529895442176 logging_writer.py:48] [113900] global_step=113900, grad_norm=0.39535707235336304, loss=1.553251028060913
I0209 22:26:32.299589 140529887049472 logging_writer.py:48] [114000] global_step=114000, grad_norm=0.3559460937976837, loss=1.4711984395980835
I0209 22:27:07.575404 140529895442176 logging_writer.py:48] [114100] global_step=114100, grad_norm=0.36526912450790405, loss=1.5424022674560547
I0209 22:27:42.806855 140529887049472 logging_writer.py:48] [114200] global_step=114200, grad_norm=0.3275879919528961, loss=1.464578628540039
I0209 22:28:18.021199 140529895442176 logging_writer.py:48] [114300] global_step=114300, grad_norm=0.3571915626525879, loss=1.4488646984100342
I0209 22:28:53.242706 140529887049472 logging_writer.py:48] [114400] global_step=114400, grad_norm=0.3836793303489685, loss=1.5074045658111572
I0209 22:29:11.655752 140699726837568 spec.py:321] Evaluating on the training split.
I0209 22:29:14.707601 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 22:33:19.349477 140699726837568 spec.py:333] Evaluating on the validation split.
I0209 22:33:22.089667 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 22:36:26.402839 140699726837568 spec.py:349] Evaluating on the test split.
I0209 22:36:29.153420 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 22:39:48.498308 140699726837568 submission_runner.py:408] Time since start: 67835.46s, 	Step: 114454, 	{'train/accuracy': 0.68068528175354, 'train/loss': 1.5048444271087646, 'train/bleu': 34.31321273170774, 'validation/accuracy': 0.6864762902259827, 'validation/loss': 1.4497908353805542, 'validation/bleu': 30.02917097359945, 'validation/num_examples': 3000, 'test/accuracy': 0.7002847194671631, 'test/loss': 1.361093282699585, 'test/bleu': 30.209978551978672, 'test/num_examples': 3003, 'score': 40354.538182258606, 'total_duration': 67835.46319293976, 'accumulated_submission_time': 40354.538182258606, 'accumulated_eval_time': 27475.601682901382, 'accumulated_logging_time': 1.6685771942138672}
I0209 22:39:48.533916 140529895442176 logging_writer.py:48] [114454] accumulated_eval_time=27475.601683, accumulated_logging_time=1.668577, accumulated_submission_time=40354.538182, global_step=114454, preemption_count=0, score=40354.538182, test/accuracy=0.700285, test/bleu=30.209979, test/loss=1.361093, test/num_examples=3003, total_duration=67835.463193, train/accuracy=0.680685, train/bleu=34.313213, train/loss=1.504844, validation/accuracy=0.686476, validation/bleu=30.029171, validation/loss=1.449791, validation/num_examples=3000
I0209 22:40:05.059578 140529887049472 logging_writer.py:48] [114500] global_step=114500, grad_norm=0.35704293847084045, loss=1.5326497554779053
I0209 22:40:40.174629 140529895442176 logging_writer.py:48] [114600] global_step=114600, grad_norm=0.36446309089660645, loss=1.597904086112976
I0209 22:41:15.366924 140529887049472 logging_writer.py:48] [114700] global_step=114700, grad_norm=0.3929949402809143, loss=1.5320698022842407
I0209 22:41:50.592143 140529895442176 logging_writer.py:48] [114800] global_step=114800, grad_norm=0.3770495057106018, loss=1.5119214057922363
I0209 22:42:25.827059 140529887049472 logging_writer.py:48] [114900] global_step=114900, grad_norm=0.38998645544052124, loss=1.6122335195541382
I0209 22:43:01.055783 140529895442176 logging_writer.py:48] [115000] global_step=115000, grad_norm=0.37531745433807373, loss=1.5123857259750366
I0209 22:43:36.312712 140529887049472 logging_writer.py:48] [115100] global_step=115100, grad_norm=0.38429179787635803, loss=1.5094977617263794
I0209 22:44:11.596139 140529895442176 logging_writer.py:48] [115200] global_step=115200, grad_norm=0.3938276469707489, loss=1.5743051767349243
I0209 22:44:46.872699 140529887049472 logging_writer.py:48] [115300] global_step=115300, grad_norm=0.34965234994888306, loss=1.5083601474761963
I0209 22:45:22.115413 140529895442176 logging_writer.py:48] [115400] global_step=115400, grad_norm=0.3503483831882477, loss=1.5424344539642334
I0209 22:45:57.336678 140529887049472 logging_writer.py:48] [115500] global_step=115500, grad_norm=0.3673097491264343, loss=1.5033150911331177
I0209 22:46:32.557492 140529895442176 logging_writer.py:48] [115600] global_step=115600, grad_norm=0.36031612753868103, loss=1.5136990547180176
I0209 22:47:07.790683 140529887049472 logging_writer.py:48] [115700] global_step=115700, grad_norm=0.35416632890701294, loss=1.449854850769043
I0209 22:47:43.044922 140529895442176 logging_writer.py:48] [115800] global_step=115800, grad_norm=0.36223047971725464, loss=1.5645378828048706
I0209 22:48:18.262165 140529887049472 logging_writer.py:48] [115900] global_step=115900, grad_norm=0.36924293637275696, loss=1.510746955871582
I0209 22:48:53.506449 140529895442176 logging_writer.py:48] [116000] global_step=116000, grad_norm=0.3755933344364166, loss=1.5150904655456543
I0209 22:49:28.739190 140529887049472 logging_writer.py:48] [116100] global_step=116100, grad_norm=0.35671472549438477, loss=1.509507179260254
I0209 22:50:03.989005 140529895442176 logging_writer.py:48] [116200] global_step=116200, grad_norm=0.38250184059143066, loss=1.5803238153457642
I0209 22:50:39.224799 140529887049472 logging_writer.py:48] [116300] global_step=116300, grad_norm=0.3647165596485138, loss=1.4148069620132446
I0209 22:51:14.539602 140529895442176 logging_writer.py:48] [116400] global_step=116400, grad_norm=0.35309818387031555, loss=1.5101937055587769
I0209 22:51:49.768379 140529887049472 logging_writer.py:48] [116500] global_step=116500, grad_norm=0.3576568365097046, loss=1.4669877290725708
I0209 22:52:25.004104 140529895442176 logging_writer.py:48] [116600] global_step=116600, grad_norm=0.4161119759082794, loss=1.450616717338562
I0209 22:53:00.226885 140529887049472 logging_writer.py:48] [116700] global_step=116700, grad_norm=0.362690806388855, loss=1.552046775817871
I0209 22:53:35.528447 140529895442176 logging_writer.py:48] [116800] global_step=116800, grad_norm=0.3843567669391632, loss=1.5511783361434937
I0209 22:53:48.651034 140699726837568 spec.py:321] Evaluating on the training split.
I0209 22:53:51.700522 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 22:57:27.701792 140699726837568 spec.py:333] Evaluating on the validation split.
I0209 22:57:30.436874 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 23:00:12.811614 140699726837568 spec.py:349] Evaluating on the test split.
I0209 23:00:15.554435 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 23:02:44.113627 140699726837568 submission_runner.py:408] Time since start: 69211.08s, 	Step: 116839, 	{'train/accuracy': 0.6848151087760925, 'train/loss': 1.493071436882019, 'train/bleu': 33.9288495000838, 'validation/accuracy': 0.6874682307243347, 'validation/loss': 1.4417047500610352, 'validation/bleu': 30.30958769276966, 'validation/num_examples': 3000, 'test/accuracy': 0.7036197781562805, 'test/loss': 1.3505662679672241, 'test/bleu': 30.64315992147018, 'test/num_examples': 3003, 'score': 41194.56631875038, 'total_duration': 69211.07856273651, 'accumulated_submission_time': 41194.56631875038, 'accumulated_eval_time': 28011.064224004745, 'accumulated_logging_time': 1.7157025337219238}
I0209 23:02:44.146399 140529887049472 logging_writer.py:48] [116839] accumulated_eval_time=28011.064224, accumulated_logging_time=1.715703, accumulated_submission_time=41194.566319, global_step=116839, preemption_count=0, score=41194.566319, test/accuracy=0.703620, test/bleu=30.643160, test/loss=1.350566, test/num_examples=3003, total_duration=69211.078563, train/accuracy=0.684815, train/bleu=33.928850, train/loss=1.493071, validation/accuracy=0.687468, validation/bleu=30.309588, validation/loss=1.441705, validation/num_examples=3000
I0209 23:03:05.914248 140529895442176 logging_writer.py:48] [116900] global_step=116900, grad_norm=0.41244247555732727, loss=1.5413482189178467
I0209 23:03:41.067867 140529887049472 logging_writer.py:48] [117000] global_step=117000, grad_norm=0.35304614901542664, loss=1.436435341835022
I0209 23:04:16.214328 140529895442176 logging_writer.py:48] [117100] global_step=117100, grad_norm=0.3653474748134613, loss=1.5213648080825806
I0209 23:04:51.424092 140529887049472 logging_writer.py:48] [117200] global_step=117200, grad_norm=0.39595943689346313, loss=1.514991044998169
I0209 23:05:26.660326 140529895442176 logging_writer.py:48] [117300] global_step=117300, grad_norm=0.38030242919921875, loss=1.571630835533142
I0209 23:06:01.888378 140529887049472 logging_writer.py:48] [117400] global_step=117400, grad_norm=0.3897210359573364, loss=1.5237904787063599
I0209 23:06:37.096870 140529895442176 logging_writer.py:48] [117500] global_step=117500, grad_norm=0.3831878900527954, loss=1.4847984313964844
I0209 23:07:12.307254 140529887049472 logging_writer.py:48] [117600] global_step=117600, grad_norm=0.3760574460029602, loss=1.4848783016204834
I0209 23:07:47.522743 140529895442176 logging_writer.py:48] [117700] global_step=117700, grad_norm=0.3559180498123169, loss=1.4680368900299072
I0209 23:08:22.753795 140529887049472 logging_writer.py:48] [117800] global_step=117800, grad_norm=0.4116697907447815, loss=1.4788410663604736
I0209 23:08:58.016479 140529895442176 logging_writer.py:48] [117900] global_step=117900, grad_norm=0.3722570836544037, loss=1.4599578380584717
I0209 23:09:33.254019 140529887049472 logging_writer.py:48] [118000] global_step=118000, grad_norm=0.37944546341896057, loss=1.3837264776229858
I0209 23:10:08.486658 140529895442176 logging_writer.py:48] [118100] global_step=118100, grad_norm=0.3646163046360016, loss=1.431820273399353
I0209 23:10:43.721979 140529887049472 logging_writer.py:48] [118200] global_step=118200, grad_norm=0.37963631749153137, loss=1.5231022834777832
I0209 23:11:18.978731 140529895442176 logging_writer.py:48] [118300] global_step=118300, grad_norm=0.3836459815502167, loss=1.4996100664138794
I0209 23:11:54.219165 140529887049472 logging_writer.py:48] [118400] global_step=118400, grad_norm=0.34626656770706177, loss=1.475318193435669
I0209 23:12:29.494906 140529895442176 logging_writer.py:48] [118500] global_step=118500, grad_norm=0.38979050517082214, loss=1.6057813167572021
I0209 23:13:04.728746 140529887049472 logging_writer.py:48] [118600] global_step=118600, grad_norm=0.39290866255760193, loss=1.6413624286651611
I0209 23:13:39.976493 140529895442176 logging_writer.py:48] [118700] global_step=118700, grad_norm=0.3980821967124939, loss=1.5518989562988281
I0209 23:14:15.243280 140529887049472 logging_writer.py:48] [118800] global_step=118800, grad_norm=0.37662070989608765, loss=1.4058914184570312
I0209 23:14:50.520674 140529895442176 logging_writer.py:48] [118900] global_step=118900, grad_norm=0.3691873550415039, loss=1.5122758150100708
I0209 23:15:25.746440 140529887049472 logging_writer.py:48] [119000] global_step=119000, grad_norm=0.3771347999572754, loss=1.469448447227478
I0209 23:16:00.957490 140529895442176 logging_writer.py:48] [119100] global_step=119100, grad_norm=0.3747962415218353, loss=1.4324320554733276
I0209 23:16:36.205964 140529887049472 logging_writer.py:48] [119200] global_step=119200, grad_norm=0.40788671374320984, loss=1.5149112939834595
I0209 23:16:44.389715 140699726837568 spec.py:321] Evaluating on the training split.
I0209 23:16:47.420421 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 23:20:40.817212 140699726837568 spec.py:333] Evaluating on the validation split.
I0209 23:20:43.554587 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 23:23:30.359476 140699726837568 spec.py:349] Evaluating on the test split.
I0209 23:23:33.107150 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 23:26:13.916496 140699726837568 submission_runner.py:408] Time since start: 70620.88s, 	Step: 119225, 	{'train/accuracy': 0.6958141326904297, 'train/loss': 1.417291522026062, 'train/bleu': 35.5492473489212, 'validation/accuracy': 0.6902580261230469, 'validation/loss': 1.4339897632598877, 'validation/bleu': 30.490945218469427, 'validation/num_examples': 3000, 'test/accuracy': 0.7048050761222839, 'test/loss': 1.3403512239456177, 'test/bleu': 30.319124766205878, 'test/num_examples': 3003, 'score': 42034.72462916374, 'total_duration': 70620.8813958168, 'accumulated_submission_time': 42034.72462916374, 'accumulated_eval_time': 28580.59091734886, 'accumulated_logging_time': 1.7584803104400635}
I0209 23:26:13.949166 140529895442176 logging_writer.py:48] [119225] accumulated_eval_time=28580.590917, accumulated_logging_time=1.758480, accumulated_submission_time=42034.724629, global_step=119225, preemption_count=0, score=42034.724629, test/accuracy=0.704805, test/bleu=30.319125, test/loss=1.340351, test/num_examples=3003, total_duration=70620.881396, train/accuracy=0.695814, train/bleu=35.549247, train/loss=1.417292, validation/accuracy=0.690258, validation/bleu=30.490945, validation/loss=1.433990, validation/num_examples=3000
I0209 23:26:40.635282 140529887049472 logging_writer.py:48] [119300] global_step=119300, grad_norm=0.382635235786438, loss=1.4653058052062988
I0209 23:27:15.795578 140529895442176 logging_writer.py:48] [119400] global_step=119400, grad_norm=0.39889511466026306, loss=1.5019094944000244
I0209 23:27:50.985024 140529887049472 logging_writer.py:48] [119500] global_step=119500, grad_norm=0.37144219875335693, loss=1.4808377027511597
I0209 23:28:26.208241 140529895442176 logging_writer.py:48] [119600] global_step=119600, grad_norm=0.3771893382072449, loss=1.4812545776367188
I0209 23:29:01.425476 140529887049472 logging_writer.py:48] [119700] global_step=119700, grad_norm=0.37528494000434875, loss=1.4030288457870483
I0209 23:29:36.662600 140529895442176 logging_writer.py:48] [119800] global_step=119800, grad_norm=0.3874989151954651, loss=1.4906301498413086
I0209 23:30:11.906346 140529887049472 logging_writer.py:48] [119900] global_step=119900, grad_norm=0.38917842507362366, loss=1.4921902418136597
I0209 23:30:47.174415 140529895442176 logging_writer.py:48] [120000] global_step=120000, grad_norm=0.3955007791519165, loss=1.4695651531219482
I0209 23:31:22.423610 140529887049472 logging_writer.py:48] [120100] global_step=120100, grad_norm=0.4411202669143677, loss=1.4508278369903564
I0209 23:31:57.672536 140529895442176 logging_writer.py:48] [120200] global_step=120200, grad_norm=0.37703409790992737, loss=1.4868881702423096
I0209 23:32:32.926456 140529887049472 logging_writer.py:48] [120300] global_step=120300, grad_norm=0.4044746160507202, loss=1.488084077835083
I0209 23:33:08.190857 140529895442176 logging_writer.py:48] [120400] global_step=120400, grad_norm=0.3783559501171112, loss=1.5326566696166992
I0209 23:33:43.460055 140529887049472 logging_writer.py:48] [120500] global_step=120500, grad_norm=0.39250633120536804, loss=1.4044151306152344
I0209 23:34:18.696492 140529895442176 logging_writer.py:48] [120600] global_step=120600, grad_norm=0.399446576833725, loss=1.5087082386016846
I0209 23:34:53.973177 140529887049472 logging_writer.py:48] [120700] global_step=120700, grad_norm=0.40741440653800964, loss=1.4669703245162964
I0209 23:35:29.216817 140529895442176 logging_writer.py:48] [120800] global_step=120800, grad_norm=0.36363545060157776, loss=1.5050140619277954
I0209 23:36:04.435497 140529887049472 logging_writer.py:48] [120900] global_step=120900, grad_norm=0.38090014457702637, loss=1.4215993881225586
I0209 23:36:39.732820 140529895442176 logging_writer.py:48] [121000] global_step=121000, grad_norm=0.3799152672290802, loss=1.439409613609314
I0209 23:37:15.011296 140529887049472 logging_writer.py:48] [121100] global_step=121100, grad_norm=0.3887914717197418, loss=1.5044206380844116
I0209 23:37:50.229773 140529895442176 logging_writer.py:48] [121200] global_step=121200, grad_norm=0.3820667266845703, loss=1.4070980548858643
I0209 23:38:25.475725 140529887049472 logging_writer.py:48] [121300] global_step=121300, grad_norm=0.39592015743255615, loss=1.3990159034729004
I0209 23:39:00.675110 140529895442176 logging_writer.py:48] [121400] global_step=121400, grad_norm=0.3826805055141449, loss=1.4530633687973022
I0209 23:39:35.907857 140529887049472 logging_writer.py:48] [121500] global_step=121500, grad_norm=0.4008502662181854, loss=1.5658601522445679
I0209 23:40:11.147192 140529895442176 logging_writer.py:48] [121600] global_step=121600, grad_norm=0.4033983051776886, loss=1.405330777168274
I0209 23:40:14.040337 140699726837568 spec.py:321] Evaluating on the training split.
I0209 23:40:17.066733 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 23:44:36.914026 140699726837568 spec.py:333] Evaluating on the validation split.
I0209 23:44:39.641181 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 23:47:31.506346 140699726837568 spec.py:349] Evaluating on the test split.
I0209 23:47:34.247924 140699726837568 workload.py:181] Translating evaluation dataset.
I0209 23:50:14.139765 140699726837568 submission_runner.py:408] Time since start: 72061.10s, 	Step: 121610, 	{'train/accuracy': 0.6900114417076111, 'train/loss': 1.4532064199447632, 'train/bleu': 34.7617892280219, 'validation/accuracy': 0.6903696060180664, 'validation/loss': 1.4234853982925415, 'validation/bleu': 30.629143421025496, 'validation/num_examples': 3000, 'test/accuracy': 0.7055488228797913, 'test/loss': 1.332283616065979, 'test/bleu': 30.548803892158418, 'test/num_examples': 3003, 'score': 42874.72810292244, 'total_duration': 72061.10469961166, 'accumulated_submission_time': 42874.72810292244, 'accumulated_eval_time': 29180.690301418304, 'accumulated_logging_time': 1.8024253845214844}
I0209 23:50:14.172612 140529887049472 logging_writer.py:48] [121610] accumulated_eval_time=29180.690301, accumulated_logging_time=1.802425, accumulated_submission_time=42874.728103, global_step=121610, preemption_count=0, score=42874.728103, test/accuracy=0.705549, test/bleu=30.548804, test/loss=1.332284, test/num_examples=3003, total_duration=72061.104700, train/accuracy=0.690011, train/bleu=34.761789, train/loss=1.453206, validation/accuracy=0.690370, validation/bleu=30.629143, validation/loss=1.423485, validation/num_examples=3000
I0209 23:50:46.108592 140529895442176 logging_writer.py:48] [121700] global_step=121700, grad_norm=0.41209739446640015, loss=1.4300535917282104
I0209 23:51:21.268460 140529887049472 logging_writer.py:48] [121800] global_step=121800, grad_norm=0.39486056566238403, loss=1.512807011604309
I0209 23:51:56.517755 140529895442176 logging_writer.py:48] [121900] global_step=121900, grad_norm=0.3785687983036041, loss=1.4448620080947876
I0209 23:52:31.728327 140529887049472 logging_writer.py:48] [122000] global_step=122000, grad_norm=0.3949534296989441, loss=1.4146956205368042
I0209 23:53:06.975949 140529895442176 logging_writer.py:48] [122100] global_step=122100, grad_norm=0.404070645570755, loss=1.502815842628479
I0209 23:53:42.256730 140529887049472 logging_writer.py:48] [122200] global_step=122200, grad_norm=0.39463961124420166, loss=1.4572665691375732
I0209 23:54:17.474654 140529895442176 logging_writer.py:48] [122300] global_step=122300, grad_norm=0.4167035222053528, loss=1.4244098663330078
I0209 23:54:52.705479 140529887049472 logging_writer.py:48] [122400] global_step=122400, grad_norm=0.4010874032974243, loss=1.5556049346923828
I0209 23:55:27.986054 140529895442176 logging_writer.py:48] [122500] global_step=122500, grad_norm=0.3905661404132843, loss=1.4353063106536865
I0209 23:56:03.203173 140529887049472 logging_writer.py:48] [122600] global_step=122600, grad_norm=0.39033669233322144, loss=1.4313626289367676
I0209 23:56:38.472077 140529895442176 logging_writer.py:48] [122700] global_step=122700, grad_norm=0.40353503823280334, loss=1.5309619903564453
I0209 23:57:13.752388 140529887049472 logging_writer.py:48] [122800] global_step=122800, grad_norm=0.3991757929325104, loss=1.3881220817565918
I0209 23:57:49.058375 140529895442176 logging_writer.py:48] [122900] global_step=122900, grad_norm=0.4183753728866577, loss=1.4587922096252441
I0209 23:58:24.325820 140529887049472 logging_writer.py:48] [123000] global_step=123000, grad_norm=0.37757328152656555, loss=1.459594488143921
I0209 23:58:59.564731 140529895442176 logging_writer.py:48] [123100] global_step=123100, grad_norm=0.41112345457077026, loss=1.436872959136963
I0209 23:59:34.790070 140529887049472 logging_writer.py:48] [123200] global_step=123200, grad_norm=0.39707353711128235, loss=1.4838218688964844
I0210 00:00:09.995657 140529895442176 logging_writer.py:48] [123300] global_step=123300, grad_norm=0.386417031288147, loss=1.4259477853775024
I0210 00:00:45.245554 140529887049472 logging_writer.py:48] [123400] global_step=123400, grad_norm=0.40698856115341187, loss=1.3945293426513672
I0210 00:01:20.479327 140529895442176 logging_writer.py:48] [123500] global_step=123500, grad_norm=0.39262524247169495, loss=1.4812061786651611
I0210 00:01:55.756835 140529887049472 logging_writer.py:48] [123600] global_step=123600, grad_norm=0.4180727005004883, loss=1.4413070678710938
I0210 00:02:31.018604 140529895442176 logging_writer.py:48] [123700] global_step=123700, grad_norm=0.39828211069107056, loss=1.4679150581359863
I0210 00:03:06.308434 140529887049472 logging_writer.py:48] [123800] global_step=123800, grad_norm=0.38983404636383057, loss=1.4504905939102173
I0210 00:03:41.574970 140529895442176 logging_writer.py:48] [123900] global_step=123900, grad_norm=0.3858232796192169, loss=1.4817789793014526
I0210 00:04:14.429458 140699726837568 spec.py:321] Evaluating on the training split.
I0210 00:04:17.460507 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 00:08:03.053130 140699726837568 spec.py:333] Evaluating on the validation split.
I0210 00:08:05.779899 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 00:10:41.548890 140699726837568 spec.py:349] Evaluating on the test split.
I0210 00:10:44.283828 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 00:13:11.446510 140699726837568 submission_runner.py:408] Time since start: 73438.41s, 	Step: 123995, 	{'train/accuracy': 0.6892982721328735, 'train/loss': 1.4594749212265015, 'train/bleu': 34.6818584249282, 'validation/accuracy': 0.6919319033622742, 'validation/loss': 1.4199069738388062, 'validation/bleu': 30.847756824941126, 'validation/num_examples': 3000, 'test/accuracy': 0.7058973908424377, 'test/loss': 1.32646906375885, 'test/bleu': 30.809119513639406, 'test/num_examples': 3003, 'score': 43714.89873600006, 'total_duration': 73438.41144561768, 'accumulated_submission_time': 43714.89873600006, 'accumulated_eval_time': 29717.70730495453, 'accumulated_logging_time': 1.845533847808838}
I0210 00:13:11.478652 140529887049472 logging_writer.py:48] [123995] accumulated_eval_time=29717.707305, accumulated_logging_time=1.845534, accumulated_submission_time=43714.898736, global_step=123995, preemption_count=0, score=43714.898736, test/accuracy=0.705897, test/bleu=30.809120, test/loss=1.326469, test/num_examples=3003, total_duration=73438.411446, train/accuracy=0.689298, train/bleu=34.681858, train/loss=1.459475, validation/accuracy=0.691932, validation/bleu=30.847757, validation/loss=1.419907, validation/num_examples=3000
I0210 00:13:13.608429 140529895442176 logging_writer.py:48] [124000] global_step=124000, grad_norm=0.39324143528938293, loss=1.4276334047317505
I0210 00:13:48.694266 140529887049472 logging_writer.py:48] [124100] global_step=124100, grad_norm=0.43059679865837097, loss=1.4328196048736572
I0210 00:14:23.829266 140529895442176 logging_writer.py:48] [124200] global_step=124200, grad_norm=0.3853418827056885, loss=1.4721267223358154
I0210 00:14:59.033072 140529887049472 logging_writer.py:48] [124300] global_step=124300, grad_norm=0.39396607875823975, loss=1.4779080152511597
I0210 00:15:34.267380 140529895442176 logging_writer.py:48] [124400] global_step=124400, grad_norm=0.42898374795913696, loss=1.4845842123031616
I0210 00:16:09.506002 140529887049472 logging_writer.py:48] [124500] global_step=124500, grad_norm=0.4132343530654907, loss=1.513885498046875
I0210 00:16:44.724270 140529895442176 logging_writer.py:48] [124600] global_step=124600, grad_norm=0.4071480929851532, loss=1.4627233743667603
I0210 00:17:19.957007 140529887049472 logging_writer.py:48] [124700] global_step=124700, grad_norm=0.4015537202358246, loss=1.4923421144485474
I0210 00:17:55.172466 140529895442176 logging_writer.py:48] [124800] global_step=124800, grad_norm=0.40185466408729553, loss=1.3719159364700317
I0210 00:18:30.443178 140529887049472 logging_writer.py:48] [124900] global_step=124900, grad_norm=0.3975907266139984, loss=1.4440537691116333
I0210 00:19:05.729482 140529895442176 logging_writer.py:48] [125000] global_step=125000, grad_norm=0.3873033821582794, loss=1.3480300903320312
I0210 00:19:41.178844 140529887049472 logging_writer.py:48] [125100] global_step=125100, grad_norm=0.3801952302455902, loss=1.5045884847640991
I0210 00:20:16.489070 140529895442176 logging_writer.py:48] [125200] global_step=125200, grad_norm=0.41196945309638977, loss=1.4598116874694824
I0210 00:20:51.718573 140529887049472 logging_writer.py:48] [125300] global_step=125300, grad_norm=0.40158987045288086, loss=1.4630602598190308
I0210 00:21:26.942191 140529895442176 logging_writer.py:48] [125400] global_step=125400, grad_norm=0.3787859380245209, loss=1.4735755920410156
I0210 00:22:02.180174 140529887049472 logging_writer.py:48] [125500] global_step=125500, grad_norm=0.4016682505607605, loss=1.452293872833252
I0210 00:22:37.427786 140529895442176 logging_writer.py:48] [125600] global_step=125600, grad_norm=0.4086749255657196, loss=1.444551944732666
I0210 00:23:12.665247 140529887049472 logging_writer.py:48] [125700] global_step=125700, grad_norm=0.3763223886489868, loss=1.4246714115142822
I0210 00:23:47.896285 140529895442176 logging_writer.py:48] [125800] global_step=125800, grad_norm=0.4174245595932007, loss=1.481200098991394
I0210 00:24:23.109868 140529887049472 logging_writer.py:48] [125900] global_step=125900, grad_norm=0.38515743613243103, loss=1.3775057792663574
I0210 00:24:58.348711 140529895442176 logging_writer.py:48] [126000] global_step=126000, grad_norm=0.42166668176651, loss=1.4539872407913208
I0210 00:25:33.662048 140529887049472 logging_writer.py:48] [126100] global_step=126100, grad_norm=0.392098993062973, loss=1.4469901323318481
I0210 00:26:08.900461 140529895442176 logging_writer.py:48] [126200] global_step=126200, grad_norm=0.424227774143219, loss=1.477883219718933
I0210 00:26:44.143704 140529887049472 logging_writer.py:48] [126300] global_step=126300, grad_norm=0.4028926491737366, loss=1.4625880718231201
I0210 00:27:11.688527 140699726837568 spec.py:321] Evaluating on the training split.
I0210 00:27:14.724983 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 00:31:01.771618 140699726837568 spec.py:333] Evaluating on the validation split.
I0210 00:31:04.505306 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 00:33:52.118075 140699726837568 spec.py:349] Evaluating on the test split.
I0210 00:33:54.851891 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 00:36:36.229860 140699726837568 submission_runner.py:408] Time since start: 74843.19s, 	Step: 126380, 	{'train/accuracy': 0.697314977645874, 'train/loss': 1.4237242937088013, 'train/bleu': 36.055260217867456, 'validation/accuracy': 0.6940025687217712, 'validation/loss': 1.413859486579895, 'validation/bleu': 30.82466800846016, 'validation/num_examples': 3000, 'test/accuracy': 0.7090000510215759, 'test/loss': 1.3189260959625244, 'test/bleu': 30.86978933852004, 'test/num_examples': 3003, 'score': 44555.023113012314, 'total_duration': 74843.19479346275, 'accumulated_submission_time': 44555.023113012314, 'accumulated_eval_time': 30282.248594284058, 'accumulated_logging_time': 1.8876619338989258}
I0210 00:36:36.264312 140529895442176 logging_writer.py:48] [126380] accumulated_eval_time=30282.248594, accumulated_logging_time=1.887662, accumulated_submission_time=44555.023113, global_step=126380, preemption_count=0, score=44555.023113, test/accuracy=0.709000, test/bleu=30.869789, test/loss=1.318926, test/num_examples=3003, total_duration=74843.194793, train/accuracy=0.697315, train/bleu=36.055260, train/loss=1.423724, validation/accuracy=0.694003, validation/bleu=30.824668, validation/loss=1.413859, validation/num_examples=3000
I0210 00:36:43.640102 140529887049472 logging_writer.py:48] [126400] global_step=126400, grad_norm=0.39027711749076843, loss=1.4883158206939697
I0210 00:37:18.735775 140529895442176 logging_writer.py:48] [126500] global_step=126500, grad_norm=0.39126822352409363, loss=1.3868696689605713
I0210 00:37:53.895431 140529887049472 logging_writer.py:48] [126600] global_step=126600, grad_norm=0.40564265847206116, loss=1.483822226524353
I0210 00:38:29.095443 140529895442176 logging_writer.py:48] [126700] global_step=126700, grad_norm=0.41292259097099304, loss=1.434920310974121
I0210 00:39:04.322185 140529887049472 logging_writer.py:48] [126800] global_step=126800, grad_norm=0.36812132596969604, loss=1.4065009355545044
I0210 00:39:39.553354 140529895442176 logging_writer.py:48] [126900] global_step=126900, grad_norm=0.3992677628993988, loss=1.4994078874588013
I0210 00:40:14.783734 140529887049472 logging_writer.py:48] [127000] global_step=127000, grad_norm=0.38127249479293823, loss=1.3825297355651855
I0210 00:40:50.005504 140529895442176 logging_writer.py:48] [127100] global_step=127100, grad_norm=0.3946763873100281, loss=1.4093751907348633
I0210 00:41:25.234313 140529887049472 logging_writer.py:48] [127200] global_step=127200, grad_norm=0.41847947239875793, loss=1.4567734003067017
I0210 00:42:00.504434 140529895442176 logging_writer.py:48] [127300] global_step=127300, grad_norm=0.3947021961212158, loss=1.464678406715393
I0210 00:42:35.738146 140529887049472 logging_writer.py:48] [127400] global_step=127400, grad_norm=0.4157921075820923, loss=1.5974476337432861
I0210 00:43:10.962229 140529895442176 logging_writer.py:48] [127500] global_step=127500, grad_norm=0.4013287127017975, loss=1.412940263748169
I0210 00:43:46.202583 140529887049472 logging_writer.py:48] [127600] global_step=127600, grad_norm=0.3956862688064575, loss=1.4893935918807983
I0210 00:44:21.463425 140529895442176 logging_writer.py:48] [127700] global_step=127700, grad_norm=0.4051364064216614, loss=1.442705512046814
I0210 00:44:56.718413 140529887049472 logging_writer.py:48] [127800] global_step=127800, grad_norm=0.4186311364173889, loss=1.4657520055770874
I0210 00:45:31.939675 140529895442176 logging_writer.py:48] [127900] global_step=127900, grad_norm=0.4175795614719391, loss=1.3975365161895752
I0210 00:46:07.159045 140529887049472 logging_writer.py:48] [128000] global_step=128000, grad_norm=0.4290406107902527, loss=1.4503008127212524
I0210 00:46:42.426989 140529895442176 logging_writer.py:48] [128100] global_step=128100, grad_norm=0.3903140723705292, loss=1.4018640518188477
I0210 00:47:17.692286 140529887049472 logging_writer.py:48] [128200] global_step=128200, grad_norm=0.39119988679885864, loss=1.3567167520523071
I0210 00:47:52.981125 140529895442176 logging_writer.py:48] [128300] global_step=128300, grad_norm=0.39041852951049805, loss=1.3754328489303589
I0210 00:48:28.242616 140529887049472 logging_writer.py:48] [128400] global_step=128400, grad_norm=0.40999120473861694, loss=1.4141905307769775
I0210 00:49:03.532197 140529895442176 logging_writer.py:48] [128500] global_step=128500, grad_norm=0.4015287160873413, loss=1.354580044746399
I0210 00:49:38.780613 140529887049472 logging_writer.py:48] [128600] global_step=128600, grad_norm=0.40689417719841003, loss=1.4424099922180176
I0210 00:50:14.035192 140529895442176 logging_writer.py:48] [128700] global_step=128700, grad_norm=0.3814528286457062, loss=1.4510352611541748
I0210 00:50:36.332983 140699726837568 spec.py:321] Evaluating on the training split.
I0210 00:50:39.381269 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 00:54:37.357009 140699726837568 spec.py:333] Evaluating on the validation split.
I0210 00:54:40.112342 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 00:57:33.169400 140699726837568 spec.py:349] Evaluating on the test split.
I0210 00:57:35.914016 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 01:00:09.776067 140699726837568 submission_runner.py:408] Time since start: 76256.74s, 	Step: 128765, 	{'train/accuracy': 0.7001673579216003, 'train/loss': 1.3921157121658325, 'train/bleu': 36.054005536466065, 'validation/accuracy': 0.6933701634407043, 'validation/loss': 1.4130243062973022, 'validation/bleu': 30.574519850326517, 'validation/num_examples': 3000, 'test/accuracy': 0.7091976404190063, 'test/loss': 1.3158727884292603, 'test/bleu': 30.909184782342955, 'test/num_examples': 3003, 'score': 45395.007142305374, 'total_duration': 76256.74099755287, 'accumulated_submission_time': 45395.007142305374, 'accumulated_eval_time': 30855.691635131836, 'accumulated_logging_time': 1.931976318359375}
I0210 01:00:09.809670 140529887049472 logging_writer.py:48] [128765] accumulated_eval_time=30855.691635, accumulated_logging_time=1.931976, accumulated_submission_time=45395.007142, global_step=128765, preemption_count=0, score=45395.007142, test/accuracy=0.709198, test/bleu=30.909185, test/loss=1.315873, test/num_examples=3003, total_duration=76256.740998, train/accuracy=0.700167, train/bleu=36.054006, train/loss=1.392116, validation/accuracy=0.693370, validation/bleu=30.574520, validation/loss=1.413024, validation/num_examples=3000
I0210 01:00:22.452787 140529895442176 logging_writer.py:48] [128800] global_step=128800, grad_norm=0.38767361640930176, loss=1.3766472339630127
I0210 01:00:57.548585 140529887049472 logging_writer.py:48] [128900] global_step=128900, grad_norm=0.4105089604854584, loss=1.4603736400604248
I0210 01:01:32.716544 140529895442176 logging_writer.py:48] [129000] global_step=129000, grad_norm=0.42366331815719604, loss=1.4877398014068604
I0210 01:02:07.915328 140529887049472 logging_writer.py:48] [129100] global_step=129100, grad_norm=0.39912429451942444, loss=1.4075616598129272
I0210 01:02:43.183693 140529895442176 logging_writer.py:48] [129200] global_step=129200, grad_norm=0.40540558099746704, loss=1.4379938840866089
I0210 01:03:18.583263 140529887049472 logging_writer.py:48] [129300] global_step=129300, grad_norm=0.41379258036613464, loss=1.4599651098251343
I0210 01:03:53.908033 140529895442176 logging_writer.py:48] [129400] global_step=129400, grad_norm=0.3883695900440216, loss=1.4165256023406982
I0210 01:04:29.231306 140529887049472 logging_writer.py:48] [129500] global_step=129500, grad_norm=0.4081844389438629, loss=1.3745405673980713
I0210 01:05:04.543408 140529895442176 logging_writer.py:48] [129600] global_step=129600, grad_norm=0.3971066474914551, loss=1.4052835702896118
I0210 01:05:39.843773 140529887049472 logging_writer.py:48] [129700] global_step=129700, grad_norm=0.40714287757873535, loss=1.3861101865768433
I0210 01:06:15.105431 140529895442176 logging_writer.py:48] [129800] global_step=129800, grad_norm=0.4097230136394501, loss=1.3759576082229614
I0210 01:06:50.332610 140529887049472 logging_writer.py:48] [129900] global_step=129900, grad_norm=0.4070148766040802, loss=1.4547330141067505
I0210 01:07:25.568744 140529895442176 logging_writer.py:48] [130000] global_step=130000, grad_norm=0.3942948877811432, loss=1.3604928255081177
I0210 01:08:00.806717 140529887049472 logging_writer.py:48] [130100] global_step=130100, grad_norm=0.3909713327884674, loss=1.4082138538360596
I0210 01:08:36.047120 140529895442176 logging_writer.py:48] [130200] global_step=130200, grad_norm=0.40105199813842773, loss=1.4480093717575073
I0210 01:09:11.281811 140529887049472 logging_writer.py:48] [130300] global_step=130300, grad_norm=0.41192516684532166, loss=1.4253854751586914
I0210 01:09:46.510807 140529895442176 logging_writer.py:48] [130400] global_step=130400, grad_norm=0.39977777004241943, loss=1.4448062181472778
I0210 01:10:21.752042 140529887049472 logging_writer.py:48] [130500] global_step=130500, grad_norm=0.39515256881713867, loss=1.3913789987564087
I0210 01:10:56.954418 140529895442176 logging_writer.py:48] [130600] global_step=130600, grad_norm=0.4038377106189728, loss=1.439775824546814
I0210 01:11:32.170386 140529887049472 logging_writer.py:48] [130700] global_step=130700, grad_norm=0.38514286279678345, loss=1.3796515464782715
I0210 01:12:07.417367 140529895442176 logging_writer.py:48] [130800] global_step=130800, grad_norm=0.38334840536117554, loss=1.440798282623291
I0210 01:12:42.639563 140529887049472 logging_writer.py:48] [130900] global_step=130900, grad_norm=0.37915802001953125, loss=1.4583210945129395
I0210 01:13:17.861839 140529895442176 logging_writer.py:48] [131000] global_step=131000, grad_norm=0.41198867559432983, loss=1.431395411491394
I0210 01:13:53.058503 140529887049472 logging_writer.py:48] [131100] global_step=131100, grad_norm=0.40547314286231995, loss=1.4609981775283813
I0210 01:14:10.059704 140699726837568 spec.py:321] Evaluating on the training split.
I0210 01:14:13.093395 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 01:18:05.415634 140699726837568 spec.py:333] Evaluating on the validation split.
I0210 01:18:08.140147 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 01:20:42.952630 140699726837568 spec.py:349] Evaluating on the test split.
I0210 01:20:45.693892 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 01:23:10.024670 140699726837568 submission_runner.py:408] Time since start: 77636.99s, 	Step: 131150, 	{'train/accuracy': 0.6955586671829224, 'train/loss': 1.4250540733337402, 'train/bleu': 36.228446900630956, 'validation/accuracy': 0.6936181783676147, 'validation/loss': 1.4108177423477173, 'validation/bleu': 30.819000657109928, 'validation/num_examples': 3000, 'test/accuracy': 0.7093021869659424, 'test/loss': 1.3146555423736572, 'test/bleu': 30.79200209401173, 'test/num_examples': 3003, 'score': 46235.168941020966, 'total_duration': 77636.98960375786, 'accumulated_submission_time': 46235.168941020966, 'accumulated_eval_time': 31395.656549215317, 'accumulated_logging_time': 1.9757180213928223}
I0210 01:23:10.058430 140529895442176 logging_writer.py:48] [131150] accumulated_eval_time=31395.656549, accumulated_logging_time=1.975718, accumulated_submission_time=46235.168941, global_step=131150, preemption_count=0, score=46235.168941, test/accuracy=0.709302, test/bleu=30.792002, test/loss=1.314656, test/num_examples=3003, total_duration=77636.989604, train/accuracy=0.695559, train/bleu=36.228447, train/loss=1.425054, validation/accuracy=0.693618, validation/bleu=30.819001, validation/loss=1.410818, validation/num_examples=3000
I0210 01:23:27.967513 140529887049472 logging_writer.py:48] [131200] global_step=131200, grad_norm=0.3999476432800293, loss=1.4472192525863647
I0210 01:24:03.075398 140529895442176 logging_writer.py:48] [131300] global_step=131300, grad_norm=0.3915768563747406, loss=1.440723180770874
I0210 01:24:38.273028 140529887049472 logging_writer.py:48] [131400] global_step=131400, grad_norm=0.4107993543148041, loss=1.4116277694702148
I0210 01:25:13.516059 140529895442176 logging_writer.py:48] [131500] global_step=131500, grad_norm=0.3951020836830139, loss=1.3642479181289673
I0210 01:25:48.741211 140529887049472 logging_writer.py:48] [131600] global_step=131600, grad_norm=0.39996853470802307, loss=1.4812452793121338
I0210 01:26:23.992091 140529895442176 logging_writer.py:48] [131700] global_step=131700, grad_norm=0.4153810143470764, loss=1.481867790222168
I0210 01:26:59.200784 140529887049472 logging_writer.py:48] [131800] global_step=131800, grad_norm=0.40244367718696594, loss=1.4319554567337036
I0210 01:27:34.471935 140529895442176 logging_writer.py:48] [131900] global_step=131900, grad_norm=0.3835444748401642, loss=1.330935001373291
I0210 01:28:09.740925 140529887049472 logging_writer.py:48] [132000] global_step=132000, grad_norm=0.4028504192829132, loss=1.4952744245529175
I0210 01:28:44.970380 140529895442176 logging_writer.py:48] [132100] global_step=132100, grad_norm=0.3982565104961395, loss=1.3966889381408691
I0210 01:29:20.198251 140529887049472 logging_writer.py:48] [132200] global_step=132200, grad_norm=0.398743212223053, loss=1.421830415725708
I0210 01:29:55.414694 140529895442176 logging_writer.py:48] [132300] global_step=132300, grad_norm=0.4042187035083771, loss=1.4654620885849
I0210 01:30:30.661365 140529887049472 logging_writer.py:48] [132400] global_step=132400, grad_norm=0.39073991775512695, loss=1.3767637014389038
I0210 01:31:05.924789 140529895442176 logging_writer.py:48] [132500] global_step=132500, grad_norm=0.3898079991340637, loss=1.4242538213729858
I0210 01:31:41.157036 140529887049472 logging_writer.py:48] [132600] global_step=132600, grad_norm=0.39876168966293335, loss=1.4580250978469849
I0210 01:32:16.372718 140529895442176 logging_writer.py:48] [132700] global_step=132700, grad_norm=0.3917706310749054, loss=1.40366530418396
I0210 01:32:51.580421 140529887049472 logging_writer.py:48] [132800] global_step=132800, grad_norm=0.39517882466316223, loss=1.462496280670166
I0210 01:33:26.810262 140529895442176 logging_writer.py:48] [132900] global_step=132900, grad_norm=0.40240076184272766, loss=1.4262181520462036
I0210 01:34:02.030793 140529887049472 logging_writer.py:48] [133000] global_step=133000, grad_norm=0.3787532150745392, loss=1.3552772998809814
I0210 01:34:37.315538 140529895442176 logging_writer.py:48] [133100] global_step=133100, grad_norm=0.40307849645614624, loss=1.4723405838012695
I0210 01:35:12.593456 140529887049472 logging_writer.py:48] [133200] global_step=133200, grad_norm=0.39314404129981995, loss=1.432515025138855
I0210 01:35:47.875592 140529895442176 logging_writer.py:48] [133300] global_step=133300, grad_norm=0.3814772963523865, loss=1.356593370437622
I0210 01:35:58.887563 140699726837568 spec.py:321] Evaluating on the training split.
I0210 01:36:01.937392 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 01:39:51.689278 140699726837568 spec.py:333] Evaluating on the validation split.
I0210 01:39:54.436938 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 01:42:33.711327 140699726837568 spec.py:349] Evaluating on the test split.
I0210 01:42:36.452907 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 01:45:01.153042 140699726837568 submission_runner.py:408] Time since start: 78948.12s, 	Step: 133333, 	{'train/accuracy': 0.6961641311645508, 'train/loss': 1.423098087310791, 'train/bleu': 36.12654870431947, 'validation/accuracy': 0.6939653754234314, 'validation/loss': 1.4107656478881836, 'validation/bleu': 30.919892623504534, 'validation/num_examples': 3000, 'test/accuracy': 0.70927894115448, 'test/loss': 1.3146251440048218, 'test/bleu': 30.84290524056852, 'test/num_examples': 3003, 'score': 47003.91788029671, 'total_duration': 78948.11796474457, 'accumulated_submission_time': 47003.91788029671, 'accumulated_eval_time': 31937.92197227478, 'accumulated_logging_time': 2.0215542316436768}
I0210 01:45:01.186551 140529887049472 logging_writer.py:48] [133333] accumulated_eval_time=31937.921972, accumulated_logging_time=2.021554, accumulated_submission_time=47003.917880, global_step=133333, preemption_count=0, score=47003.917880, test/accuracy=0.709279, test/bleu=30.842905, test/loss=1.314625, test/num_examples=3003, total_duration=78948.117965, train/accuracy=0.696164, train/bleu=36.126549, train/loss=1.423098, validation/accuracy=0.693965, validation/bleu=30.919893, validation/loss=1.410766, validation/num_examples=3000
I0210 01:45:01.220662 140529895442176 logging_writer.py:48] [133333] global_step=133333, preemption_count=0, score=47003.917880
I0210 01:45:02.794909 140699726837568 checkpoints.py:490] Saving checkpoint at step: 133333
I0210 01:45:06.778972 140699726837568 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_1/wmt_jax/trial_4/checkpoint_133333
I0210 01:45:06.783741 140699726837568 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_1/wmt_jax/trial_4/checkpoint_133333.
I0210 01:45:06.841021 140699726837568 submission_runner.py:583] Tuning trial 4/5
I0210 01:45:06.841192 140699726837568 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.004958460849689891, one_minus_beta1=0.13625575743, beta2=0.6291854735396584, weight_decay=0.1147386261512052, warmup_factor=0.02)
I0210 01:45:06.848071 140699726837568 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0006454141112044454, 'train/loss': 11.175609588623047, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.208685874938965, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.19086742401123, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 30.8999125957489, 'total_duration': 908.4347548484802, 'accumulated_submission_time': 30.8999125957489, 'accumulated_eval_time': 877.5347895622253, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (2382, {'train/accuracy': 0.5327548980712891, 'train/loss': 2.6247782707214355, 'train/bleu': 24.206353735215085, 'validation/accuracy': 0.537860631942749, 'validation/loss': 2.5653810501098633, 'validation/bleu': 20.049483860443015, 'validation/num_examples': 3000, 'test/accuracy': 0.5347859263420105, 'test/loss': 2.591620922088623, 'test/bleu': 18.522566174985347, 'test/num_examples': 3003, 'score': 870.8914546966553, 'total_duration': 2274.2477111816406, 'accumulated_submission_time': 870.8914546966553, 'accumulated_eval_time': 1403.2590272426605, 'accumulated_logging_time': 0.02254319190979004, 'global_step': 2382, 'preemption_count': 0}), (4766, {'train/accuracy': 0.5796718597412109, 'train/loss': 2.213855266571045, 'train/bleu': 26.36870746881244, 'validation/accuracy': 0.5924167037010193, 'validation/loss': 2.0987660884857178, 'validation/bleu': 23.469450249820703, 'validation/num_examples': 3000, 'test/accuracy': 0.5945848822593689, 'test/loss': 2.0823488235473633, 'test/bleu': 21.990632388952925, 'test/num_examples': 3003, 'score': 1711.0473158359528, 'total_duration': 3626.4316835403442, 'accumulated_submission_time': 1711.0473158359528, 'accumulated_eval_time': 1915.1850049495697, 'accumulated_logging_time': 0.047638893127441406, 'global_step': 4766, 'preemption_count': 0}), (7150, {'train/accuracy': 0.5893054604530334, 'train/loss': 2.118350028991699, 'train/bleu': 27.176336003425604, 'validation/accuracy': 0.6037743091583252, 'validation/loss': 2.024528980255127, 'validation/bleu': 24.357176752244964, 'validation/num_examples': 3000, 'test/accuracy': 0.6053221821784973, 'test/loss': 2.0033843517303467, 'test/bleu': 22.893728461005857, 'test/num_examples': 3003, 'score': 2551.1354496479034, 'total_duration': 4959.091591835022, 'accumulated_submission_time': 2551.1354496479034, 'accumulated_eval_time': 2407.65248298645, 'accumulated_logging_time': 0.07714414596557617, 'global_step': 7150, 'preemption_count': 0}), (9535, {'train/accuracy': 0.5954850912094116, 'train/loss': 2.119326114654541, 'train/bleu': 27.711826590662206, 'validation/accuracy': 0.6125404238700867, 'validation/loss': 1.9642142057418823, 'validation/bleu': 24.82005795946175, 'validation/num_examples': 3000, 'test/accuracy': 0.6162105798721313, 'test/loss': 1.9303609132766724, 'test/bleu': 23.417765370115543, 'test/num_examples': 3003, 'score': 3391.3415517807007, 'total_duration': 6374.207869529724, 'accumulated_submission_time': 3391.3415517807007, 'accumulated_eval_time': 2982.4601430892944, 'accumulated_logging_time': 0.1054227352142334, 'global_step': 9535, 'preemption_count': 0}), (11917, {'train/accuracy': 0.5949356555938721, 'train/loss': 2.093808650970459, 'train/bleu': 27.32630178120262, 'validation/accuracy': 0.6108169555664062, 'validation/loss': 1.9556013345718384, 'validation/bleu': 23.938898942260515, 'validation/num_examples': 3000, 'test/accuracy': 0.6186973452568054, 'test/loss': 1.9178035259246826, 'test/bleu': 22.844513408768293, 'test/num_examples': 3003, 'score': 4231.374727487564, 'total_duration': 7720.172181844711, 'accumulated_submission_time': 4231.374727487564, 'accumulated_eval_time': 3488.283415555954, 'accumulated_logging_time': 0.13197827339172363, 'global_step': 11917, 'preemption_count': 0}), (14301, {'train/accuracy': 0.597704291343689, 'train/loss': 2.0802969932556152, 'train/bleu': 28.308626342993755, 'validation/accuracy': 0.6167685389518738, 'validation/loss': 1.930830478668213, 'validation/bleu': 25.063348149842113, 'validation/num_examples': 3000, 'test/accuracy': 0.621997594833374, 'test/loss': 1.8840601444244385, 'test/bleu': 23.81654558217464, 'test/num_examples': 3003, 'score': 5071.9056560993195, 'total_duration': 9049.865262746811, 'accumulated_submission_time': 5071.9056560993195, 'accumulated_eval_time': 3977.3424422740936, 'accumulated_logging_time': 0.15987753868103027, 'global_step': 14301, 'preemption_count': 0}), (16684, {'train/accuracy': 0.5992457866668701, 'train/loss': 2.0773322582244873, 'train/bleu': 28.287729139742407, 'validation/accuracy': 0.6202650666236877, 'validation/loss': 1.911595344543457, 'validation/bleu': 25.16760638686699, 'validation/num_examples': 3000, 'test/accuracy': 0.6254836916923523, 'test/loss': 1.8665118217468262, 'test/bleu': 23.955626051084025, 'test/num_examples': 3003, 'score': 5911.835096359253, 'total_duration': 10376.551353693008, 'accumulated_submission_time': 5911.835096359253, 'accumulated_eval_time': 4463.997585058212, 'accumulated_logging_time': 0.1864314079284668, 'global_step': 16684, 'preemption_count': 0}), (19068, {'train/accuracy': 0.6137471199035645, 'train/loss': 1.926009178161621, 'train/bleu': 29.14411802678976, 'validation/accuracy': 0.6178720593452454, 'validation/loss': 1.9107155799865723, 'validation/bleu': 25.27364721195347, 'validation/num_examples': 3000, 'test/accuracy': 0.6246935129165649, 'test/loss': 1.8686076402664185, 'test/bleu': 24.06795003200436, 'test/num_examples': 3003, 'score': 6751.987172842026, 'total_duration': 11724.933151721954, 'accumulated_submission_time': 6751.987172842026, 'accumulated_eval_time': 4972.11691904068, 'accumulated_logging_time': 0.21974945068359375, 'global_step': 19068, 'preemption_count': 0}), (21452, {'train/accuracy': 0.6039170026779175, 'train/loss': 2.04196834564209, 'train/bleu': 28.499650206528244, 'validation/accuracy': 0.6211702227592468, 'validation/loss': 1.8938138484954834, 'validation/bleu': 25.326143244982028, 'validation/num_examples': 3000, 'test/accuracy': 0.6274592280387878, 'test/loss': 1.8478097915649414, 'test/bleu': 24.41130121879812, 'test/num_examples': 3003, 'score': 7591.907975435257, 'total_duration': 13117.867561101913, 'accumulated_submission_time': 7591.907975435257, 'accumulated_eval_time': 5525.0277309417725, 'accumulated_logging_time': 0.24841761589050293, 'global_step': 21452, 'preemption_count': 0}), (23837, {'train/accuracy': 0.6014580130577087, 'train/loss': 2.0570669174194336, 'train/bleu': 28.452707209093468, 'validation/accuracy': 0.6210214495658875, 'validation/loss': 1.8938277959823608, 'validation/bleu': 25.541055572678147, 'validation/num_examples': 3000, 'test/accuracy': 0.6271222233772278, 'test/loss': 1.8466172218322754, 'test/bleu': 24.527137407140724, 'test/num_examples': 3003, 'score': 8432.142497062683, 'total_duration': 14456.54249548912, 'accumulated_submission_time': 8432.142497062683, 'accumulated_eval_time': 6023.364975690842, 'accumulated_logging_time': 0.2760303020477295, 'global_step': 23837, 'preemption_count': 0}), (26220, {'train/accuracy': 0.6070371866226196, 'train/loss': 2.0028188228607178, 'train/bleu': 28.626233950948446, 'validation/accuracy': 0.6237616539001465, 'validation/loss': 1.874969720840454, 'validation/bleu': 25.473166487207465, 'validation/num_examples': 3000, 'test/accuracy': 0.6297716498374939, 'test/loss': 1.828667402267456, 'test/bleu': 24.472345680734758, 'test/num_examples': 3003, 'score': 9272.08500289917, 'total_duration': 15889.645047426224, 'accumulated_submission_time': 9272.08500289917, 'accumulated_eval_time': 6616.417924404144, 'accumulated_logging_time': 0.30393409729003906, 'global_step': 26220, 'preemption_count': 0}), (28604, {'train/accuracy': 0.6041083335876465, 'train/loss': 2.042267322540283, 'train/bleu': 28.460018032548266, 'validation/accuracy': 0.6240839958190918, 'validation/loss': 1.880605697631836, 'validation/bleu': 25.687265643742247, 'validation/num_examples': 3000, 'test/accuracy': 0.6322584748268127, 'test/loss': 1.824683427810669, 'test/bleu': 24.058762358703923, 'test/num_examples': 3003, 'score': 10112.033434867859, 'total_duration': 17247.554879665375, 'accumulated_submission_time': 10112.033434867859, 'accumulated_eval_time': 7134.273945808411, 'accumulated_logging_time': 0.3334333896636963, 'global_step': 28604, 'preemption_count': 0}), (30989, {'train/accuracy': 0.6057415008544922, 'train/loss': 2.0303850173950195, 'train/bleu': 28.31511913979044, 'validation/accuracy': 0.6273201704025269, 'validation/loss': 1.8572407960891724, 'validation/bleu': 25.756574842278006, 'validation/num_examples': 3000, 'test/accuracy': 0.6323397755622864, 'test/loss': 1.8168457746505737, 'test/bleu': 24.526306536985867, 'test/num_examples': 3003, 'score': 10952.269091129303, 'total_duration': 18601.201991081238, 'accumulated_submission_time': 10952.269091129303, 'accumulated_eval_time': 7647.579026222229, 'accumulated_logging_time': 0.3638780117034912, 'global_step': 30989, 'preemption_count': 0}), (33374, {'train/accuracy': 0.60844486951828, 'train/loss': 2.0008599758148193, 'train/bleu': 28.007482098976688, 'validation/accuracy': 0.6259438991546631, 'validation/loss': 1.856594443321228, 'validation/bleu': 25.505005927829227, 'validation/num_examples': 3000, 'test/accuracy': 0.6319795846939087, 'test/loss': 1.811674952507019, 'test/bleu': 24.446853717819966, 'test/num_examples': 3003, 'score': 11792.176321744919, 'total_duration': 20039.755168437958, 'accumulated_submission_time': 11792.176321744919, 'accumulated_eval_time': 8246.12324142456, 'accumulated_logging_time': 0.39251065254211426, 'global_step': 33374, 'preemption_count': 0}), (35758, {'train/accuracy': 0.6062001585960388, 'train/loss': 2.0129001140594482, 'train/bleu': 28.496158366728405, 'validation/accuracy': 0.6244683861732483, 'validation/loss': 1.8566988706588745, 'validation/bleu': 25.335470426237546, 'validation/num_examples': 3000, 'test/accuracy': 0.6337923407554626, 'test/loss': 1.8009341955184937, 'test/bleu': 24.817481823924734, 'test/num_examples': 3003, 'score': 12632.082436800003, 'total_duration': 21398.67829966545, 'accumulated_submission_time': 12632.082436800003, 'accumulated_eval_time': 8765.0341360569, 'accumulated_logging_time': 0.4228677749633789, 'global_step': 35758, 'preemption_count': 0}), (38143, {'train/accuracy': 0.6146321892738342, 'train/loss': 1.9567898511886597, 'train/bleu': 29.38206898813265, 'validation/accuracy': 0.628361701965332, 'validation/loss': 1.8436554670333862, 'validation/bleu': 25.92643316412883, 'validation/num_examples': 3000, 'test/accuracy': 0.6382081508636475, 'test/loss': 1.7820231914520264, 'test/bleu': 25.00556614291527, 'test/num_examples': 3003, 'score': 13472.220349311829, 'total_duration': 22704.81827187538, 'accumulated_submission_time': 13472.220349311829, 'accumulated_eval_time': 9230.926263809204, 'accumulated_logging_time': 0.4587104320526123, 'global_step': 38143, 'preemption_count': 0}), (40527, {'train/accuracy': 0.611473023891449, 'train/loss': 1.9678258895874023, 'train/bleu': 28.94678678261578, 'validation/accuracy': 0.6278533339500427, 'validation/loss': 1.835180640220642, 'validation/bleu': 25.68769018292687, 'validation/num_examples': 3000, 'test/accuracy': 0.6370925903320312, 'test/loss': 1.7789210081100464, 'test/bleu': 24.969637842391354, 'test/num_examples': 3003, 'score': 14312.36628293991, 'total_duration': 24036.061758756638, 'accumulated_submission_time': 14312.36628293991, 'accumulated_eval_time': 9721.91780424118, 'accumulated_logging_time': 0.48862719535827637, 'global_step': 40527, 'preemption_count': 0}), (42911, {'train/accuracy': 0.6131593585014343, 'train/loss': 1.972790241241455, 'train/bleu': 29.089999188219597, 'validation/accuracy': 0.631461501121521, 'validation/loss': 1.8121095895767212, 'validation/bleu': 26.098148061288406, 'validation/num_examples': 3000, 'test/accuracy': 0.6399396061897278, 'test/loss': 1.767079472541809, 'test/bleu': 25.392114429594827, 'test/num_examples': 3003, 'score': 15152.403260946274, 'total_duration': 25408.2639605999, 'accumulated_submission_time': 15152.403260946274, 'accumulated_eval_time': 10253.975495576859, 'accumulated_logging_time': 0.5196661949157715, 'global_step': 42911, 'preemption_count': 0}), (45296, {'train/accuracy': 0.6127845048904419, 'train/loss': 1.955664873123169, 'train/bleu': 28.965295301006385, 'validation/accuracy': 0.6328253746032715, 'validation/loss': 1.8045439720153809, 'validation/bleu': 25.967067669048774, 'validation/num_examples': 3000, 'test/accuracy': 0.6396374702453613, 'test/loss': 1.7606006860733032, 'test/bleu': 24.73479737642498, 'test/num_examples': 3003, 'score': 15992.33811044693, 'total_duration': 26808.432076215744, 'accumulated_submission_time': 15992.33811044693, 'accumulated_eval_time': 10814.103539466858, 'accumulated_logging_time': 0.5507168769836426, 'global_step': 45296, 'preemption_count': 0}), (47681, {'train/accuracy': 0.6143538355827332, 'train/loss': 1.9528555870056152, 'train/bleu': 29.107341526376448, 'validation/accuracy': 0.6314491033554077, 'validation/loss': 1.8058665990829468, 'validation/bleu': 25.943934302859418, 'validation/num_examples': 3000, 'test/accuracy': 0.6436465382575989, 'test/loss': 1.74813711643219, 'test/bleu': 25.42608692174256, 'test/num_examples': 3003, 'score': 16832.49195432663, 'total_duration': 28155.61889219284, 'accumulated_submission_time': 16832.49195432663, 'accumulated_eval_time': 11321.025541305542, 'accumulated_logging_time': 0.5871026515960693, 'global_step': 47681, 'preemption_count': 0}), (50064, {'train/accuracy': 0.6843953728675842, 'train/loss': 1.4730702638626099, 'train/bleu': 34.336430518788745, 'validation/accuracy': 0.635949969291687, 'validation/loss': 1.7930233478546143, 'validation/bleu': 26.441084450311465, 'validation/num_examples': 3000, 'test/accuracy': 0.6434489488601685, 'test/loss': 1.7467548847198486, 'test/bleu': 25.14636965746108, 'test/num_examples': 3003, 'score': 17672.648672819138, 'total_duration': 29476.605541706085, 'accumulated_submission_time': 17672.648672819138, 'accumulated_eval_time': 11801.747661113739, 'accumulated_logging_time': 0.6183607578277588, 'global_step': 50064, 'preemption_count': 0}), (52447, {'train/accuracy': 0.6166776418685913, 'train/loss': 1.935380220413208, 'train/bleu': 28.009326838866563, 'validation/accuracy': 0.6369170546531677, 'validation/loss': 1.7796038389205933, 'validation/bleu': 26.137517861350776, 'validation/num_examples': 3000, 'test/accuracy': 0.6453315019607544, 'test/loss': 1.7311948537826538, 'test/bleu': 25.591061740837908, 'test/num_examples': 3003, 'score': 18512.559674978256, 'total_duration': 30892.52356314659, 'accumulated_submission_time': 18512.559674978256, 'accumulated_eval_time': 12377.643058538437, 'accumulated_logging_time': 0.6505851745605469, 'global_step': 52447, 'preemption_count': 0}), (54831, {'train/accuracy': 0.6168438792228699, 'train/loss': 1.9355334043502808, 'train/bleu': 29.60037342386599, 'validation/accuracy': 0.639086902141571, 'validation/loss': 1.7692811489105225, 'validation/bleu': 26.58346724934059, 'validation/num_examples': 3000, 'test/accuracy': 0.6477136611938477, 'test/loss': 1.711084008216858, 'test/bleu': 25.65318269635807, 'test/num_examples': 3003, 'score': 19352.45453119278, 'total_duration': 32285.287123441696, 'accumulated_submission_time': 19352.45453119278, 'accumulated_eval_time': 12930.402215003967, 'accumulated_logging_time': 0.6834166049957275, 'global_step': 54831, 'preemption_count': 0}), (57216, {'train/accuracy': 0.6217406392097473, 'train/loss': 1.8886256217956543, 'train/bleu': 30.00592278859685, 'validation/accuracy': 0.6395828723907471, 'validation/loss': 1.764250636100769, 'validation/bleu': 26.488397964326836, 'validation/num_examples': 3000, 'test/accuracy': 0.6459241509437561, 'test/loss': 1.7121566534042358, 'test/bleu': 25.754728648283326, 'test/num_examples': 3003, 'score': 20192.690421819687, 'total_duration': 33723.60852479935, 'accumulated_submission_time': 20192.690421819687, 'accumulated_eval_time': 13528.37666606903, 'accumulated_logging_time': 0.7170102596282959, 'global_step': 57216, 'preemption_count': 0}), (59601, {'train/accuracy': 0.6205396056175232, 'train/loss': 1.9234768152236938, 'train/bleu': 29.974718274766825, 'validation/accuracy': 0.6435505747795105, 'validation/loss': 1.7510372400283813, 'validation/bleu': 26.64118576387656, 'validation/num_examples': 3000, 'test/accuracy': 0.6510720252990723, 'test/loss': 1.6877105236053467, 'test/bleu': 26.27622806513249, 'test/num_examples': 3003, 'score': 21033.113726854324, 'total_duration': 35132.104048252106, 'accumulated_submission_time': 21033.113726854324, 'accumulated_eval_time': 14096.32988357544, 'accumulated_logging_time': 0.7592217922210693, 'global_step': 59601, 'preemption_count': 0}), (61986, {'train/accuracy': 0.620170533657074, 'train/loss': 1.924217939376831, 'train/bleu': 29.704826058174017, 'validation/accuracy': 0.6417403221130371, 'validation/loss': 1.7392182350158691, 'validation/bleu': 24.938980730325106, 'validation/num_examples': 3000, 'test/accuracy': 0.6511649489402771, 'test/loss': 1.6913459300994873, 'test/bleu': 25.027113486174702, 'test/num_examples': 3003, 'score': 21873.043880462646, 'total_duration': 36801.8515625, 'accumulated_submission_time': 21873.043880462646, 'accumulated_eval_time': 14926.03948044777, 'accumulated_logging_time': 0.7936761379241943, 'global_step': 61986, 'preemption_count': 0}), (64370, {'train/accuracy': 0.6249828934669495, 'train/loss': 1.8825191259384155, 'train/bleu': 30.120804428595495, 'validation/accuracy': 0.6434885859489441, 'validation/loss': 1.73857581615448, 'validation/bleu': 27.00111898618707, 'validation/num_examples': 3000, 'test/accuracy': 0.6523153781890869, 'test/loss': 1.6770296096801758, 'test/bleu': 26.33216731822289, 'test/num_examples': 3003, 'score': 22713.07751083374, 'total_duration': 38225.61360192299, 'accumulated_submission_time': 22713.07751083374, 'accumulated_eval_time': 15509.657069206238, 'accumulated_logging_time': 0.826836109161377, 'global_step': 64370, 'preemption_count': 0}), (66755, {'train/accuracy': 0.6223474144935608, 'train/loss': 1.897873878479004, 'train/bleu': 29.606168008502756, 'validation/accuracy': 0.6448525190353394, 'validation/loss': 1.712667465209961, 'validation/bleu': 26.909955268306703, 'validation/num_examples': 3000, 'test/accuracy': 0.6545000672340393, 'test/loss': 1.6544857025146484, 'test/bleu': 26.066548963880994, 'test/num_examples': 3003, 'score': 23553.293513298035, 'total_duration': 39577.4663040638, 'accumulated_submission_time': 23553.293513298035, 'accumulated_eval_time': 16021.182207584381, 'accumulated_logging_time': 0.8614444732666016, 'global_step': 66755, 'preemption_count': 0}), (69140, {'train/accuracy': 0.6475690007209778, 'train/loss': 1.7080274820327759, 'train/bleu': 31.347088927075134, 'validation/accuracy': 0.6484978199005127, 'validation/loss': 1.7034063339233398, 'validation/bleu': 27.044254202767398, 'validation/num_examples': 3000, 'test/accuracy': 0.6583929061889648, 'test/loss': 1.6351416110992432, 'test/bleu': 26.70894910112787, 'test/num_examples': 3003, 'score': 24393.235889673233, 'total_duration': 40994.1373796463, 'accumulated_submission_time': 24393.235889673233, 'accumulated_eval_time': 16597.801033496857, 'accumulated_logging_time': 0.8966538906097412, 'global_step': 69140, 'preemption_count': 0}), (71525, {'train/accuracy': 0.6270906329154968, 'train/loss': 1.8595634698867798, 'train/bleu': 30.490072894391158, 'validation/accuracy': 0.6483118534088135, 'validation/loss': 1.6941295862197876, 'validation/bleu': 27.250290188229883, 'validation/num_examples': 3000, 'test/accuracy': 0.6592528223991394, 'test/loss': 1.626090168952942, 'test/bleu': 26.910900978321823, 'test/num_examples': 3003, 'score': 25233.399693965912, 'total_duration': 42368.234537124634, 'accumulated_submission_time': 25233.399693965912, 'accumulated_eval_time': 17131.623816490173, 'accumulated_logging_time': 0.9319117069244385, 'global_step': 71525, 'preemption_count': 0}), (73910, {'train/accuracy': 0.6320022940635681, 'train/loss': 1.8371987342834473, 'train/bleu': 30.474995803724568, 'validation/accuracy': 0.6498989462852478, 'validation/loss': 1.6905559301376343, 'validation/bleu': 27.39593869430475, 'validation/num_examples': 3000, 'test/accuracy': 0.658288300037384, 'test/loss': 1.6216771602630615, 'test/bleu': 26.540084120816232, 'test/num_examples': 3003, 'score': 26073.622700452805, 'total_duration': 43699.67828798294, 'accumulated_submission_time': 26073.622700452805, 'accumulated_eval_time': 17622.726563453674, 'accumulated_logging_time': 0.9732100963592529, 'global_step': 73910, 'preemption_count': 0}), (76295, {'train/accuracy': 0.6367040872573853, 'train/loss': 1.7800288200378418, 'train/bleu': 31.028093772921565, 'validation/accuracy': 0.6544989943504333, 'validation/loss': 1.6648274660110474, 'validation/bleu': 27.578319945120132, 'validation/num_examples': 3000, 'test/accuracy': 0.6646563410758972, 'test/loss': 1.5929890871047974, 'test/bleu': 27.065361558941973, 'test/num_examples': 3003, 'score': 26913.55896472931, 'total_duration': 45257.52446436882, 'accumulated_submission_time': 26913.55896472931, 'accumulated_eval_time': 18340.518897771835, 'accumulated_logging_time': 1.016244649887085, 'global_step': 76295, 'preemption_count': 0}), (78680, {'train/accuracy': 0.6351809501647949, 'train/loss': 1.8063995838165283, 'train/bleu': 30.51627421938524, 'validation/accuracy': 0.6567060351371765, 'validation/loss': 1.6517740488052368, 'validation/bleu': 27.850444978291744, 'validation/num_examples': 3000, 'test/accuracy': 0.6647725701332092, 'test/loss': 1.5881857872009277, 'test/bleu': 27.320551583794046, 'test/num_examples': 3003, 'score': 27753.546751499176, 'total_duration': 46657.269523620605, 'accumulated_submission_time': 27753.546751499176, 'accumulated_eval_time': 18900.159744501114, 'accumulated_logging_time': 1.0552592277526855, 'global_step': 78680, 'preemption_count': 0}), (81065, {'train/accuracy': 0.6338257789611816, 'train/loss': 1.8154295682907104, 'train/bleu': 30.37254047659617, 'validation/accuracy': 0.657177209854126, 'validation/loss': 1.6409265995025635, 'validation/bleu': 27.84527507937404, 'validation/num_examples': 3000, 'test/accuracy': 0.6671895980834961, 'test/loss': 1.572178840637207, 'test/bleu': 27.59701393455958, 'test/num_examples': 3003, 'score': 28593.618614435196, 'total_duration': 48083.00713443756, 'accumulated_submission_time': 28593.618614435196, 'accumulated_eval_time': 19485.708025693893, 'accumulated_logging_time': 1.0978131294250488, 'global_step': 81065, 'preemption_count': 0}), (83449, {'train/accuracy': 0.6420011520385742, 'train/loss': 1.7547723054885864, 'train/bleu': 31.104798039546385, 'validation/accuracy': 0.6573383808135986, 'validation/loss': 1.6285314559936523, 'validation/bleu': 28.031213845797343, 'validation/num_examples': 3000, 'test/accuracy': 0.6720237135887146, 'test/loss': 1.551444411277771, 'test/bleu': 28.043043785349038, 'test/num_examples': 3003, 'score': 29433.67586684227, 'total_duration': 49598.384884119034, 'accumulated_submission_time': 29433.67586684227, 'accumulated_eval_time': 20160.911412000656, 'accumulated_logging_time': 1.1356170177459717, 'global_step': 83449, 'preemption_count': 0}), (85834, {'train/accuracy': 0.6379316449165344, 'train/loss': 1.7899552583694458, 'train/bleu': 31.381088323768665, 'validation/accuracy': 0.6602894067764282, 'validation/loss': 1.6165797710418701, 'validation/bleu': 27.88452353796739, 'validation/num_examples': 3000, 'test/accuracy': 0.673267126083374, 'test/loss': 1.5379685163497925, 'test/bleu': 28.295872994291173, 'test/num_examples': 3003, 'score': 30273.843912363052, 'total_duration': 51023.77744102478, 'accumulated_submission_time': 30273.843912363052, 'accumulated_eval_time': 20746.017300128937, 'accumulated_logging_time': 1.178145170211792, 'global_step': 85834, 'preemption_count': 0}), (88220, {'train/accuracy': 0.6552841663360596, 'train/loss': 1.6534067392349243, 'train/bleu': 32.29373394632669, 'validation/accuracy': 0.6635503768920898, 'validation/loss': 1.599921703338623, 'validation/bleu': 28.25207895536669, 'validation/num_examples': 3000, 'test/accuracy': 0.6726628541946411, 'test/loss': 1.5305873155593872, 'test/bleu': 28.065620186470788, 'test/num_examples': 3003, 'score': 31114.019277334213, 'total_duration': 52334.566292762756, 'accumulated_submission_time': 31114.019277334213, 'accumulated_eval_time': 21216.517703533173, 'accumulated_logging_time': 1.216465711593628, 'global_step': 88220, 'preemption_count': 0}), (90603, {'train/accuracy': 0.6459221243858337, 'train/loss': 1.7254306077957153, 'train/bleu': 31.6283829980775, 'validation/accuracy': 0.6639595031738281, 'validation/loss': 1.5904563665390015, 'validation/bleu': 28.325600095389117, 'validation/num_examples': 3000, 'test/accuracy': 0.6796583533287048, 'test/loss': 1.5105689764022827, 'test/bleu': 28.84191883449241, 'test/num_examples': 3003, 'score': 31953.927941083908, 'total_duration': 53740.905447244644, 'accumulated_submission_time': 31953.927941083908, 'accumulated_eval_time': 21782.829572200775, 'accumulated_logging_time': 1.2534148693084717, 'global_step': 90603, 'preemption_count': 0}), (92989, {'train/accuracy': 0.6447259783744812, 'train/loss': 1.7305859327316284, 'train/bleu': 31.694070491390008, 'validation/accuracy': 0.6674808859825134, 'validation/loss': 1.5697031021118164, 'validation/bleu': 28.977459287447072, 'validation/num_examples': 3000, 'test/accuracy': 0.6814130544662476, 'test/loss': 1.4934310913085938, 'test/bleu': 28.439179032853332, 'test/num_examples': 3003, 'score': 32794.00373888016, 'total_duration': 55094.324538469315, 'accumulated_submission_time': 32794.00373888016, 'accumulated_eval_time': 22296.06116771698, 'accumulated_logging_time': 1.2910587787628174, 'global_step': 92989, 'preemption_count': 0}), (95375, {'train/accuracy': 0.6559216380119324, 'train/loss': 1.6479884386062622, 'train/bleu': 32.57261780221355, 'validation/accuracy': 0.6692167520523071, 'validation/loss': 1.5522282123565674, 'validation/bleu': 28.733379976232605, 'validation/num_examples': 3000, 'test/accuracy': 0.6810063719749451, 'test/loss': 1.4797627925872803, 'test/bleu': 28.512990721724428, 'test/num_examples': 3003, 'score': 33634.21266889572, 'total_duration': 56520.830107450485, 'accumulated_submission_time': 33634.21266889572, 'accumulated_eval_time': 22882.237318992615, 'accumulated_logging_time': 1.3375027179718018, 'global_step': 95375, 'preemption_count': 0}), (97760, {'train/accuracy': 0.6507320404052734, 'train/loss': 1.6990152597427368, 'train/bleu': 32.16376730049248, 'validation/accuracy': 0.6716469526290894, 'validation/loss': 1.5430229902267456, 'validation/bleu': 28.933022568504928, 'validation/num_examples': 3000, 'test/accuracy': 0.6845040917396545, 'test/loss': 1.4635298252105713, 'test/bleu': 28.750973036590516, 'test/num_examples': 3003, 'score': 34474.18307328224, 'total_duration': 57909.06037116051, 'accumulated_submission_time': 34474.18307328224, 'accumulated_eval_time': 23430.37916445732, 'accumulated_logging_time': 1.3787147998809814, 'global_step': 97760, 'preemption_count': 0}), (100145, {'train/accuracy': 0.6949189305305481, 'train/loss': 1.418954849243164, 'train/bleu': 35.23068643277161, 'validation/accuracy': 0.673246443271637, 'validation/loss': 1.528011441230774, 'validation/bleu': 29.01353235327636, 'validation/num_examples': 3000, 'test/accuracy': 0.6881529688835144, 'test/loss': 1.4465465545654297, 'test/bleu': 29.32560119300237, 'test/num_examples': 3003, 'score': 35314.18997120857, 'total_duration': 59360.596106767654, 'accumulated_submission_time': 35314.18997120857, 'accumulated_eval_time': 24041.794478416443, 'accumulated_logging_time': 1.417504072189331, 'global_step': 100145, 'preemption_count': 0}), (102530, {'train/accuracy': 0.6625471115112305, 'train/loss': 1.6055678129196167, 'train/bleu': 32.88659371950133, 'validation/accuracy': 0.6756394505500793, 'validation/loss': 1.5099588632583618, 'validation/bleu': 29.382846968354542, 'validation/num_examples': 3000, 'test/accuracy': 0.6910231709480286, 'test/loss': 1.4268368482589722, 'test/bleu': 29.189676258834265, 'test/num_examples': 3003, 'score': 36154.3091506958, 'total_duration': 60769.31097340584, 'accumulated_submission_time': 36154.3091506958, 'accumulated_eval_time': 24610.273792028427, 'accumulated_logging_time': 1.4581010341644287, 'global_step': 102530, 'preemption_count': 0}), (104916, {'train/accuracy': 0.6604390144348145, 'train/loss': 1.6284128427505493, 'train/bleu': 32.50623094459274, 'validation/accuracy': 0.6783796548843384, 'validation/loss': 1.500727891921997, 'validation/bleu': 29.517135048861, 'validation/num_examples': 3000, 'test/accuracy': 0.6924060583114624, 'test/loss': 1.420128583908081, 'test/bleu': 29.2301595025155, 'test/num_examples': 3003, 'score': 36994.25947546959, 'total_duration': 62180.81967806816, 'accumulated_submission_time': 36994.25947546959, 'accumulated_eval_time': 25181.712441921234, 'accumulated_logging_time': 1.5033392906188965, 'global_step': 104916, 'preemption_count': 0}), (107300, {'train/accuracy': 0.6785178780555725, 'train/loss': 1.5146962404251099, 'train/bleu': 33.844285592149795, 'validation/accuracy': 0.6805495023727417, 'validation/loss': 1.4880130290985107, 'validation/bleu': 29.774191434883186, 'validation/num_examples': 3000, 'test/accuracy': 0.693591296672821, 'test/loss': 1.4022724628448486, 'test/bleu': 29.437192795959454, 'test/num_examples': 3003, 'score': 37834.20816755295, 'total_duration': 63637.71153593063, 'accumulated_submission_time': 37834.20816755295, 'accumulated_eval_time': 25798.535117149353, 'accumulated_logging_time': 1.5452971458435059, 'global_step': 107300, 'preemption_count': 0}), (109684, {'train/accuracy': 0.6720339059829712, 'train/loss': 1.5570589303970337, 'train/bleu': 32.979392765729266, 'validation/accuracy': 0.6831905245780945, 'validation/loss': 1.4739441871643066, 'validation/bleu': 30.149647077355855, 'validation/num_examples': 3000, 'test/accuracy': 0.6969845294952393, 'test/loss': 1.3863232135772705, 'test/bleu': 29.99551478418762, 'test/num_examples': 3003, 'score': 38674.33937954903, 'total_duration': 65013.49522304535, 'accumulated_submission_time': 38674.33937954903, 'accumulated_eval_time': 26334.07120013237, 'accumulated_logging_time': 1.5848398208618164, 'global_step': 109684, 'preemption_count': 0}), (112068, {'train/accuracy': 0.6718950271606445, 'train/loss': 1.5602552890777588, 'train/bleu': 33.424706492563324, 'validation/accuracy': 0.6845420598983765, 'validation/loss': 1.466409683227539, 'validation/bleu': 30.091905902654414, 'validation/num_examples': 3000, 'test/accuracy': 0.6990064382553101, 'test/loss': 1.3751919269561768, 'test/bleu': 29.9190960808925, 'test/num_examples': 3003, 'score': 39514.344621658325, 'total_duration': 66358.30993127823, 'accumulated_submission_time': 39514.344621658325, 'accumulated_eval_time': 26838.75919866562, 'accumulated_logging_time': 1.627079963684082, 'global_step': 112068, 'preemption_count': 0}), (114454, {'train/accuracy': 0.68068528175354, 'train/loss': 1.5048444271087646, 'train/bleu': 34.31321273170774, 'validation/accuracy': 0.6864762902259827, 'validation/loss': 1.4497908353805542, 'validation/bleu': 30.02917097359945, 'validation/num_examples': 3000, 'test/accuracy': 0.7002847194671631, 'test/loss': 1.361093282699585, 'test/bleu': 30.209978551978672, 'test/num_examples': 3003, 'score': 40354.538182258606, 'total_duration': 67835.46319293976, 'accumulated_submission_time': 40354.538182258606, 'accumulated_eval_time': 27475.601682901382, 'accumulated_logging_time': 1.6685771942138672, 'global_step': 114454, 'preemption_count': 0}), (116839, {'train/accuracy': 0.6848151087760925, 'train/loss': 1.493071436882019, 'train/bleu': 33.9288495000838, 'validation/accuracy': 0.6874682307243347, 'validation/loss': 1.4417047500610352, 'validation/bleu': 30.30958769276966, 'validation/num_examples': 3000, 'test/accuracy': 0.7036197781562805, 'test/loss': 1.3505662679672241, 'test/bleu': 30.64315992147018, 'test/num_examples': 3003, 'score': 41194.56631875038, 'total_duration': 69211.07856273651, 'accumulated_submission_time': 41194.56631875038, 'accumulated_eval_time': 28011.064224004745, 'accumulated_logging_time': 1.7157025337219238, 'global_step': 116839, 'preemption_count': 0}), (119225, {'train/accuracy': 0.6958141326904297, 'train/loss': 1.417291522026062, 'train/bleu': 35.5492473489212, 'validation/accuracy': 0.6902580261230469, 'validation/loss': 1.4339897632598877, 'validation/bleu': 30.490945218469427, 'validation/num_examples': 3000, 'test/accuracy': 0.7048050761222839, 'test/loss': 1.3403512239456177, 'test/bleu': 30.319124766205878, 'test/num_examples': 3003, 'score': 42034.72462916374, 'total_duration': 70620.8813958168, 'accumulated_submission_time': 42034.72462916374, 'accumulated_eval_time': 28580.59091734886, 'accumulated_logging_time': 1.7584803104400635, 'global_step': 119225, 'preemption_count': 0}), (121610, {'train/accuracy': 0.6900114417076111, 'train/loss': 1.4532064199447632, 'train/bleu': 34.7617892280219, 'validation/accuracy': 0.6903696060180664, 'validation/loss': 1.4234853982925415, 'validation/bleu': 30.629143421025496, 'validation/num_examples': 3000, 'test/accuracy': 0.7055488228797913, 'test/loss': 1.332283616065979, 'test/bleu': 30.548803892158418, 'test/num_examples': 3003, 'score': 42874.72810292244, 'total_duration': 72061.10469961166, 'accumulated_submission_time': 42874.72810292244, 'accumulated_eval_time': 29180.690301418304, 'accumulated_logging_time': 1.8024253845214844, 'global_step': 121610, 'preemption_count': 0}), (123995, {'train/accuracy': 0.6892982721328735, 'train/loss': 1.4594749212265015, 'train/bleu': 34.6818584249282, 'validation/accuracy': 0.6919319033622742, 'validation/loss': 1.4199069738388062, 'validation/bleu': 30.847756824941126, 'validation/num_examples': 3000, 'test/accuracy': 0.7058973908424377, 'test/loss': 1.32646906375885, 'test/bleu': 30.809119513639406, 'test/num_examples': 3003, 'score': 43714.89873600006, 'total_duration': 73438.41144561768, 'accumulated_submission_time': 43714.89873600006, 'accumulated_eval_time': 29717.70730495453, 'accumulated_logging_time': 1.845533847808838, 'global_step': 123995, 'preemption_count': 0}), (126380, {'train/accuracy': 0.697314977645874, 'train/loss': 1.4237242937088013, 'train/bleu': 36.055260217867456, 'validation/accuracy': 0.6940025687217712, 'validation/loss': 1.413859486579895, 'validation/bleu': 30.82466800846016, 'validation/num_examples': 3000, 'test/accuracy': 0.7090000510215759, 'test/loss': 1.3189260959625244, 'test/bleu': 30.86978933852004, 'test/num_examples': 3003, 'score': 44555.023113012314, 'total_duration': 74843.19479346275, 'accumulated_submission_time': 44555.023113012314, 'accumulated_eval_time': 30282.248594284058, 'accumulated_logging_time': 1.8876619338989258, 'global_step': 126380, 'preemption_count': 0}), (128765, {'train/accuracy': 0.7001673579216003, 'train/loss': 1.3921157121658325, 'train/bleu': 36.054005536466065, 'validation/accuracy': 0.6933701634407043, 'validation/loss': 1.4130243062973022, 'validation/bleu': 30.574519850326517, 'validation/num_examples': 3000, 'test/accuracy': 0.7091976404190063, 'test/loss': 1.3158727884292603, 'test/bleu': 30.909184782342955, 'test/num_examples': 3003, 'score': 45395.007142305374, 'total_duration': 76256.74099755287, 'accumulated_submission_time': 45395.007142305374, 'accumulated_eval_time': 30855.691635131836, 'accumulated_logging_time': 1.931976318359375, 'global_step': 128765, 'preemption_count': 0}), (131150, {'train/accuracy': 0.6955586671829224, 'train/loss': 1.4250540733337402, 'train/bleu': 36.228446900630956, 'validation/accuracy': 0.6936181783676147, 'validation/loss': 1.4108177423477173, 'validation/bleu': 30.819000657109928, 'validation/num_examples': 3000, 'test/accuracy': 0.7093021869659424, 'test/loss': 1.3146555423736572, 'test/bleu': 30.79200209401173, 'test/num_examples': 3003, 'score': 46235.168941020966, 'total_duration': 77636.98960375786, 'accumulated_submission_time': 46235.168941020966, 'accumulated_eval_time': 31395.656549215317, 'accumulated_logging_time': 1.9757180213928223, 'global_step': 131150, 'preemption_count': 0}), (133333, {'train/accuracy': 0.6961641311645508, 'train/loss': 1.423098087310791, 'train/bleu': 36.12654870431947, 'validation/accuracy': 0.6939653754234314, 'validation/loss': 1.4107656478881836, 'validation/bleu': 30.919892623504534, 'validation/num_examples': 3000, 'test/accuracy': 0.70927894115448, 'test/loss': 1.3146251440048218, 'test/bleu': 30.84290524056852, 'test/num_examples': 3003, 'score': 47003.91788029671, 'total_duration': 78948.11796474457, 'accumulated_submission_time': 47003.91788029671, 'accumulated_eval_time': 31937.92197227478, 'accumulated_logging_time': 2.0215542316436768, 'global_step': 133333, 'preemption_count': 0})], 'global_step': 133333}
I0210 01:45:06.848304 140699726837568 submission_runner.py:586] Timing: 47003.91788029671
I0210 01:45:06.848354 140699726837568 submission_runner.py:588] Total number of evals: 57
I0210 01:45:06.848395 140699726837568 submission_runner.py:589] ====================
I0210 01:45:06.848437 140699726837568 submission_runner.py:542] Using RNG seed 529981238
I0210 01:45:06.849932 140699726837568 submission_runner.py:551] --- Tuning run 5/5 ---
I0210 01:45:06.850054 140699726837568 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_1/wmt_jax/trial_5.
I0210 01:45:06.850532 140699726837568 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_1/wmt_jax/trial_5/hparams.json.
I0210 01:45:06.851432 140699726837568 submission_runner.py:206] Initializing dataset.
I0210 01:45:06.854022 140699726837568 dataset_info.py:578] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0210 01:45:06.857013 140699726837568 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0210 01:45:06.894696 140699726837568 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0210 01:45:07.605375 140699726837568 submission_runner.py:213] Initializing model.
I0210 01:45:14.237083 140699726837568 submission_runner.py:255] Initializing optimizer.
I0210 01:45:15.050936 140699726837568 submission_runner.py:262] Initializing metrics bundle.
I0210 01:45:15.051100 140699726837568 submission_runner.py:280] Initializing checkpoint and logger.
I0210 01:45:15.052283 140699726837568 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_1/wmt_jax/trial_5 with prefix checkpoint_
I0210 01:45:15.052415 140699726837568 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_1/wmt_jax/trial_5/meta_data_0.json.
I0210 01:45:15.052642 140699726837568 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0210 01:45:15.052725 140699726837568 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0210 01:45:15.622872 140699726837568 logger_utils.py:220] Unable to record git information. Continuing without it.
I0210 01:45:16.180132 140699726837568 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_1/wmt_jax/trial_5/flags_0.json.
I0210 01:45:16.183784 140699726837568 submission_runner.py:314] Starting training loop.
I0210 01:45:47.507957 140529929012992 logging_writer.py:48] [0] global_step=0, grad_norm=5.070984363555908, loss=11.151057243347168
I0210 01:45:47.520029 140699726837568 spec.py:321] Evaluating on the training split.
I0210 01:45:50.246088 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 01:50:39.631900 140699726837568 spec.py:333] Evaluating on the validation split.
I0210 01:50:42.382828 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 01:55:31.798527 140699726837568 spec.py:349] Evaluating on the test split.
I0210 01:55:34.542536 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 02:00:24.015362 140699726837568 submission_runner.py:408] Time since start: 907.83s, 	Step: 1, 	{'train/accuracy': 0.0005905735306441784, 'train/loss': 11.173606872558594, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.208685874938965, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.19086742401123, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 31.33621072769165, 'total_duration': 907.8314986228943, 'accumulated_submission_time': 31.33621072769165, 'accumulated_eval_time': 876.4952499866486, 'accumulated_logging_time': 0}
I0210 02:00:24.024641 140529937405696 logging_writer.py:48] [1] accumulated_eval_time=876.495250, accumulated_logging_time=0, accumulated_submission_time=31.336211, global_step=1, preemption_count=0, score=31.336211, test/accuracy=0.000709, test/bleu=0.000000, test/loss=11.190867, test/num_examples=3003, total_duration=907.831499, train/accuracy=0.000591, train/bleu=0.000000, train/loss=11.173607, validation/accuracy=0.000484, validation/bleu=0.000000, validation/loss=11.208686, validation/num_examples=3000
I0210 02:01:00.070791 140529929012992 logging_writer.py:48] [100] global_step=100, grad_norm=0.17051535844802856, loss=8.175846099853516
I0210 02:01:36.151451 140529937405696 logging_writer.py:48] [200] global_step=200, grad_norm=0.3092207610607147, loss=7.413442134857178
I0210 02:02:12.363259 140529929012992 logging_writer.py:48] [300] global_step=300, grad_norm=0.43103235960006714, loss=6.79016637802124
I0210 02:02:48.572439 140529937405696 logging_writer.py:48] [400] global_step=400, grad_norm=0.4988194704055786, loss=6.3037495613098145
I0210 02:03:24.756243 140529929012992 logging_writer.py:48] [500] global_step=500, grad_norm=0.42326468229293823, loss=5.836796283721924
I0210 02:04:00.913256 140529937405696 logging_writer.py:48] [600] global_step=600, grad_norm=0.4175631105899811, loss=5.529267311096191
I0210 02:04:37.072595 140529929012992 logging_writer.py:48] [700] global_step=700, grad_norm=0.6128252744674683, loss=5.361035346984863
I0210 02:05:13.227154 140529937405696 logging_writer.py:48] [800] global_step=800, grad_norm=0.5135059952735901, loss=4.984952449798584
I0210 02:05:49.396011 140529929012992 logging_writer.py:48] [900] global_step=900, grad_norm=0.403224915266037, loss=4.8841753005981445
I0210 02:06:25.557446 140529937405696 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.5988842248916626, loss=4.588549613952637
I0210 02:07:01.671830 140529929012992 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.476958304643631, loss=4.260438919067383
I0210 02:07:37.828612 140529937405696 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.4557066857814789, loss=4.094794750213623
I0210 02:08:13.949321 140529929012992 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.5648809671401978, loss=3.9599010944366455
I0210 02:08:50.118450 140529937405696 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.7199504375457764, loss=3.7761683464050293
I0210 02:09:26.274233 140529929012992 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.5615197420120239, loss=3.6272003650665283
I0210 02:10:02.411352 140529937405696 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.44908031821250916, loss=3.469597339630127
I0210 02:10:38.512226 140529929012992 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.5819913148880005, loss=3.400522232055664
I0210 02:11:14.648911 140529937405696 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.43784135580062866, loss=3.2916994094848633
I0210 02:11:50.808808 140529929012992 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.36867624521255493, loss=3.266019821166992
I0210 02:12:26.988439 140529937405696 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.46700549125671387, loss=3.265847682952881
I0210 02:13:03.111103 140529929012992 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.4306173324584961, loss=3.134432792663574
I0210 02:13:39.260373 140529937405696 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.3668747544288635, loss=3.0881896018981934
I0210 02:14:15.434651 140529929012992 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.5213031768798828, loss=3.0543956756591797
I0210 02:14:24.200083 140699726837568 spec.py:321] Evaluating on the training split.
I0210 02:14:27.240044 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 02:17:16.261405 140699726837568 spec.py:333] Evaluating on the validation split.
I0210 02:17:18.994763 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 02:20:04.518681 140699726837568 spec.py:349] Evaluating on the test split.
I0210 02:20:07.249345 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 02:22:48.030839 140699726837568 submission_runner.py:408] Time since start: 2251.85s, 	Step: 2326, 	{'train/accuracy': 0.5102726817131042, 'train/loss': 2.8770992755889893, 'train/bleu': 22.340060181734597, 'validation/accuracy': 0.5084871649742126, 'validation/loss': 2.875525951385498, 'validation/bleu': 18.049709526813007, 'validation/num_examples': 3000, 'test/accuracy': 0.5088954567909241, 'test/loss': 2.922269582748413, 'test/bleu': 16.326466481940816, 'test/num_examples': 3003, 'score': 871.4278464317322, 'total_duration': 2251.8469796180725, 'accumulated_submission_time': 871.4278464317322, 'accumulated_eval_time': 1380.3259477615356, 'accumulated_logging_time': 0.02058124542236328}
I0210 02:22:48.045496 140529937405696 logging_writer.py:48] [2326] accumulated_eval_time=1380.325948, accumulated_logging_time=0.020581, accumulated_submission_time=871.427846, global_step=2326, preemption_count=0, score=871.427846, test/accuracy=0.508895, test/bleu=16.326466, test/loss=2.922270, test/num_examples=3003, total_duration=2251.846980, train/accuracy=0.510273, train/bleu=22.340060, train/loss=2.877099, validation/accuracy=0.508487, validation/bleu=18.049710, validation/loss=2.875526, validation/num_examples=3000
I0210 02:23:15.015923 140529929012992 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.36342653632164, loss=2.974353790283203
I0210 02:23:51.013409 140529937405696 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.32595929503440857, loss=2.966503381729126
I0210 02:24:27.142695 140529929012992 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.34682226181030273, loss=2.8014068603515625
I0210 02:25:03.240058 140529937405696 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.3596723675727844, loss=2.8108580112457275
I0210 02:25:39.366245 140529929012992 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.29393985867500305, loss=2.7600018978118896
I0210 02:26:15.521157 140529937405696 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.2550356388092041, loss=2.6971724033355713
I0210 02:26:51.674239 140529929012992 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.27004867792129517, loss=2.7565670013427734
I0210 02:27:27.847805 140529937405696 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.2824717164039612, loss=2.633340835571289
I0210 02:28:03.974807 140529929012992 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.3221835196018219, loss=2.6205246448516846
I0210 02:28:40.165541 140529937405696 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.2616874873638153, loss=2.5980358123779297
I0210 02:29:16.319178 140529929012992 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.22213274240493774, loss=2.6171083450317383
I0210 02:29:52.470415 140529937405696 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.2963797450065613, loss=2.5511562824249268
I0210 02:30:28.562077 140529929012992 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.20662355422973633, loss=2.5642542839050293
I0210 02:31:04.693899 140529937405696 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.21513433754444122, loss=2.440582036972046
I0210 02:31:40.787584 140529929012992 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.19283199310302734, loss=2.5334651470184326
I0210 02:32:16.923991 140529937405696 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.21484452486038208, loss=2.500131130218506
I0210 02:32:53.026009 140529929012992 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.20695380866527557, loss=2.4875731468200684
I0210 02:33:29.134516 140529937405696 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.1899617612361908, loss=2.3834939002990723
I0210 02:34:05.244234 140529929012992 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.21068406105041504, loss=2.4106204509735107
I0210 02:34:41.369202 140529937405696 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.19723203778266907, loss=2.3744540214538574
I0210 02:35:17.493263 140529929012992 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.19219087064266205, loss=2.3879706859588623
I0210 02:35:53.655819 140529937405696 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.17308390140533447, loss=2.3189752101898193
I0210 02:36:29.798135 140529929012992 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.2046860307455063, loss=2.3631153106689453
I0210 02:36:48.332619 140699726837568 spec.py:321] Evaluating on the training split.
I0210 02:36:51.391759 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 02:39:44.180201 140699726837568 spec.py:333] Evaluating on the validation split.
I0210 02:39:46.912134 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 02:42:22.757580 140699726837568 spec.py:349] Evaluating on the test split.
I0210 02:42:25.490266 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 02:44:49.735199 140699726837568 submission_runner.py:408] Time since start: 3573.55s, 	Step: 4653, 	{'train/accuracy': 0.5776917338371277, 'train/loss': 2.2594664096832275, 'train/bleu': 27.71633074707988, 'validation/accuracy': 0.5888209342956543, 'validation/loss': 2.168548107147217, 'validation/bleu': 23.508677243219108, 'validation/num_examples': 3000, 'test/accuracy': 0.5914473533630371, 'test/loss': 2.134783983230591, 'test/bleu': 22.110663990581415, 'test/num_examples': 3003, 'score': 1711.6315701007843, 'total_duration': 3573.551340818405, 'accumulated_submission_time': 1711.6315701007843, 'accumulated_eval_time': 1861.7284874916077, 'accumulated_logging_time': 0.04536008834838867}
I0210 02:44:49.750815 140529937405696 logging_writer.py:48] [4653] accumulated_eval_time=1861.728487, accumulated_logging_time=0.045360, accumulated_submission_time=1711.631570, global_step=4653, preemption_count=0, score=1711.631570, test/accuracy=0.591447, test/bleu=22.110664, test/loss=2.134784, test/num_examples=3003, total_duration=3573.551341, train/accuracy=0.577692, train/bleu=27.716331, train/loss=2.259466, validation/accuracy=0.588821, validation/bleu=23.508677, validation/loss=2.168548, validation/num_examples=3000
I0210 02:45:07.037293 140529929012992 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.16910776495933533, loss=2.2099595069885254
I0210 02:45:43.041527 140529937405696 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.1659485548734665, loss=2.2872283458709717
I0210 02:46:19.117105 140529929012992 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.16478367149829865, loss=2.3121144771575928
I0210 02:46:55.249195 140529937405696 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.1692609190940857, loss=2.329301357269287
I0210 02:47:31.330303 140529929012992 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.17939835786819458, loss=2.3069608211517334
I0210 02:48:07.408850 140529937405696 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.16439685225486755, loss=2.2808003425598145
I0210 02:48:43.485785 140529929012992 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.17661713063716888, loss=2.235220432281494
I0210 02:49:19.644277 140529937405696 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.16243314743041992, loss=2.2494561672210693
I0210 02:49:55.743474 140529929012992 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.18624766170978546, loss=2.1724462509155273
I0210 02:50:31.859890 140529937405696 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.16855460405349731, loss=2.25785493850708
I0210 02:51:07.986652 140529929012992 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.16008414328098297, loss=2.2052104473114014
I0210 02:51:44.101985 140529937405696 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.1532440483570099, loss=2.2850162982940674
I0210 02:52:20.206704 140529929012992 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.15623177587985992, loss=2.2348732948303223
I0210 02:52:56.317013 140529937405696 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.17860211431980133, loss=2.1671853065490723
I0210 02:53:32.434578 140529929012992 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.1748354434967041, loss=2.182267189025879
I0210 02:54:08.543048 140529937405696 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.16094642877578735, loss=2.244262933731079
I0210 02:54:44.651899 140529929012992 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.15026211738586426, loss=2.208944797515869
I0210 02:55:20.740064 140529937405696 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.15895214676856995, loss=2.221987247467041
I0210 02:55:56.869579 140529929012992 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.14836642146110535, loss=2.183750867843628
I0210 02:56:33.001496 140529937405696 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.15252608060836792, loss=2.09362530708313
I0210 02:57:09.131231 140529929012992 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.1732713133096695, loss=2.1080501079559326
I0210 02:57:45.227156 140529937405696 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.18554748594760895, loss=2.1849350929260254
I0210 02:58:21.310039 140529929012992 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.1840573400259018, loss=2.1079516410827637
I0210 02:58:49.908341 140699726837568 spec.py:321] Evaluating on the training split.
I0210 02:58:52.949750 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 03:01:23.002435 140699726837568 spec.py:333] Evaluating on the validation split.
I0210 03:01:25.741094 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 03:03:57.826955 140699726837568 spec.py:349] Evaluating on the test split.
I0210 03:04:00.563008 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 03:06:21.594192 140699726837568 submission_runner.py:408] Time since start: 4865.41s, 	Step: 6981, 	{'train/accuracy': 0.6117618680000305, 'train/loss': 1.9756730794906616, 'train/bleu': 29.191234059510283, 'validation/accuracy': 0.6168429255485535, 'validation/loss': 1.9347010850906372, 'validation/bleu': 25.35601131488413, 'validation/num_examples': 3000, 'test/accuracy': 0.621997594833374, 'test/loss': 1.889957070350647, 'test/bleu': 24.20917412371607, 'test/num_examples': 3003, 'score': 2551.706431388855, 'total_duration': 4865.410325527191, 'accumulated_submission_time': 2551.706431388855, 'accumulated_eval_time': 2313.41427898407, 'accumulated_logging_time': 0.07245087623596191}
I0210 03:06:21.609731 140529937405696 logging_writer.py:48] [6981] accumulated_eval_time=2313.414279, accumulated_logging_time=0.072451, accumulated_submission_time=2551.706431, global_step=6981, preemption_count=0, score=2551.706431, test/accuracy=0.621998, test/bleu=24.209174, test/loss=1.889957, test/num_examples=3003, total_duration=4865.410326, train/accuracy=0.611762, train/bleu=29.191234, train/loss=1.975673, validation/accuracy=0.616843, validation/bleu=25.356011, validation/loss=1.934701, validation/num_examples=3000
I0210 03:06:28.813638 140529929012992 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.1748182475566864, loss=2.1866614818573
I0210 03:07:04.774795 140529937405696 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.1761217564344406, loss=2.21593976020813
I0210 03:07:40.776925 140529929012992 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.14670120179653168, loss=2.134803056716919
I0210 03:08:16.839824 140529937405696 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.17321372032165527, loss=2.0827977657318115
I0210 03:08:52.931489 140529929012992 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.15234576165676117, loss=2.1269657611846924
I0210 03:09:29.046940 140529937405696 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.15725566446781158, loss=2.10703444480896
I0210 03:10:05.185494 140529929012992 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.16088253259658813, loss=2.189812421798706
I0210 03:10:41.318053 140529937405696 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.2059328854084015, loss=2.050563097000122
I0210 03:11:17.464507 140529929012992 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.22697028517723083, loss=1.9785172939300537
I0210 03:11:53.608011 140529937405696 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.16223102807998657, loss=2.135852336883545
I0210 03:12:29.731611 140529929012992 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.2306804209947586, loss=2.069878339767456
I0210 03:13:05.819878 140529937405696 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.16053110361099243, loss=2.0045883655548096
I0210 03:13:41.906709 140529929012992 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.3059137165546417, loss=1.9854543209075928
I0210 03:14:18.036033 140529937405696 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.17978478968143463, loss=2.1443958282470703
I0210 03:14:54.179559 140529929012992 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.1659790426492691, loss=2.079263687133789
I0210 03:15:30.475846 140529937405696 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.17732736468315125, loss=2.040019989013672
I0210 03:16:06.650993 140529929012992 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.16704031825065613, loss=2.089723825454712
I0210 03:16:42.835741 140529937405696 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.1811896413564682, loss=2.0459723472595215
I0210 03:17:19.031248 140529929012992 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.16240081191062927, loss=2.0166420936584473
I0210 03:17:55.117824 140529937405696 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.16506347060203552, loss=2.042754650115967
I0210 03:18:31.280572 140529929012992 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.17272676527500153, loss=2.1017985343933105
I0210 03:19:07.422723 140529937405696 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.20888270437717438, loss=2.0517759323120117
I0210 03:19:43.551668 140529929012992 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.27239343523979187, loss=2.059960126876831
I0210 03:20:19.690279 140529937405696 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.23550856113433838, loss=1.9933805465698242
I0210 03:20:21.944699 140699726837568 spec.py:321] Evaluating on the training split.
I0210 03:20:24.975121 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 03:23:08.449928 140699726837568 spec.py:333] Evaluating on the validation split.
I0210 03:23:11.171658 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 03:25:45.713608 140699726837568 spec.py:349] Evaluating on the test split.
I0210 03:25:48.449660 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 03:28:11.276072 140699726837568 submission_runner.py:408] Time since start: 6175.09s, 	Step: 9308, 	{'train/accuracy': 0.6131592392921448, 'train/loss': 1.9403873682022095, 'train/bleu': 29.211597013206095, 'validation/accuracy': 0.6278781294822693, 'validation/loss': 1.825577735900879, 'validation/bleu': 26.295598101877676, 'validation/num_examples': 3000, 'test/accuracy': 0.6395677328109741, 'test/loss': 1.7598669528961182, 'test/bleu': 25.220842753067227, 'test/num_examples': 3003, 'score': 3391.9585361480713, 'total_duration': 6175.092213869095, 'accumulated_submission_time': 3391.9585361480713, 'accumulated_eval_time': 2782.7456068992615, 'accumulated_logging_time': 0.09807252883911133}
I0210 03:28:11.291869 140529929012992 logging_writer.py:48] [9308] accumulated_eval_time=2782.745607, accumulated_logging_time=0.098073, accumulated_submission_time=3391.958536, global_step=9308, preemption_count=0, score=3391.958536, test/accuracy=0.639568, test/bleu=25.220843, test/loss=1.759867, test/num_examples=3003, total_duration=6175.092214, train/accuracy=0.613159, train/bleu=29.211597, train/loss=1.940387, validation/accuracy=0.627878, validation/bleu=26.295598, validation/loss=1.825578, validation/num_examples=3000
I0210 03:28:44.780300 140529937405696 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.17705300450325012, loss=2.0302815437316895
I0210 03:29:20.850670 140529929012992 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.17073142528533936, loss=1.9994395971298218
I0210 03:29:56.889171 140529937405696 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.18084348738193512, loss=1.9566296339035034
I0210 03:30:32.986640 140529929012992 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.184726744890213, loss=1.9385499954223633
I0210 03:31:09.072430 140529937405696 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.1484643965959549, loss=1.9810309410095215
I0210 03:31:45.262109 140529929012992 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.22402673959732056, loss=2.1063828468322754
I0210 03:32:21.429142 140529937405696 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.1679266095161438, loss=2.02577543258667
I0210 03:32:57.594277 140529929012992 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.17984357476234436, loss=1.9891852140426636
I0210 03:33:33.732051 140529937405696 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.1894044429063797, loss=1.964367389678955
I0210 03:34:09.861604 140529929012992 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.2695261240005493, loss=1.9811033010482788
I0210 03:34:45.988003 140529937405696 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.28498542308807373, loss=1.9844111204147339
I0210 03:35:22.185990 140529929012992 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.2052018940448761, loss=1.9328447580337524
I0210 03:35:58.348095 140529937405696 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.20295463502407074, loss=2.039377212524414
I0210 03:36:34.500136 140529929012992 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.2084333598613739, loss=1.9808722734451294
I0210 03:37:10.648098 140529937405696 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.22004175186157227, loss=1.9183237552642822
I0210 03:37:46.723571 140529929012992 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.26619815826416016, loss=2.094435453414917
I0210 03:38:22.872843 140529937405696 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.17940078675746918, loss=2.0415399074554443
I0210 03:38:58.969405 140529929012992 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.19660137593746185, loss=1.9974615573883057
I0210 03:39:35.103456 140529937405696 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.20768561959266663, loss=1.880592942237854
I0210 03:40:11.212730 140529929012992 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.2290785312652588, loss=2.0451509952545166
I0210 03:40:47.317786 140529937405696 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.1974937468767166, loss=1.9057356119155884
I0210 03:41:23.447830 140529929012992 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.2086630016565323, loss=2.0824925899505615
I0210 03:41:59.575093 140529937405696 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.19359412789344788, loss=1.9541171789169312
I0210 03:42:11.562905 140699726837568 spec.py:321] Evaluating on the training split.
I0210 03:42:14.591034 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 03:45:08.940161 140699726837568 spec.py:333] Evaluating on the validation split.
I0210 03:45:11.672810 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 03:47:42.830751 140699726837568 spec.py:349] Evaluating on the test split.
I0210 03:47:45.552631 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 03:50:06.934144 140699726837568 submission_runner.py:408] Time since start: 7490.75s, 	Step: 11635, 	{'train/accuracy': 0.6183011531829834, 'train/loss': 1.9032855033874512, 'train/bleu': 30.174969727349268, 'validation/accuracy': 0.637698233127594, 'validation/loss': 1.7577568292617798, 'validation/bleu': 26.727296923815302, 'validation/num_examples': 3000, 'test/accuracy': 0.6465632319450378, 'test/loss': 1.6927350759506226, 'test/bleu': 25.978056856540427, 'test/num_examples': 3003, 'score': 4232.144702672958, 'total_duration': 7490.7502863407135, 'accumulated_submission_time': 4232.144702672958, 'accumulated_eval_time': 3258.116788625717, 'accumulated_logging_time': 0.12381887435913086}
I0210 03:50:06.951320 140529929012992 logging_writer.py:48] [11635] accumulated_eval_time=3258.116789, accumulated_logging_time=0.123819, accumulated_submission_time=4232.144703, global_step=11635, preemption_count=0, score=4232.144703, test/accuracy=0.646563, test/bleu=25.978057, test/loss=1.692735, test/num_examples=3003, total_duration=7490.750286, train/accuracy=0.618301, train/bleu=30.174970, train/loss=1.903286, validation/accuracy=0.637698, validation/bleu=26.727297, validation/loss=1.757757, validation/num_examples=3000
I0210 03:50:30.672814 140529937405696 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.266922265291214, loss=1.9266940355300903
I0210 03:51:06.652895 140529929012992 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.19561859965324402, loss=1.9615715742111206
I0210 03:51:42.724332 140529937405696 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.1915532350540161, loss=1.9664323329925537
I0210 03:52:18.799932 140529929012992 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.18898846209049225, loss=2.10347318649292
I0210 03:52:54.888876 140529937405696 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.18270793557167053, loss=1.904960036277771
I0210 03:53:30.986032 140529929012992 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.25805413722991943, loss=1.8799805641174316
I0210 03:54:07.064982 140529937405696 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.21251171827316284, loss=1.9697340726852417
I0210 03:54:43.153558 140529929012992 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.19539861381053925, loss=1.9674288034439087
I0210 03:55:19.286372 140529937405696 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.21158060431480408, loss=1.9074435234069824
I0210 03:55:55.393147 140529929012992 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.20386336743831635, loss=1.9388583898544312
I0210 03:56:31.559911 140529937405696 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.17938275635242462, loss=1.9823812246322632
I0210 03:57:07.716109 140529929012992 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.1761532872915268, loss=1.9904440641403198
I0210 03:57:43.877745 140529937405696 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.16777248680591583, loss=1.9450703859329224
I0210 03:58:20.028531 140529929012992 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.2460557073354721, loss=1.9528138637542725
I0210 03:58:56.177921 140529937405696 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.223826602101326, loss=1.9462623596191406
I0210 03:59:32.291518 140529929012992 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.2743147611618042, loss=1.9718044996261597
I0210 04:00:08.398455 140529937405696 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.2009875327348709, loss=1.9772427082061768
I0210 04:00:44.505803 140529929012992 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.21460197865962982, loss=1.9441187381744385
I0210 04:01:20.594904 140529937405696 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.17810523509979248, loss=1.9425278902053833
I0210 04:01:56.770375 140529929012992 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.2601333260536194, loss=1.8919836282730103
I0210 04:02:32.863890 140529937405696 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.18034628033638, loss=2.001988172531128
I0210 04:03:08.978163 140529929012992 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.19148242473602295, loss=1.9933902025222778
I0210 04:03:45.097960 140529937405696 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.25407928228378296, loss=1.9025778770446777
I0210 04:04:07.233223 140699726837568 spec.py:321] Evaluating on the training split.
I0210 04:04:10.263135 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 04:07:22.279322 140699726837568 spec.py:333] Evaluating on the validation split.
I0210 04:07:25.000349 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 04:09:48.247429 140699726837568 spec.py:349] Evaluating on the test split.
I0210 04:09:50.974781 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 04:11:56.206141 140699726837568 submission_runner.py:408] Time since start: 8800.02s, 	Step: 13963, 	{'train/accuracy': 0.6274576783180237, 'train/loss': 1.8290431499481201, 'train/bleu': 30.427990031021476, 'validation/accuracy': 0.6435381770133972, 'validation/loss': 1.7107415199279785, 'validation/bleu': 27.2194505792734, 'validation/num_examples': 3000, 'test/accuracy': 0.6531288027763367, 'test/loss': 1.6488524675369263, 'test/bleu': 26.288234040380576, 'test/num_examples': 3003, 'score': 5072.34353518486, 'total_duration': 8800.02228808403, 'accumulated_submission_time': 5072.34353518486, 'accumulated_eval_time': 3727.0896623134613, 'accumulated_logging_time': 0.15128850936889648}
I0210 04:11:56.223438 140529929012992 logging_writer.py:48] [13963] accumulated_eval_time=3727.089662, accumulated_logging_time=0.151289, accumulated_submission_time=5072.343535, global_step=13963, preemption_count=0, score=5072.343535, test/accuracy=0.653129, test/bleu=26.288234, test/loss=1.648852, test/num_examples=3003, total_duration=8800.022288, train/accuracy=0.627458, train/bleu=30.427990, train/loss=1.829043, validation/accuracy=0.643538, validation/bleu=27.219451, validation/loss=1.710742, validation/num_examples=3000
I0210 04:12:09.878349 140529937405696 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.22834698855876923, loss=1.9577769041061401
I0210 04:12:45.862578 140529929012992 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.20646201074123383, loss=1.9239925146102905
I0210 04:13:21.965940 140529937405696 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.17228537797927856, loss=1.8787424564361572
I0210 04:13:58.035250 140529929012992 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.17694486677646637, loss=1.963179349899292
I0210 04:14:34.117394 140529937405696 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.17833194136619568, loss=1.8710137605667114
I0210 04:15:10.316594 140529929012992 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.1858508139848709, loss=1.8587020635604858
I0210 04:15:46.456581 140529937405696 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.25461116433143616, loss=1.9609222412109375
I0210 04:16:22.556873 140529929012992 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.19770556688308716, loss=1.9408104419708252
I0210 04:16:58.664433 140529937405696 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.20421120524406433, loss=1.8959145545959473
I0210 04:17:34.795294 140529929012992 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.21709348261356354, loss=2.0126917362213135
I0210 04:18:10.872369 140529937405696 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.18928909301757812, loss=1.9183034896850586
I0210 04:18:46.969994 140529929012992 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.18935047090053558, loss=1.9420377016067505
I0210 04:19:23.110704 140529937405696 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.2479722648859024, loss=1.8581205606460571
I0210 04:19:59.252757 140529929012992 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.1956435739994049, loss=1.965345859527588
I0210 04:20:35.344689 140529937405696 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.18852591514587402, loss=1.9706202745437622
I0210 04:21:11.423005 140529929012992 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.19063527882099152, loss=1.9205936193466187
I0210 04:21:47.555078 140529937405696 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.17300434410572052, loss=1.8984788656234741
I0210 04:22:23.723924 140529929012992 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.1856945902109146, loss=1.9159188270568848
I0210 04:22:59.855513 140529937405696 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.2081672102212906, loss=1.8574986457824707
I0210 04:23:35.985970 140529929012992 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.18543781340122223, loss=1.862593173980713
I0210 04:24:12.093023 140529937405696 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.20911167562007904, loss=1.9137377738952637
I0210 04:24:48.173453 140529929012992 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.19298534095287323, loss=1.8728936910629272
I0210 04:25:24.322793 140529937405696 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.18604183197021484, loss=1.9159691333770752
I0210 04:25:56.506675 140699726837568 spec.py:321] Evaluating on the training split.
I0210 04:25:59.532743 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 04:29:29.116505 140699726837568 spec.py:333] Evaluating on the validation split.
I0210 04:29:31.853494 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 04:32:10.313286 140699726837568 spec.py:349] Evaluating on the test split.
I0210 04:32:13.042895 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 04:34:56.708020 140699726837568 submission_runner.py:408] Time since start: 10180.52s, 	Step: 16291, 	{'train/accuracy': 0.6281635761260986, 'train/loss': 1.8319660425186157, 'train/bleu': 30.128216497598615, 'validation/accuracy': 0.6473942995071411, 'validation/loss': 1.6852028369903564, 'validation/bleu': 27.31210332692129, 'validation/num_examples': 3000, 'test/accuracy': 0.6560688018798828, 'test/loss': 1.6182109117507935, 'test/bleu': 26.360974311764373, 'test/num_examples': 3003, 'score': 5912.543488740921, 'total_duration': 10180.52417087555, 'accumulated_submission_time': 5912.543488740921, 'accumulated_eval_time': 4267.290963888168, 'accumulated_logging_time': 0.17849230766296387}
I0210 04:34:56.725123 140529929012992 logging_writer.py:48] [16291] accumulated_eval_time=4267.290964, accumulated_logging_time=0.178492, accumulated_submission_time=5912.543489, global_step=16291, preemption_count=0, score=5912.543489, test/accuracy=0.656069, test/bleu=26.360974, test/loss=1.618211, test/num_examples=3003, total_duration=10180.524171, train/accuracy=0.628164, train/bleu=30.128216, train/loss=1.831966, validation/accuracy=0.647394, validation/bleu=27.312103, validation/loss=1.685203, validation/num_examples=3000
I0210 04:35:00.320829 140529937405696 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.19864553213119507, loss=1.8655977249145508
I0210 04:35:36.316365 140529929012992 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.20833584666252136, loss=1.894513487815857
I0210 04:36:12.341224 140529937405696 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.17366445064544678, loss=1.9524863958358765
I0210 04:36:48.381824 140529929012992 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.1950228065252304, loss=1.9025620222091675
I0210 04:37:24.568083 140529937405696 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.20138753950595856, loss=1.8490326404571533
I0210 04:38:00.763635 140529929012992 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.18687371909618378, loss=1.9153848886489868
I0210 04:38:36.903397 140529937405696 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.2961971163749695, loss=1.890979528427124
I0210 04:39:13.012470 140529929012992 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.20897705852985382, loss=1.8897111415863037
I0210 04:39:49.122794 140529937405696 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.18332943320274353, loss=1.908059000968933
I0210 04:40:25.230772 140529929012992 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.23273134231567383, loss=1.977016806602478
I0210 04:41:01.375925 140529937405696 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.19770547747612, loss=1.868393898010254
I0210 04:41:37.507557 140529929012992 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.19533158838748932, loss=1.8634400367736816
I0210 04:42:13.706667 140529937405696 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.19088798761367798, loss=1.9946906566619873
I0210 04:42:49.871746 140529929012992 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.240804523229599, loss=1.8855822086334229
I0210 04:43:26.052387 140529937405696 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.21130602061748505, loss=1.8974945545196533
I0210 04:44:02.219731 140529929012992 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.2315051555633545, loss=1.895397663116455
I0210 04:44:38.407646 140529937405696 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.2058085799217224, loss=1.867114543914795
I0210 04:45:14.538894 140529929012992 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.18976156413555145, loss=1.768176555633545
I0210 04:45:50.631940 140529937405696 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.23763662576675415, loss=1.9007154703140259
I0210 04:46:26.759656 140529929012992 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.1921335607767105, loss=1.940794825553894
I0210 04:47:02.853349 140529937405696 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.21890097856521606, loss=1.838201880455017
I0210 04:47:39.007922 140529929012992 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.2009137123823166, loss=1.9015450477600098
I0210 04:48:15.184932 140529937405696 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.18195350468158722, loss=1.8661880493164062
I0210 04:48:51.349375 140529929012992 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.18485234677791595, loss=1.8568708896636963
I0210 04:48:56.859775 140699726837568 spec.py:321] Evaluating on the training split.
I0210 04:48:59.899072 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 04:52:13.836295 140699726837568 spec.py:333] Evaluating on the validation split.
I0210 04:52:16.559612 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 04:54:57.810414 140699726837568 spec.py:349] Evaluating on the test split.
I0210 04:55:00.532608 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 04:57:33.722235 140699726837568 submission_runner.py:408] Time since start: 11537.54s, 	Step: 18617, 	{'train/accuracy': 0.631950318813324, 'train/loss': 1.8058927059173584, 'train/bleu': 30.512143847900504, 'validation/accuracy': 0.6519696116447449, 'validation/loss': 1.6625659465789795, 'validation/bleu': 27.902452308781214, 'validation/num_examples': 3000, 'test/accuracy': 0.6614025831222534, 'test/loss': 1.5896934270858765, 'test/bleu': 27.088659970344388, 'test/num_examples': 3003, 'score': 6752.588777065277, 'total_duration': 11537.538368225098, 'accumulated_submission_time': 6752.588777065277, 'accumulated_eval_time': 4784.1533625125885, 'accumulated_logging_time': 0.2073345184326172}
I0210 04:57:33.739430 140529937405696 logging_writer.py:48] [18617] accumulated_eval_time=4784.153363, accumulated_logging_time=0.207335, accumulated_submission_time=6752.588777, global_step=18617, preemption_count=0, score=6752.588777, test/accuracy=0.661403, test/bleu=27.088660, test/loss=1.589693, test/num_examples=3003, total_duration=11537.538368, train/accuracy=0.631950, train/bleu=30.512144, train/loss=1.805893, validation/accuracy=0.651970, validation/bleu=27.902452, validation/loss=1.662566, validation/num_examples=3000
I0210 04:58:03.934754 140529929012992 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.19188378751277924, loss=1.8561328649520874
I0210 04:58:39.900861 140529937405696 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.21873793005943298, loss=1.9160985946655273
I0210 04:59:15.933359 140529929012992 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.2169426530599594, loss=1.8698780536651611
I0210 04:59:51.990642 140529937405696 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.21612976491451263, loss=1.8538196086883545
I0210 05:00:28.122089 140529929012992 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.21319712698459625, loss=1.9269332885742188
I0210 05:01:04.251348 140529937405696 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.3512253761291504, loss=1.8971881866455078
I0210 05:01:40.342046 140529929012992 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.2030823826789856, loss=1.8481026887893677
I0210 05:02:16.418420 140529937405696 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.2388714700937271, loss=1.9313616752624512
I0210 05:02:52.532898 140529929012992 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.18912550806999207, loss=1.8362746238708496
I0210 05:03:28.646080 140529937405696 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.23497486114501953, loss=1.9223368167877197
I0210 05:04:04.740093 140529929012992 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.2741515040397644, loss=1.9202425479888916
I0210 05:04:40.839499 140529937405696 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.19809472560882568, loss=1.9063167572021484
I0210 05:05:16.950481 140529929012992 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.20204636454582214, loss=1.8317762613296509
I0210 05:05:53.057445 140529937405696 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.20601613819599152, loss=1.8848750591278076
I0210 05:06:29.198431 140529929012992 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.1863604187965393, loss=1.9429408311843872
I0210 05:07:05.288560 140529937405696 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.18053172528743744, loss=1.7839009761810303
I0210 05:07:41.416573 140529929012992 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.21694359183311462, loss=1.7959388494491577
I0210 05:08:17.555726 140529937405696 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.21631906926631927, loss=1.9091359376907349
I0210 05:08:53.679761 140529929012992 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.2678109109401703, loss=1.8310186862945557
I0210 05:09:29.838420 140529937405696 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.20588307082653046, loss=1.738971471786499
I0210 05:10:05.988592 140529929012992 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.2680663764476776, loss=1.9108412265777588
I0210 05:10:42.081493 140529937405696 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.3020363450050354, loss=1.8568309545516968
I0210 05:11:18.277153 140529929012992 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.21716932952404022, loss=1.8819762468338013
I0210 05:11:33.901818 140699726837568 spec.py:321] Evaluating on the training split.
I0210 05:11:36.924412 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 05:15:01.847646 140699726837568 spec.py:333] Evaluating on the validation split.
I0210 05:15:04.565126 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 05:17:30.514008 140699726837568 spec.py:349] Evaluating on the test split.
I0210 05:17:33.239961 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 05:19:45.686022 140699726837568 submission_runner.py:408] Time since start: 12869.50s, 	Step: 20945, 	{'train/accuracy': 0.6327740550041199, 'train/loss': 1.7765021324157715, 'train/bleu': 30.94379672227982, 'validation/accuracy': 0.6536186933517456, 'validation/loss': 1.6433725357055664, 'validation/bleu': 27.590168338973967, 'validation/num_examples': 3000, 'test/accuracy': 0.6645168662071228, 'test/loss': 1.5687122344970703, 'test/bleu': 27.15118496076002, 'test/num_examples': 3003, 'score': 7592.666503190994, 'total_duration': 12869.502164840698, 'accumulated_submission_time': 7592.666503190994, 'accumulated_eval_time': 5275.937527179718, 'accumulated_logging_time': 0.2346513271331787}
I0210 05:19:45.704260 140529937405696 logging_writer.py:48] [20945] accumulated_eval_time=5275.937527, accumulated_logging_time=0.234651, accumulated_submission_time=7592.666503, global_step=20945, preemption_count=0, score=7592.666503, test/accuracy=0.664517, test/bleu=27.151185, test/loss=1.568712, test/num_examples=3003, total_duration=12869.502165, train/accuracy=0.632774, train/bleu=30.943797, train/loss=1.776502, validation/accuracy=0.653619, validation/bleu=27.590168, validation/loss=1.643373, validation/num_examples=3000
I0210 05:20:05.864084 140529929012992 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.337741881608963, loss=1.8115167617797852
I0210 05:20:41.836105 140529937405696 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.20403781533241272, loss=1.7956441640853882
I0210 05:21:17.857029 140529929012992 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.2187398374080658, loss=1.881908893585205
I0210 05:21:53.952130 140529937405696 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.19070743024349213, loss=1.8903601169586182
I0210 05:22:30.060502 140529929012992 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.2064361870288849, loss=1.8935812711715698
I0210 05:23:06.209907 140529937405696 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.2227965146303177, loss=1.891223430633545
I0210 05:23:42.335812 140529929012992 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.19599798321723938, loss=1.9055014848709106
I0210 05:24:18.439420 140529937405696 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.20272114872932434, loss=1.85115647315979
I0210 05:24:54.590427 140529929012992 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.20948658883571625, loss=1.824419379234314
I0210 05:25:30.678556 140529937405696 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.19252392649650574, loss=1.8038856983184814
I0210 05:26:06.825032 140529929012992 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.2093307077884674, loss=1.9000064134597778
I0210 05:26:42.918857 140529937405696 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.23205266892910004, loss=1.7755037546157837
I0210 05:27:19.010167 140529929012992 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.18597182631492615, loss=1.8325767517089844
I0210 05:27:55.127103 140529937405696 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.19511725008487701, loss=1.8432540893554688
I0210 05:28:31.233312 140529929012992 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.1845076084136963, loss=1.8471242189407349
I0210 05:29:07.374878 140529937405696 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.18500536680221558, loss=1.7984578609466553
I0210 05:29:43.462283 140529929012992 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.19988414645195007, loss=1.8723413944244385
I0210 05:30:19.575136 140529937405696 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.312673956155777, loss=1.818956732749939
I0210 05:30:55.672409 140529929012992 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.20362891256809235, loss=1.7927401065826416
I0210 05:31:31.797332 140529937405696 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.18393971025943756, loss=1.8486627340316772
I0210 05:32:07.917013 140529929012992 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.2051376849412918, loss=1.8170818090438843
I0210 05:32:44.026138 140529937405696 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.23257474601268768, loss=1.8004193305969238
I0210 05:33:20.157204 140529929012992 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.25847485661506653, loss=1.875152587890625
I0210 05:33:45.866056 140699726837568 spec.py:321] Evaluating on the training split.
I0210 05:33:48.896891 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 05:36:56.713815 140699726837568 spec.py:333] Evaluating on the validation split.
I0210 05:36:59.443378 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 05:39:33.321359 140699726837568 spec.py:349] Evaluating on the test split.
I0210 05:39:36.056437 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 05:42:02.606931 140699726837568 submission_runner.py:408] Time since start: 14206.42s, 	Step: 23273, 	{'train/accuracy': 0.6339693069458008, 'train/loss': 1.7776130437850952, 'train/bleu': 30.655361633286923, 'validation/accuracy': 0.6550569534301758, 'validation/loss': 1.6391043663024902, 'validation/bleu': 27.783212559536747, 'validation/num_examples': 3000, 'test/accuracy': 0.6667364239692688, 'test/loss': 1.5609209537506104, 'test/bleu': 27.47751462370135, 'test/num_examples': 3003, 'score': 8432.74540233612, 'total_duration': 14206.423070907593, 'accumulated_submission_time': 8432.74540233612, 'accumulated_eval_time': 5772.678349494934, 'accumulated_logging_time': 0.2628781795501709}
I0210 05:42:02.625273 140529937405696 logging_writer.py:48] [23273] accumulated_eval_time=5772.678349, accumulated_logging_time=0.262878, accumulated_submission_time=8432.745402, global_step=23273, preemption_count=0, score=8432.745402, test/accuracy=0.666736, test/bleu=27.477515, test/loss=1.560921, test/num_examples=3003, total_duration=14206.423071, train/accuracy=0.633969, train/bleu=30.655362, train/loss=1.777613, validation/accuracy=0.655057, validation/bleu=27.783213, validation/loss=1.639104, validation/num_examples=3000
I0210 05:42:12.732056 140529929012992 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.2105681449174881, loss=1.9206700325012207
I0210 05:42:48.784367 140529937405696 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.20052863657474518, loss=1.8986512422561646
I0210 05:43:24.850061 140529929012992 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.24915242195129395, loss=1.766963243484497
I0210 05:44:00.977874 140529937405696 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.322039395570755, loss=1.8313652276992798
I0210 05:44:37.126918 140529929012992 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.2176099419593811, loss=1.7571278810501099
I0210 05:45:13.258124 140529937405696 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.30785366892814636, loss=1.8155131340026855
I0210 05:45:49.368077 140529929012992 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.2030951827764511, loss=1.8696666955947876
I0210 05:46:25.495975 140529937405696 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.20799605548381805, loss=1.7541210651397705
I0210 05:47:01.586595 140529929012992 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.19995345175266266, loss=1.8026633262634277
I0210 05:47:37.775580 140529937405696 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.20357008278369904, loss=1.8751636743545532
I0210 05:48:13.927595 140529929012992 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.21864090859889984, loss=1.896356225013733
I0210 05:48:50.060455 140529937405696 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.21913786232471466, loss=1.8431521654129028
I0210 05:49:26.204565 140529929012992 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.23506514728069305, loss=1.8539202213287354
I0210 05:50:02.316360 140529937405696 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.22770948708057404, loss=1.8416789770126343
I0210 05:50:38.395197 140529929012992 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.21246764063835144, loss=1.8753618001937866
I0210 05:51:14.505740 140529937405696 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.24479207396507263, loss=1.874853253364563
I0210 05:51:50.589178 140529929012992 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.19930316507816315, loss=1.8352500200271606
I0210 05:52:26.662503 140529937405696 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.288522869348526, loss=1.7787915468215942
I0210 05:53:02.804064 140529929012992 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.2240910828113556, loss=1.8993735313415527
I0210 05:53:38.943842 140529937405696 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.20574632287025452, loss=1.823388695716858
I0210 05:54:15.039227 140529929012992 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.21667662262916565, loss=1.8061836957931519
I0210 05:54:51.168442 140529937405696 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.2396477460861206, loss=1.8313229084014893
I0210 05:55:27.363560 140529929012992 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.1940460354089737, loss=1.8449478149414062
I0210 05:56:02.870250 140699726837568 spec.py:321] Evaluating on the training split.
I0210 05:56:05.893352 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 05:58:53.770276 140699726837568 spec.py:333] Evaluating on the validation split.
I0210 05:58:56.497164 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 06:01:20.430388 140699726837568 spec.py:349] Evaluating on the test split.
I0210 06:01:23.162189 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 06:03:38.119695 140699726837568 submission_runner.py:408] Time since start: 15501.94s, 	Step: 25600, 	{'train/accuracy': 0.6479560136795044, 'train/loss': 1.6769108772277832, 'train/bleu': 31.84595571310796, 'validation/accuracy': 0.6565200686454773, 'validation/loss': 1.6232635974884033, 'validation/bleu': 27.658885874545486, 'validation/num_examples': 3000, 'test/accuracy': 0.6655627489089966, 'test/loss': 1.5492188930511475, 'test/bleu': 27.11589523195827, 'test/num_examples': 3003, 'score': 9272.903683662415, 'total_duration': 15501.935836553574, 'accumulated_submission_time': 9272.903683662415, 'accumulated_eval_time': 6227.92778301239, 'accumulated_logging_time': 0.29166626930236816}
I0210 06:03:38.137969 140529937405696 logging_writer.py:48] [25600] accumulated_eval_time=6227.927783, accumulated_logging_time=0.291666, accumulated_submission_time=9272.903684, global_step=25600, preemption_count=0, score=9272.903684, test/accuracy=0.665563, test/bleu=27.115895, test/loss=1.549219, test/num_examples=3003, total_duration=15501.935837, train/accuracy=0.647956, train/bleu=31.845956, train/loss=1.676911, validation/accuracy=0.656520, validation/bleu=27.658886, validation/loss=1.623264, validation/num_examples=3000
I0210 06:03:38.523985 140529929012992 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.20100240409374237, loss=1.8253686428070068
I0210 06:04:14.523334 140529937405696 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.19072113931179047, loss=1.9195594787597656
I0210 06:04:50.607085 140529929012992 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.20421646535396576, loss=1.8540148735046387
I0210 06:05:26.717708 140529937405696 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.25354450941085815, loss=1.8237218856811523
I0210 06:06:02.791198 140529929012992 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.2654416859149933, loss=1.8367470502853394
I0210 06:06:38.851181 140529937405696 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.1906834840774536, loss=1.8574124574661255
I0210 06:07:14.978342 140529929012992 logging_writer.py:48] [26200] global_step=26200, grad_norm=3.6450676918029785, loss=2.3459763526916504
I0210 06:07:51.097112 140529937405696 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.2645203173160553, loss=1.8575738668441772
I0210 06:08:27.208526 140529929012992 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.21940229833126068, loss=1.790359616279602
I0210 06:09:03.305375 140529937405696 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.18275584280490875, loss=1.7803579568862915
I0210 06:09:39.461527 140529929012992 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.18265016376972198, loss=1.7564269304275513
I0210 06:10:15.659621 140529937405696 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.1931484341621399, loss=1.7974846363067627
I0210 06:10:51.765672 140529929012992 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.17558005452156067, loss=1.7556569576263428
I0210 06:11:27.837658 140529937405696 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.19268915057182312, loss=1.7893790006637573
I0210 06:12:03.935721 140529929012992 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.21279466152191162, loss=1.8341413736343384
I0210 06:12:40.127517 140529937405696 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.21307863295078278, loss=1.803102731704712
I0210 06:13:16.325749 140529929012992 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.20292535424232483, loss=1.7727200984954834
I0210 06:13:52.465590 140529937405696 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.18860340118408203, loss=1.8043748140335083
I0210 06:14:28.591342 140529929012992 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.19407804310321808, loss=1.7959232330322266
I0210 06:15:04.712915 140529937405696 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.19119495153427124, loss=1.8417547941207886
I0210 06:15:40.807388 140529929012992 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.21604827046394348, loss=1.7810451984405518
I0210 06:16:16.893207 140529937405696 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.1904250979423523, loss=1.8266202211380005
I0210 06:16:53.007841 140529929012992 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.23249052464962006, loss=1.774746060371399
I0210 06:17:29.087827 140529937405696 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.23272758722305298, loss=1.847904086112976
I0210 06:17:38.192475 140699726837568 spec.py:321] Evaluating on the training split.
I0210 06:17:41.218451 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 06:22:19.012427 140699726837568 spec.py:333] Evaluating on the validation split.
I0210 06:22:21.730859 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 06:26:16.799839 140699726837568 spec.py:349] Evaluating on the test split.
I0210 06:26:19.529404 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 06:30:24.097994 140699726837568 submission_runner.py:408] Time since start: 17107.91s, 	Step: 27927, 	{'train/accuracy': 0.6404833793640137, 'train/loss': 1.7259860038757324, 'train/bleu': 31.049956913695816, 'validation/accuracy': 0.6574624180793762, 'validation/loss': 1.6162623167037964, 'validation/bleu': 27.821747606245758, 'validation/num_examples': 3000, 'test/accuracy': 0.6682470440864563, 'test/loss': 1.5407686233520508, 'test/bleu': 27.582243730159053, 'test/num_examples': 3003, 'score': 10112.873045921326, 'total_duration': 17107.914113998413, 'accumulated_submission_time': 10112.873045921326, 'accumulated_eval_time': 6993.833231925964, 'accumulated_logging_time': 0.3207814693450928}
I0210 06:30:24.116738 140529929012992 logging_writer.py:48] [27927] accumulated_eval_time=6993.833232, accumulated_logging_time=0.320781, accumulated_submission_time=10112.873046, global_step=27927, preemption_count=0, score=10112.873046, test/accuracy=0.668247, test/bleu=27.582244, test/loss=1.540769, test/num_examples=3003, total_duration=17107.914114, train/accuracy=0.640483, train/bleu=31.049957, train/loss=1.725986, validation/accuracy=0.657462, validation/bleu=27.821748, validation/loss=1.616262, validation/num_examples=3000
I0210 06:30:50.665971 140529937405696 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.23405447602272034, loss=1.8658392429351807
I0210 06:31:26.650071 140529929012992 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.2208193838596344, loss=1.8461440801620483
I0210 06:32:02.752069 140529937405696 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.24097834527492523, loss=1.872079849243164
I0210 06:32:38.878627 140529929012992 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.2122751921415329, loss=1.8106215000152588
I0210 06:33:15.005829 140529937405696 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.2093174010515213, loss=1.8150438070297241
I0210 06:33:51.096410 140529929012992 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.20493057370185852, loss=1.7937442064285278
I0210 06:34:27.231628 140529937405696 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.27211612462997437, loss=1.818663239479065
I0210 06:35:03.346097 140529929012992 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.27619701623916626, loss=1.7628259658813477
I0210 06:35:39.458889 140529937405696 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.3066316545009613, loss=1.873732566833496
I0210 06:36:15.564576 140529929012992 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.241231769323349, loss=1.7589455842971802
I0210 06:36:51.670359 140529937405696 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.19367632269859314, loss=1.9160674810409546
I0210 06:37:27.744268 140529929012992 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.21120202541351318, loss=1.7396873235702515
I0210 06:38:03.857103 140529937405696 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.23362964391708374, loss=1.8095455169677734
I0210 06:38:39.948496 140529929012992 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.20086205005645752, loss=1.8098894357681274
I0210 06:39:16.029830 140529937405696 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.19726800918579102, loss=1.9220243692398071
I0210 06:39:52.121545 140529929012992 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.22976097464561462, loss=1.892354965209961
I0210 06:40:28.217063 140529937405696 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.24373885989189148, loss=1.8293497562408447
I0210 06:41:04.363882 140529929012992 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.21028760075569153, loss=1.811087965965271
I0210 06:41:40.505511 140529937405696 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.20108796656131744, loss=1.791662335395813
I0210 06:42:16.585095 140529929012992 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.2365647256374359, loss=1.7937167882919312
I0210 06:42:52.672523 140529937405696 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.20413032174110413, loss=1.7408955097198486
I0210 06:43:28.809344 140529929012992 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.2087807059288025, loss=1.7877947092056274
I0210 06:44:04.945366 140529937405696 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.23044270277023315, loss=1.8555301427841187
I0210 06:44:24.221879 140699726837568 spec.py:321] Evaluating on the training split.
I0210 06:44:27.246695 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 06:47:52.058631 140699726837568 spec.py:333] Evaluating on the validation split.
I0210 06:47:54.787238 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 06:50:57.910165 140699726837568 spec.py:349] Evaluating on the test split.
I0210 06:51:00.644446 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 06:53:24.691127 140699726837568 submission_runner.py:408] Time since start: 18488.51s, 	Step: 30255, 	{'train/accuracy': 0.636620819568634, 'train/loss': 1.76451575756073, 'train/bleu': 31.611790041850664, 'validation/accuracy': 0.6559621095657349, 'validation/loss': 1.605042576789856, 'validation/bleu': 27.67252673218362, 'validation/num_examples': 3000, 'test/accuracy': 0.6700947284698486, 'test/loss': 1.525154709815979, 'test/bleu': 27.54839713124831, 'test/num_examples': 3003, 'score': 10952.896437168121, 'total_duration': 18488.50726699829, 'accumulated_submission_time': 10952.896437168121, 'accumulated_eval_time': 7534.302425146103, 'accumulated_logging_time': 0.34938669204711914}
I0210 06:53:24.710372 140529929012992 logging_writer.py:48] [30255] accumulated_eval_time=7534.302425, accumulated_logging_time=0.349387, accumulated_submission_time=10952.896437, global_step=30255, preemption_count=0, score=10952.896437, test/accuracy=0.670095, test/bleu=27.548397, test/loss=1.525155, test/num_examples=3003, total_duration=18488.507267, train/accuracy=0.636621, train/bleu=31.611790, train/loss=1.764516, validation/accuracy=0.655962, validation/bleu=27.672527, validation/loss=1.605043, validation/num_examples=3000
I0210 06:53:41.275507 140529937405696 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.25110846757888794, loss=1.8168390989303589
I0210 06:54:17.311215 140529929012992 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.24605485796928406, loss=1.7957172393798828
I0210 06:54:53.340217 140529937405696 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.27699223160743713, loss=1.7994085550308228
I0210 06:55:29.408978 140529929012992 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.2505511939525604, loss=1.8537195920944214
I0210 06:56:05.487004 140529937405696 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.28216752409935, loss=1.8677458763122559
I0210 06:56:41.654641 140529929012992 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.23866277933120728, loss=1.8393878936767578
I0210 06:57:17.852159 140529937405696 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.1760905534029007, loss=1.6934261322021484
I0210 06:57:54.012057 140529929012992 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.22087210416793823, loss=1.8655383586883545
I0210 06:58:30.133401 140529937405696 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.19768409430980682, loss=1.767601490020752
I0210 06:59:06.272353 140529929012992 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.23483628034591675, loss=1.7959502935409546
I0210 06:59:42.390693 140529937405696 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.2055700421333313, loss=1.8749884366989136
I0210 07:00:18.508984 140529929012992 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.37966474890708923, loss=1.8839654922485352
I0210 07:00:54.608186 140529937405696 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.19934765994548798, loss=1.81368887424469
I0210 07:01:30.702046 140529929012992 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.18575461208820343, loss=1.7583800554275513
I0210 07:02:06.831808 140529937405696 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.2073892205953598, loss=1.8181447982788086
I0210 07:02:42.933866 140529929012992 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.23963093757629395, loss=1.6941471099853516
I0210 07:03:18.996078 140529937405696 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.20220863819122314, loss=1.804381012916565
I0210 07:03:55.099436 140529929012992 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.20841480791568756, loss=1.838364839553833
I0210 07:04:31.222140 140529937405696 logging_writer.py:48] [32100] global_step=32100, grad_norm=2.075460910797119, loss=1.7875263690948486
I0210 07:05:07.375068 140529929012992 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.23191526532173157, loss=1.8252432346343994
I0210 07:05:43.454800 140529937405696 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.20302441716194153, loss=1.839707612991333
I0210 07:06:19.553083 140529929012992 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.20645178854465485, loss=1.7598432302474976
I0210 07:06:55.632260 140529937405696 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.1983608603477478, loss=1.7693135738372803
I0210 07:07:24.991589 140699726837568 spec.py:321] Evaluating on the training split.
I0210 07:07:28.027572 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 07:11:22.698611 140699726837568 spec.py:333] Evaluating on the validation split.
I0210 07:11:25.433347 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 07:14:01.632663 140699726837568 spec.py:349] Evaluating on the test split.
I0210 07:14:04.381552 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 07:16:17.984388 140699726837568 submission_runner.py:408] Time since start: 19861.80s, 	Step: 32583, 	{'train/accuracy': 0.6440646648406982, 'train/loss': 1.7038763761520386, 'train/bleu': 31.360567916844502, 'validation/accuracy': 0.6583923101425171, 'validation/loss': 1.5940138101577759, 'validation/bleu': 27.90004436424476, 'validation/num_examples': 3000, 'test/accuracy': 0.6725466251373291, 'test/loss': 1.5128222703933716, 'test/bleu': 27.34830812687249, 'test/num_examples': 3003, 'score': 11793.089814901352, 'total_duration': 19861.800530195236, 'accumulated_submission_time': 11793.089814901352, 'accumulated_eval_time': 8067.295173883438, 'accumulated_logging_time': 0.3822178840637207}
I0210 07:16:18.004261 140529929012992 logging_writer.py:48] [32583] accumulated_eval_time=8067.295174, accumulated_logging_time=0.382218, accumulated_submission_time=11793.089815, global_step=32583, preemption_count=0, score=11793.089815, test/accuracy=0.672547, test/bleu=27.348308, test/loss=1.512822, test/num_examples=3003, total_duration=19861.800530, train/accuracy=0.644065, train/bleu=31.360568, train/loss=1.703876, validation/accuracy=0.658392, validation/bleu=27.900044, validation/loss=1.594014, validation/num_examples=3000
I0210 07:16:24.475357 140529937405696 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.21493618190288544, loss=1.8425558805465698
I0210 07:17:00.441174 140529929012992 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.19748800992965698, loss=1.8309935331344604
I0210 07:17:36.491533 140529937405696 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.2526845335960388, loss=1.8722273111343384
I0210 07:18:12.566134 140529929012992 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.1987638920545578, loss=1.7854325771331787
I0210 07:18:48.700206 140529937405696 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.2098037302494049, loss=1.7685279846191406
I0210 07:19:24.821658 140529929012992 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.193833127617836, loss=1.8293548822402954
I0210 07:20:00.914530 140529937405696 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.1935563087463379, loss=1.7919058799743652
I0210 07:20:37.009688 140529929012992 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.2267344743013382, loss=1.7643870115280151
I0210 07:21:13.119407 140529937405696 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.2813020348548889, loss=1.8137410879135132
I0210 07:21:49.210791 140529929012992 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.2089276909828186, loss=1.8044729232788086
I0210 07:22:25.357719 140529937405696 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.2355727106332779, loss=1.759328842163086
I0210 07:23:01.472578 140529929012992 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.24761220812797546, loss=1.6913471221923828
I0210 07:23:37.647917 140529937405696 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.24004746973514557, loss=1.6965819597244263
I0210 07:24:13.782595 140529929012992 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.23466676473617554, loss=1.7937862873077393
I0210 07:24:49.858271 140529937405696 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.20370113849639893, loss=1.7813405990600586
I0210 07:25:25.971884 140529929012992 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.19330808520317078, loss=1.7551814317703247
I0210 07:26:02.091615 140529937405696 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.20395313203334808, loss=1.723215103149414
I0210 07:26:38.183274 140529929012992 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.2231176793575287, loss=1.8115745782852173
I0210 07:27:14.320262 140529937405696 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.19046486914157867, loss=1.7877390384674072
I0210 07:27:50.501292 140529929012992 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.21377147734165192, loss=1.7865651845932007
I0210 07:28:26.702014 140529937405696 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.21750439703464508, loss=1.7269264459609985
I0210 07:29:02.911224 140529929012992 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.20095881819725037, loss=1.8556660413742065
I0210 07:29:38.995826 140529937405696 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.30980026721954346, loss=1.798591136932373
I0210 07:30:15.138038 140529929012992 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.20527929067611694, loss=1.740515112876892
I0210 07:30:18.117967 140699726837568 spec.py:321] Evaluating on the training split.
I0210 07:30:21.142351 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 07:33:31.596055 140699726837568 spec.py:333] Evaluating on the validation split.
I0210 07:33:34.317958 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 07:36:07.289977 140699726837568 spec.py:349] Evaluating on the test split.
I0210 07:36:10.029672 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 07:38:24.387099 140699726837568 submission_runner.py:408] Time since start: 21188.20s, 	Step: 34910, 	{'train/accuracy': 0.6403083801269531, 'train/loss': 1.734409213066101, 'train/bleu': 31.28614280491014, 'validation/accuracy': 0.6616036891937256, 'validation/loss': 1.5844405889511108, 'validation/bleu': 28.197664218772356, 'validation/num_examples': 3000, 'test/accuracy': 0.6729998588562012, 'test/loss': 1.5030328035354614, 'test/bleu': 27.251809248166786, 'test/num_examples': 3003, 'score': 12633.118577480316, 'total_duration': 21188.203240394592, 'accumulated_submission_time': 12633.118577480316, 'accumulated_eval_time': 8553.564245939255, 'accumulated_logging_time': 0.4125397205352783}
I0210 07:38:24.406771 140529937405696 logging_writer.py:48] [34910] accumulated_eval_time=8553.564246, accumulated_logging_time=0.412540, accumulated_submission_time=12633.118577, global_step=34910, preemption_count=0, score=12633.118577, test/accuracy=0.673000, test/bleu=27.251809, test/loss=1.503033, test/num_examples=3003, total_duration=21188.203240, train/accuracy=0.640308, train/bleu=31.286143, train/loss=1.734409, validation/accuracy=0.661604, validation/bleu=28.197664, validation/loss=1.584441, validation/num_examples=3000
I0210 07:38:57.085043 140529929012992 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.2952125072479248, loss=1.8489768505096436
I0210 07:39:33.146434 140529937405696 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.2210424393415451, loss=1.8067042827606201
I0210 07:40:09.212495 140529929012992 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.197575643658638, loss=1.793070673942566
I0210 07:40:45.287489 140529937405696 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.197159081697464, loss=1.7460880279541016
I0210 07:41:21.387858 140529929012992 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.20613747835159302, loss=1.8103529214859009
I0210 07:41:57.500568 140529937405696 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.20886608958244324, loss=1.8297547101974487
I0210 07:42:33.592077 140529929012992 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.18026289343833923, loss=1.7597805261611938
I0210 07:43:09.748611 140529937405696 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.20797374844551086, loss=1.7755175828933716
I0210 07:43:45.871587 140529929012992 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.20815478265285492, loss=1.8785579204559326
I0210 07:44:21.955252 140529937405696 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.24346333742141724, loss=1.7999602556228638
I0210 07:44:58.043476 140529929012992 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.19434435665607452, loss=1.7428269386291504
I0210 07:45:34.122362 140529937405696 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.20658910274505615, loss=1.8348888158798218
I0210 07:46:10.234701 140529929012992 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.2403499186038971, loss=1.826077938079834
I0210 07:46:46.330723 140529937405696 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.26535746455192566, loss=1.8629800081253052
I0210 07:47:22.480763 140529929012992 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.2140405923128128, loss=1.799678087234497
I0210 07:47:58.587958 140529937405696 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.21353140473365784, loss=1.7147494554519653
I0210 07:48:34.707851 140529929012992 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.18618644773960114, loss=1.743412733078003
I0210 07:49:10.781204 140529937405696 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.21407146751880646, loss=1.7402976751327515
I0210 07:49:46.946748 140529929012992 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.18167203664779663, loss=1.8181954622268677
I0210 07:50:23.085614 140529937405696 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.21198980510234833, loss=1.8545513153076172
I0210 07:50:59.179955 140529929012992 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.1963038593530655, loss=1.7819467782974243
I0210 07:51:35.350603 140529937405696 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.24837838113307953, loss=1.8318144083023071
I0210 07:52:11.434719 140529929012992 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.23324403166770935, loss=1.8466209173202515
I0210 07:52:24.512149 140699726837568 spec.py:321] Evaluating on the training split.
I0210 07:52:27.537107 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 07:55:46.056467 140699726837568 spec.py:333] Evaluating on the validation split.
I0210 07:55:48.785058 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 07:58:35.726424 140699726837568 spec.py:349] Evaluating on the test split.
I0210 07:58:38.446677 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 08:01:06.352908 140699726837568 submission_runner.py:408] Time since start: 22550.17s, 	Step: 37238, 	{'train/accuracy': 0.6411272883415222, 'train/loss': 1.7296642065048218, 'train/bleu': 31.164002746131025, 'validation/accuracy': 0.6619756817817688, 'validation/loss': 1.5773353576660156, 'validation/bleu': 28.1188025528331, 'validation/num_examples': 3000, 'test/accuracy': 0.6725582480430603, 'test/loss': 1.502700686454773, 'test/bleu': 27.285918920544557, 'test/num_examples': 3003, 'score': 13473.14170718193, 'total_duration': 22550.16905093193, 'accumulated_submission_time': 13473.14170718193, 'accumulated_eval_time': 9075.40495300293, 'accumulated_logging_time': 0.44240593910217285}
I0210 08:01:06.372872 140529937405696 logging_writer.py:48] [37238] accumulated_eval_time=9075.404953, accumulated_logging_time=0.442406, accumulated_submission_time=13473.141707, global_step=37238, preemption_count=0, score=13473.141707, test/accuracy=0.672558, test/bleu=27.285919, test/loss=1.502701, test/num_examples=3003, total_duration=22550.169051, train/accuracy=0.641127, train/bleu=31.164003, train/loss=1.729664, validation/accuracy=0.661976, validation/bleu=28.118803, validation/loss=1.577335, validation/num_examples=3000
I0210 08:01:28.984308 140529929012992 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.22836722433567047, loss=1.7844070196151733
I0210 08:02:04.974495 140529937405696 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.18711940944194794, loss=1.7206974029541016
I0210 08:02:41.004362 140529929012992 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.24934759736061096, loss=1.7864080667495728
I0210 08:03:17.080710 140529937405696 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.23558402061462402, loss=1.7629317045211792
I0210 08:03:53.144743 140529929012992 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.2259576916694641, loss=1.6577870845794678
I0210 08:04:29.294973 140529937405696 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.29475516080856323, loss=1.7675365209579468
I0210 08:05:05.453752 140529929012992 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.2079368531703949, loss=1.7460753917694092
I0210 08:05:41.624309 140529937405696 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.20708133280277252, loss=1.7719498872756958
I0210 08:06:17.895367 140529929012992 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.1931043118238449, loss=1.8058823347091675
I0210 08:06:53.974770 140529937405696 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.21205218136310577, loss=1.776667594909668
I0210 08:07:30.076982 140529929012992 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.20347292721271515, loss=1.681862235069275
I0210 08:08:06.224272 140529937405696 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.2420249730348587, loss=1.7943817377090454
I0210 08:08:42.335880 140529929012992 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.21682430803775787, loss=1.7247698307037354
I0210 08:09:18.426721 140529937405696 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.22676979005336761, loss=1.7259812355041504
I0210 08:09:54.561132 140529929012992 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.19294913113117218, loss=1.6992571353912354
I0210 08:10:30.673648 140529937405696 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.22137747704982758, loss=1.7907824516296387
I0210 08:11:06.795957 140529929012992 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.23969122767448425, loss=1.7216249704360962
I0210 08:11:42.879317 140529937405696 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.21032752096652985, loss=1.8743774890899658
I0210 08:12:18.998164 140529929012992 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.19765517115592957, loss=1.785940408706665
I0210 08:12:55.195266 140529937405696 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.2192583531141281, loss=1.7702308893203735
I0210 08:13:31.288818 140529929012992 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.2262672334909439, loss=1.792019009590149
I0210 08:14:07.437824 140529937405696 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.21950334310531616, loss=1.8591502904891968
I0210 08:14:43.622500 140529929012992 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.27523812651634216, loss=1.7915226221084595
I0210 08:15:06.509963 140699726837568 spec.py:321] Evaluating on the training split.
I0210 08:15:09.538125 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 08:18:53.104244 140699726837568 spec.py:333] Evaluating on the validation split.
I0210 08:18:55.839107 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 08:21:32.097888 140699726837568 spec.py:349] Evaluating on the test split.
I0210 08:21:34.850077 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 08:24:09.274651 140699726837568 submission_runner.py:408] Time since start: 23933.09s, 	Step: 39565, 	{'train/accuracy': 0.6465423107147217, 'train/loss': 1.6878236532211304, 'train/bleu': 31.692694397409078, 'validation/accuracy': 0.6620872616767883, 'validation/loss': 1.5737826824188232, 'validation/bleu': 28.22098356404612, 'validation/num_examples': 3000, 'test/accuracy': 0.6753936409950256, 'test/loss': 1.4936915636062622, 'test/bleu': 27.86312619079604, 'test/num_examples': 3003, 'score': 14313.19044804573, 'total_duration': 23933.09076309204, 'accumulated_submission_time': 14313.19044804573, 'accumulated_eval_time': 9618.169555902481, 'accumulated_logging_time': 0.4737052917480469}
I0210 08:24:09.298881 140529937405696 logging_writer.py:48] [39565] accumulated_eval_time=9618.169556, accumulated_logging_time=0.473705, accumulated_submission_time=14313.190448, global_step=39565, preemption_count=0, score=14313.190448, test/accuracy=0.675394, test/bleu=27.863126, test/loss=1.493692, test/num_examples=3003, total_duration=23933.090763, train/accuracy=0.646542, train/bleu=31.692694, train/loss=1.687824, validation/accuracy=0.662087, validation/bleu=28.220984, validation/loss=1.573783, validation/num_examples=3000
I0210 08:24:22.263832 140529929012992 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.2306813895702362, loss=1.751894235610962
I0210 08:24:58.269747 140529937405696 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.3049461841583252, loss=1.7330305576324463
I0210 08:25:34.401492 140529929012992 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.22036033868789673, loss=1.7776697874069214
I0210 08:26:10.531725 140529937405696 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.21596786379814148, loss=1.7532318830490112
I0210 08:26:46.611762 140529929012992 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.1956908255815506, loss=1.83701753616333
I0210 08:27:22.719056 140529937405696 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.2064642310142517, loss=1.7666709423065186
I0210 08:27:58.805901 140529929012992 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.23770102858543396, loss=1.7467656135559082
I0210 08:28:34.900238 140529937405696 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.212225541472435, loss=1.7532705068588257
I0210 08:29:11.006932 140529929012992 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.22483223676681519, loss=1.7647993564605713
I0210 08:29:47.093929 140529937405696 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.20577245950698853, loss=1.7839772701263428
I0210 08:30:23.193208 140529929012992 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.20785018801689148, loss=1.8075568675994873
I0210 08:30:59.281676 140529937405696 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.210548534989357, loss=1.748152256011963
I0210 08:31:35.364699 140529929012992 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.19081547856330872, loss=1.7113288640975952
I0210 08:32:11.528831 140529937405696 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.1819247156381607, loss=1.6853175163269043
I0210 08:32:47.669936 140529929012992 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.18946178257465363, loss=1.7499868869781494
I0210 08:33:23.850369 140529937405696 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.2553095519542694, loss=1.8239729404449463
I0210 08:33:59.924716 140529929012992 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.21473602950572968, loss=1.788103699684143
I0210 08:34:36.023637 140529937405696 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.24446772038936615, loss=1.7690144777297974
I0210 08:35:12.120635 140529929012992 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.22126170992851257, loss=1.7091033458709717
I0210 08:35:48.250513 140529937405696 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.2018265426158905, loss=1.727657675743103
I0210 08:36:24.398276 140529929012992 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.2872175872325897, loss=1.7790237665176392
I0210 08:37:00.527993 140529937405696 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.19507652521133423, loss=1.8119398355484009
I0210 08:37:36.609112 140529929012992 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.20935887098312378, loss=1.8187270164489746
I0210 08:38:09.559630 140699726837568 spec.py:321] Evaluating on the training split.
I0210 08:38:12.600977 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 08:42:00.959533 140699726837568 spec.py:333] Evaluating on the validation split.
I0210 08:42:03.699854 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 08:44:40.980026 140699726837568 spec.py:349] Evaluating on the test split.
I0210 08:44:43.714138 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 08:47:16.446135 140699726837568 submission_runner.py:408] Time since start: 25320.26s, 	Step: 41893, 	{'train/accuracy': 0.6447368264198303, 'train/loss': 1.7097009420394897, 'train/bleu': 31.56412929199626, 'validation/accuracy': 0.6631907820701599, 'validation/loss': 1.5640273094177246, 'validation/bleu': 28.26689393593714, 'validation/num_examples': 3000, 'test/accuracy': 0.6745802164077759, 'test/loss': 1.4858415126800537, 'test/bleu': 28.026926555958124, 'test/num_examples': 3003, 'score': 15153.36160159111, 'total_duration': 25320.262265205383, 'accumulated_submission_time': 15153.36160159111, 'accumulated_eval_time': 10165.055995941162, 'accumulated_logging_time': 0.5107996463775635}
I0210 08:47:16.466773 140529937405696 logging_writer.py:48] [41893] accumulated_eval_time=10165.055996, accumulated_logging_time=0.510800, accumulated_submission_time=15153.361602, global_step=41893, preemption_count=0, score=15153.361602, test/accuracy=0.674580, test/bleu=28.026927, test/loss=1.485842, test/num_examples=3003, total_duration=25320.262265, train/accuracy=0.644737, train/bleu=31.564129, train/loss=1.709701, validation/accuracy=0.663191, validation/bleu=28.266894, validation/loss=1.564027, validation/num_examples=3000
I0210 08:47:19.357598 140529929012992 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.21260292828083038, loss=1.779586672782898
I0210 08:47:55.325067 140529937405696 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.18477290868759155, loss=1.7542164325714111
I0210 08:48:31.379228 140529929012992 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.19013135135173798, loss=1.7546366453170776
I0210 08:49:07.450099 140529937405696 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.21390099823474884, loss=1.751999855041504
I0210 08:49:43.527596 140529929012992 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.19047732651233673, loss=1.7301284074783325
I0210 08:50:19.620366 140529937405696 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.24539174139499664, loss=1.7374391555786133
I0210 08:50:55.678134 140529929012992 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.18769489228725433, loss=1.7328187227249146
I0210 08:51:31.797697 140529937405696 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.2479640245437622, loss=1.7310258150100708
I0210 08:52:07.972559 140529929012992 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.18916243314743042, loss=1.7404659986495972
I0210 08:52:44.080069 140529937405696 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.22596390545368195, loss=1.7544680833816528
I0210 08:53:20.199423 140529929012992 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.2199225127696991, loss=1.7019318342208862
I0210 08:53:56.316161 140529937405696 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.19717144966125488, loss=1.7511405944824219
I0210 08:54:32.437067 140529929012992 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.24193021655082703, loss=1.7953134775161743
I0210 08:55:08.561630 140529937405696 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.21697868406772614, loss=1.7729299068450928
I0210 08:55:44.685563 140529929012992 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.21046710014343262, loss=1.7120873928070068
I0210 08:56:20.782073 140529937405696 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.18732991814613342, loss=1.784127116203308
I0210 08:56:56.859202 140529929012992 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.20291092991828918, loss=1.7461824417114258
I0210 08:57:32.971502 140529937405696 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.2098243683576584, loss=1.7255308628082275
I0210 08:58:09.082802 140529929012992 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.24600116908550262, loss=1.780468225479126
I0210 08:58:45.223950 140529937405696 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.2456022948026657, loss=1.7583824396133423
I0210 08:59:21.354382 140529929012992 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.2239508330821991, loss=1.731322169303894
I0210 08:59:57.431753 140529937405696 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.21346516907215118, loss=1.7347133159637451
I0210 09:00:33.502579 140529929012992 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.2283799648284912, loss=1.723908543586731
I0210 09:01:09.594783 140529937405696 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.2154347002506256, loss=1.8230204582214355
I0210 09:01:16.568429 140699726837568 spec.py:321] Evaluating on the training split.
I0210 09:01:19.595058 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 09:05:18.160195 140699726837568 spec.py:333] Evaluating on the validation split.
I0210 09:05:20.887502 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 09:08:17.669537 140699726837568 spec.py:349] Evaluating on the test split.
I0210 09:08:20.399529 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 09:10:45.243459 140699726837568 submission_runner.py:408] Time since start: 26729.06s, 	Step: 44221, 	{'train/accuracy': 0.6637552380561829, 'train/loss': 1.5781522989273071, 'train/bleu': 32.93034087770076, 'validation/accuracy': 0.6653606295585632, 'validation/loss': 1.5589519739151, 'validation/bleu': 28.47223571166506, 'validation/num_examples': 3000, 'test/accuracy': 0.6778572201728821, 'test/loss': 1.4742341041564941, 'test/bleu': 28.249781697948197, 'test/num_examples': 3003, 'score': 15993.378037929535, 'total_duration': 26729.05960392952, 'accumulated_submission_time': 15993.378037929535, 'accumulated_eval_time': 10733.730982542038, 'accumulated_logging_time': 0.5428597927093506}
I0210 09:10:45.265413 140529929012992 logging_writer.py:48] [44221] accumulated_eval_time=10733.730983, accumulated_logging_time=0.542860, accumulated_submission_time=15993.378038, global_step=44221, preemption_count=0, score=15993.378038, test/accuracy=0.677857, test/bleu=28.249782, test/loss=1.474234, test/num_examples=3003, total_duration=26729.059604, train/accuracy=0.663755, train/bleu=32.930341, train/loss=1.578152, validation/accuracy=0.665361, validation/bleu=28.472236, validation/loss=1.558952, validation/num_examples=3000
I0210 09:11:14.036327 140529937405696 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.1975647658109665, loss=1.7420547008514404
I0210 09:11:50.053038 140529929012992 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.21004807949066162, loss=1.7760552167892456
I0210 09:12:26.157393 140529937405696 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.21272698044776917, loss=1.778976559638977
I0210 09:13:02.261749 140529929012992 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.20795226097106934, loss=1.7721242904663086
I0210 09:13:38.393361 140529937405696 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.1797877550125122, loss=1.6576805114746094
I0210 09:14:14.483362 140529929012992 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.18954487144947052, loss=1.7067866325378418
I0210 09:14:50.562004 140529937405696 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.20865567028522491, loss=1.9103376865386963
I0210 09:15:26.688513 140529929012992 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.21565145254135132, loss=1.7005289793014526
I0210 09:16:02.807987 140529937405696 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.21133695542812347, loss=1.7338837385177612
I0210 09:16:38.898519 140529929012992 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.1974935382604599, loss=1.7940099239349365
I0210 09:17:15.012996 140529937405696 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.23307348787784576, loss=1.7435742616653442
I0210 09:17:51.116756 140529929012992 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.20232589542865753, loss=1.7803527116775513
I0210 09:18:27.216452 140529937405696 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.22248783707618713, loss=1.840928316116333
I0210 09:19:03.343621 140529929012992 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.20397192239761353, loss=1.7806715965270996
I0210 09:19:39.428423 140529937405696 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.19270843267440796, loss=1.7669912576675415
I0210 09:20:15.572718 140529929012992 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.18678675591945648, loss=1.7476611137390137
I0210 09:20:51.690678 140529937405696 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.21911431849002838, loss=1.8279519081115723
I0210 09:21:27.841912 140529929012992 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.2507117986679077, loss=1.7220911979675293
I0210 09:22:03.980192 140529937405696 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.21926650404930115, loss=1.8226597309112549
I0210 09:22:40.076373 140529929012992 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.1969023197889328, loss=1.7518099546432495
I0210 09:23:16.184518 140529937405696 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.19524097442626953, loss=1.740527868270874
I0210 09:23:52.333646 140529929012992 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.1911812722682953, loss=1.7264187335968018
I0210 09:24:28.432011 140529937405696 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.1898954063653946, loss=1.6868841648101807
I0210 09:24:45.479749 140699726837568 spec.py:321] Evaluating on the training split.
I0210 09:24:48.509137 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 09:29:12.180901 140699726837568 spec.py:333] Evaluating on the validation split.
I0210 09:29:14.903998 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 09:32:20.582828 140699726837568 spec.py:349] Evaluating on the test split.
I0210 09:32:23.313182 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 09:35:07.805861 140699726837568 submission_runner.py:408] Time since start: 28191.62s, 	Step: 46549, 	{'train/accuracy': 0.6493027806282043, 'train/loss': 1.6772871017456055, 'train/bleu': 31.197649864945813, 'validation/accuracy': 0.6661293506622314, 'validation/loss': 1.5487326383590698, 'validation/bleu': 28.404300656637634, 'validation/num_examples': 3000, 'test/accuracy': 0.6787520051002502, 'test/loss': 1.4658998250961304, 'test/bleu': 28.08245586929687, 'test/num_examples': 3003, 'score': 16833.506383895874, 'total_duration': 28191.62198972702, 'accumulated_submission_time': 16833.506383895874, 'accumulated_eval_time': 11356.05702495575, 'accumulated_logging_time': 0.5749258995056152}
I0210 09:35:07.827446 140529929012992 logging_writer.py:48] [46549] accumulated_eval_time=11356.057025, accumulated_logging_time=0.574926, accumulated_submission_time=16833.506384, global_step=46549, preemption_count=0, score=16833.506384, test/accuracy=0.678752, test/bleu=28.082456, test/loss=1.465900, test/num_examples=3003, total_duration=28191.621990, train/accuracy=0.649303, train/bleu=31.197650, train/loss=1.677287, validation/accuracy=0.666129, validation/bleu=28.404301, validation/loss=1.548733, validation/num_examples=3000
I0210 09:35:26.510706 140529937405696 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.31204280257225037, loss=1.7533880472183228
I0210 09:36:02.469655 140529929012992 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.2896186411380768, loss=1.7479478120803833
I0210 09:36:38.527122 140529937405696 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.20687219500541687, loss=1.7559865713119507
I0210 09:37:14.605519 140529929012992 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.2148210108280182, loss=1.749969720840454
I0210 09:37:50.687008 140529937405696 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.20237959921360016, loss=1.84210205078125
I0210 09:38:26.778084 140529929012992 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.19728761911392212, loss=1.8444912433624268
I0210 09:39:02.875207 140529937405696 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.2635258436203003, loss=1.8247413635253906
I0210 09:39:39.019774 140529929012992 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.20642007887363434, loss=1.672726035118103
I0210 09:40:15.147100 140529937405696 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.22654244303703308, loss=1.7225896120071411
I0210 09:40:51.259688 140529929012992 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.1997561901807785, loss=1.777525544166565
I0210 09:41:27.374631 140529937405696 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.21140973269939423, loss=1.7062722444534302
I0210 09:42:03.478876 140529929012992 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.18505819141864777, loss=1.8178428411483765
I0210 09:42:39.572355 140529937405696 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.21215470135211945, loss=1.7356383800506592
I0210 09:43:15.708407 140529929012992 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.20063799619674683, loss=1.7593779563903809
I0210 09:43:51.863844 140529937405696 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.20345373451709747, loss=1.7113856077194214
I0210 09:44:27.945006 140529929012992 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.20290204882621765, loss=1.6574370861053467
I0210 09:45:04.068793 140529937405696 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.2054683119058609, loss=1.7551922798156738
I0210 09:45:40.172677 140529929012992 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.20564651489257812, loss=1.7728018760681152
I0210 09:46:16.268595 140529937405696 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.22093388438224792, loss=1.7738205194473267
I0210 09:46:52.365497 140529929012992 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.21370382606983185, loss=1.742142915725708
I0210 09:47:28.443800 140529937405696 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.24572348594665527, loss=1.7512693405151367
I0210 09:48:04.542004 140529929012992 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.2341555804014206, loss=1.7941713333129883
I0210 09:48:40.674460 140529937405696 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.18773306906223297, loss=1.6966642141342163
I0210 09:49:07.876289 140699726837568 spec.py:321] Evaluating on the training split.
I0210 09:49:10.904037 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 09:53:04.323934 140699726837568 spec.py:333] Evaluating on the validation split.
I0210 09:53:07.047269 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 09:55:49.155932 140699726837568 spec.py:349] Evaluating on the test split.
I0210 09:55:51.892913 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 09:58:18.249286 140699726837568 submission_runner.py:408] Time since start: 29582.07s, 	Step: 48877, 	{'train/accuracy': 0.6498969793319702, 'train/loss': 1.671565294265747, 'train/bleu': 31.67379922402138, 'validation/accuracy': 0.6682496070861816, 'validation/loss': 1.5388306379318237, 'validation/bleu': 28.527888487046358, 'validation/num_examples': 3000, 'test/accuracy': 0.6791703104972839, 'test/loss': 1.4580789804458618, 'test/bleu': 28.135240299714553, 'test/num_examples': 3003, 'score': 17673.46990466118, 'total_duration': 29582.065421819687, 'accumulated_submission_time': 17673.46990466118, 'accumulated_eval_time': 11906.429987430573, 'accumulated_logging_time': 0.6067063808441162}
I0210 09:58:18.271408 140529929012992 logging_writer.py:48] [48877] accumulated_eval_time=11906.429987, accumulated_logging_time=0.606706, accumulated_submission_time=17673.469905, global_step=48877, preemption_count=0, score=17673.469905, test/accuracy=0.679170, test/bleu=28.135240, test/loss=1.458079, test/num_examples=3003, total_duration=29582.065422, train/accuracy=0.649897, train/bleu=31.673799, train/loss=1.671565, validation/accuracy=0.668250, validation/bleu=28.527888, validation/loss=1.538831, validation/num_examples=3000
I0210 09:58:26.908506 140529937405696 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.2116057127714157, loss=1.8115308284759521
I0210 09:59:02.832197 140529929012992 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.2084895819425583, loss=1.7450940608978271
I0210 09:59:38.895834 140529937405696 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.21392668783664703, loss=1.7267552614212036
I0210 10:00:15.050169 140529929012992 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.22148986160755157, loss=1.7113841772079468
I0210 10:00:51.136422 140529937405696 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.21907085180282593, loss=1.756542682647705
I0210 10:01:27.200862 140529929012992 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.20231054723262787, loss=1.7875847816467285
I0210 10:02:03.291123 140529937405696 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.23583129048347473, loss=1.6897832155227661
I0210 10:02:39.434291 140529929012992 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.2055211067199707, loss=1.742573857307434
I0210 10:03:15.581500 140529937405696 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.20979273319244385, loss=1.806004524230957
I0210 10:03:51.699864 140529929012992 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.22421492636203766, loss=1.8254982233047485
I0210 10:04:27.783097 140529937405696 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.1981268972158432, loss=1.8391424417495728
I0210 10:05:03.871664 140529929012992 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.42087048292160034, loss=1.8956972360610962
I0210 10:05:39.968823 140529937405696 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.20781143009662628, loss=1.7541741132736206
I0210 10:06:16.107933 140529929012992 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.19810302555561066, loss=1.7425068616867065
I0210 10:06:52.232528 140529937405696 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.6444675326347351, loss=1.7686442136764526
I0210 10:07:28.372013 140529929012992 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.19695650041103363, loss=1.7614513635635376
I0210 10:08:04.467010 140529937405696 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.21119444072246552, loss=1.7037361860275269
I0210 10:08:40.577343 140529929012992 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.2427954077720642, loss=1.7370704412460327
I0210 10:09:16.679720 140529937405696 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.23216137290000916, loss=1.735551357269287
I0210 10:09:52.781696 140529929012992 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.21522442996501923, loss=1.8021193742752075
I0210 10:10:28.882442 140529937405696 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.23743024468421936, loss=1.7725653648376465
I0210 10:11:05.051053 140529929012992 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.2212187647819519, loss=1.7110300064086914
I0210 10:11:41.158699 140529937405696 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.20079566538333893, loss=1.677711844444275
I0210 10:12:17.255430 140529929012992 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.20252098143100739, loss=1.7148301601409912
I0210 10:12:18.426293 140699726837568 spec.py:321] Evaluating on the training split.
I0210 10:12:21.453281 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 10:15:48.847969 140699726837568 spec.py:333] Evaluating on the validation split.
I0210 10:15:51.573610 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 10:18:53.201967 140699726837568 spec.py:349] Evaluating on the test split.
I0210 10:18:55.965926 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 10:21:35.734580 140699726837568 submission_runner.py:408] Time since start: 30979.55s, 	Step: 51205, 	{'train/accuracy': 0.6544747352600098, 'train/loss': 1.6260355710983276, 'train/bleu': 32.19056354138812, 'validation/accuracy': 0.6689439415931702, 'validation/loss': 1.538604497909546, 'validation/bleu': 29.092913139909744, 'validation/num_examples': 3000, 'test/accuracy': 0.6821568012237549, 'test/loss': 1.45171320438385, 'test/bleu': 28.69368597124545, 'test/num_examples': 3003, 'score': 18513.543728113174, 'total_duration': 30979.550669908524, 'accumulated_submission_time': 18513.543728113174, 'accumulated_eval_time': 12463.738171815872, 'accumulated_logging_time': 0.638897180557251}
I0210 10:21:35.763068 140529937405696 logging_writer.py:48] [51205] accumulated_eval_time=12463.738172, accumulated_logging_time=0.638897, accumulated_submission_time=18513.543728, global_step=51205, preemption_count=0, score=18513.543728, test/accuracy=0.682157, test/bleu=28.693686, test/loss=1.451713, test/num_examples=3003, total_duration=30979.550670, train/accuracy=0.654475, train/bleu=32.190564, train/loss=1.626036, validation/accuracy=0.668944, validation/bleu=29.092913, validation/loss=1.538604, validation/num_examples=3000
I0210 10:22:10.231600 140529929012992 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.20864087343215942, loss=1.7782602310180664
I0210 10:22:46.222746 140529937405696 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.25026893615722656, loss=1.6658393144607544
I0210 10:23:22.327215 140529929012992 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.21062146127223969, loss=1.6923658847808838
I0210 10:23:58.388483 140529937405696 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.21696698665618896, loss=1.8084874153137207
I0210 10:24:34.484797 140529929012992 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.21441394090652466, loss=1.7457088232040405
I0210 10:25:10.563210 140529937405696 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.2135108858346939, loss=1.8173589706420898
I0210 10:25:46.629129 140529929012992 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.22454631328582764, loss=1.706009030342102
I0210 10:26:22.704938 140529937405696 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.21791720390319824, loss=1.7162052392959595
I0210 10:26:58.772546 140529929012992 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.18832865357398987, loss=1.7511695623397827
I0210 10:27:34.887809 140529937405696 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.2580929696559906, loss=1.803850531578064
I0210 10:28:10.987363 140529929012992 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.2004818469285965, loss=1.7189326286315918
I0210 10:28:47.161598 140529937405696 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.19593358039855957, loss=1.7979848384857178
I0210 10:29:23.316827 140529929012992 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.20021238923072815, loss=1.7386078834533691
I0210 10:29:59.409354 140529937405696 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.20422670245170593, loss=1.7573206424713135
I0210 10:30:35.511970 140529929012992 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.21109747886657715, loss=1.677454948425293
I0210 10:31:11.615220 140529937405696 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.20950926840305328, loss=1.7333753108978271
I0210 10:31:47.763023 140529929012992 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.19363877177238464, loss=1.7179590463638306
I0210 10:32:23.881807 140529937405696 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.21898892521858215, loss=1.710563063621521
I0210 10:32:59.998088 140529929012992 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.21295878291130066, loss=1.6615039110183716
I0210 10:33:36.094319 140529937405696 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.1915375292301178, loss=1.6860531568527222
I0210 10:34:12.272294 140529929012992 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.23341666162014008, loss=1.6421340703964233
I0210 10:34:48.416085 140529937405696 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.19335134327411652, loss=1.7426631450653076
I0210 10:35:24.528612 140529929012992 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.1989729404449463, loss=1.726098895072937
I0210 10:35:35.813537 140699726837568 spec.py:321] Evaluating on the training split.
I0210 10:35:38.842425 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 10:39:20.249617 140699726837568 spec.py:333] Evaluating on the validation split.
I0210 10:39:23.005595 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 10:42:16.193671 140699726837568 spec.py:349] Evaluating on the test split.
I0210 10:42:18.937347 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 10:44:59.685987 140699726837568 submission_runner.py:408] Time since start: 32383.50s, 	Step: 53533, 	{'train/accuracy': 0.6519391536712646, 'train/loss': 1.6503171920776367, 'train/bleu': 32.23124119536476, 'validation/accuracy': 0.6698243021965027, 'validation/loss': 1.5270503759384155, 'validation/bleu': 28.783845775866695, 'validation/num_examples': 3000, 'test/accuracy': 0.6848992109298706, 'test/loss': 1.4424127340316772, 'test/bleu': 28.1405950398642, 'test/num_examples': 3003, 'score': 19353.507332086563, 'total_duration': 32383.502128124237, 'accumulated_submission_time': 19353.507332086563, 'accumulated_eval_time': 13027.610567808151, 'accumulated_logging_time': 0.6803698539733887}
I0210 10:44:59.708528 140529937405696 logging_writer.py:48] [53533] accumulated_eval_time=13027.610568, accumulated_logging_time=0.680370, accumulated_submission_time=19353.507332, global_step=53533, preemption_count=0, score=19353.507332, test/accuracy=0.684899, test/bleu=28.140595, test/loss=1.442413, test/num_examples=3003, total_duration=32383.502128, train/accuracy=0.651939, train/bleu=32.231241, train/loss=1.650317, validation/accuracy=0.669824, validation/bleu=28.783846, validation/loss=1.527050, validation/num_examples=3000
I0210 10:45:24.115118 140529929012992 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.18343661725521088, loss=1.685475468635559
I0210 10:46:00.136040 140529937405696 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.23112745583057404, loss=1.7793235778808594
I0210 10:46:36.249448 140529929012992 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.1965901106595993, loss=1.7069209814071655
I0210 10:47:12.374680 140529937405696 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.22652406990528107, loss=1.7192398309707642
I0210 10:47:48.630836 140529929012992 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.20855215191841125, loss=1.704306721687317
I0210 10:48:24.777013 140529937405696 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.2176324874162674, loss=1.8579800128936768
I0210 10:49:00.890103 140529929012992 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.1894586831331253, loss=1.7626999616622925
I0210 10:49:37.027955 140529937405696 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.21334730088710785, loss=1.7362205982208252
I0210 10:50:13.142362 140529929012992 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.26250720024108887, loss=1.7178725004196167
I0210 10:50:49.222555 140529937405696 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.21541915833950043, loss=1.813004493713379
I0210 10:51:25.332942 140529929012992 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.20307745039463043, loss=1.7490938901901245
I0210 10:52:01.501235 140529937405696 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.21281878650188446, loss=1.7632672786712646
I0210 10:52:37.635723 140529929012992 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.20302696526050568, loss=1.6755529642105103
I0210 10:53:13.723460 140529937405696 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.21298760175704956, loss=1.760489583015442
I0210 10:53:49.867591 140529929012992 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.256404310464859, loss=1.774977445602417
I0210 10:54:25.962941 140529937405696 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.21181078255176544, loss=1.7119812965393066
I0210 10:55:02.142560 140529929012992 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.21659620106220245, loss=1.6757421493530273
I0210 10:55:38.320902 140529937405696 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.18141940236091614, loss=1.6747666597366333
I0210 10:56:14.448323 140529929012992 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.20624816417694092, loss=1.7211356163024902
I0210 10:56:50.559475 140529937405696 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.20234234631061554, loss=1.7821788787841797
I0210 10:57:26.723696 140529929012992 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.2552279531955719, loss=1.8026583194732666
I0210 10:58:02.831619 140529937405696 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.22553199529647827, loss=1.608911395072937
I0210 10:58:38.928448 140529929012992 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.5491846203804016, loss=1.7241172790527344
I0210 10:58:59.979591 140699726837568 spec.py:321] Evaluating on the training split.
I0210 10:59:03.021117 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 11:02:54.295598 140699726837568 spec.py:333] Evaluating on the validation split.
I0210 11:02:57.017855 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 11:05:56.013479 140699726837568 spec.py:349] Evaluating on the test split.
I0210 11:05:58.736241 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 11:08:36.451459 140699726837568 submission_runner.py:408] Time since start: 33800.27s, 	Step: 55860, 	{'train/accuracy': 0.6507437229156494, 'train/loss': 1.672051191329956, 'train/bleu': 32.41903297717362, 'validation/accuracy': 0.6705806255340576, 'validation/loss': 1.521011233329773, 'validation/bleu': 29.09612197316751, 'validation/num_examples': 3000, 'test/accuracy': 0.68580561876297, 'test/loss': 1.4303960800170898, 'test/bleu': 29.12164847468511, 'test/num_examples': 3003, 'score': 20193.692486524582, 'total_duration': 33800.267602682114, 'accumulated_submission_time': 20193.692486524582, 'accumulated_eval_time': 13604.082396030426, 'accumulated_logging_time': 0.7132043838500977}
I0210 11:08:36.473915 140529937405696 logging_writer.py:48] [55860] accumulated_eval_time=13604.082396, accumulated_logging_time=0.713204, accumulated_submission_time=20193.692487, global_step=55860, preemption_count=0, score=20193.692487, test/accuracy=0.685806, test/bleu=29.121648, test/loss=1.430396, test/num_examples=3003, total_duration=33800.267603, train/accuracy=0.650744, train/bleu=32.419033, train/loss=1.672051, validation/accuracy=0.670581, validation/bleu=29.096122, validation/loss=1.521011, validation/num_examples=3000
I0210 11:08:51.194626 140529929012992 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.20000006258487701, loss=1.6970289945602417
I0210 11:09:27.188821 140529937405696 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.1862965077161789, loss=1.7373415231704712
I0210 11:10:03.403257 140529929012992 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.2061363160610199, loss=1.7172212600708008
I0210 11:10:39.551645 140529937405696 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.23845763504505157, loss=1.7987278699874878
I0210 11:11:15.646966 140529929012992 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.20114973187446594, loss=1.7581148147583008
I0210 11:11:51.768589 140529937405696 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.1906370371580124, loss=1.699926495552063
I0210 11:12:27.851659 140529929012992 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.2025313377380371, loss=1.7654832601547241
I0210 11:13:03.928025 140529937405696 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.21868091821670532, loss=1.7478559017181396
I0210 11:13:40.027229 140529929012992 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.19481070339679718, loss=1.631963849067688
I0210 11:14:16.136069 140529937405696 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.18534442782402039, loss=1.6986942291259766
I0210 11:14:52.257081 140529929012992 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.2055109739303589, loss=1.6377657651901245
I0210 11:15:28.344124 140529937405696 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.20323707163333893, loss=1.5899280309677124
I0210 11:16:04.509755 140529929012992 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.19763228297233582, loss=1.793406367301941
I0210 11:16:40.639353 140529937405696 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.22898197174072266, loss=1.7562810182571411
I0210 11:17:16.759155 140529929012992 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.8156439065933228, loss=1.7368931770324707
I0210 11:17:52.862112 140529937405696 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.24551866948604584, loss=1.69871985912323
I0210 11:18:28.959468 140529929012992 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.18649016320705414, loss=1.6765066385269165
I0210 11:19:05.061228 140529937405696 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.21749162673950195, loss=1.65453040599823
I0210 11:19:41.209585 140529929012992 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.1905604749917984, loss=1.690903663635254
I0210 11:20:17.318133 140529937405696 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.19958063960075378, loss=1.648163080215454
I0210 11:20:53.388196 140529929012992 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.20345093309879303, loss=1.7580543756484985
I0210 11:21:29.507125 140529937405696 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.2189054936170578, loss=1.778266191482544
I0210 11:22:05.601228 140529929012992 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.6585904955863953, loss=1.788739562034607
I0210 11:22:36.744102 140699726837568 spec.py:321] Evaluating on the training split.
I0210 11:22:39.770535 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 11:26:34.554359 140699726837568 spec.py:333] Evaluating on the validation split.
I0210 11:26:37.279861 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 11:29:42.141452 140699726837568 spec.py:349] Evaluating on the test split.
I0210 11:29:44.868853 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 11:33:11.031192 140699726837568 submission_runner.py:408] Time since start: 35274.85s, 	Step: 58188, 	{'train/accuracy': 0.6537730693817139, 'train/loss': 1.6372880935668945, 'train/bleu': 32.227818247947184, 'validation/accuracy': 0.6702582836151123, 'validation/loss': 1.5269571542739868, 'validation/bleu': 28.863251676095942, 'validation/num_examples': 3000, 'test/accuracy': 0.6871768236160278, 'test/loss': 1.433237910270691, 'test/bleu': 28.67539179023136, 'test/num_examples': 3003, 'score': 21033.878321647644, 'total_duration': 35274.847340106964, 'accumulated_submission_time': 21033.878321647644, 'accumulated_eval_time': 14238.36943602562, 'accumulated_logging_time': 0.7463061809539795}
I0210 11:33:11.053943 140529937405696 logging_writer.py:48] [58188] accumulated_eval_time=14238.369436, accumulated_logging_time=0.746306, accumulated_submission_time=21033.878322, global_step=58188, preemption_count=0, score=21033.878322, test/accuracy=0.687177, test/bleu=28.675392, test/loss=1.433238, test/num_examples=3003, total_duration=35274.847340, train/accuracy=0.653773, train/bleu=32.227818, train/loss=1.637288, validation/accuracy=0.670258, validation/bleu=28.863252, validation/loss=1.526957, validation/num_examples=3000
I0210 11:33:15.741983 140529929012992 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.19522608816623688, loss=1.6894776821136475
I0210 11:33:51.689211 140529937405696 logging_writer.py:48] [58300] global_step=58300, grad_norm=0.2130526453256607, loss=1.6513497829437256
I0210 11:34:27.697156 140529929012992 logging_writer.py:48] [58400] global_step=58400, grad_norm=0.2067847102880478, loss=1.734565019607544
I0210 11:35:03.748778 140529937405696 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.1992114782333374, loss=1.7228422164916992
I0210 11:35:39.819907 140529929012992 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.20689508318901062, loss=1.6788856983184814
I0210 11:36:15.967684 140529937405696 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.3505353629589081, loss=1.7048912048339844
I0210 11:36:52.070442 140529929012992 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.22412697970867157, loss=1.696652889251709
I0210 11:37:28.247234 140529937405696 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.20242652297019958, loss=1.8122512102127075
I0210 11:38:04.387322 140529929012992 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.2576484680175781, loss=1.7061185836791992
I0210 11:38:40.497250 140529937405696 logging_writer.py:48] [59100] global_step=59100, grad_norm=0.19837723672389984, loss=1.656973123550415
I0210 11:39:16.664242 140529929012992 logging_writer.py:48] [59200] global_step=59200, grad_norm=0.3318652808666229, loss=1.7904155254364014
I0210 11:39:52.779372 140529937405696 logging_writer.py:48] [59300] global_step=59300, grad_norm=0.2314968854188919, loss=1.7060784101486206
I0210 11:40:28.861451 140529929012992 logging_writer.py:48] [59400] global_step=59400, grad_norm=0.20531640946865082, loss=1.724346399307251
I0210 11:41:05.074963 140529937405696 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.21259275078773499, loss=1.6744413375854492
I0210 11:41:41.194835 140529929012992 logging_writer.py:48] [59600] global_step=59600, grad_norm=0.23887811601161957, loss=1.757128357887268
I0210 11:42:17.289748 140529937405696 logging_writer.py:48] [59700] global_step=59700, grad_norm=0.2112218141555786, loss=1.6677378416061401
I0210 11:42:53.370127 140529929012992 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.2010868936777115, loss=1.6421869993209839
I0210 11:43:29.502882 140529937405696 logging_writer.py:48] [59900] global_step=59900, grad_norm=0.19169360399246216, loss=1.6553831100463867
I0210 11:44:05.671050 140529929012992 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.20543967187404633, loss=1.747973918914795
I0210 11:44:41.790809 140529937405696 logging_writer.py:48] [60100] global_step=60100, grad_norm=0.20372049510478973, loss=1.6506614685058594
I0210 11:45:17.936185 140529929012992 logging_writer.py:48] [60200] global_step=60200, grad_norm=0.41191598773002625, loss=1.7372486591339111
I0210 11:45:54.044635 140529937405696 logging_writer.py:48] [60300] global_step=60300, grad_norm=0.20644858479499817, loss=1.71651029586792
I0210 11:46:30.134793 140529929012992 logging_writer.py:48] [60400] global_step=60400, grad_norm=0.20370209217071533, loss=1.6889848709106445
I0210 11:47:06.281435 140529937405696 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.19709111750125885, loss=1.7118520736694336
I0210 11:47:11.076911 140699726837568 spec.py:321] Evaluating on the training split.
I0210 11:47:14.107084 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 11:51:09.692366 140699726837568 spec.py:333] Evaluating on the validation split.
I0210 11:51:12.420546 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 11:55:28.252733 140699726837568 spec.py:349] Evaluating on the test split.
I0210 11:55:30.982753 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 11:59:13.766282 140699726837568 submission_runner.py:408] Time since start: 36837.58s, 	Step: 60515, 	{'train/accuracy': 0.6578980684280396, 'train/loss': 1.6208689212799072, 'train/bleu': 32.07658106977582, 'validation/accuracy': 0.6717089414596558, 'validation/loss': 1.5151821374893188, 'validation/bleu': 28.92682264501012, 'validation/num_examples': 3000, 'test/accuracy': 0.6871535778045654, 'test/loss': 1.4229494333267212, 'test/bleu': 29.073916460981607, 'test/num_examples': 3003, 'score': 21873.81586742401, 'total_duration': 36837.5823905468, 'accumulated_submission_time': 21873.81586742401, 'accumulated_eval_time': 14961.058718919754, 'accumulated_logging_time': 0.7788941860198975}
I0210 11:59:13.789761 140529929012992 logging_writer.py:48] [60515] accumulated_eval_time=14961.058719, accumulated_logging_time=0.778894, accumulated_submission_time=21873.815867, global_step=60515, preemption_count=0, score=21873.815867, test/accuracy=0.687154, test/bleu=29.073916, test/loss=1.422949, test/num_examples=3003, total_duration=36837.582391, train/accuracy=0.657898, train/bleu=32.076581, train/loss=1.620869, validation/accuracy=0.671709, validation/bleu=28.926823, validation/loss=1.515182, validation/num_examples=3000
I0210 11:59:44.727276 140529937405696 logging_writer.py:48] [60600] global_step=60600, grad_norm=0.1890348494052887, loss=1.6881952285766602
I0210 12:00:20.776672 140529929012992 logging_writer.py:48] [60700] global_step=60700, grad_norm=0.4234640300273895, loss=1.6697895526885986
I0210 12:00:56.871237 140529937405696 logging_writer.py:48] [60800] global_step=60800, grad_norm=0.2126556932926178, loss=1.7175441980361938
I0210 12:01:32.975321 140529929012992 logging_writer.py:48] [60900] global_step=60900, grad_norm=0.21319928765296936, loss=1.651219367980957
I0210 12:02:09.064467 140529937405696 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.20114901661872864, loss=1.6757251024246216
I0210 12:02:45.183110 140529929012992 logging_writer.py:48] [61100] global_step=61100, grad_norm=0.3057858347892761, loss=1.7294901609420776
I0210 12:03:21.292177 140529937405696 logging_writer.py:48] [61200] global_step=61200, grad_norm=0.22205348312854767, loss=1.7556954622268677
I0210 12:03:57.402588 140529929012992 logging_writer.py:48] [61300] global_step=61300, grad_norm=0.21661168336868286, loss=1.6775158643722534
I0210 12:04:33.633927 140529937405696 logging_writer.py:48] [61400] global_step=61400, grad_norm=0.19303418695926666, loss=1.6779521703720093
I0210 12:05:09.784718 140529929012992 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.44595521688461304, loss=1.6530836820602417
I0210 12:05:45.887236 140529937405696 logging_writer.py:48] [61600] global_step=61600, grad_norm=0.1998763233423233, loss=1.6216177940368652
I0210 12:06:21.975959 140529929012992 logging_writer.py:48] [61700] global_step=61700, grad_norm=0.18989239633083344, loss=1.6968032121658325
I0210 12:06:58.090584 140529937405696 logging_writer.py:48] [61800] global_step=61800, grad_norm=0.1950865536928177, loss=1.6913971900939941
I0210 12:07:34.263890 140529929012992 logging_writer.py:48] [61900] global_step=61900, grad_norm=0.20776070654392242, loss=1.7520277500152588
I0210 12:08:10.388791 140529937405696 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.21266795694828033, loss=1.748783826828003
I0210 12:08:46.581618 140529929012992 logging_writer.py:48] [62100] global_step=62100, grad_norm=0.2143588662147522, loss=1.6730036735534668
I0210 12:09:22.760768 140529937405696 logging_writer.py:48] [62200] global_step=62200, grad_norm=0.19346658885478973, loss=1.7222046852111816
I0210 12:09:58.910441 140529929012992 logging_writer.py:48] [62300] global_step=62300, grad_norm=0.19974230229854584, loss=1.6226978302001953
I0210 12:10:35.164918 140529937405696 logging_writer.py:48] [62400] global_step=62400, grad_norm=0.20762793719768524, loss=1.7023005485534668
I0210 12:11:11.333952 140529929012992 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.20409069955348969, loss=1.7447739839553833
I0210 12:11:47.480677 140529937405696 logging_writer.py:48] [62600] global_step=62600, grad_norm=0.2194925993680954, loss=1.770397663116455
I0210 12:12:23.551625 140529929012992 logging_writer.py:48] [62700] global_step=62700, grad_norm=0.1946060061454773, loss=1.7576007843017578
I0210 12:12:59.666800 140529937405696 logging_writer.py:48] [62800] global_step=62800, grad_norm=0.19249752163887024, loss=1.68000066280365
I0210 12:13:13.840838 140699726837568 spec.py:321] Evaluating on the training split.
I0210 12:13:16.876897 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 12:16:59.782788 140699726837568 spec.py:333] Evaluating on the validation split.
I0210 12:17:02.522982 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 12:19:41.845122 140699726837568 spec.py:349] Evaluating on the test split.
I0210 12:19:44.580158 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 12:22:09.685860 140699726837568 submission_runner.py:408] Time since start: 38213.50s, 	Step: 62841, 	{'train/accuracy': 0.6713626980781555, 'train/loss': 1.508202075958252, 'train/bleu': 33.69910168169331, 'validation/accuracy': 0.674411952495575, 'validation/loss': 1.5051673650741577, 'validation/bleu': 29.20011186816427, 'validation/num_examples': 3000, 'test/accuracy': 0.6859566569328308, 'test/loss': 1.4162017107009888, 'test/bleu': 28.6389104597473, 'test/num_examples': 3003, 'score': 22713.780904769897, 'total_duration': 38213.50196003914, 'accumulated_submission_time': 22713.780904769897, 'accumulated_eval_time': 15496.90365743637, 'accumulated_logging_time': 0.8135547637939453}
I0210 12:22:09.715941 140529929012992 logging_writer.py:48] [62841] accumulated_eval_time=15496.903657, accumulated_logging_time=0.813555, accumulated_submission_time=22713.780905, global_step=62841, preemption_count=0, score=22713.780905, test/accuracy=0.685957, test/bleu=28.638910, test/loss=1.416202, test/num_examples=3003, total_duration=38213.501960, train/accuracy=0.671363, train/bleu=33.699102, train/loss=1.508202, validation/accuracy=0.674412, validation/bleu=29.200112, validation/loss=1.505167, validation/num_examples=3000
I0210 12:22:31.293831 140529937405696 logging_writer.py:48] [62900] global_step=62900, grad_norm=0.21317818760871887, loss=1.7681825160980225
I0210 12:23:07.259528 140529929012992 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.19206646084785461, loss=1.7254598140716553
I0210 12:23:43.418447 140529937405696 logging_writer.py:48] [63100] global_step=63100, grad_norm=0.19946393370628357, loss=1.703194499015808
I0210 12:24:19.612151 140529929012992 logging_writer.py:48] [63200] global_step=63200, grad_norm=0.2250264585018158, loss=1.7216242551803589
I0210 12:24:55.752890 140529937405696 logging_writer.py:48] [63300] global_step=63300, grad_norm=0.2598137855529785, loss=1.7307472229003906
I0210 12:25:31.877874 140529929012992 logging_writer.py:48] [63400] global_step=63400, grad_norm=0.21256284415721893, loss=1.6440142393112183
I0210 12:26:08.011634 140529937405696 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.22530536353588104, loss=1.6234525442123413
I0210 12:26:44.097579 140529929012992 logging_writer.py:48] [63600] global_step=63600, grad_norm=0.19676995277404785, loss=1.6565120220184326
I0210 12:27:20.253678 140529937405696 logging_writer.py:48] [63700] global_step=63700, grad_norm=0.20154738426208496, loss=1.6895636320114136
I0210 12:27:56.431503 140529929012992 logging_writer.py:48] [63800] global_step=63800, grad_norm=0.23702019453048706, loss=1.7355929613113403
I0210 12:28:32.529540 140529937405696 logging_writer.py:48] [63900] global_step=63900, grad_norm=0.21464191377162933, loss=1.7666889429092407
I0210 12:29:08.634482 140529929012992 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.20482227206230164, loss=1.6177343130111694
I0210 12:29:44.764521 140529937405696 logging_writer.py:48] [64100] global_step=64100, grad_norm=0.21361584961414337, loss=1.6824504137039185
I0210 12:30:20.834692 140529929012992 logging_writer.py:48] [64200] global_step=64200, grad_norm=0.2011457234621048, loss=1.7640966176986694
I0210 12:30:56.936130 140529937405696 logging_writer.py:48] [64300] global_step=64300, grad_norm=0.21647608280181885, loss=1.6883052587509155
I0210 12:31:33.033203 140529929012992 logging_writer.py:48] [64400] global_step=64400, grad_norm=0.1954369843006134, loss=1.6853262186050415
I0210 12:32:09.153121 140529937405696 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.20113041996955872, loss=1.7645477056503296
I0210 12:32:45.239325 140529929012992 logging_writer.py:48] [64600] global_step=64600, grad_norm=0.19185876846313477, loss=1.6578320264816284
I0210 12:33:21.418712 140529937405696 logging_writer.py:48] [64700] global_step=64700, grad_norm=0.19572070240974426, loss=1.6040520668029785
I0210 12:33:57.550647 140529929012992 logging_writer.py:48] [64800] global_step=64800, grad_norm=0.19633539021015167, loss=1.700496792793274
I0210 12:34:33.646039 140529937405696 logging_writer.py:48] [64900] global_step=64900, grad_norm=0.2206203192472458, loss=1.7797707319259644
I0210 12:35:09.765060 140529929012992 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.19932827353477478, loss=1.6416168212890625
I0210 12:35:45.882832 140529937405696 logging_writer.py:48] [65100] global_step=65100, grad_norm=0.20604653656482697, loss=1.653279423713684
I0210 12:36:09.810965 140699726837568 spec.py:321] Evaluating on the training split.
I0210 12:36:12.834940 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 12:40:12.034500 140699726837568 spec.py:333] Evaluating on the validation split.
I0210 12:40:14.757522 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 12:42:53.068385 140699726837568 spec.py:349] Evaluating on the test split.
I0210 12:42:55.791454 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 12:45:26.087721 140699726837568 submission_runner.py:408] Time since start: 39609.90s, 	Step: 65168, 	{'train/accuracy': 0.65871661901474, 'train/loss': 1.6069515943527222, 'train/bleu': 32.66574289741518, 'validation/accuracy': 0.6746847629547119, 'validation/loss': 1.4960321187973022, 'validation/bleu': 29.051769116288494, 'validation/num_examples': 3000, 'test/accuracy': 0.6899308562278748, 'test/loss': 1.4054077863693237, 'test/bleu': 28.924297355169422, 'test/num_examples': 3003, 'score': 23553.788153648376, 'total_duration': 39609.90386343002, 'accumulated_submission_time': 23553.788153648376, 'accumulated_eval_time': 16053.180361270905, 'accumulated_logging_time': 0.8553059101104736}
I0210 12:45:26.112740 140529929012992 logging_writer.py:48] [65168] accumulated_eval_time=16053.180361, accumulated_logging_time=0.855306, accumulated_submission_time=23553.788154, global_step=65168, preemption_count=0, score=23553.788154, test/accuracy=0.689931, test/bleu=28.924297, test/loss=1.405408, test/num_examples=3003, total_duration=39609.903863, train/accuracy=0.658717, train/bleu=32.665743, train/loss=1.606952, validation/accuracy=0.674685, validation/bleu=29.051769, validation/loss=1.496032, validation/num_examples=3000
I0210 12:45:37.972101 140529937405696 logging_writer.py:48] [65200] global_step=65200, grad_norm=0.22522108256816864, loss=1.6901271343231201
I0210 12:46:13.918374 140529929012992 logging_writer.py:48] [65300] global_step=65300, grad_norm=0.2066037654876709, loss=1.7879353761672974
I0210 12:46:49.952669 140529937405696 logging_writer.py:48] [65400] global_step=65400, grad_norm=0.18452075123786926, loss=1.6428308486938477
I0210 12:47:25.999150 140529929012992 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.23298266530036926, loss=1.6704707145690918
I0210 12:48:02.112093 140529937405696 logging_writer.py:48] [65600] global_step=65600, grad_norm=0.3820696473121643, loss=1.6166133880615234
I0210 12:48:38.239492 140529929012992 logging_writer.py:48] [65700] global_step=65700, grad_norm=0.20444777607917786, loss=1.69853675365448
I0210 12:49:14.353892 140529937405696 logging_writer.py:48] [65800] global_step=65800, grad_norm=0.21522144973278046, loss=1.6481093168258667
I0210 12:49:50.480118 140529929012992 logging_writer.py:48] [65900] global_step=65900, grad_norm=0.20352782309055328, loss=1.735805869102478
I0210 12:50:26.620259 140529937405696 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.262844979763031, loss=1.7101454734802246
I0210 12:51:02.788002 140529929012992 logging_writer.py:48] [66100] global_step=66100, grad_norm=0.20460672676563263, loss=1.6928349733352661
I0210 12:51:38.902317 140529937405696 logging_writer.py:48] [66200] global_step=66200, grad_norm=0.22979015111923218, loss=1.6016905307769775
I0210 12:52:15.036545 140529929012992 logging_writer.py:48] [66300] global_step=66300, grad_norm=0.19004283845424652, loss=1.6118851900100708
I0210 12:52:51.151046 140529937405696 logging_writer.py:48] [66400] global_step=66400, grad_norm=0.19867639243602753, loss=1.6348987817764282
I0210 12:53:27.308341 140529929012992 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.2159930169582367, loss=1.7605175971984863
I0210 12:54:03.577160 140529937405696 logging_writer.py:48] [66600] global_step=66600, grad_norm=0.23726019263267517, loss=1.7612515687942505
I0210 12:54:39.685405 140529929012992 logging_writer.py:48] [66700] global_step=66700, grad_norm=0.2230241447687149, loss=1.6669150590896606
I0210 12:55:15.770382 140529937405696 logging_writer.py:48] [66800] global_step=66800, grad_norm=0.21160860359668732, loss=1.6967151165008545
I0210 12:55:51.861371 140529929012992 logging_writer.py:48] [66900] global_step=66900, grad_norm=0.19544608891010284, loss=1.6346064805984497
I0210 12:56:27.959126 140529937405696 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.2078736275434494, loss=1.6678897142410278
I0210 12:57:04.053917 140529929012992 logging_writer.py:48] [67100] global_step=67100, grad_norm=0.19807149469852448, loss=1.6991938352584839
I0210 12:57:40.149361 140529937405696 logging_writer.py:48] [67200] global_step=67200, grad_norm=0.2017858475446701, loss=1.7678844928741455
I0210 12:58:16.274073 140529929012992 logging_writer.py:48] [67300] global_step=67300, grad_norm=0.18943527340888977, loss=1.6625926494598389
I0210 12:58:52.401680 140529937405696 logging_writer.py:48] [67400] global_step=67400, grad_norm=0.3159969747066498, loss=1.642381191253662
I0210 12:59:26.392184 140699726837568 spec.py:321] Evaluating on the training split.
I0210 12:59:29.423214 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 13:03:24.781725 140699726837568 spec.py:333] Evaluating on the validation split.
I0210 13:03:27.508214 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 13:05:50.735917 140699726837568 spec.py:349] Evaluating on the test split.
I0210 13:05:53.467216 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 13:08:08.756047 140699726837568 submission_runner.py:408] Time since start: 40972.57s, 	Step: 67496, 	{'train/accuracy': 0.6580450534820557, 'train/loss': 1.6125664710998535, 'train/bleu': 32.3845854245405, 'validation/accuracy': 0.6748707294464111, 'validation/loss': 1.4918677806854248, 'validation/bleu': 29.354178552155997, 'validation/num_examples': 3000, 'test/accuracy': 0.689059317111969, 'test/loss': 1.4040769338607788, 'test/bleu': 29.383744057926585, 'test/num_examples': 3003, 'score': 24393.980915784836, 'total_duration': 40972.572177410126, 'accumulated_submission_time': 24393.980915784836, 'accumulated_eval_time': 16575.54416203499, 'accumulated_logging_time': 0.8911569118499756}
I0210 13:08:08.781721 140529929012992 logging_writer.py:48] [67496] accumulated_eval_time=16575.544162, accumulated_logging_time=0.891157, accumulated_submission_time=24393.980916, global_step=67496, preemption_count=0, score=24393.980916, test/accuracy=0.689059, test/bleu=29.383744, test/loss=1.404077, test/num_examples=3003, total_duration=40972.572177, train/accuracy=0.658045, train/bleu=32.384585, train/loss=1.612566, validation/accuracy=0.674871, validation/bleu=29.354179, validation/loss=1.491868, validation/num_examples=3000
I0210 13:08:10.598796 140529937405696 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.27167391777038574, loss=1.6666784286499023
I0210 13:08:46.548494 140529929012992 logging_writer.py:48] [67600] global_step=67600, grad_norm=0.21443462371826172, loss=1.7073110342025757
I0210 13:09:22.600855 140529937405696 logging_writer.py:48] [67700] global_step=67700, grad_norm=0.20050691068172455, loss=1.6922730207443237
I0210 13:09:58.670719 140529929012992 logging_writer.py:48] [67800] global_step=67800, grad_norm=0.20517633855342865, loss=1.6990890502929688
I0210 13:10:34.704547 140529937405696 logging_writer.py:48] [67900] global_step=67900, grad_norm=0.19644030928611755, loss=1.5927940607070923
I0210 13:11:10.787011 140529929012992 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.2091284692287445, loss=1.769935131072998
I0210 13:11:46.868799 140529937405696 logging_writer.py:48] [68100] global_step=68100, grad_norm=0.20788773894309998, loss=1.656049132347107
I0210 13:12:22.975588 140529929012992 logging_writer.py:48] [68200] global_step=68200, grad_norm=0.2200009524822235, loss=1.686600923538208
I0210 13:12:59.049855 140529937405696 logging_writer.py:48] [68300] global_step=68300, grad_norm=0.21393047273159027, loss=1.7217620611190796
I0210 13:13:35.210252 140529929012992 logging_writer.py:48] [68400] global_step=68400, grad_norm=0.21072590351104736, loss=1.7808126211166382
I0210 13:14:11.340975 140529937405696 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.2080724686384201, loss=1.6850037574768066
I0210 13:14:47.554786 140529929012992 logging_writer.py:48] [68600] global_step=68600, grad_norm=0.19394135475158691, loss=1.5996791124343872
I0210 13:15:23.734688 140529937405696 logging_writer.py:48] [68700] global_step=68700, grad_norm=0.20379199087619781, loss=1.6384737491607666
I0210 13:15:59.989726 140529929012992 logging_writer.py:48] [68800] global_step=68800, grad_norm=0.21042557060718536, loss=1.744529366493225
I0210 13:16:36.063805 140529937405696 logging_writer.py:48] [68900] global_step=68900, grad_norm=0.21962597966194153, loss=1.723374366760254
I0210 13:17:12.154806 140529929012992 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.21239539980888367, loss=1.6758488416671753
I0210 13:17:48.298712 140529937405696 logging_writer.py:48] [69100] global_step=69100, grad_norm=0.22159257531166077, loss=1.7216217517852783
I0210 13:18:24.487681 140529929012992 logging_writer.py:48] [69200] global_step=69200, grad_norm=0.2070690393447876, loss=1.6991361379623413
I0210 13:19:00.687000 140529937405696 logging_writer.py:48] [69300] global_step=69300, grad_norm=0.2227402776479721, loss=1.765769124031067
I0210 13:19:36.834991 140529929012992 logging_writer.py:48] [69400] global_step=69400, grad_norm=0.21759097278118134, loss=1.6183215379714966
I0210 13:20:12.952388 140529937405696 logging_writer.py:48] [69500] global_step=69500, grad_norm=0.18130478262901306, loss=1.660219669342041
I0210 13:20:49.075407 140529929012992 logging_writer.py:48] [69600] global_step=69600, grad_norm=0.19577431678771973, loss=1.6721397638320923
I0210 13:21:25.208685 140529937405696 logging_writer.py:48] [69700] global_step=69700, grad_norm=0.1929456740617752, loss=1.6884862184524536
I0210 13:22:01.363039 140529929012992 logging_writer.py:48] [69800] global_step=69800, grad_norm=0.20639893412590027, loss=1.6559921503067017
I0210 13:22:09.028733 140699726837568 spec.py:321] Evaluating on the training split.
I0210 13:22:12.066942 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 13:25:59.475744 140699726837568 spec.py:333] Evaluating on the validation split.
I0210 13:26:02.225086 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 13:29:13.082695 140699726837568 spec.py:349] Evaluating on the test split.
I0210 13:29:15.820710 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 13:32:09.103863 140699726837568 submission_runner.py:408] Time since start: 42412.92s, 	Step: 69823, 	{'train/accuracy': 0.6646310091018677, 'train/loss': 1.5558394193649292, 'train/bleu': 33.108476509492625, 'validation/accuracy': 0.6774001717567444, 'validation/loss': 1.4817792177200317, 'validation/bleu': 29.732924813863225, 'validation/num_examples': 3000, 'test/accuracy': 0.6909418702125549, 'test/loss': 1.3975272178649902, 'test/bleu': 29.22117196182941, 'test/num_examples': 3003, 'score': 25234.14297890663, 'total_duration': 42412.92000794411, 'accumulated_submission_time': 25234.14297890663, 'accumulated_eval_time': 17175.61923766136, 'accumulated_logging_time': 0.9281957149505615}
I0210 13:32:09.129863 140529937405696 logging_writer.py:48] [69823] accumulated_eval_time=17175.619238, accumulated_logging_time=0.928196, accumulated_submission_time=25234.142979, global_step=69823, preemption_count=0, score=25234.142979, test/accuracy=0.690942, test/bleu=29.221172, test/loss=1.397527, test/num_examples=3003, total_duration=42412.920008, train/accuracy=0.664631, train/bleu=33.108477, train/loss=1.555839, validation/accuracy=0.677400, validation/bleu=29.732925, validation/loss=1.481779, validation/num_examples=3000
I0210 13:32:37.135608 140529929012992 logging_writer.py:48] [69900] global_step=69900, grad_norm=0.18861664831638336, loss=1.6420432329177856
I0210 13:33:13.098918 140529937405696 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.18853133916854858, loss=1.6733129024505615
I0210 13:33:49.209300 140529929012992 logging_writer.py:48] [70100] global_step=70100, grad_norm=0.20572087168693542, loss=1.680412769317627
I0210 13:34:25.260366 140529937405696 logging_writer.py:48] [70200] global_step=70200, grad_norm=0.2037266194820404, loss=1.617271900177002
I0210 13:35:01.313995 140529929012992 logging_writer.py:48] [70300] global_step=70300, grad_norm=0.2062959223985672, loss=1.6318477392196655
I0210 13:35:37.414676 140529937405696 logging_writer.py:48] [70400] global_step=70400, grad_norm=0.20647767186164856, loss=1.6741111278533936
I0210 13:36:13.513492 140529929012992 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.26944872736930847, loss=1.6449954509735107
I0210 13:36:49.639834 140529937405696 logging_writer.py:48] [70600] global_step=70600, grad_norm=0.18602128326892853, loss=1.6845312118530273
I0210 13:37:25.768678 140529929012992 logging_writer.py:48] [70700] global_step=70700, grad_norm=0.21395938098430634, loss=1.654672384262085
I0210 13:38:01.843376 140529937405696 logging_writer.py:48] [70800] global_step=70800, grad_norm=0.25303715467453003, loss=1.6572991609573364
I0210 13:38:37.954010 140529929012992 logging_writer.py:48] [70900] global_step=70900, grad_norm=0.18978334963321686, loss=1.6489686965942383
I0210 13:39:14.049209 140529937405696 logging_writer.py:48] [71000] global_step=71000, grad_norm=0.21219995617866516, loss=1.622092843055725
I0210 13:39:50.149830 140529929012992 logging_writer.py:48] [71100] global_step=71100, grad_norm=0.21390824019908905, loss=1.6553345918655396
I0210 13:40:26.277126 140529937405696 logging_writer.py:48] [71200] global_step=71200, grad_norm=0.19000771641731262, loss=1.657983422279358
I0210 13:41:02.390300 140529929012992 logging_writer.py:48] [71300] global_step=71300, grad_norm=0.21030957996845245, loss=1.6777409315109253
I0210 13:41:38.515487 140529937405696 logging_writer.py:48] [71400] global_step=71400, grad_norm=0.19803962111473083, loss=1.6908456087112427
I0210 13:42:14.619394 140529929012992 logging_writer.py:48] [71500] global_step=71500, grad_norm=0.19670183956623077, loss=1.6734848022460938
I0210 13:42:50.721340 140529937405696 logging_writer.py:48] [71600] global_step=71600, grad_norm=0.26474282145500183, loss=1.609135389328003
I0210 13:43:26.813381 140529929012992 logging_writer.py:48] [71700] global_step=71700, grad_norm=0.23982372879981995, loss=1.724717140197754
I0210 13:44:02.913018 140529937405696 logging_writer.py:48] [71800] global_step=71800, grad_norm=0.19652844965457916, loss=1.6236532926559448
I0210 13:44:39.063482 140529929012992 logging_writer.py:48] [71900] global_step=71900, grad_norm=0.2148548662662506, loss=1.6568238735198975
I0210 13:45:15.227268 140529937405696 logging_writer.py:48] [72000] global_step=72000, grad_norm=0.2064012885093689, loss=1.6001559495925903
I0210 13:45:51.360535 140529929012992 logging_writer.py:48] [72100] global_step=72100, grad_norm=0.20312833786010742, loss=1.6285444498062134
I0210 13:46:09.204450 140699726837568 spec.py:321] Evaluating on the training split.
I0210 13:46:12.247077 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 13:50:17.658957 140699726837568 spec.py:333] Evaluating on the validation split.
I0210 13:50:20.398548 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 13:52:48.940476 140699726837568 spec.py:349] Evaluating on the test split.
I0210 13:52:51.694487 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 13:55:10.435674 140699726837568 submission_runner.py:408] Time since start: 43794.25s, 	Step: 72151, 	{'train/accuracy': 0.6619483828544617, 'train/loss': 1.584561824798584, 'train/bleu': 32.684329463700614, 'validation/accuracy': 0.676978588104248, 'validation/loss': 1.4765617847442627, 'validation/bleu': 29.445052111202394, 'validation/num_examples': 3000, 'test/accuracy': 0.6935332417488098, 'test/loss': 1.3811521530151367, 'test/bleu': 29.335350984322613, 'test/num_examples': 3003, 'score': 26074.134548187256, 'total_duration': 43794.25180768967, 'accumulated_submission_time': 26074.134548187256, 'accumulated_eval_time': 17716.850410461426, 'accumulated_logging_time': 0.9643950462341309}
I0210 13:55:10.461261 140529937405696 logging_writer.py:48] [72151] accumulated_eval_time=17716.850410, accumulated_logging_time=0.964395, accumulated_submission_time=26074.134548, global_step=72151, preemption_count=0, score=26074.134548, test/accuracy=0.693533, test/bleu=29.335351, test/loss=1.381152, test/num_examples=3003, total_duration=43794.251808, train/accuracy=0.661948, train/bleu=32.684329, train/loss=1.584562, validation/accuracy=0.676979, validation/bleu=29.445052, validation/loss=1.476562, validation/num_examples=3000
I0210 13:55:28.444441 140529929012992 logging_writer.py:48] [72200] global_step=72200, grad_norm=0.18988096714019775, loss=1.6440222263336182
I0210 13:56:04.471233 140529937405696 logging_writer.py:48] [72300] global_step=72300, grad_norm=0.2114441841840744, loss=1.6244829893112183
I0210 13:56:40.507039 140529929012992 logging_writer.py:48] [72400] global_step=72400, grad_norm=0.1937679797410965, loss=1.7047055959701538
I0210 13:57:16.566517 140529937405696 logging_writer.py:48] [72500] global_step=72500, grad_norm=0.2008480727672577, loss=1.7139378786087036
I0210 13:57:52.646938 140529929012992 logging_writer.py:48] [72600] global_step=72600, grad_norm=0.1931891143321991, loss=1.6302834749221802
I0210 13:58:28.756920 140529937405696 logging_writer.py:48] [72700] global_step=72700, grad_norm=0.19042442739009857, loss=1.5841903686523438
I0210 13:59:04.824006 140529929012992 logging_writer.py:48] [72800] global_step=72800, grad_norm=0.20746734738349915, loss=1.6993306875228882
I0210 13:59:40.910298 140529937405696 logging_writer.py:48] [72900] global_step=72900, grad_norm=0.2527875006198883, loss=1.7584019899368286
I0210 14:00:17.043102 140529929012992 logging_writer.py:48] [73000] global_step=73000, grad_norm=0.2045127898454666, loss=1.6727315187454224
I0210 14:00:53.133718 140529937405696 logging_writer.py:48] [73100] global_step=73100, grad_norm=0.2000875473022461, loss=1.6501432657241821
I0210 14:01:29.230744 140529929012992 logging_writer.py:48] [73200] global_step=73200, grad_norm=0.1968884915113449, loss=1.672668695449829
I0210 14:02:05.410656 140529937405696 logging_writer.py:48] [73300] global_step=73300, grad_norm=0.1993316113948822, loss=1.6491472721099854
I0210 14:02:41.581482 140529929012992 logging_writer.py:48] [73400] global_step=73400, grad_norm=0.2112922966480255, loss=1.6032153367996216
I0210 14:03:17.728360 140529937405696 logging_writer.py:48] [73500] global_step=73500, grad_norm=0.22601088881492615, loss=1.7032279968261719
I0210 14:03:53.835139 140529929012992 logging_writer.py:48] [73600] global_step=73600, grad_norm=0.20124734938144684, loss=1.540589451789856
I0210 14:04:29.934709 140529937405696 logging_writer.py:48] [73700] global_step=73700, grad_norm=0.19818584620952606, loss=1.7387980222702026
I0210 14:05:06.067806 140529929012992 logging_writer.py:48] [73800] global_step=73800, grad_norm=0.21523481607437134, loss=1.6656264066696167
I0210 14:05:42.249289 140529937405696 logging_writer.py:48] [73900] global_step=73900, grad_norm=0.1885761022567749, loss=1.6773825883865356
I0210 14:06:18.390361 140529929012992 logging_writer.py:48] [74000] global_step=74000, grad_norm=0.2121933251619339, loss=1.7362487316131592
I0210 14:06:54.496231 140529937405696 logging_writer.py:48] [74100] global_step=74100, grad_norm=0.20485778152942657, loss=1.6997610330581665
I0210 14:07:30.679452 140529929012992 logging_writer.py:48] [74200] global_step=74200, grad_norm=0.20931003987789154, loss=1.6116328239440918
I0210 14:08:06.808813 140529937405696 logging_writer.py:48] [74300] global_step=74300, grad_norm=0.18453197181224823, loss=1.6794925928115845
I0210 14:08:42.950637 140529929012992 logging_writer.py:48] [74400] global_step=74400, grad_norm=0.19860875606536865, loss=1.6255762577056885
I0210 14:09:10.473382 140699726837568 spec.py:321] Evaluating on the training split.
I0210 14:09:13.496098 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 14:12:51.815967 140699726837568 spec.py:333] Evaluating on the validation split.
I0210 14:12:54.545473 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 14:15:17.166662 140699726837568 spec.py:349] Evaluating on the test split.
I0210 14:15:19.887318 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 14:17:28.318988 140699726837568 submission_runner.py:408] Time since start: 45132.14s, 	Step: 74478, 	{'train/accuracy': 0.6594235301017761, 'train/loss': 1.5999988317489624, 'train/bleu': 32.67356970090248, 'validation/accuracy': 0.6777969002723694, 'validation/loss': 1.4699158668518066, 'validation/bleu': 29.571455647456066, 'validation/num_examples': 3000, 'test/accuracy': 0.69459068775177, 'test/loss': 1.3774783611297607, 'test/bleu': 29.20670844457939, 'test/num_examples': 3003, 'score': 26914.059263944626, 'total_duration': 45132.13513803482, 'accumulated_submission_time': 26914.059263944626, 'accumulated_eval_time': 18214.695971250534, 'accumulated_logging_time': 1.0013093948364258}
I0210 14:17:28.344665 140529937405696 logging_writer.py:48] [74478] accumulated_eval_time=18214.695971, accumulated_logging_time=1.001309, accumulated_submission_time=26914.059264, global_step=74478, preemption_count=0, score=26914.059264, test/accuracy=0.694591, test/bleu=29.206708, test/loss=1.377478, test/num_examples=3003, total_duration=45132.135138, train/accuracy=0.659424, train/bleu=32.673570, train/loss=1.599999, validation/accuracy=0.677797, validation/bleu=29.571456, validation/loss=1.469916, validation/num_examples=3000
I0210 14:17:36.607445 140529929012992 logging_writer.py:48] [74500] global_step=74500, grad_norm=0.19477562606334686, loss=1.6276400089263916
I0210 14:18:12.537336 140529937405696 logging_writer.py:48] [74600] global_step=74600, grad_norm=0.19581767916679382, loss=1.6343692541122437
I0210 14:18:48.578801 140529929012992 logging_writer.py:48] [74700] global_step=74700, grad_norm=0.20434880256652832, loss=1.6777312755584717
I0210 14:19:24.612273 140529937405696 logging_writer.py:48] [74800] global_step=74800, grad_norm=0.2237016260623932, loss=1.6819299459457397
I0210 14:20:00.738075 140529929012992 logging_writer.py:48] [74900] global_step=74900, grad_norm=0.21095924079418182, loss=1.647300362586975
I0210 14:20:36.834907 140529937405696 logging_writer.py:48] [75000] global_step=75000, grad_norm=0.20689000189304352, loss=1.6577752828598022
I0210 14:21:12.946378 140529929012992 logging_writer.py:48] [75100] global_step=75100, grad_norm=0.19487518072128296, loss=1.6591582298278809
I0210 14:21:49.089241 140529937405696 logging_writer.py:48] [75200] global_step=75200, grad_norm=0.25349095463752747, loss=1.6563396453857422
I0210 14:22:25.203031 140529929012992 logging_writer.py:48] [75300] global_step=75300, grad_norm=0.19217544794082642, loss=1.6144869327545166
I0210 14:23:01.354790 140529937405696 logging_writer.py:48] [75400] global_step=75400, grad_norm=0.19231094419956207, loss=1.7212814092636108
I0210 14:23:37.506655 140529929012992 logging_writer.py:48] [75500] global_step=75500, grad_norm=0.21902798116207123, loss=1.661359429359436
I0210 14:24:13.659547 140529937405696 logging_writer.py:48] [75600] global_step=75600, grad_norm=0.19668935239315033, loss=1.5715391635894775
I0210 14:24:49.826659 140529929012992 logging_writer.py:48] [75700] global_step=75700, grad_norm=0.2009468823671341, loss=1.6732759475708008
I0210 14:25:25.981024 140529937405696 logging_writer.py:48] [75800] global_step=75800, grad_norm=0.21800731122493744, loss=1.6236873865127563
I0210 14:26:02.065847 140529929012992 logging_writer.py:48] [75900] global_step=75900, grad_norm=0.21309266984462738, loss=1.7573491334915161
I0210 14:26:38.142015 140529937405696 logging_writer.py:48] [76000] global_step=76000, grad_norm=0.20085319876670837, loss=1.6017597913742065
I0210 14:27:14.229748 140529929012992 logging_writer.py:48] [76100] global_step=76100, grad_norm=0.2219114750623703, loss=1.6321552991867065
I0210 14:27:50.385291 140529937405696 logging_writer.py:48] [76200] global_step=76200, grad_norm=0.21621964871883392, loss=1.6648205518722534
I0210 14:28:26.498067 140529929012992 logging_writer.py:48] [76300] global_step=76300, grad_norm=0.22910071909427643, loss=1.682788372039795
I0210 14:29:02.590761 140529937405696 logging_writer.py:48] [76400] global_step=76400, grad_norm=0.1930811107158661, loss=1.6015770435333252
I0210 14:29:38.699545 140529929012992 logging_writer.py:48] [76500] global_step=76500, grad_norm=0.19972331821918488, loss=1.6121516227722168
I0210 14:30:14.811402 140529937405696 logging_writer.py:48] [76600] global_step=76600, grad_norm=0.20754654705524445, loss=1.6447229385375977
I0210 14:30:50.989220 140529929012992 logging_writer.py:48] [76700] global_step=76700, grad_norm=0.20781895518302917, loss=1.688093900680542
I0210 14:31:27.077476 140529937405696 logging_writer.py:48] [76800] global_step=76800, grad_norm=0.20865990221500397, loss=1.5725553035736084
I0210 14:31:28.612232 140699726837568 spec.py:321] Evaluating on the training split.
I0210 14:31:31.677198 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 14:35:21.502578 140699726837568 spec.py:333] Evaluating on the validation split.
I0210 14:35:24.223331 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 14:38:04.987019 140699726837568 spec.py:349] Evaluating on the test split.
I0210 14:38:07.716830 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 14:40:25.757046 140699726837568 submission_runner.py:408] Time since start: 46509.57s, 	Step: 76806, 	{'train/accuracy': 0.6694381833076477, 'train/loss': 1.5369466543197632, 'train/bleu': 33.0531892644505, 'validation/accuracy': 0.6802147626876831, 'validation/loss': 1.4623948335647583, 'validation/bleu': 29.47381621833204, 'validation/num_examples': 3000, 'test/accuracy': 0.6945674419403076, 'test/loss': 1.3712241649627686, 'test/bleu': 29.462568269990985, 'test/num_examples': 3003, 'score': 27754.242109537125, 'total_duration': 46509.57319569588, 'accumulated_submission_time': 27754.242109537125, 'accumulated_eval_time': 18751.840750455856, 'accumulated_logging_time': 1.0371296405792236}
I0210 14:40:25.783801 140529929012992 logging_writer.py:48] [76806] accumulated_eval_time=18751.840750, accumulated_logging_time=1.037130, accumulated_submission_time=27754.242110, global_step=76806, preemption_count=0, score=27754.242110, test/accuracy=0.694567, test/bleu=29.462568, test/loss=1.371224, test/num_examples=3003, total_duration=46509.573196, train/accuracy=0.669438, train/bleu=33.053189, train/loss=1.536947, validation/accuracy=0.680215, validation/bleu=29.473816, validation/loss=1.462395, validation/num_examples=3000
I0210 14:40:59.992476 140529937405696 logging_writer.py:48] [76900] global_step=76900, grad_norm=0.20370028913021088, loss=1.7356550693511963
I0210 14:41:36.027039 140529929012992 logging_writer.py:48] [77000] global_step=77000, grad_norm=0.2040497213602066, loss=1.6338952779769897
I0210 14:42:12.139905 140529937405696 logging_writer.py:48] [77100] global_step=77100, grad_norm=0.1853119432926178, loss=1.6373387575149536
I0210 14:42:48.505676 140529929012992 logging_writer.py:48] [77200] global_step=77200, grad_norm=0.20311704277992249, loss=1.6459071636199951
I0210 14:43:24.632627 140529937405696 logging_writer.py:48] [77300] global_step=77300, grad_norm=0.2096024602651596, loss=1.664336085319519
I0210 14:44:00.692645 140529929012992 logging_writer.py:48] [77400] global_step=77400, grad_norm=0.20507024228572845, loss=1.6409111022949219
I0210 14:44:36.872840 140529937405696 logging_writer.py:48] [77500] global_step=77500, grad_norm=0.21144580841064453, loss=1.6251143217086792
I0210 14:45:13.015324 140529929012992 logging_writer.py:48] [77600] global_step=77600, grad_norm=0.21145237982273102, loss=1.6502962112426758
I0210 14:45:49.079970 140529937405696 logging_writer.py:48] [77700] global_step=77700, grad_norm=0.21035265922546387, loss=1.6816178560256958
I0210 14:46:25.225208 140529929012992 logging_writer.py:48] [77800] global_step=77800, grad_norm=0.2102072685956955, loss=1.6235789060592651
I0210 14:47:01.332289 140529937405696 logging_writer.py:48] [77900] global_step=77900, grad_norm=0.2053505778312683, loss=1.695101022720337
I0210 14:47:37.439539 140529929012992 logging_writer.py:48] [78000] global_step=78000, grad_norm=0.22626297175884247, loss=1.631004810333252
I0210 14:48:13.585263 140529937405696 logging_writer.py:48] [78100] global_step=78100, grad_norm=0.19864509999752045, loss=1.6346982717514038
I0210 14:48:49.710255 140529929012992 logging_writer.py:48] [78200] global_step=78200, grad_norm=0.22885102033615112, loss=1.7012932300567627
I0210 14:49:25.804356 140529937405696 logging_writer.py:48] [78300] global_step=78300, grad_norm=0.1972191482782364, loss=1.679883360862732
I0210 14:50:01.927305 140529929012992 logging_writer.py:48] [78400] global_step=78400, grad_norm=0.20295001566410065, loss=1.6735975742340088
I0210 14:50:38.004900 140529937405696 logging_writer.py:48] [78500] global_step=78500, grad_norm=0.21801511943340302, loss=1.6112267971038818
I0210 14:51:14.135628 140529929012992 logging_writer.py:48] [78600] global_step=78600, grad_norm=0.19312402606010437, loss=1.5357342958450317
I0210 14:51:50.240365 140529937405696 logging_writer.py:48] [78700] global_step=78700, grad_norm=0.20715577900409698, loss=1.6438696384429932
I0210 14:52:26.354634 140529929012992 logging_writer.py:48] [78800] global_step=78800, grad_norm=0.21587371826171875, loss=1.6749992370605469
I0210 14:53:02.451231 140529937405696 logging_writer.py:48] [78900] global_step=78900, grad_norm=0.20573101937770844, loss=1.638032078742981
I0210 14:53:38.573513 140529929012992 logging_writer.py:48] [79000] global_step=79000, grad_norm=0.28538426756858826, loss=1.6347299814224243
I0210 14:54:14.703260 140529937405696 logging_writer.py:48] [79100] global_step=79100, grad_norm=0.2034561187028885, loss=1.5713527202606201
I0210 14:54:26.005509 140699726837568 spec.py:321] Evaluating on the training split.
I0210 14:54:29.040346 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 14:58:26.565642 140699726837568 spec.py:333] Evaluating on the validation split.
I0210 14:58:29.298812 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 15:01:23.439671 140699726837568 spec.py:349] Evaluating on the test split.
I0210 15:01:26.164728 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 15:03:55.108559 140699726837568 submission_runner.py:408] Time since start: 47918.92s, 	Step: 79133, 	{'train/accuracy': 0.6636430025100708, 'train/loss': 1.570115566253662, 'train/bleu': 32.54973127642245, 'validation/accuracy': 0.6804007291793823, 'validation/loss': 1.45924973487854, 'validation/bleu': 29.552745828566785, 'validation/num_examples': 3000, 'test/accuracy': 0.6958921551704407, 'test/loss': 1.3631484508514404, 'test/bleu': 29.431835227142336, 'test/num_examples': 3003, 'score': 28594.37763762474, 'total_duration': 47918.92470264435, 'accumulated_submission_time': 28594.37763762474, 'accumulated_eval_time': 19320.94375896454, 'accumulated_logging_time': 1.0745155811309814}
I0210 15:03:55.135628 140529929012992 logging_writer.py:48] [79133] accumulated_eval_time=19320.943759, accumulated_logging_time=1.074516, accumulated_submission_time=28594.377638, global_step=79133, preemption_count=0, score=28594.377638, test/accuracy=0.695892, test/bleu=29.431835, test/loss=1.363148, test/num_examples=3003, total_duration=47918.924703, train/accuracy=0.663643, train/bleu=32.549731, train/loss=1.570116, validation/accuracy=0.680401, validation/bleu=29.552746, validation/loss=1.459250, validation/num_examples=3000
I0210 15:04:19.564274 140529937405696 logging_writer.py:48] [79200] global_step=79200, grad_norm=0.2283399999141693, loss=1.6724587678909302
I0210 15:04:55.518465 140529929012992 logging_writer.py:48] [79300] global_step=79300, grad_norm=0.19803635776042938, loss=1.623448371887207
I0210 15:05:31.566201 140529937405696 logging_writer.py:48] [79400] global_step=79400, grad_norm=0.2078300565481186, loss=1.6275821924209595
I0210 15:06:07.647902 140529929012992 logging_writer.py:48] [79500] global_step=79500, grad_norm=0.21334534883499146, loss=1.5952644348144531
I0210 15:06:43.732315 140529937405696 logging_writer.py:48] [79600] global_step=79600, grad_norm=0.2136061191558838, loss=1.6843550205230713
I0210 15:07:19.847032 140529929012992 logging_writer.py:48] [79700] global_step=79700, grad_norm=0.20806509256362915, loss=1.6373096704483032
I0210 15:07:55.931207 140529937405696 logging_writer.py:48] [79800] global_step=79800, grad_norm=0.22074870765209198, loss=1.635177731513977
I0210 15:08:32.030578 140529929012992 logging_writer.py:48] [79900] global_step=79900, grad_norm=0.21005892753601074, loss=1.6526923179626465
I0210 15:09:08.135921 140529937405696 logging_writer.py:48] [80000] global_step=80000, grad_norm=0.20146936178207397, loss=1.6768697500228882
I0210 15:09:44.257070 140529929012992 logging_writer.py:48] [80100] global_step=80100, grad_norm=0.21135981380939484, loss=1.6065484285354614
I0210 15:10:20.323846 140529937405696 logging_writer.py:48] [80200] global_step=80200, grad_norm=0.21971188485622406, loss=1.6702063083648682
I0210 15:10:56.437653 140529929012992 logging_writer.py:48] [80300] global_step=80300, grad_norm=0.1994701325893402, loss=1.6797184944152832
I0210 15:11:32.573736 140529937405696 logging_writer.py:48] [80400] global_step=80400, grad_norm=2.5033485889434814, loss=1.5966997146606445
I0210 15:12:08.701700 140529929012992 logging_writer.py:48] [80500] global_step=80500, grad_norm=0.20378610491752625, loss=1.75694739818573
I0210 15:12:44.811933 140529937405696 logging_writer.py:48] [80600] global_step=80600, grad_norm=0.21913182735443115, loss=1.6083344221115112
I0210 15:13:20.925252 140529929012992 logging_writer.py:48] [80700] global_step=80700, grad_norm=0.20267118513584137, loss=1.6441935300827026
I0210 15:13:57.024411 140529937405696 logging_writer.py:48] [80800] global_step=80800, grad_norm=0.21272362768650055, loss=1.616555094718933
I0210 15:14:33.116731 140529929012992 logging_writer.py:48] [80900] global_step=80900, grad_norm=0.20765599608421326, loss=1.545143485069275
I0210 15:15:09.316599 140529937405696 logging_writer.py:48] [81000] global_step=81000, grad_norm=0.19394171237945557, loss=1.698850393295288
I0210 15:15:45.481195 140529929012992 logging_writer.py:48] [81100] global_step=81100, grad_norm=0.20509815216064453, loss=1.6566696166992188
I0210 15:16:21.625923 140529937405696 logging_writer.py:48] [81200] global_step=81200, grad_norm=0.19885291159152985, loss=1.6670002937316895
I0210 15:16:57.713133 140529929012992 logging_writer.py:48] [81300] global_step=81300, grad_norm=0.24113385379314423, loss=1.6407159566879272
I0210 15:17:33.818411 140529937405696 logging_writer.py:48] [81400] global_step=81400, grad_norm=0.19951863586902618, loss=1.624534010887146
I0210 15:17:55.205672 140699726837568 spec.py:321] Evaluating on the training split.
I0210 15:17:58.232309 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 15:21:44.528444 140699726837568 spec.py:333] Evaluating on the validation split.
I0210 15:21:47.257115 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 15:24:12.670578 140699726837568 spec.py:349] Evaluating on the test split.
I0210 15:24:15.413200 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 15:26:24.608533 140699726837568 submission_runner.py:408] Time since start: 49268.42s, 	Step: 81461, 	{'train/accuracy': 0.6872650384902954, 'train/loss': 1.421802282333374, 'train/bleu': 34.87201890341066, 'validation/accuracy': 0.6823846101760864, 'validation/loss': 1.4490602016448975, 'validation/bleu': 30.02667336732894, 'validation/num_examples': 3000, 'test/accuracy': 0.6979489922523499, 'test/loss': 1.3512593507766724, 'test/bleu': 29.82934247972239, 'test/num_examples': 3003, 'score': 29434.364958763123, 'total_duration': 49268.42466902733, 'accumulated_submission_time': 29434.364958763123, 'accumulated_eval_time': 19830.34655547142, 'accumulated_logging_time': 1.111635684967041}
I0210 15:26:24.635141 140529929012992 logging_writer.py:48] [81461] accumulated_eval_time=19830.346555, accumulated_logging_time=1.111636, accumulated_submission_time=29434.364959, global_step=81461, preemption_count=0, score=29434.364959, test/accuracy=0.697949, test/bleu=29.829342, test/loss=1.351259, test/num_examples=3003, total_duration=49268.424669, train/accuracy=0.687265, train/bleu=34.872019, train/loss=1.421802, validation/accuracy=0.682385, validation/bleu=30.026673, validation/loss=1.449060, validation/num_examples=3000
I0210 15:26:38.999990 140529937405696 logging_writer.py:48] [81500] global_step=81500, grad_norm=0.21828046441078186, loss=1.6560804843902588
I0210 15:27:14.970505 140529929012992 logging_writer.py:48] [81600] global_step=81600, grad_norm=0.21842798590660095, loss=1.6059342622756958
I0210 15:27:51.054912 140529937405696 logging_writer.py:48] [81700] global_step=81700, grad_norm=0.20981097221374512, loss=1.617211103439331
I0210 15:28:27.122500 140529929012992 logging_writer.py:48] [81800] global_step=81800, grad_norm=0.20852962136268616, loss=1.611051321029663
I0210 15:29:03.250860 140529937405696 logging_writer.py:48] [81900] global_step=81900, grad_norm=0.19055230915546417, loss=1.592005729675293
I0210 15:29:39.385092 140529929012992 logging_writer.py:48] [82000] global_step=82000, grad_norm=0.2114313691854477, loss=1.6840869188308716
I0210 15:30:15.507372 140529937405696 logging_writer.py:48] [82100] global_step=82100, grad_norm=0.21898816525936127, loss=1.5409047603607178
I0210 15:30:51.673173 140529929012992 logging_writer.py:48] [82200] global_step=82200, grad_norm=0.18949072062969208, loss=1.6274722814559937
I0210 15:31:27.736685 140529937405696 logging_writer.py:48] [82300] global_step=82300, grad_norm=0.22785651683807373, loss=1.5964785814285278
I0210 15:32:03.855563 140529929012992 logging_writer.py:48] [82400] global_step=82400, grad_norm=0.21495357155799866, loss=1.653304100036621
I0210 15:32:39.986252 140529937405696 logging_writer.py:48] [82500] global_step=82500, grad_norm=0.20487579703330994, loss=1.6367146968841553
I0210 15:33:16.091513 140529929012992 logging_writer.py:48] [82600] global_step=82600, grad_norm=0.2273554503917694, loss=1.7119204998016357
I0210 15:33:52.224569 140529937405696 logging_writer.py:48] [82700] global_step=82700, grad_norm=0.19973093271255493, loss=1.642802357673645
I0210 15:34:28.354477 140529929012992 logging_writer.py:48] [82800] global_step=82800, grad_norm=0.2118755728006363, loss=1.6935381889343262
I0210 15:35:04.523454 140529937405696 logging_writer.py:48] [82900] global_step=82900, grad_norm=0.2077702283859253, loss=1.6130856275558472
I0210 15:35:40.622991 140529929012992 logging_writer.py:48] [83000] global_step=83000, grad_norm=0.2406856119632721, loss=1.643550157546997
I0210 15:36:16.765443 140529937405696 logging_writer.py:48] [83100] global_step=83100, grad_norm=0.20897310972213745, loss=1.6084967851638794
I0210 15:36:52.945405 140529929012992 logging_writer.py:48] [83200] global_step=83200, grad_norm=0.20963504910469055, loss=1.6268013715744019
I0210 15:37:29.100326 140529937405696 logging_writer.py:48] [83300] global_step=83300, grad_norm=0.20859205722808838, loss=1.5951204299926758
I0210 15:38:05.240466 140529929012992 logging_writer.py:48] [83400] global_step=83400, grad_norm=0.20272468030452728, loss=1.6144946813583374
I0210 15:38:41.438776 140529937405696 logging_writer.py:48] [83500] global_step=83500, grad_norm=0.19817335903644562, loss=1.5485241413116455
I0210 15:39:17.619477 140529929012992 logging_writer.py:48] [83600] global_step=83600, grad_norm=0.2199746072292328, loss=1.5951708555221558
I0210 15:39:53.711781 140529937405696 logging_writer.py:48] [83700] global_step=83700, grad_norm=0.20439374446868896, loss=1.7076247930526733
I0210 15:40:24.842472 140699726837568 spec.py:321] Evaluating on the training split.
I0210 15:40:27.886944 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 15:44:58.580004 140699726837568 spec.py:333] Evaluating on the validation split.
I0210 15:45:01.327382 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 15:48:56.561742 140699726837568 spec.py:349] Evaluating on the test split.
I0210 15:48:59.301470 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 15:53:04.669148 140699726837568 submission_runner.py:408] Time since start: 50868.49s, 	Step: 83788, 	{'train/accuracy': 0.6727224588394165, 'train/loss': 1.5115309953689575, 'train/bleu': 33.53385947656073, 'validation/accuracy': 0.6825209856033325, 'validation/loss': 1.441343903541565, 'validation/bleu': 29.710900837467445, 'validation/num_examples': 3000, 'test/accuracy': 0.6982162594795227, 'test/loss': 1.3485827445983887, 'test/bleu': 29.646421542412785, 'test/num_examples': 3003, 'score': 30274.487243413925, 'total_duration': 50868.4852745533, 'accumulated_submission_time': 30274.487243413925, 'accumulated_eval_time': 20590.173165798187, 'accumulated_logging_time': 1.1480538845062256}
I0210 15:53:04.696559 140529929012992 logging_writer.py:48] [83788] accumulated_eval_time=20590.173166, accumulated_logging_time=1.148054, accumulated_submission_time=30274.487243, global_step=83788, preemption_count=0, score=30274.487243, test/accuracy=0.698216, test/bleu=29.646422, test/loss=1.348583, test/num_examples=3003, total_duration=50868.485275, train/accuracy=0.672722, train/bleu=33.533859, train/loss=1.511531, validation/accuracy=0.682521, validation/bleu=29.710901, validation/loss=1.441344, validation/num_examples=3000
I0210 15:53:09.383059 140529937405696 logging_writer.py:48] [83800] global_step=83800, grad_norm=0.20192882418632507, loss=1.6215139627456665
I0210 15:53:45.311166 140529929012992 logging_writer.py:48] [83900] global_step=83900, grad_norm=0.2126590460538864, loss=1.6291353702545166
I0210 15:54:21.387219 140529937405696 logging_writer.py:48] [84000] global_step=84000, grad_norm=0.21400271356105804, loss=1.6584563255310059
I0210 15:54:57.486547 140529929012992 logging_writer.py:48] [84100] global_step=84100, grad_norm=0.1846512258052826, loss=1.6345008611679077
I0210 15:55:33.616909 140529937405696 logging_writer.py:48] [84200] global_step=84200, grad_norm=0.23395083844661713, loss=1.6536028385162354
I0210 15:56:09.729313 140529929012992 logging_writer.py:48] [84300] global_step=84300, grad_norm=0.20358656346797943, loss=1.5502318143844604
I0210 15:56:45.794671 140529937405696 logging_writer.py:48] [84400] global_step=84400, grad_norm=0.20073924958705902, loss=1.632954716682434
I0210 15:57:21.869681 140529929012992 logging_writer.py:48] [84500] global_step=84500, grad_norm=0.20247569680213928, loss=1.582374095916748
I0210 15:57:58.015628 140529937405696 logging_writer.py:48] [84600] global_step=84600, grad_norm=0.23100225627422333, loss=1.5911303758621216
I0210 15:58:34.117637 140529929012992 logging_writer.py:48] [84700] global_step=84700, grad_norm=0.19824227690696716, loss=1.6266337633132935
I0210 15:59:10.186519 140529937405696 logging_writer.py:48] [84800] global_step=84800, grad_norm=0.21148359775543213, loss=1.6182706356048584
I0210 15:59:46.279907 140529929012992 logging_writer.py:48] [84900] global_step=84900, grad_norm=0.1957930475473404, loss=1.6514359712600708
I0210 16:00:22.421913 140529937405696 logging_writer.py:48] [85000] global_step=85000, grad_norm=0.2416703999042511, loss=1.5573089122772217
I0210 16:00:58.504588 140529929012992 logging_writer.py:48] [85100] global_step=85100, grad_norm=0.21561646461486816, loss=1.594115972518921
I0210 16:01:34.605878 140529937405696 logging_writer.py:48] [85200] global_step=85200, grad_norm=0.20891103148460388, loss=1.6121411323547363
I0210 16:02:10.731062 140529929012992 logging_writer.py:48] [85300] global_step=85300, grad_norm=0.20117270946502686, loss=1.5512031316757202
I0210 16:02:46.813808 140529937405696 logging_writer.py:48] [85400] global_step=85400, grad_norm=0.205962136387825, loss=1.6756987571716309
I0210 16:03:22.888031 140529929012992 logging_writer.py:48] [85500] global_step=85500, grad_norm=0.22641749680042267, loss=1.5677094459533691
I0210 16:03:58.996002 140529937405696 logging_writer.py:48] [85600] global_step=85600, grad_norm=0.20925219357013702, loss=1.5858885049819946
I0210 16:04:35.072073 140529929012992 logging_writer.py:48] [85700] global_step=85700, grad_norm=0.22923654317855835, loss=1.6701849699020386
I0210 16:05:11.195846 140529937405696 logging_writer.py:48] [85800] global_step=85800, grad_norm=0.20951972901821136, loss=1.5877292156219482
I0210 16:05:47.307936 140529929012992 logging_writer.py:48] [85900] global_step=85900, grad_norm=0.20557273924350739, loss=1.5614007711410522
I0210 16:06:23.461335 140529937405696 logging_writer.py:48] [86000] global_step=86000, grad_norm=0.2030470073223114, loss=1.6126151084899902
I0210 16:06:59.545441 140529929012992 logging_writer.py:48] [86100] global_step=86100, grad_norm=0.20383968949317932, loss=1.5439000129699707
I0210 16:07:04.685904 140699726837568 spec.py:321] Evaluating on the training split.
I0210 16:07:07.709269 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 16:10:46.380693 140699726837568 spec.py:333] Evaluating on the validation split.
I0210 16:10:49.110281 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 16:13:38.475210 140699726837568 spec.py:349] Evaluating on the test split.
I0210 16:13:41.207146 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 16:16:26.447779 140699726837568 submission_runner.py:408] Time since start: 52270.26s, 	Step: 86116, 	{'train/accuracy': 0.6685099005699158, 'train/loss': 1.5362203121185303, 'train/bleu': 33.509164043830715, 'validation/accuracy': 0.6834757328033447, 'validation/loss': 1.4383317232131958, 'validation/bleu': 30.021007104354624, 'validation/num_examples': 3000, 'test/accuracy': 0.7002963423728943, 'test/loss': 1.3392986059188843, 'test/bleu': 30.026394171826187, 'test/num_examples': 3003, 'score': 31114.393693447113, 'total_duration': 52270.26392364502, 'accumulated_submission_time': 31114.393693447113, 'accumulated_eval_time': 21151.934986829758, 'accumulated_logging_time': 1.1864585876464844}
I0210 16:16:26.475619 140529937405696 logging_writer.py:48] [86116] accumulated_eval_time=21151.934987, accumulated_logging_time=1.186459, accumulated_submission_time=31114.393693, global_step=86116, preemption_count=0, score=31114.393693, test/accuracy=0.700296, test/bleu=30.026394, test/loss=1.339299, test/num_examples=3003, total_duration=52270.263924, train/accuracy=0.668510, train/bleu=33.509164, train/loss=1.536220, validation/accuracy=0.683476, validation/bleu=30.021007, validation/loss=1.438332, validation/num_examples=3000
I0210 16:16:57.016171 140529929012992 logging_writer.py:48] [86200] global_step=86200, grad_norm=0.20122995972633362, loss=1.5836026668548584
I0210 16:17:33.008318 140529937405696 logging_writer.py:48] [86300] global_step=86300, grad_norm=0.20472848415374756, loss=1.5943334102630615
I0210 16:18:09.069722 140529929012992 logging_writer.py:48] [86400] global_step=86400, grad_norm=0.19889463484287262, loss=1.6207937002182007
I0210 16:18:45.149989 140529937405696 logging_writer.py:48] [86500] global_step=86500, grad_norm=0.2096666693687439, loss=1.6572567224502563
I0210 16:19:21.225484 140529929012992 logging_writer.py:48] [86600] global_step=86600, grad_norm=0.2115703821182251, loss=1.6427693367004395
I0210 16:19:57.480020 140529937405696 logging_writer.py:48] [86700] global_step=86700, grad_norm=0.2047865390777588, loss=1.5935442447662354
I0210 16:20:33.621675 140529929012992 logging_writer.py:48] [86800] global_step=86800, grad_norm=0.20749720931053162, loss=1.5685991048812866
I0210 16:21:09.780816 140529937405696 logging_writer.py:48] [86900] global_step=86900, grad_norm=0.20260214805603027, loss=1.616770625114441
I0210 16:21:45.967484 140529929012992 logging_writer.py:48] [87000] global_step=87000, grad_norm=0.19604600965976715, loss=1.5129822492599487
I0210 16:22:22.100286 140529937405696 logging_writer.py:48] [87100] global_step=87100, grad_norm=0.21649585664272308, loss=1.487438678741455
I0210 16:22:58.180563 140529929012992 logging_writer.py:48] [87200] global_step=87200, grad_norm=0.2039044201374054, loss=1.5748405456542969
I0210 16:23:34.273365 140529937405696 logging_writer.py:48] [87300] global_step=87300, grad_norm=0.20985086262226105, loss=1.5518966913223267
I0210 16:24:10.365404 140529929012992 logging_writer.py:48] [87400] global_step=87400, grad_norm=0.21185550093650818, loss=1.6617578268051147
I0210 16:24:46.440364 140529937405696 logging_writer.py:48] [87500] global_step=87500, grad_norm=0.20482437312602997, loss=1.5353589057922363
I0210 16:25:22.525357 140529929012992 logging_writer.py:48] [87600] global_step=87600, grad_norm=0.20947396755218506, loss=1.5625194311141968
I0210 16:25:58.616579 140529937405696 logging_writer.py:48] [87700] global_step=87700, grad_norm=0.20463892817497253, loss=1.5565556287765503
I0210 16:26:34.761851 140529929012992 logging_writer.py:48] [87800] global_step=87800, grad_norm=0.21877701580524445, loss=1.6169414520263672
I0210 16:27:10.887565 140529937405696 logging_writer.py:48] [87900] global_step=87900, grad_norm=0.20685969293117523, loss=1.583495855331421
I0210 16:27:47.039093 140529929012992 logging_writer.py:48] [88000] global_step=88000, grad_norm=0.21230818331241608, loss=1.563702940940857
I0210 16:28:23.172774 140529937405696 logging_writer.py:48] [88100] global_step=88100, grad_norm=0.23691868782043457, loss=1.5748640298843384
I0210 16:28:59.246932 140529929012992 logging_writer.py:48] [88200] global_step=88200, grad_norm=0.21765545010566711, loss=1.6649086475372314
I0210 16:29:35.322538 140529937405696 logging_writer.py:48] [88300] global_step=88300, grad_norm=0.20860813558101654, loss=1.67875075340271
I0210 16:30:11.428561 140529929012992 logging_writer.py:48] [88400] global_step=88400, grad_norm=0.20372189581394196, loss=1.6018072366714478
I0210 16:30:26.646886 140699726837568 spec.py:321] Evaluating on the training split.
I0210 16:30:29.669167 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 16:34:45.941219 140699726837568 spec.py:333] Evaluating on the validation split.
I0210 16:34:48.667481 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 16:37:38.958542 140699726837568 spec.py:349] Evaluating on the test split.
I0210 16:37:41.685870 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 16:40:13.517755 140699726837568 submission_runner.py:408] Time since start: 53697.33s, 	Step: 88444, 	{'train/accuracy': 0.6788003444671631, 'train/loss': 1.4714220762252808, 'train/bleu': 33.85755785036112, 'validation/accuracy': 0.6860795021057129, 'validation/loss': 1.4285391569137573, 'validation/bleu': 30.326511463742502, 'validation/num_examples': 3000, 'test/accuracy': 0.7007378935813904, 'test/loss': 1.3346697092056274, 'test/bleu': 30.07429817068781, 'test/num_examples': 3003, 'score': 31954.47871041298, 'total_duration': 53697.333899497986, 'accumulated_submission_time': 31954.47871041298, 'accumulated_eval_time': 21738.805801153183, 'accumulated_logging_time': 1.2259869575500488}
I0210 16:40:13.547282 140529937405696 logging_writer.py:48] [88444] accumulated_eval_time=21738.805801, accumulated_logging_time=1.225987, accumulated_submission_time=31954.478710, global_step=88444, preemption_count=0, score=31954.478710, test/accuracy=0.700738, test/bleu=30.074298, test/loss=1.334670, test/num_examples=3003, total_duration=53697.333899, train/accuracy=0.678800, train/bleu=33.857558, train/loss=1.471422, validation/accuracy=0.686080, validation/bleu=30.326511, validation/loss=1.428539, validation/num_examples=3000
I0210 16:40:34.041206 140529929012992 logging_writer.py:48] [88500] global_step=88500, grad_norm=0.21401043236255646, loss=1.55502450466156
I0210 16:41:10.046578 140529937405696 logging_writer.py:48] [88600] global_step=88600, grad_norm=0.2245829701423645, loss=1.609430193901062
I0210 16:41:46.166736 140529929012992 logging_writer.py:48] [88700] global_step=88700, grad_norm=0.22973497211933136, loss=1.6257487535476685
I0210 16:42:22.234515 140529937405696 logging_writer.py:48] [88800] global_step=88800, grad_norm=0.23510169982910156, loss=1.6351302862167358
I0210 16:42:58.315062 140529929012992 logging_writer.py:48] [88900] global_step=88900, grad_norm=0.2096254527568817, loss=1.6041710376739502
I0210 16:43:34.404015 140529937405696 logging_writer.py:48] [89000] global_step=89000, grad_norm=0.2496403157711029, loss=1.6109180450439453
I0210 16:44:10.510834 140529929012992 logging_writer.py:48] [89100] global_step=89100, grad_norm=0.1989392340183258, loss=1.5929920673370361
I0210 16:44:46.630707 140529937405696 logging_writer.py:48] [89200] global_step=89200, grad_norm=0.2077791392803192, loss=1.6093780994415283
I0210 16:45:22.721163 140529929012992 logging_writer.py:48] [89300] global_step=89300, grad_norm=0.2050550878047943, loss=1.543314814567566
I0210 16:45:58.793924 140529937405696 logging_writer.py:48] [89400] global_step=89400, grad_norm=0.2092439830303192, loss=1.667539358139038
I0210 16:46:34.903806 140529929012992 logging_writer.py:48] [89500] global_step=89500, grad_norm=0.20790095627307892, loss=1.552728533744812
I0210 16:47:10.999317 140529937405696 logging_writer.py:48] [89600] global_step=89600, grad_norm=0.20684906840324402, loss=1.5195873975753784
I0210 16:47:47.152552 140529929012992 logging_writer.py:48] [89700] global_step=89700, grad_norm=0.20534665882587433, loss=1.5479199886322021
I0210 16:48:23.288459 140529937405696 logging_writer.py:48] [89800] global_step=89800, grad_norm=0.2083500474691391, loss=1.5265384912490845
I0210 16:48:59.401386 140529929012992 logging_writer.py:48] [89900] global_step=89900, grad_norm=0.206577867269516, loss=1.5306719541549683
I0210 16:49:35.458935 140529937405696 logging_writer.py:48] [90000] global_step=90000, grad_norm=0.20590466260910034, loss=1.5373518466949463
I0210 16:50:11.587127 140529929012992 logging_writer.py:48] [90100] global_step=90100, grad_norm=0.21182890236377716, loss=1.6132292747497559
I0210 16:50:47.694956 140529937405696 logging_writer.py:48] [90200] global_step=90200, grad_norm=0.2179766297340393, loss=1.6250935792922974
I0210 16:51:23.810421 140529929012992 logging_writer.py:48] [90300] global_step=90300, grad_norm=0.2206593006849289, loss=1.5738316774368286
I0210 16:51:59.936763 140529937405696 logging_writer.py:48] [90400] global_step=90400, grad_norm=0.22131574153900146, loss=1.538629412651062
I0210 16:52:36.017477 140529929012992 logging_writer.py:48] [90500] global_step=90500, grad_norm=0.24730797111988068, loss=1.5926661491394043
I0210 16:53:12.118576 140529937405696 logging_writer.py:48] [90600] global_step=90600, grad_norm=0.2243325561285019, loss=1.6037589311599731
I0210 16:53:48.211431 140529929012992 logging_writer.py:48] [90700] global_step=90700, grad_norm=0.20652230083942413, loss=1.6362425088882446
I0210 16:54:13.580824 140699726837568 spec.py:321] Evaluating on the training split.
I0210 16:54:16.624904 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 16:58:43.372605 140699726837568 spec.py:333] Evaluating on the validation split.
I0210 16:58:46.109448 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 17:02:08.962382 140699726837568 spec.py:349] Evaluating on the test split.
I0210 17:02:11.704812 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 17:05:29.153893 140699726837568 submission_runner.py:408] Time since start: 55212.97s, 	Step: 90772, 	{'train/accuracy': 0.6721503138542175, 'train/loss': 1.5121186971664429, 'train/bleu': 33.66060615621202, 'validation/accuracy': 0.6863027215003967, 'validation/loss': 1.4202866554260254, 'validation/bleu': 30.17695861596039, 'validation/num_examples': 3000, 'test/accuracy': 0.7035732865333557, 'test/loss': 1.3192323446273804, 'test/bleu': 30.333156321061402, 'test/num_examples': 3003, 'score': 32794.429097890854, 'total_duration': 55212.970027923584, 'accumulated_submission_time': 32794.429097890854, 'accumulated_eval_time': 22414.37882566452, 'accumulated_logging_time': 1.265702247619629}
I0210 17:05:29.181537 140529937405696 logging_writer.py:48] [90772] accumulated_eval_time=22414.378826, accumulated_logging_time=1.265702, accumulated_submission_time=32794.429098, global_step=90772, preemption_count=0, score=32794.429098, test/accuracy=0.703573, test/bleu=30.333156, test/loss=1.319232, test/num_examples=3003, total_duration=55212.970028, train/accuracy=0.672150, train/bleu=33.660606, train/loss=1.512119, validation/accuracy=0.686303, validation/bleu=30.176959, validation/loss=1.420287, validation/num_examples=3000
I0210 17:05:39.611675 140529929012992 logging_writer.py:48] [90800] global_step=90800, grad_norm=0.21689891815185547, loss=1.6680171489715576
I0210 17:06:15.569409 140529937405696 logging_writer.py:48] [90900] global_step=90900, grad_norm=0.21080242097377777, loss=1.5893412828445435
I0210 17:06:51.593929 140529929012992 logging_writer.py:48] [91000] global_step=91000, grad_norm=0.21713471412658691, loss=1.5463076829910278
I0210 17:07:27.673186 140529937405696 logging_writer.py:48] [91100] global_step=91100, grad_norm=0.22869080305099487, loss=1.5822341442108154
I0210 17:08:03.833207 140529929012992 logging_writer.py:48] [91200] global_step=91200, grad_norm=0.21402598917484283, loss=1.5225752592086792
I0210 17:08:39.911574 140529937405696 logging_writer.py:48] [91300] global_step=91300, grad_norm=0.21017281711101532, loss=1.510607123374939
I0210 17:09:15.996041 140529929012992 logging_writer.py:48] [91400] global_step=91400, grad_norm=0.21419759094715118, loss=1.6114124059677124
I0210 17:09:52.079351 140529937405696 logging_writer.py:48] [91500] global_step=91500, grad_norm=0.19818656146526337, loss=1.5805951356887817
I0210 17:10:28.257531 140529929012992 logging_writer.py:48] [91600] global_step=91600, grad_norm=0.21245624125003815, loss=1.5812623500823975
I0210 17:11:04.356789 140529937405696 logging_writer.py:48] [91700] global_step=91700, grad_norm=0.22480422258377075, loss=1.5367376804351807
I0210 17:11:40.468066 140529929012992 logging_writer.py:48] [91800] global_step=91800, grad_norm=0.20610205829143524, loss=1.5357333421707153
I0210 17:12:16.588292 140529937405696 logging_writer.py:48] [91900] global_step=91900, grad_norm=0.19824931025505066, loss=1.5808510780334473
I0210 17:12:52.741389 140529929012992 logging_writer.py:48] [92000] global_step=92000, grad_norm=0.2262965589761734, loss=1.6266310214996338
I0210 17:13:28.792043 140529937405696 logging_writer.py:48] [92100] global_step=92100, grad_norm=0.21310701966285706, loss=1.6529103517532349
I0210 17:14:04.892176 140529929012992 logging_writer.py:48] [92200] global_step=92200, grad_norm=0.20602865517139435, loss=1.5308376550674438
I0210 17:14:41.085964 140529937405696 logging_writer.py:48] [92300] global_step=92300, grad_norm=0.21053801476955414, loss=1.558933138847351
I0210 17:15:17.209495 140529929012992 logging_writer.py:48] [92400] global_step=92400, grad_norm=0.20920629799365997, loss=1.5313986539840698
I0210 17:15:53.303881 140529937405696 logging_writer.py:48] [92500] global_step=92500, grad_norm=0.220546692609787, loss=1.6322829723358154
I0210 17:16:29.421533 140529929012992 logging_writer.py:48] [92600] global_step=92600, grad_norm=0.20757734775543213, loss=1.684180736541748
I0210 17:17:05.595267 140529937405696 logging_writer.py:48] [92700] global_step=92700, grad_norm=0.2097109854221344, loss=1.5321602821350098
I0210 17:17:41.681001 140529929012992 logging_writer.py:48] [92800] global_step=92800, grad_norm=0.23373661935329437, loss=1.5913013219833374
I0210 17:18:17.764678 140529937405696 logging_writer.py:48] [92900] global_step=92900, grad_norm=0.21373288333415985, loss=1.5600454807281494
I0210 17:18:53.838586 140529929012992 logging_writer.py:48] [93000] global_step=93000, grad_norm=0.22117166221141815, loss=1.533273696899414
I0210 17:19:29.352997 140699726837568 spec.py:321] Evaluating on the training split.
I0210 17:19:32.395525 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 17:23:52.507429 140699726837568 spec.py:333] Evaluating on the validation split.
I0210 17:23:55.234256 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 17:26:42.067451 140699726837568 spec.py:349] Evaluating on the test split.
I0210 17:26:44.796846 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 17:29:20.810935 140699726837568 submission_runner.py:408] Time since start: 56644.63s, 	Step: 93100, 	{'train/accuracy': 0.6721413135528564, 'train/loss': 1.5171512365341187, 'train/bleu': 33.86056555520624, 'validation/accuracy': 0.6868358850479126, 'validation/loss': 1.417452335357666, 'validation/bleu': 30.403902679657968, 'validation/num_examples': 3000, 'test/accuracy': 0.7037127614021301, 'test/loss': 1.315152645111084, 'test/bleu': 30.392667213925773, 'test/num_examples': 3003, 'score': 33634.516040802, 'total_duration': 56644.627078294754, 'accumulated_submission_time': 33634.516040802, 'accumulated_eval_time': 23005.836725473404, 'accumulated_logging_time': 1.3031470775604248}
I0210 17:29:20.840292 140529937405696 logging_writer.py:48] [93100] accumulated_eval_time=23005.836725, accumulated_logging_time=1.303147, accumulated_submission_time=33634.516041, global_step=93100, preemption_count=0, score=33634.516041, test/accuracy=0.703713, test/bleu=30.392667, test/loss=1.315153, test/num_examples=3003, total_duration=56644.627078, train/accuracy=0.672141, train/bleu=33.860566, train/loss=1.517151, validation/accuracy=0.686836, validation/bleu=30.403903, validation/loss=1.417452, validation/num_examples=3000
I0210 17:29:21.221536 140529929012992 logging_writer.py:48] [93100] global_step=93100, grad_norm=0.20675697922706604, loss=1.5039774179458618
I0210 17:29:57.162110 140529937405696 logging_writer.py:48] [93200] global_step=93200, grad_norm=0.214847132563591, loss=1.597554087638855
I0210 17:30:33.204099 140529929012992 logging_writer.py:48] [93300] global_step=93300, grad_norm=0.20935527980327606, loss=1.6039800643920898
I0210 17:31:09.335109 140529937405696 logging_writer.py:48] [93400] global_step=93400, grad_norm=0.20670455694198608, loss=1.6111472845077515
I0210 17:31:45.465889 140529929012992 logging_writer.py:48] [93500] global_step=93500, grad_norm=0.19544990360736847, loss=1.5902948379516602
I0210 17:32:21.571254 140529937405696 logging_writer.py:48] [93600] global_step=93600, grad_norm=0.20573095977306366, loss=1.5797076225280762
I0210 17:32:57.755708 140529929012992 logging_writer.py:48] [93700] global_step=93700, grad_norm=0.22609873116016388, loss=1.6621488332748413
I0210 17:33:33.879927 140529937405696 logging_writer.py:48] [93800] global_step=93800, grad_norm=0.2382349967956543, loss=1.6901365518569946
I0210 17:34:10.033253 140529929012992 logging_writer.py:48] [93900] global_step=93900, grad_norm=0.20874013006687164, loss=1.5374054908752441
I0210 17:34:46.114962 140529937405696 logging_writer.py:48] [94000] global_step=94000, grad_norm=0.20276078581809998, loss=1.5869507789611816
I0210 17:35:22.236599 140529929012992 logging_writer.py:48] [94100] global_step=94100, grad_norm=0.2265661358833313, loss=1.6239700317382812
I0210 17:35:58.301001 140529937405696 logging_writer.py:48] [94200] global_step=94200, grad_norm=0.20576277375221252, loss=1.541067123413086
I0210 17:36:34.405796 140529929012992 logging_writer.py:48] [94300] global_step=94300, grad_norm=0.2141174077987671, loss=1.5308713912963867
I0210 17:37:10.532413 140529937405696 logging_writer.py:48] [94400] global_step=94400, grad_norm=0.22015415132045746, loss=1.638506531715393
I0210 17:37:46.658540 140529929012992 logging_writer.py:48] [94500] global_step=94500, grad_norm=0.24930845201015472, loss=1.568524718284607
I0210 17:38:22.749408 140529937405696 logging_writer.py:48] [94600] global_step=94600, grad_norm=0.20735318958759308, loss=1.56003999710083
I0210 17:38:58.862301 140529929012992 logging_writer.py:48] [94700] global_step=94700, grad_norm=0.20877033472061157, loss=1.6284129619598389
I0210 17:39:34.973142 140529937405696 logging_writer.py:48] [94800] global_step=94800, grad_norm=0.23426854610443115, loss=1.6588290929794312
I0210 17:40:11.100935 140529929012992 logging_writer.py:48] [94900] global_step=94900, grad_norm=0.21269705891609192, loss=1.614314317703247
I0210 17:40:47.196617 140529937405696 logging_writer.py:48] [95000] global_step=95000, grad_norm=0.19713428616523743, loss=1.560782551765442
I0210 17:41:23.269450 140529929012992 logging_writer.py:48] [95100] global_step=95100, grad_norm=0.2083522528409958, loss=1.461333990097046
I0210 17:41:59.380365 140529937405696 logging_writer.py:48] [95200] global_step=95200, grad_norm=0.2274768054485321, loss=1.6416454315185547
I0210 17:42:35.483345 140529929012992 logging_writer.py:48] [95300] global_step=95300, grad_norm=0.21882545948028564, loss=1.5903873443603516
I0210 17:43:11.549309 140529937405696 logging_writer.py:48] [95400] global_step=95400, grad_norm=0.212552011013031, loss=1.5816504955291748
I0210 17:43:21.011974 140699726837568 spec.py:321] Evaluating on the training split.
I0210 17:43:24.042789 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 17:46:45.242815 140699726837568 spec.py:333] Evaluating on the validation split.
I0210 17:46:47.971384 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 17:49:16.140750 140699726837568 spec.py:349] Evaluating on the test split.
I0210 17:49:18.871821 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 17:51:28.899443 140699726837568 submission_runner.py:408] Time since start: 57972.72s, 	Step: 95428, 	{'train/accuracy': 0.6813426613807678, 'train/loss': 1.451947808265686, 'train/bleu': 34.027032781976644, 'validation/accuracy': 0.6877037882804871, 'validation/loss': 1.4141026735305786, 'validation/bleu': 30.14882976469225, 'validation/num_examples': 3000, 'test/accuracy': 0.7046656608581543, 'test/loss': 1.3117072582244873, 'test/bleu': 30.250803153996866, 'test/num_examples': 3003, 'score': 34474.600940704346, 'total_duration': 57972.71556472778, 'accumulated_submission_time': 34474.600940704346, 'accumulated_eval_time': 23493.724118709564, 'accumulated_logging_time': 1.3443577289581299}
I0210 17:51:28.930542 140529929012992 logging_writer.py:48] [95428] accumulated_eval_time=23493.724119, accumulated_logging_time=1.344358, accumulated_submission_time=34474.600941, global_step=95428, preemption_count=0, score=34474.600941, test/accuracy=0.704666, test/bleu=30.250803, test/loss=1.311707, test/num_examples=3003, total_duration=57972.715565, train/accuracy=0.681343, train/bleu=34.027033, train/loss=1.451948, validation/accuracy=0.687704, validation/bleu=30.148830, validation/loss=1.414103, validation/num_examples=3000
I0210 17:51:55.188778 140529937405696 logging_writer.py:48] [95500] global_step=95500, grad_norm=0.20295853912830353, loss=1.5819848775863647
I0210 17:52:31.216840 140529929012992 logging_writer.py:48] [95600] global_step=95600, grad_norm=0.20262007415294647, loss=1.5063339471817017
I0210 17:53:07.256006 140529937405696 logging_writer.py:48] [95700] global_step=95700, grad_norm=0.21617604792118073, loss=1.566701054573059
I0210 17:53:43.327718 140529929012992 logging_writer.py:48] [95800] global_step=95800, grad_norm=0.21581152081489563, loss=1.5656543970108032
I0210 17:54:19.445344 140529937405696 logging_writer.py:48] [95900] global_step=95900, grad_norm=0.21253640949726105, loss=1.5569648742675781
I0210 17:54:55.574497 140529929012992 logging_writer.py:48] [96000] global_step=96000, grad_norm=0.22048477828502655, loss=1.6249439716339111
I0210 17:55:31.720086 140529937405696 logging_writer.py:48] [96100] global_step=96100, grad_norm=0.2262437790632248, loss=1.540981650352478
I0210 17:56:08.004759 140529929012992 logging_writer.py:48] [96200] global_step=96200, grad_norm=0.22240138053894043, loss=1.5815629959106445
I0210 17:56:44.159787 140529937405696 logging_writer.py:48] [96300] global_step=96300, grad_norm=0.20744343101978302, loss=1.5316990613937378
I0210 17:57:20.306221 140529929012992 logging_writer.py:48] [96400] global_step=96400, grad_norm=0.23026128113269806, loss=1.6365255117416382
I0210 17:57:56.432014 140529937405696 logging_writer.py:48] [96500] global_step=96500, grad_norm=0.21765543520450592, loss=1.5005967617034912
I0210 17:58:32.522871 140529929012992 logging_writer.py:48] [96600] global_step=96600, grad_norm=0.2303413450717926, loss=1.5811687707901
I0210 17:59:08.615388 140529937405696 logging_writer.py:48] [96700] global_step=96700, grad_norm=0.21686460077762604, loss=1.6380386352539062
I0210 17:59:44.698195 140529929012992 logging_writer.py:48] [96800] global_step=96800, grad_norm=0.22079487144947052, loss=1.5793050527572632
I0210 18:00:20.768925 140529937405696 logging_writer.py:48] [96900] global_step=96900, grad_norm=0.22202253341674805, loss=1.5605942010879517
I0210 18:00:56.821303 140529929012992 logging_writer.py:48] [97000] global_step=97000, grad_norm=0.21193543076515198, loss=1.6222035884857178
I0210 18:01:32.987994 140529937405696 logging_writer.py:48] [97100] global_step=97100, grad_norm=0.20391491055488586, loss=1.50929856300354
I0210 18:02:09.146432 140529929012992 logging_writer.py:48] [97200] global_step=97200, grad_norm=0.21824601292610168, loss=1.6165374517440796
I0210 18:02:45.291533 140529937405696 logging_writer.py:48] [97300] global_step=97300, grad_norm=0.23401246964931488, loss=1.5994313955307007
I0210 18:03:21.375841 140529929012992 logging_writer.py:48] [97400] global_step=97400, grad_norm=0.236773282289505, loss=1.5091432332992554
I0210 18:03:57.488067 140529937405696 logging_writer.py:48] [97500] global_step=97500, grad_norm=0.20717525482177734, loss=1.498234510421753
I0210 18:04:33.622977 140529929012992 logging_writer.py:48] [97600] global_step=97600, grad_norm=0.2078975886106491, loss=1.5287240743637085
I0210 18:05:09.727346 140529937405696 logging_writer.py:48] [97700] global_step=97700, grad_norm=0.2273881584405899, loss=1.6329244375228882
I0210 18:05:28.967582 140699726837568 spec.py:321] Evaluating on the training split.
I0210 18:05:32.008458 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 18:09:27.219788 140699726837568 spec.py:333] Evaluating on the validation split.
I0210 18:09:29.970586 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 18:12:20.037543 140699726837568 spec.py:349] Evaluating on the test split.
I0210 18:12:22.782909 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 18:15:16.618224 140699726837568 submission_runner.py:408] Time since start: 59400.43s, 	Step: 97755, 	{'train/accuracy': 0.6785141825675964, 'train/loss': 1.4695278406143188, 'train/bleu': 34.25009675294788, 'validation/accuracy': 0.6903820037841797, 'validation/loss': 1.4033619165420532, 'validation/bleu': 30.519843494061867, 'validation/num_examples': 3000, 'test/accuracy': 0.7072570323944092, 'test/loss': 1.2985440492630005, 'test/bleu': 30.597164472048394, 'test/num_examples': 3003, 'score': 35314.54950070381, 'total_duration': 59400.434348106384, 'accumulated_submission_time': 35314.54950070381, 'accumulated_eval_time': 24081.374705553055, 'accumulated_logging_time': 1.387169599533081}
I0210 18:15:16.648146 140529929012992 logging_writer.py:48] [97755] accumulated_eval_time=24081.374706, accumulated_logging_time=1.387170, accumulated_submission_time=35314.549501, global_step=97755, preemption_count=0, score=35314.549501, test/accuracy=0.707257, test/bleu=30.597164, test/loss=1.298544, test/num_examples=3003, total_duration=59400.434348, train/accuracy=0.678514, train/bleu=34.250097, train/loss=1.469528, validation/accuracy=0.690382, validation/bleu=30.519843, validation/loss=1.403362, validation/num_examples=3000
I0210 18:15:33.203896 140529937405696 logging_writer.py:48] [97800] global_step=97800, grad_norm=0.22144319117069244, loss=1.5192068815231323
I0210 18:16:09.213027 140529929012992 logging_writer.py:48] [97900] global_step=97900, grad_norm=0.21470309793949127, loss=1.4920557737350464
I0210 18:16:45.268483 140529937405696 logging_writer.py:48] [98000] global_step=98000, grad_norm=0.21753713488578796, loss=1.6014512777328491
I0210 18:17:21.338033 140529929012992 logging_writer.py:48] [98100] global_step=98100, grad_norm=0.24114499986171722, loss=1.5932265520095825
I0210 18:17:57.401984 140529937405696 logging_writer.py:48] [98200] global_step=98200, grad_norm=0.2132887989282608, loss=1.4919853210449219
I0210 18:18:33.537225 140529929012992 logging_writer.py:48] [98300] global_step=98300, grad_norm=0.23346750438213348, loss=1.6096023321151733
I0210 18:19:09.632440 140529937405696 logging_writer.py:48] [98400] global_step=98400, grad_norm=0.22414104640483856, loss=1.6120179891586304
I0210 18:19:45.790907 140529929012992 logging_writer.py:48] [98500] global_step=98500, grad_norm=0.22720326483249664, loss=1.5547499656677246
I0210 18:20:21.927823 140529937405696 logging_writer.py:48] [98600] global_step=98600, grad_norm=0.2158091813325882, loss=1.4560797214508057
I0210 18:20:58.018172 140529929012992 logging_writer.py:48] [98700] global_step=98700, grad_norm=0.2305813431739807, loss=1.5193005800247192
I0210 18:21:34.124317 140529937405696 logging_writer.py:48] [98800] global_step=98800, grad_norm=0.21401703357696533, loss=1.6066220998764038
I0210 18:22:10.275327 140529929012992 logging_writer.py:48] [98900] global_step=98900, grad_norm=0.22103624045848846, loss=1.5788540840148926
I0210 18:22:46.418521 140529937405696 logging_writer.py:48] [99000] global_step=99000, grad_norm=0.2126665711402893, loss=1.5486462116241455
I0210 18:23:22.558023 140529929012992 logging_writer.py:48] [99100] global_step=99100, grad_norm=0.21086648106575012, loss=1.5388996601104736
I0210 18:23:58.669096 140529937405696 logging_writer.py:48] [99200] global_step=99200, grad_norm=0.20872029662132263, loss=1.5091966390609741
I0210 18:24:34.736672 140529929012992 logging_writer.py:48] [99300] global_step=99300, grad_norm=0.21036046743392944, loss=1.511195182800293
I0210 18:25:10.843518 140529937405696 logging_writer.py:48] [99400] global_step=99400, grad_norm=0.20657917857170105, loss=1.572157859802246
I0210 18:25:46.973043 140529929012992 logging_writer.py:48] [99500] global_step=99500, grad_norm=0.2228180468082428, loss=1.5232279300689697
I0210 18:26:23.113732 140529937405696 logging_writer.py:48] [99600] global_step=99600, grad_norm=0.21897168457508087, loss=1.5305596590042114
I0210 18:26:59.184257 140529929012992 logging_writer.py:48] [99700] global_step=99700, grad_norm=0.21390803158283234, loss=1.5638055801391602
I0210 18:27:35.323514 140529937405696 logging_writer.py:48] [99800] global_step=99800, grad_norm=0.2246534824371338, loss=1.50856614112854
I0210 18:28:11.403877 140529929012992 logging_writer.py:48] [99900] global_step=99900, grad_norm=0.22953711450099945, loss=1.51629638671875
I0210 18:28:47.554852 140529937405696 logging_writer.py:48] [100000] global_step=100000, grad_norm=0.2267599254846573, loss=1.548972249031067
I0210 18:29:16.888608 140699726837568 spec.py:321] Evaluating on the training split.
I0210 18:29:19.940673 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 18:32:55.124833 140699726837568 spec.py:333] Evaluating on the validation split.
I0210 18:32:57.881754 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 18:35:51.024629 140699726837568 spec.py:349] Evaluating on the test split.
I0210 18:35:53.768912 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 18:38:14.825463 140699726837568 submission_runner.py:408] Time since start: 60778.64s, 	Step: 100083, 	{'train/accuracy': 0.6936557292938232, 'train/loss': 1.3875911235809326, 'train/bleu': 35.65112004922339, 'validation/accuracy': 0.6906548142433167, 'validation/loss': 1.400636911392212, 'validation/bleu': 30.85509335453766, 'validation/num_examples': 3000, 'test/accuracy': 0.706187903881073, 'test/loss': 1.299963116645813, 'test/bleu': 30.344433313070358, 'test/num_examples': 3003, 'score': 36154.70352482796, 'total_duration': 60778.641570568085, 'accumulated_submission_time': 36154.70352482796, 'accumulated_eval_time': 24619.3114862442, 'accumulated_logging_time': 1.4285976886749268}
I0210 18:38:14.861784 140529929012992 logging_writer.py:48] [100083] accumulated_eval_time=24619.311486, accumulated_logging_time=1.428598, accumulated_submission_time=36154.703525, global_step=100083, preemption_count=0, score=36154.703525, test/accuracy=0.706188, test/bleu=30.344433, test/loss=1.299963, test/num_examples=3003, total_duration=60778.641571, train/accuracy=0.693656, train/bleu=35.651120, train/loss=1.387591, validation/accuracy=0.690655, validation/bleu=30.855093, validation/loss=1.400637, validation/num_examples=3000
I0210 18:38:21.341477 140529937405696 logging_writer.py:48] [100100] global_step=100100, grad_norm=0.2351120561361313, loss=1.5735186338424683
I0210 18:38:57.276535 140529929012992 logging_writer.py:48] [100200] global_step=100200, grad_norm=0.2049102485179901, loss=1.5391271114349365
I0210 18:39:33.279812 140529937405696 logging_writer.py:48] [100300] global_step=100300, grad_norm=0.2105485051870346, loss=1.4618074893951416
I0210 18:40:09.345127 140529929012992 logging_writer.py:48] [100400] global_step=100400, grad_norm=0.20985671877861023, loss=1.604016661643982
I0210 18:40:45.446798 140529937405696 logging_writer.py:48] [100500] global_step=100500, grad_norm=0.2303229719400406, loss=1.5736713409423828
I0210 18:41:21.574400 140529929012992 logging_writer.py:48] [100600] global_step=100600, grad_norm=0.21478451788425446, loss=1.5439226627349854
I0210 18:41:57.695821 140529937405696 logging_writer.py:48] [100700] global_step=100700, grad_norm=0.21176180243492126, loss=1.4815618991851807
I0210 18:42:33.814259 140529929012992 logging_writer.py:48] [100800] global_step=100800, grad_norm=0.22505725920200348, loss=1.4718537330627441
I0210 18:43:09.903805 140529937405696 logging_writer.py:48] [100900] global_step=100900, grad_norm=0.2881436347961426, loss=1.62471604347229
I0210 18:43:46.029394 140529929012992 logging_writer.py:48] [101000] global_step=101000, grad_norm=0.22520294785499573, loss=1.5442848205566406
I0210 18:44:22.151864 140529937405696 logging_writer.py:48] [101100] global_step=101100, grad_norm=0.20533888041973114, loss=1.4960345029830933
I0210 18:44:58.245855 140529929012992 logging_writer.py:48] [101200] global_step=101200, grad_norm=0.23015138506889343, loss=1.5402023792266846
I0210 18:45:34.367204 140529937405696 logging_writer.py:48] [101300] global_step=101300, grad_norm=0.22432535886764526, loss=1.5112197399139404
I0210 18:46:10.487687 140529929012992 logging_writer.py:48] [101400] global_step=101400, grad_norm=0.2297448366880417, loss=1.531257152557373
I0210 18:46:46.622483 140529937405696 logging_writer.py:48] [101500] global_step=101500, grad_norm=0.22739532589912415, loss=1.5851191282272339
I0210 18:47:22.698646 140529929012992 logging_writer.py:48] [101600] global_step=101600, grad_norm=0.22409860789775848, loss=1.6190284490585327
I0210 18:47:58.782989 140529937405696 logging_writer.py:48] [101700] global_step=101700, grad_norm=0.2546785771846771, loss=1.5982518196105957
I0210 18:48:34.917998 140529929012992 logging_writer.py:48] [101800] global_step=101800, grad_norm=0.23355181515216827, loss=1.6036509275436401
I0210 18:49:11.011764 140529937405696 logging_writer.py:48] [101900] global_step=101900, grad_norm=0.22702531516551971, loss=1.5952013731002808
I0210 18:49:47.099123 140529929012992 logging_writer.py:48] [102000] global_step=102000, grad_norm=0.22403842210769653, loss=1.572771430015564
I0210 18:50:23.191258 140529937405696 logging_writer.py:48] [102100] global_step=102100, grad_norm=0.23135384917259216, loss=1.4538050889968872
I0210 18:50:59.345088 140529929012992 logging_writer.py:48] [102200] global_step=102200, grad_norm=0.2262912541627884, loss=1.5456362962722778
I0210 18:51:35.440413 140529937405696 logging_writer.py:48] [102300] global_step=102300, grad_norm=0.21706141531467438, loss=1.5368832349777222
I0210 18:52:11.497016 140529929012992 logging_writer.py:48] [102400] global_step=102400, grad_norm=0.22609449923038483, loss=1.5300971269607544
I0210 18:52:14.833646 140699726837568 spec.py:321] Evaluating on the training split.
I0210 18:52:17.855527 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 18:56:15.140054 140699726837568 spec.py:333] Evaluating on the validation split.
I0210 18:56:17.865569 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 18:59:34.557837 140699726837568 spec.py:349] Evaluating on the test split.
I0210 18:59:37.281073 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 19:02:41.467132 140699726837568 submission_runner.py:408] Time since start: 62245.28s, 	Step: 102411, 	{'train/accuracy': 0.6837599873542786, 'train/loss': 1.4399648904800415, 'train/bleu': 34.42078439556147, 'validation/accuracy': 0.691696286201477, 'validation/loss': 1.3963810205459595, 'validation/bleu': 30.65930224150146, 'validation/num_examples': 3000, 'test/accuracy': 0.7067921757698059, 'test/loss': 1.2964038848876953, 'test/bleu': 30.490371241411076, 'test/num_examples': 3003, 'score': 36994.592054605484, 'total_duration': 62245.283281087875, 'accumulated_submission_time': 36994.592054605484, 'accumulated_eval_time': 25245.94492340088, 'accumulated_logging_time': 1.4772801399230957}
I0210 19:02:41.496104 140529937405696 logging_writer.py:48] [102411] accumulated_eval_time=25245.944923, accumulated_logging_time=1.477280, accumulated_submission_time=36994.592055, global_step=102411, preemption_count=0, score=36994.592055, test/accuracy=0.706792, test/bleu=30.490371, test/loss=1.296404, test/num_examples=3003, total_duration=62245.283281, train/accuracy=0.683760, train/bleu=34.420784, train/loss=1.439965, validation/accuracy=0.691696, validation/bleu=30.659302, validation/loss=1.396381, validation/num_examples=3000
I0210 19:03:13.854540 140529929012992 logging_writer.py:48] [102500] global_step=102500, grad_norm=0.2181887924671173, loss=1.524982213973999
I0210 19:03:49.876286 140529937405696 logging_writer.py:48] [102600] global_step=102600, grad_norm=0.22916753590106964, loss=1.5686014890670776
I0210 19:04:25.939818 140529929012992 logging_writer.py:48] [102700] global_step=102700, grad_norm=0.23092226684093475, loss=1.6336987018585205
I0210 19:05:02.085389 140529937405696 logging_writer.py:48] [102800] global_step=102800, grad_norm=0.2127358317375183, loss=1.5096344947814941
I0210 19:05:38.215879 140529929012992 logging_writer.py:48] [102900] global_step=102900, grad_norm=0.22165212035179138, loss=1.5074546337127686
I0210 19:06:14.327886 140529937405696 logging_writer.py:48] [103000] global_step=103000, grad_norm=0.22516131401062012, loss=1.509155035018921
I0210 19:06:50.413682 140529929012992 logging_writer.py:48] [103100] global_step=103100, grad_norm=0.2281632125377655, loss=1.5334782600402832
I0210 19:07:26.569171 140529937405696 logging_writer.py:48] [103200] global_step=103200, grad_norm=0.22553880512714386, loss=1.522238850593567
I0210 19:08:02.732922 140529929012992 logging_writer.py:48] [103300] global_step=103300, grad_norm=0.22066901624202728, loss=1.5333213806152344
I0210 19:08:38.858150 140529937405696 logging_writer.py:48] [103400] global_step=103400, grad_norm=0.22554315626621246, loss=1.5353556871414185
I0210 19:09:14.995309 140529929012992 logging_writer.py:48] [103500] global_step=103500, grad_norm=0.23170106112957, loss=1.4695297479629517
I0210 19:09:51.133917 140529937405696 logging_writer.py:48] [103600] global_step=103600, grad_norm=0.2350451648235321, loss=1.4872969388961792
I0210 19:10:27.244576 140529929012992 logging_writer.py:48] [103700] global_step=103700, grad_norm=0.22327663004398346, loss=1.552586317062378
I0210 19:11:03.326914 140529937405696 logging_writer.py:48] [103800] global_step=103800, grad_norm=0.22358636558055878, loss=1.5118895769119263
I0210 19:11:39.472795 140529929012992 logging_writer.py:48] [103900] global_step=103900, grad_norm=0.22235192358493805, loss=1.4815822839736938
I0210 19:12:15.575062 140529937405696 logging_writer.py:48] [104000] global_step=104000, grad_norm=0.2216886579990387, loss=1.506057620048523
I0210 19:12:51.628907 140529929012992 logging_writer.py:48] [104100] global_step=104100, grad_norm=0.22970032691955566, loss=1.5128659009933472
I0210 19:13:27.718065 140529937405696 logging_writer.py:48] [104200] global_step=104200, grad_norm=0.2312365621328354, loss=1.5090745687484741
I0210 19:14:03.803660 140529929012992 logging_writer.py:48] [104300] global_step=104300, grad_norm=0.22850213944911957, loss=1.509944200515747
I0210 19:14:39.924867 140529937405696 logging_writer.py:48] [104400] global_step=104400, grad_norm=0.23884108662605286, loss=1.530866026878357
I0210 19:15:16.027868 140529929012992 logging_writer.py:48] [104500] global_step=104500, grad_norm=0.23018090426921844, loss=1.5621861219406128
I0210 19:15:52.137415 140529937405696 logging_writer.py:48] [104600] global_step=104600, grad_norm=0.22100305557250977, loss=1.543861746788025
I0210 19:16:28.200617 140529929012992 logging_writer.py:48] [104700] global_step=104700, grad_norm=0.22891609370708466, loss=1.5181286334991455
I0210 19:16:41.650156 140699726837568 spec.py:321] Evaluating on the training split.
I0210 19:16:44.680927 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 19:20:51.743121 140699726837568 spec.py:333] Evaluating on the validation split.
I0210 19:20:54.475063 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 19:23:22.642376 140699726837568 spec.py:349] Evaluating on the test split.
I0210 19:23:25.364685 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 19:25:39.132411 140699726837568 submission_runner.py:408] Time since start: 63622.95s, 	Step: 104739, 	{'train/accuracy': 0.6803861260414124, 'train/loss': 1.4522088766098022, 'train/bleu': 34.19161231852919, 'validation/accuracy': 0.6913491487503052, 'validation/loss': 1.3914282321929932, 'validation/bleu': 30.690073446109658, 'validation/num_examples': 3000, 'test/accuracy': 0.7083609700202942, 'test/loss': 1.2872958183288574, 'test/bleu': 30.748461460186437, 'test/num_examples': 3003, 'score': 37834.66395068169, 'total_duration': 63622.94854474068, 'accumulated_submission_time': 37834.66395068169, 'accumulated_eval_time': 25783.427124261856, 'accumulated_logging_time': 1.5160844326019287}
I0210 19:25:39.161827 140529937405696 logging_writer.py:48] [104739] accumulated_eval_time=25783.427124, accumulated_logging_time=1.516084, accumulated_submission_time=37834.663951, global_step=104739, preemption_count=0, score=37834.663951, test/accuracy=0.708361, test/bleu=30.748461, test/loss=1.287296, test/num_examples=3003, total_duration=63622.948545, train/accuracy=0.680386, train/bleu=34.191612, train/loss=1.452209, validation/accuracy=0.691349, validation/bleu=30.690073, validation/loss=1.391428, validation/num_examples=3000
I0210 19:26:01.462542 140529929012992 logging_writer.py:48] [104800] global_step=104800, grad_norm=0.2219512164592743, loss=1.4764444828033447
I0210 19:26:37.467641 140529937405696 logging_writer.py:48] [104900] global_step=104900, grad_norm=0.23394320905208588, loss=1.5401177406311035
I0210 19:27:13.546093 140529929012992 logging_writer.py:48] [105000] global_step=105000, grad_norm=0.22083422541618347, loss=1.5051487684249878
I0210 19:27:49.588429 140529937405696 logging_writer.py:48] [105100] global_step=105100, grad_norm=0.21193614602088928, loss=1.465396761894226
I0210 19:28:25.653484 140529929012992 logging_writer.py:48] [105200] global_step=105200, grad_norm=0.2211693376302719, loss=1.571036696434021
I0210 19:29:01.710123 140529937405696 logging_writer.py:48] [105300] global_step=105300, grad_norm=0.22088563442230225, loss=1.5244836807250977
I0210 19:29:37.799767 140529929012992 logging_writer.py:48] [105400] global_step=105400, grad_norm=0.22846181690692902, loss=1.515620231628418
I0210 19:30:13.886098 140529937405696 logging_writer.py:48] [105500] global_step=105500, grad_norm=0.2328403741121292, loss=1.4619156122207642
I0210 19:30:49.959193 140529929012992 logging_writer.py:48] [105600] global_step=105600, grad_norm=0.24035902321338654, loss=1.4894071817398071
I0210 19:31:26.060598 140529937405696 logging_writer.py:48] [105700] global_step=105700, grad_norm=0.22401458024978638, loss=1.505610466003418
I0210 19:32:02.151186 140529929012992 logging_writer.py:48] [105800] global_step=105800, grad_norm=0.2263164520263672, loss=1.5554395914077759
I0210 19:32:38.274339 140529937405696 logging_writer.py:48] [105900] global_step=105900, grad_norm=0.2345452755689621, loss=1.5665266513824463
I0210 19:33:14.385992 140529929012992 logging_writer.py:48] [106000] global_step=106000, grad_norm=0.21246154606342316, loss=1.5212100744247437
I0210 19:33:50.487797 140529937405696 logging_writer.py:48] [106100] global_step=106100, grad_norm=0.2263028770685196, loss=1.5235016345977783
I0210 19:34:26.566426 140529929012992 logging_writer.py:48] [106200] global_step=106200, grad_norm=0.24370326101779938, loss=1.4069067239761353
I0210 19:35:02.673074 140529937405696 logging_writer.py:48] [106300] global_step=106300, grad_norm=0.22902710735797882, loss=1.5011539459228516
I0210 19:35:38.762250 140529929012992 logging_writer.py:48] [106400] global_step=106400, grad_norm=0.23109163343906403, loss=1.4751428365707397
I0210 19:36:14.894375 140529937405696 logging_writer.py:48] [106500] global_step=106500, grad_norm=0.22929391264915466, loss=1.5542659759521484
I0210 19:36:50.997920 140529929012992 logging_writer.py:48] [106600] global_step=106600, grad_norm=0.2444438338279724, loss=1.5872294902801514
I0210 19:37:27.152921 140529937405696 logging_writer.py:48] [106700] global_step=106700, grad_norm=0.21998707950115204, loss=1.4826123714447021
I0210 19:38:03.291737 140529929012992 logging_writer.py:48] [106800] global_step=106800, grad_norm=0.22738799452781677, loss=1.526810884475708
I0210 19:38:39.382778 140529937405696 logging_writer.py:48] [106900] global_step=106900, grad_norm=0.21177338063716888, loss=1.4655201435089111
I0210 19:39:15.485064 140529929012992 logging_writer.py:48] [107000] global_step=107000, grad_norm=0.22519533336162567, loss=1.5280200242996216
I0210 19:39:39.382067 140699726837568 spec.py:321] Evaluating on the training split.
I0210 19:39:42.410452 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 19:43:42.909462 140699726837568 spec.py:333] Evaluating on the validation split.
I0210 19:43:45.633060 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 19:46:13.088020 140699726837568 spec.py:349] Evaluating on the test split.
I0210 19:46:15.829834 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 19:48:54.788921 140699726837568 submission_runner.py:408] Time since start: 65018.61s, 	Step: 107068, 	{'train/accuracy': 0.691776692867279, 'train/loss': 1.3914330005645752, 'train/bleu': 34.81961982311779, 'validation/accuracy': 0.6922914981842041, 'validation/loss': 1.3859933614730835, 'validation/bleu': 30.583219009274575, 'validation/num_examples': 3000, 'test/accuracy': 0.708570122718811, 'test/loss': 1.2839752435684204, 'test/bleu': 30.670776102657882, 'test/num_examples': 3003, 'score': 38674.80152177811, 'total_duration': 65018.60505485535, 'accumulated_submission_time': 38674.80152177811, 'accumulated_eval_time': 26338.833918571472, 'accumulated_logging_time': 1.5553884506225586}
I0210 19:48:54.818915 140529937405696 logging_writer.py:48] [107068] accumulated_eval_time=26338.833919, accumulated_logging_time=1.555388, accumulated_submission_time=38674.801522, global_step=107068, preemption_count=0, score=38674.801522, test/accuracy=0.708570, test/bleu=30.670776, test/loss=1.283975, test/num_examples=3003, total_duration=65018.605055, train/accuracy=0.691777, train/bleu=34.819620, train/loss=1.391433, validation/accuracy=0.692291, validation/bleu=30.583219, validation/loss=1.385993, validation/num_examples=3000
I0210 19:49:06.692164 140529929012992 logging_writer.py:48] [107100] global_step=107100, grad_norm=0.2329591065645218, loss=1.527521014213562
I0210 19:49:42.718035 140529937405696 logging_writer.py:48] [107200] global_step=107200, grad_norm=0.23157846927642822, loss=1.5073177814483643
I0210 19:50:18.804424 140529929012992 logging_writer.py:48] [107300] global_step=107300, grad_norm=0.2306155264377594, loss=1.5230705738067627
I0210 19:50:54.893584 140529937405696 logging_writer.py:48] [107400] global_step=107400, grad_norm=0.23217003047466278, loss=1.593038558959961
I0210 19:51:31.003396 140529929012992 logging_writer.py:48] [107500] global_step=107500, grad_norm=0.2402208000421524, loss=1.4731335639953613
I0210 19:52:07.124421 140529937405696 logging_writer.py:48] [107600] global_step=107600, grad_norm=0.236308753490448, loss=1.448575735092163
I0210 19:52:43.200823 140529929012992 logging_writer.py:48] [107700] global_step=107700, grad_norm=0.23352813720703125, loss=1.5317659378051758
I0210 19:53:19.292092 140529937405696 logging_writer.py:48] [107800] global_step=107800, grad_norm=0.23628221452236176, loss=1.5840204954147339
I0210 19:53:55.359318 140529929012992 logging_writer.py:48] [107900] global_step=107900, grad_norm=0.23121382296085358, loss=1.5138295888900757
I0210 19:54:31.456996 140529937405696 logging_writer.py:48] [108000] global_step=108000, grad_norm=0.22358858585357666, loss=1.523728847503662
I0210 19:55:07.564793 140529929012992 logging_writer.py:48] [108100] global_step=108100, grad_norm=0.22040881216526031, loss=1.4874848127365112
I0210 19:55:43.656625 140529937405696 logging_writer.py:48] [108200] global_step=108200, grad_norm=0.2278510183095932, loss=1.5323923826217651
I0210 19:56:19.766901 140529929012992 logging_writer.py:48] [108300] global_step=108300, grad_norm=0.23385272920131683, loss=1.5651370286941528
I0210 19:56:55.862744 140529937405696 logging_writer.py:48] [108400] global_step=108400, grad_norm=0.22941036522388458, loss=1.5512386560440063
I0210 19:57:31.976784 140529929012992 logging_writer.py:48] [108500] global_step=108500, grad_norm=0.23823930323123932, loss=1.5017263889312744
I0210 19:58:08.084324 140529937405696 logging_writer.py:48] [108600] global_step=108600, grad_norm=0.2360074371099472, loss=1.5587717294692993
I0210 19:58:44.179671 140529929012992 logging_writer.py:48] [108700] global_step=108700, grad_norm=0.21755146980285645, loss=1.4934126138687134
I0210 19:59:20.271917 140529937405696 logging_writer.py:48] [108800] global_step=108800, grad_norm=0.22773586213588715, loss=1.5200550556182861
I0210 19:59:56.396698 140529929012992 logging_writer.py:48] [108900] global_step=108900, grad_norm=0.22477689385414124, loss=1.457635521888733
I0210 20:00:32.475453 140529937405696 logging_writer.py:48] [109000] global_step=109000, grad_norm=0.24986830353736877, loss=1.497214674949646
I0210 20:01:08.558173 140529929012992 logging_writer.py:48] [109100] global_step=109100, grad_norm=0.2227126508951187, loss=1.5087617635726929
I0210 20:01:44.658175 140529937405696 logging_writer.py:48] [109200] global_step=109200, grad_norm=0.2227516621351242, loss=1.4877219200134277
I0210 20:02:20.718319 140529929012992 logging_writer.py:48] [109300] global_step=109300, grad_norm=0.24478264153003693, loss=1.5236200094223022
I0210 20:02:55.103296 140699726837568 spec.py:321] Evaluating on the training split.
I0210 20:02:58.129250 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 20:07:20.386357 140699726837568 spec.py:333] Evaluating on the validation split.
I0210 20:07:23.115967 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 20:10:01.897032 140699726837568 spec.py:349] Evaluating on the test split.
I0210 20:10:04.634959 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 20:12:30.275149 140699726837568 submission_runner.py:408] Time since start: 66434.09s, 	Step: 109397, 	{'train/accuracy': 0.6863054037094116, 'train/loss': 1.4225127696990967, 'train/bleu': 34.77338731179625, 'validation/accuracy': 0.692328691482544, 'validation/loss': 1.3812284469604492, 'validation/bleu': 30.934935314671602, 'validation/num_examples': 3000, 'test/accuracy': 0.7111498713493347, 'test/loss': 1.274350881576538, 'test/bleu': 31.09661239196818, 'test/num_examples': 3003, 'score': 39515.00077295303, 'total_duration': 66434.0912425518, 'accumulated_submission_time': 39515.00077295303, 'accumulated_eval_time': 26914.005674123764, 'accumulated_logging_time': 1.5969746112823486}
I0210 20:12:30.311111 140529937405696 logging_writer.py:48] [109397] accumulated_eval_time=26914.005674, accumulated_logging_time=1.596975, accumulated_submission_time=39515.000773, global_step=109397, preemption_count=0, score=39515.000773, test/accuracy=0.711150, test/bleu=31.096612, test/loss=1.274351, test/num_examples=3003, total_duration=66434.091243, train/accuracy=0.686305, train/bleu=34.773387, train/loss=1.422513, validation/accuracy=0.692329, validation/bleu=30.934935, validation/loss=1.381228, validation/num_examples=3000
I0210 20:12:31.771888 140529929012992 logging_writer.py:48] [109400] global_step=109400, grad_norm=0.23967230319976807, loss=1.561097502708435
I0210 20:13:07.801060 140529937405696 logging_writer.py:48] [109500] global_step=109500, grad_norm=0.23638588190078735, loss=1.4845011234283447
I0210 20:13:43.828757 140529929012992 logging_writer.py:48] [109600] global_step=109600, grad_norm=0.2162398397922516, loss=1.494231939315796
I0210 20:14:19.972477 140529937405696 logging_writer.py:48] [109700] global_step=109700, grad_norm=0.23492462933063507, loss=1.540624976158142
I0210 20:14:56.129963 140529929012992 logging_writer.py:48] [109800] global_step=109800, grad_norm=0.22515664994716644, loss=1.4971612691879272
I0210 20:15:32.202818 140529937405696 logging_writer.py:48] [109900] global_step=109900, grad_norm=0.21611112356185913, loss=1.5463424921035767
I0210 20:16:08.320334 140529929012992 logging_writer.py:48] [110000] global_step=110000, grad_norm=0.22091227769851685, loss=1.5072880983352661
I0210 20:16:44.440861 140529937405696 logging_writer.py:48] [110100] global_step=110100, grad_norm=0.22667211294174194, loss=1.5090830326080322
I0210 20:17:20.547715 140529929012992 logging_writer.py:48] [110200] global_step=110200, grad_norm=0.2264353632926941, loss=1.5241590738296509
I0210 20:17:56.648027 140529937405696 logging_writer.py:48] [110300] global_step=110300, grad_norm=0.22182567417621613, loss=1.5510261058807373
I0210 20:18:32.745945 140529929012992 logging_writer.py:48] [110400] global_step=110400, grad_norm=0.23500582575798035, loss=1.4842897653579712
I0210 20:19:08.866309 140529937405696 logging_writer.py:48] [110500] global_step=110500, grad_norm=0.23149842023849487, loss=1.497776985168457
I0210 20:19:44.992783 140529929012992 logging_writer.py:48] [110600] global_step=110600, grad_norm=0.23939645290374756, loss=1.4986525774002075
I0210 20:20:21.087360 140529937405696 logging_writer.py:48] [110700] global_step=110700, grad_norm=0.2429097443819046, loss=1.4902288913726807
I0210 20:20:57.168090 140529929012992 logging_writer.py:48] [110800] global_step=110800, grad_norm=0.2320558726787567, loss=1.4979192018508911
I0210 20:21:33.279844 140529937405696 logging_writer.py:48] [110900] global_step=110900, grad_norm=0.22782401740550995, loss=1.5879443883895874
I0210 20:22:09.370511 140529929012992 logging_writer.py:48] [111000] global_step=111000, grad_norm=0.22070686519145966, loss=1.4949469566345215
I0210 20:22:45.452848 140529937405696 logging_writer.py:48] [111100] global_step=111100, grad_norm=0.21996749937534332, loss=1.4834109544754028
I0210 20:23:21.564489 140529929012992 logging_writer.py:48] [111200] global_step=111200, grad_norm=0.23128822445869446, loss=1.446215033531189
I0210 20:23:57.624394 140529937405696 logging_writer.py:48] [111300] global_step=111300, grad_norm=0.23351609706878662, loss=1.4947998523712158
I0210 20:24:33.683423 140529929012992 logging_writer.py:48] [111400] global_step=111400, grad_norm=0.2353193163871765, loss=1.4981303215026855
I0210 20:25:09.771856 140529937405696 logging_writer.py:48] [111500] global_step=111500, grad_norm=0.23064060509204865, loss=1.5214242935180664
I0210 20:25:45.847802 140529929012992 logging_writer.py:48] [111600] global_step=111600, grad_norm=0.2337629348039627, loss=1.508481502532959
I0210 20:26:21.972376 140529937405696 logging_writer.py:48] [111700] global_step=111700, grad_norm=0.229960635304451, loss=1.4668638706207275
I0210 20:26:30.376140 140699726837568 spec.py:321] Evaluating on the training split.
I0210 20:26:33.403090 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 20:30:33.043612 140699726837568 spec.py:333] Evaluating on the validation split.
I0210 20:30:35.771461 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 20:33:15.327687 140699726837568 spec.py:349] Evaluating on the test split.
I0210 20:33:18.063007 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 20:35:35.120147 140699726837568 submission_runner.py:408] Time since start: 67818.94s, 	Step: 111725, 	{'train/accuracy': 0.6838766932487488, 'train/loss': 1.4372414350509644, 'train/bleu': 34.7839582194981, 'validation/accuracy': 0.6921178698539734, 'validation/loss': 1.376046061515808, 'validation/bleu': 30.739287863649857, 'validation/num_examples': 3000, 'test/accuracy': 0.7117308974266052, 'test/loss': 1.2688062191009521, 'test/bleu': 31.122962263411495, 'test/num_examples': 3003, 'score': 40354.97988796234, 'total_duration': 67818.93628644943, 'accumulated_submission_time': 40354.97988796234, 'accumulated_eval_time': 27458.749623537064, 'accumulated_logging_time': 1.6440293788909912}
I0210 20:35:35.151965 140529929012992 logging_writer.py:48] [111725] accumulated_eval_time=27458.749624, accumulated_logging_time=1.644029, accumulated_submission_time=40354.979888, global_step=111725, preemption_count=0, score=40354.979888, test/accuracy=0.711731, test/bleu=31.122962, test/loss=1.268806, test/num_examples=3003, total_duration=67818.936286, train/accuracy=0.683877, train/bleu=34.783958, train/loss=1.437241, validation/accuracy=0.692118, validation/bleu=30.739288, validation/loss=1.376046, validation/num_examples=3000
I0210 20:36:02.493966 140529937405696 logging_writer.py:48] [111800] global_step=111800, grad_norm=0.2206556648015976, loss=1.4210789203643799
I0210 20:36:38.551360 140529929012992 logging_writer.py:48] [111900] global_step=111900, grad_norm=0.23179899156093597, loss=1.4804991483688354
I0210 20:37:14.611887 140529937405696 logging_writer.py:48] [112000] global_step=112000, grad_norm=0.24751004576683044, loss=1.4624652862548828
I0210 20:37:50.695851 140529929012992 logging_writer.py:48] [112100] global_step=112100, grad_norm=0.21980059146881104, loss=1.4964510202407837
I0210 20:38:26.792930 140529937405696 logging_writer.py:48] [112200] global_step=112200, grad_norm=0.22910238802433014, loss=1.4397132396697998
I0210 20:39:02.912774 140529929012992 logging_writer.py:48] [112300] global_step=112300, grad_norm=0.2368898242712021, loss=1.499050498008728
I0210 20:39:39.005694 140529937405696 logging_writer.py:48] [112400] global_step=112400, grad_norm=0.22907046973705292, loss=1.4935616254806519
I0210 20:40:15.107862 140529929012992 logging_writer.py:48] [112500] global_step=112500, grad_norm=0.23425675928592682, loss=1.4668413400650024
I0210 20:40:51.268789 140529937405696 logging_writer.py:48] [112600] global_step=112600, grad_norm=0.23852990567684174, loss=1.492243766784668
I0210 20:41:27.438625 140529929012992 logging_writer.py:48] [112700] global_step=112700, grad_norm=0.238766610622406, loss=1.5287994146347046
I0210 20:42:03.546569 140529937405696 logging_writer.py:48] [112800] global_step=112800, grad_norm=0.236974835395813, loss=1.5433070659637451
I0210 20:42:39.644418 140529929012992 logging_writer.py:48] [112900] global_step=112900, grad_norm=0.2357098013162613, loss=1.442620038986206
I0210 20:43:15.775758 140529937405696 logging_writer.py:48] [113000] global_step=113000, grad_norm=0.23434998095035553, loss=1.5221320390701294
I0210 20:43:51.897074 140529929012992 logging_writer.py:48] [113100] global_step=113100, grad_norm=0.2566785514354706, loss=1.5462303161621094
I0210 20:44:28.064758 140529937405696 logging_writer.py:48] [113200] global_step=113200, grad_norm=0.2551428973674774, loss=1.5358937978744507
I0210 20:45:04.148922 140529929012992 logging_writer.py:48] [113300] global_step=113300, grad_norm=0.24139176309108734, loss=1.5423301458358765
I0210 20:45:40.273796 140529937405696 logging_writer.py:48] [113400] global_step=113400, grad_norm=0.2334727793931961, loss=1.501747965812683
I0210 20:46:16.355820 140529929012992 logging_writer.py:48] [113500] global_step=113500, grad_norm=0.21942958235740662, loss=1.4095964431762695
I0210 20:46:52.445548 140529937405696 logging_writer.py:48] [113600] global_step=113600, grad_norm=0.24472515285015106, loss=1.4641836881637573
I0210 20:47:28.577572 140529929012992 logging_writer.py:48] [113700] global_step=113700, grad_norm=0.2362114042043686, loss=1.4494181871414185
I0210 20:48:04.731823 140529937405696 logging_writer.py:48] [113800] global_step=113800, grad_norm=0.2290194034576416, loss=1.4620623588562012
I0210 20:48:40.895475 140529929012992 logging_writer.py:48] [113900] global_step=113900, grad_norm=0.23424328863620758, loss=1.5018044710159302
I0210 20:49:16.997119 140529937405696 logging_writer.py:48] [114000] global_step=114000, grad_norm=0.23007024824619293, loss=1.4157977104187012
I0210 20:49:35.480811 140699726837568 spec.py:321] Evaluating on the training split.
I0210 20:49:38.509160 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 20:53:57.591761 140699726837568 spec.py:333] Evaluating on the validation split.
I0210 20:54:00.336682 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 20:56:51.707844 140699726837568 spec.py:349] Evaluating on the test split.
I0210 20:56:54.435666 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 20:59:15.520585 140699726837568 submission_runner.py:408] Time since start: 69239.34s, 	Step: 114053, 	{'train/accuracy': 0.6951395273208618, 'train/loss': 1.3722827434539795, 'train/bleu': 35.48070667530032, 'validation/accuracy': 0.6938289403915405, 'validation/loss': 1.3752260208129883, 'validation/bleu': 30.685051686956182, 'validation/num_examples': 3000, 'test/accuracy': 0.7118470668792725, 'test/loss': 1.2679171562194824, 'test/bleu': 31.035798009097697, 'test/num_examples': 3003, 'score': 41195.22360944748, 'total_duration': 69239.3367304802, 'accumulated_submission_time': 41195.22360944748, 'accumulated_eval_time': 28038.789351701736, 'accumulated_logging_time': 1.6860826015472412}
I0210 20:59:15.550654 140529929012992 logging_writer.py:48] [114053] accumulated_eval_time=28038.789352, accumulated_logging_time=1.686083, accumulated_submission_time=41195.223609, global_step=114053, preemption_count=0, score=41195.223609, test/accuracy=0.711847, test/bleu=31.035798, test/loss=1.267917, test/num_examples=3003, total_duration=69239.336730, train/accuracy=0.695140, train/bleu=35.480707, train/loss=1.372283, validation/accuracy=0.693829, validation/bleu=30.685052, validation/loss=1.375226, validation/num_examples=3000
I0210 20:59:32.803184 140529937405696 logging_writer.py:48] [114100] global_step=114100, grad_norm=0.23116491734981537, loss=1.4838677644729614
I0210 21:00:08.885937 140529929012992 logging_writer.py:48] [114200] global_step=114200, grad_norm=0.22463233768939972, loss=1.4136561155319214
I0210 21:00:44.921221 140529937405696 logging_writer.py:48] [114300] global_step=114300, grad_norm=0.23311802744865417, loss=1.4004225730895996
I0210 21:01:20.976819 140529929012992 logging_writer.py:48] [114400] global_step=114400, grad_norm=0.22239311039447784, loss=1.462478518486023
I0210 21:01:57.038216 140529937405696 logging_writer.py:48] [114500] global_step=114500, grad_norm=0.22642897069454193, loss=1.4855352640151978
I0210 21:02:33.164163 140529929012992 logging_writer.py:48] [114600] global_step=114600, grad_norm=0.24993929266929626, loss=1.5429049730300903
I0210 21:03:09.306701 140529937405696 logging_writer.py:48] [114700] global_step=114700, grad_norm=0.2270030379295349, loss=1.4824775457382202
I0210 21:03:45.397929 140529929012992 logging_writer.py:48] [114800] global_step=114800, grad_norm=0.23683704435825348, loss=1.457809329032898
I0210 21:04:21.474830 140529937405696 logging_writer.py:48] [114900] global_step=114900, grad_norm=0.22489501535892487, loss=1.562313437461853
I0210 21:04:57.533667 140529929012992 logging_writer.py:48] [115000] global_step=115000, grad_norm=0.23688733577728271, loss=1.459273338317871
I0210 21:05:33.654107 140529937405696 logging_writer.py:48] [115100] global_step=115100, grad_norm=0.24121607840061188, loss=1.462807297706604
I0210 21:06:09.748545 140529929012992 logging_writer.py:48] [115200] global_step=115200, grad_norm=0.2416195422410965, loss=1.5268288850784302
I0210 21:06:45.912235 140529937405696 logging_writer.py:48] [115300] global_step=115300, grad_norm=0.23435653746128082, loss=1.4720340967178345
I0210 21:07:22.048280 140529929012992 logging_writer.py:48] [115400] global_step=115400, grad_norm=0.23053492605686188, loss=1.504417896270752
I0210 21:07:58.157378 140529937405696 logging_writer.py:48] [115500] global_step=115500, grad_norm=0.23614682257175446, loss=1.453835368156433
I0210 21:08:34.345163 140529929012992 logging_writer.py:48] [115600] global_step=115600, grad_norm=0.2316272109746933, loss=1.4747461080551147
I0210 21:09:10.473242 140529937405696 logging_writer.py:48] [115700] global_step=115700, grad_norm=0.22985650599002838, loss=1.411618709564209
I0210 21:09:46.583895 140529929012992 logging_writer.py:48] [115800] global_step=115800, grad_norm=0.2296702116727829, loss=1.5124479532241821
I0210 21:10:22.667268 140529937405696 logging_writer.py:48] [115900] global_step=115900, grad_norm=0.22264279425144196, loss=1.4732781648635864
I0210 21:10:58.768860 140529929012992 logging_writer.py:48] [116000] global_step=116000, grad_norm=0.23334266245365143, loss=1.475378155708313
I0210 21:11:34.882240 140529937405696 logging_writer.py:48] [116100] global_step=116100, grad_norm=0.23565168678760529, loss=1.4661208391189575
I0210 21:12:10.978011 140529929012992 logging_writer.py:48] [116200] global_step=116200, grad_norm=0.2382647842168808, loss=1.5392746925354004
I0210 21:12:47.099028 140529937405696 logging_writer.py:48] [116300] global_step=116300, grad_norm=0.2378259152173996, loss=1.3753126859664917
I0210 21:13:15.702907 140699726837568 spec.py:321] Evaluating on the training split.
I0210 21:13:18.730406 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 21:17:23.429020 140699726837568 spec.py:333] Evaluating on the validation split.
I0210 21:17:26.151648 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 21:20:12.820496 140699726837568 spec.py:349] Evaluating on the test split.
I0210 21:20:15.548798 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 21:22:52.836088 140699726837568 submission_runner.py:408] Time since start: 70656.65s, 	Step: 116381, 	{'train/accuracy': 0.6891108155250549, 'train/loss': 1.4094853401184082, 'train/bleu': 35.24992178839545, 'validation/accuracy': 0.6954160332679749, 'validation/loss': 1.3703862428665161, 'validation/bleu': 30.92770434969378, 'validation/num_examples': 3000, 'test/accuracy': 0.713706374168396, 'test/loss': 1.2614657878875732, 'test/bleu': 31.349120365228714, 'test/num_examples': 3003, 'score': 42035.293724536896, 'total_duration': 70656.65222883224, 'accumulated_submission_time': 42035.293724536896, 'accumulated_eval_time': 28615.922476530075, 'accumulated_logging_time': 1.726020097732544}
I0210 21:22:52.868740 140529929012992 logging_writer.py:48] [116381] accumulated_eval_time=28615.922477, accumulated_logging_time=1.726020, accumulated_submission_time=42035.293725, global_step=116381, preemption_count=0, score=42035.293725, test/accuracy=0.713706, test/bleu=31.349120, test/loss=1.261466, test/num_examples=3003, total_duration=70656.652229, train/accuracy=0.689111, train/bleu=35.249922, train/loss=1.409485, validation/accuracy=0.695416, validation/bleu=30.927704, validation/loss=1.370386, validation/num_examples=3000
I0210 21:23:00.061839 140529937405696 logging_writer.py:48] [116400] global_step=116400, grad_norm=0.22645486891269684, loss=1.473624587059021
I0210 21:23:35.967384 140529929012992 logging_writer.py:48] [116500] global_step=116500, grad_norm=0.23188677430152893, loss=1.4328521490097046
I0210 21:24:12.032842 140529937405696 logging_writer.py:48] [116600] global_step=116600, grad_norm=0.22054049372673035, loss=1.4182432889938354
I0210 21:24:48.120985 140529929012992 logging_writer.py:48] [116700] global_step=116700, grad_norm=0.2506026327610016, loss=1.5112990140914917
I0210 21:25:24.208501 140529937405696 logging_writer.py:48] [116800] global_step=116800, grad_norm=0.2530103027820587, loss=1.518571376800537
I0210 21:26:00.295160 140529929012992 logging_writer.py:48] [116900] global_step=116900, grad_norm=0.23552630841732025, loss=1.506237506866455
I0210 21:26:36.415410 140529937405696 logging_writer.py:48] [117000] global_step=117000, grad_norm=0.22757235169410706, loss=1.4067480564117432
I0210 21:27:12.485809 140529929012992 logging_writer.py:48] [117100] global_step=117100, grad_norm=0.23322369158267975, loss=1.4832472801208496
I0210 21:27:48.597341 140529937405696 logging_writer.py:48] [117200] global_step=117200, grad_norm=0.24518869817256927, loss=1.4788029193878174
I0210 21:28:24.741948 140529929012992 logging_writer.py:48] [117300] global_step=117300, grad_norm=0.24630485475063324, loss=1.5368541479110718
I0210 21:29:00.932522 140529937405696 logging_writer.py:48] [117400] global_step=117400, grad_norm=0.23689916729927063, loss=1.4907476902008057
I0210 21:29:37.017058 140529929012992 logging_writer.py:48] [117500] global_step=117500, grad_norm=0.22672058641910553, loss=1.446768045425415
I0210 21:30:13.118370 140529937405696 logging_writer.py:48] [117600] global_step=117600, grad_norm=0.23380598425865173, loss=1.4529393911361694
I0210 21:30:49.203058 140529929012992 logging_writer.py:48] [117700] global_step=117700, grad_norm=0.2347860485315323, loss=1.4459447860717773
I0210 21:31:25.283309 140529937405696 logging_writer.py:48] [117800] global_step=117800, grad_norm=0.24047939479351044, loss=1.448452115058899
I0210 21:32:01.384675 140529929012992 logging_writer.py:48] [117900] global_step=117900, grad_norm=0.24570445716381073, loss=1.4318827390670776
I0210 21:32:37.538259 140529937405696 logging_writer.py:48] [118000] global_step=118000, grad_norm=0.2196415364742279, loss=1.3568898439407349
I0210 21:33:13.702027 140529929012992 logging_writer.py:48] [118100] global_step=118100, grad_norm=0.22952750325202942, loss=1.4030025005340576
I0210 21:33:49.795698 140529937405696 logging_writer.py:48] [118200] global_step=118200, grad_norm=0.23422598838806152, loss=1.4960049390792847
I0210 21:34:25.900480 140529929012992 logging_writer.py:48] [118300] global_step=118300, grad_norm=0.23565663397312164, loss=1.4716076850891113
I0210 21:35:02.041956 140529937405696 logging_writer.py:48] [118400] global_step=118400, grad_norm=0.23194436728954315, loss=1.4472249746322632
I0210 21:35:38.121913 140529929012992 logging_writer.py:48] [118500] global_step=118500, grad_norm=0.2446443736553192, loss=1.577407956123352
I0210 21:36:14.210151 140529937405696 logging_writer.py:48] [118600] global_step=118600, grad_norm=0.24093075096607208, loss=1.6157649755477905
I0210 21:36:50.324112 140529929012992 logging_writer.py:48] [118700] global_step=118700, grad_norm=0.23073096573352814, loss=1.5291000604629517
I0210 21:36:52.947709 140699726837568 spec.py:321] Evaluating on the training split.
I0210 21:36:55.981431 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 21:41:04.003801 140699726837568 spec.py:333] Evaluating on the validation split.
I0210 21:41:06.727536 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 21:43:37.912164 140699726837568 spec.py:349] Evaluating on the test split.
I0210 21:43:40.640484 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 21:46:06.622147 140699726837568 submission_runner.py:408] Time since start: 72050.44s, 	Step: 118709, 	{'train/accuracy': 0.6923836469650269, 'train/loss': 1.3908350467681885, 'train/bleu': 34.80738047912399, 'validation/accuracy': 0.6954408288002014, 'validation/loss': 1.3662816286087036, 'validation/bleu': 31.010793828916093, 'validation/num_examples': 3000, 'test/accuracy': 0.7125559449195862, 'test/loss': 1.260074496269226, 'test/bleu': 31.390352867613373, 'test/num_examples': 3003, 'score': 42875.2884888649, 'total_duration': 72050.43827652931, 'accumulated_submission_time': 42875.2884888649, 'accumulated_eval_time': 29169.59684228897, 'accumulated_logging_time': 1.7702577114105225}
I0210 21:46:06.662172 140529937405696 logging_writer.py:48] [118709] accumulated_eval_time=29169.596842, accumulated_logging_time=1.770258, accumulated_submission_time=42875.288489, global_step=118709, preemption_count=0, score=42875.288489, test/accuracy=0.712556, test/bleu=31.390353, test/loss=1.260074, test/num_examples=3003, total_duration=72050.438277, train/accuracy=0.692384, train/bleu=34.807380, train/loss=1.390835, validation/accuracy=0.695441, validation/bleu=31.010794, validation/loss=1.366282, validation/num_examples=3000
I0210 21:46:39.718034 140529929012992 logging_writer.py:48] [118800] global_step=118800, grad_norm=0.23000456392765045, loss=1.3871716260910034
I0210 21:47:15.731485 140529937405696 logging_writer.py:48] [118900] global_step=118900, grad_norm=0.23593875765800476, loss=1.489702820777893
I0210 21:47:51.816559 140529929012992 logging_writer.py:48] [119000] global_step=119000, grad_norm=0.23196075856685638, loss=1.4465445280075073
I0210 21:48:27.900789 140529937405696 logging_writer.py:48] [119100] global_step=119100, grad_norm=0.22857211530208588, loss=1.4107283353805542
I0210 21:49:03.945773 140529929012992 logging_writer.py:48] [119200] global_step=119200, grad_norm=0.23596052825450897, loss=1.4884001016616821
I0210 21:49:40.074386 140529937405696 logging_writer.py:48] [119300] global_step=119300, grad_norm=0.2344030737876892, loss=1.4490950107574463
I0210 21:50:16.199307 140529929012992 logging_writer.py:48] [119400] global_step=119400, grad_norm=0.24414455890655518, loss=1.4749997854232788
I0210 21:50:52.288836 140529937405696 logging_writer.py:48] [119500] global_step=119500, grad_norm=0.24296864867210388, loss=1.4655894041061401
I0210 21:51:28.445514 140529929012992 logging_writer.py:48] [119600] global_step=119600, grad_norm=0.2389468103647232, loss=1.46729576587677
I0210 21:52:04.622885 140529937405696 logging_writer.py:48] [119700] global_step=119700, grad_norm=0.23013938963413239, loss=1.3889069557189941
I0210 21:52:40.760653 140529929012992 logging_writer.py:48] [119800] global_step=119800, grad_norm=0.22058437764644623, loss=1.4775068759918213
I0210 21:53:16.866945 140529937405696 logging_writer.py:48] [119900] global_step=119900, grad_norm=0.2388342022895813, loss=1.4693669080734253
I0210 21:53:52.989606 140529929012992 logging_writer.py:48] [120000] global_step=120000, grad_norm=0.23974624276161194, loss=1.4601173400878906
I0210 21:54:29.185228 140529937405696 logging_writer.py:48] [120100] global_step=120100, grad_norm=0.23037569224834442, loss=1.4289048910140991
I0210 21:55:05.304401 140529929012992 logging_writer.py:48] [120200] global_step=120200, grad_norm=0.23832127451896667, loss=1.4761497974395752
I0210 21:55:41.464631 140529937405696 logging_writer.py:48] [120300] global_step=120300, grad_norm=0.23843976855278015, loss=1.4782830476760864
I0210 21:56:17.609830 140529929012992 logging_writer.py:48] [120400] global_step=120400, grad_norm=0.23911932110786438, loss=1.5071402788162231
I0210 21:56:53.800873 140529937405696 logging_writer.py:48] [120500] global_step=120500, grad_norm=0.24055525660514832, loss=1.3950155973434448
I0210 21:57:29.910519 140529929012992 logging_writer.py:48] [120600] global_step=120600, grad_norm=0.2456226497888565, loss=1.493788719177246
I0210 21:58:06.078451 140529937405696 logging_writer.py:48] [120700] global_step=120700, grad_norm=0.24663032591342926, loss=1.4521446228027344
I0210 21:58:42.165877 140529929012992 logging_writer.py:48] [120800] global_step=120800, grad_norm=0.2330859899520874, loss=1.4922438859939575
I0210 21:59:18.303824 140529937405696 logging_writer.py:48] [120900] global_step=120900, grad_norm=0.25366875529289246, loss=1.4061355590820312
I0210 21:59:54.391761 140529929012992 logging_writer.py:48] [121000] global_step=121000, grad_norm=0.2274453341960907, loss=1.4208720922470093
I0210 22:00:06.751657 140699726837568 spec.py:321] Evaluating on the training split.
I0210 22:00:09.778554 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 22:04:10.313198 140699726837568 spec.py:333] Evaluating on the validation split.
I0210 22:04:13.031871 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 22:06:39.340870 140699726837568 spec.py:349] Evaluating on the test split.
I0210 22:06:42.079497 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 22:08:53.297065 140699726837568 submission_runner.py:408] Time since start: 73417.11s, 	Step: 121036, 	{'train/accuracy': 0.6926527619361877, 'train/loss': 1.3857016563415527, 'train/bleu': 35.392830009300184, 'validation/accuracy': 0.6950812935829163, 'validation/loss': 1.3645596504211426, 'validation/bleu': 30.888696962120584, 'validation/num_examples': 3000, 'test/accuracy': 0.7132415771484375, 'test/loss': 1.257387399673462, 'test/bleu': 31.336217699826122, 'test/num_examples': 3003, 'score': 43715.2911632061, 'total_duration': 73417.11321353912, 'accumulated_submission_time': 43715.2911632061, 'accumulated_eval_time': 29696.142201185226, 'accumulated_logging_time': 1.8214778900146484}
I0210 22:08:53.329131 140529937405696 logging_writer.py:48] [121036] accumulated_eval_time=29696.142201, accumulated_logging_time=1.821478, accumulated_submission_time=43715.291163, global_step=121036, preemption_count=0, score=43715.291163, test/accuracy=0.713242, test/bleu=31.336218, test/loss=1.257387, test/num_examples=3003, total_duration=73417.113214, train/accuracy=0.692653, train/bleu=35.392830, train/loss=1.385702, validation/accuracy=0.695081, validation/bleu=30.888697, validation/loss=1.364560, validation/num_examples=3000
I0210 22:09:16.672805 140529929012992 logging_writer.py:48] [121100] global_step=121100, grad_norm=0.24123741686344147, loss=1.495885968208313
I0210 22:09:52.631109 140529937405696 logging_writer.py:48] [121200] global_step=121200, grad_norm=0.22595952451229095, loss=1.391587257385254
I0210 22:10:28.670417 140529929012992 logging_writer.py:48] [121300] global_step=121300, grad_norm=0.24098722636699677, loss=1.3926950693130493
I0210 22:11:04.749311 140529937405696 logging_writer.py:48] [121400] global_step=121400, grad_norm=0.23627403378486633, loss=1.4392139911651611
I0210 22:11:40.855937 140529929012992 logging_writer.py:48] [121500] global_step=121500, grad_norm=0.2376154512166977, loss=1.5574411153793335
I0210 22:12:16.935147 140529937405696 logging_writer.py:48] [121600] global_step=121600, grad_norm=0.23000146448612213, loss=1.4065990447998047
I0210 22:12:53.027146 140529929012992 logging_writer.py:48] [121700] global_step=121700, grad_norm=0.23126837611198425, loss=1.426125168800354
I0210 22:13:29.119491 140529937405696 logging_writer.py:48] [121800] global_step=121800, grad_norm=0.23368288576602936, loss=1.4997239112854004
I0210 22:14:05.237070 140529929012992 logging_writer.py:48] [121900] global_step=121900, grad_norm=0.24337643384933472, loss=1.4404634237289429
I0210 22:14:41.317509 140529937405696 logging_writer.py:48] [122000] global_step=122000, grad_norm=0.23557354509830475, loss=1.409119963645935
I0210 22:15:17.419008 140529929012992 logging_writer.py:48] [122100] global_step=122100, grad_norm=0.23791643977165222, loss=1.5016814470291138
I0210 22:15:53.510795 140529937405696 logging_writer.py:48] [122200] global_step=122200, grad_norm=0.23338164389133453, loss=1.4487016201019287
I0210 22:16:29.636278 140529929012992 logging_writer.py:48] [122300] global_step=122300, grad_norm=0.2416648268699646, loss=1.419712781906128
I0210 22:17:05.746894 140529937405696 logging_writer.py:48] [122400] global_step=122400, grad_norm=0.2508336305618286, loss=1.553164005279541
I0210 22:17:41.888865 140529929012992 logging_writer.py:48] [122500] global_step=122500, grad_norm=0.23217938840389252, loss=1.4233739376068115
I0210 22:18:18.022522 140529937405696 logging_writer.py:48] [122600] global_step=122600, grad_norm=0.22930029034614563, loss=1.4285691976547241
I0210 22:18:54.095873 140529929012992 logging_writer.py:48] [122700] global_step=122700, grad_norm=0.24063800275325775, loss=1.535872220993042
I0210 22:19:30.186157 140529937405696 logging_writer.py:48] [122800] global_step=122800, grad_norm=0.235714390873909, loss=1.3839881420135498
I0210 22:20:06.362668 140529929012992 logging_writer.py:48] [122900] global_step=122900, grad_norm=0.2436053305864334, loss=1.4597821235656738
I0210 22:20:42.538597 140529937405696 logging_writer.py:48] [123000] global_step=123000, grad_norm=0.23246634006500244, loss=1.456102728843689
I0210 22:21:18.692724 140529929012992 logging_writer.py:48] [123100] global_step=123100, grad_norm=0.226566880941391, loss=1.4382227659225464
I0210 22:21:54.791055 140529937405696 logging_writer.py:48] [123200] global_step=123200, grad_norm=0.234506756067276, loss=1.4779798984527588
I0210 22:22:30.885564 140529929012992 logging_writer.py:48] [123300] global_step=123300, grad_norm=0.2532735764980316, loss=1.4277560710906982
I0210 22:22:53.366868 140699726837568 spec.py:321] Evaluating on the training split.
I0210 22:22:56.419848 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 22:26:50.417213 140699726837568 spec.py:333] Evaluating on the validation split.
I0210 22:26:53.155452 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 22:29:34.481317 140699726837568 spec.py:349] Evaluating on the test split.
I0210 22:29:37.216371 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 22:31:59.557941 140699726837568 submission_runner.py:408] Time since start: 74803.37s, 	Step: 123364, 	{'train/accuracy': 0.6927144527435303, 'train/loss': 1.3827232122421265, 'train/bleu': 35.22963795099508, 'validation/accuracy': 0.6956640481948853, 'validation/loss': 1.36439049243927, 'validation/bleu': 30.82173282251918, 'validation/num_examples': 3000, 'test/accuracy': 0.7135552763938904, 'test/loss': 1.2557114362716675, 'test/bleu': 31.546370535616788, 'test/num_examples': 3003, 'score': 44555.2459564209, 'total_duration': 74803.37408471107, 'accumulated_submission_time': 44555.2459564209, 'accumulated_eval_time': 30242.333233594894, 'accumulated_logging_time': 1.863266944885254}
I0210 22:31:59.590838 140529937405696 logging_writer.py:48] [123364] accumulated_eval_time=30242.333234, accumulated_logging_time=1.863267, accumulated_submission_time=44555.245956, global_step=123364, preemption_count=0, score=44555.245956, test/accuracy=0.713555, test/bleu=31.546371, test/loss=1.255711, test/num_examples=3003, total_duration=74803.374085, train/accuracy=0.692714, train/bleu=35.229638, train/loss=1.382723, validation/accuracy=0.695664, validation/bleu=30.821733, validation/loss=1.364390, validation/num_examples=3000
I0210 22:32:12.903064 140529929012992 logging_writer.py:48] [123400] global_step=123400, grad_norm=0.24200204014778137, loss=1.3917838335037231
I0210 22:32:48.878147 140529937405696 logging_writer.py:48] [123500] global_step=123500, grad_norm=0.24151362478733063, loss=1.4840110540390015
I0210 22:33:24.942444 140529929012992 logging_writer.py:48] [123600] global_step=123600, grad_norm=0.23698441684246063, loss=1.441922903060913
I0210 22:34:01.201425 140529937405696 logging_writer.py:48] [123700] global_step=123700, grad_norm=0.24257805943489075, loss=1.4702509641647339
I0210 22:34:37.378944 140529929012992 logging_writer.py:48] [123800] global_step=123800, grad_norm=0.22994999587535858, loss=1.4567210674285889
I0210 22:35:13.491104 140529937405696 logging_writer.py:48] [123900] global_step=123900, grad_norm=0.23819509148597717, loss=1.4875353574752808
I0210 22:35:49.573825 140529929012992 logging_writer.py:48] [124000] global_step=124000, grad_norm=0.22930552065372467, loss=1.4270049333572388
I0210 22:36:25.683178 140529937405696 logging_writer.py:48] [124100] global_step=124100, grad_norm=0.23779164254665375, loss=1.4356720447540283
I0210 22:37:01.800816 140529929012992 logging_writer.py:48] [124200] global_step=124200, grad_norm=0.23907123506069183, loss=1.473639726638794
I0210 22:37:37.909222 140529937405696 logging_writer.py:48] [124300] global_step=124300, grad_norm=0.2569718062877655, loss=1.4766160249710083
I0210 22:38:13.984625 140529929012992 logging_writer.py:48] [124400] global_step=124400, grad_norm=0.24160198867321014, loss=1.4984265565872192
I0210 22:38:50.123793 140529937405696 logging_writer.py:48] [124500] global_step=124500, grad_norm=0.2396622747182846, loss=1.5229061841964722
I0210 22:39:26.228205 140529929012992 logging_writer.py:48] [124600] global_step=124600, grad_norm=0.2430945634841919, loss=1.4747339487075806
I0210 22:40:02.338855 140529937405696 logging_writer.py:48] [124700] global_step=124700, grad_norm=0.23911263048648834, loss=1.4974061250686646
I0210 22:40:38.439152 140529929012992 logging_writer.py:48] [124800] global_step=124800, grad_norm=0.23000404238700867, loss=1.3735530376434326
I0210 22:41:14.550216 140529937405696 logging_writer.py:48] [124900] global_step=124900, grad_norm=0.22826896607875824, loss=1.4509562253952026
I0210 22:41:50.713078 140529929012992 logging_writer.py:48] [125000] global_step=125000, grad_norm=0.2375958412885666, loss=1.3506174087524414
I0210 22:42:26.867424 140529937405696 logging_writer.py:48] [125100] global_step=125100, grad_norm=0.24315762519836426, loss=1.5059906244277954
I0210 22:43:02.963590 140529929012992 logging_writer.py:48] [125200] global_step=125200, grad_norm=0.22576378285884857, loss=1.4688535928726196
I0210 22:43:39.050517 140529937405696 logging_writer.py:48] [125300] global_step=125300, grad_norm=0.23495028913021088, loss=1.4744700193405151
I0210 22:44:15.152352 140529929012992 logging_writer.py:48] [125400] global_step=125400, grad_norm=0.23781663179397583, loss=1.4875069856643677
I0210 22:44:51.253296 140529937405696 logging_writer.py:48] [125500] global_step=125500, grad_norm=0.23476465046405792, loss=1.4638454914093018
I0210 22:45:27.391998 140529929012992 logging_writer.py:48] [125600] global_step=125600, grad_norm=0.23294298350811005, loss=1.4604181051254272
I0210 22:45:59.564321 140699726837568 spec.py:321] Evaluating on the training split.
I0210 22:46:02.604120 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 22:50:12.032483 140699726837568 spec.py:333] Evaluating on the validation split.
I0210 22:50:14.758380 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 22:52:59.826084 140699726837568 spec.py:349] Evaluating on the test split.
I0210 22:53:02.565389 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 22:55:36.186830 140699726837568 submission_runner.py:408] Time since start: 76220.00s, 	Step: 125691, 	{'train/accuracy': 0.6953275799751282, 'train/loss': 1.366080403327942, 'train/bleu': 35.52793475734423, 'validation/accuracy': 0.6961103677749634, 'validation/loss': 1.3615988492965698, 'validation/bleu': 30.92001661383089, 'validation/num_examples': 3000, 'test/accuracy': 0.7137877345085144, 'test/loss': 1.2536863088607788, 'test/bleu': 31.459191760161236, 'test/num_examples': 3003, 'score': 45395.13310265541, 'total_duration': 76220.00295948982, 'accumulated_submission_time': 45395.13310265541, 'accumulated_eval_time': 30818.95568537712, 'accumulated_logging_time': 1.906174659729004}
I0210 22:55:36.219592 140529937405696 logging_writer.py:48] [125691] accumulated_eval_time=30818.955685, accumulated_logging_time=1.906175, accumulated_submission_time=45395.133103, global_step=125691, preemption_count=0, score=45395.133103, test/accuracy=0.713788, test/bleu=31.459192, test/loss=1.253686, test/num_examples=3003, total_duration=76220.002959, train/accuracy=0.695328, train/bleu=35.527935, train/loss=1.366080, validation/accuracy=0.696110, validation/bleu=30.920017, validation/loss=1.361599, validation/num_examples=3000
I0210 22:55:39.836279 140529929012992 logging_writer.py:48] [125700] global_step=125700, grad_norm=0.22807756066322327, loss=1.434448480606079
I0210 22:56:15.843942 140529937405696 logging_writer.py:48] [125800] global_step=125800, grad_norm=0.24802491068840027, loss=1.4957948923110962
I0210 22:56:51.910537 140529929012992 logging_writer.py:48] [125900] global_step=125900, grad_norm=0.22324590384960175, loss=1.3926866054534912
I0210 22:57:28.024631 140529937405696 logging_writer.py:48] [126000] global_step=126000, grad_norm=0.2421707808971405, loss=1.4615689516067505
I0210 22:58:04.102953 140529929012992 logging_writer.py:48] [126100] global_step=126100, grad_norm=0.24917294085025787, loss=1.459047794342041
I0210 22:58:40.199423 140529937405696 logging_writer.py:48] [126200] global_step=126200, grad_norm=0.2457820177078247, loss=1.4810410737991333
I0210 22:59:16.292620 140529929012992 logging_writer.py:48] [126300] global_step=126300, grad_norm=0.2337818443775177, loss=1.48039710521698
I0210 22:59:52.388216 140529937405696 logging_writer.py:48] [126400] global_step=126400, grad_norm=0.23864538967609406, loss=1.5069217681884766
I0210 23:00:28.496768 140529929012992 logging_writer.py:48] [126500] global_step=126500, grad_norm=0.23440800607204437, loss=1.401353120803833
I0210 23:01:04.604749 140529937405696 logging_writer.py:48] [126600] global_step=126600, grad_norm=0.23597772419452667, loss=1.506786584854126
I0210 23:01:40.782149 140529929012992 logging_writer.py:48] [126700] global_step=126700, grad_norm=0.22912655770778656, loss=1.447417140007019
I0210 23:02:16.938577 140529937405696 logging_writer.py:48] [126800] global_step=126800, grad_norm=0.2311948835849762, loss=1.417661428451538
I0210 23:02:53.072232 140529929012992 logging_writer.py:48] [126900] global_step=126900, grad_norm=0.23990949988365173, loss=1.519046664237976
I0210 23:03:29.205101 140529937405696 logging_writer.py:48] [127000] global_step=127000, grad_norm=0.23456496000289917, loss=1.4068925380706787
I0210 23:04:05.292712 140529929012992 logging_writer.py:48] [127100] global_step=127100, grad_norm=0.2295192927122116, loss=1.428123950958252
I0210 23:04:41.413533 140529937405696 logging_writer.py:48] [127200] global_step=127200, grad_norm=0.23070059716701508, loss=1.4759278297424316
I0210 23:05:17.520449 140529929012992 logging_writer.py:48] [127300] global_step=127300, grad_norm=0.22701255977153778, loss=1.489296317100525
I0210 23:05:53.626658 140529937405696 logging_writer.py:48] [127400] global_step=127400, grad_norm=0.23839245736598969, loss=1.616769552230835
I0210 23:06:29.737765 140529929012992 logging_writer.py:48] [127500] global_step=127500, grad_norm=0.22474296391010284, loss=1.4330568313598633
I0210 23:07:05.856784 140529937405696 logging_writer.py:48] [127600] global_step=127600, grad_norm=0.24112284183502197, loss=1.5176968574523926
I0210 23:07:41.999986 140529929012992 logging_writer.py:48] [127700] global_step=127700, grad_norm=0.23503319919109344, loss=1.4630109071731567
I0210 23:08:18.104100 140529937405696 logging_writer.py:48] [127800] global_step=127800, grad_norm=0.2581046223640442, loss=1.4842880964279175
I0210 23:08:54.275912 140529929012992 logging_writer.py:48] [127900] global_step=127900, grad_norm=0.23575818538665771, loss=1.4209318161010742
I0210 23:09:30.385962 140529937405696 logging_writer.py:48] [128000] global_step=128000, grad_norm=0.2479785829782486, loss=1.4759232997894287
I0210 23:09:36.242721 140699726837568 spec.py:321] Evaluating on the training split.
I0210 23:09:39.275789 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 23:13:48.232434 140699726837568 spec.py:333] Evaluating on the validation split.
I0210 23:13:50.958812 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 23:16:25.556027 140699726837568 spec.py:349] Evaluating on the test split.
I0210 23:16:28.289917 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 23:18:50.160343 140699726837568 submission_runner.py:408] Time since start: 77613.98s, 	Step: 128018, 	{'train/accuracy': 0.6966461539268494, 'train/loss': 1.3577289581298828, 'train/bleu': 35.648902859429185, 'validation/accuracy': 0.6961227655410767, 'validation/loss': 1.362878441810608, 'validation/bleu': 30.908921117943162, 'validation/num_examples': 3000, 'test/accuracy': 0.7140550017356873, 'test/loss': 1.2544089555740356, 'test/bleu': 31.35299748207434, 'test/num_examples': 3003, 'score': 46235.07247853279, 'total_duration': 77613.97647738457, 'accumulated_submission_time': 46235.07247853279, 'accumulated_eval_time': 31372.873242139816, 'accumulated_logging_time': 1.9501900672912598}
I0210 23:18:50.192940 140529929012992 logging_writer.py:48] [128018] accumulated_eval_time=31372.873242, accumulated_logging_time=1.950190, accumulated_submission_time=46235.072479, global_step=128018, preemption_count=0, score=46235.072479, test/accuracy=0.714055, test/bleu=31.352997, test/loss=1.254409, test/num_examples=3003, total_duration=77613.976477, train/accuracy=0.696646, train/bleu=35.648903, train/loss=1.357729, validation/accuracy=0.696123, validation/bleu=30.908921, validation/loss=1.362878, validation/num_examples=3000
I0210 23:19:20.043121 140529937405696 logging_writer.py:48] [128100] global_step=128100, grad_norm=0.22732986509799957, loss=1.425419807434082
I0210 23:19:56.018950 140529929012992 logging_writer.py:48] [128200] global_step=128200, grad_norm=0.2384292632341385, loss=1.37783944606781
I0210 23:20:32.080041 140529937405696 logging_writer.py:48] [128300] global_step=128300, grad_norm=0.23102031648159027, loss=1.4017488956451416
I0210 23:21:08.157205 140529929012992 logging_writer.py:48] [128400] global_step=128400, grad_norm=0.23986873030662537, loss=1.4384992122650146
I0210 23:21:44.268405 140529937405696 logging_writer.py:48] [128500] global_step=128500, grad_norm=0.24005159735679626, loss=1.3739980459213257
I0210 23:22:20.419967 140529929012992 logging_writer.py:48] [128600] global_step=128600, grad_norm=0.2364584058523178, loss=1.4789278507232666
I0210 23:22:56.530777 140529937405696 logging_writer.py:48] [128700] global_step=128700, grad_norm=0.22801688313484192, loss=1.474921703338623
I0210 23:23:32.625987 140529929012992 logging_writer.py:48] [128800] global_step=128800, grad_norm=0.23401281237602234, loss=1.3915890455245972
I0210 23:24:08.770101 140529937405696 logging_writer.py:48] [128900] global_step=128900, grad_norm=0.24244911968708038, loss=1.487160325050354
I0210 23:24:44.897387 140529929012992 logging_writer.py:48] [129000] global_step=129000, grad_norm=0.23409835994243622, loss=1.5160950422286987
I0210 23:25:21.015378 140529937405696 logging_writer.py:48] [129100] global_step=129100, grad_norm=0.23082943260669708, loss=1.4396944046020508
I0210 23:25:57.111009 140529929012992 logging_writer.py:48] [129200] global_step=129200, grad_norm=0.24134178459644318, loss=1.4602656364440918
I0210 23:26:33.286698 140529937405696 logging_writer.py:48] [129300] global_step=129300, grad_norm=0.24428552389144897, loss=1.4874054193496704
I0210 23:27:09.442546 140529929012992 logging_writer.py:48] [129400] global_step=129400, grad_norm=0.2440033107995987, loss=1.4430466890335083
I0210 23:27:45.559059 140529937405696 logging_writer.py:48] [129500] global_step=129500, grad_norm=0.24280333518981934, loss=1.4073169231414795
I0210 23:28:21.674327 140529929012992 logging_writer.py:48] [129600] global_step=129600, grad_norm=0.22832636535167694, loss=1.4349764585494995
I0210 23:28:57.740632 140529937405696 logging_writer.py:48] [129700] global_step=129700, grad_norm=0.2310025691986084, loss=1.407983660697937
I0210 23:29:33.833094 140529929012992 logging_writer.py:48] [129800] global_step=129800, grad_norm=0.22829259932041168, loss=1.4069525003433228
I0210 23:30:09.940871 140529937405696 logging_writer.py:48] [129900] global_step=129900, grad_norm=0.2561780512332916, loss=1.4868354797363281
I0210 23:30:46.182642 140529929012992 logging_writer.py:48] [130000] global_step=130000, grad_norm=0.22807885706424713, loss=1.38620924949646
I0210 23:31:22.357113 140529937405696 logging_writer.py:48] [130100] global_step=130100, grad_norm=0.2316734790802002, loss=1.4316428899765015
I0210 23:31:58.439032 140529929012992 logging_writer.py:48] [130200] global_step=130200, grad_norm=0.23477201163768768, loss=1.4775843620300293
I0210 23:32:34.585413 140529937405696 logging_writer.py:48] [130300] global_step=130300, grad_norm=0.24381797015666962, loss=1.456944465637207
I0210 23:32:50.198000 140699726837568 spec.py:321] Evaluating on the training split.
I0210 23:32:53.229330 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 23:36:52.117629 140699726837568 spec.py:333] Evaluating on the validation split.
I0210 23:36:54.840446 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 23:39:30.597770 140699726837568 spec.py:349] Evaluating on the test split.
I0210 23:39:33.333400 140699726837568 workload.py:181] Translating evaluation dataset.
I0210 23:41:59.413138 140699726837568 submission_runner.py:408] Time since start: 79003.23s, 	Step: 130345, 	{'train/accuracy': 0.6950264573097229, 'train/loss': 1.3692277669906616, 'train/bleu': 35.27114728847377, 'validation/accuracy': 0.6961600184440613, 'validation/loss': 1.3616727590560913, 'validation/bleu': 30.8710946996457, 'validation/num_examples': 3000, 'test/accuracy': 0.7141711711883545, 'test/loss': 1.253180742263794, 'test/bleu': 31.45476105787029, 'test/num_examples': 3003, 'score': 47074.992399930954, 'total_duration': 79003.22927308083, 'accumulated_submission_time': 47074.992399930954, 'accumulated_eval_time': 31922.088319778442, 'accumulated_logging_time': 1.9932560920715332}
I0210 23:41:59.446133 140529929012992 logging_writer.py:48] [130345] accumulated_eval_time=31922.088320, accumulated_logging_time=1.993256, accumulated_submission_time=47074.992400, global_step=130345, preemption_count=0, score=47074.992400, test/accuracy=0.714171, test/bleu=31.454761, test/loss=1.253181, test/num_examples=3003, total_duration=79003.229273, train/accuracy=0.695026, train/bleu=35.271147, train/loss=1.369228, validation/accuracy=0.696160, validation/bleu=30.871095, validation/loss=1.361673, validation/num_examples=3000
I0210 23:42:19.558870 140529937405696 logging_writer.py:48] [130400] global_step=130400, grad_norm=0.24322500824928284, loss=1.4789894819259644
I0210 23:42:55.521043 140529929012992 logging_writer.py:48] [130500] global_step=130500, grad_norm=0.24173112213611603, loss=1.4266448020935059
I0210 23:43:31.552550 140529937405696 logging_writer.py:48] [130600] global_step=130600, grad_norm=0.236031174659729, loss=1.4720520973205566
I0210 23:44:07.654787 140529929012992 logging_writer.py:48] [130700] global_step=130700, grad_norm=0.2295823097229004, loss=1.4146989583969116
I0210 23:44:43.775619 140529937405696 logging_writer.py:48] [130800] global_step=130800, grad_norm=0.23693908751010895, loss=1.4714452028274536
I0210 23:45:19.878449 140529929012992 logging_writer.py:48] [130900] global_step=130900, grad_norm=0.23467189073562622, loss=1.493454098701477
I0210 23:45:55.977903 140529937405696 logging_writer.py:48] [131000] global_step=131000, grad_norm=0.23771363496780396, loss=1.4697612524032593
I0210 23:46:32.100380 140529929012992 logging_writer.py:48] [131100] global_step=131100, grad_norm=0.2421714961528778, loss=1.4891749620437622
I0210 23:47:08.224761 140529937405696 logging_writer.py:48] [131200] global_step=131200, grad_norm=0.23957043886184692, loss=1.480682373046875
I0210 23:47:44.279098 140529929012992 logging_writer.py:48] [131300] global_step=131300, grad_norm=0.23527014255523682, loss=1.466233253479004
I0210 23:48:20.349959 140529937405696 logging_writer.py:48] [131400] global_step=131400, grad_norm=0.24680541455745697, loss=1.4474046230316162
I0210 23:48:56.415115 140529929012992 logging_writer.py:48] [131500] global_step=131500, grad_norm=0.23637613654136658, loss=1.4013584852218628
I0210 23:49:32.570436 140529937405696 logging_writer.py:48] [131600] global_step=131600, grad_norm=0.23986321687698364, loss=1.5209277868270874
I0210 23:50:08.680619 140529929012992 logging_writer.py:48] [131700] global_step=131700, grad_norm=0.23983213305473328, loss=1.5144104957580566
I0210 23:50:44.784790 140529937405696 logging_writer.py:48] [131800] global_step=131800, grad_norm=0.23702795803546906, loss=1.457841157913208
I0210 23:51:20.931228 140529929012992 logging_writer.py:48] [131900] global_step=131900, grad_norm=0.22596336901187897, loss=1.3668668270111084
I0210 23:51:57.058872 140529937405696 logging_writer.py:48] [132000] global_step=132000, grad_norm=0.23845010995864868, loss=1.537095308303833
I0210 23:52:33.171675 140529929012992 logging_writer.py:48] [132100] global_step=132100, grad_norm=0.23549985885620117, loss=1.4376537799835205
I0210 23:53:09.307205 140529937405696 logging_writer.py:48] [132200] global_step=132200, grad_norm=0.23581907153129578, loss=1.4657154083251953
I0210 23:53:45.503575 140529929012992 logging_writer.py:48] [132300] global_step=132300, grad_norm=0.24014976620674133, loss=1.4988025426864624
I0210 23:54:21.673729 140529937405696 logging_writer.py:48] [132400] global_step=132400, grad_norm=0.2265782356262207, loss=1.4020329713821411
I0210 23:54:57.772592 140529929012992 logging_writer.py:48] [132500] global_step=132500, grad_norm=0.23306311666965485, loss=1.4541356563568115
I0210 23:55:33.925183 140529937405696 logging_writer.py:48] [132600] global_step=132600, grad_norm=0.23005065321922302, loss=1.4885047674179077
I0210 23:55:59.671639 140699726837568 spec.py:321] Evaluating on the training split.
I0210 23:56:02.724489 140699726837568 workload.py:181] Translating evaluation dataset.
I0211 00:00:21.877051 140699726837568 spec.py:333] Evaluating on the validation split.
I0211 00:00:24.591708 140699726837568 workload.py:181] Translating evaluation dataset.
I0211 00:02:59.251479 140699726837568 spec.py:349] Evaluating on the test split.
I0211 00:03:01.996864 140699726837568 workload.py:181] Translating evaluation dataset.
I0211 00:05:23.013092 140699726837568 submission_runner.py:408] Time since start: 80406.83s, 	Step: 132673, 	{'train/accuracy': 0.6964210271835327, 'train/loss': 1.3611470460891724, 'train/bleu': 35.60231110750975, 'validation/accuracy': 0.6960855722427368, 'validation/loss': 1.3613717555999756, 'validation/bleu': 30.815935514007432, 'validation/num_examples': 3000, 'test/accuracy': 0.7141944169998169, 'test/loss': 1.252968668937683, 'test/bleu': 31.444790126895835, 'test/num_examples': 3003, 'score': 47915.134707927704, 'total_duration': 80406.8292388916, 'accumulated_submission_time': 47915.134707927704, 'accumulated_eval_time': 32485.429721593857, 'accumulated_logging_time': 2.036381959915161}
I0211 00:05:23.047810 140529929012992 logging_writer.py:48] [132673] accumulated_eval_time=32485.429722, accumulated_logging_time=2.036382, accumulated_submission_time=47915.134708, global_step=132673, preemption_count=0, score=47915.134708, test/accuracy=0.714194, test/bleu=31.444790, test/loss=1.252969, test/num_examples=3003, total_duration=80406.829239, train/accuracy=0.696421, train/bleu=35.602311, train/loss=1.361147, validation/accuracy=0.696086, validation/bleu=30.815936, validation/loss=1.361372, validation/num_examples=3000
I0211 00:05:33.118662 140529937405696 logging_writer.py:48] [132700] global_step=132700, grad_norm=0.24284899234771729, loss=1.4458365440368652
I0211 00:06:09.092020 140529929012992 logging_writer.py:48] [132800] global_step=132800, grad_norm=0.22713148593902588, loss=1.5015695095062256
I0211 00:06:45.094719 140529937405696 logging_writer.py:48] [132900] global_step=132900, grad_norm=0.2394719272851944, loss=1.4632011651992798
I0211 00:07:21.162267 140529929012992 logging_writer.py:48] [133000] global_step=133000, grad_norm=0.23949171602725983, loss=1.4011075496673584
I0211 00:07:57.251341 140529937405696 logging_writer.py:48] [133100] global_step=133100, grad_norm=0.24515202641487122, loss=1.504763126373291
I0211 00:08:33.352806 140529929012992 logging_writer.py:48] [133200] global_step=133200, grad_norm=0.2266680896282196, loss=1.462960958480835
I0211 00:09:09.426805 140529937405696 logging_writer.py:48] [133300] global_step=133300, grad_norm=0.23105664551258087, loss=1.391717553138733
I0211 00:09:19.294629 140529929012992 logging_writer.py:48] [133329] global_step=133329, preemption_count=0, score=48151.323969
I0211 00:09:20.712755 140699726837568 checkpoints.py:490] Saving checkpoint at step: 133329
I0211 00:09:24.677510 140699726837568 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_1/wmt_jax/trial_5/checkpoint_133329
I0211 00:09:24.682465 140699726837568 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_1/wmt_jax/trial_5/checkpoint_133329.
I0211 00:09:24.757348 140699726837568 submission_runner.py:583] Tuning trial 5/5
I0211 00:09:24.757532 140699726837568 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.1, label_smoothing=0.0, learning_rate=0.0017486387539278373, one_minus_beta1=0.06733926164, beta2=0.9955159689799007, weight_decay=0.08121616522670176, warmup_factor=0.02)
I0211 00:09:24.763178 140699726837568 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0005905735306441784, 'train/loss': 11.173606872558594, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.208685874938965, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.19086742401123, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 31.33621072769165, 'total_duration': 907.8314986228943, 'accumulated_submission_time': 31.33621072769165, 'accumulated_eval_time': 876.4952499866486, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (2326, {'train/accuracy': 0.5102726817131042, 'train/loss': 2.8770992755889893, 'train/bleu': 22.340060181734597, 'validation/accuracy': 0.5084871649742126, 'validation/loss': 2.875525951385498, 'validation/bleu': 18.049709526813007, 'validation/num_examples': 3000, 'test/accuracy': 0.5088954567909241, 'test/loss': 2.922269582748413, 'test/bleu': 16.326466481940816, 'test/num_examples': 3003, 'score': 871.4278464317322, 'total_duration': 2251.8469796180725, 'accumulated_submission_time': 871.4278464317322, 'accumulated_eval_time': 1380.3259477615356, 'accumulated_logging_time': 0.02058124542236328, 'global_step': 2326, 'preemption_count': 0}), (4653, {'train/accuracy': 0.5776917338371277, 'train/loss': 2.2594664096832275, 'train/bleu': 27.71633074707988, 'validation/accuracy': 0.5888209342956543, 'validation/loss': 2.168548107147217, 'validation/bleu': 23.508677243219108, 'validation/num_examples': 3000, 'test/accuracy': 0.5914473533630371, 'test/loss': 2.134783983230591, 'test/bleu': 22.110663990581415, 'test/num_examples': 3003, 'score': 1711.6315701007843, 'total_duration': 3573.551340818405, 'accumulated_submission_time': 1711.6315701007843, 'accumulated_eval_time': 1861.7284874916077, 'accumulated_logging_time': 0.04536008834838867, 'global_step': 4653, 'preemption_count': 0}), (6981, {'train/accuracy': 0.6117618680000305, 'train/loss': 1.9756730794906616, 'train/bleu': 29.191234059510283, 'validation/accuracy': 0.6168429255485535, 'validation/loss': 1.9347010850906372, 'validation/bleu': 25.35601131488413, 'validation/num_examples': 3000, 'test/accuracy': 0.621997594833374, 'test/loss': 1.889957070350647, 'test/bleu': 24.20917412371607, 'test/num_examples': 3003, 'score': 2551.706431388855, 'total_duration': 4865.410325527191, 'accumulated_submission_time': 2551.706431388855, 'accumulated_eval_time': 2313.41427898407, 'accumulated_logging_time': 0.07245087623596191, 'global_step': 6981, 'preemption_count': 0}), (9308, {'train/accuracy': 0.6131592392921448, 'train/loss': 1.9403873682022095, 'train/bleu': 29.211597013206095, 'validation/accuracy': 0.6278781294822693, 'validation/loss': 1.825577735900879, 'validation/bleu': 26.295598101877676, 'validation/num_examples': 3000, 'test/accuracy': 0.6395677328109741, 'test/loss': 1.7598669528961182, 'test/bleu': 25.220842753067227, 'test/num_examples': 3003, 'score': 3391.9585361480713, 'total_duration': 6175.092213869095, 'accumulated_submission_time': 3391.9585361480713, 'accumulated_eval_time': 2782.7456068992615, 'accumulated_logging_time': 0.09807252883911133, 'global_step': 9308, 'preemption_count': 0}), (11635, {'train/accuracy': 0.6183011531829834, 'train/loss': 1.9032855033874512, 'train/bleu': 30.174969727349268, 'validation/accuracy': 0.637698233127594, 'validation/loss': 1.7577568292617798, 'validation/bleu': 26.727296923815302, 'validation/num_examples': 3000, 'test/accuracy': 0.6465632319450378, 'test/loss': 1.6927350759506226, 'test/bleu': 25.978056856540427, 'test/num_examples': 3003, 'score': 4232.144702672958, 'total_duration': 7490.7502863407135, 'accumulated_submission_time': 4232.144702672958, 'accumulated_eval_time': 3258.116788625717, 'accumulated_logging_time': 0.12381887435913086, 'global_step': 11635, 'preemption_count': 0}), (13963, {'train/accuracy': 0.6274576783180237, 'train/loss': 1.8290431499481201, 'train/bleu': 30.427990031021476, 'validation/accuracy': 0.6435381770133972, 'validation/loss': 1.7107415199279785, 'validation/bleu': 27.2194505792734, 'validation/num_examples': 3000, 'test/accuracy': 0.6531288027763367, 'test/loss': 1.6488524675369263, 'test/bleu': 26.288234040380576, 'test/num_examples': 3003, 'score': 5072.34353518486, 'total_duration': 8800.02228808403, 'accumulated_submission_time': 5072.34353518486, 'accumulated_eval_time': 3727.0896623134613, 'accumulated_logging_time': 0.15128850936889648, 'global_step': 13963, 'preemption_count': 0}), (16291, {'train/accuracy': 0.6281635761260986, 'train/loss': 1.8319660425186157, 'train/bleu': 30.128216497598615, 'validation/accuracy': 0.6473942995071411, 'validation/loss': 1.6852028369903564, 'validation/bleu': 27.31210332692129, 'validation/num_examples': 3000, 'test/accuracy': 0.6560688018798828, 'test/loss': 1.6182109117507935, 'test/bleu': 26.360974311764373, 'test/num_examples': 3003, 'score': 5912.543488740921, 'total_duration': 10180.52417087555, 'accumulated_submission_time': 5912.543488740921, 'accumulated_eval_time': 4267.290963888168, 'accumulated_logging_time': 0.17849230766296387, 'global_step': 16291, 'preemption_count': 0}), (18617, {'train/accuracy': 0.631950318813324, 'train/loss': 1.8058927059173584, 'train/bleu': 30.512143847900504, 'validation/accuracy': 0.6519696116447449, 'validation/loss': 1.6625659465789795, 'validation/bleu': 27.902452308781214, 'validation/num_examples': 3000, 'test/accuracy': 0.6614025831222534, 'test/loss': 1.5896934270858765, 'test/bleu': 27.088659970344388, 'test/num_examples': 3003, 'score': 6752.588777065277, 'total_duration': 11537.538368225098, 'accumulated_submission_time': 6752.588777065277, 'accumulated_eval_time': 4784.1533625125885, 'accumulated_logging_time': 0.2073345184326172, 'global_step': 18617, 'preemption_count': 0}), (20945, {'train/accuracy': 0.6327740550041199, 'train/loss': 1.7765021324157715, 'train/bleu': 30.94379672227982, 'validation/accuracy': 0.6536186933517456, 'validation/loss': 1.6433725357055664, 'validation/bleu': 27.590168338973967, 'validation/num_examples': 3000, 'test/accuracy': 0.6645168662071228, 'test/loss': 1.5687122344970703, 'test/bleu': 27.15118496076002, 'test/num_examples': 3003, 'score': 7592.666503190994, 'total_duration': 12869.502164840698, 'accumulated_submission_time': 7592.666503190994, 'accumulated_eval_time': 5275.937527179718, 'accumulated_logging_time': 0.2346513271331787, 'global_step': 20945, 'preemption_count': 0}), (23273, {'train/accuracy': 0.6339693069458008, 'train/loss': 1.7776130437850952, 'train/bleu': 30.655361633286923, 'validation/accuracy': 0.6550569534301758, 'validation/loss': 1.6391043663024902, 'validation/bleu': 27.783212559536747, 'validation/num_examples': 3000, 'test/accuracy': 0.6667364239692688, 'test/loss': 1.5609209537506104, 'test/bleu': 27.47751462370135, 'test/num_examples': 3003, 'score': 8432.74540233612, 'total_duration': 14206.423070907593, 'accumulated_submission_time': 8432.74540233612, 'accumulated_eval_time': 5772.678349494934, 'accumulated_logging_time': 0.2628781795501709, 'global_step': 23273, 'preemption_count': 0}), (25600, {'train/accuracy': 0.6479560136795044, 'train/loss': 1.6769108772277832, 'train/bleu': 31.84595571310796, 'validation/accuracy': 0.6565200686454773, 'validation/loss': 1.6232635974884033, 'validation/bleu': 27.658885874545486, 'validation/num_examples': 3000, 'test/accuracy': 0.6655627489089966, 'test/loss': 1.5492188930511475, 'test/bleu': 27.11589523195827, 'test/num_examples': 3003, 'score': 9272.903683662415, 'total_duration': 15501.935836553574, 'accumulated_submission_time': 9272.903683662415, 'accumulated_eval_time': 6227.92778301239, 'accumulated_logging_time': 0.29166626930236816, 'global_step': 25600, 'preemption_count': 0}), (27927, {'train/accuracy': 0.6404833793640137, 'train/loss': 1.7259860038757324, 'train/bleu': 31.049956913695816, 'validation/accuracy': 0.6574624180793762, 'validation/loss': 1.6162623167037964, 'validation/bleu': 27.821747606245758, 'validation/num_examples': 3000, 'test/accuracy': 0.6682470440864563, 'test/loss': 1.5407686233520508, 'test/bleu': 27.582243730159053, 'test/num_examples': 3003, 'score': 10112.873045921326, 'total_duration': 17107.914113998413, 'accumulated_submission_time': 10112.873045921326, 'accumulated_eval_time': 6993.833231925964, 'accumulated_logging_time': 0.3207814693450928, 'global_step': 27927, 'preemption_count': 0}), (30255, {'train/accuracy': 0.636620819568634, 'train/loss': 1.76451575756073, 'train/bleu': 31.611790041850664, 'validation/accuracy': 0.6559621095657349, 'validation/loss': 1.605042576789856, 'validation/bleu': 27.67252673218362, 'validation/num_examples': 3000, 'test/accuracy': 0.6700947284698486, 'test/loss': 1.525154709815979, 'test/bleu': 27.54839713124831, 'test/num_examples': 3003, 'score': 10952.896437168121, 'total_duration': 18488.50726699829, 'accumulated_submission_time': 10952.896437168121, 'accumulated_eval_time': 7534.302425146103, 'accumulated_logging_time': 0.34938669204711914, 'global_step': 30255, 'preemption_count': 0}), (32583, {'train/accuracy': 0.6440646648406982, 'train/loss': 1.7038763761520386, 'train/bleu': 31.360567916844502, 'validation/accuracy': 0.6583923101425171, 'validation/loss': 1.5940138101577759, 'validation/bleu': 27.90004436424476, 'validation/num_examples': 3000, 'test/accuracy': 0.6725466251373291, 'test/loss': 1.5128222703933716, 'test/bleu': 27.34830812687249, 'test/num_examples': 3003, 'score': 11793.089814901352, 'total_duration': 19861.800530195236, 'accumulated_submission_time': 11793.089814901352, 'accumulated_eval_time': 8067.295173883438, 'accumulated_logging_time': 0.3822178840637207, 'global_step': 32583, 'preemption_count': 0}), (34910, {'train/accuracy': 0.6403083801269531, 'train/loss': 1.734409213066101, 'train/bleu': 31.28614280491014, 'validation/accuracy': 0.6616036891937256, 'validation/loss': 1.5844405889511108, 'validation/bleu': 28.197664218772356, 'validation/num_examples': 3000, 'test/accuracy': 0.6729998588562012, 'test/loss': 1.5030328035354614, 'test/bleu': 27.251809248166786, 'test/num_examples': 3003, 'score': 12633.118577480316, 'total_duration': 21188.203240394592, 'accumulated_submission_time': 12633.118577480316, 'accumulated_eval_time': 8553.564245939255, 'accumulated_logging_time': 0.4125397205352783, 'global_step': 34910, 'preemption_count': 0}), (37238, {'train/accuracy': 0.6411272883415222, 'train/loss': 1.7296642065048218, 'train/bleu': 31.164002746131025, 'validation/accuracy': 0.6619756817817688, 'validation/loss': 1.5773353576660156, 'validation/bleu': 28.1188025528331, 'validation/num_examples': 3000, 'test/accuracy': 0.6725582480430603, 'test/loss': 1.502700686454773, 'test/bleu': 27.285918920544557, 'test/num_examples': 3003, 'score': 13473.14170718193, 'total_duration': 22550.16905093193, 'accumulated_submission_time': 13473.14170718193, 'accumulated_eval_time': 9075.40495300293, 'accumulated_logging_time': 0.44240593910217285, 'global_step': 37238, 'preemption_count': 0}), (39565, {'train/accuracy': 0.6465423107147217, 'train/loss': 1.6878236532211304, 'train/bleu': 31.692694397409078, 'validation/accuracy': 0.6620872616767883, 'validation/loss': 1.5737826824188232, 'validation/bleu': 28.22098356404612, 'validation/num_examples': 3000, 'test/accuracy': 0.6753936409950256, 'test/loss': 1.4936915636062622, 'test/bleu': 27.86312619079604, 'test/num_examples': 3003, 'score': 14313.19044804573, 'total_duration': 23933.09076309204, 'accumulated_submission_time': 14313.19044804573, 'accumulated_eval_time': 9618.169555902481, 'accumulated_logging_time': 0.4737052917480469, 'global_step': 39565, 'preemption_count': 0}), (41893, {'train/accuracy': 0.6447368264198303, 'train/loss': 1.7097009420394897, 'train/bleu': 31.56412929199626, 'validation/accuracy': 0.6631907820701599, 'validation/loss': 1.5640273094177246, 'validation/bleu': 28.26689393593714, 'validation/num_examples': 3000, 'test/accuracy': 0.6745802164077759, 'test/loss': 1.4858415126800537, 'test/bleu': 28.026926555958124, 'test/num_examples': 3003, 'score': 15153.36160159111, 'total_duration': 25320.262265205383, 'accumulated_submission_time': 15153.36160159111, 'accumulated_eval_time': 10165.055995941162, 'accumulated_logging_time': 0.5107996463775635, 'global_step': 41893, 'preemption_count': 0}), (44221, {'train/accuracy': 0.6637552380561829, 'train/loss': 1.5781522989273071, 'train/bleu': 32.93034087770076, 'validation/accuracy': 0.6653606295585632, 'validation/loss': 1.5589519739151, 'validation/bleu': 28.47223571166506, 'validation/num_examples': 3000, 'test/accuracy': 0.6778572201728821, 'test/loss': 1.4742341041564941, 'test/bleu': 28.249781697948197, 'test/num_examples': 3003, 'score': 15993.378037929535, 'total_duration': 26729.05960392952, 'accumulated_submission_time': 15993.378037929535, 'accumulated_eval_time': 10733.730982542038, 'accumulated_logging_time': 0.5428597927093506, 'global_step': 44221, 'preemption_count': 0}), (46549, {'train/accuracy': 0.6493027806282043, 'train/loss': 1.6772871017456055, 'train/bleu': 31.197649864945813, 'validation/accuracy': 0.6661293506622314, 'validation/loss': 1.5487326383590698, 'validation/bleu': 28.404300656637634, 'validation/num_examples': 3000, 'test/accuracy': 0.6787520051002502, 'test/loss': 1.4658998250961304, 'test/bleu': 28.08245586929687, 'test/num_examples': 3003, 'score': 16833.506383895874, 'total_duration': 28191.62198972702, 'accumulated_submission_time': 16833.506383895874, 'accumulated_eval_time': 11356.05702495575, 'accumulated_logging_time': 0.5749258995056152, 'global_step': 46549, 'preemption_count': 0}), (48877, {'train/accuracy': 0.6498969793319702, 'train/loss': 1.671565294265747, 'train/bleu': 31.67379922402138, 'validation/accuracy': 0.6682496070861816, 'validation/loss': 1.5388306379318237, 'validation/bleu': 28.527888487046358, 'validation/num_examples': 3000, 'test/accuracy': 0.6791703104972839, 'test/loss': 1.4580789804458618, 'test/bleu': 28.135240299714553, 'test/num_examples': 3003, 'score': 17673.46990466118, 'total_duration': 29582.065421819687, 'accumulated_submission_time': 17673.46990466118, 'accumulated_eval_time': 11906.429987430573, 'accumulated_logging_time': 0.6067063808441162, 'global_step': 48877, 'preemption_count': 0}), (51205, {'train/accuracy': 0.6544747352600098, 'train/loss': 1.6260355710983276, 'train/bleu': 32.19056354138812, 'validation/accuracy': 0.6689439415931702, 'validation/loss': 1.538604497909546, 'validation/bleu': 29.092913139909744, 'validation/num_examples': 3000, 'test/accuracy': 0.6821568012237549, 'test/loss': 1.45171320438385, 'test/bleu': 28.69368597124545, 'test/num_examples': 3003, 'score': 18513.543728113174, 'total_duration': 30979.550669908524, 'accumulated_submission_time': 18513.543728113174, 'accumulated_eval_time': 12463.738171815872, 'accumulated_logging_time': 0.638897180557251, 'global_step': 51205, 'preemption_count': 0}), (53533, {'train/accuracy': 0.6519391536712646, 'train/loss': 1.6503171920776367, 'train/bleu': 32.23124119536476, 'validation/accuracy': 0.6698243021965027, 'validation/loss': 1.5270503759384155, 'validation/bleu': 28.783845775866695, 'validation/num_examples': 3000, 'test/accuracy': 0.6848992109298706, 'test/loss': 1.4424127340316772, 'test/bleu': 28.1405950398642, 'test/num_examples': 3003, 'score': 19353.507332086563, 'total_duration': 32383.502128124237, 'accumulated_submission_time': 19353.507332086563, 'accumulated_eval_time': 13027.610567808151, 'accumulated_logging_time': 0.6803698539733887, 'global_step': 53533, 'preemption_count': 0}), (55860, {'train/accuracy': 0.6507437229156494, 'train/loss': 1.672051191329956, 'train/bleu': 32.41903297717362, 'validation/accuracy': 0.6705806255340576, 'validation/loss': 1.521011233329773, 'validation/bleu': 29.09612197316751, 'validation/num_examples': 3000, 'test/accuracy': 0.68580561876297, 'test/loss': 1.4303960800170898, 'test/bleu': 29.12164847468511, 'test/num_examples': 3003, 'score': 20193.692486524582, 'total_duration': 33800.267602682114, 'accumulated_submission_time': 20193.692486524582, 'accumulated_eval_time': 13604.082396030426, 'accumulated_logging_time': 0.7132043838500977, 'global_step': 55860, 'preemption_count': 0}), (58188, {'train/accuracy': 0.6537730693817139, 'train/loss': 1.6372880935668945, 'train/bleu': 32.227818247947184, 'validation/accuracy': 0.6702582836151123, 'validation/loss': 1.5269571542739868, 'validation/bleu': 28.863251676095942, 'validation/num_examples': 3000, 'test/accuracy': 0.6871768236160278, 'test/loss': 1.433237910270691, 'test/bleu': 28.67539179023136, 'test/num_examples': 3003, 'score': 21033.878321647644, 'total_duration': 35274.847340106964, 'accumulated_submission_time': 21033.878321647644, 'accumulated_eval_time': 14238.36943602562, 'accumulated_logging_time': 0.7463061809539795, 'global_step': 58188, 'preemption_count': 0}), (60515, {'train/accuracy': 0.6578980684280396, 'train/loss': 1.6208689212799072, 'train/bleu': 32.07658106977582, 'validation/accuracy': 0.6717089414596558, 'validation/loss': 1.5151821374893188, 'validation/bleu': 28.92682264501012, 'validation/num_examples': 3000, 'test/accuracy': 0.6871535778045654, 'test/loss': 1.4229494333267212, 'test/bleu': 29.073916460981607, 'test/num_examples': 3003, 'score': 21873.81586742401, 'total_duration': 36837.5823905468, 'accumulated_submission_time': 21873.81586742401, 'accumulated_eval_time': 14961.058718919754, 'accumulated_logging_time': 0.7788941860198975, 'global_step': 60515, 'preemption_count': 0}), (62841, {'train/accuracy': 0.6713626980781555, 'train/loss': 1.508202075958252, 'train/bleu': 33.69910168169331, 'validation/accuracy': 0.674411952495575, 'validation/loss': 1.5051673650741577, 'validation/bleu': 29.20011186816427, 'validation/num_examples': 3000, 'test/accuracy': 0.6859566569328308, 'test/loss': 1.4162017107009888, 'test/bleu': 28.6389104597473, 'test/num_examples': 3003, 'score': 22713.780904769897, 'total_duration': 38213.50196003914, 'accumulated_submission_time': 22713.780904769897, 'accumulated_eval_time': 15496.90365743637, 'accumulated_logging_time': 0.8135547637939453, 'global_step': 62841, 'preemption_count': 0}), (65168, {'train/accuracy': 0.65871661901474, 'train/loss': 1.6069515943527222, 'train/bleu': 32.66574289741518, 'validation/accuracy': 0.6746847629547119, 'validation/loss': 1.4960321187973022, 'validation/bleu': 29.051769116288494, 'validation/num_examples': 3000, 'test/accuracy': 0.6899308562278748, 'test/loss': 1.4054077863693237, 'test/bleu': 28.924297355169422, 'test/num_examples': 3003, 'score': 23553.788153648376, 'total_duration': 39609.90386343002, 'accumulated_submission_time': 23553.788153648376, 'accumulated_eval_time': 16053.180361270905, 'accumulated_logging_time': 0.8553059101104736, 'global_step': 65168, 'preemption_count': 0}), (67496, {'train/accuracy': 0.6580450534820557, 'train/loss': 1.6125664710998535, 'train/bleu': 32.3845854245405, 'validation/accuracy': 0.6748707294464111, 'validation/loss': 1.4918677806854248, 'validation/bleu': 29.354178552155997, 'validation/num_examples': 3000, 'test/accuracy': 0.689059317111969, 'test/loss': 1.4040769338607788, 'test/bleu': 29.383744057926585, 'test/num_examples': 3003, 'score': 24393.980915784836, 'total_duration': 40972.572177410126, 'accumulated_submission_time': 24393.980915784836, 'accumulated_eval_time': 16575.54416203499, 'accumulated_logging_time': 0.8911569118499756, 'global_step': 67496, 'preemption_count': 0}), (69823, {'train/accuracy': 0.6646310091018677, 'train/loss': 1.5558394193649292, 'train/bleu': 33.108476509492625, 'validation/accuracy': 0.6774001717567444, 'validation/loss': 1.4817792177200317, 'validation/bleu': 29.732924813863225, 'validation/num_examples': 3000, 'test/accuracy': 0.6909418702125549, 'test/loss': 1.3975272178649902, 'test/bleu': 29.22117196182941, 'test/num_examples': 3003, 'score': 25234.14297890663, 'total_duration': 42412.92000794411, 'accumulated_submission_time': 25234.14297890663, 'accumulated_eval_time': 17175.61923766136, 'accumulated_logging_time': 0.9281957149505615, 'global_step': 69823, 'preemption_count': 0}), (72151, {'train/accuracy': 0.6619483828544617, 'train/loss': 1.584561824798584, 'train/bleu': 32.684329463700614, 'validation/accuracy': 0.676978588104248, 'validation/loss': 1.4765617847442627, 'validation/bleu': 29.445052111202394, 'validation/num_examples': 3000, 'test/accuracy': 0.6935332417488098, 'test/loss': 1.3811521530151367, 'test/bleu': 29.335350984322613, 'test/num_examples': 3003, 'score': 26074.134548187256, 'total_duration': 43794.25180768967, 'accumulated_submission_time': 26074.134548187256, 'accumulated_eval_time': 17716.850410461426, 'accumulated_logging_time': 0.9643950462341309, 'global_step': 72151, 'preemption_count': 0}), (74478, {'train/accuracy': 0.6594235301017761, 'train/loss': 1.5999988317489624, 'train/bleu': 32.67356970090248, 'validation/accuracy': 0.6777969002723694, 'validation/loss': 1.4699158668518066, 'validation/bleu': 29.571455647456066, 'validation/num_examples': 3000, 'test/accuracy': 0.69459068775177, 'test/loss': 1.3774783611297607, 'test/bleu': 29.20670844457939, 'test/num_examples': 3003, 'score': 26914.059263944626, 'total_duration': 45132.13513803482, 'accumulated_submission_time': 26914.059263944626, 'accumulated_eval_time': 18214.695971250534, 'accumulated_logging_time': 1.0013093948364258, 'global_step': 74478, 'preemption_count': 0}), (76806, {'train/accuracy': 0.6694381833076477, 'train/loss': 1.5369466543197632, 'train/bleu': 33.0531892644505, 'validation/accuracy': 0.6802147626876831, 'validation/loss': 1.4623948335647583, 'validation/bleu': 29.47381621833204, 'validation/num_examples': 3000, 'test/accuracy': 0.6945674419403076, 'test/loss': 1.3712241649627686, 'test/bleu': 29.462568269990985, 'test/num_examples': 3003, 'score': 27754.242109537125, 'total_duration': 46509.57319569588, 'accumulated_submission_time': 27754.242109537125, 'accumulated_eval_time': 18751.840750455856, 'accumulated_logging_time': 1.0371296405792236, 'global_step': 76806, 'preemption_count': 0}), (79133, {'train/accuracy': 0.6636430025100708, 'train/loss': 1.570115566253662, 'train/bleu': 32.54973127642245, 'validation/accuracy': 0.6804007291793823, 'validation/loss': 1.45924973487854, 'validation/bleu': 29.552745828566785, 'validation/num_examples': 3000, 'test/accuracy': 0.6958921551704407, 'test/loss': 1.3631484508514404, 'test/bleu': 29.431835227142336, 'test/num_examples': 3003, 'score': 28594.37763762474, 'total_duration': 47918.92470264435, 'accumulated_submission_time': 28594.37763762474, 'accumulated_eval_time': 19320.94375896454, 'accumulated_logging_time': 1.0745155811309814, 'global_step': 79133, 'preemption_count': 0}), (81461, {'train/accuracy': 0.6872650384902954, 'train/loss': 1.421802282333374, 'train/bleu': 34.87201890341066, 'validation/accuracy': 0.6823846101760864, 'validation/loss': 1.4490602016448975, 'validation/bleu': 30.02667336732894, 'validation/num_examples': 3000, 'test/accuracy': 0.6979489922523499, 'test/loss': 1.3512593507766724, 'test/bleu': 29.82934247972239, 'test/num_examples': 3003, 'score': 29434.364958763123, 'total_duration': 49268.42466902733, 'accumulated_submission_time': 29434.364958763123, 'accumulated_eval_time': 19830.34655547142, 'accumulated_logging_time': 1.111635684967041, 'global_step': 81461, 'preemption_count': 0}), (83788, {'train/accuracy': 0.6727224588394165, 'train/loss': 1.5115309953689575, 'train/bleu': 33.53385947656073, 'validation/accuracy': 0.6825209856033325, 'validation/loss': 1.441343903541565, 'validation/bleu': 29.710900837467445, 'validation/num_examples': 3000, 'test/accuracy': 0.6982162594795227, 'test/loss': 1.3485827445983887, 'test/bleu': 29.646421542412785, 'test/num_examples': 3003, 'score': 30274.487243413925, 'total_duration': 50868.4852745533, 'accumulated_submission_time': 30274.487243413925, 'accumulated_eval_time': 20590.173165798187, 'accumulated_logging_time': 1.1480538845062256, 'global_step': 83788, 'preemption_count': 0}), (86116, {'train/accuracy': 0.6685099005699158, 'train/loss': 1.5362203121185303, 'train/bleu': 33.509164043830715, 'validation/accuracy': 0.6834757328033447, 'validation/loss': 1.4383317232131958, 'validation/bleu': 30.021007104354624, 'validation/num_examples': 3000, 'test/accuracy': 0.7002963423728943, 'test/loss': 1.3392986059188843, 'test/bleu': 30.026394171826187, 'test/num_examples': 3003, 'score': 31114.393693447113, 'total_duration': 52270.26392364502, 'accumulated_submission_time': 31114.393693447113, 'accumulated_eval_time': 21151.934986829758, 'accumulated_logging_time': 1.1864585876464844, 'global_step': 86116, 'preemption_count': 0}), (88444, {'train/accuracy': 0.6788003444671631, 'train/loss': 1.4714220762252808, 'train/bleu': 33.85755785036112, 'validation/accuracy': 0.6860795021057129, 'validation/loss': 1.4285391569137573, 'validation/bleu': 30.326511463742502, 'validation/num_examples': 3000, 'test/accuracy': 0.7007378935813904, 'test/loss': 1.3346697092056274, 'test/bleu': 30.07429817068781, 'test/num_examples': 3003, 'score': 31954.47871041298, 'total_duration': 53697.333899497986, 'accumulated_submission_time': 31954.47871041298, 'accumulated_eval_time': 21738.805801153183, 'accumulated_logging_time': 1.2259869575500488, 'global_step': 88444, 'preemption_count': 0}), (90772, {'train/accuracy': 0.6721503138542175, 'train/loss': 1.5121186971664429, 'train/bleu': 33.66060615621202, 'validation/accuracy': 0.6863027215003967, 'validation/loss': 1.4202866554260254, 'validation/bleu': 30.17695861596039, 'validation/num_examples': 3000, 'test/accuracy': 0.7035732865333557, 'test/loss': 1.3192323446273804, 'test/bleu': 30.333156321061402, 'test/num_examples': 3003, 'score': 32794.429097890854, 'total_duration': 55212.970027923584, 'accumulated_submission_time': 32794.429097890854, 'accumulated_eval_time': 22414.37882566452, 'accumulated_logging_time': 1.265702247619629, 'global_step': 90772, 'preemption_count': 0}), (93100, {'train/accuracy': 0.6721413135528564, 'train/loss': 1.5171512365341187, 'train/bleu': 33.86056555520624, 'validation/accuracy': 0.6868358850479126, 'validation/loss': 1.417452335357666, 'validation/bleu': 30.403902679657968, 'validation/num_examples': 3000, 'test/accuracy': 0.7037127614021301, 'test/loss': 1.315152645111084, 'test/bleu': 30.392667213925773, 'test/num_examples': 3003, 'score': 33634.516040802, 'total_duration': 56644.627078294754, 'accumulated_submission_time': 33634.516040802, 'accumulated_eval_time': 23005.836725473404, 'accumulated_logging_time': 1.3031470775604248, 'global_step': 93100, 'preemption_count': 0}), (95428, {'train/accuracy': 0.6813426613807678, 'train/loss': 1.451947808265686, 'train/bleu': 34.027032781976644, 'validation/accuracy': 0.6877037882804871, 'validation/loss': 1.4141026735305786, 'validation/bleu': 30.14882976469225, 'validation/num_examples': 3000, 'test/accuracy': 0.7046656608581543, 'test/loss': 1.3117072582244873, 'test/bleu': 30.250803153996866, 'test/num_examples': 3003, 'score': 34474.600940704346, 'total_duration': 57972.71556472778, 'accumulated_submission_time': 34474.600940704346, 'accumulated_eval_time': 23493.724118709564, 'accumulated_logging_time': 1.3443577289581299, 'global_step': 95428, 'preemption_count': 0}), (97755, {'train/accuracy': 0.6785141825675964, 'train/loss': 1.4695278406143188, 'train/bleu': 34.25009675294788, 'validation/accuracy': 0.6903820037841797, 'validation/loss': 1.4033619165420532, 'validation/bleu': 30.519843494061867, 'validation/num_examples': 3000, 'test/accuracy': 0.7072570323944092, 'test/loss': 1.2985440492630005, 'test/bleu': 30.597164472048394, 'test/num_examples': 3003, 'score': 35314.54950070381, 'total_duration': 59400.434348106384, 'accumulated_submission_time': 35314.54950070381, 'accumulated_eval_time': 24081.374705553055, 'accumulated_logging_time': 1.387169599533081, 'global_step': 97755, 'preemption_count': 0}), (100083, {'train/accuracy': 0.6936557292938232, 'train/loss': 1.3875911235809326, 'train/bleu': 35.65112004922339, 'validation/accuracy': 0.6906548142433167, 'validation/loss': 1.400636911392212, 'validation/bleu': 30.85509335453766, 'validation/num_examples': 3000, 'test/accuracy': 0.706187903881073, 'test/loss': 1.299963116645813, 'test/bleu': 30.344433313070358, 'test/num_examples': 3003, 'score': 36154.70352482796, 'total_duration': 60778.641570568085, 'accumulated_submission_time': 36154.70352482796, 'accumulated_eval_time': 24619.3114862442, 'accumulated_logging_time': 1.4285976886749268, 'global_step': 100083, 'preemption_count': 0}), (102411, {'train/accuracy': 0.6837599873542786, 'train/loss': 1.4399648904800415, 'train/bleu': 34.42078439556147, 'validation/accuracy': 0.691696286201477, 'validation/loss': 1.3963810205459595, 'validation/bleu': 30.65930224150146, 'validation/num_examples': 3000, 'test/accuracy': 0.7067921757698059, 'test/loss': 1.2964038848876953, 'test/bleu': 30.490371241411076, 'test/num_examples': 3003, 'score': 36994.592054605484, 'total_duration': 62245.283281087875, 'accumulated_submission_time': 36994.592054605484, 'accumulated_eval_time': 25245.94492340088, 'accumulated_logging_time': 1.4772801399230957, 'global_step': 102411, 'preemption_count': 0}), (104739, {'train/accuracy': 0.6803861260414124, 'train/loss': 1.4522088766098022, 'train/bleu': 34.19161231852919, 'validation/accuracy': 0.6913491487503052, 'validation/loss': 1.3914282321929932, 'validation/bleu': 30.690073446109658, 'validation/num_examples': 3000, 'test/accuracy': 0.7083609700202942, 'test/loss': 1.2872958183288574, 'test/bleu': 30.748461460186437, 'test/num_examples': 3003, 'score': 37834.66395068169, 'total_duration': 63622.94854474068, 'accumulated_submission_time': 37834.66395068169, 'accumulated_eval_time': 25783.427124261856, 'accumulated_logging_time': 1.5160844326019287, 'global_step': 104739, 'preemption_count': 0}), (107068, {'train/accuracy': 0.691776692867279, 'train/loss': 1.3914330005645752, 'train/bleu': 34.81961982311779, 'validation/accuracy': 0.6922914981842041, 'validation/loss': 1.3859933614730835, 'validation/bleu': 30.583219009274575, 'validation/num_examples': 3000, 'test/accuracy': 0.708570122718811, 'test/loss': 1.2839752435684204, 'test/bleu': 30.670776102657882, 'test/num_examples': 3003, 'score': 38674.80152177811, 'total_duration': 65018.60505485535, 'accumulated_submission_time': 38674.80152177811, 'accumulated_eval_time': 26338.833918571472, 'accumulated_logging_time': 1.5553884506225586, 'global_step': 107068, 'preemption_count': 0}), (109397, {'train/accuracy': 0.6863054037094116, 'train/loss': 1.4225127696990967, 'train/bleu': 34.77338731179625, 'validation/accuracy': 0.692328691482544, 'validation/loss': 1.3812284469604492, 'validation/bleu': 30.934935314671602, 'validation/num_examples': 3000, 'test/accuracy': 0.7111498713493347, 'test/loss': 1.274350881576538, 'test/bleu': 31.09661239196818, 'test/num_examples': 3003, 'score': 39515.00077295303, 'total_duration': 66434.0912425518, 'accumulated_submission_time': 39515.00077295303, 'accumulated_eval_time': 26914.005674123764, 'accumulated_logging_time': 1.5969746112823486, 'global_step': 109397, 'preemption_count': 0}), (111725, {'train/accuracy': 0.6838766932487488, 'train/loss': 1.4372414350509644, 'train/bleu': 34.7839582194981, 'validation/accuracy': 0.6921178698539734, 'validation/loss': 1.376046061515808, 'validation/bleu': 30.739287863649857, 'validation/num_examples': 3000, 'test/accuracy': 0.7117308974266052, 'test/loss': 1.2688062191009521, 'test/bleu': 31.122962263411495, 'test/num_examples': 3003, 'score': 40354.97988796234, 'total_duration': 67818.93628644943, 'accumulated_submission_time': 40354.97988796234, 'accumulated_eval_time': 27458.749623537064, 'accumulated_logging_time': 1.6440293788909912, 'global_step': 111725, 'preemption_count': 0}), (114053, {'train/accuracy': 0.6951395273208618, 'train/loss': 1.3722827434539795, 'train/bleu': 35.48070667530032, 'validation/accuracy': 0.6938289403915405, 'validation/loss': 1.3752260208129883, 'validation/bleu': 30.685051686956182, 'validation/num_examples': 3000, 'test/accuracy': 0.7118470668792725, 'test/loss': 1.2679171562194824, 'test/bleu': 31.035798009097697, 'test/num_examples': 3003, 'score': 41195.22360944748, 'total_duration': 69239.3367304802, 'accumulated_submission_time': 41195.22360944748, 'accumulated_eval_time': 28038.789351701736, 'accumulated_logging_time': 1.6860826015472412, 'global_step': 114053, 'preemption_count': 0}), (116381, {'train/accuracy': 0.6891108155250549, 'train/loss': 1.4094853401184082, 'train/bleu': 35.24992178839545, 'validation/accuracy': 0.6954160332679749, 'validation/loss': 1.3703862428665161, 'validation/bleu': 30.92770434969378, 'validation/num_examples': 3000, 'test/accuracy': 0.713706374168396, 'test/loss': 1.2614657878875732, 'test/bleu': 31.349120365228714, 'test/num_examples': 3003, 'score': 42035.293724536896, 'total_duration': 70656.65222883224, 'accumulated_submission_time': 42035.293724536896, 'accumulated_eval_time': 28615.922476530075, 'accumulated_logging_time': 1.726020097732544, 'global_step': 116381, 'preemption_count': 0}), (118709, {'train/accuracy': 0.6923836469650269, 'train/loss': 1.3908350467681885, 'train/bleu': 34.80738047912399, 'validation/accuracy': 0.6954408288002014, 'validation/loss': 1.3662816286087036, 'validation/bleu': 31.010793828916093, 'validation/num_examples': 3000, 'test/accuracy': 0.7125559449195862, 'test/loss': 1.260074496269226, 'test/bleu': 31.390352867613373, 'test/num_examples': 3003, 'score': 42875.2884888649, 'total_duration': 72050.43827652931, 'accumulated_submission_time': 42875.2884888649, 'accumulated_eval_time': 29169.59684228897, 'accumulated_logging_time': 1.7702577114105225, 'global_step': 118709, 'preemption_count': 0}), (121036, {'train/accuracy': 0.6926527619361877, 'train/loss': 1.3857016563415527, 'train/bleu': 35.392830009300184, 'validation/accuracy': 0.6950812935829163, 'validation/loss': 1.3645596504211426, 'validation/bleu': 30.888696962120584, 'validation/num_examples': 3000, 'test/accuracy': 0.7132415771484375, 'test/loss': 1.257387399673462, 'test/bleu': 31.336217699826122, 'test/num_examples': 3003, 'score': 43715.2911632061, 'total_duration': 73417.11321353912, 'accumulated_submission_time': 43715.2911632061, 'accumulated_eval_time': 29696.142201185226, 'accumulated_logging_time': 1.8214778900146484, 'global_step': 121036, 'preemption_count': 0}), (123364, {'train/accuracy': 0.6927144527435303, 'train/loss': 1.3827232122421265, 'train/bleu': 35.22963795099508, 'validation/accuracy': 0.6956640481948853, 'validation/loss': 1.36439049243927, 'validation/bleu': 30.82173282251918, 'validation/num_examples': 3000, 'test/accuracy': 0.7135552763938904, 'test/loss': 1.2557114362716675, 'test/bleu': 31.546370535616788, 'test/num_examples': 3003, 'score': 44555.2459564209, 'total_duration': 74803.37408471107, 'accumulated_submission_time': 44555.2459564209, 'accumulated_eval_time': 30242.333233594894, 'accumulated_logging_time': 1.863266944885254, 'global_step': 123364, 'preemption_count': 0}), (125691, {'train/accuracy': 0.6953275799751282, 'train/loss': 1.366080403327942, 'train/bleu': 35.52793475734423, 'validation/accuracy': 0.6961103677749634, 'validation/loss': 1.3615988492965698, 'validation/bleu': 30.92001661383089, 'validation/num_examples': 3000, 'test/accuracy': 0.7137877345085144, 'test/loss': 1.2536863088607788, 'test/bleu': 31.459191760161236, 'test/num_examples': 3003, 'score': 45395.13310265541, 'total_duration': 76220.00295948982, 'accumulated_submission_time': 45395.13310265541, 'accumulated_eval_time': 30818.95568537712, 'accumulated_logging_time': 1.906174659729004, 'global_step': 125691, 'preemption_count': 0}), (128018, {'train/accuracy': 0.6966461539268494, 'train/loss': 1.3577289581298828, 'train/bleu': 35.648902859429185, 'validation/accuracy': 0.6961227655410767, 'validation/loss': 1.362878441810608, 'validation/bleu': 30.908921117943162, 'validation/num_examples': 3000, 'test/accuracy': 0.7140550017356873, 'test/loss': 1.2544089555740356, 'test/bleu': 31.35299748207434, 'test/num_examples': 3003, 'score': 46235.07247853279, 'total_duration': 77613.97647738457, 'accumulated_submission_time': 46235.07247853279, 'accumulated_eval_time': 31372.873242139816, 'accumulated_logging_time': 1.9501900672912598, 'global_step': 128018, 'preemption_count': 0}), (130345, {'train/accuracy': 0.6950264573097229, 'train/loss': 1.3692277669906616, 'train/bleu': 35.27114728847377, 'validation/accuracy': 0.6961600184440613, 'validation/loss': 1.3616727590560913, 'validation/bleu': 30.8710946996457, 'validation/num_examples': 3000, 'test/accuracy': 0.7141711711883545, 'test/loss': 1.253180742263794, 'test/bleu': 31.45476105787029, 'test/num_examples': 3003, 'score': 47074.992399930954, 'total_duration': 79003.22927308083, 'accumulated_submission_time': 47074.992399930954, 'accumulated_eval_time': 31922.088319778442, 'accumulated_logging_time': 1.9932560920715332, 'global_step': 130345, 'preemption_count': 0}), (132673, {'train/accuracy': 0.6964210271835327, 'train/loss': 1.3611470460891724, 'train/bleu': 35.60231110750975, 'validation/accuracy': 0.6960855722427368, 'validation/loss': 1.3613717555999756, 'validation/bleu': 30.815935514007432, 'validation/num_examples': 3000, 'test/accuracy': 0.7141944169998169, 'test/loss': 1.252968668937683, 'test/bleu': 31.444790126895835, 'test/num_examples': 3003, 'score': 47915.134707927704, 'total_duration': 80406.8292388916, 'accumulated_submission_time': 47915.134707927704, 'accumulated_eval_time': 32485.429721593857, 'accumulated_logging_time': 2.036381959915161, 'global_step': 132673, 'preemption_count': 0})], 'global_step': 133329}
I0211 00:09:24.763400 140699726837568 submission_runner.py:586] Timing: 48151.32396864891
I0211 00:09:24.763454 140699726837568 submission_runner.py:588] Total number of evals: 58
I0211 00:09:24.763507 140699726837568 submission_runner.py:589] ====================
I0211 00:09:24.764456 140699726837568 submission_runner.py:673] Final wmt score: 46984.7851524353
