python3 submission_runner.py --framework=jax --workload=librispeech_deepspeech --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=prize_qualification/study_1 --overwrite=true --save_checkpoints=false --num_tuning_trials=5 --rng_seed=808887856 --max_global_steps=48000 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab 2>&1 | tee -a /logs/librispeech_deepspeech_jax_02-11-2024-00-20-46.log
I0211 00:21:07.711077 139803787056960 logger_utils.py:76] Creating experiment directory at /experiment_runs/prize_qualification/study_1/librispeech_deepspeech_jax.
I0211 00:21:08.730232 139803787056960 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Host Interpreter CUDA
I0211 00:21:08.731067 139803787056960 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0211 00:21:08.731215 139803787056960 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0211 00:21:08.732331 139803787056960 submission_runner.py:542] Using RNG seed 808887856
I0211 00:21:09.909762 139803787056960 submission_runner.py:551] --- Tuning run 1/5 ---
I0211 00:21:09.909960 139803787056960 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_1/librispeech_deepspeech_jax/trial_1.
I0211 00:21:09.910360 139803787056960 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_1/librispeech_deepspeech_jax/trial_1/hparams.json.
I0211 00:21:10.090837 139803787056960 submission_runner.py:206] Initializing dataset.
I0211 00:21:10.091049 139803787056960 submission_runner.py:213] Initializing model.
I0211 00:21:12.607678 139803787056960 submission_runner.py:255] Initializing optimizer.
I0211 00:21:13.314751 139803787056960 submission_runner.py:262] Initializing metrics bundle.
I0211 00:21:13.314935 139803787056960 submission_runner.py:280] Initializing checkpoint and logger.
I0211 00:21:13.315958 139803787056960 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_1/librispeech_deepspeech_jax/trial_1 with prefix checkpoint_
I0211 00:21:13.316103 139803787056960 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_1/librispeech_deepspeech_jax/trial_1/meta_data_0.json.
I0211 00:21:13.316298 139803787056960 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0211 00:21:13.316365 139803787056960 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0211 00:21:13.616615 139803787056960 logger_utils.py:220] Unable to record git information. Continuing without it.
I0211 00:21:13.887423 139803787056960 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_1/librispeech_deepspeech_jax/trial_1/flags_0.json.
I0211 00:21:13.901392 139803787056960 submission_runner.py:314] Starting training loop.
I0211 00:21:14.195926 139803787056960 input_pipeline.py:20] Loading split = train-clean-100
I0211 00:21:14.254532 139803787056960 input_pipeline.py:20] Loading split = train-clean-360
I0211 00:21:14.417147 139803787056960 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0211 00:21:59.064043 139640939050752 logging_writer.py:48] [0] global_step=0, grad_norm=18.907827377319336, loss=33.32863235473633
I0211 00:21:59.098314 139803787056960 spec.py:321] Evaluating on the training split.
I0211 00:21:59.359864 139803787056960 input_pipeline.py:20] Loading split = train-clean-100
I0211 00:21:59.395740 139803787056960 input_pipeline.py:20] Loading split = train-clean-360
I0211 00:21:59.771725 139803787056960 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0211 00:24:14.695309 139803787056960 spec.py:333] Evaluating on the validation split.
I0211 00:24:14.893631 139803787056960 input_pipeline.py:20] Loading split = dev-clean
I0211 00:24:14.899504 139803787056960 input_pipeline.py:20] Loading split = dev-other
I0211 00:25:37.885773 139803787056960 spec.py:349] Evaluating on the test split.
I0211 00:25:38.085175 139803787056960 input_pipeline.py:20] Loading split = test-clean
I0211 00:26:24.621814 139803787056960 submission_runner.py:408] Time since start: 310.72s, 	Step: 1, 	{'train/ctc_loss': Array(31.839237, dtype=float32), 'train/wer': 4.706369864439956, 'validation/ctc_loss': Array(30.812881, dtype=float32), 'validation/wer': 4.233381928420402, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.939672, dtype=float32), 'test/wer': 4.561290191538196, 'test/num_examples': 2472, 'score': 45.19680690765381, 'total_duration': 310.71803855895996, 'accumulated_submission_time': 45.19680690765381, 'accumulated_eval_time': 265.52112197875977, 'accumulated_logging_time': 0}
I0211 00:26:24.652382 139634856163072 logging_writer.py:48] [1] accumulated_eval_time=265.521122, accumulated_logging_time=0, accumulated_submission_time=45.196807, global_step=1, preemption_count=0, score=45.196807, test/ctc_loss=30.939672470092773, test/num_examples=2472, test/wer=4.561290, total_duration=310.718039, train/ctc_loss=31.839237213134766, train/wer=4.706370, validation/ctc_loss=30.812881469726562, validation/num_examples=5348, validation/wer=4.233382
I0211 00:27:49.922492 139647309473536 logging_writer.py:48] [100] global_step=100, grad_norm=5.131544589996338, loss=8.375030517578125
I0211 00:29:06.825964 139647317866240 logging_writer.py:48] [200] global_step=200, grad_norm=1.0536582469940186, loss=6.452946186065674
I0211 00:30:25.393761 139647309473536 logging_writer.py:48] [300] global_step=300, grad_norm=0.5918302536010742, loss=6.039233207702637
I0211 00:31:43.505930 139647317866240 logging_writer.py:48] [400] global_step=400, grad_norm=0.5341817140579224, loss=5.867501735687256
I0211 00:33:00.545562 139647309473536 logging_writer.py:48] [500] global_step=500, grad_norm=0.5897762775421143, loss=5.834247589111328
I0211 00:34:21.704005 139647317866240 logging_writer.py:48] [600] global_step=600, grad_norm=0.5370457172393799, loss=5.8096513748168945
I0211 00:35:46.967269 139647309473536 logging_writer.py:48] [700] global_step=700, grad_norm=0.4610822796821594, loss=5.73892879486084
I0211 00:37:11.540218 139647317866240 logging_writer.py:48] [800] global_step=800, grad_norm=0.40557557344436646, loss=5.597835540771484
I0211 00:38:38.126525 139647309473536 logging_writer.py:48] [900] global_step=900, grad_norm=0.702613353729248, loss=5.370424747467041
I0211 00:40:03.208532 139647317866240 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.0269855260849, loss=5.026307582855225
I0211 00:41:25.664177 139647359829760 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.198407530784607, loss=4.509186267852783
I0211 00:42:43.940801 139647351437056 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.9916284680366516, loss=4.246293544769287
I0211 00:44:00.673956 139647359829760 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.2936592102050781, loss=3.9105842113494873
I0211 00:45:18.946665 139647351437056 logging_writer.py:48] [1400] global_step=1400, grad_norm=1.7837783098220825, loss=3.667926549911499
I0211 00:46:44.165318 139647359829760 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.4682979583740234, loss=3.526956081390381
I0211 00:48:10.217452 139647351437056 logging_writer.py:48] [1600] global_step=1600, grad_norm=4.093447685241699, loss=3.2863776683807373
I0211 00:49:35.726092 139647359829760 logging_writer.py:48] [1700] global_step=1700, grad_norm=2.619372606277466, loss=3.185415267944336
I0211 00:50:25.231380 139803787056960 spec.py:321] Evaluating on the training split.
I0211 00:51:02.588948 139803787056960 spec.py:333] Evaluating on the validation split.
I0211 00:51:48.230319 139803787056960 spec.py:349] Evaluating on the test split.
I0211 00:52:10.913261 139803787056960 submission_runner.py:408] Time since start: 1857.01s, 	Step: 1756, 	{'train/ctc_loss': Array(6.2416306, dtype=float32), 'train/wer': 0.941288541945583, 'validation/ctc_loss': Array(6.160726, dtype=float32), 'validation/wer': 0.8952083956863011, 'validation/num_examples': 5348, 'test/ctc_loss': Array(6.0450172, dtype=float32), 'test/wer': 0.8979343123514716, 'test/num_examples': 2472, 'score': 1485.6898527145386, 'total_duration': 1857.0092232227325, 'accumulated_submission_time': 1485.6898527145386, 'accumulated_eval_time': 371.200421333313, 'accumulated_logging_time': 0.04808688163757324}
I0211 00:52:10.943000 139647359829760 logging_writer.py:48] [1756] accumulated_eval_time=371.200421, accumulated_logging_time=0.048087, accumulated_submission_time=1485.689853, global_step=1756, preemption_count=0, score=1485.689853, test/ctc_loss=6.045017242431641, test/num_examples=2472, test/wer=0.897934, total_duration=1857.009223, train/ctc_loss=6.241630554199219, train/wer=0.941289, validation/ctc_loss=6.160726070404053, validation/num_examples=5348, validation/wer=0.895208
I0211 00:52:45.230345 139647351437056 logging_writer.py:48] [1800] global_step=1800, grad_norm=2.7273380756378174, loss=3.0896928310394287
I0211 00:54:01.175138 139647359829760 logging_writer.py:48] [1900] global_step=1900, grad_norm=2.6139917373657227, loss=2.985832691192627
I0211 00:55:17.835905 139647351437056 logging_writer.py:48] [2000] global_step=2000, grad_norm=4.32314395904541, loss=2.862997531890869
I0211 00:56:38.853517 139647032149760 logging_writer.py:48] [2100] global_step=2100, grad_norm=1.8185456991195679, loss=2.767890691757202
I0211 00:57:56.021282 139647023757056 logging_writer.py:48] [2200] global_step=2200, grad_norm=2.861588478088379, loss=2.6941070556640625
I0211 00:59:13.939387 139647032149760 logging_writer.py:48] [2300] global_step=2300, grad_norm=3.0001211166381836, loss=2.6179685592651367
I0211 01:00:38.570522 139647023757056 logging_writer.py:48] [2400] global_step=2400, grad_norm=2.7142114639282227, loss=2.597555160522461
I0211 01:02:04.425052 139647032149760 logging_writer.py:48] [2500] global_step=2500, grad_norm=2.9538614749908447, loss=2.597254514694214
I0211 01:03:32.678597 139647023757056 logging_writer.py:48] [2600] global_step=2600, grad_norm=2.8153700828552246, loss=2.4670612812042236
I0211 01:04:58.962553 139647032149760 logging_writer.py:48] [2700] global_step=2700, grad_norm=2.508173942565918, loss=2.396179676055908
I0211 01:06:27.444043 139647023757056 logging_writer.py:48] [2800] global_step=2800, grad_norm=2.21309757232666, loss=2.4116251468658447
I0211 01:07:55.838003 139647032149760 logging_writer.py:48] [2900] global_step=2900, grad_norm=2.9920730590820312, loss=2.2821969985961914
I0211 01:09:24.310116 139647023757056 logging_writer.py:48] [3000] global_step=3000, grad_norm=4.228219032287598, loss=2.249695301055908
I0211 01:10:50.420083 139647359829760 logging_writer.py:48] [3100] global_step=3100, grad_norm=4.362528324127197, loss=2.2424733638763428
I0211 01:12:05.997289 139647351437056 logging_writer.py:48] [3200] global_step=3200, grad_norm=3.3008382320404053, loss=2.2098398208618164
I0211 01:13:24.286761 139647359829760 logging_writer.py:48] [3300] global_step=3300, grad_norm=3.299011707305908, loss=2.201037645339966
I0211 01:14:43.065491 139647351437056 logging_writer.py:48] [3400] global_step=3400, grad_norm=5.6953444480896, loss=2.174208402633667
I0211 01:16:05.004581 139647359829760 logging_writer.py:48] [3500] global_step=3500, grad_norm=3.598855495452881, loss=2.120543956756592
I0211 01:16:11.554705 139803787056960 spec.py:321] Evaluating on the training split.
I0211 01:17:04.960015 139803787056960 spec.py:333] Evaluating on the validation split.
I0211 01:17:51.926158 139803787056960 spec.py:349] Evaluating on the test split.
I0211 01:18:17.587223 139803787056960 submission_runner.py:408] Time since start: 3423.68s, 	Step: 3509, 	{'train/ctc_loss': Array(3.7979012, dtype=float32), 'train/wer': 0.7728577897344379, 'validation/ctc_loss': Array(4.057168, dtype=float32), 'validation/wer': 0.784469525087616, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3.663008, dtype=float32), 'test/wer': 0.7430382060812869, 'test/num_examples': 2472, 'score': 2926.2204928398132, 'total_duration': 3423.682983160019, 'accumulated_submission_time': 2926.2204928398132, 'accumulated_eval_time': 497.2301824092865, 'accumulated_logging_time': 0.0906984806060791}
I0211 01:18:17.613247 139647359829760 logging_writer.py:48] [3509] accumulated_eval_time=497.230182, accumulated_logging_time=0.090698, accumulated_submission_time=2926.220493, global_step=3509, preemption_count=0, score=2926.220493, test/ctc_loss=3.663007974624634, test/num_examples=2472, test/wer=0.743038, total_duration=3423.682983, train/ctc_loss=3.797901153564453, train/wer=0.772858, validation/ctc_loss=4.057168006896973, validation/num_examples=5348, validation/wer=0.784470
I0211 01:19:26.802937 139647351437056 logging_writer.py:48] [3600] global_step=3600, grad_norm=3.2587664127349854, loss=2.1843619346618652
I0211 01:20:42.598731 139647359829760 logging_writer.py:48] [3700] global_step=3700, grad_norm=4.221707820892334, loss=2.1376569271087646
I0211 01:22:01.395105 139647351437056 logging_writer.py:48] [3800] global_step=3800, grad_norm=10.193707466125488, loss=2.0914571285247803
I0211 01:23:25.492430 139647359829760 logging_writer.py:48] [3900] global_step=3900, grad_norm=3.4663102626800537, loss=2.087495803833008
I0211 01:24:50.823951 139647351437056 logging_writer.py:48] [4000] global_step=4000, grad_norm=4.0582380294799805, loss=2.0314536094665527
I0211 01:26:20.181678 139647359829760 logging_writer.py:48] [4100] global_step=4100, grad_norm=2.8326163291931152, loss=2.0373356342315674
I0211 01:27:39.882352 139647032149760 logging_writer.py:48] [4200] global_step=4200, grad_norm=3.6495230197906494, loss=1.9670014381408691
I0211 01:28:58.224650 139647023757056 logging_writer.py:48] [4300] global_step=4300, grad_norm=3.198472738265991, loss=1.9724061489105225
I0211 01:30:19.265390 139647032149760 logging_writer.py:48] [4400] global_step=4400, grad_norm=3.3286564350128174, loss=1.9907665252685547
I0211 01:31:36.665083 139647023757056 logging_writer.py:48] [4500] global_step=4500, grad_norm=3.322932720184326, loss=1.9986714124679565
I0211 01:33:03.788922 139647032149760 logging_writer.py:48] [4600] global_step=4600, grad_norm=5.302097797393799, loss=2.0176050662994385
I0211 01:34:29.370846 139647023757056 logging_writer.py:48] [4700] global_step=4700, grad_norm=2.993861436843872, loss=1.915127158164978
I0211 01:35:54.326484 139647032149760 logging_writer.py:48] [4800] global_step=4800, grad_norm=4.378385543823242, loss=2.0062060356140137
I0211 01:37:20.817348 139647023757056 logging_writer.py:48] [4900] global_step=4900, grad_norm=4.627344131469727, loss=2.0002222061157227
I0211 01:38:50.801180 139647032149760 logging_writer.py:48] [5000] global_step=5000, grad_norm=5.017622947692871, loss=2.0095055103302
I0211 01:40:15.038268 139647023757056 logging_writer.py:48] [5100] global_step=5100, grad_norm=3.322375774383545, loss=1.908208966255188
I0211 01:41:40.215793 139647032149760 logging_writer.py:48] [5200] global_step=5200, grad_norm=3.085657835006714, loss=1.8919965028762817
I0211 01:42:17.714303 139803787056960 spec.py:321] Evaluating on the training split.
I0211 01:43:12.266809 139803787056960 spec.py:333] Evaluating on the validation split.
I0211 01:44:05.323616 139803787056960 spec.py:349] Evaluating on the test split.
I0211 01:44:32.098075 139803787056960 submission_runner.py:408] Time since start: 4998.19s, 	Step: 5251, 	{'train/ctc_loss': Array(0.6250547, dtype=float32), 'train/wer': 0.21471079694904355, 'validation/ctc_loss': Array(1.0514781, dtype=float32), 'validation/wer': 0.2947179393108509, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.7044876, dtype=float32), 'test/wer': 0.22361018016371134, 'test/num_examples': 2472, 'score': 4366.24077963829, 'total_duration': 4998.193810939789, 'accumulated_submission_time': 4366.24077963829, 'accumulated_eval_time': 631.6112017631531, 'accumulated_logging_time': 0.12792444229125977}
I0211 01:44:32.126070 139646740309760 logging_writer.py:48] [5251] accumulated_eval_time=631.611202, accumulated_logging_time=0.127924, accumulated_submission_time=4366.240780, global_step=5251, preemption_count=0, score=4366.240780, test/ctc_loss=0.7044876217842102, test/num_examples=2472, test/wer=0.223610, total_duration=4998.193811, train/ctc_loss=0.6250547170639038, train/wer=0.214711, validation/ctc_loss=1.0514781475067139, validation/num_examples=5348, validation/wer=0.294718
I0211 01:45:09.950634 139646731917056 logging_writer.py:48] [5300] global_step=5300, grad_norm=4.176156520843506, loss=1.8567184209823608
I0211 01:46:24.923891 139646740309760 logging_writer.py:48] [5400] global_step=5400, grad_norm=4.265374660491943, loss=1.8361849784851074
I0211 01:47:41.355714 139646731917056 logging_writer.py:48] [5500] global_step=5500, grad_norm=3.162346363067627, loss=1.8707683086395264
I0211 01:48:56.551910 139646740309760 logging_writer.py:48] [5600] global_step=5600, grad_norm=3.4739532470703125, loss=1.853661060333252
I0211 01:50:24.464813 139646731917056 logging_writer.py:48] [5700] global_step=5700, grad_norm=2.2800686359405518, loss=1.793788194656372
I0211 01:51:49.811901 139646740309760 logging_writer.py:48] [5800] global_step=5800, grad_norm=2.8026211261749268, loss=1.8865249156951904
I0211 01:53:15.467299 139646731917056 logging_writer.py:48] [5900] global_step=5900, grad_norm=2.9538004398345947, loss=1.9099127054214478
I0211 01:54:40.955626 139646740309760 logging_writer.py:48] [6000] global_step=6000, grad_norm=3.1795144081115723, loss=1.8006536960601807
I0211 01:56:09.632562 139646731917056 logging_writer.py:48] [6100] global_step=6100, grad_norm=3.7033751010894775, loss=1.8428623676300049
I0211 01:57:36.494910 139648015189760 logging_writer.py:48] [6200] global_step=6200, grad_norm=2.959261655807495, loss=1.8125618696212769
I0211 01:58:51.825346 139648006797056 logging_writer.py:48] [6300] global_step=6300, grad_norm=3.6089746952056885, loss=1.827085256576538
I0211 02:00:10.422340 139648015189760 logging_writer.py:48] [6400] global_step=6400, grad_norm=2.9748189449310303, loss=1.8101365566253662
I0211 02:01:28.456208 139648006797056 logging_writer.py:48] [6500] global_step=6500, grad_norm=2.625206708908081, loss=1.8054189682006836
I0211 02:02:48.626398 139648015189760 logging_writer.py:48] [6600] global_step=6600, grad_norm=3.31874942779541, loss=1.7837516069412231
I0211 02:04:15.253167 139648006797056 logging_writer.py:48] [6700] global_step=6700, grad_norm=3.6629936695098877, loss=1.7583128213882446
I0211 02:05:40.022079 139648015189760 logging_writer.py:48] [6800] global_step=6800, grad_norm=3.0085666179656982, loss=1.773826241493225
I0211 02:07:07.051633 139648006797056 logging_writer.py:48] [6900] global_step=6900, grad_norm=3.235292673110962, loss=1.7476445436477661
I0211 02:08:30.692570 139648015189760 logging_writer.py:48] [7000] global_step=7000, grad_norm=2.9389450550079346, loss=1.7727887630462646
I0211 02:08:32.420769 139803787056960 spec.py:321] Evaluating on the training split.
I0211 02:09:36.260926 139803787056960 spec.py:333] Evaluating on the validation split.
I0211 02:10:27.464638 139803787056960 spec.py:349] Evaluating on the test split.
I0211 02:10:53.536656 139803787056960 submission_runner.py:408] Time since start: 6579.63s, 	Step: 7003, 	{'train/ctc_loss': Array(0.48509657, dtype=float32), 'train/wer': 0.16434877315711757, 'validation/ctc_loss': Array(0.82567525, dtype=float32), 'validation/wer': 0.23964779825636967, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.522345, dtype=float32), 'test/wer': 0.17057664574573964, 'test/num_examples': 2472, 'score': 5806.4535665512085, 'total_duration': 6579.632403612137, 'accumulated_submission_time': 5806.4535665512085, 'accumulated_eval_time': 772.7243013381958, 'accumulated_logging_time': 0.16758346557617188}
I0211 02:10:53.564293 139648015189760 logging_writer.py:48] [7003] accumulated_eval_time=772.724301, accumulated_logging_time=0.167583, accumulated_submission_time=5806.453567, global_step=7003, preemption_count=0, score=5806.453567, test/ctc_loss=0.5223450064659119, test/num_examples=2472, test/wer=0.170577, total_duration=6579.632404, train/ctc_loss=0.4850965738296509, train/wer=0.164349, validation/ctc_loss=0.8256752490997314, validation/num_examples=5348, validation/wer=0.239648
I0211 02:12:07.189767 139648006797056 logging_writer.py:48] [7100] global_step=7100, grad_norm=2.4936447143554688, loss=1.7290805578231812
I0211 02:13:22.640533 139648015189760 logging_writer.py:48] [7200] global_step=7200, grad_norm=3.391585111618042, loss=1.7602070569992065
I0211 02:14:41.018894 139648015189760 logging_writer.py:48] [7300] global_step=7300, grad_norm=4.490617752075195, loss=1.7122644186019897
I0211 02:15:57.159069 139648006797056 logging_writer.py:48] [7400] global_step=7400, grad_norm=2.5008199214935303, loss=1.7018762826919556
I0211 02:17:14.411729 139648015189760 logging_writer.py:48] [7500] global_step=7500, grad_norm=3.7231955528259277, loss=1.7133982181549072
I0211 02:18:36.175950 139648006797056 logging_writer.py:48] [7600] global_step=7600, grad_norm=2.235790491104126, loss=1.7441866397857666
I0211 02:20:01.995776 139648015189760 logging_writer.py:48] [7700] global_step=7700, grad_norm=3.4222240447998047, loss=1.751357913017273
I0211 02:21:31.097446 139648006797056 logging_writer.py:48] [7800] global_step=7800, grad_norm=3.5172417163848877, loss=1.7593835592269897
I0211 02:22:58.539094 139648015189760 logging_writer.py:48] [7900] global_step=7900, grad_norm=3.5350522994995117, loss=1.71341872215271
I0211 02:24:26.299308 139648006797056 logging_writer.py:48] [8000] global_step=8000, grad_norm=2.978165626525879, loss=1.6720869541168213
I0211 02:25:51.260221 139648015189760 logging_writer.py:48] [8100] global_step=8100, grad_norm=2.5718185901641846, loss=1.6810832023620605
I0211 02:27:18.910723 139648006797056 logging_writer.py:48] [8200] global_step=8200, grad_norm=2.787795066833496, loss=1.7524919509887695
I0211 02:28:44.044406 139648015189760 logging_writer.py:48] [8300] global_step=8300, grad_norm=2.8851635456085205, loss=1.7076209783554077
I0211 02:29:59.759518 139648006797056 logging_writer.py:48] [8400] global_step=8400, grad_norm=2.9792304039001465, loss=1.694433569908142
I0211 02:31:17.584374 139648015189760 logging_writer.py:48] [8500] global_step=8500, grad_norm=3.4704740047454834, loss=1.7266783714294434
I0211 02:32:40.503417 139648006797056 logging_writer.py:48] [8600] global_step=8600, grad_norm=3.2805912494659424, loss=1.688355565071106
I0211 02:34:04.114844 139648015189760 logging_writer.py:48] [8700] global_step=8700, grad_norm=1.7813458442687988, loss=1.6521308422088623
I0211 02:34:53.690079 139803787056960 spec.py:321] Evaluating on the training split.
I0211 02:35:52.286606 139803787056960 spec.py:333] Evaluating on the validation split.
I0211 02:36:43.356117 139803787056960 spec.py:349] Evaluating on the test split.
I0211 02:37:09.824875 139803787056960 submission_runner.py:408] Time since start: 8155.92s, 	Step: 8757, 	{'train/ctc_loss': Array(0.42493558, dtype=float32), 'train/wer': 0.14487703581291178, 'validation/ctc_loss': Array(0.76556903, dtype=float32), 'validation/wer': 0.21989437809552315, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4660455, dtype=float32), 'test/wer': 0.1513212682550322, 'test/num_examples': 2472, 'score': 7246.49645781517, 'total_duration': 8155.920823574066, 'accumulated_submission_time': 7246.49645781517, 'accumulated_eval_time': 908.856529712677, 'accumulated_logging_time': 0.20881009101867676}
I0211 02:37:09.852061 139647001429760 logging_writer.py:48] [8757] accumulated_eval_time=908.856530, accumulated_logging_time=0.208810, accumulated_submission_time=7246.496458, global_step=8757, preemption_count=0, score=7246.496458, test/ctc_loss=0.4660454988479614, test/num_examples=2472, test/wer=0.151321, total_duration=8155.920824, train/ctc_loss=0.42493557929992676, train/wer=0.144877, validation/ctc_loss=0.7655690312385559, validation/num_examples=5348, validation/wer=0.219894
I0211 02:37:43.527067 139646993037056 logging_writer.py:48] [8800] global_step=8800, grad_norm=2.2132911682128906, loss=1.6205379962921143
I0211 02:39:00.373985 139647001429760 logging_writer.py:48] [8900] global_step=8900, grad_norm=4.1262125968933105, loss=1.6476290225982666
I0211 02:40:15.439657 139646993037056 logging_writer.py:48] [9000] global_step=9000, grad_norm=2.9122796058654785, loss=1.7183743715286255
I0211 02:41:38.563798 139647001429760 logging_writer.py:48] [9100] global_step=9100, grad_norm=4.385451793670654, loss=1.6857576370239258
I0211 02:43:05.078965 139646993037056 logging_writer.py:48] [9200] global_step=9200, grad_norm=2.62835693359375, loss=1.6536741256713867
I0211 02:44:31.648551 139647001429760 logging_writer.py:48] [9300] global_step=9300, grad_norm=2.5660629272460938, loss=1.6345806121826172
I0211 02:45:47.399885 139646993037056 logging_writer.py:48] [9400] global_step=9400, grad_norm=3.1177546977996826, loss=1.675661563873291
I0211 02:47:04.506275 139647001429760 logging_writer.py:48] [9500] global_step=9500, grad_norm=2.7990214824676514, loss=1.6376614570617676
I0211 02:48:20.340895 139646993037056 logging_writer.py:48] [9600] global_step=9600, grad_norm=2.9441542625427246, loss=1.7014424800872803
I0211 02:49:43.341329 139647001429760 logging_writer.py:48] [9700] global_step=9700, grad_norm=3.6530632972717285, loss=1.6918896436691284
I0211 02:51:12.332288 139646993037056 logging_writer.py:48] [9800] global_step=9800, grad_norm=3.411895990371704, loss=1.6653918027877808
I0211 02:52:37.805147 139647001429760 logging_writer.py:48] [9900] global_step=9900, grad_norm=4.094379901885986, loss=1.570101261138916
I0211 02:54:08.133275 139646993037056 logging_writer.py:48] [10000] global_step=10000, grad_norm=2.6829986572265625, loss=1.5890591144561768
I0211 02:55:33.104004 139647001429760 logging_writer.py:48] [10100] global_step=10100, grad_norm=2.8822600841522217, loss=1.6493589878082275
I0211 02:57:00.396662 139646993037056 logging_writer.py:48] [10200] global_step=10200, grad_norm=3.230015754699707, loss=1.639818549156189
I0211 02:58:32.519598 139647001429760 logging_writer.py:48] [10300] global_step=10300, grad_norm=1.616878628730774, loss=1.5731936693191528
I0211 02:59:51.715274 139646993037056 logging_writer.py:48] [10400] global_step=10400, grad_norm=3.2129454612731934, loss=1.6374750137329102
I0211 03:01:10.222686 139647001429760 logging_writer.py:48] [10500] global_step=10500, grad_norm=4.36890172958374, loss=1.6257578134536743
I0211 03:01:10.228561 139803787056960 spec.py:321] Evaluating on the training split.
I0211 03:02:07.209467 139803787056960 spec.py:333] Evaluating on the validation split.
I0211 03:02:58.969363 139803787056960 spec.py:349] Evaluating on the test split.
I0211 03:03:26.106330 139803787056960 submission_runner.py:408] Time since start: 9732.20s, 	Step: 10501, 	{'train/ctc_loss': Array(0.41917217, dtype=float32), 'train/wer': 0.14101285135848834, 'validation/ctc_loss': Array(0.72048366, dtype=float32), 'validation/wer': 0.208675671239754, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.43311206, dtype=float32), 'test/wer': 0.1437044258932017, 'test/num_examples': 2472, 'score': 8686.787279844284, 'total_duration': 9732.202117919922, 'accumulated_submission_time': 8686.787279844284, 'accumulated_eval_time': 1044.7315225601196, 'accumulated_logging_time': 0.2510659694671631}
I0211 03:03:26.132536 139647001429760 logging_writer.py:48] [10501] accumulated_eval_time=1044.731523, accumulated_logging_time=0.251066, accumulated_submission_time=8686.787280, global_step=10501, preemption_count=0, score=8686.787280, test/ctc_loss=0.4331120550632477, test/num_examples=2472, test/wer=0.143704, total_duration=9732.202118, train/ctc_loss=0.41917216777801514, train/wer=0.141013, validation/ctc_loss=0.720483660697937, validation/num_examples=5348, validation/wer=0.208676
I0211 03:04:41.031225 139646993037056 logging_writer.py:48] [10600] global_step=10600, grad_norm=2.067746162414551, loss=1.5911030769348145
I0211 03:05:57.401492 139647001429760 logging_writer.py:48] [10700] global_step=10700, grad_norm=3.601412773132324, loss=1.6485755443572998
I0211 03:07:17.090353 139646993037056 logging_writer.py:48] [10800] global_step=10800, grad_norm=3.5842483043670654, loss=1.655178189277649
I0211 03:08:44.857552 139647001429760 logging_writer.py:48] [10900] global_step=10900, grad_norm=3.014280080795288, loss=1.6640056371688843
I0211 03:10:11.383203 139646993037056 logging_writer.py:48] [11000] global_step=11000, grad_norm=2.9522433280944824, loss=1.601648211479187
I0211 03:11:39.302329 139647001429760 logging_writer.py:48] [11100] global_step=11100, grad_norm=2.6340951919555664, loss=1.5954701900482178
I0211 03:13:06.631430 139646993037056 logging_writer.py:48] [11200] global_step=11200, grad_norm=2.8372440338134766, loss=1.573568344116211
I0211 03:14:35.713822 139647001429760 logging_writer.py:48] [11300] global_step=11300, grad_norm=3.582219123840332, loss=1.680881381034851
I0211 03:16:01.350891 139647001429760 logging_writer.py:48] [11400] global_step=11400, grad_norm=2.373464345932007, loss=1.6368087530136108
I0211 03:17:18.926284 139646993037056 logging_writer.py:48] [11500] global_step=11500, grad_norm=3.3255882263183594, loss=1.5847193002700806
I0211 03:18:36.588054 139647001429760 logging_writer.py:48] [11600] global_step=11600, grad_norm=3.167328119277954, loss=1.583288311958313
I0211 03:19:59.080811 139646993037056 logging_writer.py:48] [11700] global_step=11700, grad_norm=3.0539369583129883, loss=1.6048673391342163
I0211 03:21:25.952871 139647001429760 logging_writer.py:48] [11800] global_step=11800, grad_norm=3.631342649459839, loss=1.5977858304977417
I0211 03:22:55.735655 139646993037056 logging_writer.py:48] [11900] global_step=11900, grad_norm=2.6400585174560547, loss=1.6101027727127075
I0211 03:24:22.102587 139647001429760 logging_writer.py:48] [12000] global_step=12000, grad_norm=3.2355101108551025, loss=1.5900259017944336
I0211 03:25:47.761790 139646993037056 logging_writer.py:48] [12100] global_step=12100, grad_norm=3.00327205657959, loss=1.6647554636001587
I0211 03:27:16.545600 139647001429760 logging_writer.py:48] [12200] global_step=12200, grad_norm=2.8705646991729736, loss=1.5723844766616821
I0211 03:27:26.267045 139803787056960 spec.py:321] Evaluating on the training split.
I0211 03:28:26.979798 139803787056960 spec.py:333] Evaluating on the validation split.
I0211 03:29:18.139266 139803787056960 spec.py:349] Evaluating on the test split.
I0211 03:29:44.615149 139803787056960 submission_runner.py:408] Time since start: 11310.71s, 	Step: 12213, 	{'train/ctc_loss': Array(0.38169208, dtype=float32), 'train/wer': 0.12917171846085126, 'validation/ctc_loss': Array(0.70423025, dtype=float32), 'validation/wer': 0.20394489124033327, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4196876, dtype=float32), 'test/wer': 0.13728596672963256, 'test/num_examples': 2472, 'score': 10126.841429710388, 'total_duration': 11310.710699796677, 'accumulated_submission_time': 10126.841429710388, 'accumulated_eval_time': 1183.076649427414, 'accumulated_logging_time': 0.2884364128112793}
I0211 03:29:44.641692 139647431509760 logging_writer.py:48] [12213] accumulated_eval_time=1183.076649, accumulated_logging_time=0.288436, accumulated_submission_time=10126.841430, global_step=12213, preemption_count=0, score=10126.841430, test/ctc_loss=0.4196875989437103, test/num_examples=2472, test/wer=0.137286, total_duration=11310.710700, train/ctc_loss=0.3816920816898346, train/wer=0.129172, validation/ctc_loss=0.7042302489280701, validation/num_examples=5348, validation/wer=0.203945
I0211 03:30:50.963031 139647423117056 logging_writer.py:48] [12300] global_step=12300, grad_norm=3.341167688369751, loss=1.625841498374939
I0211 03:32:12.437389 139646776149760 logging_writer.py:48] [12400] global_step=12400, grad_norm=2.0404248237609863, loss=1.5426853895187378
I0211 03:33:30.026417 139646767757056 logging_writer.py:48] [12500] global_step=12500, grad_norm=3.790127992630005, loss=1.5844652652740479
I0211 03:34:48.919454 139646776149760 logging_writer.py:48] [12600] global_step=12600, grad_norm=3.75459623336792, loss=1.6085408926010132
I0211 03:36:08.581359 139646767757056 logging_writer.py:48] [12700] global_step=12700, grad_norm=3.354726552963257, loss=1.5382903814315796
I0211 03:37:33.637780 139646776149760 logging_writer.py:48] [12800] global_step=12800, grad_norm=2.435901165008545, loss=1.612585186958313
I0211 03:39:00.826319 139646767757056 logging_writer.py:48] [12900] global_step=12900, grad_norm=3.7249362468719482, loss=1.5550315380096436
I0211 03:40:27.549193 139646776149760 logging_writer.py:48] [13000] global_step=13000, grad_norm=3.715796709060669, loss=1.6392085552215576
I0211 03:41:55.314031 139646767757056 logging_writer.py:48] [13100] global_step=13100, grad_norm=3.462203025817871, loss=1.5880554914474487
I0211 03:43:23.608156 139646776149760 logging_writer.py:48] [13200] global_step=13200, grad_norm=2.892800807952881, loss=1.5878971815109253
I0211 03:44:51.878673 139646767757056 logging_writer.py:48] [13300] global_step=13300, grad_norm=3.1053504943847656, loss=1.5679606199264526
I0211 03:46:17.153141 139646776149760 logging_writer.py:48] [13400] global_step=13400, grad_norm=5.298421859741211, loss=1.5693448781967163
I0211 03:47:34.587038 139646767757056 logging_writer.py:48] [13500] global_step=13500, grad_norm=3.040256977081299, loss=1.5938489437103271
I0211 03:48:51.062639 139646776149760 logging_writer.py:48] [13600] global_step=13600, grad_norm=3.501657009124756, loss=1.5473129749298096
I0211 03:50:10.173543 139646767757056 logging_writer.py:48] [13700] global_step=13700, grad_norm=3.859849452972412, loss=1.614911675453186
I0211 03:51:32.279866 139646776149760 logging_writer.py:48] [13800] global_step=13800, grad_norm=2.6652114391326904, loss=1.615319013595581
I0211 03:52:56.882385 139646767757056 logging_writer.py:48] [13900] global_step=13900, grad_norm=2.7443346977233887, loss=1.5562567710876465
I0211 03:53:45.460365 139803787056960 spec.py:321] Evaluating on the training split.
I0211 03:54:40.648293 139803787056960 spec.py:333] Evaluating on the validation split.
I0211 03:55:32.066437 139803787056960 spec.py:349] Evaluating on the test split.
I0211 03:55:58.239454 139803787056960 submission_runner.py:408] Time since start: 12884.34s, 	Step: 13956, 	{'train/ctc_loss': Array(0.3189625, dtype=float32), 'train/wer': 0.1119997919862711, 'validation/ctc_loss': Array(0.6466541, dtype=float32), 'validation/wer': 0.1890187976095079, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.38722602, dtype=float32), 'test/wer': 0.12702861901570084, 'test/num_examples': 2472, 'score': 11567.578044652939, 'total_duration': 12884.33522772789, 'accumulated_submission_time': 11567.578044652939, 'accumulated_eval_time': 1315.8529777526855, 'accumulated_logging_time': 0.32673048973083496}
I0211 03:55:58.269230 139646776149760 logging_writer.py:48] [13956] accumulated_eval_time=1315.852978, accumulated_logging_time=0.326730, accumulated_submission_time=11567.578045, global_step=13956, preemption_count=0, score=11567.578045, test/ctc_loss=0.38722601532936096, test/num_examples=2472, test/wer=0.127029, total_duration=12884.335228, train/ctc_loss=0.3189625144004822, train/wer=0.112000, validation/ctc_loss=0.6466541290283203, validation/num_examples=5348, validation/wer=0.189019
I0211 03:56:31.898863 139646767757056 logging_writer.py:48] [14000] global_step=14000, grad_norm=4.477115154266357, loss=1.5668529272079468
I0211 03:57:46.950370 139646776149760 logging_writer.py:48] [14100] global_step=14100, grad_norm=3.9511849880218506, loss=1.5716235637664795
I0211 03:59:02.497412 139646767757056 logging_writer.py:48] [14200] global_step=14200, grad_norm=4.244082450866699, loss=1.5179622173309326
I0211 04:00:29.741423 139646776149760 logging_writer.py:48] [14300] global_step=14300, grad_norm=3.128523826599121, loss=1.5169110298156738
I0211 04:01:56.523065 139646767757056 logging_writer.py:48] [14400] global_step=14400, grad_norm=3.485260486602783, loss=1.6125516891479492
I0211 04:03:20.511490 139647431509760 logging_writer.py:48] [14500] global_step=14500, grad_norm=2.2210311889648438, loss=1.4833520650863647
I0211 04:04:41.400228 139647423117056 logging_writer.py:48] [14600] global_step=14600, grad_norm=2.8984017372131348, loss=1.4979251623153687
I0211 04:05:58.645653 139647431509760 logging_writer.py:48] [14700] global_step=14700, grad_norm=2.4844298362731934, loss=1.526841163635254
I0211 04:07:22.182374 139647423117056 logging_writer.py:48] [14800] global_step=14800, grad_norm=2.189592123031616, loss=1.5701342821121216
I0211 04:08:47.122552 139647431509760 logging_writer.py:48] [14900] global_step=14900, grad_norm=2.6809256076812744, loss=1.5583902597427368
I0211 04:10:16.236566 139647423117056 logging_writer.py:48] [15000] global_step=15000, grad_norm=2.7555975914001465, loss=1.4916303157806396
I0211 04:11:46.082078 139647431509760 logging_writer.py:48] [15100] global_step=15100, grad_norm=4.392340183258057, loss=1.5303955078125
I0211 04:13:14.719873 139647423117056 logging_writer.py:48] [15200] global_step=15200, grad_norm=4.067487716674805, loss=1.5601118803024292
I0211 04:14:45.088178 139647431509760 logging_writer.py:48] [15300] global_step=15300, grad_norm=3.6347780227661133, loss=1.539668083190918
I0211 04:16:13.908953 139647423117056 logging_writer.py:48] [15400] global_step=15400, grad_norm=3.4891107082366943, loss=1.554335594177246
I0211 04:17:41.490517 139647431509760 logging_writer.py:48] [15500] global_step=15500, grad_norm=2.971856117248535, loss=1.4500030279159546
I0211 04:18:57.558238 139647423117056 logging_writer.py:48] [15600] global_step=15600, grad_norm=2.388181447982788, loss=1.5271092653274536
I0211 04:19:58.448931 139803787056960 spec.py:321] Evaluating on the training split.
I0211 04:20:52.872297 139803787056960 spec.py:333] Evaluating on the validation split.
I0211 04:21:46.038180 139803787056960 spec.py:349] Evaluating on the test split.
I0211 04:22:12.177273 139803787056960 submission_runner.py:408] Time since start: 14458.27s, 	Step: 15677, 	{'train/ctc_loss': Array(0.30618104, dtype=float32), 'train/wer': 0.1042991131516367, 'validation/ctc_loss': Array(0.6405959, dtype=float32), 'validation/wer': 0.1845680025488284, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3840812, dtype=float32), 'test/wer': 0.1235756504783377, 'test/num_examples': 2472, 'score': 13007.677670955658, 'total_duration': 14458.273012399673, 'accumulated_submission_time': 13007.677670955658, 'accumulated_eval_time': 1449.5785381793976, 'accumulated_logging_time': 0.3680403232574463}
I0211 04:22:12.203029 139647431509760 logging_writer.py:48] [15677] accumulated_eval_time=1449.578538, accumulated_logging_time=0.368040, accumulated_submission_time=13007.677671, global_step=15677, preemption_count=0, score=13007.677671, test/ctc_loss=0.3840812146663666, test/num_examples=2472, test/wer=0.123576, total_duration=14458.273012, train/ctc_loss=0.30618104338645935, train/wer=0.104299, validation/ctc_loss=0.6405959129333496, validation/num_examples=5348, validation/wer=0.184568
I0211 04:22:30.806049 139647423117056 logging_writer.py:48] [15700] global_step=15700, grad_norm=2.211635112762451, loss=1.5208443403244019
I0211 04:23:46.205393 139647431509760 logging_writer.py:48] [15800] global_step=15800, grad_norm=4.478662967681885, loss=1.5438425540924072
I0211 04:25:01.357724 139647423117056 logging_writer.py:48] [15900] global_step=15900, grad_norm=2.7238094806671143, loss=1.4590741395950317
I0211 04:26:23.142215 139647431509760 logging_writer.py:48] [16000] global_step=16000, grad_norm=3.0260133743286133, loss=1.5846244096755981
I0211 04:27:51.799601 139647423117056 logging_writer.py:48] [16100] global_step=16100, grad_norm=2.627228260040283, loss=1.4935314655303955
I0211 04:29:18.934754 139647431509760 logging_writer.py:48] [16200] global_step=16200, grad_norm=2.869176149368286, loss=1.5565640926361084
I0211 04:30:44.531141 139647423117056 logging_writer.py:48] [16300] global_step=16300, grad_norm=3.0083229541778564, loss=1.5652472972869873
I0211 04:32:08.416664 139647431509760 logging_writer.py:48] [16400] global_step=16400, grad_norm=2.240834951400757, loss=1.478258490562439
I0211 04:33:37.328399 139647431509760 logging_writer.py:48] [16500] global_step=16500, grad_norm=3.271113157272339, loss=1.5486456155776978
I0211 04:34:54.159137 139647423117056 logging_writer.py:48] [16600] global_step=16600, grad_norm=2.5161774158477783, loss=1.5217084884643555
I0211 04:36:10.606107 139647431509760 logging_writer.py:48] [16700] global_step=16700, grad_norm=2.7338428497314453, loss=1.4333175420761108
I0211 04:37:27.842974 139647423117056 logging_writer.py:48] [16800] global_step=16800, grad_norm=4.422771453857422, loss=1.5653245449066162
I0211 04:38:54.136317 139647431509760 logging_writer.py:48] [16900] global_step=16900, grad_norm=2.774292230606079, loss=1.5245723724365234
I0211 04:40:21.323455 139647423117056 logging_writer.py:48] [17000] global_step=17000, grad_norm=2.8885953426361084, loss=1.5923174619674683
I0211 04:41:47.244533 139647431509760 logging_writer.py:48] [17100] global_step=17100, grad_norm=3.0488524436950684, loss=1.5060019493103027
I0211 04:43:16.942619 139647423117056 logging_writer.py:48] [17200] global_step=17200, grad_norm=2.83622145652771, loss=1.6000216007232666
I0211 04:44:43.156212 139647431509760 logging_writer.py:48] [17300] global_step=17300, grad_norm=2.4253170490264893, loss=1.5736234188079834
I0211 04:46:08.957064 139647423117056 logging_writer.py:48] [17400] global_step=17400, grad_norm=3.37007737159729, loss=1.5549907684326172
I0211 04:46:12.226051 139803787056960 spec.py:321] Evaluating on the training split.
I0211 04:47:16.288013 139803787056960 spec.py:333] Evaluating on the validation split.
I0211 04:48:08.680607 139803787056960 spec.py:349] Evaluating on the test split.
I0211 04:48:35.994057 139803787056960 submission_runner.py:408] Time since start: 16042.09s, 	Step: 17405, 	{'train/ctc_loss': Array(0.306939, dtype=float32), 'train/wer': 0.10795475875087818, 'validation/ctc_loss': Array(0.6359483, dtype=float32), 'validation/wer': 0.18245363352867916, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.36800343, dtype=float32), 'test/wer': 0.1195945808705543, 'test/num_examples': 2472, 'score': 14447.619881868362, 'total_duration': 16042.089720010757, 'accumulated_submission_time': 14447.619881868362, 'accumulated_eval_time': 1593.3436903953552, 'accumulated_logging_time': 0.40529966354370117}
I0211 04:48:36.022676 139647431509760 logging_writer.py:48] [17405] accumulated_eval_time=1593.343690, accumulated_logging_time=0.405300, accumulated_submission_time=14447.619882, global_step=17405, preemption_count=0, score=14447.619882, test/ctc_loss=0.3680034279823303, test/num_examples=2472, test/wer=0.119595, total_duration=16042.089720, train/ctc_loss=0.3069390058517456, train/wer=0.107955, validation/ctc_loss=0.6359483003616333, validation/num_examples=5348, validation/wer=0.182454
I0211 04:49:48.750751 139647423117056 logging_writer.py:48] [17500] global_step=17500, grad_norm=2.99718976020813, loss=1.520827054977417
I0211 04:51:10.238441 139646776149760 logging_writer.py:48] [17600] global_step=17600, grad_norm=3.2735166549682617, loss=1.4665896892547607
I0211 04:52:27.940495 139646767757056 logging_writer.py:48] [17700] global_step=17700, grad_norm=2.2705771923065186, loss=1.559656023979187
I0211 04:53:46.526013 139646776149760 logging_writer.py:48] [17800] global_step=17800, grad_norm=3.2979722023010254, loss=1.5468947887420654
I0211 04:55:11.855876 139646767757056 logging_writer.py:48] [17900] global_step=17900, grad_norm=4.4218058586120605, loss=1.4700826406478882
I0211 04:56:40.310148 139646776149760 logging_writer.py:48] [18000] global_step=18000, grad_norm=3.0426907539367676, loss=1.5637813806533813
I0211 04:58:09.540214 139646767757056 logging_writer.py:48] [18100] global_step=18100, grad_norm=2.691981077194214, loss=1.5194220542907715
I0211 04:59:33.757367 139646776149760 logging_writer.py:48] [18200] global_step=18200, grad_norm=2.414426565170288, loss=1.490257978439331
I0211 05:00:59.960815 139646767757056 logging_writer.py:48] [18300] global_step=18300, grad_norm=3.1350648403167725, loss=1.5421546697616577
I0211 05:02:28.835451 139646776149760 logging_writer.py:48] [18400] global_step=18400, grad_norm=2.892510414123535, loss=1.540515422821045
I0211 05:03:56.940281 139646767757056 logging_writer.py:48] [18500] global_step=18500, grad_norm=4.265777587890625, loss=1.5437768697738647
I0211 05:05:18.164622 139646776149760 logging_writer.py:48] [18600] global_step=18600, grad_norm=1.8456997871398926, loss=1.5080167055130005
I0211 05:06:35.425162 139646767757056 logging_writer.py:48] [18700] global_step=18700, grad_norm=3.0571532249450684, loss=1.499295711517334
I0211 05:07:53.525452 139646776149760 logging_writer.py:48] [18800] global_step=18800, grad_norm=2.675809621810913, loss=1.458747148513794
I0211 05:09:16.021278 139646767757056 logging_writer.py:48] [18900] global_step=18900, grad_norm=2.725855588912964, loss=1.4508253335952759
I0211 05:10:43.291664 139646776149760 logging_writer.py:48] [19000] global_step=19000, grad_norm=2.570739984512329, loss=1.5254366397857666
I0211 05:12:08.656347 139646767757056 logging_writer.py:48] [19100] global_step=19100, grad_norm=2.8707165718078613, loss=1.5119845867156982
I0211 05:12:36.613334 139803787056960 spec.py:321] Evaluating on the training split.
I0211 05:13:31.616735 139803787056960 spec.py:333] Evaluating on the validation split.
I0211 05:14:24.093871 139803787056960 spec.py:349] Evaluating on the test split.
I0211 05:14:50.853848 139803787056960 submission_runner.py:408] Time since start: 17616.95s, 	Step: 19134, 	{'train/ctc_loss': Array(0.3205848, dtype=float32), 'train/wer': 0.10751738544118036, 'validation/ctc_loss': Array(0.61774206, dtype=float32), 'validation/wer': 0.17917105148826476, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.35935163, dtype=float32), 'test/wer': 0.11469949017935124, 'test/num_examples': 2472, 'score': 15888.127633094788, 'total_duration': 17616.94973897934, 'accumulated_submission_time': 15888.127633094788, 'accumulated_eval_time': 1727.5816078186035, 'accumulated_logging_time': 0.44574427604675293}
I0211 05:14:50.882304 139646776149760 logging_writer.py:48] [19134] accumulated_eval_time=1727.581608, accumulated_logging_time=0.445744, accumulated_submission_time=15888.127633, global_step=19134, preemption_count=0, score=15888.127633, test/ctc_loss=0.35935163497924805, test/num_examples=2472, test/wer=0.114699, total_duration=17616.949739, train/ctc_loss=0.32058480381965637, train/wer=0.107517, validation/ctc_loss=0.6177420616149902, validation/num_examples=5348, validation/wer=0.179171
I0211 05:15:41.441492 139646767757056 logging_writer.py:48] [19200] global_step=19200, grad_norm=4.015627861022949, loss=1.5027703046798706
I0211 05:16:57.636581 139646776149760 logging_writer.py:48] [19300] global_step=19300, grad_norm=3.015108346939087, loss=1.5214377641677856
I0211 05:18:14.422406 139646767757056 logging_writer.py:48] [19400] global_step=19400, grad_norm=2.9676315784454346, loss=1.509463906288147
I0211 05:19:35.382108 139646776149760 logging_writer.py:48] [19500] global_step=19500, grad_norm=3.0748345851898193, loss=1.4396131038665771
I0211 05:21:01.024253 139647431509760 logging_writer.py:48] [19600] global_step=19600, grad_norm=2.0954718589782715, loss=1.4905567169189453
I0211 05:22:17.252840 139647423117056 logging_writer.py:48] [19700] global_step=19700, grad_norm=2.61444354057312, loss=1.5313199758529663
I0211 05:23:36.306481 139647431509760 logging_writer.py:48] [19800] global_step=19800, grad_norm=2.1095151901245117, loss=1.4836407899856567
I0211 05:24:58.783886 139647423117056 logging_writer.py:48] [19900] global_step=19900, grad_norm=3.5639193058013916, loss=1.506018042564392
I0211 05:26:24.699113 139647431509760 logging_writer.py:48] [20000] global_step=20000, grad_norm=2.6912777423858643, loss=1.4832541942596436
I0211 05:27:53.342526 139647423117056 logging_writer.py:48] [20100] global_step=20100, grad_norm=1.8748618364334106, loss=1.4301495552062988
I0211 05:29:19.357925 139647431509760 logging_writer.py:48] [20200] global_step=20200, grad_norm=2.4433257579803467, loss=1.5356333255767822
I0211 05:30:45.606166 139647423117056 logging_writer.py:48] [20300] global_step=20300, grad_norm=3.260251998901367, loss=1.5197076797485352
I0211 05:32:14.584069 139647431509760 logging_writer.py:48] [20400] global_step=20400, grad_norm=2.9646294116973877, loss=1.4811689853668213
I0211 05:33:38.817863 139647423117056 logging_writer.py:48] [20500] global_step=20500, grad_norm=3.1394219398498535, loss=1.521657943725586
I0211 05:35:07.439306 139646776149760 logging_writer.py:48] [20600] global_step=20600, grad_norm=1.83290696144104, loss=1.496631145477295
I0211 05:36:24.959282 139646767757056 logging_writer.py:48] [20700] global_step=20700, grad_norm=2.7410967350006104, loss=1.4489498138427734
I0211 05:37:42.597700 139646776149760 logging_writer.py:48] [20800] global_step=20800, grad_norm=2.3645167350769043, loss=1.4409632682800293
I0211 05:38:51.745422 139803787056960 spec.py:321] Evaluating on the training split.
I0211 05:39:46.471510 139803787056960 spec.py:333] Evaluating on the validation split.
I0211 05:40:38.859164 139803787056960 spec.py:349] Evaluating on the test split.
I0211 05:41:05.760877 139803787056960 submission_runner.py:408] Time since start: 19191.86s, 	Step: 20886, 	{'train/ctc_loss': Array(0.31008214, dtype=float32), 'train/wer': 0.10548975123328146, 'validation/ctc_loss': Array(0.5979301, dtype=float32), 'validation/wer': 0.1724417583054153, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.34600043, dtype=float32), 'test/wer': 0.11216054272540775, 'test/num_examples': 2472, 'score': 17328.907950878143, 'total_duration': 19191.85638141632, 'accumulated_submission_time': 17328.907950878143, 'accumulated_eval_time': 1861.594043970108, 'accumulated_logging_time': 0.4855766296386719}
I0211 05:41:05.789841 139646776149760 logging_writer.py:48] [20886] accumulated_eval_time=1861.594044, accumulated_logging_time=0.485577, accumulated_submission_time=17328.907951, global_step=20886, preemption_count=0, score=17328.907951, test/ctc_loss=0.34600043296813965, test/num_examples=2472, test/wer=0.112161, total_duration=19191.856381, train/ctc_loss=0.3100821375846863, train/wer=0.105490, validation/ctc_loss=0.5979300737380981, validation/num_examples=5348, validation/wer=0.172442
I0211 05:41:17.318290 139646767757056 logging_writer.py:48] [20900] global_step=20900, grad_norm=2.446469306945801, loss=1.4682562351226807
I0211 05:42:32.570167 139646776149760 logging_writer.py:48] [21000] global_step=21000, grad_norm=3.0023515224456787, loss=1.4770188331604004
I0211 05:43:48.839510 139646767757056 logging_writer.py:48] [21100] global_step=21100, grad_norm=2.8982021808624268, loss=1.4546304941177368
I0211 05:45:15.038590 139646776149760 logging_writer.py:48] [21200] global_step=21200, grad_norm=3.3637988567352295, loss=1.4523389339447021
I0211 05:46:42.136116 139646767757056 logging_writer.py:48] [21300] global_step=21300, grad_norm=2.3119139671325684, loss=1.3912907838821411
I0211 05:48:10.283100 139646776149760 logging_writer.py:48] [21400] global_step=21400, grad_norm=2.7726566791534424, loss=1.4624779224395752
I0211 05:49:41.773275 139646767757056 logging_writer.py:48] [21500] global_step=21500, grad_norm=3.6177587509155273, loss=1.4896870851516724
I0211 05:51:05.808721 139646776149760 logging_writer.py:48] [21600] global_step=21600, grad_norm=3.610952377319336, loss=1.513980507850647
I0211 05:52:29.393486 139646776149760 logging_writer.py:48] [21700] global_step=21700, grad_norm=2.5225446224212646, loss=1.474308729171753
I0211 05:53:49.449871 139646767757056 logging_writer.py:48] [21800] global_step=21800, grad_norm=2.603407382965088, loss=1.4512747526168823
I0211 05:55:10.949959 139646776149760 logging_writer.py:48] [21900] global_step=21900, grad_norm=2.2070975303649902, loss=1.4388184547424316
I0211 05:56:30.599330 139646767757056 logging_writer.py:48] [22000] global_step=22000, grad_norm=4.0136518478393555, loss=1.430403232574463
I0211 05:57:55.794905 139646776149760 logging_writer.py:48] [22100] global_step=22100, grad_norm=2.177917003631592, loss=1.466399073600769
I0211 05:59:25.549680 139646767757056 logging_writer.py:48] [22200] global_step=22200, grad_norm=2.790142059326172, loss=1.4819881916046143
I0211 06:00:52.446450 139646776149760 logging_writer.py:48] [22300] global_step=22300, grad_norm=2.240314483642578, loss=1.4948093891143799
I0211 06:02:19.343466 139646767757056 logging_writer.py:48] [22400] global_step=22400, grad_norm=3.9575490951538086, loss=1.4315714836120605
I0211 06:03:50.641938 139646776149760 logging_writer.py:48] [22500] global_step=22500, grad_norm=2.6749467849731445, loss=1.5156471729278564
I0211 06:05:06.080744 139803787056960 spec.py:321] Evaluating on the training split.
I0211 06:06:01.672183 139803787056960 spec.py:333] Evaluating on the validation split.
I0211 06:06:53.548115 139803787056960 spec.py:349] Evaluating on the test split.
I0211 06:07:20.385867 139803787056960 submission_runner.py:408] Time since start: 20766.48s, 	Step: 22583, 	{'train/ctc_loss': Array(0.28917006, dtype=float32), 'train/wer': 0.09840518571869535, 'validation/ctc_loss': Array(0.57588047, dtype=float32), 'validation/wer': 0.1671896270407523, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33355513, dtype=float32), 'test/wer': 0.10809822679909817, 'test/num_examples': 2472, 'score': 18769.116664409637, 'total_duration': 20766.481579065323, 'accumulated_submission_time': 18769.116664409637, 'accumulated_eval_time': 1995.896348953247, 'accumulated_logging_time': 0.5268707275390625}
I0211 06:07:20.416337 139646776149760 logging_writer.py:48] [22583] accumulated_eval_time=1995.896349, accumulated_logging_time=0.526871, accumulated_submission_time=18769.116664, global_step=22583, preemption_count=0, score=18769.116664, test/ctc_loss=0.33355513215065, test/num_examples=2472, test/wer=0.108098, total_duration=20766.481579, train/ctc_loss=0.2891700565814972, train/wer=0.098405, validation/ctc_loss=0.5758804678916931, validation/num_examples=5348, validation/wer=0.167190
I0211 06:07:34.377291 139646767757056 logging_writer.py:48] [22600] global_step=22600, grad_norm=3.1683382987976074, loss=1.4853595495224
I0211 06:08:56.025057 139646776149760 logging_writer.py:48] [22700] global_step=22700, grad_norm=2.0160555839538574, loss=1.3742470741271973
I0211 06:10:15.524235 139646767757056 logging_writer.py:48] [22800] global_step=22800, grad_norm=3.138728380203247, loss=1.4141147136688232
I0211 06:11:36.518119 139646776149760 logging_writer.py:48] [22900] global_step=22900, grad_norm=2.281850814819336, loss=1.4958312511444092
I0211 06:12:57.112132 139646767757056 logging_writer.py:48] [23000] global_step=23000, grad_norm=4.4308390617370605, loss=1.4740896224975586
I0211 06:14:25.690384 139646776149760 logging_writer.py:48] [23100] global_step=23100, grad_norm=2.130237579345703, loss=1.3956141471862793
I0211 06:15:51.993777 139646767757056 logging_writer.py:48] [23200] global_step=23200, grad_norm=3.0975778102874756, loss=1.4904013872146606
I0211 06:17:21.198360 139646776149760 logging_writer.py:48] [23300] global_step=23300, grad_norm=2.2168116569519043, loss=1.4393730163574219
I0211 06:18:48.565468 139646767757056 logging_writer.py:48] [23400] global_step=23400, grad_norm=3.3392155170440674, loss=1.4835880994796753
I0211 06:20:17.582640 139646776149760 logging_writer.py:48] [23500] global_step=23500, grad_norm=2.576129913330078, loss=1.4540222883224487
I0211 06:21:45.775446 139646767757056 logging_writer.py:48] [23600] global_step=23600, grad_norm=2.6096160411834717, loss=1.4010202884674072
I0211 06:23:17.999022 139647431509760 logging_writer.py:48] [23700] global_step=23700, grad_norm=2.671372175216675, loss=1.3797332048416138
I0211 06:24:37.534773 139647423117056 logging_writer.py:48] [23800] global_step=23800, grad_norm=2.9352500438690186, loss=1.4190951585769653
I0211 06:25:56.267372 139647431509760 logging_writer.py:48] [23900] global_step=23900, grad_norm=2.8093178272247314, loss=1.4804340600967407
I0211 06:27:18.214782 139647423117056 logging_writer.py:48] [24000] global_step=24000, grad_norm=2.2775535583496094, loss=1.4737082719802856
I0211 06:28:44.341365 139647431509760 logging_writer.py:48] [24100] global_step=24100, grad_norm=3.391479253768921, loss=1.4131652116775513
I0211 06:30:07.561383 139647423117056 logging_writer.py:48] [24200] global_step=24200, grad_norm=3.032845973968506, loss=1.432965874671936
I0211 06:31:20.859436 139803787056960 spec.py:321] Evaluating on the training split.
I0211 06:32:25.200421 139803787056960 spec.py:333] Evaluating on the validation split.
I0211 06:33:18.188476 139803787056960 spec.py:349] Evaluating on the test split.
I0211 06:33:45.949981 139803787056960 submission_runner.py:408] Time since start: 22352.05s, 	Step: 24285, 	{'train/ctc_loss': Array(0.27114815, dtype=float32), 'train/wer': 0.09292787466745492, 'validation/ctc_loss': Array(0.5687677, dtype=float32), 'validation/wer': 0.16331811116367534, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32747313, dtype=float32), 'test/wer': 0.10639205411004814, 'test/num_examples': 2472, 'score': 20209.478974580765, 'total_duration': 22352.04579281807, 'accumulated_submission_time': 20209.478974580765, 'accumulated_eval_time': 2140.9842009544373, 'accumulated_logging_time': 0.5696592330932617}
I0211 06:33:45.980492 139647431509760 logging_writer.py:48] [24285] accumulated_eval_time=2140.984201, accumulated_logging_time=0.569659, accumulated_submission_time=20209.478975, global_step=24285, preemption_count=0, score=20209.478975, test/ctc_loss=0.32747313380241394, test/num_examples=2472, test/wer=0.106392, total_duration=22352.045793, train/ctc_loss=0.271148145198822, train/wer=0.092928, validation/ctc_loss=0.5687677264213562, validation/num_examples=5348, validation/wer=0.163318
I0211 06:33:57.991943 139647423117056 logging_writer.py:48] [24300] global_step=24300, grad_norm=2.302245855331421, loss=1.3702278137207031
I0211 06:35:14.057212 139647431509760 logging_writer.py:48] [24400] global_step=24400, grad_norm=4.7450714111328125, loss=1.4783908128738403
I0211 06:36:30.105491 139647423117056 logging_writer.py:48] [24500] global_step=24500, grad_norm=2.651883602142334, loss=1.3578935861587524
I0211 06:37:56.157769 139647431509760 logging_writer.py:48] [24600] global_step=24600, grad_norm=3.05206298828125, loss=1.4134538173675537
I0211 06:39:25.225679 139647423117056 logging_writer.py:48] [24700] global_step=24700, grad_norm=2.45392107963562, loss=1.486385703086853
I0211 06:40:47.316085 139647431509760 logging_writer.py:48] [24800] global_step=24800, grad_norm=3.3732802867889404, loss=1.4554775953292847
I0211 06:42:07.965677 139647423117056 logging_writer.py:48] [24900] global_step=24900, grad_norm=2.678985834121704, loss=1.437308669090271
I0211 06:43:26.125436 139647431509760 logging_writer.py:48] [25000] global_step=25000, grad_norm=2.8972291946411133, loss=1.4239788055419922
I0211 06:44:46.249525 139647423117056 logging_writer.py:48] [25100] global_step=25100, grad_norm=3.122499942779541, loss=1.4125629663467407
I0211 06:46:12.930123 139647431509760 logging_writer.py:48] [25200] global_step=25200, grad_norm=3.2985403537750244, loss=1.424086570739746
I0211 06:47:36.439768 139647423117056 logging_writer.py:48] [25300] global_step=25300, grad_norm=2.085707664489746, loss=1.3704473972320557
I0211 06:49:04.824615 139647431509760 logging_writer.py:48] [25400] global_step=25400, grad_norm=2.448457717895508, loss=1.403536319732666
I0211 06:50:34.263175 139647423117056 logging_writer.py:48] [25500] global_step=25500, grad_norm=3.8367578983306885, loss=1.4104639291763306
I0211 06:52:04.227694 139647431509760 logging_writer.py:48] [25600] global_step=25600, grad_norm=2.999727249145508, loss=1.4298955202102661
I0211 06:53:32.144822 139647423117056 logging_writer.py:48] [25700] global_step=25700, grad_norm=3.130363702774048, loss=1.407050371170044
I0211 06:54:57.280599 139646776149760 logging_writer.py:48] [25800] global_step=25800, grad_norm=3.940199613571167, loss=1.4162307977676392
I0211 06:56:15.441071 139646767757056 logging_writer.py:48] [25900] global_step=25900, grad_norm=2.6197869777679443, loss=1.4049605131149292
I0211 06:57:33.468260 139646776149760 logging_writer.py:48] [26000] global_step=26000, grad_norm=2.1472268104553223, loss=1.4115468263626099
I0211 06:57:46.024446 139803787056960 spec.py:321] Evaluating on the training split.
I0211 06:58:45.473294 139803787056960 spec.py:333] Evaluating on the validation split.
I0211 06:59:37.003985 139803787056960 spec.py:349] Evaluating on the test split.
I0211 07:00:04.400172 139803787056960 submission_runner.py:408] Time since start: 23930.50s, 	Step: 26018, 	{'train/ctc_loss': Array(0.24425977, dtype=float32), 'train/wer': 0.0852656563812306, 'validation/ctc_loss': Array(0.5455414, dtype=float32), 'validation/wer': 0.15750601002152986, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31358135, dtype=float32), 'test/wer': 0.10033920337984685, 'test/num_examples': 2472, 'score': 21649.439897060394, 'total_duration': 23930.495884418488, 'accumulated_submission_time': 21649.439897060394, 'accumulated_eval_time': 2279.357107400894, 'accumulated_logging_time': 0.6119377613067627}
I0211 07:00:04.431789 139647585109760 logging_writer.py:48] [26018] accumulated_eval_time=2279.357107, accumulated_logging_time=0.611938, accumulated_submission_time=21649.439897, global_step=26018, preemption_count=0, score=21649.439897, test/ctc_loss=0.31358134746551514, test/num_examples=2472, test/wer=0.100339, total_duration=23930.495884, train/ctc_loss=0.244259774684906, train/wer=0.085266, validation/ctc_loss=0.5455414056777954, validation/num_examples=5348, validation/wer=0.157506
I0211 07:01:07.464929 139647576717056 logging_writer.py:48] [26100] global_step=26100, grad_norm=4.723387718200684, loss=1.4209716320037842
I0211 07:02:22.537309 139647585109760 logging_writer.py:48] [26200] global_step=26200, grad_norm=3.031304121017456, loss=1.3974207639694214
I0211 07:03:39.934711 139647576717056 logging_writer.py:48] [26300] global_step=26300, grad_norm=2.6678335666656494, loss=1.316620945930481
I0211 07:05:08.670072 139647585109760 logging_writer.py:48] [26400] global_step=26400, grad_norm=3.2163281440734863, loss=1.4016257524490356
I0211 07:06:37.822466 139647576717056 logging_writer.py:48] [26500] global_step=26500, grad_norm=3.104677438735962, loss=1.3914183378219604
I0211 07:08:05.906632 139647585109760 logging_writer.py:48] [26600] global_step=26600, grad_norm=4.161975383758545, loss=1.3917652368545532
I0211 07:09:35.574498 139647576717056 logging_writer.py:48] [26700] global_step=26700, grad_norm=3.022969961166382, loss=1.3478986024856567
I0211 07:11:04.791883 139646929749760 logging_writer.py:48] [26800] global_step=26800, grad_norm=3.228734016418457, loss=1.3699469566345215
I0211 07:12:23.107468 139646921357056 logging_writer.py:48] [26900] global_step=26900, grad_norm=4.020666599273682, loss=1.3905909061431885
I0211 07:13:40.805930 139646929749760 logging_writer.py:48] [27000] global_step=27000, grad_norm=5.030369758605957, loss=1.3886692523956299
I0211 07:15:01.193855 139646921357056 logging_writer.py:48] [27100] global_step=27100, grad_norm=2.0822207927703857, loss=1.374711036682129
I0211 07:16:26.650454 139646929749760 logging_writer.py:48] [27200] global_step=27200, grad_norm=2.33028244972229, loss=1.3813329935073853
I0211 07:17:54.646843 139646921357056 logging_writer.py:48] [27300] global_step=27300, grad_norm=2.296567678451538, loss=1.3862473964691162
I0211 07:19:17.639283 139646929749760 logging_writer.py:48] [27400] global_step=27400, grad_norm=3.380387306213379, loss=1.3679242134094238
I0211 07:20:44.284678 139646921357056 logging_writer.py:48] [27500] global_step=27500, grad_norm=2.5197653770446777, loss=1.3715475797653198
I0211 07:22:08.934047 139646929749760 logging_writer.py:48] [27600] global_step=27600, grad_norm=2.1832683086395264, loss=1.297451376914978
I0211 07:23:36.262815 139646921357056 logging_writer.py:48] [27700] global_step=27700, grad_norm=2.3404879570007324, loss=1.3829153776168823
I0211 07:24:04.689959 139803787056960 spec.py:321] Evaluating on the training split.
I0211 07:25:07.121478 139803787056960 spec.py:333] Evaluating on the validation split.
I0211 07:25:59.890069 139803787056960 spec.py:349] Evaluating on the test split.
I0211 07:26:25.755331 139803787056960 submission_runner.py:408] Time since start: 25511.85s, 	Step: 27733, 	{'train/ctc_loss': Array(0.23917654, dtype=float32), 'train/wer': 0.08220740301316341, 'validation/ctc_loss': Array(0.533727, dtype=float32), 'validation/wer': 0.15428135589947575, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30254042, dtype=float32), 'test/wer': 0.09863303069079682, 'test/num_examples': 2472, 'score': 23089.615846395493, 'total_duration': 25511.851219892502, 'accumulated_submission_time': 23089.615846395493, 'accumulated_eval_time': 2420.4198529720306, 'accumulated_logging_time': 0.6570084095001221}
I0211 07:26:25.784173 139648015189760 logging_writer.py:48] [27733] accumulated_eval_time=2420.419853, accumulated_logging_time=0.657008, accumulated_submission_time=23089.615846, global_step=27733, preemption_count=0, score=23089.615846, test/ctc_loss=0.3025404214859009, test/num_examples=2472, test/wer=0.098633, total_duration=25511.851220, train/ctc_loss=0.23917654156684875, train/wer=0.082207, validation/ctc_loss=0.5337269902229309, validation/num_examples=5348, validation/wer=0.154281
I0211 07:27:17.604387 139648006797056 logging_writer.py:48] [27800] global_step=27800, grad_norm=2.5711891651153564, loss=1.350386381149292
I0211 07:28:38.248410 139648015189760 logging_writer.py:48] [27900] global_step=27900, grad_norm=1.7657114267349243, loss=1.3236987590789795
I0211 07:29:54.941580 139648006797056 logging_writer.py:48] [28000] global_step=28000, grad_norm=2.270087242126465, loss=1.3171172142028809
I0211 07:31:15.459210 139648015189760 logging_writer.py:48] [28100] global_step=28100, grad_norm=3.087554931640625, loss=1.3544121980667114
I0211 07:32:39.652062 139648006797056 logging_writer.py:48] [28200] global_step=28200, grad_norm=2.2832374572753906, loss=1.3338096141815186
I0211 07:34:05.127533 139648015189760 logging_writer.py:48] [28300] global_step=28300, grad_norm=2.7353174686431885, loss=1.298649787902832
I0211 07:35:31.309002 139648006797056 logging_writer.py:48] [28400] global_step=28400, grad_norm=3.354595422744751, loss=1.3149125576019287
I0211 07:37:01.620115 139648015189760 logging_writer.py:48] [28500] global_step=28500, grad_norm=4.3120293617248535, loss=1.3729338645935059
I0211 07:38:29.525359 139648006797056 logging_writer.py:48] [28600] global_step=28600, grad_norm=3.042825698852539, loss=1.345802664756775
I0211 07:39:54.861024 139648015189760 logging_writer.py:48] [28700] global_step=28700, grad_norm=2.031658887863159, loss=1.3901115655899048
I0211 07:41:23.426515 139648006797056 logging_writer.py:48] [28800] global_step=28800, grad_norm=3.5846219062805176, loss=1.4075300693511963
I0211 07:42:47.518374 139648015189760 logging_writer.py:48] [28900] global_step=28900, grad_norm=2.241325616836548, loss=1.3293535709381104
I0211 07:44:04.685184 139648006797056 logging_writer.py:48] [29000] global_step=29000, grad_norm=3.5273332595825195, loss=1.3511072397232056
I0211 07:45:24.750427 139648015189760 logging_writer.py:48] [29100] global_step=29100, grad_norm=2.175257921218872, loss=1.3307995796203613
I0211 07:46:49.300000 139648006797056 logging_writer.py:48] [29200] global_step=29200, grad_norm=2.229468584060669, loss=1.3194772005081177
I0211 07:48:12.245725 139648015189760 logging_writer.py:48] [29300] global_step=29300, grad_norm=4.011227607727051, loss=1.357275128364563
I0211 07:49:40.149066 139648006797056 logging_writer.py:48] [29400] global_step=29400, grad_norm=2.2987136840820312, loss=1.3211966753005981
I0211 07:50:26.375385 139803787056960 spec.py:321] Evaluating on the training split.
I0211 07:51:29.832849 139803787056960 spec.py:333] Evaluating on the validation split.
I0211 07:52:23.147557 139803787056960 spec.py:349] Evaluating on the test split.
I0211 07:52:48.939321 139803787056960 submission_runner.py:408] Time since start: 27095.03s, 	Step: 29456, 	{'train/ctc_loss': Array(0.25039864, dtype=float32), 'train/wer': 0.08649137632171953, 'validation/ctc_loss': Array(0.51835567, dtype=float32), 'validation/wer': 0.14964712243065545, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29362866, dtype=float32), 'test/wer': 0.09568785164422237, 'test/num_examples': 2472, 'score': 24530.126829862595, 'total_duration': 27095.03487610817, 'accumulated_submission_time': 24530.126829862595, 'accumulated_eval_time': 2562.9808316230774, 'accumulated_logging_time': 0.6976122856140137}
I0211 07:52:48.972058 139648015189760 logging_writer.py:48] [29456] accumulated_eval_time=2562.980832, accumulated_logging_time=0.697612, accumulated_submission_time=24530.126830, global_step=29456, preemption_count=0, score=24530.126830, test/ctc_loss=0.29362866282463074, test/num_examples=2472, test/wer=0.095688, total_duration=27095.034876, train/ctc_loss=0.2503986358642578, train/wer=0.086491, validation/ctc_loss=0.518355667591095, validation/num_examples=5348, validation/wer=0.149647
I0211 07:53:23.061421 139648006797056 logging_writer.py:48] [29500] global_step=29500, grad_norm=2.83525013923645, loss=1.415220022201538
I0211 07:54:38.652773 139648015189760 logging_writer.py:48] [29600] global_step=29600, grad_norm=2.4746739864349365, loss=1.4098069667816162
I0211 07:55:53.635322 139648006797056 logging_writer.py:48] [29700] global_step=29700, grad_norm=4.190944194793701, loss=1.3080767393112183
I0211 07:57:17.161505 139648015189760 logging_writer.py:48] [29800] global_step=29800, grad_norm=4.383731365203857, loss=1.3164130449295044
I0211 07:58:47.151295 139648015189760 logging_writer.py:48] [29900] global_step=29900, grad_norm=2.7107062339782715, loss=1.308430552482605
I0211 08:00:04.412310 139648006797056 logging_writer.py:48] [30000] global_step=30000, grad_norm=3.4005448818206787, loss=1.3597012758255005
I0211 08:01:22.366470 139648015189760 logging_writer.py:48] [30100] global_step=30100, grad_norm=1.68476140499115, loss=1.2799854278564453
I0211 08:02:42.477991 139648006797056 logging_writer.py:48] [30200] global_step=30200, grad_norm=1.9680358171463013, loss=1.3564941883087158
I0211 08:04:05.612455 139648015189760 logging_writer.py:48] [30300] global_step=30300, grad_norm=3.0851223468780518, loss=1.2710678577423096
I0211 08:05:35.057533 139648006797056 logging_writer.py:48] [30400] global_step=30400, grad_norm=4.823398590087891, loss=1.3539307117462158
I0211 08:07:03.265206 139648015189760 logging_writer.py:48] [30500] global_step=30500, grad_norm=3.2557809352874756, loss=1.2751822471618652
I0211 08:08:29.230242 139648006797056 logging_writer.py:48] [30600] global_step=30600, grad_norm=1.8445372581481934, loss=1.365497350692749
I0211 08:09:59.883042 139648015189760 logging_writer.py:48] [30700] global_step=30700, grad_norm=2.3109195232391357, loss=1.3158965110778809
I0211 08:11:26.887201 139648006797056 logging_writer.py:48] [30800] global_step=30800, grad_norm=2.8567423820495605, loss=1.3408081531524658
I0211 08:12:57.675596 139648015189760 logging_writer.py:48] [30900] global_step=30900, grad_norm=3.4248530864715576, loss=1.3059250116348267
I0211 08:14:15.702382 139648006797056 logging_writer.py:48] [31000] global_step=31000, grad_norm=3.3598620891571045, loss=1.3261494636535645
I0211 08:15:36.343380 139648015189760 logging_writer.py:48] [31100] global_step=31100, grad_norm=2.164987087249756, loss=1.283035397529602
I0211 08:16:49.203892 139803787056960 spec.py:321] Evaluating on the training split.
I0211 08:17:51.139381 139803787056960 spec.py:333] Evaluating on the validation split.
I0211 08:18:43.167545 139803787056960 spec.py:349] Evaluating on the test split.
I0211 08:19:09.073482 139803787056960 submission_runner.py:408] Time since start: 28675.17s, 	Step: 31194, 	{'train/ctc_loss': Array(0.22763942, dtype=float32), 'train/wer': 0.07687507799825284, 'validation/ctc_loss': Array(0.5053736, dtype=float32), 'validation/wer': 0.14656728810450195, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28454825, dtype=float32), 'test/wer': 0.09156460097901814, 'test/num_examples': 2472, 'score': 25970.27191734314, 'total_duration': 28675.169151067734, 'accumulated_submission_time': 25970.27191734314, 'accumulated_eval_time': 2702.8475642204285, 'accumulated_logging_time': 0.7462007999420166}
I0211 08:19:09.105383 139648015189760 logging_writer.py:48] [31194] accumulated_eval_time=2702.847564, accumulated_logging_time=0.746201, accumulated_submission_time=25970.271917, global_step=31194, preemption_count=0, score=25970.271917, test/ctc_loss=0.28454825282096863, test/num_examples=2472, test/wer=0.091565, total_duration=28675.169151, train/ctc_loss=0.22763942182064056, train/wer=0.076875, validation/ctc_loss=0.5053735971450806, validation/num_examples=5348, validation/wer=0.146567
I0211 08:19:14.507810 139648006797056 logging_writer.py:48] [31200] global_step=31200, grad_norm=2.346144437789917, loss=1.3536746501922607
I0211 08:20:29.646314 139648015189760 logging_writer.py:48] [31300] global_step=31300, grad_norm=2.821977376937866, loss=1.2717020511627197
I0211 08:21:45.275238 139648006797056 logging_writer.py:48] [31400] global_step=31400, grad_norm=2.2333273887634277, loss=1.2782008647918701
I0211 08:23:11.834321 139648015189760 logging_writer.py:48] [31500] global_step=31500, grad_norm=3.5145552158355713, loss=1.3358466625213623
I0211 08:24:40.779458 139648006797056 logging_writer.py:48] [31600] global_step=31600, grad_norm=2.241168975830078, loss=1.3500088453292847
I0211 08:26:08.379435 139648015189760 logging_writer.py:48] [31700] global_step=31700, grad_norm=3.001166582107544, loss=1.3264013528823853
I0211 08:27:34.704939 139648006797056 logging_writer.py:48] [31800] global_step=31800, grad_norm=2.63181209564209, loss=1.191532015800476
I0211 08:29:03.959722 139648015189760 logging_writer.py:48] [31900] global_step=31900, grad_norm=3.7510733604431152, loss=1.3356709480285645
I0211 08:30:26.588749 139648015189760 logging_writer.py:48] [32000] global_step=32000, grad_norm=2.4920647144317627, loss=1.2828285694122314
I0211 08:31:43.265686 139648006797056 logging_writer.py:48] [32100] global_step=32100, grad_norm=3.6276395320892334, loss=1.2772369384765625
I0211 08:33:01.942381 139648015189760 logging_writer.py:48] [32200] global_step=32200, grad_norm=2.232475757598877, loss=1.2373591661453247
I0211 08:34:18.915776 139648006797056 logging_writer.py:48] [32300] global_step=32300, grad_norm=3.7090084552764893, loss=1.2577167749404907
I0211 08:35:44.643569 139648015189760 logging_writer.py:48] [32400] global_step=32400, grad_norm=2.56779146194458, loss=1.2733970880508423
I0211 08:37:11.393852 139648006797056 logging_writer.py:48] [32500] global_step=32500, grad_norm=2.376363754272461, loss=1.2443084716796875
I0211 08:38:38.775437 139648015189760 logging_writer.py:48] [32600] global_step=32600, grad_norm=2.543330192565918, loss=1.3270188570022583
I0211 08:40:06.762318 139648006797056 logging_writer.py:48] [32700] global_step=32700, grad_norm=2.770979166030884, loss=1.3631446361541748
I0211 08:41:34.982430 139648015189760 logging_writer.py:48] [32800] global_step=32800, grad_norm=3.5626633167266846, loss=1.3028651475906372
I0211 08:43:00.212591 139648006797056 logging_writer.py:48] [32900] global_step=32900, grad_norm=2.6052653789520264, loss=1.3202569484710693
I0211 08:43:09.703831 139803787056960 spec.py:321] Evaluating on the training split.
I0211 08:44:05.517495 139803787056960 spec.py:333] Evaluating on the validation split.
I0211 08:44:57.565764 139803787056960 spec.py:349] Evaluating on the test split.
I0211 08:45:25.094798 139803787056960 submission_runner.py:408] Time since start: 30251.19s, 	Step: 32913, 	{'train/ctc_loss': Array(0.2309214, dtype=float32), 'train/wer': 0.07894939615449814, 'validation/ctc_loss': Array(0.49034488, dtype=float32), 'validation/wer': 0.1433522886355079, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27736688, dtype=float32), 'test/wer': 0.08959437775475798, 'test/num_examples': 2472, 'score': 27410.788177251816, 'total_duration': 30251.190588474274, 'accumulated_submission_time': 27410.788177251816, 'accumulated_eval_time': 2838.235775709152, 'accumulated_logging_time': 0.7904810905456543}
I0211 08:45:25.126863 139648015189760 logging_writer.py:48] [32913] accumulated_eval_time=2838.235776, accumulated_logging_time=0.790481, accumulated_submission_time=27410.788177, global_step=32913, preemption_count=0, score=27410.788177, test/ctc_loss=0.27736687660217285, test/num_examples=2472, test/wer=0.089594, total_duration=30251.190588, train/ctc_loss=0.2309214025735855, train/wer=0.078949, validation/ctc_loss=0.4903448820114136, validation/num_examples=5348, validation/wer=0.143352
I0211 08:46:38.101873 139648015189760 logging_writer.py:48] [33000] global_step=33000, grad_norm=3.3115034103393555, loss=1.3034392595291138
I0211 08:47:55.717232 139648006797056 logging_writer.py:48] [33100] global_step=33100, grad_norm=3.3274309635162354, loss=1.2677295207977295
I0211 08:49:15.360983 139648015189760 logging_writer.py:48] [33200] global_step=33200, grad_norm=2.8896701335906982, loss=1.2675538063049316
I0211 08:50:35.846550 139648006797056 logging_writer.py:48] [33300] global_step=33300, grad_norm=3.824125051498413, loss=1.2872662544250488
I0211 08:52:02.529055 139648015189760 logging_writer.py:48] [33400] global_step=33400, grad_norm=1.8282701969146729, loss=1.2448804378509521
I0211 08:53:28.962949 139648006797056 logging_writer.py:48] [33500] global_step=33500, grad_norm=3.709430694580078, loss=1.2970596551895142
I0211 08:54:58.168744 139648015189760 logging_writer.py:48] [33600] global_step=33600, grad_norm=1.9770487546920776, loss=1.2734038829803467
I0211 08:56:28.248174 139648006797056 logging_writer.py:48] [33700] global_step=33700, grad_norm=2.81101393699646, loss=1.2653034925460815
I0211 08:57:56.619728 139648015189760 logging_writer.py:48] [33800] global_step=33800, grad_norm=2.5607364177703857, loss=1.2248390913009644
I0211 08:59:24.916508 139648006797056 logging_writer.py:48] [33900] global_step=33900, grad_norm=3.0440735816955566, loss=1.3258405923843384
I0211 09:00:57.580754 139648015189760 logging_writer.py:48] [34000] global_step=34000, grad_norm=2.341054677963257, loss=1.2878053188323975
I0211 09:02:14.002360 139648006797056 logging_writer.py:48] [34100] global_step=34100, grad_norm=4.446737289428711, loss=1.2920451164245605
I0211 09:03:33.670930 139648015189760 logging_writer.py:48] [34200] global_step=34200, grad_norm=2.2034740447998047, loss=1.2558449506759644
I0211 09:04:52.277115 139648006797056 logging_writer.py:48] [34300] global_step=34300, grad_norm=2.392671585083008, loss=1.258901834487915
I0211 09:06:14.891542 139648015189760 logging_writer.py:48] [34400] global_step=34400, grad_norm=2.2678792476654053, loss=1.2759196758270264
I0211 09:07:45.674494 139648006797056 logging_writer.py:48] [34500] global_step=34500, grad_norm=4.749729633331299, loss=1.2303037643432617
I0211 09:09:15.724663 139648015189760 logging_writer.py:48] [34600] global_step=34600, grad_norm=4.309546947479248, loss=1.2760858535766602
I0211 09:09:26.330583 139803787056960 spec.py:321] Evaluating on the training split.
I0211 09:10:26.481310 139803787056960 spec.py:333] Evaluating on the validation split.
I0211 09:11:18.455650 139803787056960 spec.py:349] Evaluating on the test split.
I0211 09:11:44.350698 139803787056960 submission_runner.py:408] Time since start: 31830.45s, 	Step: 34613, 	{'train/ctc_loss': Array(0.21600437, dtype=float32), 'train/wer': 0.07201317177659132, 'validation/ctc_loss': Array(0.47284636, dtype=float32), 'validation/wer': 0.13711538275872057, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2623799, dtype=float32), 'test/wer': 0.08478053338208112, 'test/num_examples': 2472, 'score': 28851.9081697464, 'total_duration': 31830.446347236633, 'accumulated_submission_time': 28851.9081697464, 'accumulated_eval_time': 2976.252999305725, 'accumulated_logging_time': 0.8371679782867432}
I0211 09:11:44.380056 139648015189760 logging_writer.py:48] [34613] accumulated_eval_time=2976.252999, accumulated_logging_time=0.837168, accumulated_submission_time=28851.908170, global_step=34613, preemption_count=0, score=28851.908170, test/ctc_loss=0.262379914522171, test/num_examples=2472, test/wer=0.084781, total_duration=31830.446347, train/ctc_loss=0.2160043716430664, train/wer=0.072013, validation/ctc_loss=0.4728463590145111, validation/num_examples=5348, validation/wer=0.137115
I0211 09:12:50.861369 139648006797056 logging_writer.py:48] [34700] global_step=34700, grad_norm=2.356289863586426, loss=1.2211495637893677
I0211 09:14:06.084403 139648015189760 logging_writer.py:48] [34800] global_step=34800, grad_norm=3.2783262729644775, loss=1.2821731567382812
I0211 09:15:27.849941 139648006797056 logging_writer.py:48] [34900] global_step=34900, grad_norm=2.7683050632476807, loss=1.245308756828308
I0211 09:16:55.269981 139648015189760 logging_writer.py:48] [35000] global_step=35000, grad_norm=2.6737945079803467, loss=1.2303065061569214
I0211 09:18:18.210964 139648015189760 logging_writer.py:48] [35100] global_step=35100, grad_norm=2.1563572883605957, loss=1.2123051881790161
I0211 09:19:38.019115 139648006797056 logging_writer.py:48] [35200] global_step=35200, grad_norm=2.8522729873657227, loss=1.2504743337631226
I0211 09:20:57.065300 139648015189760 logging_writer.py:48] [35300] global_step=35300, grad_norm=4.129001617431641, loss=1.3075001239776611
I0211 09:22:19.084367 139648006797056 logging_writer.py:48] [35400] global_step=35400, grad_norm=2.5969080924987793, loss=1.2178916931152344
I0211 09:23:44.168005 139648015189760 logging_writer.py:48] [35500] global_step=35500, grad_norm=3.1638975143432617, loss=1.2644954919815063
I0211 09:25:14.678990 139648006797056 logging_writer.py:48] [35600] global_step=35600, grad_norm=2.663975238800049, loss=1.2634211778640747
I0211 09:26:39.626467 139648015189760 logging_writer.py:48] [35700] global_step=35700, grad_norm=2.6153271198272705, loss=1.2664066553115845
I0211 09:28:06.067708 139648006797056 logging_writer.py:48] [35800] global_step=35800, grad_norm=2.5691568851470947, loss=1.2390891313552856
I0211 09:29:33.764274 139648015189760 logging_writer.py:48] [35900] global_step=35900, grad_norm=3.1164605617523193, loss=1.2200989723205566
I0211 09:31:01.685502 139648006797056 logging_writer.py:48] [36000] global_step=36000, grad_norm=2.544584274291992, loss=1.2613189220428467
I0211 09:32:26.711673 139648015189760 logging_writer.py:48] [36100] global_step=36100, grad_norm=2.4699182510375977, loss=1.2513524293899536
I0211 09:33:42.904139 139648006797056 logging_writer.py:48] [36200] global_step=36200, grad_norm=3.1496665477752686, loss=1.1905189752578735
I0211 09:35:02.176019 139648015189760 logging_writer.py:48] [36300] global_step=36300, grad_norm=2.402111768722534, loss=1.249666690826416
I0211 09:35:44.987749 139803787056960 spec.py:321] Evaluating on the training split.
I0211 09:36:41.259571 139803787056960 spec.py:333] Evaluating on the validation split.
I0211 09:37:33.440145 139803787056960 spec.py:349] Evaluating on the test split.
I0211 09:38:00.428967 139803787056960 submission_runner.py:408] Time since start: 33406.52s, 	Step: 36352, 	{'train/ctc_loss': Array(0.16080004, dtype=float32), 'train/wer': 0.05573834342937384, 'validation/ctc_loss': Array(0.46172354, dtype=float32), 'validation/wer': 0.13400658447338695, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25495288, dtype=float32), 'test/wer': 0.08098226799098165, 'test/num_examples': 2472, 'score': 30292.434127807617, 'total_duration': 33406.52474331856, 'accumulated_submission_time': 30292.434127807617, 'accumulated_eval_time': 3111.6914982795715, 'accumulated_logging_time': 0.8780829906463623}
I0211 09:38:00.459409 139648015189760 logging_writer.py:48] [36352] accumulated_eval_time=3111.691498, accumulated_logging_time=0.878083, accumulated_submission_time=30292.434128, global_step=36352, preemption_count=0, score=30292.434128, test/ctc_loss=0.25495287775993347, test/num_examples=2472, test/wer=0.080982, total_duration=33406.524743, train/ctc_loss=0.160800039768219, train/wer=0.055738, validation/ctc_loss=0.46172353625297546, validation/num_examples=5348, validation/wer=0.134007
I0211 09:38:37.756803 139648006797056 logging_writer.py:48] [36400] global_step=36400, grad_norm=3.349782943725586, loss=1.2353541851043701
I0211 09:39:53.173338 139648015189760 logging_writer.py:48] [36500] global_step=36500, grad_norm=3.519033193588257, loss=1.2506901025772095
I0211 09:41:10.496385 139648006797056 logging_writer.py:48] [36600] global_step=36600, grad_norm=5.745288372039795, loss=1.1917544603347778
I0211 09:42:36.430278 139648015189760 logging_writer.py:48] [36700] global_step=36700, grad_norm=2.642188549041748, loss=1.2382829189300537
I0211 09:44:05.850472 139648006797056 logging_writer.py:48] [36800] global_step=36800, grad_norm=3.041470766067505, loss=1.2140886783599854
I0211 09:45:31.271437 139648015189760 logging_writer.py:48] [36900] global_step=36900, grad_norm=2.5919718742370605, loss=1.2483693361282349
I0211 09:47:01.497805 139648006797056 logging_writer.py:48] [37000] global_step=37000, grad_norm=6.786576271057129, loss=1.1902470588684082
I0211 09:48:31.263957 139648015189760 logging_writer.py:48] [37100] global_step=37100, grad_norm=2.3416833877563477, loss=1.2204763889312744
I0211 09:49:49.139467 139648006797056 logging_writer.py:48] [37200] global_step=37200, grad_norm=3.400024175643921, loss=1.2117269039154053
I0211 09:51:08.044534 139648015189760 logging_writer.py:48] [37300] global_step=37300, grad_norm=4.0126214027404785, loss=1.21915602684021
I0211 09:52:24.444966 139648006797056 logging_writer.py:48] [37400] global_step=37400, grad_norm=3.248011589050293, loss=1.2434394359588623
I0211 09:53:47.673773 139648015189760 logging_writer.py:48] [37500] global_step=37500, grad_norm=3.0689311027526855, loss=1.189469337463379
I0211 09:55:15.167308 139648006797056 logging_writer.py:48] [37600] global_step=37600, grad_norm=4.423409461975098, loss=1.1942139863967896
I0211 09:56:40.221851 139648015189760 logging_writer.py:48] [37700] global_step=37700, grad_norm=2.831361770629883, loss=1.207126498222351
I0211 09:58:06.909892 139648006797056 logging_writer.py:48] [37800] global_step=37800, grad_norm=3.3200058937072754, loss=1.2407615184783936
I0211 09:59:34.058373 139648015189760 logging_writer.py:48] [37900] global_step=37900, grad_norm=3.1058201789855957, loss=1.2366702556610107
I0211 10:01:03.041954 139648006797056 logging_writer.py:48] [38000] global_step=38000, grad_norm=3.2229108810424805, loss=1.2097537517547607
I0211 10:02:00.836256 139803787056960 spec.py:321] Evaluating on the training split.
I0211 10:03:02.611653 139803787056960 spec.py:333] Evaluating on the validation split.
I0211 10:03:55.772750 139803787056960 spec.py:349] Evaluating on the test split.
I0211 10:04:22.637787 139803787056960 submission_runner.py:408] Time since start: 34988.73s, 	Step: 38068, 	{'train/ctc_loss': Array(0.17805463, dtype=float32), 'train/wer': 0.060590061455935464, 'validation/ctc_loss': Array(0.4462943, dtype=float32), 'validation/wer': 0.1298164650453286, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24388216, dtype=float32), 'test/wer': 0.07785428472772328, 'test/num_examples': 2472, 'score': 31732.729076623917, 'total_duration': 34988.733575344086, 'accumulated_submission_time': 31732.729076623917, 'accumulated_eval_time': 3253.490313768387, 'accumulated_logging_time': 0.9202170372009277}
I0211 10:04:22.678306 139648015189760 logging_writer.py:48] [38068] accumulated_eval_time=3253.490314, accumulated_logging_time=0.920217, accumulated_submission_time=31732.729077, global_step=38068, preemption_count=0, score=31732.729077, test/ctc_loss=0.2438821643590927, test/num_examples=2472, test/wer=0.077854, total_duration=34988.733575, train/ctc_loss=0.17805463075637817, train/wer=0.060590, validation/ctc_loss=0.44629430770874023, validation/num_examples=5348, validation/wer=0.129816
I0211 10:04:47.411230 139648006797056 logging_writer.py:48] [38100] global_step=38100, grad_norm=2.5723538398742676, loss=1.2328732013702393
I0211 10:06:10.948670 139648015189760 logging_writer.py:48] [38200] global_step=38200, grad_norm=2.9405417442321777, loss=1.251871109008789
I0211 10:07:27.690761 139648006797056 logging_writer.py:48] [38300] global_step=38300, grad_norm=2.571787118911743, loss=1.1424471139907837
I0211 10:08:48.912342 139648015189760 logging_writer.py:48] [38400] global_step=38400, grad_norm=3.2569549083709717, loss=1.2650787830352783
I0211 10:10:14.788629 139648006797056 logging_writer.py:48] [38500] global_step=38500, grad_norm=2.8558712005615234, loss=1.2258390188217163
I0211 10:11:40.905136 139648015189760 logging_writer.py:48] [38600] global_step=38600, grad_norm=2.537435531616211, loss=1.1827362775802612
I0211 10:13:07.567622 139648006797056 logging_writer.py:48] [38700] global_step=38700, grad_norm=3.2304816246032715, loss=1.2393174171447754
I0211 10:14:37.066282 139648015189760 logging_writer.py:48] [38800] global_step=38800, grad_norm=2.2433478832244873, loss=1.1803667545318604
I0211 10:16:04.184401 139648006797056 logging_writer.py:48] [38900] global_step=38900, grad_norm=3.530845880508423, loss=1.2013517618179321
I0211 10:17:34.324701 139648015189760 logging_writer.py:48] [39000] global_step=39000, grad_norm=2.769773006439209, loss=1.1286522150039673
I0211 10:18:58.857085 139648006797056 logging_writer.py:48] [39100] global_step=39100, grad_norm=3.1620922088623047, loss=1.1878920793533325
I0211 10:20:22.319982 139648015189760 logging_writer.py:48] [39200] global_step=39200, grad_norm=3.4830844402313232, loss=1.188293218612671
I0211 10:21:39.443615 139648006797056 logging_writer.py:48] [39300] global_step=39300, grad_norm=6.0242085456848145, loss=1.193405270576477
I0211 10:22:58.031893 139648015189760 logging_writer.py:48] [39400] global_step=39400, grad_norm=2.810929775238037, loss=1.2064132690429688
I0211 10:24:17.173508 139648006797056 logging_writer.py:48] [39500] global_step=39500, grad_norm=3.121642827987671, loss=1.1734633445739746
I0211 10:25:42.798908 139648015189760 logging_writer.py:48] [39600] global_step=39600, grad_norm=3.2910473346710205, loss=1.1916720867156982
I0211 10:27:11.936321 139648006797056 logging_writer.py:48] [39700] global_step=39700, grad_norm=2.932281970977783, loss=1.2152353525161743
I0211 10:28:23.281317 139803787056960 spec.py:321] Evaluating on the training split.
I0211 10:29:17.987799 139803787056960 spec.py:333] Evaluating on the validation split.
I0211 10:30:10.893415 139803787056960 spec.py:349] Evaluating on the test split.
I0211 10:30:37.230239 139803787056960 submission_runner.py:408] Time since start: 36563.32s, 	Step: 39784, 	{'train/ctc_loss': Array(0.22174585, dtype=float32), 'train/wer': 0.07601850696723313, 'validation/ctc_loss': Array(0.43836048, dtype=float32), 'validation/wer': 0.1261187329233324, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2375296, dtype=float32), 'test/wer': 0.0756606341275161, 'test/num_examples': 2472, 'score': 33173.24671959877, 'total_duration': 36563.321739435196, 'accumulated_submission_time': 33173.24671959877, 'accumulated_eval_time': 3387.4322237968445, 'accumulated_logging_time': 0.9764108657836914}
I0211 10:30:37.271736 139647585109760 logging_writer.py:48] [39784] accumulated_eval_time=3387.432224, accumulated_logging_time=0.976411, accumulated_submission_time=33173.246720, global_step=39784, preemption_count=0, score=33173.246720, test/ctc_loss=0.23752960562705994, test/num_examples=2472, test/wer=0.075661, total_duration=36563.321739, train/ctc_loss=0.22174584865570068, train/wer=0.076019, validation/ctc_loss=0.4383604824542999, validation/num_examples=5348, validation/wer=0.126119
I0211 10:30:50.119111 139647576717056 logging_writer.py:48] [39800] global_step=39800, grad_norm=3.6436266899108887, loss=1.1750829219818115
I0211 10:32:05.348744 139647585109760 logging_writer.py:48] [39900] global_step=39900, grad_norm=2.765669584274292, loss=1.15669584274292
I0211 10:33:21.301088 139647576717056 logging_writer.py:48] [40000] global_step=40000, grad_norm=2.9082908630371094, loss=1.211700677871704
I0211 10:34:49.280276 139647585109760 logging_writer.py:48] [40100] global_step=40100, grad_norm=2.9461474418640137, loss=1.243691086769104
I0211 10:36:16.398259 139647585109760 logging_writer.py:48] [40200] global_step=40200, grad_norm=3.8604791164398193, loss=1.1398597955703735
I0211 10:37:32.409927 139647576717056 logging_writer.py:48] [40300] global_step=40300, grad_norm=2.4377377033233643, loss=1.158201813697815
I0211 10:38:50.767506 139647585109760 logging_writer.py:48] [40400] global_step=40400, grad_norm=2.97514009475708, loss=1.189711093902588
I0211 10:40:12.764824 139647576717056 logging_writer.py:48] [40500] global_step=40500, grad_norm=3.3938701152801514, loss=1.1439307928085327
I0211 10:41:35.861441 139647585109760 logging_writer.py:48] [40600] global_step=40600, grad_norm=2.4582653045654297, loss=1.1393699645996094
I0211 10:43:06.070959 139647576717056 logging_writer.py:48] [40700] global_step=40700, grad_norm=2.2887046337127686, loss=1.1963763236999512
I0211 10:44:36.074602 139647585109760 logging_writer.py:48] [40800] global_step=40800, grad_norm=2.3018455505371094, loss=1.179232120513916
I0211 10:46:07.724157 139647576717056 logging_writer.py:48] [40900] global_step=40900, grad_norm=2.9046154022216797, loss=1.1660516262054443
I0211 10:47:39.016358 139647585109760 logging_writer.py:48] [41000] global_step=41000, grad_norm=4.91130256652832, loss=1.1420570611953735
I0211 10:49:07.138094 139647576717056 logging_writer.py:48] [41100] global_step=41100, grad_norm=4.158076763153076, loss=1.1564844846725464
I0211 10:50:37.842956 139647585109760 logging_writer.py:48] [41200] global_step=41200, grad_norm=3.124741315841675, loss=1.1552815437316895
I0211 10:51:54.332862 139647576717056 logging_writer.py:48] [41300] global_step=41300, grad_norm=3.4604790210723877, loss=1.149261713027954
I0211 10:53:10.950034 139647585109760 logging_writer.py:48] [41400] global_step=41400, grad_norm=2.287402629852295, loss=1.167318344116211
I0211 10:54:27.185822 139647576717056 logging_writer.py:48] [41500] global_step=41500, grad_norm=2.270174264907837, loss=1.1897097826004028
I0211 10:54:37.565575 139803787056960 spec.py:321] Evaluating on the training split.
I0211 10:55:33.642313 139803787056960 spec.py:333] Evaluating on the validation split.
I0211 10:56:27.588753 139803787056960 spec.py:349] Evaluating on the test split.
I0211 10:56:53.633683 139803787056960 submission_runner.py:408] Time since start: 38139.73s, 	Step: 41515, 	{'train/ctc_loss': Array(0.22483462, dtype=float32), 'train/wer': 0.0773355834725792, 'validation/ctc_loss': Array(0.4263075, dtype=float32), 'validation/wer': 0.1229906253318787, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22996153, dtype=float32), 'test/wer': 0.0736294761643613, 'test/num_examples': 2472, 'score': 34613.45397615433, 'total_duration': 38139.72946357727, 'accumulated_submission_time': 34613.45397615433, 'accumulated_eval_time': 3523.4975786209106, 'accumulated_logging_time': 1.033158540725708}
I0211 10:56:53.664873 139648015189760 logging_writer.py:48] [41515] accumulated_eval_time=3523.497579, accumulated_logging_time=1.033159, accumulated_submission_time=34613.453976, global_step=41515, preemption_count=0, score=34613.453976, test/ctc_loss=0.22996152937412262, test/num_examples=2472, test/wer=0.073629, total_duration=38139.729464, train/ctc_loss=0.2248346209526062, train/wer=0.077336, validation/ctc_loss=0.4263074994087219, validation/num_examples=5348, validation/wer=0.122991
I0211 10:57:58.488213 139648006797056 logging_writer.py:48] [41600] global_step=41600, grad_norm=2.3409361839294434, loss=1.0994305610656738
I0211 10:59:13.984718 139648015189760 logging_writer.py:48] [41700] global_step=41700, grad_norm=2.7892210483551025, loss=1.1617740392684937
I0211 11:00:38.249224 139648006797056 logging_writer.py:48] [41800] global_step=41800, grad_norm=4.304002285003662, loss=1.1543190479278564
I0211 11:02:08.246503 139648015189760 logging_writer.py:48] [41900] global_step=41900, grad_norm=2.4979026317596436, loss=1.1458879709243774
I0211 11:03:36.803304 139648006797056 logging_writer.py:48] [42000] global_step=42000, grad_norm=2.438650131225586, loss=1.161781907081604
I0211 11:05:07.699134 139648015189760 logging_writer.py:48] [42100] global_step=42100, grad_norm=3.0385589599609375, loss=1.1595326662063599
I0211 11:06:34.119625 139648006797056 logging_writer.py:48] [42200] global_step=42200, grad_norm=3.623372793197632, loss=1.1268998384475708
I0211 11:07:56.527537 139647687509760 logging_writer.py:48] [42300] global_step=42300, grad_norm=2.730659008026123, loss=1.1111371517181396
I0211 11:09:12.505160 139647679117056 logging_writer.py:48] [42400] global_step=42400, grad_norm=2.7337775230407715, loss=1.1746234893798828
I0211 11:10:32.644446 139647687509760 logging_writer.py:48] [42500] global_step=42500, grad_norm=2.4799857139587402, loss=1.1293317079544067
I0211 11:11:54.513733 139647679117056 logging_writer.py:48] [42600] global_step=42600, grad_norm=2.629497528076172, loss=1.155036449432373
I0211 11:13:19.661692 139647687509760 logging_writer.py:48] [42700] global_step=42700, grad_norm=3.2937326431274414, loss=1.175199031829834
I0211 11:14:50.892425 139647679117056 logging_writer.py:48] [42800] global_step=42800, grad_norm=2.8510091304779053, loss=1.1113202571868896
I0211 11:16:20.259213 139647687509760 logging_writer.py:48] [42900] global_step=42900, grad_norm=2.4492058753967285, loss=1.1326615810394287
I0211 11:17:49.651654 139647679117056 logging_writer.py:48] [43000] global_step=43000, grad_norm=3.2809560298919678, loss=1.1914112567901611
I0211 11:19:17.511922 139647687509760 logging_writer.py:48] [43100] global_step=43100, grad_norm=4.5945234298706055, loss=1.1597464084625244
I0211 11:20:44.408658 139647679117056 logging_writer.py:48] [43200] global_step=43200, grad_norm=3.580484390258789, loss=1.146817684173584
I0211 11:20:53.737326 139803787056960 spec.py:321] Evaluating on the training split.
I0211 11:21:47.098061 139803787056960 spec.py:333] Evaluating on the validation split.
I0211 11:22:40.857933 139803787056960 spec.py:349] Evaluating on the test split.
I0211 11:23:08.148077 139803787056960 submission_runner.py:408] Time since start: 39714.24s, 	Step: 43212, 	{'train/ctc_loss': Array(0.25475532, dtype=float32), 'train/wer': 0.08800182508438091, 'validation/ctc_loss': Array(0.41781008, dtype=float32), 'validation/wer': 0.12053834345462795, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22553718, dtype=float32), 'test/wer': 0.07169987609936425, 'test/num_examples': 2472, 'score': 36053.44173717499, 'total_duration': 39714.23803758621, 'accumulated_submission_time': 36053.44173717499, 'accumulated_eval_time': 3657.8997716903687, 'accumulated_logging_time': 1.0785846710205078}
I0211 11:23:08.190971 139647001429760 logging_writer.py:48] [43212] accumulated_eval_time=3657.899772, accumulated_logging_time=1.078585, accumulated_submission_time=36053.441737, global_step=43212, preemption_count=0, score=36053.441737, test/ctc_loss=0.22553718090057373, test/num_examples=2472, test/wer=0.071700, total_duration=39714.238038, train/ctc_loss=0.25475531816482544, train/wer=0.088002, validation/ctc_loss=0.4178100824356079, validation/num_examples=5348, validation/wer=0.120538
I0211 11:24:21.204428 139647001429760 logging_writer.py:48] [43300] global_step=43300, grad_norm=4.041889667510986, loss=1.1212366819381714
I0211 11:25:39.929463 139646993037056 logging_writer.py:48] [43400] global_step=43400, grad_norm=4.430372714996338, loss=1.1406381130218506
I0211 11:26:58.138127 139647001429760 logging_writer.py:48] [43500] global_step=43500, grad_norm=2.8885772228240967, loss=1.080768346786499
I0211 11:28:18.729264 139646993037056 logging_writer.py:48] [43600] global_step=43600, grad_norm=4.769314289093018, loss=1.152011752128601
I0211 11:29:44.762506 139647001429760 logging_writer.py:48] [43700] global_step=43700, grad_norm=2.2236814498901367, loss=1.1130274534225464
I0211 11:31:16.144077 139646993037056 logging_writer.py:48] [43800] global_step=43800, grad_norm=2.451604127883911, loss=1.1914024353027344
I0211 11:32:44.605040 139647001429760 logging_writer.py:48] [43900] global_step=43900, grad_norm=2.7893893718719482, loss=1.145472764968872
I0211 11:34:15.294821 139646993037056 logging_writer.py:48] [44000] global_step=44000, grad_norm=3.679457902908325, loss=1.1506158113479614
I0211 11:35:43.920049 139647001429760 logging_writer.py:48] [44100] global_step=44100, grad_norm=3.1541755199432373, loss=1.1621930599212646
I0211 11:37:13.450152 139646993037056 logging_writer.py:48] [44200] global_step=44200, grad_norm=3.0212669372558594, loss=1.0779982805252075
I0211 11:38:40.991703 139647001429760 logging_writer.py:48] [44300] global_step=44300, grad_norm=3.1991400718688965, loss=1.077568769454956
I0211 11:39:56.510702 139646993037056 logging_writer.py:48] [44400] global_step=44400, grad_norm=3.837510108947754, loss=1.1119105815887451
I0211 11:41:15.919088 139647001429760 logging_writer.py:48] [44500] global_step=44500, grad_norm=5.80328893661499, loss=1.109800100326538
I0211 11:42:34.748970 139646993037056 logging_writer.py:48] [44600] global_step=44600, grad_norm=2.074899196624756, loss=1.0855339765548706
I0211 11:43:59.457521 139647001429760 logging_writer.py:48] [44700] global_step=44700, grad_norm=2.9117860794067383, loss=1.186834454536438
I0211 11:45:25.280959 139646993037056 logging_writer.py:48] [44800] global_step=44800, grad_norm=3.3405568599700928, loss=1.1719272136688232
I0211 11:46:55.577768 139647001429760 logging_writer.py:48] [44900] global_step=44900, grad_norm=3.2653310298919678, loss=1.08940589427948
I0211 11:47:08.584607 139803787056960 spec.py:321] Evaluating on the training split.
I0211 11:48:01.356248 139803787056960 spec.py:333] Evaluating on the validation split.
I0211 11:48:55.047574 139803787056960 spec.py:349] Evaluating on the test split.
I0211 11:49:21.718731 139803787056960 submission_runner.py:408] Time since start: 41287.81s, 	Step: 44916, 	{'train/ctc_loss': Array(0.22422256, dtype=float32), 'train/wer': 0.07490182456181667, 'validation/ctc_loss': Array(0.41305763, dtype=float32), 'validation/wer': 0.11930254786294255, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22203259, dtype=float32), 'test/wer': 0.07165925294010116, 'test/num_examples': 2472, 'score': 37493.750953912735, 'total_duration': 41287.81129693985, 'accumulated_submission_time': 37493.750953912735, 'accumulated_eval_time': 3791.0279178619385, 'accumulated_logging_time': 1.1358962059020996}
I0211 11:49:21.761490 139647001429760 logging_writer.py:48] [44916] accumulated_eval_time=3791.027918, accumulated_logging_time=1.135896, accumulated_submission_time=37493.750954, global_step=44916, preemption_count=0, score=37493.750954, test/ctc_loss=0.2220325917005539, test/num_examples=2472, test/wer=0.071659, total_duration=41287.811297, train/ctc_loss=0.2242225557565689, train/wer=0.074902, validation/ctc_loss=0.4130576252937317, validation/num_examples=5348, validation/wer=0.119303
I0211 11:50:25.728000 139646993037056 logging_writer.py:48] [45000] global_step=45000, grad_norm=4.3756489753723145, loss=1.1238529682159424
I0211 11:51:41.059913 139647001429760 logging_writer.py:48] [45100] global_step=45100, grad_norm=2.353116750717163, loss=1.0771639347076416
I0211 11:53:09.295305 139646993037056 logging_writer.py:48] [45200] global_step=45200, grad_norm=2.9818012714385986, loss=1.1579240560531616
I0211 11:54:39.928900 139647001429760 logging_writer.py:48] [45300] global_step=45300, grad_norm=4.105555057525635, loss=1.035703420639038
I0211 11:56:02.128415 139647001429760 logging_writer.py:48] [45400] global_step=45400, grad_norm=2.968747138977051, loss=1.0743680000305176
I0211 11:57:18.578035 139646993037056 logging_writer.py:48] [45500] global_step=45500, grad_norm=3.5178754329681396, loss=1.1791197061538696
I0211 11:58:36.990195 139647001429760 logging_writer.py:48] [45600] global_step=45600, grad_norm=2.615274667739868, loss=1.0214701890945435
I0211 12:00:00.737881 139646993037056 logging_writer.py:48] [45700] global_step=45700, grad_norm=4.588207721710205, loss=1.126606822013855
I0211 12:01:28.414086 139647001429760 logging_writer.py:48] [45800] global_step=45800, grad_norm=3.0747268199920654, loss=1.1215293407440186
I0211 12:02:59.732047 139646993037056 logging_writer.py:48] [45900] global_step=45900, grad_norm=2.8503055572509766, loss=1.0654922723770142
I0211 12:04:27.435001 139647001429760 logging_writer.py:48] [46000] global_step=46000, grad_norm=2.6202762126922607, loss=1.1673407554626465
I0211 12:05:58.136660 139646993037056 logging_writer.py:48] [46100] global_step=46100, grad_norm=4.056462287902832, loss=1.1297948360443115
I0211 12:07:25.417593 139647001429760 logging_writer.py:48] [46200] global_step=46200, grad_norm=3.450468063354492, loss=1.0640202760696411
I0211 12:08:54.273788 139646993037056 logging_writer.py:48] [46300] global_step=46300, grad_norm=5.051031112670898, loss=1.0661396980285645
I0211 12:10:21.251588 139647001429760 logging_writer.py:48] [46400] global_step=46400, grad_norm=2.500298261642456, loss=1.1130808591842651
I0211 12:11:39.478757 139646993037056 logging_writer.py:48] [46500] global_step=46500, grad_norm=2.8870797157287598, loss=1.0830568075180054
I0211 12:12:57.548537 139647001429760 logging_writer.py:48] [46600] global_step=46600, grad_norm=3.419583797454834, loss=1.1165409088134766
I0211 12:13:21.992331 139803787056960 spec.py:321] Evaluating on the training split.
I0211 12:14:16.098520 139803787056960 spec.py:333] Evaluating on the validation split.
I0211 12:15:07.704058 139803787056960 spec.py:349] Evaluating on the test split.
I0211 12:15:35.282624 139803787056960 submission_runner.py:408] Time since start: 42861.38s, 	Step: 46632, 	{'train/ctc_loss': Array(0.20445894, dtype=float32), 'train/wer': 0.07095298008083463, 'validation/ctc_loss': Array(0.410703, dtype=float32), 'validation/wer': 0.11835639186305841, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22045316, dtype=float32), 'test/wer': 0.07111084029004935, 'test/num_examples': 2472, 'score': 38933.89561486244, 'total_duration': 42861.37547492981, 'accumulated_submission_time': 38933.89561486244, 'accumulated_eval_time': 3924.3125166893005, 'accumulated_logging_time': 1.1946141719818115}
I0211 12:15:35.323489 139647585109760 logging_writer.py:48] [46632] accumulated_eval_time=3924.312517, accumulated_logging_time=1.194614, accumulated_submission_time=38933.895615, global_step=46632, preemption_count=0, score=38933.895615, test/ctc_loss=0.2204531580209732, test/num_examples=2472, test/wer=0.071111, total_duration=42861.375475, train/ctc_loss=0.20445893704891205, train/wer=0.070953, validation/ctc_loss=0.41070300340652466, validation/num_examples=5348, validation/wer=0.118356
I0211 12:16:27.338073 139647576717056 logging_writer.py:48] [46700] global_step=46700, grad_norm=2.989954710006714, loss=1.1138840913772583
I0211 12:17:42.605879 139647585109760 logging_writer.py:48] [46800] global_step=46800, grad_norm=4.357436180114746, loss=1.1216926574707031
I0211 12:19:07.203695 139647576717056 logging_writer.py:48] [46900] global_step=46900, grad_norm=2.695979118347168, loss=1.1146169900894165
I0211 12:20:37.183356 139647585109760 logging_writer.py:48] [47000] global_step=47000, grad_norm=2.9616050720214844, loss=1.116626262664795
I0211 12:22:05.024351 139647576717056 logging_writer.py:48] [47100] global_step=47100, grad_norm=4.384101867675781, loss=1.0871626138687134
I0211 12:23:35.609955 139647585109760 logging_writer.py:48] [47200] global_step=47200, grad_norm=7.72083044052124, loss=1.1253305673599243
I0211 12:25:02.835946 139647576717056 logging_writer.py:48] [47300] global_step=47300, grad_norm=2.9543704986572266, loss=1.1047004461288452
I0211 12:26:35.157384 139647257429760 logging_writer.py:48] [47400] global_step=47400, grad_norm=2.7391159534454346, loss=1.1375762224197388
I0211 12:27:51.474779 139647249037056 logging_writer.py:48] [47500] global_step=47500, grad_norm=3.0313448905944824, loss=1.1137096881866455
I0211 12:29:09.405348 139647257429760 logging_writer.py:48] [47600] global_step=47600, grad_norm=4.094090461730957, loss=1.1447503566741943
I0211 12:30:29.891594 139647249037056 logging_writer.py:48] [47700] global_step=47700, grad_norm=2.3704206943511963, loss=1.1772394180297852
I0211 12:31:55.446977 139647257429760 logging_writer.py:48] [47800] global_step=47800, grad_norm=2.957508087158203, loss=1.0938786268234253
I0211 12:33:21.604539 139647249037056 logging_writer.py:48] [47900] global_step=47900, grad_norm=2.4097208976745605, loss=1.1159348487854004
I0211 12:34:48.656689 139803787056960 spec.py:321] Evaluating on the training split.
I0211 12:35:45.612992 139803787056960 spec.py:333] Evaluating on the validation split.
I0211 12:36:38.895593 139803787056960 spec.py:349] Evaluating on the test split.
I0211 12:37:05.396361 139803787056960 submission_runner.py:408] Time since start: 44151.49s, 	Step: 48000, 	{'train/ctc_loss': Array(0.18561524, dtype=float32), 'train/wer': 0.0648674808861434, 'validation/ctc_loss': Array(0.4110458, dtype=float32), 'validation/wer': 0.11803778831207701, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22013809, dtype=float32), 'test/wer': 0.0707452318566815, 'test/num_examples': 2472, 'score': 40087.15713596344, 'total_duration': 44151.489364147186, 'accumulated_submission_time': 40087.15713596344, 'accumulated_eval_time': 4061.046733379364, 'accumulated_logging_time': 1.2506194114685059}
I0211 12:37:05.439097 139647800149760 logging_writer.py:48] [48000] accumulated_eval_time=4061.046733, accumulated_logging_time=1.250619, accumulated_submission_time=40087.157136, global_step=48000, preemption_count=0, score=40087.157136, test/ctc_loss=0.2201380878686905, test/num_examples=2472, test/wer=0.070745, total_duration=44151.489364, train/ctc_loss=0.18561524152755737, train/wer=0.064867, validation/ctc_loss=0.41104578971862793, validation/num_examples=5348, validation/wer=0.118038
I0211 12:37:05.465522 139647791757056 logging_writer.py:48] [48000] global_step=48000, preemption_count=0, score=40087.157136
I0211 12:37:05.629374 139803787056960 checkpoints.py:490] Saving checkpoint at step: 48000
I0211 12:37:06.632907 139803787056960 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_1/librispeech_deepspeech_jax/trial_1/checkpoint_48000
I0211 12:37:06.652873 139803787056960 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_1/librispeech_deepspeech_jax/trial_1/checkpoint_48000.
I0211 12:37:07.945796 139803787056960 submission_runner.py:583] Tuning trial 1/5
I0211 12:37:07.946053 139803787056960 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.1, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0211 12:37:07.958307 139803787056960 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(31.839237, dtype=float32), 'train/wer': 4.706369864439956, 'validation/ctc_loss': Array(30.812881, dtype=float32), 'validation/wer': 4.233381928420402, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.939672, dtype=float32), 'test/wer': 4.561290191538196, 'test/num_examples': 2472, 'score': 45.19680690765381, 'total_duration': 310.71803855895996, 'accumulated_submission_time': 45.19680690765381, 'accumulated_eval_time': 265.52112197875977, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1756, {'train/ctc_loss': Array(6.2416306, dtype=float32), 'train/wer': 0.941288541945583, 'validation/ctc_loss': Array(6.160726, dtype=float32), 'validation/wer': 0.8952083956863011, 'validation/num_examples': 5348, 'test/ctc_loss': Array(6.0450172, dtype=float32), 'test/wer': 0.8979343123514716, 'test/num_examples': 2472, 'score': 1485.6898527145386, 'total_duration': 1857.0092232227325, 'accumulated_submission_time': 1485.6898527145386, 'accumulated_eval_time': 371.200421333313, 'accumulated_logging_time': 0.04808688163757324, 'global_step': 1756, 'preemption_count': 0}), (3509, {'train/ctc_loss': Array(3.7979012, dtype=float32), 'train/wer': 0.7728577897344379, 'validation/ctc_loss': Array(4.057168, dtype=float32), 'validation/wer': 0.784469525087616, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3.663008, dtype=float32), 'test/wer': 0.7430382060812869, 'test/num_examples': 2472, 'score': 2926.2204928398132, 'total_duration': 3423.682983160019, 'accumulated_submission_time': 2926.2204928398132, 'accumulated_eval_time': 497.2301824092865, 'accumulated_logging_time': 0.0906984806060791, 'global_step': 3509, 'preemption_count': 0}), (5251, {'train/ctc_loss': Array(0.6250547, dtype=float32), 'train/wer': 0.21471079694904355, 'validation/ctc_loss': Array(1.0514781, dtype=float32), 'validation/wer': 0.2947179393108509, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.7044876, dtype=float32), 'test/wer': 0.22361018016371134, 'test/num_examples': 2472, 'score': 4366.24077963829, 'total_duration': 4998.193810939789, 'accumulated_submission_time': 4366.24077963829, 'accumulated_eval_time': 631.6112017631531, 'accumulated_logging_time': 0.12792444229125977, 'global_step': 5251, 'preemption_count': 0}), (7003, {'train/ctc_loss': Array(0.48509657, dtype=float32), 'train/wer': 0.16434877315711757, 'validation/ctc_loss': Array(0.82567525, dtype=float32), 'validation/wer': 0.23964779825636967, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.522345, dtype=float32), 'test/wer': 0.17057664574573964, 'test/num_examples': 2472, 'score': 5806.4535665512085, 'total_duration': 6579.632403612137, 'accumulated_submission_time': 5806.4535665512085, 'accumulated_eval_time': 772.7243013381958, 'accumulated_logging_time': 0.16758346557617188, 'global_step': 7003, 'preemption_count': 0}), (8757, {'train/ctc_loss': Array(0.42493558, dtype=float32), 'train/wer': 0.14487703581291178, 'validation/ctc_loss': Array(0.76556903, dtype=float32), 'validation/wer': 0.21989437809552315, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4660455, dtype=float32), 'test/wer': 0.1513212682550322, 'test/num_examples': 2472, 'score': 7246.49645781517, 'total_duration': 8155.920823574066, 'accumulated_submission_time': 7246.49645781517, 'accumulated_eval_time': 908.856529712677, 'accumulated_logging_time': 0.20881009101867676, 'global_step': 8757, 'preemption_count': 0}), (10501, {'train/ctc_loss': Array(0.41917217, dtype=float32), 'train/wer': 0.14101285135848834, 'validation/ctc_loss': Array(0.72048366, dtype=float32), 'validation/wer': 0.208675671239754, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.43311206, dtype=float32), 'test/wer': 0.1437044258932017, 'test/num_examples': 2472, 'score': 8686.787279844284, 'total_duration': 9732.202117919922, 'accumulated_submission_time': 8686.787279844284, 'accumulated_eval_time': 1044.7315225601196, 'accumulated_logging_time': 0.2510659694671631, 'global_step': 10501, 'preemption_count': 0}), (12213, {'train/ctc_loss': Array(0.38169208, dtype=float32), 'train/wer': 0.12917171846085126, 'validation/ctc_loss': Array(0.70423025, dtype=float32), 'validation/wer': 0.20394489124033327, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4196876, dtype=float32), 'test/wer': 0.13728596672963256, 'test/num_examples': 2472, 'score': 10126.841429710388, 'total_duration': 11310.710699796677, 'accumulated_submission_time': 10126.841429710388, 'accumulated_eval_time': 1183.076649427414, 'accumulated_logging_time': 0.2884364128112793, 'global_step': 12213, 'preemption_count': 0}), (13956, {'train/ctc_loss': Array(0.3189625, dtype=float32), 'train/wer': 0.1119997919862711, 'validation/ctc_loss': Array(0.6466541, dtype=float32), 'validation/wer': 0.1890187976095079, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.38722602, dtype=float32), 'test/wer': 0.12702861901570084, 'test/num_examples': 2472, 'score': 11567.578044652939, 'total_duration': 12884.33522772789, 'accumulated_submission_time': 11567.578044652939, 'accumulated_eval_time': 1315.8529777526855, 'accumulated_logging_time': 0.32673048973083496, 'global_step': 13956, 'preemption_count': 0}), (15677, {'train/ctc_loss': Array(0.30618104, dtype=float32), 'train/wer': 0.1042991131516367, 'validation/ctc_loss': Array(0.6405959, dtype=float32), 'validation/wer': 0.1845680025488284, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3840812, dtype=float32), 'test/wer': 0.1235756504783377, 'test/num_examples': 2472, 'score': 13007.677670955658, 'total_duration': 14458.273012399673, 'accumulated_submission_time': 13007.677670955658, 'accumulated_eval_time': 1449.5785381793976, 'accumulated_logging_time': 0.3680403232574463, 'global_step': 15677, 'preemption_count': 0}), (17405, {'train/ctc_loss': Array(0.306939, dtype=float32), 'train/wer': 0.10795475875087818, 'validation/ctc_loss': Array(0.6359483, dtype=float32), 'validation/wer': 0.18245363352867916, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.36800343, dtype=float32), 'test/wer': 0.1195945808705543, 'test/num_examples': 2472, 'score': 14447.619881868362, 'total_duration': 16042.089720010757, 'accumulated_submission_time': 14447.619881868362, 'accumulated_eval_time': 1593.3436903953552, 'accumulated_logging_time': 0.40529966354370117, 'global_step': 17405, 'preemption_count': 0}), (19134, {'train/ctc_loss': Array(0.3205848, dtype=float32), 'train/wer': 0.10751738544118036, 'validation/ctc_loss': Array(0.61774206, dtype=float32), 'validation/wer': 0.17917105148826476, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.35935163, dtype=float32), 'test/wer': 0.11469949017935124, 'test/num_examples': 2472, 'score': 15888.127633094788, 'total_duration': 17616.94973897934, 'accumulated_submission_time': 15888.127633094788, 'accumulated_eval_time': 1727.5816078186035, 'accumulated_logging_time': 0.44574427604675293, 'global_step': 19134, 'preemption_count': 0}), (20886, {'train/ctc_loss': Array(0.31008214, dtype=float32), 'train/wer': 0.10548975123328146, 'validation/ctc_loss': Array(0.5979301, dtype=float32), 'validation/wer': 0.1724417583054153, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.34600043, dtype=float32), 'test/wer': 0.11216054272540775, 'test/num_examples': 2472, 'score': 17328.907950878143, 'total_duration': 19191.85638141632, 'accumulated_submission_time': 17328.907950878143, 'accumulated_eval_time': 1861.594043970108, 'accumulated_logging_time': 0.4855766296386719, 'global_step': 20886, 'preemption_count': 0}), (22583, {'train/ctc_loss': Array(0.28917006, dtype=float32), 'train/wer': 0.09840518571869535, 'validation/ctc_loss': Array(0.57588047, dtype=float32), 'validation/wer': 0.1671896270407523, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33355513, dtype=float32), 'test/wer': 0.10809822679909817, 'test/num_examples': 2472, 'score': 18769.116664409637, 'total_duration': 20766.481579065323, 'accumulated_submission_time': 18769.116664409637, 'accumulated_eval_time': 1995.896348953247, 'accumulated_logging_time': 0.5268707275390625, 'global_step': 22583, 'preemption_count': 0}), (24285, {'train/ctc_loss': Array(0.27114815, dtype=float32), 'train/wer': 0.09292787466745492, 'validation/ctc_loss': Array(0.5687677, dtype=float32), 'validation/wer': 0.16331811116367534, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32747313, dtype=float32), 'test/wer': 0.10639205411004814, 'test/num_examples': 2472, 'score': 20209.478974580765, 'total_duration': 22352.04579281807, 'accumulated_submission_time': 20209.478974580765, 'accumulated_eval_time': 2140.9842009544373, 'accumulated_logging_time': 0.5696592330932617, 'global_step': 24285, 'preemption_count': 0}), (26018, {'train/ctc_loss': Array(0.24425977, dtype=float32), 'train/wer': 0.0852656563812306, 'validation/ctc_loss': Array(0.5455414, dtype=float32), 'validation/wer': 0.15750601002152986, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31358135, dtype=float32), 'test/wer': 0.10033920337984685, 'test/num_examples': 2472, 'score': 21649.439897060394, 'total_duration': 23930.495884418488, 'accumulated_submission_time': 21649.439897060394, 'accumulated_eval_time': 2279.357107400894, 'accumulated_logging_time': 0.6119377613067627, 'global_step': 26018, 'preemption_count': 0}), (27733, {'train/ctc_loss': Array(0.23917654, dtype=float32), 'train/wer': 0.08220740301316341, 'validation/ctc_loss': Array(0.533727, dtype=float32), 'validation/wer': 0.15428135589947575, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30254042, dtype=float32), 'test/wer': 0.09863303069079682, 'test/num_examples': 2472, 'score': 23089.615846395493, 'total_duration': 25511.851219892502, 'accumulated_submission_time': 23089.615846395493, 'accumulated_eval_time': 2420.4198529720306, 'accumulated_logging_time': 0.6570084095001221, 'global_step': 27733, 'preemption_count': 0}), (29456, {'train/ctc_loss': Array(0.25039864, dtype=float32), 'train/wer': 0.08649137632171953, 'validation/ctc_loss': Array(0.51835567, dtype=float32), 'validation/wer': 0.14964712243065545, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29362866, dtype=float32), 'test/wer': 0.09568785164422237, 'test/num_examples': 2472, 'score': 24530.126829862595, 'total_duration': 27095.03487610817, 'accumulated_submission_time': 24530.126829862595, 'accumulated_eval_time': 2562.9808316230774, 'accumulated_logging_time': 0.6976122856140137, 'global_step': 29456, 'preemption_count': 0}), (31194, {'train/ctc_loss': Array(0.22763942, dtype=float32), 'train/wer': 0.07687507799825284, 'validation/ctc_loss': Array(0.5053736, dtype=float32), 'validation/wer': 0.14656728810450195, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28454825, dtype=float32), 'test/wer': 0.09156460097901814, 'test/num_examples': 2472, 'score': 25970.27191734314, 'total_duration': 28675.169151067734, 'accumulated_submission_time': 25970.27191734314, 'accumulated_eval_time': 2702.8475642204285, 'accumulated_logging_time': 0.7462007999420166, 'global_step': 31194, 'preemption_count': 0}), (32913, {'train/ctc_loss': Array(0.2309214, dtype=float32), 'train/wer': 0.07894939615449814, 'validation/ctc_loss': Array(0.49034488, dtype=float32), 'validation/wer': 0.1433522886355079, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27736688, dtype=float32), 'test/wer': 0.08959437775475798, 'test/num_examples': 2472, 'score': 27410.788177251816, 'total_duration': 30251.190588474274, 'accumulated_submission_time': 27410.788177251816, 'accumulated_eval_time': 2838.235775709152, 'accumulated_logging_time': 0.7904810905456543, 'global_step': 32913, 'preemption_count': 0}), (34613, {'train/ctc_loss': Array(0.21600437, dtype=float32), 'train/wer': 0.07201317177659132, 'validation/ctc_loss': Array(0.47284636, dtype=float32), 'validation/wer': 0.13711538275872057, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2623799, dtype=float32), 'test/wer': 0.08478053338208112, 'test/num_examples': 2472, 'score': 28851.9081697464, 'total_duration': 31830.446347236633, 'accumulated_submission_time': 28851.9081697464, 'accumulated_eval_time': 2976.252999305725, 'accumulated_logging_time': 0.8371679782867432, 'global_step': 34613, 'preemption_count': 0}), (36352, {'train/ctc_loss': Array(0.16080004, dtype=float32), 'train/wer': 0.05573834342937384, 'validation/ctc_loss': Array(0.46172354, dtype=float32), 'validation/wer': 0.13400658447338695, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25495288, dtype=float32), 'test/wer': 0.08098226799098165, 'test/num_examples': 2472, 'score': 30292.434127807617, 'total_duration': 33406.52474331856, 'accumulated_submission_time': 30292.434127807617, 'accumulated_eval_time': 3111.6914982795715, 'accumulated_logging_time': 0.8780829906463623, 'global_step': 36352, 'preemption_count': 0}), (38068, {'train/ctc_loss': Array(0.17805463, dtype=float32), 'train/wer': 0.060590061455935464, 'validation/ctc_loss': Array(0.4462943, dtype=float32), 'validation/wer': 0.1298164650453286, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24388216, dtype=float32), 'test/wer': 0.07785428472772328, 'test/num_examples': 2472, 'score': 31732.729076623917, 'total_duration': 34988.733575344086, 'accumulated_submission_time': 31732.729076623917, 'accumulated_eval_time': 3253.490313768387, 'accumulated_logging_time': 0.9202170372009277, 'global_step': 38068, 'preemption_count': 0}), (39784, {'train/ctc_loss': Array(0.22174585, dtype=float32), 'train/wer': 0.07601850696723313, 'validation/ctc_loss': Array(0.43836048, dtype=float32), 'validation/wer': 0.1261187329233324, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2375296, dtype=float32), 'test/wer': 0.0756606341275161, 'test/num_examples': 2472, 'score': 33173.24671959877, 'total_duration': 36563.321739435196, 'accumulated_submission_time': 33173.24671959877, 'accumulated_eval_time': 3387.4322237968445, 'accumulated_logging_time': 0.9764108657836914, 'global_step': 39784, 'preemption_count': 0}), (41515, {'train/ctc_loss': Array(0.22483462, dtype=float32), 'train/wer': 0.0773355834725792, 'validation/ctc_loss': Array(0.4263075, dtype=float32), 'validation/wer': 0.1229906253318787, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22996153, dtype=float32), 'test/wer': 0.0736294761643613, 'test/num_examples': 2472, 'score': 34613.45397615433, 'total_duration': 38139.72946357727, 'accumulated_submission_time': 34613.45397615433, 'accumulated_eval_time': 3523.4975786209106, 'accumulated_logging_time': 1.033158540725708, 'global_step': 41515, 'preemption_count': 0}), (43212, {'train/ctc_loss': Array(0.25475532, dtype=float32), 'train/wer': 0.08800182508438091, 'validation/ctc_loss': Array(0.41781008, dtype=float32), 'validation/wer': 0.12053834345462795, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22553718, dtype=float32), 'test/wer': 0.07169987609936425, 'test/num_examples': 2472, 'score': 36053.44173717499, 'total_duration': 39714.23803758621, 'accumulated_submission_time': 36053.44173717499, 'accumulated_eval_time': 3657.8997716903687, 'accumulated_logging_time': 1.0785846710205078, 'global_step': 43212, 'preemption_count': 0}), (44916, {'train/ctc_loss': Array(0.22422256, dtype=float32), 'train/wer': 0.07490182456181667, 'validation/ctc_loss': Array(0.41305763, dtype=float32), 'validation/wer': 0.11930254786294255, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22203259, dtype=float32), 'test/wer': 0.07165925294010116, 'test/num_examples': 2472, 'score': 37493.750953912735, 'total_duration': 41287.81129693985, 'accumulated_submission_time': 37493.750953912735, 'accumulated_eval_time': 3791.0279178619385, 'accumulated_logging_time': 1.1358962059020996, 'global_step': 44916, 'preemption_count': 0}), (46632, {'train/ctc_loss': Array(0.20445894, dtype=float32), 'train/wer': 0.07095298008083463, 'validation/ctc_loss': Array(0.410703, dtype=float32), 'validation/wer': 0.11835639186305841, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22045316, dtype=float32), 'test/wer': 0.07111084029004935, 'test/num_examples': 2472, 'score': 38933.89561486244, 'total_duration': 42861.37547492981, 'accumulated_submission_time': 38933.89561486244, 'accumulated_eval_time': 3924.3125166893005, 'accumulated_logging_time': 1.1946141719818115, 'global_step': 46632, 'preemption_count': 0}), (48000, {'train/ctc_loss': Array(0.18561524, dtype=float32), 'train/wer': 0.0648674808861434, 'validation/ctc_loss': Array(0.4110458, dtype=float32), 'validation/wer': 0.11803778831207701, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22013809, dtype=float32), 'test/wer': 0.0707452318566815, 'test/num_examples': 2472, 'score': 40087.15713596344, 'total_duration': 44151.489364147186, 'accumulated_submission_time': 40087.15713596344, 'accumulated_eval_time': 4061.046733379364, 'accumulated_logging_time': 1.2506194114685059, 'global_step': 48000, 'preemption_count': 0})], 'global_step': 48000}
I0211 12:37:07.958538 139803787056960 submission_runner.py:586] Timing: 40087.15713596344
I0211 12:37:07.958602 139803787056960 submission_runner.py:588] Total number of evals: 29
I0211 12:37:07.958661 139803787056960 submission_runner.py:589] ====================
I0211 12:37:07.958728 139803787056960 submission_runner.py:542] Using RNG seed 808887856
I0211 12:37:07.961921 139803787056960 submission_runner.py:551] --- Tuning run 2/5 ---
I0211 12:37:07.962047 139803787056960 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_1/librispeech_deepspeech_jax/trial_2.
I0211 12:37:07.963578 139803787056960 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_1/librispeech_deepspeech_jax/trial_2/hparams.json.
I0211 12:37:07.965946 139803787056960 submission_runner.py:206] Initializing dataset.
I0211 12:37:07.966066 139803787056960 submission_runner.py:213] Initializing model.
I0211 12:37:09.114147 139803787056960 submission_runner.py:255] Initializing optimizer.
I0211 12:37:09.255212 139803787056960 submission_runner.py:262] Initializing metrics bundle.
I0211 12:37:09.255423 139803787056960 submission_runner.py:280] Initializing checkpoint and logger.
I0211 12:37:09.356282 139803787056960 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_1/librispeech_deepspeech_jax/trial_2 with prefix checkpoint_
I0211 12:37:09.356408 139803787056960 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_1/librispeech_deepspeech_jax/trial_2/meta_data_0.json.
I0211 12:37:09.356729 139803787056960 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0211 12:37:09.356808 139803787056960 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0211 12:37:09.919985 139803787056960 logger_utils.py:220] Unable to record git information. Continuing without it.
I0211 12:37:10.389170 139803787056960 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_1/librispeech_deepspeech_jax/trial_2/flags_0.json.
I0211 12:37:10.517577 139803787056960 submission_runner.py:314] Starting training loop.
I0211 12:37:10.520927 139803787056960 input_pipeline.py:20] Loading split = train-clean-100
I0211 12:37:10.565588 139803787056960 input_pipeline.py:20] Loading split = train-clean-360
I0211 12:37:11.200668 139803787056960 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0211 12:37:26.185313 139647758186240 logging_writer.py:48] [0] global_step=0, grad_norm=18.08152961730957, loss=33.3547477722168
I0211 12:37:26.201161 139803787056960 spec.py:321] Evaluating on the training split.
I0211 12:39:22.278546 139803787056960 spec.py:333] Evaluating on the validation split.
I0211 12:40:30.682765 139803787056960 spec.py:349] Evaluating on the test split.
I0211 12:41:05.156134 139803787056960 submission_runner.py:408] Time since start: 234.64s, 	Step: 1, 	{'train/ctc_loss': Array(31.934551, dtype=float32), 'train/wer': 4.645983478794528, 'validation/ctc_loss': Array(30.812979, dtype=float32), 'validation/wer': 4.233507438910182, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.939756, dtype=float32), 'test/wer': 4.561208945219669, 'test/num_examples': 2472, 'score': 15.68349552154541, 'total_duration': 234.63548374176025, 'accumulated_submission_time': 15.68349552154541, 'accumulated_eval_time': 218.9519054889679, 'accumulated_logging_time': 0}
I0211 12:41:05.176016 139647833720576 logging_writer.py:48] [1] accumulated_eval_time=218.951905, accumulated_logging_time=0, accumulated_submission_time=15.683496, global_step=1, preemption_count=0, score=15.683496, test/ctc_loss=30.939756393432617, test/num_examples=2472, test/wer=4.561209, total_duration=234.635484, train/ctc_loss=31.934551239013672, train/wer=4.645983, validation/ctc_loss=30.812978744506836, validation/num_examples=5348, validation/wer=4.233507
I0211 12:42:31.080079 139647766578944 logging_writer.py:48] [100] global_step=100, grad_norm=2.5242908000946045, loss=7.075326919555664
I0211 12:43:48.214977 139647774971648 logging_writer.py:48] [200] global_step=200, grad_norm=1.5059903860092163, loss=6.113150596618652
I0211 12:45:05.817563 139647766578944 logging_writer.py:48] [300] global_step=300, grad_norm=0.5311671495437622, loss=5.8837385177612305
I0211 12:46:23.029148 139647774971648 logging_writer.py:48] [400] global_step=400, grad_norm=1.0505541563034058, loss=5.832569122314453
I0211 12:47:45.687805 139647766578944 logging_writer.py:48] [500] global_step=500, grad_norm=1.2825002670288086, loss=5.736110687255859
I0211 12:49:15.858583 139647774971648 logging_writer.py:48] [600] global_step=600, grad_norm=1.6838932037353516, loss=5.571105003356934
I0211 12:50:42.991155 139647766578944 logging_writer.py:48] [700] global_step=700, grad_norm=0.7035455107688904, loss=5.30106258392334
I0211 12:52:14.473574 139647774971648 logging_writer.py:48] [800] global_step=800, grad_norm=1.2238985300064087, loss=4.8200788497924805
I0211 12:53:42.584529 139647766578944 logging_writer.py:48] [900] global_step=900, grad_norm=1.4909697771072388, loss=4.401895523071289
I0211 12:55:11.723330 139647774971648 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.47641122341156, loss=4.0144524574279785
I0211 12:56:35.609416 139647833720576 logging_writer.py:48] [1100] global_step=1100, grad_norm=3.566312313079834, loss=3.7903988361358643
I0211 12:57:53.189190 139647825327872 logging_writer.py:48] [1200] global_step=1200, grad_norm=4.740507125854492, loss=3.6618895530700684
I0211 12:59:10.022718 139647833720576 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.932847023010254, loss=3.3293161392211914
I0211 13:00:32.171063 139647825327872 logging_writer.py:48] [1400] global_step=1400, grad_norm=2.1489784717559814, loss=3.2291622161865234
I0211 13:01:59.078460 139647833720576 logging_writer.py:48] [1500] global_step=1500, grad_norm=2.239551067352295, loss=3.1137523651123047
I0211 13:03:28.181209 139647825327872 logging_writer.py:48] [1600] global_step=1600, grad_norm=3.2106335163116455, loss=3.0365231037139893
I0211 13:04:58.626595 139647833720576 logging_writer.py:48] [1700] global_step=1700, grad_norm=2.0507938861846924, loss=2.8983912467956543
I0211 13:05:05.666027 139803787056960 spec.py:321] Evaluating on the training split.
I0211 13:05:46.155169 139803787056960 spec.py:333] Evaluating on the validation split.
I0211 13:06:33.792751 139803787056960 spec.py:349] Evaluating on the test split.
I0211 13:06:57.783862 139803787056960 submission_runner.py:408] Time since start: 1787.26s, 	Step: 1709, 	{'train/ctc_loss': Array(6.530197, dtype=float32), 'train/wer': 0.936823569833362, 'validation/ctc_loss': Array(6.3885655, dtype=float32), 'validation/wer': 0.8922154532376879, 'validation/num_examples': 5348, 'test/ctc_loss': Array(6.318093, dtype=float32), 'test/wer': 0.895131314362318, 'test/num_examples': 2472, 'score': 1456.0923523902893, 'total_duration': 1787.2602503299713, 'accumulated_submission_time': 1456.0923523902893, 'accumulated_eval_time': 331.06379795074463, 'accumulated_logging_time': 0.03165459632873535}
I0211 13:06:57.818110 139647833720576 logging_writer.py:48] [1709] accumulated_eval_time=331.063798, accumulated_logging_time=0.031655, accumulated_submission_time=1456.092352, global_step=1709, preemption_count=0, score=1456.092352, test/ctc_loss=6.3180928230285645, test/num_examples=2472, test/wer=0.895131, total_duration=1787.260250, train/ctc_loss=6.5301971435546875, train/wer=0.936824, validation/ctc_loss=6.388565540313721, validation/num_examples=5348, validation/wer=0.892215
I0211 13:08:07.526533 139647825327872 logging_writer.py:48] [1800] global_step=1800, grad_norm=3.278658151626587, loss=2.888505458831787
I0211 13:09:24.135349 139647833720576 logging_writer.py:48] [1900] global_step=1900, grad_norm=2.9053266048431396, loss=2.725348949432373
I0211 13:10:50.386096 139647825327872 logging_writer.py:48] [2000] global_step=2000, grad_norm=2.7090671062469482, loss=2.6559107303619385
I0211 13:12:20.159774 139647833720576 logging_writer.py:48] [2100] global_step=2100, grad_norm=2.450167655944824, loss=2.6327593326568604
I0211 13:13:37.215737 139647825327872 logging_writer.py:48] [2200] global_step=2200, grad_norm=2.5878870487213135, loss=2.6130495071411133
I0211 13:14:56.991634 139647833720576 logging_writer.py:48] [2300] global_step=2300, grad_norm=3.716158151626587, loss=2.511782646179199
I0211 13:16:16.812916 139647825327872 logging_writer.py:48] [2400] global_step=2400, grad_norm=2.506098747253418, loss=2.4632930755615234
I0211 13:17:44.027987 139647833720576 logging_writer.py:48] [2500] global_step=2500, grad_norm=3.2349400520324707, loss=2.4713504314422607
I0211 13:19:13.353951 139647825327872 logging_writer.py:48] [2600] global_step=2600, grad_norm=3.2569634914398193, loss=2.3397016525268555
I0211 13:20:43.735446 139647833720576 logging_writer.py:48] [2700] global_step=2700, grad_norm=2.352757453918457, loss=2.347508668899536
I0211 13:22:15.421200 139647825327872 logging_writer.py:48] [2800] global_step=2800, grad_norm=3.3872854709625244, loss=2.3190104961395264
I0211 13:23:44.361659 139647833720576 logging_writer.py:48] [2900] global_step=2900, grad_norm=1.902252435684204, loss=2.265009880065918
I0211 13:25:11.341117 139647825327872 logging_writer.py:48] [3000] global_step=3000, grad_norm=3.078403949737549, loss=2.1975603103637695
I0211 13:26:39.694254 139647833720576 logging_writer.py:48] [3100] global_step=3100, grad_norm=3.6631665229797363, loss=2.1956233978271484
I0211 13:27:55.816474 139647825327872 logging_writer.py:48] [3200] global_step=3200, grad_norm=2.872180461883545, loss=2.213019609451294
I0211 13:29:15.015383 139647833720576 logging_writer.py:48] [3300] global_step=3300, grad_norm=2.7191131114959717, loss=2.235421895980835
I0211 13:30:35.155638 139647825327872 logging_writer.py:48] [3400] global_step=3400, grad_norm=5.170846462249756, loss=2.1683013439178467
I0211 13:30:58.182610 139803787056960 spec.py:321] Evaluating on the training split.
I0211 13:31:48.781318 139803787056960 spec.py:333] Evaluating on the validation split.
I0211 13:32:39.208168 139803787056960 spec.py:349] Evaluating on the test split.
I0211 13:33:05.815811 139803787056960 submission_runner.py:408] Time since start: 3355.30s, 	Step: 3429, 	{'train/ctc_loss': Array(2.8725617, dtype=float32), 'train/wer': 0.6321504898078738, 'validation/ctc_loss': Array(2.7419052, dtype=float32), 'validation/wer': 0.5899089566216438, 'validation/num_examples': 5348, 'test/ctc_loss': Array(2.2226102, dtype=float32), 'test/wer': 0.5148985436597404, 'test/num_examples': 2472, 'score': 2896.3682050704956, 'total_duration': 3355.2950081825256, 'accumulated_submission_time': 2896.3682050704956, 'accumulated_eval_time': 458.69384694099426, 'accumulated_logging_time': 0.08218216896057129}
I0211 13:33:05.843980 139647833720576 logging_writer.py:48] [3429] accumulated_eval_time=458.693847, accumulated_logging_time=0.082182, accumulated_submission_time=2896.368205, global_step=3429, preemption_count=0, score=2896.368205, test/ctc_loss=2.2226102352142334, test/num_examples=2472, test/wer=0.514899, total_duration=3355.295008, train/ctc_loss=2.8725616931915283, train/wer=0.632150, validation/ctc_loss=2.7419052124023438, validation/num_examples=5348, validation/wer=0.589909
I0211 13:33:59.931120 139647825327872 logging_writer.py:48] [3500] global_step=3500, grad_norm=2.5189924240112305, loss=2.0398833751678467
I0211 13:35:15.686137 139647833720576 logging_writer.py:48] [3600] global_step=3600, grad_norm=2.7711563110351562, loss=2.136078357696533
I0211 13:36:38.389354 139647825327872 logging_writer.py:48] [3700] global_step=3700, grad_norm=3.329045534133911, loss=2.078533411026001
I0211 13:38:06.225743 139647833720576 logging_writer.py:48] [3800] global_step=3800, grad_norm=3.627455949783325, loss=2.0277020931243896
I0211 13:39:36.159282 139647825327872 logging_writer.py:48] [3900] global_step=3900, grad_norm=2.164419174194336, loss=1.902420163154602
I0211 13:41:05.080717 139647833720576 logging_writer.py:48] [4000] global_step=4000, grad_norm=2.404573917388916, loss=2.0200464725494385
I0211 13:42:33.801510 139647825327872 logging_writer.py:48] [4100] global_step=4100, grad_norm=1.8839994668960571, loss=2.0115132331848145
I0211 13:43:55.955567 139647833720576 logging_writer.py:48] [4200] global_step=4200, grad_norm=2.2844748497009277, loss=1.9597593545913696
I0211 13:45:12.542280 139647825327872 logging_writer.py:48] [4300] global_step=4300, grad_norm=4.921151161193848, loss=1.912290096282959
I0211 13:46:27.502049 139647833720576 logging_writer.py:48] [4400] global_step=4400, grad_norm=3.001487970352173, loss=1.9007326364517212
I0211 13:47:47.974406 139647825327872 logging_writer.py:48] [4500] global_step=4500, grad_norm=2.294607639312744, loss=1.9312187433242798
I0211 13:49:16.436234 139647833720576 logging_writer.py:48] [4600] global_step=4600, grad_norm=2.378049850463867, loss=1.958261489868164
I0211 13:50:46.485363 139647825327872 logging_writer.py:48] [4700] global_step=4700, grad_norm=2.184464454650879, loss=1.8807339668273926
I0211 13:52:15.757756 139647833720576 logging_writer.py:48] [4800] global_step=4800, grad_norm=2.9800961017608643, loss=1.8762829303741455
I0211 13:53:45.911949 139647825327872 logging_writer.py:48] [4900] global_step=4900, grad_norm=2.208218812942505, loss=1.8535276651382446
I0211 13:55:16.568895 139647833720576 logging_writer.py:48] [5000] global_step=5000, grad_norm=3.1875760555267334, loss=1.8046122789382935
I0211 13:56:42.710266 139647825327872 logging_writer.py:48] [5100] global_step=5100, grad_norm=6.528563022613525, loss=1.8962033987045288
I0211 13:57:06.056555 139803787056960 spec.py:321] Evaluating on the training split.
I0211 13:57:59.962662 139803787056960 spec.py:333] Evaluating on the validation split.
I0211 13:58:51.823212 139803787056960 spec.py:349] Evaluating on the test split.
I0211 13:59:18.160277 139803787056960 submission_runner.py:408] Time since start: 4927.64s, 	Step: 5129, 	{'train/ctc_loss': Array(0.80511504, dtype=float32), 'train/wer': 0.25221808143547275, 'validation/ctc_loss': Array(0.916454, dtype=float32), 'validation/wer': 0.2608590710292826, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5947173, dtype=float32), 'test/wer': 0.19468649076838707, 'test/num_examples': 2472, 'score': 4336.491911649704, 'total_duration': 4927.635001182556, 'accumulated_submission_time': 4336.491911649704, 'accumulated_eval_time': 590.7899439334869, 'accumulated_logging_time': 0.1266040802001953}
I0211 13:59:18.193186 139647833720576 logging_writer.py:48] [5129] accumulated_eval_time=590.789944, accumulated_logging_time=0.126604, accumulated_submission_time=4336.491912, global_step=5129, preemption_count=0, score=4336.491912, test/ctc_loss=0.5947173237800598, test/num_examples=2472, test/wer=0.194686, total_duration=4927.635001, train/ctc_loss=0.8051150441169739, train/wer=0.252218, validation/ctc_loss=0.916454017162323, validation/num_examples=5348, validation/wer=0.260859
I0211 14:00:16.369850 139647833720576 logging_writer.py:48] [5200] global_step=5200, grad_norm=2.779154062271118, loss=1.8181949853897095
I0211 14:01:34.353671 139647825327872 logging_writer.py:48] [5300] global_step=5300, grad_norm=3.0150043964385986, loss=1.7794671058654785
I0211 14:02:53.472943 139647833720576 logging_writer.py:48] [5400] global_step=5400, grad_norm=2.5082356929779053, loss=1.8373363018035889
I0211 14:04:17.844440 139647825327872 logging_writer.py:48] [5500] global_step=5500, grad_norm=2.3344292640686035, loss=1.8041585683822632
I0211 14:05:44.694474 139647833720576 logging_writer.py:48] [5600] global_step=5600, grad_norm=4.028637886047363, loss=1.7869513034820557
I0211 14:07:14.589987 139647825327872 logging_writer.py:48] [5700] global_step=5700, grad_norm=2.702340602874756, loss=1.6940302848815918
I0211 14:08:41.227203 139647833720576 logging_writer.py:48] [5800] global_step=5800, grad_norm=3.0405070781707764, loss=1.8194152116775513
I0211 14:10:11.321283 139647825327872 logging_writer.py:48] [5900] global_step=5900, grad_norm=3.0059752464294434, loss=1.752241849899292
I0211 14:11:41.315759 139647833720576 logging_writer.py:48] [6000] global_step=6000, grad_norm=4.190880298614502, loss=1.7349390983581543
I0211 14:13:10.958216 139647825327872 logging_writer.py:48] [6100] global_step=6100, grad_norm=2.187859296798706, loss=1.7487950325012207
I0211 14:14:39.409506 139647833720576 logging_writer.py:48] [6200] global_step=6200, grad_norm=2.3186938762664795, loss=1.7042368650436401
I0211 14:15:56.131739 139647825327872 logging_writer.py:48] [6300] global_step=6300, grad_norm=5.123795986175537, loss=1.784973382949829
I0211 14:17:13.394800 139647833720576 logging_writer.py:48] [6400] global_step=6400, grad_norm=3.5725879669189453, loss=1.7606923580169678
I0211 14:18:36.086883 139647825327872 logging_writer.py:48] [6500] global_step=6500, grad_norm=2.098280191421509, loss=1.7190041542053223
I0211 14:19:57.903831 139647833720576 logging_writer.py:48] [6600] global_step=6600, grad_norm=2.430300235748291, loss=1.7517588138580322
I0211 14:21:25.665773 139647825327872 logging_writer.py:48] [6700] global_step=6700, grad_norm=2.12735915184021, loss=1.737680196762085
I0211 14:22:58.332590 139647833720576 logging_writer.py:48] [6800] global_step=6800, grad_norm=3.194391965866089, loss=1.687588095664978
I0211 14:23:19.489390 139803787056960 spec.py:321] Evaluating on the training split.
I0211 14:24:13.252616 139803787056960 spec.py:333] Evaluating on the validation split.
I0211 14:25:07.015700 139803787056960 spec.py:349] Evaluating on the test split.
I0211 14:25:34.190463 139803787056960 submission_runner.py:408] Time since start: 6503.67s, 	Step: 6825, 	{'train/ctc_loss': Array(0.6916581, dtype=float32), 'train/wer': 0.22251410780307454, 'validation/ctc_loss': Array(0.80308455, dtype=float32), 'validation/wer': 0.23031174874730875, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.49312124, dtype=float32), 'test/wer': 0.16295980338390917, 'test/num_examples': 2472, 'score': 5777.701329469681, 'total_duration': 6503.6665942668915, 'accumulated_submission_time': 5777.701329469681, 'accumulated_eval_time': 725.4847946166992, 'accumulated_logging_time': 0.17483735084533691}
I0211 14:25:34.228898 139647833720576 logging_writer.py:48] [6825] accumulated_eval_time=725.484795, accumulated_logging_time=0.174837, accumulated_submission_time=5777.701329, global_step=6825, preemption_count=0, score=5777.701329, test/ctc_loss=0.4931212365627289, test/num_examples=2472, test/wer=0.162960, total_duration=6503.666594, train/ctc_loss=0.691658079624176, train/wer=0.222514, validation/ctc_loss=0.8030845522880554, validation/num_examples=5348, validation/wer=0.230312
I0211 14:26:32.052967 139647825327872 logging_writer.py:48] [6900] global_step=6900, grad_norm=1.6073541641235352, loss=1.7285419702529907
I0211 14:27:47.237041 139647833720576 logging_writer.py:48] [7000] global_step=7000, grad_norm=3.3530149459838867, loss=1.7181034088134766
I0211 14:29:05.452738 139647825327872 logging_writer.py:48] [7100] global_step=7100, grad_norm=2.7625598907470703, loss=1.7454373836517334
I0211 14:30:36.270350 139647833720576 logging_writer.py:48] [7200] global_step=7200, grad_norm=2.471088171005249, loss=1.6615039110183716
I0211 14:31:57.772075 139647833720576 logging_writer.py:48] [7300] global_step=7300, grad_norm=3.509791374206543, loss=1.6643325090408325
I0211 14:33:15.646786 139647825327872 logging_writer.py:48] [7400] global_step=7400, grad_norm=2.079749822616577, loss=1.6704210042953491
I0211 14:34:34.633397 139647833720576 logging_writer.py:48] [7500] global_step=7500, grad_norm=3.1938672065734863, loss=1.6363435983657837
I0211 14:35:57.184566 139647825327872 logging_writer.py:48] [7600] global_step=7600, grad_norm=1.685958743095398, loss=1.6483261585235596
I0211 14:37:22.498075 139647833720576 logging_writer.py:48] [7700] global_step=7700, grad_norm=2.8201904296875, loss=1.6745967864990234
I0211 14:38:53.160449 139647825327872 logging_writer.py:48] [7800] global_step=7800, grad_norm=2.888727903366089, loss=1.654434323310852
I0211 14:40:22.557084 139647833720576 logging_writer.py:48] [7900] global_step=7900, grad_norm=2.6148509979248047, loss=1.7143726348876953
I0211 14:41:51.185062 139647825327872 logging_writer.py:48] [8000] global_step=8000, grad_norm=3.6551380157470703, loss=1.6477116346359253
I0211 14:43:22.003347 139647833720576 logging_writer.py:48] [8100] global_step=8100, grad_norm=4.342436790466309, loss=1.6992765665054321
I0211 14:44:47.333616 139647825327872 logging_writer.py:48] [8200] global_step=8200, grad_norm=2.1580328941345215, loss=1.7226958274841309
I0211 14:46:12.479132 139647833720576 logging_writer.py:48] [8300] global_step=8300, grad_norm=2.0826797485351562, loss=1.6163835525512695
I0211 14:47:28.536417 139647825327872 logging_writer.py:48] [8400] global_step=8400, grad_norm=1.823144555091858, loss=1.6009246110916138
I0211 14:48:49.401802 139647833720576 logging_writer.py:48] [8500] global_step=8500, grad_norm=3.290600538253784, loss=1.628413200378418
I0211 14:49:35.030962 139803787056960 spec.py:321] Evaluating on the training split.
I0211 14:50:29.531385 139803787056960 spec.py:333] Evaluating on the validation split.
I0211 14:51:22.215899 139803787056960 spec.py:349] Evaluating on the test split.
I0211 14:51:48.532706 139803787056960 submission_runner.py:408] Time since start: 8078.01s, 	Step: 8557, 	{'train/ctc_loss': Array(0.58069324, dtype=float32), 'train/wer': 0.18633398316252628, 'validation/ctc_loss': Array(0.71140873, dtype=float32), 'validation/wer': 0.20462071695453624, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.42479584, dtype=float32), 'test/wer': 0.13968273312615523, 'test/num_examples': 2472, 'score': 7218.413636207581, 'total_duration': 8078.009069681168, 'accumulated_submission_time': 7218.413636207581, 'accumulated_eval_time': 858.9805746078491, 'accumulated_logging_time': 0.23076987266540527}
I0211 14:51:48.565360 139647833720576 logging_writer.py:48] [8557] accumulated_eval_time=858.980575, accumulated_logging_time=0.230770, accumulated_submission_time=7218.413636, global_step=8557, preemption_count=0, score=7218.413636, test/ctc_loss=0.42479583621025085, test/num_examples=2472, test/wer=0.139683, total_duration=8078.009070, train/ctc_loss=0.580693244934082, train/wer=0.186334, validation/ctc_loss=0.7114087343215942, validation/num_examples=5348, validation/wer=0.204621
I0211 14:52:22.014908 139647825327872 logging_writer.py:48] [8600] global_step=8600, grad_norm=2.121006727218628, loss=1.574975848197937
I0211 14:53:37.021318 139647833720576 logging_writer.py:48] [8700] global_step=8700, grad_norm=2.2809622287750244, loss=1.57125723361969
I0211 14:54:56.390285 139647825327872 logging_writer.py:48] [8800] global_step=8800, grad_norm=1.6022014617919922, loss=1.5491949319839478
I0211 14:56:24.942965 139647833720576 logging_writer.py:48] [8900] global_step=8900, grad_norm=2.0467865467071533, loss=1.6191871166229248
I0211 14:57:52.659013 139647825327872 logging_writer.py:48] [9000] global_step=9000, grad_norm=2.1643738746643066, loss=1.6068392992019653
I0211 14:59:22.691269 139647833720576 logging_writer.py:48] [9100] global_step=9100, grad_norm=2.2167115211486816, loss=1.615797758102417
I0211 15:00:52.161758 139647825327872 logging_writer.py:48] [9200] global_step=9200, grad_norm=2.4783496856689453, loss=1.5892468690872192
I0211 15:02:18.608632 139647833720576 logging_writer.py:48] [9300] global_step=9300, grad_norm=5.034454345703125, loss=1.608137607574463
I0211 15:03:35.425288 139647825327872 logging_writer.py:48] [9400] global_step=9400, grad_norm=1.7017582654953003, loss=1.5861777067184448
I0211 15:04:51.681269 139647833720576 logging_writer.py:48] [9500] global_step=9500, grad_norm=2.4029359817504883, loss=1.565812587738037
I0211 15:06:13.149596 139647825327872 logging_writer.py:48] [9600] global_step=9600, grad_norm=3.4348814487457275, loss=1.6578547954559326
I0211 15:07:36.052749 139647833720576 logging_writer.py:48] [9700] global_step=9700, grad_norm=3.2678143978118896, loss=1.6119858026504517
I0211 15:09:03.147898 139647825327872 logging_writer.py:48] [9800] global_step=9800, grad_norm=3.5828471183776855, loss=1.5505428314208984
I0211 15:10:34.389003 139647833720576 logging_writer.py:48] [9900] global_step=9900, grad_norm=2.773165702819824, loss=1.5497339963912964
I0211 15:12:05.129933 139647825327872 logging_writer.py:48] [10000] global_step=10000, grad_norm=2.567110300064087, loss=1.5795509815216064
I0211 15:13:33.624517 139647833720576 logging_writer.py:48] [10100] global_step=10100, grad_norm=3.8056390285491943, loss=1.580740213394165
I0211 15:15:01.257931 139647825327872 logging_writer.py:48] [10200] global_step=10200, grad_norm=1.7125989198684692, loss=1.5667157173156738
I0211 15:15:49.302810 139803787056960 spec.py:321] Evaluating on the training split.
I0211 15:16:44.039558 139803787056960 spec.py:333] Evaluating on the validation split.
I0211 15:17:38.233934 139803787056960 spec.py:349] Evaluating on the test split.
I0211 15:18:05.853080 139803787056960 submission_runner.py:408] Time since start: 9655.33s, 	Step: 10255, 	{'train/ctc_loss': Array(0.5201113, dtype=float32), 'train/wer': 0.16978276952047885, 'validation/ctc_loss': Array(0.67553055, dtype=float32), 'validation/wer': 0.19477297083329312, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.40202525, dtype=float32), 'test/wer': 0.1314565433753783, 'test/num_examples': 2472, 'score': 8659.064038991928, 'total_duration': 9655.32750749588, 'accumulated_submission_time': 8659.064038991928, 'accumulated_eval_time': 995.5229451656342, 'accumulated_logging_time': 0.27828240394592285}
I0211 15:18:05.888278 139647833720576 logging_writer.py:48] [10255] accumulated_eval_time=995.522945, accumulated_logging_time=0.278282, accumulated_submission_time=8659.064039, global_step=10255, preemption_count=0, score=8659.064039, test/ctc_loss=0.4020252525806427, test/num_examples=2472, test/wer=0.131457, total_duration=9655.327507, train/ctc_loss=0.5201113224029541, train/wer=0.169783, validation/ctc_loss=0.6755305528640747, validation/num_examples=5348, validation/wer=0.194773
I0211 15:18:44.074584 139647833720576 logging_writer.py:48] [10300] global_step=10300, grad_norm=2.8914339542388916, loss=1.5811723470687866
I0211 15:20:02.199921 139647825327872 logging_writer.py:48] [10400] global_step=10400, grad_norm=2.903110980987549, loss=1.6473215818405151
I0211 15:21:18.271804 139647833720576 logging_writer.py:48] [10500] global_step=10500, grad_norm=2.1344380378723145, loss=1.5463789701461792
I0211 15:22:40.975247 139647825327872 logging_writer.py:48] [10600] global_step=10600, grad_norm=2.1965253353118896, loss=1.5703669786453247
I0211 15:24:00.880827 139647833720576 logging_writer.py:48] [10700] global_step=10700, grad_norm=2.6330296993255615, loss=1.5629392862319946
I0211 15:25:29.114212 139647825327872 logging_writer.py:48] [10800] global_step=10800, grad_norm=2.956754207611084, loss=1.5937504768371582
I0211 15:26:56.532545 139647833720576 logging_writer.py:48] [10900] global_step=10900, grad_norm=2.3300201892852783, loss=1.6034358739852905
I0211 15:28:28.037309 139647825327872 logging_writer.py:48] [11000] global_step=11000, grad_norm=2.212700843811035, loss=1.4870808124542236
I0211 15:29:59.177409 139647833720576 logging_writer.py:48] [11100] global_step=11100, grad_norm=2.971588134765625, loss=1.6443276405334473
I0211 15:31:26.737425 139647825327872 logging_writer.py:48] [11200] global_step=11200, grad_norm=3.0492074489593506, loss=1.5639311075210571
I0211 15:32:58.705167 139647833720576 logging_writer.py:48] [11300] global_step=11300, grad_norm=2.273115634918213, loss=1.5452373027801514
I0211 15:34:22.920833 139647833720576 logging_writer.py:48] [11400] global_step=11400, grad_norm=3.7771081924438477, loss=1.4372540712356567
I0211 15:35:39.003136 139647825327872 logging_writer.py:48] [11500] global_step=11500, grad_norm=3.2893874645233154, loss=1.5681908130645752
I0211 15:36:57.046887 139647833720576 logging_writer.py:48] [11600] global_step=11600, grad_norm=6.702749252319336, loss=1.4960659742355347
I0211 15:38:22.074762 139647825327872 logging_writer.py:48] [11700] global_step=11700, grad_norm=2.8948426246643066, loss=1.5059317350387573
I0211 15:39:50.287598 139647833720576 logging_writer.py:48] [11800] global_step=11800, grad_norm=2.840407133102417, loss=1.5790611505508423
I0211 15:41:22.413458 139647825327872 logging_writer.py:48] [11900] global_step=11900, grad_norm=2.8465607166290283, loss=1.5585647821426392
I0211 15:42:06.151886 139803787056960 spec.py:321] Evaluating on the training split.
I0211 15:43:00.266049 139803787056960 spec.py:333] Evaluating on the validation split.
I0211 15:43:54.580526 139803787056960 spec.py:349] Evaluating on the test split.
I0211 15:44:21.171557 139803787056960 submission_runner.py:408] Time since start: 11230.65s, 	Step: 11949, 	{'train/ctc_loss': Array(0.50205463, dtype=float32), 'train/wer': 0.16703938660946557, 'validation/ctc_loss': Array(0.64501023, dtype=float32), 'validation/wer': 0.1888836324666673, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.37967756, dtype=float32), 'test/wer': 0.1240834399691264, 'test/num_examples': 2472, 'score': 10099.236858606339, 'total_duration': 11230.648217201233, 'accumulated_submission_time': 10099.236858606339, 'accumulated_eval_time': 1130.5369474887848, 'accumulated_logging_time': 0.3322751522064209}
I0211 15:44:21.205707 139647833720576 logging_writer.py:48] [11949] accumulated_eval_time=1130.536947, accumulated_logging_time=0.332275, accumulated_submission_time=10099.236859, global_step=11949, preemption_count=0, score=10099.236859, test/ctc_loss=0.37967756390571594, test/num_examples=2472, test/wer=0.124083, total_duration=11230.648217, train/ctc_loss=0.5020546317100525, train/wer=0.167039, validation/ctc_loss=0.645010232925415, validation/num_examples=5348, validation/wer=0.188884
I0211 15:45:00.667723 139647825327872 logging_writer.py:48] [12000] global_step=12000, grad_norm=3.4587223529815674, loss=1.5206506252288818
I0211 15:46:16.455500 139647833720576 logging_writer.py:48] [12100] global_step=12100, grad_norm=2.1536285877227783, loss=1.4717556238174438
I0211 15:47:34.866928 139647825327872 logging_writer.py:48] [12200] global_step=12200, grad_norm=2.9042367935180664, loss=1.5087060928344727
I0211 15:49:02.956970 139647833720576 logging_writer.py:48] [12300] global_step=12300, grad_norm=2.499351739883423, loss=1.4768940210342407
I0211 15:50:30.176224 139647833720576 logging_writer.py:48] [12400] global_step=12400, grad_norm=2.1811537742614746, loss=1.452900767326355
I0211 15:51:47.506603 139647825327872 logging_writer.py:48] [12500] global_step=12500, grad_norm=2.6446304321289062, loss=1.5469896793365479
I0211 15:53:09.851574 139647833720576 logging_writer.py:48] [12600] global_step=12600, grad_norm=2.4897875785827637, loss=1.554674506187439
I0211 15:54:32.399967 139647825327872 logging_writer.py:48] [12700] global_step=12700, grad_norm=2.2738595008850098, loss=1.4544329643249512
I0211 15:55:58.853832 139647833720576 logging_writer.py:48] [12800] global_step=12800, grad_norm=5.08098840713501, loss=1.5037392377853394
I0211 15:57:28.927699 139647825327872 logging_writer.py:48] [12900] global_step=12900, grad_norm=4.559365749359131, loss=1.5267351865768433
I0211 15:59:00.141747 139647833720576 logging_writer.py:48] [13000] global_step=13000, grad_norm=2.4041199684143066, loss=1.5474722385406494
I0211 16:00:30.015274 139647825327872 logging_writer.py:48] [13100] global_step=13100, grad_norm=1.9774963855743408, loss=1.5072575807571411
I0211 16:01:59.448630 139647833720576 logging_writer.py:48] [13200] global_step=13200, grad_norm=1.4992142915725708, loss=1.4580689668655396
I0211 16:03:30.355904 139647825327872 logging_writer.py:48] [13300] global_step=13300, grad_norm=2.317089796066284, loss=1.4998159408569336
I0211 16:04:58.924047 139647833720576 logging_writer.py:48] [13400] global_step=13400, grad_norm=2.7351648807525635, loss=1.4686415195465088
I0211 16:06:15.299964 139647825327872 logging_writer.py:48] [13500] global_step=13500, grad_norm=4.173064231872559, loss=1.4932091236114502
I0211 16:07:32.547424 139647833720576 logging_writer.py:48] [13600] global_step=13600, grad_norm=2.7244067192077637, loss=1.509418249130249
I0211 16:08:21.963495 139803787056960 spec.py:321] Evaluating on the training split.
I0211 16:09:18.209235 139803787056960 spec.py:333] Evaluating on the validation split.
I0211 16:10:11.171082 139803787056960 spec.py:349] Evaluating on the test split.
I0211 16:10:38.692389 139803787056960 submission_runner.py:408] Time since start: 12808.17s, 	Step: 13663, 	{'train/ctc_loss': Array(0.44674954, dtype=float32), 'train/wer': 0.14886399142971923, 'validation/ctc_loss': Array(0.61529595, dtype=float32), 'validation/wer': 0.17750079650887746, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3549172, dtype=float32), 'test/wer': 0.11644628602766437, 'test/num_examples': 2472, 'score': 11539.904070854187, 'total_duration': 12808.168608665466, 'accumulated_submission_time': 11539.904070854187, 'accumulated_eval_time': 1267.2597525119781, 'accumulated_logging_time': 0.38460445404052734}
I0211 16:10:38.729463 139647833720576 logging_writer.py:48] [13663] accumulated_eval_time=1267.259753, accumulated_logging_time=0.384604, accumulated_submission_time=11539.904071, global_step=13663, preemption_count=0, score=11539.904071, test/ctc_loss=0.3549171984195709, test/num_examples=2472, test/wer=0.116446, total_duration=12808.168609, train/ctc_loss=0.4467495381832123, train/wer=0.148864, validation/ctc_loss=0.615295946598053, validation/num_examples=5348, validation/wer=0.177501
I0211 16:11:07.555424 139647825327872 logging_writer.py:48] [13700] global_step=13700, grad_norm=2.896322250366211, loss=1.479301929473877
I0211 16:12:22.727382 139647833720576 logging_writer.py:48] [13800] global_step=13800, grad_norm=3.9810094833374023, loss=1.501875400543213
I0211 16:13:39.446482 139647825327872 logging_writer.py:48] [13900] global_step=13900, grad_norm=3.017486095428467, loss=1.4601249694824219
I0211 16:15:08.856411 139647833720576 logging_writer.py:48] [14000] global_step=14000, grad_norm=3.1601738929748535, loss=1.5033751726150513
I0211 16:16:37.217143 139647825327872 logging_writer.py:48] [14100] global_step=14100, grad_norm=2.9815497398376465, loss=1.480848789215088
I0211 16:18:06.143279 139647833720576 logging_writer.py:48] [14200] global_step=14200, grad_norm=1.9157389402389526, loss=1.4558970928192139
I0211 16:19:36.143956 139647825327872 logging_writer.py:48] [14300] global_step=14300, grad_norm=2.182605028152466, loss=1.438950538635254
I0211 16:21:03.775111 139647833720576 logging_writer.py:48] [14400] global_step=14400, grad_norm=3.0043725967407227, loss=1.5137628316879272
I0211 16:22:25.541924 139647833720576 logging_writer.py:48] [14500] global_step=14500, grad_norm=2.7896599769592285, loss=1.4679450988769531
I0211 16:23:42.354083 139647825327872 logging_writer.py:48] [14600] global_step=14600, grad_norm=2.4482171535491943, loss=1.3884011507034302
I0211 16:25:01.632961 139647833720576 logging_writer.py:48] [14700] global_step=14700, grad_norm=1.997904658317566, loss=1.4715700149536133
I0211 16:26:22.306926 139647825327872 logging_writer.py:48] [14800] global_step=14800, grad_norm=6.241908550262451, loss=1.4547351598739624
I0211 16:27:50.728532 139647833720576 logging_writer.py:48] [14900] global_step=14900, grad_norm=2.815757989883423, loss=1.5200550556182861
I0211 16:29:21.004298 139647825327872 logging_writer.py:48] [15000] global_step=15000, grad_norm=1.677581548690796, loss=1.4247937202453613
I0211 16:30:50.217219 139647833720576 logging_writer.py:48] [15100] global_step=15100, grad_norm=2.2200753688812256, loss=1.4762598276138306
I0211 16:32:19.582300 139647825327872 logging_writer.py:48] [15200] global_step=15200, grad_norm=1.7347341775894165, loss=1.409881830215454
I0211 16:33:48.240195 139647833720576 logging_writer.py:48] [15300] global_step=15300, grad_norm=2.702388048171997, loss=1.519905686378479
I0211 16:34:38.712300 139803787056960 spec.py:321] Evaluating on the training split.
I0211 16:35:32.256808 139803787056960 spec.py:333] Evaluating on the validation split.
I0211 16:36:26.370793 139803787056960 spec.py:349] Evaluating on the test split.
I0211 16:36:53.448641 139803787056960 submission_runner.py:408] Time since start: 14382.92s, 	Step: 15356, 	{'train/ctc_loss': Array(0.44082975, dtype=float32), 'train/wer': 0.14504916390054487, 'validation/ctc_loss': Array(0.59986717, dtype=float32), 'validation/wer': 0.17326240381551888, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3436971, dtype=float32), 'test/wer': 0.11252615115877562, 'test/num_examples': 2472, 'score': 12979.79869055748, 'total_duration': 14382.923528432846, 'accumulated_submission_time': 12979.79869055748, 'accumulated_eval_time': 1401.9886286258698, 'accumulated_logging_time': 0.43735527992248535}
I0211 16:36:53.481967 139647833720576 logging_writer.py:48] [15356] accumulated_eval_time=1401.988629, accumulated_logging_time=0.437355, accumulated_submission_time=12979.798691, global_step=15356, preemption_count=0, score=12979.798691, test/ctc_loss=0.34369710087776184, test/num_examples=2472, test/wer=0.112526, total_duration=14382.923528, train/ctc_loss=0.4408297538757324, train/wer=0.145049, validation/ctc_loss=0.5998671650886536, validation/num_examples=5348, validation/wer=0.173262
I0211 16:37:27.884583 139647825327872 logging_writer.py:48] [15400] global_step=15400, grad_norm=2.2442238330841064, loss=1.5033555030822754
I0211 16:38:48.437207 139647833720576 logging_writer.py:48] [15500] global_step=15500, grad_norm=3.9308300018310547, loss=1.428916573524475
I0211 16:40:07.582383 139647825327872 logging_writer.py:48] [15600] global_step=15600, grad_norm=3.582031488418579, loss=1.4472661018371582
I0211 16:41:27.782095 139647833720576 logging_writer.py:48] [15700] global_step=15700, grad_norm=4.606708526611328, loss=1.5087246894836426
I0211 16:42:48.354361 139647825327872 logging_writer.py:48] [15800] global_step=15800, grad_norm=3.6254220008850098, loss=1.4681929349899292
I0211 16:44:15.814437 139647833720576 logging_writer.py:48] [15900] global_step=15900, grad_norm=2.129671096801758, loss=1.4448710680007935
I0211 16:45:44.199368 139647825327872 logging_writer.py:48] [16000] global_step=16000, grad_norm=2.5836005210876465, loss=1.4500889778137207
I0211 16:47:11.166870 139647833720576 logging_writer.py:48] [16100] global_step=16100, grad_norm=2.733354091644287, loss=1.4392906427383423
I0211 16:48:39.745286 139647825327872 logging_writer.py:48] [16200] global_step=16200, grad_norm=2.6469929218292236, loss=1.4468460083007812
I0211 16:50:10.052533 139647833720576 logging_writer.py:48] [16300] global_step=16300, grad_norm=2.4669454097747803, loss=1.4421640634536743
I0211 16:51:38.182566 139647825327872 logging_writer.py:48] [16400] global_step=16400, grad_norm=1.607539176940918, loss=1.373276948928833
I0211 16:53:07.758458 139647833720576 logging_writer.py:48] [16500] global_step=16500, grad_norm=2.0193629264831543, loss=1.4426268339157104
I0211 16:54:25.181350 139647825327872 logging_writer.py:48] [16600] global_step=16600, grad_norm=4.533573150634766, loss=1.431746244430542
I0211 16:55:41.224646 139647833720576 logging_writer.py:48] [16700] global_step=16700, grad_norm=3.4300148487091064, loss=1.3661863803863525
I0211 16:57:02.520270 139647825327872 logging_writer.py:48] [16800] global_step=16800, grad_norm=3.149193048477173, loss=1.4786726236343384
I0211 16:58:24.205530 139647833720576 logging_writer.py:48] [16900] global_step=16900, grad_norm=2.6334965229034424, loss=1.4094016551971436
I0211 16:59:52.655468 139647825327872 logging_writer.py:48] [17000] global_step=17000, grad_norm=2.4810006618499756, loss=1.4089152812957764
I0211 17:00:54.520601 139803787056960 spec.py:321] Evaluating on the training split.
I0211 17:01:48.885957 139803787056960 spec.py:333] Evaluating on the validation split.
I0211 17:02:41.030295 139803787056960 spec.py:349] Evaluating on the test split.
I0211 17:03:08.203259 139803787056960 submission_runner.py:408] Time since start: 15957.68s, 	Step: 17068, 	{'train/ctc_loss': Array(0.4190414, dtype=float32), 'train/wer': 0.14002370251112453, 'validation/ctc_loss': Array(0.58253497, dtype=float32), 'validation/wer': 0.168261293530417, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3291357, dtype=float32), 'test/wer': 0.10757012572867793, 'test/num_examples': 2472, 'score': 14420.747940063477, 'total_duration': 15957.678583145142, 'accumulated_submission_time': 14420.747940063477, 'accumulated_eval_time': 1535.6642746925354, 'accumulated_logging_time': 0.4859333038330078}
I0211 17:03:08.248014 139647833720576 logging_writer.py:48] [17068] accumulated_eval_time=1535.664275, accumulated_logging_time=0.485933, accumulated_submission_time=14420.747940, global_step=17068, preemption_count=0, score=14420.747940, test/ctc_loss=0.3291356861591339, test/num_examples=2472, test/wer=0.107570, total_duration=15957.678583, train/ctc_loss=0.41904139518737793, train/wer=0.140024, validation/ctc_loss=0.5825349688529968, validation/num_examples=5348, validation/wer=0.168261
I0211 17:03:33.621783 139647825327872 logging_writer.py:48] [17100] global_step=17100, grad_norm=2.4913361072540283, loss=1.3966606855392456
I0211 17:04:49.679760 139647833720576 logging_writer.py:48] [17200] global_step=17200, grad_norm=2.033844232559204, loss=1.4733210802078247
I0211 17:06:07.073199 139647825327872 logging_writer.py:48] [17300] global_step=17300, grad_norm=2.7169482707977295, loss=1.472898244857788
I0211 17:07:33.962594 139647833720576 logging_writer.py:48] [17400] global_step=17400, grad_norm=2.150552988052368, loss=1.4524048566818237
I0211 17:09:02.269147 139647825327872 logging_writer.py:48] [17500] global_step=17500, grad_norm=2.3485774993896484, loss=1.3803133964538574
I0211 17:10:25.597921 139647833720576 logging_writer.py:48] [17600] global_step=17600, grad_norm=1.9458357095718384, loss=1.439535140991211
I0211 17:11:43.413019 139647825327872 logging_writer.py:48] [17700] global_step=17700, grad_norm=1.8455501794815063, loss=1.422070026397705
I0211 17:13:01.371483 139647833720576 logging_writer.py:48] [17800] global_step=17800, grad_norm=3.541903257369995, loss=1.4561564922332764
I0211 17:14:25.725397 139647825327872 logging_writer.py:48] [17900] global_step=17900, grad_norm=2.0137112140655518, loss=1.335553526878357
I0211 17:15:54.730080 139647833720576 logging_writer.py:48] [18000] global_step=18000, grad_norm=2.2738120555877686, loss=1.4482847452163696
I0211 17:17:21.031287 139647825327872 logging_writer.py:48] [18100] global_step=18100, grad_norm=2.326723575592041, loss=1.3995505571365356
I0211 17:18:53.104765 139647833720576 logging_writer.py:48] [18200] global_step=18200, grad_norm=1.7075397968292236, loss=1.406201720237732
I0211 17:20:20.482214 139647825327872 logging_writer.py:48] [18300] global_step=18300, grad_norm=2.4214694499969482, loss=1.458338975906372
I0211 17:21:49.824111 139647833720576 logging_writer.py:48] [18400] global_step=18400, grad_norm=3.3153443336486816, loss=1.4094289541244507
I0211 17:23:18.614441 139647825327872 logging_writer.py:48] [18500] global_step=18500, grad_norm=2.258139133453369, loss=1.3626409769058228
I0211 17:24:42.990285 139647833720576 logging_writer.py:48] [18600] global_step=18600, grad_norm=3.050014019012451, loss=1.4453649520874023
I0211 17:25:58.634771 139647825327872 logging_writer.py:48] [18700] global_step=18700, grad_norm=1.713128924369812, loss=1.373740792274475
I0211 17:27:08.505135 139803787056960 spec.py:321] Evaluating on the training split.
I0211 17:28:03.867793 139803787056960 spec.py:333] Evaluating on the validation split.
I0211 17:28:55.940481 139803787056960 spec.py:349] Evaluating on the test split.
I0211 17:29:22.611546 139803787056960 submission_runner.py:408] Time since start: 17532.09s, 	Step: 18794, 	{'train/ctc_loss': Array(0.39970213, dtype=float32), 'train/wer': 0.13711078888237052, 'validation/ctc_loss': Array(0.5615143, dtype=float32), 'validation/wer': 0.16312501810247448, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31767148, dtype=float32), 'test/wer': 0.10389372981536774, 'test/num_examples': 2472, 'score': 15860.9145257473, 'total_duration': 17532.088482141495, 'accumulated_submission_time': 15860.9145257473, 'accumulated_eval_time': 1669.7652735710144, 'accumulated_logging_time': 0.5466783046722412}
I0211 17:29:22.649208 139647833720576 logging_writer.py:48] [18794] accumulated_eval_time=1669.765274, accumulated_logging_time=0.546678, accumulated_submission_time=15860.914526, global_step=18794, preemption_count=0, score=15860.914526, test/ctc_loss=0.3176714777946472, test/num_examples=2472, test/wer=0.103894, total_duration=17532.088482, train/ctc_loss=0.39970213174819946, train/wer=0.137111, validation/ctc_loss=0.5615143179893494, validation/num_examples=5348, validation/wer=0.163125
I0211 17:29:28.013824 139647825327872 logging_writer.py:48] [18800] global_step=18800, grad_norm=4.139267921447754, loss=1.358838438987732
I0211 17:30:43.514867 139647833720576 logging_writer.py:48] [18900] global_step=18900, grad_norm=2.1125967502593994, loss=1.396273136138916
I0211 17:31:58.951247 139647825327872 logging_writer.py:48] [19000] global_step=19000, grad_norm=1.890871524810791, loss=1.40311598777771
I0211 17:33:20.591402 139647833720576 logging_writer.py:48] [19100] global_step=19100, grad_norm=2.4297244548797607, loss=1.413781762123108
I0211 17:34:47.179071 139647825327872 logging_writer.py:48] [19200] global_step=19200, grad_norm=2.922274112701416, loss=1.3559871912002563
I0211 17:36:17.182036 139647833720576 logging_writer.py:48] [19300] global_step=19300, grad_norm=2.4129369258880615, loss=1.3534176349639893
I0211 17:37:47.843363 139647825327872 logging_writer.py:48] [19400] global_step=19400, grad_norm=1.8725638389587402, loss=1.3878196477890015
I0211 17:39:16.575840 139647833720576 logging_writer.py:48] [19500] global_step=19500, grad_norm=2.2520411014556885, loss=1.3730881214141846
I0211 17:40:45.406688 139647833720576 logging_writer.py:48] [19600] global_step=19600, grad_norm=2.1467955112457275, loss=1.4520245790481567
I0211 17:42:03.282335 139647825327872 logging_writer.py:48] [19700] global_step=19700, grad_norm=2.532078266143799, loss=1.4155526161193848
I0211 17:43:20.404138 139647833720576 logging_writer.py:48] [19800] global_step=19800, grad_norm=2.230217218399048, loss=1.4045814275741577
I0211 17:44:41.603389 139647825327872 logging_writer.py:48] [19900] global_step=19900, grad_norm=2.389711380004883, loss=1.3606675863265991
I0211 17:46:07.999401 139647833720576 logging_writer.py:48] [20000] global_step=20000, grad_norm=2.061183214187622, loss=1.3766300678253174
I0211 17:47:38.385668 139647825327872 logging_writer.py:48] [20100] global_step=20100, grad_norm=1.9194257259368896, loss=1.3910977840423584
I0211 17:49:08.747046 139647833720576 logging_writer.py:48] [20200] global_step=20200, grad_norm=1.7287472486495972, loss=1.4046999216079712
I0211 17:50:36.000448 139647825327872 logging_writer.py:48] [20300] global_step=20300, grad_norm=3.7412447929382324, loss=1.4354411363601685
I0211 17:52:06.306744 139647833720576 logging_writer.py:48] [20400] global_step=20400, grad_norm=2.5690348148345947, loss=1.436620831489563
I0211 17:53:23.207108 139803787056960 spec.py:321] Evaluating on the training split.
I0211 17:54:17.641133 139803787056960 spec.py:333] Evaluating on the validation split.
I0211 17:55:11.648887 139803787056960 spec.py:349] Evaluating on the test split.
I0211 17:55:37.885840 139803787056960 submission_runner.py:408] Time since start: 19107.36s, 	Step: 20489, 	{'train/ctc_loss': Array(0.3484167, dtype=float32), 'train/wer': 0.11707835565631837, 'validation/ctc_loss': Array(0.5440495, dtype=float32), 'validation/wer': 0.15924384757233748, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30769292, dtype=float32), 'test/wer': 0.10174070237442366, 'test/num_examples': 2472, 'score': 17301.38058924675, 'total_duration': 19107.35986685753, 'accumulated_submission_time': 17301.38058924675, 'accumulated_eval_time': 1804.4356830120087, 'accumulated_logging_time': 0.6045889854431152}
I0211 17:55:37.923466 139647833720576 logging_writer.py:48] [20489] accumulated_eval_time=1804.435683, accumulated_logging_time=0.604589, accumulated_submission_time=17301.380589, global_step=20489, preemption_count=0, score=17301.380589, test/ctc_loss=0.30769291520118713, test/num_examples=2472, test/wer=0.101741, total_duration=19107.359867, train/ctc_loss=0.34841668605804443, train/wer=0.117078, validation/ctc_loss=0.5440495014190674, validation/num_examples=5348, validation/wer=0.159244
I0211 17:55:47.190909 139647825327872 logging_writer.py:48] [20500] global_step=20500, grad_norm=2.645779609680176, loss=1.4440515041351318
I0211 17:57:07.478020 139647833720576 logging_writer.py:48] [20600] global_step=20600, grad_norm=2.4172468185424805, loss=1.3524993658065796
I0211 17:58:24.341687 139647825327872 logging_writer.py:48] [20700] global_step=20700, grad_norm=1.7619729042053223, loss=1.3372324705123901
I0211 17:59:40.659713 139647833720576 logging_writer.py:48] [20800] global_step=20800, grad_norm=2.046016216278076, loss=1.3445018529891968
I0211 18:00:59.044691 139647825327872 logging_writer.py:48] [20900] global_step=20900, grad_norm=2.1274254322052, loss=1.4081978797912598
I0211 18:02:22.009846 139647833720576 logging_writer.py:48] [21000] global_step=21000, grad_norm=2.05951189994812, loss=1.3932193517684937
I0211 18:03:51.126668 139647825327872 logging_writer.py:48] [21100] global_step=21100, grad_norm=2.2747907638549805, loss=1.3691829442977905
I0211 18:05:17.499492 139647833720576 logging_writer.py:48] [21200] global_step=21200, grad_norm=1.5498249530792236, loss=1.4215693473815918
I0211 18:06:47.241119 139647825327872 logging_writer.py:48] [21300] global_step=21300, grad_norm=2.341456413269043, loss=1.3766493797302246
I0211 18:08:17.437244 139647833720576 logging_writer.py:48] [21400] global_step=21400, grad_norm=2.535338878631592, loss=1.4026191234588623
I0211 18:09:42.557410 139647825327872 logging_writer.py:48] [21500] global_step=21500, grad_norm=2.807258129119873, loss=1.3607113361358643
I0211 18:11:14.285507 139647833720576 logging_writer.py:48] [21600] global_step=21600, grad_norm=2.6782515048980713, loss=1.3431892395019531
I0211 18:12:37.058604 139647833720576 logging_writer.py:48] [21700] global_step=21700, grad_norm=1.8679906129837036, loss=1.4050095081329346
I0211 18:13:55.376405 139647825327872 logging_writer.py:48] [21800] global_step=21800, grad_norm=2.8129308223724365, loss=1.3292343616485596
I0211 18:15:14.295269 139647833720576 logging_writer.py:48] [21900] global_step=21900, grad_norm=2.190737247467041, loss=1.3702948093414307
I0211 18:16:36.174337 139647825327872 logging_writer.py:48] [22000] global_step=22000, grad_norm=1.681443691253662, loss=1.3639860153198242
I0211 18:18:05.111274 139647833720576 logging_writer.py:48] [22100] global_step=22100, grad_norm=2.244577646255493, loss=1.3792445659637451
I0211 18:19:33.982426 139647825327872 logging_writer.py:48] [22200] global_step=22200, grad_norm=2.3369572162628174, loss=1.367558479309082
I0211 18:19:38.251734 139803787056960 spec.py:321] Evaluating on the training split.
I0211 18:20:32.544277 139803787056960 spec.py:333] Evaluating on the validation split.
I0211 18:21:25.757062 139803787056960 spec.py:349] Evaluating on the test split.
I0211 18:21:52.937966 139803787056960 submission_runner.py:408] Time since start: 20682.41s, 	Step: 22206, 	{'train/ctc_loss': Array(0.3176922, dtype=float32), 'train/wer': 0.10810351565818763, 'validation/ctc_loss': Array(0.52992153, dtype=float32), 'validation/wer': 0.15386620581789393, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2998111, dtype=float32), 'test/wer': 0.09725184327585157, 'test/num_examples': 2472, 'score': 18741.621086359024, 'total_duration': 20682.414192676544, 'accumulated_submission_time': 18741.621086359024, 'accumulated_eval_time': 1939.11581158638, 'accumulated_logging_time': 0.6574249267578125}
I0211 18:21:52.978567 139647833720576 logging_writer.py:48] [22206] accumulated_eval_time=1939.115812, accumulated_logging_time=0.657425, accumulated_submission_time=18741.621086, global_step=22206, preemption_count=0, score=18741.621086, test/ctc_loss=0.29981109499931335, test/num_examples=2472, test/wer=0.097252, total_duration=20682.414193, train/ctc_loss=0.31769219040870667, train/wer=0.108104, validation/ctc_loss=0.5299215316772461, validation/num_examples=5348, validation/wer=0.153866
I0211 18:23:04.711980 139647825327872 logging_writer.py:48] [22300] global_step=22300, grad_norm=2.1557066440582275, loss=1.3444148302078247
I0211 18:24:19.975249 139647833720576 logging_writer.py:48] [22400] global_step=22400, grad_norm=2.7402782440185547, loss=1.3982765674591064
I0211 18:25:48.853348 139647825327872 logging_writer.py:48] [22500] global_step=22500, grad_norm=2.0973763465881348, loss=1.3727030754089355
I0211 18:27:19.005262 139647833720576 logging_writer.py:48] [22600] global_step=22600, grad_norm=4.339597225189209, loss=1.3683542013168335
I0211 18:28:48.373921 139647833720576 logging_writer.py:48] [22700] global_step=22700, grad_norm=2.1540188789367676, loss=1.361324667930603
I0211 18:30:04.715180 139647825327872 logging_writer.py:48] [22800] global_step=22800, grad_norm=1.8784821033477783, loss=1.394622802734375
I0211 18:31:26.101535 139647833720576 logging_writer.py:48] [22900] global_step=22900, grad_norm=2.3060669898986816, loss=1.3685499429702759
I0211 18:32:48.065343 139647825327872 logging_writer.py:48] [23000] global_step=23000, grad_norm=1.6974666118621826, loss=1.3147094249725342
I0211 18:34:12.401547 139647833720576 logging_writer.py:48] [23100] global_step=23100, grad_norm=1.96953547000885, loss=1.2763136625289917
I0211 18:35:41.041336 139647825327872 logging_writer.py:48] [23200] global_step=23200, grad_norm=2.5905344486236572, loss=1.3795442581176758
I0211 18:37:08.080414 139647833720576 logging_writer.py:48] [23300] global_step=23300, grad_norm=2.2515528202056885, loss=1.3361365795135498
I0211 18:38:39.196525 139647825327872 logging_writer.py:48] [23400] global_step=23400, grad_norm=2.445526361465454, loss=1.3613885641098022
I0211 18:40:07.698132 139647833720576 logging_writer.py:48] [23500] global_step=23500, grad_norm=2.4324207305908203, loss=1.307523488998413
I0211 18:41:39.284559 139647825327872 logging_writer.py:48] [23600] global_step=23600, grad_norm=2.082054615020752, loss=1.4072345495224
I0211 18:43:12.298862 139647833720576 logging_writer.py:48] [23700] global_step=23700, grad_norm=2.007934093475342, loss=1.3022792339324951
I0211 18:44:31.088533 139647825327872 logging_writer.py:48] [23800] global_step=23800, grad_norm=2.4357035160064697, loss=1.2834793329238892
I0211 18:45:49.534870 139647833720576 logging_writer.py:48] [23900] global_step=23900, grad_norm=2.973919630050659, loss=1.4371141195297241
I0211 18:45:53.205132 139803787056960 spec.py:321] Evaluating on the training split.
I0211 18:46:47.730056 139803787056960 spec.py:333] Evaluating on the validation split.
I0211 18:47:41.776819 139803787056960 spec.py:349] Evaluating on the test split.
I0211 18:48:08.237221 139803787056960 submission_runner.py:408] Time since start: 22257.71s, 	Step: 23906, 	{'train/ctc_loss': Array(0.35585138, dtype=float32), 'train/wer': 0.12282533840613252, 'validation/ctc_loss': Array(0.5189788, dtype=float32), 'validation/wer': 0.1510470471243616, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29122898, dtype=float32), 'test/wer': 0.09520037373306522, 'test/num_examples': 2472, 'score': 20181.759190559387, 'total_duration': 22257.7140955925, 'accumulated_submission_time': 20181.759190559387, 'accumulated_eval_time': 2074.142430305481, 'accumulated_logging_time': 0.7140963077545166}
I0211 18:48:08.274457 139647833720576 logging_writer.py:48] [23906] accumulated_eval_time=2074.142430, accumulated_logging_time=0.714096, accumulated_submission_time=20181.759191, global_step=23906, preemption_count=0, score=20181.759191, test/ctc_loss=0.2912289798259735, test/num_examples=2472, test/wer=0.095200, total_duration=22257.714096, train/ctc_loss=0.3558513820171356, train/wer=0.122825, validation/ctc_loss=0.5189787745475769, validation/num_examples=5348, validation/wer=0.151047
I0211 18:49:19.868920 139647825327872 logging_writer.py:48] [24000] global_step=24000, grad_norm=2.0843424797058105, loss=1.3848247528076172
I0211 18:50:34.949947 139647833720576 logging_writer.py:48] [24100] global_step=24100, grad_norm=3.222447156906128, loss=1.305060863494873
I0211 18:51:54.525067 139647825327872 logging_writer.py:48] [24200] global_step=24200, grad_norm=2.0591185092926025, loss=1.329668402671814
I0211 18:53:23.039377 139647833720576 logging_writer.py:48] [24300] global_step=24300, grad_norm=2.0538601875305176, loss=1.3579922914505005
I0211 18:54:53.556476 139647825327872 logging_writer.py:48] [24400] global_step=24400, grad_norm=2.0895280838012695, loss=1.351325511932373
I0211 18:56:23.317958 139647833720576 logging_writer.py:48] [24500] global_step=24500, grad_norm=2.6257457733154297, loss=1.3297295570373535
I0211 18:57:51.603241 139647825327872 logging_writer.py:48] [24600] global_step=24600, grad_norm=3.208740472793579, loss=1.3199738264083862
I0211 18:59:19.902172 139647833720576 logging_writer.py:48] [24700] global_step=24700, grad_norm=2.312230110168457, loss=1.341950535774231
I0211 19:00:43.165213 139647833720576 logging_writer.py:48] [24800] global_step=24800, grad_norm=2.031111478805542, loss=1.3381474018096924
I0211 19:01:58.952882 139647825327872 logging_writer.py:48] [24900] global_step=24900, grad_norm=2.5601367950439453, loss=1.280423641204834
I0211 19:03:21.475556 139647833720576 logging_writer.py:48] [25000] global_step=25000, grad_norm=2.5699779987335205, loss=1.293811559677124
I0211 19:04:44.284704 139647825327872 logging_writer.py:48] [25100] global_step=25100, grad_norm=2.6368305683135986, loss=1.3018133640289307
I0211 19:06:11.608406 139647833720576 logging_writer.py:48] [25200] global_step=25200, grad_norm=2.5571670532226562, loss=1.3282617330551147
I0211 19:07:38.217641 139647825327872 logging_writer.py:48] [25300] global_step=25300, grad_norm=2.0813233852386475, loss=1.3140335083007812
I0211 19:09:07.230679 139647833720576 logging_writer.py:48] [25400] global_step=25400, grad_norm=2.2869534492492676, loss=1.3425078392028809
I0211 19:10:33.525197 139647825327872 logging_writer.py:48] [25500] global_step=25500, grad_norm=2.828691005706787, loss=1.3267687559127808
I0211 19:12:01.439857 139647833720576 logging_writer.py:48] [25600] global_step=25600, grad_norm=2.0271174907684326, loss=1.332058310508728
I0211 19:12:08.468793 139803787056960 spec.py:321] Evaluating on the training split.
I0211 19:13:03.560559 139803787056960 spec.py:333] Evaluating on the validation split.
I0211 19:13:56.036884 139803787056960 spec.py:349] Evaluating on the test split.
I0211 19:14:23.683881 139803787056960 submission_runner.py:408] Time since start: 23833.16s, 	Step: 25609, 	{'train/ctc_loss': Array(0.325746, dtype=float32), 'train/wer': 0.10997155858930603, 'validation/ctc_loss': Array(0.5139698, dtype=float32), 'validation/wer': 0.14872993038995144, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28193888, dtype=float32), 'test/wer': 0.09316921576991043, 'test/num_examples': 2472, 'score': 21621.865079164505, 'total_duration': 23833.15885949135, 'accumulated_submission_time': 21621.865079164505, 'accumulated_eval_time': 2209.3501620292664, 'accumulated_logging_time': 0.7669525146484375}
I0211 19:14:23.725062 139647833720576 logging_writer.py:48] [25609] accumulated_eval_time=2209.350162, accumulated_logging_time=0.766953, accumulated_submission_time=21621.865079, global_step=25609, preemption_count=0, score=21621.865079, test/ctc_loss=0.2819388806819916, test/num_examples=2472, test/wer=0.093169, total_duration=23833.158859, train/ctc_loss=0.32574599981307983, train/wer=0.109972, validation/ctc_loss=0.5139697790145874, validation/num_examples=5348, validation/wer=0.148730
I0211 19:15:33.789117 139647825327872 logging_writer.py:48] [25700] global_step=25700, grad_norm=2.4264986515045166, loss=1.3390564918518066
I0211 19:16:53.335601 139647833720576 logging_writer.py:48] [25800] global_step=25800, grad_norm=2.0520312786102295, loss=1.2444920539855957
I0211 19:18:10.780318 139647825327872 logging_writer.py:48] [25900] global_step=25900, grad_norm=2.4925053119659424, loss=1.289730191230774
I0211 19:19:26.848239 139647833720576 logging_writer.py:48] [26000] global_step=26000, grad_norm=2.7605984210968018, loss=1.2689974308013916
I0211 19:20:48.923824 139647825327872 logging_writer.py:48] [26100] global_step=26100, grad_norm=1.7695250511169434, loss=1.269619107246399
I0211 19:22:14.928531 139647833720576 logging_writer.py:48] [26200] global_step=26200, grad_norm=3.091646909713745, loss=1.3077375888824463
I0211 19:23:42.292247 139647825327872 logging_writer.py:48] [26300] global_step=26300, grad_norm=1.5233083963394165, loss=1.2977843284606934
I0211 19:25:11.163096 139647833720576 logging_writer.py:48] [26400] global_step=26400, grad_norm=1.8477592468261719, loss=1.31862211227417
I0211 19:26:41.800056 139647825327872 logging_writer.py:48] [26500] global_step=26500, grad_norm=2.735541820526123, loss=1.3020401000976562
I0211 19:28:12.129466 139647833720576 logging_writer.py:48] [26600] global_step=26600, grad_norm=2.039727210998535, loss=1.3068867921829224
I0211 19:29:40.727195 139647825327872 logging_writer.py:48] [26700] global_step=26700, grad_norm=1.8825712203979492, loss=1.306166172027588
I0211 19:31:07.533179 139647833720576 logging_writer.py:48] [26800] global_step=26800, grad_norm=2.450685977935791, loss=1.2673979997634888
I0211 19:32:25.584687 139647825327872 logging_writer.py:48] [26900] global_step=26900, grad_norm=2.345759630203247, loss=1.2794499397277832
I0211 19:33:42.350308 139647833720576 logging_writer.py:48] [27000] global_step=27000, grad_norm=2.82930064201355, loss=1.3131986856460571
I0211 19:35:03.813042 139647825327872 logging_writer.py:48] [27100] global_step=27100, grad_norm=2.6564154624938965, loss=1.2787964344024658
I0211 19:36:24.987487 139647833720576 logging_writer.py:48] [27200] global_step=27200, grad_norm=2.421727180480957, loss=1.2728798389434814
I0211 19:37:54.590331 139647825327872 logging_writer.py:48] [27300] global_step=27300, grad_norm=2.587657928466797, loss=1.2905694246292114
I0211 19:38:24.479899 139803787056960 spec.py:321] Evaluating on the training split.
I0211 19:39:20.176436 139803787056960 spec.py:333] Evaluating on the validation split.
I0211 19:40:13.515655 139803787056960 spec.py:349] Evaluating on the test split.
I0211 19:40:41.800719 139803787056960 submission_runner.py:408] Time since start: 25411.28s, 	Step: 27335, 	{'train/ctc_loss': Array(0.30888176, dtype=float32), 'train/wer': 0.10399692085847333, 'validation/ctc_loss': Array(0.4966396, dtype=float32), 'validation/wer': 0.14369020149260936, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2696854, dtype=float32), 'test/wer': 0.08837568297686511, 'test/num_examples': 2472, 'score': 23062.529305696487, 'total_duration': 25411.276579618454, 'accumulated_submission_time': 23062.529305696487, 'accumulated_eval_time': 2346.664496421814, 'accumulated_logging_time': 0.8250465393066406}
I0211 19:40:41.844504 139647833720576 logging_writer.py:48] [27335] accumulated_eval_time=2346.664496, accumulated_logging_time=0.825047, accumulated_submission_time=23062.529306, global_step=27335, preemption_count=0, score=23062.529306, test/ctc_loss=0.26968538761138916, test/num_examples=2472, test/wer=0.088376, total_duration=25411.276580, train/ctc_loss=0.3088817596435547, train/wer=0.103997, validation/ctc_loss=0.496639609336853, validation/num_examples=5348, validation/wer=0.143690
I0211 19:41:31.931115 139647825327872 logging_writer.py:48] [27400] global_step=27400, grad_norm=1.8940805196762085, loss=1.278145432472229
I0211 19:42:48.081754 139647833720576 logging_writer.py:48] [27500] global_step=27500, grad_norm=2.020688056945801, loss=1.3196425437927246
I0211 19:44:06.001005 139647825327872 logging_writer.py:48] [27600] global_step=27600, grad_norm=1.837441086769104, loss=1.3204554319381714
I0211 19:45:35.764987 139647833720576 logging_writer.py:48] [27700] global_step=27700, grad_norm=3.717480182647705, loss=1.346123456954956
I0211 19:47:03.894616 139647825327872 logging_writer.py:48] [27800] global_step=27800, grad_norm=1.9906139373779297, loss=1.3025636672973633
I0211 19:48:26.083106 139647833720576 logging_writer.py:48] [27900] global_step=27900, grad_norm=2.269550085067749, loss=1.3121998310089111
I0211 19:49:42.218501 139647825327872 logging_writer.py:48] [28000] global_step=28000, grad_norm=1.629737377166748, loss=1.2407958507537842
I0211 19:51:02.200706 139647833720576 logging_writer.py:48] [28100] global_step=28100, grad_norm=2.3986146450042725, loss=1.2558555603027344
I0211 19:52:24.856318 139647825327872 logging_writer.py:48] [28200] global_step=28200, grad_norm=2.3210041522979736, loss=1.269252896308899
I0211 19:53:50.233420 139647833720576 logging_writer.py:48] [28300] global_step=28300, grad_norm=2.584465980529785, loss=1.2794952392578125
I0211 19:55:18.139159 139647825327872 logging_writer.py:48] [28400] global_step=28400, grad_norm=2.206775665283203, loss=1.2508158683776855
I0211 19:56:47.183967 139647833720576 logging_writer.py:48] [28500] global_step=28500, grad_norm=3.2320120334625244, loss=1.301299810409546
I0211 19:58:17.214268 139647825327872 logging_writer.py:48] [28600] global_step=28600, grad_norm=3.402160406112671, loss=1.3325564861297607
I0211 19:59:46.296453 139647833720576 logging_writer.py:48] [28700] global_step=28700, grad_norm=2.488206624984741, loss=1.2944377660751343
I0211 20:01:14.798652 139647825327872 logging_writer.py:48] [28800] global_step=28800, grad_norm=4.025384902954102, loss=1.2436556816101074
I0211 20:02:40.428838 139647833720576 logging_writer.py:48] [28900] global_step=28900, grad_norm=1.8550732135772705, loss=1.2312591075897217
I0211 20:03:56.358122 139647825327872 logging_writer.py:48] [29000] global_step=29000, grad_norm=2.642643451690674, loss=1.257932424545288
I0211 20:04:42.386420 139803787056960 spec.py:321] Evaluating on the training split.
I0211 20:05:37.364323 139803787056960 spec.py:333] Evaluating on the validation split.
I0211 20:06:32.488779 139803787056960 spec.py:349] Evaluating on the test split.
I0211 20:06:58.864677 139803787056960 submission_runner.py:408] Time since start: 26988.34s, 	Step: 29061, 	{'train/ctc_loss': Array(0.28284952, dtype=float32), 'train/wer': 0.09740008831971737, 'validation/ctc_loss': Array(0.49027404, dtype=float32), 'validation/wer': 0.14099655328885757, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2685104, dtype=float32), 'test/wer': 0.08597891658034246, 'test/num_examples': 2472, 'score': 24502.98046898842, 'total_duration': 26988.340623378754, 'accumulated_submission_time': 24502.98046898842, 'accumulated_eval_time': 2483.1363592147827, 'accumulated_logging_time': 0.8873429298400879}
I0211 20:06:58.903966 139647833720576 logging_writer.py:48] [29061] accumulated_eval_time=2483.136359, accumulated_logging_time=0.887343, accumulated_submission_time=24502.980469, global_step=29061, preemption_count=0, score=24502.980469, test/ctc_loss=0.2685104012489319, test/num_examples=2472, test/wer=0.085979, total_duration=26988.340623, train/ctc_loss=0.28284952044487, train/wer=0.097400, validation/ctc_loss=0.490274041891098, validation/num_examples=5348, validation/wer=0.140997
I0211 20:07:29.094871 139647825327872 logging_writer.py:48] [29100] global_step=29100, grad_norm=2.172086715698242, loss=1.2205673456192017
I0211 20:08:44.700517 139647833720576 logging_writer.py:48] [29200] global_step=29200, grad_norm=2.5437865257263184, loss=1.2692850828170776
I0211 20:10:00.037068 139647825327872 logging_writer.py:48] [29300] global_step=29300, grad_norm=2.921132802963257, loss=1.2766444683074951
I0211 20:11:28.434571 139647833720576 logging_writer.py:48] [29400] global_step=29400, grad_norm=3.178339719772339, loss=1.2490779161453247
I0211 20:12:59.288203 139647825327872 logging_writer.py:48] [29500] global_step=29500, grad_norm=2.2530405521392822, loss=1.282141923904419
I0211 20:14:28.241594 139647833720576 logging_writer.py:48] [29600] global_step=29600, grad_norm=2.2576019763946533, loss=1.3135102987289429
I0211 20:15:56.499647 139647825327872 logging_writer.py:48] [29700] global_step=29700, grad_norm=5.4916911125183105, loss=1.2860608100891113
I0211 20:17:24.353504 139647833720576 logging_writer.py:48] [29800] global_step=29800, grad_norm=4.907692909240723, loss=1.2436261177062988
I0211 20:18:52.777590 139647833720576 logging_writer.py:48] [29900] global_step=29900, grad_norm=4.083035469055176, loss=1.230334758758545
I0211 20:20:08.525206 139647825327872 logging_writer.py:48] [30000] global_step=30000, grad_norm=2.083855152130127, loss=1.2460219860076904
I0211 20:21:27.862899 139647833720576 logging_writer.py:48] [30100] global_step=30100, grad_norm=3.839254140853882, loss=1.2587366104125977
I0211 20:22:51.022601 139647825327872 logging_writer.py:48] [30200] global_step=30200, grad_norm=8.197991371154785, loss=1.3045339584350586
I0211 20:24:15.108424 139647833720576 logging_writer.py:48] [30300] global_step=30300, grad_norm=1.8541728258132935, loss=1.2385860681533813
I0211 20:25:46.133643 139647825327872 logging_writer.py:48] [30400] global_step=30400, grad_norm=2.273627281188965, loss=1.283927321434021
I0211 20:27:15.859808 139647833720576 logging_writer.py:48] [30500] global_step=30500, grad_norm=1.8504791259765625, loss=1.2225861549377441
I0211 20:28:45.826526 139647825327872 logging_writer.py:48] [30600] global_step=30600, grad_norm=2.031099557876587, loss=1.3117212057113647
I0211 20:30:12.715491 139647833720576 logging_writer.py:48] [30700] global_step=30700, grad_norm=2.973184108734131, loss=1.232795238494873
I0211 20:30:59.670950 139803787056960 spec.py:321] Evaluating on the training split.
I0211 20:31:53.436478 139803787056960 spec.py:333] Evaluating on the validation split.
I0211 20:32:46.951212 139803787056960 spec.py:349] Evaluating on the test split.
I0211 20:33:13.545229 139803787056960 submission_runner.py:408] Time since start: 28563.02s, 	Step: 30754, 	{'train/ctc_loss': Array(0.26444355, dtype=float32), 'train/wer': 0.0926417954318846, 'validation/ctc_loss': Array(0.47725204, dtype=float32), 'validation/wer': 0.13774293520762332, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2558537, dtype=float32), 'test/wer': 0.08309467227266265, 'test/num_examples': 2472, 'score': 25943.658811092377, 'total_duration': 28563.02157688141, 'accumulated_submission_time': 25943.658811092377, 'accumulated_eval_time': 2617.004650115967, 'accumulated_logging_time': 0.9421923160552979}
I0211 20:33:13.581225 139647833720576 logging_writer.py:48] [30754] accumulated_eval_time=2617.004650, accumulated_logging_time=0.942192, accumulated_submission_time=25943.658811, global_step=30754, preemption_count=0, score=25943.658811, test/ctc_loss=0.25585371255874634, test/num_examples=2472, test/wer=0.083095, total_duration=28563.021577, train/ctc_loss=0.2644435465335846, train/wer=0.092642, validation/ctc_loss=0.4772520363330841, validation/num_examples=5348, validation/wer=0.137743
I0211 20:33:48.920264 139647825327872 logging_writer.py:48] [30800] global_step=30800, grad_norm=1.6342196464538574, loss=1.2381333112716675
I0211 20:35:07.892143 139647833720576 logging_writer.py:48] [30900] global_step=30900, grad_norm=2.1016321182250977, loss=1.2077929973602295
I0211 20:36:24.860585 139647825327872 logging_writer.py:48] [31000] global_step=31000, grad_norm=1.642496943473816, loss=1.1961313486099243
I0211 20:37:41.950312 139647833720576 logging_writer.py:48] [31100] global_step=31100, grad_norm=2.818814277648926, loss=1.2388101816177368
I0211 20:39:01.368298 139647825327872 logging_writer.py:48] [31200] global_step=31200, grad_norm=1.7794151306152344, loss=1.2563674449920654
I0211 20:40:23.391566 139647833720576 logging_writer.py:48] [31300] global_step=31300, grad_norm=3.084167957305908, loss=1.186914086341858
I0211 20:41:51.428175 139647825327872 logging_writer.py:48] [31400] global_step=31400, grad_norm=3.1322195529937744, loss=1.2197834253311157
I0211 20:43:22.387704 139647833720576 logging_writer.py:48] [31500] global_step=31500, grad_norm=1.7085564136505127, loss=1.2736568450927734
I0211 20:44:50.090930 139647825327872 logging_writer.py:48] [31600] global_step=31600, grad_norm=2.462881326675415, loss=1.2352838516235352
I0211 20:46:18.533069 139647833720576 logging_writer.py:48] [31700] global_step=31700, grad_norm=1.5759280920028687, loss=1.2859973907470703
I0211 20:47:46.121389 139647825327872 logging_writer.py:48] [31800] global_step=31800, grad_norm=1.7009286880493164, loss=1.2261974811553955
I0211 20:49:16.338807 139647833720576 logging_writer.py:48] [31900] global_step=31900, grad_norm=2.5405702590942383, loss=1.2394490242004395
I0211 20:50:39.751891 139647833720576 logging_writer.py:48] [32000] global_step=32000, grad_norm=3.1351211071014404, loss=1.222790241241455
I0211 20:51:56.599766 139647825327872 logging_writer.py:48] [32100] global_step=32100, grad_norm=4.003054141998291, loss=1.266539454460144
I0211 20:53:15.110336 139647833720576 logging_writer.py:48] [32200] global_step=32200, grad_norm=2.048490285873413, loss=1.2095071077346802
I0211 20:54:34.649906 139647825327872 logging_writer.py:48] [32300] global_step=32300, grad_norm=2.281975746154785, loss=1.2295873165130615
I0211 20:55:57.267256 139647833720576 logging_writer.py:48] [32400] global_step=32400, grad_norm=3.087923526763916, loss=1.2109956741333008
I0211 20:57:13.999446 139803787056960 spec.py:321] Evaluating on the training split.
I0211 20:58:08.900239 139803787056960 spec.py:333] Evaluating on the validation split.
I0211 20:59:01.331369 139803787056960 spec.py:349] Evaluating on the test split.
I0211 20:59:29.035252 139803787056960 submission_runner.py:408] Time since start: 30138.51s, 	Step: 32487, 	{'train/ctc_loss': Array(0.25097135, dtype=float32), 'train/wer': 0.08781035517125606, 'validation/ctc_loss': Array(0.46989283, dtype=float32), 'validation/wer': 0.136758160595499, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2501571, dtype=float32), 'test/wer': 0.08171348485771739, 'test/num_examples': 2472, 'score': 27383.986397266388, 'total_duration': 30138.511751174927, 'accumulated_submission_time': 27383.986397266388, 'accumulated_eval_time': 2752.034639120102, 'accumulated_logging_time': 0.9958689212799072}
I0211 20:59:29.078737 139647833720576 logging_writer.py:48] [32487] accumulated_eval_time=2752.034639, accumulated_logging_time=0.995869, accumulated_submission_time=27383.986397, global_step=32487, preemption_count=0, score=27383.986397, test/ctc_loss=0.25015708804130554, test/num_examples=2472, test/wer=0.081713, total_duration=30138.511751, train/ctc_loss=0.25097134709358215, train/wer=0.087810, validation/ctc_loss=0.46989282965660095, validation/num_examples=5348, validation/wer=0.136758
I0211 20:59:39.947950 139647825327872 logging_writer.py:48] [32500] global_step=32500, grad_norm=2.323786973953247, loss=1.1995564699172974
I0211 21:00:55.090914 139647833720576 logging_writer.py:48] [32600] global_step=32600, grad_norm=2.367642879486084, loss=1.2299385070800781
I0211 21:02:12.102830 139647825327872 logging_writer.py:48] [32700] global_step=32700, grad_norm=1.802762746810913, loss=1.3023163080215454
I0211 21:03:40.839300 139647833720576 logging_writer.py:48] [32800] global_step=32800, grad_norm=3.250220537185669, loss=1.2570229768753052
I0211 21:05:11.619042 139647825327872 logging_writer.py:48] [32900] global_step=32900, grad_norm=2.7572245597839355, loss=1.2232862710952759
I0211 21:06:38.895494 139647833720576 logging_writer.py:48] [33000] global_step=33000, grad_norm=1.7882113456726074, loss=1.2676937580108643
I0211 21:07:55.087984 139647825327872 logging_writer.py:48] [33100] global_step=33100, grad_norm=2.3298535346984863, loss=1.226910948753357
I0211 21:09:10.306656 139647833720576 logging_writer.py:48] [33200] global_step=33200, grad_norm=1.467307209968567, loss=1.1863393783569336
I0211 21:10:28.491241 139647825327872 logging_writer.py:48] [33300] global_step=33300, grad_norm=2.210949182510376, loss=1.2710829973220825
I0211 21:11:49.756524 139647833720576 logging_writer.py:48] [33400] global_step=33400, grad_norm=1.5670849084854126, loss=1.1422615051269531
I0211 21:13:16.141382 139647825327872 logging_writer.py:48] [33500] global_step=33500, grad_norm=2.2162256240844727, loss=1.2489538192749023
I0211 21:14:45.698451 139647833720576 logging_writer.py:48] [33600] global_step=33600, grad_norm=2.016901969909668, loss=1.2543185949325562
I0211 21:16:14.657282 139647825327872 logging_writer.py:48] [33700] global_step=33700, grad_norm=2.4209232330322266, loss=1.226365327835083
I0211 21:17:43.891149 139647833720576 logging_writer.py:48] [33800] global_step=33800, grad_norm=3.9201743602752686, loss=1.197413682937622
I0211 21:19:13.176056 139647825327872 logging_writer.py:48] [33900] global_step=33900, grad_norm=3.2566380500793457, loss=1.2606894969940186
I0211 21:20:44.729151 139647833720576 logging_writer.py:48] [34000] global_step=34000, grad_norm=2.0775351524353027, loss=1.1877466440200806
I0211 21:22:00.031647 139647825327872 logging_writer.py:48] [34100] global_step=34100, grad_norm=2.7621212005615234, loss=1.1994837522506714
I0211 21:23:15.306288 139647833720576 logging_writer.py:48] [34200] global_step=34200, grad_norm=2.7032110691070557, loss=1.1557880640029907
I0211 21:23:29.440798 139803787056960 spec.py:321] Evaluating on the training split.
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0211 21:24:43.436791 139803787056960 spec.py:333] Evaluating on the validation split.
I0211 21:25:35.860582 139803787056960 spec.py:349] Evaluating on the test split.
I0211 21:26:02.224502 139803787056960 submission_runner.py:408] Time since start: 31731.70s, 	Step: 34220, 	{'train/ctc_loss': Array(0.15385501, dtype=float32), 'train/wer': 0.0554912004761095, 'validation/ctc_loss': Array(0.4573218, dtype=float32), 'validation/wer': 0.13210461782055863, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24460703, dtype=float32), 'test/wer': 0.07889017528893222, 'test/num_examples': 2472, 'score': 28824.258416175842, 'total_duration': 31731.703934669495, 'accumulated_submission_time': 28824.258416175842, 'accumulated_eval_time': 2904.8154451847076, 'accumulated_logging_time': 1.0548536777496338}
I0211 21:26:02.256950 139647833720576 logging_writer.py:48] [34220] accumulated_eval_time=2904.815445, accumulated_logging_time=1.054854, accumulated_submission_time=28824.258416, global_step=34220, preemption_count=0, score=28824.258416, test/ctc_loss=0.24460703134536743, test/num_examples=2472, test/wer=0.078890, total_duration=31731.703935, train/ctc_loss=0.15385501086711884, train/wer=0.055491, validation/ctc_loss=0.45732179284095764, validation/num_examples=5348, validation/wer=0.132105
I0211 21:27:04.112681 139647825327872 logging_writer.py:48] [34300] global_step=34300, grad_norm=1.7590266466140747, loss=1.237589716911316
I0211 21:28:19.320961 139647833720576 logging_writer.py:48] [34400] global_step=34400, grad_norm=1.8646527528762817, loss=1.1766680479049683
I0211 21:29:37.237540 139647825327872 logging_writer.py:48] [34500] global_step=34500, grad_norm=2.9760279655456543, loss=1.1984103918075562
I0211 21:31:05.753740 139647833720576 logging_writer.py:48] [34600] global_step=34600, grad_norm=1.5040016174316406, loss=1.2119393348693848
I0211 21:32:34.295618 139647825327872 logging_writer.py:48] [34700] global_step=34700, grad_norm=3.5649912357330322, loss=1.1947615146636963
I0211 21:34:04.665491 139647833720576 logging_writer.py:48] [34800] global_step=34800, grad_norm=2.102776050567627, loss=1.2325775623321533
I0211 21:35:33.577939 139647825327872 logging_writer.py:48] [34900] global_step=34900, grad_norm=2.306626319885254, loss=1.1652687788009644
I0211 21:37:02.785175 139647833720576 logging_writer.py:48] [35000] global_step=35000, grad_norm=1.598448395729065, loss=1.1842411756515503
I0211 21:38:25.568666 139647833720576 logging_writer.py:48] [35100] global_step=35100, grad_norm=3.358767032623291, loss=1.2299587726593018
I0211 21:39:41.894344 139647825327872 logging_writer.py:48] [35200] global_step=35200, grad_norm=1.9829903841018677, loss=1.2102575302124023
I0211 21:41:02.599155 139647833720576 logging_writer.py:48] [35300] global_step=35300, grad_norm=2.892965316772461, loss=1.168238639831543
I0211 21:42:25.308173 139647825327872 logging_writer.py:48] [35400] global_step=35400, grad_norm=2.2916042804718018, loss=1.162959337234497
I0211 21:43:53.303314 139647833720576 logging_writer.py:48] [35500] global_step=35500, grad_norm=2.4904303550720215, loss=1.2324740886688232
I0211 21:45:25.067349 139647825327872 logging_writer.py:48] [35600] global_step=35600, grad_norm=3.031172513961792, loss=1.2202715873718262
I0211 21:46:54.041840 139647833720576 logging_writer.py:48] [35700] global_step=35700, grad_norm=2.63029146194458, loss=1.2034958600997925
I0211 21:48:21.458938 139647825327872 logging_writer.py:48] [35800] global_step=35800, grad_norm=4.274133682250977, loss=1.214101791381836
I0211 21:49:47.376048 139647833720576 logging_writer.py:48] [35900] global_step=35900, grad_norm=2.713757276535034, loss=1.2085093259811401
I0211 21:50:02.804199 139803787056960 spec.py:321] Evaluating on the training split.
I0211 21:50:59.411155 139803787056960 spec.py:333] Evaluating on the validation split.
I0211 21:51:52.957376 139803787056960 spec.py:349] Evaluating on the test split.
I0211 21:52:19.305438 139803787056960 submission_runner.py:408] Time since start: 33308.78s, 	Step: 35919, 	{'train/ctc_loss': Array(0.14717703, dtype=float32), 'train/wer': 0.050968087151221086, 'validation/ctc_loss': Array(0.44906735, dtype=float32), 'validation/wer': 0.12888961835156454, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23849553, dtype=float32), 'test/wer': 0.07606686572014705, 'test/num_examples': 2472, 'score': 30264.718192100525, 'total_duration': 33308.779076337814, 'accumulated_submission_time': 30264.718192100525, 'accumulated_eval_time': 3041.3079862594604, 'accumulated_logging_time': 1.1017398834228516}
I0211 21:52:19.349987 139647833720576 logging_writer.py:48] [35919] accumulated_eval_time=3041.307986, accumulated_logging_time=1.101740, accumulated_submission_time=30264.718192, global_step=35919, preemption_count=0, score=30264.718192, test/ctc_loss=0.23849552869796753, test/num_examples=2472, test/wer=0.076067, total_duration=33308.779076, train/ctc_loss=0.14717702567577362, train/wer=0.050968, validation/ctc_loss=0.4490673542022705, validation/num_examples=5348, validation/wer=0.128890
I0211 21:53:22.235657 139647825327872 logging_writer.py:48] [36000] global_step=36000, grad_norm=1.880778431892395, loss=1.2065696716308594
I0211 21:54:42.262503 139647833720576 logging_writer.py:48] [36100] global_step=36100, grad_norm=2.216590404510498, loss=1.1708970069885254
I0211 21:55:59.457860 139647825327872 logging_writer.py:48] [36200] global_step=36200, grad_norm=2.2594592571258545, loss=1.1856063604354858
I0211 21:57:16.381308 139647833720576 logging_writer.py:48] [36300] global_step=36300, grad_norm=2.384655714035034, loss=1.2070997953414917
I0211 21:58:39.791585 139647825327872 logging_writer.py:48] [36400] global_step=36400, grad_norm=2.7299816608428955, loss=1.1607609987258911
I0211 22:00:06.761526 139647833720576 logging_writer.py:48] [36500] global_step=36500, grad_norm=1.722813606262207, loss=1.1292277574539185
I0211 22:01:33.009248 139647825327872 logging_writer.py:48] [36600] global_step=36600, grad_norm=3.368478536605835, loss=1.2002393007278442
I0211 22:03:01.632129 139647833720576 logging_writer.py:48] [36700] global_step=36700, grad_norm=2.4088351726531982, loss=1.1771639585494995
I0211 22:04:31.421191 139647825327872 logging_writer.py:48] [36800] global_step=36800, grad_norm=1.7328006029129028, loss=1.1881755590438843
I0211 22:06:00.156143 139647833720576 logging_writer.py:48] [36900] global_step=36900, grad_norm=1.8729814291000366, loss=1.1404320001602173
I0211 22:07:31.282912 139647825327872 logging_writer.py:48] [37000] global_step=37000, grad_norm=4.488489627838135, loss=1.173181176185608
I0211 22:09:02.147215 139647833720576 logging_writer.py:48] [37100] global_step=37100, grad_norm=2.418973922729492, loss=1.160858154296875
I0211 22:10:19.821023 139647825327872 logging_writer.py:48] [37200] global_step=37200, grad_norm=2.301226854324341, loss=1.2015759944915771
I0211 22:11:36.698407 139647833720576 logging_writer.py:48] [37300] global_step=37300, grad_norm=1.5740970373153687, loss=1.176294207572937
I0211 22:12:54.761026 139647825327872 logging_writer.py:48] [37400] global_step=37400, grad_norm=2.1498546600341797, loss=1.1521079540252686
I0211 22:14:19.106354 139647833720576 logging_writer.py:48] [37500] global_step=37500, grad_norm=1.8593528270721436, loss=1.1318962574005127
I0211 22:15:47.447460 139647825327872 logging_writer.py:48] [37600] global_step=37600, grad_norm=3.7029333114624023, loss=1.1545699834823608
I0211 22:16:20.328514 139803787056960 spec.py:321] Evaluating on the training split.
I0211 22:17:16.678989 139803787056960 spec.py:333] Evaluating on the validation split.
I0211 22:18:10.730837 139803787056960 spec.py:349] Evaluating on the test split.
I0211 22:18:37.628238 139803787056960 submission_runner.py:408] Time since start: 34887.10s, 	Step: 37639, 	{'train/ctc_loss': Array(0.15311022, dtype=float32), 'train/wer': 0.05433547398187056, 'validation/ctc_loss': Array(0.4453374, dtype=float32), 'validation/wer': 0.1284261950046825, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23440033, dtype=float32), 'test/wer': 0.07395446143846607, 'test/num_examples': 2472, 'score': 31705.608791828156, 'total_duration': 34887.10219669342, 'accumulated_submission_time': 31705.608791828156, 'accumulated_eval_time': 3178.5993349552155, 'accumulated_logging_time': 1.1614861488342285}
I0211 22:18:37.668596 139647833720576 logging_writer.py:48] [37639] accumulated_eval_time=3178.599335, accumulated_logging_time=1.161486, accumulated_submission_time=31705.608792, global_step=37639, preemption_count=0, score=31705.608792, test/ctc_loss=0.23440033197402954, test/num_examples=2472, test/wer=0.073954, total_duration=34887.102197, train/ctc_loss=0.15311022102832794, train/wer=0.054335, validation/ctc_loss=0.4453374147415161, validation/num_examples=5348, validation/wer=0.128426
I0211 22:19:24.317000 139647825327872 logging_writer.py:48] [37700] global_step=37700, grad_norm=1.518358588218689, loss=1.157041072845459
I0211 22:20:39.978267 139647833720576 logging_writer.py:48] [37800] global_step=37800, grad_norm=2.5733654499053955, loss=1.198428750038147
I0211 22:22:03.942906 139647825327872 logging_writer.py:48] [37900] global_step=37900, grad_norm=3.455158233642578, loss=1.1500740051269531
I0211 22:23:35.006680 139647833720576 logging_writer.py:48] [38000] global_step=38000, grad_norm=2.6549763679504395, loss=1.1184983253479004
I0211 22:25:03.579872 139647825327872 logging_writer.py:48] [38100] global_step=38100, grad_norm=2.24835205078125, loss=1.1768819093704224
I0211 22:26:26.427611 139647833720576 logging_writer.py:48] [38200] global_step=38200, grad_norm=2.368673324584961, loss=1.2108865976333618
I0211 22:27:43.346810 139647825327872 logging_writer.py:48] [38300] global_step=38300, grad_norm=2.9377543926239014, loss=1.1020557880401611
I0211 22:29:02.869117 139647833720576 logging_writer.py:48] [38400] global_step=38400, grad_norm=3.8290727138519287, loss=1.0977208614349365
I0211 22:30:25.333980 139647825327872 logging_writer.py:48] [38500] global_step=38500, grad_norm=1.5168331861495972, loss=1.1501082181930542
I0211 22:31:52.492764 139647833720576 logging_writer.py:48] [38600] global_step=38600, grad_norm=2.280026435852051, loss=1.1079074144363403
I0211 22:33:21.632988 139647825327872 logging_writer.py:48] [38700] global_step=38700, grad_norm=2.5824713706970215, loss=1.1451663970947266
I0211 22:34:49.421641 139647833720576 logging_writer.py:48] [38800] global_step=38800, grad_norm=1.9482448101043701, loss=1.099292516708374
I0211 22:36:19.546466 139647825327872 logging_writer.py:48] [38900] global_step=38900, grad_norm=2.2853894233703613, loss=1.1710827350616455
I0211 22:37:50.357693 139647833720576 logging_writer.py:48] [39000] global_step=39000, grad_norm=2.110783576965332, loss=1.1330493688583374
I0211 22:39:18.429513 139647825327872 logging_writer.py:48] [39100] global_step=39100, grad_norm=2.203601121902466, loss=1.1833159923553467
I0211 22:40:44.033316 139647833720576 logging_writer.py:48] [39200] global_step=39200, grad_norm=3.153125047683716, loss=1.1785539388656616
I0211 22:42:01.334014 139647825327872 logging_writer.py:48] [39300] global_step=39300, grad_norm=3.1597142219543457, loss=1.1566616296768188
I0211 22:42:38.226490 139803787056960 spec.py:321] Evaluating on the training split.
I0211 22:43:34.324735 139803787056960 spec.py:333] Evaluating on the validation split.
I0211 22:44:26.804070 139803787056960 spec.py:349] Evaluating on the test split.
I0211 22:44:54.208293 139803787056960 submission_runner.py:408] Time since start: 36463.68s, 	Step: 39347, 	{'train/ctc_loss': Array(0.13366854, dtype=float32), 'train/wer': 0.04803719008264463, 'validation/ctc_loss': Array(0.43505618, dtype=float32), 'validation/wer': 0.12496017455612733, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23139994, dtype=float32), 'test/wer': 0.07346698352730892, 'test/num_examples': 2472, 'score': 33146.07905125618, 'total_duration': 36463.68469786644, 'accumulated_submission_time': 33146.07905125618, 'accumulated_eval_time': 3314.5751991271973, 'accumulated_logging_time': 1.2173850536346436}
I0211 22:44:54.252392 139647833720576 logging_writer.py:48] [39347] accumulated_eval_time=3314.575199, accumulated_logging_time=1.217385, accumulated_submission_time=33146.079051, global_step=39347, preemption_count=0, score=33146.079051, test/ctc_loss=0.23139993846416473, test/num_examples=2472, test/wer=0.073467, total_duration=36463.684698, train/ctc_loss=0.13366854190826416, train/wer=0.048037, validation/ctc_loss=0.4350561797618866, validation/num_examples=5348, validation/wer=0.124960
I0211 22:45:35.449350 139647825327872 logging_writer.py:48] [39400] global_step=39400, grad_norm=1.7585816383361816, loss=1.1838138103485107
I0211 22:46:50.715100 139647833720576 logging_writer.py:48] [39500] global_step=39500, grad_norm=2.7436842918395996, loss=1.1790010929107666
I0211 22:48:06.009351 139647825327872 logging_writer.py:48] [39600] global_step=39600, grad_norm=2.360567331314087, loss=1.1425232887268066
I0211 22:49:34.033832 139647833720576 logging_writer.py:48] [39700] global_step=39700, grad_norm=3.6719493865966797, loss=1.1788899898529053
I0211 22:51:04.450466 139647825327872 logging_writer.py:48] [39800] global_step=39800, grad_norm=2.0562286376953125, loss=1.1589709520339966
I0211 22:52:34.816706 139647833720576 logging_writer.py:48] [39900] global_step=39900, grad_norm=2.3603200912475586, loss=1.2012369632720947
I0211 22:54:04.127290 139647825327872 logging_writer.py:48] [40000] global_step=40000, grad_norm=3.0095794200897217, loss=1.1234955787658691
I0211 22:55:34.233479 139647833720576 logging_writer.py:48] [40100] global_step=40100, grad_norm=3.3756518363952637, loss=1.2297959327697754
I0211 22:57:05.142769 139647833720576 logging_writer.py:48] [40200] global_step=40200, grad_norm=2.4385883808135986, loss=1.0923229455947876
I0211 22:58:21.270262 139647825327872 logging_writer.py:48] [40300] global_step=40300, grad_norm=2.085212230682373, loss=1.127669095993042
I0211 22:59:39.727073 139647833720576 logging_writer.py:48] [40400] global_step=40400, grad_norm=2.558190107345581, loss=1.1070952415466309
I0211 23:01:01.169330 139647825327872 logging_writer.py:48] [40500] global_step=40500, grad_norm=4.820621013641357, loss=1.1411670446395874
I0211 23:02:26.359297 139647833720576 logging_writer.py:48] [40600] global_step=40600, grad_norm=1.9977062940597534, loss=1.153394341468811
I0211 23:03:57.052098 139647825327872 logging_writer.py:48] [40700] global_step=40700, grad_norm=4.159877777099609, loss=1.167375922203064
I0211 23:05:27.344544 139647833720576 logging_writer.py:48] [40800] global_step=40800, grad_norm=1.4703819751739502, loss=1.1269296407699585
I0211 23:06:56.846913 139647825327872 logging_writer.py:48] [40900] global_step=40900, grad_norm=2.4345006942749023, loss=1.1578269004821777
I0211 23:08:23.251288 139647833720576 logging_writer.py:48] [41000] global_step=41000, grad_norm=7.617218494415283, loss=1.1688288450241089
I0211 23:08:54.678770 139803787056960 spec.py:321] Evaluating on the training split.
I0211 23:09:50.514838 139803787056960 spec.py:333] Evaluating on the validation split.
I0211 23:10:43.178295 139803787056960 spec.py:349] Evaluating on the test split.
I0211 23:11:10.352506 139803787056960 submission_runner.py:408] Time since start: 38039.83s, 	Step: 41036, 	{'train/ctc_loss': Array(0.14206897, dtype=float32), 'train/wer': 0.05088739730555363, 'validation/ctc_loss': Array(0.43153396, dtype=float32), 'validation/wer': 0.12453536982148547, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22643007, dtype=float32), 'test/wer': 0.07287794771799402, 'test/num_examples': 2472, 'score': 34586.41890668869, 'total_duration': 38039.82860040665, 'accumulated_submission_time': 34586.41890668869, 'accumulated_eval_time': 3450.242693901062, 'accumulated_logging_time': 1.2768633365631104}
I0211 23:11:10.391860 139647833720576 logging_writer.py:48] [41036] accumulated_eval_time=3450.242694, accumulated_logging_time=1.276863, accumulated_submission_time=34586.418907, global_step=41036, preemption_count=0, score=34586.418907, test/ctc_loss=0.22643007338047028, test/num_examples=2472, test/wer=0.072878, total_duration=38039.828600, train/ctc_loss=0.14206896722316742, train/wer=0.050887, validation/ctc_loss=0.43153396248817444, validation/num_examples=5348, validation/wer=0.124535
I0211 23:11:59.336836 139647825327872 logging_writer.py:48] [41100] global_step=41100, grad_norm=1.7579972743988037, loss=1.0828335285186768
I0211 23:13:18.688443 139647833720576 logging_writer.py:48] [41200] global_step=41200, grad_norm=2.961876630783081, loss=1.1316769123077393
I0211 23:14:35.813696 139647825327872 logging_writer.py:48] [41300] global_step=41300, grad_norm=3.4126698970794678, loss=1.0813512802124023
I0211 23:15:55.285509 139647833720576 logging_writer.py:48] [41400] global_step=41400, grad_norm=2.9627928733825684, loss=1.1396678686141968
I0211 23:17:17.770404 139647825327872 logging_writer.py:48] [41500] global_step=41500, grad_norm=3.5638136863708496, loss=1.157787799835205
I0211 23:18:40.139064 139647833720576 logging_writer.py:48] [41600] global_step=41600, grad_norm=3.204838275909424, loss=1.1417086124420166
I0211 23:20:07.231426 139647825327872 logging_writer.py:48] [41700] global_step=41700, grad_norm=2.48940372467041, loss=1.1709191799163818
I0211 23:21:38.562124 139647833720576 logging_writer.py:48] [41800] global_step=41800, grad_norm=3.151171922683716, loss=1.1208361387252808
I0211 23:23:07.247630 139647825327872 logging_writer.py:48] [41900] global_step=41900, grad_norm=3.7612380981445312, loss=1.1065027713775635
I0211 23:24:37.080152 139647833720576 logging_writer.py:48] [42000] global_step=42000, grad_norm=2.2233941555023193, loss=1.1628763675689697
I0211 23:26:07.349655 139647825327872 logging_writer.py:48] [42100] global_step=42100, grad_norm=2.3494598865509033, loss=1.1219319105148315
I0211 23:27:36.297980 139647833720576 logging_writer.py:48] [42200] global_step=42200, grad_norm=3.022038459777832, loss=1.100888729095459
I0211 23:28:58.673035 139647833720576 logging_writer.py:48] [42300] global_step=42300, grad_norm=1.865530252456665, loss=1.065748691558838
I0211 23:30:18.153971 139647825327872 logging_writer.py:48] [42400] global_step=42400, grad_norm=2.9606516361236572, loss=1.1471251249313354
I0211 23:31:39.513737 139647833720576 logging_writer.py:48] [42500] global_step=42500, grad_norm=2.1729743480682373, loss=1.1572513580322266
I0211 23:33:01.345016 139647825327872 logging_writer.py:48] [42600] global_step=42600, grad_norm=2.379002809524536, loss=1.0908483266830444
I0211 23:34:28.835326 139647833720576 logging_writer.py:48] [42700] global_step=42700, grad_norm=1.6197024583816528, loss=1.1201056241989136
I0211 23:35:10.937161 139803787056960 spec.py:321] Evaluating on the training split.
I0211 23:36:08.775255 139803787056960 spec.py:333] Evaluating on the validation split.
I0211 23:37:02.952916 139803787056960 spec.py:349] Evaluating on the test split.
I0211 23:37:30.246009 139803787056960 submission_runner.py:408] Time since start: 39619.72s, 	Step: 42749, 	{'train/ctc_loss': Array(0.13182025, dtype=float32), 'train/wer': 0.04785386859286012, 'validation/ctc_loss': Array(0.42808613, dtype=float32), 'validation/wer': 0.12368576035220175, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22455934, dtype=float32), 'test/wer': 0.0707452318566815, 'test/num_examples': 2472, 'score': 36026.87464380264, 'total_duration': 39619.72004675865, 'accumulated_submission_time': 36026.87464380264, 'accumulated_eval_time': 3589.543283224106, 'accumulated_logging_time': 1.3324298858642578}
I0211 23:37:30.288463 139647833720576 logging_writer.py:48] [42749] accumulated_eval_time=3589.543283, accumulated_logging_time=1.332430, accumulated_submission_time=36026.874644, global_step=42749, preemption_count=0, score=36026.874644, test/ctc_loss=0.22455933690071106, test/num_examples=2472, test/wer=0.070745, total_duration=39619.720047, train/ctc_loss=0.13182024657726288, train/wer=0.047854, validation/ctc_loss=0.42808613181114197, validation/num_examples=5348, validation/wer=0.123686
I0211 23:38:09.692498 139647825327872 logging_writer.py:48] [42800] global_step=42800, grad_norm=1.6764739751815796, loss=1.0834378004074097
I0211 23:39:24.864468 139647833720576 logging_writer.py:48] [42900] global_step=42900, grad_norm=4.440556526184082, loss=1.0915240049362183
I0211 23:40:44.468840 139647825327872 logging_writer.py:48] [43000] global_step=43000, grad_norm=2.3935530185699463, loss=1.134077787399292
I0211 23:42:12.304685 139647833720576 logging_writer.py:48] [43100] global_step=43100, grad_norm=2.6687960624694824, loss=1.1009067296981812
I0211 23:43:42.712789 139647825327872 logging_writer.py:48] [43200] global_step=43200, grad_norm=2.758016347885132, loss=1.1053082942962646
I0211 23:45:08.473042 139647833720576 logging_writer.py:48] [43300] global_step=43300, grad_norm=7.901157379150391, loss=1.2323096990585327
I0211 23:46:24.668455 139647825327872 logging_writer.py:48] [43400] global_step=43400, grad_norm=2.2895874977111816, loss=1.0950193405151367
I0211 23:47:43.254228 139647833720576 logging_writer.py:48] [43500] global_step=43500, grad_norm=2.6083250045776367, loss=1.0946887731552124
I0211 23:49:03.899612 139647825327872 logging_writer.py:48] [43600] global_step=43600, grad_norm=4.280158519744873, loss=1.1478731632232666
I0211 23:50:29.865442 139647833720576 logging_writer.py:48] [43700] global_step=43700, grad_norm=2.4341742992401123, loss=1.1517480611801147
I0211 23:51:55.298990 139647825327872 logging_writer.py:48] [43800] global_step=43800, grad_norm=1.728756308555603, loss=1.161556601524353
I0211 23:53:25.315902 139647833720576 logging_writer.py:48] [43900] global_step=43900, grad_norm=2.398367404937744, loss=1.1243376731872559
I0211 23:54:56.531524 139647825327872 logging_writer.py:48] [44000] global_step=44000, grad_norm=2.6505565643310547, loss=1.1046136617660522
I0211 23:56:29.460592 139647833720576 logging_writer.py:48] [44100] global_step=44100, grad_norm=1.9828455448150635, loss=1.1006911993026733
I0211 23:57:59.511016 139647825327872 logging_writer.py:48] [44200] global_step=44200, grad_norm=3.4867403507232666, loss=1.1285204887390137
I0211 23:59:32.373133 139647833720576 logging_writer.py:48] [44300] global_step=44300, grad_norm=2.722689390182495, loss=1.129749059677124
I0212 00:00:49.440045 139647825327872 logging_writer.py:48] [44400] global_step=44400, grad_norm=2.6295695304870605, loss=1.083397388458252
I0212 00:01:30.922969 139803787056960 spec.py:321] Evaluating on the training split.
I0212 00:02:26.688504 139803787056960 spec.py:333] Evaluating on the validation split.
I0212 00:03:21.606650 139803787056960 spec.py:349] Evaluating on the test split.
I0212 00:03:48.267300 139803787056960 submission_runner.py:408] Time since start: 41197.74s, 	Step: 44453, 	{'train/ctc_loss': Array(0.15589492, dtype=float32), 'train/wer': 0.05179092625976479, 'validation/ctc_loss': Array(0.42629203, dtype=float32), 'validation/wer': 0.12280718692373789, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22446273, dtype=float32), 'test/wer': 0.0706233623788922, 'test/num_examples': 2472, 'score': 37467.42148900032, 'total_duration': 41197.74384975433, 'accumulated_submission_time': 37467.42148900032, 'accumulated_eval_time': 3726.8818225860596, 'accumulated_logging_time': 1.390561580657959}
I0212 00:03:48.312616 139647833720576 logging_writer.py:48] [44453] accumulated_eval_time=3726.881823, accumulated_logging_time=1.390562, accumulated_submission_time=37467.421489, global_step=44453, preemption_count=0, score=37467.421489, test/ctc_loss=0.22446273267269135, test/num_examples=2472, test/wer=0.070623, total_duration=41197.743850, train/ctc_loss=0.1558949202299118, train/wer=0.051791, validation/ctc_loss=0.4262920320034027, validation/num_examples=5348, validation/wer=0.122807
I0212 00:04:24.689222 139647825327872 logging_writer.py:48] [44500] global_step=44500, grad_norm=2.828962564468384, loss=1.1260513067245483
I0212 00:05:40.195672 139647833720576 logging_writer.py:48] [44600] global_step=44600, grad_norm=5.743124008178711, loss=1.1030468940734863
I0212 00:06:56.251677 139647825327872 logging_writer.py:48] [44700] global_step=44700, grad_norm=2.2321548461914062, loss=1.1827441453933716
I0212 00:08:18.282018 139647833720576 logging_writer.py:48] [44800] global_step=44800, grad_norm=2.379377841949463, loss=1.18699312210083
I0212 00:09:49.134529 139647825327872 logging_writer.py:48] [44900] global_step=44900, grad_norm=2.1594295501708984, loss=1.147541880607605
I0212 00:11:19.674766 139647833720576 logging_writer.py:48] [45000] global_step=45000, grad_norm=7.298234939575195, loss=1.0615888833999634
I0212 00:12:48.296921 139647825327872 logging_writer.py:48] [45100] global_step=45100, grad_norm=5.141013145446777, loss=1.093132495880127
I0212 00:14:17.144757 139647833720576 logging_writer.py:48] [45200] global_step=45200, grad_norm=2.6783180236816406, loss=1.144120454788208
I0212 00:15:48.930279 139647825327872 logging_writer.py:48] [45300] global_step=45300, grad_norm=2.133617401123047, loss=1.051910161972046
I0212 00:17:12.140669 139647833720576 logging_writer.py:48] [45400] global_step=45400, grad_norm=2.227928876876831, loss=1.0666320323944092
I0212 00:18:30.661612 139647825327872 logging_writer.py:48] [45500] global_step=45500, grad_norm=3.7350857257843018, loss=1.157508373260498
I0212 00:19:52.419484 139647833720576 logging_writer.py:48] [45600] global_step=45600, grad_norm=2.2412922382354736, loss=1.079030990600586
I0212 00:21:16.426400 139647825327872 logging_writer.py:48] [45700] global_step=45700, grad_norm=2.2712740898132324, loss=1.099956750869751
I0212 00:22:44.416572 139647833720576 logging_writer.py:48] [45800] global_step=45800, grad_norm=2.124265432357788, loss=1.0714218616485596
I0212 00:24:10.977656 139647825327872 logging_writer.py:48] [45900] global_step=45900, grad_norm=2.8901941776275635, loss=1.102367639541626
I0212 00:25:41.188002 139647833720576 logging_writer.py:48] [46000] global_step=46000, grad_norm=2.474900960922241, loss=1.1542121171951294
I0212 00:27:12.091045 139647825327872 logging_writer.py:48] [46100] global_step=46100, grad_norm=2.1430160999298096, loss=1.1121258735656738
I0212 00:27:48.588765 139803787056960 spec.py:321] Evaluating on the training split.
I0212 00:28:45.085058 139803787056960 spec.py:333] Evaluating on the validation split.
I0212 00:29:39.136009 139803787056960 spec.py:349] Evaluating on the test split.
I0212 00:30:06.611264 139803787056960 submission_runner.py:408] Time since start: 42776.09s, 	Step: 46141, 	{'train/ctc_loss': Array(0.13782987, dtype=float32), 'train/wer': 0.04953880407124682, 'validation/ctc_loss': Array(0.4256302, dtype=float32), 'validation/wer': 0.12244031010745629, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2233816, dtype=float32), 'test/wer': 0.07060305079926066, 'test/num_examples': 2472, 'score': 38907.607313632965, 'total_duration': 42776.08789777756, 'accumulated_submission_time': 38907.607313632965, 'accumulated_eval_time': 3864.898614168167, 'accumulated_logging_time': 1.456418752670288}
I0212 00:30:06.650400 139647833720576 logging_writer.py:48] [46141] accumulated_eval_time=3864.898614, accumulated_logging_time=1.456419, accumulated_submission_time=38907.607314, global_step=46141, preemption_count=0, score=38907.607314, test/ctc_loss=0.22338159382343292, test/num_examples=2472, test/wer=0.070603, total_duration=42776.087898, train/ctc_loss=0.13782986998558044, train/wer=0.049539, validation/ctc_loss=0.42563021183013916, validation/num_examples=5348, validation/wer=0.122440
I0212 00:30:52.326073 139647825327872 logging_writer.py:48] [46200] global_step=46200, grad_norm=2.1348257064819336, loss=1.0856080055236816
I0212 00:32:08.841835 139647833720576 logging_writer.py:48] [46300] global_step=46300, grad_norm=2.406222343444824, loss=1.1065220832824707
I0212 00:33:29.579268 139647833720576 logging_writer.py:48] [46400] global_step=46400, grad_norm=3.457545042037964, loss=1.1520432233810425
I0212 00:34:49.768542 139647825327872 logging_writer.py:48] [46500] global_step=46500, grad_norm=2.51861834526062, loss=1.1194647550582886
I0212 00:36:10.652150 139647833720576 logging_writer.py:48] [46600] global_step=46600, grad_norm=2.401272773742676, loss=1.1098145246505737
I0212 00:37:32.540245 139647825327872 logging_writer.py:48] [46700] global_step=46700, grad_norm=2.7356348037719727, loss=1.1012921333312988
I0212 00:39:00.495475 139647833720576 logging_writer.py:48] [46800] global_step=46800, grad_norm=2.353146553039551, loss=1.0816203355789185
I0212 00:40:28.391485 139647825327872 logging_writer.py:48] [46900] global_step=46900, grad_norm=2.9303174018859863, loss=1.1457656621932983
I0212 00:41:58.903061 139647833720576 logging_writer.py:48] [47000] global_step=47000, grad_norm=1.9544731378555298, loss=1.0627167224884033
I0212 00:43:29.735259 139647825327872 logging_writer.py:48] [47100] global_step=47100, grad_norm=2.2601046562194824, loss=1.070360541343689
I0212 00:44:58.628843 139647833720576 logging_writer.py:48] [47200] global_step=47200, grad_norm=3.116266965866089, loss=1.0947425365447998
I0212 00:46:30.651376 139647825327872 logging_writer.py:48] [47300] global_step=47300, grad_norm=3.784693956375122, loss=1.127339482307434
I0212 00:48:01.468357 139647833720576 logging_writer.py:48] [47400] global_step=47400, grad_norm=2.1673598289489746, loss=1.1358153820037842
I0212 00:49:18.056969 139647825327872 logging_writer.py:48] [47500] global_step=47500, grad_norm=1.7593735456466675, loss=1.0819934606552124
I0212 00:50:39.569544 139647833720576 logging_writer.py:48] [47600] global_step=47600, grad_norm=4.43398904800415, loss=1.1153056621551514
I0212 00:52:03.358408 139647825327872 logging_writer.py:48] [47700] global_step=47700, grad_norm=2.802980661392212, loss=1.15535569190979
I0212 00:53:29.160836 139647833720576 logging_writer.py:48] [47800] global_step=47800, grad_norm=3.628182888031006, loss=1.128110408782959
I0212 00:54:06.867710 139803787056960 spec.py:321] Evaluating on the training split.
I0212 00:55:03.360548 139803787056960 spec.py:333] Evaluating on the validation split.
I0212 00:55:57.019472 139803787056960 spec.py:349] Evaluating on the test split.
I0212 00:56:24.860464 139803787056960 submission_runner.py:408] Time since start: 44354.34s, 	Step: 47843, 	{'train/ctc_loss': Array(0.11888938, dtype=float32), 'train/wer': 0.04392925380302572, 'validation/ctc_loss': Array(0.42518348, dtype=float32), 'validation/wer': 0.12226652635237553, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22332466, dtype=float32), 'test/wer': 0.07054211606036602, 'test/num_examples': 2472, 'score': 40347.73718690872, 'total_duration': 44354.33743262291, 'accumulated_submission_time': 40347.73718690872, 'accumulated_eval_time': 4002.885982275009, 'accumulated_logging_time': 1.5108904838562012}
I0212 00:56:24.906273 139647833720576 logging_writer.py:48] [47843] accumulated_eval_time=4002.885982, accumulated_logging_time=1.510890, accumulated_submission_time=40347.737187, global_step=47843, preemption_count=0, score=40347.737187, test/ctc_loss=0.22332465648651123, test/num_examples=2472, test/wer=0.070542, total_duration=44354.337433, train/ctc_loss=0.11888938397169113, train/wer=0.043929, validation/ctc_loss=0.4251834750175476, validation/num_examples=5348, validation/wer=0.122267
I0212 00:57:08.884461 139647825327872 logging_writer.py:48] [47900] global_step=47900, grad_norm=6.070465564727783, loss=1.1320347785949707
I0212 00:58:24.230658 139803787056960 spec.py:321] Evaluating on the training split.
I0212 00:59:18.788022 139803787056960 spec.py:333] Evaluating on the validation split.
I0212 01:00:07.982860 139803787056960 spec.py:349] Evaluating on the test split.
I0212 01:00:32.675647 139803787056960 submission_runner.py:408] Time since start: 44602.16s, 	Step: 48000, 	{'train/ctc_loss': Array(0.12519462, dtype=float32), 'train/wer': 0.04541185510896711, 'validation/ctc_loss': Array(0.42519876, dtype=float32), 'validation/wer': 0.12229549031155565, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22333445, dtype=float32), 'test/wer': 0.07048118132147137, 'test/num_examples': 2472, 'score': 40467.03829741478, 'total_duration': 44602.15502882004, 'accumulated_submission_time': 40467.03829741478, 'accumulated_eval_time': 4131.328007936478, 'accumulated_logging_time': 1.5726885795593262}
I0212 01:00:32.705995 139647833720576 logging_writer.py:48] [48000] accumulated_eval_time=4131.328008, accumulated_logging_time=1.572689, accumulated_submission_time=40467.038297, global_step=48000, preemption_count=0, score=40467.038297, test/ctc_loss=0.2233344465494156, test/num_examples=2472, test/wer=0.070481, total_duration=44602.155029, train/ctc_loss=0.12519462406635284, train/wer=0.045412, validation/ctc_loss=0.4251987636089325, validation/num_examples=5348, validation/wer=0.122295
I0212 01:00:32.729205 139647825327872 logging_writer.py:48] [48000] global_step=48000, preemption_count=0, score=40467.038297
I0212 01:00:32.968287 139803787056960 checkpoints.py:490] Saving checkpoint at step: 48000
I0212 01:00:33.971995 139803787056960 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_1/librispeech_deepspeech_jax/trial_2/checkpoint_48000
I0212 01:00:33.991300 139803787056960 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_1/librispeech_deepspeech_jax/trial_2/checkpoint_48000.
I0212 01:00:35.362115 139803787056960 submission_runner.py:583] Tuning trial 2/5
I0212 01:00:35.362412 139803787056960 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.2, learning_rate=0.0008445074561975979, one_minus_beta1=0.11042418465, beta2=0.9978504782314613, weight_decay=0.08135402759553023, warmup_factor=0.05)
I0212 01:00:35.376090 139803787056960 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(31.934551, dtype=float32), 'train/wer': 4.645983478794528, 'validation/ctc_loss': Array(30.812979, dtype=float32), 'validation/wer': 4.233507438910182, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.939756, dtype=float32), 'test/wer': 4.561208945219669, 'test/num_examples': 2472, 'score': 15.68349552154541, 'total_duration': 234.63548374176025, 'accumulated_submission_time': 15.68349552154541, 'accumulated_eval_time': 218.9519054889679, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1709, {'train/ctc_loss': Array(6.530197, dtype=float32), 'train/wer': 0.936823569833362, 'validation/ctc_loss': Array(6.3885655, dtype=float32), 'validation/wer': 0.8922154532376879, 'validation/num_examples': 5348, 'test/ctc_loss': Array(6.318093, dtype=float32), 'test/wer': 0.895131314362318, 'test/num_examples': 2472, 'score': 1456.0923523902893, 'total_duration': 1787.2602503299713, 'accumulated_submission_time': 1456.0923523902893, 'accumulated_eval_time': 331.06379795074463, 'accumulated_logging_time': 0.03165459632873535, 'global_step': 1709, 'preemption_count': 0}), (3429, {'train/ctc_loss': Array(2.8725617, dtype=float32), 'train/wer': 0.6321504898078738, 'validation/ctc_loss': Array(2.7419052, dtype=float32), 'validation/wer': 0.5899089566216438, 'validation/num_examples': 5348, 'test/ctc_loss': Array(2.2226102, dtype=float32), 'test/wer': 0.5148985436597404, 'test/num_examples': 2472, 'score': 2896.3682050704956, 'total_duration': 3355.2950081825256, 'accumulated_submission_time': 2896.3682050704956, 'accumulated_eval_time': 458.69384694099426, 'accumulated_logging_time': 0.08218216896057129, 'global_step': 3429, 'preemption_count': 0}), (5129, {'train/ctc_loss': Array(0.80511504, dtype=float32), 'train/wer': 0.25221808143547275, 'validation/ctc_loss': Array(0.916454, dtype=float32), 'validation/wer': 0.2608590710292826, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5947173, dtype=float32), 'test/wer': 0.19468649076838707, 'test/num_examples': 2472, 'score': 4336.491911649704, 'total_duration': 4927.635001182556, 'accumulated_submission_time': 4336.491911649704, 'accumulated_eval_time': 590.7899439334869, 'accumulated_logging_time': 0.1266040802001953, 'global_step': 5129, 'preemption_count': 0}), (6825, {'train/ctc_loss': Array(0.6916581, dtype=float32), 'train/wer': 0.22251410780307454, 'validation/ctc_loss': Array(0.80308455, dtype=float32), 'validation/wer': 0.23031174874730875, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.49312124, dtype=float32), 'test/wer': 0.16295980338390917, 'test/num_examples': 2472, 'score': 5777.701329469681, 'total_duration': 6503.6665942668915, 'accumulated_submission_time': 5777.701329469681, 'accumulated_eval_time': 725.4847946166992, 'accumulated_logging_time': 0.17483735084533691, 'global_step': 6825, 'preemption_count': 0}), (8557, {'train/ctc_loss': Array(0.58069324, dtype=float32), 'train/wer': 0.18633398316252628, 'validation/ctc_loss': Array(0.71140873, dtype=float32), 'validation/wer': 0.20462071695453624, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.42479584, dtype=float32), 'test/wer': 0.13968273312615523, 'test/num_examples': 2472, 'score': 7218.413636207581, 'total_duration': 8078.009069681168, 'accumulated_submission_time': 7218.413636207581, 'accumulated_eval_time': 858.9805746078491, 'accumulated_logging_time': 0.23076987266540527, 'global_step': 8557, 'preemption_count': 0}), (10255, {'train/ctc_loss': Array(0.5201113, dtype=float32), 'train/wer': 0.16978276952047885, 'validation/ctc_loss': Array(0.67553055, dtype=float32), 'validation/wer': 0.19477297083329312, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.40202525, dtype=float32), 'test/wer': 0.1314565433753783, 'test/num_examples': 2472, 'score': 8659.064038991928, 'total_duration': 9655.32750749588, 'accumulated_submission_time': 8659.064038991928, 'accumulated_eval_time': 995.5229451656342, 'accumulated_logging_time': 0.27828240394592285, 'global_step': 10255, 'preemption_count': 0}), (11949, {'train/ctc_loss': Array(0.50205463, dtype=float32), 'train/wer': 0.16703938660946557, 'validation/ctc_loss': Array(0.64501023, dtype=float32), 'validation/wer': 0.1888836324666673, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.37967756, dtype=float32), 'test/wer': 0.1240834399691264, 'test/num_examples': 2472, 'score': 10099.236858606339, 'total_duration': 11230.648217201233, 'accumulated_submission_time': 10099.236858606339, 'accumulated_eval_time': 1130.5369474887848, 'accumulated_logging_time': 0.3322751522064209, 'global_step': 11949, 'preemption_count': 0}), (13663, {'train/ctc_loss': Array(0.44674954, dtype=float32), 'train/wer': 0.14886399142971923, 'validation/ctc_loss': Array(0.61529595, dtype=float32), 'validation/wer': 0.17750079650887746, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3549172, dtype=float32), 'test/wer': 0.11644628602766437, 'test/num_examples': 2472, 'score': 11539.904070854187, 'total_duration': 12808.168608665466, 'accumulated_submission_time': 11539.904070854187, 'accumulated_eval_time': 1267.2597525119781, 'accumulated_logging_time': 0.38460445404052734, 'global_step': 13663, 'preemption_count': 0}), (15356, {'train/ctc_loss': Array(0.44082975, dtype=float32), 'train/wer': 0.14504916390054487, 'validation/ctc_loss': Array(0.59986717, dtype=float32), 'validation/wer': 0.17326240381551888, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3436971, dtype=float32), 'test/wer': 0.11252615115877562, 'test/num_examples': 2472, 'score': 12979.79869055748, 'total_duration': 14382.923528432846, 'accumulated_submission_time': 12979.79869055748, 'accumulated_eval_time': 1401.9886286258698, 'accumulated_logging_time': 0.43735527992248535, 'global_step': 15356, 'preemption_count': 0}), (17068, {'train/ctc_loss': Array(0.4190414, dtype=float32), 'train/wer': 0.14002370251112453, 'validation/ctc_loss': Array(0.58253497, dtype=float32), 'validation/wer': 0.168261293530417, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3291357, dtype=float32), 'test/wer': 0.10757012572867793, 'test/num_examples': 2472, 'score': 14420.747940063477, 'total_duration': 15957.678583145142, 'accumulated_submission_time': 14420.747940063477, 'accumulated_eval_time': 1535.6642746925354, 'accumulated_logging_time': 0.4859333038330078, 'global_step': 17068, 'preemption_count': 0}), (18794, {'train/ctc_loss': Array(0.39970213, dtype=float32), 'train/wer': 0.13711078888237052, 'validation/ctc_loss': Array(0.5615143, dtype=float32), 'validation/wer': 0.16312501810247448, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31767148, dtype=float32), 'test/wer': 0.10389372981536774, 'test/num_examples': 2472, 'score': 15860.9145257473, 'total_duration': 17532.088482141495, 'accumulated_submission_time': 15860.9145257473, 'accumulated_eval_time': 1669.7652735710144, 'accumulated_logging_time': 0.5466783046722412, 'global_step': 18794, 'preemption_count': 0}), (20489, {'train/ctc_loss': Array(0.3484167, dtype=float32), 'train/wer': 0.11707835565631837, 'validation/ctc_loss': Array(0.5440495, dtype=float32), 'validation/wer': 0.15924384757233748, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30769292, dtype=float32), 'test/wer': 0.10174070237442366, 'test/num_examples': 2472, 'score': 17301.38058924675, 'total_duration': 19107.35986685753, 'accumulated_submission_time': 17301.38058924675, 'accumulated_eval_time': 1804.4356830120087, 'accumulated_logging_time': 0.6045889854431152, 'global_step': 20489, 'preemption_count': 0}), (22206, {'train/ctc_loss': Array(0.3176922, dtype=float32), 'train/wer': 0.10810351565818763, 'validation/ctc_loss': Array(0.52992153, dtype=float32), 'validation/wer': 0.15386620581789393, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2998111, dtype=float32), 'test/wer': 0.09725184327585157, 'test/num_examples': 2472, 'score': 18741.621086359024, 'total_duration': 20682.414192676544, 'accumulated_submission_time': 18741.621086359024, 'accumulated_eval_time': 1939.11581158638, 'accumulated_logging_time': 0.6574249267578125, 'global_step': 22206, 'preemption_count': 0}), (23906, {'train/ctc_loss': Array(0.35585138, dtype=float32), 'train/wer': 0.12282533840613252, 'validation/ctc_loss': Array(0.5189788, dtype=float32), 'validation/wer': 0.1510470471243616, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29122898, dtype=float32), 'test/wer': 0.09520037373306522, 'test/num_examples': 2472, 'score': 20181.759190559387, 'total_duration': 22257.7140955925, 'accumulated_submission_time': 20181.759190559387, 'accumulated_eval_time': 2074.142430305481, 'accumulated_logging_time': 0.7140963077545166, 'global_step': 23906, 'preemption_count': 0}), (25609, {'train/ctc_loss': Array(0.325746, dtype=float32), 'train/wer': 0.10997155858930603, 'validation/ctc_loss': Array(0.5139698, dtype=float32), 'validation/wer': 0.14872993038995144, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28193888, dtype=float32), 'test/wer': 0.09316921576991043, 'test/num_examples': 2472, 'score': 21621.865079164505, 'total_duration': 23833.15885949135, 'accumulated_submission_time': 21621.865079164505, 'accumulated_eval_time': 2209.3501620292664, 'accumulated_logging_time': 0.7669525146484375, 'global_step': 25609, 'preemption_count': 0}), (27335, {'train/ctc_loss': Array(0.30888176, dtype=float32), 'train/wer': 0.10399692085847333, 'validation/ctc_loss': Array(0.4966396, dtype=float32), 'validation/wer': 0.14369020149260936, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2696854, dtype=float32), 'test/wer': 0.08837568297686511, 'test/num_examples': 2472, 'score': 23062.529305696487, 'total_duration': 25411.276579618454, 'accumulated_submission_time': 23062.529305696487, 'accumulated_eval_time': 2346.664496421814, 'accumulated_logging_time': 0.8250465393066406, 'global_step': 27335, 'preemption_count': 0}), (29061, {'train/ctc_loss': Array(0.28284952, dtype=float32), 'train/wer': 0.09740008831971737, 'validation/ctc_loss': Array(0.49027404, dtype=float32), 'validation/wer': 0.14099655328885757, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2685104, dtype=float32), 'test/wer': 0.08597891658034246, 'test/num_examples': 2472, 'score': 24502.98046898842, 'total_duration': 26988.340623378754, 'accumulated_submission_time': 24502.98046898842, 'accumulated_eval_time': 2483.1363592147827, 'accumulated_logging_time': 0.8873429298400879, 'global_step': 29061, 'preemption_count': 0}), (30754, {'train/ctc_loss': Array(0.26444355, dtype=float32), 'train/wer': 0.0926417954318846, 'validation/ctc_loss': Array(0.47725204, dtype=float32), 'validation/wer': 0.13774293520762332, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2558537, dtype=float32), 'test/wer': 0.08309467227266265, 'test/num_examples': 2472, 'score': 25943.658811092377, 'total_duration': 28563.02157688141, 'accumulated_submission_time': 25943.658811092377, 'accumulated_eval_time': 2617.004650115967, 'accumulated_logging_time': 0.9421923160552979, 'global_step': 30754, 'preemption_count': 0}), (32487, {'train/ctc_loss': Array(0.25097135, dtype=float32), 'train/wer': 0.08781035517125606, 'validation/ctc_loss': Array(0.46989283, dtype=float32), 'validation/wer': 0.136758160595499, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2501571, dtype=float32), 'test/wer': 0.08171348485771739, 'test/num_examples': 2472, 'score': 27383.986397266388, 'total_duration': 30138.511751174927, 'accumulated_submission_time': 27383.986397266388, 'accumulated_eval_time': 2752.034639120102, 'accumulated_logging_time': 0.9958689212799072, 'global_step': 32487, 'preemption_count': 0}), (34220, {'train/ctc_loss': Array(0.15385501, dtype=float32), 'train/wer': 0.0554912004761095, 'validation/ctc_loss': Array(0.4573218, dtype=float32), 'validation/wer': 0.13210461782055863, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24460703, dtype=float32), 'test/wer': 0.07889017528893222, 'test/num_examples': 2472, 'score': 28824.258416175842, 'total_duration': 31731.703934669495, 'accumulated_submission_time': 28824.258416175842, 'accumulated_eval_time': 2904.8154451847076, 'accumulated_logging_time': 1.0548536777496338, 'global_step': 34220, 'preemption_count': 0}), (35919, {'train/ctc_loss': Array(0.14717703, dtype=float32), 'train/wer': 0.050968087151221086, 'validation/ctc_loss': Array(0.44906735, dtype=float32), 'validation/wer': 0.12888961835156454, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23849553, dtype=float32), 'test/wer': 0.07606686572014705, 'test/num_examples': 2472, 'score': 30264.718192100525, 'total_duration': 33308.779076337814, 'accumulated_submission_time': 30264.718192100525, 'accumulated_eval_time': 3041.3079862594604, 'accumulated_logging_time': 1.1017398834228516, 'global_step': 35919, 'preemption_count': 0}), (37639, {'train/ctc_loss': Array(0.15311022, dtype=float32), 'train/wer': 0.05433547398187056, 'validation/ctc_loss': Array(0.4453374, dtype=float32), 'validation/wer': 0.1284261950046825, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23440033, dtype=float32), 'test/wer': 0.07395446143846607, 'test/num_examples': 2472, 'score': 31705.608791828156, 'total_duration': 34887.10219669342, 'accumulated_submission_time': 31705.608791828156, 'accumulated_eval_time': 3178.5993349552155, 'accumulated_logging_time': 1.1614861488342285, 'global_step': 37639, 'preemption_count': 0}), (39347, {'train/ctc_loss': Array(0.13366854, dtype=float32), 'train/wer': 0.04803719008264463, 'validation/ctc_loss': Array(0.43505618, dtype=float32), 'validation/wer': 0.12496017455612733, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23139994, dtype=float32), 'test/wer': 0.07346698352730892, 'test/num_examples': 2472, 'score': 33146.07905125618, 'total_duration': 36463.68469786644, 'accumulated_submission_time': 33146.07905125618, 'accumulated_eval_time': 3314.5751991271973, 'accumulated_logging_time': 1.2173850536346436, 'global_step': 39347, 'preemption_count': 0}), (41036, {'train/ctc_loss': Array(0.14206897, dtype=float32), 'train/wer': 0.05088739730555363, 'validation/ctc_loss': Array(0.43153396, dtype=float32), 'validation/wer': 0.12453536982148547, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22643007, dtype=float32), 'test/wer': 0.07287794771799402, 'test/num_examples': 2472, 'score': 34586.41890668869, 'total_duration': 38039.82860040665, 'accumulated_submission_time': 34586.41890668869, 'accumulated_eval_time': 3450.242693901062, 'accumulated_logging_time': 1.2768633365631104, 'global_step': 41036, 'preemption_count': 0}), (42749, {'train/ctc_loss': Array(0.13182025, dtype=float32), 'train/wer': 0.04785386859286012, 'validation/ctc_loss': Array(0.42808613, dtype=float32), 'validation/wer': 0.12368576035220175, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22455934, dtype=float32), 'test/wer': 0.0707452318566815, 'test/num_examples': 2472, 'score': 36026.87464380264, 'total_duration': 39619.72004675865, 'accumulated_submission_time': 36026.87464380264, 'accumulated_eval_time': 3589.543283224106, 'accumulated_logging_time': 1.3324298858642578, 'global_step': 42749, 'preemption_count': 0}), (44453, {'train/ctc_loss': Array(0.15589492, dtype=float32), 'train/wer': 0.05179092625976479, 'validation/ctc_loss': Array(0.42629203, dtype=float32), 'validation/wer': 0.12280718692373789, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22446273, dtype=float32), 'test/wer': 0.0706233623788922, 'test/num_examples': 2472, 'score': 37467.42148900032, 'total_duration': 41197.74384975433, 'accumulated_submission_time': 37467.42148900032, 'accumulated_eval_time': 3726.8818225860596, 'accumulated_logging_time': 1.390561580657959, 'global_step': 44453, 'preemption_count': 0}), (46141, {'train/ctc_loss': Array(0.13782987, dtype=float32), 'train/wer': 0.04953880407124682, 'validation/ctc_loss': Array(0.4256302, dtype=float32), 'validation/wer': 0.12244031010745629, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2233816, dtype=float32), 'test/wer': 0.07060305079926066, 'test/num_examples': 2472, 'score': 38907.607313632965, 'total_duration': 42776.08789777756, 'accumulated_submission_time': 38907.607313632965, 'accumulated_eval_time': 3864.898614168167, 'accumulated_logging_time': 1.456418752670288, 'global_step': 46141, 'preemption_count': 0}), (47843, {'train/ctc_loss': Array(0.11888938, dtype=float32), 'train/wer': 0.04392925380302572, 'validation/ctc_loss': Array(0.42518348, dtype=float32), 'validation/wer': 0.12226652635237553, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22332466, dtype=float32), 'test/wer': 0.07054211606036602, 'test/num_examples': 2472, 'score': 40347.73718690872, 'total_duration': 44354.33743262291, 'accumulated_submission_time': 40347.73718690872, 'accumulated_eval_time': 4002.885982275009, 'accumulated_logging_time': 1.5108904838562012, 'global_step': 47843, 'preemption_count': 0}), (48000, {'train/ctc_loss': Array(0.12519462, dtype=float32), 'train/wer': 0.04541185510896711, 'validation/ctc_loss': Array(0.42519876, dtype=float32), 'validation/wer': 0.12229549031155565, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22333445, dtype=float32), 'test/wer': 0.07048118132147137, 'test/num_examples': 2472, 'score': 40467.03829741478, 'total_duration': 44602.15502882004, 'accumulated_submission_time': 40467.03829741478, 'accumulated_eval_time': 4131.328007936478, 'accumulated_logging_time': 1.5726885795593262, 'global_step': 48000, 'preemption_count': 0})], 'global_step': 48000}
I0212 01:00:35.376385 139803787056960 submission_runner.py:586] Timing: 40467.03829741478
I0212 01:00:35.376449 139803787056960 submission_runner.py:588] Total number of evals: 30
I0212 01:00:35.376505 139803787056960 submission_runner.py:589] ====================
I0212 01:00:35.376571 139803787056960 submission_runner.py:542] Using RNG seed 808887856
I0212 01:00:35.380005 139803787056960 submission_runner.py:551] --- Tuning run 3/5 ---
I0212 01:00:35.380154 139803787056960 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_1/librispeech_deepspeech_jax/trial_3.
I0212 01:00:35.382934 139803787056960 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_1/librispeech_deepspeech_jax/trial_3/hparams.json.
I0212 01:00:35.385246 139803787056960 submission_runner.py:206] Initializing dataset.
I0212 01:00:35.385378 139803787056960 submission_runner.py:213] Initializing model.
I0212 01:00:36.489642 139803787056960 submission_runner.py:255] Initializing optimizer.
I0212 01:00:36.647693 139803787056960 submission_runner.py:262] Initializing metrics bundle.
I0212 01:00:36.647885 139803787056960 submission_runner.py:280] Initializing checkpoint and logger.
I0212 01:00:36.651825 139803787056960 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_1/librispeech_deepspeech_jax/trial_3 with prefix checkpoint_
I0212 01:00:36.651970 139803787056960 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_1/librispeech_deepspeech_jax/trial_3/meta_data_0.json.
I0212 01:00:36.652349 139803787056960 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0212 01:00:36.652433 139803787056960 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0212 01:00:37.252182 139803787056960 logger_utils.py:220] Unable to record git information. Continuing without it.
I0212 01:00:37.796952 139803787056960 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_1/librispeech_deepspeech_jax/trial_3/flags_0.json.
I0212 01:00:37.814881 139803787056960 submission_runner.py:314] Starting training loop.
I0212 01:00:37.818502 139803787056960 input_pipeline.py:20] Loading split = train-clean-100
I0212 01:00:37.874736 139803787056960 input_pipeline.py:20] Loading split = train-clean-360
I0212 01:00:38.005676 139803787056960 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0212 01:00:53.111574 139647716222720 logging_writer.py:48] [0] global_step=0, grad_norm=18.487438201904297, loss=32.706390380859375
I0212 01:00:53.126768 139803787056960 spec.py:321] Evaluating on the training split.
I0212 01:02:51.041043 139803787056960 spec.py:333] Evaluating on the validation split.
I0212 01:03:57.724765 139803787056960 spec.py:349] Evaluating on the test split.
I0212 01:04:32.142790 139803787056960 submission_runner.py:408] Time since start: 234.33s, 	Step: 1, 	{'train/ctc_loss': Array(31.498302, dtype=float32), 'train/wer': 4.839384963377855, 'validation/ctc_loss': Array(30.813007, dtype=float32), 'validation/wer': 4.233526748216303, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.939787, dtype=float32), 'test/wer': 4.560619909410355, 'test/num_examples': 2472, 'score': 15.311807870864868, 'total_duration': 234.32508182525635, 'accumulated_submission_time': 15.311807870864868, 'accumulated_eval_time': 219.01320672035217, 'accumulated_logging_time': 0}
I0212 01:04:32.164423 139647833720576 logging_writer.py:48] [1] accumulated_eval_time=219.013207, accumulated_logging_time=0, accumulated_submission_time=15.311808, global_step=1, preemption_count=0, score=15.311808, test/ctc_loss=30.939786911010742, test/num_examples=2472, test/wer=4.560620, total_duration=234.325082, train/ctc_loss=31.498302459716797, train/wer=4.839385, validation/ctc_loss=30.813007354736328, validation/num_examples=5348, validation/wer=4.233527
I0212 01:05:57.636142 139647517968128 logging_writer.py:48] [100] global_step=100, grad_norm=5.000580310821533, loss=8.500216484069824
I0212 01:07:14.924560 139647526360832 logging_writer.py:48] [200] global_step=200, grad_norm=1.1592822074890137, loss=6.384143352508545
I0212 01:08:32.567448 139647517968128 logging_writer.py:48] [300] global_step=300, grad_norm=0.5609198212623596, loss=6.026815891265869
I0212 01:09:49.735409 139647526360832 logging_writer.py:48] [400] global_step=400, grad_norm=0.4647340476512909, loss=5.885781288146973
I0212 01:11:16.059880 139647517968128 logging_writer.py:48] [500] global_step=500, grad_norm=0.3990075886249542, loss=5.849607467651367
I0212 01:12:43.793562 139647526360832 logging_writer.py:48] [600] global_step=600, grad_norm=0.721221923828125, loss=5.7690043449401855
I0212 01:14:10.236840 139647517968128 logging_writer.py:48] [700] global_step=700, grad_norm=0.4057527184486389, loss=5.721275806427002
I0212 01:15:40.363595 139647526360832 logging_writer.py:48] [800] global_step=800, grad_norm=0.4720337390899658, loss=5.6250152587890625
I0212 01:17:10.315472 139647517968128 logging_writer.py:48] [900] global_step=900, grad_norm=0.767170250415802, loss=5.38309383392334
I0212 01:18:39.605475 139647526360832 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.69770348072052, loss=5.05912971496582
I0212 01:20:02.759524 139647833720576 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.0413898229599, loss=4.574700832366943
I0212 01:21:20.931017 139647825327872 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.0461639165878296, loss=4.223183631896973
I0212 01:22:39.583360 139647833720576 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.7766133546829224, loss=3.849079132080078
I0212 01:24:01.197163 139647825327872 logging_writer.py:48] [1400] global_step=1400, grad_norm=2.187563419342041, loss=3.691351890563965
I0212 01:25:25.532586 139647833720576 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.8809380531311035, loss=3.501269578933716
I0212 01:26:53.941822 139647825327872 logging_writer.py:48] [1600] global_step=1600, grad_norm=1.731078028678894, loss=3.3212733268737793
I0212 01:28:26.213759 139647833720576 logging_writer.py:48] [1700] global_step=1700, grad_norm=4.375469207763672, loss=3.2063891887664795
I0212 01:28:32.600938 139803787056960 spec.py:321] Evaluating on the training split.
I0212 01:29:13.359580 139803787056960 spec.py:333] Evaluating on the validation split.
I0212 01:30:00.686441 139803787056960 spec.py:349] Evaluating on the test split.
I0212 01:30:24.780979 139803787056960 submission_runner.py:408] Time since start: 1786.96s, 	Step: 1709, 	{'train/ctc_loss': Array(6.1106343, dtype=float32), 'train/wer': 0.9224396410326059, 'validation/ctc_loss': Array(6.18663, dtype=float32), 'validation/wer': 0.886992285932205, 'validation/num_examples': 5348, 'test/ctc_loss': Array(6.0562162, dtype=float32), 'test/wer': 0.888387869924644, 'test/num_examples': 2472, 'score': 1455.6672337055206, 'total_duration': 1786.9586765766144, 'accumulated_submission_time': 1455.6672337055206, 'accumulated_eval_time': 331.1859018802643, 'accumulated_logging_time': 0.032968759536743164}
I0212 01:30:24.816840 139647833720576 logging_writer.py:48] [1709] accumulated_eval_time=331.185902, accumulated_logging_time=0.032969, accumulated_submission_time=1455.667234, global_step=1709, preemption_count=0, score=1455.667234, test/ctc_loss=6.056216239929199, test/num_examples=2472, test/wer=0.888388, total_duration=1786.958677, train/ctc_loss=6.1106343269348145, train/wer=0.922440, validation/ctc_loss=6.186629772186279, validation/num_examples=5348, validation/wer=0.886992
I0212 01:31:34.934768 139647825327872 logging_writer.py:48] [1800] global_step=1800, grad_norm=2.460082530975342, loss=3.062730312347412
I0212 01:32:51.990394 139647833720576 logging_writer.py:48] [1900] global_step=1900, grad_norm=2.411466360092163, loss=2.956240177154541
I0212 01:34:15.150598 139647825327872 logging_writer.py:48] [2000] global_step=2000, grad_norm=2.0726399421691895, loss=2.8466856479644775
I0212 01:35:41.873259 139647833720576 logging_writer.py:48] [2100] global_step=2100, grad_norm=2.869633913040161, loss=2.819251298904419
I0212 01:37:00.572195 139647825327872 logging_writer.py:48] [2200] global_step=2200, grad_norm=2.5009007453918457, loss=2.7471039295196533
I0212 01:38:20.289551 139647833720576 logging_writer.py:48] [2300] global_step=2300, grad_norm=3.2233846187591553, loss=2.6585769653320312
I0212 01:39:42.498936 139647825327872 logging_writer.py:48] [2400] global_step=2400, grad_norm=3.9214653968811035, loss=2.6399450302124023
I0212 01:41:09.577912 139647833720576 logging_writer.py:48] [2500] global_step=2500, grad_norm=2.5860962867736816, loss=2.523744583129883
I0212 01:42:35.762457 139647825327872 logging_writer.py:48] [2600] global_step=2600, grad_norm=2.8457350730895996, loss=2.3718791007995605
I0212 01:44:04.441764 139647833720576 logging_writer.py:48] [2700] global_step=2700, grad_norm=2.7952098846435547, loss=2.439253330230713
I0212 01:45:35.732058 139647825327872 logging_writer.py:48] [2800] global_step=2800, grad_norm=4.442480564117432, loss=2.395112991333008
I0212 01:47:06.080084 139647833720576 logging_writer.py:48] [2900] global_step=2900, grad_norm=2.937487840652466, loss=2.32682204246521
I0212 01:48:36.696198 139647825327872 logging_writer.py:48] [3000] global_step=3000, grad_norm=2.930358648300171, loss=2.2808122634887695
I0212 01:50:09.872232 139647833720576 logging_writer.py:48] [3100] global_step=3100, grad_norm=2.161212682723999, loss=2.2000937461853027
I0212 01:51:27.576850 139647825327872 logging_writer.py:48] [3200] global_step=3200, grad_norm=4.165107250213623, loss=2.2788631916046143
I0212 01:52:43.165824 139647833720576 logging_writer.py:48] [3300] global_step=3300, grad_norm=4.223355770111084, loss=2.2208752632141113
I0212 01:54:01.557932 139647825327872 logging_writer.py:48] [3400] global_step=3400, grad_norm=4.285852432250977, loss=2.2150092124938965
I0212 01:54:25.160360 139803787056960 spec.py:321] Evaluating on the training split.
I0212 01:55:11.333451 139803787056960 spec.py:333] Evaluating on the validation split.
I0212 01:56:02.758915 139803787056960 spec.py:349] Evaluating on the test split.
I0212 01:56:29.098755 139803787056960 submission_runner.py:408] Time since start: 3351.28s, 	Step: 3429, 	{'train/ctc_loss': Array(4.5104766, dtype=float32), 'train/wer': 0.8586798150220665, 'validation/ctc_loss': Array(4.5426497, dtype=float32), 'validation/wer': 0.8317097425104029, 'validation/num_examples': 5348, 'test/ctc_loss': Array(4.1909637, dtype=float32), 'test/wer': 0.8059228566205594, 'test/num_examples': 2472, 'score': 2895.917924642563, 'total_duration': 3351.280608177185, 'accumulated_submission_time': 2895.917924642563, 'accumulated_eval_time': 455.1211357116699, 'accumulated_logging_time': 0.09017682075500488}
I0212 01:56:29.132882 139647833720576 logging_writer.py:48] [3429] accumulated_eval_time=455.121136, accumulated_logging_time=0.090177, accumulated_submission_time=2895.917925, global_step=3429, preemption_count=0, score=2895.917925, test/ctc_loss=4.1909637451171875, test/num_examples=2472, test/wer=0.805923, total_duration=3351.280608, train/ctc_loss=4.510476589202881, train/wer=0.858680, validation/ctc_loss=4.542649745941162, validation/num_examples=5348, validation/wer=0.831710
I0212 01:57:24.614652 139647825327872 logging_writer.py:48] [3500] global_step=3500, grad_norm=2.6978273391723633, loss=2.183894395828247
I0212 01:58:40.131922 139647833720576 logging_writer.py:48] [3600] global_step=3600, grad_norm=3.9436426162719727, loss=2.1633145809173584
I0212 02:00:03.745401 139647825327872 logging_writer.py:48] [3700] global_step=3700, grad_norm=3.2370193004608154, loss=2.135883092880249
I0212 02:01:31.860467 139647833720576 logging_writer.py:48] [3800] global_step=3800, grad_norm=4.027213096618652, loss=2.1247427463531494
I0212 02:03:01.402345 139647825327872 logging_writer.py:48] [3900] global_step=3900, grad_norm=4.277883529663086, loss=2.088900327682495
I0212 02:04:30.669921 139647833720576 logging_writer.py:48] [4000] global_step=4000, grad_norm=4.222257137298584, loss=2.0626814365386963
I0212 02:06:02.177000 139647825327872 logging_writer.py:48] [4100] global_step=4100, grad_norm=3.440420627593994, loss=2.0320053100585938
I0212 02:07:25.361191 139647833720576 logging_writer.py:48] [4200] global_step=4200, grad_norm=4.165499210357666, loss=1.9839709997177124
I0212 02:08:41.901366 139647825327872 logging_writer.py:48] [4300] global_step=4300, grad_norm=3.8082714080810547, loss=2.025269031524658
I0212 02:10:02.213961 139647833720576 logging_writer.py:48] [4400] global_step=4400, grad_norm=3.273906946182251, loss=1.989821434020996
I0212 02:11:23.985602 139647825327872 logging_writer.py:48] [4500] global_step=4500, grad_norm=4.191954135894775, loss=2.040018320083618
I0212 02:12:51.041704 139647833720576 logging_writer.py:48] [4600] global_step=4600, grad_norm=4.689402103424072, loss=1.9641246795654297
I0212 02:14:21.914689 139647825327872 logging_writer.py:48] [4700] global_step=4700, grad_norm=4.443674087524414, loss=1.9422508478164673
I0212 02:15:51.756773 139647833720576 logging_writer.py:48] [4800] global_step=4800, grad_norm=3.2640249729156494, loss=1.99582040309906
I0212 02:17:21.586677 139647825327872 logging_writer.py:48] [4900] global_step=4900, grad_norm=3.083923816680908, loss=1.9018968343734741
I0212 02:18:51.124291 139647833720576 logging_writer.py:48] [5000] global_step=5000, grad_norm=3.265986204147339, loss=1.8868322372436523
I0212 02:20:22.073188 139647825327872 logging_writer.py:48] [5100] global_step=5100, grad_norm=2.81796932220459, loss=1.984877109527588
I0212 02:20:29.817020 139803787056960 spec.py:321] Evaluating on the training split.
I0212 02:21:30.912692 139803787056960 spec.py:333] Evaluating on the validation split.
I0212 02:22:24.272799 139803787056960 spec.py:349] Evaluating on the test split.
I0212 02:22:50.510906 139803787056960 submission_runner.py:408] Time since start: 4932.69s, 	Step: 5110, 	{'train/ctc_loss': Array(0.7030768, dtype=float32), 'train/wer': 0.22973511201229005, 'validation/ctc_loss': Array(1.0333956, dtype=float32), 'validation/wer': 0.2909333153113143, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.694737, dtype=float32), 'test/wer': 0.2199134726707696, 'test/num_examples': 2472, 'score': 4336.514578104019, 'total_duration': 4932.689397573471, 'accumulated_submission_time': 4336.514578104019, 'accumulated_eval_time': 595.8084809780121, 'accumulated_logging_time': 0.14051008224487305}
I0212 02:22:50.546992 139647833720576 logging_writer.py:48] [5110] accumulated_eval_time=595.808481, accumulated_logging_time=0.140510, accumulated_submission_time=4336.514578, global_step=5110, preemption_count=0, score=4336.514578, test/ctc_loss=0.6947370171546936, test/num_examples=2472, test/wer=0.219913, total_duration=4932.689398, train/ctc_loss=0.7030767798423767, train/wer=0.229735, validation/ctc_loss=1.0333956480026245, validation/num_examples=5348, validation/wer=0.290933
I0212 02:24:03.716119 139647833720576 logging_writer.py:48] [5200] global_step=5200, grad_norm=4.257408618927002, loss=1.89610755443573
I0212 02:25:23.260349 139647825327872 logging_writer.py:48] [5300] global_step=5300, grad_norm=3.763401508331299, loss=1.8047959804534912
I0212 02:26:43.683003 139647833720576 logging_writer.py:48] [5400] global_step=5400, grad_norm=3.8143551349639893, loss=1.8255255222320557
I0212 02:28:04.250031 139647825327872 logging_writer.py:48] [5500] global_step=5500, grad_norm=2.6777572631835938, loss=1.8942312002182007
I0212 02:29:30.860806 139647833720576 logging_writer.py:48] [5600] global_step=5600, grad_norm=3.133068799972534, loss=1.8907207250595093
I0212 02:30:59.681830 139647825327872 logging_writer.py:48] [5700] global_step=5700, grad_norm=2.806553363800049, loss=1.880196452140808
I0212 02:32:26.927883 139647833720576 logging_writer.py:48] [5800] global_step=5800, grad_norm=3.9215598106384277, loss=1.8550444841384888
I0212 02:33:56.748901 139647825327872 logging_writer.py:48] [5900] global_step=5900, grad_norm=3.001173496246338, loss=1.8347551822662354
I0212 02:35:25.664211 139647833720576 logging_writer.py:48] [6000] global_step=6000, grad_norm=3.190516471862793, loss=1.7252870798110962
I0212 02:36:53.451495 139647825327872 logging_writer.py:48] [6100] global_step=6100, grad_norm=2.76828932762146, loss=1.8312013149261475
I0212 02:38:23.149591 139647833720576 logging_writer.py:48] [6200] global_step=6200, grad_norm=2.178821086883545, loss=1.7839552164077759
I0212 02:39:39.373217 139647825327872 logging_writer.py:48] [6300] global_step=6300, grad_norm=2.5760679244995117, loss=1.8208558559417725
I0212 02:40:56.576873 139647833720576 logging_writer.py:48] [6400] global_step=6400, grad_norm=3.4716739654541016, loss=1.7608877420425415
I0212 02:42:15.072689 139647825327872 logging_writer.py:48] [6500] global_step=6500, grad_norm=3.5277698040008545, loss=1.7978003025054932
I0212 02:43:38.925286 139647833720576 logging_writer.py:48] [6600] global_step=6600, grad_norm=3.2313411235809326, loss=1.8498327732086182
I0212 02:45:07.393674 139647825327872 logging_writer.py:48] [6700] global_step=6700, grad_norm=2.4181058406829834, loss=1.7903351783752441
I0212 02:46:38.351167 139647833720576 logging_writer.py:48] [6800] global_step=6800, grad_norm=2.3457915782928467, loss=1.7840516567230225
I0212 02:46:51.479845 139803787056960 spec.py:321] Evaluating on the training split.
I0212 02:47:47.367059 139803787056960 spec.py:333] Evaluating on the validation split.
I0212 02:48:40.760220 139803787056960 spec.py:349] Evaluating on the test split.
I0212 02:49:07.353063 139803787056960 submission_runner.py:408] Time since start: 6509.53s, 	Step: 6817, 	{'train/ctc_loss': Array(0.5106037, dtype=float32), 'train/wer': 0.170897716019303, 'validation/ctc_loss': Array(0.83905643, dtype=float32), 'validation/wer': 0.24012087625631173, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5300292, dtype=float32), 'test/wer': 0.17266873844778907, 'test/num_examples': 2472, 'score': 5777.360721826553, 'total_duration': 6509.532005548477, 'accumulated_submission_time': 5777.360721826553, 'accumulated_eval_time': 731.6756029129028, 'accumulated_logging_time': 0.19184613227844238}
I0212 02:49:07.387677 139647833720576 logging_writer.py:48] [6817] accumulated_eval_time=731.675603, accumulated_logging_time=0.191846, accumulated_submission_time=5777.360722, global_step=6817, preemption_count=0, score=5777.360722, test/ctc_loss=0.5300291776657104, test/num_examples=2472, test/wer=0.172669, total_duration=6509.532006, train/ctc_loss=0.5106037259101868, train/wer=0.170898, validation/ctc_loss=0.8390564322471619, validation/num_examples=5348, validation/wer=0.240121
I0212 02:50:11.447449 139647825327872 logging_writer.py:48] [6900] global_step=6900, grad_norm=2.6102607250213623, loss=1.7516919374465942
I0212 02:51:26.408769 139647833720576 logging_writer.py:48] [7000] global_step=7000, grad_norm=2.8454537391662598, loss=1.7662850618362427
I0212 02:52:49.790842 139647825327872 logging_writer.py:48] [7100] global_step=7100, grad_norm=5.261972427368164, loss=1.7433992624282837
I0212 02:54:17.314889 139647833720576 logging_writer.py:48] [7200] global_step=7200, grad_norm=2.768723249435425, loss=1.7632251977920532
I0212 02:55:39.542168 139647833720576 logging_writer.py:48] [7300] global_step=7300, grad_norm=4.296779632568359, loss=1.7860546112060547
I0212 02:56:59.378062 139647825327872 logging_writer.py:48] [7400] global_step=7400, grad_norm=2.844169855117798, loss=1.7351593971252441
I0212 02:58:17.283675 139647833720576 logging_writer.py:48] [7500] global_step=7500, grad_norm=2.289072036743164, loss=1.712589979171753
I0212 02:59:35.453367 139647825327872 logging_writer.py:48] [7600] global_step=7600, grad_norm=2.2775516510009766, loss=1.7954636812210083
I0212 03:01:05.311122 139647833720576 logging_writer.py:48] [7700] global_step=7700, grad_norm=3.092625856399536, loss=1.7017146348953247
I0212 03:02:32.483823 139647825327872 logging_writer.py:48] [7800] global_step=7800, grad_norm=2.3983020782470703, loss=1.7341527938842773
I0212 03:03:57.889504 139647833720576 logging_writer.py:48] [7900] global_step=7900, grad_norm=4.367555141448975, loss=1.7271591424942017
I0212 03:05:29.672879 139647825327872 logging_writer.py:48] [8000] global_step=8000, grad_norm=3.206181287765503, loss=1.7053672075271606
I0212 03:06:58.549378 139647833720576 logging_writer.py:48] [8100] global_step=8100, grad_norm=3.569972515106201, loss=1.7101536989212036
I0212 03:08:27.478567 139647825327872 logging_writer.py:48] [8200] global_step=8200, grad_norm=2.735633373260498, loss=1.7725752592086792
I0212 03:09:53.783560 139647833720576 logging_writer.py:48] [8300] global_step=8300, grad_norm=2.1066572666168213, loss=1.6333147287368774
I0212 03:11:10.041634 139647825327872 logging_writer.py:48] [8400] global_step=8400, grad_norm=2.4869983196258545, loss=1.6509896516799927
I0212 03:12:27.327143 139647833720576 logging_writer.py:48] [8500] global_step=8500, grad_norm=2.4115242958068848, loss=1.656196117401123
I0212 03:13:08.043995 139803787056960 spec.py:321] Evaluating on the training split.
I0212 03:14:06.623655 139803787056960 spec.py:333] Evaluating on the validation split.
I0212 03:14:59.112733 139803787056960 spec.py:349] Evaluating on the test split.
I0212 03:15:25.681867 139803787056960 submission_runner.py:408] Time since start: 8087.86s, 	Step: 8552, 	{'train/ctc_loss': Array(0.4068033, dtype=float32), 'train/wer': 0.14157673023287692, 'validation/ctc_loss': Array(0.7618631, dtype=float32), 'validation/wer': 0.22024194560568466, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4711684, dtype=float32), 'test/wer': 0.15463205573497452, 'test/num_examples': 2472, 'score': 7217.927873134613, 'total_duration': 8087.86146068573, 'accumulated_submission_time': 7217.927873134613, 'accumulated_eval_time': 869.3080587387085, 'accumulated_logging_time': 0.24324870109558105}
I0212 03:15:25.721782 139647833720576 logging_writer.py:48] [8552] accumulated_eval_time=869.308059, accumulated_logging_time=0.243249, accumulated_submission_time=7217.927873, global_step=8552, preemption_count=0, score=7217.927873, test/ctc_loss=0.4711683988571167, test/num_examples=2472, test/wer=0.154632, total_duration=8087.861461, train/ctc_loss=0.40680330991744995, train/wer=0.141577, validation/ctc_loss=0.761863112449646, validation/num_examples=5348, validation/wer=0.220242
I0212 03:16:02.516414 139647825327872 logging_writer.py:48] [8600] global_step=8600, grad_norm=2.685936450958252, loss=1.7026376724243164
I0212 03:17:17.982590 139647833720576 logging_writer.py:48] [8700] global_step=8700, grad_norm=2.2990596294403076, loss=1.636132836341858
I0212 03:18:35.911622 139647825327872 logging_writer.py:48] [8800] global_step=8800, grad_norm=2.920847177505493, loss=1.6957899332046509
I0212 03:20:07.465412 139647833720576 logging_writer.py:48] [8900] global_step=8900, grad_norm=4.814645290374756, loss=1.6655020713806152
I0212 03:21:38.684639 139647825327872 logging_writer.py:48] [9000] global_step=9000, grad_norm=2.7251412868499756, loss=1.6568018198013306
I0212 03:23:09.769898 139647833720576 logging_writer.py:48] [9100] global_step=9100, grad_norm=2.890577793121338, loss=1.768274188041687
I0212 03:24:40.391523 139647825327872 logging_writer.py:48] [9200] global_step=9200, grad_norm=3.5105369091033936, loss=1.639914870262146
I0212 03:26:10.818973 139647833720576 logging_writer.py:48] [9300] global_step=9300, grad_norm=2.6902143955230713, loss=1.6875182390213013
I0212 03:27:28.582723 139647825327872 logging_writer.py:48] [9400] global_step=9400, grad_norm=3.5838353633880615, loss=1.6445671319961548
I0212 03:28:46.450214 139647833720576 logging_writer.py:48] [9500] global_step=9500, grad_norm=3.4641427993774414, loss=1.7067694664001465
I0212 03:30:07.680958 139647825327872 logging_writer.py:48] [9600] global_step=9600, grad_norm=4.986451148986816, loss=1.7197908163070679
I0212 03:31:30.386426 139647833720576 logging_writer.py:48] [9700] global_step=9700, grad_norm=2.0445396900177, loss=1.6423265933990479
I0212 03:32:57.683275 139647825327872 logging_writer.py:48] [9800] global_step=9800, grad_norm=3.4016642570495605, loss=1.6519204378128052
I0212 03:34:26.805267 139647833720576 logging_writer.py:48] [9900] global_step=9900, grad_norm=3.4604809284210205, loss=1.644081950187683
I0212 03:35:56.962074 139647825327872 logging_writer.py:48] [10000] global_step=10000, grad_norm=2.4597880840301514, loss=1.6189435720443726
I0212 03:37:27.586169 139647833720576 logging_writer.py:48] [10100] global_step=10100, grad_norm=3.24198579788208, loss=1.6257004737854004
I0212 03:38:56.527814 139647825327872 logging_writer.py:48] [10200] global_step=10200, grad_norm=3.648413896560669, loss=1.6230436563491821
I0212 03:39:26.268797 139803787056960 spec.py:321] Evaluating on the training split.
I0212 03:40:23.479229 139803787056960 spec.py:333] Evaluating on the validation split.
I0212 03:41:15.902256 139803787056960 spec.py:349] Evaluating on the test split.
I0212 03:41:43.126069 139803787056960 submission_runner.py:408] Time since start: 9665.30s, 	Step: 10235, 	{'train/ctc_loss': Array(0.38560042, dtype=float32), 'train/wer': 0.13040989764057173, 'validation/ctc_loss': Array(0.7276246, dtype=float32), 'validation/wer': 0.2111665717292449, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4391571, dtype=float32), 'test/wer': 0.14025145735583858, 'test/num_examples': 2472, 'score': 8658.387535095215, 'total_duration': 9665.303247451782, 'accumulated_submission_time': 8658.387535095215, 'accumulated_eval_time': 1006.1574850082397, 'accumulated_logging_time': 0.299619197845459}
I0212 03:41:43.163426 139647833720576 logging_writer.py:48] [10235] accumulated_eval_time=1006.157485, accumulated_logging_time=0.299619, accumulated_submission_time=8658.387535, global_step=10235, preemption_count=0, score=8658.387535, test/ctc_loss=0.439157098531723, test/num_examples=2472, test/wer=0.140251, total_duration=9665.303247, train/ctc_loss=0.38560041785240173, train/wer=0.130410, validation/ctc_loss=0.7276245951652527, validation/num_examples=5348, validation/wer=0.211167
I0212 03:42:37.020601 139647833720576 logging_writer.py:48] [10300] global_step=10300, grad_norm=3.2554287910461426, loss=1.5719263553619385
I0212 03:43:54.883843 139647825327872 logging_writer.py:48] [10400] global_step=10400, grad_norm=3.287479877471924, loss=1.6792758703231812
I0212 03:45:11.503120 139647833720576 logging_writer.py:48] [10500] global_step=10500, grad_norm=2.662562370300293, loss=1.579426646232605
I0212 03:46:34.400494 139647825327872 logging_writer.py:48] [10600] global_step=10600, grad_norm=3.290133476257324, loss=1.6414470672607422
I0212 03:47:59.480988 139647833720576 logging_writer.py:48] [10700] global_step=10700, grad_norm=4.313396453857422, loss=1.641101598739624
I0212 03:49:26.246870 139647825327872 logging_writer.py:48] [10800] global_step=10800, grad_norm=5.424245834350586, loss=1.6143383979797363
I0212 03:50:55.672441 139647833720576 logging_writer.py:48] [10900] global_step=10900, grad_norm=3.2924516201019287, loss=1.6604410409927368
I0212 03:52:26.994360 139647825327872 logging_writer.py:48] [11000] global_step=11000, grad_norm=2.35922908782959, loss=1.6371530294418335
I0212 03:53:59.255737 139647833720576 logging_writer.py:48] [11100] global_step=11100, grad_norm=2.2032406330108643, loss=1.6281614303588867
I0212 03:55:29.346067 139647825327872 logging_writer.py:48] [11200] global_step=11200, grad_norm=2.725094795227051, loss=1.6268056631088257
I0212 03:56:57.928008 139647833720576 logging_writer.py:48] [11300] global_step=11300, grad_norm=2.645819902420044, loss=1.670686960220337
I0212 03:58:20.091752 139647833720576 logging_writer.py:48] [11400] global_step=11400, grad_norm=1.9705300331115723, loss=1.5573186874389648
I0212 03:59:38.235920 139647825327872 logging_writer.py:48] [11500] global_step=11500, grad_norm=3.4559168815612793, loss=1.5875298976898193
I0212 04:00:55.274852 139647833720576 logging_writer.py:48] [11600] global_step=11600, grad_norm=2.787170648574829, loss=1.555309772491455
I0212 04:02:17.920785 139647825327872 logging_writer.py:48] [11700] global_step=11700, grad_norm=3.555402994155884, loss=1.626357913017273
I0212 04:03:44.966755 139647833720576 logging_writer.py:48] [11800] global_step=11800, grad_norm=4.154740333557129, loss=1.6428264379501343
I0212 04:05:15.758073 139647825327872 logging_writer.py:48] [11900] global_step=11900, grad_norm=4.189207077026367, loss=1.597528338432312
I0212 04:05:43.481023 139803787056960 spec.py:321] Evaluating on the training split.
I0212 04:06:38.982344 139803787056960 spec.py:333] Evaluating on the validation split.
I0212 04:07:31.648569 139803787056960 spec.py:349] Evaluating on the test split.
I0212 04:07:58.621546 139803787056960 submission_runner.py:408] Time since start: 11240.80s, 	Step: 11932, 	{'train/ctc_loss': Array(0.4022625, dtype=float32), 'train/wer': 0.13475438216836183, 'validation/ctc_loss': Array(0.71573716, dtype=float32), 'validation/wer': 0.20575996601562122, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.42614245, dtype=float32), 'test/wer': 0.1368797351370016, 'test/num_examples': 2472, 'score': 10098.617139339447, 'total_duration': 11240.799752235413, 'accumulated_submission_time': 10098.617139339447, 'accumulated_eval_time': 1141.2911870479584, 'accumulated_logging_time': 0.3535733222961426}
I0212 04:07:58.661140 139647833720576 logging_writer.py:48] [11932] accumulated_eval_time=1141.291187, accumulated_logging_time=0.353573, accumulated_submission_time=10098.617139, global_step=11932, preemption_count=0, score=10098.617139, test/ctc_loss=0.42614245414733887, test/num_examples=2472, test/wer=0.136880, total_duration=11240.799752, train/ctc_loss=0.40226250886917114, train/wer=0.134754, validation/ctc_loss=0.7157371640205383, validation/num_examples=5348, validation/wer=0.205760
I0212 04:08:51.574414 139647825327872 logging_writer.py:48] [12000] global_step=12000, grad_norm=3.607084035873413, loss=1.603469729423523
I0212 04:10:07.456461 139647833720576 logging_writer.py:48] [12100] global_step=12100, grad_norm=3.1100120544433594, loss=1.621894121170044
I0212 04:11:27.877892 139647825327872 logging_writer.py:48] [12200] global_step=12200, grad_norm=2.0579073429107666, loss=1.6011440753936768
I0212 04:12:58.474666 139647833720576 logging_writer.py:48] [12300] global_step=12300, grad_norm=3.281386137008667, loss=1.6099603176116943
I0212 04:14:25.241343 139647833720576 logging_writer.py:48] [12400] global_step=12400, grad_norm=2.6425209045410156, loss=1.580072045326233
I0212 04:15:43.781204 139647825327872 logging_writer.py:48] [12500] global_step=12500, grad_norm=2.2376387119293213, loss=1.5816497802734375
I0212 04:17:04.388446 139647833720576 logging_writer.py:48] [12600] global_step=12600, grad_norm=2.4195754528045654, loss=1.6700881719589233
I0212 04:18:30.603829 139647825327872 logging_writer.py:48] [12700] global_step=12700, grad_norm=3.179032564163208, loss=1.615920901298523
I0212 04:19:56.921986 139647833720576 logging_writer.py:48] [12800] global_step=12800, grad_norm=5.2259697914123535, loss=1.5781954526901245
I0212 04:21:24.701095 139647825327872 logging_writer.py:48] [12900] global_step=12900, grad_norm=2.7492523193359375, loss=1.5540475845336914
I0212 04:22:56.008853 139647833720576 logging_writer.py:48] [13000] global_step=13000, grad_norm=3.6386544704437256, loss=1.7102571725845337
I0212 04:24:27.861806 139647825327872 logging_writer.py:48] [13100] global_step=13100, grad_norm=3.5086278915405273, loss=1.6230069398880005
I0212 04:25:56.090631 139647833720576 logging_writer.py:48] [13200] global_step=13200, grad_norm=3.454226016998291, loss=1.63175368309021
I0212 04:27:21.409361 139647825327872 logging_writer.py:48] [13300] global_step=13300, grad_norm=3.5719916820526123, loss=1.5767768621444702
I0212 04:28:52.142569 139647833720576 logging_writer.py:48] [13400] global_step=13400, grad_norm=7.4554901123046875, loss=1.5717288255691528
I0212 04:30:08.313836 139647825327872 logging_writer.py:48] [13500] global_step=13500, grad_norm=2.5879032611846924, loss=1.5876771211624146
I0212 04:31:24.908799 139647833720576 logging_writer.py:48] [13600] global_step=13600, grad_norm=3.939230442047119, loss=1.642202615737915
I0212 04:31:59.112884 139803787056960 spec.py:321] Evaluating on the training split.
I0212 04:32:54.251650 139803787056960 spec.py:333] Evaluating on the validation split.
I0212 04:33:47.946397 139803787056960 spec.py:349] Evaluating on the test split.
I0212 04:34:14.759622 139803787056960 submission_runner.py:408] Time since start: 12816.94s, 	Step: 13643, 	{'train/ctc_loss': Array(0.37538254, dtype=float32), 'train/wer': 0.12491624494064966, 'validation/ctc_loss': Array(0.7008779, dtype=float32), 'validation/wer': 0.20287322475066857, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.42028874, dtype=float32), 'test/wer': 0.13389393293116406, 'test/num_examples': 2472, 'score': 11538.98053908348, 'total_duration': 12816.938221931458, 'accumulated_submission_time': 11538.98053908348, 'accumulated_eval_time': 1276.9315330982208, 'accumulated_logging_time': 0.4093668460845947}
I0212 04:34:14.792590 139647833720576 logging_writer.py:48] [13643] accumulated_eval_time=1276.931533, accumulated_logging_time=0.409367, accumulated_submission_time=11538.980539, global_step=13643, preemption_count=0, score=11538.980539, test/ctc_loss=0.42028874158859253, test/num_examples=2472, test/wer=0.133894, total_duration=12816.938222, train/ctc_loss=0.37538254261016846, train/wer=0.124916, validation/ctc_loss=0.7008779048919678, validation/num_examples=5348, validation/wer=0.202873
I0212 04:34:59.217287 139647825327872 logging_writer.py:48] [13700] global_step=13700, grad_norm=3.3558220863342285, loss=1.5943766832351685
I0212 04:36:14.779930 139647833720576 logging_writer.py:48] [13800] global_step=13800, grad_norm=2.848196029663086, loss=1.5625300407409668
I0212 04:37:29.902520 139647825327872 logging_writer.py:48] [13900] global_step=13900, grad_norm=2.3584628105163574, loss=1.5777326822280884
I0212 04:38:59.539224 139647833720576 logging_writer.py:48] [14000] global_step=14000, grad_norm=2.2327399253845215, loss=1.6230779886245728
I0212 04:40:31.136599 139647825327872 logging_writer.py:48] [14100] global_step=14100, grad_norm=3.289933919906616, loss=1.6324206590652466
I0212 04:41:56.060573 139647833720576 logging_writer.py:48] [14200] global_step=14200, grad_norm=2.4128756523132324, loss=1.5735169649124146
I0212 04:43:22.711059 139647825327872 logging_writer.py:48] [14300] global_step=14300, grad_norm=3.444101333618164, loss=1.5013530254364014
I0212 04:44:52.282696 139647833720576 logging_writer.py:48] [14400] global_step=14400, grad_norm=2.9675326347351074, loss=1.5755095481872559
I0212 04:46:15.779067 139647833720576 logging_writer.py:48] [14500] global_step=14500, grad_norm=3.583435535430908, loss=1.62691068649292
I0212 04:47:34.011804 139647825327872 logging_writer.py:48] [14600] global_step=14600, grad_norm=3.087092161178589, loss=1.515120267868042
I0212 04:48:57.142655 139647833720576 logging_writer.py:48] [14700] global_step=14700, grad_norm=3.5483312606811523, loss=1.5522618293762207
I0212 04:50:24.582438 139647825327872 logging_writer.py:48] [14800] global_step=14800, grad_norm=2.658625841140747, loss=1.5872069597244263
I0212 04:51:50.588159 139647833720576 logging_writer.py:48] [14900] global_step=14900, grad_norm=2.5127956867218018, loss=1.6492279767990112
I0212 04:53:21.797008 139647825327872 logging_writer.py:48] [15000] global_step=15000, grad_norm=2.4043118953704834, loss=1.5420260429382324
I0212 04:54:48.025445 139647833720576 logging_writer.py:48] [15100] global_step=15100, grad_norm=3.8180034160614014, loss=1.5847182273864746
I0212 04:56:17.046956 139647825327872 logging_writer.py:48] [15200] global_step=15200, grad_norm=2.891359567642212, loss=1.5203866958618164
I0212 04:57:46.023513 139647833720576 logging_writer.py:48] [15300] global_step=15300, grad_norm=3.3806135654449463, loss=1.6098369359970093
I0212 04:58:14.996317 139803787056960 spec.py:321] Evaluating on the training split.
I0212 04:59:13.400688 139803787056960 spec.py:333] Evaluating on the validation split.
I0212 05:00:08.290710 139803787056960 spec.py:349] Evaluating on the test split.
I0212 05:00:35.745216 139803787056960 submission_runner.py:408] Time since start: 14397.92s, 	Step: 15334, 	{'train/ctc_loss': Array(0.3517334, dtype=float32), 'train/wer': 0.12161177798296911, 'validation/ctc_loss': Array(0.66360116, dtype=float32), 'validation/wer': 0.19245585409888297, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3942068, dtype=float32), 'test/wer': 0.12922226961590802, 'test/num_examples': 2472, 'score': 12979.097982645035, 'total_duration': 14397.922987937927, 'accumulated_submission_time': 12979.097982645035, 'accumulated_eval_time': 1417.6731894016266, 'accumulated_logging_time': 0.45882654190063477}
I0212 05:00:35.785208 139647833720576 logging_writer.py:48] [15334] accumulated_eval_time=1417.673189, accumulated_logging_time=0.458827, accumulated_submission_time=12979.097983, global_step=15334, preemption_count=0, score=12979.097983, test/ctc_loss=0.39420679211616516, test/num_examples=2472, test/wer=0.129222, total_duration=14397.922988, train/ctc_loss=0.35173338651657104, train/wer=0.121612, validation/ctc_loss=0.6636011600494385, validation/num_examples=5348, validation/wer=0.192456
I0212 05:01:26.578145 139647825327872 logging_writer.py:48] [15400] global_step=15400, grad_norm=2.574814796447754, loss=1.5285916328430176
I0212 05:02:47.646540 139647833720576 logging_writer.py:48] [15500] global_step=15500, grad_norm=2.886040210723877, loss=1.5736432075500488
I0212 05:04:05.911002 139647825327872 logging_writer.py:48] [15600] global_step=15600, grad_norm=2.4457643032073975, loss=1.6168651580810547
I0212 05:05:26.544241 139647833720576 logging_writer.py:48] [15700] global_step=15700, grad_norm=3.577627658843994, loss=1.5978347063064575
I0212 05:06:53.012184 139647825327872 logging_writer.py:48] [15800] global_step=15800, grad_norm=3.5870931148529053, loss=1.5903358459472656
I0212 05:08:20.622137 139647833720576 logging_writer.py:48] [15900] global_step=15900, grad_norm=2.1261043548583984, loss=1.5712275505065918
I0212 05:09:50.492576 139647825327872 logging_writer.py:48] [16000] global_step=16000, grad_norm=2.5121304988861084, loss=1.5512785911560059
I0212 05:11:20.796828 139647833720576 logging_writer.py:48] [16100] global_step=16100, grad_norm=2.778848171234131, loss=1.6344685554504395
I0212 05:12:52.251060 139647825327872 logging_writer.py:48] [16200] global_step=16200, grad_norm=3.348154067993164, loss=1.6176743507385254
I0212 05:14:18.264915 139647833720576 logging_writer.py:48] [16300] global_step=16300, grad_norm=4.374825954437256, loss=1.567828893661499
I0212 05:15:49.263669 139647825327872 logging_writer.py:48] [16400] global_step=16400, grad_norm=1.9624221324920654, loss=1.4983487129211426
I0212 05:17:19.192929 139647833720576 logging_writer.py:48] [16500] global_step=16500, grad_norm=2.7852697372436523, loss=1.5225552320480347
I0212 05:18:37.723609 139647825327872 logging_writer.py:48] [16600] global_step=16600, grad_norm=2.486396551132202, loss=1.561776876449585
I0212 05:19:59.156556 139647833720576 logging_writer.py:48] [16700] global_step=16700, grad_norm=2.9222679138183594, loss=1.5615735054016113
I0212 05:21:21.723082 139647825327872 logging_writer.py:48] [16800] global_step=16800, grad_norm=2.8495376110076904, loss=1.5544880628585815
I0212 05:22:48.774400 139647833720576 logging_writer.py:48] [16900] global_step=16900, grad_norm=2.9120352268218994, loss=1.505669355392456
I0212 05:24:17.564029 139647825327872 logging_writer.py:48] [17000] global_step=17000, grad_norm=4.966492652893066, loss=1.6415495872497559
I0212 05:24:35.838356 139803787056960 spec.py:321] Evaluating on the training split.
I0212 05:25:32.309362 139803787056960 spec.py:333] Evaluating on the validation split.
I0212 05:26:24.715511 139803787056960 spec.py:349] Evaluating on the test split.
I0212 05:26:51.926960 139803787056960 submission_runner.py:408] Time since start: 15974.11s, 	Step: 17023, 	{'train/ctc_loss': Array(0.3794418, dtype=float32), 'train/wer': 0.1212333939606667, 'validation/ctc_loss': Array(0.6458081, dtype=float32), 'validation/wer': 0.18757059965050155, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.37999377, dtype=float32), 'test/wer': 0.12337253468202222, 'test/num_examples': 2472, 'score': 14419.062114953995, 'total_duration': 15974.105421543121, 'accumulated_submission_time': 14419.062114953995, 'accumulated_eval_time': 1553.7552139759064, 'accumulated_logging_time': 0.5167844295501709}
I0212 05:26:51.962540 139647833720576 logging_writer.py:48] [17023] accumulated_eval_time=1553.755214, accumulated_logging_time=0.516784, accumulated_submission_time=14419.062115, global_step=17023, preemption_count=0, score=14419.062115, test/ctc_loss=0.3799937665462494, test/num_examples=2472, test/wer=0.123373, total_duration=15974.105422, train/ctc_loss=0.3794417977333069, train/wer=0.121233, validation/ctc_loss=0.6458081007003784, validation/num_examples=5348, validation/wer=0.187571
I0212 05:27:50.841758 139647825327872 logging_writer.py:48] [17100] global_step=17100, grad_norm=3.200728178024292, loss=1.5716394186019897
I0212 05:29:06.109786 139647833720576 logging_writer.py:48] [17200] global_step=17200, grad_norm=3.10516095161438, loss=1.533159852027893
I0212 05:30:34.278051 139647825327872 logging_writer.py:48] [17300] global_step=17300, grad_norm=3.1813108921051025, loss=1.5091290473937988
I0212 05:32:04.287693 139647833720576 logging_writer.py:48] [17400] global_step=17400, grad_norm=2.6582159996032715, loss=1.5711873769760132
I0212 05:33:34.463075 139647825327872 logging_writer.py:48] [17500] global_step=17500, grad_norm=2.7358651161193848, loss=1.5713820457458496
I0212 05:34:57.801314 139647833720576 logging_writer.py:48] [17600] global_step=17600, grad_norm=4.496094703674316, loss=1.5025172233581543
I0212 05:36:17.169932 139647825327872 logging_writer.py:48] [17700] global_step=17700, grad_norm=2.860853672027588, loss=1.5652313232421875
I0212 05:37:39.117994 139647833720576 logging_writer.py:48] [17800] global_step=17800, grad_norm=2.2889018058776855, loss=1.515376329421997
I0212 05:39:03.814712 139647825327872 logging_writer.py:48] [17900] global_step=17900, grad_norm=3.8833463191986084, loss=1.503079891204834
I0212 05:40:31.601621 139647833720576 logging_writer.py:48] [18000] global_step=18000, grad_norm=4.034919261932373, loss=1.5254307985305786
I0212 05:42:02.263184 139647825327872 logging_writer.py:48] [18100] global_step=18100, grad_norm=1.9310429096221924, loss=1.5105969905853271
I0212 05:43:28.379313 139647833720576 logging_writer.py:48] [18200] global_step=18200, grad_norm=3.7913167476654053, loss=1.57734215259552
I0212 05:44:59.223582 139647825327872 logging_writer.py:48] [18300] global_step=18300, grad_norm=2.0327632427215576, loss=1.5613514184951782
I0212 05:46:28.060322 139647833720576 logging_writer.py:48] [18400] global_step=18400, grad_norm=4.519606113433838, loss=1.5280570983886719
I0212 05:47:58.472209 139647825327872 logging_writer.py:48] [18500] global_step=18500, grad_norm=3.3209428787231445, loss=1.5536550283432007
I0212 05:49:21.384543 139647833720576 logging_writer.py:48] [18600] global_step=18600, grad_norm=3.381678581237793, loss=1.5243679285049438
I0212 05:50:40.041354 139647825327872 logging_writer.py:48] [18700] global_step=18700, grad_norm=2.92278790473938, loss=1.4852651357650757
I0212 05:50:52.032267 139803787056960 spec.py:321] Evaluating on the training split.
I0212 05:51:48.935238 139803787056960 spec.py:333] Evaluating on the validation split.
I0212 05:52:41.022908 139803787056960 spec.py:349] Evaluating on the test split.
I0212 05:53:07.881287 139803787056960 submission_runner.py:408] Time since start: 17550.06s, 	Step: 18717, 	{'train/ctc_loss': Array(0.28394952, dtype=float32), 'train/wer': 0.0967809750394815, 'validation/ctc_loss': Array(0.6158726, dtype=float32), 'validation/wer': 0.17783870936597893, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.36024228, dtype=float32), 'test/wer': 0.11589787337761258, 'test/num_examples': 2472, 'score': 15859.043215751648, 'total_duration': 17550.060350894928, 'accumulated_submission_time': 15859.043215751648, 'accumulated_eval_time': 1689.598258972168, 'accumulated_logging_time': 0.5695686340332031}
I0212 05:53:07.918365 139647833720576 logging_writer.py:48] [18717] accumulated_eval_time=1689.598259, accumulated_logging_time=0.569569, accumulated_submission_time=15859.043216, global_step=18717, preemption_count=0, score=15859.043216, test/ctc_loss=0.3602422773838043, test/num_examples=2472, test/wer=0.115898, total_duration=17550.060351, train/ctc_loss=0.28394952416419983, train/wer=0.096781, validation/ctc_loss=0.6158726215362549, validation/num_examples=5348, validation/wer=0.177839
I0212 05:54:12.267623 139647825327872 logging_writer.py:48] [18800] global_step=18800, grad_norm=4.013697624206543, loss=1.4579708576202393
I0212 05:55:27.889571 139647833720576 logging_writer.py:48] [18900] global_step=18900, grad_norm=2.0114691257476807, loss=1.5463156700134277
I0212 05:56:43.750505 139647825327872 logging_writer.py:48] [19000] global_step=19000, grad_norm=3.4977591037750244, loss=1.4890525341033936
I0212 05:58:09.665362 139647833720576 logging_writer.py:48] [19100] global_step=19100, grad_norm=3.726818799972534, loss=1.5415407419204712
I0212 05:59:38.519453 139647825327872 logging_writer.py:48] [19200] global_step=19200, grad_norm=2.4446237087249756, loss=1.430783748626709
I0212 06:01:09.100133 139647833720576 logging_writer.py:48] [19300] global_step=19300, grad_norm=3.089019775390625, loss=1.4674814939498901
I0212 06:02:40.961082 139647825327872 logging_writer.py:48] [19400] global_step=19400, grad_norm=2.751838445663452, loss=1.5427392721176147
I0212 06:04:09.159595 139647833720576 logging_writer.py:48] [19500] global_step=19500, grad_norm=2.5526199340820312, loss=1.439784288406372
I0212 06:05:35.778617 139647833720576 logging_writer.py:48] [19600] global_step=19600, grad_norm=3.5718555450439453, loss=1.4643818140029907
I0212 06:06:51.791609 139647825327872 logging_writer.py:48] [19700] global_step=19700, grad_norm=2.6540870666503906, loss=1.498729944229126
I0212 06:08:09.244030 139647833720576 logging_writer.py:48] [19800] global_step=19800, grad_norm=3.6067006587982178, loss=1.5335667133331299
I0212 06:09:27.862335 139647825327872 logging_writer.py:48] [19900] global_step=19900, grad_norm=2.5045785903930664, loss=1.4403753280639648
I0212 06:10:54.818354 139647833720576 logging_writer.py:48] [20000] global_step=20000, grad_norm=2.2189877033233643, loss=1.4346243143081665
I0212 06:12:25.208682 139647825327872 logging_writer.py:48] [20100] global_step=20100, grad_norm=3.220034599304199, loss=1.589850902557373
I0212 06:13:56.291743 139647833720576 logging_writer.py:48] [20200] global_step=20200, grad_norm=2.568044900894165, loss=1.5384405851364136
I0212 06:15:24.836965 139647825327872 logging_writer.py:48] [20300] global_step=20300, grad_norm=2.483631134033203, loss=1.4963624477386475
I0212 06:16:56.471220 139647833720576 logging_writer.py:48] [20400] global_step=20400, grad_norm=2.1388144493103027, loss=1.5011651515960693
I0212 06:17:08.143939 139803787056960 spec.py:321] Evaluating on the training split.
I0212 06:18:04.973879 139803787056960 spec.py:333] Evaluating on the validation split.
I0212 06:18:57.117151 139803787056960 spec.py:349] Evaluating on the test split.
I0212 06:19:23.290611 139803787056960 submission_runner.py:408] Time since start: 19125.47s, 	Step: 20414, 	{'train/ctc_loss': Array(0.3042081, dtype=float32), 'train/wer': 0.10375509051843565, 'validation/ctc_loss': Array(0.6088648, dtype=float32), 'validation/wer': 0.17660291377429352, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3576674, dtype=float32), 'test/wer': 0.11669002498324295, 'test/num_examples': 2472, 'score': 17299.182085752487, 'total_duration': 19125.46996498108, 'accumulated_submission_time': 17299.182085752487, 'accumulated_eval_time': 1824.739266872406, 'accumulated_logging_time': 0.6228508949279785}
I0212 06:19:23.324547 139647833720576 logging_writer.py:48] [20414] accumulated_eval_time=1824.739267, accumulated_logging_time=0.622851, accumulated_submission_time=17299.182086, global_step=20414, preemption_count=0, score=17299.182086, test/ctc_loss=0.35766738653182983, test/num_examples=2472, test/wer=0.116690, total_duration=19125.469965, train/ctc_loss=0.30420809984207153, train/wer=0.103755, validation/ctc_loss=0.6088647842407227, validation/num_examples=5348, validation/wer=0.176603
I0212 06:20:30.042216 139647825327872 logging_writer.py:48] [20500] global_step=20500, grad_norm=2.7832581996917725, loss=1.5903480052947998
I0212 06:21:49.076354 139647833720576 logging_writer.py:48] [20600] global_step=20600, grad_norm=3.0755271911621094, loss=1.5167957544326782
I0212 06:23:06.455279 139647825327872 logging_writer.py:48] [20700] global_step=20700, grad_norm=2.35510516166687, loss=1.4198464155197144
I0212 06:24:24.710748 139647833720576 logging_writer.py:48] [20800] global_step=20800, grad_norm=2.935667037963867, loss=1.4847965240478516
I0212 06:25:46.349280 139647825327872 logging_writer.py:48] [20900] global_step=20900, grad_norm=2.479339599609375, loss=1.5133112668991089
I0212 06:27:10.621237 139647833720576 logging_writer.py:48] [21000] global_step=21000, grad_norm=2.3158457279205322, loss=1.5026435852050781
I0212 06:28:38.149408 139647825327872 logging_writer.py:48] [21100] global_step=21100, grad_norm=2.285484552383423, loss=1.4098103046417236
I0212 06:30:04.205999 139647833720576 logging_writer.py:48] [21200] global_step=21200, grad_norm=3.015674114227295, loss=1.497635006904602
I0212 06:31:33.356574 139647825327872 logging_writer.py:48] [21300] global_step=21300, grad_norm=2.8663580417633057, loss=1.4482513666152954
I0212 06:33:06.878603 139647833720576 logging_writer.py:48] [21400] global_step=21400, grad_norm=2.254070997238159, loss=1.4769093990325928
I0212 06:34:38.782767 139647825327872 logging_writer.py:48] [21500] global_step=21500, grad_norm=2.6010043621063232, loss=1.5076950788497925
I0212 06:36:07.952831 139647833720576 logging_writer.py:48] [21600] global_step=21600, grad_norm=2.819831609725952, loss=1.4923179149627686
I0212 06:37:31.542692 139647833720576 logging_writer.py:48] [21700] global_step=21700, grad_norm=3.20790696144104, loss=1.5028375387191772
I0212 06:38:48.083570 139647825327872 logging_writer.py:48] [21800] global_step=21800, grad_norm=2.5050270557403564, loss=1.5030704736709595
I0212 06:40:08.819908 139647833720576 logging_writer.py:48] [21900] global_step=21900, grad_norm=2.2201340198516846, loss=1.513606309890747
I0212 06:41:34.751118 139647825327872 logging_writer.py:48] [22000] global_step=22000, grad_norm=2.9713187217712402, loss=1.4952869415283203
I0212 06:43:04.474376 139647833720576 logging_writer.py:48] [22100] global_step=22100, grad_norm=3.5376029014587402, loss=1.4687621593475342
I0212 06:43:23.799334 139803787056960 spec.py:321] Evaluating on the training split.
I0212 06:44:19.849476 139803787056960 spec.py:333] Evaluating on the validation split.
I0212 06:45:12.359636 139803787056960 spec.py:349] Evaluating on the test split.
I0212 06:45:40.274258 139803787056960 submission_runner.py:408] Time since start: 20702.45s, 	Step: 22123, 	{'train/ctc_loss': Array(0.3945768, dtype=float32), 'train/wer': 0.13109240002638117, 'validation/ctc_loss': Array(0.5891843, dtype=float32), 'validation/wer': 0.1709356324280487, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.34208703, dtype=float32), 'test/wer': 0.11181524587167144, 'test/num_examples': 2472, 'score': 18739.56921505928, 'total_duration': 20702.452353477478, 'accumulated_submission_time': 18739.56921505928, 'accumulated_eval_time': 1961.207276582718, 'accumulated_logging_time': 0.6727504730224609}
I0212 06:45:40.310897 139647833720576 logging_writer.py:48] [22123] accumulated_eval_time=1961.207277, accumulated_logging_time=0.672750, accumulated_submission_time=18739.569215, global_step=22123, preemption_count=0, score=18739.569215, test/ctc_loss=0.3420870304107666, test/num_examples=2472, test/wer=0.111815, total_duration=20702.452353, train/ctc_loss=0.3945767879486084, train/wer=0.131092, validation/ctc_loss=0.5891842842102051, validation/num_examples=5348, validation/wer=0.170936
I0212 06:46:39.959583 139647825327872 logging_writer.py:48] [22200] global_step=22200, grad_norm=4.030900001525879, loss=1.5259932279586792
I0212 06:47:55.413098 139647833720576 logging_writer.py:48] [22300] global_step=22300, grad_norm=3.8591907024383545, loss=1.453776240348816
I0212 06:49:19.263076 139647825327872 logging_writer.py:48] [22400] global_step=22400, grad_norm=3.8141071796417236, loss=1.4368499517440796
I0212 06:50:47.912251 139647833720576 logging_writer.py:48] [22500] global_step=22500, grad_norm=3.0850775241851807, loss=1.4597009420394897
I0212 06:52:17.507435 139647825327872 logging_writer.py:48] [22600] global_step=22600, grad_norm=2.964036464691162, loss=1.5009164810180664
I0212 06:53:45.939858 139647833720576 logging_writer.py:48] [22700] global_step=22700, grad_norm=1.636615514755249, loss=1.4247797727584839
I0212 06:55:02.032539 139647825327872 logging_writer.py:48] [22800] global_step=22800, grad_norm=2.589238405227661, loss=1.4977370500564575
I0212 06:56:18.547314 139647833720576 logging_writer.py:48] [22900] global_step=22900, grad_norm=2.7621846199035645, loss=1.4617239236831665
I0212 06:57:41.847155 139647825327872 logging_writer.py:48] [23000] global_step=23000, grad_norm=2.3581318855285645, loss=1.4136167764663696
I0212 06:59:10.070129 139647833720576 logging_writer.py:48] [23100] global_step=23100, grad_norm=1.8444911241531372, loss=1.4099451303482056
I0212 07:00:38.726893 139647825327872 logging_writer.py:48] [23200] global_step=23200, grad_norm=2.052072048187256, loss=1.4964197874069214
I0212 07:02:09.759164 139647833720576 logging_writer.py:48] [23300] global_step=23300, grad_norm=2.5423777103424072, loss=1.429192066192627
I0212 07:03:38.025754 139647825327872 logging_writer.py:48] [23400] global_step=23400, grad_norm=2.0757923126220703, loss=1.4873710870742798
I0212 07:05:07.902155 139647833720576 logging_writer.py:48] [23500] global_step=23500, grad_norm=2.5759499073028564, loss=1.4677619934082031
I0212 07:06:37.762825 139647825327872 logging_writer.py:48] [23600] global_step=23600, grad_norm=3.244927167892456, loss=1.460274577140808
I0212 07:08:08.278154 139647833720576 logging_writer.py:48] [23700] global_step=23700, grad_norm=2.890079975128174, loss=1.383805513381958
I0212 07:09:25.865772 139647825327872 logging_writer.py:48] [23800] global_step=23800, grad_norm=2.983729600906372, loss=1.4670400619506836
I0212 07:09:40.742850 139803787056960 spec.py:321] Evaluating on the training split.
I0212 07:10:37.404740 139803787056960 spec.py:333] Evaluating on the validation split.
I0212 07:11:30.749551 139803787056960 spec.py:349] Evaluating on the test split.
I0212 07:11:57.987729 139803787056960 submission_runner.py:408] Time since start: 22280.17s, 	Step: 23821, 	{'train/ctc_loss': Array(0.4185183, dtype=float32), 'train/wer': 0.13682009992292293, 'validation/ctc_loss': Array(0.5801254, dtype=float32), 'validation/wer': 0.1673344468366529, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33332494, dtype=float32), 'test/wer': 0.10553896776552313, 'test/num_examples': 2472, 'score': 20179.91077518463, 'total_duration': 22280.166800022125, 'accumulated_submission_time': 20179.91077518463, 'accumulated_eval_time': 2098.446216583252, 'accumulated_logging_time': 0.7275550365447998}
I0212 07:11:58.022472 139647833720576 logging_writer.py:48] [23821] accumulated_eval_time=2098.446217, accumulated_logging_time=0.727555, accumulated_submission_time=20179.910775, global_step=23821, preemption_count=0, score=20179.910775, test/ctc_loss=0.33332493901252747, test/num_examples=2472, test/wer=0.105539, total_duration=22280.166800, train/ctc_loss=0.4185183048248291, train/wer=0.136820, validation/ctc_loss=0.5801253914833069, validation/num_examples=5348, validation/wer=0.167334
I0212 07:12:58.202499 139647825327872 logging_writer.py:48] [23900] global_step=23900, grad_norm=2.599670886993408, loss=1.5286673307418823
I0212 07:14:13.812692 139647833720576 logging_writer.py:48] [24000] global_step=24000, grad_norm=3.8309884071350098, loss=1.459638237953186
I0212 07:15:29.161939 139647825327872 logging_writer.py:48] [24100] global_step=24100, grad_norm=2.446838140487671, loss=1.4424480199813843
I0212 07:16:56.583385 139647833720576 logging_writer.py:48] [24200] global_step=24200, grad_norm=2.1706039905548096, loss=1.4375044107437134
I0212 07:18:26.709063 139647825327872 logging_writer.py:48] [24300] global_step=24300, grad_norm=3.278149366378784, loss=1.3543411493301392
I0212 07:19:55.733628 139647833720576 logging_writer.py:48] [24400] global_step=24400, grad_norm=3.4402759075164795, loss=1.394586205482483
I0212 07:21:26.409201 139647825327872 logging_writer.py:48] [24500] global_step=24500, grad_norm=2.7063679695129395, loss=1.3539214134216309
I0212 07:22:53.598893 139647833720576 logging_writer.py:48] [24600] global_step=24600, grad_norm=2.712310552597046, loss=1.4243016242980957
I0212 07:24:19.603690 139647825327872 logging_writer.py:48] [24700] global_step=24700, grad_norm=2.4728238582611084, loss=1.3953921794891357
I0212 07:25:42.890405 139647833720576 logging_writer.py:48] [24800] global_step=24800, grad_norm=3.270570755004883, loss=1.3793658018112183
I0212 07:27:03.490875 139647825327872 logging_writer.py:48] [24900] global_step=24900, grad_norm=2.5744683742523193, loss=1.3221765756607056
I0212 07:28:25.073887 139647833720576 logging_writer.py:48] [25000] global_step=25000, grad_norm=3.4272191524505615, loss=1.441596508026123
I0212 07:29:44.202362 139647825327872 logging_writer.py:48] [25100] global_step=25100, grad_norm=2.445753812789917, loss=1.4213165044784546
I0212 07:31:13.093353 139647833720576 logging_writer.py:48] [25200] global_step=25200, grad_norm=2.812669277191162, loss=1.4109352827072144
I0212 07:32:40.805567 139647825327872 logging_writer.py:48] [25300] global_step=25300, grad_norm=3.4549272060394287, loss=1.4449838399887085
I0212 07:34:08.349277 139647833720576 logging_writer.py:48] [25400] global_step=25400, grad_norm=2.5021204948425293, loss=1.450722575187683
I0212 07:35:39.458951 139647825327872 logging_writer.py:48] [25500] global_step=25500, grad_norm=3.3147988319396973, loss=1.5130919218063354
I0212 07:35:58.988338 139803787056960 spec.py:321] Evaluating on the training split.
I0212 07:36:52.299270 139803787056960 spec.py:333] Evaluating on the validation split.
I0212 07:37:43.907030 139803787056960 spec.py:349] Evaluating on the test split.
I0212 07:38:10.011538 139803787056960 submission_runner.py:408] Time since start: 23852.19s, 	Step: 25522, 	{'train/ctc_loss': Array(0.45643923, dtype=float32), 'train/wer': 0.14958464822721365, 'validation/ctc_loss': Array(0.560944, dtype=float32), 'validation/wer': 0.16224644467401064, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32449305, dtype=float32), 'test/wer': 0.10533585196920764, 'test/num_examples': 2472, 'score': 21620.787089586258, 'total_duration': 23852.190286397934, 'accumulated_submission_time': 21620.787089586258, 'accumulated_eval_time': 2229.4631350040436, 'accumulated_logging_time': 0.7792990207672119}
I0212 07:38:10.049597 139647833720576 logging_writer.py:48] [25522] accumulated_eval_time=2229.463135, accumulated_logging_time=0.779299, accumulated_submission_time=21620.787090, global_step=25522, preemption_count=0, score=21620.787090, test/ctc_loss=0.32449305057525635, test/num_examples=2472, test/wer=0.105336, total_duration=23852.190286, train/ctc_loss=0.45643922686576843, train/wer=0.149585, validation/ctc_loss=0.5609440207481384, validation/num_examples=5348, validation/wer=0.162246
I0212 07:39:09.600908 139647825327872 logging_writer.py:48] [25600] global_step=25600, grad_norm=2.2756872177124023, loss=1.3923591375350952
I0212 07:40:25.414927 139647833720576 logging_writer.py:48] [25700] global_step=25700, grad_norm=4.201688289642334, loss=1.405746579170227
I0212 07:41:48.104086 139647833720576 logging_writer.py:48] [25800] global_step=25800, grad_norm=2.8390426635742188, loss=1.3854880332946777
I0212 07:43:08.415314 139647825327872 logging_writer.py:48] [25900] global_step=25900, grad_norm=3.8590261936187744, loss=1.4364924430847168
I0212 07:44:26.628839 139647833720576 logging_writer.py:48] [26000] global_step=26000, grad_norm=4.21331787109375, loss=1.371483564376831
I0212 07:45:48.172391 139647825327872 logging_writer.py:48] [26100] global_step=26100, grad_norm=2.4916255474090576, loss=1.4820040464401245
I0212 07:47:15.820724 139647833720576 logging_writer.py:48] [26200] global_step=26200, grad_norm=2.859285593032837, loss=1.3505613803863525
I0212 07:48:45.916408 139647825327872 logging_writer.py:48] [26300] global_step=26300, grad_norm=3.3768301010131836, loss=1.3899341821670532
I0212 07:50:18.395366 139647833720576 logging_writer.py:48] [26400] global_step=26400, grad_norm=2.9098000526428223, loss=1.348545789718628
I0212 07:51:50.743118 139647825327872 logging_writer.py:48] [26500] global_step=26500, grad_norm=3.1427853107452393, loss=1.4352033138275146
I0212 07:53:22.256081 139647833720576 logging_writer.py:48] [26600] global_step=26600, grad_norm=2.7356081008911133, loss=1.4315860271453857
I0212 07:54:49.047433 139647825327872 logging_writer.py:48] [26700] global_step=26700, grad_norm=3.0756168365478516, loss=1.4031208753585815
I0212 07:56:19.261258 139647833720576 logging_writer.py:48] [26800] global_step=26800, grad_norm=2.8738746643066406, loss=1.3954992294311523
I0212 07:57:36.160260 139647825327872 logging_writer.py:48] [26900] global_step=26900, grad_norm=2.7423577308654785, loss=1.4254456758499146
I0212 07:58:56.472094 139647833720576 logging_writer.py:48] [27000] global_step=27000, grad_norm=2.097662925720215, loss=1.326249122619629
I0212 08:00:17.980508 139647825327872 logging_writer.py:48] [27100] global_step=27100, grad_norm=3.0472192764282227, loss=1.3531198501586914
I0212 08:01:40.911884 139647833720576 logging_writer.py:48] [27200] global_step=27200, grad_norm=3.0380795001983643, loss=1.3442867994308472
I0212 08:02:10.156996 139803787056960 spec.py:321] Evaluating on the training split.
I0212 08:03:02.514521 139803787056960 spec.py:333] Evaluating on the validation split.
I0212 08:03:54.576401 139803787056960 spec.py:349] Evaluating on the test split.
I0212 08:04:21.488360 139803787056960 submission_runner.py:408] Time since start: 25423.67s, 	Step: 27235, 	{'train/ctc_loss': Array(0.39219317, dtype=float32), 'train/wer': 0.12764995891536565, 'validation/ctc_loss': Array(0.5473959, dtype=float32), 'validation/wer': 0.15966865230697935, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3108912, dtype=float32), 'test/wer': 0.09948611703532184, 'test/num_examples': 2472, 'score': 23060.805052042007, 'total_duration': 25423.667890787125, 'accumulated_submission_time': 23060.805052042007, 'accumulated_eval_time': 2360.7889981269836, 'accumulated_logging_time': 0.8352978229522705}
I0212 08:04:21.525592 139647833720576 logging_writer.py:48] [27235] accumulated_eval_time=2360.788998, accumulated_logging_time=0.835298, accumulated_submission_time=23060.805052, global_step=27235, preemption_count=0, score=23060.805052, test/ctc_loss=0.31089121103286743, test/num_examples=2472, test/wer=0.099486, total_duration=25423.667891, train/ctc_loss=0.39219316840171814, train/wer=0.127650, validation/ctc_loss=0.5473958849906921, validation/num_examples=5348, validation/wer=0.159669
I0212 08:05:11.725066 139647825327872 logging_writer.py:48] [27300] global_step=27300, grad_norm=2.8390655517578125, loss=1.4160407781600952
I0212 08:06:27.397699 139647833720576 logging_writer.py:48] [27400] global_step=27400, grad_norm=2.6303656101226807, loss=1.4588265419006348
I0212 08:07:53.254559 139647825327872 logging_writer.py:48] [27500] global_step=27500, grad_norm=2.96248197555542, loss=1.3679293394088745
I0212 08:09:22.197221 139647833720576 logging_writer.py:48] [27600] global_step=27600, grad_norm=2.67091965675354, loss=1.3799458742141724
I0212 08:10:52.464415 139647825327872 logging_writer.py:48] [27700] global_step=27700, grad_norm=2.2321321964263916, loss=1.41690194606781
I0212 08:12:21.101171 139647833720576 logging_writer.py:48] [27800] global_step=27800, grad_norm=1.9099198579788208, loss=1.3739486932754517
I0212 08:13:40.683371 139647833720576 logging_writer.py:48] [27900] global_step=27900, grad_norm=3.363294839859009, loss=1.3342324495315552
I0212 08:14:58.581503 139647825327872 logging_writer.py:48] [28000] global_step=28000, grad_norm=3.1719162464141846, loss=1.3241345882415771
I0212 08:16:18.894002 139647833720576 logging_writer.py:48] [28100] global_step=28100, grad_norm=3.342794895172119, loss=1.3958722352981567
I0212 08:17:42.998161 139647825327872 logging_writer.py:48] [28200] global_step=28200, grad_norm=2.790116310119629, loss=1.3436698913574219
I0212 08:19:09.258052 139647833720576 logging_writer.py:48] [28300] global_step=28300, grad_norm=2.175966739654541, loss=1.3590881824493408
I0212 08:20:38.225787 139647825327872 logging_writer.py:48] [28400] global_step=28400, grad_norm=3.578810453414917, loss=1.3351750373840332
I0212 08:22:04.770092 139647833720576 logging_writer.py:48] [28500] global_step=28500, grad_norm=2.5242021083831787, loss=1.3943811655044556
I0212 08:23:33.428706 139647825327872 logging_writer.py:48] [28600] global_step=28600, grad_norm=2.2159578800201416, loss=1.3988803625106812
I0212 08:25:02.061323 139647833720576 logging_writer.py:48] [28700] global_step=28700, grad_norm=3.418538808822632, loss=1.3823672533035278
I0212 08:26:33.813588 139647825327872 logging_writer.py:48] [28800] global_step=28800, grad_norm=2.040938377380371, loss=1.3644752502441406
I0212 08:27:59.042610 139647833720576 logging_writer.py:48] [28900] global_step=28900, grad_norm=2.633244514465332, loss=1.3324072360992432
I0212 08:28:21.534945 139803787056960 spec.py:321] Evaluating on the training split.
I0212 08:29:16.003395 139803787056960 spec.py:333] Evaluating on the validation split.
I0212 08:30:09.928854 139803787056960 spec.py:349] Evaluating on the test split.
I0212 08:30:37.571948 139803787056960 submission_runner.py:408] Time since start: 26999.75s, 	Step: 28931, 	{'train/ctc_loss': Array(0.35523257, dtype=float32), 'train/wer': 0.11911170550520063, 'validation/ctc_loss': Array(0.526833, dtype=float32), 'validation/wer': 0.15387586047095397, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2973004, dtype=float32), 'test/wer': 0.0954034895293807, 'test/num_examples': 2472, 'score': 24500.7278380394, 'total_duration': 26999.747796297073, 'accumulated_submission_time': 24500.7278380394, 'accumulated_eval_time': 2496.8168222904205, 'accumulated_logging_time': 0.8881025314331055}
I0212 08:30:37.609323 139647833720576 logging_writer.py:48] [28931] accumulated_eval_time=2496.816822, accumulated_logging_time=0.888103, accumulated_submission_time=24500.727838, global_step=28931, preemption_count=0, score=24500.727838, test/ctc_loss=0.29730039834976196, test/num_examples=2472, test/wer=0.095403, total_duration=26999.747796, train/ctc_loss=0.3552325665950775, train/wer=0.119112, validation/ctc_loss=0.5268329977989197, validation/num_examples=5348, validation/wer=0.153876
I0212 08:31:30.611138 139647825327872 logging_writer.py:48] [29000] global_step=29000, grad_norm=2.4205362796783447, loss=1.402751088142395
I0212 08:32:45.548591 139647833720576 logging_writer.py:48] [29100] global_step=29100, grad_norm=2.5694894790649414, loss=1.381419062614441
I0212 08:34:00.829195 139647825327872 logging_writer.py:48] [29200] global_step=29200, grad_norm=3.8287439346313477, loss=1.338410496711731
I0212 08:35:25.700146 139647833720576 logging_writer.py:48] [29300] global_step=29300, grad_norm=2.803403854370117, loss=1.38534414768219
I0212 08:36:55.845983 139647825327872 logging_writer.py:48] [29400] global_step=29400, grad_norm=3.0584840774536133, loss=1.3772306442260742
I0212 08:38:26.039652 139647833720576 logging_writer.py:48] [29500] global_step=29500, grad_norm=3.938005208969116, loss=1.3318727016448975
I0212 08:39:54.234383 139647825327872 logging_writer.py:48] [29600] global_step=29600, grad_norm=2.3448290824890137, loss=1.3605095148086548
I0212 08:41:23.397721 139647833720576 logging_writer.py:48] [29700] global_step=29700, grad_norm=2.7943549156188965, loss=1.3707497119903564
I0212 08:42:51.291155 139647825327872 logging_writer.py:48] [29800] global_step=29800, grad_norm=2.4604406356811523, loss=1.377455234527588
I0212 08:44:19.252166 139647833720576 logging_writer.py:48] [29900] global_step=29900, grad_norm=3.201202630996704, loss=1.33595871925354
I0212 08:45:36.588373 139647825327872 logging_writer.py:48] [30000] global_step=30000, grad_norm=3.133950710296631, loss=1.3659050464630127
I0212 08:46:53.023877 139647833720576 logging_writer.py:48] [30100] global_step=30100, grad_norm=5.509609222412109, loss=1.3735463619232178
I0212 08:48:15.172044 139647825327872 logging_writer.py:48] [30200] global_step=30200, grad_norm=3.023379325866699, loss=1.3599765300750732
I0212 08:49:39.112076 139647833720576 logging_writer.py:48] [30300] global_step=30300, grad_norm=2.5576462745666504, loss=1.3240360021591187
I0212 08:51:04.826789 139647825327872 logging_writer.py:48] [30400] global_step=30400, grad_norm=2.545865535736084, loss=1.3268455266952515
I0212 08:52:36.338488 139647833720576 logging_writer.py:48] [30500] global_step=30500, grad_norm=4.249021053314209, loss=1.3750813007354736
I0212 08:54:08.231905 139647825327872 logging_writer.py:48] [30600] global_step=30600, grad_norm=2.405458927154541, loss=1.3116945028305054
I0212 08:54:38.124444 139803787056960 spec.py:321] Evaluating on the training split.
I0212 08:55:35.421634 139803787056960 spec.py:333] Evaluating on the validation split.
I0212 08:56:28.746570 139803787056960 spec.py:349] Evaluating on the test split.
I0212 08:56:56.476272 139803787056960 submission_runner.py:408] Time since start: 28578.66s, 	Step: 30635, 	{'train/ctc_loss': Array(0.30507174, dtype=float32), 'train/wer': 0.1035346025532191, 'validation/ctc_loss': Array(0.50986946, dtype=float32), 'validation/wer': 0.14723345916564487, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28395033, dtype=float32), 'test/wer': 0.09123961570491337, 'test/num_examples': 2472, 'score': 25941.15482234955, 'total_duration': 28578.65523004532, 'accumulated_submission_time': 25941.15482234955, 'accumulated_eval_time': 2635.162611246109, 'accumulated_logging_time': 0.941051721572876}
I0212 08:56:56.520149 139647833720576 logging_writer.py:48] [30635] accumulated_eval_time=2635.162611, accumulated_logging_time=0.941052, accumulated_submission_time=25941.154822, global_step=30635, preemption_count=0, score=25941.154822, test/ctc_loss=0.2839503288269043, test/num_examples=2472, test/wer=0.091240, total_duration=28578.655230, train/ctc_loss=0.30507174134254456, train/wer=0.103535, validation/ctc_loss=0.5098694562911987, validation/num_examples=5348, validation/wer=0.147233
I0212 08:57:46.084647 139647825327872 logging_writer.py:48] [30700] global_step=30700, grad_norm=3.7217705249786377, loss=1.2726516723632812
I0212 08:59:01.295562 139647833720576 logging_writer.py:48] [30800] global_step=30800, grad_norm=2.1488935947418213, loss=1.3196606636047363
I0212 09:00:29.674228 139647833720576 logging_writer.py:48] [30900] global_step=30900, grad_norm=2.0557010173797607, loss=1.2976999282836914
I0212 09:01:47.814715 139647825327872 logging_writer.py:48] [31000] global_step=31000, grad_norm=2.763568162918091, loss=1.3592489957809448
I0212 09:03:05.703871 139647833720576 logging_writer.py:48] [31100] global_step=31100, grad_norm=5.077847003936768, loss=1.3217484951019287
I0212 09:04:26.727247 139647825327872 logging_writer.py:48] [31200] global_step=31200, grad_norm=2.4217920303344727, loss=1.3127955198287964
I0212 09:05:54.305819 139647833720576 logging_writer.py:48] [31300] global_step=31300, grad_norm=2.5968868732452393, loss=1.2525906562805176
I0212 09:07:19.569788 139647825327872 logging_writer.py:48] [31400] global_step=31400, grad_norm=2.683711290359497, loss=1.2901190519332886
I0212 09:08:49.789818 139647833720576 logging_writer.py:48] [31500] global_step=31500, grad_norm=2.950249671936035, loss=1.3064618110656738
I0212 09:10:20.972730 139647825327872 logging_writer.py:48] [31600] global_step=31600, grad_norm=2.6338448524475098, loss=1.3108733892440796
I0212 09:11:48.378881 139647833720576 logging_writer.py:48] [31700] global_step=31700, grad_norm=2.7463440895080566, loss=1.3951197862625122
I0212 09:13:19.876296 139647825327872 logging_writer.py:48] [31800] global_step=31800, grad_norm=3.2612624168395996, loss=1.278956651687622
I0212 09:14:51.337141 139647833720576 logging_writer.py:48] [31900] global_step=31900, grad_norm=3.4314687252044678, loss=1.3628474473953247
I0212 09:16:17.103006 139647833720576 logging_writer.py:48] [32000] global_step=32000, grad_norm=2.9722354412078857, loss=1.3029755353927612
I0212 09:17:34.898084 139647825327872 logging_writer.py:48] [32100] global_step=32100, grad_norm=2.918010711669922, loss=1.403588056564331
I0212 09:18:54.282492 139647833720576 logging_writer.py:48] [32200] global_step=32200, grad_norm=2.9675118923187256, loss=1.2924927473068237
I0212 09:20:18.727693 139647825327872 logging_writer.py:48] [32300] global_step=32300, grad_norm=2.7854013442993164, loss=1.3265539407730103
I0212 09:20:56.945337 139803787056960 spec.py:321] Evaluating on the training split.
I0212 09:21:51.577028 139803787056960 spec.py:333] Evaluating on the validation split.
I0212 09:22:44.008837 139803787056960 spec.py:349] Evaluating on the test split.
I0212 09:23:10.807868 139803787056960 submission_runner.py:408] Time since start: 30152.99s, 	Step: 32348, 	{'train/ctc_loss': Array(0.3419567, dtype=float32), 'train/wer': 0.11450993651970943, 'validation/ctc_loss': Array(0.5040512, dtype=float32), 'validation/wer': 0.14678934512488293, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28116906, dtype=float32), 'test/wer': 0.08896471878618, 'test/num_examples': 2472, 'score': 27381.489032030106, 'total_duration': 30152.98643374443, 'accumulated_submission_time': 27381.489032030106, 'accumulated_eval_time': 2769.01868224144, 'accumulated_logging_time': 1.0033252239227295}
I0212 09:23:10.848191 139647833720576 logging_writer.py:48] [32348] accumulated_eval_time=2769.018682, accumulated_logging_time=1.003325, accumulated_submission_time=27381.489032, global_step=32348, preemption_count=0, score=27381.489032, test/ctc_loss=0.281169056892395, test/num_examples=2472, test/wer=0.088965, total_duration=30152.986434, train/ctc_loss=0.3419567048549652, train/wer=0.114510, validation/ctc_loss=0.5040512084960938, validation/num_examples=5348, validation/wer=0.146789
I0212 09:23:50.610863 139647825327872 logging_writer.py:48] [32400] global_step=32400, grad_norm=3.447918176651001, loss=1.319913625717163
I0212 09:25:07.118351 139647833720576 logging_writer.py:48] [32500] global_step=32500, grad_norm=2.2996718883514404, loss=1.28886079788208
I0212 09:26:28.433025 139647825327872 logging_writer.py:48] [32600] global_step=32600, grad_norm=2.4770562648773193, loss=1.2819172143936157
I0212 09:27:59.748284 139647833720576 logging_writer.py:48] [32700] global_step=32700, grad_norm=4.419465065002441, loss=1.3331531286239624
I0212 09:29:32.308786 139647825327872 logging_writer.py:48] [32800] global_step=32800, grad_norm=3.266632080078125, loss=1.338472843170166
I0212 09:31:00.056669 139647833720576 logging_writer.py:48] [32900] global_step=32900, grad_norm=2.347764730453491, loss=1.3204752206802368
I0212 09:32:26.722361 139647833720576 logging_writer.py:48] [33000] global_step=33000, grad_norm=2.456232786178589, loss=1.3331351280212402
I0212 09:33:43.519060 139647825327872 logging_writer.py:48] [33100] global_step=33100, grad_norm=2.1439852714538574, loss=1.2512485980987549
I0212 09:35:03.005066 139647833720576 logging_writer.py:48] [33200] global_step=33200, grad_norm=2.5036633014678955, loss=1.3217922449111938
I0212 09:36:23.481247 139647825327872 logging_writer.py:48] [33300] global_step=33300, grad_norm=3.6525776386260986, loss=1.2755664587020874
I0212 09:37:48.566553 139647833720576 logging_writer.py:48] [33400] global_step=33400, grad_norm=2.409271001815796, loss=1.2589925527572632
I0212 09:39:16.442339 139647825327872 logging_writer.py:48] [33500] global_step=33500, grad_norm=2.2451558113098145, loss=1.2578836679458618
I0212 09:40:45.771629 139647833720576 logging_writer.py:48] [33600] global_step=33600, grad_norm=2.2897753715515137, loss=1.3462086915969849
I0212 09:42:17.025425 139647825327872 logging_writer.py:48] [33700] global_step=33700, grad_norm=2.6638214588165283, loss=1.2849868535995483
I0212 09:43:47.809607 139647833720576 logging_writer.py:48] [33800] global_step=33800, grad_norm=2.5071277618408203, loss=1.2718027830123901
I0212 09:45:18.970293 139647825327872 logging_writer.py:48] [33900] global_step=33900, grad_norm=2.4134864807128906, loss=1.3325698375701904
I0212 09:46:51.232402 139647833720576 logging_writer.py:48] [34000] global_step=34000, grad_norm=2.3515748977661133, loss=1.283193826675415
I0212 09:47:11.634675 139803787056960 spec.py:321] Evaluating on the training split.
I0212 09:48:07.361813 139803787056960 spec.py:333] Evaluating on the validation split.
I0212 09:49:00.975701 139803787056960 spec.py:349] Evaluating on the test split.
I0212 09:49:28.651122 139803787056960 submission_runner.py:408] Time since start: 31730.83s, 	Step: 34027, 	{'train/ctc_loss': Array(0.28990674, dtype=float32), 'train/wer': 0.09801625494599508, 'validation/ctc_loss': Array(0.47910196, dtype=float32), 'validation/wer': 0.1395676646359713, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2677237, dtype=float32), 'test/wer': 0.08563361972660614, 'test/num_examples': 2472, 'score': 28822.18370938301, 'total_duration': 31730.829365730286, 'accumulated_submission_time': 28822.18370938301, 'accumulated_eval_time': 2906.028322458267, 'accumulated_logging_time': 1.0652475357055664}
I0212 09:49:28.692220 139647833720576 logging_writer.py:48] [34027] accumulated_eval_time=2906.028322, accumulated_logging_time=1.065248, accumulated_submission_time=28822.183709, global_step=34027, preemption_count=0, score=28822.183709, test/ctc_loss=0.2677237093448639, test/num_examples=2472, test/wer=0.085634, total_duration=31730.829366, train/ctc_loss=0.28990674018859863, train/wer=0.098016, validation/ctc_loss=0.4791019558906555, validation/num_examples=5348, validation/wer=0.139568
I0212 09:50:24.677780 139647825327872 logging_writer.py:48] [34100] global_step=34100, grad_norm=2.604738235473633, loss=1.2797507047653198
I0212 09:51:39.835570 139647833720576 logging_writer.py:48] [34200] global_step=34200, grad_norm=3.901022434234619, loss=1.262921929359436
I0212 09:52:55.191932 139647825327872 logging_writer.py:48] [34300] global_step=34300, grad_norm=2.90384578704834, loss=1.2704343795776367
I0212 09:54:13.257042 139647833720576 logging_writer.py:48] [34400] global_step=34400, grad_norm=4.169870376586914, loss=1.2542898654937744
I0212 09:55:43.643499 139647825327872 logging_writer.py:48] [34500] global_step=34500, grad_norm=2.511889696121216, loss=1.2261229753494263
I0212 09:57:13.487799 139647833720576 logging_writer.py:48] [34600] global_step=34600, grad_norm=3.101330280303955, loss=1.2523574829101562
I0212 09:58:47.154323 139647825327872 logging_writer.py:48] [34700] global_step=34700, grad_norm=1.9495317935943604, loss=1.2848036289215088
I0212 10:00:16.828074 139647833720576 logging_writer.py:48] [34800] global_step=34800, grad_norm=3.242684841156006, loss=1.2911421060562134
I0212 10:01:49.517117 139647825327872 logging_writer.py:48] [34900] global_step=34900, grad_norm=4.723918914794922, loss=1.286826491355896
I0212 10:03:20.451640 139647833720576 logging_writer.py:48] [35000] global_step=35000, grad_norm=2.2623655796051025, loss=1.2961312532424927
I0212 10:04:43.848391 139647833720576 logging_writer.py:48] [35100] global_step=35100, grad_norm=2.92480731010437, loss=1.2765309810638428
I0212 10:06:00.523367 139647825327872 logging_writer.py:48] [35200] global_step=35200, grad_norm=2.9364848136901855, loss=1.2333531379699707
I0212 10:07:19.373243 139647833720576 logging_writer.py:48] [35300] global_step=35300, grad_norm=5.955417156219482, loss=1.2440693378448486
I0212 10:08:44.338120 139647825327872 logging_writer.py:48] [35400] global_step=35400, grad_norm=2.9348721504211426, loss=1.168015956878662
I0212 10:10:10.140480 139647833720576 logging_writer.py:48] [35500] global_step=35500, grad_norm=2.1554219722747803, loss=1.2577098608016968
I0212 10:11:41.000197 139647825327872 logging_writer.py:48] [35600] global_step=35600, grad_norm=2.7446794509887695, loss=1.27938973903656
I0212 10:13:10.410216 139647833720576 logging_writer.py:48] [35700] global_step=35700, grad_norm=2.62160587310791, loss=1.2759509086608887
I0212 10:13:28.871618 139803787056960 spec.py:321] Evaluating on the training split.
I0212 10:14:24.059929 139803787056960 spec.py:333] Evaluating on the validation split.
I0212 10:15:18.128261 139803787056960 spec.py:349] Evaluating on the test split.
I0212 10:15:45.929002 139803787056960 submission_runner.py:408] Time since start: 33308.11s, 	Step: 35723, 	{'train/ctc_loss': Array(0.27798745, dtype=float32), 'train/wer': 0.09665978392926217, 'validation/ctc_loss': Array(0.4688031, dtype=float32), 'validation/wer': 0.13611129884047618, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26008534, dtype=float32), 'test/wer': 0.08209940487071679, 'test/num_examples': 2472, 'score': 30262.275601148605, 'total_duration': 33308.1079826355, 'accumulated_submission_time': 30262.275601148605, 'accumulated_eval_time': 3043.079663515091, 'accumulated_logging_time': 1.1228668689727783}
I0212 10:15:45.971136 139647833720576 logging_writer.py:48] [35723] accumulated_eval_time=3043.079664, accumulated_logging_time=1.122867, accumulated_submission_time=30262.275601, global_step=35723, preemption_count=0, score=30262.275601, test/ctc_loss=0.2600853443145752, test/num_examples=2472, test/wer=0.082099, total_duration=33308.107983, train/ctc_loss=0.27798745036125183, train/wer=0.096660, validation/ctc_loss=0.4688031077384949, validation/num_examples=5348, validation/wer=0.136111
I0212 10:16:45.386569 139647825327872 logging_writer.py:48] [35800] global_step=35800, grad_norm=4.1706719398498535, loss=1.2131736278533936
I0212 10:18:00.557587 139647833720576 logging_writer.py:48] [35900] global_step=35900, grad_norm=2.9036924839019775, loss=1.2944929599761963
I0212 10:19:29.507440 139647825327872 logging_writer.py:48] [36000] global_step=36000, grad_norm=3.0758042335510254, loss=1.2781471014022827
I0212 10:20:56.322803 139647833720576 logging_writer.py:48] [36100] global_step=36100, grad_norm=3.098395347595215, loss=1.2531347274780273
I0212 10:22:14.924832 139647825327872 logging_writer.py:48] [36200] global_step=36200, grad_norm=4.150625228881836, loss=1.2440769672393799
I0212 10:23:37.029558 139647833720576 logging_writer.py:48] [36300] global_step=36300, grad_norm=2.7039031982421875, loss=1.2068918943405151
I0212 10:24:56.799014 139647825327872 logging_writer.py:48] [36400] global_step=36400, grad_norm=2.7877326011657715, loss=1.2263572216033936
I0212 10:26:24.311920 139647833720576 logging_writer.py:48] [36500] global_step=36500, grad_norm=2.7102062702178955, loss=1.2800862789154053
I0212 10:27:54.658942 139647825327872 logging_writer.py:48] [36600] global_step=36600, grad_norm=3.159655809402466, loss=1.2780457735061646
I0212 10:29:20.541580 139647833720576 logging_writer.py:48] [36700] global_step=36700, grad_norm=3.2015798091888428, loss=1.1777111291885376
I0212 10:30:48.243789 139647825327872 logging_writer.py:48] [36800] global_step=36800, grad_norm=3.702798843383789, loss=1.229210615158081
I0212 10:32:19.193737 139647833720576 logging_writer.py:48] [36900] global_step=36900, grad_norm=3.0543830394744873, loss=1.2532435655593872
I0212 10:33:50.467437 139647825327872 logging_writer.py:48] [37000] global_step=37000, grad_norm=3.6605913639068604, loss=1.2065318822860718
I0212 10:35:21.835682 139647833720576 logging_writer.py:48] [37100] global_step=37100, grad_norm=6.103061199188232, loss=1.183860182762146
I0212 10:36:39.948352 139647825327872 logging_writer.py:48] [37200] global_step=37200, grad_norm=2.626112699508667, loss=1.2368202209472656
I0212 10:38:01.183303 139647833720576 logging_writer.py:48] [37300] global_step=37300, grad_norm=2.768603563308716, loss=1.2629798650741577
I0212 10:39:21.741712 139647825327872 logging_writer.py:48] [37400] global_step=37400, grad_norm=2.414546012878418, loss=1.209280014038086
I0212 10:39:46.455156 139803787056960 spec.py:321] Evaluating on the training split.
I0212 10:40:42.057944 139803787056960 spec.py:333] Evaluating on the validation split.
I0212 10:41:34.998116 139803787056960 spec.py:349] Evaluating on the test split.
I0212 10:42:01.516979 139803787056960 submission_runner.py:408] Time since start: 34883.70s, 	Step: 37430, 	{'train/ctc_loss': Array(0.27347106, dtype=float32), 'train/wer': 0.09367223214970893, 'validation/ctc_loss': Array(0.45819047, dtype=float32), 'validation/wer': 0.13243287602460005, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25269088, dtype=float32), 'test/wer': 0.08014949322608819, 'test/num_examples': 2472, 'score': 31702.6688849926, 'total_duration': 34883.69545674324, 'accumulated_submission_time': 31702.6688849926, 'accumulated_eval_time': 3178.134963274002, 'accumulated_logging_time': 1.1837427616119385}
I0212 10:42:01.554933 139647833720576 logging_writer.py:48] [37430] accumulated_eval_time=3178.134963, accumulated_logging_time=1.183743, accumulated_submission_time=31702.668885, global_step=37430, preemption_count=0, score=31702.668885, test/ctc_loss=0.2526908814907074, test/num_examples=2472, test/wer=0.080149, total_duration=34883.695457, train/ctc_loss=0.27347105741500854, train/wer=0.093672, validation/ctc_loss=0.4581904709339142, validation/num_examples=5348, validation/wer=0.132433
I0212 10:42:55.226639 139647825327872 logging_writer.py:48] [37500] global_step=37500, grad_norm=4.518216133117676, loss=1.1602256298065186
I0212 10:44:10.509861 139647833720576 logging_writer.py:48] [37600] global_step=37600, grad_norm=4.989013195037842, loss=1.2046759128570557
I0212 10:45:36.318250 139647825327872 logging_writer.py:48] [37700] global_step=37700, grad_norm=3.4113073348999023, loss=1.253882884979248
I0212 10:47:05.828641 139647833720576 logging_writer.py:48] [37800] global_step=37800, grad_norm=5.605469226837158, loss=1.2105737924575806
I0212 10:48:36.548179 139647825327872 logging_writer.py:48] [37900] global_step=37900, grad_norm=2.5876266956329346, loss=1.2035316228866577
I0212 10:50:09.409282 139647833720576 logging_writer.py:48] [38000] global_step=38000, grad_norm=2.7990894317626953, loss=1.2353585958480835
I0212 10:51:40.948946 139647825327872 logging_writer.py:48] [38100] global_step=38100, grad_norm=2.1447927951812744, loss=1.1650800704956055
I0212 10:53:01.741509 139647833720576 logging_writer.py:48] [38200] global_step=38200, grad_norm=2.534940242767334, loss=1.2391645908355713
I0212 10:54:18.732693 139647825327872 logging_writer.py:48] [38300] global_step=38300, grad_norm=2.9583137035369873, loss=1.2319012880325317
I0212 10:55:41.329050 139647833720576 logging_writer.py:48] [38400] global_step=38400, grad_norm=4.000342845916748, loss=1.1379551887512207
I0212 10:57:07.423386 139647825327872 logging_writer.py:48] [38500] global_step=38500, grad_norm=2.5242717266082764, loss=1.1948411464691162
I0212 10:58:35.136385 139647833720576 logging_writer.py:48] [38600] global_step=38600, grad_norm=2.8279340267181396, loss=1.210511326789856
I0212 11:00:02.528623 139647825327872 logging_writer.py:48] [38700] global_step=38700, grad_norm=2.7622804641723633, loss=1.228394865989685
I0212 11:01:34.844310 139647833720576 logging_writer.py:48] [38800] global_step=38800, grad_norm=3.8998329639434814, loss=1.1910055875778198
I0212 11:03:02.754648 139647825327872 logging_writer.py:48] [38900] global_step=38900, grad_norm=4.410441875457764, loss=1.187110185623169
I0212 11:04:33.156880 139647833720576 logging_writer.py:48] [39000] global_step=39000, grad_norm=2.7374448776245117, loss=1.1903424263000488
I0212 11:06:02.428177 139647825327872 logging_writer.py:48] [39100] global_step=39100, grad_norm=2.2124972343444824, loss=1.223223328590393
I0212 11:06:02.436107 139803787056960 spec.py:321] Evaluating on the training split.
I0212 11:06:56.448632 139803787056960 spec.py:333] Evaluating on the validation split.
I0212 11:07:50.244603 139803787056960 spec.py:349] Evaluating on the test split.
I0212 11:08:17.888005 139803787056960 submission_runner.py:408] Time since start: 36460.07s, 	Step: 39101, 	{'train/ctc_loss': Array(0.26958352, dtype=float32), 'train/wer': 0.0919348818513308, 'validation/ctc_loss': Array(0.44324875, dtype=float32), 'validation/wer': 0.12828137520878188, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24324615, dtype=float32), 'test/wer': 0.07671683626835658, 'test/num_examples': 2472, 'score': 33143.4619538784, 'total_duration': 36460.06577825546, 'accumulated_submission_time': 33143.4619538784, 'accumulated_eval_time': 3313.579571247101, 'accumulated_logging_time': 1.2389507293701172}
I0212 11:08:17.929276 139647833720576 logging_writer.py:48] [39101] accumulated_eval_time=3313.579571, accumulated_logging_time=1.238951, accumulated_submission_time=33143.461954, global_step=39101, preemption_count=0, score=33143.461954, test/ctc_loss=0.2432461529970169, test/num_examples=2472, test/wer=0.076717, total_duration=36460.065778, train/ctc_loss=0.269583523273468, train/wer=0.091935, validation/ctc_loss=0.4432487487792969, validation/num_examples=5348, validation/wer=0.128281
I0212 11:09:39.394909 139647833720576 logging_writer.py:48] [39200] global_step=39200, grad_norm=3.7564940452575684, loss=1.201231598854065
I0212 11:10:58.896053 139647825327872 logging_writer.py:48] [39300] global_step=39300, grad_norm=2.665071964263916, loss=1.2102631330490112
I0212 11:12:18.318571 139647833720576 logging_writer.py:48] [39400] global_step=39400, grad_norm=3.977163553237915, loss=1.18190598487854
I0212 11:13:40.473388 139647825327872 logging_writer.py:48] [39500] global_step=39500, grad_norm=3.034337282180786, loss=1.2088488340377808
I0212 11:15:08.418012 139647833720576 logging_writer.py:48] [39600] global_step=39600, grad_norm=2.7488720417022705, loss=1.1687361001968384
I0212 11:16:38.531409 139647825327872 logging_writer.py:48] [39700] global_step=39700, grad_norm=3.3493056297302246, loss=1.1642054319381714
I0212 11:18:07.671317 139647833720576 logging_writer.py:48] [39800] global_step=39800, grad_norm=2.715607166290283, loss=1.2458256483078003
I0212 11:19:37.418292 139647825327872 logging_writer.py:48] [39900] global_step=39900, grad_norm=2.9435744285583496, loss=1.14535391330719
I0212 11:21:07.560374 139647833720576 logging_writer.py:48] [40000] global_step=40000, grad_norm=4.180883884429932, loss=1.2175748348236084
I0212 11:22:37.502673 139647825327872 logging_writer.py:48] [40100] global_step=40100, grad_norm=3.559021472930908, loss=1.2092576026916504
I0212 11:24:06.937772 139647833720576 logging_writer.py:48] [40200] global_step=40200, grad_norm=3.3811192512512207, loss=1.165444254875183
I0212 11:25:23.698446 139647825327872 logging_writer.py:48] [40300] global_step=40300, grad_norm=2.49505877494812, loss=1.2019623517990112
I0212 11:26:40.821267 139647833720576 logging_writer.py:48] [40400] global_step=40400, grad_norm=3.5969948768615723, loss=1.183907151222229
I0212 11:28:00.959796 139647825327872 logging_writer.py:48] [40500] global_step=40500, grad_norm=3.122593879699707, loss=1.2044317722320557
I0212 11:29:24.698535 139647833720576 logging_writer.py:48] [40600] global_step=40600, grad_norm=3.7085459232330322, loss=1.2056390047073364
I0212 11:30:51.331845 139647825327872 logging_writer.py:48] [40700] global_step=40700, grad_norm=4.419180870056152, loss=1.2050838470458984
I0212 11:32:18.808718 139803787056960 spec.py:321] Evaluating on the training split.
I0212 11:33:14.610800 139803787056960 spec.py:333] Evaluating on the validation split.
I0212 11:34:07.254783 139803787056960 spec.py:349] Evaluating on the test split.
I0212 11:34:33.934301 139803787056960 submission_runner.py:408] Time since start: 38036.11s, 	Step: 40799, 	{'train/ctc_loss': Array(0.25799805, dtype=float32), 'train/wer': 0.08621888395569983, 'validation/ctc_loss': Array(0.43293205, dtype=float32), 'validation/wer': 0.12576151076011083, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23475613, dtype=float32), 'test/wer': 0.07387321511993988, 'test/num_examples': 2472, 'score': 34584.25316166878, 'total_duration': 38036.112648010254, 'accumulated_submission_time': 34584.25316166878, 'accumulated_eval_time': 3448.6984837055206, 'accumulated_logging_time': 1.2968308925628662}
I0212 11:34:33.974337 139647833720576 logging_writer.py:48] [40799] accumulated_eval_time=3448.698484, accumulated_logging_time=1.296831, accumulated_submission_time=34584.253162, global_step=40799, preemption_count=0, score=34584.253162, test/ctc_loss=0.23475612699985504, test/num_examples=2472, test/wer=0.073873, total_duration=38036.112648, train/ctc_loss=0.2579980492591858, train/wer=0.086219, validation/ctc_loss=0.432932049036026, validation/num_examples=5348, validation/wer=0.125762
I0212 11:34:35.589227 139647825327872 logging_writer.py:48] [40800] global_step=40800, grad_norm=3.369168281555176, loss=1.1652374267578125
I0212 11:35:51.160590 139647833720576 logging_writer.py:48] [40900] global_step=40900, grad_norm=2.949444532394409, loss=1.20084810256958
I0212 11:37:06.620775 139647825327872 logging_writer.py:48] [41000] global_step=41000, grad_norm=2.144299268722534, loss=1.2041563987731934
I0212 11:38:38.890326 139647833720576 logging_writer.py:48] [41100] global_step=41100, grad_norm=3.736642599105835, loss=1.1318113803863525
I0212 11:40:12.973301 139647833720576 logging_writer.py:48] [41200] global_step=41200, grad_norm=3.12691593170166, loss=1.109512209892273
I0212 11:41:31.699292 139647825327872 logging_writer.py:48] [41300] global_step=41300, grad_norm=2.687089204788208, loss=1.189131259918213
I0212 11:42:52.410922 139647833720576 logging_writer.py:48] [41400] global_step=41400, grad_norm=2.6496455669403076, loss=1.181139349937439
I0212 11:44:14.936279 139647825327872 logging_writer.py:48] [41500] global_step=41500, grad_norm=2.567533016204834, loss=1.1289016008377075
I0212 11:45:37.510117 139647833720576 logging_writer.py:48] [41600] global_step=41600, grad_norm=3.4259157180786133, loss=1.1796780824661255
I0212 11:47:05.189984 139647825327872 logging_writer.py:48] [41700] global_step=41700, grad_norm=3.233893394470215, loss=1.1484375
I0212 11:48:37.135342 139647833720576 logging_writer.py:48] [41800] global_step=41800, grad_norm=2.6218533515930176, loss=1.1073229312896729
I0212 11:50:10.276653 139647825327872 logging_writer.py:48] [41900] global_step=41900, grad_norm=3.1939187049865723, loss=1.1318702697753906
I0212 11:51:36.754077 139647833720576 logging_writer.py:48] [42000] global_step=42000, grad_norm=2.832932472229004, loss=1.1968052387237549
I0212 11:53:10.616267 139647825327872 logging_writer.py:48] [42100] global_step=42100, grad_norm=3.2779905796051025, loss=1.1168460845947266
I0212 11:54:42.748631 139647833720576 logging_writer.py:48] [42200] global_step=42200, grad_norm=2.7234227657318115, loss=1.1945372819900513
I0212 11:56:07.480394 139647833720576 logging_writer.py:48] [42300] global_step=42300, grad_norm=4.280524253845215, loss=1.1188417673110962
I0212 11:57:24.410936 139647825327872 logging_writer.py:48] [42400] global_step=42400, grad_norm=2.339834690093994, loss=1.1523463726043701
I0212 11:58:34.040604 139803787056960 spec.py:321] Evaluating on the training split.
I0212 11:59:28.426969 139803787056960 spec.py:333] Evaluating on the validation split.
I0212 12:00:20.704249 139803787056960 spec.py:349] Evaluating on the test split.
I0212 12:00:47.739257 139803787056960 submission_runner.py:408] Time since start: 39609.92s, 	Step: 42492, 	{'train/ctc_loss': Array(0.22737612, dtype=float32), 'train/wer': 0.07978613680331388, 'validation/ctc_loss': Array(0.4232022, dtype=float32), 'validation/wer': 0.12189964953609392, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22964896, dtype=float32), 'test/wer': 0.07320293299209879, 'test/num_examples': 2472, 'score': 36024.23282289505, 'total_duration': 39609.91804718971, 'accumulated_submission_time': 36024.23282289505, 'accumulated_eval_time': 3582.3909134864807, 'accumulated_logging_time': 1.352367639541626}
I0212 12:00:47.780212 139647833720576 logging_writer.py:48] [42492] accumulated_eval_time=3582.390913, accumulated_logging_time=1.352368, accumulated_submission_time=36024.232823, global_step=42492, preemption_count=0, score=36024.232823, test/ctc_loss=0.22964896261692047, test/num_examples=2472, test/wer=0.073203, total_duration=39609.918047, train/ctc_loss=0.22737611830234528, train/wer=0.079786, validation/ctc_loss=0.42320218682289124, validation/num_examples=5348, validation/wer=0.121900
I0212 12:00:54.623658 139647825327872 logging_writer.py:48] [42500] global_step=42500, grad_norm=2.7025914192199707, loss=1.1125088930130005
I0212 12:02:09.896348 139647833720576 logging_writer.py:48] [42600] global_step=42600, grad_norm=2.8679914474487305, loss=1.1546109914779663
I0212 12:03:25.138350 139647825327872 logging_writer.py:48] [42700] global_step=42700, grad_norm=2.067718505859375, loss=1.1552846431732178
I0212 12:04:54.222843 139647833720576 logging_writer.py:48] [42800] global_step=42800, grad_norm=2.3824379444122314, loss=1.1722468137741089
I0212 12:06:20.263037 139647825327872 logging_writer.py:48] [42900] global_step=42900, grad_norm=2.88991117477417, loss=1.214190125465393
I0212 12:07:47.833129 139647833720576 logging_writer.py:48] [43000] global_step=43000, grad_norm=4.518304824829102, loss=1.169042706489563
I0212 12:09:20.619355 139647825327872 logging_writer.py:48] [43100] global_step=43100, grad_norm=3.13846755027771, loss=1.0968323945999146
I0212 12:10:51.709175 139647833720576 logging_writer.py:48] [43200] global_step=43200, grad_norm=2.7478480339050293, loss=1.1350387334823608
I0212 12:12:17.669668 139647833720576 logging_writer.py:48] [43300] global_step=43300, grad_norm=2.495234966278076, loss=1.1563282012939453
I0212 12:13:37.210012 139647825327872 logging_writer.py:48] [43400] global_step=43400, grad_norm=3.095294713973999, loss=1.1275405883789062
I0212 12:14:57.423952 139647833720576 logging_writer.py:48] [43500] global_step=43500, grad_norm=2.538274049758911, loss=1.150983214378357
I0212 12:16:23.675544 139647825327872 logging_writer.py:48] [43600] global_step=43600, grad_norm=4.254637718200684, loss=1.157118320465088
I0212 12:17:51.534630 139647833720576 logging_writer.py:48] [43700] global_step=43700, grad_norm=3.793395757675171, loss=1.1055272817611694
I0212 12:19:20.179015 139647825327872 logging_writer.py:48] [43800] global_step=43800, grad_norm=3.580704927444458, loss=1.2004377841949463
I0212 12:20:52.153144 139647833720576 logging_writer.py:48] [43900] global_step=43900, grad_norm=2.8270342350006104, loss=1.118312954902649
I0212 12:22:23.828593 139647825327872 logging_writer.py:48] [44000] global_step=44000, grad_norm=2.676856517791748, loss=1.1614571809768677
I0212 12:23:55.485877 139647833720576 logging_writer.py:48] [44100] global_step=44100, grad_norm=3.0803678035736084, loss=1.1533854007720947
I0212 12:24:47.907238 139803787056960 spec.py:321] Evaluating on the training split.
I0212 12:25:41.864240 139803787056960 spec.py:333] Evaluating on the validation split.
I0212 12:26:34.424240 139803787056960 spec.py:349] Evaluating on the test split.
I0212 12:27:00.884569 139803787056960 submission_runner.py:408] Time since start: 41183.06s, 	Step: 44160, 	{'train/ctc_loss': Array(0.22378671, dtype=float32), 'train/wer': 0.07772823632425457, 'validation/ctc_loss': Array(0.4190319, dtype=float32), 'validation/wer': 0.12144588084227194, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22551502, dtype=float32), 'test/wer': 0.07176081083825889, 'test/num_examples': 2472, 'score': 37464.273461818695, 'total_duration': 41183.06321454048, 'accumulated_submission_time': 37464.273461818695, 'accumulated_eval_time': 3715.3618457317352, 'accumulated_logging_time': 1.4084465503692627}
I0212 12:27:00.928281 139647833720576 logging_writer.py:48] [44160] accumulated_eval_time=3715.361846, accumulated_logging_time=1.408447, accumulated_submission_time=37464.273462, global_step=44160, preemption_count=0, score=37464.273462, test/ctc_loss=0.22551502287387848, test/num_examples=2472, test/wer=0.071761, total_duration=41183.063215, train/ctc_loss=0.22378671169281006, train/wer=0.077728, validation/ctc_loss=0.41903188824653625, validation/num_examples=5348, validation/wer=0.121446
I0212 12:27:32.604171 139647825327872 logging_writer.py:48] [44200] global_step=44200, grad_norm=2.391214370727539, loss=1.1441352367401123
I0212 12:28:51.920977 139647833720576 logging_writer.py:48] [44300] global_step=44300, grad_norm=3.4266347885131836, loss=1.0966838598251343
I0212 12:30:10.230357 139647825327872 logging_writer.py:48] [44400] global_step=44400, grad_norm=3.5226974487304688, loss=1.1398136615753174
I0212 12:31:31.517434 139647833720576 logging_writer.py:48] [44500] global_step=44500, grad_norm=4.882168769836426, loss=1.132667899131775
I0212 12:32:52.134636 139647825327872 logging_writer.py:48] [44600] global_step=44600, grad_norm=5.075389385223389, loss=1.1126939058303833
I0212 12:34:17.512444 139647833720576 logging_writer.py:48] [44700] global_step=44700, grad_norm=4.251006603240967, loss=1.1383711099624634
I0212 12:35:46.136060 139647825327872 logging_writer.py:48] [44800] global_step=44800, grad_norm=2.638840913772583, loss=1.1801512241363525
I0212 12:37:19.362351 139647833720576 logging_writer.py:48] [44900] global_step=44900, grad_norm=2.9744951725006104, loss=1.1113063097000122
I0212 12:38:52.328971 139647825327872 logging_writer.py:48] [45000] global_step=45000, grad_norm=5.680925369262695, loss=1.1064680814743042
I0212 12:40:20.819179 139647833720576 logging_writer.py:48] [45100] global_step=45100, grad_norm=2.427510976791382, loss=1.1376618146896362
I0212 12:41:47.865547 139647825327872 logging_writer.py:48] [45200] global_step=45200, grad_norm=3.190744161605835, loss=1.1989436149597168
I0212 12:43:18.809294 139647833720576 logging_writer.py:48] [45300] global_step=45300, grad_norm=2.4438133239746094, loss=1.0753172636032104
I0212 12:44:43.481593 139647833720576 logging_writer.py:48] [45400] global_step=45400, grad_norm=3.4316203594207764, loss=1.0758936405181885
I0212 12:45:59.690268 139647825327872 logging_writer.py:48] [45500] global_step=45500, grad_norm=2.4066221714019775, loss=1.162100911140442
I0212 12:47:20.979194 139647833720576 logging_writer.py:48] [45600] global_step=45600, grad_norm=3.712692975997925, loss=1.0928646326065063
I0212 12:48:45.065255 139647825327872 logging_writer.py:48] [45700] global_step=45700, grad_norm=3.949251890182495, loss=1.1597617864608765
I0212 12:50:13.904504 139647833720576 logging_writer.py:48] [45800] global_step=45800, grad_norm=2.432584762573242, loss=1.1425752639770508
I0212 12:51:01.487244 139803787056960 spec.py:321] Evaluating on the training split.
I0212 12:51:56.373973 139803787056960 spec.py:333] Evaluating on the validation split.
I0212 12:52:48.562794 139803787056960 spec.py:349] Evaluating on the test split.
I0212 12:53:15.479090 139803787056960 submission_runner.py:408] Time since start: 42757.66s, 	Step: 45856, 	{'train/ctc_loss': Array(0.21560626, dtype=float32), 'train/wer': 0.0740736650284389, 'validation/ctc_loss': Array(0.4162607, dtype=float32), 'validation/wer': 0.12051903414850787, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22425716, dtype=float32), 'test/wer': 0.07084678975483924, 'test/num_examples': 2472, 'score': 38904.74507904053, 'total_duration': 42757.65633749962, 'accumulated_submission_time': 38904.74507904053, 'accumulated_eval_time': 3849.3458960056305, 'accumulated_logging_time': 1.4675178527832031}
I0212 12:53:15.520298 139647833720576 logging_writer.py:48] [45856] accumulated_eval_time=3849.345896, accumulated_logging_time=1.467518, accumulated_submission_time=38904.745079, global_step=45856, preemption_count=0, score=38904.745079, test/ctc_loss=0.22425715625286102, test/num_examples=2472, test/wer=0.070847, total_duration=42757.656337, train/ctc_loss=0.21560625731945038, train/wer=0.074074, validation/ctc_loss=0.416260689496994, validation/num_examples=5348, validation/wer=0.120519
I0212 12:53:49.650978 139647825327872 logging_writer.py:48] [45900] global_step=45900, grad_norm=3.3939459323883057, loss=1.1282240152359009
I0212 12:55:04.826969 139647833720576 logging_writer.py:48] [46000] global_step=46000, grad_norm=2.783038377761841, loss=1.1745545864105225
I0212 12:56:29.468721 139647825327872 logging_writer.py:48] [46100] global_step=46100, grad_norm=3.5796618461608887, loss=1.1629271507263184
I0212 12:58:00.997374 139647833720576 logging_writer.py:48] [46200] global_step=46200, grad_norm=4.5791425704956055, loss=1.134329915046692
I0212 12:59:33.318843 139647825327872 logging_writer.py:48] [46300] global_step=46300, grad_norm=3.255126476287842, loss=1.1345090866088867
I0212 13:00:59.309032 139647833720576 logging_writer.py:48] [46400] global_step=46400, grad_norm=3.5168697834014893, loss=1.1551623344421387
I0212 13:02:16.549813 139647825327872 logging_writer.py:48] [46500] global_step=46500, grad_norm=4.938607215881348, loss=1.1286877393722534
I0212 13:03:35.855905 139647833720576 logging_writer.py:48] [46600] global_step=46600, grad_norm=2.792527675628662, loss=1.1440463066101074
I0212 13:04:56.307159 139647825327872 logging_writer.py:48] [46700] global_step=46700, grad_norm=3.711604595184326, loss=1.1373252868652344
I0212 13:06:22.634408 139647833720576 logging_writer.py:48] [46800] global_step=46800, grad_norm=3.065760850906372, loss=1.1751482486724854
I0212 13:07:51.594376 139647825327872 logging_writer.py:48] [46900] global_step=46900, grad_norm=2.1900835037231445, loss=1.1495901346206665
I0212 13:09:24.019594 139647833720576 logging_writer.py:48] [47000] global_step=47000, grad_norm=3.671980142593384, loss=1.1254637241363525
I0212 13:10:55.354639 139647825327872 logging_writer.py:48] [47100] global_step=47100, grad_norm=2.751631259918213, loss=1.1105990409851074
I0212 13:12:28.105960 139647833720576 logging_writer.py:48] [47200] global_step=47200, grad_norm=4.256897449493408, loss=1.12917959690094
I0212 13:13:55.720610 139647825327872 logging_writer.py:48] [47300] global_step=47300, grad_norm=4.191689491271973, loss=1.1342544555664062
I0212 13:15:27.053642 139647833720576 logging_writer.py:48] [47400] global_step=47400, grad_norm=4.280519008636475, loss=1.1221998929977417
I0212 13:16:44.497169 139647825327872 logging_writer.py:48] [47500] global_step=47500, grad_norm=3.584953784942627, loss=1.1157288551330566
I0212 13:17:16.132689 139803787056960 spec.py:321] Evaluating on the training split.
I0212 13:18:10.165329 139803787056960 spec.py:333] Evaluating on the validation split.
I0212 13:19:02.810450 139803787056960 spec.py:349] Evaluating on the test split.
I0212 13:19:29.362796 139803787056960 submission_runner.py:408] Time since start: 44331.54s, 	Step: 47543, 	{'train/ctc_loss': Array(0.21957077, dtype=float32), 'train/wer': 0.07548130217406059, 'validation/ctc_loss': Array(0.41540474, dtype=float32), 'validation/wer': 0.1202293945567066, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22386399, dtype=float32), 'test/wer': 0.0707046086974184, 'test/num_examples': 2472, 'score': 40345.268634557724, 'total_duration': 44331.542063474655, 'accumulated_submission_time': 40345.268634557724, 'accumulated_eval_time': 3982.5702333450317, 'accumulated_logging_time': 1.5255703926086426}
I0212 13:19:29.403236 139647833720576 logging_writer.py:48] [47543] accumulated_eval_time=3982.570233, accumulated_logging_time=1.525570, accumulated_submission_time=40345.268635, global_step=47543, preemption_count=0, score=40345.268635, test/ctc_loss=0.22386398911476135, test/num_examples=2472, test/wer=0.070705, total_duration=44331.542063, train/ctc_loss=0.21957077085971832, train/wer=0.075481, validation/ctc_loss=0.415404736995697, validation/num_examples=5348, validation/wer=0.120229
I0212 13:20:14.164303 139647825327872 logging_writer.py:48] [47600] global_step=47600, grad_norm=1.9162962436676025, loss=1.1208702325820923
I0212 13:21:29.746123 139647833720576 logging_writer.py:48] [47700] global_step=47700, grad_norm=3.7727088928222656, loss=1.1967767477035522
I0212 13:22:44.922488 139647825327872 logging_writer.py:48] [47800] global_step=47800, grad_norm=4.324095249176025, loss=1.1270054578781128
I0212 13:24:09.937520 139647833720576 logging_writer.py:48] [47900] global_step=47900, grad_norm=3.7606775760650635, loss=1.1278083324432373
I0212 13:25:36.541752 139803787056960 spec.py:321] Evaluating on the training split.
I0212 13:26:30.623074 139803787056960 spec.py:333] Evaluating on the validation split.
I0212 13:27:26.319320 139803787056960 spec.py:349] Evaluating on the test split.
I0212 13:27:53.188471 139803787056960 submission_runner.py:408] Time since start: 44835.37s, 	Step: 48000, 	{'train/ctc_loss': Array(0.22205196, dtype=float32), 'train/wer': 0.07654986522911052, 'validation/ctc_loss': Array(0.41554552, dtype=float32), 'validation/wer': 0.12041283296484741, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22391605, dtype=float32), 'test/wer': 0.0706233623788922, 'test/num_examples': 2472, 'score': 40712.370055913925, 'total_duration': 44835.370416641235, 'accumulated_submission_time': 40712.370055913925, 'accumulated_eval_time': 4119.21386384964, 'accumulated_logging_time': 1.5837607383728027}
I0212 13:27:53.227441 139647833720576 logging_writer.py:48] [48000] accumulated_eval_time=4119.213864, accumulated_logging_time=1.583761, accumulated_submission_time=40712.370056, global_step=48000, preemption_count=0, score=40712.370056, test/ctc_loss=0.22391605377197266, test/num_examples=2472, test/wer=0.070623, total_duration=44835.370417, train/ctc_loss=0.2220519632101059, train/wer=0.076550, validation/ctc_loss=0.4155455231666565, validation/num_examples=5348, validation/wer=0.120413
I0212 13:27:53.249767 139647825327872 logging_writer.py:48] [48000] global_step=48000, preemption_count=0, score=40712.370056
I0212 13:27:53.472832 139803787056960 checkpoints.py:490] Saving checkpoint at step: 48000
I0212 13:27:54.477591 139803787056960 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_1/librispeech_deepspeech_jax/trial_3/checkpoint_48000
I0212 13:27:54.497278 139803787056960 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_1/librispeech_deepspeech_jax/trial_3/checkpoint_48000.
I0212 13:27:55.905439 139803787056960 submission_runner.py:583] Tuning trial 3/5
I0212 13:27:55.905682 139803787056960 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0212 13:27:55.919817 139803787056960 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(31.498302, dtype=float32), 'train/wer': 4.839384963377855, 'validation/ctc_loss': Array(30.813007, dtype=float32), 'validation/wer': 4.233526748216303, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.939787, dtype=float32), 'test/wer': 4.560619909410355, 'test/num_examples': 2472, 'score': 15.311807870864868, 'total_duration': 234.32508182525635, 'accumulated_submission_time': 15.311807870864868, 'accumulated_eval_time': 219.01320672035217, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1709, {'train/ctc_loss': Array(6.1106343, dtype=float32), 'train/wer': 0.9224396410326059, 'validation/ctc_loss': Array(6.18663, dtype=float32), 'validation/wer': 0.886992285932205, 'validation/num_examples': 5348, 'test/ctc_loss': Array(6.0562162, dtype=float32), 'test/wer': 0.888387869924644, 'test/num_examples': 2472, 'score': 1455.6672337055206, 'total_duration': 1786.9586765766144, 'accumulated_submission_time': 1455.6672337055206, 'accumulated_eval_time': 331.1859018802643, 'accumulated_logging_time': 0.032968759536743164, 'global_step': 1709, 'preemption_count': 0}), (3429, {'train/ctc_loss': Array(4.5104766, dtype=float32), 'train/wer': 0.8586798150220665, 'validation/ctc_loss': Array(4.5426497, dtype=float32), 'validation/wer': 0.8317097425104029, 'validation/num_examples': 5348, 'test/ctc_loss': Array(4.1909637, dtype=float32), 'test/wer': 0.8059228566205594, 'test/num_examples': 2472, 'score': 2895.917924642563, 'total_duration': 3351.280608177185, 'accumulated_submission_time': 2895.917924642563, 'accumulated_eval_time': 455.1211357116699, 'accumulated_logging_time': 0.09017682075500488, 'global_step': 3429, 'preemption_count': 0}), (5110, {'train/ctc_loss': Array(0.7030768, dtype=float32), 'train/wer': 0.22973511201229005, 'validation/ctc_loss': Array(1.0333956, dtype=float32), 'validation/wer': 0.2909333153113143, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.694737, dtype=float32), 'test/wer': 0.2199134726707696, 'test/num_examples': 2472, 'score': 4336.514578104019, 'total_duration': 4932.689397573471, 'accumulated_submission_time': 4336.514578104019, 'accumulated_eval_time': 595.8084809780121, 'accumulated_logging_time': 0.14051008224487305, 'global_step': 5110, 'preemption_count': 0}), (6817, {'train/ctc_loss': Array(0.5106037, dtype=float32), 'train/wer': 0.170897716019303, 'validation/ctc_loss': Array(0.83905643, dtype=float32), 'validation/wer': 0.24012087625631173, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5300292, dtype=float32), 'test/wer': 0.17266873844778907, 'test/num_examples': 2472, 'score': 5777.360721826553, 'total_duration': 6509.532005548477, 'accumulated_submission_time': 5777.360721826553, 'accumulated_eval_time': 731.6756029129028, 'accumulated_logging_time': 0.19184613227844238, 'global_step': 6817, 'preemption_count': 0}), (8552, {'train/ctc_loss': Array(0.4068033, dtype=float32), 'train/wer': 0.14157673023287692, 'validation/ctc_loss': Array(0.7618631, dtype=float32), 'validation/wer': 0.22024194560568466, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4711684, dtype=float32), 'test/wer': 0.15463205573497452, 'test/num_examples': 2472, 'score': 7217.927873134613, 'total_duration': 8087.86146068573, 'accumulated_submission_time': 7217.927873134613, 'accumulated_eval_time': 869.3080587387085, 'accumulated_logging_time': 0.24324870109558105, 'global_step': 8552, 'preemption_count': 0}), (10235, {'train/ctc_loss': Array(0.38560042, dtype=float32), 'train/wer': 0.13040989764057173, 'validation/ctc_loss': Array(0.7276246, dtype=float32), 'validation/wer': 0.2111665717292449, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4391571, dtype=float32), 'test/wer': 0.14025145735583858, 'test/num_examples': 2472, 'score': 8658.387535095215, 'total_duration': 9665.303247451782, 'accumulated_submission_time': 8658.387535095215, 'accumulated_eval_time': 1006.1574850082397, 'accumulated_logging_time': 0.299619197845459, 'global_step': 10235, 'preemption_count': 0}), (11932, {'train/ctc_loss': Array(0.4022625, dtype=float32), 'train/wer': 0.13475438216836183, 'validation/ctc_loss': Array(0.71573716, dtype=float32), 'validation/wer': 0.20575996601562122, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.42614245, dtype=float32), 'test/wer': 0.1368797351370016, 'test/num_examples': 2472, 'score': 10098.617139339447, 'total_duration': 11240.799752235413, 'accumulated_submission_time': 10098.617139339447, 'accumulated_eval_time': 1141.2911870479584, 'accumulated_logging_time': 0.3535733222961426, 'global_step': 11932, 'preemption_count': 0}), (13643, {'train/ctc_loss': Array(0.37538254, dtype=float32), 'train/wer': 0.12491624494064966, 'validation/ctc_loss': Array(0.7008779, dtype=float32), 'validation/wer': 0.20287322475066857, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.42028874, dtype=float32), 'test/wer': 0.13389393293116406, 'test/num_examples': 2472, 'score': 11538.98053908348, 'total_duration': 12816.938221931458, 'accumulated_submission_time': 11538.98053908348, 'accumulated_eval_time': 1276.9315330982208, 'accumulated_logging_time': 0.4093668460845947, 'global_step': 13643, 'preemption_count': 0}), (15334, {'train/ctc_loss': Array(0.3517334, dtype=float32), 'train/wer': 0.12161177798296911, 'validation/ctc_loss': Array(0.66360116, dtype=float32), 'validation/wer': 0.19245585409888297, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3942068, dtype=float32), 'test/wer': 0.12922226961590802, 'test/num_examples': 2472, 'score': 12979.097982645035, 'total_duration': 14397.922987937927, 'accumulated_submission_time': 12979.097982645035, 'accumulated_eval_time': 1417.6731894016266, 'accumulated_logging_time': 0.45882654190063477, 'global_step': 15334, 'preemption_count': 0}), (17023, {'train/ctc_loss': Array(0.3794418, dtype=float32), 'train/wer': 0.1212333939606667, 'validation/ctc_loss': Array(0.6458081, dtype=float32), 'validation/wer': 0.18757059965050155, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.37999377, dtype=float32), 'test/wer': 0.12337253468202222, 'test/num_examples': 2472, 'score': 14419.062114953995, 'total_duration': 15974.105421543121, 'accumulated_submission_time': 14419.062114953995, 'accumulated_eval_time': 1553.7552139759064, 'accumulated_logging_time': 0.5167844295501709, 'global_step': 17023, 'preemption_count': 0}), (18717, {'train/ctc_loss': Array(0.28394952, dtype=float32), 'train/wer': 0.0967809750394815, 'validation/ctc_loss': Array(0.6158726, dtype=float32), 'validation/wer': 0.17783870936597893, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.36024228, dtype=float32), 'test/wer': 0.11589787337761258, 'test/num_examples': 2472, 'score': 15859.043215751648, 'total_duration': 17550.060350894928, 'accumulated_submission_time': 15859.043215751648, 'accumulated_eval_time': 1689.598258972168, 'accumulated_logging_time': 0.5695686340332031, 'global_step': 18717, 'preemption_count': 0}), (20414, {'train/ctc_loss': Array(0.3042081, dtype=float32), 'train/wer': 0.10375509051843565, 'validation/ctc_loss': Array(0.6088648, dtype=float32), 'validation/wer': 0.17660291377429352, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3576674, dtype=float32), 'test/wer': 0.11669002498324295, 'test/num_examples': 2472, 'score': 17299.182085752487, 'total_duration': 19125.46996498108, 'accumulated_submission_time': 17299.182085752487, 'accumulated_eval_time': 1824.739266872406, 'accumulated_logging_time': 0.6228508949279785, 'global_step': 20414, 'preemption_count': 0}), (22123, {'train/ctc_loss': Array(0.3945768, dtype=float32), 'train/wer': 0.13109240002638117, 'validation/ctc_loss': Array(0.5891843, dtype=float32), 'validation/wer': 0.1709356324280487, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.34208703, dtype=float32), 'test/wer': 0.11181524587167144, 'test/num_examples': 2472, 'score': 18739.56921505928, 'total_duration': 20702.452353477478, 'accumulated_submission_time': 18739.56921505928, 'accumulated_eval_time': 1961.207276582718, 'accumulated_logging_time': 0.6727504730224609, 'global_step': 22123, 'preemption_count': 0}), (23821, {'train/ctc_loss': Array(0.4185183, dtype=float32), 'train/wer': 0.13682009992292293, 'validation/ctc_loss': Array(0.5801254, dtype=float32), 'validation/wer': 0.1673344468366529, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33332494, dtype=float32), 'test/wer': 0.10553896776552313, 'test/num_examples': 2472, 'score': 20179.91077518463, 'total_duration': 22280.166800022125, 'accumulated_submission_time': 20179.91077518463, 'accumulated_eval_time': 2098.446216583252, 'accumulated_logging_time': 0.7275550365447998, 'global_step': 23821, 'preemption_count': 0}), (25522, {'train/ctc_loss': Array(0.45643923, dtype=float32), 'train/wer': 0.14958464822721365, 'validation/ctc_loss': Array(0.560944, dtype=float32), 'validation/wer': 0.16224644467401064, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32449305, dtype=float32), 'test/wer': 0.10533585196920764, 'test/num_examples': 2472, 'score': 21620.787089586258, 'total_duration': 23852.190286397934, 'accumulated_submission_time': 21620.787089586258, 'accumulated_eval_time': 2229.4631350040436, 'accumulated_logging_time': 0.7792990207672119, 'global_step': 25522, 'preemption_count': 0}), (27235, {'train/ctc_loss': Array(0.39219317, dtype=float32), 'train/wer': 0.12764995891536565, 'validation/ctc_loss': Array(0.5473959, dtype=float32), 'validation/wer': 0.15966865230697935, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3108912, dtype=float32), 'test/wer': 0.09948611703532184, 'test/num_examples': 2472, 'score': 23060.805052042007, 'total_duration': 25423.667890787125, 'accumulated_submission_time': 23060.805052042007, 'accumulated_eval_time': 2360.7889981269836, 'accumulated_logging_time': 0.8352978229522705, 'global_step': 27235, 'preemption_count': 0}), (28931, {'train/ctc_loss': Array(0.35523257, dtype=float32), 'train/wer': 0.11911170550520063, 'validation/ctc_loss': Array(0.526833, dtype=float32), 'validation/wer': 0.15387586047095397, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2973004, dtype=float32), 'test/wer': 0.0954034895293807, 'test/num_examples': 2472, 'score': 24500.7278380394, 'total_duration': 26999.747796297073, 'accumulated_submission_time': 24500.7278380394, 'accumulated_eval_time': 2496.8168222904205, 'accumulated_logging_time': 0.8881025314331055, 'global_step': 28931, 'preemption_count': 0}), (30635, {'train/ctc_loss': Array(0.30507174, dtype=float32), 'train/wer': 0.1035346025532191, 'validation/ctc_loss': Array(0.50986946, dtype=float32), 'validation/wer': 0.14723345916564487, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28395033, dtype=float32), 'test/wer': 0.09123961570491337, 'test/num_examples': 2472, 'score': 25941.15482234955, 'total_duration': 28578.65523004532, 'accumulated_submission_time': 25941.15482234955, 'accumulated_eval_time': 2635.162611246109, 'accumulated_logging_time': 0.941051721572876, 'global_step': 30635, 'preemption_count': 0}), (32348, {'train/ctc_loss': Array(0.3419567, dtype=float32), 'train/wer': 0.11450993651970943, 'validation/ctc_loss': Array(0.5040512, dtype=float32), 'validation/wer': 0.14678934512488293, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28116906, dtype=float32), 'test/wer': 0.08896471878618, 'test/num_examples': 2472, 'score': 27381.489032030106, 'total_duration': 30152.98643374443, 'accumulated_submission_time': 27381.489032030106, 'accumulated_eval_time': 2769.01868224144, 'accumulated_logging_time': 1.0033252239227295, 'global_step': 32348, 'preemption_count': 0}), (34027, {'train/ctc_loss': Array(0.28990674, dtype=float32), 'train/wer': 0.09801625494599508, 'validation/ctc_loss': Array(0.47910196, dtype=float32), 'validation/wer': 0.1395676646359713, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2677237, dtype=float32), 'test/wer': 0.08563361972660614, 'test/num_examples': 2472, 'score': 28822.18370938301, 'total_duration': 31730.829365730286, 'accumulated_submission_time': 28822.18370938301, 'accumulated_eval_time': 2906.028322458267, 'accumulated_logging_time': 1.0652475357055664, 'global_step': 34027, 'preemption_count': 0}), (35723, {'train/ctc_loss': Array(0.27798745, dtype=float32), 'train/wer': 0.09665978392926217, 'validation/ctc_loss': Array(0.4688031, dtype=float32), 'validation/wer': 0.13611129884047618, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26008534, dtype=float32), 'test/wer': 0.08209940487071679, 'test/num_examples': 2472, 'score': 30262.275601148605, 'total_duration': 33308.1079826355, 'accumulated_submission_time': 30262.275601148605, 'accumulated_eval_time': 3043.079663515091, 'accumulated_logging_time': 1.1228668689727783, 'global_step': 35723, 'preemption_count': 0}), (37430, {'train/ctc_loss': Array(0.27347106, dtype=float32), 'train/wer': 0.09367223214970893, 'validation/ctc_loss': Array(0.45819047, dtype=float32), 'validation/wer': 0.13243287602460005, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25269088, dtype=float32), 'test/wer': 0.08014949322608819, 'test/num_examples': 2472, 'score': 31702.6688849926, 'total_duration': 34883.69545674324, 'accumulated_submission_time': 31702.6688849926, 'accumulated_eval_time': 3178.134963274002, 'accumulated_logging_time': 1.1837427616119385, 'global_step': 37430, 'preemption_count': 0}), (39101, {'train/ctc_loss': Array(0.26958352, dtype=float32), 'train/wer': 0.0919348818513308, 'validation/ctc_loss': Array(0.44324875, dtype=float32), 'validation/wer': 0.12828137520878188, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24324615, dtype=float32), 'test/wer': 0.07671683626835658, 'test/num_examples': 2472, 'score': 33143.4619538784, 'total_duration': 36460.06577825546, 'accumulated_submission_time': 33143.4619538784, 'accumulated_eval_time': 3313.579571247101, 'accumulated_logging_time': 1.2389507293701172, 'global_step': 39101, 'preemption_count': 0}), (40799, {'train/ctc_loss': Array(0.25799805, dtype=float32), 'train/wer': 0.08621888395569983, 'validation/ctc_loss': Array(0.43293205, dtype=float32), 'validation/wer': 0.12576151076011083, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23475613, dtype=float32), 'test/wer': 0.07387321511993988, 'test/num_examples': 2472, 'score': 34584.25316166878, 'total_duration': 38036.112648010254, 'accumulated_submission_time': 34584.25316166878, 'accumulated_eval_time': 3448.6984837055206, 'accumulated_logging_time': 1.2968308925628662, 'global_step': 40799, 'preemption_count': 0}), (42492, {'train/ctc_loss': Array(0.22737612, dtype=float32), 'train/wer': 0.07978613680331388, 'validation/ctc_loss': Array(0.4232022, dtype=float32), 'validation/wer': 0.12189964953609392, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22964896, dtype=float32), 'test/wer': 0.07320293299209879, 'test/num_examples': 2472, 'score': 36024.23282289505, 'total_duration': 39609.91804718971, 'accumulated_submission_time': 36024.23282289505, 'accumulated_eval_time': 3582.3909134864807, 'accumulated_logging_time': 1.352367639541626, 'global_step': 42492, 'preemption_count': 0}), (44160, {'train/ctc_loss': Array(0.22378671, dtype=float32), 'train/wer': 0.07772823632425457, 'validation/ctc_loss': Array(0.4190319, dtype=float32), 'validation/wer': 0.12144588084227194, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22551502, dtype=float32), 'test/wer': 0.07176081083825889, 'test/num_examples': 2472, 'score': 37464.273461818695, 'total_duration': 41183.06321454048, 'accumulated_submission_time': 37464.273461818695, 'accumulated_eval_time': 3715.3618457317352, 'accumulated_logging_time': 1.4084465503692627, 'global_step': 44160, 'preemption_count': 0}), (45856, {'train/ctc_loss': Array(0.21560626, dtype=float32), 'train/wer': 0.0740736650284389, 'validation/ctc_loss': Array(0.4162607, dtype=float32), 'validation/wer': 0.12051903414850787, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22425716, dtype=float32), 'test/wer': 0.07084678975483924, 'test/num_examples': 2472, 'score': 38904.74507904053, 'total_duration': 42757.65633749962, 'accumulated_submission_time': 38904.74507904053, 'accumulated_eval_time': 3849.3458960056305, 'accumulated_logging_time': 1.4675178527832031, 'global_step': 45856, 'preemption_count': 0}), (47543, {'train/ctc_loss': Array(0.21957077, dtype=float32), 'train/wer': 0.07548130217406059, 'validation/ctc_loss': Array(0.41540474, dtype=float32), 'validation/wer': 0.1202293945567066, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22386399, dtype=float32), 'test/wer': 0.0707046086974184, 'test/num_examples': 2472, 'score': 40345.268634557724, 'total_duration': 44331.542063474655, 'accumulated_submission_time': 40345.268634557724, 'accumulated_eval_time': 3982.5702333450317, 'accumulated_logging_time': 1.5255703926086426, 'global_step': 47543, 'preemption_count': 0}), (48000, {'train/ctc_loss': Array(0.22205196, dtype=float32), 'train/wer': 0.07654986522911052, 'validation/ctc_loss': Array(0.41554552, dtype=float32), 'validation/wer': 0.12041283296484741, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22391605, dtype=float32), 'test/wer': 0.0706233623788922, 'test/num_examples': 2472, 'score': 40712.370055913925, 'total_duration': 44835.370416641235, 'accumulated_submission_time': 40712.370055913925, 'accumulated_eval_time': 4119.21386384964, 'accumulated_logging_time': 1.5837607383728027, 'global_step': 48000, 'preemption_count': 0})], 'global_step': 48000}
I0212 13:27:55.920053 139803787056960 submission_runner.py:586] Timing: 40712.370055913925
I0212 13:27:55.920128 139803787056960 submission_runner.py:588] Total number of evals: 30
I0212 13:27:55.920181 139803787056960 submission_runner.py:589] ====================
I0212 13:27:55.920242 139803787056960 submission_runner.py:542] Using RNG seed 808887856
I0212 13:27:55.923759 139803787056960 submission_runner.py:551] --- Tuning run 4/5 ---
I0212 13:27:55.923911 139803787056960 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_1/librispeech_deepspeech_jax/trial_4.
I0212 13:27:55.926846 139803787056960 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_1/librispeech_deepspeech_jax/trial_4/hparams.json.
I0212 13:27:55.929248 139803787056960 submission_runner.py:206] Initializing dataset.
I0212 13:27:55.929374 139803787056960 submission_runner.py:213] Initializing model.
I0212 13:27:57.144152 139803787056960 submission_runner.py:255] Initializing optimizer.
I0212 13:27:57.297091 139803787056960 submission_runner.py:262] Initializing metrics bundle.
I0212 13:27:57.297276 139803787056960 submission_runner.py:280] Initializing checkpoint and logger.
I0212 13:27:57.301211 139803787056960 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_1/librispeech_deepspeech_jax/trial_4 with prefix checkpoint_
I0212 13:27:57.301352 139803787056960 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_1/librispeech_deepspeech_jax/trial_4/meta_data_0.json.
I0212 13:27:57.301680 139803787056960 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0212 13:27:57.301759 139803787056960 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0212 13:27:57.845394 139803787056960 logger_utils.py:220] Unable to record git information. Continuing without it.
I0212 13:27:58.333162 139803787056960 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_1/librispeech_deepspeech_jax/trial_4/flags_0.json.
I0212 13:27:58.354084 139803787056960 submission_runner.py:314] Starting training loop.
I0212 13:27:58.357762 139803787056960 input_pipeline.py:20] Loading split = train-clean-100
I0212 13:27:58.877342 139803787056960 input_pipeline.py:20] Loading split = train-clean-360
I0212 13:27:59.011165 139803787056960 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0212 13:28:14.471817 139646875539200 logging_writer.py:48] [0] global_step=0, grad_norm=18.24744415283203, loss=32.952850341796875
I0212 13:28:14.487015 139803787056960 spec.py:321] Evaluating on the training split.
I0212 13:30:05.408660 139803787056960 spec.py:333] Evaluating on the validation split.
I0212 13:31:13.536479 139803787056960 spec.py:349] Evaluating on the test split.
I0212 13:31:48.397418 139803787056960 submission_runner.py:408] Time since start: 230.04s, 	Step: 1, 	{'train/ctc_loss': Array(31.425632, dtype=float32), 'train/wer': 4.572452038869513, 'validation/ctc_loss': Array(30.812973, dtype=float32), 'validation/wer': 4.2336619133591435, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.93976, dtype=float32), 'test/wer': 4.561148010480775, 'test/num_examples': 2472, 'score': 16.132816791534424, 'total_duration': 230.04052543640137, 'accumulated_submission_time': 16.132816791534424, 'accumulated_eval_time': 213.90761828422546, 'accumulated_logging_time': 0}
I0212 13:31:48.414915 139647618680576 logging_writer.py:48] [1] accumulated_eval_time=213.907618, accumulated_logging_time=0, accumulated_submission_time=16.132817, global_step=1, preemption_count=0, score=16.132817, test/ctc_loss=30.939760208129883, test/num_examples=2472, test/wer=4.561148, total_duration=230.040525, train/ctc_loss=31.42563247680664, train/wer=4.572452, validation/ctc_loss=30.812973022460938, validation/num_examples=5348, validation/wer=4.233662
I0212 13:33:16.141736 139646892324608 logging_writer.py:48] [100] global_step=100, grad_norm=3.531517744064331, loss=5.840846538543701
I0212 13:34:33.308883 139646900717312 logging_writer.py:48] [200] global_step=200, grad_norm=2.114623785018921, loss=4.804646968841553
I0212 13:35:49.527672 139646892324608 logging_writer.py:48] [300] global_step=300, grad_norm=3.9947774410247803, loss=3.6787497997283936
I0212 13:37:07.225343 139646900717312 logging_writer.py:48] [400] global_step=400, grad_norm=3.177779197692871, loss=3.225188732147217
I0212 13:38:32.146643 139646892324608 logging_writer.py:48] [500] global_step=500, grad_norm=2.1979894638061523, loss=2.9968020915985107
I0212 13:40:02.820013 139646900717312 logging_writer.py:48] [600] global_step=600, grad_norm=2.9020936489105225, loss=2.907789945602417
I0212 13:41:30.077769 139646892324608 logging_writer.py:48] [700] global_step=700, grad_norm=1.9310024976730347, loss=2.7190656661987305
I0212 13:43:00.357852 139646900717312 logging_writer.py:48] [800] global_step=800, grad_norm=3.506096839904785, loss=2.608016014099121
I0212 13:44:31.497730 139646892324608 logging_writer.py:48] [900] global_step=900, grad_norm=3.035384178161621, loss=2.5369346141815186
I0212 13:46:02.181178 139646900717312 logging_writer.py:48] [1000] global_step=1000, grad_norm=3.122873544692993, loss=2.523362874984741
I0212 13:47:26.860476 139647618680576 logging_writer.py:48] [1100] global_step=1100, grad_norm=2.8050804138183594, loss=2.429924488067627
I0212 13:48:44.433473 139647610287872 logging_writer.py:48] [1200] global_step=1200, grad_norm=3.230290412902832, loss=2.5301499366760254
I0212 13:50:03.980624 139647618680576 logging_writer.py:48] [1300] global_step=1300, grad_norm=2.6413938999176025, loss=2.3672661781311035
I0212 13:51:25.983627 139647610287872 logging_writer.py:48] [1400] global_step=1400, grad_norm=2.9814445972442627, loss=2.341914176940918
I0212 13:52:54.653431 139647618680576 logging_writer.py:48] [1500] global_step=1500, grad_norm=2.770745277404785, loss=2.2997028827667236
I0212 13:54:23.815318 139647610287872 logging_writer.py:48] [1600] global_step=1600, grad_norm=2.658773183822632, loss=2.297691583633423
I0212 13:55:48.523332 139803787056960 spec.py:321] Evaluating on the training split.
I0212 13:56:41.155155 139803787056960 spec.py:333] Evaluating on the validation split.
I0212 13:57:34.060315 139803787056960 spec.py:349] Evaluating on the test split.
I0212 13:58:00.785938 139803787056960 submission_runner.py:408] Time since start: 1802.42s, 	Step: 1695, 	{'train/ctc_loss': Array(1.6061379, dtype=float32), 'train/wer': 0.43652306734930557, 'validation/ctc_loss': Array(1.737651, dtype=float32), 'validation/wer': 0.44028114349710845, 'validation/num_examples': 5348, 'test/ctc_loss': Array(1.3081437, dtype=float32), 'test/wer': 0.38206081286941684, 'test/num_examples': 2472, 'score': 1456.1565909385681, 'total_duration': 1802.423523426056, 'accumulated_submission_time': 1456.1565909385681, 'accumulated_eval_time': 346.16198992729187, 'accumulated_logging_time': 0.031929969787597656}
I0212 13:58:00.824768 139647035000576 logging_writer.py:48] [1695] accumulated_eval_time=346.161990, accumulated_logging_time=0.031930, accumulated_submission_time=1456.156591, global_step=1695, preemption_count=0, score=1456.156591, test/ctc_loss=1.3081437349319458, test/num_examples=2472, test/wer=0.382061, total_duration=1802.423523, train/ctc_loss=1.6061378717422485, train/wer=0.436523, validation/ctc_loss=1.737650990486145, validation/num_examples=5348, validation/wer=0.440281
I0212 13:58:05.622699 139647026607872 logging_writer.py:48] [1700] global_step=1700, grad_norm=3.396299123764038, loss=2.257751703262329
I0212 13:59:21.551188 139647035000576 logging_writer.py:48] [1800] global_step=1800, grad_norm=3.3218822479248047, loss=2.3230345249176025
I0212 14:00:37.519082 139647026607872 logging_writer.py:48] [1900] global_step=1900, grad_norm=2.119739294052124, loss=2.173126220703125
I0212 14:02:00.381116 139647035000576 logging_writer.py:48] [2000] global_step=2000, grad_norm=3.871584177017212, loss=2.211610794067383
I0212 14:03:31.056921 139647035000576 logging_writer.py:48] [2100] global_step=2100, grad_norm=2.9306812286376953, loss=2.1886117458343506
I0212 14:04:52.170099 139647026607872 logging_writer.py:48] [2200] global_step=2200, grad_norm=3.157576084136963, loss=2.1491827964782715
I0212 14:06:10.141082 139647035000576 logging_writer.py:48] [2300] global_step=2300, grad_norm=2.5049469470977783, loss=2.099112033843994
I0212 14:07:31.456849 139647026607872 logging_writer.py:48] [2400] global_step=2400, grad_norm=3.0561492443084717, loss=2.157116174697876
I0212 14:08:57.270223 139647035000576 logging_writer.py:48] [2500] global_step=2500, grad_norm=3.0273892879486084, loss=2.091714382171631
I0212 14:10:26.637814 139647026607872 logging_writer.py:48] [2600] global_step=2600, grad_norm=3.3064379692077637, loss=2.1035208702087402
I0212 14:11:54.924927 139647035000576 logging_writer.py:48] [2700] global_step=2700, grad_norm=5.218916893005371, loss=2.0834007263183594
I0212 14:13:25.554512 139647026607872 logging_writer.py:48] [2800] global_step=2800, grad_norm=3.2598254680633545, loss=2.1113736629486084
I0212 14:14:59.052368 139647035000576 logging_writer.py:48] [2900] global_step=2900, grad_norm=3.2173750400543213, loss=2.072175979614258
I0212 14:16:31.733586 139647026607872 logging_writer.py:48] [3000] global_step=3000, grad_norm=3.621152639389038, loss=2.099534511566162
I0212 14:18:00.829236 139647035000576 logging_writer.py:48] [3100] global_step=3100, grad_norm=2.791844129562378, loss=2.006990671157837
I0212 14:19:17.915208 139647026607872 logging_writer.py:48] [3200] global_step=3200, grad_norm=2.0760703086853027, loss=2.0424141883850098
I0212 14:20:36.714234 139647035000576 logging_writer.py:48] [3300] global_step=3300, grad_norm=3.429320812225342, loss=2.040874719619751
I0212 14:21:57.872902 139647026607872 logging_writer.py:48] [3400] global_step=3400, grad_norm=4.663326740264893, loss=1.9757094383239746
I0212 14:22:01.699933 139803787056960 spec.py:321] Evaluating on the training split.
I0212 14:22:55.177591 139803787056960 spec.py:333] Evaluating on the validation split.
I0212 14:23:49.084921 139803787056960 spec.py:349] Evaluating on the test split.
I0212 14:24:16.047840 139803787056960 submission_runner.py:408] Time since start: 3377.69s, 	Step: 3406, 	{'train/ctc_loss': Array(0.86962605, dtype=float32), 'train/wer': 0.25982874715626536, 'validation/ctc_loss': Array(1.0091406, dtype=float32), 'validation/wer': 0.2794732421290441, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.67142403, dtype=float32), 'test/wer': 0.20847805333820812, 'test/num_examples': 2472, 'score': 2896.9300112724304, 'total_duration': 3377.6903870105743, 'accumulated_submission_time': 2896.9300112724304, 'accumulated_eval_time': 480.506609916687, 'accumulated_logging_time': 0.09938645362854004}
I0212 14:24:16.074007 139648048760576 logging_writer.py:48] [3406] accumulated_eval_time=480.506610, accumulated_logging_time=0.099386, accumulated_submission_time=2896.930011, global_step=3406, preemption_count=0, score=2896.930011, test/ctc_loss=0.6714240312576294, test/num_examples=2472, test/wer=0.208478, total_duration=3377.690387, train/ctc_loss=0.8696260452270508, train/wer=0.259829, validation/ctc_loss=1.0091406106948853, validation/num_examples=5348, validation/wer=0.279473
I0212 14:25:27.959434 139648040367872 logging_writer.py:48] [3500] global_step=3500, grad_norm=2.1038658618927, loss=1.9123777151107788
I0212 14:26:43.853012 139648048760576 logging_writer.py:48] [3600] global_step=3600, grad_norm=3.377941131591797, loss=2.055893898010254
I0212 14:28:07.914911 139648040367872 logging_writer.py:48] [3700] global_step=3700, grad_norm=2.5580086708068848, loss=1.969295859336853
I0212 14:29:34.701359 139648048760576 logging_writer.py:48] [3800] global_step=3800, grad_norm=7.554489612579346, loss=2.0244596004486084
I0212 14:31:04.924249 139648040367872 logging_writer.py:48] [3900] global_step=3900, grad_norm=2.882530927658081, loss=1.9754226207733154
I0212 14:32:35.209229 139648048760576 logging_writer.py:48] [4000] global_step=4000, grad_norm=2.1680543422698975, loss=1.9773935079574585
I0212 14:34:04.211525 139648040367872 logging_writer.py:48] [4100] global_step=4100, grad_norm=2.019540309906006, loss=1.99569833278656
I0212 14:35:27.807816 139647721080576 logging_writer.py:48] [4200] global_step=4200, grad_norm=2.6503946781158447, loss=2.0271201133728027
I0212 14:36:46.780975 139647712687872 logging_writer.py:48] [4300] global_step=4300, grad_norm=2.162365436553955, loss=2.0016579627990723
I0212 14:38:07.898696 139647721080576 logging_writer.py:48] [4400] global_step=4400, grad_norm=2.481666326522827, loss=1.8868052959442139
I0212 14:39:30.177626 139647712687872 logging_writer.py:48] [4500] global_step=4500, grad_norm=2.2183477878570557, loss=1.9367763996124268
I0212 14:40:59.082783 139647721080576 logging_writer.py:48] [4600] global_step=4600, grad_norm=3.51434588432312, loss=1.993239164352417
I0212 14:42:29.476399 139647712687872 logging_writer.py:48] [4700] global_step=4700, grad_norm=2.695077896118164, loss=1.911024570465088
I0212 14:44:02.021116 139647721080576 logging_writer.py:48] [4800] global_step=4800, grad_norm=2.6572446823120117, loss=2.0211095809936523
I0212 14:45:32.462379 139647712687872 logging_writer.py:48] [4900] global_step=4900, grad_norm=4.46311092376709, loss=2.0300216674804688
I0212 14:47:06.081898 139647721080576 logging_writer.py:48] [5000] global_step=5000, grad_norm=4.09312629699707, loss=1.9314597845077515
I0212 14:48:17.352464 139803787056960 spec.py:321] Evaluating on the training split.
I0212 14:49:11.725255 139803787056960 spec.py:333] Evaluating on the validation split.
I0212 14:50:04.085144 139803787056960 spec.py:349] Evaluating on the test split.
I0212 14:50:31.427679 139803787056960 submission_runner.py:408] Time since start: 4953.07s, 	Step: 5080, 	{'train/ctc_loss': Array(0.8732126, dtype=float32), 'train/wer': 0.2650956794108236, 'validation/ctc_loss': Array(0.9360504, dtype=float32), 'validation/wer': 0.26124525715168423, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6041575, dtype=float32), 'test/wer': 0.18893831373265899, 'test/num_examples': 2472, 'score': 4338.122195243835, 'total_duration': 4953.06637597084, 'accumulated_submission_time': 4338.122195243835, 'accumulated_eval_time': 614.574684381485, 'accumulated_logging_time': 0.14055585861206055}
I0212 14:50:31.465145 139647137400576 logging_writer.py:48] [5080] accumulated_eval_time=614.574684, accumulated_logging_time=0.140556, accumulated_submission_time=4338.122195, global_step=5080, preemption_count=0, score=4338.122195, test/ctc_loss=0.6041575074195862, test/num_examples=2472, test/wer=0.188938, total_duration=4953.066376, train/ctc_loss=0.8732125759124756, train/wer=0.265096, validation/ctc_loss=0.9360504150390625, validation/num_examples=5348, validation/wer=0.261245
I0212 14:50:47.514351 139647129007872 logging_writer.py:48] [5100] global_step=5100, grad_norm=2.436960220336914, loss=1.9691998958587646
I0212 14:52:08.072215 139647137400576 logging_writer.py:48] [5200] global_step=5200, grad_norm=1.9215866327285767, loss=2.014857530593872
I0212 14:53:28.031364 139647129007872 logging_writer.py:48] [5300] global_step=5300, grad_norm=2.540933847427368, loss=1.9567718505859375
I0212 14:54:49.588748 139647137400576 logging_writer.py:48] [5400] global_step=5400, grad_norm=4.290652275085449, loss=1.952436089515686
I0212 14:56:10.949581 139647129007872 logging_writer.py:48] [5500] global_step=5500, grad_norm=3.300837755203247, loss=1.9937553405761719
I0212 14:57:37.566244 139647137400576 logging_writer.py:48] [5600] global_step=5600, grad_norm=2.5136847496032715, loss=1.8549352884292603
I0212 14:59:07.857828 139647129007872 logging_writer.py:48] [5700] global_step=5700, grad_norm=3.1703591346740723, loss=1.8779999017715454
I0212 15:00:36.373326 139647137400576 logging_writer.py:48] [5800] global_step=5800, grad_norm=1.7713581323623657, loss=1.9921120405197144
I0212 15:02:06.726801 139647129007872 logging_writer.py:48] [5900] global_step=5900, grad_norm=1.766378402709961, loss=1.931836485862732
I0212 15:03:37.992211 139647137400576 logging_writer.py:48] [6000] global_step=6000, grad_norm=2.771909713745117, loss=1.912480354309082
I0212 15:05:07.521899 139647129007872 logging_writer.py:48] [6100] global_step=6100, grad_norm=4.20241641998291, loss=1.956581473350525
I0212 15:06:38.668291 139647137400576 logging_writer.py:48] [6200] global_step=6200, grad_norm=5.840641975402832, loss=1.9996557235717773
I0212 15:07:55.021839 139647129007872 logging_writer.py:48] [6300] global_step=6300, grad_norm=4.785707473754883, loss=1.930148959159851
I0212 15:09:16.746425 139647137400576 logging_writer.py:48] [6400] global_step=6400, grad_norm=3.437028169631958, loss=1.8669109344482422
I0212 15:10:36.981633 139647129007872 logging_writer.py:48] [6500] global_step=6500, grad_norm=4.253653526306152, loss=1.8936902284622192
I0212 15:12:01.191686 139647137400576 logging_writer.py:48] [6600] global_step=6600, grad_norm=5.16156005859375, loss=1.9342905282974243
I0212 15:13:29.104292 139647129007872 logging_writer.py:48] [6700] global_step=6700, grad_norm=4.453255653381348, loss=2.125277280807495
I0212 15:14:31.912709 139803787056960 spec.py:321] Evaluating on the training split.
I0212 15:15:26.429355 139803787056960 spec.py:333] Evaluating on the validation split.
I0212 15:16:18.949161 139803787056960 spec.py:349] Evaluating on the test split.
I0212 15:16:46.460179 139803787056960 submission_runner.py:408] Time since start: 6528.10s, 	Step: 6770, 	{'train/ctc_loss': Array(0.7836887, dtype=float32), 'train/wer': 0.23927822214244848, 'validation/ctc_loss': Array(0.9076595, dtype=float32), 'validation/wer': 0.25304845670370835, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5918397, dtype=float32), 'test/wer': 0.18637905469908395, 'test/num_examples': 2472, 'score': 5778.4819252491, 'total_duration': 6528.09968495369, 'accumulated_submission_time': 5778.4819252491, 'accumulated_eval_time': 749.1158313751221, 'accumulated_logging_time': 0.19507622718811035}
I0212 15:16:46.496526 139647137400576 logging_writer.py:48] [6770] accumulated_eval_time=749.115831, accumulated_logging_time=0.195076, accumulated_submission_time=5778.481925, global_step=6770, preemption_count=0, score=5778.481925, test/ctc_loss=0.5918396711349487, test/num_examples=2472, test/wer=0.186379, total_duration=6528.099685, train/ctc_loss=0.7836887240409851, train/wer=0.239278, validation/ctc_loss=0.9076594710350037, validation/num_examples=5348, validation/wer=0.253048
I0212 15:17:10.233062 139647129007872 logging_writer.py:48] [6800] global_step=6800, grad_norm=3.15925931930542, loss=1.8930132389068604
I0212 15:18:25.558017 139647137400576 logging_writer.py:48] [6900] global_step=6900, grad_norm=1.8556562662124634, loss=1.812612771987915
I0212 15:19:43.350348 139647129007872 logging_writer.py:48] [7000] global_step=7000, grad_norm=3.386796236038208, loss=1.9742915630340576
I0212 15:21:12.734741 139647137400576 logging_writer.py:48] [7100] global_step=7100, grad_norm=3.2327373027801514, loss=1.887396216392517
I0212 15:22:41.904540 139647129007872 logging_writer.py:48] [7200] global_step=7200, grad_norm=2.3232266902923584, loss=1.8816757202148438
I0212 15:24:03.441888 139648048760576 logging_writer.py:48] [7300] global_step=7300, grad_norm=2.5411159992218018, loss=1.8889260292053223
I0212 15:25:20.825167 139648040367872 logging_writer.py:48] [7400] global_step=7400, grad_norm=4.617328643798828, loss=1.8953841924667358
I0212 15:26:38.227954 139648048760576 logging_writer.py:48] [7500] global_step=7500, grad_norm=4.9083051681518555, loss=1.924910068511963
I0212 15:28:01.792454 139648040367872 logging_writer.py:48] [7600] global_step=7600, grad_norm=2.44425630569458, loss=1.811381220817566
I0212 15:29:29.212370 139648048760576 logging_writer.py:48] [7700] global_step=7700, grad_norm=2.9560887813568115, loss=1.858917236328125
I0212 15:31:02.713355 139648040367872 logging_writer.py:48] [7800] global_step=7800, grad_norm=2.252519130706787, loss=1.8714449405670166
I0212 15:32:32.310536 139648048760576 logging_writer.py:48] [7900] global_step=7900, grad_norm=1.9419220685958862, loss=1.8690977096557617
I0212 15:34:04.194939 139648040367872 logging_writer.py:48] [8000] global_step=8000, grad_norm=3.0291409492492676, loss=1.8917183876037598
I0212 15:35:32.959089 139648048760576 logging_writer.py:48] [8100] global_step=8100, grad_norm=2.1451964378356934, loss=1.9055595397949219
I0212 15:37:05.304026 139648040367872 logging_writer.py:48] [8200] global_step=8200, grad_norm=2.684016466140747, loss=1.9270998239517212
I0212 15:38:32.320055 139647137400576 logging_writer.py:48] [8300] global_step=8300, grad_norm=2.5281178951263428, loss=1.884741187095642
I0212 15:39:50.376180 139647129007872 logging_writer.py:48] [8400] global_step=8400, grad_norm=1.5770140886306763, loss=1.860801339149475
I0212 15:40:47.439688 139803787056960 spec.py:321] Evaluating on the training split.
I0212 15:41:42.932163 139803787056960 spec.py:333] Evaluating on the validation split.
I0212 15:42:36.278486 139803787056960 spec.py:349] Evaluating on the test split.
I0212 15:43:03.376133 139803787056960 submission_runner.py:408] Time since start: 8105.02s, 	Step: 8473, 	{'train/ctc_loss': Array(0.7977896, dtype=float32), 'train/wer': 0.23808503154608807, 'validation/ctc_loss': Array(0.8691856, dtype=float32), 'validation/wer': 0.24392480956196838, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5595584, dtype=float32), 'test/wer': 0.17303434688115696, 'test/num_examples': 2472, 'score': 7219.33767747879, 'total_duration': 8105.016730308533, 'accumulated_submission_time': 7219.33767747879, 'accumulated_eval_time': 885.0470359325409, 'accumulated_logging_time': 0.2474684715270996}
I0212 15:43:03.413762 139647137400576 logging_writer.py:48] [8473] accumulated_eval_time=885.047036, accumulated_logging_time=0.247468, accumulated_submission_time=7219.337677, global_step=8473, preemption_count=0, score=7219.337677, test/ctc_loss=0.5595583915710449, test/num_examples=2472, test/wer=0.173034, total_duration=8105.016730, train/ctc_loss=0.7977895736694336, train/wer=0.238085, validation/ctc_loss=0.8691856265068054, validation/num_examples=5348, validation/wer=0.243925
I0212 15:43:24.969651 139647129007872 logging_writer.py:48] [8500] global_step=8500, grad_norm=2.089789628982544, loss=1.797385334968567
I0212 15:44:40.762041 139647137400576 logging_writer.py:48] [8600] global_step=8600, grad_norm=2.8695497512817383, loss=1.8721919059753418
I0212 15:45:56.046115 139647129007872 logging_writer.py:48] [8700] global_step=8700, grad_norm=3.7681760787963867, loss=1.8186542987823486
I0212 15:47:23.412134 139647137400576 logging_writer.py:48] [8800] global_step=8800, grad_norm=2.706364631652832, loss=1.8327714204788208
I0212 15:48:53.179115 139647129007872 logging_writer.py:48] [8900] global_step=8900, grad_norm=4.499279975891113, loss=1.876714825630188
I0212 15:50:20.310663 139647137400576 logging_writer.py:48] [9000] global_step=9000, grad_norm=2.330402135848999, loss=1.8711445331573486
I0212 15:51:51.527573 139647129007872 logging_writer.py:48] [9100] global_step=9100, grad_norm=2.2568514347076416, loss=1.884703278541565
I0212 15:53:23.025262 139647137400576 logging_writer.py:48] [9200] global_step=9200, grad_norm=2.0019712448120117, loss=1.8311009407043457
I0212 15:54:52.970562 139648048760576 logging_writer.py:48] [9300] global_step=9300, grad_norm=2.0857152938842773, loss=1.820193886756897
I0212 15:56:09.234082 139648040367872 logging_writer.py:48] [9400] global_step=9400, grad_norm=2.5579731464385986, loss=1.8190525770187378
I0212 15:57:28.549303 139648048760576 logging_writer.py:48] [9500] global_step=9500, grad_norm=2.738819122314453, loss=1.8563117980957031
I0212 15:58:50.564641 139648040367872 logging_writer.py:48] [9600] global_step=9600, grad_norm=3.0206639766693115, loss=1.9356625080108643
I0212 16:00:17.528010 139648048760576 logging_writer.py:48] [9700] global_step=9700, grad_norm=2.52601957321167, loss=1.821936011314392
I0212 16:01:48.138192 139648040367872 logging_writer.py:48] [9800] global_step=9800, grad_norm=2.9519708156585693, loss=1.8488517999649048
I0212 16:03:19.196973 139648048760576 logging_writer.py:48] [9900] global_step=9900, grad_norm=2.277923822402954, loss=1.8450896739959717
I0212 16:04:48.663716 139648040367872 logging_writer.py:48] [10000] global_step=10000, grad_norm=3.19818377494812, loss=1.84511399269104
I0212 16:06:15.248385 139648048760576 logging_writer.py:48] [10100] global_step=10100, grad_norm=4.358137607574463, loss=1.8549017906188965
I0212 16:07:03.597903 139803787056960 spec.py:321] Evaluating on the training split.
I0212 16:07:58.499584 139803787056960 spec.py:333] Evaluating on the validation split.
I0212 16:08:51.143151 139803787056960 spec.py:349] Evaluating on the test split.
I0212 16:09:18.045016 139803787056960 submission_runner.py:408] Time since start: 9679.68s, 	Step: 10157, 	{'train/ctc_loss': Array(0.70206696, dtype=float32), 'train/wer': 0.21623922703828005, 'validation/ctc_loss': Array(0.8392556, dtype=float32), 'validation/wer': 0.236529345317976, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5371239, dtype=float32), 'test/wer': 0.1710641236568968, 'test/num_examples': 2472, 'score': 8659.357873678207, 'total_duration': 9679.683911561966, 'accumulated_submission_time': 8659.357873678207, 'accumulated_eval_time': 1019.487206697464, 'accumulated_logging_time': 0.3779764175415039}
I0212 16:09:18.080564 139647618680576 logging_writer.py:48] [10157] accumulated_eval_time=1019.487207, accumulated_logging_time=0.377976, accumulated_submission_time=8659.357874, global_step=10157, preemption_count=0, score=8659.357874, test/ctc_loss=0.5371239185333252, test/num_examples=2472, test/wer=0.171064, total_duration=9679.683912, train/ctc_loss=0.702066957950592, train/wer=0.216239, validation/ctc_loss=0.8392555713653564, validation/num_examples=5348, validation/wer=0.236529
I0212 16:09:51.151478 139647610287872 logging_writer.py:48] [10200] global_step=10200, grad_norm=3.33205509185791, loss=1.8071492910385132
I0212 16:11:11.320685 139647618680576 logging_writer.py:48] [10300] global_step=10300, grad_norm=2.412304401397705, loss=1.7631891965866089
I0212 16:12:27.885243 139647610287872 logging_writer.py:48] [10400] global_step=10400, grad_norm=4.25335168838501, loss=1.8309993743896484
I0212 16:13:45.420372 139647618680576 logging_writer.py:48] [10500] global_step=10500, grad_norm=4.337802886962891, loss=1.795422077178955
I0212 16:15:06.870751 139647610287872 logging_writer.py:48] [10600] global_step=10600, grad_norm=2.1242613792419434, loss=1.8167147636413574
I0212 16:16:29.750232 139647618680576 logging_writer.py:48] [10700] global_step=10700, grad_norm=2.3655247688293457, loss=1.799721121788025
I0212 16:17:57.142626 139647610287872 logging_writer.py:48] [10800] global_step=10800, grad_norm=2.328446388244629, loss=1.8221803903579712
I0212 16:19:24.082978 139647618680576 logging_writer.py:48] [10900] global_step=10900, grad_norm=2.449197292327881, loss=1.9007763862609863
I0212 16:20:53.840065 139647610287872 logging_writer.py:48] [11000] global_step=11000, grad_norm=5.048068046569824, loss=1.7796802520751953
I0212 16:22:27.766907 139647618680576 logging_writer.py:48] [11100] global_step=11100, grad_norm=4.0536651611328125, loss=1.8508265018463135
I0212 16:23:57.547712 139647610287872 logging_writer.py:48] [11200] global_step=11200, grad_norm=2.3374698162078857, loss=1.8166719675064087
I0212 16:25:31.234295 139647618680576 logging_writer.py:48] [11300] global_step=11300, grad_norm=2.4591856002807617, loss=1.8151458501815796
I0212 16:26:54.010442 139647618680576 logging_writer.py:48] [11400] global_step=11400, grad_norm=4.543449878692627, loss=1.8405941724777222
I0212 16:28:12.728101 139647610287872 logging_writer.py:48] [11500] global_step=11500, grad_norm=2.111583709716797, loss=1.8284449577331543
I0212 16:29:33.636486 139647618680576 logging_writer.py:48] [11600] global_step=11600, grad_norm=4.110109329223633, loss=1.8189671039581299
I0212 16:31:00.320964 139647610287872 logging_writer.py:48] [11700] global_step=11700, grad_norm=2.1536829471588135, loss=1.8063138723373413
I0212 16:32:30.126057 139647618680576 logging_writer.py:48] [11800] global_step=11800, grad_norm=3.161432981491089, loss=1.808497428894043
I0212 16:33:18.822707 139803787056960 spec.py:321] Evaluating on the training split.
I0212 16:34:12.855045 139803787056960 spec.py:333] Evaluating on the validation split.
I0212 16:35:05.290356 139803787056960 spec.py:349] Evaluating on the test split.
I0212 16:35:31.916449 139803787056960 submission_runner.py:408] Time since start: 11253.56s, 	Step: 11856, 	{'train/ctc_loss': Array(0.71111774, dtype=float32), 'train/wer': 0.221663515637457, 'validation/ctc_loss': Array(0.8253648, dtype=float32), 'validation/wer': 0.23265782944089905, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5218247, dtype=float32), 'test/wer': 0.16379257814880263, 'test/num_examples': 2472, 'score': 10100.010554075241, 'total_duration': 11253.55560541153, 'accumulated_submission_time': 10100.010554075241, 'accumulated_eval_time': 1152.574282169342, 'accumulated_logging_time': 0.43176698684692383}
I0212 16:35:31.952186 139647035000576 logging_writer.py:48] [11856] accumulated_eval_time=1152.574282, accumulated_logging_time=0.431767, accumulated_submission_time=10100.010554, global_step=11856, preemption_count=0, score=10100.010554, test/ctc_loss=0.5218247175216675, test/num_examples=2472, test/wer=0.163793, total_duration=11253.555605, train/ctc_loss=0.7111177444458008, train/wer=0.221664, validation/ctc_loss=0.8253648281097412, validation/num_examples=5348, validation/wer=0.232658
I0212 16:36:06.510174 139647026607872 logging_writer.py:48] [11900] global_step=11900, grad_norm=2.8193869590759277, loss=1.8068979978561401
I0212 16:37:21.755373 139647035000576 logging_writer.py:48] [12000] global_step=12000, grad_norm=2.9892337322235107, loss=1.7787340879440308
I0212 16:38:44.177770 139647026607872 logging_writer.py:48] [12100] global_step=12100, grad_norm=2.3531494140625, loss=1.7891217470169067
I0212 16:40:14.734617 139647035000576 logging_writer.py:48] [12200] global_step=12200, grad_norm=2.773684024810791, loss=1.775400996208191
I0212 16:41:42.178565 139647026607872 logging_writer.py:48] [12300] global_step=12300, grad_norm=2.035210371017456, loss=1.8008966445922852
I0212 16:43:12.744316 139647035000576 logging_writer.py:48] [12400] global_step=12400, grad_norm=2.5095272064208984, loss=1.7779921293258667
I0212 16:44:30.159628 139647026607872 logging_writer.py:48] [12500] global_step=12500, grad_norm=2.590230941772461, loss=1.834383249282837
I0212 16:45:48.167009 139647035000576 logging_writer.py:48] [12600] global_step=12600, grad_norm=4.017258167266846, loss=1.8219817876815796
I0212 16:47:07.643185 139647026607872 logging_writer.py:48] [12700] global_step=12700, grad_norm=3.8225088119506836, loss=1.742414116859436
I0212 16:48:34.870048 139647035000576 logging_writer.py:48] [12800] global_step=12800, grad_norm=2.1507747173309326, loss=1.7672419548034668
I0212 16:50:05.677541 139647026607872 logging_writer.py:48] [12900] global_step=12900, grad_norm=2.2647788524627686, loss=1.7676081657409668
I0212 16:51:35.697515 139647035000576 logging_writer.py:48] [13000] global_step=13000, grad_norm=2.6403491497039795, loss=1.8280185461044312
I0212 16:53:04.847981 139647026607872 logging_writer.py:48] [13100] global_step=13100, grad_norm=2.2313380241394043, loss=1.8094719648361206
I0212 16:54:34.218260 139647035000576 logging_writer.py:48] [13200] global_step=13200, grad_norm=2.6028778553009033, loss=1.8161453008651733
I0212 16:56:02.653128 139647026607872 logging_writer.py:48] [13300] global_step=13300, grad_norm=2.578352451324463, loss=1.7676653861999512
I0212 16:57:37.134747 139647035000576 logging_writer.py:48] [13400] global_step=13400, grad_norm=2.1461944580078125, loss=1.7526235580444336
I0212 16:58:54.890463 139647026607872 logging_writer.py:48] [13500] global_step=13500, grad_norm=3.5234029293060303, loss=1.8111408948898315
I0212 16:59:32.103215 139803787056960 spec.py:321] Evaluating on the training split.
I0212 17:00:26.502126 139803787056960 spec.py:333] Evaluating on the validation split.
I0212 17:01:18.573984 139803787056960 spec.py:349] Evaluating on the test split.
I0212 17:01:46.227972 139803787056960 submission_runner.py:408] Time since start: 12827.87s, 	Step: 13549, 	{'train/ctc_loss': Array(0.66694856, dtype=float32), 'train/wer': 0.20693933669420195, 'validation/ctc_loss': Array(0.79508895, dtype=float32), 'validation/wer': 0.22329281597265802, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.49158484, dtype=float32), 'test/wer': 0.15511953364613165, 'test/num_examples': 2472, 'score': 11540.073407173157, 'total_duration': 12827.867657899857, 'accumulated_submission_time': 11540.073407173157, 'accumulated_eval_time': 1286.6929433345795, 'accumulated_logging_time': 0.48450231552124023}
I0212 17:01:46.265214 139648048760576 logging_writer.py:48] [13549] accumulated_eval_time=1286.692943, accumulated_logging_time=0.484502, accumulated_submission_time=11540.073407, global_step=13549, preemption_count=0, score=11540.073407, test/ctc_loss=0.491584837436676, test/num_examples=2472, test/wer=0.155120, total_duration=12827.867658, train/ctc_loss=0.6669485569000244, train/wer=0.206939, validation/ctc_loss=0.7950889468193054, validation/num_examples=5348, validation/wer=0.223293
I0212 17:02:25.374643 139648040367872 logging_writer.py:48] [13600] global_step=13600, grad_norm=2.6519038677215576, loss=1.7164021730422974
I0212 17:03:40.719366 139648048760576 logging_writer.py:48] [13700] global_step=13700, grad_norm=2.0720484256744385, loss=1.7612330913543701
I0212 17:04:55.840398 139648040367872 logging_writer.py:48] [13800] global_step=13800, grad_norm=3.4036221504211426, loss=1.7848503589630127
I0212 17:06:18.276580 139648048760576 logging_writer.py:48] [13900] global_step=13900, grad_norm=1.9573110342025757, loss=1.7726268768310547
I0212 17:07:46.480512 139648040367872 logging_writer.py:48] [14000] global_step=14000, grad_norm=2.841176748275757, loss=1.8231502771377563
I0212 17:09:17.142590 139648048760576 logging_writer.py:48] [14100] global_step=14100, grad_norm=2.238607168197632, loss=1.754382610321045
I0212 17:10:43.789348 139648040367872 logging_writer.py:48] [14200] global_step=14200, grad_norm=1.824169635772705, loss=1.7291104793548584
I0212 17:12:12.063925 139648048760576 logging_writer.py:48] [14300] global_step=14300, grad_norm=3.1120059490203857, loss=1.7344367504119873
I0212 17:13:41.659148 139648040367872 logging_writer.py:48] [14400] global_step=14400, grad_norm=3.895988702774048, loss=1.7148308753967285
I0212 17:15:03.370069 139647721080576 logging_writer.py:48] [14500] global_step=14500, grad_norm=2.963336229324341, loss=1.7860337495803833
I0212 17:16:18.556834 139647712687872 logging_writer.py:48] [14600] global_step=14600, grad_norm=3.2313730716705322, loss=1.739124059677124
I0212 17:17:37.220392 139647721080576 logging_writer.py:48] [14700] global_step=14700, grad_norm=3.0643515586853027, loss=1.7578833103179932
I0212 17:19:00.125896 139647712687872 logging_writer.py:48] [14800] global_step=14800, grad_norm=4.5912699699401855, loss=1.725285530090332
I0212 17:20:25.299455 139647721080576 logging_writer.py:48] [14900] global_step=14900, grad_norm=3.893186092376709, loss=1.7533584833145142
I0212 17:21:54.772378 139647712687872 logging_writer.py:48] [15000] global_step=15000, grad_norm=2.9617249965667725, loss=1.803337812423706
I0212 17:23:24.419604 139647721080576 logging_writer.py:48] [15100] global_step=15100, grad_norm=3.280235528945923, loss=1.7967708110809326
I0212 17:24:51.604408 139647712687872 logging_writer.py:48] [15200] global_step=15200, grad_norm=3.244460344314575, loss=1.6981050968170166
I0212 17:25:47.679483 139803787056960 spec.py:321] Evaluating on the training split.
I0212 17:26:52.459302 139803787056960 spec.py:333] Evaluating on the validation split.
I0212 17:27:46.305417 139803787056960 spec.py:349] Evaluating on the test split.
I0212 17:28:13.833821 139803787056960 submission_runner.py:408] Time since start: 14415.47s, 	Step: 15263, 	{'train/ctc_loss': Array(0.44968167, dtype=float32), 'train/wer': 0.14862021894488905, 'validation/ctc_loss': Array(0.77568215, dtype=float32), 'validation/wer': 0.2198847234424631, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.47875628, dtype=float32), 'test/wer': 0.152722767249609, 'test/num_examples': 2472, 'score': 12981.39877486229, 'total_duration': 14415.474444389343, 'accumulated_submission_time': 12981.39877486229, 'accumulated_eval_time': 1432.8420944213867, 'accumulated_logging_time': 0.5385165214538574}
I0212 17:28:13.872242 139647137400576 logging_writer.py:48] [15263] accumulated_eval_time=1432.842094, accumulated_logging_time=0.538517, accumulated_submission_time=12981.398775, global_step=15263, preemption_count=0, score=12981.398775, test/ctc_loss=0.47875627875328064, test/num_examples=2472, test/wer=0.152723, total_duration=14415.474444, train/ctc_loss=0.44968166947364807, train/wer=0.148620, validation/ctc_loss=0.7756821513175964, validation/num_examples=5348, validation/wer=0.219885
I0212 17:28:42.541021 139647129007872 logging_writer.py:48] [15300] global_step=15300, grad_norm=3.491558790206909, loss=1.7681219577789307
I0212 17:29:57.795643 139647137400576 logging_writer.py:48] [15400] global_step=15400, grad_norm=3.068593740463257, loss=1.8042970895767212
I0212 17:31:17.959116 139648048760576 logging_writer.py:48] [15500] global_step=15500, grad_norm=4.144041538238525, loss=1.6934490203857422
I0212 17:32:35.056391 139648040367872 logging_writer.py:48] [15600] global_step=15600, grad_norm=2.9124979972839355, loss=1.6643949747085571
I0212 17:33:54.836842 139648048760576 logging_writer.py:48] [15700] global_step=15700, grad_norm=2.6758944988250732, loss=1.7780653238296509
I0212 17:35:16.557048 139648040367872 logging_writer.py:48] [15800] global_step=15800, grad_norm=6.151065349578857, loss=1.7838094234466553
I0212 17:36:42.286918 139648048760576 logging_writer.py:48] [15900] global_step=15900, grad_norm=2.5304737091064453, loss=1.6955469846725464
I0212 17:38:11.750303 139648040367872 logging_writer.py:48] [16000] global_step=16000, grad_norm=1.7402881383895874, loss=1.757046103477478
I0212 17:39:40.535242 139648048760576 logging_writer.py:48] [16100] global_step=16100, grad_norm=3.888204336166382, loss=1.729162335395813
I0212 17:41:12.142717 139648040367872 logging_writer.py:48] [16200] global_step=16200, grad_norm=2.678356885910034, loss=1.6735934019088745
I0212 17:42:41.712145 139648048760576 logging_writer.py:48] [16300] global_step=16300, grad_norm=5.708896160125732, loss=1.7320142984390259
I0212 17:44:13.541694 139648040367872 logging_writer.py:48] [16400] global_step=16400, grad_norm=2.958174705505371, loss=1.7280060052871704
I0212 17:45:44.225807 139648048760576 logging_writer.py:48] [16500] global_step=16500, grad_norm=1.7571090459823608, loss=1.7320579290390015
I0212 17:47:00.086843 139648040367872 logging_writer.py:48] [16600] global_step=16600, grad_norm=2.46683406829834, loss=1.7621320486068726
I0212 17:48:19.794208 139648048760576 logging_writer.py:48] [16700] global_step=16700, grad_norm=2.3116543292999268, loss=1.7247483730316162
I0212 17:49:39.879890 139648040367872 logging_writer.py:48] [16800] global_step=16800, grad_norm=2.7232885360717773, loss=1.8015761375427246
I0212 17:51:03.007067 139648048760576 logging_writer.py:48] [16900] global_step=16900, grad_norm=2.4042792320251465, loss=1.7334156036376953
I0212 17:52:15.116674 139803787056960 spec.py:321] Evaluating on the training split.
I0212 17:53:12.385193 139803787056960 spec.py:333] Evaluating on the validation split.
I0212 17:54:08.046408 139803787056960 spec.py:349] Evaluating on the test split.
I0212 17:54:34.468999 139803787056960 submission_runner.py:408] Time since start: 15996.11s, 	Step: 16982, 	{'train/ctc_loss': Array(0.42352477, dtype=float32), 'train/wer': 0.1388208673010756, 'validation/ctc_loss': Array(0.74860513, dtype=float32), 'validation/wer': 0.2146615561369802, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.46607754, dtype=float32), 'test/wer': 0.14851827026587858, 'test/num_examples': 2472, 'score': 14422.54798579216, 'total_duration': 15996.109202861786, 'accumulated_submission_time': 14422.54798579216, 'accumulated_eval_time': 1572.188811302185, 'accumulated_logging_time': 0.5987732410430908}
I0212 17:54:34.504939 139647465080576 logging_writer.py:48] [16982] accumulated_eval_time=1572.188811, accumulated_logging_time=0.598773, accumulated_submission_time=14422.547986, global_step=16982, preemption_count=0, score=14422.547986, test/ctc_loss=0.4660775363445282, test/num_examples=2472, test/wer=0.148518, total_duration=15996.109203, train/ctc_loss=0.42352476716041565, train/wer=0.138821, validation/ctc_loss=0.7486051321029663, validation/num_examples=5348, validation/wer=0.214662
I0212 17:54:49.180152 139647456687872 logging_writer.py:48] [17000] global_step=17000, grad_norm=2.5942351818084717, loss=1.6984206438064575
I0212 17:56:04.238423 139647465080576 logging_writer.py:48] [17100] global_step=17100, grad_norm=3.1081838607788086, loss=1.77198326587677
I0212 17:57:21.638170 139647456687872 logging_writer.py:48] [17200] global_step=17200, grad_norm=2.4454524517059326, loss=1.7188979387283325
I0212 17:58:53.074134 139647465080576 logging_writer.py:48] [17300] global_step=17300, grad_norm=2.7785580158233643, loss=1.7124916315078735
I0212 18:00:23.546718 139647456687872 logging_writer.py:48] [17400] global_step=17400, grad_norm=3.089444875717163, loss=1.7987229824066162
I0212 18:01:52.802882 139647465080576 logging_writer.py:48] [17500] global_step=17500, grad_norm=2.5188698768615723, loss=1.6867693662643433
I0212 18:03:18.027727 139647465080576 logging_writer.py:48] [17600] global_step=17600, grad_norm=2.548490285873413, loss=1.698153018951416
I0212 18:04:40.072182 139647456687872 logging_writer.py:48] [17700] global_step=17700, grad_norm=1.8413214683532715, loss=1.707287073135376
I0212 18:05:58.077488 139647465080576 logging_writer.py:48] [17800] global_step=17800, grad_norm=4.534060478210449, loss=1.6872706413269043
I0212 18:07:19.613023 139647456687872 logging_writer.py:48] [17900] global_step=17900, grad_norm=2.7440261840820312, loss=1.6532001495361328
I0212 18:08:45.655861 139647465080576 logging_writer.py:48] [18000] global_step=18000, grad_norm=3.884021759033203, loss=1.763545274734497
I0212 18:10:15.629147 139647456687872 logging_writer.py:48] [18100] global_step=18100, grad_norm=1.9321303367614746, loss=1.7138861417770386
I0212 18:11:44.024604 139647465080576 logging_writer.py:48] [18200] global_step=18200, grad_norm=2.1348118782043457, loss=1.7644696235656738
I0212 18:13:13.792262 139647456687872 logging_writer.py:48] [18300] global_step=18300, grad_norm=3.596648693084717, loss=1.7549092769622803
I0212 18:14:44.271803 139647465080576 logging_writer.py:48] [18400] global_step=18400, grad_norm=2.923536777496338, loss=1.6889421939849854
I0212 18:16:13.725206 139647456687872 logging_writer.py:48] [18500] global_step=18500, grad_norm=2.3448312282562256, loss=1.6528559923171997
I0212 18:17:40.907187 139647137400576 logging_writer.py:48] [18600] global_step=18600, grad_norm=3.487884283065796, loss=1.721449375152588
I0212 18:18:34.668343 139803787056960 spec.py:321] Evaluating on the training split.
I0212 18:19:30.881004 139803787056960 spec.py:333] Evaluating on the validation split.
I0212 18:20:25.044439 139803787056960 spec.py:349] Evaluating on the test split.
I0212 18:20:52.468559 139803787056960 submission_runner.py:408] Time since start: 17574.11s, 	Step: 18668, 	{'train/ctc_loss': Array(0.41050774, dtype=float32), 'train/wer': 0.13493603394696577, 'validation/ctc_loss': Array(0.7495671, dtype=float32), 'validation/wer': 0.21057763789258233, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4562326, dtype=float32), 'test/wer': 0.14309507850425526, 'test/num_examples': 2472, 'score': 15862.62443113327, 'total_duration': 17574.10783100128, 'accumulated_submission_time': 15862.62443113327, 'accumulated_eval_time': 1709.9824848175049, 'accumulated_logging_time': 0.6509594917297363}
I0212 18:20:52.505835 139647137400576 logging_writer.py:48] [18668] accumulated_eval_time=1709.982485, accumulated_logging_time=0.650959, accumulated_submission_time=15862.624431, global_step=18668, preemption_count=0, score=15862.624431, test/ctc_loss=0.45623260736465454, test/num_examples=2472, test/wer=0.143095, total_duration=17574.107831, train/ctc_loss=0.4105077385902405, train/wer=0.134936, validation/ctc_loss=0.7495670914649963, validation/num_examples=5348, validation/wer=0.210578
I0212 18:21:17.409442 139647129007872 logging_writer.py:48] [18700] global_step=18700, grad_norm=2.399611234664917, loss=1.6118032932281494
I0212 18:22:33.124413 139647137400576 logging_writer.py:48] [18800] global_step=18800, grad_norm=2.325178384780884, loss=1.68459951877594
I0212 18:23:49.125164 139647129007872 logging_writer.py:48] [18900] global_step=18900, grad_norm=3.2093701362609863, loss=1.734899878501892
I0212 18:25:09.050686 139647137400576 logging_writer.py:48] [19000] global_step=19000, grad_norm=3.943361520767212, loss=1.6923264265060425
I0212 18:26:38.205034 139647129007872 logging_writer.py:48] [19100] global_step=19100, grad_norm=2.890267848968506, loss=1.6958156824111938
I0212 18:28:05.475266 139647137400576 logging_writer.py:48] [19200] global_step=19200, grad_norm=3.029494047164917, loss=1.6112983226776123
I0212 18:29:35.635375 139647129007872 logging_writer.py:48] [19300] global_step=19300, grad_norm=3.5870580673217773, loss=1.6922972202301025
I0212 18:31:07.893877 139647137400576 logging_writer.py:48] [19400] global_step=19400, grad_norm=2.418896198272705, loss=1.656938910484314
I0212 18:32:37.406090 139647129007872 logging_writer.py:48] [19500] global_step=19500, grad_norm=2.327988624572754, loss=1.6823514699935913
I0212 18:34:06.349836 139647465080576 logging_writer.py:48] [19600] global_step=19600, grad_norm=2.499631881713867, loss=1.6722756624221802
I0212 18:35:23.227777 139647456687872 logging_writer.py:48] [19700] global_step=19700, grad_norm=1.8594000339508057, loss=1.6819549798965454
I0212 18:36:42.374412 139647465080576 logging_writer.py:48] [19800] global_step=19800, grad_norm=2.8326148986816406, loss=1.7135220766067505
I0212 18:38:08.140664 139647456687872 logging_writer.py:48] [19900] global_step=19900, grad_norm=3.948509931564331, loss=1.6714543104171753
I0212 18:39:35.193499 139647465080576 logging_writer.py:48] [20000] global_step=20000, grad_norm=1.9961481094360352, loss=1.6657192707061768
I0212 18:41:06.868721 139647456687872 logging_writer.py:48] [20100] global_step=20100, grad_norm=1.9247266054153442, loss=1.6606365442276
I0212 18:42:35.434641 139647465080576 logging_writer.py:48] [20200] global_step=20200, grad_norm=4.3839030265808105, loss=1.7239866256713867
I0212 18:44:07.163340 139647456687872 logging_writer.py:48] [20300] global_step=20300, grad_norm=1.803037405014038, loss=1.661417007446289
I0212 18:44:52.663896 139803787056960 spec.py:321] Evaluating on the training split.
I0212 18:45:49.369005 139803787056960 spec.py:333] Evaluating on the validation split.
I0212 18:46:43.927596 139803787056960 spec.py:349] Evaluating on the test split.
I0212 18:47:10.949489 139803787056960 submission_runner.py:408] Time since start: 19152.58s, 	Step: 20356, 	{'train/ctc_loss': Array(0.380554, dtype=float32), 'train/wer': 0.1286975115825664, 'validation/ctc_loss': Array(0.72028995, dtype=float32), 'validation/wer': 0.20350077719957133, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.43282682, dtype=float32), 'test/wer': 0.1374281477870534, 'test/num_examples': 2472, 'score': 17302.69507241249, 'total_duration': 19152.583884239197, 'accumulated_submission_time': 17302.69507241249, 'accumulated_eval_time': 1848.2566344738007, 'accumulated_logging_time': 0.7045087814331055}
I0212 18:47:10.984638 139647465080576 logging_writer.py:48] [20356] accumulated_eval_time=1848.256634, accumulated_logging_time=0.704509, accumulated_submission_time=17302.695072, global_step=20356, preemption_count=0, score=17302.695072, test/ctc_loss=0.43282681703567505, test/num_examples=2472, test/wer=0.137428, total_duration=19152.583884, train/ctc_loss=0.3805539906024933, train/wer=0.128698, validation/ctc_loss=0.720289945602417, validation/num_examples=5348, validation/wer=0.203501
I0212 18:47:45.355147 139647456687872 logging_writer.py:48] [20400] global_step=20400, grad_norm=3.149635076522827, loss=1.7550569772720337
I0212 18:49:01.462277 139647465080576 logging_writer.py:48] [20500] global_step=20500, grad_norm=4.047664642333984, loss=1.7117937803268433
I0212 18:50:26.269582 139647137400576 logging_writer.py:48] [20600] global_step=20600, grad_norm=1.5811177492141724, loss=1.6346489191055298
I0212 18:51:42.716473 139647129007872 logging_writer.py:48] [20700] global_step=20700, grad_norm=3.128411054611206, loss=1.6711094379425049
I0212 18:52:59.037670 139647137400576 logging_writer.py:48] [20800] global_step=20800, grad_norm=5.630666255950928, loss=1.657379388809204
I0212 18:54:19.973795 139647129007872 logging_writer.py:48] [20900] global_step=20900, grad_norm=3.7282426357269287, loss=1.6390912532806396
I0212 18:55:44.410160 139647137400576 logging_writer.py:48] [21000] global_step=21000, grad_norm=1.8571313619613647, loss=1.739454984664917
I0212 18:57:12.256200 139647129007872 logging_writer.py:48] [21100] global_step=21100, grad_norm=2.10182785987854, loss=1.637182593345642
I0212 18:58:42.774890 139647137400576 logging_writer.py:48] [21200] global_step=21200, grad_norm=2.21217942237854, loss=1.6977275609970093
I0212 19:00:14.313868 139647129007872 logging_writer.py:48] [21300] global_step=21300, grad_norm=3.132479190826416, loss=1.6586968898773193
I0212 19:01:46.213069 139647137400576 logging_writer.py:48] [21400] global_step=21400, grad_norm=2.9754230976104736, loss=1.6158677339553833
I0212 19:03:18.488172 139647129007872 logging_writer.py:48] [21500] global_step=21500, grad_norm=2.460033893585205, loss=1.658481240272522
I0212 19:04:49.173568 139647137400576 logging_writer.py:48] [21600] global_step=21600, grad_norm=2.8959808349609375, loss=1.7086811065673828
I0212 19:06:15.285096 139647137400576 logging_writer.py:48] [21700] global_step=21700, grad_norm=3.7970428466796875, loss=1.6298320293426514
I0212 19:07:36.437501 139647129007872 logging_writer.py:48] [21800] global_step=21800, grad_norm=2.0608723163604736, loss=1.5913968086242676
I0212 19:09:00.256185 139647137400576 logging_writer.py:48] [21900] global_step=21900, grad_norm=2.496713399887085, loss=1.5916893482208252
I0212 19:10:25.349283 139647129007872 logging_writer.py:48] [22000] global_step=22000, grad_norm=3.815741539001465, loss=1.6273068189620972
I0212 19:11:11.256105 139803787056960 spec.py:321] Evaluating on the training split.
I0212 19:12:06.829993 139803787056960 spec.py:333] Evaluating on the validation split.
I0212 19:12:59.611449 139803787056960 spec.py:349] Evaluating on the test split.
I0212 19:13:27.410324 139803787056960 submission_runner.py:408] Time since start: 20729.05s, 	Step: 22053, 	{'train/ctc_loss': Array(0.3735527, dtype=float32), 'train/wer': 0.1240751796336965, 'validation/ctc_loss': Array(0.6839187, dtype=float32), 'validation/wer': 0.19496606389449395, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4144221, dtype=float32), 'test/wer': 0.13328458554221762, 'test/num_examples': 2472, 'score': 18742.878882169724, 'total_duration': 20729.05018377304, 'accumulated_submission_time': 18742.878882169724, 'accumulated_eval_time': 1984.4048948287964, 'accumulated_logging_time': 0.7557229995727539}
I0212 19:13:27.447391 139647137400576 logging_writer.py:48] [22053] accumulated_eval_time=1984.404895, accumulated_logging_time=0.755723, accumulated_submission_time=18742.878882, global_step=22053, preemption_count=0, score=18742.878882, test/ctc_loss=0.41442209482192993, test/num_examples=2472, test/wer=0.133285, total_duration=20729.050184, train/ctc_loss=0.37355270981788635, train/wer=0.124075, validation/ctc_loss=0.6839187145233154, validation/num_examples=5348, validation/wer=0.194966
I0212 19:14:03.609977 139647129007872 logging_writer.py:48] [22100] global_step=22100, grad_norm=2.416926145553589, loss=1.6435929536819458
I0212 19:15:19.133887 139647137400576 logging_writer.py:48] [22200] global_step=22200, grad_norm=2.131301164627075, loss=1.6335803270339966
I0212 19:16:36.958055 139647129007872 logging_writer.py:48] [22300] global_step=22300, grad_norm=2.564190149307251, loss=1.5707886219024658
I0212 19:18:07.608297 139647137400576 logging_writer.py:48] [22400] global_step=22400, grad_norm=3.8281517028808594, loss=1.6579093933105469
I0212 19:19:38.643224 139647129007872 logging_writer.py:48] [22500] global_step=22500, grad_norm=3.6398003101348877, loss=1.6041946411132812
I0212 19:21:08.317278 139647137400576 logging_writer.py:48] [22600] global_step=22600, grad_norm=2.5088891983032227, loss=1.6142661571502686
I0212 19:22:36.915681 139647137400576 logging_writer.py:48] [22700] global_step=22700, grad_norm=2.209747791290283, loss=1.5778570175170898
I0212 19:23:55.834028 139647129007872 logging_writer.py:48] [22800] global_step=22800, grad_norm=2.594825029373169, loss=1.6340420246124268
I0212 19:25:14.982938 139647137400576 logging_writer.py:48] [22900] global_step=22900, grad_norm=2.620907783508301, loss=1.65923273563385
I0212 19:26:38.663268 139647129007872 logging_writer.py:48] [23000] global_step=23000, grad_norm=2.2503268718719482, loss=1.6064298152923584
I0212 19:28:06.963456 139647137400576 logging_writer.py:48] [23100] global_step=23100, grad_norm=2.42309832572937, loss=1.6382722854614258
I0212 19:29:38.780193 139647129007872 logging_writer.py:48] [23200] global_step=23200, grad_norm=2.3090009689331055, loss=1.6692124605178833
I0212 19:31:06.802086 139647137400576 logging_writer.py:48] [23300] global_step=23300, grad_norm=4.16505241394043, loss=1.6472049951553345
I0212 19:32:34.296027 139647129007872 logging_writer.py:48] [23400] global_step=23400, grad_norm=4.354201316833496, loss=1.5884122848510742
I0212 19:34:05.669548 139647137400576 logging_writer.py:48] [23500] global_step=23500, grad_norm=1.7521613836288452, loss=1.6771526336669922
I0212 19:35:37.123698 139647129007872 logging_writer.py:48] [23600] global_step=23600, grad_norm=4.064580917358398, loss=1.628556489944458
I0212 19:37:08.881337 139647465080576 logging_writer.py:48] [23700] global_step=23700, grad_norm=1.9997259378433228, loss=1.6804122924804688
I0212 19:37:28.087055 139803787056960 spec.py:321] Evaluating on the training split.
I0212 19:38:25.050829 139803787056960 spec.py:333] Evaluating on the validation split.
I0212 19:39:19.571324 139803787056960 spec.py:349] Evaluating on the test split.
I0212 19:39:48.046580 139803787056960 submission_runner.py:408] Time since start: 22309.69s, 	Step: 23726, 	{'train/ctc_loss': Array(0.34353802, dtype=float32), 'train/wer': 0.11765667258895292, 'validation/ctc_loss': Array(0.66541904, dtype=float32), 'validation/wer': 0.18818849744634428, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.39957988, dtype=float32), 'test/wer': 0.12749578534722644, 'test/num_examples': 2472, 'score': 20183.429557085037, 'total_duration': 22309.68611574173, 'accumulated_submission_time': 20183.429557085037, 'accumulated_eval_time': 2124.35813331604, 'accumulated_logging_time': 0.811715841293335}
I0212 19:39:48.085577 139648048760576 logging_writer.py:48] [23726] accumulated_eval_time=2124.358133, accumulated_logging_time=0.811716, accumulated_submission_time=20183.429557, global_step=23726, preemption_count=0, score=20183.429557, test/ctc_loss=0.39957988262176514, test/num_examples=2472, test/wer=0.127496, total_duration=22309.686116, train/ctc_loss=0.3435380160808563, train/wer=0.117657, validation/ctc_loss=0.6654190421104431, validation/num_examples=5348, validation/wer=0.188188
I0212 19:40:44.844357 139648040367872 logging_writer.py:48] [23800] global_step=23800, grad_norm=1.5127662420272827, loss=1.5457236766815186
I0212 19:42:00.562378 139648048760576 logging_writer.py:48] [23900] global_step=23900, grad_norm=3.2866861820220947, loss=1.6699528694152832
I0212 19:43:15.765302 139648040367872 logging_writer.py:48] [24000] global_step=24000, grad_norm=1.7319867610931396, loss=1.634185791015625
I0212 19:44:32.322759 139648048760576 logging_writer.py:48] [24100] global_step=24100, grad_norm=2.3431148529052734, loss=1.6365866661071777
I0212 19:46:02.160608 139648040367872 logging_writer.py:48] [24200] global_step=24200, grad_norm=4.011814594268799, loss=1.6707186698913574
I0212 19:47:31.511625 139648048760576 logging_writer.py:48] [24300] global_step=24300, grad_norm=1.6882001161575317, loss=1.5290383100509644
I0212 19:49:02.601698 139648040367872 logging_writer.py:48] [24400] global_step=24400, grad_norm=2.7663464546203613, loss=1.589598298072815
I0212 19:50:34.866115 139648048760576 logging_writer.py:48] [24500] global_step=24500, grad_norm=2.6164398193359375, loss=1.5098541975021362
I0212 19:52:00.974355 139648040367872 logging_writer.py:48] [24600] global_step=24600, grad_norm=2.935234785079956, loss=1.614363431930542
I0212 19:53:26.846043 139648048760576 logging_writer.py:48] [24700] global_step=24700, grad_norm=2.578639030456543, loss=1.5797041654586792
I0212 19:54:51.237414 139647506040576 logging_writer.py:48] [24800] global_step=24800, grad_norm=2.3948628902435303, loss=1.526788353919983
I0212 19:56:07.218032 139647497647872 logging_writer.py:48] [24900] global_step=24900, grad_norm=1.924438238143921, loss=1.542916178703308
I0212 19:57:22.546542 139647506040576 logging_writer.py:48] [25000] global_step=25000, grad_norm=2.4053804874420166, loss=1.5544333457946777
I0212 19:58:45.712008 139647497647872 logging_writer.py:48] [25100] global_step=25100, grad_norm=2.2039997577667236, loss=1.5785350799560547
I0212 20:00:13.549325 139647506040576 logging_writer.py:48] [25200] global_step=25200, grad_norm=2.959568500518799, loss=1.691446304321289
I0212 20:01:41.953817 139647497647872 logging_writer.py:48] [25300] global_step=25300, grad_norm=2.2389185428619385, loss=1.6071579456329346
I0212 20:03:12.606368 139647506040576 logging_writer.py:48] [25400] global_step=25400, grad_norm=2.6389200687408447, loss=1.5720683336257935
I0212 20:03:48.712229 139803787056960 spec.py:321] Evaluating on the training split.
I0212 20:04:44.004077 139803787056960 spec.py:333] Evaluating on the validation split.
I0212 20:05:37.056228 139803787056960 spec.py:349] Evaluating on the test split.
I0212 20:06:04.095415 139803787056960 submission_runner.py:408] Time since start: 23885.73s, 	Step: 25440, 	{'train/ctc_loss': Array(0.38745674, dtype=float32), 'train/wer': 0.12446508289515897, 'validation/ctc_loss': Array(0.6517793, dtype=float32), 'validation/wer': 0.18645065989553666, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.38806707, dtype=float32), 'test/wer': 0.12479434525623058, 'test/num_examples': 2472, 'score': 21623.96768260002, 'total_duration': 23885.734208345413, 'accumulated_submission_time': 21623.96768260002, 'accumulated_eval_time': 2259.7342801094055, 'accumulated_logging_time': 0.8676271438598633}
I0212 20:06:04.141623 139647506040576 logging_writer.py:48] [25440] accumulated_eval_time=2259.734280, accumulated_logging_time=0.867627, accumulated_submission_time=21623.967683, global_step=25440, preemption_count=0, score=21623.967683, test/ctc_loss=0.3880670666694641, test/num_examples=2472, test/wer=0.124794, total_duration=23885.734208, train/ctc_loss=0.3874567449092865, train/wer=0.124465, validation/ctc_loss=0.651779294013977, validation/num_examples=5348, validation/wer=0.186451
I0212 20:06:50.970737 139647497647872 logging_writer.py:48] [25500] global_step=25500, grad_norm=2.6738150119781494, loss=1.579098105430603
I0212 20:08:06.710318 139647506040576 logging_writer.py:48] [25600] global_step=25600, grad_norm=2.4125192165374756, loss=1.5819491147994995
I0212 20:09:28.532846 139647497647872 logging_writer.py:48] [25700] global_step=25700, grad_norm=2.893203020095825, loss=1.5645861625671387
I0212 20:10:55.183581 139648048760576 logging_writer.py:48] [25800] global_step=25800, grad_norm=2.5046770572662354, loss=1.490402102470398
I0212 20:12:13.430596 139648040367872 logging_writer.py:48] [25900] global_step=25900, grad_norm=2.3755578994750977, loss=1.5202312469482422
I0212 20:13:33.428683 139648048760576 logging_writer.py:48] [26000] global_step=26000, grad_norm=3.2623214721679688, loss=1.5412046909332275
I0212 20:14:54.226849 139648040367872 logging_writer.py:48] [26100] global_step=26100, grad_norm=2.62886118888855, loss=1.549157977104187
I0212 20:16:23.072123 139648048760576 logging_writer.py:48] [26200] global_step=26200, grad_norm=2.463172674179077, loss=1.5391151905059814
I0212 20:17:52.553584 139648040367872 logging_writer.py:48] [26300] global_step=26300, grad_norm=2.714874029159546, loss=1.5416134595870972
I0212 20:19:23.593556 139648048760576 logging_writer.py:48] [26400] global_step=26400, grad_norm=2.4065232276916504, loss=1.57659113407135
I0212 20:20:53.293631 139648040367872 logging_writer.py:48] [26500] global_step=26500, grad_norm=3.109189748764038, loss=1.6105366945266724
I0212 20:22:23.741184 139648048760576 logging_writer.py:48] [26600] global_step=26600, grad_norm=1.7368130683898926, loss=1.5259628295898438
I0212 20:23:54.412408 139648040367872 logging_writer.py:48] [26700] global_step=26700, grad_norm=2.4174320697784424, loss=1.4604648351669312
I0212 20:25:25.801975 139647506040576 logging_writer.py:48] [26800] global_step=26800, grad_norm=4.724864959716797, loss=1.5415456295013428
I0212 20:26:41.864408 139647497647872 logging_writer.py:48] [26900] global_step=26900, grad_norm=1.8374115228652954, loss=1.4703588485717773
I0212 20:27:58.554386 139647506040576 logging_writer.py:48] [27000] global_step=27000, grad_norm=3.1162662506103516, loss=1.5744699239730835
I0212 20:29:18.955405 139647497647872 logging_writer.py:48] [27100] global_step=27100, grad_norm=1.3994837999343872, loss=1.527405858039856
I0212 20:30:04.821363 139803787056960 spec.py:321] Evaluating on the training split.
I0212 20:31:00.874284 139803787056960 spec.py:333] Evaluating on the validation split.
I0212 20:31:54.003277 139803787056960 spec.py:349] Evaluating on the test split.
I0212 20:32:21.114556 139803787056960 submission_runner.py:408] Time since start: 25462.75s, 	Step: 27158, 	{'train/ctc_loss': Array(0.3255008, dtype=float32), 'train/wer': 0.11006443645805297, 'validation/ctc_loss': Array(0.624412, dtype=float32), 'validation/wer': 0.17998204234530832, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.37232503, dtype=float32), 'test/wer': 0.11697438709808462, 'test/num_examples': 2472, 'score': 23064.55658864975, 'total_duration': 25462.754029750824, 'accumulated_submission_time': 23064.55658864975, 'accumulated_eval_time': 2396.0211186408997, 'accumulated_logging_time': 0.9317009449005127}
I0212 20:32:21.154643 139647541880576 logging_writer.py:48] [27158] accumulated_eval_time=2396.021119, accumulated_logging_time=0.931701, accumulated_submission_time=23064.556589, global_step=27158, preemption_count=0, score=23064.556589, test/ctc_loss=0.37232503294944763, test/num_examples=2472, test/wer=0.116974, total_duration=25462.754030, train/ctc_loss=0.3255007863044739, train/wer=0.110064, validation/ctc_loss=0.6244120001792908, validation/num_examples=5348, validation/wer=0.179982
I0212 20:32:53.514750 139647533487872 logging_writer.py:48] [27200] global_step=27200, grad_norm=2.854710340499878, loss=1.6216579675674438
I0212 20:34:08.997609 139647541880576 logging_writer.py:48] [27300] global_step=27300, grad_norm=2.9193263053894043, loss=1.4765273332595825
I0212 20:35:29.187792 139647533487872 logging_writer.py:48] [27400] global_step=27400, grad_norm=3.179516553878784, loss=1.6272271871566772
I0212 20:36:58.576997 139647541880576 logging_writer.py:48] [27500] global_step=27500, grad_norm=2.2891337871551514, loss=1.5593194961547852
I0212 20:38:31.582609 139647533487872 logging_writer.py:48] [27600] global_step=27600, grad_norm=3.113251209259033, loss=1.494563341140747
I0212 20:39:58.958566 139647541880576 logging_writer.py:48] [27700] global_step=27700, grad_norm=2.3645496368408203, loss=1.6180490255355835
I0212 20:41:27.365028 139647533487872 logging_writer.py:48] [27800] global_step=27800, grad_norm=2.6613872051239014, loss=1.5203477144241333
I0212 20:42:49.400125 139647541880576 logging_writer.py:48] [27900] global_step=27900, grad_norm=3.3515946865081787, loss=1.544302225112915
I0212 20:44:05.479538 139647533487872 logging_writer.py:48] [28000] global_step=28000, grad_norm=1.7054016590118408, loss=1.471361756324768
I0212 20:45:24.002629 139647541880576 logging_writer.py:48] [28100] global_step=28100, grad_norm=2.621058940887451, loss=1.5170667171478271
I0212 20:46:51.057063 139647533487872 logging_writer.py:48] [28200] global_step=28200, grad_norm=2.368110418319702, loss=1.4997667074203491
I0212 20:48:19.407925 139647541880576 logging_writer.py:48] [28300] global_step=28300, grad_norm=2.052147150039673, loss=1.5128437280654907
I0212 20:49:52.009180 139647533487872 logging_writer.py:48] [28400] global_step=28400, grad_norm=2.0297019481658936, loss=1.4213212728500366
I0212 20:51:24.889892 139647541880576 logging_writer.py:48] [28500] global_step=28500, grad_norm=2.3189916610717773, loss=1.5320743322372437
I0212 20:52:55.766859 139647533487872 logging_writer.py:48] [28600] global_step=28600, grad_norm=2.6161272525787354, loss=1.4991707801818848
I0212 20:54:21.469173 139647541880576 logging_writer.py:48] [28700] global_step=28700, grad_norm=2.4924018383026123, loss=1.4706586599349976
I0212 20:55:50.405694 139647533487872 logging_writer.py:48] [28800] global_step=28800, grad_norm=2.308635950088501, loss=1.4803671836853027
I0212 20:56:21.826599 139803787056960 spec.py:321] Evaluating on the training split.
I0212 20:57:18.606398 139803787056960 spec.py:333] Evaluating on the validation split.
I0212 20:58:13.667526 139803787056960 spec.py:349] Evaluating on the test split.
I0212 20:58:40.438296 139803787056960 submission_runner.py:408] Time since start: 27042.08s, 	Step: 28839, 	{'train/ctc_loss': Array(0.2880375, dtype=float32), 'train/wer': 0.09923988480241726, 'validation/ctc_loss': Array(0.6031776, dtype=float32), 'validation/wer': 0.17224866524421445, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.35233474, dtype=float32), 'test/wer': 0.11291207117177503, 'test/num_examples': 2472, 'score': 24505.141978740692, 'total_duration': 27042.07692527771, 'accumulated_submission_time': 24505.141978740692, 'accumulated_eval_time': 2534.6256392002106, 'accumulated_logging_time': 0.9876172542572021}
I0212 20:58:40.478343 139647035000576 logging_writer.py:48] [28839] accumulated_eval_time=2534.625639, accumulated_logging_time=0.987617, accumulated_submission_time=24505.141979, global_step=28839, preemption_count=0, score=24505.141979, test/ctc_loss=0.35233473777770996, test/num_examples=2472, test/wer=0.112912, total_duration=27042.076925, train/ctc_loss=0.28803750872612, train/wer=0.099240, validation/ctc_loss=0.6031776070594788, validation/num_examples=5348, validation/wer=0.172249
I0212 20:59:31.976552 139647035000576 logging_writer.py:48] [28900] global_step=28900, grad_norm=1.7425457239151, loss=1.4734495878219604
I0212 21:00:51.888182 139647026607872 logging_writer.py:48] [29000] global_step=29000, grad_norm=2.737706184387207, loss=1.4937556982040405
I0212 21:02:14.460320 139647035000576 logging_writer.py:48] [29100] global_step=29100, grad_norm=2.7147624492645264, loss=1.5112282037734985
I0212 21:03:34.787357 139647026607872 logging_writer.py:48] [29200] global_step=29200, grad_norm=3.6786482334136963, loss=1.5302932262420654
I0212 21:05:03.569627 139647035000576 logging_writer.py:48] [29300] global_step=29300, grad_norm=3.4870760440826416, loss=1.434566617012024
I0212 21:06:31.982135 139647026607872 logging_writer.py:48] [29400] global_step=29400, grad_norm=2.3469598293304443, loss=1.5314149856567383
I0212 21:08:00.378622 139647035000576 logging_writer.py:48] [29500] global_step=29500, grad_norm=1.8800714015960693, loss=1.5286078453063965
I0212 21:09:31.093282 139647026607872 logging_writer.py:48] [29600] global_step=29600, grad_norm=2.2690505981445312, loss=1.4844563007354736
I0212 21:10:59.973196 139647035000576 logging_writer.py:48] [29700] global_step=29700, grad_norm=3.996563196182251, loss=1.4971859455108643
I0212 21:12:28.007747 139647026607872 logging_writer.py:48] [29800] global_step=29800, grad_norm=2.0652124881744385, loss=1.4233852624893188
I0212 21:13:55.510033 139647035000576 logging_writer.py:48] [29900] global_step=29900, grad_norm=3.027418851852417, loss=1.4704688787460327
I0212 21:15:12.956252 139647026607872 logging_writer.py:48] [30000] global_step=30000, grad_norm=2.2271604537963867, loss=1.489141583442688
I0212 21:16:32.744345 139647035000576 logging_writer.py:48] [30100] global_step=30100, grad_norm=2.294498920440674, loss=1.4955796003341675
I0212 21:17:56.024154 139647026607872 logging_writer.py:48] [30200] global_step=30200, grad_norm=4.444397926330566, loss=1.4818050861358643
I0212 21:19:20.170649 139647035000576 logging_writer.py:48] [30300] global_step=30300, grad_norm=2.2288730144500732, loss=1.498864769935608
I0212 21:20:46.918613 139647026607872 logging_writer.py:48] [30400] global_step=30400, grad_norm=2.034374952316284, loss=1.4832671880722046
I0212 21:22:16.997702 139647035000576 logging_writer.py:48] [30500] global_step=30500, grad_norm=2.7675721645355225, loss=1.4616920948028564
I0212 21:22:40.886703 139803787056960 spec.py:321] Evaluating on the training split.
I0212 21:23:37.435162 139803787056960 spec.py:333] Evaluating on the validation split.
I0212 21:24:31.738256 139803787056960 spec.py:349] Evaluating on the test split.
I0212 21:24:58.525111 139803787056960 submission_runner.py:408] Time since start: 28620.17s, 	Step: 30528, 	{'train/ctc_loss': Array(0.2754457, dtype=float32), 'train/wer': 0.09157254099449863, 'validation/ctc_loss': Array(0.5681978, dtype=float32), 'validation/wer': 0.1625843575311121, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33330095, dtype=float32), 'test/wer': 0.1068389088619422, 'test/num_examples': 2472, 'score': 25945.459337949753, 'total_duration': 28620.165376901627, 'accumulated_submission_time': 25945.459337949753, 'accumulated_eval_time': 2672.2584941387177, 'accumulated_logging_time': 1.0459904670715332}
I0212 21:24:58.560090 139647035000576 logging_writer.py:48] [30528] accumulated_eval_time=2672.258494, accumulated_logging_time=1.045990, accumulated_submission_time=25945.459338, global_step=30528, preemption_count=0, score=25945.459338, test/ctc_loss=0.33330094814300537, test/num_examples=2472, test/wer=0.106839, total_duration=28620.165377, train/ctc_loss=0.27544569969177246, train/wer=0.091573, validation/ctc_loss=0.5681977868080139, validation/num_examples=5348, validation/wer=0.162584
I0212 21:25:54.442894 139647026607872 logging_writer.py:48] [30600] global_step=30600, grad_norm=1.9443817138671875, loss=1.458326816558838
I0212 21:27:09.903176 139647035000576 logging_writer.py:48] [30700] global_step=30700, grad_norm=3.647639751434326, loss=1.5440090894699097
I0212 21:28:36.632909 139647026607872 logging_writer.py:48] [30800] global_step=30800, grad_norm=2.586503505706787, loss=1.48887038230896
I0212 21:30:10.612030 139647035000576 logging_writer.py:48] [30900] global_step=30900, grad_norm=1.907767653465271, loss=1.4271044731140137
I0212 21:31:29.351896 139647026607872 logging_writer.py:48] [31000] global_step=31000, grad_norm=2.7209997177124023, loss=1.4392505884170532
I0212 21:32:48.384292 139647035000576 logging_writer.py:48] [31100] global_step=31100, grad_norm=1.6903798580169678, loss=1.4437488317489624
I0212 21:34:09.793469 139647026607872 logging_writer.py:48] [31200] global_step=31200, grad_norm=1.904425859451294, loss=1.3985109329223633
I0212 21:35:33.420585 139647035000576 logging_writer.py:48] [31300] global_step=31300, grad_norm=1.5266602039337158, loss=1.371961236000061
I0212 21:37:00.947329 139647026607872 logging_writer.py:48] [31400] global_step=31400, grad_norm=2.930889844894409, loss=1.4499118328094482
I0212 21:38:27.712800 139647035000576 logging_writer.py:48] [31500] global_step=31500, grad_norm=3.140793800354004, loss=1.4632447957992554
I0212 21:39:57.621876 139647026607872 logging_writer.py:48] [31600] global_step=31600, grad_norm=2.9443817138671875, loss=1.4318008422851562
I0212 21:41:26.541892 139647035000576 logging_writer.py:48] [31700] global_step=31700, grad_norm=1.7432835102081299, loss=1.4276782274246216
I0212 21:42:54.943939 139647026607872 logging_writer.py:48] [31800] global_step=31800, grad_norm=1.9061098098754883, loss=1.422357439994812
I0212 21:44:26.119110 139647035000576 logging_writer.py:48] [31900] global_step=31900, grad_norm=1.7063244581222534, loss=1.4004331827163696
I0212 21:45:50.179805 139647035000576 logging_writer.py:48] [32000] global_step=32000, grad_norm=1.8996546268463135, loss=1.4386757612228394
I0212 21:47:08.012064 139647026607872 logging_writer.py:48] [32100] global_step=32100, grad_norm=1.5502790212631226, loss=1.4169256687164307
I0212 21:48:27.877009 139647035000576 logging_writer.py:48] [32200] global_step=32200, grad_norm=2.2428407669067383, loss=1.3669933080673218
I0212 21:48:58.709245 139803787056960 spec.py:321] Evaluating on the training split.
I0212 21:49:56.462343 139803787056960 spec.py:333] Evaluating on the validation split.
I0212 21:50:49.593786 139803787056960 spec.py:349] Evaluating on the test split.
I0212 21:51:17.219670 139803787056960 submission_runner.py:408] Time since start: 30198.86s, 	Step: 32240, 	{'train/ctc_loss': Array(0.25709972, dtype=float32), 'train/wer': 0.0892764857881137, 'validation/ctc_loss': Array(0.5593565, dtype=float32), 'validation/wer': 0.15988105467430028, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32057795, dtype=float32), 'test/wer': 0.1023500497633701, 'test/num_examples': 2472, 'score': 27385.520001888275, 'total_duration': 30198.859600305557, 'accumulated_submission_time': 27385.520001888275, 'accumulated_eval_time': 2810.763015270233, 'accumulated_logging_time': 1.0972692966461182}
I0212 21:51:17.256978 139647035000576 logging_writer.py:48] [32240] accumulated_eval_time=2810.763015, accumulated_logging_time=1.097269, accumulated_submission_time=27385.520002, global_step=32240, preemption_count=0, score=27385.520002, test/ctc_loss=0.3205779492855072, test/num_examples=2472, test/wer=0.102350, total_duration=30198.859600, train/ctc_loss=0.2570997178554535, train/wer=0.089276, validation/ctc_loss=0.5593565106391907, validation/num_examples=5348, validation/wer=0.159881
I0212 21:52:03.542081 139647026607872 logging_writer.py:48] [32300] global_step=32300, grad_norm=2.3380484580993652, loss=1.485477089881897
I0212 21:53:19.920077 139647035000576 logging_writer.py:48] [32400] global_step=32400, grad_norm=3.2062809467315674, loss=1.43512761592865
I0212 21:54:45.112483 139647026607872 logging_writer.py:48] [32500] global_step=32500, grad_norm=1.6962299346923828, loss=1.3882431983947754
I0212 21:56:14.947344 139647035000576 logging_writer.py:48] [32600] global_step=32600, grad_norm=3.7782793045043945, loss=1.447271704673767
I0212 21:57:46.118331 139647026607872 logging_writer.py:48] [32700] global_step=32700, grad_norm=2.501910448074341, loss=1.4735761880874634
I0212 21:59:15.513211 139647035000576 logging_writer.py:48] [32800] global_step=32800, grad_norm=1.9420483112335205, loss=1.4340722560882568
I0212 22:00:45.163222 139647026607872 logging_writer.py:48] [32900] global_step=32900, grad_norm=1.7913265228271484, loss=1.3756226301193237
I0212 22:02:12.319303 139647035000576 logging_writer.py:48] [33000] global_step=33000, grad_norm=1.913084864616394, loss=1.4763953685760498
I0212 22:03:28.397756 139647026607872 logging_writer.py:48] [33100] global_step=33100, grad_norm=1.6083632707595825, loss=1.381980299949646
I0212 22:04:48.001524 139647035000576 logging_writer.py:48] [33200] global_step=33200, grad_norm=1.5657998323440552, loss=1.3933237791061401
I0212 22:06:11.510727 139647026607872 logging_writer.py:48] [33300] global_step=33300, grad_norm=2.9190027713775635, loss=1.4126074314117432
I0212 22:07:37.329725 139647035000576 logging_writer.py:48] [33400] global_step=33400, grad_norm=1.8573273420333862, loss=1.3794198036193848
I0212 22:09:04.744787 139647026607872 logging_writer.py:48] [33500] global_step=33500, grad_norm=2.26438307762146, loss=1.4246116876602173
I0212 22:10:32.974480 139647035000576 logging_writer.py:48] [33600] global_step=33600, grad_norm=2.5386033058166504, loss=1.4375836849212646
I0212 22:12:01.364373 139647026607872 logging_writer.py:48] [33700] global_step=33700, grad_norm=2.403245210647583, loss=1.4069515466690063
I0212 22:13:29.083109 139647035000576 logging_writer.py:48] [33800] global_step=33800, grad_norm=2.1657960414886475, loss=1.4764074087142944
I0212 22:15:00.803082 139647026607872 logging_writer.py:48] [33900] global_step=33900, grad_norm=2.5079686641693115, loss=1.415501356124878
I0212 22:15:17.552756 139803787056960 spec.py:321] Evaluating on the training split.
I0212 22:16:13.500067 139803787056960 spec.py:333] Evaluating on the validation split.
I0212 22:17:07.908732 139803787056960 spec.py:349] Evaluating on the test split.
I0212 22:17:35.970329 139803787056960 submission_runner.py:408] Time since start: 31777.61s, 	Step: 33920, 	{'train/ctc_loss': Array(0.25858888, dtype=float32), 'train/wer': 0.08586270598575797, 'validation/ctc_loss': Array(0.5291889, dtype=float32), 'validation/wer': 0.15174218214468463, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29892173, dtype=float32), 'test/wer': 0.09550504742753844, 'test/num_examples': 2472, 'score': 28825.72840666771, 'total_duration': 31777.60964488983, 'accumulated_submission_time': 28825.72840666771, 'accumulated_eval_time': 2949.174058675766, 'accumulated_logging_time': 1.1517837047576904}
I0212 22:17:36.006603 139647035000576 logging_writer.py:48] [33920] accumulated_eval_time=2949.174059, accumulated_logging_time=1.151784, accumulated_submission_time=28825.728407, global_step=33920, preemption_count=0, score=28825.728407, test/ctc_loss=0.29892173409461975, test/num_examples=2472, test/wer=0.095505, total_duration=31777.609645, train/ctc_loss=0.25858888030052185, train/wer=0.085863, validation/ctc_loss=0.529188871383667, validation/num_examples=5348, validation/wer=0.151742
I0212 22:18:41.143365 139647035000576 logging_writer.py:48] [34000] global_step=34000, grad_norm=1.803501009941101, loss=1.336531639099121
I0212 22:19:59.772632 139647026607872 logging_writer.py:48] [34100] global_step=34100, grad_norm=2.5248239040374756, loss=1.4233027696609497
I0212 22:21:21.097367 139647035000576 logging_writer.py:48] [34200] global_step=34200, grad_norm=3.6854336261749268, loss=1.3775243759155273
I0212 22:22:46.802092 139647026607872 logging_writer.py:48] [34300] global_step=34300, grad_norm=1.4589208364486694, loss=1.3546260595321655
I0212 22:24:11.341013 139647035000576 logging_writer.py:48] [34400] global_step=34400, grad_norm=1.840536117553711, loss=1.3391262292861938
I0212 22:25:40.418874 139647026607872 logging_writer.py:48] [34500] global_step=34500, grad_norm=2.1288890838623047, loss=1.3403313159942627
I0212 22:27:10.489705 139647035000576 logging_writer.py:48] [34600] global_step=34600, grad_norm=2.080869436264038, loss=1.4218831062316895
I0212 22:28:42.640957 139647026607872 logging_writer.py:48] [34700] global_step=34700, grad_norm=3.3705849647521973, loss=1.3909945487976074
I0212 22:30:14.448682 139647035000576 logging_writer.py:48] [34800] global_step=34800, grad_norm=3.327174663543701, loss=1.3887566328048706
I0212 22:31:44.527101 139647026607872 logging_writer.py:48] [34900] global_step=34900, grad_norm=2.055110454559326, loss=1.362903118133545
I0212 22:33:15.986820 139647035000576 logging_writer.py:48] [35000] global_step=35000, grad_norm=1.5544633865356445, loss=1.363976001739502
I0212 22:34:38.650213 139647035000576 logging_writer.py:48] [35100] global_step=35100, grad_norm=2.162980794906616, loss=1.3812578916549683
I0212 22:35:57.146333 139647026607872 logging_writer.py:48] [35200] global_step=35200, grad_norm=1.607356309890747, loss=1.3430404663085938
I0212 22:37:17.093654 139647035000576 logging_writer.py:48] [35300] global_step=35300, grad_norm=1.8318212032318115, loss=1.4071180820465088
I0212 22:38:41.038497 139647026607872 logging_writer.py:48] [35400] global_step=35400, grad_norm=2.01474666595459, loss=1.31965172290802
I0212 22:40:11.523103 139647035000576 logging_writer.py:48] [35500] global_step=35500, grad_norm=2.0267598628997803, loss=1.420991063117981
I0212 22:41:36.349218 139803787056960 spec.py:321] Evaluating on the training split.
I0212 22:42:32.576783 139803787056960 spec.py:333] Evaluating on the validation split.
I0212 22:43:26.968987 139803787056960 spec.py:349] Evaluating on the test split.
I0212 22:43:53.698765 139803787056960 submission_runner.py:408] Time since start: 33355.34s, 	Step: 35595, 	{'train/ctc_loss': Array(0.24284497, dtype=float32), 'train/wer': 0.08170056815489198, 'validation/ctc_loss': Array(0.510084, dtype=float32), 'validation/wer': 0.14698243818608378, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28692654, dtype=float32), 'test/wer': 0.09213332520870147, 'test/num_examples': 2472, 'score': 30265.98497748375, 'total_duration': 33355.33820748329, 'accumulated_submission_time': 30265.98497748375, 'accumulated_eval_time': 3086.517218351364, 'accumulated_logging_time': 1.2039318084716797}
I0212 22:43:53.739517 139647035000576 logging_writer.py:48] [35595] accumulated_eval_time=3086.517218, accumulated_logging_time=1.203932, accumulated_submission_time=30265.984977, global_step=35595, preemption_count=0, score=30265.984977, test/ctc_loss=0.2869265377521515, test/num_examples=2472, test/wer=0.092133, total_duration=33355.338207, train/ctc_loss=0.24284496903419495, train/wer=0.081701, validation/ctc_loss=0.5100839734077454, validation/num_examples=5348, validation/wer=0.146982
I0212 22:43:58.475899 139647026607872 logging_writer.py:48] [35600] global_step=35600, grad_norm=4.647920608520508, loss=1.4290167093276978
I0212 22:45:14.246284 139647035000576 logging_writer.py:48] [35700] global_step=35700, grad_norm=2.048124313354492, loss=1.3955528736114502
I0212 22:46:30.317085 139647026607872 logging_writer.py:48] [35800] global_step=35800, grad_norm=1.5550848245620728, loss=1.3142507076263428
I0212 22:47:58.027307 139647035000576 logging_writer.py:48] [35900] global_step=35900, grad_norm=3.772749900817871, loss=1.3331530094146729
I0212 22:49:22.978553 139647026607872 logging_writer.py:48] [36000] global_step=36000, grad_norm=2.4659464359283447, loss=1.365268588066101
I0212 22:50:51.567412 139647035000576 logging_writer.py:48] [36100] global_step=36100, grad_norm=4.305313587188721, loss=1.3881958723068237
I0212 22:52:12.933026 139647026607872 logging_writer.py:48] [36200] global_step=36200, grad_norm=3.0675241947174072, loss=1.3647822141647339
I0212 22:53:31.088988 139647035000576 logging_writer.py:48] [36300] global_step=36300, grad_norm=3.885664701461792, loss=1.2941679954528809
I0212 22:54:52.882044 139647026607872 logging_writer.py:48] [36400] global_step=36400, grad_norm=1.8593186140060425, loss=1.3005764484405518
I0212 22:56:21.314605 139647035000576 logging_writer.py:48] [36500] global_step=36500, grad_norm=1.8042436838150024, loss=1.344857096672058
I0212 22:57:52.861150 139647026607872 logging_writer.py:48] [36600] global_step=36600, grad_norm=2.1018965244293213, loss=1.313753604888916
I0212 22:59:21.423630 139647035000576 logging_writer.py:48] [36700] global_step=36700, grad_norm=2.4251325130462646, loss=1.3256144523620605
I0212 23:00:49.897487 139647026607872 logging_writer.py:48] [36800] global_step=36800, grad_norm=2.395401954650879, loss=1.2923002243041992
I0212 23:02:18.193452 139647035000576 logging_writer.py:48] [36900] global_step=36900, grad_norm=2.2214643955230713, loss=1.309892177581787
I0212 23:03:48.857203 139647026607872 logging_writer.py:48] [37000] global_step=37000, grad_norm=2.0132906436920166, loss=1.3433845043182373
I0212 23:05:19.433691 139647035000576 logging_writer.py:48] [37100] global_step=37100, grad_norm=1.8293492794036865, loss=1.3282400369644165
I0212 23:06:36.980729 139647026607872 logging_writer.py:48] [37200] global_step=37200, grad_norm=2.2175137996673584, loss=1.3124793767929077
I0212 23:07:53.753199 139803787056960 spec.py:321] Evaluating on the training split.
I0212 23:08:50.779088 139803787056960 spec.py:333] Evaluating on the validation split.
I0212 23:09:43.418966 139803787056960 spec.py:349] Evaluating on the test split.
I0212 23:10:10.347381 139803787056960 submission_runner.py:408] Time since start: 34931.99s, 	Step: 37298, 	{'train/ctc_loss': Array(0.2170262, dtype=float32), 'train/wer': 0.07354023870506134, 'validation/ctc_loss': Array(0.48365703, dtype=float32), 'validation/wer': 0.13989592284001273, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27283636, dtype=float32), 'test/wer': 0.08638514817297341, 'test/num_examples': 2472, 'score': 31705.910080432892, 'total_duration': 34931.98729777336, 'accumulated_submission_time': 31705.910080432892, 'accumulated_eval_time': 3223.1055147647858, 'accumulated_logging_time': 1.2620007991790771}
I0212 23:10:10.386076 139647035000576 logging_writer.py:48] [37298] accumulated_eval_time=3223.105515, accumulated_logging_time=1.262001, accumulated_submission_time=31705.910080, global_step=37298, preemption_count=0, score=31705.910080, test/ctc_loss=0.2728363573551178, test/num_examples=2472, test/wer=0.086385, total_duration=34931.987298, train/ctc_loss=0.21702620387077332, train/wer=0.073540, validation/ctc_loss=0.48365703225135803, validation/num_examples=5348, validation/wer=0.139896
I0212 23:10:12.780500 139647026607872 logging_writer.py:48] [37300] global_step=37300, grad_norm=1.658340334892273, loss=1.3208361864089966
I0212 23:11:27.904158 139647035000576 logging_writer.py:48] [37400] global_step=37400, grad_norm=1.9818710088729858, loss=1.3181989192962646
I0212 23:12:44.293453 139647026607872 logging_writer.py:48] [37500] global_step=37500, grad_norm=1.9345659017562866, loss=1.3195174932479858
I0212 23:14:06.084212 139647035000576 logging_writer.py:48] [37600] global_step=37600, grad_norm=1.9218169450759888, loss=1.3035173416137695
I0212 23:15:33.342715 139647026607872 logging_writer.py:48] [37700] global_step=37700, grad_norm=3.245814800262451, loss=1.3533694744110107
I0212 23:17:04.540369 139647035000576 logging_writer.py:48] [37800] global_step=37800, grad_norm=2.853503465652466, loss=1.2851613759994507
I0212 23:18:34.908967 139647026607872 logging_writer.py:48] [37900] global_step=37900, grad_norm=1.9795970916748047, loss=1.311270833015442
I0212 23:20:07.501096 139647035000576 logging_writer.py:48] [38000] global_step=38000, grad_norm=4.048401832580566, loss=1.3161064386367798
I0212 23:21:35.844701 139647026607872 logging_writer.py:48] [38100] global_step=38100, grad_norm=3.3330609798431396, loss=1.3270485401153564
I0212 23:22:57.464854 139647035000576 logging_writer.py:48] [38200] global_step=38200, grad_norm=4.173828125, loss=1.2882112264633179
I0212 23:24:16.393210 139647026607872 logging_writer.py:48] [38300] global_step=38300, grad_norm=2.2007718086242676, loss=1.291039228439331
I0212 23:25:38.247495 139647035000576 logging_writer.py:48] [38400] global_step=38400, grad_norm=2.7000186443328857, loss=1.2628470659255981
I0212 23:27:03.755106 139647026607872 logging_writer.py:48] [38500] global_step=38500, grad_norm=2.3924992084503174, loss=1.268754243850708
I0212 23:28:31.923669 139647035000576 logging_writer.py:48] [38600] global_step=38600, grad_norm=2.148719310760498, loss=1.2579797506332397
I0212 23:30:00.790518 139647026607872 logging_writer.py:48] [38700] global_step=38700, grad_norm=2.436135768890381, loss=1.3376555442810059
I0212 23:31:30.343595 139647035000576 logging_writer.py:48] [38800] global_step=38800, grad_norm=2.087966203689575, loss=1.2425103187561035
I0212 23:32:58.379692 139647026607872 logging_writer.py:48] [38900] global_step=38900, grad_norm=2.998300552368164, loss=1.2226301431655884
I0212 23:34:10.570285 139803787056960 spec.py:321] Evaluating on the training split.
I0212 23:35:09.083700 139803787056960 spec.py:333] Evaluating on the validation split.
I0212 23:36:03.049521 139803787056960 spec.py:349] Evaluating on the test split.
I0212 23:36:30.835936 139803787056960 submission_runner.py:408] Time since start: 36512.47s, 	Step: 38983, 	{'train/ctc_loss': Array(0.20192882, dtype=float32), 'train/wer': 0.06945914796639427, 'validation/ctc_loss': Array(0.46267018, dtype=float32), 'validation/wer': 0.1323266748409396, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2571431, dtype=float32), 'test/wer': 0.08262750594113705, 'test/num_examples': 2472, 'score': 33146.00746154785, 'total_duration': 36512.47426152229, 'accumulated_submission_time': 33146.00746154785, 'accumulated_eval_time': 3363.3636882305145, 'accumulated_logging_time': 1.3168060779571533}
I0212 23:36:30.882902 139647035000576 logging_writer.py:48] [38983] accumulated_eval_time=3363.363688, accumulated_logging_time=1.316806, accumulated_submission_time=33146.007462, global_step=38983, preemption_count=0, score=33146.007462, test/ctc_loss=0.25714311003685, test/num_examples=2472, test/wer=0.082628, total_duration=36512.474262, train/ctc_loss=0.20192882418632507, train/wer=0.069459, validation/ctc_loss=0.4626701772212982, validation/num_examples=5348, validation/wer=0.132327
I0212 23:36:44.454923 139647026607872 logging_writer.py:48] [39000] global_step=39000, grad_norm=2.7052531242370605, loss=1.2776049375534058
I0212 23:37:59.823224 139647035000576 logging_writer.py:48] [39100] global_step=39100, grad_norm=1.9909062385559082, loss=1.2853741645812988
I0212 23:39:19.634078 139647035000576 logging_writer.py:48] [39200] global_step=39200, grad_norm=1.6402584314346313, loss=1.2529125213623047
I0212 23:40:37.169301 139647026607872 logging_writer.py:48] [39300] global_step=39300, grad_norm=1.913630723953247, loss=1.279530644416809
I0212 23:41:55.260278 139647035000576 logging_writer.py:48] [39400] global_step=39400, grad_norm=4.221696376800537, loss=1.2584823369979858
I0212 23:43:14.453005 139647026607872 logging_writer.py:48] [39500] global_step=39500, grad_norm=2.379977226257324, loss=1.2192753553390503
I0212 23:44:44.078621 139647035000576 logging_writer.py:48] [39600] global_step=39600, grad_norm=1.7493749856948853, loss=1.2484397888183594
I0212 23:46:12.500344 139647026607872 logging_writer.py:48] [39700] global_step=39700, grad_norm=2.0156211853027344, loss=1.257204294204712
I0212 23:47:42.881889 139647035000576 logging_writer.py:48] [39800] global_step=39800, grad_norm=2.5676722526550293, loss=1.2388020753860474
I0212 23:49:13.736114 139647026607872 logging_writer.py:48] [39900] global_step=39900, grad_norm=4.641219615936279, loss=1.2814853191375732
I0212 23:50:42.291118 139647035000576 logging_writer.py:48] [40000] global_step=40000, grad_norm=3.0921072959899902, loss=1.2517399787902832
I0212 23:52:08.676165 139647026607872 logging_writer.py:48] [40100] global_step=40100, grad_norm=2.5650713443756104, loss=1.2741572856903076
I0212 23:53:38.001996 139647035000576 logging_writer.py:48] [40200] global_step=40200, grad_norm=2.108989715576172, loss=1.236710786819458
I0212 23:54:55.478500 139647026607872 logging_writer.py:48] [40300] global_step=40300, grad_norm=2.058466672897339, loss=1.2354567050933838
I0212 23:56:14.975643 139647035000576 logging_writer.py:48] [40400] global_step=40400, grad_norm=1.479670524597168, loss=1.2124943733215332
I0212 23:57:36.630640 139647026607872 logging_writer.py:48] [40500] global_step=40500, grad_norm=2.706373929977417, loss=1.2351411581039429
I0212 23:59:03.357115 139647035000576 logging_writer.py:48] [40600] global_step=40600, grad_norm=2.7672317028045654, loss=1.2417854070663452
I0213 00:00:32.221081 139803787056960 spec.py:321] Evaluating on the training split.
I0213 00:01:28.557414 139803787056960 spec.py:333] Evaluating on the validation split.
I0213 00:02:21.606375 139803787056960 spec.py:349] Evaluating on the test split.
I0213 00:02:48.390443 139803787056960 submission_runner.py:408] Time since start: 38090.03s, 	Step: 40699, 	{'train/ctc_loss': Array(0.1680867, dtype=float32), 'train/wer': 0.05816655225220468, 'validation/ctc_loss': Array(0.44180185, dtype=float32), 'validation/wer': 0.12681386794365546, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24477644, dtype=float32), 'test/wer': 0.07754961103325006, 'test/num_examples': 2472, 'score': 34587.25555706024, 'total_duration': 38090.030962228775, 'accumulated_submission_time': 34587.25555706024, 'accumulated_eval_time': 3499.5277755260468, 'accumulated_logging_time': 1.3820362091064453}
I0213 00:02:48.431540 139647035000576 logging_writer.py:48] [40699] accumulated_eval_time=3499.527776, accumulated_logging_time=1.382036, accumulated_submission_time=34587.255557, global_step=40699, preemption_count=0, score=34587.255557, test/ctc_loss=0.24477644264698029, test/num_examples=2472, test/wer=0.077550, total_duration=38090.030962, train/ctc_loss=0.1680866926908493, train/wer=0.058167, validation/ctc_loss=0.44180184602737427, validation/num_examples=5348, validation/wer=0.126814
I0213 00:02:50.074656 139647026607872 logging_writer.py:48] [40700] global_step=40700, grad_norm=2.4254543781280518, loss=1.2627874612808228
I0213 00:04:05.451535 139647035000576 logging_writer.py:48] [40800] global_step=40800, grad_norm=1.6230138540267944, loss=1.2642760276794434
I0213 00:05:21.168926 139647026607872 logging_writer.py:48] [40900] global_step=40900, grad_norm=1.3434925079345703, loss=1.2611738443374634
I0213 00:06:49.134892 139647035000576 logging_writer.py:48] [41000] global_step=41000, grad_norm=2.061147928237915, loss=1.242142915725708
I0213 00:08:20.466412 139647026607872 logging_writer.py:48] [41100] global_step=41100, grad_norm=1.8388159275054932, loss=1.1955190896987915
I0213 00:09:53.864034 139647035000576 logging_writer.py:48] [41200] global_step=41200, grad_norm=2.815192699432373, loss=1.1915779113769531
I0213 00:11:10.954123 139647026607872 logging_writer.py:48] [41300] global_step=41300, grad_norm=1.6051546335220337, loss=1.1903774738311768
I0213 00:12:30.606012 139647035000576 logging_writer.py:48] [41400] global_step=41400, grad_norm=1.7258301973342896, loss=1.1611050367355347
I0213 00:13:54.352830 139647026607872 logging_writer.py:48] [41500] global_step=41500, grad_norm=2.0572011470794678, loss=1.2248152494430542
I0213 00:15:20.500066 139647035000576 logging_writer.py:48] [41600] global_step=41600, grad_norm=2.8893299102783203, loss=1.2063302993774414
I0213 00:16:49.901999 139647026607872 logging_writer.py:48] [41700] global_step=41700, grad_norm=2.697312593460083, loss=1.2172698974609375
I0213 00:18:16.870592 139647035000576 logging_writer.py:48] [41800] global_step=41800, grad_norm=2.286925792694092, loss=1.1566264629364014
I0213 00:19:45.614728 139647026607872 logging_writer.py:48] [41900] global_step=41900, grad_norm=1.9202507734298706, loss=1.1723941564559937
I0213 00:21:14.971480 139647035000576 logging_writer.py:48] [42000] global_step=42000, grad_norm=2.28143572807312, loss=1.226061224937439
I0213 00:22:46.532222 139647026607872 logging_writer.py:48] [42100] global_step=42100, grad_norm=3.0706589221954346, loss=1.1699405908584595
I0213 00:24:17.639264 139647035000576 logging_writer.py:48] [42200] global_step=42200, grad_norm=2.4318034648895264, loss=1.17888343334198
I0213 00:25:39.333193 139647035000576 logging_writer.py:48] [42300] global_step=42300, grad_norm=1.6646126508712769, loss=1.1684439182281494
I0213 00:26:48.920954 139803787056960 spec.py:321] Evaluating on the training split.
I0213 00:27:46.277597 139803787056960 spec.py:333] Evaluating on the validation split.
I0213 00:28:39.542589 139803787056960 spec.py:349] Evaluating on the test split.
I0213 00:29:08.073124 139803787056960 submission_runner.py:408] Time since start: 39669.71s, 	Step: 42391, 	{'train/ctc_loss': Array(0.16477492, dtype=float32), 'train/wer': 0.055599824287188286, 'validation/ctc_loss': Array(0.42381123, dtype=float32), 'validation/wer': 0.12162931925041273, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23368566, dtype=float32), 'test/wer': 0.07391383827920298, 'test/num_examples': 2472, 'score': 36027.65841794014, 'total_duration': 39669.712934970856, 'accumulated_submission_time': 36027.65841794014, 'accumulated_eval_time': 3638.6739313602448, 'accumulated_logging_time': 1.4387574195861816}
I0213 00:29:08.120067 139647035000576 logging_writer.py:48] [42391] accumulated_eval_time=3638.673931, accumulated_logging_time=1.438757, accumulated_submission_time=36027.658418, global_step=42391, preemption_count=0, score=36027.658418, test/ctc_loss=0.2336856573820114, test/num_examples=2472, test/wer=0.073914, total_duration=39669.712935, train/ctc_loss=0.16477492451667786, train/wer=0.055600, validation/ctc_loss=0.4238112270832062, validation/num_examples=5348, validation/wer=0.121629
I0213 00:29:15.778796 139647026607872 logging_writer.py:48] [42400] global_step=42400, grad_norm=2.333245038986206, loss=1.2046101093292236
I0213 00:30:31.049436 139647035000576 logging_writer.py:48] [42500] global_step=42500, grad_norm=2.0002877712249756, loss=1.2075059413909912
I0213 00:31:46.406507 139647026607872 logging_writer.py:48] [42600] global_step=42600, grad_norm=1.82819402217865, loss=1.2038480043411255
I0213 00:33:05.755287 139647035000576 logging_writer.py:48] [42700] global_step=42700, grad_norm=1.7823392152786255, loss=1.2176955938339233
I0213 00:34:34.322074 139647026607872 logging_writer.py:48] [42800] global_step=42800, grad_norm=4.57911491394043, loss=1.1795963048934937
I0213 00:36:05.079465 139647035000576 logging_writer.py:48] [42900] global_step=42900, grad_norm=3.5064549446105957, loss=1.1846193075180054
I0213 00:37:35.560106 139647026607872 logging_writer.py:48] [43000] global_step=43000, grad_norm=2.2270374298095703, loss=1.253423810005188
I0213 00:39:04.351261 139647035000576 logging_writer.py:48] [43100] global_step=43100, grad_norm=3.7746620178222656, loss=1.1397656202316284
I0213 00:40:31.475658 139647026607872 logging_writer.py:48] [43200] global_step=43200, grad_norm=2.066120147705078, loss=1.1597446203231812
I0213 00:41:59.165048 139647035000576 logging_writer.py:48] [43300] global_step=43300, grad_norm=2.328852415084839, loss=1.1459851264953613
I0213 00:43:16.667297 139647026607872 logging_writer.py:48] [43400] global_step=43400, grad_norm=3.0310323238372803, loss=1.1694761514663696
I0213 00:44:33.196163 139647035000576 logging_writer.py:48] [43500] global_step=43500, grad_norm=3.818905830383301, loss=1.1123825311660767
I0213 00:45:55.452141 139647026607872 logging_writer.py:48] [43600] global_step=43600, grad_norm=3.3978445529937744, loss=1.1567500829696655
I0213 00:47:19.069290 139647035000576 logging_writer.py:48] [43700] global_step=43700, grad_norm=2.495311737060547, loss=1.1918069124221802
I0213 00:48:48.219065 139647026607872 logging_writer.py:48] [43800] global_step=43800, grad_norm=2.080751419067383, loss=1.1649137735366821
I0213 00:50:20.662714 139647035000576 logging_writer.py:48] [43900] global_step=43900, grad_norm=3.120093822479248, loss=1.1809022426605225
I0213 00:51:49.650372 139647026607872 logging_writer.py:48] [44000] global_step=44000, grad_norm=2.1487860679626465, loss=1.1496104001998901
I0213 00:53:08.612896 139803787056960 spec.py:321] Evaluating on the training split.
I0213 00:54:03.944329 139803787056960 spec.py:333] Evaluating on the validation split.
I0213 00:54:56.513333 139803787056960 spec.py:349] Evaluating on the test split.
I0213 00:55:23.361347 139803787056960 submission_runner.py:408] Time since start: 41245.00s, 	Step: 44091, 	{'train/ctc_loss': Array(0.16973253, dtype=float32), 'train/wer': 0.05735663542313822, 'validation/ctc_loss': Array(0.4103643, dtype=float32), 'validation/wer': 0.11811502553655734, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2251991, dtype=float32), 'test/wer': 0.07064367395852375, 'test/num_examples': 2472, 'score': 37468.0641913414, 'total_duration': 41244.99890422821, 'accumulated_submission_time': 37468.0641913414, 'accumulated_eval_time': 3773.414123773575, 'accumulated_logging_time': 1.5009901523590088}
I0213 00:55:23.399975 139647035000576 logging_writer.py:48] [44091] accumulated_eval_time=3773.414124, accumulated_logging_time=1.500990, accumulated_submission_time=37468.064191, global_step=44091, preemption_count=0, score=37468.064191, test/ctc_loss=0.22519910335540771, test/num_examples=2472, test/wer=0.070644, total_duration=41244.998904, train/ctc_loss=0.16973252594470978, train/wer=0.057357, validation/ctc_loss=0.4103643000125885, validation/num_examples=5348, validation/wer=0.118115
I0213 00:55:31.202093 139647026607872 logging_writer.py:48] [44100] global_step=44100, grad_norm=1.5301374197006226, loss=1.1292701959609985
I0213 00:56:46.664395 139647035000576 logging_writer.py:48] [44200] global_step=44200, grad_norm=2.189945697784424, loss=1.151794672012329
I0213 00:58:06.613121 139647035000576 logging_writer.py:48] [44300] global_step=44300, grad_norm=1.587515950202942, loss=1.1270127296447754
I0213 00:59:23.983967 139647026607872 logging_writer.py:48] [44400] global_step=44400, grad_norm=1.788873314857483, loss=1.1546262502670288
I0213 01:00:43.483350 139647035000576 logging_writer.py:48] [44500] global_step=44500, grad_norm=1.4359537363052368, loss=1.1916199922561646
I0213 01:02:06.439377 139647026607872 logging_writer.py:48] [44600] global_step=44600, grad_norm=1.7980432510375977, loss=1.1296535730361938
I0213 01:03:34.599180 139647035000576 logging_writer.py:48] [44700] global_step=44700, grad_norm=2.9697208404541016, loss=1.1458183526992798
I0213 01:05:04.420257 139647026607872 logging_writer.py:48] [44800] global_step=44800, grad_norm=2.62080717086792, loss=1.1980053186416626
I0213 01:06:33.758427 139647035000576 logging_writer.py:48] [44900] global_step=44900, grad_norm=1.7613005638122559, loss=1.1742103099822998
I0213 01:08:00.682760 139647026607872 logging_writer.py:48] [45000] global_step=45000, grad_norm=1.888398289680481, loss=1.1380023956298828
I0213 01:09:29.834377 139647035000576 logging_writer.py:48] [45100] global_step=45100, grad_norm=2.3087592124938965, loss=1.1320075988769531
I0213 01:10:59.562348 139647026607872 logging_writer.py:48] [45200] global_step=45200, grad_norm=2.212585926055908, loss=1.1663024425506592
I0213 01:12:27.557667 139647035000576 logging_writer.py:48] [45300] global_step=45300, grad_norm=1.5621120929718018, loss=1.1090478897094727
I0213 01:13:50.236092 139647035000576 logging_writer.py:48] [45400] global_step=45400, grad_norm=2.968441963195801, loss=1.0455185174942017
I0213 01:15:09.725137 139647026607872 logging_writer.py:48] [45500] global_step=45500, grad_norm=1.5409538745880127, loss=1.2208551168441772
I0213 01:16:30.461978 139647035000576 logging_writer.py:48] [45600] global_step=45600, grad_norm=2.163865327835083, loss=1.0734013319015503
I0213 01:17:55.327435 139647026607872 logging_writer.py:48] [45700] global_step=45700, grad_norm=2.682863235473633, loss=1.15314781665802
I0213 01:19:22.182078 139647035000576 logging_writer.py:48] [45800] global_step=45800, grad_norm=1.789364218711853, loss=1.1065984964370728
I0213 01:19:23.536928 139803787056960 spec.py:321] Evaluating on the training split.
I0213 01:20:19.147755 139803787056960 spec.py:333] Evaluating on the validation split.
I0213 01:21:13.359360 139803787056960 spec.py:349] Evaluating on the test split.
I0213 01:21:40.737667 139803787056960 submission_runner.py:408] Time since start: 42822.38s, 	Step: 45803, 	{'train/ctc_loss': Array(0.14928834, dtype=float32), 'train/wer': 0.050186845837355144, 'validation/ctc_loss': Array(0.40272284, dtype=float32), 'validation/wer': 0.11580756345520724, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22022197, dtype=float32), 'test/wer': 0.06936404444173623, 'test/num_examples': 2472, 'score': 38908.11180472374, 'total_duration': 42822.377161979675, 'accumulated_submission_time': 38908.11180472374, 'accumulated_eval_time': 3910.608511686325, 'accumulated_logging_time': 1.557424783706665}
I0213 01:21:40.779572 139647035000576 logging_writer.py:48] [45803] accumulated_eval_time=3910.608512, accumulated_logging_time=1.557425, accumulated_submission_time=38908.111805, global_step=45803, preemption_count=0, score=38908.111805, test/ctc_loss=0.22022196650505066, test/num_examples=2472, test/wer=0.069364, total_duration=42822.377162, train/ctc_loss=0.1492883414030075, train/wer=0.050187, validation/ctc_loss=0.4027228355407715, validation/num_examples=5348, validation/wer=0.115808
I0213 01:22:55.027614 139647026607872 logging_writer.py:48] [45900] global_step=45900, grad_norm=1.5219285488128662, loss=1.1344330310821533
I0213 01:24:10.772675 139647035000576 logging_writer.py:48] [46000] global_step=46000, grad_norm=2.23694109916687, loss=1.1515313386917114
I0213 01:25:36.697524 139647026607872 logging_writer.py:48] [46100] global_step=46100, grad_norm=2.4547972679138184, loss=1.1242700815200806
I0213 01:27:05.957723 139647035000576 logging_writer.py:48] [46200] global_step=46200, grad_norm=2.981755256652832, loss=1.1488642692565918
I0213 01:28:36.076627 139647026607872 logging_writer.py:48] [46300] global_step=46300, grad_norm=1.6173675060272217, loss=1.1553040742874146
I0213 01:30:02.425989 139647035000576 logging_writer.py:48] [46400] global_step=46400, grad_norm=3.3538098335266113, loss=1.1566691398620605
I0213 01:31:19.130720 139647026607872 logging_writer.py:48] [46500] global_step=46500, grad_norm=1.8603265285491943, loss=1.0926927328109741
I0213 01:32:37.776844 139647035000576 logging_writer.py:48] [46600] global_step=46600, grad_norm=3.535947322845459, loss=1.1429493427276611
I0213 01:33:57.625450 139647026607872 logging_writer.py:48] [46700] global_step=46700, grad_norm=2.250861167907715, loss=1.1663966178894043
I0213 01:35:24.949853 139647035000576 logging_writer.py:48] [46800] global_step=46800, grad_norm=1.668595790863037, loss=1.1572747230529785
I0213 01:36:53.087015 139647026607872 logging_writer.py:48] [46900] global_step=46900, grad_norm=2.439046621322632, loss=1.1687372922897339
I0213 01:38:19.610743 139647035000576 logging_writer.py:48] [47000] global_step=47000, grad_norm=1.776142954826355, loss=1.1422603130340576
I0213 01:39:50.172298 139647026607872 logging_writer.py:48] [47100] global_step=47100, grad_norm=1.714951753616333, loss=1.1028940677642822
I0213 01:41:21.568920 139647035000576 logging_writer.py:48] [47200] global_step=47200, grad_norm=7.102499485015869, loss=1.1286876201629639
I0213 01:42:51.258350 139647026607872 logging_writer.py:48] [47300] global_step=47300, grad_norm=2.046248435974121, loss=1.1177047491073608
I0213 01:44:22.559857 139647035000576 logging_writer.py:48] [47400] global_step=47400, grad_norm=3.1724941730499268, loss=1.186072826385498
I0213 01:45:40.539352 139647026607872 logging_writer.py:48] [47500] global_step=47500, grad_norm=1.7971936464309692, loss=1.072827696800232
I0213 01:45:41.167877 139803787056960 spec.py:321] Evaluating on the training split.
I0213 01:46:37.587928 139803787056960 spec.py:333] Evaluating on the validation split.
I0213 01:47:31.902052 139803787056960 spec.py:349] Evaluating on the test split.
I0213 01:47:58.246170 139803787056960 submission_runner.py:408] Time since start: 44399.89s, 	Step: 47502, 	{'train/ctc_loss': Array(0.16172755, dtype=float32), 'train/wer': 0.05604972504790242, 'validation/ctc_loss': Array(0.4016892, dtype=float32), 'validation/wer': 0.11590410998580766, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21897092, dtype=float32), 'test/wer': 0.0687953202120529, 'test/num_examples': 2472, 'score': 40348.411856889725, 'total_duration': 44399.88564610481, 'accumulated_submission_time': 40348.411856889725, 'accumulated_eval_time': 4047.680432319641, 'accumulated_logging_time': 1.617690086364746}
I0213 01:47:58.283302 139647035000576 logging_writer.py:48] [47502] accumulated_eval_time=4047.680432, accumulated_logging_time=1.617690, accumulated_submission_time=40348.411857, global_step=47502, preemption_count=0, score=40348.411857, test/ctc_loss=0.21897092461585999, test/num_examples=2472, test/wer=0.068795, total_duration=44399.885646, train/ctc_loss=0.16172754764556885, train/wer=0.056050, validation/ctc_loss=0.40168920159339905, validation/num_examples=5348, validation/wer=0.115904
I0213 01:49:12.816702 139647026607872 logging_writer.py:48] [47600] global_step=47600, grad_norm=2.3573079109191895, loss=1.1400362253189087
I0213 01:50:27.861835 139647035000576 logging_writer.py:48] [47700] global_step=47700, grad_norm=1.9470241069793701, loss=1.1499189138412476
I0213 01:51:44.530072 139647026607872 logging_writer.py:48] [47800] global_step=47800, grad_norm=3.191131830215454, loss=1.1396845579147339
I0213 01:53:13.153312 139647035000576 logging_writer.py:48] [47900] global_step=47900, grad_norm=2.5836310386657715, loss=1.1391223669052124
I0213 01:54:41.181408 139803787056960 spec.py:321] Evaluating on the training split.
I0213 01:55:36.338533 139803787056960 spec.py:333] Evaluating on the validation split.
I0213 01:56:29.623502 139803787056960 spec.py:349] Evaluating on the test split.
I0213 01:56:57.151890 139803787056960 submission_runner.py:408] Time since start: 44938.79s, 	Step: 48000, 	{'train/ctc_loss': Array(0.1899466, dtype=float32), 'train/wer': 0.059771190902950265, 'validation/ctc_loss': Array(0.40191585, dtype=float32), 'validation/wer': 0.11582687276132732, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21901312, dtype=float32), 'test/wer': 0.0687953202120529, 'test/num_examples': 2472, 'score': 40751.27304506302, 'total_duration': 44938.79483628273, 'accumulated_submission_time': 40751.27304506302, 'accumulated_eval_time': 4183.648021221161, 'accumulated_logging_time': 1.6699771881103516}
I0213 01:56:57.185082 139647035000576 logging_writer.py:48] [48000] accumulated_eval_time=4183.648021, accumulated_logging_time=1.669977, accumulated_submission_time=40751.273045, global_step=48000, preemption_count=0, score=40751.273045, test/ctc_loss=0.21901312470436096, test/num_examples=2472, test/wer=0.068795, total_duration=44938.794836, train/ctc_loss=0.18994660675525665, train/wer=0.059771, validation/ctc_loss=0.40191584825515747, validation/num_examples=5348, validation/wer=0.115827
I0213 01:56:57.207749 139647026607872 logging_writer.py:48] [48000] global_step=48000, preemption_count=0, score=40751.273045
I0213 01:56:57.447274 139803787056960 checkpoints.py:490] Saving checkpoint at step: 48000
I0213 01:56:58.461925 139803787056960 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_1/librispeech_deepspeech_jax/trial_4/checkpoint_48000
I0213 01:56:58.481686 139803787056960 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_1/librispeech_deepspeech_jax/trial_4/checkpoint_48000.
I0213 01:56:59.823754 139803787056960 submission_runner.py:583] Tuning trial 4/5
I0213 01:56:59.824030 139803787056960 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.004958460849689891, one_minus_beta1=0.13625575743, beta2=0.6291854735396584, weight_decay=0.1147386261512052, warmup_factor=0.02)
I0213 01:56:59.838903 139803787056960 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(31.425632, dtype=float32), 'train/wer': 4.572452038869513, 'validation/ctc_loss': Array(30.812973, dtype=float32), 'validation/wer': 4.2336619133591435, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.93976, dtype=float32), 'test/wer': 4.561148010480775, 'test/num_examples': 2472, 'score': 16.132816791534424, 'total_duration': 230.04052543640137, 'accumulated_submission_time': 16.132816791534424, 'accumulated_eval_time': 213.90761828422546, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1695, {'train/ctc_loss': Array(1.6061379, dtype=float32), 'train/wer': 0.43652306734930557, 'validation/ctc_loss': Array(1.737651, dtype=float32), 'validation/wer': 0.44028114349710845, 'validation/num_examples': 5348, 'test/ctc_loss': Array(1.3081437, dtype=float32), 'test/wer': 0.38206081286941684, 'test/num_examples': 2472, 'score': 1456.1565909385681, 'total_duration': 1802.423523426056, 'accumulated_submission_time': 1456.1565909385681, 'accumulated_eval_time': 346.16198992729187, 'accumulated_logging_time': 0.031929969787597656, 'global_step': 1695, 'preemption_count': 0}), (3406, {'train/ctc_loss': Array(0.86962605, dtype=float32), 'train/wer': 0.25982874715626536, 'validation/ctc_loss': Array(1.0091406, dtype=float32), 'validation/wer': 0.2794732421290441, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.67142403, dtype=float32), 'test/wer': 0.20847805333820812, 'test/num_examples': 2472, 'score': 2896.9300112724304, 'total_duration': 3377.6903870105743, 'accumulated_submission_time': 2896.9300112724304, 'accumulated_eval_time': 480.506609916687, 'accumulated_logging_time': 0.09938645362854004, 'global_step': 3406, 'preemption_count': 0}), (5080, {'train/ctc_loss': Array(0.8732126, dtype=float32), 'train/wer': 0.2650956794108236, 'validation/ctc_loss': Array(0.9360504, dtype=float32), 'validation/wer': 0.26124525715168423, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6041575, dtype=float32), 'test/wer': 0.18893831373265899, 'test/num_examples': 2472, 'score': 4338.122195243835, 'total_duration': 4953.06637597084, 'accumulated_submission_time': 4338.122195243835, 'accumulated_eval_time': 614.574684381485, 'accumulated_logging_time': 0.14055585861206055, 'global_step': 5080, 'preemption_count': 0}), (6770, {'train/ctc_loss': Array(0.7836887, dtype=float32), 'train/wer': 0.23927822214244848, 'validation/ctc_loss': Array(0.9076595, dtype=float32), 'validation/wer': 0.25304845670370835, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5918397, dtype=float32), 'test/wer': 0.18637905469908395, 'test/num_examples': 2472, 'score': 5778.4819252491, 'total_duration': 6528.09968495369, 'accumulated_submission_time': 5778.4819252491, 'accumulated_eval_time': 749.1158313751221, 'accumulated_logging_time': 0.19507622718811035, 'global_step': 6770, 'preemption_count': 0}), (8473, {'train/ctc_loss': Array(0.7977896, dtype=float32), 'train/wer': 0.23808503154608807, 'validation/ctc_loss': Array(0.8691856, dtype=float32), 'validation/wer': 0.24392480956196838, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5595584, dtype=float32), 'test/wer': 0.17303434688115696, 'test/num_examples': 2472, 'score': 7219.33767747879, 'total_duration': 8105.016730308533, 'accumulated_submission_time': 7219.33767747879, 'accumulated_eval_time': 885.0470359325409, 'accumulated_logging_time': 0.2474684715270996, 'global_step': 8473, 'preemption_count': 0}), (10157, {'train/ctc_loss': Array(0.70206696, dtype=float32), 'train/wer': 0.21623922703828005, 'validation/ctc_loss': Array(0.8392556, dtype=float32), 'validation/wer': 0.236529345317976, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5371239, dtype=float32), 'test/wer': 0.1710641236568968, 'test/num_examples': 2472, 'score': 8659.357873678207, 'total_duration': 9679.683911561966, 'accumulated_submission_time': 8659.357873678207, 'accumulated_eval_time': 1019.487206697464, 'accumulated_logging_time': 0.3779764175415039, 'global_step': 10157, 'preemption_count': 0}), (11856, {'train/ctc_loss': Array(0.71111774, dtype=float32), 'train/wer': 0.221663515637457, 'validation/ctc_loss': Array(0.8253648, dtype=float32), 'validation/wer': 0.23265782944089905, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5218247, dtype=float32), 'test/wer': 0.16379257814880263, 'test/num_examples': 2472, 'score': 10100.010554075241, 'total_duration': 11253.55560541153, 'accumulated_submission_time': 10100.010554075241, 'accumulated_eval_time': 1152.574282169342, 'accumulated_logging_time': 0.43176698684692383, 'global_step': 11856, 'preemption_count': 0}), (13549, {'train/ctc_loss': Array(0.66694856, dtype=float32), 'train/wer': 0.20693933669420195, 'validation/ctc_loss': Array(0.79508895, dtype=float32), 'validation/wer': 0.22329281597265802, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.49158484, dtype=float32), 'test/wer': 0.15511953364613165, 'test/num_examples': 2472, 'score': 11540.073407173157, 'total_duration': 12827.867657899857, 'accumulated_submission_time': 11540.073407173157, 'accumulated_eval_time': 1286.6929433345795, 'accumulated_logging_time': 0.48450231552124023, 'global_step': 13549, 'preemption_count': 0}), (15263, {'train/ctc_loss': Array(0.44968167, dtype=float32), 'train/wer': 0.14862021894488905, 'validation/ctc_loss': Array(0.77568215, dtype=float32), 'validation/wer': 0.2198847234424631, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.47875628, dtype=float32), 'test/wer': 0.152722767249609, 'test/num_examples': 2472, 'score': 12981.39877486229, 'total_duration': 14415.474444389343, 'accumulated_submission_time': 12981.39877486229, 'accumulated_eval_time': 1432.8420944213867, 'accumulated_logging_time': 0.5385165214538574, 'global_step': 15263, 'preemption_count': 0}), (16982, {'train/ctc_loss': Array(0.42352477, dtype=float32), 'train/wer': 0.1388208673010756, 'validation/ctc_loss': Array(0.74860513, dtype=float32), 'validation/wer': 0.2146615561369802, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.46607754, dtype=float32), 'test/wer': 0.14851827026587858, 'test/num_examples': 2472, 'score': 14422.54798579216, 'total_duration': 15996.109202861786, 'accumulated_submission_time': 14422.54798579216, 'accumulated_eval_time': 1572.188811302185, 'accumulated_logging_time': 0.5987732410430908, 'global_step': 16982, 'preemption_count': 0}), (18668, {'train/ctc_loss': Array(0.41050774, dtype=float32), 'train/wer': 0.13493603394696577, 'validation/ctc_loss': Array(0.7495671, dtype=float32), 'validation/wer': 0.21057763789258233, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4562326, dtype=float32), 'test/wer': 0.14309507850425526, 'test/num_examples': 2472, 'score': 15862.62443113327, 'total_duration': 17574.10783100128, 'accumulated_submission_time': 15862.62443113327, 'accumulated_eval_time': 1709.9824848175049, 'accumulated_logging_time': 0.6509594917297363, 'global_step': 18668, 'preemption_count': 0}), (20356, {'train/ctc_loss': Array(0.380554, dtype=float32), 'train/wer': 0.1286975115825664, 'validation/ctc_loss': Array(0.72028995, dtype=float32), 'validation/wer': 0.20350077719957133, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.43282682, dtype=float32), 'test/wer': 0.1374281477870534, 'test/num_examples': 2472, 'score': 17302.69507241249, 'total_duration': 19152.583884239197, 'accumulated_submission_time': 17302.69507241249, 'accumulated_eval_time': 1848.2566344738007, 'accumulated_logging_time': 0.7045087814331055, 'global_step': 20356, 'preemption_count': 0}), (22053, {'train/ctc_loss': Array(0.3735527, dtype=float32), 'train/wer': 0.1240751796336965, 'validation/ctc_loss': Array(0.6839187, dtype=float32), 'validation/wer': 0.19496606389449395, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4144221, dtype=float32), 'test/wer': 0.13328458554221762, 'test/num_examples': 2472, 'score': 18742.878882169724, 'total_duration': 20729.05018377304, 'accumulated_submission_time': 18742.878882169724, 'accumulated_eval_time': 1984.4048948287964, 'accumulated_logging_time': 0.7557229995727539, 'global_step': 22053, 'preemption_count': 0}), (23726, {'train/ctc_loss': Array(0.34353802, dtype=float32), 'train/wer': 0.11765667258895292, 'validation/ctc_loss': Array(0.66541904, dtype=float32), 'validation/wer': 0.18818849744634428, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.39957988, dtype=float32), 'test/wer': 0.12749578534722644, 'test/num_examples': 2472, 'score': 20183.429557085037, 'total_duration': 22309.68611574173, 'accumulated_submission_time': 20183.429557085037, 'accumulated_eval_time': 2124.35813331604, 'accumulated_logging_time': 0.811715841293335, 'global_step': 23726, 'preemption_count': 0}), (25440, {'train/ctc_loss': Array(0.38745674, dtype=float32), 'train/wer': 0.12446508289515897, 'validation/ctc_loss': Array(0.6517793, dtype=float32), 'validation/wer': 0.18645065989553666, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.38806707, dtype=float32), 'test/wer': 0.12479434525623058, 'test/num_examples': 2472, 'score': 21623.96768260002, 'total_duration': 23885.734208345413, 'accumulated_submission_time': 21623.96768260002, 'accumulated_eval_time': 2259.7342801094055, 'accumulated_logging_time': 0.8676271438598633, 'global_step': 25440, 'preemption_count': 0}), (27158, {'train/ctc_loss': Array(0.3255008, dtype=float32), 'train/wer': 0.11006443645805297, 'validation/ctc_loss': Array(0.624412, dtype=float32), 'validation/wer': 0.17998204234530832, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.37232503, dtype=float32), 'test/wer': 0.11697438709808462, 'test/num_examples': 2472, 'score': 23064.55658864975, 'total_duration': 25462.754029750824, 'accumulated_submission_time': 23064.55658864975, 'accumulated_eval_time': 2396.0211186408997, 'accumulated_logging_time': 0.9317009449005127, 'global_step': 27158, 'preemption_count': 0}), (28839, {'train/ctc_loss': Array(0.2880375, dtype=float32), 'train/wer': 0.09923988480241726, 'validation/ctc_loss': Array(0.6031776, dtype=float32), 'validation/wer': 0.17224866524421445, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.35233474, dtype=float32), 'test/wer': 0.11291207117177503, 'test/num_examples': 2472, 'score': 24505.141978740692, 'total_duration': 27042.07692527771, 'accumulated_submission_time': 24505.141978740692, 'accumulated_eval_time': 2534.6256392002106, 'accumulated_logging_time': 0.9876172542572021, 'global_step': 28839, 'preemption_count': 0}), (30528, {'train/ctc_loss': Array(0.2754457, dtype=float32), 'train/wer': 0.09157254099449863, 'validation/ctc_loss': Array(0.5681978, dtype=float32), 'validation/wer': 0.1625843575311121, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33330095, dtype=float32), 'test/wer': 0.1068389088619422, 'test/num_examples': 2472, 'score': 25945.459337949753, 'total_duration': 28620.165376901627, 'accumulated_submission_time': 25945.459337949753, 'accumulated_eval_time': 2672.2584941387177, 'accumulated_logging_time': 1.0459904670715332, 'global_step': 30528, 'preemption_count': 0}), (32240, {'train/ctc_loss': Array(0.25709972, dtype=float32), 'train/wer': 0.0892764857881137, 'validation/ctc_loss': Array(0.5593565, dtype=float32), 'validation/wer': 0.15988105467430028, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32057795, dtype=float32), 'test/wer': 0.1023500497633701, 'test/num_examples': 2472, 'score': 27385.520001888275, 'total_duration': 30198.859600305557, 'accumulated_submission_time': 27385.520001888275, 'accumulated_eval_time': 2810.763015270233, 'accumulated_logging_time': 1.0972692966461182, 'global_step': 32240, 'preemption_count': 0}), (33920, {'train/ctc_loss': Array(0.25858888, dtype=float32), 'train/wer': 0.08586270598575797, 'validation/ctc_loss': Array(0.5291889, dtype=float32), 'validation/wer': 0.15174218214468463, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29892173, dtype=float32), 'test/wer': 0.09550504742753844, 'test/num_examples': 2472, 'score': 28825.72840666771, 'total_duration': 31777.60964488983, 'accumulated_submission_time': 28825.72840666771, 'accumulated_eval_time': 2949.174058675766, 'accumulated_logging_time': 1.1517837047576904, 'global_step': 33920, 'preemption_count': 0}), (35595, {'train/ctc_loss': Array(0.24284497, dtype=float32), 'train/wer': 0.08170056815489198, 'validation/ctc_loss': Array(0.510084, dtype=float32), 'validation/wer': 0.14698243818608378, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28692654, dtype=float32), 'test/wer': 0.09213332520870147, 'test/num_examples': 2472, 'score': 30265.98497748375, 'total_duration': 33355.33820748329, 'accumulated_submission_time': 30265.98497748375, 'accumulated_eval_time': 3086.517218351364, 'accumulated_logging_time': 1.2039318084716797, 'global_step': 35595, 'preemption_count': 0}), (37298, {'train/ctc_loss': Array(0.2170262, dtype=float32), 'train/wer': 0.07354023870506134, 'validation/ctc_loss': Array(0.48365703, dtype=float32), 'validation/wer': 0.13989592284001273, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27283636, dtype=float32), 'test/wer': 0.08638514817297341, 'test/num_examples': 2472, 'score': 31705.910080432892, 'total_duration': 34931.98729777336, 'accumulated_submission_time': 31705.910080432892, 'accumulated_eval_time': 3223.1055147647858, 'accumulated_logging_time': 1.2620007991790771, 'global_step': 37298, 'preemption_count': 0}), (38983, {'train/ctc_loss': Array(0.20192882, dtype=float32), 'train/wer': 0.06945914796639427, 'validation/ctc_loss': Array(0.46267018, dtype=float32), 'validation/wer': 0.1323266748409396, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2571431, dtype=float32), 'test/wer': 0.08262750594113705, 'test/num_examples': 2472, 'score': 33146.00746154785, 'total_duration': 36512.47426152229, 'accumulated_submission_time': 33146.00746154785, 'accumulated_eval_time': 3363.3636882305145, 'accumulated_logging_time': 1.3168060779571533, 'global_step': 38983, 'preemption_count': 0}), (40699, {'train/ctc_loss': Array(0.1680867, dtype=float32), 'train/wer': 0.05816655225220468, 'validation/ctc_loss': Array(0.44180185, dtype=float32), 'validation/wer': 0.12681386794365546, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24477644, dtype=float32), 'test/wer': 0.07754961103325006, 'test/num_examples': 2472, 'score': 34587.25555706024, 'total_duration': 38090.030962228775, 'accumulated_submission_time': 34587.25555706024, 'accumulated_eval_time': 3499.5277755260468, 'accumulated_logging_time': 1.3820362091064453, 'global_step': 40699, 'preemption_count': 0}), (42391, {'train/ctc_loss': Array(0.16477492, dtype=float32), 'train/wer': 0.055599824287188286, 'validation/ctc_loss': Array(0.42381123, dtype=float32), 'validation/wer': 0.12162931925041273, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23368566, dtype=float32), 'test/wer': 0.07391383827920298, 'test/num_examples': 2472, 'score': 36027.65841794014, 'total_duration': 39669.712934970856, 'accumulated_submission_time': 36027.65841794014, 'accumulated_eval_time': 3638.6739313602448, 'accumulated_logging_time': 1.4387574195861816, 'global_step': 42391, 'preemption_count': 0}), (44091, {'train/ctc_loss': Array(0.16973253, dtype=float32), 'train/wer': 0.05735663542313822, 'validation/ctc_loss': Array(0.4103643, dtype=float32), 'validation/wer': 0.11811502553655734, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2251991, dtype=float32), 'test/wer': 0.07064367395852375, 'test/num_examples': 2472, 'score': 37468.0641913414, 'total_duration': 41244.99890422821, 'accumulated_submission_time': 37468.0641913414, 'accumulated_eval_time': 3773.414123773575, 'accumulated_logging_time': 1.5009901523590088, 'global_step': 44091, 'preemption_count': 0}), (45803, {'train/ctc_loss': Array(0.14928834, dtype=float32), 'train/wer': 0.050186845837355144, 'validation/ctc_loss': Array(0.40272284, dtype=float32), 'validation/wer': 0.11580756345520724, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22022197, dtype=float32), 'test/wer': 0.06936404444173623, 'test/num_examples': 2472, 'score': 38908.11180472374, 'total_duration': 42822.377161979675, 'accumulated_submission_time': 38908.11180472374, 'accumulated_eval_time': 3910.608511686325, 'accumulated_logging_time': 1.557424783706665, 'global_step': 45803, 'preemption_count': 0}), (47502, {'train/ctc_loss': Array(0.16172755, dtype=float32), 'train/wer': 0.05604972504790242, 'validation/ctc_loss': Array(0.4016892, dtype=float32), 'validation/wer': 0.11590410998580766, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21897092, dtype=float32), 'test/wer': 0.0687953202120529, 'test/num_examples': 2472, 'score': 40348.411856889725, 'total_duration': 44399.88564610481, 'accumulated_submission_time': 40348.411856889725, 'accumulated_eval_time': 4047.680432319641, 'accumulated_logging_time': 1.617690086364746, 'global_step': 47502, 'preemption_count': 0}), (48000, {'train/ctc_loss': Array(0.1899466, dtype=float32), 'train/wer': 0.059771190902950265, 'validation/ctc_loss': Array(0.40191585, dtype=float32), 'validation/wer': 0.11582687276132732, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21901312, dtype=float32), 'test/wer': 0.0687953202120529, 'test/num_examples': 2472, 'score': 40751.27304506302, 'total_duration': 44938.79483628273, 'accumulated_submission_time': 40751.27304506302, 'accumulated_eval_time': 4183.648021221161, 'accumulated_logging_time': 1.6699771881103516, 'global_step': 48000, 'preemption_count': 0})], 'global_step': 48000}
I0213 01:56:59.839144 139803787056960 submission_runner.py:586] Timing: 40751.27304506302
I0213 01:56:59.839223 139803787056960 submission_runner.py:588] Total number of evals: 30
I0213 01:56:59.839312 139803787056960 submission_runner.py:589] ====================
I0213 01:56:59.839389 139803787056960 submission_runner.py:542] Using RNG seed 808887856
I0213 01:56:59.842823 139803787056960 submission_runner.py:551] --- Tuning run 5/5 ---
I0213 01:56:59.842952 139803787056960 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_1/librispeech_deepspeech_jax/trial_5.
I0213 01:56:59.845898 139803787056960 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_1/librispeech_deepspeech_jax/trial_5/hparams.json.
I0213 01:56:59.848397 139803787056960 submission_runner.py:206] Initializing dataset.
I0213 01:56:59.848524 139803787056960 submission_runner.py:213] Initializing model.
I0213 01:57:01.077311 139803787056960 submission_runner.py:255] Initializing optimizer.
I0213 01:57:01.220500 139803787056960 submission_runner.py:262] Initializing metrics bundle.
I0213 01:57:01.220675 139803787056960 submission_runner.py:280] Initializing checkpoint and logger.
I0213 01:57:01.224825 139803787056960 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_1/librispeech_deepspeech_jax/trial_5 with prefix checkpoint_
I0213 01:57:01.224976 139803787056960 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_1/librispeech_deepspeech_jax/trial_5/meta_data_0.json.
I0213 01:57:01.225374 139803787056960 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0213 01:57:01.225468 139803787056960 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0213 01:57:01.859211 139803787056960 logger_utils.py:220] Unable to record git information. Continuing without it.
I0213 01:57:02.371193 139803787056960 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_1/librispeech_deepspeech_jax/trial_5/flags_0.json.
I0213 01:57:02.392080 139803787056960 submission_runner.py:314] Starting training loop.
I0213 01:57:02.395787 139803787056960 input_pipeline.py:20] Loading split = train-clean-100
I0213 01:57:02.438338 139803787056960 input_pipeline.py:20] Loading split = train-clean-360
I0213 01:57:03.068017 139803787056960 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0213 01:57:18.555850 139646875539200 logging_writer.py:48] [0] global_step=0, grad_norm=16.314170837402344, loss=32.89440155029297
I0213 01:57:18.570779 139803787056960 spec.py:321] Evaluating on the training split.
I0213 01:59:21.142624 139803787056960 spec.py:333] Evaluating on the validation split.
I0213 02:00:28.320818 139803787056960 spec.py:349] Evaluating on the test split.
I0213 02:01:03.363986 139803787056960 submission_runner.py:408] Time since start: 240.97s, 	Step: 1, 	{'train/ctc_loss': Array(30.994576, dtype=float32), 'train/wer': 4.707897006283109, 'validation/ctc_loss': Array(30.812992, dtype=float32), 'validation/wer': 4.233575021481603, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.939766, dtype=float32), 'test/wer': 4.561066764162249, 'test/num_examples': 2472, 'score': 16.178600549697876, 'total_duration': 240.9686577320099, 'accumulated_submission_time': 16.178600549697876, 'accumulated_eval_time': 224.78997611999512, 'accumulated_logging_time': 0}
I0213 02:01:03.380781 139647035000576 logging_writer.py:48] [1] accumulated_eval_time=224.789976, accumulated_logging_time=0, accumulated_submission_time=16.178601, global_step=1, preemption_count=0, score=16.178601, test/ctc_loss=30.93976593017578, test/num_examples=2472, test/wer=4.561067, total_duration=240.968658, train/ctc_loss=30.99457550048828, train/wer=4.707897, validation/ctc_loss=30.812992095947266, validation/num_examples=5348, validation/wer=4.233575
I0213 02:02:29.787850 139646925895424 logging_writer.py:48] [100] global_step=100, grad_norm=1.047131061553955, loss=6.1214423179626465
I0213 02:03:48.147795 139646934288128 logging_writer.py:48] [200] global_step=200, grad_norm=0.36795318126678467, loss=5.825891494750977
I0213 02:05:05.756791 139646925895424 logging_writer.py:48] [300] global_step=300, grad_norm=0.9897778034210205, loss=5.68695592880249
I0213 02:06:23.980051 139646934288128 logging_writer.py:48] [400] global_step=400, grad_norm=1.2301009893417358, loss=5.078728199005127
I0213 02:07:49.408949 139646925895424 logging_writer.py:48] [500] global_step=500, grad_norm=0.8466424942016602, loss=4.218776702880859
I0213 02:09:16.582682 139646934288128 logging_writer.py:48] [600] global_step=600, grad_norm=1.4701292514801025, loss=3.7333741188049316
I0213 02:10:42.998762 139646925895424 logging_writer.py:48] [700] global_step=700, grad_norm=2.831468343734741, loss=3.3242082595825195
I0213 02:12:11.951428 139646934288128 logging_writer.py:48] [800] global_step=800, grad_norm=1.589080572128296, loss=3.068364143371582
I0213 02:13:40.193210 139646925895424 logging_writer.py:48] [900] global_step=900, grad_norm=3.211487054824829, loss=2.941020965576172
I0213 02:15:11.006512 139646934288128 logging_writer.py:48] [1000] global_step=1000, grad_norm=2.5095832347869873, loss=2.8112361431121826
I0213 02:16:35.046245 139647035000576 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.9391000270843506, loss=2.6205899715423584
I0213 02:17:54.563276 139647026607872 logging_writer.py:48] [1200] global_step=1200, grad_norm=2.1809656620025635, loss=2.606700897216797
I0213 02:19:15.442739 139647035000576 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.890928030014038, loss=2.477482557296753
I0213 02:20:40.936060 139647026607872 logging_writer.py:48] [1400] global_step=1400, grad_norm=2.182807207107544, loss=2.3801565170288086
I0213 02:22:09.841798 139647035000576 logging_writer.py:48] [1500] global_step=1500, grad_norm=2.297564744949341, loss=2.3753902912139893
I0213 02:23:40.465366 139647026607872 logging_writer.py:48] [1600] global_step=1600, grad_norm=3.3278207778930664, loss=2.2742230892181396
I0213 02:25:03.433855 139803787056960 spec.py:321] Evaluating on the training split.
I0213 02:25:58.225497 139803787056960 spec.py:333] Evaluating on the validation split.
I0213 02:26:49.541700 139803787056960 spec.py:349] Evaluating on the test split.
I0213 02:27:16.664108 139803787056960 submission_runner.py:408] Time since start: 1814.27s, 	Step: 1692, 	{'train/ctc_loss': Array(2.0471356, dtype=float32), 'train/wer': 0.4961872165120053, 'validation/ctc_loss': Array(2.49202, dtype=float32), 'validation/wer': 0.547013333075876, 'validation/num_examples': 5348, 'test/ctc_loss': Array(2.0015655, dtype=float32), 'test/wer': 0.4741332033392237, 'test/num_examples': 2472, 'score': 1456.1483154296875, 'total_duration': 1814.265554189682, 'accumulated_submission_time': 1456.1483154296875, 'accumulated_eval_time': 358.0138473510742, 'accumulated_logging_time': 0.028545856475830078}
I0213 02:27:16.700923 139647035000576 logging_writer.py:48] [1692] accumulated_eval_time=358.013847, accumulated_logging_time=0.028546, accumulated_submission_time=1456.148315, global_step=1692, preemption_count=0, score=1456.148315, test/ctc_loss=2.001565456390381, test/num_examples=2472, test/wer=0.474133, total_duration=1814.265554, train/ctc_loss=2.047135591506958, train/wer=0.496187, validation/ctc_loss=2.4920198917388916, validation/num_examples=5348, validation/wer=0.547013
I0213 02:27:23.761651 139647026607872 logging_writer.py:48] [1700] global_step=1700, grad_norm=1.8867518901824951, loss=2.232844352722168
I0213 02:28:39.748858 139647035000576 logging_writer.py:48] [1800] global_step=1800, grad_norm=2.0974676609039307, loss=2.1664373874664307
I0213 02:29:55.545888 139647026607872 logging_writer.py:48] [1900] global_step=1900, grad_norm=1.92518150806427, loss=2.1870920658111572
I0213 02:31:23.006848 139647035000576 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.9742491245269775, loss=2.0246546268463135
I0213 02:32:52.180502 139647035000576 logging_writer.py:48] [2100] global_step=2100, grad_norm=2.567237377166748, loss=2.1128320693969727
I0213 02:34:08.183816 139647026607872 logging_writer.py:48] [2200] global_step=2200, grad_norm=4.352479457855225, loss=2.1412298679351807
I0213 02:35:30.290266 139647035000576 logging_writer.py:48] [2300] global_step=2300, grad_norm=2.582414388656616, loss=2.018096446990967
I0213 02:36:54.871836 139647026607872 logging_writer.py:48] [2400] global_step=2400, grad_norm=3.7164723873138428, loss=2.0464091300964355
I0213 02:38:22.071061 139647035000576 logging_writer.py:48] [2500] global_step=2500, grad_norm=2.098641872406006, loss=2.0538790225982666
I0213 02:39:49.103383 139647026607872 logging_writer.py:48] [2600] global_step=2600, grad_norm=2.6859958171844482, loss=1.9233487844467163
I0213 02:41:18.438673 139647035000576 logging_writer.py:48] [2700] global_step=2700, grad_norm=2.196730136871338, loss=1.940828561782837
I0213 02:42:47.968644 139647026607872 logging_writer.py:48] [2800] global_step=2800, grad_norm=2.326263189315796, loss=1.9597870111465454
I0213 02:44:15.000705 139647035000576 logging_writer.py:48] [2900] global_step=2900, grad_norm=2.7072370052337646, loss=1.9225847721099854
I0213 02:45:41.660869 139647026607872 logging_writer.py:48] [3000] global_step=3000, grad_norm=2.75264048576355, loss=1.9146732091903687
I0213 02:47:13.268095 139647035000576 logging_writer.py:48] [3100] global_step=3100, grad_norm=2.0804314613342285, loss=1.871281385421753
I0213 02:48:30.539869 139647026607872 logging_writer.py:48] [3200] global_step=3200, grad_norm=3.2089295387268066, loss=1.9642270803451538
I0213 02:49:48.149432 139647035000576 logging_writer.py:48] [3300] global_step=3300, grad_norm=2.2794995307922363, loss=1.908044457435608
I0213 02:51:10.258918 139647026607872 logging_writer.py:48] [3400] global_step=3400, grad_norm=3.932331085205078, loss=1.9071104526519775
I0213 02:51:17.067999 139803787056960 spec.py:321] Evaluating on the training split.
I0213 02:52:12.252327 139803787056960 spec.py:333] Evaluating on the validation split.
I0213 02:53:04.447180 139803787056960 spec.py:349] Evaluating on the test split.
I0213 02:53:32.399497 139803787056960 submission_runner.py:408] Time since start: 3390.00s, 	Step: 3409, 	{'train/ctc_loss': Array(0.7372342, dtype=float32), 'train/wer': 0.22996908588842316, 'validation/ctc_loss': Array(0.913854, dtype=float32), 'validation/wer': 0.25889917645809396, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5924073, dtype=float32), 'test/wer': 0.18442914305445535, 'test/num_examples': 2472, 'score': 2896.421770811081, 'total_duration': 3390.00124835968, 'accumulated_submission_time': 2896.421770811081, 'accumulated_eval_time': 493.33926486968994, 'accumulated_logging_time': 0.08494257926940918}
I0213 02:53:32.433056 139647035000576 logging_writer.py:48] [3409] accumulated_eval_time=493.339265, accumulated_logging_time=0.084943, accumulated_submission_time=2896.421771, global_step=3409, preemption_count=0, score=2896.421771, test/ctc_loss=0.5924072861671448, test/num_examples=2472, test/wer=0.184429, total_duration=3390.001248, train/ctc_loss=0.7372341752052307, train/wer=0.229969, validation/ctc_loss=0.9138540029525757, validation/num_examples=5348, validation/wer=0.258899
I0213 02:54:42.108104 139647026607872 logging_writer.py:48] [3500] global_step=3500, grad_norm=2.1817638874053955, loss=1.836454153060913
I0213 02:55:57.887190 139647035000576 logging_writer.py:48] [3600] global_step=3600, grad_norm=3.3424618244171143, loss=1.8820470571517944
I0213 02:57:19.596962 139647026607872 logging_writer.py:48] [3700] global_step=3700, grad_norm=3.807504177093506, loss=1.9324451684951782
I0213 02:58:48.033859 139647035000576 logging_writer.py:48] [3800] global_step=3800, grad_norm=5.212904930114746, loss=1.8714250326156616
I0213 03:00:18.184927 139647026607872 logging_writer.py:48] [3900] global_step=3900, grad_norm=4.240532875061035, loss=1.7575352191925049
I0213 03:01:48.534070 139647035000576 logging_writer.py:48] [4000] global_step=4000, grad_norm=1.8412821292877197, loss=1.7763221263885498
I0213 03:03:17.758763 139647026607872 logging_writer.py:48] [4100] global_step=4100, grad_norm=3.693422317504883, loss=1.8039276599884033
I0213 03:04:40.026983 139647035000576 logging_writer.py:48] [4200] global_step=4200, grad_norm=3.4931325912475586, loss=1.7469223737716675
I0213 03:05:59.220413 139647026607872 logging_writer.py:48] [4300] global_step=4300, grad_norm=2.6748173236846924, loss=1.7569327354431152
I0213 03:07:19.941442 139647035000576 logging_writer.py:48] [4400] global_step=4400, grad_norm=2.4102725982666016, loss=1.8010848760604858
I0213 03:08:44.020431 139647026607872 logging_writer.py:48] [4500] global_step=4500, grad_norm=3.781616687774658, loss=1.775971531867981
I0213 03:10:10.304789 139647035000576 logging_writer.py:48] [4600] global_step=4600, grad_norm=1.9447240829467773, loss=1.7705800533294678
I0213 03:11:37.229980 139647026607872 logging_writer.py:48] [4700] global_step=4700, grad_norm=2.4325649738311768, loss=1.795839786529541
I0213 03:13:06.565309 139647035000576 logging_writer.py:48] [4800] global_step=4800, grad_norm=2.7663626670837402, loss=1.7575337886810303
I0213 03:14:37.008580 139647026607872 logging_writer.py:48] [4900] global_step=4900, grad_norm=3.3780226707458496, loss=1.7424473762512207
I0213 03:16:06.502722 139647035000576 logging_writer.py:48] [5000] global_step=5000, grad_norm=2.1473519802093506, loss=1.7955749034881592
I0213 03:17:32.998127 139803787056960 spec.py:321] Evaluating on the training split.
I0213 03:18:27.470635 139803787056960 spec.py:333] Evaluating on the validation split.
I0213 03:19:21.665771 139803787056960 spec.py:349] Evaluating on the test split.
I0213 03:19:48.634634 139803787056960 submission_runner.py:408] Time since start: 4966.24s, 	Step: 5098, 	{'train/ctc_loss': Array(0.63931066, dtype=float32), 'train/wer': 0.2019689524050605, 'validation/ctc_loss': Array(0.7830972, dtype=float32), 'validation/wer': 0.2235824555644593, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.48912373, dtype=float32), 'test/wer': 0.15932403062986208, 'test/num_examples': 2472, 'score': 4336.899610280991, 'total_duration': 4966.236393213272, 'accumulated_submission_time': 4336.899610280991, 'accumulated_eval_time': 628.9697065353394, 'accumulated_logging_time': 0.13415312767028809}
I0213 03:19:48.677086 139647035000576 logging_writer.py:48] [5098] accumulated_eval_time=628.969707, accumulated_logging_time=0.134153, accumulated_submission_time=4336.899610, global_step=5098, preemption_count=0, score=4336.899610, test/ctc_loss=0.48912373185157776, test/num_examples=2472, test/wer=0.159324, total_duration=4966.236393, train/ctc_loss=0.6393106579780579, train/wer=0.201969, validation/ctc_loss=0.7830972075462341, validation/num_examples=5348, validation/wer=0.223582
I0213 03:19:51.094717 139647026607872 logging_writer.py:48] [5100] global_step=5100, grad_norm=2.1486053466796875, loss=1.7607905864715576
I0213 03:21:11.097856 139647035000576 logging_writer.py:48] [5200] global_step=5200, grad_norm=2.395881175994873, loss=1.7219141721725464
I0213 03:22:29.244262 139647026607872 logging_writer.py:48] [5300] global_step=5300, grad_norm=2.4783737659454346, loss=1.7344675064086914
I0213 03:23:49.377274 139647035000576 logging_writer.py:48] [5400] global_step=5400, grad_norm=3.9019927978515625, loss=1.6969795227050781
I0213 03:25:15.204457 139647026607872 logging_writer.py:48] [5500] global_step=5500, grad_norm=3.3534719944000244, loss=1.7456159591674805
I0213 03:26:43.419458 139647035000576 logging_writer.py:48] [5600] global_step=5600, grad_norm=3.7315993309020996, loss=1.7008861303329468
I0213 03:28:14.405346 139647026607872 logging_writer.py:48] [5700] global_step=5700, grad_norm=3.8582894802093506, loss=1.6507956981658936
I0213 03:29:44.624225 139647035000576 logging_writer.py:48] [5800] global_step=5800, grad_norm=2.2718799114227295, loss=1.7731846570968628
I0213 03:31:11.573157 139647026607872 logging_writer.py:48] [5900] global_step=5900, grad_norm=3.457037925720215, loss=1.6527535915374756
I0213 03:32:40.163386 139647035000576 logging_writer.py:48] [6000] global_step=6000, grad_norm=3.6861374378204346, loss=1.714461326599121
I0213 03:34:11.591414 139647026607872 logging_writer.py:48] [6100] global_step=6100, grad_norm=2.54514479637146, loss=1.7557880878448486
I0213 03:35:42.872955 139647035000576 logging_writer.py:48] [6200] global_step=6200, grad_norm=3.065911293029785, loss=1.7228034734725952
I0213 03:37:01.388234 139647026607872 logging_writer.py:48] [6300] global_step=6300, grad_norm=3.6579928398132324, loss=1.7166497707366943
I0213 03:38:21.119768 139647035000576 logging_writer.py:48] [6400] global_step=6400, grad_norm=2.822355270385742, loss=1.628327488899231
I0213 03:39:43.185096 139647026607872 logging_writer.py:48] [6500] global_step=6500, grad_norm=3.1168296337127686, loss=1.6962227821350098
I0213 03:41:08.742518 139647035000576 logging_writer.py:48] [6600] global_step=6600, grad_norm=3.7100889682769775, loss=1.6413793563842773
I0213 03:42:35.381652 139647026607872 logging_writer.py:48] [6700] global_step=6700, grad_norm=2.7120962142944336, loss=1.6969702243804932
I0213 03:43:49.146505 139803787056960 spec.py:321] Evaluating on the training split.
I0213 03:44:42.837452 139803787056960 spec.py:333] Evaluating on the validation split.
I0213 03:45:35.430671 139803787056960 spec.py:349] Evaluating on the test split.
I0213 03:46:02.379370 139803787056960 submission_runner.py:408] Time since start: 6539.98s, 	Step: 6781, 	{'train/ctc_loss': Array(0.7021821, dtype=float32), 'train/wer': 0.21959843927816616, 'validation/ctc_loss': Array(0.74043995, dtype=float32), 'validation/wer': 0.2126533883004914, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.44423807, dtype=float32), 'test/wer': 0.14409034590620112, 'test/num_examples': 2472, 'score': 5777.278906345367, 'total_duration': 6539.980969905853, 'accumulated_submission_time': 5777.278906345367, 'accumulated_eval_time': 762.1963560581207, 'accumulated_logging_time': 0.19386816024780273}
I0213 03:46:02.417556 139647035000576 logging_writer.py:48] [6781] accumulated_eval_time=762.196356, accumulated_logging_time=0.193868, accumulated_submission_time=5777.278906, global_step=6781, preemption_count=0, score=5777.278906, test/ctc_loss=0.4442380666732788, test/num_examples=2472, test/wer=0.144090, total_duration=6539.980970, train/ctc_loss=0.7021821141242981, train/wer=0.219598, validation/ctc_loss=0.7404399514198303, validation/num_examples=5348, validation/wer=0.212653
I0213 03:46:17.858757 139647026607872 logging_writer.py:48] [6800] global_step=6800, grad_norm=6.492822647094727, loss=1.6306158304214478
I0213 03:47:34.047315 139647035000576 logging_writer.py:48] [6900] global_step=6900, grad_norm=6.159027576446533, loss=1.7142634391784668
I0213 03:48:49.968693 139647026607872 logging_writer.py:48] [7000] global_step=7000, grad_norm=3.929028272628784, loss=1.7226693630218506
I0213 03:50:17.156690 139647035000576 logging_writer.py:48] [7100] global_step=7100, grad_norm=3.2089009284973145, loss=1.6297409534454346
I0213 03:51:46.466785 139647026607872 logging_writer.py:48] [7200] global_step=7200, grad_norm=2.832939624786377, loss=1.6266047954559326
I0213 03:53:10.482311 139647035000576 logging_writer.py:48] [7300] global_step=7300, grad_norm=3.546928882598877, loss=1.7136551141738892
I0213 03:54:26.617359 139647026607872 logging_writer.py:48] [7400] global_step=7400, grad_norm=4.456803321838379, loss=1.676185965538025
I0213 03:55:48.150100 139647035000576 logging_writer.py:48] [7500] global_step=7500, grad_norm=2.313530683517456, loss=1.6043459177017212
I0213 03:57:13.783144 139647026607872 logging_writer.py:48] [7600] global_step=7600, grad_norm=2.32596755027771, loss=1.6249456405639648
I0213 03:58:40.817350 139647035000576 logging_writer.py:48] [7700] global_step=7700, grad_norm=3.0875940322875977, loss=1.64097261428833
I0213 04:00:10.232673 139647026607872 logging_writer.py:48] [7800] global_step=7800, grad_norm=2.297267436981201, loss=1.5722674131393433
I0213 04:01:40.232087 139647035000576 logging_writer.py:48] [7900] global_step=7900, grad_norm=2.2040584087371826, loss=1.6613095998764038
I0213 04:03:09.850708 139647026607872 logging_writer.py:48] [8000] global_step=8000, grad_norm=3.5604031085968018, loss=1.6511621475219727
I0213 04:04:39.923391 139647035000576 logging_writer.py:48] [8100] global_step=8100, grad_norm=2.4406094551086426, loss=1.6233186721801758
I0213 04:06:10.898925 139647026607872 logging_writer.py:48] [8200] global_step=8200, grad_norm=2.3076205253601074, loss=1.644352674484253
I0213 04:07:34.923876 139647035000576 logging_writer.py:48] [8300] global_step=8300, grad_norm=2.5100815296173096, loss=1.6295958757400513
I0213 04:08:52.154792 139647026607872 logging_writer.py:48] [8400] global_step=8400, grad_norm=3.02577805519104, loss=1.6495758295059204
I0213 04:10:02.994419 139803787056960 spec.py:321] Evaluating on the training split.
I0213 04:10:56.574389 139803787056960 spec.py:333] Evaluating on the validation split.
I0213 04:11:49.503231 139803787056960 spec.py:349] Evaluating on the test split.
I0213 04:12:16.559599 139803787056960 submission_runner.py:408] Time since start: 8114.16s, 	Step: 8491, 	{'train/ctc_loss': Array(0.5751879, dtype=float32), 'train/wer': 0.1796062016228983, 'validation/ctc_loss': Array(0.7267631, dtype=float32), 'validation/wer': 0.206619230137965, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.44064087, dtype=float32), 'test/wer': 0.141490463713363, 'test/num_examples': 2472, 'score': 7217.765897512436, 'total_duration': 8114.1622631549835, 'accumulated_submission_time': 7217.765897512436, 'accumulated_eval_time': 895.7563931941986, 'accumulated_logging_time': 0.24855613708496094}
I0213 04:12:16.595731 139647035000576 logging_writer.py:48] [8491] accumulated_eval_time=895.756393, accumulated_logging_time=0.248556, accumulated_submission_time=7217.765898, global_step=8491, preemption_count=0, score=7217.765898, test/ctc_loss=0.4406408667564392, test/num_examples=2472, test/wer=0.141490, total_duration=8114.162263, train/ctc_loss=0.5751879215240479, train/wer=0.179606, validation/ctc_loss=0.726763129234314, validation/num_examples=5348, validation/wer=0.206619
I0213 04:12:24.263842 139647026607872 logging_writer.py:48] [8500] global_step=8500, grad_norm=3.2782745361328125, loss=1.613585352897644
I0213 04:13:39.573686 139647035000576 logging_writer.py:48] [8600] global_step=8600, grad_norm=2.8799142837524414, loss=1.5933009386062622
I0213 04:14:54.895843 139647026607872 logging_writer.py:48] [8700] global_step=8700, grad_norm=2.6368720531463623, loss=1.610871434211731
I0213 04:16:20.089522 139647035000576 logging_writer.py:48] [8800] global_step=8800, grad_norm=2.9713051319122314, loss=1.6141504049301147
I0213 04:17:50.464169 139647026607872 logging_writer.py:48] [8900] global_step=8900, grad_norm=2.394414186477661, loss=1.6359527111053467
I0213 04:19:21.634330 139647035000576 logging_writer.py:48] [9000] global_step=9000, grad_norm=2.9255430698394775, loss=1.596504807472229
I0213 04:20:52.758370 139647026607872 logging_writer.py:48] [9100] global_step=9100, grad_norm=3.4035351276397705, loss=1.6411771774291992
I0213 04:22:24.537111 139647035000576 logging_writer.py:48] [9200] global_step=9200, grad_norm=2.1435818672180176, loss=1.5862610340118408
I0213 04:23:52.379379 139647035000576 logging_writer.py:48] [9300] global_step=9300, grad_norm=4.646833896636963, loss=1.614306926727295
I0213 04:25:08.435344 139647026607872 logging_writer.py:48] [9400] global_step=9400, grad_norm=2.383327007293701, loss=1.5717211961746216
I0213 04:26:26.782686 139647035000576 logging_writer.py:48] [9500] global_step=9500, grad_norm=5.27329158782959, loss=1.5927164554595947
I0213 04:27:48.117449 139647026607872 logging_writer.py:48] [9600] global_step=9600, grad_norm=2.9965288639068604, loss=1.6146624088287354
I0213 04:29:13.610810 139647035000576 logging_writer.py:48] [9700] global_step=9700, grad_norm=4.099022388458252, loss=1.6424000263214111
I0213 04:30:42.686035 139647026607872 logging_writer.py:48] [9800] global_step=9800, grad_norm=2.9266865253448486, loss=1.6200623512268066
I0213 04:32:13.239454 139647035000576 logging_writer.py:48] [9900] global_step=9900, grad_norm=3.2698185443878174, loss=1.5735012292861938
I0213 04:33:41.060199 139647026607872 logging_writer.py:48] [10000] global_step=10000, grad_norm=2.015469789505005, loss=1.5530612468719482
I0213 04:35:11.684132 139647035000576 logging_writer.py:48] [10100] global_step=10100, grad_norm=4.587024211883545, loss=1.6491119861602783
I0213 04:36:17.092473 139803787056960 spec.py:321] Evaluating on the training split.
I0213 04:37:11.251434 139803787056960 spec.py:333] Evaluating on the validation split.
I0213 04:38:03.739341 139803787056960 spec.py:349] Evaluating on the test split.
I0213 04:38:31.153834 139803787056960 submission_runner.py:408] Time since start: 9688.75s, 	Step: 10176, 	{'train/ctc_loss': Array(0.53614444, dtype=float32), 'train/wer': 0.17392318103752233, 'validation/ctc_loss': Array(0.6622156, dtype=float32), 'validation/wer': 0.19043803160933412, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.39185888, dtype=float32), 'test/wer': 0.1262770905693336, 'test/num_examples': 2472, 'score': 8658.173372507095, 'total_duration': 9688.754879951477, 'accumulated_submission_time': 8658.173372507095, 'accumulated_eval_time': 1029.8109738826752, 'accumulated_logging_time': 0.3024752140045166}
I0213 04:38:31.189813 139647035000576 logging_writer.py:48] [10176] accumulated_eval_time=1029.810974, accumulated_logging_time=0.302475, accumulated_submission_time=8658.173373, global_step=10176, preemption_count=0, score=8658.173373, test/ctc_loss=0.39185887575149536, test/num_examples=2472, test/wer=0.126277, total_duration=9688.754880, train/ctc_loss=0.5361444354057312, train/wer=0.173923, validation/ctc_loss=0.6622155904769897, validation/num_examples=5348, validation/wer=0.190438
I0213 04:38:50.006605 139647026607872 logging_writer.py:48] [10200] global_step=10200, grad_norm=2.58436918258667, loss=1.5940518379211426
I0213 04:40:10.006927 139647035000576 logging_writer.py:48] [10300] global_step=10300, grad_norm=2.605592966079712, loss=1.5291684865951538
I0213 04:41:28.697709 139647026607872 logging_writer.py:48] [10400] global_step=10400, grad_norm=2.51567006111145, loss=1.5877634286880493
I0213 04:42:47.281197 139647035000576 logging_writer.py:48] [10500] global_step=10500, grad_norm=2.289616823196411, loss=1.5227967500686646
I0213 04:44:09.511126 139647026607872 logging_writer.py:48] [10600] global_step=10600, grad_norm=3.350846767425537, loss=1.5306769609451294
I0213 04:45:35.269492 139647035000576 logging_writer.py:48] [10700] global_step=10700, grad_norm=2.434887409210205, loss=1.5933418273925781
I0213 04:47:04.622628 139647026607872 logging_writer.py:48] [10800] global_step=10800, grad_norm=3.2034318447113037, loss=1.5746877193450928
I0213 04:48:33.605076 139647035000576 logging_writer.py:48] [10900] global_step=10900, grad_norm=3.10457444190979, loss=1.6415176391601562
I0213 04:50:03.534355 139647026607872 logging_writer.py:48] [11000] global_step=11000, grad_norm=2.6640262603759766, loss=1.597593903541565
I0213 04:51:34.896606 139647035000576 logging_writer.py:48] [11100] global_step=11100, grad_norm=2.050816059112549, loss=1.5644927024841309
I0213 04:53:04.919533 139647026607872 logging_writer.py:48] [11200] global_step=11200, grad_norm=2.496239185333252, loss=1.5915476083755493
I0213 04:54:35.146607 139647035000576 logging_writer.py:48] [11300] global_step=11300, grad_norm=3.054809808731079, loss=1.5939825773239136
I0213 04:55:59.131527 139647035000576 logging_writer.py:48] [11400] global_step=11400, grad_norm=2.689744710922241, loss=1.4933305978775024
I0213 04:57:16.251531 139647026607872 logging_writer.py:48] [11500] global_step=11500, grad_norm=2.9961326122283936, loss=1.530976414680481
I0213 04:58:34.954080 139647035000576 logging_writer.py:48] [11600] global_step=11600, grad_norm=2.3144237995147705, loss=1.5682017803192139
I0213 05:00:00.623102 139647026607872 logging_writer.py:48] [11700] global_step=11700, grad_norm=2.9844679832458496, loss=1.5483300685882568
I0213 05:01:24.979907 139647035000576 logging_writer.py:48] [11800] global_step=11800, grad_norm=3.3239777088165283, loss=1.6474357843399048
I0213 05:02:31.191845 139803787056960 spec.py:321] Evaluating on the training split.
I0213 05:03:27.195090 139803787056960 spec.py:333] Evaluating on the validation split.
I0213 05:04:20.071058 139803787056960 spec.py:349] Evaluating on the test split.
I0213 05:04:46.939941 139803787056960 submission_runner.py:408] Time since start: 11264.54s, 	Step: 11874, 	{'train/ctc_loss': Array(0.4290752, dtype=float32), 'train/wer': 0.1424052505581408, 'validation/ctc_loss': Array(0.65312153, dtype=float32), 'validation/wer': 0.18638307732411635, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.38414696, dtype=float32), 'test/wer': 0.12262100623565494, 'test/num_examples': 2472, 'score': 10098.086151361465, 'total_duration': 11264.540845394135, 'accumulated_submission_time': 10098.086151361465, 'accumulated_eval_time': 1165.5521621704102, 'accumulated_logging_time': 0.3547070026397705}
I0213 05:04:46.978882 139647035000576 logging_writer.py:48] [11874] accumulated_eval_time=1165.552162, accumulated_logging_time=0.354707, accumulated_submission_time=10098.086151, global_step=11874, preemption_count=0, score=10098.086151, test/ctc_loss=0.38414695858955383, test/num_examples=2472, test/wer=0.122621, total_duration=11264.540845, train/ctc_loss=0.4290752112865448, train/wer=0.142405, validation/ctc_loss=0.6531215310096741, validation/num_examples=5348, validation/wer=0.186383
I0213 05:05:07.459548 139647026607872 logging_writer.py:48] [11900] global_step=11900, grad_norm=1.9119799137115479, loss=1.5805392265319824
I0213 05:06:23.042809 139647035000576 logging_writer.py:48] [12000] global_step=12000, grad_norm=4.324441909790039, loss=1.5701736211776733
I0213 05:07:38.948368 139647026607872 logging_writer.py:48] [12100] global_step=12100, grad_norm=5.692121505737305, loss=1.5568783283233643
I0213 05:09:07.627546 139647035000576 logging_writer.py:48] [12200] global_step=12200, grad_norm=2.9120538234710693, loss=1.5495946407318115
I0213 05:10:37.176836 139647026607872 logging_writer.py:48] [12300] global_step=12300, grad_norm=2.0107791423797607, loss=1.5456256866455078
I0213 05:12:04.369190 139647035000576 logging_writer.py:48] [12400] global_step=12400, grad_norm=2.5848395824432373, loss=1.5245476961135864
I0213 05:13:23.005761 139647026607872 logging_writer.py:48] [12500] global_step=12500, grad_norm=3.240267515182495, loss=1.5305628776550293
I0213 05:14:46.180587 139647035000576 logging_writer.py:48] [12600] global_step=12600, grad_norm=5.261512756347656, loss=1.5977245569229126
I0213 05:16:06.301225 139647026607872 logging_writer.py:48] [12700] global_step=12700, grad_norm=3.069227695465088, loss=1.5389478206634521
I0213 05:17:33.562723 139647035000576 logging_writer.py:48] [12800] global_step=12800, grad_norm=3.6676011085510254, loss=1.5082697868347168
I0213 05:19:05.382587 139647026607872 logging_writer.py:48] [12900] global_step=12900, grad_norm=3.422369956970215, loss=1.4938256740570068
I0213 05:20:35.081648 139647035000576 logging_writer.py:48] [13000] global_step=13000, grad_norm=3.4102096557617188, loss=1.6071887016296387
I0213 05:22:05.651682 139647026607872 logging_writer.py:48] [13100] global_step=13100, grad_norm=3.404372215270996, loss=1.5765012502670288
I0213 05:23:36.179308 139647035000576 logging_writer.py:48] [13200] global_step=13200, grad_norm=2.883368968963623, loss=1.5612982511520386
I0213 05:25:06.505459 139647026607872 logging_writer.py:48] [13300] global_step=13300, grad_norm=2.1491613388061523, loss=1.4960851669311523
I0213 05:26:40.306997 139647035000576 logging_writer.py:48] [13400] global_step=13400, grad_norm=2.6083686351776123, loss=1.5525436401367188
I0213 05:27:57.607206 139647026607872 logging_writer.py:48] [13500] global_step=13500, grad_norm=2.814817428588867, loss=1.5646618604660034
I0213 05:28:47.316502 139803787056960 spec.py:321] Evaluating on the training split.
I0213 05:29:44.758691 139803787056960 spec.py:333] Evaluating on the validation split.
I0213 05:30:39.318181 139803787056960 spec.py:349] Evaluating on the test split.
I0213 05:31:06.210654 139803787056960 submission_runner.py:408] Time since start: 12843.81s, 	Step: 13566, 	{'train/ctc_loss': Array(0.5048986, dtype=float32), 'train/wer': 0.16362521757216827, 'validation/ctc_loss': Array(0.636037, dtype=float32), 'validation/wer': 0.18039719242689015, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.37574464, dtype=float32), 'test/wer': 0.12229602096155018, 'test/num_examples': 2472, 'score': 11538.334949493408, 'total_duration': 12843.813130378723, 'accumulated_submission_time': 11538.334949493408, 'accumulated_eval_time': 1304.4409563541412, 'accumulated_logging_time': 0.4097585678100586}
I0213 05:31:06.244996 139647035000576 logging_writer.py:48] [13566] accumulated_eval_time=1304.440956, accumulated_logging_time=0.409759, accumulated_submission_time=11538.334949, global_step=13566, preemption_count=0, score=11538.334949, test/ctc_loss=0.37574464082717896, test/num_examples=2472, test/wer=0.122296, total_duration=12843.813130, train/ctc_loss=0.5048986077308655, train/wer=0.163625, validation/ctc_loss=0.6360369920730591, validation/num_examples=5348, validation/wer=0.180397
I0213 05:31:32.652261 139647026607872 logging_writer.py:48] [13600] global_step=13600, grad_norm=2.7132632732391357, loss=1.524584412574768
I0213 05:32:48.920929 139647035000576 logging_writer.py:48] [13700] global_step=13700, grad_norm=2.942852020263672, loss=1.5581691265106201
I0213 05:34:04.833943 139647026607872 logging_writer.py:48] [13800] global_step=13800, grad_norm=5.414521217346191, loss=1.516243577003479
I0213 05:35:25.508917 139647035000576 logging_writer.py:48] [13900] global_step=13900, grad_norm=3.0926408767700195, loss=1.5014344453811646
I0213 05:36:53.339910 139647026607872 logging_writer.py:48] [14000] global_step=14000, grad_norm=2.584702968597412, loss=1.5873076915740967
I0213 05:38:23.577124 139647035000576 logging_writer.py:48] [14100] global_step=14100, grad_norm=4.114477157592773, loss=1.517543911933899
I0213 05:39:53.944903 139647026607872 logging_writer.py:48] [14200] global_step=14200, grad_norm=3.03471302986145, loss=1.5242520570755005
I0213 05:41:24.044938 139647035000576 logging_writer.py:48] [14300] global_step=14300, grad_norm=2.536799669265747, loss=1.484785556793213
I0213 05:42:52.524086 139647026607872 logging_writer.py:48] [14400] global_step=14400, grad_norm=2.5129764080047607, loss=1.5350943803787231
I0213 05:44:16.899137 139647035000576 logging_writer.py:48] [14500] global_step=14500, grad_norm=3.291010618209839, loss=1.5707441568374634
I0213 05:45:36.294597 139647026607872 logging_writer.py:48] [14600] global_step=14600, grad_norm=3.142620325088501, loss=1.5490244626998901
I0213 05:46:56.653808 139647035000576 logging_writer.py:48] [14700] global_step=14700, grad_norm=4.195681095123291, loss=1.5447769165039062
I0213 05:48:17.453114 139647026607872 logging_writer.py:48] [14800] global_step=14800, grad_norm=3.1169965267181396, loss=1.582102656364441
I0213 05:49:46.565415 139647035000576 logging_writer.py:48] [14900] global_step=14900, grad_norm=5.322237968444824, loss=1.574635624885559
I0213 05:51:17.560513 139647026607872 logging_writer.py:48] [15000] global_step=15000, grad_norm=3.7139663696289062, loss=1.5147920846939087
I0213 05:52:49.600257 139647035000576 logging_writer.py:48] [15100] global_step=15100, grad_norm=2.14481782913208, loss=1.5060350894927979
I0213 05:54:19.785351 139647026607872 logging_writer.py:48] [15200] global_step=15200, grad_norm=3.171349048614502, loss=1.506493330001831
I0213 05:55:06.816665 139803787056960 spec.py:321] Evaluating on the training split.
I0213 05:56:02.597104 139803787056960 spec.py:333] Evaluating on the validation split.
I0213 05:56:56.796345 139803787056960 spec.py:349] Evaluating on the test split.
I0213 05:57:23.489590 139803787056960 submission_runner.py:408] Time since start: 14421.09s, 	Step: 15256, 	{'train/ctc_loss': Array(0.45856845, dtype=float32), 'train/wer': 0.15020440881763528, 'validation/ctc_loss': Array(0.6351292, dtype=float32), 'validation/wer': 0.18133369377371425, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.36284128, dtype=float32), 'test/wer': 0.11731968395182094, 'test/num_examples': 2472, 'score': 12978.819166183472, 'total_duration': 14421.089455366135, 'accumulated_submission_time': 12978.819166183472, 'accumulated_eval_time': 1441.1059172153473, 'accumulated_logging_time': 0.4590444564819336}
I0213 05:57:23.525969 139647035000576 logging_writer.py:48] [15256] accumulated_eval_time=1441.105917, accumulated_logging_time=0.459044, accumulated_submission_time=12978.819166, global_step=15256, preemption_count=0, score=12978.819166, test/ctc_loss=0.36284127831459045, test/num_examples=2472, test/wer=0.117320, total_duration=14421.089455, train/ctc_loss=0.4585684537887573, train/wer=0.150204, validation/ctc_loss=0.6351292133331299, validation/num_examples=5348, validation/wer=0.181334
I0213 05:57:57.342627 139647026607872 logging_writer.py:48] [15300] global_step=15300, grad_norm=1.884417176246643, loss=1.5798914432525635
I0213 05:59:13.505991 139647035000576 logging_writer.py:48] [15400] global_step=15400, grad_norm=4.129794120788574, loss=1.564617395401001
I0213 06:00:33.729075 139647035000576 logging_writer.py:48] [15500] global_step=15500, grad_norm=3.1632015705108643, loss=1.4625370502471924
I0213 06:01:51.462978 139647026607872 logging_writer.py:48] [15600] global_step=15600, grad_norm=2.810079574584961, loss=1.5104217529296875
I0213 06:03:11.129279 139647035000576 logging_writer.py:48] [15700] global_step=15700, grad_norm=2.4433698654174805, loss=1.523703694343567
I0213 06:04:32.112429 139647026607872 logging_writer.py:48] [15800] global_step=15800, grad_norm=2.233919143676758, loss=1.5113941431045532
I0213 06:06:01.797117 139647035000576 logging_writer.py:48] [15900] global_step=15900, grad_norm=3.2072031497955322, loss=1.4897159337997437
I0213 06:07:31.546332 139647026607872 logging_writer.py:48] [16000] global_step=16000, grad_norm=2.262146472930908, loss=1.5255684852600098
I0213 06:08:59.961415 139647035000576 logging_writer.py:48] [16100] global_step=16100, grad_norm=3.765927791595459, loss=1.4834625720977783
I0213 06:10:29.221504 139647026607872 logging_writer.py:48] [16200] global_step=16200, grad_norm=2.041480541229248, loss=1.5066193342208862
I0213 06:12:01.698718 139647035000576 logging_writer.py:48] [16300] global_step=16300, grad_norm=2.687833070755005, loss=1.483542799949646
I0213 06:13:31.173274 139647026607872 logging_writer.py:48] [16400] global_step=16400, grad_norm=2.1226816177368164, loss=1.5480425357818604
I0213 06:15:02.581167 139647035000576 logging_writer.py:48] [16500] global_step=16500, grad_norm=5.474573135375977, loss=1.5027364492416382
I0213 06:16:20.912018 139647026607872 logging_writer.py:48] [16600] global_step=16600, grad_norm=4.767574787139893, loss=1.5060193538665771
I0213 06:17:42.071385 139647035000576 logging_writer.py:48] [16700] global_step=16700, grad_norm=2.2313969135284424, loss=1.4766590595245361
I0213 06:19:05.418148 139647026607872 logging_writer.py:48] [16800] global_step=16800, grad_norm=2.5822958946228027, loss=1.4977408647537231
I0213 06:20:30.679594 139647035000576 logging_writer.py:48] [16900] global_step=16900, grad_norm=3.989785671234131, loss=1.4578570127487183
I0213 06:21:23.658906 139803787056960 spec.py:321] Evaluating on the training split.
I0213 06:22:19.635338 139803787056960 spec.py:333] Evaluating on the validation split.
I0213 06:23:12.817025 139803787056960 spec.py:349] Evaluating on the test split.
I0213 06:23:39.531978 139803787056960 submission_runner.py:408] Time since start: 15997.13s, 	Step: 16959, 	{'train/ctc_loss': Array(0.44045535, dtype=float32), 'train/wer': 0.1457255361385011, 'validation/ctc_loss': Array(0.6102114, dtype=float32), 'validation/wer': 0.17462370989698486, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3535885, dtype=float32), 'test/wer': 0.11360266487924765, 'test/num_examples': 2472, 'score': 14418.86126089096, 'total_duration': 15997.133904457092, 'accumulated_submission_time': 14418.86126089096, 'accumulated_eval_time': 1576.9730966091156, 'accumulated_logging_time': 0.5129210948944092}
I0213 06:23:39.570243 139647035000576 logging_writer.py:48] [16959] accumulated_eval_time=1576.973097, accumulated_logging_time=0.512921, accumulated_submission_time=14418.861261, global_step=16959, preemption_count=0, score=14418.861261, test/ctc_loss=0.3535884916782379, test/num_examples=2472, test/wer=0.113603, total_duration=15997.133904, train/ctc_loss=0.4404553472995758, train/wer=0.145726, validation/ctc_loss=0.6102113723754883, validation/num_examples=5348, validation/wer=0.174624
I0213 06:24:11.836951 139647026607872 logging_writer.py:48] [17000] global_step=17000, grad_norm=2.505185842514038, loss=1.5679283142089844
I0213 06:25:28.268084 139647035000576 logging_writer.py:48] [17100] global_step=17100, grad_norm=4.012244701385498, loss=1.501578450202942
I0213 06:26:44.378120 139647026607872 logging_writer.py:48] [17200] global_step=17200, grad_norm=3.0379860401153564, loss=1.502998948097229
I0213 06:28:16.483059 139647035000576 logging_writer.py:48] [17300] global_step=17300, grad_norm=3.2027881145477295, loss=1.5320931673049927
I0213 06:29:44.447113 139647026607872 logging_writer.py:48] [17400] global_step=17400, grad_norm=5.7885894775390625, loss=1.5984399318695068
I0213 06:31:14.066830 139647035000576 logging_writer.py:48] [17500] global_step=17500, grad_norm=4.688427925109863, loss=1.4988954067230225
I0213 06:32:35.579673 139647035000576 logging_writer.py:48] [17600] global_step=17600, grad_norm=3.5521726608276367, loss=1.5206466913223267
I0213 06:33:51.281856 139647026607872 logging_writer.py:48] [17700] global_step=17700, grad_norm=3.693819522857666, loss=1.5166124105453491
I0213 06:35:14.501347 139647035000576 logging_writer.py:48] [17800] global_step=17800, grad_norm=3.968338966369629, loss=1.437074899673462
I0213 06:36:41.852141 139647026607872 logging_writer.py:48] [17900] global_step=17900, grad_norm=1.8415509462356567, loss=1.521573543548584
I0213 06:38:11.288476 139647035000576 logging_writer.py:48] [18000] global_step=18000, grad_norm=2.797175168991089, loss=1.513426661491394
I0213 06:39:38.362754 139647026607872 logging_writer.py:48] [18100] global_step=18100, grad_norm=2.561429977416992, loss=1.4786425828933716
I0213 06:41:09.399988 139647035000576 logging_writer.py:48] [18200] global_step=18200, grad_norm=3.773123025894165, loss=1.5335681438446045
I0213 06:42:36.939841 139647026607872 logging_writer.py:48] [18300] global_step=18300, grad_norm=3.327052593231201, loss=1.493153691291809
I0213 06:44:07.371406 139647035000576 logging_writer.py:48] [18400] global_step=18400, grad_norm=5.383256435394287, loss=1.5358738899230957
I0213 06:45:33.831648 139647026607872 logging_writer.py:48] [18500] global_step=18500, grad_norm=1.8749276399612427, loss=1.525648832321167
I0213 06:46:59.272702 139647035000576 logging_writer.py:48] [18600] global_step=18600, grad_norm=1.616416096687317, loss=1.4551137685775757
I0213 06:47:40.176783 139803787056960 spec.py:321] Evaluating on the training split.
I0213 06:48:35.598621 139803787056960 spec.py:333] Evaluating on the validation split.
I0213 06:49:27.782978 139803787056960 spec.py:349] Evaluating on the test split.
I0213 06:49:54.787524 139803787056960 submission_runner.py:408] Time since start: 17572.39s, 	Step: 18655, 	{'train/ctc_loss': Array(0.40981236, dtype=float32), 'train/wer': 0.13409648405560098, 'validation/ctc_loss': Array(0.5838607, dtype=float32), 'validation/wer': 0.1677399422651747, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33918792, dtype=float32), 'test/wer': 0.10795604574167733, 'test/num_examples': 2472, 'score': 15859.378022432327, 'total_duration': 17572.3883395195, 'accumulated_submission_time': 15859.378022432327, 'accumulated_eval_time': 1711.5768485069275, 'accumulated_logging_time': 0.5694046020507812}
I0213 06:49:54.823427 139647035000576 logging_writer.py:48] [18655] accumulated_eval_time=1711.576849, accumulated_logging_time=0.569405, accumulated_submission_time=15859.378022, global_step=18655, preemption_count=0, score=15859.378022, test/ctc_loss=0.3391879200935364, test/num_examples=2472, test/wer=0.107956, total_duration=17572.388340, train/ctc_loss=0.4098123610019684, train/wer=0.134096, validation/ctc_loss=0.5838606953620911, validation/num_examples=5348, validation/wer=0.167740
I0213 06:50:29.418736 139647026607872 logging_writer.py:48] [18700] global_step=18700, grad_norm=3.3354640007019043, loss=1.4337141513824463
I0213 06:51:44.654813 139647035000576 logging_writer.py:48] [18800] global_step=18800, grad_norm=2.2800281047821045, loss=1.514841914176941
I0213 06:52:59.864902 139647026607872 logging_writer.py:48] [18900] global_step=18900, grad_norm=2.967135190963745, loss=1.5086256265640259
I0213 06:54:20.036550 139647035000576 logging_writer.py:48] [19000] global_step=19000, grad_norm=2.013291358947754, loss=1.4089876413345337
I0213 06:55:48.017025 139647026607872 logging_writer.py:48] [19100] global_step=19100, grad_norm=2.374229669570923, loss=1.4610612392425537
I0213 06:57:17.330865 139647035000576 logging_writer.py:48] [19200] global_step=19200, grad_norm=2.808135509490967, loss=1.4309972524642944
I0213 06:58:49.420255 139647026607872 logging_writer.py:48] [19300] global_step=19300, grad_norm=3.297950267791748, loss=1.4777731895446777
I0213 07:00:21.794569 139647035000576 logging_writer.py:48] [19400] global_step=19400, grad_norm=3.061525344848633, loss=1.4430506229400635
I0213 07:01:52.982337 139647026607872 logging_writer.py:48] [19500] global_step=19500, grad_norm=1.7538566589355469, loss=1.5064060688018799
I0213 07:03:18.678789 139647035000576 logging_writer.py:48] [19600] global_step=19600, grad_norm=3.531912326812744, loss=1.5488479137420654
I0213 07:04:36.382625 139647026607872 logging_writer.py:48] [19700] global_step=19700, grad_norm=3.4878299236297607, loss=1.4523183107376099
I0213 07:05:55.659682 139647035000576 logging_writer.py:48] [19800] global_step=19800, grad_norm=5.051174163818359, loss=1.486507773399353
I0213 07:07:18.354849 139647026607872 logging_writer.py:48] [19900] global_step=19900, grad_norm=1.9725232124328613, loss=1.402555227279663
I0213 07:08:45.377186 139647035000576 logging_writer.py:48] [20000] global_step=20000, grad_norm=2.353712558746338, loss=1.4584428071975708
I0213 07:10:13.937004 139647026607872 logging_writer.py:48] [20100] global_step=20100, grad_norm=2.068889856338501, loss=1.4331331253051758
I0213 07:11:41.902254 139647035000576 logging_writer.py:48] [20200] global_step=20200, grad_norm=2.3850629329681396, loss=1.4677083492279053
I0213 07:13:10.381262 139647026607872 logging_writer.py:48] [20300] global_step=20300, grad_norm=2.3268349170684814, loss=1.4626376628875732
I0213 07:13:54.854717 139803787056960 spec.py:321] Evaluating on the training split.
I0213 07:14:48.768179 139803787056960 spec.py:333] Evaluating on the validation split.
I0213 07:15:41.018264 139803787056960 spec.py:349] Evaluating on the test split.
I0213 07:16:07.960933 139803787056960 submission_runner.py:408] Time since start: 19145.56s, 	Step: 20349, 	{'train/ctc_loss': Array(0.42425936, dtype=float32), 'train/wer': 0.14089816744684, 'validation/ctc_loss': Array(0.5756924, dtype=float32), 'validation/wer': 0.16473734516350155, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33319625, dtype=float32), 'test/wer': 0.1072654520342047, 'test/num_examples': 2472, 'score': 17299.322156190872, 'total_duration': 19145.56060743332, 'accumulated_submission_time': 17299.322156190872, 'accumulated_eval_time': 1844.6749074459076, 'accumulated_logging_time': 0.621281623840332}
I0213 07:16:07.999999 139647035000576 logging_writer.py:48] [20349] accumulated_eval_time=1844.674907, accumulated_logging_time=0.621282, accumulated_submission_time=17299.322156, global_step=20349, preemption_count=0, score=17299.322156, test/ctc_loss=0.3331962525844574, test/num_examples=2472, test/wer=0.107265, total_duration=19145.560607, train/ctc_loss=0.42425936460494995, train/wer=0.140898, validation/ctc_loss=0.5756924152374268, validation/num_examples=5348, validation/wer=0.164737
I0213 07:16:47.138350 139647026607872 logging_writer.py:48] [20400] global_step=20400, grad_norm=3.0265390872955322, loss=1.5582444667816162
I0213 07:18:02.405685 139647035000576 logging_writer.py:48] [20500] global_step=20500, grad_norm=2.770110607147217, loss=1.4925518035888672
I0213 07:19:29.471819 139647035000576 logging_writer.py:48] [20600] global_step=20600, grad_norm=2.2458364963531494, loss=1.4142669439315796
I0213 07:20:48.473184 139647026607872 logging_writer.py:48] [20700] global_step=20700, grad_norm=2.737262725830078, loss=1.4482439756393433
I0213 07:22:09.249198 139647035000576 logging_writer.py:48] [20800] global_step=20800, grad_norm=2.639446258544922, loss=1.4531586170196533
I0213 07:23:30.099975 139647026607872 logging_writer.py:48] [20900] global_step=20900, grad_norm=2.9268136024475098, loss=1.4512709379196167
I0213 07:24:57.392229 139647035000576 logging_writer.py:48] [21000] global_step=21000, grad_norm=5.703014373779297, loss=1.473867416381836
I0213 07:26:26.675022 139647026607872 logging_writer.py:48] [21100] global_step=21100, grad_norm=3.2041828632354736, loss=1.440291166305542
I0213 07:27:56.451624 139647035000576 logging_writer.py:48] [21200] global_step=21200, grad_norm=2.8719985485076904, loss=1.4710464477539062
I0213 07:29:25.980191 139647026607872 logging_writer.py:48] [21300] global_step=21300, grad_norm=3.371950626373291, loss=1.4871677160263062
I0213 07:30:57.387917 139647035000576 logging_writer.py:48] [21400] global_step=21400, grad_norm=3.23972225189209, loss=1.448272466659546
I0213 07:32:29.699732 139647026607872 logging_writer.py:48] [21500] global_step=21500, grad_norm=3.431676149368286, loss=1.3691973686218262
I0213 07:33:58.158691 139647035000576 logging_writer.py:48] [21600] global_step=21600, grad_norm=4.401457786560059, loss=1.461221694946289
I0213 07:35:21.896417 139647035000576 logging_writer.py:48] [21700] global_step=21700, grad_norm=3.0320188999176025, loss=1.4519726037979126
I0213 07:36:41.241861 139647026607872 logging_writer.py:48] [21800] global_step=21800, grad_norm=2.111002206802368, loss=1.4660917520523071
I0213 07:37:58.627931 139647035000576 logging_writer.py:48] [21900] global_step=21900, grad_norm=3.0600662231445312, loss=1.523660659790039
I0213 07:39:25.403610 139647026607872 logging_writer.py:48] [22000] global_step=22000, grad_norm=2.723048686981201, loss=1.3966643810272217
I0213 07:40:08.484425 139803787056960 spec.py:321] Evaluating on the training split.
I0213 07:41:02.526526 139803787056960 spec.py:333] Evaluating on the validation split.
I0213 07:41:57.464575 139803787056960 spec.py:349] Evaluating on the test split.
I0213 07:42:25.283417 139803787056960 submission_runner.py:408] Time since start: 20722.89s, 	Step: 22051, 	{'train/ctc_loss': Array(0.4327474, dtype=float32), 'train/wer': 0.13708161804188293, 'validation/ctc_loss': Array(0.5562388, dtype=float32), 'validation/wer': 0.15930177549069774, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32314736, dtype=float32), 'test/wer': 0.1023906729226332, 'test/num_examples': 2472, 'score': 18739.717533826828, 'total_duration': 20722.885338544846, 'accumulated_submission_time': 18739.717533826828, 'accumulated_eval_time': 1981.4680063724518, 'accumulated_logging_time': 0.6765701770782471}
I0213 07:42:25.320707 139647035000576 logging_writer.py:48] [22051] accumulated_eval_time=1981.468006, accumulated_logging_time=0.676570, accumulated_submission_time=18739.717534, global_step=22051, preemption_count=0, score=18739.717534, test/ctc_loss=0.32314735651016235, test/num_examples=2472, test/wer=0.102391, total_duration=20722.885339, train/ctc_loss=0.43274739384651184, train/wer=0.137082, validation/ctc_loss=0.5562387704849243, validation/num_examples=5348, validation/wer=0.159302
I0213 07:43:02.814253 139647026607872 logging_writer.py:48] [22100] global_step=22100, grad_norm=6.582115650177002, loss=1.4117317199707031
I0213 07:44:17.873045 139647035000576 logging_writer.py:48] [22200] global_step=22200, grad_norm=4.011613845825195, loss=1.4373681545257568
I0213 07:45:39.221920 139647026607872 logging_writer.py:48] [22300] global_step=22300, grad_norm=2.4824163913726807, loss=1.3987665176391602
I0213 07:47:07.170828 139647035000576 logging_writer.py:48] [22400] global_step=22400, grad_norm=1.8596384525299072, loss=1.4857538938522339
I0213 07:48:39.458889 139647026607872 logging_writer.py:48] [22500] global_step=22500, grad_norm=2.149975299835205, loss=1.4695419073104858
I0213 07:50:07.163148 139647035000576 logging_writer.py:48] [22600] global_step=22600, grad_norm=5.1283745765686035, loss=1.432803988456726
I0213 07:51:34.116876 139647035000576 logging_writer.py:48] [22700] global_step=22700, grad_norm=2.6956567764282227, loss=1.4051662683486938
I0213 07:52:51.538921 139647026607872 logging_writer.py:48] [22800] global_step=22800, grad_norm=2.7404263019561768, loss=1.4476523399353027
I0213 07:54:12.412256 139647035000576 logging_writer.py:48] [22900] global_step=22900, grad_norm=1.643277883529663, loss=1.4775587320327759
I0213 07:55:32.471853 139647026607872 logging_writer.py:48] [23000] global_step=23000, grad_norm=2.505303144454956, loss=1.4326387643814087
I0213 07:57:00.977996 139647035000576 logging_writer.py:48] [23100] global_step=23100, grad_norm=1.7893767356872559, loss=1.35368013381958
I0213 07:58:29.459398 139647026607872 logging_writer.py:48] [23200] global_step=23200, grad_norm=2.5263259410858154, loss=1.4252033233642578
I0213 08:00:00.077166 139647035000576 logging_writer.py:48] [23300] global_step=23300, grad_norm=3.2932140827178955, loss=1.4672101736068726
I0213 08:01:27.510808 139647026607872 logging_writer.py:48] [23400] global_step=23400, grad_norm=2.997298240661621, loss=1.458711862564087
I0213 08:02:58.629216 139647035000576 logging_writer.py:48] [23500] global_step=23500, grad_norm=3.4491970539093018, loss=1.4533032178878784
I0213 08:04:28.763031 139647026607872 logging_writer.py:48] [23600] global_step=23600, grad_norm=2.3313095569610596, loss=1.485819697380066
I0213 08:05:58.970568 139647035000576 logging_writer.py:48] [23700] global_step=23700, grad_norm=3.6080498695373535, loss=1.3819657564163208
I0213 08:06:25.383970 139803787056960 spec.py:321] Evaluating on the training split.
I0213 08:07:20.682518 139803787056960 spec.py:333] Evaluating on the validation split.
I0213 08:08:13.845398 139803787056960 spec.py:349] Evaluating on the test split.
I0213 08:08:40.976125 139803787056960 submission_runner.py:408] Time since start: 22298.58s, 	Step: 23736, 	{'train/ctc_loss': Array(0.36796176, dtype=float32), 'train/wer': 0.12416072206866494, 'validation/ctc_loss': Array(0.54934376, dtype=float32), 'validation/wer': 0.1599776012049007, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3138135, dtype=float32), 'test/wer': 0.10019702232242601, 'test/num_examples': 2472, 'score': 20179.681025505066, 'total_duration': 22298.57732820511, 'accumulated_submission_time': 20179.681025505066, 'accumulated_eval_time': 2117.053550004959, 'accumulated_logging_time': 0.740778923034668}
I0213 08:08:41.015349 139647035000576 logging_writer.py:48] [23736] accumulated_eval_time=2117.053550, accumulated_logging_time=0.740779, accumulated_submission_time=20179.681026, global_step=23736, preemption_count=0, score=20179.681026, test/ctc_loss=0.3138135075569153, test/num_examples=2472, test/wer=0.100197, total_duration=22298.577328, train/ctc_loss=0.3679617643356323, train/wer=0.124161, validation/ctc_loss=0.5493437647819519, validation/num_examples=5348, validation/wer=0.159978
I0213 08:09:29.934962 139647026607872 logging_writer.py:48] [23800] global_step=23800, grad_norm=2.5577445030212402, loss=1.3731603622436523
I0213 08:10:45.795015 139647035000576 logging_writer.py:48] [23900] global_step=23900, grad_norm=2.7089436054229736, loss=1.4114127159118652
I0213 08:12:01.081776 139647026607872 logging_writer.py:48] [24000] global_step=24000, grad_norm=4.777346611022949, loss=1.4108963012695312
I0213 08:13:16.552389 139647035000576 logging_writer.py:48] [24100] global_step=24100, grad_norm=2.965034246444702, loss=1.472176194190979
I0213 08:14:45.688615 139647026607872 logging_writer.py:48] [24200] global_step=24200, grad_norm=2.067309617996216, loss=1.4072290658950806
I0213 08:16:14.997069 139647035000576 logging_writer.py:48] [24300] global_step=24300, grad_norm=2.8787543773651123, loss=1.3895788192749023
I0213 08:17:48.175805 139647026607872 logging_writer.py:48] [24400] global_step=24400, grad_norm=2.035259485244751, loss=1.4222108125686646
I0213 08:19:19.108963 139647035000576 logging_writer.py:48] [24500] global_step=24500, grad_norm=5.077612400054932, loss=1.3231326341629028
I0213 08:20:50.049678 139647026607872 logging_writer.py:48] [24600] global_step=24600, grad_norm=2.2515969276428223, loss=1.3884673118591309
I0213 08:22:19.204816 139647035000576 logging_writer.py:48] [24700] global_step=24700, grad_norm=6.27147102355957, loss=1.4383524656295776
I0213 08:23:41.808711 139647035000576 logging_writer.py:48] [24800] global_step=24800, grad_norm=1.9555284976959229, loss=1.3524525165557861
I0213 08:25:01.220021 139647026607872 logging_writer.py:48] [24900] global_step=24900, grad_norm=2.211012840270996, loss=1.3610836267471313
I0213 08:26:23.349980 139647035000576 logging_writer.py:48] [25000] global_step=25000, grad_norm=3.506000518798828, loss=1.431066870689392
I0213 08:27:49.439285 139647026607872 logging_writer.py:48] [25100] global_step=25100, grad_norm=1.805806040763855, loss=1.3650215864181519
I0213 08:29:15.370552 139647035000576 logging_writer.py:48] [25200] global_step=25200, grad_norm=2.034996747970581, loss=1.435657024383545
I0213 08:30:45.348362 139647026607872 logging_writer.py:48] [25300] global_step=25300, grad_norm=2.735011577606201, loss=1.4113218784332275
I0213 08:32:16.376894 139647035000576 logging_writer.py:48] [25400] global_step=25400, grad_norm=3.3555784225463867, loss=1.371042013168335
I0213 08:32:41.375424 139803787056960 spec.py:321] Evaluating on the training split.
I0213 08:33:35.617218 139803787056960 spec.py:333] Evaluating on the validation split.
I0213 08:34:29.796863 139803787056960 spec.py:349] Evaluating on the test split.
I0213 08:34:56.563775 139803787056960 submission_runner.py:408] Time since start: 23874.17s, 	Step: 25430, 	{'train/ctc_loss': Array(0.39533722, dtype=float32), 'train/wer': 0.13457298677187282, 'validation/ctc_loss': Array(0.53948, dtype=float32), 'validation/wer': 0.1550923467565193, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30784765, dtype=float32), 'test/wer': 0.09753620539069323, 'test/num_examples': 2472, 'score': 21619.953918218613, 'total_duration': 23874.16535425186, 'accumulated_submission_time': 21619.953918218613, 'accumulated_eval_time': 2252.2356536388397, 'accumulated_logging_time': 0.7953715324401855}
I0213 08:34:56.605583 139647035000576 logging_writer.py:48] [25430] accumulated_eval_time=2252.235654, accumulated_logging_time=0.795372, accumulated_submission_time=21619.953918, global_step=25430, preemption_count=0, score=21619.953918, test/ctc_loss=0.30784764885902405, test/num_examples=2472, test/wer=0.097536, total_duration=23874.165354, train/ctc_loss=0.39533722400665283, train/wer=0.134573, validation/ctc_loss=0.5394799709320068, validation/num_examples=5348, validation/wer=0.155092
I0213 08:35:50.288635 139647026607872 logging_writer.py:48] [25500] global_step=25500, grad_norm=2.6234588623046875, loss=1.3547639846801758
I0213 08:37:06.117001 139647035000576 logging_writer.py:48] [25600] global_step=25600, grad_norm=2.2684922218322754, loss=1.3602356910705566
I0213 08:38:31.296553 139647026607872 logging_writer.py:48] [25700] global_step=25700, grad_norm=2.892819881439209, loss=1.4015249013900757
I0213 08:39:56.697258 139647035000576 logging_writer.py:48] [25800] global_step=25800, grad_norm=2.826887369155884, loss=1.3336633443832397
I0213 08:41:13.831871 139647026607872 logging_writer.py:48] [25900] global_step=25900, grad_norm=3.57218599319458, loss=1.3488421440124512
I0213 08:42:32.100933 139647035000576 logging_writer.py:48] [26000] global_step=26000, grad_norm=2.357003688812256, loss=1.3790063858032227
I0213 08:43:55.603936 139647026607872 logging_writer.py:48] [26100] global_step=26100, grad_norm=2.320481061935425, loss=1.3423669338226318
I0213 08:45:19.141244 139647035000576 logging_writer.py:48] [26200] global_step=26200, grad_norm=3.6095895767211914, loss=1.3775964975357056
I0213 08:46:48.651734 139647026607872 logging_writer.py:48] [26300] global_step=26300, grad_norm=2.095801591873169, loss=1.3174426555633545
I0213 08:48:18.256583 139647035000576 logging_writer.py:48] [26400] global_step=26400, grad_norm=1.8733021020889282, loss=1.3281283378601074
I0213 08:49:49.305283 139647026607872 logging_writer.py:48] [26500] global_step=26500, grad_norm=1.716383695602417, loss=1.3621268272399902
I0213 08:51:19.717806 139647035000576 logging_writer.py:48] [26600] global_step=26600, grad_norm=4.272918224334717, loss=1.3807151317596436
I0213 08:52:50.393829 139647026607872 logging_writer.py:48] [26700] global_step=26700, grad_norm=2.327392101287842, loss=1.372033953666687
I0213 08:54:20.008112 139647035000576 logging_writer.py:48] [26800] global_step=26800, grad_norm=4.043055534362793, loss=1.4028387069702148
I0213 08:55:38.450489 139647026607872 logging_writer.py:48] [26900] global_step=26900, grad_norm=2.651437997817993, loss=1.3242048025131226
I0213 08:56:55.762717 139647035000576 logging_writer.py:48] [27000] global_step=27000, grad_norm=2.2253148555755615, loss=1.3295575380325317
I0213 08:58:18.612779 139647026607872 logging_writer.py:48] [27100] global_step=27100, grad_norm=2.793287754058838, loss=1.3088774681091309
I0213 08:58:57.000469 139803787056960 spec.py:321] Evaluating on the training split.
I0213 08:59:51.930675 139803787056960 spec.py:333] Evaluating on the validation split.
I0213 09:00:45.475054 139803787056960 spec.py:349] Evaluating on the test split.
I0213 09:01:13.458045 139803787056960 submission_runner.py:408] Time since start: 25451.06s, 	Step: 27147, 	{'train/ctc_loss': Array(0.3278709, dtype=float32), 'train/wer': 0.11051153691868931, 'validation/ctc_loss': Array(0.52832973, dtype=float32), 'validation/wer': 0.1517132181855045, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2963604, dtype=float32), 'test/wer': 0.09406292527369854, 'test/num_examples': 2472, 'score': 23060.258680820465, 'total_duration': 25451.059435129166, 'accumulated_submission_time': 23060.258680820465, 'accumulated_eval_time': 2388.6867899894714, 'accumulated_logging_time': 0.8549208641052246}
I0213 09:01:13.495869 139647035000576 logging_writer.py:48] [27147] accumulated_eval_time=2388.686790, accumulated_logging_time=0.854921, accumulated_submission_time=23060.258681, global_step=27147, preemption_count=0, score=23060.258681, test/ctc_loss=0.29636040329933167, test/num_examples=2472, test/wer=0.094063, total_duration=25451.059435, train/ctc_loss=0.3278709053993225, train/wer=0.110512, validation/ctc_loss=0.5283297300338745, validation/num_examples=5348, validation/wer=0.151713
I0213 09:01:54.055814 139647026607872 logging_writer.py:48] [27200] global_step=27200, grad_norm=2.289519786834717, loss=1.3521958589553833
I0213 09:03:09.999544 139647035000576 logging_writer.py:48] [27300] global_step=27300, grad_norm=2.2844879627227783, loss=1.3875237703323364
I0213 09:04:32.981553 139647026607872 logging_writer.py:48] [27400] global_step=27400, grad_norm=3.2016587257385254, loss=1.431450605392456
I0213 09:06:04.037586 139647035000576 logging_writer.py:48] [27500] global_step=27500, grad_norm=1.5819544792175293, loss=1.3782577514648438
I0213 09:07:33.052236 139647026607872 logging_writer.py:48] [27600] global_step=27600, grad_norm=2.313157081604004, loss=1.362331509590149
I0213 09:09:02.696276 139647035000576 logging_writer.py:48] [27700] global_step=27700, grad_norm=2.999521017074585, loss=1.4046046733856201
I0213 09:10:32.512747 139647026607872 logging_writer.py:48] [27800] global_step=27800, grad_norm=1.7102909088134766, loss=1.371340036392212
I0213 09:11:53.734495 139647035000576 logging_writer.py:48] [27900] global_step=27900, grad_norm=3.3607046604156494, loss=1.359044075012207
I0213 09:13:14.716324 139647026607872 logging_writer.py:48] [28000] global_step=28000, grad_norm=3.111943244934082, loss=1.367093801498413
I0213 09:14:33.652813 139647035000576 logging_writer.py:48] [28100] global_step=28100, grad_norm=4.103700637817383, loss=1.3504436016082764
I0213 09:15:59.292791 139647026607872 logging_writer.py:48] [28200] global_step=28200, grad_norm=2.518566131591797, loss=1.3522025346755981
I0213 09:17:26.113359 139647035000576 logging_writer.py:48] [28300] global_step=28300, grad_norm=1.7517575025558472, loss=1.2758554220199585
I0213 09:18:56.161421 139647026607872 logging_writer.py:48] [28400] global_step=28400, grad_norm=4.204479217529297, loss=1.3799251317977905
I0213 09:20:29.283196 139647035000576 logging_writer.py:48] [28500] global_step=28500, grad_norm=2.9414780139923096, loss=1.3006503582000732
I0213 09:21:59.940387 139647026607872 logging_writer.py:48] [28600] global_step=28600, grad_norm=2.269808530807495, loss=1.3240811824798584
I0213 09:23:31.729609 139647035000576 logging_writer.py:48] [28700] global_step=28700, grad_norm=2.110093593597412, loss=1.3387945890426636
I0213 09:25:02.024736 139647026607872 logging_writer.py:48] [28800] global_step=28800, grad_norm=2.7866580486297607, loss=1.343160629272461
I0213 09:25:13.561535 139803787056960 spec.py:321] Evaluating on the training split.
I0213 09:26:07.970995 139803787056960 spec.py:333] Evaluating on the validation split.
I0213 09:27:00.668752 139803787056960 spec.py:349] Evaluating on the test split.
I0213 09:27:28.561010 139803787056960 submission_runner.py:408] Time since start: 27026.16s, 	Step: 28814, 	{'train/ctc_loss': Array(0.33591753, dtype=float32), 'train/wer': 0.11227693090867955, 'validation/ctc_loss': Array(0.49735403, dtype=float32), 'validation/wer': 0.1428309373702656, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2814619, dtype=float32), 'test/wer': 0.08965531249365263, 'test/num_examples': 2472, 'score': 24500.23553586006, 'total_duration': 27026.162631988525, 'accumulated_submission_time': 24500.23553586006, 'accumulated_eval_time': 2523.680060863495, 'accumulated_logging_time': 0.9110238552093506}
I0213 09:27:28.596130 139647035000576 logging_writer.py:48] [28814] accumulated_eval_time=2523.680061, accumulated_logging_time=0.911024, accumulated_submission_time=24500.235536, global_step=28814, preemption_count=0, score=24500.235536, test/ctc_loss=0.2814618945121765, test/num_examples=2472, test/wer=0.089655, total_duration=27026.162632, train/ctc_loss=0.33591753244400024, train/wer=0.112277, validation/ctc_loss=0.49735403060913086, validation/num_examples=5348, validation/wer=0.142831
I0213 09:28:40.732251 139647035000576 logging_writer.py:48] [28900] global_step=28900, grad_norm=2.823608636856079, loss=1.2996631860733032
I0213 09:29:58.912834 139647026607872 logging_writer.py:48] [29000] global_step=29000, grad_norm=5.212362289428711, loss=1.3265376091003418
I0213 09:31:22.444146 139647035000576 logging_writer.py:48] [29100] global_step=29100, grad_norm=2.865828037261963, loss=1.334218978881836
I0213 09:32:47.311642 139647026607872 logging_writer.py:48] [29200] global_step=29200, grad_norm=2.9821393489837646, loss=1.357210397720337
I0213 09:34:15.622043 139647035000576 logging_writer.py:48] [29300] global_step=29300, grad_norm=2.445218086242676, loss=1.2963849306106567
I0213 09:35:45.189625 139647026607872 logging_writer.py:48] [29400] global_step=29400, grad_norm=1.945306658744812, loss=1.3843519687652588
I0213 09:37:15.277719 139647035000576 logging_writer.py:48] [29500] global_step=29500, grad_norm=3.245479106903076, loss=1.362208366394043
I0213 09:38:45.542644 139647026607872 logging_writer.py:48] [29600] global_step=29600, grad_norm=4.558198928833008, loss=1.3813467025756836
I0213 09:40:10.746890 139647035000576 logging_writer.py:48] [29700] global_step=29700, grad_norm=1.414889931678772, loss=1.4151666164398193
I0213 09:41:39.803107 139647026607872 logging_writer.py:48] [29800] global_step=29800, grad_norm=2.555647850036621, loss=1.317352056503296
I0213 09:43:10.772517 139647035000576 logging_writer.py:48] [29900] global_step=29900, grad_norm=2.293886661529541, loss=1.2763079404830933
I0213 09:44:29.728633 139647026607872 logging_writer.py:48] [30000] global_step=30000, grad_norm=3.4388489723205566, loss=1.3259025812149048
I0213 09:45:47.044822 139647035000576 logging_writer.py:48] [30100] global_step=30100, grad_norm=2.2678844928741455, loss=1.3178608417510986
I0213 09:47:07.177826 139647026607872 logging_writer.py:48] [30200] global_step=30200, grad_norm=2.3978047370910645, loss=1.3303831815719604
I0213 09:48:33.879806 139647035000576 logging_writer.py:48] [30300] global_step=30300, grad_norm=2.8298404216766357, loss=1.3611923456192017
I0213 09:50:05.273273 139647026607872 logging_writer.py:48] [30400] global_step=30400, grad_norm=2.604747772216797, loss=1.3112801313400269
I0213 09:51:28.657632 139803787056960 spec.py:321] Evaluating on the training split.
I0213 09:52:22.931731 139803787056960 spec.py:333] Evaluating on the validation split.
I0213 09:53:17.307032 139803787056960 spec.py:349] Evaluating on the test split.
I0213 09:53:44.591784 139803787056960 submission_runner.py:408] Time since start: 28602.19s, 	Step: 30496, 	{'train/ctc_loss': Array(0.32313046, dtype=float32), 'train/wer': 0.1091377633158311, 'validation/ctc_loss': Array(0.49119592, dtype=float32), 'validation/wer': 0.14111240912557807, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2728875, dtype=float32), 'test/wer': 0.08817256718054964, 'test/num_examples': 2472, 'score': 25940.209047079086, 'total_duration': 28602.19198703766, 'accumulated_submission_time': 25940.209047079086, 'accumulated_eval_time': 2659.6065866947174, 'accumulated_logging_time': 0.9619748592376709}
I0213 09:53:44.636897 139647035000576 logging_writer.py:48] [30496] accumulated_eval_time=2659.606587, accumulated_logging_time=0.961975, accumulated_submission_time=25940.209047, global_step=30496, preemption_count=0, score=25940.209047, test/ctc_loss=0.2728874981403351, test/num_examples=2472, test/wer=0.088173, total_duration=28602.191987, train/ctc_loss=0.32313045859336853, train/wer=0.109138, validation/ctc_loss=0.4911959171295166, validation/num_examples=5348, validation/wer=0.141112
I0213 09:53:48.520823 139647026607872 logging_writer.py:48] [30500] global_step=30500, grad_norm=3.2045631408691406, loss=1.2988859415054321
I0213 09:55:03.705053 139647035000576 logging_writer.py:48] [30600] global_step=30600, grad_norm=2.874371290206909, loss=1.2656333446502686
I0213 09:56:19.869029 139647026607872 logging_writer.py:48] [30700] global_step=30700, grad_norm=2.1932969093322754, loss=1.287187933921814
I0213 09:57:46.397956 139647035000576 logging_writer.py:48] [30800] global_step=30800, grad_norm=2.3162155151367188, loss=1.324463963508606
I0213 09:59:19.907515 139647035000576 logging_writer.py:48] [30900] global_step=30900, grad_norm=4.0282979011535645, loss=1.2225046157836914
I0213 10:00:37.231514 139647026607872 logging_writer.py:48] [31000] global_step=31000, grad_norm=2.6555252075195312, loss=1.287078857421875
I0213 10:01:54.384821 139647035000576 logging_writer.py:48] [31100] global_step=31100, grad_norm=1.813650369644165, loss=1.3288277387619019
I0213 10:03:15.387389 139647026607872 logging_writer.py:48] [31200] global_step=31200, grad_norm=3.948963165283203, loss=1.2759984731674194
I0213 10:04:35.259281 139647035000576 logging_writer.py:48] [31300] global_step=31300, grad_norm=2.7347164154052734, loss=1.2250032424926758
I0213 10:06:03.418913 139647026607872 logging_writer.py:48] [31400] global_step=31400, grad_norm=2.748588800430298, loss=1.2761131525039673
I0213 10:07:31.343649 139647035000576 logging_writer.py:48] [31500] global_step=31500, grad_norm=2.118450403213501, loss=1.3112822771072388
I0213 10:09:00.877765 139647026607872 logging_writer.py:48] [31600] global_step=31600, grad_norm=3.9844415187835693, loss=1.268837809562683
I0213 10:10:34.880814 139647035000576 logging_writer.py:48] [31700] global_step=31700, grad_norm=2.2013325691223145, loss=1.3408342599868774
I0213 10:12:06.577881 139647026607872 logging_writer.py:48] [31800] global_step=31800, grad_norm=5.279231071472168, loss=1.2872310876846313
I0213 10:13:36.949760 139647035000576 logging_writer.py:48] [31900] global_step=31900, grad_norm=2.490426540374756, loss=1.3182625770568848
I0213 10:15:01.690498 139647035000576 logging_writer.py:48] [32000] global_step=32000, grad_norm=2.4268205165863037, loss=1.328755259513855
I0213 10:16:18.609730 139647026607872 logging_writer.py:48] [32100] global_step=32100, grad_norm=2.463102340698242, loss=1.3114590644836426
I0213 10:17:36.050744 139647035000576 logging_writer.py:48] [32200] global_step=32200, grad_norm=1.471606969833374, loss=1.2470979690551758
I0213 10:17:44.989714 139803787056960 spec.py:321] Evaluating on the training split.
I0213 10:18:40.671752 139803787056960 spec.py:333] Evaluating on the validation split.
I0213 10:19:33.856849 139803787056960 spec.py:349] Evaluating on the test split.
I0213 10:20:01.916713 139803787056960 submission_runner.py:408] Time since start: 30179.52s, 	Step: 32213, 	{'train/ctc_loss': Array(0.32220346, dtype=float32), 'train/wer': 0.10950485906512467, 'validation/ctc_loss': Array(0.48185685, dtype=float32), 'validation/wer': 0.13907527732990915, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26595923, dtype=float32), 'test/wer': 0.08541019235065911, 'test/num_examples': 2472, 'score': 27380.473700761795, 'total_duration': 30179.518819332123, 'accumulated_submission_time': 27380.473700761795, 'accumulated_eval_time': 2796.5278537273407, 'accumulated_logging_time': 1.0235011577606201}
I0213 10:20:01.956392 139647035000576 logging_writer.py:48] [32213] accumulated_eval_time=2796.527854, accumulated_logging_time=1.023501, accumulated_submission_time=27380.473701, global_step=32213, preemption_count=0, score=27380.473701, test/ctc_loss=0.265959233045578, test/num_examples=2472, test/wer=0.085410, total_duration=30179.518819, train/ctc_loss=0.32220345735549927, train/wer=0.109505, validation/ctc_loss=0.4818568527698517, validation/num_examples=5348, validation/wer=0.139075
I0213 10:21:08.749088 139647026607872 logging_writer.py:48] [32300] global_step=32300, grad_norm=1.9750205278396606, loss=1.2913821935653687
I0213 10:22:24.700427 139647035000576 logging_writer.py:48] [32400] global_step=32400, grad_norm=5.8980607986450195, loss=1.286336898803711
I0213 10:23:49.174874 139647026607872 logging_writer.py:48] [32500] global_step=32500, grad_norm=2.4341554641723633, loss=1.2824198007583618
I0213 10:25:17.894088 139647035000576 logging_writer.py:48] [32600] global_step=32600, grad_norm=1.8386961221694946, loss=1.2879141569137573
I0213 10:26:48.672352 139647026607872 logging_writer.py:48] [32700] global_step=32700, grad_norm=2.012239694595337, loss=1.316219687461853
I0213 10:28:16.851379 139647035000576 logging_writer.py:48] [32800] global_step=32800, grad_norm=2.803767442703247, loss=1.3389484882354736
I0213 10:29:45.119182 139647026607872 logging_writer.py:48] [32900] global_step=32900, grad_norm=3.9222841262817383, loss=1.3022375106811523
I0213 10:31:14.764562 139647035000576 logging_writer.py:48] [33000] global_step=33000, grad_norm=1.9261950254440308, loss=1.3321906328201294
I0213 10:32:32.036951 139647026607872 logging_writer.py:48] [33100] global_step=33100, grad_norm=3.052598714828491, loss=1.2498337030410767
I0213 10:33:52.479428 139647035000576 logging_writer.py:48] [33200] global_step=33200, grad_norm=3.8155648708343506, loss=1.2737513780593872
I0213 10:35:12.004778 139647026607872 logging_writer.py:48] [33300] global_step=33300, grad_norm=1.8464293479919434, loss=1.2707070112228394
I0213 10:36:39.920924 139647035000576 logging_writer.py:48] [33400] global_step=33400, grad_norm=2.0961251258850098, loss=1.2698185443878174
I0213 10:38:08.881646 139647026607872 logging_writer.py:48] [33500] global_step=33500, grad_norm=1.6093441247940063, loss=1.2425681352615356
I0213 10:39:38.065616 139647035000576 logging_writer.py:48] [33600] global_step=33600, grad_norm=2.238654613494873, loss=1.3202928304672241
I0213 10:41:10.103867 139647026607872 logging_writer.py:48] [33700] global_step=33700, grad_norm=2.8319108486175537, loss=1.3441053628921509
I0213 10:42:39.090289 139647035000576 logging_writer.py:48] [33800] global_step=33800, grad_norm=1.8527616262435913, loss=1.3113493919372559
I0213 10:44:03.285947 139803787056960 spec.py:321] Evaluating on the training split.
I0213 10:44:57.793193 139803787056960 spec.py:333] Evaluating on the validation split.
I0213 10:45:53.044945 139803787056960 spec.py:349] Evaluating on the test split.
I0213 10:46:20.306852 139803787056960 submission_runner.py:408] Time since start: 31757.91s, 	Step: 33892, 	{'train/ctc_loss': Array(0.25985754, dtype=float32), 'train/wer': 0.08877919440370077, 'validation/ctc_loss': Array(0.46532407, dtype=float32), 'validation/wer': 0.13267424235110112, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25858718, dtype=float32), 'test/wer': 0.08199784697255906, 'test/num_examples': 2472, 'score': 28821.714812994003, 'total_duration': 31757.905738592148, 'accumulated_submission_time': 28821.714812994003, 'accumulated_eval_time': 2933.5398259162903, 'accumulated_logging_time': 1.0806150436401367}
I0213 10:46:20.346876 139647035000576 logging_writer.py:48] [33892] accumulated_eval_time=2933.539826, accumulated_logging_time=1.080615, accumulated_submission_time=28821.714813, global_step=33892, preemption_count=0, score=28821.714813, test/ctc_loss=0.25858718156814575, test/num_examples=2472, test/wer=0.081998, total_duration=31757.905739, train/ctc_loss=0.25985753536224365, train/wer=0.088779, validation/ctc_loss=0.4653240740299225, validation/num_examples=5348, validation/wer=0.132674
I0213 10:46:27.333734 139647026607872 logging_writer.py:48] [33900] global_step=33900, grad_norm=2.3604164123535156, loss=1.308416485786438
I0213 10:47:46.787812 139647035000576 logging_writer.py:48] [34000] global_step=34000, grad_norm=1.9029546976089478, loss=1.278395175933838
I0213 10:49:06.580470 139647026607872 logging_writer.py:48] [34100] global_step=34100, grad_norm=4.5272393226623535, loss=1.2840888500213623
I0213 10:50:26.474797 139647035000576 logging_writer.py:48] [34200] global_step=34200, grad_norm=2.0503079891204834, loss=1.207749843597412
I0213 10:51:50.476388 139647026607872 logging_writer.py:48] [34300] global_step=34300, grad_norm=2.673003673553467, loss=1.2762470245361328
I0213 10:53:17.637383 139647035000576 logging_writer.py:48] [34400] global_step=34400, grad_norm=3.458407163619995, loss=1.2512106895446777
I0213 10:54:48.005904 139647026607872 logging_writer.py:48] [34500] global_step=34500, grad_norm=2.4492108821868896, loss=1.2408177852630615
I0213 10:56:16.051173 139647035000576 logging_writer.py:48] [34600] global_step=34600, grad_norm=1.5101428031921387, loss=1.274092197418213
I0213 10:57:45.327853 139647026607872 logging_writer.py:48] [34700] global_step=34700, grad_norm=2.0997154712677, loss=1.186387300491333
I0213 10:59:14.013818 139647035000576 logging_writer.py:48] [34800] global_step=34800, grad_norm=2.8097965717315674, loss=1.2275952100753784
I0213 11:00:44.276425 139647026607872 logging_writer.py:48] [34900] global_step=34900, grad_norm=2.958378791809082, loss=1.2238713502883911
I0213 11:02:13.565237 139647035000576 logging_writer.py:48] [35000] global_step=35000, grad_norm=1.956230640411377, loss=1.28095281124115
I0213 11:03:36.176937 139647035000576 logging_writer.py:48] [35100] global_step=35100, grad_norm=2.1127877235412598, loss=1.2394264936447144
I0213 11:04:55.659463 139647026607872 logging_writer.py:48] [35200] global_step=35200, grad_norm=2.419370651245117, loss=1.2034130096435547
I0213 11:06:19.858888 139647035000576 logging_writer.py:48] [35300] global_step=35300, grad_norm=3.594658613204956, loss=1.289324164390564
I0213 11:07:45.607576 139647026607872 logging_writer.py:48] [35400] global_step=35400, grad_norm=2.3389554023742676, loss=1.1823097467422485
I0213 11:09:16.042676 139647035000576 logging_writer.py:48] [35500] global_step=35500, grad_norm=2.785623550415039, loss=1.2544918060302734
I0213 11:10:20.619660 139803787056960 spec.py:321] Evaluating on the training split.
I0213 11:11:15.081886 139803787056960 spec.py:333] Evaluating on the validation split.
I0213 11:12:09.174530 139803787056960 spec.py:349] Evaluating on the test split.
I0213 11:12:35.933173 139803787056960 submission_runner.py:408] Time since start: 33333.53s, 	Step: 35575, 	{'train/ctc_loss': Array(0.23919861, dtype=float32), 'train/wer': 0.08236487997293873, 'validation/ctc_loss': Array(0.45785353, dtype=float32), 'validation/wer': 0.13102329667783388, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25288898, dtype=float32), 'test/wer': 0.08211971645034834, 'test/num_examples': 2472, 'score': 30261.89965081215, 'total_duration': 33333.532770872116, 'accumulated_submission_time': 30261.89965081215, 'accumulated_eval_time': 3068.84513258934, 'accumulated_logging_time': 1.1369514465332031}
I0213 11:12:35.975559 139647035000576 logging_writer.py:48] [35575] accumulated_eval_time=3068.845133, accumulated_logging_time=1.136951, accumulated_submission_time=30261.899651, global_step=35575, preemption_count=0, score=30261.899651, test/ctc_loss=0.2528889775276184, test/num_examples=2472, test/wer=0.082120, total_duration=33333.532771, train/ctc_loss=0.23919861018657684, train/wer=0.082365, validation/ctc_loss=0.4578535258769989, validation/num_examples=5348, validation/wer=0.131023
I0213 11:12:55.716722 139647026607872 logging_writer.py:48] [35600] global_step=35600, grad_norm=3.8712804317474365, loss=1.2225403785705566
I0213 11:14:11.058047 139647035000576 logging_writer.py:48] [35700] global_step=35700, grad_norm=2.835965394973755, loss=1.224797010421753
I0213 11:15:27.287778 139647026607872 logging_writer.py:48] [35800] global_step=35800, grad_norm=4.041187763214111, loss=1.2151483297348022
I0213 11:16:58.791136 139647035000576 logging_writer.py:48] [35900] global_step=35900, grad_norm=2.9985923767089844, loss=1.2791602611541748
I0213 11:18:28.646848 139647026607872 logging_writer.py:48] [36000] global_step=36000, grad_norm=4.280546188354492, loss=1.2975276708602905
I0213 11:19:55.894010 139647035000576 logging_writer.py:48] [36100] global_step=36100, grad_norm=1.6085562705993652, loss=1.2395676374435425
I0213 11:21:12.598659 139647026607872 logging_writer.py:48] [36200] global_step=36200, grad_norm=3.5640878677368164, loss=1.217989444732666
I0213 11:22:29.518408 139647035000576 logging_writer.py:48] [36300] global_step=36300, grad_norm=3.7193050384521484, loss=1.2073215246200562
I0213 11:23:50.962215 139647026607872 logging_writer.py:48] [36400] global_step=36400, grad_norm=2.458252191543579, loss=1.2632648944854736
I0213 11:25:19.517390 139647035000576 logging_writer.py:48] [36500] global_step=36500, grad_norm=2.273898124694824, loss=1.2110388278961182
I0213 11:26:52.948118 139647026607872 logging_writer.py:48] [36600] global_step=36600, grad_norm=6.6831955909729, loss=1.214834451675415
I0213 11:28:22.882130 139647035000576 logging_writer.py:48] [36700] global_step=36700, grad_norm=4.276651859283447, loss=1.1906870603561401
I0213 11:29:47.821283 139647026607872 logging_writer.py:48] [36800] global_step=36800, grad_norm=1.9433125257492065, loss=1.2305766344070435
I0213 11:31:16.493953 139647035000576 logging_writer.py:48] [36900] global_step=36900, grad_norm=15.032586097717285, loss=1.2244086265563965
I0213 11:32:47.045377 139647026607872 logging_writer.py:48] [37000] global_step=37000, grad_norm=3.247345447540283, loss=1.198341727256775
I0213 11:34:18.444191 139647035000576 logging_writer.py:48] [37100] global_step=37100, grad_norm=7.12019681930542, loss=1.2025097608566284
I0213 11:35:35.043621 139647026607872 logging_writer.py:48] [37200] global_step=37200, grad_norm=2.5335350036621094, loss=1.166397213935852
I0213 11:36:36.618406 139803787056960 spec.py:321] Evaluating on the training split.
I0213 11:37:31.482843 139803787056960 spec.py:333] Evaluating on the validation split.
I0213 11:38:24.261620 139803787056960 spec.py:349] Evaluating on the test split.
I0213 11:38:51.068111 139803787056960 submission_runner.py:408] Time since start: 34908.67s, 	Step: 37279, 	{'train/ctc_loss': Array(0.26334742, dtype=float32), 'train/wer': 0.09129235530240172, 'validation/ctc_loss': Array(0.44023106, dtype=float32), 'validation/wer': 0.12639871786207363, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24081011, dtype=float32), 'test/wer': 0.0775699226128816, 'test/num_examples': 2472, 'score': 31702.451380491257, 'total_duration': 34908.66898369789, 'accumulated_submission_time': 31702.451380491257, 'accumulated_eval_time': 3203.287886619568, 'accumulated_logging_time': 1.198265790939331}
I0213 11:38:51.114737 139647035000576 logging_writer.py:48] [37279] accumulated_eval_time=3203.287887, accumulated_logging_time=1.198266, accumulated_submission_time=31702.451380, global_step=37279, preemption_count=0, score=31702.451380, test/ctc_loss=0.2408101111650467, test/num_examples=2472, test/wer=0.077570, total_duration=34908.668984, train/ctc_loss=0.26334741711616516, train/wer=0.091292, validation/ctc_loss=0.440231055021286, validation/num_examples=5348, validation/wer=0.126399
I0213 11:39:07.939269 139647026607872 logging_writer.py:48] [37300] global_step=37300, grad_norm=3.879265308380127, loss=1.2217501401901245
I0213 11:40:23.329413 139647035000576 logging_writer.py:48] [37400] global_step=37400, grad_norm=1.9833577871322632, loss=1.2116811275482178
I0213 11:41:39.117120 139647026607872 logging_writer.py:48] [37500] global_step=37500, grad_norm=1.7902292013168335, loss=1.1599353551864624
I0213 11:43:01.460560 139647035000576 logging_writer.py:48] [37600] global_step=37600, grad_norm=3.287342071533203, loss=1.1881144046783447
I0213 11:44:33.131737 139647026607872 logging_writer.py:48] [37700] global_step=37700, grad_norm=3.64296555519104, loss=1.2673746347427368
I0213 11:46:03.728771 139647035000576 logging_writer.py:48] [37800] global_step=37800, grad_norm=2.104584217071533, loss=1.2084181308746338
I0213 11:47:33.458114 139647026607872 logging_writer.py:48] [37900] global_step=37900, grad_norm=2.4972445964813232, loss=1.1846238374710083
I0213 11:49:03.748056 139647035000576 logging_writer.py:48] [38000] global_step=38000, grad_norm=2.0928971767425537, loss=1.1437983512878418
I0213 11:50:33.558655 139647026607872 logging_writer.py:48] [38100] global_step=38100, grad_norm=3.4329147338867188, loss=1.2234710454940796
I0213 11:51:53.831973 139647035000576 logging_writer.py:48] [38200] global_step=38200, grad_norm=2.01633882522583, loss=1.1964936256408691
I0213 11:53:12.808958 139647026607872 logging_writer.py:48] [38300] global_step=38300, grad_norm=2.7440602779388428, loss=1.2192661762237549
I0213 11:54:32.307637 139647035000576 logging_writer.py:48] [38400] global_step=38400, grad_norm=4.009853839874268, loss=1.2092673778533936
I0213 11:55:55.253354 139647026607872 logging_writer.py:48] [38500] global_step=38500, grad_norm=1.996691107749939, loss=1.169630765914917
I0213 11:57:22.714910 139647035000576 logging_writer.py:48] [38600] global_step=38600, grad_norm=2.432157039642334, loss=1.1882810592651367
I0213 11:58:53.676563 139647026607872 logging_writer.py:48] [38700] global_step=38700, grad_norm=2.2335145473480225, loss=1.2264857292175293
I0213 12:00:24.231692 139647035000576 logging_writer.py:48] [38800] global_step=38800, grad_norm=2.2803401947021484, loss=1.1632730960845947
I0213 12:01:56.187223 139647026607872 logging_writer.py:48] [38900] global_step=38900, grad_norm=3.9040346145629883, loss=1.209263801574707
I0213 12:02:51.370993 139803787056960 spec.py:321] Evaluating on the training split.
I0213 12:03:46.006922 139803787056960 spec.py:333] Evaluating on the validation split.
I0213 12:04:39.795696 139803787056960 spec.py:349] Evaluating on the test split.
I0213 12:05:07.764949 139803787056960 submission_runner.py:408] Time since start: 36485.36s, 	Step: 38963, 	{'train/ctc_loss': Array(0.23935167, dtype=float32), 'train/wer': 0.08118491272805226, 'validation/ctc_loss': Array(0.4321774, dtype=float32), 'validation/wer': 0.12471880822962626, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23291391, dtype=float32), 'test/wer': 0.07533564885341133, 'test/num_examples': 2472, 'score': 33142.61997103691, 'total_duration': 36485.36349272728, 'accumulated_submission_time': 33142.61997103691, 'accumulated_eval_time': 3339.6725568771362, 'accumulated_logging_time': 1.260697364807129}
I0213 12:05:07.815284 139647035000576 logging_writer.py:48] [38963] accumulated_eval_time=3339.672557, accumulated_logging_time=1.260697, accumulated_submission_time=33142.619971, global_step=38963, preemption_count=0, score=33142.619971, test/ctc_loss=0.23291391134262085, test/num_examples=2472, test/wer=0.075336, total_duration=36485.363493, train/ctc_loss=0.23935167491436005, train/wer=0.081185, validation/ctc_loss=0.4321773946285248, validation/num_examples=5348, validation/wer=0.124719
I0213 12:05:36.499593 139647026607872 logging_writer.py:48] [39000] global_step=39000, grad_norm=3.6018688678741455, loss=1.1209059953689575
I0213 12:06:52.064928 139647035000576 logging_writer.py:48] [39100] global_step=39100, grad_norm=2.169468641281128, loss=1.1946406364440918
I0213 12:08:13.164707 139647035000576 logging_writer.py:48] [39200] global_step=39200, grad_norm=1.8743377923965454, loss=1.2012220621109009
I0213 12:09:32.232868 139647026607872 logging_writer.py:48] [39300] global_step=39300, grad_norm=1.7850087881088257, loss=1.1761192083358765
I0213 12:10:50.801952 139647035000576 logging_writer.py:48] [39400] global_step=39400, grad_norm=2.2180423736572266, loss=1.1698966026306152
I0213 12:12:13.751816 139647026607872 logging_writer.py:48] [39500] global_step=39500, grad_norm=1.9984049797058105, loss=1.1774765253067017
I0213 12:13:41.261842 139647035000576 logging_writer.py:48] [39600] global_step=39600, grad_norm=2.724717378616333, loss=1.16080904006958
I0213 12:15:11.102917 139647026607872 logging_writer.py:48] [39700] global_step=39700, grad_norm=3.6965956687927246, loss=1.2054767608642578
I0213 12:16:41.696668 139647035000576 logging_writer.py:48] [39800] global_step=39800, grad_norm=4.3374810218811035, loss=1.1584111452102661
I0213 12:18:12.574504 139647026607872 logging_writer.py:48] [39900] global_step=39900, grad_norm=2.1907622814178467, loss=1.182106375694275
I0213 12:19:43.571646 139647035000576 logging_writer.py:48] [40000] global_step=40000, grad_norm=2.696120023727417, loss=1.218227505683899
I0213 12:21:14.306102 139647026607872 logging_writer.py:48] [40100] global_step=40100, grad_norm=3.065608263015747, loss=1.2355014085769653
I0213 12:22:42.584266 139647035000576 logging_writer.py:48] [40200] global_step=40200, grad_norm=1.943253993988037, loss=1.170129418373108
I0213 12:23:59.222726 139647026607872 logging_writer.py:48] [40300] global_step=40300, grad_norm=2.061337471008301, loss=1.1587045192718506
I0213 12:25:17.609633 139647035000576 logging_writer.py:48] [40400] global_step=40400, grad_norm=1.6745754480361938, loss=1.139169454574585
I0213 12:26:37.027933 139647026607872 logging_writer.py:48] [40500] global_step=40500, grad_norm=3.2540223598480225, loss=1.1978778839111328
I0213 12:28:05.009727 139647035000576 logging_writer.py:48] [40600] global_step=40600, grad_norm=3.1575098037719727, loss=1.1878448724746704
I0213 12:29:08.882788 139803787056960 spec.py:321] Evaluating on the training split.
I0213 12:30:04.465306 139803787056960 spec.py:333] Evaluating on the validation split.
I0213 12:30:57.206970 139803787056960 spec.py:349] Evaluating on the test split.
I0213 12:31:24.083298 139803787056960 submission_runner.py:408] Time since start: 38061.68s, 	Step: 40672, 	{'train/ctc_loss': Array(0.24396911, dtype=float32), 'train/wer': 0.08135381171086638, 'validation/ctc_loss': Array(0.42279685, dtype=float32), 'validation/wer': 0.12214101586259497, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22748674, dtype=float32), 'test/wer': 0.07358885300509821, 'test/num_examples': 2472, 'score': 34583.59595036507, 'total_duration': 38061.68483424187, 'accumulated_submission_time': 34583.59595036507, 'accumulated_eval_time': 3474.866818666458, 'accumulated_logging_time': 1.3295516967773438}
I0213 12:31:24.127891 139647035000576 logging_writer.py:48] [40672] accumulated_eval_time=3474.866819, accumulated_logging_time=1.329552, accumulated_submission_time=34583.595950, global_step=40672, preemption_count=0, score=34583.595950, test/ctc_loss=0.2274867445230484, test/num_examples=2472, test/wer=0.073589, total_duration=38061.684834, train/ctc_loss=0.2439691126346588, train/wer=0.081354, validation/ctc_loss=0.4227968454360962, validation/num_examples=5348, validation/wer=0.122141
I0213 12:31:46.223773 139647026607872 logging_writer.py:48] [40700] global_step=40700, grad_norm=2.2319881916046143, loss=1.214358925819397
I0213 12:33:01.587029 139647035000576 logging_writer.py:48] [40800] global_step=40800, grad_norm=1.6395305395126343, loss=1.1701009273529053
I0213 12:34:20.265599 139647026607872 logging_writer.py:48] [40900] global_step=40900, grad_norm=2.8683993816375732, loss=1.2032911777496338
I0213 12:35:48.871507 139647035000576 logging_writer.py:48] [41000] global_step=41000, grad_norm=2.4718592166900635, loss=1.1724520921707153
I0213 12:37:18.043402 139647026607872 logging_writer.py:48] [41100] global_step=41100, grad_norm=2.206737995147705, loss=1.1684606075286865
I0213 12:38:52.174795 139647035000576 logging_writer.py:48] [41200] global_step=41200, grad_norm=2.514500379562378, loss=1.0719021558761597
I0213 12:40:11.517954 139647026607872 logging_writer.py:48] [41300] global_step=41300, grad_norm=2.7864444255828857, loss=1.1234416961669922
I0213 12:41:33.747591 139647035000576 logging_writer.py:48] [41400] global_step=41400, grad_norm=4.718106746673584, loss=1.1926279067993164
I0213 12:42:55.792669 139647026607872 logging_writer.py:48] [41500] global_step=41500, grad_norm=1.9920835494995117, loss=1.129189133644104
I0213 12:44:21.040356 139647035000576 logging_writer.py:48] [41600] global_step=41600, grad_norm=3.856318712234497, loss=1.131656527519226
I0213 12:45:49.847130 139647026607872 logging_writer.py:48] [41700] global_step=41700, grad_norm=3.2261507511138916, loss=1.148829698562622
I0213 12:47:19.116458 139647035000576 logging_writer.py:48] [41800] global_step=41800, grad_norm=2.4866888523101807, loss=1.1330238580703735
I0213 12:48:51.082902 139647026607872 logging_writer.py:48] [41900] global_step=41900, grad_norm=2.821925640106201, loss=1.1063870191574097
I0213 12:50:19.506144 139647035000576 logging_writer.py:48] [42000] global_step=42000, grad_norm=3.2047812938690186, loss=1.1495859622955322
I0213 12:51:48.794339 139647026607872 logging_writer.py:48] [42100] global_step=42100, grad_norm=1.8976125717163086, loss=1.1865609884262085
I0213 12:53:22.528375 139647035000576 logging_writer.py:48] [42200] global_step=42200, grad_norm=3.3737130165100098, loss=1.1975023746490479
I0213 12:54:46.026687 139647035000576 logging_writer.py:48] [42300] global_step=42300, grad_norm=2.7245073318481445, loss=1.1275779008865356
I0213 12:55:24.407530 139803787056960 spec.py:321] Evaluating on the training split.
I0213 12:56:21.903171 139803787056960 spec.py:333] Evaluating on the validation split.
I0213 12:57:16.563060 139803787056960 spec.py:349] Evaluating on the test split.
I0213 12:57:43.368553 139803787056960 submission_runner.py:408] Time since start: 39640.97s, 	Step: 42350, 	{'train/ctc_loss': Array(0.20017296, dtype=float32), 'train/wer': 0.06986092349424936, 'validation/ctc_loss': Array(0.41711703, dtype=float32), 'validation/wer': 0.1192253106384622, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22420189, dtype=float32), 'test/wer': 0.07271545508094164, 'test/num_examples': 2472, 'score': 36023.78817820549, 'total_duration': 39640.969696998596, 'accumulated_submission_time': 36023.78817820549, 'accumulated_eval_time': 3613.8211603164673, 'accumulated_logging_time': 1.3897809982299805}
I0213 12:57:43.408900 139647035000576 logging_writer.py:48] [42350] accumulated_eval_time=3613.821160, accumulated_logging_time=1.389781, accumulated_submission_time=36023.788178, global_step=42350, preemption_count=0, score=36023.788178, test/ctc_loss=0.22420188784599304, test/num_examples=2472, test/wer=0.072715, total_duration=39640.969697, train/ctc_loss=0.20017296075820923, train/wer=0.069861, validation/ctc_loss=0.41711702942848206, validation/num_examples=5348, validation/wer=0.119225
I0213 12:58:21.848399 139647026607872 logging_writer.py:48] [42400] global_step=42400, grad_norm=2.2215354442596436, loss=1.1583161354064941
I0213 12:59:37.074513 139647035000576 logging_writer.py:48] [42500] global_step=42500, grad_norm=2.8590946197509766, loss=1.1409763097763062
I0213 13:00:52.691306 139647026607872 logging_writer.py:48] [42600] global_step=42600, grad_norm=3.049668550491333, loss=1.1928493976593018
I0213 13:02:15.772487 139647035000576 logging_writer.py:48] [42700] global_step=42700, grad_norm=2.8190066814422607, loss=1.1920379400253296
I0213 13:03:44.952231 139647026607872 logging_writer.py:48] [42800] global_step=42800, grad_norm=2.3560690879821777, loss=1.144311785697937
I0213 13:05:16.182100 139647035000576 logging_writer.py:48] [42900] global_step=42900, grad_norm=3.081242084503174, loss=1.1742764711380005
I0213 13:06:48.140496 139647026607872 logging_writer.py:48] [43000] global_step=43000, grad_norm=3.662806510925293, loss=1.1981089115142822
I0213 13:08:20.055036 139647035000576 logging_writer.py:48] [43100] global_step=43100, grad_norm=2.6733343601226807, loss=1.1354501247406006
I0213 13:09:51.162294 139647026607872 logging_writer.py:48] [43200] global_step=43200, grad_norm=2.9795565605163574, loss=1.1287651062011719
I0213 13:11:18.685839 139647035000576 logging_writer.py:48] [43300] global_step=43300, grad_norm=2.667510509490967, loss=1.1704953908920288
I0213 13:12:36.575966 139647026607872 logging_writer.py:48] [43400] global_step=43400, grad_norm=2.97047758102417, loss=1.1279752254486084
I0213 13:13:55.392510 139647035000576 logging_writer.py:48] [43500] global_step=43500, grad_norm=1.8346502780914307, loss=1.1635410785675049
I0213 13:15:16.879499 139647026607872 logging_writer.py:48] [43600] global_step=43600, grad_norm=2.709157705307007, loss=1.1567137241363525
I0213 13:16:48.031705 139647035000576 logging_writer.py:48] [43700] global_step=43700, grad_norm=1.8431648015975952, loss=1.123321294784546
I0213 13:18:18.298606 139647026607872 logging_writer.py:48] [43800] global_step=43800, grad_norm=2.1847457885742188, loss=1.1629154682159424
I0213 13:19:51.263932 139647035000576 logging_writer.py:48] [43900] global_step=43900, grad_norm=4.701211929321289, loss=1.1511235237121582
I0213 13:21:24.274726 139647026607872 logging_writer.py:48] [44000] global_step=44000, grad_norm=3.1540586948394775, loss=1.1410753726959229
I0213 13:21:43.944086 139803787056960 spec.py:321] Evaluating on the training split.
I0213 13:22:38.594545 139803787056960 spec.py:333] Evaluating on the validation split.
I0213 13:23:31.938077 139803787056960 spec.py:349] Evaluating on the test split.
I0213 13:23:58.729442 139803787056960 submission_runner.py:408] Time since start: 41216.33s, 	Step: 44022, 	{'train/ctc_loss': Array(0.19830093, dtype=float32), 'train/wer': 0.06993784171697597, 'validation/ctc_loss': Array(0.41334906, dtype=float32), 'validation/wer': 0.11827915463857806, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22245006, dtype=float32), 'test/wer': 0.07200454979383747, 'test/num_examples': 2472, 'score': 37464.233345746994, 'total_duration': 41216.33184504509, 'accumulated_submission_time': 37464.233345746994, 'accumulated_eval_time': 3748.6010749340057, 'accumulated_logging_time': 1.4484024047851562}
I0213 13:23:58.769586 139647035000576 logging_writer.py:48] [44022] accumulated_eval_time=3748.601075, accumulated_logging_time=1.448402, accumulated_submission_time=37464.233346, global_step=44022, preemption_count=0, score=37464.233346, test/ctc_loss=0.22245006263256073, test/num_examples=2472, test/wer=0.072005, total_duration=41216.331845, train/ctc_loss=0.19830092787742615, train/wer=0.069938, validation/ctc_loss=0.41334906220436096, validation/num_examples=5348, validation/wer=0.118279
I0213 13:24:58.094274 139647026607872 logging_writer.py:48] [44100] global_step=44100, grad_norm=2.048917531967163, loss=1.1853703260421753
I0213 13:26:13.791455 139647035000576 logging_writer.py:48] [44200] global_step=44200, grad_norm=2.011528491973877, loss=1.15743887424469
I0213 13:27:45.777428 139647035000576 logging_writer.py:48] [44300] global_step=44300, grad_norm=4.47291898727417, loss=1.1287214756011963
I0213 13:29:02.723767 139647026607872 logging_writer.py:48] [44400] global_step=44400, grad_norm=1.6431689262390137, loss=1.1492295265197754
I0213 13:30:24.636900 139647035000576 logging_writer.py:48] [44500] global_step=44500, grad_norm=3.3213248252868652, loss=1.1773613691329956
I0213 13:31:48.064198 139647026607872 logging_writer.py:48] [44600] global_step=44600, grad_norm=1.851366400718689, loss=1.1584441661834717
I0213 13:33:12.395437 139647035000576 logging_writer.py:48] [44700] global_step=44700, grad_norm=2.0601656436920166, loss=1.1872882843017578
I0213 13:34:44.453017 139647026607872 logging_writer.py:48] [44800] global_step=44800, grad_norm=4.19893217086792, loss=1.171570897102356
I0213 13:36:11.699296 139647035000576 logging_writer.py:48] [44900] global_step=44900, grad_norm=4.4837775230407715, loss=1.1620334386825562
I0213 13:37:45.287377 139647026607872 logging_writer.py:48] [45000] global_step=45000, grad_norm=2.5396721363067627, loss=1.1354442834854126
I0213 13:39:13.106570 139647035000576 logging_writer.py:48] [45100] global_step=45100, grad_norm=1.8242970705032349, loss=1.1280816793441772
I0213 13:40:44.784313 139647026607872 logging_writer.py:48] [45200] global_step=45200, grad_norm=3.669626235961914, loss=1.127131700515747
I0213 13:42:14.842792 139647035000576 logging_writer.py:48] [45300] global_step=45300, grad_norm=1.8408172130584717, loss=1.1024954319000244
I0213 13:43:37.937078 139647035000576 logging_writer.py:48] [45400] global_step=45400, grad_norm=3.3096632957458496, loss=1.1184930801391602
I0213 13:44:55.729764 139647026607872 logging_writer.py:48] [45500] global_step=45500, grad_norm=2.3510727882385254, loss=1.128332257270813
I0213 13:46:14.963184 139647035000576 logging_writer.py:48] [45600] global_step=45600, grad_norm=2.48537278175354, loss=1.1049724817276
I0213 13:47:39.776414 139647026607872 logging_writer.py:48] [45700] global_step=45700, grad_norm=3.3996012210845947, loss=1.1489275693893433
I0213 13:48:00.106912 139803787056960 spec.py:321] Evaluating on the training split.
I0213 13:48:53.481317 139803787056960 spec.py:333] Evaluating on the validation split.
I0213 13:49:47.280530 139803787056960 spec.py:349] Evaluating on the test split.
I0213 13:50:15.747125 139803787056960 submission_runner.py:408] Time since start: 42793.35s, 	Step: 45723, 	{'train/ctc_loss': Array(0.2161998, dtype=float32), 'train/wer': 0.07555089789664933, 'validation/ctc_loss': Array(0.4112349, dtype=float32), 'validation/wer': 0.11751643704683472, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22004013, dtype=float32), 'test/wer': 0.07104990555115472, 'test/num_examples': 2472, 'score': 38905.48064374924, 'total_duration': 42793.348590135574, 'accumulated_submission_time': 38905.48064374924, 'accumulated_eval_time': 3884.2349379062653, 'accumulated_logging_time': 1.505363941192627}
I0213 13:50:15.783432 139647035000576 logging_writer.py:48] [45723] accumulated_eval_time=3884.234938, accumulated_logging_time=1.505364, accumulated_submission_time=38905.480644, global_step=45723, preemption_count=0, score=38905.480644, test/ctc_loss=0.22004012763500214, test/num_examples=2472, test/wer=0.071050, total_duration=42793.348590, train/ctc_loss=0.21619980037212372, train/wer=0.075551, validation/ctc_loss=0.41123488545417786, validation/num_examples=5348, validation/wer=0.117516
I0213 13:51:15.000309 139647026607872 logging_writer.py:48] [45800] global_step=45800, grad_norm=1.789645791053772, loss=1.0992558002471924
I0213 13:52:30.545869 139647035000576 logging_writer.py:48] [45900] global_step=45900, grad_norm=14.435990333557129, loss=1.118632197380066
I0213 13:53:55.895050 139647026607872 logging_writer.py:48] [46000] global_step=46000, grad_norm=3.419922113418579, loss=1.1635633707046509
I0213 13:55:24.306062 139647035000576 logging_writer.py:48] [46100] global_step=46100, grad_norm=3.0494308471679688, loss=1.0802514553070068
I0213 13:56:54.911668 139647026607872 logging_writer.py:48] [46200] global_step=46200, grad_norm=5.3531622886657715, loss=1.1203137636184692
I0213 13:58:26.209881 139647035000576 logging_writer.py:48] [46300] global_step=46300, grad_norm=1.6528074741363525, loss=1.1096547842025757
I0213 13:59:54.390830 139647035000576 logging_writer.py:48] [46400] global_step=46400, grad_norm=2.3864362239837646, loss=1.13763427734375
I0213 14:01:10.772685 139647026607872 logging_writer.py:48] [46500] global_step=46500, grad_norm=2.7256581783294678, loss=1.107193112373352
I0213 14:02:29.479464 139647035000576 logging_writer.py:48] [46600] global_step=46600, grad_norm=8.095402717590332, loss=1.098091721534729
I0213 14:03:52.211975 139647026607872 logging_writer.py:48] [46700] global_step=46700, grad_norm=1.930355429649353, loss=1.2330787181854248
I0213 14:05:19.456326 139647035000576 logging_writer.py:48] [46800] global_step=46800, grad_norm=2.7016284465789795, loss=1.151505470275879
I0213 14:06:48.694540 139647026607872 logging_writer.py:48] [46900] global_step=46900, grad_norm=1.9656773805618286, loss=1.122018814086914
I0213 14:08:18.812371 139647035000576 logging_writer.py:48] [47000] global_step=47000, grad_norm=2.4076008796691895, loss=1.0765036344528198
I0213 14:09:52.223564 139647026607872 logging_writer.py:48] [47100] global_step=47100, grad_norm=3.2775588035583496, loss=1.1147358417510986
I0213 14:11:23.024687 139647035000576 logging_writer.py:48] [47200] global_step=47200, grad_norm=3.5249147415161133, loss=1.1289058923721313
I0213 14:12:51.040526 139647026607872 logging_writer.py:48] [47300] global_step=47300, grad_norm=1.6774524450302124, loss=1.1743627786636353
I0213 14:14:16.309795 139803787056960 spec.py:321] Evaluating on the training split.
I0213 14:15:21.329372 139803787056960 spec.py:333] Evaluating on the validation split.
I0213 14:16:14.017427 139803787056960 spec.py:349] Evaluating on the test split.
I0213 14:16:41.651649 139803787056960 submission_runner.py:408] Time since start: 44379.25s, 	Step: 47391, 	{'train/ctc_loss': Array(0.14322816, dtype=float32), 'train/wer': 0.05121731875656876, 'validation/ctc_loss': Array(0.4108572, dtype=float32), 'validation/wer': 0.1175357463529548, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2199478, dtype=float32), 'test/wer': 0.07127333292710174, 'test/num_examples': 2472, 'score': 40345.91604781151, 'total_duration': 44379.25354409218, 'accumulated_submission_time': 40345.91604781151, 'accumulated_eval_time': 4029.571016550064, 'accumulated_logging_time': 1.5600640773773193}
I0213 14:16:41.696015 139647035000576 logging_writer.py:48] [47391] accumulated_eval_time=4029.571017, accumulated_logging_time=1.560064, accumulated_submission_time=40345.916048, global_step=47391, preemption_count=0, score=40345.916048, test/ctc_loss=0.21994780004024506, test/num_examples=2472, test/wer=0.071273, total_duration=44379.253544, train/ctc_loss=0.14322815835475922, train/wer=0.051217, validation/ctc_loss=0.4108572006225586, validation/num_examples=5348, validation/wer=0.117536
I0213 14:16:49.352543 139647026607872 logging_writer.py:48] [47400] global_step=47400, grad_norm=1.9288551807403564, loss=1.1654329299926758
I0213 14:18:04.875493 139647035000576 logging_writer.py:48] [47500] global_step=47500, grad_norm=1.745322585105896, loss=1.1230024099349976
I0213 14:19:20.390047 139647026607872 logging_writer.py:48] [47600] global_step=47600, grad_norm=3.561663866043091, loss=1.164920449256897
I0213 14:20:36.704427 139647035000576 logging_writer.py:48] [47700] global_step=47700, grad_norm=2.3139376640319824, loss=1.147118091583252
I0213 14:21:54.487771 139647026607872 logging_writer.py:48] [47800] global_step=47800, grad_norm=2.56168270111084, loss=1.123396635055542
I0213 14:23:22.648826 139647035000576 logging_writer.py:48] [47900] global_step=47900, grad_norm=3.2798261642456055, loss=1.145650029182434
I0213 14:24:51.244949 139803787056960 spec.py:321] Evaluating on the training split.
I0213 14:25:47.274902 139803787056960 spec.py:333] Evaluating on the validation split.
I0213 14:26:41.016286 139803787056960 spec.py:349] Evaluating on the test split.
I0213 14:27:07.570775 139803787056960 submission_runner.py:408] Time since start: 45005.18s, 	Step: 48000, 	{'train/ctc_loss': Array(0.13066882, dtype=float32), 'train/wer': 0.04626666252369263, 'validation/ctc_loss': Array(0.41104025, dtype=float32), 'validation/wer': 0.1175357463529548, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21999335, dtype=float32), 'test/wer': 0.07125302134747019, 'test/num_examples': 2472, 'score': 40835.422513484955, 'total_duration': 45005.17563033104, 'accumulated_submission_time': 40835.422513484955, 'accumulated_eval_time': 4165.893843173981, 'accumulated_logging_time': 1.620795726776123}
I0213 14:27:07.606705 139647035000576 logging_writer.py:48] [48000] accumulated_eval_time=4165.893843, accumulated_logging_time=1.620796, accumulated_submission_time=40835.422513, global_step=48000, preemption_count=0, score=40835.422513, test/ctc_loss=0.21999335289001465, test/num_examples=2472, test/wer=0.071253, total_duration=45005.175630, train/ctc_loss=0.13066881895065308, train/wer=0.046267, validation/ctc_loss=0.4110402464866638, validation/num_examples=5348, validation/wer=0.117536
I0213 14:27:07.629798 139647026607872 logging_writer.py:48] [48000] global_step=48000, preemption_count=0, score=40835.422513
I0213 14:27:07.859156 139803787056960 checkpoints.py:490] Saving checkpoint at step: 48000
I0213 14:27:08.871591 139803787056960 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_1/librispeech_deepspeech_jax/trial_5/checkpoint_48000
I0213 14:27:08.891389 139803787056960 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_1/librispeech_deepspeech_jax/trial_5/checkpoint_48000.
I0213 14:27:10.173000 139803787056960 submission_runner.py:583] Tuning trial 5/5
I0213 14:27:10.173246 139803787056960 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.1, label_smoothing=0.0, learning_rate=0.0017486387539278373, one_minus_beta1=0.06733926164, beta2=0.9955159689799007, weight_decay=0.08121616522670176, warmup_factor=0.02)
I0213 14:27:10.187281 139803787056960 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(30.994576, dtype=float32), 'train/wer': 4.707897006283109, 'validation/ctc_loss': Array(30.812992, dtype=float32), 'validation/wer': 4.233575021481603, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.939766, dtype=float32), 'test/wer': 4.561066764162249, 'test/num_examples': 2472, 'score': 16.178600549697876, 'total_duration': 240.9686577320099, 'accumulated_submission_time': 16.178600549697876, 'accumulated_eval_time': 224.78997611999512, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1692, {'train/ctc_loss': Array(2.0471356, dtype=float32), 'train/wer': 0.4961872165120053, 'validation/ctc_loss': Array(2.49202, dtype=float32), 'validation/wer': 0.547013333075876, 'validation/num_examples': 5348, 'test/ctc_loss': Array(2.0015655, dtype=float32), 'test/wer': 0.4741332033392237, 'test/num_examples': 2472, 'score': 1456.1483154296875, 'total_duration': 1814.265554189682, 'accumulated_submission_time': 1456.1483154296875, 'accumulated_eval_time': 358.0138473510742, 'accumulated_logging_time': 0.028545856475830078, 'global_step': 1692, 'preemption_count': 0}), (3409, {'train/ctc_loss': Array(0.7372342, dtype=float32), 'train/wer': 0.22996908588842316, 'validation/ctc_loss': Array(0.913854, dtype=float32), 'validation/wer': 0.25889917645809396, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5924073, dtype=float32), 'test/wer': 0.18442914305445535, 'test/num_examples': 2472, 'score': 2896.421770811081, 'total_duration': 3390.00124835968, 'accumulated_submission_time': 2896.421770811081, 'accumulated_eval_time': 493.33926486968994, 'accumulated_logging_time': 0.08494257926940918, 'global_step': 3409, 'preemption_count': 0}), (5098, {'train/ctc_loss': Array(0.63931066, dtype=float32), 'train/wer': 0.2019689524050605, 'validation/ctc_loss': Array(0.7830972, dtype=float32), 'validation/wer': 0.2235824555644593, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.48912373, dtype=float32), 'test/wer': 0.15932403062986208, 'test/num_examples': 2472, 'score': 4336.899610280991, 'total_duration': 4966.236393213272, 'accumulated_submission_time': 4336.899610280991, 'accumulated_eval_time': 628.9697065353394, 'accumulated_logging_time': 0.13415312767028809, 'global_step': 5098, 'preemption_count': 0}), (6781, {'train/ctc_loss': Array(0.7021821, dtype=float32), 'train/wer': 0.21959843927816616, 'validation/ctc_loss': Array(0.74043995, dtype=float32), 'validation/wer': 0.2126533883004914, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.44423807, dtype=float32), 'test/wer': 0.14409034590620112, 'test/num_examples': 2472, 'score': 5777.278906345367, 'total_duration': 6539.980969905853, 'accumulated_submission_time': 5777.278906345367, 'accumulated_eval_time': 762.1963560581207, 'accumulated_logging_time': 0.19386816024780273, 'global_step': 6781, 'preemption_count': 0}), (8491, {'train/ctc_loss': Array(0.5751879, dtype=float32), 'train/wer': 0.1796062016228983, 'validation/ctc_loss': Array(0.7267631, dtype=float32), 'validation/wer': 0.206619230137965, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.44064087, dtype=float32), 'test/wer': 0.141490463713363, 'test/num_examples': 2472, 'score': 7217.765897512436, 'total_duration': 8114.1622631549835, 'accumulated_submission_time': 7217.765897512436, 'accumulated_eval_time': 895.7563931941986, 'accumulated_logging_time': 0.24855613708496094, 'global_step': 8491, 'preemption_count': 0}), (10176, {'train/ctc_loss': Array(0.53614444, dtype=float32), 'train/wer': 0.17392318103752233, 'validation/ctc_loss': Array(0.6622156, dtype=float32), 'validation/wer': 0.19043803160933412, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.39185888, dtype=float32), 'test/wer': 0.1262770905693336, 'test/num_examples': 2472, 'score': 8658.173372507095, 'total_duration': 9688.754879951477, 'accumulated_submission_time': 8658.173372507095, 'accumulated_eval_time': 1029.8109738826752, 'accumulated_logging_time': 0.3024752140045166, 'global_step': 10176, 'preemption_count': 0}), (11874, {'train/ctc_loss': Array(0.4290752, dtype=float32), 'train/wer': 0.1424052505581408, 'validation/ctc_loss': Array(0.65312153, dtype=float32), 'validation/wer': 0.18638307732411635, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.38414696, dtype=float32), 'test/wer': 0.12262100623565494, 'test/num_examples': 2472, 'score': 10098.086151361465, 'total_duration': 11264.540845394135, 'accumulated_submission_time': 10098.086151361465, 'accumulated_eval_time': 1165.5521621704102, 'accumulated_logging_time': 0.3547070026397705, 'global_step': 11874, 'preemption_count': 0}), (13566, {'train/ctc_loss': Array(0.5048986, dtype=float32), 'train/wer': 0.16362521757216827, 'validation/ctc_loss': Array(0.636037, dtype=float32), 'validation/wer': 0.18039719242689015, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.37574464, dtype=float32), 'test/wer': 0.12229602096155018, 'test/num_examples': 2472, 'score': 11538.334949493408, 'total_duration': 12843.813130378723, 'accumulated_submission_time': 11538.334949493408, 'accumulated_eval_time': 1304.4409563541412, 'accumulated_logging_time': 0.4097585678100586, 'global_step': 13566, 'preemption_count': 0}), (15256, {'train/ctc_loss': Array(0.45856845, dtype=float32), 'train/wer': 0.15020440881763528, 'validation/ctc_loss': Array(0.6351292, dtype=float32), 'validation/wer': 0.18133369377371425, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.36284128, dtype=float32), 'test/wer': 0.11731968395182094, 'test/num_examples': 2472, 'score': 12978.819166183472, 'total_duration': 14421.089455366135, 'accumulated_submission_time': 12978.819166183472, 'accumulated_eval_time': 1441.1059172153473, 'accumulated_logging_time': 0.4590444564819336, 'global_step': 15256, 'preemption_count': 0}), (16959, {'train/ctc_loss': Array(0.44045535, dtype=float32), 'train/wer': 0.1457255361385011, 'validation/ctc_loss': Array(0.6102114, dtype=float32), 'validation/wer': 0.17462370989698486, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3535885, dtype=float32), 'test/wer': 0.11360266487924765, 'test/num_examples': 2472, 'score': 14418.86126089096, 'total_duration': 15997.133904457092, 'accumulated_submission_time': 14418.86126089096, 'accumulated_eval_time': 1576.9730966091156, 'accumulated_logging_time': 0.5129210948944092, 'global_step': 16959, 'preemption_count': 0}), (18655, {'train/ctc_loss': Array(0.40981236, dtype=float32), 'train/wer': 0.13409648405560098, 'validation/ctc_loss': Array(0.5838607, dtype=float32), 'validation/wer': 0.1677399422651747, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33918792, dtype=float32), 'test/wer': 0.10795604574167733, 'test/num_examples': 2472, 'score': 15859.378022432327, 'total_duration': 17572.3883395195, 'accumulated_submission_time': 15859.378022432327, 'accumulated_eval_time': 1711.5768485069275, 'accumulated_logging_time': 0.5694046020507812, 'global_step': 18655, 'preemption_count': 0}), (20349, {'train/ctc_loss': Array(0.42425936, dtype=float32), 'train/wer': 0.14089816744684, 'validation/ctc_loss': Array(0.5756924, dtype=float32), 'validation/wer': 0.16473734516350155, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33319625, dtype=float32), 'test/wer': 0.1072654520342047, 'test/num_examples': 2472, 'score': 17299.322156190872, 'total_duration': 19145.56060743332, 'accumulated_submission_time': 17299.322156190872, 'accumulated_eval_time': 1844.6749074459076, 'accumulated_logging_time': 0.621281623840332, 'global_step': 20349, 'preemption_count': 0}), (22051, {'train/ctc_loss': Array(0.4327474, dtype=float32), 'train/wer': 0.13708161804188293, 'validation/ctc_loss': Array(0.5562388, dtype=float32), 'validation/wer': 0.15930177549069774, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32314736, dtype=float32), 'test/wer': 0.1023906729226332, 'test/num_examples': 2472, 'score': 18739.717533826828, 'total_duration': 20722.885338544846, 'accumulated_submission_time': 18739.717533826828, 'accumulated_eval_time': 1981.4680063724518, 'accumulated_logging_time': 0.6765701770782471, 'global_step': 22051, 'preemption_count': 0}), (23736, {'train/ctc_loss': Array(0.36796176, dtype=float32), 'train/wer': 0.12416072206866494, 'validation/ctc_loss': Array(0.54934376, dtype=float32), 'validation/wer': 0.1599776012049007, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3138135, dtype=float32), 'test/wer': 0.10019702232242601, 'test/num_examples': 2472, 'score': 20179.681025505066, 'total_duration': 22298.57732820511, 'accumulated_submission_time': 20179.681025505066, 'accumulated_eval_time': 2117.053550004959, 'accumulated_logging_time': 0.740778923034668, 'global_step': 23736, 'preemption_count': 0}), (25430, {'train/ctc_loss': Array(0.39533722, dtype=float32), 'train/wer': 0.13457298677187282, 'validation/ctc_loss': Array(0.53948, dtype=float32), 'validation/wer': 0.1550923467565193, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30784765, dtype=float32), 'test/wer': 0.09753620539069323, 'test/num_examples': 2472, 'score': 21619.953918218613, 'total_duration': 23874.16535425186, 'accumulated_submission_time': 21619.953918218613, 'accumulated_eval_time': 2252.2356536388397, 'accumulated_logging_time': 0.7953715324401855, 'global_step': 25430, 'preemption_count': 0}), (27147, {'train/ctc_loss': Array(0.3278709, dtype=float32), 'train/wer': 0.11051153691868931, 'validation/ctc_loss': Array(0.52832973, dtype=float32), 'validation/wer': 0.1517132181855045, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2963604, dtype=float32), 'test/wer': 0.09406292527369854, 'test/num_examples': 2472, 'score': 23060.258680820465, 'total_duration': 25451.059435129166, 'accumulated_submission_time': 23060.258680820465, 'accumulated_eval_time': 2388.6867899894714, 'accumulated_logging_time': 0.8549208641052246, 'global_step': 27147, 'preemption_count': 0}), (28814, {'train/ctc_loss': Array(0.33591753, dtype=float32), 'train/wer': 0.11227693090867955, 'validation/ctc_loss': Array(0.49735403, dtype=float32), 'validation/wer': 0.1428309373702656, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2814619, dtype=float32), 'test/wer': 0.08965531249365263, 'test/num_examples': 2472, 'score': 24500.23553586006, 'total_duration': 27026.162631988525, 'accumulated_submission_time': 24500.23553586006, 'accumulated_eval_time': 2523.680060863495, 'accumulated_logging_time': 0.9110238552093506, 'global_step': 28814, 'preemption_count': 0}), (30496, {'train/ctc_loss': Array(0.32313046, dtype=float32), 'train/wer': 0.1091377633158311, 'validation/ctc_loss': Array(0.49119592, dtype=float32), 'validation/wer': 0.14111240912557807, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2728875, dtype=float32), 'test/wer': 0.08817256718054964, 'test/num_examples': 2472, 'score': 25940.209047079086, 'total_duration': 28602.19198703766, 'accumulated_submission_time': 25940.209047079086, 'accumulated_eval_time': 2659.6065866947174, 'accumulated_logging_time': 0.9619748592376709, 'global_step': 30496, 'preemption_count': 0}), (32213, {'train/ctc_loss': Array(0.32220346, dtype=float32), 'train/wer': 0.10950485906512467, 'validation/ctc_loss': Array(0.48185685, dtype=float32), 'validation/wer': 0.13907527732990915, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26595923, dtype=float32), 'test/wer': 0.08541019235065911, 'test/num_examples': 2472, 'score': 27380.473700761795, 'total_duration': 30179.518819332123, 'accumulated_submission_time': 27380.473700761795, 'accumulated_eval_time': 2796.5278537273407, 'accumulated_logging_time': 1.0235011577606201, 'global_step': 32213, 'preemption_count': 0}), (33892, {'train/ctc_loss': Array(0.25985754, dtype=float32), 'train/wer': 0.08877919440370077, 'validation/ctc_loss': Array(0.46532407, dtype=float32), 'validation/wer': 0.13267424235110112, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25858718, dtype=float32), 'test/wer': 0.08199784697255906, 'test/num_examples': 2472, 'score': 28821.714812994003, 'total_duration': 31757.905738592148, 'accumulated_submission_time': 28821.714812994003, 'accumulated_eval_time': 2933.5398259162903, 'accumulated_logging_time': 1.0806150436401367, 'global_step': 33892, 'preemption_count': 0}), (35575, {'train/ctc_loss': Array(0.23919861, dtype=float32), 'train/wer': 0.08236487997293873, 'validation/ctc_loss': Array(0.45785353, dtype=float32), 'validation/wer': 0.13102329667783388, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25288898, dtype=float32), 'test/wer': 0.08211971645034834, 'test/num_examples': 2472, 'score': 30261.89965081215, 'total_duration': 33333.532770872116, 'accumulated_submission_time': 30261.89965081215, 'accumulated_eval_time': 3068.84513258934, 'accumulated_logging_time': 1.1369514465332031, 'global_step': 35575, 'preemption_count': 0}), (37279, {'train/ctc_loss': Array(0.26334742, dtype=float32), 'train/wer': 0.09129235530240172, 'validation/ctc_loss': Array(0.44023106, dtype=float32), 'validation/wer': 0.12639871786207363, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24081011, dtype=float32), 'test/wer': 0.0775699226128816, 'test/num_examples': 2472, 'score': 31702.451380491257, 'total_duration': 34908.66898369789, 'accumulated_submission_time': 31702.451380491257, 'accumulated_eval_time': 3203.287886619568, 'accumulated_logging_time': 1.198265790939331, 'global_step': 37279, 'preemption_count': 0}), (38963, {'train/ctc_loss': Array(0.23935167, dtype=float32), 'train/wer': 0.08118491272805226, 'validation/ctc_loss': Array(0.4321774, dtype=float32), 'validation/wer': 0.12471880822962626, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23291391, dtype=float32), 'test/wer': 0.07533564885341133, 'test/num_examples': 2472, 'score': 33142.61997103691, 'total_duration': 36485.36349272728, 'accumulated_submission_time': 33142.61997103691, 'accumulated_eval_time': 3339.6725568771362, 'accumulated_logging_time': 1.260697364807129, 'global_step': 38963, 'preemption_count': 0}), (40672, {'train/ctc_loss': Array(0.24396911, dtype=float32), 'train/wer': 0.08135381171086638, 'validation/ctc_loss': Array(0.42279685, dtype=float32), 'validation/wer': 0.12214101586259497, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22748674, dtype=float32), 'test/wer': 0.07358885300509821, 'test/num_examples': 2472, 'score': 34583.59595036507, 'total_duration': 38061.68483424187, 'accumulated_submission_time': 34583.59595036507, 'accumulated_eval_time': 3474.866818666458, 'accumulated_logging_time': 1.3295516967773438, 'global_step': 40672, 'preemption_count': 0}), (42350, {'train/ctc_loss': Array(0.20017296, dtype=float32), 'train/wer': 0.06986092349424936, 'validation/ctc_loss': Array(0.41711703, dtype=float32), 'validation/wer': 0.1192253106384622, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22420189, dtype=float32), 'test/wer': 0.07271545508094164, 'test/num_examples': 2472, 'score': 36023.78817820549, 'total_duration': 39640.969696998596, 'accumulated_submission_time': 36023.78817820549, 'accumulated_eval_time': 3613.8211603164673, 'accumulated_logging_time': 1.3897809982299805, 'global_step': 42350, 'preemption_count': 0}), (44022, {'train/ctc_loss': Array(0.19830093, dtype=float32), 'train/wer': 0.06993784171697597, 'validation/ctc_loss': Array(0.41334906, dtype=float32), 'validation/wer': 0.11827915463857806, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22245006, dtype=float32), 'test/wer': 0.07200454979383747, 'test/num_examples': 2472, 'score': 37464.233345746994, 'total_duration': 41216.33184504509, 'accumulated_submission_time': 37464.233345746994, 'accumulated_eval_time': 3748.6010749340057, 'accumulated_logging_time': 1.4484024047851562, 'global_step': 44022, 'preemption_count': 0}), (45723, {'train/ctc_loss': Array(0.2161998, dtype=float32), 'train/wer': 0.07555089789664933, 'validation/ctc_loss': Array(0.4112349, dtype=float32), 'validation/wer': 0.11751643704683472, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22004013, dtype=float32), 'test/wer': 0.07104990555115472, 'test/num_examples': 2472, 'score': 38905.48064374924, 'total_duration': 42793.348590135574, 'accumulated_submission_time': 38905.48064374924, 'accumulated_eval_time': 3884.2349379062653, 'accumulated_logging_time': 1.505363941192627, 'global_step': 45723, 'preemption_count': 0}), (47391, {'train/ctc_loss': Array(0.14322816, dtype=float32), 'train/wer': 0.05121731875656876, 'validation/ctc_loss': Array(0.4108572, dtype=float32), 'validation/wer': 0.1175357463529548, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2199478, dtype=float32), 'test/wer': 0.07127333292710174, 'test/num_examples': 2472, 'score': 40345.91604781151, 'total_duration': 44379.25354409218, 'accumulated_submission_time': 40345.91604781151, 'accumulated_eval_time': 4029.571016550064, 'accumulated_logging_time': 1.5600640773773193, 'global_step': 47391, 'preemption_count': 0}), (48000, {'train/ctc_loss': Array(0.13066882, dtype=float32), 'train/wer': 0.04626666252369263, 'validation/ctc_loss': Array(0.41104025, dtype=float32), 'validation/wer': 0.1175357463529548, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21999335, dtype=float32), 'test/wer': 0.07125302134747019, 'test/num_examples': 2472, 'score': 40835.422513484955, 'total_duration': 45005.17563033104, 'accumulated_submission_time': 40835.422513484955, 'accumulated_eval_time': 4165.893843173981, 'accumulated_logging_time': 1.620795726776123, 'global_step': 48000, 'preemption_count': 0})], 'global_step': 48000}
I0213 14:27:10.187530 139803787056960 submission_runner.py:586] Timing: 40835.422513484955
I0213 14:27:10.187598 139803787056960 submission_runner.py:588] Total number of evals: 30
I0213 14:27:10.187655 139803787056960 submission_runner.py:589] ====================
I0213 14:27:10.224801 139803787056960 submission_runner.py:673] Final librispeech_deepspeech score: 40087.15713596344
