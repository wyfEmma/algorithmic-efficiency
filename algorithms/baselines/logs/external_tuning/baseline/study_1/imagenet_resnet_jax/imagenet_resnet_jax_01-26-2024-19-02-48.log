python3 submission_runner.py --framework=jax --workload=imagenet_resnet --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --data_dir=/data/imagenet/jax --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=prize_qualification/study_1 --overwrite=true --save_checkpoints=false --num_tuning_trials=5 --rng_seed=916031063 --max_global_steps=186666 --imagenet_v2_data_dir=/data/imagenet/jax 2>&1 | tee -a /logs/imagenet_resnet_jax_01-26-2024-19-02-48.log
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
I0126 19:03:09.630706 139822745589568 logger_utils.py:76] Creating experiment directory at /experiment_runs/prize_qualification/study_1/imagenet_resnet_jax.
I0126 19:03:10.706087 139822745589568 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I0126 19:03:10.707062 139822745589568 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0126 19:03:10.707206 139822745589568 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0126 19:03:10.708595 139822745589568 submission_runner.py:542] Using RNG seed 916031063
I0126 19:03:11.868288 139822745589568 submission_runner.py:551] --- Tuning run 1/5 ---
I0126 19:03:11.868479 139822745589568 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_1/imagenet_resnet_jax/trial_1.
I0126 19:03:11.868852 139822745589568 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_1/imagenet_resnet_jax/trial_1/hparams.json.
I0126 19:03:12.048344 139822745589568 submission_runner.py:206] Initializing dataset.
I0126 19:03:12.065601 139822745589568 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0126 19:03:12.080677 139822745589568 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0126 19:03:12.451655 139822745589568 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0126 19:03:13.621155 139822745589568 submission_runner.py:213] Initializing model.
I0126 19:03:23.839234 139822745589568 submission_runner.py:255] Initializing optimizer.
I0126 19:03:25.540735 139822745589568 submission_runner.py:262] Initializing metrics bundle.
I0126 19:03:25.540917 139822745589568 submission_runner.py:280] Initializing checkpoint and logger.
I0126 19:03:25.542019 139822745589568 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_1/imagenet_resnet_jax/trial_1 with prefix checkpoint_
I0126 19:03:25.542150 139822745589568 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_1/imagenet_resnet_jax/trial_1/meta_data_0.json.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0126 19:03:25.850574 139822745589568 logger_utils.py:220] Unable to record git information. Continuing without it.
I0126 19:03:26.128603 139822745589568 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_1/imagenet_resnet_jax/trial_1/flags_0.json.
I0126 19:03:26.138535 139822745589568 submission_runner.py:314] Starting training loop.
I0126 19:04:22.430693 139651717256960 logging_writer.py:48] [0] global_step=0, grad_norm=0.5895074009895325, loss=6.928974628448486
I0126 19:04:22.449882 139822745589568 spec.py:321] Evaluating on the training split.
I0126 19:04:23.458961 139822745589568 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0126 19:04:23.468740 139822745589568 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0126 19:04:23.556414 139822745589568 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0126 19:04:36.861889 139822745589568 spec.py:333] Evaluating on the validation split.
I0126 19:04:38.114722 139822745589568 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0126 19:04:38.140797 139822745589568 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0126 19:04:38.208095 139822745589568 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0126 19:04:54.537138 139822745589568 spec.py:349] Evaluating on the test split.
I0126 19:04:55.328947 139822745589568 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0126 19:04:55.334500 139822745589568 dataset_builder.py:528] Reusing dataset imagenet_v2 (/data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0)
I0126 19:04:55.373885 139822745589568 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0126 19:04:59.297026 139822745589568 submission_runner.py:408] Time since start: 93.16s, 	Step: 1, 	{'train/accuracy': 0.000996492337435484, 'train/loss': 6.9111857414245605, 'validation/accuracy': 0.001019999966956675, 'validation/loss': 6.91091251373291, 'validation/num_examples': 50000, 'test/accuracy': 0.0012000000569969416, 'test/loss': 6.910790920257568, 'test/num_examples': 10000, 'score': 56.31122303009033, 'total_duration': 93.1584370136261, 'accumulated_submission_time': 56.31122303009033, 'accumulated_eval_time': 36.84709548950195, 'accumulated_logging_time': 0}
I0126 19:04:59.316853 139637196564224 logging_writer.py:48] [1] accumulated_eval_time=36.847095, accumulated_logging_time=0, accumulated_submission_time=56.311223, global_step=1, preemption_count=0, score=56.311223, test/accuracy=0.001200, test/loss=6.910791, test/num_examples=10000, total_duration=93.158437, train/accuracy=0.000996, train/loss=6.911186, validation/accuracy=0.001020, validation/loss=6.910913, validation/num_examples=50000
I0126 19:05:33.373101 139637188171520 logging_writer.py:48] [100] global_step=100, grad_norm=0.5772567987442017, loss=6.899452209472656
I0126 19:06:07.569322 139637196564224 logging_writer.py:48] [200] global_step=200, grad_norm=0.6113818287849426, loss=6.865882873535156
I0126 19:06:41.816716 139637188171520 logging_writer.py:48] [300] global_step=300, grad_norm=0.625784158706665, loss=6.790506362915039
I0126 19:07:16.074388 139637196564224 logging_writer.py:48] [400] global_step=400, grad_norm=0.6774555444717407, loss=6.702958106994629
I0126 19:07:50.335682 139637188171520 logging_writer.py:48] [500] global_step=500, grad_norm=0.7048503756523132, loss=6.610506057739258
I0126 19:08:24.607059 139637196564224 logging_writer.py:48] [600] global_step=600, grad_norm=0.7116144299507141, loss=6.535238742828369
I0126 19:08:58.875787 139637188171520 logging_writer.py:48] [700] global_step=700, grad_norm=0.759596049785614, loss=6.447327613830566
I0126 19:09:33.132525 139637196564224 logging_writer.py:48] [800] global_step=800, grad_norm=0.8377950191497803, loss=6.326313018798828
I0126 19:10:07.399710 139637188171520 logging_writer.py:48] [900] global_step=900, grad_norm=1.352582573890686, loss=6.24613094329834
I0126 19:10:41.720597 139637196564224 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.293196201324463, loss=6.17104434967041
I0126 19:11:15.981931 139637188171520 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.929782748222351, loss=6.088641166687012
I0126 19:11:50.226078 139637196564224 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.395042896270752, loss=5.990081310272217
I0126 19:12:24.516856 139637188171520 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.704973578453064, loss=5.945969581604004
I0126 19:12:58.780971 139637196564224 logging_writer.py:48] [1400] global_step=1400, grad_norm=2.5060513019561768, loss=5.887012958526611
I0126 19:13:29.390793 139822745589568 spec.py:321] Evaluating on the training split.
I0126 19:13:36.667489 139822745589568 spec.py:333] Evaluating on the validation split.
I0126 19:13:44.867412 139822745589568 spec.py:349] Evaluating on the test split.
I0126 19:13:47.181506 139822745589568 submission_runner.py:408] Time since start: 621.04s, 	Step: 1491, 	{'train/accuracy': 0.0701729878783226, 'train/loss': 5.387164115905762, 'validation/accuracy': 0.065420001745224, 'validation/loss': 5.45823860168457, 'validation/num_examples': 50000, 'test/accuracy': 0.04310000315308571, 'test/loss': 5.679708957672119, 'test/num_examples': 10000, 'score': 566.3269417285919, 'total_duration': 621.0424935817719, 'accumulated_submission_time': 566.3269417285919, 'accumulated_eval_time': 54.637348890304565, 'accumulated_logging_time': 0.02869892120361328}
I0126 19:13:47.198437 139637204956928 logging_writer.py:48] [1491] accumulated_eval_time=54.637349, accumulated_logging_time=0.028699, accumulated_submission_time=566.326942, global_step=1491, preemption_count=0, score=566.326942, test/accuracy=0.043100, test/loss=5.679709, test/num_examples=10000, total_duration=621.042494, train/accuracy=0.070173, train/loss=5.387164, validation/accuracy=0.065420, validation/loss=5.458239, validation/num_examples=50000
I0126 19:13:50.664306 139637213349632 logging_writer.py:48] [1500] global_step=1500, grad_norm=2.3757593631744385, loss=5.80648136138916
I0126 19:14:24.943822 139637204956928 logging_writer.py:48] [1600] global_step=1600, grad_norm=3.0248076915740967, loss=5.640132427215576
I0126 19:14:59.205315 139637213349632 logging_writer.py:48] [1700] global_step=1700, grad_norm=2.770111560821533, loss=5.607471466064453
I0126 19:15:33.488073 139637204956928 logging_writer.py:48] [1800] global_step=1800, grad_norm=2.669863224029541, loss=5.614739418029785
I0126 19:16:07.771155 139637213349632 logging_writer.py:48] [1900] global_step=1900, grad_norm=3.0676565170288086, loss=5.4826273918151855
I0126 19:16:42.056050 139637204956928 logging_writer.py:48] [2000] global_step=2000, grad_norm=3.729973554611206, loss=5.5666069984436035
I0126 19:17:16.328987 139637213349632 logging_writer.py:48] [2100] global_step=2100, grad_norm=3.445631980895996, loss=5.451547622680664
I0126 19:17:50.626467 139637204956928 logging_writer.py:48] [2200] global_step=2200, grad_norm=4.438642501831055, loss=5.438160419464111
I0126 19:18:24.920764 139637213349632 logging_writer.py:48] [2300] global_step=2300, grad_norm=4.120720863342285, loss=5.354284286499023
I0126 19:18:59.232069 139637204956928 logging_writer.py:48] [2400] global_step=2400, grad_norm=4.999114990234375, loss=5.330583572387695
I0126 19:19:33.542395 139637213349632 logging_writer.py:48] [2500] global_step=2500, grad_norm=4.61124849319458, loss=5.230302333831787
I0126 19:20:07.850486 139637204956928 logging_writer.py:48] [2600] global_step=2600, grad_norm=4.429361343383789, loss=5.205065727233887
I0126 19:20:42.130979 139637213349632 logging_writer.py:48] [2700] global_step=2700, grad_norm=4.988426685333252, loss=5.1666059494018555
I0126 19:21:16.420979 139637204956928 logging_writer.py:48] [2800] global_step=2800, grad_norm=3.5241105556488037, loss=5.0879740715026855
I0126 19:21:50.702595 139637213349632 logging_writer.py:48] [2900] global_step=2900, grad_norm=3.997931480407715, loss=5.1210103034973145
I0126 19:22:17.205853 139822745589568 spec.py:321] Evaluating on the training split.
I0126 19:22:24.427486 139822745589568 spec.py:333] Evaluating on the validation split.
I0126 19:22:32.895564 139822745589568 spec.py:349] Evaluating on the test split.
I0126 19:22:35.205651 139822745589568 submission_runner.py:408] Time since start: 1149.07s, 	Step: 2979, 	{'train/accuracy': 0.17562180757522583, 'train/loss': 4.2726922035217285, 'validation/accuracy': 0.15737999975681305, 'validation/loss': 4.3756303787231445, 'validation/num_examples': 50000, 'test/accuracy': 0.11840000748634338, 'test/loss': 4.798498630523682, 'test/num_examples': 10000, 'score': 1076.2741117477417, 'total_duration': 1149.0670676231384, 'accumulated_submission_time': 1076.2741117477417, 'accumulated_eval_time': 72.63711643218994, 'accumulated_logging_time': 0.05626201629638672}
I0126 19:22:35.221534 139657354401536 logging_writer.py:48] [2979] accumulated_eval_time=72.637116, accumulated_logging_time=0.056262, accumulated_submission_time=1076.274112, global_step=2979, preemption_count=0, score=1076.274112, test/accuracy=0.118400, test/loss=4.798499, test/num_examples=10000, total_duration=1149.067068, train/accuracy=0.175622, train/loss=4.272692, validation/accuracy=0.157380, validation/loss=4.375630, validation/num_examples=50000
I0126 19:22:42.867589 139657362794240 logging_writer.py:48] [3000] global_step=3000, grad_norm=3.831789255142212, loss=5.012396335601807
I0126 19:23:17.111649 139657354401536 logging_writer.py:48] [3100] global_step=3100, grad_norm=4.3626580238342285, loss=4.919944763183594
I0126 19:23:51.365423 139657362794240 logging_writer.py:48] [3200] global_step=3200, grad_norm=4.951066017150879, loss=4.988751411437988
I0126 19:24:25.630287 139657354401536 logging_writer.py:48] [3300] global_step=3300, grad_norm=6.482789993286133, loss=4.852258682250977
I0126 19:24:59.896784 139657362794240 logging_writer.py:48] [3400] global_step=3400, grad_norm=4.8726582527160645, loss=4.822033882141113
I0126 19:25:34.158175 139657354401536 logging_writer.py:48] [3500] global_step=3500, grad_norm=3.394551992416382, loss=4.80084228515625
I0126 19:26:08.448700 139657362794240 logging_writer.py:48] [3600] global_step=3600, grad_norm=6.8550639152526855, loss=4.811044216156006
I0126 19:26:42.716606 139657354401536 logging_writer.py:48] [3700] global_step=3700, grad_norm=4.431732654571533, loss=4.686102867126465
I0126 19:27:16.998449 139657362794240 logging_writer.py:48] [3800] global_step=3800, grad_norm=4.863382339477539, loss=4.6667799949646
I0126 19:27:51.290039 139657354401536 logging_writer.py:48] [3900] global_step=3900, grad_norm=5.929996967315674, loss=4.658666610717773
I0126 19:28:25.552947 139657362794240 logging_writer.py:48] [4000] global_step=4000, grad_norm=3.9148645401000977, loss=4.7302069664001465
I0126 19:28:59.860349 139657354401536 logging_writer.py:48] [4100] global_step=4100, grad_norm=6.791965484619141, loss=4.5623369216918945
I0126 19:29:34.135489 139657362794240 logging_writer.py:48] [4200] global_step=4200, grad_norm=3.26137638092041, loss=4.599891185760498
I0126 19:30:08.427856 139657354401536 logging_writer.py:48] [4300] global_step=4300, grad_norm=4.483529090881348, loss=4.475386142730713
I0126 19:30:42.709846 139657362794240 logging_writer.py:48] [4400] global_step=4400, grad_norm=9.046587944030762, loss=4.3758039474487305
I0126 19:31:05.432142 139822745589568 spec.py:321] Evaluating on the training split.
I0126 19:31:12.836415 139822745589568 spec.py:333] Evaluating on the validation split.
I0126 19:31:21.176312 139822745589568 spec.py:349] Evaluating on the test split.
I0126 19:31:23.493890 139822745589568 submission_runner.py:408] Time since start: 1677.36s, 	Step: 4468, 	{'train/accuracy': 0.2839604616165161, 'train/loss': 3.509944200515747, 'validation/accuracy': 0.2582399845123291, 'validation/loss': 3.6567907333374023, 'validation/num_examples': 50000, 'test/accuracy': 0.19550001621246338, 'test/loss': 4.1657586097717285, 'test/num_examples': 10000, 'score': 1586.424509525299, 'total_duration': 1677.3552947044373, 'accumulated_submission_time': 1586.424509525299, 'accumulated_eval_time': 90.69883179664612, 'accumulated_logging_time': 0.08253097534179688}
I0126 19:31:23.510566 139657354401536 logging_writer.py:48] [4468] accumulated_eval_time=90.698832, accumulated_logging_time=0.082531, accumulated_submission_time=1586.424510, global_step=4468, preemption_count=0, score=1586.424510, test/accuracy=0.195500, test/loss=4.165759, test/num_examples=10000, total_duration=1677.355295, train/accuracy=0.283960, train/loss=3.509944, validation/accuracy=0.258240, validation/loss=3.656791, validation/num_examples=50000
I0126 19:31:34.816281 139657362794240 logging_writer.py:48] [4500] global_step=4500, grad_norm=4.270728588104248, loss=4.4645094871521
I0126 19:32:09.030235 139657354401536 logging_writer.py:48] [4600] global_step=4600, grad_norm=8.197526931762695, loss=4.384858131408691
I0126 19:32:43.268302 139657362794240 logging_writer.py:48] [4700] global_step=4700, grad_norm=4.137369632720947, loss=4.26632833480835
I0126 19:33:17.510554 139657354401536 logging_writer.py:48] [4800] global_step=4800, grad_norm=4.4427361488342285, loss=4.48989200592041
I0126 19:33:51.756039 139657362794240 logging_writer.py:48] [4900] global_step=4900, grad_norm=4.5336174964904785, loss=4.247706413269043
I0126 19:34:26.014866 139657354401536 logging_writer.py:48] [5000] global_step=5000, grad_norm=4.612009525299072, loss=4.266610622406006
I0126 19:35:00.272103 139657362794240 logging_writer.py:48] [5100] global_step=5100, grad_norm=8.081154823303223, loss=4.142897129058838
I0126 19:35:34.617374 139657354401536 logging_writer.py:48] [5200] global_step=5200, grad_norm=6.059261798858643, loss=4.134467124938965
I0126 19:36:08.864434 139657362794240 logging_writer.py:48] [5300] global_step=5300, grad_norm=3.2196877002716064, loss=4.120340824127197
I0126 19:36:43.103160 139657354401536 logging_writer.py:48] [5400] global_step=5400, grad_norm=4.821855068206787, loss=4.091835021972656
I0126 19:37:17.371177 139657362794240 logging_writer.py:48] [5500] global_step=5500, grad_norm=5.791485786437988, loss=4.06736946105957
I0126 19:37:51.637930 139657354401536 logging_writer.py:48] [5600] global_step=5600, grad_norm=10.132582664489746, loss=4.1694655418396
I0126 19:38:25.933557 139657362794240 logging_writer.py:48] [5700] global_step=5700, grad_norm=7.987112998962402, loss=3.9863884449005127
I0126 19:39:00.215423 139657354401536 logging_writer.py:48] [5800] global_step=5800, grad_norm=5.142765522003174, loss=4.015437602996826
I0126 19:39:34.484897 139657362794240 logging_writer.py:48] [5900] global_step=5900, grad_norm=5.198458671569824, loss=4.100183963775635
I0126 19:39:53.780463 139822745589568 spec.py:321] Evaluating on the training split.
I0126 19:40:01.124026 139822745589568 spec.py:333] Evaluating on the validation split.
I0126 19:40:09.548056 139822745589568 spec.py:349] Evaluating on the test split.
I0126 19:40:11.855609 139822745589568 submission_runner.py:408] Time since start: 2205.72s, 	Step: 5958, 	{'train/accuracy': 0.37035635113716125, 'train/loss': 2.9588210582733154, 'validation/accuracy': 0.34237998723983765, 'validation/loss': 3.106684684753418, 'validation/num_examples': 50000, 'test/accuracy': 0.26080000400543213, 'test/loss': 3.706625461578369, 'test/num_examples': 10000, 'score': 2096.635529756546, 'total_duration': 2205.71702504158, 'accumulated_submission_time': 2096.635529756546, 'accumulated_eval_time': 108.77395868301392, 'accumulated_logging_time': 0.10939264297485352}
I0126 19:40:11.872917 139657455048448 logging_writer.py:48] [5958] accumulated_eval_time=108.773959, accumulated_logging_time=0.109393, accumulated_submission_time=2096.635530, global_step=5958, preemption_count=0, score=2096.635530, test/accuracy=0.260800, test/loss=3.706625, test/num_examples=10000, total_duration=2205.717025, train/accuracy=0.370356, train/loss=2.958821, validation/accuracy=0.342380, validation/loss=3.106685, validation/num_examples=50000
I0126 19:40:26.601448 139657471833856 logging_writer.py:48] [6000] global_step=6000, grad_norm=6.555472373962402, loss=4.005924224853516
I0126 19:41:00.834094 139657455048448 logging_writer.py:48] [6100] global_step=6100, grad_norm=3.696709156036377, loss=3.9733691215515137
I0126 19:41:35.217038 139657471833856 logging_writer.py:48] [6200] global_step=6200, grad_norm=8.859808921813965, loss=3.9406189918518066
I0126 19:42:09.494932 139657455048448 logging_writer.py:48] [6300] global_step=6300, grad_norm=5.924485683441162, loss=3.8748068809509277
I0126 19:42:43.738768 139657471833856 logging_writer.py:48] [6400] global_step=6400, grad_norm=6.574971675872803, loss=3.9508349895477295
I0126 19:43:18.015424 139657455048448 logging_writer.py:48] [6500] global_step=6500, grad_norm=4.736406326293945, loss=3.8130717277526855
I0126 19:43:52.278683 139657471833856 logging_writer.py:48] [6600] global_step=6600, grad_norm=6.053013324737549, loss=3.8931727409362793
I0126 19:44:26.548506 139657455048448 logging_writer.py:48] [6700] global_step=6700, grad_norm=8.603790283203125, loss=3.8447861671447754
I0126 19:45:00.814331 139657471833856 logging_writer.py:48] [6800] global_step=6800, grad_norm=8.308513641357422, loss=3.731717109680176
I0126 19:45:35.079168 139657455048448 logging_writer.py:48] [6900] global_step=6900, grad_norm=4.939874172210693, loss=3.7732720375061035
I0126 19:46:09.367455 139657471833856 logging_writer.py:48] [7000] global_step=7000, grad_norm=7.132235527038574, loss=3.716707229614258
I0126 19:46:43.643320 139657455048448 logging_writer.py:48] [7100] global_step=7100, grad_norm=3.89044189453125, loss=3.7151241302490234
I0126 19:47:17.924777 139657471833856 logging_writer.py:48] [7200] global_step=7200, grad_norm=4.2491068840026855, loss=3.7288947105407715
I0126 19:47:52.255092 139657455048448 logging_writer.py:48] [7300] global_step=7300, grad_norm=5.357142448425293, loss=3.727902889251709
I0126 19:48:26.541364 139657471833856 logging_writer.py:48] [7400] global_step=7400, grad_norm=3.671941041946411, loss=3.6264710426330566
I0126 19:48:42.061899 139822745589568 spec.py:321] Evaluating on the training split.
I0126 19:48:49.280697 139822745589568 spec.py:333] Evaluating on the validation split.
I0126 19:48:57.823806 139822745589568 spec.py:349] Evaluating on the test split.
I0126 19:49:00.102156 139822745589568 submission_runner.py:408] Time since start: 2733.96s, 	Step: 7447, 	{'train/accuracy': 0.437519907951355, 'train/loss': 2.5694799423217773, 'validation/accuracy': 0.407260000705719, 'validation/loss': 2.7285168170928955, 'validation/num_examples': 50000, 'test/accuracy': 0.314300000667572, 'test/loss': 3.3685951232910156, 'test/num_examples': 10000, 'score': 2606.764097929001, 'total_duration': 2733.9635617733, 'accumulated_submission_time': 2606.764097929001, 'accumulated_eval_time': 126.81416702270508, 'accumulated_logging_time': 0.13728642463684082}
I0126 19:49:00.119625 139659954833152 logging_writer.py:48] [7447] accumulated_eval_time=126.814167, accumulated_logging_time=0.137286, accumulated_submission_time=2606.764098, global_step=7447, preemption_count=0, score=2606.764098, test/accuracy=0.314300, test/loss=3.368595, test/num_examples=10000, total_duration=2733.963562, train/accuracy=0.437520, train/loss=2.569480, validation/accuracy=0.407260, validation/loss=2.728517, validation/num_examples=50000
I0126 19:49:18.609525 139659963225856 logging_writer.py:48] [7500] global_step=7500, grad_norm=4.91544771194458, loss=3.681046485900879
I0126 19:49:52.820839 139659954833152 logging_writer.py:48] [7600] global_step=7600, grad_norm=7.34246826171875, loss=3.740280866622925
I0126 19:50:27.078397 139659963225856 logging_writer.py:48] [7700] global_step=7700, grad_norm=4.483015537261963, loss=3.733088731765747
I0126 19:51:01.360326 139659954833152 logging_writer.py:48] [7800] global_step=7800, grad_norm=6.156131267547607, loss=3.5267584323883057
I0126 19:51:35.631367 139659963225856 logging_writer.py:48] [7900] global_step=7900, grad_norm=6.3159613609313965, loss=3.684309720993042
I0126 19:52:09.868655 139659954833152 logging_writer.py:48] [8000] global_step=8000, grad_norm=4.924932956695557, loss=3.6138246059417725
I0126 19:52:44.132908 139659963225856 logging_writer.py:48] [8100] global_step=8100, grad_norm=4.100943088531494, loss=3.5411882400512695
I0126 19:53:18.421031 139659954833152 logging_writer.py:48] [8200] global_step=8200, grad_norm=6.226868152618408, loss=3.608050584793091
I0126 19:53:52.710372 139659963225856 logging_writer.py:48] [8300] global_step=8300, grad_norm=7.3953022956848145, loss=3.5445573329925537
I0126 19:54:26.953629 139659954833152 logging_writer.py:48] [8400] global_step=8400, grad_norm=4.865201473236084, loss=3.46103835105896
I0126 19:55:01.203031 139659963225856 logging_writer.py:48] [8500] global_step=8500, grad_norm=5.623788356781006, loss=3.5927982330322266
I0126 19:55:35.430860 139659954833152 logging_writer.py:48] [8600] global_step=8600, grad_norm=6.063503742218018, loss=3.5028271675109863
I0126 19:56:09.682566 139659963225856 logging_writer.py:48] [8700] global_step=8700, grad_norm=4.759899616241455, loss=3.393589496612549
I0126 19:56:43.915763 139659954833152 logging_writer.py:48] [8800] global_step=8800, grad_norm=4.820744037628174, loss=3.5024118423461914
I0126 19:57:18.124025 139659963225856 logging_writer.py:48] [8900] global_step=8900, grad_norm=4.640816688537598, loss=3.4457085132598877
I0126 19:57:30.180521 139822745589568 spec.py:321] Evaluating on the training split.
I0126 19:57:37.599904 139822745589568 spec.py:333] Evaluating on the validation split.
I0126 19:57:45.996183 139822745589568 spec.py:349] Evaluating on the test split.
I0126 19:57:48.277336 139822745589568 submission_runner.py:408] Time since start: 3262.14s, 	Step: 8937, 	{'train/accuracy': 0.49557557702064514, 'train/loss': 2.2581822872161865, 'validation/accuracy': 0.46515998244285583, 'validation/loss': 2.4037249088287354, 'validation/num_examples': 50000, 'test/accuracy': 0.3574000298976898, 'test/loss': 3.0720314979553223, 'test/num_examples': 10000, 'score': 3116.7672028541565, 'total_duration': 3262.1387367248535, 'accumulated_submission_time': 3116.7672028541565, 'accumulated_eval_time': 144.91092801094055, 'accumulated_logging_time': 0.16506457328796387}
I0126 19:57:48.297631 139659837429504 logging_writer.py:48] [8937] accumulated_eval_time=144.910928, accumulated_logging_time=0.165065, accumulated_submission_time=3116.767203, global_step=8937, preemption_count=0, score=3116.767203, test/accuracy=0.357400, test/loss=3.072031, test/num_examples=10000, total_duration=3262.138737, train/accuracy=0.495576, train/loss=2.258182, validation/accuracy=0.465160, validation/loss=2.403725, validation/num_examples=50000
I0126 19:58:10.157904 139659845822208 logging_writer.py:48] [9000] global_step=9000, grad_norm=4.50288200378418, loss=3.4859390258789062
I0126 19:58:44.332271 139659837429504 logging_writer.py:48] [9100] global_step=9100, grad_norm=4.2429728507995605, loss=3.4087109565734863
I0126 19:59:18.527845 139659845822208 logging_writer.py:48] [9200] global_step=9200, grad_norm=5.031585216522217, loss=3.4421119689941406
I0126 19:59:52.764587 139659837429504 logging_writer.py:48] [9300] global_step=9300, grad_norm=5.146710395812988, loss=3.401054620742798
I0126 20:00:27.058667 139659845822208 logging_writer.py:48] [9400] global_step=9400, grad_norm=4.777846813201904, loss=3.3029255867004395
I0126 20:01:01.277187 139659837429504 logging_writer.py:48] [9500] global_step=9500, grad_norm=5.258566379547119, loss=3.442873001098633
I0126 20:01:35.529276 139659845822208 logging_writer.py:48] [9600] global_step=9600, grad_norm=6.7564287185668945, loss=3.3723440170288086
I0126 20:02:09.793925 139659837429504 logging_writer.py:48] [9700] global_step=9700, grad_norm=3.6445183753967285, loss=3.3211004734039307
I0126 20:02:44.034823 139659845822208 logging_writer.py:48] [9800] global_step=9800, grad_norm=4.57077169418335, loss=3.3833916187286377
I0126 20:03:18.280767 139659837429504 logging_writer.py:48] [9900] global_step=9900, grad_norm=4.605046272277832, loss=3.389096975326538
I0126 20:03:52.520132 139659845822208 logging_writer.py:48] [10000] global_step=10000, grad_norm=6.50069522857666, loss=3.416236400604248
I0126 20:04:26.800840 139659837429504 logging_writer.py:48] [10100] global_step=10100, grad_norm=3.5985121726989746, loss=3.3524703979492188
I0126 20:05:01.009385 139659845822208 logging_writer.py:48] [10200] global_step=10200, grad_norm=6.76072359085083, loss=3.320722818374634
I0126 20:05:35.202021 139659837429504 logging_writer.py:48] [10300] global_step=10300, grad_norm=5.937306880950928, loss=3.2602453231811523
I0126 20:06:09.552684 139659845822208 logging_writer.py:48] [10400] global_step=10400, grad_norm=5.093777656555176, loss=3.331470012664795
I0126 20:06:18.573827 139822745589568 spec.py:321] Evaluating on the training split.
I0126 20:06:26.012567 139822745589568 spec.py:333] Evaluating on the validation split.
I0126 20:06:36.021314 139822745589568 spec.py:349] Evaluating on the test split.
I0126 20:06:38.317160 139822745589568 submission_runner.py:408] Time since start: 3792.18s, 	Step: 10428, 	{'train/accuracy': 0.5627790093421936, 'train/loss': 1.9515559673309326, 'validation/accuracy': 0.4867599904537201, 'validation/loss': 2.320361614227295, 'validation/num_examples': 50000, 'test/accuracy': 0.37290000915527344, 'test/loss': 2.9771018028259277, 'test/num_examples': 10000, 'score': 3626.985067844391, 'total_duration': 3792.1785800457, 'accumulated_submission_time': 3626.985067844391, 'accumulated_eval_time': 164.654226064682, 'accumulated_logging_time': 0.1945652961730957}
I0126 20:06:38.339060 139659820644096 logging_writer.py:48] [10428] accumulated_eval_time=164.654226, accumulated_logging_time=0.194565, accumulated_submission_time=3626.985068, global_step=10428, preemption_count=0, score=3626.985068, test/accuracy=0.372900, test/loss=2.977102, test/num_examples=10000, total_duration=3792.178580, train/accuracy=0.562779, train/loss=1.951556, validation/accuracy=0.486760, validation/loss=2.320362, validation/num_examples=50000
I0126 20:07:03.282851 139659829036800 logging_writer.py:48] [10500] global_step=10500, grad_norm=4.167823314666748, loss=3.266547441482544
I0126 20:07:37.429849 139659820644096 logging_writer.py:48] [10600] global_step=10600, grad_norm=6.493260860443115, loss=3.2156219482421875
I0126 20:08:11.598130 139659829036800 logging_writer.py:48] [10700] global_step=10700, grad_norm=8.52490234375, loss=3.2133259773254395
I0126 20:08:45.786797 139659820644096 logging_writer.py:48] [10800] global_step=10800, grad_norm=5.009561538696289, loss=3.305443286895752
I0126 20:09:20.014045 139659829036800 logging_writer.py:48] [10900] global_step=10900, grad_norm=4.3906121253967285, loss=3.15867018699646
I0126 20:09:54.195291 139659820644096 logging_writer.py:48] [11000] global_step=11000, grad_norm=4.763702392578125, loss=3.1563665866851807
I0126 20:10:28.401960 139659829036800 logging_writer.py:48] [11100] global_step=11100, grad_norm=7.332118511199951, loss=3.329965829849243
I0126 20:11:02.611158 139659820644096 logging_writer.py:48] [11200] global_step=11200, grad_norm=5.129617214202881, loss=3.293384552001953
I0126 20:11:36.818509 139659829036800 logging_writer.py:48] [11300] global_step=11300, grad_norm=6.028299808502197, loss=3.2185726165771484
I0126 20:12:11.044220 139659820644096 logging_writer.py:48] [11400] global_step=11400, grad_norm=5.914187431335449, loss=3.185037136077881
I0126 20:12:45.336452 139659829036800 logging_writer.py:48] [11500] global_step=11500, grad_norm=7.655546188354492, loss=3.2082512378692627
I0126 20:13:19.559121 139659820644096 logging_writer.py:48] [11600] global_step=11600, grad_norm=6.708667755126953, loss=3.195308208465576
I0126 20:13:53.787781 139659829036800 logging_writer.py:48] [11700] global_step=11700, grad_norm=5.583789348602295, loss=3.1415820121765137
I0126 20:14:28.009510 139659820644096 logging_writer.py:48] [11800] global_step=11800, grad_norm=5.712433815002441, loss=3.1835312843322754
I0126 20:15:02.202547 139659829036800 logging_writer.py:48] [11900] global_step=11900, grad_norm=6.001020908355713, loss=3.148294448852539
I0126 20:15:08.457413 139822745589568 spec.py:321] Evaluating on the training split.
I0126 20:15:15.857411 139822745589568 spec.py:333] Evaluating on the validation split.
I0126 20:15:29.307966 139822745589568 spec.py:349] Evaluating on the test split.
I0126 20:15:31.560285 139822745589568 submission_runner.py:408] Time since start: 4325.42s, 	Step: 11920, 	{'train/accuracy': 0.5782246589660645, 'train/loss': 1.846057415008545, 'validation/accuracy': 0.52947998046875, 'validation/loss': 2.106097936630249, 'validation/num_examples': 50000, 'test/accuracy': 0.4123000204563141, 'test/loss': 2.767740249633789, 'test/num_examples': 10000, 'score': 4137.04333615303, 'total_duration': 4325.421687364578, 'accumulated_submission_time': 4137.04333615303, 'accumulated_eval_time': 187.7570457458496, 'accumulated_logging_time': 0.22736477851867676}
I0126 20:15:31.578166 139659820644096 logging_writer.py:48] [11920] accumulated_eval_time=187.757046, accumulated_logging_time=0.227365, accumulated_submission_time=4137.043336, global_step=11920, preemption_count=0, score=4137.043336, test/accuracy=0.412300, test/loss=2.767740, test/num_examples=10000, total_duration=4325.421687, train/accuracy=0.578225, train/loss=1.846057, validation/accuracy=0.529480, validation/loss=2.106098, validation/num_examples=50000
I0126 20:15:59.240907 139659845822208 logging_writer.py:48] [12000] global_step=12000, grad_norm=6.381893157958984, loss=3.182943820953369
I0126 20:16:33.392143 139659820644096 logging_writer.py:48] [12100] global_step=12100, grad_norm=6.389663219451904, loss=3.220639228820801
I0126 20:17:07.588425 139659845822208 logging_writer.py:48] [12200] global_step=12200, grad_norm=4.453591346740723, loss=3.1947760581970215
I0126 20:17:41.774788 139659820644096 logging_writer.py:48] [12300] global_step=12300, grad_norm=4.868540287017822, loss=3.076131820678711
I0126 20:18:15.973700 139659845822208 logging_writer.py:48] [12400] global_step=12400, grad_norm=4.916029930114746, loss=3.1708712577819824
I0126 20:18:50.201594 139659820644096 logging_writer.py:48] [12500] global_step=12500, grad_norm=5.517735481262207, loss=3.1819570064544678
I0126 20:19:24.408626 139659845822208 logging_writer.py:48] [12600] global_step=12600, grad_norm=3.6453394889831543, loss=3.232417106628418
I0126 20:19:58.568595 139659820644096 logging_writer.py:48] [12700] global_step=12700, grad_norm=3.6493561267852783, loss=3.1098239421844482
I0126 20:20:32.735326 139659845822208 logging_writer.py:48] [12800] global_step=12800, grad_norm=5.706775188446045, loss=3.1393632888793945
I0126 20:21:06.898646 139659820644096 logging_writer.py:48] [12900] global_step=12900, grad_norm=7.533092498779297, loss=3.181645631790161
I0126 20:21:41.100521 139659845822208 logging_writer.py:48] [13000] global_step=13000, grad_norm=4.946208953857422, loss=3.1071839332580566
I0126 20:22:15.256862 139659820644096 logging_writer.py:48] [13100] global_step=13100, grad_norm=5.343477249145508, loss=3.164422035217285
I0126 20:22:49.429663 139659845822208 logging_writer.py:48] [13200] global_step=13200, grad_norm=4.13609504699707, loss=3.121568202972412
I0126 20:23:23.584830 139659820644096 logging_writer.py:48] [13300] global_step=13300, grad_norm=2.5819101333618164, loss=3.093426465988159
I0126 20:23:57.755815 139659845822208 logging_writer.py:48] [13400] global_step=13400, grad_norm=5.2800140380859375, loss=3.2263426780700684
I0126 20:24:01.619348 139822745589568 spec.py:321] Evaluating on the training split.
I0126 20:24:09.233510 139822745589568 spec.py:333] Evaluating on the validation split.
I0126 20:24:20.874409 139822745589568 spec.py:349] Evaluating on the test split.
I0126 20:24:23.207060 139822745589568 submission_runner.py:408] Time since start: 4857.07s, 	Step: 13413, 	{'train/accuracy': 0.5894252061843872, 'train/loss': 1.8045969009399414, 'validation/accuracy': 0.5416799783706665, 'validation/loss': 2.0362939834594727, 'validation/num_examples': 50000, 'test/accuracy': 0.4132000207901001, 'test/loss': 2.7203760147094727, 'test/num_examples': 10000, 'score': 4647.025430679321, 'total_duration': 4857.068476676941, 'accumulated_submission_time': 4647.025430679321, 'accumulated_eval_time': 209.34475588798523, 'accumulated_logging_time': 0.25449466705322266}
I0126 20:24:23.225175 139659946440448 logging_writer.py:48] [13413] accumulated_eval_time=209.344756, accumulated_logging_time=0.254495, accumulated_submission_time=4647.025431, global_step=13413, preemption_count=0, score=4647.025431, test/accuracy=0.413200, test/loss=2.720376, test/num_examples=10000, total_duration=4857.068477, train/accuracy=0.589425, train/loss=1.804597, validation/accuracy=0.541680, validation/loss=2.036294, validation/num_examples=50000
I0126 20:24:53.247660 139659954833152 logging_writer.py:48] [13500] global_step=13500, grad_norm=6.340548992156982, loss=3.1953868865966797
I0126 20:25:27.385938 139659946440448 logging_writer.py:48] [13600] global_step=13600, grad_norm=5.540302276611328, loss=3.1057167053222656
I0126 20:26:01.547132 139659954833152 logging_writer.py:48] [13700] global_step=13700, grad_norm=4.409440994262695, loss=2.9903461933135986
I0126 20:26:35.706458 139659946440448 logging_writer.py:48] [13800] global_step=13800, grad_norm=6.418600082397461, loss=3.141829013824463
I0126 20:27:09.892075 139659954833152 logging_writer.py:48] [13900] global_step=13900, grad_norm=7.01458215713501, loss=3.054094076156616
I0126 20:27:44.094612 139659946440448 logging_writer.py:48] [14000] global_step=14000, grad_norm=4.757302284240723, loss=3.2167093753814697
I0126 20:28:18.279853 139659954833152 logging_writer.py:48] [14100] global_step=14100, grad_norm=4.13300085067749, loss=3.08553409576416
I0126 20:28:52.451351 139659946440448 logging_writer.py:48] [14200] global_step=14200, grad_norm=4.596252918243408, loss=3.021465539932251
I0126 20:29:26.603576 139659954833152 logging_writer.py:48] [14300] global_step=14300, grad_norm=4.581105709075928, loss=3.083350658416748
I0126 20:30:00.745742 139659946440448 logging_writer.py:48] [14400] global_step=14400, grad_norm=7.843330383300781, loss=3.0923004150390625
I0126 20:30:34.909129 139659954833152 logging_writer.py:48] [14500] global_step=14500, grad_norm=5.114778995513916, loss=3.0809762477874756
I0126 20:31:09.079502 139659946440448 logging_writer.py:48] [14600] global_step=14600, grad_norm=6.177337169647217, loss=3.0728232860565186
I0126 20:31:43.263886 139659954833152 logging_writer.py:48] [14700] global_step=14700, grad_norm=6.43451452255249, loss=3.139662265777588
I0126 20:32:17.432142 139659946440448 logging_writer.py:48] [14800] global_step=14800, grad_norm=5.089831829071045, loss=3.0474350452423096
I0126 20:32:51.602383 139659954833152 logging_writer.py:48] [14900] global_step=14900, grad_norm=5.43201208114624, loss=2.9903690814971924
I0126 20:32:53.407919 139822745589568 spec.py:321] Evaluating on the training split.
I0126 20:33:01.468144 139822745589568 spec.py:333] Evaluating on the validation split.
I0126 20:33:13.497593 139822745589568 spec.py:349] Evaluating on the test split.
I0126 20:33:15.713575 139822745589568 submission_runner.py:408] Time since start: 5389.57s, 	Step: 14907, 	{'train/accuracy': 0.5977359414100647, 'train/loss': 1.8043289184570312, 'validation/accuracy': 0.5502399802207947, 'validation/loss': 2.0121490955352783, 'validation/num_examples': 50000, 'test/accuracy': 0.428600013256073, 'test/loss': 2.705935001373291, 'test/num_examples': 10000, 'score': 5157.151046037674, 'total_duration': 5389.574882030487, 'accumulated_submission_time': 5157.151046037674, 'accumulated_eval_time': 231.6502606868744, 'accumulated_logging_time': 0.28139376640319824}
I0126 20:33:15.750856 139659845822208 logging_writer.py:48] [14907] accumulated_eval_time=231.650261, accumulated_logging_time=0.281394, accumulated_submission_time=5157.151046, global_step=14907, preemption_count=0, score=5157.151046, test/accuracy=0.428600, test/loss=2.705935, test/num_examples=10000, total_duration=5389.574882, train/accuracy=0.597736, train/loss=1.804329, validation/accuracy=0.550240, validation/loss=2.012149, validation/num_examples=50000
I0126 20:33:47.901667 139659854214912 logging_writer.py:48] [15000] global_step=15000, grad_norm=6.613935470581055, loss=2.983919143676758
I0126 20:34:22.054561 139659845822208 logging_writer.py:48] [15100] global_step=15100, grad_norm=4.360640048980713, loss=3.076157569885254
I0126 20:34:56.205000 139659854214912 logging_writer.py:48] [15200] global_step=15200, grad_norm=3.869245767593384, loss=3.0746641159057617
I0126 20:35:30.395191 139659845822208 logging_writer.py:48] [15300] global_step=15300, grad_norm=5.6894917488098145, loss=3.045903444290161
I0126 20:36:04.587428 139659854214912 logging_writer.py:48] [15400] global_step=15400, grad_norm=6.758702278137207, loss=3.082629442214966
I0126 20:36:38.761927 139659845822208 logging_writer.py:48] [15500] global_step=15500, grad_norm=7.6156005859375, loss=3.022310733795166
I0126 20:37:12.939096 139659854214912 logging_writer.py:48] [15600] global_step=15600, grad_norm=4.674737930297852, loss=3.197625160217285
I0126 20:37:47.139238 139659845822208 logging_writer.py:48] [15700] global_step=15700, grad_norm=4.922648906707764, loss=3.155409574508667
I0126 20:38:21.345525 139659854214912 logging_writer.py:48] [15800] global_step=15800, grad_norm=3.868298292160034, loss=3.0097179412841797
I0126 20:38:55.522339 139659845822208 logging_writer.py:48] [15900] global_step=15900, grad_norm=5.148065567016602, loss=3.014676570892334
I0126 20:39:29.691810 139659854214912 logging_writer.py:48] [16000] global_step=16000, grad_norm=9.116355895996094, loss=3.1389763355255127
I0126 20:40:03.851316 139659845822208 logging_writer.py:48] [16100] global_step=16100, grad_norm=5.713602066040039, loss=2.976804494857788
I0126 20:40:38.056543 139659854214912 logging_writer.py:48] [16200] global_step=16200, grad_norm=4.851357936859131, loss=3.023141622543335
I0126 20:41:12.235157 139659845822208 logging_writer.py:48] [16300] global_step=16300, grad_norm=3.775148630142212, loss=2.9075160026550293
I0126 20:41:45.807510 139822745589568 spec.py:321] Evaluating on the training split.
I0126 20:41:53.274981 139822745589568 spec.py:333] Evaluating on the validation split.
I0126 20:42:04.537421 139822745589568 spec.py:349] Evaluating on the test split.
I0126 20:42:06.778688 139822745589568 submission_runner.py:408] Time since start: 5920.64s, 	Step: 16400, 	{'train/accuracy': 0.5902822017669678, 'train/loss': 1.8064643144607544, 'validation/accuracy': 0.5517199635505676, 'validation/loss': 1.9904471635818481, 'validation/num_examples': 50000, 'test/accuracy': 0.43080002069473267, 'test/loss': 2.7112877368927, 'test/num_examples': 10000, 'score': 5667.149292945862, 'total_duration': 5920.640057086945, 'accumulated_submission_time': 5667.149292945862, 'accumulated_eval_time': 252.6213595867157, 'accumulated_logging_time': 0.3276073932647705}
I0126 20:42:06.799123 139659820644096 logging_writer.py:48] [16400] accumulated_eval_time=252.621360, accumulated_logging_time=0.327607, accumulated_submission_time=5667.149293, global_step=16400, preemption_count=0, score=5667.149293, test/accuracy=0.430800, test/loss=2.711288, test/num_examples=10000, total_duration=5920.640057, train/accuracy=0.590282, train/loss=1.806464, validation/accuracy=0.551720, validation/loss=1.990447, validation/num_examples=50000
I0126 20:42:07.149886 139659829036800 logging_writer.py:48] [16400] global_step=16400, grad_norm=3.455562114715576, loss=3.006168842315674
I0126 20:42:41.234596 139659820644096 logging_writer.py:48] [16500] global_step=16500, grad_norm=4.608307361602783, loss=2.961912155151367
I0126 20:43:15.369190 139659829036800 logging_writer.py:48] [16600] global_step=16600, grad_norm=4.608582496643066, loss=2.971184015274048
I0126 20:43:49.508394 139659820644096 logging_writer.py:48] [16700] global_step=16700, grad_norm=5.225165367126465, loss=3.0735530853271484
I0126 20:44:23.645343 139659829036800 logging_writer.py:48] [16800] global_step=16800, grad_norm=3.6180291175842285, loss=2.990039348602295
I0126 20:44:57.777070 139659820644096 logging_writer.py:48] [16900] global_step=16900, grad_norm=4.574426174163818, loss=2.9801549911499023
I0126 20:45:31.915080 139659829036800 logging_writer.py:48] [17000] global_step=17000, grad_norm=4.785045623779297, loss=3.0246763229370117
I0126 20:46:06.076488 139659820644096 logging_writer.py:48] [17100] global_step=17100, grad_norm=2.468384265899658, loss=3.0000829696655273
I0126 20:46:40.228243 139659829036800 logging_writer.py:48] [17200] global_step=17200, grad_norm=5.929401874542236, loss=2.9868061542510986
I0126 20:47:14.382504 139659820644096 logging_writer.py:48] [17300] global_step=17300, grad_norm=3.8814361095428467, loss=2.923262119293213
I0126 20:47:48.539341 139659829036800 logging_writer.py:48] [17400] global_step=17400, grad_norm=3.9210643768310547, loss=2.9689791202545166
I0126 20:48:22.664367 139659820644096 logging_writer.py:48] [17500] global_step=17500, grad_norm=3.474013328552246, loss=3.0550363063812256
I0126 20:48:56.814403 139659829036800 logging_writer.py:48] [17600] global_step=17600, grad_norm=4.339324951171875, loss=3.0171375274658203
I0126 20:49:30.992199 139659820644096 logging_writer.py:48] [17700] global_step=17700, grad_norm=4.4251708984375, loss=3.1029820442199707
I0126 20:50:05.194304 139659829036800 logging_writer.py:48] [17800] global_step=17800, grad_norm=4.626692771911621, loss=3.0032477378845215
I0126 20:50:37.065945 139822745589568 spec.py:321] Evaluating on the training split.
I0126 20:50:45.302096 139822745589568 spec.py:333] Evaluating on the validation split.
I0126 20:50:58.366382 139822745589568 spec.py:349] Evaluating on the test split.
I0126 20:51:00.647140 139822745589568 submission_runner.py:408] Time since start: 6454.51s, 	Step: 17895, 	{'train/accuracy': 0.5936902165412903, 'train/loss': 1.7567952871322632, 'validation/accuracy': 0.5552600026130676, 'validation/loss': 1.939975380897522, 'validation/num_examples': 50000, 'test/accuracy': 0.4360000193119049, 'test/loss': 2.634429931640625, 'test/num_examples': 10000, 'score': 6177.358016967773, 'total_duration': 6454.508526086807, 'accumulated_submission_time': 6177.358016967773, 'accumulated_eval_time': 276.202490568161, 'accumulated_logging_time': 0.35756540298461914}
I0126 20:51:00.678805 139659829036800 logging_writer.py:48] [17895] accumulated_eval_time=276.202491, accumulated_logging_time=0.357565, accumulated_submission_time=6177.358017, global_step=17895, preemption_count=0, score=6177.358017, test/accuracy=0.436000, test/loss=2.634430, test/num_examples=10000, total_duration=6454.508526, train/accuracy=0.593690, train/loss=1.756795, validation/accuracy=0.555260, validation/loss=1.939975, validation/num_examples=50000
I0126 20:51:02.733309 139659837429504 logging_writer.py:48] [17900] global_step=17900, grad_norm=3.342442035675049, loss=3.0672101974487305
I0126 20:51:36.818653 139659829036800 logging_writer.py:48] [18000] global_step=18000, grad_norm=3.470538854598999, loss=3.132720708847046
I0126 20:52:10.943831 139659837429504 logging_writer.py:48] [18100] global_step=18100, grad_norm=2.559720754623413, loss=2.9775636196136475
I0126 20:52:45.081059 139659829036800 logging_writer.py:48] [18200] global_step=18200, grad_norm=3.8288211822509766, loss=2.9187889099121094
I0126 20:53:19.266425 139659837429504 logging_writer.py:48] [18300] global_step=18300, grad_norm=4.171499729156494, loss=2.9551684856414795
I0126 20:53:53.429953 139659829036800 logging_writer.py:48] [18400] global_step=18400, grad_norm=3.994952917098999, loss=3.0951180458068848
I0126 20:54:27.602698 139659837429504 logging_writer.py:48] [18500] global_step=18500, grad_norm=3.350830316543579, loss=2.940547466278076
I0126 20:55:01.760992 139659829036800 logging_writer.py:48] [18600] global_step=18600, grad_norm=4.47796630859375, loss=2.9966325759887695
I0126 20:55:35.917245 139659837429504 logging_writer.py:48] [18700] global_step=18700, grad_norm=4.0706377029418945, loss=2.9345927238464355
I0126 20:56:10.098814 139659829036800 logging_writer.py:48] [18800] global_step=18800, grad_norm=2.7947607040405273, loss=3.015052556991577
I0126 20:56:44.259412 139659837429504 logging_writer.py:48] [18900] global_step=18900, grad_norm=2.971529245376587, loss=3.043416738510132
I0126 20:57:18.434840 139659829036800 logging_writer.py:48] [19000] global_step=19000, grad_norm=3.796992301940918, loss=3.084406614303589
I0126 20:57:52.604721 139659837429504 logging_writer.py:48] [19100] global_step=19100, grad_norm=3.9584896564483643, loss=2.972977876663208
I0126 20:58:26.785806 139659829036800 logging_writer.py:48] [19200] global_step=19200, grad_norm=3.9303689002990723, loss=3.095721960067749
I0126 20:59:00.953807 139659837429504 logging_writer.py:48] [19300] global_step=19300, grad_norm=2.967951536178589, loss=2.9514589309692383
I0126 20:59:30.761694 139822745589568 spec.py:321] Evaluating on the training split.
I0126 20:59:38.445261 139822745589568 spec.py:333] Evaluating on the validation split.
I0126 20:59:51.226166 139822745589568 spec.py:349] Evaluating on the test split.
I0126 20:59:53.444284 139822745589568 submission_runner.py:408] Time since start: 6987.31s, 	Step: 19389, 	{'train/accuracy': 0.6022600531578064, 'train/loss': 1.7681397199630737, 'validation/accuracy': 0.5593400001525879, 'validation/loss': 1.9684282541275024, 'validation/num_examples': 50000, 'test/accuracy': 0.4310000240802765, 'test/loss': 2.670055866241455, 'test/num_examples': 10000, 'score': 6687.38344168663, 'total_duration': 6987.305696725845, 'accumulated_submission_time': 6687.38344168663, 'accumulated_eval_time': 298.885041475296, 'accumulated_logging_time': 0.39752960205078125}
I0126 20:59:53.462869 139659829036800 logging_writer.py:48] [19389] accumulated_eval_time=298.885041, accumulated_logging_time=0.397530, accumulated_submission_time=6687.383442, global_step=19389, preemption_count=0, score=6687.383442, test/accuracy=0.431000, test/loss=2.670056, test/num_examples=10000, total_duration=6987.305697, train/accuracy=0.602260, train/loss=1.768140, validation/accuracy=0.559340, validation/loss=1.968428, validation/num_examples=50000
I0126 20:59:57.559440 139659845822208 logging_writer.py:48] [19400] global_step=19400, grad_norm=3.070547580718994, loss=3.1473982334136963
I0126 21:00:31.638866 139659829036800 logging_writer.py:48] [19500] global_step=19500, grad_norm=3.053816080093384, loss=3.0202436447143555
I0126 21:01:05.760163 139659845822208 logging_writer.py:48] [19600] global_step=19600, grad_norm=3.47135329246521, loss=2.9665894508361816
I0126 21:01:39.911664 139659829036800 logging_writer.py:48] [19700] global_step=19700, grad_norm=4.130599021911621, loss=3.007687568664551
I0126 21:02:14.072531 139659845822208 logging_writer.py:48] [19800] global_step=19800, grad_norm=3.6564738750457764, loss=2.9512851238250732
I0126 21:02:48.220357 139659829036800 logging_writer.py:48] [19900] global_step=19900, grad_norm=3.2013869285583496, loss=3.0296790599823
I0126 21:03:22.333730 139659845822208 logging_writer.py:48] [20000] global_step=20000, grad_norm=3.4867095947265625, loss=2.9269425868988037
I0126 21:03:56.470265 139659829036800 logging_writer.py:48] [20100] global_step=20100, grad_norm=3.993466854095459, loss=2.8745205402374268
I0126 21:04:30.611107 139659845822208 logging_writer.py:48] [20200] global_step=20200, grad_norm=3.077279567718506, loss=2.963019847869873
I0126 21:05:04.748714 139659829036800 logging_writer.py:48] [20300] global_step=20300, grad_norm=3.0003833770751953, loss=3.0460431575775146
I0126 21:05:38.871508 139659845822208 logging_writer.py:48] [20400] global_step=20400, grad_norm=3.39762282371521, loss=2.9549174308776855
I0126 21:06:13.089459 139659829036800 logging_writer.py:48] [20500] global_step=20500, grad_norm=3.858011484146118, loss=2.902580499649048
I0126 21:06:47.232845 139659845822208 logging_writer.py:48] [20600] global_step=20600, grad_norm=3.1908602714538574, loss=2.860809326171875
I0126 21:07:21.400680 139659829036800 logging_writer.py:48] [20700] global_step=20700, grad_norm=2.966269016265869, loss=2.917957067489624
I0126 21:07:55.587390 139659845822208 logging_writer.py:48] [20800] global_step=20800, grad_norm=4.856483459472656, loss=2.8963444232940674
I0126 21:08:23.704103 139822745589568 spec.py:321] Evaluating on the training split.
I0126 21:08:31.668411 139822745589568 spec.py:333] Evaluating on the validation split.
I0126 21:08:44.911034 139822745589568 spec.py:349] Evaluating on the test split.
I0126 21:08:47.491422 139822745589568 submission_runner.py:408] Time since start: 7521.35s, 	Step: 20884, 	{'train/accuracy': 0.6381337642669678, 'train/loss': 1.5879325866699219, 'validation/accuracy': 0.5699599981307983, 'validation/loss': 1.8987358808517456, 'validation/num_examples': 50000, 'test/accuracy': 0.4415000081062317, 'test/loss': 2.5937812328338623, 'test/num_examples': 10000, 'score': 7197.566004276276, 'total_duration': 7521.352801322937, 'accumulated_submission_time': 7197.566004276276, 'accumulated_eval_time': 322.6722848415375, 'accumulated_logging_time': 0.42600297927856445}
I0126 21:08:47.511561 139659954833152 logging_writer.py:48] [20884] accumulated_eval_time=322.672285, accumulated_logging_time=0.426003, accumulated_submission_time=7197.566004, global_step=20884, preemption_count=0, score=7197.566004, test/accuracy=0.441500, test/loss=2.593781, test/num_examples=10000, total_duration=7521.352801, train/accuracy=0.638134, train/loss=1.587933, validation/accuracy=0.569960, validation/loss=1.898736, validation/num_examples=50000
I0126 21:08:53.307955 139659963225856 logging_writer.py:48] [20900] global_step=20900, grad_norm=3.9177374839782715, loss=3.1112093925476074
I0126 21:09:27.420628 139659954833152 logging_writer.py:48] [21000] global_step=21000, grad_norm=3.150444269180298, loss=3.009394884109497
I0126 21:10:01.538002 139659963225856 logging_writer.py:48] [21100] global_step=21100, grad_norm=3.989727735519409, loss=3.011427640914917
I0126 21:10:35.679231 139659954833152 logging_writer.py:48] [21200] global_step=21200, grad_norm=3.296393632888794, loss=2.9768288135528564
I0126 21:11:09.811814 139659963225856 logging_writer.py:48] [21300] global_step=21300, grad_norm=4.003594398498535, loss=2.9709722995758057
I0126 21:11:43.940361 139659954833152 logging_writer.py:48] [21400] global_step=21400, grad_norm=2.670090675354004, loss=2.8934385776519775
I0126 21:12:18.093923 139659963225856 logging_writer.py:48] [21500] global_step=21500, grad_norm=3.338603973388672, loss=2.966660261154175
I0126 21:12:52.245089 139659954833152 logging_writer.py:48] [21600] global_step=21600, grad_norm=3.785179615020752, loss=3.033090353012085
I0126 21:13:26.416328 139659963225856 logging_writer.py:48] [21700] global_step=21700, grad_norm=2.717430353164673, loss=2.8933777809143066
I0126 21:14:00.573695 139659954833152 logging_writer.py:48] [21800] global_step=21800, grad_norm=3.3959591388702393, loss=2.951789617538452
I0126 21:14:34.705628 139659963225856 logging_writer.py:48] [21900] global_step=21900, grad_norm=3.1433935165405273, loss=3.0138933658599854
I0126 21:15:08.978394 139659954833152 logging_writer.py:48] [22000] global_step=22000, grad_norm=2.7971558570861816, loss=2.8941264152526855
I0126 21:15:43.116613 139659963225856 logging_writer.py:48] [22100] global_step=22100, grad_norm=2.787041664123535, loss=2.8562071323394775
I0126 21:16:17.272121 139659954833152 logging_writer.py:48] [22200] global_step=22200, grad_norm=3.0768625736236572, loss=2.865262031555176
I0126 21:16:51.425792 139659963225856 logging_writer.py:48] [22300] global_step=22300, grad_norm=2.9711835384368896, loss=2.9316415786743164
I0126 21:17:17.811996 139822745589568 spec.py:321] Evaluating on the training split.
I0126 21:17:25.708204 139822745589568 spec.py:333] Evaluating on the validation split.
I0126 21:17:38.615756 139822745589568 spec.py:349] Evaluating on the test split.
I0126 21:17:40.851325 139822745589568 submission_runner.py:408] Time since start: 8054.71s, 	Step: 22379, 	{'train/accuracy': 0.6238042116165161, 'train/loss': 1.6296091079711914, 'validation/accuracy': 0.5750600099563599, 'validation/loss': 1.8640074729919434, 'validation/num_examples': 50000, 'test/accuracy': 0.4531000256538391, 'test/loss': 2.565328359603882, 'test/num_examples': 10000, 'score': 7707.807578086853, 'total_duration': 8054.712727546692, 'accumulated_submission_time': 7707.807578086853, 'accumulated_eval_time': 345.71156549453735, 'accumulated_logging_time': 0.45580077171325684}
I0126 21:17:40.870058 139658738538240 logging_writer.py:48] [22379] accumulated_eval_time=345.711565, accumulated_logging_time=0.455801, accumulated_submission_time=7707.807578, global_step=22379, preemption_count=0, score=7707.807578, test/accuracy=0.453100, test/loss=2.565328, test/num_examples=10000, total_duration=8054.712728, train/accuracy=0.623804, train/loss=1.629609, validation/accuracy=0.575060, validation/loss=1.864007, validation/num_examples=50000
I0126 21:17:48.375217 139658746930944 logging_writer.py:48] [22400] global_step=22400, grad_norm=3.1613128185272217, loss=2.8903119564056396
I0126 21:18:22.442931 139658738538240 logging_writer.py:48] [22500] global_step=22500, grad_norm=3.1197638511657715, loss=2.902193307876587
I0126 21:18:56.565356 139658746930944 logging_writer.py:48] [22600] global_step=22600, grad_norm=3.419682502746582, loss=2.8442559242248535
I0126 21:19:30.689462 139658738538240 logging_writer.py:48] [22700] global_step=22700, grad_norm=3.2246994972229004, loss=2.832413673400879
I0126 21:20:04.834234 139658746930944 logging_writer.py:48] [22800] global_step=22800, grad_norm=2.4129371643066406, loss=2.854679822921753
I0126 21:20:38.997785 139658738538240 logging_writer.py:48] [22900] global_step=22900, grad_norm=2.8645431995391846, loss=2.838942527770996
I0126 21:21:13.245095 139658746930944 logging_writer.py:48] [23000] global_step=23000, grad_norm=3.043039321899414, loss=2.93412446975708
I0126 21:21:47.388190 139658738538240 logging_writer.py:48] [23100] global_step=23100, grad_norm=3.2010388374328613, loss=2.969801902770996
I0126 21:22:21.546723 139658746930944 logging_writer.py:48] [23200] global_step=23200, grad_norm=3.5876965522766113, loss=2.899742364883423
I0126 21:22:55.694706 139658738538240 logging_writer.py:48] [23300] global_step=23300, grad_norm=3.357844591140747, loss=2.9019784927368164
I0126 21:23:29.802797 139658746930944 logging_writer.py:48] [23400] global_step=23400, grad_norm=3.116811513900757, loss=3.0068628787994385
I0126 21:24:03.958917 139658738538240 logging_writer.py:48] [23500] global_step=23500, grad_norm=3.042734146118164, loss=2.9658560752868652
I0126 21:24:38.114790 139658746930944 logging_writer.py:48] [23600] global_step=23600, grad_norm=2.6236960887908936, loss=2.9145545959472656
I0126 21:25:12.257214 139658738538240 logging_writer.py:48] [23700] global_step=23700, grad_norm=2.825791835784912, loss=2.8753702640533447
I0126 21:25:46.404740 139658746930944 logging_writer.py:48] [23800] global_step=23800, grad_norm=2.653874158859253, loss=2.8920443058013916
I0126 21:26:11.093160 139822745589568 spec.py:321] Evaluating on the training split.
I0126 21:26:18.927536 139822745589568 spec.py:333] Evaluating on the validation split.
I0126 21:26:31.045835 139822745589568 spec.py:349] Evaluating on the test split.
I0126 21:26:33.297387 139822745589568 submission_runner.py:408] Time since start: 8587.16s, 	Step: 23874, 	{'train/accuracy': 0.6375358700752258, 'train/loss': 1.5796483755111694, 'validation/accuracy': 0.586899995803833, 'validation/loss': 1.8150631189346313, 'validation/num_examples': 50000, 'test/accuracy': 0.4646000266075134, 'test/loss': 2.491010904312134, 'test/num_examples': 10000, 'score': 8217.97511434555, 'total_duration': 8587.158815145493, 'accumulated_submission_time': 8217.97511434555, 'accumulated_eval_time': 367.9157955646515, 'accumulated_logging_time': 0.4832274913787842}
I0126 21:26:33.314793 139658738538240 logging_writer.py:48] [23874] accumulated_eval_time=367.915796, accumulated_logging_time=0.483227, accumulated_submission_time=8217.975114, global_step=23874, preemption_count=0, score=8217.975114, test/accuracy=0.464600, test/loss=2.491011, test/num_examples=10000, total_duration=8587.158815, train/accuracy=0.637536, train/loss=1.579648, validation/accuracy=0.586900, validation/loss=1.815063, validation/num_examples=50000
I0126 21:26:42.521454 139658746930944 logging_writer.py:48] [23900] global_step=23900, grad_norm=3.13690447807312, loss=2.839627504348755
I0126 21:27:16.612817 139658738538240 logging_writer.py:48] [24000] global_step=24000, grad_norm=2.774341344833374, loss=2.968200445175171
I0126 21:27:50.766931 139658746930944 logging_writer.py:48] [24100] global_step=24100, grad_norm=2.958223342895508, loss=2.926229238510132
I0126 21:28:24.906170 139658738538240 logging_writer.py:48] [24200] global_step=24200, grad_norm=2.866769313812256, loss=2.8796238899230957
I0126 21:28:59.063659 139658746930944 logging_writer.py:48] [24300] global_step=24300, grad_norm=3.7956900596618652, loss=2.895205020904541
I0126 21:29:33.190728 139658738538240 logging_writer.py:48] [24400] global_step=24400, grad_norm=2.8799822330474854, loss=2.9260973930358887
I0126 21:30:07.307745 139658746930944 logging_writer.py:48] [24500] global_step=24500, grad_norm=3.2377097606658936, loss=2.9053828716278076
I0126 21:30:41.451036 139658738538240 logging_writer.py:48] [24600] global_step=24600, grad_norm=3.2237844467163086, loss=2.8754734992980957
I0126 21:31:15.603585 139658746930944 logging_writer.py:48] [24700] global_step=24700, grad_norm=2.6032965183258057, loss=2.835528612136841
I0126 21:31:49.745701 139658738538240 logging_writer.py:48] [24800] global_step=24800, grad_norm=2.970270872116089, loss=2.865117073059082
I0126 21:32:23.847715 139658746930944 logging_writer.py:48] [24900] global_step=24900, grad_norm=3.0393970012664795, loss=2.9631588459014893
I0126 21:32:57.950634 139658738538240 logging_writer.py:48] [25000] global_step=25000, grad_norm=3.456171989440918, loss=2.862123727798462
I0126 21:33:32.064416 139658746930944 logging_writer.py:48] [25100] global_step=25100, grad_norm=3.028481960296631, loss=2.867504596710205
I0126 21:34:06.202845 139658738538240 logging_writer.py:48] [25200] global_step=25200, grad_norm=2.8016202449798584, loss=2.8218536376953125
I0126 21:34:40.346296 139658746930944 logging_writer.py:48] [25300] global_step=25300, grad_norm=3.4732515811920166, loss=2.872253179550171
I0126 21:35:03.323065 139822745589568 spec.py:321] Evaluating on the training split.
I0126 21:35:11.167186 139822745589568 spec.py:333] Evaluating on the validation split.
I0126 21:35:23.360137 139822745589568 spec.py:349] Evaluating on the test split.
I0126 21:35:25.603615 139822745589568 submission_runner.py:408] Time since start: 9119.46s, 	Step: 25369, 	{'train/accuracy': 0.6294642686843872, 'train/loss': 1.597659945487976, 'validation/accuracy': 0.584119975566864, 'validation/loss': 1.8133976459503174, 'validation/num_examples': 50000, 'test/accuracy': 0.4585000276565552, 'test/loss': 2.4793083667755127, 'test/num_examples': 10000, 'score': 8727.92223238945, 'total_duration': 9119.464904785156, 'accumulated_submission_time': 8727.92223238945, 'accumulated_eval_time': 390.196186542511, 'accumulated_logging_time': 0.512209415435791}
I0126 21:35:25.624331 139658730145536 logging_writer.py:48] [25369] accumulated_eval_time=390.196187, accumulated_logging_time=0.512209, accumulated_submission_time=8727.922232, global_step=25369, preemption_count=0, score=8727.922232, test/accuracy=0.458500, test/loss=2.479308, test/num_examples=10000, total_duration=9119.464905, train/accuracy=0.629464, train/loss=1.597660, validation/accuracy=0.584120, validation/loss=1.813398, validation/num_examples=50000
I0126 21:35:36.553992 139658738538240 logging_writer.py:48] [25400] global_step=25400, grad_norm=2.5518786907196045, loss=2.9992573261260986
I0126 21:36:10.656958 139658730145536 logging_writer.py:48] [25500] global_step=25500, grad_norm=3.002793788909912, loss=2.8484177589416504
I0126 21:36:44.766749 139658738538240 logging_writer.py:48] [25600] global_step=25600, grad_norm=4.056746482849121, loss=2.904926300048828
I0126 21:37:18.891602 139658730145536 logging_writer.py:48] [25700] global_step=25700, grad_norm=3.176267623901367, loss=2.9227864742279053
I0126 21:37:53.017635 139658738538240 logging_writer.py:48] [25800] global_step=25800, grad_norm=2.863089084625244, loss=2.867391586303711
I0126 21:38:27.156542 139658730145536 logging_writer.py:48] [25900] global_step=25900, grad_norm=2.607569694519043, loss=2.980680227279663
I0126 21:39:01.322032 139658738538240 logging_writer.py:48] [26000] global_step=26000, grad_norm=3.6182360649108887, loss=2.831902027130127
I0126 21:39:35.475935 139658730145536 logging_writer.py:48] [26100] global_step=26100, grad_norm=3.563011407852173, loss=2.8637161254882812
I0126 21:40:09.661006 139658738538240 logging_writer.py:48] [26200] global_step=26200, grad_norm=3.073167562484741, loss=2.9313251972198486
I0126 21:40:43.802234 139658730145536 logging_writer.py:48] [26300] global_step=26300, grad_norm=2.5756499767303467, loss=2.8918747901916504
I0126 21:41:17.937781 139658738538240 logging_writer.py:48] [26400] global_step=26400, grad_norm=3.4324536323547363, loss=2.927607536315918
I0126 21:41:52.069954 139658730145536 logging_writer.py:48] [26500] global_step=26500, grad_norm=2.9725468158721924, loss=2.8308610916137695
I0126 21:42:26.208913 139658738538240 logging_writer.py:48] [26600] global_step=26600, grad_norm=2.742927074432373, loss=2.7587156295776367
I0126 21:43:00.332918 139658730145536 logging_writer.py:48] [26700] global_step=26700, grad_norm=3.7855775356292725, loss=2.757131576538086
I0126 21:43:34.453130 139658738538240 logging_writer.py:48] [26800] global_step=26800, grad_norm=2.930192470550537, loss=2.8928990364074707
I0126 21:43:55.729924 139822745589568 spec.py:321] Evaluating on the training split.
I0126 21:44:04.081555 139822745589568 spec.py:333] Evaluating on the validation split.
I0126 21:44:15.100860 139822745589568 spec.py:349] Evaluating on the test split.
I0126 21:44:17.314604 139822745589568 submission_runner.py:408] Time since start: 9651.18s, 	Step: 26864, 	{'train/accuracy': 0.6300422549247742, 'train/loss': 1.63661527633667, 'validation/accuracy': 0.5836600065231323, 'validation/loss': 1.8408207893371582, 'validation/num_examples': 50000, 'test/accuracy': 0.46410003304481506, 'test/loss': 2.510469913482666, 'test/num_examples': 10000, 'score': 9237.966356039047, 'total_duration': 9651.175999879837, 'accumulated_submission_time': 9237.966356039047, 'accumulated_eval_time': 411.78082633018494, 'accumulated_logging_time': 0.5465049743652344}
I0126 21:44:17.337100 139658109384448 logging_writer.py:48] [26864] accumulated_eval_time=411.780826, accumulated_logging_time=0.546505, accumulated_submission_time=9237.966356, global_step=26864, preemption_count=0, score=9237.966356, test/accuracy=0.464100, test/loss=2.510470, test/num_examples=10000, total_duration=9651.176000, train/accuracy=0.630042, train/loss=1.636615, validation/accuracy=0.583660, validation/loss=1.840821, validation/num_examples=50000
I0126 21:44:30.883821 139658730145536 logging_writer.py:48] [26900] global_step=26900, grad_norm=2.9868216514587402, loss=2.8829360008239746
I0126 21:45:04.948159 139658109384448 logging_writer.py:48] [27000] global_step=27000, grad_norm=2.6907153129577637, loss=2.9841275215148926
I0126 21:45:39.054787 139658730145536 logging_writer.py:48] [27100] global_step=27100, grad_norm=2.7487680912017822, loss=2.955291509628296
I0126 21:46:13.235139 139658109384448 logging_writer.py:48] [27200] global_step=27200, grad_norm=2.6079928874969482, loss=2.9048843383789062
I0126 21:46:47.364539 139658730145536 logging_writer.py:48] [27300] global_step=27300, grad_norm=3.0172770023345947, loss=2.7634546756744385
I0126 21:47:21.490622 139658109384448 logging_writer.py:48] [27400] global_step=27400, grad_norm=3.3103878498077393, loss=2.852492332458496
I0126 21:47:55.609160 139658730145536 logging_writer.py:48] [27500] global_step=27500, grad_norm=2.8861560821533203, loss=2.802116632461548
I0126 21:48:29.725546 139658109384448 logging_writer.py:48] [27600] global_step=27600, grad_norm=2.8153257369995117, loss=2.8629283905029297
I0126 21:49:03.827813 139658730145536 logging_writer.py:48] [27700] global_step=27700, grad_norm=2.6451356410980225, loss=2.8192527294158936
I0126 21:49:37.934130 139658109384448 logging_writer.py:48] [27800] global_step=27800, grad_norm=2.632279396057129, loss=2.8019275665283203
I0126 21:50:12.026110 139658730145536 logging_writer.py:48] [27900] global_step=27900, grad_norm=3.3928520679473877, loss=2.912503719329834
I0126 21:50:46.120722 139658109384448 logging_writer.py:48] [28000] global_step=28000, grad_norm=3.129453182220459, loss=2.7988269329071045
I0126 21:51:20.232077 139658730145536 logging_writer.py:48] [28100] global_step=28100, grad_norm=3.0277516841888428, loss=2.8577427864074707
I0126 21:51:54.336583 139658109384448 logging_writer.py:48] [28200] global_step=28200, grad_norm=3.438598871231079, loss=2.810746669769287
I0126 21:52:28.459038 139658730145536 logging_writer.py:48] [28300] global_step=28300, grad_norm=3.434372901916504, loss=2.7471704483032227
I0126 21:52:47.652533 139822745589568 spec.py:321] Evaluating on the training split.
I0126 21:52:55.805762 139822745589568 spec.py:333] Evaluating on the validation split.
I0126 21:53:07.658499 139822745589568 spec.py:349] Evaluating on the test split.
I0126 21:53:09.947337 139822745589568 submission_runner.py:408] Time since start: 10183.81s, 	Step: 28358, 	{'train/accuracy': 0.6334103941917419, 'train/loss': 1.5800617933273315, 'validation/accuracy': 0.5934999585151672, 'validation/loss': 1.7827062606811523, 'validation/num_examples': 50000, 'test/accuracy': 0.4733000099658966, 'test/loss': 2.4336440563201904, 'test/num_examples': 10000, 'score': 9747.298719406128, 'total_duration': 10183.808728456497, 'accumulated_submission_time': 9747.298719406128, 'accumulated_eval_time': 434.0755660533905, 'accumulated_logging_time': 1.5038611888885498}
I0126 21:53:09.969464 139658738538240 logging_writer.py:48] [28358] accumulated_eval_time=434.075566, accumulated_logging_time=1.503861, accumulated_submission_time=9747.298719, global_step=28358, preemption_count=0, score=9747.298719, test/accuracy=0.473300, test/loss=2.433644, test/num_examples=10000, total_duration=10183.808728, train/accuracy=0.633410, train/loss=1.580062, validation/accuracy=0.593500, validation/loss=1.782706, validation/num_examples=50000
I0126 21:53:24.647542 139658746930944 logging_writer.py:48] [28400] global_step=28400, grad_norm=2.8834176063537598, loss=2.8643550872802734
I0126 21:53:58.672412 139658738538240 logging_writer.py:48] [28500] global_step=28500, grad_norm=3.2258684635162354, loss=2.8209500312805176
I0126 21:54:32.712901 139658746930944 logging_writer.py:48] [28600] global_step=28600, grad_norm=3.219870090484619, loss=2.8144447803497314
I0126 21:55:06.800581 139658738538240 logging_writer.py:48] [28700] global_step=28700, grad_norm=2.9703495502471924, loss=2.855285167694092
I0126 21:55:40.886716 139658746930944 logging_writer.py:48] [28800] global_step=28800, grad_norm=3.4859230518341064, loss=2.8085947036743164
I0126 21:56:15.007724 139658738538240 logging_writer.py:48] [28900] global_step=28900, grad_norm=3.1889405250549316, loss=2.918403387069702
I0126 21:56:49.095780 139658746930944 logging_writer.py:48] [29000] global_step=29000, grad_norm=3.1234869956970215, loss=2.9011313915252686
I0126 21:57:23.216762 139658738538240 logging_writer.py:48] [29100] global_step=29100, grad_norm=3.0050809383392334, loss=2.7029592990875244
I0126 21:57:57.340184 139658746930944 logging_writer.py:48] [29200] global_step=29200, grad_norm=3.0082130432128906, loss=2.808117389678955
I0126 21:58:31.454701 139658738538240 logging_writer.py:48] [29300] global_step=29300, grad_norm=2.8173484802246094, loss=2.8815646171569824
I0126 21:59:05.574580 139658746930944 logging_writer.py:48] [29400] global_step=29400, grad_norm=3.4983198642730713, loss=2.914936065673828
I0126 21:59:39.708585 139658738538240 logging_writer.py:48] [29500] global_step=29500, grad_norm=3.06331205368042, loss=2.8892099857330322
I0126 22:00:13.848448 139658746930944 logging_writer.py:48] [29600] global_step=29600, grad_norm=3.4888522624969482, loss=2.8822104930877686
I0126 22:00:47.970892 139658738538240 logging_writer.py:48] [29700] global_step=29700, grad_norm=2.9483582973480225, loss=2.774343252182007
I0126 22:01:22.103980 139658746930944 logging_writer.py:48] [29800] global_step=29800, grad_norm=3.310288190841675, loss=2.8038618564605713
I0126 22:01:39.992102 139822745589568 spec.py:321] Evaluating on the training split.
I0126 22:01:48.130441 139822745589568 spec.py:333] Evaluating on the validation split.
I0126 22:02:02.659595 139822745589568 spec.py:349] Evaluating on the test split.
I0126 22:02:04.761199 139822745589568 submission_runner.py:408] Time since start: 10718.62s, 	Step: 29854, 	{'train/accuracy': 0.6729910373687744, 'train/loss': 1.4220002889633179, 'validation/accuracy': 0.5934999585151672, 'validation/loss': 1.7916085720062256, 'validation/num_examples': 50000, 'test/accuracy': 0.4643000364303589, 'test/loss': 2.4862213134765625, 'test/num_examples': 10000, 'score': 10257.261818170547, 'total_duration': 10718.622612953186, 'accumulated_submission_time': 10257.261818170547, 'accumulated_eval_time': 458.8446247577667, 'accumulated_logging_time': 1.535851240158081}
I0126 22:02:04.781806 139656691705600 logging_writer.py:48] [29854] accumulated_eval_time=458.844625, accumulated_logging_time=1.535851, accumulated_submission_time=10257.261818, global_step=29854, preemption_count=0, score=10257.261818, test/accuracy=0.464300, test/loss=2.486221, test/num_examples=10000, total_duration=10718.622613, train/accuracy=0.672991, train/loss=1.422000, validation/accuracy=0.593500, validation/loss=1.791609, validation/num_examples=50000
I0126 22:02:20.805439 139656700098304 logging_writer.py:48] [29900] global_step=29900, grad_norm=3.367244243621826, loss=2.867873430252075
I0126 22:02:54.838029 139656691705600 logging_writer.py:48] [30000] global_step=30000, grad_norm=2.7086286544799805, loss=2.8109138011932373
I0126 22:03:28.919687 139656700098304 logging_writer.py:48] [30100] global_step=30100, grad_norm=3.8007662296295166, loss=2.838029623031616
I0126 22:04:03.027129 139656691705600 logging_writer.py:48] [30200] global_step=30200, grad_norm=3.095651626586914, loss=2.8659942150115967
I0126 22:04:37.161166 139656700098304 logging_writer.py:48] [30300] global_step=30300, grad_norm=3.2469024658203125, loss=2.737065553665161
I0126 22:05:11.305403 139656691705600 logging_writer.py:48] [30400] global_step=30400, grad_norm=2.4181089401245117, loss=2.8180558681488037
I0126 22:05:45.400172 139656700098304 logging_writer.py:48] [30500] global_step=30500, grad_norm=3.350693941116333, loss=2.8767035007476807
I0126 22:06:19.523643 139656691705600 logging_writer.py:48] [30600] global_step=30600, grad_norm=3.3107786178588867, loss=2.8654191493988037
I0126 22:06:53.634283 139656700098304 logging_writer.py:48] [30700] global_step=30700, grad_norm=2.6675865650177, loss=2.8689610958099365
I0126 22:07:27.739719 139656691705600 logging_writer.py:48] [30800] global_step=30800, grad_norm=3.366992235183716, loss=2.8380892276763916
I0126 22:08:01.831029 139656700098304 logging_writer.py:48] [30900] global_step=30900, grad_norm=2.9016377925872803, loss=2.732515573501587
I0126 22:08:35.948926 139656691705600 logging_writer.py:48] [31000] global_step=31000, grad_norm=3.1178903579711914, loss=2.826439619064331
I0126 22:09:10.058390 139656700098304 logging_writer.py:48] [31100] global_step=31100, grad_norm=2.75390887260437, loss=2.8386342525482178
I0126 22:09:44.162934 139656691705600 logging_writer.py:48] [31200] global_step=31200, grad_norm=3.5140390396118164, loss=2.8087871074676514
I0126 22:10:18.249017 139656700098304 logging_writer.py:48] [31300] global_step=31300, grad_norm=3.0076992511749268, loss=2.8655571937561035
I0126 22:10:35.061817 139822745589568 spec.py:321] Evaluating on the training split.
I0126 22:10:42.790366 139822745589568 spec.py:333] Evaluating on the validation split.
I0126 22:10:57.319479 139822745589568 spec.py:349] Evaluating on the test split.
I0126 22:10:59.571084 139822745589568 submission_runner.py:408] Time since start: 11253.43s, 	Step: 31351, 	{'train/accuracy': 0.6510881781578064, 'train/loss': 1.5065069198608398, 'validation/accuracy': 0.5972599983215332, 'validation/loss': 1.7575511932373047, 'validation/num_examples': 50000, 'test/accuracy': 0.46950003504753113, 'test/loss': 2.46608304977417, 'test/num_examples': 10000, 'score': 10767.481489896774, 'total_duration': 11253.432497739792, 'accumulated_submission_time': 10767.481489896774, 'accumulated_eval_time': 483.3538534641266, 'accumulated_logging_time': 1.5658612251281738}
I0126 22:10:59.594888 139656297445120 logging_writer.py:48] [31351] accumulated_eval_time=483.353853, accumulated_logging_time=1.565861, accumulated_submission_time=10767.481490, global_step=31351, preemption_count=0, score=10767.481490, test/accuracy=0.469500, test/loss=2.466083, test/num_examples=10000, total_duration=11253.432498, train/accuracy=0.651088, train/loss=1.506507, validation/accuracy=0.597260, validation/loss=1.757551, validation/num_examples=50000
I0126 22:11:16.624152 139656649742080 logging_writer.py:48] [31400] global_step=31400, grad_norm=2.999540328979492, loss=2.880420684814453
I0126 22:11:50.671669 139656297445120 logging_writer.py:48] [31500] global_step=31500, grad_norm=3.6439805030822754, loss=2.927426338195801
I0126 22:12:24.723242 139656649742080 logging_writer.py:48] [31600] global_step=31600, grad_norm=3.396491765975952, loss=2.9077043533325195
I0126 22:12:58.793133 139656297445120 logging_writer.py:48] [31700] global_step=31700, grad_norm=3.065476894378662, loss=2.820648431777954
I0126 22:13:32.905580 139656649742080 logging_writer.py:48] [31800] global_step=31800, grad_norm=3.1398568153381348, loss=2.875739574432373
I0126 22:14:07.025583 139656297445120 logging_writer.py:48] [31900] global_step=31900, grad_norm=3.4957330226898193, loss=2.8432424068450928
I0126 22:14:41.134281 139656649742080 logging_writer.py:48] [32000] global_step=32000, grad_norm=3.760638475418091, loss=2.898483991622925
I0126 22:15:15.275607 139656297445120 logging_writer.py:48] [32100] global_step=32100, grad_norm=4.500500679016113, loss=2.8743417263031006
I0126 22:15:49.386990 139656649742080 logging_writer.py:48] [32200] global_step=32200, grad_norm=3.187681198120117, loss=2.8570282459259033
I0126 22:16:23.497694 139656297445120 logging_writer.py:48] [32300] global_step=32300, grad_norm=2.789137601852417, loss=2.857463836669922
I0126 22:16:57.595778 139656649742080 logging_writer.py:48] [32400] global_step=32400, grad_norm=2.777818202972412, loss=2.846039295196533
I0126 22:17:31.744287 139656297445120 logging_writer.py:48] [32500] global_step=32500, grad_norm=3.2626266479492188, loss=2.788060188293457
I0126 22:18:05.865248 139656649742080 logging_writer.py:48] [32600] global_step=32600, grad_norm=3.215275287628174, loss=2.770857334136963
I0126 22:18:39.974583 139656297445120 logging_writer.py:48] [32700] global_step=32700, grad_norm=2.7475316524505615, loss=2.8278403282165527
I0126 22:19:14.101411 139656649742080 logging_writer.py:48] [32800] global_step=32800, grad_norm=2.806304931640625, loss=2.7832179069519043
I0126 22:19:29.890264 139822745589568 spec.py:321] Evaluating on the training split.
I0126 22:19:37.644166 139822745589568 spec.py:333] Evaluating on the validation split.
I0126 22:19:50.495431 139822745589568 spec.py:349] Evaluating on the test split.
I0126 22:19:52.766723 139822745589568 submission_runner.py:408] Time since start: 11786.63s, 	Step: 32848, 	{'train/accuracy': 0.6462252736091614, 'train/loss': 1.5267188549041748, 'validation/accuracy': 0.6002399921417236, 'validation/loss': 1.758679986000061, 'validation/num_examples': 50000, 'test/accuracy': 0.47610002756118774, 'test/loss': 2.4272708892822266, 'test/num_examples': 10000, 'score': 11277.717984676361, 'total_duration': 11786.628136873245, 'accumulated_submission_time': 11277.717984676361, 'accumulated_eval_time': 506.23028230667114, 'accumulated_logging_time': 1.5994904041290283}
I0126 22:19:52.788260 139656683312896 logging_writer.py:48] [32848] accumulated_eval_time=506.230282, accumulated_logging_time=1.599490, accumulated_submission_time=11277.717985, global_step=32848, preemption_count=0, score=11277.717985, test/accuracy=0.476100, test/loss=2.427271, test/num_examples=10000, total_duration=11786.628137, train/accuracy=0.646225, train/loss=1.526719, validation/accuracy=0.600240, validation/loss=1.758680, validation/num_examples=50000
I0126 22:20:10.826397 139656691705600 logging_writer.py:48] [32900] global_step=32900, grad_norm=2.8756210803985596, loss=2.849857807159424
I0126 22:20:44.911505 139656683312896 logging_writer.py:48] [33000] global_step=33000, grad_norm=3.2327218055725098, loss=2.789105176925659
I0126 22:21:19.015840 139656691705600 logging_writer.py:48] [33100] global_step=33100, grad_norm=2.7658722400665283, loss=2.695824384689331
I0126 22:21:53.134022 139656683312896 logging_writer.py:48] [33200] global_step=33200, grad_norm=3.48299503326416, loss=2.8202126026153564
I0126 22:22:27.248115 139656691705600 logging_writer.py:48] [33300] global_step=33300, grad_norm=2.802886724472046, loss=2.8796842098236084
I0126 22:23:01.345029 139656683312896 logging_writer.py:48] [33400] global_step=33400, grad_norm=2.8915724754333496, loss=2.7817468643188477
I0126 22:23:35.518618 139656691705600 logging_writer.py:48] [33500] global_step=33500, grad_norm=3.3588693141937256, loss=2.8555514812469482
I0126 22:24:09.640796 139656683312896 logging_writer.py:48] [33600] global_step=33600, grad_norm=3.0775387287139893, loss=2.9259660243988037
I0126 22:24:43.767988 139656691705600 logging_writer.py:48] [33700] global_step=33700, grad_norm=2.787010669708252, loss=2.807760000228882
I0126 22:25:17.881504 139656683312896 logging_writer.py:48] [33800] global_step=33800, grad_norm=3.207216262817383, loss=2.866903781890869
I0126 22:25:52.015499 139656691705600 logging_writer.py:48] [33900] global_step=33900, grad_norm=2.8608808517456055, loss=2.904567241668701
I0126 22:26:26.121151 139656683312896 logging_writer.py:48] [34000] global_step=34000, grad_norm=2.635570526123047, loss=2.8323607444763184
I0126 22:27:00.226803 139656691705600 logging_writer.py:48] [34100] global_step=34100, grad_norm=2.9244449138641357, loss=2.897266149520874
I0126 22:27:34.367165 139656683312896 logging_writer.py:48] [34200] global_step=34200, grad_norm=3.8041646480560303, loss=2.8766121864318848
I0126 22:28:08.509312 139656691705600 logging_writer.py:48] [34300] global_step=34300, grad_norm=2.760620355606079, loss=2.7700541019439697
I0126 22:28:22.963972 139822745589568 spec.py:321] Evaluating on the training split.
I0126 22:28:30.823312 139822745589568 spec.py:333] Evaluating on the validation split.
I0126 22:28:42.737787 139822745589568 spec.py:349] Evaluating on the test split.
I0126 22:28:45.033285 139822745589568 submission_runner.py:408] Time since start: 12318.89s, 	Step: 34344, 	{'train/accuracy': 0.6496133208274841, 'train/loss': 1.536085605621338, 'validation/accuracy': 0.6037399768829346, 'validation/loss': 1.748511791229248, 'validation/num_examples': 50000, 'test/accuracy': 0.47700002789497375, 'test/loss': 2.4422802925109863, 'test/num_examples': 10000, 'score': 11787.83618426323, 'total_duration': 12318.89468216896, 'accumulated_submission_time': 11787.83618426323, 'accumulated_eval_time': 528.2995517253876, 'accumulated_logging_time': 1.6302180290222168}
I0126 22:28:45.059332 139656674920192 logging_writer.py:48] [34344] accumulated_eval_time=528.299552, accumulated_logging_time=1.630218, accumulated_submission_time=11787.836184, global_step=34344, preemption_count=0, score=11787.836184, test/accuracy=0.477000, test/loss=2.442280, test/num_examples=10000, total_duration=12318.894682, train/accuracy=0.649613, train/loss=1.536086, validation/accuracy=0.603740, validation/loss=1.748512, validation/num_examples=50000
I0126 22:29:04.482865 139658730145536 logging_writer.py:48] [34400] global_step=34400, grad_norm=2.793731212615967, loss=2.750943422317505
I0126 22:29:38.563594 139656674920192 logging_writer.py:48] [34500] global_step=34500, grad_norm=2.777008056640625, loss=2.8135976791381836
I0126 22:30:12.686043 139658730145536 logging_writer.py:48] [34600] global_step=34600, grad_norm=2.902621030807495, loss=2.755237102508545
I0126 22:30:46.823850 139656674920192 logging_writer.py:48] [34700] global_step=34700, grad_norm=2.8773837089538574, loss=2.822458267211914
I0126 22:31:20.947784 139658730145536 logging_writer.py:48] [34800] global_step=34800, grad_norm=2.91532564163208, loss=2.8953912258148193
I0126 22:31:55.062165 139656674920192 logging_writer.py:48] [34900] global_step=34900, grad_norm=3.487867832183838, loss=2.8494582176208496
I0126 22:32:29.173880 139658730145536 logging_writer.py:48] [35000] global_step=35000, grad_norm=2.765247344970703, loss=2.777280330657959
I0126 22:33:03.286195 139656674920192 logging_writer.py:48] [35100] global_step=35100, grad_norm=2.909022092819214, loss=2.780928373336792
I0126 22:33:37.419868 139658730145536 logging_writer.py:48] [35200] global_step=35200, grad_norm=2.796358585357666, loss=2.7654874324798584
I0126 22:34:11.536772 139656674920192 logging_writer.py:48] [35300] global_step=35300, grad_norm=2.893812656402588, loss=2.7479586601257324
I0126 22:34:45.664847 139658730145536 logging_writer.py:48] [35400] global_step=35400, grad_norm=4.4360456466674805, loss=2.774078845977783
I0126 22:35:19.766592 139656674920192 logging_writer.py:48] [35500] global_step=35500, grad_norm=3.25333309173584, loss=2.8028759956359863
I0126 22:35:53.915355 139658730145536 logging_writer.py:48] [35600] global_step=35600, grad_norm=3.206470489501953, loss=2.8350865840911865
I0126 22:36:28.018055 139656674920192 logging_writer.py:48] [35700] global_step=35700, grad_norm=2.953590154647827, loss=2.8180580139160156
I0126 22:37:02.108898 139658730145536 logging_writer.py:48] [35800] global_step=35800, grad_norm=3.152738094329834, loss=2.87636399269104
I0126 22:37:15.174072 139822745589568 spec.py:321] Evaluating on the training split.
I0126 22:37:23.022593 139822745589568 spec.py:333] Evaluating on the validation split.
I0126 22:37:36.347299 139822745589568 spec.py:349] Evaluating on the test split.
I0126 22:37:38.617738 139822745589568 submission_runner.py:408] Time since start: 12852.48s, 	Step: 35840, 	{'train/accuracy': 0.645926296710968, 'train/loss': 1.52766752243042, 'validation/accuracy': 0.596560001373291, 'validation/loss': 1.7534399032592773, 'validation/num_examples': 50000, 'test/accuracy': 0.4759000241756439, 'test/loss': 2.4189414978027344, 'test/num_examples': 10000, 'score': 12297.890921831131, 'total_duration': 12852.479150056839, 'accumulated_submission_time': 12297.890921831131, 'accumulated_eval_time': 551.7431771755219, 'accumulated_logging_time': 1.6666617393493652}
I0126 22:37:38.640842 139656666527488 logging_writer.py:48] [35840] accumulated_eval_time=551.743177, accumulated_logging_time=1.666662, accumulated_submission_time=12297.890922, global_step=35840, preemption_count=0, score=12297.890922, test/accuracy=0.475900, test/loss=2.418941, test/num_examples=10000, total_duration=12852.479150, train/accuracy=0.645926, train/loss=1.527668, validation/accuracy=0.596560, validation/loss=1.753440, validation/num_examples=50000
I0126 22:37:59.408380 139656683312896 logging_writer.py:48] [35900] global_step=35900, grad_norm=3.03011155128479, loss=2.8400352001190186
I0126 22:38:33.407952 139656666527488 logging_writer.py:48] [36000] global_step=36000, grad_norm=3.4399731159210205, loss=2.8925857543945312
I0126 22:39:07.481017 139656683312896 logging_writer.py:48] [36100] global_step=36100, grad_norm=2.913571357727051, loss=2.824551582336426
I0126 22:39:41.552257 139656666527488 logging_writer.py:48] [36200] global_step=36200, grad_norm=2.761289596557617, loss=2.683762550354004
I0126 22:40:15.655527 139656683312896 logging_writer.py:48] [36300] global_step=36300, grad_norm=2.7285075187683105, loss=2.7650420665740967
I0126 22:40:49.775730 139656666527488 logging_writer.py:48] [36400] global_step=36400, grad_norm=3.3438591957092285, loss=2.853902816772461
I0126 22:41:23.895116 139656683312896 logging_writer.py:48] [36500] global_step=36500, grad_norm=3.8527448177337646, loss=2.8722610473632812
I0126 22:41:57.998765 139656666527488 logging_writer.py:48] [36600] global_step=36600, grad_norm=3.0081279277801514, loss=2.908813238143921
I0126 22:42:32.141562 139656683312896 logging_writer.py:48] [36700] global_step=36700, grad_norm=3.2097933292388916, loss=2.7373130321502686
I0126 22:43:06.262340 139656666527488 logging_writer.py:48] [36800] global_step=36800, grad_norm=3.442753791809082, loss=2.6018283367156982
I0126 22:43:40.388214 139656683312896 logging_writer.py:48] [36900] global_step=36900, grad_norm=3.0066874027252197, loss=2.7610068321228027
I0126 22:44:14.498627 139656666527488 logging_writer.py:48] [37000] global_step=37000, grad_norm=2.938222646713257, loss=2.7820520401000977
I0126 22:44:48.632483 139656683312896 logging_writer.py:48] [37100] global_step=37100, grad_norm=3.5863876342773438, loss=2.7945685386657715
I0126 22:45:22.759905 139656666527488 logging_writer.py:48] [37200] global_step=37200, grad_norm=2.9147045612335205, loss=2.6972758769989014
I0126 22:45:56.888042 139656683312896 logging_writer.py:48] [37300] global_step=37300, grad_norm=3.6405999660491943, loss=2.857644557952881
I0126 22:46:08.630291 139822745589568 spec.py:321] Evaluating on the training split.
I0126 22:46:15.550679 139822745589568 spec.py:333] Evaluating on the validation split.
I0126 22:46:27.910461 139822745589568 spec.py:349] Evaluating on the test split.
I0126 22:46:30.167301 139822745589568 submission_runner.py:408] Time since start: 13384.03s, 	Step: 37336, 	{'train/accuracy': 0.6403061151504517, 'train/loss': 1.5559779405593872, 'validation/accuracy': 0.6036199927330017, 'validation/loss': 1.7466239929199219, 'validation/num_examples': 50000, 'test/accuracy': 0.4829000234603882, 'test/loss': 2.4181418418884277, 'test/num_examples': 10000, 'score': 12807.821355819702, 'total_duration': 13384.028679132462, 'accumulated_submission_time': 12807.821355819702, 'accumulated_eval_time': 573.2801125049591, 'accumulated_logging_time': 1.6983842849731445}
I0126 22:46:30.193270 139656658134784 logging_writer.py:48] [37336] accumulated_eval_time=573.280113, accumulated_logging_time=1.698384, accumulated_submission_time=12807.821356, global_step=37336, preemption_count=0, score=12807.821356, test/accuracy=0.482900, test/loss=2.418142, test/num_examples=10000, total_duration=13384.028679, train/accuracy=0.640306, train/loss=1.555978, validation/accuracy=0.603620, validation/loss=1.746624, validation/num_examples=50000
I0126 22:46:52.333430 139656674920192 logging_writer.py:48] [37400] global_step=37400, grad_norm=3.6183457374572754, loss=2.7268190383911133
I0126 22:47:26.390932 139656658134784 logging_writer.py:48] [37500] global_step=37500, grad_norm=3.2376835346221924, loss=2.840212106704712
I0126 22:48:00.468985 139656674920192 logging_writer.py:48] [37600] global_step=37600, grad_norm=2.9616637229919434, loss=2.788484573364258
I0126 22:48:34.714825 139656658134784 logging_writer.py:48] [37700] global_step=37700, grad_norm=2.93733286857605, loss=2.8376548290252686
I0126 22:49:08.791511 139656674920192 logging_writer.py:48] [37800] global_step=37800, grad_norm=3.109877824783325, loss=2.804436683654785
I0126 22:49:42.905568 139656658134784 logging_writer.py:48] [37900] global_step=37900, grad_norm=3.3085250854492188, loss=2.7751574516296387
I0126 22:50:16.996750 139656674920192 logging_writer.py:48] [38000] global_step=38000, grad_norm=2.795886993408203, loss=2.779290199279785
I0126 22:50:51.112159 139656658134784 logging_writer.py:48] [38100] global_step=38100, grad_norm=2.927584648132324, loss=2.782151937484741
I0126 22:51:25.217181 139656674920192 logging_writer.py:48] [38200] global_step=38200, grad_norm=3.0882129669189453, loss=2.82991361618042
I0126 22:51:59.285024 139656658134784 logging_writer.py:48] [38300] global_step=38300, grad_norm=3.128349781036377, loss=2.752988338470459
I0126 22:52:33.370332 139656674920192 logging_writer.py:48] [38400] global_step=38400, grad_norm=2.814696788787842, loss=2.736801862716675
I0126 22:53:07.468738 139656658134784 logging_writer.py:48] [38500] global_step=38500, grad_norm=3.0379514694213867, loss=2.7256433963775635
I0126 22:53:41.561595 139656674920192 logging_writer.py:48] [38600] global_step=38600, grad_norm=3.4734115600585938, loss=2.7699954509735107
I0126 22:54:15.641149 139656658134784 logging_writer.py:48] [38700] global_step=38700, grad_norm=2.7973334789276123, loss=2.7507212162017822
I0126 22:54:49.834411 139656674920192 logging_writer.py:48] [38800] global_step=38800, grad_norm=3.052828073501587, loss=2.7806942462921143
I0126 22:55:00.210460 139822745589568 spec.py:321] Evaluating on the training split.
I0126 22:55:07.040517 139822745589568 spec.py:333] Evaluating on the validation split.
I0126 22:55:21.463270 139822745589568 spec.py:349] Evaluating on the test split.
I0126 22:55:23.576116 139822745589568 submission_runner.py:408] Time since start: 13917.44s, 	Step: 38832, 	{'train/accuracy': 0.6729711294174194, 'train/loss': 1.4390978813171387, 'validation/accuracy': 0.6108199954032898, 'validation/loss': 1.7267426252365112, 'validation/num_examples': 50000, 'test/accuracy': 0.49170002341270447, 'test/loss': 2.3655688762664795, 'test/num_examples': 10000, 'score': 13317.77873301506, 'total_duration': 13917.43752002716, 'accumulated_submission_time': 13317.77873301506, 'accumulated_eval_time': 596.6457276344299, 'accumulated_logging_time': 1.734304666519165}
I0126 22:55:23.596181 139656658134784 logging_writer.py:48] [38832] accumulated_eval_time=596.645728, accumulated_logging_time=1.734305, accumulated_submission_time=13317.778733, global_step=38832, preemption_count=0, score=13317.778733, test/accuracy=0.491700, test/loss=2.365569, test/num_examples=10000, total_duration=13917.437520, train/accuracy=0.672971, train/loss=1.439098, validation/accuracy=0.610820, validation/loss=1.726743, validation/num_examples=50000
I0126 22:55:47.097868 139656666527488 logging_writer.py:48] [38900] global_step=38900, grad_norm=2.903425931930542, loss=2.7534265518188477
I0126 22:56:21.164237 139656658134784 logging_writer.py:48] [39000] global_step=39000, grad_norm=3.1344845294952393, loss=2.8151257038116455
I0126 22:56:55.226271 139656666527488 logging_writer.py:48] [39100] global_step=39100, grad_norm=3.0820720195770264, loss=2.6836342811584473
I0126 22:57:29.312042 139656658134784 logging_writer.py:48] [39200] global_step=39200, grad_norm=3.8173983097076416, loss=2.8399534225463867
I0126 22:58:03.384813 139656666527488 logging_writer.py:48] [39300] global_step=39300, grad_norm=3.1733124256134033, loss=2.846566915512085
I0126 22:58:37.479598 139656658134784 logging_writer.py:48] [39400] global_step=39400, grad_norm=3.004368543624878, loss=2.8308463096618652
I0126 22:59:11.561692 139656666527488 logging_writer.py:48] [39500] global_step=39500, grad_norm=3.2529051303863525, loss=2.728149890899658
I0126 22:59:45.670522 139656658134784 logging_writer.py:48] [39600] global_step=39600, grad_norm=2.916605234146118, loss=2.7716009616851807
I0126 23:00:19.772926 139656666527488 logging_writer.py:48] [39700] global_step=39700, grad_norm=2.912614583969116, loss=2.7171645164489746
I0126 23:00:53.882548 139656658134784 logging_writer.py:48] [39800] global_step=39800, grad_norm=2.8111658096313477, loss=2.7963969707489014
I0126 23:01:28.020286 139656666527488 logging_writer.py:48] [39900] global_step=39900, grad_norm=2.8075084686279297, loss=2.756737232208252
I0126 23:02:02.141685 139656658134784 logging_writer.py:48] [40000] global_step=40000, grad_norm=3.075233221054077, loss=2.716948986053467
I0126 23:02:36.242938 139656666527488 logging_writer.py:48] [40100] global_step=40100, grad_norm=2.98128604888916, loss=2.7886571884155273
I0126 23:03:10.365896 139656658134784 logging_writer.py:48] [40200] global_step=40200, grad_norm=3.4269142150878906, loss=2.794532299041748
I0126 23:03:44.491574 139656666527488 logging_writer.py:48] [40300] global_step=40300, grad_norm=3.6114957332611084, loss=2.79060959815979
I0126 23:03:53.789104 139822745589568 spec.py:321] Evaluating on the training split.
I0126 23:04:00.439846 139822745589568 spec.py:333] Evaluating on the validation split.
I0126 23:04:12.220305 139822745589568 spec.py:349] Evaluating on the test split.
I0126 23:04:14.485652 139822745589568 submission_runner.py:408] Time since start: 14448.35s, 	Step: 40329, 	{'train/accuracy': 0.6744459271430969, 'train/loss': 1.4112752676010132, 'validation/accuracy': 0.610260009765625, 'validation/loss': 1.698612928390503, 'validation/num_examples': 50000, 'test/accuracy': 0.49410003423690796, 'test/loss': 2.351206064224243, 'test/num_examples': 10000, 'score': 13827.9108877182, 'total_duration': 14448.347055912018, 'accumulated_submission_time': 13827.9108877182, 'accumulated_eval_time': 617.3422248363495, 'accumulated_logging_time': 1.7646510601043701}
I0126 23:04:14.510623 139656649742080 logging_writer.py:48] [40329] accumulated_eval_time=617.342225, accumulated_logging_time=1.764651, accumulated_submission_time=13827.910888, global_step=40329, preemption_count=0, score=13827.910888, test/accuracy=0.494100, test/loss=2.351206, test/num_examples=10000, total_duration=14448.347056, train/accuracy=0.674446, train/loss=1.411275, validation/accuracy=0.610260, validation/loss=1.698613, validation/num_examples=50000
I0126 23:04:39.032700 139656691705600 logging_writer.py:48] [40400] global_step=40400, grad_norm=3.2119808197021484, loss=2.750105381011963
I0126 23:05:13.078223 139656649742080 logging_writer.py:48] [40500] global_step=40500, grad_norm=3.250753879547119, loss=2.792889356613159
I0126 23:05:47.165363 139656691705600 logging_writer.py:48] [40600] global_step=40600, grad_norm=2.8273448944091797, loss=2.8906772136688232
I0126 23:06:21.246026 139656649742080 logging_writer.py:48] [40700] global_step=40700, grad_norm=3.019998788833618, loss=2.73574161529541
I0126 23:06:55.355157 139656691705600 logging_writer.py:48] [40800] global_step=40800, grad_norm=3.229736804962158, loss=2.701262950897217
I0126 23:07:29.538863 139656649742080 logging_writer.py:48] [40900] global_step=40900, grad_norm=2.8741188049316406, loss=2.7570130825042725
I0126 23:08:03.663411 139656691705600 logging_writer.py:48] [41000] global_step=41000, grad_norm=2.6125190258026123, loss=2.6389639377593994
I0126 23:08:37.774649 139656649742080 logging_writer.py:48] [41100] global_step=41100, grad_norm=3.3783066272735596, loss=2.741823196411133
I0126 23:09:11.910214 139656691705600 logging_writer.py:48] [41200] global_step=41200, grad_norm=3.160846471786499, loss=2.763504981994629
I0126 23:09:46.034334 139656649742080 logging_writer.py:48] [41300] global_step=41300, grad_norm=2.9205029010772705, loss=2.8565964698791504
I0126 23:10:20.155594 139656691705600 logging_writer.py:48] [41400] global_step=41400, grad_norm=2.818539619445801, loss=2.728483200073242
I0126 23:10:54.264412 139656649742080 logging_writer.py:48] [41500] global_step=41500, grad_norm=3.2306931018829346, loss=2.919279098510742
I0126 23:11:28.343975 139656691705600 logging_writer.py:48] [41600] global_step=41600, grad_norm=3.3058698177337646, loss=2.8729164600372314
I0126 23:12:02.448112 139656649742080 logging_writer.py:48] [41700] global_step=41700, grad_norm=3.3629939556121826, loss=2.865847587585449
I0126 23:12:36.538334 139656691705600 logging_writer.py:48] [41800] global_step=41800, grad_norm=3.251615285873413, loss=2.761049509048462
I0126 23:12:44.487752 139822745589568 spec.py:321] Evaluating on the training split.
I0126 23:12:51.078858 139822745589568 spec.py:333] Evaluating on the validation split.
I0126 23:13:02.685122 139822745589568 spec.py:349] Evaluating on the test split.
I0126 23:13:05.033238 139822745589568 submission_runner.py:408] Time since start: 14978.89s, 	Step: 41825, 	{'train/accuracy': 0.6544762253761292, 'train/loss': 1.509508728981018, 'validation/accuracy': 0.6057199835777283, 'validation/loss': 1.743627667427063, 'validation/num_examples': 50000, 'test/accuracy': 0.4783000349998474, 'test/loss': 2.419309377670288, 'test/num_examples': 10000, 'score': 14337.827102661133, 'total_duration': 14978.894652605057, 'accumulated_submission_time': 14337.827102661133, 'accumulated_eval_time': 637.8876869678497, 'accumulated_logging_time': 1.7998626232147217}
I0126 23:13:05.056961 139656658134784 logging_writer.py:48] [41825] accumulated_eval_time=637.887687, accumulated_logging_time=1.799863, accumulated_submission_time=14337.827103, global_step=41825, preemption_count=0, score=14337.827103, test/accuracy=0.478300, test/loss=2.419309, test/num_examples=10000, total_duration=14978.894653, train/accuracy=0.654476, train/loss=1.509509, validation/accuracy=0.605720, validation/loss=1.743628, validation/num_examples=50000
I0126 23:13:30.973039 139656666527488 logging_writer.py:48] [41900] global_step=41900, grad_norm=3.8784854412078857, loss=2.8538191318511963
I0126 23:14:04.999514 139656658134784 logging_writer.py:48] [42000] global_step=42000, grad_norm=3.0250799655914307, loss=2.7249584197998047
I0126 23:14:39.091612 139656666527488 logging_writer.py:48] [42100] global_step=42100, grad_norm=3.406155586242676, loss=2.6795952320098877
I0126 23:15:13.195950 139656658134784 logging_writer.py:48] [42200] global_step=42200, grad_norm=3.2905654907226562, loss=2.8812637329101562
I0126 23:15:47.306984 139656666527488 logging_writer.py:48] [42300] global_step=42300, grad_norm=2.922771453857422, loss=2.7845420837402344
I0126 23:16:21.428942 139656658134784 logging_writer.py:48] [42400] global_step=42400, grad_norm=2.75652813911438, loss=2.8332788944244385
I0126 23:16:55.526930 139656666527488 logging_writer.py:48] [42500] global_step=42500, grad_norm=3.2442595958709717, loss=2.7933764457702637
I0126 23:17:29.612008 139656658134784 logging_writer.py:48] [42600] global_step=42600, grad_norm=3.3158621788024902, loss=2.7461748123168945
I0126 23:18:03.707880 139656666527488 logging_writer.py:48] [42700] global_step=42700, grad_norm=2.971275568008423, loss=2.7775444984436035
I0126 23:18:37.799874 139656658134784 logging_writer.py:48] [42800] global_step=42800, grad_norm=3.025101661682129, loss=2.7420284748077393
I0126 23:19:11.880280 139656666527488 logging_writer.py:48] [42900] global_step=42900, grad_norm=3.2713468074798584, loss=2.831430435180664
I0126 23:19:46.055471 139656658134784 logging_writer.py:48] [43000] global_step=43000, grad_norm=3.5261406898498535, loss=2.7896623611450195
I0126 23:20:20.156125 139656666527488 logging_writer.py:48] [43100] global_step=43100, grad_norm=2.983063220977783, loss=2.8614587783813477
I0126 23:20:54.250053 139656658134784 logging_writer.py:48] [43200] global_step=43200, grad_norm=3.3404793739318848, loss=2.753631114959717
I0126 23:21:28.327803 139656666527488 logging_writer.py:48] [43300] global_step=43300, grad_norm=3.360880136489868, loss=2.783824920654297
I0126 23:21:35.240953 139822745589568 spec.py:321] Evaluating on the training split.
I0126 23:21:41.679371 139822745589568 spec.py:333] Evaluating on the validation split.
I0126 23:21:53.011994 139822745589568 spec.py:349] Evaluating on the test split.
I0126 23:21:55.270136 139822745589568 submission_runner.py:408] Time since start: 15509.13s, 	Step: 43322, 	{'train/accuracy': 0.6491350531578064, 'train/loss': 1.5379236936569214, 'validation/accuracy': 0.6043199896812439, 'validation/loss': 1.748526930809021, 'validation/num_examples': 50000, 'test/accuracy': 0.4749000370502472, 'test/loss': 2.452561378479004, 'test/num_examples': 10000, 'score': 14847.952889919281, 'total_duration': 15509.131530761719, 'accumulated_submission_time': 14847.952889919281, 'accumulated_eval_time': 657.9168131351471, 'accumulated_logging_time': 1.832062005996704}
I0126 23:21:55.298285 139656691705600 logging_writer.py:48] [43322] accumulated_eval_time=657.916813, accumulated_logging_time=1.832062, accumulated_submission_time=14847.952890, global_step=43322, preemption_count=0, score=14847.952890, test/accuracy=0.474900, test/loss=2.452561, test/num_examples=10000, total_duration=15509.131531, train/accuracy=0.649135, train/loss=1.537924, validation/accuracy=0.604320, validation/loss=1.748527, validation/num_examples=50000
I0126 23:22:22.190670 139656700098304 logging_writer.py:48] [43400] global_step=43400, grad_norm=3.1205685138702393, loss=2.8574836254119873
I0126 23:22:56.243335 139656691705600 logging_writer.py:48] [43500] global_step=43500, grad_norm=3.0042383670806885, loss=2.695974826812744
I0126 23:23:30.341001 139656700098304 logging_writer.py:48] [43600] global_step=43600, grad_norm=3.4641783237457275, loss=2.786604404449463
I0126 23:24:04.436764 139656691705600 logging_writer.py:48] [43700] global_step=43700, grad_norm=2.6770541667938232, loss=2.6927881240844727
I0126 23:24:38.498475 139656700098304 logging_writer.py:48] [43800] global_step=43800, grad_norm=2.850487232208252, loss=2.768282413482666
I0126 23:25:12.553372 139656691705600 logging_writer.py:48] [43900] global_step=43900, grad_norm=2.917433261871338, loss=2.702873706817627
I0126 23:25:46.701800 139656700098304 logging_writer.py:48] [44000] global_step=44000, grad_norm=3.1239309310913086, loss=2.8039259910583496
I0126 23:26:20.779744 139656691705600 logging_writer.py:48] [44100] global_step=44100, grad_norm=2.6238961219787598, loss=2.7967305183410645
I0126 23:26:54.876176 139656700098304 logging_writer.py:48] [44200] global_step=44200, grad_norm=3.2238759994506836, loss=2.8631672859191895
I0126 23:27:28.940251 139656691705600 logging_writer.py:48] [44300] global_step=44300, grad_norm=3.0185158252716064, loss=2.673368453979492
I0126 23:28:03.045394 139656700098304 logging_writer.py:48] [44400] global_step=44400, grad_norm=2.9394805431365967, loss=2.739468812942505
I0126 23:28:37.169044 139656691705600 logging_writer.py:48] [44500] global_step=44500, grad_norm=3.397071599960327, loss=2.7971363067626953
I0126 23:29:11.274954 139656700098304 logging_writer.py:48] [44600] global_step=44600, grad_norm=3.511221408843994, loss=2.718691110610962
I0126 23:29:45.393267 139656691705600 logging_writer.py:48] [44700] global_step=44700, grad_norm=2.8348917961120605, loss=2.7372889518737793
I0126 23:30:19.483824 139656700098304 logging_writer.py:48] [44800] global_step=44800, grad_norm=2.515364170074463, loss=2.7458248138427734
I0126 23:30:25.432905 139822745589568 spec.py:321] Evaluating on the training split.
I0126 23:30:31.820736 139822745589568 spec.py:333] Evaluating on the validation split.
I0126 23:30:41.465736 139822745589568 spec.py:349] Evaluating on the test split.
I0126 23:30:43.692018 139822745589568 submission_runner.py:408] Time since start: 16037.55s, 	Step: 44819, 	{'train/accuracy': 0.6519451141357422, 'train/loss': 1.5086034536361694, 'validation/accuracy': 0.6087799668312073, 'validation/loss': 1.7206631898880005, 'validation/num_examples': 50000, 'test/accuracy': 0.48440003395080566, 'test/loss': 2.402400016784668, 'test/num_examples': 10000, 'score': 15358.028193473816, 'total_duration': 16037.553413391113, 'accumulated_submission_time': 15358.028193473816, 'accumulated_eval_time': 676.1758737564087, 'accumulated_logging_time': 1.8695974349975586}
I0126 23:30:43.717036 139656649742080 logging_writer.py:48] [44819] accumulated_eval_time=676.175874, accumulated_logging_time=1.869597, accumulated_submission_time=15358.028193, global_step=44819, preemption_count=0, score=15358.028193, test/accuracy=0.484400, test/loss=2.402400, test/num_examples=10000, total_duration=16037.553413, train/accuracy=0.651945, train/loss=1.508603, validation/accuracy=0.608780, validation/loss=1.720663, validation/num_examples=50000
I0126 23:31:11.643571 139656658134784 logging_writer.py:48] [44900] global_step=44900, grad_norm=3.8667800426483154, loss=2.7287113666534424
I0126 23:31:45.702194 139656649742080 logging_writer.py:48] [45000] global_step=45000, grad_norm=3.123729944229126, loss=2.685528039932251
I0126 23:32:19.802207 139656658134784 logging_writer.py:48] [45100] global_step=45100, grad_norm=4.1444597244262695, loss=2.7304093837738037
I0126 23:32:53.898197 139656649742080 logging_writer.py:48] [45200] global_step=45200, grad_norm=2.832792043685913, loss=2.7582201957702637
I0126 23:33:27.996215 139656658134784 logging_writer.py:48] [45300] global_step=45300, grad_norm=2.911363363265991, loss=2.8231475353240967
I0126 23:34:02.082448 139656649742080 logging_writer.py:48] [45400] global_step=45400, grad_norm=2.6692802906036377, loss=2.64880633354187
I0126 23:34:36.187731 139656658134784 logging_writer.py:48] [45500] global_step=45500, grad_norm=3.1665701866149902, loss=2.835272789001465
I0126 23:35:10.284562 139656649742080 logging_writer.py:48] [45600] global_step=45600, grad_norm=2.835514783859253, loss=2.761103868484497
I0126 23:35:44.428015 139656658134784 logging_writer.py:48] [45700] global_step=45700, grad_norm=2.967296600341797, loss=2.736701488494873
I0126 23:36:18.542732 139656649742080 logging_writer.py:48] [45800] global_step=45800, grad_norm=2.934260606765747, loss=2.781087875366211
I0126 23:36:52.643107 139656658134784 logging_writer.py:48] [45900] global_step=45900, grad_norm=2.822756767272949, loss=2.8274497985839844
I0126 23:37:26.774003 139656649742080 logging_writer.py:48] [46000] global_step=46000, grad_norm=2.828169345855713, loss=2.7566757202148438
I0126 23:38:00.897928 139656658134784 logging_writer.py:48] [46100] global_step=46100, grad_norm=2.7998368740081787, loss=2.696549654006958
I0126 23:38:35.047769 139656649742080 logging_writer.py:48] [46200] global_step=46200, grad_norm=3.5136656761169434, loss=2.809385299682617
I0126 23:39:09.151683 139656658134784 logging_writer.py:48] [46300] global_step=46300, grad_norm=3.497715711593628, loss=2.6865742206573486
I0126 23:39:13.709054 139822745589568 spec.py:321] Evaluating on the training split.
I0126 23:39:20.114701 139822745589568 spec.py:333] Evaluating on the validation split.
I0126 23:39:29.072916 139822745589568 spec.py:349] Evaluating on the test split.
I0126 23:39:31.443649 139822745589568 submission_runner.py:408] Time since start: 16565.30s, 	Step: 46315, 	{'train/accuracy': 0.6457070708274841, 'train/loss': 1.543419361114502, 'validation/accuracy': 0.6055799722671509, 'validation/loss': 1.7364466190338135, 'validation/num_examples': 50000, 'test/accuracy': 0.47050002217292786, 'test/loss': 2.4317517280578613, 'test/num_examples': 10000, 'score': 15867.962439060211, 'total_duration': 16565.30499601364, 'accumulated_submission_time': 15867.962439060211, 'accumulated_eval_time': 693.9103631973267, 'accumulated_logging_time': 1.903987169265747}
I0126 23:39:31.478551 139656691705600 logging_writer.py:48] [46315] accumulated_eval_time=693.910363, accumulated_logging_time=1.903987, accumulated_submission_time=15867.962439, global_step=46315, preemption_count=0, score=15867.962439, test/accuracy=0.470500, test/loss=2.431752, test/num_examples=10000, total_duration=16565.304996, train/accuracy=0.645707, train/loss=1.543419, validation/accuracy=0.605580, validation/loss=1.736447, validation/num_examples=50000
I0126 23:40:00.760646 139656700098304 logging_writer.py:48] [46400] global_step=46400, grad_norm=2.5078837871551514, loss=2.652620792388916
I0126 23:40:34.823464 139656691705600 logging_writer.py:48] [46500] global_step=46500, grad_norm=3.2492315769195557, loss=2.703888416290283
I0126 23:41:08.901785 139656700098304 logging_writer.py:48] [46600] global_step=46600, grad_norm=3.5491273403167725, loss=2.763079881668091
I0126 23:41:42.998799 139656691705600 logging_writer.py:48] [46700] global_step=46700, grad_norm=3.2682881355285645, loss=2.7519283294677734
I0126 23:42:17.095505 139656700098304 logging_writer.py:48] [46800] global_step=46800, grad_norm=2.729654312133789, loss=2.7118749618530273
I0126 23:42:51.175857 139656691705600 logging_writer.py:48] [46900] global_step=46900, grad_norm=3.796813726425171, loss=2.7316226959228516
I0126 23:43:25.282618 139656700098304 logging_writer.py:48] [47000] global_step=47000, grad_norm=3.123948335647583, loss=2.815481185913086
I0126 23:43:59.389697 139656691705600 logging_writer.py:48] [47100] global_step=47100, grad_norm=2.996351718902588, loss=2.6736435890197754
I0126 23:44:33.557881 139656700098304 logging_writer.py:48] [47200] global_step=47200, grad_norm=3.126826047897339, loss=2.7139220237731934
I0126 23:45:07.678242 139656691705600 logging_writer.py:48] [47300] global_step=47300, grad_norm=2.920841693878174, loss=2.7342114448547363
I0126 23:45:41.749683 139656700098304 logging_writer.py:48] [47400] global_step=47400, grad_norm=2.866102933883667, loss=2.6960878372192383
I0126 23:46:15.840220 139656691705600 logging_writer.py:48] [47500] global_step=47500, grad_norm=3.1197736263275146, loss=2.8360166549682617
I0126 23:46:49.914121 139656700098304 logging_writer.py:48] [47600] global_step=47600, grad_norm=2.9554615020751953, loss=2.836259126663208
I0126 23:47:23.988889 139656691705600 logging_writer.py:48] [47700] global_step=47700, grad_norm=3.0299458503723145, loss=2.798275947570801
I0126 23:47:58.069127 139656700098304 logging_writer.py:48] [47800] global_step=47800, grad_norm=3.5899975299835205, loss=2.7952704429626465
I0126 23:48:01.599825 139822745589568 spec.py:321] Evaluating on the training split.
I0126 23:48:07.913365 139822745589568 spec.py:333] Evaluating on the validation split.
I0126 23:48:17.634636 139822745589568 spec.py:349] Evaluating on the test split.
I0126 23:48:19.900847 139822745589568 submission_runner.py:408] Time since start: 17093.76s, 	Step: 47812, 	{'train/accuracy': 0.6532405614852905, 'train/loss': 1.498455286026001, 'validation/accuracy': 0.6109600067138672, 'validation/loss': 1.7112398147583008, 'validation/num_examples': 50000, 'test/accuracy': 0.4796000123023987, 'test/loss': 2.4064650535583496, 'test/num_examples': 10000, 'score': 16378.023522853851, 'total_duration': 17093.762252807617, 'accumulated_submission_time': 16378.023522853851, 'accumulated_eval_time': 712.2113356590271, 'accumulated_logging_time': 1.949737787246704}
I0126 23:48:19.933588 139656658134784 logging_writer.py:48] [47812] accumulated_eval_time=712.211336, accumulated_logging_time=1.949738, accumulated_submission_time=16378.023523, global_step=47812, preemption_count=0, score=16378.023523, test/accuracy=0.479600, test/loss=2.406465, test/num_examples=10000, total_duration=17093.762253, train/accuracy=0.653241, train/loss=1.498455, validation/accuracy=0.610960, validation/loss=1.711240, validation/num_examples=50000
I0126 23:48:50.243235 139656666527488 logging_writer.py:48] [47900] global_step=47900, grad_norm=3.1219143867492676, loss=2.7423667907714844
I0126 23:49:24.315934 139656658134784 logging_writer.py:48] [48000] global_step=48000, grad_norm=3.1972532272338867, loss=2.7282824516296387
I0126 23:49:58.404831 139656666527488 logging_writer.py:48] [48100] global_step=48100, grad_norm=3.129615545272827, loss=2.6636645793914795
I0126 23:50:32.503855 139656658134784 logging_writer.py:48] [48200] global_step=48200, grad_norm=2.6416079998016357, loss=2.689465045928955
I0126 23:51:06.650746 139656666527488 logging_writer.py:48] [48300] global_step=48300, grad_norm=2.9703781604766846, loss=2.7495408058166504
I0126 23:51:40.752995 139656658134784 logging_writer.py:48] [48400] global_step=48400, grad_norm=3.5839121341705322, loss=2.7088685035705566
I0126 23:52:14.870547 139656666527488 logging_writer.py:48] [48500] global_step=48500, grad_norm=2.9452240467071533, loss=2.7818613052368164
I0126 23:52:48.987988 139656658134784 logging_writer.py:48] [48600] global_step=48600, grad_norm=2.8766252994537354, loss=2.8165955543518066
I0126 23:53:23.103728 139656666527488 logging_writer.py:48] [48700] global_step=48700, grad_norm=3.3330576419830322, loss=2.675611734390259
I0126 23:53:57.213985 139656658134784 logging_writer.py:48] [48800] global_step=48800, grad_norm=3.269944906234741, loss=2.749885082244873
I0126 23:54:31.319632 139656666527488 logging_writer.py:48] [48900] global_step=48900, grad_norm=3.32377028465271, loss=2.711649179458618
I0126 23:55:05.410927 139656658134784 logging_writer.py:48] [49000] global_step=49000, grad_norm=3.1603286266326904, loss=2.7459988594055176
I0126 23:55:39.502456 139656666527488 logging_writer.py:48] [49100] global_step=49100, grad_norm=3.107450008392334, loss=2.7833266258239746
I0126 23:56:13.592989 139656658134784 logging_writer.py:48] [49200] global_step=49200, grad_norm=3.0379714965820312, loss=2.7357778549194336
I0126 23:56:47.731291 139656666527488 logging_writer.py:48] [49300] global_step=49300, grad_norm=2.9184606075286865, loss=2.646458625793457
I0126 23:56:49.909224 139822745589568 spec.py:321] Evaluating on the training split.
I0126 23:56:56.126028 139822745589568 spec.py:333] Evaluating on the validation split.
I0126 23:57:04.936458 139822745589568 spec.py:349] Evaluating on the test split.
I0126 23:57:07.218764 139822745589568 submission_runner.py:408] Time since start: 17621.08s, 	Step: 49308, 	{'train/accuracy': 0.6916453838348389, 'train/loss': 1.3557281494140625, 'validation/accuracy': 0.6181399822235107, 'validation/loss': 1.6922627687454224, 'validation/num_examples': 50000, 'test/accuracy': 0.48600003123283386, 'test/loss': 2.362870216369629, 'test/num_examples': 10000, 'score': 16887.94068956375, 'total_duration': 17621.080164194107, 'accumulated_submission_time': 16887.94068956375, 'accumulated_eval_time': 729.52081990242, 'accumulated_logging_time': 1.9927701950073242}
I0126 23:57:07.243997 139656297445120 logging_writer.py:48] [49308] accumulated_eval_time=729.520820, accumulated_logging_time=1.992770, accumulated_submission_time=16887.940690, global_step=49308, preemption_count=0, score=16887.940690, test/accuracy=0.486000, test/loss=2.362870, test/num_examples=10000, total_duration=17621.080164, train/accuracy=0.691645, train/loss=1.355728, validation/accuracy=0.618140, validation/loss=1.692263, validation/num_examples=50000
I0126 23:57:38.901113 139656649742080 logging_writer.py:48] [49400] global_step=49400, grad_norm=2.9039359092712402, loss=2.76918888092041
I0126 23:58:12.960177 139656297445120 logging_writer.py:48] [49500] global_step=49500, grad_norm=2.942528247833252, loss=2.7320497035980225
I0126 23:58:46.995977 139656649742080 logging_writer.py:48] [49600] global_step=49600, grad_norm=3.200371026992798, loss=2.735233783721924
I0126 23:59:20.783659 139656297445120 logging_writer.py:48] [49700] global_step=49700, grad_norm=3.0568339824676514, loss=2.8363711833953857
I0126 23:59:54.885188 139656649742080 logging_writer.py:48] [49800] global_step=49800, grad_norm=3.2087154388427734, loss=2.637897491455078
I0127 00:00:28.973755 139656297445120 logging_writer.py:48] [49900] global_step=49900, grad_norm=3.3552563190460205, loss=2.8019070625305176
I0127 00:01:03.029716 139656649742080 logging_writer.py:48] [50000] global_step=50000, grad_norm=3.9774396419525146, loss=2.6835696697235107
I0127 00:01:37.108011 139656297445120 logging_writer.py:48] [50100] global_step=50100, grad_norm=3.0412142276763916, loss=2.8174800872802734
I0127 00:02:11.173757 139656649742080 logging_writer.py:48] [50200] global_step=50200, grad_norm=3.2973687648773193, loss=2.734726905822754
I0127 00:02:45.225573 139656297445120 logging_writer.py:48] [50300] global_step=50300, grad_norm=3.280349016189575, loss=2.7099039554595947
I0127 00:03:19.340312 139656649742080 logging_writer.py:48] [50400] global_step=50400, grad_norm=3.085028648376465, loss=2.539888381958008
I0127 00:03:53.412527 139656297445120 logging_writer.py:48] [50500] global_step=50500, grad_norm=3.9654414653778076, loss=2.7770893573760986
I0127 00:04:27.510386 139656649742080 logging_writer.py:48] [50600] global_step=50600, grad_norm=2.9517552852630615, loss=2.7293128967285156
I0127 00:05:01.568383 139656297445120 logging_writer.py:48] [50700] global_step=50700, grad_norm=2.830873489379883, loss=2.7364683151245117
I0127 00:05:35.650763 139656649742080 logging_writer.py:48] [50800] global_step=50800, grad_norm=3.0292396545410156, loss=2.682805299758911
I0127 00:05:37.488538 139822745589568 spec.py:321] Evaluating on the training split.
I0127 00:05:43.788521 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 00:05:52.352856 139822745589568 spec.py:349] Evaluating on the test split.
I0127 00:05:54.634643 139822745589568 submission_runner.py:408] Time since start: 18148.50s, 	Step: 50807, 	{'train/accuracy': 0.671297013759613, 'train/loss': 1.4186804294586182, 'validation/accuracy': 0.6159999966621399, 'validation/loss': 1.6773052215576172, 'validation/num_examples': 50000, 'test/accuracy': 0.4921000301837921, 'test/loss': 2.351040840148926, 'test/num_examples': 10000, 'score': 17398.12221980095, 'total_duration': 18148.496007680893, 'accumulated_submission_time': 17398.12221980095, 'accumulated_eval_time': 746.666835308075, 'accumulated_logging_time': 2.0300121307373047}
I0127 00:05:54.660552 139656674920192 logging_writer.py:48] [50807] accumulated_eval_time=746.666835, accumulated_logging_time=2.030012, accumulated_submission_time=17398.122220, global_step=50807, preemption_count=0, score=17398.122220, test/accuracy=0.492100, test/loss=2.351041, test/num_examples=10000, total_duration=18148.496008, train/accuracy=0.671297, train/loss=1.418680, validation/accuracy=0.616000, validation/loss=1.677305, validation/num_examples=50000
I0127 00:06:26.686777 139656691705600 logging_writer.py:48] [50900] global_step=50900, grad_norm=2.6771020889282227, loss=2.792562961578369
I0127 00:07:00.757112 139656674920192 logging_writer.py:48] [51000] global_step=51000, grad_norm=3.2123048305511475, loss=2.729262113571167
I0127 00:07:34.850049 139656691705600 logging_writer.py:48] [51100] global_step=51100, grad_norm=3.100421667098999, loss=2.65440034866333
I0127 00:08:08.945008 139656674920192 logging_writer.py:48] [51200] global_step=51200, grad_norm=2.8334832191467285, loss=2.7656257152557373
I0127 00:08:43.055744 139656691705600 logging_writer.py:48] [51300] global_step=51300, grad_norm=2.931100606918335, loss=2.709343910217285
I0127 00:09:17.283361 139656674920192 logging_writer.py:48] [51400] global_step=51400, grad_norm=3.092731237411499, loss=2.711775302886963
I0127 00:09:51.359950 139656691705600 logging_writer.py:48] [51500] global_step=51500, grad_norm=3.4825541973114014, loss=2.773127794265747
I0127 00:10:25.419834 139656674920192 logging_writer.py:48] [51600] global_step=51600, grad_norm=2.6922695636749268, loss=2.720445156097412
I0127 00:10:59.494330 139656691705600 logging_writer.py:48] [51700] global_step=51700, grad_norm=2.8648693561553955, loss=2.778916597366333
I0127 00:11:33.556039 139656674920192 logging_writer.py:48] [51800] global_step=51800, grad_norm=2.900883197784424, loss=2.7706432342529297
I0127 00:12:07.663932 139656691705600 logging_writer.py:48] [51900] global_step=51900, grad_norm=3.3089873790740967, loss=2.7003583908081055
I0127 00:12:41.758722 139656674920192 logging_writer.py:48] [52000] global_step=52000, grad_norm=3.209724187850952, loss=2.5943548679351807
I0127 00:13:15.852512 139656691705600 logging_writer.py:48] [52100] global_step=52100, grad_norm=3.6027402877807617, loss=2.7153823375701904
I0127 00:13:49.948617 139656674920192 logging_writer.py:48] [52200] global_step=52200, grad_norm=3.485321044921875, loss=2.8101725578308105
I0127 00:14:24.017546 139656691705600 logging_writer.py:48] [52300] global_step=52300, grad_norm=3.0747241973876953, loss=2.7257540225982666
I0127 00:14:24.838693 139822745589568 spec.py:321] Evaluating on the training split.
I0127 00:14:31.077248 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 00:14:39.842740 139822745589568 spec.py:349] Evaluating on the test split.
I0127 00:14:42.126446 139822745589568 submission_runner.py:408] Time since start: 18675.99s, 	Step: 52304, 	{'train/accuracy': 0.6713567972183228, 'train/loss': 1.4391323328018188, 'validation/accuracy': 0.6169399619102478, 'validation/loss': 1.6796952486038208, 'validation/num_examples': 50000, 'test/accuracy': 0.4946000277996063, 'test/loss': 2.344132661819458, 'test/num_examples': 10000, 'score': 17908.237648248672, 'total_duration': 18675.987852096558, 'accumulated_submission_time': 17908.237648248672, 'accumulated_eval_time': 763.9545395374298, 'accumulated_logging_time': 2.067458152770996}
I0127 00:14:42.155456 139656649742080 logging_writer.py:48] [52304] accumulated_eval_time=763.954540, accumulated_logging_time=2.067458, accumulated_submission_time=17908.237648, global_step=52304, preemption_count=0, score=17908.237648, test/accuracy=0.494600, test/loss=2.344133, test/num_examples=10000, total_duration=18675.987852, train/accuracy=0.671357, train/loss=1.439132, validation/accuracy=0.616940, validation/loss=1.679695, validation/num_examples=50000
I0127 00:15:15.128490 139656658134784 logging_writer.py:48] [52400] global_step=52400, grad_norm=3.0837059020996094, loss=2.7319445610046387
I0127 00:15:49.250438 139656649742080 logging_writer.py:48] [52500] global_step=52500, grad_norm=3.1639389991760254, loss=2.6310315132141113
I0127 00:16:23.313186 139656658134784 logging_writer.py:48] [52600] global_step=52600, grad_norm=2.778911590576172, loss=2.8445796966552734
I0127 00:16:57.393550 139656649742080 logging_writer.py:48] [52700] global_step=52700, grad_norm=3.3486931324005127, loss=2.623962163925171
I0127 00:17:31.465219 139656658134784 logging_writer.py:48] [52800] global_step=52800, grad_norm=3.2989859580993652, loss=2.7028021812438965
I0127 00:18:05.528924 139656649742080 logging_writer.py:48] [52900] global_step=52900, grad_norm=3.1772754192352295, loss=2.6804659366607666
I0127 00:18:39.617080 139656658134784 logging_writer.py:48] [53000] global_step=53000, grad_norm=2.8257648944854736, loss=2.6585021018981934
I0127 00:19:13.714082 139656649742080 logging_writer.py:48] [53100] global_step=53100, grad_norm=3.0606462955474854, loss=2.7423651218414307
I0127 00:19:47.816112 139656658134784 logging_writer.py:48] [53200] global_step=53200, grad_norm=3.1405904293060303, loss=2.7070913314819336
I0127 00:20:21.915694 139656649742080 logging_writer.py:48] [53300] global_step=53300, grad_norm=2.663358211517334, loss=2.8091869354248047
I0127 00:20:56.034331 139656658134784 logging_writer.py:48] [53400] global_step=53400, grad_norm=3.3088982105255127, loss=2.770899772644043
I0127 00:21:30.118429 139656649742080 logging_writer.py:48] [53500] global_step=53500, grad_norm=2.926830768585205, loss=2.6793599128723145
I0127 00:22:04.264532 139656658134784 logging_writer.py:48] [53600] global_step=53600, grad_norm=3.274278163909912, loss=2.7566909790039062
I0127 00:22:38.377467 139656649742080 logging_writer.py:48] [53700] global_step=53700, grad_norm=2.684847116470337, loss=2.7146975994110107
I0127 00:23:12.442297 139656658134784 logging_writer.py:48] [53800] global_step=53800, grad_norm=3.0870866775512695, loss=2.705491304397583
I0127 00:23:12.448805 139822745589568 spec.py:321] Evaluating on the training split.
I0127 00:23:18.682100 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 00:23:27.372203 139822745589568 spec.py:349] Evaluating on the test split.
I0127 00:23:29.655503 139822745589568 submission_runner.py:408] Time since start: 19203.52s, 	Step: 53801, 	{'train/accuracy': 0.6638432741165161, 'train/loss': 1.4525445699691772, 'validation/accuracy': 0.6154999732971191, 'validation/loss': 1.6743123531341553, 'validation/num_examples': 50000, 'test/accuracy': 0.49660003185272217, 'test/loss': 2.3256804943084717, 'test/num_examples': 10000, 'score': 18418.46981525421, 'total_duration': 19203.51691007614, 'accumulated_submission_time': 18418.46981525421, 'accumulated_eval_time': 781.1611652374268, 'accumulated_logging_time': 2.1074891090393066}
I0127 00:23:29.687388 139656649742080 logging_writer.py:48] [53801] accumulated_eval_time=781.161165, accumulated_logging_time=2.107489, accumulated_submission_time=18418.469815, global_step=53801, preemption_count=0, score=18418.469815, test/accuracy=0.496600, test/loss=2.325680, test/num_examples=10000, total_duration=19203.516910, train/accuracy=0.663843, train/loss=1.452545, validation/accuracy=0.615500, validation/loss=1.674312, validation/num_examples=50000
I0127 00:24:03.723875 139656691705600 logging_writer.py:48] [53900] global_step=53900, grad_norm=3.0520987510681152, loss=2.796027421951294
I0127 00:24:37.776071 139656649742080 logging_writer.py:48] [54000] global_step=54000, grad_norm=2.9448933601379395, loss=2.746835231781006
I0127 00:25:11.862336 139656691705600 logging_writer.py:48] [54100] global_step=54100, grad_norm=3.069391965866089, loss=2.821646213531494
I0127 00:25:45.970006 139656649742080 logging_writer.py:48] [54200] global_step=54200, grad_norm=2.8791239261627197, loss=2.8038079738616943
I0127 00:26:20.021500 139656691705600 logging_writer.py:48] [54300] global_step=54300, grad_norm=3.5775492191314697, loss=2.721561908721924
I0127 00:26:54.110325 139656649742080 logging_writer.py:48] [54400] global_step=54400, grad_norm=3.307318687438965, loss=2.754842758178711
I0127 00:27:28.192167 139656691705600 logging_writer.py:48] [54500] global_step=54500, grad_norm=3.1332173347473145, loss=2.7548460960388184
I0127 00:28:02.320071 139656649742080 logging_writer.py:48] [54600] global_step=54600, grad_norm=3.0523407459259033, loss=2.699401378631592
I0127 00:28:36.408684 139656691705600 logging_writer.py:48] [54700] global_step=54700, grad_norm=3.4617106914520264, loss=2.8260977268218994
I0127 00:29:10.516715 139656649742080 logging_writer.py:48] [54800] global_step=54800, grad_norm=3.3737235069274902, loss=2.754415512084961
I0127 00:29:44.598939 139656691705600 logging_writer.py:48] [54900] global_step=54900, grad_norm=2.9957973957061768, loss=2.6746416091918945
I0127 00:30:18.665236 139656649742080 logging_writer.py:48] [55000] global_step=55000, grad_norm=3.750993490219116, loss=2.776207447052002
I0127 00:30:52.716057 139656691705600 logging_writer.py:48] [55100] global_step=55100, grad_norm=3.048365354537964, loss=2.683168411254883
I0127 00:31:26.771483 139656649742080 logging_writer.py:48] [55200] global_step=55200, grad_norm=2.9697911739349365, loss=2.723395824432373
I0127 00:31:59.964733 139822745589568 spec.py:321] Evaluating on the training split.
I0127 00:32:06.274445 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 00:32:14.924668 139822745589568 spec.py:349] Evaluating on the test split.
I0127 00:32:17.161756 139822745589568 submission_runner.py:408] Time since start: 19731.02s, 	Step: 55299, 	{'train/accuracy': 0.6633649468421936, 'train/loss': 1.4799798727035522, 'validation/accuracy': 0.6179400086402893, 'validation/loss': 1.6885169744491577, 'validation/num_examples': 50000, 'test/accuracy': 0.49790000915527344, 'test/loss': 2.3356235027313232, 'test/num_examples': 10000, 'score': 18928.68518781662, 'total_duration': 19731.023154973984, 'accumulated_submission_time': 18928.68518781662, 'accumulated_eval_time': 798.3581395149231, 'accumulated_logging_time': 2.149481773376465}
I0127 00:32:17.197170 139656666527488 logging_writer.py:48] [55299] accumulated_eval_time=798.358140, accumulated_logging_time=2.149482, accumulated_submission_time=18928.685188, global_step=55299, preemption_count=0, score=18928.685188, test/accuracy=0.497900, test/loss=2.335624, test/num_examples=10000, total_duration=19731.023155, train/accuracy=0.663365, train/loss=1.479980, validation/accuracy=0.617940, validation/loss=1.688517, validation/num_examples=50000
I0127 00:32:17.890811 139656674920192 logging_writer.py:48] [55300] global_step=55300, grad_norm=3.094611167907715, loss=2.7740399837493896
I0127 00:32:51.920724 139656666527488 logging_writer.py:48] [55400] global_step=55400, grad_norm=3.0059902667999268, loss=2.7304470539093018
I0127 00:33:25.969051 139656674920192 logging_writer.py:48] [55500] global_step=55500, grad_norm=2.8447020053863525, loss=2.7461256980895996
I0127 00:34:00.059723 139656666527488 logging_writer.py:48] [55600] global_step=55600, grad_norm=2.9235594272613525, loss=2.672928810119629
I0127 00:34:34.106870 139656674920192 logging_writer.py:48] [55700] global_step=55700, grad_norm=3.1986706256866455, loss=2.782358169555664
I0127 00:35:08.176113 139656666527488 logging_writer.py:48] [55800] global_step=55800, grad_norm=3.218698024749756, loss=2.716379165649414
I0127 00:35:42.218338 139656674920192 logging_writer.py:48] [55900] global_step=55900, grad_norm=3.0709919929504395, loss=2.772447109222412
I0127 00:36:16.314826 139656666527488 logging_writer.py:48] [56000] global_step=56000, grad_norm=2.9558544158935547, loss=2.8001441955566406
I0127 00:36:50.395616 139656674920192 logging_writer.py:48] [56100] global_step=56100, grad_norm=3.0366616249084473, loss=2.661224603652954
I0127 00:37:24.442076 139656666527488 logging_writer.py:48] [56200] global_step=56200, grad_norm=2.9256298542022705, loss=2.7067408561706543
I0127 00:37:58.485343 139656674920192 logging_writer.py:48] [56300] global_step=56300, grad_norm=2.7668211460113525, loss=2.7036170959472656
I0127 00:38:32.510123 139656666527488 logging_writer.py:48] [56400] global_step=56400, grad_norm=3.0174922943115234, loss=2.7079529762268066
I0127 00:39:06.557408 139656674920192 logging_writer.py:48] [56500] global_step=56500, grad_norm=2.9934298992156982, loss=2.6582870483398438
I0127 00:39:40.653412 139656666527488 logging_writer.py:48] [56600] global_step=56600, grad_norm=2.7289352416992188, loss=2.6213271617889404
I0127 00:40:14.804105 139656674920192 logging_writer.py:48] [56700] global_step=56700, grad_norm=3.155184268951416, loss=2.8538239002227783
I0127 00:40:47.332211 139822745589568 spec.py:321] Evaluating on the training split.
I0127 00:40:53.640297 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 00:41:02.442933 139822745589568 spec.py:349] Evaluating on the test split.
I0127 00:41:04.731471 139822745589568 submission_runner.py:408] Time since start: 20258.59s, 	Step: 56797, 	{'train/accuracy': 0.6719148755073547, 'train/loss': 1.425616979598999, 'validation/accuracy': 0.6231799721717834, 'validation/loss': 1.6395264863967896, 'validation/num_examples': 50000, 'test/accuracy': 0.508400022983551, 'test/loss': 2.2914958000183105, 'test/num_examples': 10000, 'score': 19438.76026201248, 'total_duration': 20258.592869758606, 'accumulated_submission_time': 19438.76026201248, 'accumulated_eval_time': 815.757345199585, 'accumulated_logging_time': 2.195958137512207}
I0127 00:41:04.757920 139656649742080 logging_writer.py:48] [56797] accumulated_eval_time=815.757345, accumulated_logging_time=2.195958, accumulated_submission_time=19438.760262, global_step=56797, preemption_count=0, score=19438.760262, test/accuracy=0.508400, test/loss=2.291496, test/num_examples=10000, total_duration=20258.592870, train/accuracy=0.671915, train/loss=1.425617, validation/accuracy=0.623180, validation/loss=1.639526, validation/num_examples=50000
I0127 00:41:06.135776 139656658134784 logging_writer.py:48] [56800] global_step=56800, grad_norm=3.5280778408050537, loss=2.688054323196411
I0127 00:41:40.198598 139656649742080 logging_writer.py:48] [56900] global_step=56900, grad_norm=3.0082976818084717, loss=2.718154191970825
I0127 00:42:14.239832 139656658134784 logging_writer.py:48] [57000] global_step=57000, grad_norm=3.3861682415008545, loss=2.746891736984253
I0127 00:42:48.322947 139656649742080 logging_writer.py:48] [57100] global_step=57100, grad_norm=3.262967109680176, loss=2.5361669063568115
I0127 00:43:22.394916 139656658134784 logging_writer.py:48] [57200] global_step=57200, grad_norm=3.1899631023406982, loss=2.7003774642944336
I0127 00:43:56.467287 139656649742080 logging_writer.py:48] [57300] global_step=57300, grad_norm=2.8087470531463623, loss=2.749546766281128
I0127 00:44:30.546218 139656658134784 logging_writer.py:48] [57400] global_step=57400, grad_norm=3.2068467140197754, loss=2.630868434906006
I0127 00:45:04.639478 139656649742080 logging_writer.py:48] [57500] global_step=57500, grad_norm=3.150590181350708, loss=2.645352363586426
I0127 00:45:38.713581 139656658134784 logging_writer.py:48] [57600] global_step=57600, grad_norm=3.182976007461548, loss=2.7235124111175537
I0127 00:46:12.792871 139656649742080 logging_writer.py:48] [57700] global_step=57700, grad_norm=3.2121024131774902, loss=2.689051866531372
I0127 00:46:47.041536 139656658134784 logging_writer.py:48] [57800] global_step=57800, grad_norm=3.3970203399658203, loss=2.777714252471924
I0127 00:47:21.125821 139656649742080 logging_writer.py:48] [57900] global_step=57900, grad_norm=3.132194757461548, loss=2.6766648292541504
I0127 00:47:55.245049 139656658134784 logging_writer.py:48] [58000] global_step=58000, grad_norm=2.910127639770508, loss=2.718860149383545
I0127 00:48:29.326104 139656649742080 logging_writer.py:48] [58100] global_step=58100, grad_norm=3.0686326026916504, loss=2.7454781532287598
I0127 00:49:03.412746 139656658134784 logging_writer.py:48] [58200] global_step=58200, grad_norm=3.0490946769714355, loss=2.6855154037475586
I0127 00:49:34.911186 139822745589568 spec.py:321] Evaluating on the training split.
I0127 00:49:41.171864 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 00:49:49.902627 139822745589568 spec.py:349] Evaluating on the test split.
I0127 00:49:52.196344 139822745589568 submission_runner.py:408] Time since start: 20786.06s, 	Step: 58294, 	{'train/accuracy': 0.7206233739852905, 'train/loss': 1.2154070138931274, 'validation/accuracy': 0.6320399641990662, 'validation/loss': 1.6034917831420898, 'validation/num_examples': 50000, 'test/accuracy': 0.5078000426292419, 'test/loss': 2.266110897064209, 'test/num_examples': 10000, 'score': 19948.852291822433, 'total_duration': 20786.057732582092, 'accumulated_submission_time': 19948.852291822433, 'accumulated_eval_time': 833.042439699173, 'accumulated_logging_time': 2.2322604656219482}
I0127 00:49:52.223476 139656297445120 logging_writer.py:48] [58294] accumulated_eval_time=833.042440, accumulated_logging_time=2.232260, accumulated_submission_time=19948.852292, global_step=58294, preemption_count=0, score=19948.852292, test/accuracy=0.507800, test/loss=2.266111, test/num_examples=10000, total_duration=20786.057733, train/accuracy=0.720623, train/loss=1.215407, validation/accuracy=0.632040, validation/loss=1.603492, validation/num_examples=50000
I0127 00:49:54.613554 139656649742080 logging_writer.py:48] [58300] global_step=58300, grad_norm=3.1755027770996094, loss=2.6574926376342773
I0127 00:50:28.637900 139656297445120 logging_writer.py:48] [58400] global_step=58400, grad_norm=3.3498504161834717, loss=2.8045482635498047
I0127 00:51:02.676679 139656649742080 logging_writer.py:48] [58500] global_step=58500, grad_norm=2.969569444656372, loss=2.6322624683380127
I0127 00:51:36.738495 139656297445120 logging_writer.py:48] [58600] global_step=58600, grad_norm=3.083385705947876, loss=2.765048027038574
I0127 00:52:10.782890 139656649742080 logging_writer.py:48] [58700] global_step=58700, grad_norm=2.837893486022949, loss=2.7942912578582764
I0127 00:52:44.988758 139656297445120 logging_writer.py:48] [58800] global_step=58800, grad_norm=3.3910763263702393, loss=2.6753923892974854
I0127 00:53:19.027619 139656649742080 logging_writer.py:48] [58900] global_step=58900, grad_norm=3.080939292907715, loss=2.5922486782073975
I0127 00:53:53.077300 139656297445120 logging_writer.py:48] [59000] global_step=59000, grad_norm=3.2703120708465576, loss=2.7082555294036865
I0127 00:54:27.134173 139656649742080 logging_writer.py:48] [59100] global_step=59100, grad_norm=2.8285059928894043, loss=2.6004748344421387
I0127 00:55:01.163485 139656297445120 logging_writer.py:48] [59200] global_step=59200, grad_norm=3.01981782913208, loss=2.774667978286743
I0127 00:55:35.212044 139656649742080 logging_writer.py:48] [59300] global_step=59300, grad_norm=3.1382126808166504, loss=2.6268510818481445
I0127 00:56:09.267127 139656297445120 logging_writer.py:48] [59400] global_step=59400, grad_norm=3.17260479927063, loss=2.568885087966919
I0127 00:56:43.333588 139656649742080 logging_writer.py:48] [59500] global_step=59500, grad_norm=3.5233347415924072, loss=2.696807384490967
I0127 00:57:17.398904 139656297445120 logging_writer.py:48] [59600] global_step=59600, grad_norm=3.2279205322265625, loss=2.6803579330444336
I0127 00:57:51.472909 139656649742080 logging_writer.py:48] [59700] global_step=59700, grad_norm=3.049877405166626, loss=2.66605806350708
I0127 00:58:22.215853 139822745589568 spec.py:321] Evaluating on the training split.
I0127 00:58:28.771555 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 00:58:37.513354 139822745589568 spec.py:349] Evaluating on the test split.
I0127 00:58:39.782599 139822745589568 submission_runner.py:408] Time since start: 21313.64s, 	Step: 59792, 	{'train/accuracy': 0.6831552982330322, 'train/loss': 1.3804254531860352, 'validation/accuracy': 0.6195999979972839, 'validation/loss': 1.6663074493408203, 'validation/num_examples': 50000, 'test/accuracy': 0.49150002002716064, 'test/loss': 2.3457841873168945, 'test/num_examples': 10000, 'score': 20458.783344745636, 'total_duration': 21313.644002199173, 'accumulated_submission_time': 20458.783344745636, 'accumulated_eval_time': 850.6091375350952, 'accumulated_logging_time': 2.270244836807251}
I0127 00:58:39.809155 139656666527488 logging_writer.py:48] [59792] accumulated_eval_time=850.609138, accumulated_logging_time=2.270245, accumulated_submission_time=20458.783345, global_step=59792, preemption_count=0, score=20458.783345, test/accuracy=0.491500, test/loss=2.345784, test/num_examples=10000, total_duration=21313.644002, train/accuracy=0.683155, train/loss=1.380425, validation/accuracy=0.619600, validation/loss=1.666307, validation/num_examples=50000
I0127 00:58:42.874561 139656691705600 logging_writer.py:48] [59800] global_step=59800, grad_norm=3.313263416290283, loss=2.661686420440674
I0127 00:59:16.905382 139656666527488 logging_writer.py:48] [59900] global_step=59900, grad_norm=3.427910327911377, loss=2.675813674926758
I0127 00:59:50.972969 139656691705600 logging_writer.py:48] [60000] global_step=60000, grad_norm=2.8026604652404785, loss=2.618821620941162
I0127 01:00:25.036848 139656666527488 logging_writer.py:48] [60100] global_step=60100, grad_norm=3.3048253059387207, loss=2.6972153186798096
I0127 01:00:59.120768 139656691705600 logging_writer.py:48] [60200] global_step=60200, grad_norm=3.348541259765625, loss=2.6328909397125244
I0127 01:01:33.200420 139656666527488 logging_writer.py:48] [60300] global_step=60300, grad_norm=3.3625266551971436, loss=2.7723162174224854
I0127 01:02:07.276421 139656691705600 logging_writer.py:48] [60400] global_step=60400, grad_norm=3.4351589679718018, loss=2.5317258834838867
I0127 01:02:41.394685 139656666527488 logging_writer.py:48] [60500] global_step=60500, grad_norm=3.4076104164123535, loss=2.726632595062256
I0127 01:03:15.475807 139656691705600 logging_writer.py:48] [60600] global_step=60600, grad_norm=3.317150592803955, loss=2.687347173690796
I0127 01:03:49.572025 139656666527488 logging_writer.py:48] [60700] global_step=60700, grad_norm=3.066563844680786, loss=2.6543900966644287
I0127 01:04:23.682728 139656691705600 logging_writer.py:48] [60800] global_step=60800, grad_norm=3.1923952102661133, loss=2.6852447986602783
I0127 01:04:57.918883 139656666527488 logging_writer.py:48] [60900] global_step=60900, grad_norm=3.766223907470703, loss=2.734837770462036
I0127 01:05:31.970799 139656691705600 logging_writer.py:48] [61000] global_step=61000, grad_norm=3.2408993244171143, loss=2.662933826446533
I0127 01:06:06.049620 139656666527488 logging_writer.py:48] [61100] global_step=61100, grad_norm=2.8863892555236816, loss=2.7189748287200928
I0127 01:06:40.101141 139656691705600 logging_writer.py:48] [61200] global_step=61200, grad_norm=3.2829036712646484, loss=2.830064296722412
I0127 01:07:09.876780 139822745589568 spec.py:321] Evaluating on the training split.
I0127 01:07:16.055649 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 01:07:24.781165 139822745589568 spec.py:349] Evaluating on the test split.
I0127 01:07:27.063461 139822745589568 submission_runner.py:408] Time since start: 21840.92s, 	Step: 61289, 	{'train/accuracy': 0.6767179369926453, 'train/loss': 1.392195224761963, 'validation/accuracy': 0.6308199763298035, 'validation/loss': 1.6244053840637207, 'validation/num_examples': 50000, 'test/accuracy': 0.5031999945640564, 'test/loss': 2.3163297176361084, 'test/num_examples': 10000, 'score': 20968.790618896484, 'total_duration': 21840.92485141754, 'accumulated_submission_time': 20968.790618896484, 'accumulated_eval_time': 867.795756816864, 'accumulated_logging_time': 2.307173728942871}
I0127 01:07:27.090291 139656297445120 logging_writer.py:48] [61289] accumulated_eval_time=867.795757, accumulated_logging_time=2.307174, accumulated_submission_time=20968.790619, global_step=61289, preemption_count=0, score=20968.790619, test/accuracy=0.503200, test/loss=2.316330, test/num_examples=10000, total_duration=21840.924851, train/accuracy=0.676718, train/loss=1.392195, validation/accuracy=0.630820, validation/loss=1.624405, validation/num_examples=50000
I0127 01:07:31.172545 139656649742080 logging_writer.py:48] [61300] global_step=61300, grad_norm=2.8897199630737305, loss=2.686492681503296
I0127 01:08:05.210770 139656297445120 logging_writer.py:48] [61400] global_step=61400, grad_norm=3.2042999267578125, loss=2.716773748397827
I0127 01:08:39.255743 139656649742080 logging_writer.py:48] [61500] global_step=61500, grad_norm=3.0441319942474365, loss=2.6713366508483887
I0127 01:09:13.285509 139656297445120 logging_writer.py:48] [61600] global_step=61600, grad_norm=3.440089464187622, loss=2.649657726287842
I0127 01:09:47.369983 139656649742080 logging_writer.py:48] [61700] global_step=61700, grad_norm=3.162254571914673, loss=2.7079291343688965
I0127 01:10:21.425169 139656297445120 logging_writer.py:48] [61800] global_step=61800, grad_norm=3.7412309646606445, loss=2.602149724960327
I0127 01:10:55.602635 139656649742080 logging_writer.py:48] [61900] global_step=61900, grad_norm=2.8184330463409424, loss=2.720536708831787
I0127 01:11:29.616279 139656297445120 logging_writer.py:48] [62000] global_step=62000, grad_norm=2.9706876277923584, loss=2.621070623397827
I0127 01:12:03.689032 139656649742080 logging_writer.py:48] [62100] global_step=62100, grad_norm=2.945399761199951, loss=2.6764073371887207
I0127 01:12:37.755432 139656297445120 logging_writer.py:48] [62200] global_step=62200, grad_norm=3.02807879447937, loss=2.6984899044036865
I0127 01:13:11.844408 139656649742080 logging_writer.py:48] [62300] global_step=62300, grad_norm=3.086958885192871, loss=2.6940276622772217
I0127 01:13:45.939664 139656297445120 logging_writer.py:48] [62400] global_step=62400, grad_norm=3.2884509563446045, loss=2.7042531967163086
I0127 01:14:20.028312 139656649742080 logging_writer.py:48] [62500] global_step=62500, grad_norm=2.9189419746398926, loss=2.7598748207092285
I0127 01:14:54.113970 139656297445120 logging_writer.py:48] [62600] global_step=62600, grad_norm=3.1972522735595703, loss=2.651604175567627
I0127 01:15:28.191371 139656649742080 logging_writer.py:48] [62700] global_step=62700, grad_norm=3.38388729095459, loss=2.7764317989349365
I0127 01:15:57.290605 139822745589568 spec.py:321] Evaluating on the training split.
I0127 01:16:03.658474 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 01:16:12.191206 139822745589568 spec.py:349] Evaluating on the test split.
I0127 01:16:14.480281 139822745589568 submission_runner.py:408] Time since start: 22368.34s, 	Step: 62787, 	{'train/accuracy': 0.682637095451355, 'train/loss': 1.37348473072052, 'validation/accuracy': 0.6302399635314941, 'validation/loss': 1.626953125, 'validation/num_examples': 50000, 'test/accuracy': 0.5047000050544739, 'test/loss': 2.289322853088379, 'test/num_examples': 10000, 'score': 21478.93054676056, 'total_duration': 22368.341687202454, 'accumulated_submission_time': 21478.93054676056, 'accumulated_eval_time': 884.985392332077, 'accumulated_logging_time': 2.344024419784546}
I0127 01:16:14.510125 139656691705600 logging_writer.py:48] [62787] accumulated_eval_time=884.985392, accumulated_logging_time=2.344024, accumulated_submission_time=21478.930547, global_step=62787, preemption_count=0, score=21478.930547, test/accuracy=0.504700, test/loss=2.289323, test/num_examples=10000, total_duration=22368.341687, train/accuracy=0.682637, train/loss=1.373485, validation/accuracy=0.630240, validation/loss=1.626953, validation/num_examples=50000
I0127 01:16:19.301640 139656700098304 logging_writer.py:48] [62800] global_step=62800, grad_norm=2.8933420181274414, loss=2.658769369125366
I0127 01:16:53.350889 139656691705600 logging_writer.py:48] [62900] global_step=62900, grad_norm=2.942957639694214, loss=2.6536245346069336
I0127 01:17:27.559316 139656700098304 logging_writer.py:48] [63000] global_step=63000, grad_norm=3.2751195430755615, loss=2.6128978729248047
I0127 01:18:01.635546 139656691705600 logging_writer.py:48] [63100] global_step=63100, grad_norm=3.4843034744262695, loss=2.7109460830688477
I0127 01:18:35.716503 139656700098304 logging_writer.py:48] [63200] global_step=63200, grad_norm=3.731313705444336, loss=2.609421730041504
I0127 01:19:09.794175 139656691705600 logging_writer.py:48] [63300] global_step=63300, grad_norm=3.123091459274292, loss=2.8279471397399902
I0127 01:19:43.852752 139656700098304 logging_writer.py:48] [63400] global_step=63400, grad_norm=3.2781643867492676, loss=2.7514467239379883
I0127 01:20:17.899716 139656691705600 logging_writer.py:48] [63500] global_step=63500, grad_norm=2.9120230674743652, loss=2.7236852645874023
I0127 01:20:51.991672 139656700098304 logging_writer.py:48] [63600] global_step=63600, grad_norm=3.1178228855133057, loss=2.6874988079071045
I0127 01:21:26.043192 139656691705600 logging_writer.py:48] [63700] global_step=63700, grad_norm=3.0858912467956543, loss=2.726034164428711
I0127 01:22:00.094573 139656700098304 logging_writer.py:48] [63800] global_step=63800, grad_norm=2.7719268798828125, loss=2.677729845046997
I0127 01:22:34.176937 139656691705600 logging_writer.py:48] [63900] global_step=63900, grad_norm=3.1681854724884033, loss=2.6438586711883545
I0127 01:23:08.348354 139656700098304 logging_writer.py:48] [64000] global_step=64000, grad_norm=3.644272565841675, loss=2.659052848815918
I0127 01:23:42.414653 139656691705600 logging_writer.py:48] [64100] global_step=64100, grad_norm=3.3024325370788574, loss=2.633836030960083
I0127 01:24:16.481242 139656700098304 logging_writer.py:48] [64200] global_step=64200, grad_norm=3.1040663719177246, loss=2.7346670627593994
I0127 01:24:44.572969 139822745589568 spec.py:321] Evaluating on the training split.
I0127 01:24:50.779710 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 01:24:59.512781 139822745589568 spec.py:349] Evaluating on the test split.
I0127 01:25:01.834495 139822745589568 submission_runner.py:408] Time since start: 22895.70s, 	Step: 64284, 	{'train/accuracy': 0.6745256781578064, 'train/loss': 1.4175406694412231, 'validation/accuracy': 0.6255800127983093, 'validation/loss': 1.6421574354171753, 'validation/num_examples': 50000, 'test/accuracy': 0.5065000057220459, 'test/loss': 2.2937440872192383, 'test/num_examples': 10000, 'score': 21988.930440187454, 'total_duration': 22895.69519519806, 'accumulated_submission_time': 21988.930440187454, 'accumulated_eval_time': 902.2461650371552, 'accumulated_logging_time': 2.3862104415893555}
I0127 01:25:01.872802 139656649742080 logging_writer.py:48] [64284] accumulated_eval_time=902.246165, accumulated_logging_time=2.386210, accumulated_submission_time=21988.930440, global_step=64284, preemption_count=0, score=21988.930440, test/accuracy=0.506500, test/loss=2.293744, test/num_examples=10000, total_duration=22895.695195, train/accuracy=0.674526, train/loss=1.417541, validation/accuracy=0.625580, validation/loss=1.642157, validation/num_examples=50000
I0127 01:25:07.679116 139656658134784 logging_writer.py:48] [64300] global_step=64300, grad_norm=3.7991902828216553, loss=2.644871711730957
I0127 01:25:41.714920 139656649742080 logging_writer.py:48] [64400] global_step=64400, grad_norm=2.988797903060913, loss=2.672950029373169
I0127 01:26:15.782732 139656658134784 logging_writer.py:48] [64500] global_step=64500, grad_norm=3.5694220066070557, loss=2.6561367511749268
I0127 01:26:49.859449 139656649742080 logging_writer.py:48] [64600] global_step=64600, grad_norm=3.6359505653381348, loss=2.649446725845337
I0127 01:27:23.942225 139656658134784 logging_writer.py:48] [64700] global_step=64700, grad_norm=3.3011131286621094, loss=2.6804182529449463
I0127 01:27:57.985477 139656649742080 logging_writer.py:48] [64800] global_step=64800, grad_norm=3.1781110763549805, loss=2.70416259765625
I0127 01:28:32.068673 139656658134784 logging_writer.py:48] [64900] global_step=64900, grad_norm=3.150611400604248, loss=2.6563687324523926
I0127 01:29:06.114953 139656649742080 logging_writer.py:48] [65000] global_step=65000, grad_norm=3.208188772201538, loss=2.661918878555298
I0127 01:29:40.298970 139656658134784 logging_writer.py:48] [65100] global_step=65100, grad_norm=3.3213813304901123, loss=2.6245672702789307
I0127 01:30:14.341567 139656649742080 logging_writer.py:48] [65200] global_step=65200, grad_norm=3.133295774459839, loss=2.5962183475494385
I0127 01:30:48.393597 139656658134784 logging_writer.py:48] [65300] global_step=65300, grad_norm=3.137591600418091, loss=2.615027666091919
I0127 01:31:22.444330 139656649742080 logging_writer.py:48] [65400] global_step=65400, grad_norm=3.028228282928467, loss=2.644660472869873
I0127 01:31:56.511453 139656658134784 logging_writer.py:48] [65500] global_step=65500, grad_norm=3.1330671310424805, loss=2.5829098224639893
I0127 01:32:30.590114 139656649742080 logging_writer.py:48] [65600] global_step=65600, grad_norm=3.355682611465454, loss=2.6493961811065674
I0127 01:33:04.675970 139656658134784 logging_writer.py:48] [65700] global_step=65700, grad_norm=3.1956422328948975, loss=2.782243251800537
I0127 01:33:32.081967 139822745589568 spec.py:321] Evaluating on the training split.
I0127 01:33:38.302801 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 01:33:47.035898 139822745589568 spec.py:349] Evaluating on the test split.
I0127 01:33:49.345306 139822745589568 submission_runner.py:408] Time since start: 23423.21s, 	Step: 65782, 	{'train/accuracy': 0.6729512214660645, 'train/loss': 1.4147884845733643, 'validation/accuracy': 0.6275599598884583, 'validation/loss': 1.630405068397522, 'validation/num_examples': 50000, 'test/accuracy': 0.504800021648407, 'test/loss': 2.2867746353149414, 'test/num_examples': 10000, 'score': 22499.075800418854, 'total_duration': 23423.206660985947, 'accumulated_submission_time': 22499.075800418854, 'accumulated_eval_time': 919.5094072818756, 'accumulated_logging_time': 2.438976287841797}
I0127 01:33:49.374774 139656297445120 logging_writer.py:48] [65782] accumulated_eval_time=919.509407, accumulated_logging_time=2.438976, accumulated_submission_time=22499.075800, global_step=65782, preemption_count=0, score=22499.075800, test/accuracy=0.504800, test/loss=2.286775, test/num_examples=10000, total_duration=23423.206661, train/accuracy=0.672951, train/loss=1.414788, validation/accuracy=0.627560, validation/loss=1.630405, validation/num_examples=50000
I0127 01:33:55.856435 139656649742080 logging_writer.py:48] [65800] global_step=65800, grad_norm=3.2874066829681396, loss=2.5641419887542725
I0127 01:34:29.915276 139656297445120 logging_writer.py:48] [65900] global_step=65900, grad_norm=3.652225971221924, loss=2.7643725872039795
I0127 01:35:03.983044 139656649742080 logging_writer.py:48] [66000] global_step=66000, grad_norm=3.548490524291992, loss=2.749180555343628
I0127 01:35:38.104633 139656297445120 logging_writer.py:48] [66100] global_step=66100, grad_norm=2.9776012897491455, loss=2.680084228515625
I0127 01:36:12.156472 139656649742080 logging_writer.py:48] [66200] global_step=66200, grad_norm=2.866905450820923, loss=2.727792739868164
I0127 01:36:46.212150 139656297445120 logging_writer.py:48] [66300] global_step=66300, grad_norm=3.1985316276550293, loss=2.7169437408447266
I0127 01:37:20.269931 139656649742080 logging_writer.py:48] [66400] global_step=66400, grad_norm=2.933952569961548, loss=2.715254783630371
I0127 01:37:54.309435 139656297445120 logging_writer.py:48] [66500] global_step=66500, grad_norm=3.224490165710449, loss=2.676598072052002
I0127 01:38:28.372053 139656649742080 logging_writer.py:48] [66600] global_step=66600, grad_norm=3.373058795928955, loss=2.6060614585876465
I0127 01:39:02.468027 139656297445120 logging_writer.py:48] [66700] global_step=66700, grad_norm=3.188265562057495, loss=2.701918601989746
I0127 01:39:36.547166 139656649742080 logging_writer.py:48] [66800] global_step=66800, grad_norm=3.188217878341675, loss=2.622732639312744
I0127 01:40:10.633182 139656297445120 logging_writer.py:48] [66900] global_step=66900, grad_norm=2.940946340560913, loss=2.6922268867492676
I0127 01:40:44.725618 139656649742080 logging_writer.py:48] [67000] global_step=67000, grad_norm=3.1362998485565186, loss=2.73380184173584
I0127 01:41:18.824502 139656297445120 logging_writer.py:48] [67100] global_step=67100, grad_norm=3.3208367824554443, loss=2.7524361610412598
I0127 01:41:53.130429 139656649742080 logging_writer.py:48] [67200] global_step=67200, grad_norm=3.1482279300689697, loss=2.591362476348877
I0127 01:42:19.510567 139822745589568 spec.py:321] Evaluating on the training split.
I0127 01:42:25.895394 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 01:42:34.445924 139822745589568 spec.py:349] Evaluating on the test split.
I0127 01:42:36.709055 139822745589568 submission_runner.py:408] Time since start: 23950.57s, 	Step: 67279, 	{'train/accuracy': 0.6883569955825806, 'train/loss': 1.3431607484817505, 'validation/accuracy': 0.6266799569129944, 'validation/loss': 1.618585467338562, 'validation/num_examples': 50000, 'test/accuracy': 0.5004000067710876, 'test/loss': 2.3088037967681885, 'test/num_examples': 10000, 'score': 23009.149444818497, 'total_duration': 23950.570457935333, 'accumulated_submission_time': 23009.149444818497, 'accumulated_eval_time': 936.7078473567963, 'accumulated_logging_time': 2.4785714149475098}
I0127 01:42:36.737613 139656658134784 logging_writer.py:48] [67279] accumulated_eval_time=936.707847, accumulated_logging_time=2.478571, accumulated_submission_time=23009.149445, global_step=67279, preemption_count=0, score=23009.149445, test/accuracy=0.500400, test/loss=2.308804, test/num_examples=10000, total_duration=23950.570458, train/accuracy=0.688357, train/loss=1.343161, validation/accuracy=0.626680, validation/loss=1.618585, validation/num_examples=50000
I0127 01:42:44.227484 139656683312896 logging_writer.py:48] [67300] global_step=67300, grad_norm=2.8746626377105713, loss=2.609245538711548
I0127 01:43:18.255059 139656658134784 logging_writer.py:48] [67400] global_step=67400, grad_norm=3.159407377243042, loss=2.7171664237976074
I0127 01:43:52.311812 139656683312896 logging_writer.py:48] [67500] global_step=67500, grad_norm=2.851811170578003, loss=2.5763278007507324
I0127 01:44:26.404188 139656658134784 logging_writer.py:48] [67600] global_step=67600, grad_norm=2.9086179733276367, loss=2.7121338844299316
I0127 01:45:00.472019 139656683312896 logging_writer.py:48] [67700] global_step=67700, grad_norm=3.226823329925537, loss=2.6927011013031006
I0127 01:45:34.530332 139656658134784 logging_writer.py:48] [67800] global_step=67800, grad_norm=3.503493547439575, loss=2.648226261138916
I0127 01:46:08.580184 139656683312896 logging_writer.py:48] [67900] global_step=67900, grad_norm=4.09946346282959, loss=2.5856199264526367
I0127 01:46:42.611392 139656658134784 logging_writer.py:48] [68000] global_step=68000, grad_norm=3.7342987060546875, loss=2.644109010696411
I0127 01:47:16.677299 139656683312896 logging_writer.py:48] [68100] global_step=68100, grad_norm=3.238673210144043, loss=2.712066888809204
I0127 01:47:50.829000 139656658134784 logging_writer.py:48] [68200] global_step=68200, grad_norm=3.729463577270508, loss=2.7363486289978027
I0127 01:48:24.859174 139656683312896 logging_writer.py:48] [68300] global_step=68300, grad_norm=2.9780781269073486, loss=2.7044804096221924
I0127 01:48:58.914796 139656658134784 logging_writer.py:48] [68400] global_step=68400, grad_norm=2.938955068588257, loss=2.6513006687164307
I0127 01:49:33.002759 139656683312896 logging_writer.py:48] [68500] global_step=68500, grad_norm=2.9961488246917725, loss=2.5889546871185303
I0127 01:50:07.071054 139656658134784 logging_writer.py:48] [68600] global_step=68600, grad_norm=3.1858770847320557, loss=2.595421314239502
I0127 01:50:41.144030 139656683312896 logging_writer.py:48] [68700] global_step=68700, grad_norm=2.838674783706665, loss=2.5808944702148438
I0127 01:51:06.828524 139822745589568 spec.py:321] Evaluating on the training split.
I0127 01:51:13.012471 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 01:51:21.725077 139822745589568 spec.py:349] Evaluating on the test split.
I0127 01:51:24.035652 139822745589568 submission_runner.py:408] Time since start: 24477.90s, 	Step: 68777, 	{'train/accuracy': 0.7035036683082581, 'train/loss': 1.274147391319275, 'validation/accuracy': 0.6340799927711487, 'validation/loss': 1.591722011566162, 'validation/num_examples': 50000, 'test/accuracy': 0.5062000155448914, 'test/loss': 2.276012897491455, 'test/num_examples': 10000, 'score': 23519.179130077362, 'total_duration': 24477.8970515728, 'accumulated_submission_time': 23519.179130077362, 'accumulated_eval_time': 953.9149236679077, 'accumulated_logging_time': 2.5172877311706543}
I0127 01:51:24.070535 139656666527488 logging_writer.py:48] [68777] accumulated_eval_time=953.914924, accumulated_logging_time=2.517288, accumulated_submission_time=23519.179130, global_step=68777, preemption_count=0, score=23519.179130, test/accuracy=0.506200, test/loss=2.276013, test/num_examples=10000, total_duration=24477.897052, train/accuracy=0.703504, train/loss=1.274147, validation/accuracy=0.634080, validation/loss=1.591722, validation/num_examples=50000
I0127 01:51:32.255908 139656674920192 logging_writer.py:48] [68800] global_step=68800, grad_norm=3.3967783451080322, loss=2.700909376144409
I0127 01:52:06.289340 139656666527488 logging_writer.py:48] [68900] global_step=68900, grad_norm=2.974081516265869, loss=2.618027448654175
I0127 01:52:40.305618 139656674920192 logging_writer.py:48] [69000] global_step=69000, grad_norm=3.593536615371704, loss=2.6630728244781494
I0127 01:53:14.336154 139656666527488 logging_writer.py:48] [69100] global_step=69100, grad_norm=3.031949520111084, loss=2.6188406944274902
I0127 01:53:48.397460 139656674920192 logging_writer.py:48] [69200] global_step=69200, grad_norm=2.950122356414795, loss=2.682007074356079
I0127 01:54:22.497346 139656666527488 logging_writer.py:48] [69300] global_step=69300, grad_norm=3.1056220531463623, loss=2.6146512031555176
I0127 01:54:56.590788 139656674920192 logging_writer.py:48] [69400] global_step=69400, grad_norm=3.8364830017089844, loss=2.6483516693115234
I0127 01:55:30.679407 139656666527488 logging_writer.py:48] [69500] global_step=69500, grad_norm=3.335090398788452, loss=2.7273142337799072
I0127 01:56:04.769409 139656674920192 logging_writer.py:48] [69600] global_step=69600, grad_norm=2.9087653160095215, loss=2.593811273574829
I0127 01:56:38.852164 139656666527488 logging_writer.py:48] [69700] global_step=69700, grad_norm=3.0331523418426514, loss=2.602473258972168
I0127 01:57:12.922350 139656674920192 logging_writer.py:48] [69800] global_step=69800, grad_norm=3.225863456726074, loss=2.5160818099975586
I0127 01:57:47.004356 139656666527488 logging_writer.py:48] [69900] global_step=69900, grad_norm=3.548069477081299, loss=2.7094621658325195
I0127 01:58:21.081041 139656674920192 logging_writer.py:48] [70000] global_step=70000, grad_norm=3.3821842670440674, loss=2.6419119834899902
I0127 01:58:55.129872 139656666527488 logging_writer.py:48] [70100] global_step=70100, grad_norm=3.3370113372802734, loss=2.687532663345337
I0127 01:59:29.189762 139656674920192 logging_writer.py:48] [70200] global_step=70200, grad_norm=3.1761021614074707, loss=2.6662158966064453
I0127 01:59:54.209607 139822745589568 spec.py:321] Evaluating on the training split.
I0127 02:00:00.442674 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 02:00:09.160852 139822745589568 spec.py:349] Evaluating on the test split.
I0127 02:00:11.891257 139822745589568 submission_runner.py:408] Time since start: 25005.75s, 	Step: 70275, 	{'train/accuracy': 0.6885961294174194, 'train/loss': 1.343567132949829, 'validation/accuracy': 0.6320799589157104, 'validation/loss': 1.6132029294967651, 'validation/num_examples': 50000, 'test/accuracy': 0.5059000253677368, 'test/loss': 2.2884953022003174, 'test/num_examples': 10000, 'score': 24029.257930278778, 'total_duration': 25005.752648115158, 'accumulated_submission_time': 24029.257930278778, 'accumulated_eval_time': 971.5965132713318, 'accumulated_logging_time': 2.5620453357696533}
I0127 02:00:11.923249 139656700098304 logging_writer.py:48] [70275] accumulated_eval_time=971.596513, accumulated_logging_time=2.562045, accumulated_submission_time=24029.257930, global_step=70275, preemption_count=0, score=24029.257930, test/accuracy=0.505900, test/loss=2.288495, test/num_examples=10000, total_duration=25005.752648, train/accuracy=0.688596, train/loss=1.343567, validation/accuracy=0.632080, validation/loss=1.613203, validation/num_examples=50000
I0127 02:00:20.794515 139658730145536 logging_writer.py:48] [70300] global_step=70300, grad_norm=2.8708579540252686, loss=2.615931749343872
I0127 02:00:54.813421 139656700098304 logging_writer.py:48] [70400] global_step=70400, grad_norm=3.1855056285858154, loss=2.6305997371673584
I0127 02:01:28.859564 139658730145536 logging_writer.py:48] [70500] global_step=70500, grad_norm=3.3384366035461426, loss=2.5643906593322754
I0127 02:02:02.914070 139656700098304 logging_writer.py:48] [70600] global_step=70600, grad_norm=3.252624034881592, loss=2.61963152885437
I0127 02:02:36.987115 139658730145536 logging_writer.py:48] [70700] global_step=70700, grad_norm=3.1211836338043213, loss=2.7168545722961426
I0127 02:03:11.043463 139656700098304 logging_writer.py:48] [70800] global_step=70800, grad_norm=3.5015275478363037, loss=2.689666509628296
I0127 02:03:45.121637 139658730145536 logging_writer.py:48] [70900] global_step=70900, grad_norm=2.9882829189300537, loss=2.5828745365142822
I0127 02:04:19.184951 139656700098304 logging_writer.py:48] [71000] global_step=71000, grad_norm=3.268050193786621, loss=2.749650001525879
I0127 02:04:53.262210 139658730145536 logging_writer.py:48] [71100] global_step=71100, grad_norm=3.549804449081421, loss=2.5383975505828857
I0127 02:05:27.329559 139656700098304 logging_writer.py:48] [71200] global_step=71200, grad_norm=3.544595241546631, loss=2.6649258136749268
I0127 02:06:01.402863 139658730145536 logging_writer.py:48] [71300] global_step=71300, grad_norm=3.468046188354492, loss=2.6589343547821045
I0127 02:06:35.532626 139656700098304 logging_writer.py:48] [71400] global_step=71400, grad_norm=3.0568227767944336, loss=2.6665353775024414
I0127 02:07:09.606645 139658730145536 logging_writer.py:48] [71500] global_step=71500, grad_norm=3.2052268981933594, loss=2.722259521484375
I0127 02:07:43.678145 139656700098304 logging_writer.py:48] [71600] global_step=71600, grad_norm=3.0802040100097656, loss=2.575066566467285
I0127 02:08:17.701546 139658730145536 logging_writer.py:48] [71700] global_step=71700, grad_norm=2.8475425243377686, loss=2.6911094188690186
I0127 02:08:42.041089 139822745589568 spec.py:321] Evaluating on the training split.
I0127 02:08:48.260771 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 02:08:57.008447 139822745589568 spec.py:349] Evaluating on the test split.
I0127 02:08:59.279852 139822745589568 submission_runner.py:408] Time since start: 25533.14s, 	Step: 71773, 	{'train/accuracy': 0.6815010905265808, 'train/loss': 1.3888893127441406, 'validation/accuracy': 0.626259982585907, 'validation/loss': 1.6451306343078613, 'validation/num_examples': 50000, 'test/accuracy': 0.5006000399589539, 'test/loss': 2.317592144012451, 'test/num_examples': 10000, 'score': 24539.311646461487, 'total_duration': 25533.141258955002, 'accumulated_submission_time': 24539.311646461487, 'accumulated_eval_time': 988.8352327346802, 'accumulated_logging_time': 2.6068990230560303}
I0127 02:08:59.312191 139656649742080 logging_writer.py:48] [71773] accumulated_eval_time=988.835233, accumulated_logging_time=2.606899, accumulated_submission_time=24539.311646, global_step=71773, preemption_count=0, score=24539.311646, test/accuracy=0.500600, test/loss=2.317592, test/num_examples=10000, total_duration=25533.141259, train/accuracy=0.681501, train/loss=1.388889, validation/accuracy=0.626260, validation/loss=1.645131, validation/num_examples=50000
I0127 02:09:08.857181 139656658134784 logging_writer.py:48] [71800] global_step=71800, grad_norm=3.4847073554992676, loss=2.5848662853240967
I0127 02:09:42.854785 139656649742080 logging_writer.py:48] [71900] global_step=71900, grad_norm=3.1414637565612793, loss=2.5756173133850098
I0127 02:10:16.897248 139656658134784 logging_writer.py:48] [72000] global_step=72000, grad_norm=3.8396973609924316, loss=2.618645429611206
I0127 02:10:50.957024 139656649742080 logging_writer.py:48] [72100] global_step=72100, grad_norm=3.277366876602173, loss=2.584871530532837
I0127 02:11:25.005983 139656658134784 logging_writer.py:48] [72200] global_step=72200, grad_norm=3.342033863067627, loss=2.5907113552093506
I0127 02:11:59.085777 139656649742080 logging_writer.py:48] [72300] global_step=72300, grad_norm=3.0746264457702637, loss=2.7226898670196533
I0127 02:12:33.210369 139656658134784 logging_writer.py:48] [72400] global_step=72400, grad_norm=3.4016571044921875, loss=2.7246315479278564
I0127 02:13:07.267912 139656649742080 logging_writer.py:48] [72500] global_step=72500, grad_norm=3.14497709274292, loss=2.6539978981018066
I0127 02:13:41.332785 139656658134784 logging_writer.py:48] [72600] global_step=72600, grad_norm=3.29584002494812, loss=2.599175453186035
I0127 02:14:15.402414 139656649742080 logging_writer.py:48] [72700] global_step=72700, grad_norm=3.3186144828796387, loss=2.654906988143921
I0127 02:14:49.479185 139656658134784 logging_writer.py:48] [72800] global_step=72800, grad_norm=3.1419320106506348, loss=2.6048803329467773
I0127 02:15:23.548213 139656649742080 logging_writer.py:48] [72900] global_step=72900, grad_norm=3.2671306133270264, loss=2.655202865600586
I0127 02:15:57.613507 139656658134784 logging_writer.py:48] [73000] global_step=73000, grad_norm=3.3714756965637207, loss=2.639650821685791
I0127 02:16:31.688705 139656649742080 logging_writer.py:48] [73100] global_step=73100, grad_norm=3.4361183643341064, loss=2.701338768005371
I0127 02:17:05.730567 139656658134784 logging_writer.py:48] [73200] global_step=73200, grad_norm=3.777233362197876, loss=2.7296297550201416
I0127 02:17:29.379366 139822745589568 spec.py:321] Evaluating on the training split.
I0127 02:17:35.613128 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 02:17:44.627175 139822745589568 spec.py:349] Evaluating on the test split.
I0127 02:17:46.914352 139822745589568 submission_runner.py:408] Time since start: 26060.78s, 	Step: 73271, 	{'train/accuracy': 0.6846500039100647, 'train/loss': 1.3535865545272827, 'validation/accuracy': 0.6345799565315247, 'validation/loss': 1.5897456407546997, 'validation/num_examples': 50000, 'test/accuracy': 0.5053000450134277, 'test/loss': 2.248891592025757, 'test/num_examples': 10000, 'score': 25049.316918611526, 'total_duration': 26060.775750637054, 'accumulated_submission_time': 25049.316918611526, 'accumulated_eval_time': 1006.3702020645142, 'accumulated_logging_time': 2.651460647583008}
I0127 02:17:46.943887 139656691705600 logging_writer.py:48] [73271] accumulated_eval_time=1006.370202, accumulated_logging_time=2.651461, accumulated_submission_time=25049.316919, global_step=73271, preemption_count=0, score=25049.316919, test/accuracy=0.505300, test/loss=2.248892, test/num_examples=10000, total_duration=26060.775751, train/accuracy=0.684650, train/loss=1.353587, validation/accuracy=0.634580, validation/loss=1.589746, validation/num_examples=50000
I0127 02:17:57.163648 139656700098304 logging_writer.py:48] [73300] global_step=73300, grad_norm=3.6475634574890137, loss=2.7419419288635254
I0127 02:18:31.144671 139656691705600 logging_writer.py:48] [73400] global_step=73400, grad_norm=3.406797409057617, loss=2.5512819290161133
I0127 02:19:05.244413 139656700098304 logging_writer.py:48] [73500] global_step=73500, grad_norm=3.245750904083252, loss=2.6989521980285645
I0127 02:19:39.309966 139656691705600 logging_writer.py:48] [73600] global_step=73600, grad_norm=2.915520668029785, loss=2.543409824371338
I0127 02:20:13.363760 139656700098304 logging_writer.py:48] [73700] global_step=73700, grad_norm=3.5919508934020996, loss=2.6159656047821045
I0127 02:20:47.449358 139656691705600 logging_writer.py:48] [73800] global_step=73800, grad_norm=3.019855260848999, loss=2.562692165374756
I0127 02:21:21.505402 139656700098304 logging_writer.py:48] [73900] global_step=73900, grad_norm=3.60601544380188, loss=2.617077589035034
I0127 02:21:55.563504 139656691705600 logging_writer.py:48] [74000] global_step=74000, grad_norm=4.00433349609375, loss=2.658902168273926
I0127 02:22:29.643831 139656700098304 logging_writer.py:48] [74100] global_step=74100, grad_norm=2.7860138416290283, loss=2.5314743518829346
I0127 02:23:03.707105 139656691705600 logging_writer.py:48] [74200] global_step=74200, grad_norm=2.856619119644165, loss=2.50647234916687
I0127 02:23:37.768233 139656700098304 logging_writer.py:48] [74300] global_step=74300, grad_norm=3.137542247772217, loss=2.6331465244293213
I0127 02:24:11.830024 139656691705600 logging_writer.py:48] [74400] global_step=74400, grad_norm=3.4271492958068848, loss=2.564539909362793
I0127 02:24:45.891376 139656700098304 logging_writer.py:48] [74500] global_step=74500, grad_norm=3.7165558338165283, loss=2.6153111457824707
I0127 02:25:19.990610 139656691705600 logging_writer.py:48] [74600] global_step=74600, grad_norm=3.5753111839294434, loss=2.6586718559265137
I0127 02:25:54.049360 139656700098304 logging_writer.py:48] [74700] global_step=74700, grad_norm=3.3871400356292725, loss=2.5710906982421875
I0127 02:26:16.979226 139822745589568 spec.py:321] Evaluating on the training split.
I0127 02:26:23.782384 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 02:26:32.395452 139822745589568 spec.py:349] Evaluating on the test split.
I0127 02:26:34.634551 139822745589568 submission_runner.py:408] Time since start: 26588.50s, 	Step: 74769, 	{'train/accuracy': 0.6935586333274841, 'train/loss': 1.3076993227005005, 'validation/accuracy': 0.6421399712562561, 'validation/loss': 1.5479406118392944, 'validation/num_examples': 50000, 'test/accuracy': 0.5216000080108643, 'test/loss': 2.2021830081939697, 'test/num_examples': 10000, 'score': 25559.289101839066, 'total_duration': 26588.49595093727, 'accumulated_submission_time': 25559.289101839066, 'accumulated_eval_time': 1024.0254747867584, 'accumulated_logging_time': 2.6931560039520264}
I0127 02:26:34.667137 139656666527488 logging_writer.py:48] [74769] accumulated_eval_time=1024.025475, accumulated_logging_time=2.693156, accumulated_submission_time=25559.289102, global_step=74769, preemption_count=0, score=25559.289102, test/accuracy=0.521600, test/loss=2.202183, test/num_examples=10000, total_duration=26588.495951, train/accuracy=0.693559, train/loss=1.307699, validation/accuracy=0.642140, validation/loss=1.547941, validation/num_examples=50000
I0127 02:26:45.557207 139656674920192 logging_writer.py:48] [74800] global_step=74800, grad_norm=3.2524755001068115, loss=2.690129041671753
I0127 02:27:19.560101 139656666527488 logging_writer.py:48] [74900] global_step=74900, grad_norm=3.423910140991211, loss=2.588355779647827
I0127 02:27:53.577036 139656674920192 logging_writer.py:48] [75000] global_step=75000, grad_norm=3.0684473514556885, loss=2.5658960342407227
I0127 02:28:27.637531 139656666527488 logging_writer.py:48] [75100] global_step=75100, grad_norm=3.3635036945343018, loss=2.6266427040100098
I0127 02:29:01.688464 139656674920192 logging_writer.py:48] [75200] global_step=75200, grad_norm=3.759256601333618, loss=2.640018939971924
I0127 02:29:35.746864 139656666527488 logging_writer.py:48] [75300] global_step=75300, grad_norm=3.631359338760376, loss=2.5941555500030518
I0127 02:30:09.806289 139656674920192 logging_writer.py:48] [75400] global_step=75400, grad_norm=2.915865898132324, loss=2.6158061027526855
I0127 02:30:43.878419 139656666527488 logging_writer.py:48] [75500] global_step=75500, grad_norm=3.4688074588775635, loss=2.6431496143341064
I0127 02:31:18.008002 139656674920192 logging_writer.py:48] [75600] global_step=75600, grad_norm=3.278050422668457, loss=2.7058281898498535
I0127 02:31:52.104501 139656666527488 logging_writer.py:48] [75700] global_step=75700, grad_norm=3.4224021434783936, loss=2.668442964553833
I0127 02:32:26.185535 139656674920192 logging_writer.py:48] [75800] global_step=75800, grad_norm=3.5518882274627686, loss=2.6553359031677246
I0127 02:33:00.274741 139656666527488 logging_writer.py:48] [75900] global_step=75900, grad_norm=3.7186641693115234, loss=2.61708664894104
I0127 02:33:34.359430 139656674920192 logging_writer.py:48] [76000] global_step=76000, grad_norm=3.2629268169403076, loss=2.65691876411438
I0127 02:34:08.431234 139656666527488 logging_writer.py:48] [76100] global_step=76100, grad_norm=3.7157349586486816, loss=2.599827766418457
I0127 02:34:42.492579 139656674920192 logging_writer.py:48] [76200] global_step=76200, grad_norm=4.075927734375, loss=2.595327377319336
I0127 02:35:04.763251 139822745589568 spec.py:321] Evaluating on the training split.
I0127 02:35:10.965102 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 02:35:19.747043 139822745589568 spec.py:349] Evaluating on the test split.
I0127 02:35:22.043518 139822745589568 submission_runner.py:408] Time since start: 27115.90s, 	Step: 76267, 	{'train/accuracy': 0.6836535334587097, 'train/loss': 1.3648991584777832, 'validation/accuracy': 0.6360799670219421, 'validation/loss': 1.5922093391418457, 'validation/num_examples': 50000, 'test/accuracy': 0.5103999972343445, 'test/loss': 2.274660348892212, 'test/num_examples': 10000, 'score': 26069.32606625557, 'total_duration': 27115.904922246933, 'accumulated_submission_time': 26069.32606625557, 'accumulated_eval_time': 1041.3056933879852, 'accumulated_logging_time': 2.7349095344543457}
I0127 02:35:22.078815 139656658134784 logging_writer.py:48] [76267] accumulated_eval_time=1041.305693, accumulated_logging_time=2.734910, accumulated_submission_time=26069.326066, global_step=76267, preemption_count=0, score=26069.326066, test/accuracy=0.510400, test/loss=2.274660, test/num_examples=10000, total_duration=27115.904922, train/accuracy=0.683654, train/loss=1.364899, validation/accuracy=0.636080, validation/loss=1.592209, validation/num_examples=50000
I0127 02:35:33.633559 139656683312896 logging_writer.py:48] [76300] global_step=76300, grad_norm=3.1420164108276367, loss=2.6020755767822266
I0127 02:36:07.653383 139656658134784 logging_writer.py:48] [76400] global_step=76400, grad_norm=3.4967968463897705, loss=2.699406385421753
I0127 02:36:41.680178 139656683312896 logging_writer.py:48] [76500] global_step=76500, grad_norm=3.6166799068450928, loss=2.5356669425964355
I0127 02:37:15.730815 139656658134784 logging_writer.py:48] [76600] global_step=76600, grad_norm=3.659996271133423, loss=2.613689422607422
I0127 02:37:49.876794 139656683312896 logging_writer.py:48] [76700] global_step=76700, grad_norm=3.73669695854187, loss=2.6540329456329346
I0127 02:38:23.942382 139656658134784 logging_writer.py:48] [76800] global_step=76800, grad_norm=3.7074289321899414, loss=2.596545696258545
I0127 02:38:58.009560 139656683312896 logging_writer.py:48] [76900] global_step=76900, grad_norm=3.445525646209717, loss=2.6916701793670654
I0127 02:39:32.323941 139656658134784 logging_writer.py:48] [77000] global_step=77000, grad_norm=3.215203046798706, loss=2.602280616760254
I0127 02:40:06.391533 139656683312896 logging_writer.py:48] [77100] global_step=77100, grad_norm=2.9445676803588867, loss=2.6190145015716553
I0127 02:40:40.467461 139656658134784 logging_writer.py:48] [77200] global_step=77200, grad_norm=4.038817882537842, loss=2.684803009033203
I0127 02:41:14.515099 139656683312896 logging_writer.py:48] [77300] global_step=77300, grad_norm=3.9009010791778564, loss=2.6400036811828613
I0127 02:41:48.577222 139656658134784 logging_writer.py:48] [77400] global_step=77400, grad_norm=3.180424451828003, loss=2.658212900161743
I0127 02:42:22.628104 139656683312896 logging_writer.py:48] [77500] global_step=77500, grad_norm=3.1913318634033203, loss=2.5944483280181885
I0127 02:42:56.684756 139656658134784 logging_writer.py:48] [77600] global_step=77600, grad_norm=3.2476727962493896, loss=2.5607850551605225
I0127 02:43:30.746966 139656683312896 logging_writer.py:48] [77700] global_step=77700, grad_norm=3.87330961227417, loss=2.679654836654663
I0127 02:43:52.347806 139822745589568 spec.py:321] Evaluating on the training split.
I0127 02:43:58.629272 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 02:44:07.343737 139822745589568 spec.py:349] Evaluating on the test split.
I0127 02:44:09.645556 139822745589568 submission_runner.py:408] Time since start: 27643.51s, 	Step: 77765, 	{'train/accuracy': 0.7120934128761292, 'train/loss': 1.2422109842300415, 'validation/accuracy': 0.638700008392334, 'validation/loss': 1.5625419616699219, 'validation/num_examples': 50000, 'test/accuracy': 0.5115000009536743, 'test/loss': 2.2503161430358887, 'test/num_examples': 10000, 'score': 26579.532474040985, 'total_duration': 27643.506959199905, 'accumulated_submission_time': 26579.532474040985, 'accumulated_eval_time': 1058.6034083366394, 'accumulated_logging_time': 2.780672311782837}
I0127 02:44:09.675959 139656297445120 logging_writer.py:48] [77765] accumulated_eval_time=1058.603408, accumulated_logging_time=2.780672, accumulated_submission_time=26579.532474, global_step=77765, preemption_count=0, score=26579.532474, test/accuracy=0.511500, test/loss=2.250316, test/num_examples=10000, total_duration=27643.506959, train/accuracy=0.712093, train/loss=1.242211, validation/accuracy=0.638700, validation/loss=1.562542, validation/num_examples=50000
I0127 02:44:21.971346 139656649742080 logging_writer.py:48] [77800] global_step=77800, grad_norm=3.5377793312072754, loss=2.635721206665039
I0127 02:44:56.009061 139656297445120 logging_writer.py:48] [77900] global_step=77900, grad_norm=3.2965807914733887, loss=2.6337666511535645
I0127 02:45:30.074631 139656649742080 logging_writer.py:48] [78000] global_step=78000, grad_norm=3.5386970043182373, loss=2.6160640716552734
I0127 02:46:04.169534 139656297445120 logging_writer.py:48] [78100] global_step=78100, grad_norm=3.8547935485839844, loss=2.64211368560791
I0127 02:46:38.256784 139656649742080 logging_writer.py:48] [78200] global_step=78200, grad_norm=3.0455665588378906, loss=2.577796459197998
I0127 02:47:12.345966 139656297445120 logging_writer.py:48] [78300] global_step=78300, grad_norm=3.123042106628418, loss=2.556988477706909
I0127 02:47:46.424340 139656649742080 logging_writer.py:48] [78400] global_step=78400, grad_norm=3.907536745071411, loss=2.6152162551879883
I0127 02:48:20.501760 139656297445120 logging_writer.py:48] [78500] global_step=78500, grad_norm=3.163329601287842, loss=2.5876669883728027
I0127 02:48:54.581868 139656649742080 logging_writer.py:48] [78600] global_step=78600, grad_norm=3.4245076179504395, loss=2.6523611545562744
I0127 02:49:28.656456 139656297445120 logging_writer.py:48] [78700] global_step=78700, grad_norm=3.0805389881134033, loss=2.4936230182647705
I0127 02:50:02.748144 139656649742080 logging_writer.py:48] [78800] global_step=78800, grad_norm=3.396883010864258, loss=2.5987448692321777
I0127 02:50:36.789713 139656297445120 logging_writer.py:48] [78900] global_step=78900, grad_norm=3.3167452812194824, loss=2.6080820560455322
I0127 02:51:10.839142 139656649742080 logging_writer.py:48] [79000] global_step=79000, grad_norm=3.539722442626953, loss=2.6848559379577637
I0127 02:51:44.908740 139656297445120 logging_writer.py:48] [79100] global_step=79100, grad_norm=3.5183656215667725, loss=2.550534963607788
I0127 02:52:18.958728 139656649742080 logging_writer.py:48] [79200] global_step=79200, grad_norm=3.6009931564331055, loss=2.761305570602417
I0127 02:52:39.889238 139822745589568 spec.py:321] Evaluating on the training split.
I0127 02:52:46.076856 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 02:52:54.809099 139822745589568 spec.py:349] Evaluating on the test split.
I0127 02:52:57.085656 139822745589568 submission_runner.py:408] Time since start: 28170.95s, 	Step: 79263, 	{'train/accuracy': 0.7058752775192261, 'train/loss': 1.2854481935501099, 'validation/accuracy': 0.6415599584579468, 'validation/loss': 1.569705605506897, 'validation/num_examples': 50000, 'test/accuracy': 0.5182999968528748, 'test/loss': 2.221109390258789, 'test/num_examples': 10000, 'score': 27089.686541080475, 'total_duration': 28170.947011709213, 'accumulated_submission_time': 27089.686541080475, 'accumulated_eval_time': 1075.7997291088104, 'accumulated_logging_time': 2.820523977279663}
I0127 02:52:57.121035 139656691705600 logging_writer.py:48] [79263] accumulated_eval_time=1075.799729, accumulated_logging_time=2.820524, accumulated_submission_time=27089.686541, global_step=79263, preemption_count=0, score=27089.686541, test/accuracy=0.518300, test/loss=2.221109, test/num_examples=10000, total_duration=28170.947012, train/accuracy=0.705875, train/loss=1.285448, validation/accuracy=0.641560, validation/loss=1.569706, validation/num_examples=50000
I0127 02:53:10.040074 139656700098304 logging_writer.py:48] [79300] global_step=79300, grad_norm=3.765334367752075, loss=2.652064561843872
I0127 02:53:44.038471 139656691705600 logging_writer.py:48] [79400] global_step=79400, grad_norm=3.6122524738311768, loss=2.53572678565979
I0127 02:54:18.071024 139656700098304 logging_writer.py:48] [79500] global_step=79500, grad_norm=3.5045583248138428, loss=2.61517596244812
I0127 02:54:52.128676 139656691705600 logging_writer.py:48] [79600] global_step=79600, grad_norm=3.8757996559143066, loss=2.5508928298950195
I0127 02:55:26.167648 139656700098304 logging_writer.py:48] [79700] global_step=79700, grad_norm=3.2949330806732178, loss=2.500154495239258
I0127 02:56:00.193203 139656691705600 logging_writer.py:48] [79800] global_step=79800, grad_norm=3.427471876144409, loss=2.546243667602539
I0127 02:56:34.292355 139656700098304 logging_writer.py:48] [79900] global_step=79900, grad_norm=4.004252910614014, loss=2.5280752182006836
I0127 02:57:08.350014 139656691705600 logging_writer.py:48] [80000] global_step=80000, grad_norm=3.1984386444091797, loss=2.6536993980407715
I0127 02:57:42.408743 139656700098304 logging_writer.py:48] [80100] global_step=80100, grad_norm=3.585839033126831, loss=2.7169015407562256
I0127 02:58:16.462006 139656691705600 logging_writer.py:48] [80200] global_step=80200, grad_norm=2.904094934463501, loss=2.5629658699035645
I0127 02:58:50.491283 139656700098304 logging_writer.py:48] [80300] global_step=80300, grad_norm=3.508413076400757, loss=2.679244041442871
I0127 02:59:24.537117 139656691705600 logging_writer.py:48] [80400] global_step=80400, grad_norm=3.6383092403411865, loss=2.5642459392547607
I0127 02:59:58.580759 139656700098304 logging_writer.py:48] [80500] global_step=80500, grad_norm=4.295048713684082, loss=2.6258440017700195
I0127 03:00:32.617286 139656691705600 logging_writer.py:48] [80600] global_step=80600, grad_norm=3.348641872406006, loss=2.6065614223480225
I0127 03:01:06.682726 139656700098304 logging_writer.py:48] [80700] global_step=80700, grad_norm=4.148528575897217, loss=2.5420022010803223
I0127 03:01:27.262613 139822745589568 spec.py:321] Evaluating on the training split.
I0127 03:01:33.509070 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 03:01:42.189120 139822745589568 spec.py:349] Evaluating on the test split.
I0127 03:01:44.400623 139822745589568 submission_runner.py:408] Time since start: 28698.26s, 	Step: 80762, 	{'train/accuracy': 0.7010523080825806, 'train/loss': 1.3063050508499146, 'validation/accuracy': 0.648140013217926, 'validation/loss': 1.5532654523849487, 'validation/num_examples': 50000, 'test/accuracy': 0.5182999968528748, 'test/loss': 2.2196383476257324, 'test/num_examples': 10000, 'score': 27599.767220258713, 'total_duration': 28698.262020349503, 'accumulated_submission_time': 27599.767220258713, 'accumulated_eval_time': 1092.9376814365387, 'accumulated_logging_time': 2.8669273853302}
I0127 03:01:44.430281 139656666527488 logging_writer.py:48] [80762] accumulated_eval_time=1092.937681, accumulated_logging_time=2.866927, accumulated_submission_time=27599.767220, global_step=80762, preemption_count=0, score=27599.767220, test/accuracy=0.518300, test/loss=2.219638, test/num_examples=10000, total_duration=28698.262020, train/accuracy=0.701052, train/loss=1.306305, validation/accuracy=0.648140, validation/loss=1.553265, validation/num_examples=50000
I0127 03:01:57.720293 139656674920192 logging_writer.py:48] [80800] global_step=80800, grad_norm=3.2649648189544678, loss=2.526474952697754
I0127 03:02:31.802451 139656666527488 logging_writer.py:48] [80900] global_step=80900, grad_norm=2.885377883911133, loss=2.56988263130188
I0127 03:03:05.855112 139656674920192 logging_writer.py:48] [81000] global_step=81000, grad_norm=3.265397071838379, loss=2.6160330772399902
I0127 03:03:39.930155 139656666527488 logging_writer.py:48] [81100] global_step=81100, grad_norm=3.1060168743133545, loss=2.496035099029541
I0127 03:04:13.981369 139656674920192 logging_writer.py:48] [81200] global_step=81200, grad_norm=3.3704071044921875, loss=2.4743244647979736
I0127 03:04:48.060698 139656666527488 logging_writer.py:48] [81300] global_step=81300, grad_norm=3.080854654312134, loss=2.623060941696167
I0127 03:05:22.124941 139656674920192 logging_writer.py:48] [81400] global_step=81400, grad_norm=3.5597119331359863, loss=2.6736297607421875
I0127 03:05:56.211594 139656666527488 logging_writer.py:48] [81500] global_step=81500, grad_norm=3.2306554317474365, loss=2.7112245559692383
I0127 03:06:30.266932 139656674920192 logging_writer.py:48] [81600] global_step=81600, grad_norm=3.2398505210876465, loss=2.601266622543335
I0127 03:07:04.314421 139656666527488 logging_writer.py:48] [81700] global_step=81700, grad_norm=3.254660129547119, loss=2.575565814971924
I0127 03:07:38.347866 139656674920192 logging_writer.py:48] [81800] global_step=81800, grad_norm=4.387158393859863, loss=2.642774820327759
I0127 03:08:12.376057 139656666527488 logging_writer.py:48] [81900] global_step=81900, grad_norm=3.720398187637329, loss=2.6267290115356445
I0127 03:08:46.471256 139656674920192 logging_writer.py:48] [82000] global_step=82000, grad_norm=3.19071364402771, loss=2.5755953788757324
I0127 03:09:20.523871 139656666527488 logging_writer.py:48] [82100] global_step=82100, grad_norm=3.973766326904297, loss=2.5346570014953613
I0127 03:09:54.578047 139656674920192 logging_writer.py:48] [82200] global_step=82200, grad_norm=3.6946351528167725, loss=2.588623046875
I0127 03:10:14.487438 139822745589568 spec.py:321] Evaluating on the training split.
I0127 03:10:20.700775 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 03:10:29.358256 139822745589568 spec.py:349] Evaluating on the test split.
I0127 03:10:31.638623 139822745589568 submission_runner.py:408] Time since start: 29225.50s, 	Step: 82260, 	{'train/accuracy': 0.6965281963348389, 'train/loss': 1.2849880456924438, 'validation/accuracy': 0.64656001329422, 'validation/loss': 1.5245449542999268, 'validation/num_examples': 50000, 'test/accuracy': 0.5174000263214111, 'test/loss': 2.2001779079437256, 'test/num_examples': 10000, 'score': 28109.76361966133, 'total_duration': 29225.500024795532, 'accumulated_submission_time': 28109.76361966133, 'accumulated_eval_time': 1110.088816165924, 'accumulated_logging_time': 2.9066665172576904}
I0127 03:10:31.668829 139656691705600 logging_writer.py:48] [82260] accumulated_eval_time=1110.088816, accumulated_logging_time=2.906667, accumulated_submission_time=28109.763620, global_step=82260, preemption_count=0, score=28109.763620, test/accuracy=0.517400, test/loss=2.200178, test/num_examples=10000, total_duration=29225.500025, train/accuracy=0.696528, train/loss=1.284988, validation/accuracy=0.646560, validation/loss=1.524545, validation/num_examples=50000
I0127 03:10:45.628708 139656700098304 logging_writer.py:48] [82300] global_step=82300, grad_norm=3.3648645877838135, loss=2.6107234954833984
I0127 03:11:19.613088 139656691705600 logging_writer.py:48] [82400] global_step=82400, grad_norm=3.2448246479034424, loss=2.597705364227295
I0127 03:11:53.646634 139656700098304 logging_writer.py:48] [82500] global_step=82500, grad_norm=3.1954078674316406, loss=2.675426959991455
I0127 03:12:27.683542 139656691705600 logging_writer.py:48] [82600] global_step=82600, grad_norm=3.3573358058929443, loss=2.5800375938415527
I0127 03:13:01.717497 139656700098304 logging_writer.py:48] [82700] global_step=82700, grad_norm=3.4763646125793457, loss=2.632202625274658
I0127 03:13:35.787678 139656691705600 logging_writer.py:48] [82800] global_step=82800, grad_norm=4.07562780380249, loss=2.5301618576049805
I0127 03:14:09.851978 139656700098304 logging_writer.py:48] [82900] global_step=82900, grad_norm=3.7940542697906494, loss=2.649474859237671
I0127 03:14:43.939627 139656691705600 logging_writer.py:48] [83000] global_step=83000, grad_norm=3.169123888015747, loss=2.6032752990722656
I0127 03:15:18.073822 139656700098304 logging_writer.py:48] [83100] global_step=83100, grad_norm=3.524595260620117, loss=2.6016788482666016
I0127 03:15:52.139307 139656691705600 logging_writer.py:48] [83200] global_step=83200, grad_norm=3.312117099761963, loss=2.4967427253723145
I0127 03:16:26.180330 139656700098304 logging_writer.py:48] [83300] global_step=83300, grad_norm=3.9674010276794434, loss=2.673055648803711
I0127 03:17:00.266603 139656691705600 logging_writer.py:48] [83400] global_step=83400, grad_norm=3.726050615310669, loss=2.5992329120635986
I0127 03:17:34.345746 139656700098304 logging_writer.py:48] [83500] global_step=83500, grad_norm=3.108640193939209, loss=2.571467876434326
I0127 03:18:08.420144 139656691705600 logging_writer.py:48] [83600] global_step=83600, grad_norm=3.4778196811676025, loss=2.576010227203369
I0127 03:18:42.486963 139656700098304 logging_writer.py:48] [83700] global_step=83700, grad_norm=3.617406129837036, loss=2.7187371253967285
I0127 03:19:01.734417 139822745589568 spec.py:321] Evaluating on the training split.
I0127 03:19:07.955052 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 03:19:16.522339 139822745589568 spec.py:349] Evaluating on the test split.
I0127 03:19:18.802825 139822745589568 submission_runner.py:408] Time since start: 29752.66s, 	Step: 83758, 	{'train/accuracy': 0.7009526491165161, 'train/loss': 1.2806954383850098, 'validation/accuracy': 0.6505799889564514, 'validation/loss': 1.5104814767837524, 'validation/num_examples': 50000, 'test/accuracy': 0.5253000259399414, 'test/loss': 2.1855990886688232, 'test/num_examples': 10000, 'score': 28619.767454862595, 'total_duration': 29752.664216518402, 'accumulated_submission_time': 28619.767454862595, 'accumulated_eval_time': 1127.1571650505066, 'accumulated_logging_time': 2.946876287460327}
I0127 03:19:18.836939 139656297445120 logging_writer.py:48] [83758] accumulated_eval_time=1127.157165, accumulated_logging_time=2.946876, accumulated_submission_time=28619.767455, global_step=83758, preemption_count=0, score=28619.767455, test/accuracy=0.525300, test/loss=2.185599, test/num_examples=10000, total_duration=29752.664217, train/accuracy=0.700953, train/loss=1.280695, validation/accuracy=0.650580, validation/loss=1.510481, validation/num_examples=50000
I0127 03:19:33.459973 139656649742080 logging_writer.py:48] [83800] global_step=83800, grad_norm=3.2588400840759277, loss=2.662227153778076
I0127 03:20:07.481744 139656297445120 logging_writer.py:48] [83900] global_step=83900, grad_norm=3.7295467853546143, loss=2.641796827316284
I0127 03:20:41.532357 139656649742080 logging_writer.py:48] [84000] global_step=84000, grad_norm=3.483687400817871, loss=2.6465039253234863
I0127 03:21:15.598436 139656297445120 logging_writer.py:48] [84100] global_step=84100, grad_norm=3.529524087905884, loss=2.622696876525879
I0127 03:21:49.637263 139656649742080 logging_writer.py:48] [84200] global_step=84200, grad_norm=3.8151402473449707, loss=2.6222968101501465
I0127 03:22:23.700350 139656297445120 logging_writer.py:48] [84300] global_step=84300, grad_norm=3.5757036209106445, loss=2.6022415161132812
I0127 03:22:57.767283 139656649742080 logging_writer.py:48] [84400] global_step=84400, grad_norm=3.5710885524749756, loss=2.6186647415161133
I0127 03:23:31.833491 139656297445120 logging_writer.py:48] [84500] global_step=84500, grad_norm=3.6320383548736572, loss=2.6069724559783936
I0127 03:24:05.884413 139656649742080 logging_writer.py:48] [84600] global_step=84600, grad_norm=4.146963596343994, loss=2.5883922576904297
I0127 03:24:39.941456 139656297445120 logging_writer.py:48] [84700] global_step=84700, grad_norm=3.3588662147521973, loss=2.6182057857513428
I0127 03:25:13.986507 139656649742080 logging_writer.py:48] [84800] global_step=84800, grad_norm=3.3975868225097656, loss=2.5258982181549072
I0127 03:25:48.006470 139656297445120 logging_writer.py:48] [84900] global_step=84900, grad_norm=3.374290704727173, loss=2.617124557495117
I0127 03:26:22.048383 139656649742080 logging_writer.py:48] [85000] global_step=85000, grad_norm=3.377333879470825, loss=2.5892534255981445
I0127 03:26:56.091865 139656297445120 logging_writer.py:48] [85100] global_step=85100, grad_norm=3.2505078315734863, loss=2.6667251586914062
I0127 03:27:30.161334 139656649742080 logging_writer.py:48] [85200] global_step=85200, grad_norm=3.4079127311706543, loss=2.580148935317993
I0127 03:27:49.038686 139822745589568 spec.py:321] Evaluating on the training split.
I0127 03:27:55.277098 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 03:28:04.187213 139822745589568 spec.py:349] Evaluating on the test split.
I0127 03:28:06.462964 139822745589568 submission_runner.py:408] Time since start: 30280.32s, 	Step: 85257, 	{'train/accuracy': 0.7001753449440002, 'train/loss': 1.2842198610305786, 'validation/accuracy': 0.6523399949073792, 'validation/loss': 1.5107489824295044, 'validation/num_examples': 50000, 'test/accuracy': 0.5213000178337097, 'test/loss': 2.2066726684570312, 'test/num_examples': 10000, 'score': 29129.908967256546, 'total_duration': 30280.324359178543, 'accumulated_submission_time': 29129.908967256546, 'accumulated_eval_time': 1144.5813839435577, 'accumulated_logging_time': 2.9912562370300293}
I0127 03:28:06.496629 139656649742080 logging_writer.py:48] [85257] accumulated_eval_time=1144.581384, accumulated_logging_time=2.991256, accumulated_submission_time=29129.908967, global_step=85257, preemption_count=0, score=29129.908967, test/accuracy=0.521300, test/loss=2.206673, test/num_examples=10000, total_duration=30280.324359, train/accuracy=0.700175, train/loss=1.284220, validation/accuracy=0.652340, validation/loss=1.510749, validation/num_examples=50000
I0127 03:28:21.441400 139656658134784 logging_writer.py:48] [85300] global_step=85300, grad_norm=3.416607618331909, loss=2.5563199520111084
I0127 03:28:55.453551 139656649742080 logging_writer.py:48] [85400] global_step=85400, grad_norm=3.772700548171997, loss=2.528876304626465
I0127 03:29:29.494364 139656658134784 logging_writer.py:48] [85500] global_step=85500, grad_norm=3.2501657009124756, loss=2.5344645977020264
I0127 03:30:03.575580 139656649742080 logging_writer.py:48] [85600] global_step=85600, grad_norm=3.1196975708007812, loss=2.5132522583007812
I0127 03:30:37.645360 139656658134784 logging_writer.py:48] [85700] global_step=85700, grad_norm=4.338793754577637, loss=2.5945534706115723
I0127 03:31:11.690041 139656649742080 logging_writer.py:48] [85800] global_step=85800, grad_norm=3.655139923095703, loss=2.5658531188964844
I0127 03:31:45.785851 139656658134784 logging_writer.py:48] [85900] global_step=85900, grad_norm=3.368900775909424, loss=2.575963020324707
I0127 03:32:19.844028 139656649742080 logging_writer.py:48] [86000] global_step=86000, grad_norm=3.5930774211883545, loss=2.5571939945220947
I0127 03:32:53.879282 139656658134784 logging_writer.py:48] [86100] global_step=86100, grad_norm=3.353541374206543, loss=2.6331064701080322
I0127 03:33:27.982356 139656649742080 logging_writer.py:48] [86200] global_step=86200, grad_norm=4.47698974609375, loss=2.6680121421813965
I0127 03:34:02.042141 139656658134784 logging_writer.py:48] [86300] global_step=86300, grad_norm=4.1951494216918945, loss=2.500633478164673
I0127 03:34:36.110531 139656649742080 logging_writer.py:48] [86400] global_step=86400, grad_norm=3.2341151237487793, loss=2.582655429840088
I0127 03:35:10.183616 139656658134784 logging_writer.py:48] [86500] global_step=86500, grad_norm=3.7900383472442627, loss=2.5907161235809326
I0127 03:35:44.232600 139656649742080 logging_writer.py:48] [86600] global_step=86600, grad_norm=3.6654789447784424, loss=2.594403028488159
I0127 03:36:18.283477 139656658134784 logging_writer.py:48] [86700] global_step=86700, grad_norm=3.5542304515838623, loss=2.6381261348724365
I0127 03:36:36.477423 139822745589568 spec.py:321] Evaluating on the training split.
I0127 03:36:42.677349 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 03:36:51.087369 139822745589568 spec.py:349] Evaluating on the test split.
I0127 03:36:53.380587 139822745589568 submission_runner.py:408] Time since start: 30807.24s, 	Step: 86755, 	{'train/accuracy': 0.7459940910339355, 'train/loss': 1.084036946296692, 'validation/accuracy': 0.6534799933433533, 'validation/loss': 1.4911538362503052, 'validation/num_examples': 50000, 'test/accuracy': 0.5329000353813171, 'test/loss': 2.1359615325927734, 'test/num_examples': 10000, 'score': 29639.830031633377, 'total_duration': 30807.24199461937, 'accumulated_submission_time': 29639.830031633377, 'accumulated_eval_time': 1161.4845032691956, 'accumulated_logging_time': 3.0346953868865967}
I0127 03:36:53.426249 139656658134784 logging_writer.py:48] [86755] accumulated_eval_time=1161.484503, accumulated_logging_time=3.034695, accumulated_submission_time=29639.830032, global_step=86755, preemption_count=0, score=29639.830032, test/accuracy=0.532900, test/loss=2.135962, test/num_examples=10000, total_duration=30807.241995, train/accuracy=0.745994, train/loss=1.084037, validation/accuracy=0.653480, validation/loss=1.491154, validation/num_examples=50000
I0127 03:37:09.093242 139656666527488 logging_writer.py:48] [86800] global_step=86800, grad_norm=3.4865033626556396, loss=2.5320358276367188
I0127 03:37:43.162494 139656658134784 logging_writer.py:48] [86900] global_step=86900, grad_norm=3.4968957901000977, loss=2.6198031902313232
I0127 03:38:17.235133 139656666527488 logging_writer.py:48] [87000] global_step=87000, grad_norm=3.556731939315796, loss=2.5655455589294434
I0127 03:38:51.304933 139656658134784 logging_writer.py:48] [87100] global_step=87100, grad_norm=3.5912985801696777, loss=2.55912709236145
I0127 03:39:25.378229 139656666527488 logging_writer.py:48] [87200] global_step=87200, grad_norm=3.5294249057769775, loss=2.533266544342041
I0127 03:39:59.481691 139656658134784 logging_writer.py:48] [87300] global_step=87300, grad_norm=4.331016540527344, loss=2.546276330947876
I0127 03:40:33.565604 139656666527488 logging_writer.py:48] [87400] global_step=87400, grad_norm=3.3055930137634277, loss=2.609179973602295
I0127 03:41:07.642280 139656658134784 logging_writer.py:48] [87500] global_step=87500, grad_norm=3.3544089794158936, loss=2.5049586296081543
I0127 03:41:41.699286 139656666527488 logging_writer.py:48] [87600] global_step=87600, grad_norm=3.7808890342712402, loss=2.5879733562469482
I0127 03:42:15.749333 139656658134784 logging_writer.py:48] [87700] global_step=87700, grad_norm=4.098466873168945, loss=2.5733466148376465
I0127 03:42:49.802006 139656666527488 logging_writer.py:48] [87800] global_step=87800, grad_norm=3.6805005073547363, loss=2.542546510696411
I0127 03:43:23.873736 139656658134784 logging_writer.py:48] [87900] global_step=87900, grad_norm=3.7915475368499756, loss=2.484661102294922
I0127 03:43:57.939168 139656666527488 logging_writer.py:48] [88000] global_step=88000, grad_norm=3.6306393146514893, loss=2.4429304599761963
I0127 03:44:32.011364 139656658134784 logging_writer.py:48] [88100] global_step=88100, grad_norm=3.604637384414673, loss=2.515970230102539
I0127 03:45:06.128331 139656666527488 logging_writer.py:48] [88200] global_step=88200, grad_norm=3.3194580078125, loss=2.565061569213867
I0127 03:45:23.663132 139822745589568 spec.py:321] Evaluating on the training split.
I0127 03:45:29.885626 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 03:45:38.460847 139822745589568 spec.py:349] Evaluating on the test split.
I0127 03:45:40.767843 139822745589568 submission_runner.py:408] Time since start: 31334.63s, 	Step: 88253, 	{'train/accuracy': 0.7254464030265808, 'train/loss': 1.1623930931091309, 'validation/accuracy': 0.6578199863433838, 'validation/loss': 1.4649604558944702, 'validation/num_examples': 50000, 'test/accuracy': 0.5353000164031982, 'test/loss': 2.1265015602111816, 'test/num_examples': 10000, 'score': 30150.005770921707, 'total_duration': 31334.62922167778, 'accumulated_submission_time': 30150.005770921707, 'accumulated_eval_time': 1178.5891389846802, 'accumulated_logging_time': 3.0897841453552246}
I0127 03:45:40.802782 139656297445120 logging_writer.py:48] [88253] accumulated_eval_time=1178.589139, accumulated_logging_time=3.089784, accumulated_submission_time=30150.005771, global_step=88253, preemption_count=0, score=30150.005771, test/accuracy=0.535300, test/loss=2.126502, test/num_examples=10000, total_duration=31334.629222, train/accuracy=0.725446, train/loss=1.162393, validation/accuracy=0.657820, validation/loss=1.464960, validation/num_examples=50000
I0127 03:45:57.210388 139656649742080 logging_writer.py:48] [88300] global_step=88300, grad_norm=4.03192138671875, loss=2.539203405380249
I0127 03:46:31.230762 139656297445120 logging_writer.py:48] [88400] global_step=88400, grad_norm=3.366760015487671, loss=2.5910980701446533
I0127 03:47:05.284012 139656649742080 logging_writer.py:48] [88500] global_step=88500, grad_norm=3.6531753540039062, loss=2.5605578422546387
I0127 03:47:39.350405 139656297445120 logging_writer.py:48] [88600] global_step=88600, grad_norm=3.4563679695129395, loss=2.5425894260406494
I0127 03:48:13.411425 139656649742080 logging_writer.py:48] [88700] global_step=88700, grad_norm=3.5900492668151855, loss=2.5748157501220703
I0127 03:48:47.469513 139656297445120 logging_writer.py:48] [88800] global_step=88800, grad_norm=3.648465633392334, loss=2.452353000640869
I0127 03:49:21.523909 139656649742080 logging_writer.py:48] [88900] global_step=88900, grad_norm=3.526183843612671, loss=2.6064400672912598
I0127 03:49:55.594951 139656297445120 logging_writer.py:48] [89000] global_step=89000, grad_norm=3.619957447052002, loss=2.5313942432403564
I0127 03:50:29.650146 139656649742080 logging_writer.py:48] [89100] global_step=89100, grad_norm=3.4824557304382324, loss=2.453479051589966
I0127 03:51:03.732991 139656297445120 logging_writer.py:48] [89200] global_step=89200, grad_norm=4.11392879486084, loss=2.539362907409668
I0127 03:51:37.794122 139656649742080 logging_writer.py:48] [89300] global_step=89300, grad_norm=3.491589069366455, loss=2.535308361053467
I0127 03:52:11.920855 139656297445120 logging_writer.py:48] [89400] global_step=89400, grad_norm=3.828113317489624, loss=2.506754159927368
I0127 03:52:45.989920 139656649742080 logging_writer.py:48] [89500] global_step=89500, grad_norm=3.5225625038146973, loss=2.475890874862671
I0127 03:53:20.071771 139656297445120 logging_writer.py:48] [89600] global_step=89600, grad_norm=3.5175631046295166, loss=2.5618367195129395
I0127 03:53:54.113011 139656649742080 logging_writer.py:48] [89700] global_step=89700, grad_norm=3.403531789779663, loss=2.455840587615967
I0127 03:54:10.951378 139822745589568 spec.py:321] Evaluating on the training split.
I0127 03:54:17.140663 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 03:54:25.930677 139822745589568 spec.py:349] Evaluating on the test split.
I0127 03:54:28.234430 139822745589568 submission_runner.py:408] Time since start: 31862.10s, 	Step: 89751, 	{'train/accuracy': 0.7215999364852905, 'train/loss': 1.1721854209899902, 'validation/accuracy': 0.6617000102996826, 'validation/loss': 1.4494667053222656, 'validation/num_examples': 50000, 'test/accuracy': 0.5382000207901001, 'test/loss': 2.107398748397827, 'test/num_examples': 10000, 'score': 30660.091092586517, 'total_duration': 31862.09580183029, 'accumulated_submission_time': 30660.091092586517, 'accumulated_eval_time': 1195.8721101284027, 'accumulated_logging_time': 3.1363139152526855}
I0127 03:54:28.267116 139656297445120 logging_writer.py:48] [89751] accumulated_eval_time=1195.872110, accumulated_logging_time=3.136314, accumulated_submission_time=30660.091093, global_step=89751, preemption_count=0, score=30660.091093, test/accuracy=0.538200, test/loss=2.107399, test/num_examples=10000, total_duration=31862.095802, train/accuracy=0.721600, train/loss=1.172185, validation/accuracy=0.661700, validation/loss=1.449467, validation/num_examples=50000
I0127 03:54:45.279056 139656658134784 logging_writer.py:48] [89800] global_step=89800, grad_norm=4.2275071144104, loss=2.4827356338500977
I0127 03:55:19.324626 139656297445120 logging_writer.py:48] [89900] global_step=89900, grad_norm=3.9507133960723877, loss=2.6311731338500977
I0127 03:55:53.366377 139656658134784 logging_writer.py:48] [90000] global_step=90000, grad_norm=3.360926389694214, loss=2.489600658416748
I0127 03:56:27.404186 139656297445120 logging_writer.py:48] [90100] global_step=90100, grad_norm=4.035330295562744, loss=2.609330892562866
I0127 03:57:01.433127 139656658134784 logging_writer.py:48] [90200] global_step=90200, grad_norm=3.3924593925476074, loss=2.5399622917175293
I0127 03:57:35.473849 139656297445120 logging_writer.py:48] [90300] global_step=90300, grad_norm=3.8965389728546143, loss=2.6083767414093018
I0127 03:58:09.670167 139656658134784 logging_writer.py:48] [90400] global_step=90400, grad_norm=3.327876329421997, loss=2.515692710876465
I0127 03:58:43.700806 139656297445120 logging_writer.py:48] [90500] global_step=90500, grad_norm=3.7983551025390625, loss=2.4815680980682373
I0127 03:59:17.718625 139656658134784 logging_writer.py:48] [90600] global_step=90600, grad_norm=3.6368374824523926, loss=2.493948221206665
I0127 03:59:51.767203 139656297445120 logging_writer.py:48] [90700] global_step=90700, grad_norm=3.4936580657958984, loss=2.636718511581421
I0127 04:00:25.827848 139656658134784 logging_writer.py:48] [90800] global_step=90800, grad_norm=3.44392466545105, loss=2.5449728965759277
I0127 04:00:59.924906 139656297445120 logging_writer.py:48] [90900] global_step=90900, grad_norm=3.8119494915008545, loss=2.4930989742279053
I0127 04:01:33.990234 139656658134784 logging_writer.py:48] [91000] global_step=91000, grad_norm=3.758816957473755, loss=2.638840913772583
I0127 04:02:08.072518 139656297445120 logging_writer.py:48] [91100] global_step=91100, grad_norm=4.10659122467041, loss=2.556053400039673
I0127 04:02:42.151811 139656658134784 logging_writer.py:48] [91200] global_step=91200, grad_norm=3.854595422744751, loss=2.4939849376678467
I0127 04:02:58.305947 139822745589568 spec.py:321] Evaluating on the training split.
I0127 04:03:04.639225 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 04:03:13.193158 139822745589568 spec.py:349] Evaluating on the test split.
I0127 04:03:15.458558 139822745589568 submission_runner.py:408] Time since start: 32389.32s, 	Step: 91249, 	{'train/accuracy': 0.7166972160339355, 'train/loss': 1.2218282222747803, 'validation/accuracy': 0.6592199802398682, 'validation/loss': 1.4793634414672852, 'validation/num_examples': 50000, 'test/accuracy': 0.5379000306129456, 'test/loss': 2.1280786991119385, 'test/num_examples': 10000, 'score': 31170.067274332047, 'total_duration': 32389.319952964783, 'accumulated_submission_time': 31170.067274332047, 'accumulated_eval_time': 1213.0246744155884, 'accumulated_logging_time': 3.181325912475586}
I0127 04:03:15.492702 139656691705600 logging_writer.py:48] [91249] accumulated_eval_time=1213.024674, accumulated_logging_time=3.181326, accumulated_submission_time=31170.067274, global_step=91249, preemption_count=0, score=31170.067274, test/accuracy=0.537900, test/loss=2.128079, test/num_examples=10000, total_duration=32389.319953, train/accuracy=0.716697, train/loss=1.221828, validation/accuracy=0.659220, validation/loss=1.479363, validation/num_examples=50000
I0127 04:03:33.182159 139656700098304 logging_writer.py:48] [91300] global_step=91300, grad_norm=4.179914474487305, loss=2.5072004795074463
I0127 04:04:07.205756 139656691705600 logging_writer.py:48] [91400] global_step=91400, grad_norm=3.5930256843566895, loss=2.5226686000823975
I0127 04:04:41.325535 139656700098304 logging_writer.py:48] [91500] global_step=91500, grad_norm=3.607337474822998, loss=2.4598870277404785
I0127 04:05:15.376085 139656691705600 logging_writer.py:48] [91600] global_step=91600, grad_norm=3.791826009750366, loss=2.530869245529175
I0127 04:05:49.418158 139656700098304 logging_writer.py:48] [91700] global_step=91700, grad_norm=3.7651724815368652, loss=2.5299272537231445
I0127 04:06:23.502270 139656691705600 logging_writer.py:48] [91800] global_step=91800, grad_norm=3.5809810161590576, loss=2.5953080654144287
I0127 04:06:57.547200 139656700098304 logging_writer.py:48] [91900] global_step=91900, grad_norm=3.5686612129211426, loss=2.5232582092285156
I0127 04:07:31.589258 139656691705600 logging_writer.py:48] [92000] global_step=92000, grad_norm=3.302396297454834, loss=2.5853779315948486
I0127 04:08:05.634001 139656700098304 logging_writer.py:48] [92100] global_step=92100, grad_norm=3.852618932723999, loss=2.6995856761932373
I0127 04:08:39.685293 139656691705600 logging_writer.py:48] [92200] global_step=92200, grad_norm=3.378164291381836, loss=2.5203471183776855
I0127 04:09:13.714919 139656700098304 logging_writer.py:48] [92300] global_step=92300, grad_norm=3.655071258544922, loss=2.4756264686584473
I0127 04:09:47.766824 139656691705600 logging_writer.py:48] [92400] global_step=92400, grad_norm=3.440560817718506, loss=2.512779474258423
I0127 04:10:21.905675 139656700098304 logging_writer.py:48] [92500] global_step=92500, grad_norm=4.082952499389648, loss=2.5087273120880127
I0127 04:10:55.975847 139656691705600 logging_writer.py:48] [92600] global_step=92600, grad_norm=3.7756712436676025, loss=2.5515434741973877
I0127 04:11:30.018870 139656700098304 logging_writer.py:48] [92700] global_step=92700, grad_norm=3.3383357524871826, loss=2.4685075283050537
I0127 04:11:45.465178 139822745589568 spec.py:321] Evaluating on the training split.
I0127 04:11:51.673917 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 04:12:00.316202 139822745589568 spec.py:349] Evaluating on the test split.
I0127 04:12:02.753502 139822745589568 submission_runner.py:408] Time since start: 32916.61s, 	Step: 92747, 	{'train/accuracy': 0.7115951776504517, 'train/loss': 1.2422090768814087, 'validation/accuracy': 0.6555399894714355, 'validation/loss': 1.498248815536499, 'validation/num_examples': 50000, 'test/accuracy': 0.5285000205039978, 'test/loss': 2.168752908706665, 'test/num_examples': 10000, 'score': 31679.979499578476, 'total_duration': 32916.61490535736, 'accumulated_submission_time': 31679.979499578476, 'accumulated_eval_time': 1230.3129467964172, 'accumulated_logging_time': 3.225436210632324}
I0127 04:12:02.781309 139656297445120 logging_writer.py:48] [92747] accumulated_eval_time=1230.312947, accumulated_logging_time=3.225436, accumulated_submission_time=31679.979500, global_step=92747, preemption_count=0, score=31679.979500, test/accuracy=0.528500, test/loss=2.168753, test/num_examples=10000, total_duration=32916.614905, train/accuracy=0.711595, train/loss=1.242209, validation/accuracy=0.655540, validation/loss=1.498249, validation/num_examples=50000
I0127 04:12:21.150025 139656649742080 logging_writer.py:48] [92800] global_step=92800, grad_norm=3.5604002475738525, loss=2.563554525375366
I0127 04:12:55.165187 139656297445120 logging_writer.py:48] [92900] global_step=92900, grad_norm=4.184345722198486, loss=2.5342018604278564
I0127 04:13:29.226435 139656649742080 logging_writer.py:48] [93000] global_step=93000, grad_norm=3.928497076034546, loss=2.5386385917663574
I0127 04:14:03.273990 139656297445120 logging_writer.py:48] [93100] global_step=93100, grad_norm=3.6770362854003906, loss=2.6040806770324707
I0127 04:14:37.336290 139656649742080 logging_writer.py:48] [93200] global_step=93200, grad_norm=3.9502694606781006, loss=2.5025081634521484
I0127 04:15:11.390061 139656297445120 logging_writer.py:48] [93300] global_step=93300, grad_norm=3.695035934448242, loss=2.560980796813965
I0127 04:15:45.429066 139656649742080 logging_writer.py:48] [93400] global_step=93400, grad_norm=3.787322521209717, loss=2.444786787033081
I0127 04:16:19.485258 139656297445120 logging_writer.py:48] [93500] global_step=93500, grad_norm=3.483804702758789, loss=2.5296871662139893
I0127 04:16:53.594919 139656649742080 logging_writer.py:48] [93600] global_step=93600, grad_norm=3.5886342525482178, loss=2.490178108215332
I0127 04:17:27.652971 139656297445120 logging_writer.py:48] [93700] global_step=93700, grad_norm=3.6196208000183105, loss=2.569310188293457
I0127 04:18:01.723254 139656649742080 logging_writer.py:48] [93800] global_step=93800, grad_norm=3.556258201599121, loss=2.5836050510406494
I0127 04:18:35.806353 139656297445120 logging_writer.py:48] [93900] global_step=93900, grad_norm=3.7972795963287354, loss=2.4794790744781494
I0127 04:19:09.892498 139656649742080 logging_writer.py:48] [94000] global_step=94000, grad_norm=3.551985740661621, loss=2.566298246383667
I0127 04:19:43.964706 139656297445120 logging_writer.py:48] [94100] global_step=94100, grad_norm=3.5545589923858643, loss=2.5228333473205566
I0127 04:20:18.060691 139656649742080 logging_writer.py:48] [94200] global_step=94200, grad_norm=4.159050464630127, loss=2.594848155975342
I0127 04:20:32.859301 139822745589568 spec.py:321] Evaluating on the training split.
I0127 04:20:39.042020 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 04:20:47.736116 139822745589568 spec.py:349] Evaluating on the test split.
I0127 04:20:50.083550 139822745589568 submission_runner.py:408] Time since start: 33443.94s, 	Step: 94245, 	{'train/accuracy': 0.7129902839660645, 'train/loss': 1.244142770767212, 'validation/accuracy': 0.6612399816513062, 'validation/loss': 1.4770818948745728, 'validation/num_examples': 50000, 'test/accuracy': 0.5364000201225281, 'test/loss': 2.1361281871795654, 'test/num_examples': 10000, 'score': 32189.998003721237, 'total_duration': 33443.94494795799, 'accumulated_submission_time': 32189.998003721237, 'accumulated_eval_time': 1247.537139415741, 'accumulated_logging_time': 3.262047529220581}
I0127 04:20:50.119736 139656700098304 logging_writer.py:48] [94245] accumulated_eval_time=1247.537139, accumulated_logging_time=3.262048, accumulated_submission_time=32189.998004, global_step=94245, preemption_count=0, score=32189.998004, test/accuracy=0.536400, test/loss=2.136128, test/num_examples=10000, total_duration=33443.944948, train/accuracy=0.712990, train/loss=1.244143, validation/accuracy=0.661240, validation/loss=1.477082, validation/num_examples=50000
I0127 04:21:09.191269 139658730145536 logging_writer.py:48] [94300] global_step=94300, grad_norm=4.0175886154174805, loss=2.560134172439575
I0127 04:21:43.228327 139656700098304 logging_writer.py:48] [94400] global_step=94400, grad_norm=3.506676197052002, loss=2.4292643070220947
I0127 04:22:17.261435 139658730145536 logging_writer.py:48] [94500] global_step=94500, grad_norm=4.057821750640869, loss=2.6176095008850098
I0127 04:22:51.327663 139656700098304 logging_writer.py:48] [94600] global_step=94600, grad_norm=4.0344414710998535, loss=2.5349271297454834
I0127 04:23:25.370944 139658730145536 logging_writer.py:48] [94700] global_step=94700, grad_norm=3.2597291469573975, loss=2.494243860244751
I0127 04:23:59.406162 139656700098304 logging_writer.py:48] [94800] global_step=94800, grad_norm=3.620897054672241, loss=2.487914800643921
I0127 04:24:33.457683 139658730145536 logging_writer.py:48] [94900] global_step=94900, grad_norm=3.9393134117126465, loss=2.574411630630493
I0127 04:25:07.512475 139656700098304 logging_writer.py:48] [95000] global_step=95000, grad_norm=3.659647226333618, loss=2.4973981380462646
I0127 04:25:41.571239 139658730145536 logging_writer.py:48] [95100] global_step=95100, grad_norm=4.18086576461792, loss=2.4351093769073486
I0127 04:26:15.650359 139656700098304 logging_writer.py:48] [95200] global_step=95200, grad_norm=3.794832944869995, loss=2.489917516708374
I0127 04:26:49.709588 139658730145536 logging_writer.py:48] [95300] global_step=95300, grad_norm=3.541402578353882, loss=2.4738597869873047
I0127 04:27:23.774513 139656700098304 logging_writer.py:48] [95400] global_step=95400, grad_norm=3.983229160308838, loss=2.499300718307495
I0127 04:27:57.852563 139658730145536 logging_writer.py:48] [95500] global_step=95500, grad_norm=3.9386391639709473, loss=2.6191539764404297
I0127 04:28:31.906847 139656700098304 logging_writer.py:48] [95600] global_step=95600, grad_norm=3.785285711288452, loss=2.5195748805999756
I0127 04:29:06.039279 139658730145536 logging_writer.py:48] [95700] global_step=95700, grad_norm=3.6908676624298096, loss=2.5074479579925537
I0127 04:29:20.133494 139822745589568 spec.py:321] Evaluating on the training split.
I0127 04:29:26.378101 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 04:29:35.121431 139822745589568 spec.py:349] Evaluating on the test split.
I0127 04:29:37.493304 139822745589568 submission_runner.py:408] Time since start: 33971.35s, 	Step: 95743, 	{'train/accuracy': 0.742586076259613, 'train/loss': 1.1307847499847412, 'validation/accuracy': 0.669219970703125, 'validation/loss': 1.4510703086853027, 'validation/num_examples': 50000, 'test/accuracy': 0.5428000092506409, 'test/loss': 2.1002559661865234, 'test/num_examples': 10000, 'score': 32699.949419021606, 'total_duration': 33971.354709625244, 'accumulated_submission_time': 32699.949419021606, 'accumulated_eval_time': 1264.8968999385834, 'accumulated_logging_time': 3.308539628982544}
I0127 04:29:37.529981 139656666527488 logging_writer.py:48] [95743] accumulated_eval_time=1264.896900, accumulated_logging_time=3.308540, accumulated_submission_time=32699.949419, global_step=95743, preemption_count=0, score=32699.949419, test/accuracy=0.542800, test/loss=2.100256, test/num_examples=10000, total_duration=33971.354710, train/accuracy=0.742586, train/loss=1.130785, validation/accuracy=0.669220, validation/loss=1.451070, validation/num_examples=50000
I0127 04:29:57.264861 139656674920192 logging_writer.py:48] [95800] global_step=95800, grad_norm=3.420642375946045, loss=2.4818739891052246
I0127 04:30:31.249767 139656666527488 logging_writer.py:48] [95900] global_step=95900, grad_norm=4.308931350708008, loss=2.6360111236572266
I0127 04:31:05.302381 139656674920192 logging_writer.py:48] [96000] global_step=96000, grad_norm=4.295559406280518, loss=2.573190212249756
I0127 04:31:39.332348 139656666527488 logging_writer.py:48] [96100] global_step=96100, grad_norm=4.283740520477295, loss=2.469309091567993
I0127 04:32:13.410458 139656674920192 logging_writer.py:48] [96200] global_step=96200, grad_norm=4.0281291007995605, loss=2.5639071464538574
I0127 04:32:47.497207 139656666527488 logging_writer.py:48] [96300] global_step=96300, grad_norm=3.5254106521606445, loss=2.4753363132476807
I0127 04:33:21.566529 139656674920192 logging_writer.py:48] [96400] global_step=96400, grad_norm=3.5486228466033936, loss=2.4687981605529785
I0127 04:33:55.654344 139656666527488 logging_writer.py:48] [96500] global_step=96500, grad_norm=3.910757541656494, loss=2.5227367877960205
I0127 04:34:29.733154 139656674920192 logging_writer.py:48] [96600] global_step=96600, grad_norm=3.6675453186035156, loss=2.4986090660095215
I0127 04:35:03.891418 139656666527488 logging_writer.py:48] [96700] global_step=96700, grad_norm=3.7262697219848633, loss=2.4999516010284424
I0127 04:35:37.995417 139656674920192 logging_writer.py:48] [96800] global_step=96800, grad_norm=3.786571979522705, loss=2.4711246490478516
I0127 04:36:12.070788 139656666527488 logging_writer.py:48] [96900] global_step=96900, grad_norm=3.4858932495117188, loss=2.5624494552612305
I0127 04:36:46.167311 139656674920192 logging_writer.py:48] [97000] global_step=97000, grad_norm=3.826160430908203, loss=2.513248920440674
I0127 04:37:20.223560 139656666527488 logging_writer.py:48] [97100] global_step=97100, grad_norm=3.8674440383911133, loss=2.524136781692505
I0127 04:37:54.315837 139656674920192 logging_writer.py:48] [97200] global_step=97200, grad_norm=4.735577583312988, loss=2.634213447570801
I0127 04:38:07.754347 139822745589568 spec.py:321] Evaluating on the training split.
I0127 04:38:14.000851 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 04:38:22.550451 139822745589568 spec.py:349] Evaluating on the test split.
I0127 04:38:24.848000 139822745589568 submission_runner.py:408] Time since start: 34498.71s, 	Step: 97241, 	{'train/accuracy': 0.7416892647743225, 'train/loss': 1.120581030845642, 'validation/accuracy': 0.6673399806022644, 'validation/loss': 1.463154911994934, 'validation/num_examples': 50000, 'test/accuracy': 0.5376999974250793, 'test/loss': 2.125854253768921, 'test/num_examples': 10000, 'score': 33210.114094257355, 'total_duration': 34498.70940423012, 'accumulated_submission_time': 33210.114094257355, 'accumulated_eval_time': 1281.9905200004578, 'accumulated_logging_time': 3.3550431728363037}
I0127 04:38:24.884348 139656649742080 logging_writer.py:48] [97241] accumulated_eval_time=1281.990520, accumulated_logging_time=3.355043, accumulated_submission_time=33210.114094, global_step=97241, preemption_count=0, score=33210.114094, test/accuracy=0.537700, test/loss=2.125854, test/num_examples=10000, total_duration=34498.709404, train/accuracy=0.741689, train/loss=1.120581, validation/accuracy=0.667340, validation/loss=1.463155, validation/num_examples=50000
I0127 04:38:45.315006 139656658134784 logging_writer.py:48] [97300] global_step=97300, grad_norm=4.2317657470703125, loss=2.4924983978271484
I0127 04:39:19.342287 139656649742080 logging_writer.py:48] [97400] global_step=97400, grad_norm=3.706061601638794, loss=2.540449619293213
I0127 04:39:53.365597 139656658134784 logging_writer.py:48] [97500] global_step=97500, grad_norm=3.5666418075561523, loss=2.4447014331817627
I0127 04:40:27.424799 139656649742080 logging_writer.py:48] [97600] global_step=97600, grad_norm=3.748178005218506, loss=2.4713237285614014
I0127 04:41:01.499111 139656658134784 logging_writer.py:48] [97700] global_step=97700, grad_norm=4.500329971313477, loss=2.485372304916382
I0127 04:41:35.609906 139656649742080 logging_writer.py:48] [97800] global_step=97800, grad_norm=3.7818691730499268, loss=2.5893750190734863
I0127 04:42:09.680729 139656658134784 logging_writer.py:48] [97900] global_step=97900, grad_norm=3.6692707538604736, loss=2.507791042327881
I0127 04:42:43.728631 139656649742080 logging_writer.py:48] [98000] global_step=98000, grad_norm=3.8162741661071777, loss=2.542571783065796
I0127 04:43:17.811947 139656658134784 logging_writer.py:48] [98100] global_step=98100, grad_norm=3.7944488525390625, loss=2.508881092071533
I0127 04:43:51.860256 139656649742080 logging_writer.py:48] [98200] global_step=98200, grad_norm=3.727444887161255, loss=2.464606761932373
I0127 04:44:25.921786 139656658134784 logging_writer.py:48] [98300] global_step=98300, grad_norm=3.8151967525482178, loss=2.4611263275146484
I0127 04:44:59.997243 139656649742080 logging_writer.py:48] [98400] global_step=98400, grad_norm=3.7200369834899902, loss=2.5796151161193848
I0127 04:45:34.045112 139656658134784 logging_writer.py:48] [98500] global_step=98500, grad_norm=4.210934162139893, loss=2.4877655506134033
I0127 04:46:08.106323 139656649742080 logging_writer.py:48] [98600] global_step=98600, grad_norm=3.7343685626983643, loss=2.5084235668182373
I0127 04:46:42.140573 139656658134784 logging_writer.py:48] [98700] global_step=98700, grad_norm=3.765826463699341, loss=2.5820133686065674
I0127 04:46:54.898935 139822745589568 spec.py:321] Evaluating on the training split.
I0127 04:47:01.258724 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 04:47:09.753099 139822745589568 spec.py:349] Evaluating on the test split.
I0127 04:47:12.014737 139822745589568 submission_runner.py:408] Time since start: 35025.88s, 	Step: 98739, 	{'train/accuracy': 0.7276586294174194, 'train/loss': 1.1531447172164917, 'validation/accuracy': 0.666920006275177, 'validation/loss': 1.4372801780700684, 'validation/num_examples': 50000, 'test/accuracy': 0.5424000024795532, 'test/loss': 2.1051735877990723, 'test/num_examples': 10000, 'score': 33720.06629896164, 'total_duration': 35025.87611365318, 'accumulated_submission_time': 33720.06629896164, 'accumulated_eval_time': 1299.1062581539154, 'accumulated_logging_time': 3.401531219482422}
I0127 04:47:12.055233 139656683312896 logging_writer.py:48] [98739] accumulated_eval_time=1299.106258, accumulated_logging_time=3.401531, accumulated_submission_time=33720.066299, global_step=98739, preemption_count=0, score=33720.066299, test/accuracy=0.542400, test/loss=2.105174, test/num_examples=10000, total_duration=35025.876114, train/accuracy=0.727659, train/loss=1.153145, validation/accuracy=0.666920, validation/loss=1.437280, validation/num_examples=50000
I0127 04:47:33.215431 139656691705600 logging_writer.py:48] [98800] global_step=98800, grad_norm=4.206744194030762, loss=2.4978630542755127
I0127 04:48:07.246955 139656683312896 logging_writer.py:48] [98900] global_step=98900, grad_norm=3.500286817550659, loss=2.48654842376709
I0127 04:48:41.262411 139656691705600 logging_writer.py:48] [99000] global_step=99000, grad_norm=3.557229518890381, loss=2.533172130584717
I0127 04:49:15.314422 139656683312896 logging_writer.py:48] [99100] global_step=99100, grad_norm=3.5269839763641357, loss=2.3830645084381104
I0127 04:49:49.376532 139656691705600 logging_writer.py:48] [99200] global_step=99200, grad_norm=3.899773120880127, loss=2.573385000228882
I0127 04:50:23.399523 139656683312896 logging_writer.py:48] [99300] global_step=99300, grad_norm=4.084817886352539, loss=2.5032310485839844
I0127 04:50:57.448072 139656691705600 logging_writer.py:48] [99400] global_step=99400, grad_norm=3.596231460571289, loss=2.459221839904785
I0127 04:51:31.474632 139656683312896 logging_writer.py:48] [99500] global_step=99500, grad_norm=3.7992196083068848, loss=2.505457878112793
I0127 04:52:05.499324 139656691705600 logging_writer.py:48] [99600] global_step=99600, grad_norm=3.9318554401397705, loss=2.4931437969207764
I0127 04:52:39.546810 139656683312896 logging_writer.py:48] [99700] global_step=99700, grad_norm=3.911254644393921, loss=2.473731517791748
I0127 04:53:13.608226 139656691705600 logging_writer.py:48] [99800] global_step=99800, grad_norm=3.665477752685547, loss=2.3849878311157227
I0127 04:53:47.726241 139656683312896 logging_writer.py:48] [99900] global_step=99900, grad_norm=3.633796453475952, loss=2.554574489593506
I0127 04:54:21.791730 139656691705600 logging_writer.py:48] [100000] global_step=100000, grad_norm=3.7313342094421387, loss=2.4691128730773926
I0127 04:54:55.878126 139656683312896 logging_writer.py:48] [100100] global_step=100100, grad_norm=4.19130802154541, loss=2.5178892612457275
I0127 04:55:29.952973 139656691705600 logging_writer.py:48] [100200] global_step=100200, grad_norm=3.6839237213134766, loss=2.3622426986694336
I0127 04:55:42.040334 139822745589568 spec.py:321] Evaluating on the training split.
I0127 04:55:48.198359 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 04:55:56.720679 139822745589568 spec.py:349] Evaluating on the test split.
I0127 04:55:58.983599 139822745589568 submission_runner.py:408] Time since start: 35552.85s, 	Step: 100237, 	{'train/accuracy': 0.7302096486091614, 'train/loss': 1.1675571203231812, 'validation/accuracy': 0.6653199791908264, 'validation/loss': 1.4513657093048096, 'validation/num_examples': 50000, 'test/accuracy': 0.5361000299453735, 'test/loss': 2.1154842376708984, 'test/num_examples': 10000, 'score': 34229.99035668373, 'total_duration': 35552.84500479698, 'accumulated_submission_time': 34229.99035668373, 'accumulated_eval_time': 1316.049479007721, 'accumulated_logging_time': 3.4516913890838623}
I0127 04:55:59.020440 139656649742080 logging_writer.py:48] [100237] accumulated_eval_time=1316.049479, accumulated_logging_time=3.451691, accumulated_submission_time=34229.990357, global_step=100237, preemption_count=0, score=34229.990357, test/accuracy=0.536100, test/loss=2.115484, test/num_examples=10000, total_duration=35552.845005, train/accuracy=0.730210, train/loss=1.167557, validation/accuracy=0.665320, validation/loss=1.451366, validation/num_examples=50000
I0127 04:56:20.830163 139656658134784 logging_writer.py:48] [100300] global_step=100300, grad_norm=4.155389785766602, loss=2.601591110229492
I0127 04:56:54.868020 139656649742080 logging_writer.py:48] [100400] global_step=100400, grad_norm=4.228233337402344, loss=2.478644847869873
I0127 04:57:28.904200 139656658134784 logging_writer.py:48] [100500] global_step=100500, grad_norm=3.886028289794922, loss=2.5240156650543213
I0127 04:58:02.962903 139656649742080 logging_writer.py:48] [100600] global_step=100600, grad_norm=3.344832181930542, loss=2.4131181240081787
I0127 04:58:37.023780 139656658134784 logging_writer.py:48] [100700] global_step=100700, grad_norm=3.945307493209839, loss=2.593538284301758
I0127 04:59:11.092370 139656649742080 logging_writer.py:48] [100800] global_step=100800, grad_norm=4.233120441436768, loss=2.421221971511841
I0127 04:59:45.137307 139656658134784 logging_writer.py:48] [100900] global_step=100900, grad_norm=3.7589550018310547, loss=2.5495553016662598
I0127 05:00:19.257658 139656649742080 logging_writer.py:48] [101000] global_step=101000, grad_norm=4.072672367095947, loss=2.6205215454101562
I0127 05:00:53.288998 139656658134784 logging_writer.py:48] [101100] global_step=101100, grad_norm=3.703352928161621, loss=2.448763370513916
I0127 05:01:27.324442 139656649742080 logging_writer.py:48] [101200] global_step=101200, grad_norm=3.8373374938964844, loss=2.4919750690460205
I0127 05:02:01.411996 139656658134784 logging_writer.py:48] [101300] global_step=101300, grad_norm=4.128203868865967, loss=2.449085235595703
I0127 05:02:35.480983 139656649742080 logging_writer.py:48] [101400] global_step=101400, grad_norm=3.8256044387817383, loss=2.5231082439422607
I0127 05:03:09.524449 139656658134784 logging_writer.py:48] [101500] global_step=101500, grad_norm=3.861971855163574, loss=2.4530231952667236
I0127 05:03:43.614389 139656649742080 logging_writer.py:48] [101600] global_step=101600, grad_norm=3.802438735961914, loss=2.4538164138793945
I0127 05:04:17.668236 139656658134784 logging_writer.py:48] [101700] global_step=101700, grad_norm=3.6810057163238525, loss=2.408104419708252
I0127 05:04:29.064722 139822745589568 spec.py:321] Evaluating on the training split.
I0127 05:04:35.338057 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 05:04:43.874041 139822745589568 spec.py:349] Evaluating on the test split.
I0127 05:04:46.158000 139822745589568 submission_runner.py:408] Time since start: 36080.02s, 	Step: 101735, 	{'train/accuracy': 0.7242107391357422, 'train/loss': 1.1790367364883423, 'validation/accuracy': 0.6686800122261047, 'validation/loss': 1.4343260526657104, 'validation/num_examples': 50000, 'test/accuracy': 0.5398000478744507, 'test/loss': 2.1134259700775146, 'test/num_examples': 10000, 'score': 34739.97491002083, 'total_duration': 36080.01940727234, 'accumulated_submission_time': 34739.97491002083, 'accumulated_eval_time': 1333.1427223682404, 'accumulated_logging_time': 3.4981353282928467}
I0127 05:04:46.199050 139656683312896 logging_writer.py:48] [101735] accumulated_eval_time=1333.142722, accumulated_logging_time=3.498135, accumulated_submission_time=34739.974910, global_step=101735, preemption_count=0, score=34739.974910, test/accuracy=0.539800, test/loss=2.113426, test/num_examples=10000, total_duration=36080.019407, train/accuracy=0.724211, train/loss=1.179037, validation/accuracy=0.668680, validation/loss=1.434326, validation/num_examples=50000
I0127 05:05:08.653593 139656691705600 logging_writer.py:48] [101800] global_step=101800, grad_norm=4.029446125030518, loss=2.4761250019073486
I0127 05:05:42.659578 139656683312896 logging_writer.py:48] [101900] global_step=101900, grad_norm=3.937335729598999, loss=2.6009016036987305
I0127 05:06:16.871885 139656691705600 logging_writer.py:48] [102000] global_step=102000, grad_norm=4.6321282386779785, loss=2.543566942214966
I0127 05:06:50.930207 139656683312896 logging_writer.py:48] [102100] global_step=102100, grad_norm=4.242277145385742, loss=2.4725215435028076
I0127 05:07:24.978604 139656691705600 logging_writer.py:48] [102200] global_step=102200, grad_norm=3.544116735458374, loss=2.4415712356567383
I0127 05:07:59.021733 139656683312896 logging_writer.py:48] [102300] global_step=102300, grad_norm=3.707035779953003, loss=2.400872230529785
I0127 05:08:33.059423 139656691705600 logging_writer.py:48] [102400] global_step=102400, grad_norm=3.884516954421997, loss=2.4094667434692383
I0127 05:09:07.111699 139656683312896 logging_writer.py:48] [102500] global_step=102500, grad_norm=4.179984092712402, loss=2.504072666168213
I0127 05:09:41.138461 139656691705600 logging_writer.py:48] [102600] global_step=102600, grad_norm=4.103744029998779, loss=2.524567127227783
I0127 05:10:15.190419 139656683312896 logging_writer.py:48] [102700] global_step=102700, grad_norm=3.5369396209716797, loss=2.3515748977661133
I0127 05:10:49.258322 139656691705600 logging_writer.py:48] [102800] global_step=102800, grad_norm=3.5854029655456543, loss=2.456561326980591
I0127 05:11:23.318571 139656683312896 logging_writer.py:48] [102900] global_step=102900, grad_norm=3.571648120880127, loss=2.517874240875244
I0127 05:11:57.387628 139656691705600 logging_writer.py:48] [103000] global_step=103000, grad_norm=4.102245330810547, loss=2.5544044971466064
I0127 05:12:31.554167 139656683312896 logging_writer.py:48] [103100] global_step=103100, grad_norm=4.081295013427734, loss=2.444924831390381
I0127 05:13:05.629823 139656691705600 logging_writer.py:48] [103200] global_step=103200, grad_norm=4.820535659790039, loss=2.5099308490753174
I0127 05:13:16.341090 139822745589568 spec.py:321] Evaluating on the training split.
I0127 05:13:22.640784 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 05:13:31.146694 139822745589568 spec.py:349] Evaluating on the test split.
I0127 05:13:33.446909 139822745589568 submission_runner.py:408] Time since start: 36607.31s, 	Step: 103233, 	{'train/accuracy': 0.7329798936843872, 'train/loss': 1.1520893573760986, 'validation/accuracy': 0.676859974861145, 'validation/loss': 1.4166473150253296, 'validation/num_examples': 50000, 'test/accuracy': 0.5520000457763672, 'test/loss': 2.0522189140319824, 'test/num_examples': 10000, 'score': 35250.05642032623, 'total_duration': 36607.308309316635, 'accumulated_submission_time': 35250.05642032623, 'accumulated_eval_time': 1350.2484858036041, 'accumulated_logging_time': 3.5491249561309814}
I0127 05:13:33.483929 139656674920192 logging_writer.py:48] [103233] accumulated_eval_time=1350.248486, accumulated_logging_time=3.549125, accumulated_submission_time=35250.056420, global_step=103233, preemption_count=0, score=35250.056420, test/accuracy=0.552000, test/loss=2.052219, test/num_examples=10000, total_duration=36607.308309, train/accuracy=0.732980, train/loss=1.152089, validation/accuracy=0.676860, validation/loss=1.416647, validation/num_examples=50000
I0127 05:13:56.605520 139656700098304 logging_writer.py:48] [103300] global_step=103300, grad_norm=3.8673431873321533, loss=2.519176483154297
I0127 05:14:30.633335 139656674920192 logging_writer.py:48] [103400] global_step=103400, grad_norm=3.7835288047790527, loss=2.4253106117248535
I0127 05:15:04.677378 139656700098304 logging_writer.py:48] [103500] global_step=103500, grad_norm=4.264770030975342, loss=2.4420716762542725
I0127 05:15:38.743840 139656674920192 logging_writer.py:48] [103600] global_step=103600, grad_norm=4.64362907409668, loss=2.587144136428833
I0127 05:16:12.820165 139656700098304 logging_writer.py:48] [103700] global_step=103700, grad_norm=3.7418887615203857, loss=2.4610776901245117
I0127 05:16:46.859433 139656674920192 logging_writer.py:48] [103800] global_step=103800, grad_norm=3.634521722793579, loss=2.4092884063720703
I0127 05:17:20.892938 139656700098304 logging_writer.py:48] [103900] global_step=103900, grad_norm=4.268362045288086, loss=2.43351411819458
I0127 05:17:54.962760 139656674920192 logging_writer.py:48] [104000] global_step=104000, grad_norm=4.441500663757324, loss=2.4573981761932373
I0127 05:18:29.066297 139656700098304 logging_writer.py:48] [104100] global_step=104100, grad_norm=3.6924514770507812, loss=2.5357134342193604
I0127 05:19:03.140679 139656674920192 logging_writer.py:48] [104200] global_step=104200, grad_norm=3.5849719047546387, loss=2.427475929260254
I0127 05:19:37.222385 139656700098304 logging_writer.py:48] [104300] global_step=104300, grad_norm=4.503270149230957, loss=2.427182674407959
I0127 05:20:11.263885 139656674920192 logging_writer.py:48] [104400] global_step=104400, grad_norm=4.409451484680176, loss=2.4478912353515625
I0127 05:20:45.325244 139656700098304 logging_writer.py:48] [104500] global_step=104500, grad_norm=3.804178237915039, loss=2.5126819610595703
I0127 05:21:19.399049 139656674920192 logging_writer.py:48] [104600] global_step=104600, grad_norm=3.750042200088501, loss=2.4637691974639893
I0127 05:21:53.450371 139656700098304 logging_writer.py:48] [104700] global_step=104700, grad_norm=4.1609344482421875, loss=2.4974775314331055
I0127 05:22:03.485542 139822745589568 spec.py:321] Evaluating on the training split.
I0127 05:22:09.804592 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 05:22:18.366536 139822745589568 spec.py:349] Evaluating on the test split.
I0127 05:22:20.655572 139822745589568 submission_runner.py:408] Time since start: 37134.52s, 	Step: 104731, 	{'train/accuracy': 0.7315847873687744, 'train/loss': 1.1411164999008179, 'validation/accuracy': 0.6702199578285217, 'validation/loss': 1.417891502380371, 'validation/num_examples': 50000, 'test/accuracy': 0.5373000502586365, 'test/loss': 2.095768928527832, 'test/num_examples': 10000, 'score': 35759.99800825119, 'total_duration': 37134.51687049866, 'accumulated_submission_time': 35759.99800825119, 'accumulated_eval_time': 1367.418380498886, 'accumulated_logging_time': 3.5954058170318604}
I0127 05:22:20.691080 139656649742080 logging_writer.py:48] [104731] accumulated_eval_time=1367.418380, accumulated_logging_time=3.595406, accumulated_submission_time=35759.998008, global_step=104731, preemption_count=0, score=35759.998008, test/accuracy=0.537300, test/loss=2.095769, test/num_examples=10000, total_duration=37134.516870, train/accuracy=0.731585, train/loss=1.141116, validation/accuracy=0.670220, validation/loss=1.417892, validation/num_examples=50000
I0127 05:22:44.535504 139656658134784 logging_writer.py:48] [104800] global_step=104800, grad_norm=4.296968936920166, loss=2.5144026279449463
I0127 05:23:18.549990 139656649742080 logging_writer.py:48] [104900] global_step=104900, grad_norm=4.580443382263184, loss=2.470856189727783
I0127 05:23:52.590869 139656658134784 logging_writer.py:48] [105000] global_step=105000, grad_norm=5.000778675079346, loss=2.4398348331451416
I0127 05:24:26.673467 139656649742080 logging_writer.py:48] [105100] global_step=105100, grad_norm=3.8907105922698975, loss=2.46038818359375
I0127 05:25:00.791778 139656658134784 logging_writer.py:48] [105200] global_step=105200, grad_norm=3.978957176208496, loss=2.365924835205078
I0127 05:25:34.867008 139656649742080 logging_writer.py:48] [105300] global_step=105300, grad_norm=3.7174599170684814, loss=2.4375827312469482
I0127 05:26:08.919440 139656658134784 logging_writer.py:48] [105400] global_step=105400, grad_norm=4.203310489654541, loss=2.5358386039733887
I0127 05:26:42.976104 139656649742080 logging_writer.py:48] [105500] global_step=105500, grad_norm=3.9714739322662354, loss=2.4934241771698
I0127 05:27:17.042009 139656658134784 logging_writer.py:48] [105600] global_step=105600, grad_norm=3.8687503337860107, loss=2.4180548191070557
I0127 05:27:51.099573 139656649742080 logging_writer.py:48] [105700] global_step=105700, grad_norm=4.696436882019043, loss=2.5978598594665527
I0127 05:28:25.161000 139656658134784 logging_writer.py:48] [105800] global_step=105800, grad_norm=4.148306369781494, loss=2.5039713382720947
I0127 05:28:59.233212 139656649742080 logging_writer.py:48] [105900] global_step=105900, grad_norm=3.7668097019195557, loss=2.4279026985168457
I0127 05:29:33.295703 139656658134784 logging_writer.py:48] [106000] global_step=106000, grad_norm=4.586601257324219, loss=2.44895076751709
I0127 05:30:07.368359 139656649742080 logging_writer.py:48] [106100] global_step=106100, grad_norm=4.2676100730896, loss=2.465250253677368
I0127 05:30:41.444120 139656658134784 logging_writer.py:48] [106200] global_step=106200, grad_norm=4.284851551055908, loss=2.548003911972046
I0127 05:30:50.838796 139822745589568 spec.py:321] Evaluating on the training split.
I0127 05:30:57.008675 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 05:31:05.766237 139822745589568 spec.py:349] Evaluating on the test split.
I0127 05:31:08.029744 139822745589568 submission_runner.py:408] Time since start: 37661.89s, 	Step: 106229, 	{'train/accuracy': 0.7540457248687744, 'train/loss': 1.072227954864502, 'validation/accuracy': 0.6693599820137024, 'validation/loss': 1.442069172859192, 'validation/num_examples': 50000, 'test/accuracy': 0.5469000339508057, 'test/loss': 2.085392951965332, 'test/num_examples': 10000, 'score': 36270.085739851, 'total_duration': 37661.89115190506, 'accumulated_submission_time': 36270.085739851, 'accumulated_eval_time': 1384.609278678894, 'accumulated_logging_time': 3.6411328315734863}
I0127 05:31:08.066419 139656700098304 logging_writer.py:48] [106229] accumulated_eval_time=1384.609279, accumulated_logging_time=3.641133, accumulated_submission_time=36270.085740, global_step=106229, preemption_count=0, score=36270.085740, test/accuracy=0.546900, test/loss=2.085393, test/num_examples=10000, total_duration=37661.891152, train/accuracy=0.754046, train/loss=1.072228, validation/accuracy=0.669360, validation/loss=1.442069, validation/num_examples=50000
I0127 05:31:32.576904 139658730145536 logging_writer.py:48] [106300] global_step=106300, grad_norm=4.098526477813721, loss=2.4643664360046387
I0127 05:32:06.625528 139656700098304 logging_writer.py:48] [106400] global_step=106400, grad_norm=4.688436985015869, loss=2.5188446044921875
I0127 05:32:40.679662 139658730145536 logging_writer.py:48] [106500] global_step=106500, grad_norm=4.283195972442627, loss=2.4622716903686523
I0127 05:33:14.725402 139656700098304 logging_writer.py:48] [106600] global_step=106600, grad_norm=4.284930229187012, loss=2.5204052925109863
I0127 05:33:48.769677 139658730145536 logging_writer.py:48] [106700] global_step=106700, grad_norm=4.693350315093994, loss=2.372617483139038
I0127 05:34:22.811079 139656700098304 logging_writer.py:48] [106800] global_step=106800, grad_norm=3.986429214477539, loss=2.4676513671875
I0127 05:34:56.896413 139658730145536 logging_writer.py:48] [106900] global_step=106900, grad_norm=3.6456706523895264, loss=2.375422477722168
I0127 05:35:30.967248 139656700098304 logging_writer.py:48] [107000] global_step=107000, grad_norm=4.033966541290283, loss=2.5472614765167236
I0127 05:36:05.032213 139658730145536 logging_writer.py:48] [107100] global_step=107100, grad_norm=4.203770637512207, loss=2.4877119064331055
I0127 05:36:39.091215 139656700098304 logging_writer.py:48] [107200] global_step=107200, grad_norm=3.890812873840332, loss=2.3985023498535156
I0127 05:37:13.238901 139658730145536 logging_writer.py:48] [107300] global_step=107300, grad_norm=3.9288108348846436, loss=2.534114122390747
I0127 05:37:47.306324 139656700098304 logging_writer.py:48] [107400] global_step=107400, grad_norm=4.0484395027160645, loss=2.4670512676239014
I0127 05:38:21.359520 139658730145536 logging_writer.py:48] [107500] global_step=107500, grad_norm=3.8997247219085693, loss=2.3944053649902344
I0127 05:38:55.421435 139656700098304 logging_writer.py:48] [107600] global_step=107600, grad_norm=4.068571090698242, loss=2.5735690593719482
I0127 05:39:29.464791 139658730145536 logging_writer.py:48] [107700] global_step=107700, grad_norm=3.815566062927246, loss=2.349282741546631
I0127 05:39:38.141569 139822745589568 spec.py:321] Evaluating on the training split.
I0127 05:39:44.299605 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 05:39:52.759115 139822745589568 spec.py:349] Evaluating on the test split.
I0127 05:39:55.053428 139822745589568 submission_runner.py:408] Time since start: 38188.91s, 	Step: 107727, 	{'train/accuracy': 0.751973032951355, 'train/loss': 1.068299651145935, 'validation/accuracy': 0.6773999929428101, 'validation/loss': 1.4052557945251465, 'validation/num_examples': 50000, 'test/accuracy': 0.5508000254631042, 'test/loss': 2.0545578002929688, 'test/num_examples': 10000, 'score': 36780.10208392143, 'total_duration': 38188.91479277611, 'accumulated_submission_time': 36780.10208392143, 'accumulated_eval_time': 1401.521045923233, 'accumulated_logging_time': 3.6874375343322754}
I0127 05:39:55.093494 139656658134784 logging_writer.py:48] [107727] accumulated_eval_time=1401.521046, accumulated_logging_time=3.687438, accumulated_submission_time=36780.102084, global_step=107727, preemption_count=0, score=36780.102084, test/accuracy=0.550800, test/loss=2.054558, test/num_examples=10000, total_duration=38188.914793, train/accuracy=0.751973, train/loss=1.068300, validation/accuracy=0.677400, validation/loss=1.405256, validation/num_examples=50000
I0127 05:40:20.265162 139656666527488 logging_writer.py:48] [107800] global_step=107800, grad_norm=3.833503246307373, loss=2.4396519660949707
I0127 05:40:54.296413 139656658134784 logging_writer.py:48] [107900] global_step=107900, grad_norm=3.950427293777466, loss=2.4490818977355957
I0127 05:41:28.342063 139656666527488 logging_writer.py:48] [108000] global_step=108000, grad_norm=4.535797119140625, loss=2.5541512966156006
I0127 05:42:02.380307 139656658134784 logging_writer.py:48] [108100] global_step=108100, grad_norm=4.17794132232666, loss=2.401798963546753
I0127 05:42:36.412709 139656666527488 logging_writer.py:48] [108200] global_step=108200, grad_norm=4.541777610778809, loss=2.4750537872314453
I0127 05:43:10.468864 139656658134784 logging_writer.py:48] [108300] global_step=108300, grad_norm=3.985410451889038, loss=2.433933973312378
I0127 05:43:44.679358 139656666527488 logging_writer.py:48] [108400] global_step=108400, grad_norm=4.423402786254883, loss=2.4864749908447266
I0127 05:44:18.728910 139656658134784 logging_writer.py:48] [108500] global_step=108500, grad_norm=4.226834774017334, loss=2.494312286376953
I0127 05:44:52.830600 139656666527488 logging_writer.py:48] [108600] global_step=108600, grad_norm=4.377978324890137, loss=2.410099983215332
I0127 05:45:26.904646 139656658134784 logging_writer.py:48] [108700] global_step=108700, grad_norm=3.755066156387329, loss=2.413424253463745
I0127 05:46:00.981484 139656666527488 logging_writer.py:48] [108800] global_step=108800, grad_norm=4.179275035858154, loss=2.4416658878326416
I0127 05:46:35.040205 139656658134784 logging_writer.py:48] [108900] global_step=108900, grad_norm=4.0926313400268555, loss=2.4729554653167725
I0127 05:47:09.120497 139656666527488 logging_writer.py:48] [109000] global_step=109000, grad_norm=3.6592681407928467, loss=2.434206485748291
I0127 05:47:43.203328 139656658134784 logging_writer.py:48] [109100] global_step=109100, grad_norm=4.286437034606934, loss=2.404134511947632
I0127 05:48:17.253394 139656666527488 logging_writer.py:48] [109200] global_step=109200, grad_norm=3.834111452102661, loss=2.3509483337402344
I0127 05:48:25.227978 139822745589568 spec.py:321] Evaluating on the training split.
I0127 05:48:31.434502 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 05:48:40.086209 139822745589568 spec.py:349] Evaluating on the test split.
I0127 05:48:42.392062 139822745589568 submission_runner.py:408] Time since start: 38716.25s, 	Step: 109225, 	{'train/accuracy': 0.7469507455825806, 'train/loss': 1.086888074874878, 'validation/accuracy': 0.6785999536514282, 'validation/loss': 1.3956547975540161, 'validation/num_examples': 50000, 'test/accuracy': 0.5574000477790833, 'test/loss': 2.0311355590820312, 'test/num_examples': 10000, 'score': 37290.17399263382, 'total_duration': 38716.25345778465, 'accumulated_submission_time': 37290.17399263382, 'accumulated_eval_time': 1418.6850700378418, 'accumulated_logging_time': 3.740187406539917}
I0127 05:48:42.429850 139656649742080 logging_writer.py:48] [109225] accumulated_eval_time=1418.685070, accumulated_logging_time=3.740187, accumulated_submission_time=37290.173993, global_step=109225, preemption_count=0, score=37290.173993, test/accuracy=0.557400, test/loss=2.031136, test/num_examples=10000, total_duration=38716.253458, train/accuracy=0.746951, train/loss=1.086888, validation/accuracy=0.678600, validation/loss=1.395655, validation/num_examples=50000
I0127 05:49:08.278772 139656658134784 logging_writer.py:48] [109300] global_step=109300, grad_norm=4.294342041015625, loss=2.3612356185913086
I0127 05:49:42.410164 139656649742080 logging_writer.py:48] [109400] global_step=109400, grad_norm=4.199807643890381, loss=2.427273750305176
I0127 05:50:16.454625 139656658134784 logging_writer.py:48] [109500] global_step=109500, grad_norm=4.3861985206604, loss=2.4811668395996094
I0127 05:50:50.507443 139656649742080 logging_writer.py:48] [109600] global_step=109600, grad_norm=4.149271488189697, loss=2.4432852268218994
I0127 05:51:24.568358 139656658134784 logging_writer.py:48] [109700] global_step=109700, grad_norm=4.546437740325928, loss=2.458918571472168
I0127 05:51:58.617878 139656649742080 logging_writer.py:48] [109800] global_step=109800, grad_norm=4.115285873413086, loss=2.3826868534088135
I0127 05:52:32.644558 139656658134784 logging_writer.py:48] [109900] global_step=109900, grad_norm=3.996967315673828, loss=2.470153570175171
I0127 05:53:06.693833 139656649742080 logging_writer.py:48] [110000] global_step=110000, grad_norm=4.586512565612793, loss=2.4658989906311035
I0127 05:53:40.728241 139656658134784 logging_writer.py:48] [110100] global_step=110100, grad_norm=3.943218231201172, loss=2.43206787109375
I0127 05:54:14.747150 139656649742080 logging_writer.py:48] [110200] global_step=110200, grad_norm=3.9628052711486816, loss=2.4370012283325195
I0127 05:54:48.812435 139656658134784 logging_writer.py:48] [110300] global_step=110300, grad_norm=4.216770172119141, loss=2.5131912231445312
I0127 05:55:22.844150 139656649742080 logging_writer.py:48] [110400] global_step=110400, grad_norm=4.462193965911865, loss=2.4676125049591064
I0127 05:55:57.061131 139656658134784 logging_writer.py:48] [110500] global_step=110500, grad_norm=4.137764930725098, loss=2.4427096843719482
I0127 05:56:31.130805 139656649742080 logging_writer.py:48] [110600] global_step=110600, grad_norm=3.7988710403442383, loss=2.372077465057373
I0127 05:57:05.183504 139656658134784 logging_writer.py:48] [110700] global_step=110700, grad_norm=4.63096284866333, loss=2.4682788848876953
I0127 05:57:12.488294 139822745589568 spec.py:321] Evaluating on the training split.
I0127 05:57:18.841996 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 05:57:27.339013 139822745589568 spec.py:349] Evaluating on the test split.
I0127 05:57:29.590676 139822745589568 submission_runner.py:408] Time since start: 39243.45s, 	Step: 110723, 	{'train/accuracy': 0.7480069994926453, 'train/loss': 1.080083966255188, 'validation/accuracy': 0.6832799911499023, 'validation/loss': 1.3639898300170898, 'validation/num_examples': 50000, 'test/accuracy': 0.5574000477790833, 'test/loss': 2.017634391784668, 'test/num_examples': 10000, 'score': 37800.17114710808, 'total_duration': 39243.452078580856, 'accumulated_submission_time': 37800.17114710808, 'accumulated_eval_time': 1435.787403345108, 'accumulated_logging_time': 3.788019895553589}
I0127 05:57:29.626435 139656674920192 logging_writer.py:48] [110723] accumulated_eval_time=1435.787403, accumulated_logging_time=3.788020, accumulated_submission_time=37800.171147, global_step=110723, preemption_count=0, score=37800.171147, test/accuracy=0.557400, test/loss=2.017634, test/num_examples=10000, total_duration=39243.452079, train/accuracy=0.748007, train/loss=1.080084, validation/accuracy=0.683280, validation/loss=1.363990, validation/num_examples=50000
I0127 05:57:56.183936 139656683312896 logging_writer.py:48] [110800] global_step=110800, grad_norm=4.575665473937988, loss=2.3987035751342773
I0127 05:58:30.212214 139656674920192 logging_writer.py:48] [110900] global_step=110900, grad_norm=4.445017337799072, loss=2.443371534347534
I0127 05:59:04.271675 139656683312896 logging_writer.py:48] [111000] global_step=111000, grad_norm=4.394418716430664, loss=2.505255699157715
I0127 05:59:38.325078 139656674920192 logging_writer.py:48] [111100] global_step=111100, grad_norm=3.944864511489868, loss=2.413240671157837
I0127 06:00:12.399701 139656683312896 logging_writer.py:48] [111200] global_step=111200, grad_norm=4.068756103515625, loss=2.403566837310791
I0127 06:00:46.471428 139656674920192 logging_writer.py:48] [111300] global_step=111300, grad_norm=4.050548076629639, loss=2.5064051151275635
I0127 06:01:20.522441 139656683312896 logging_writer.py:48] [111400] global_step=111400, grad_norm=4.4486470222473145, loss=2.371483564376831
I0127 06:01:54.739641 139656674920192 logging_writer.py:48] [111500] global_step=111500, grad_norm=4.109907150268555, loss=2.366779327392578
I0127 06:02:28.782166 139656683312896 logging_writer.py:48] [111600] global_step=111600, grad_norm=4.0245232582092285, loss=2.4212729930877686
I0127 06:03:02.829703 139656674920192 logging_writer.py:48] [111700] global_step=111700, grad_norm=4.279763698577881, loss=2.4248099327087402
I0127 06:03:36.903633 139656683312896 logging_writer.py:48] [111800] global_step=111800, grad_norm=4.180862903594971, loss=2.368238925933838
I0127 06:04:10.977005 139656674920192 logging_writer.py:48] [111900] global_step=111900, grad_norm=4.152929306030273, loss=2.3529062271118164
I0127 06:04:45.058370 139656683312896 logging_writer.py:48] [112000] global_step=112000, grad_norm=3.994809865951538, loss=2.418269395828247
I0127 06:05:19.131306 139656674920192 logging_writer.py:48] [112100] global_step=112100, grad_norm=4.4894280433654785, loss=2.3667798042297363
I0127 06:05:53.185194 139656683312896 logging_writer.py:48] [112200] global_step=112200, grad_norm=3.966195583343506, loss=2.3320014476776123
I0127 06:05:59.772336 139822745589568 spec.py:321] Evaluating on the training split.
I0127 06:06:06.071840 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 06:06:14.619630 139822745589568 spec.py:349] Evaluating on the test split.
I0127 06:06:16.918953 139822745589568 submission_runner.py:408] Time since start: 39770.78s, 	Step: 112221, 	{'train/accuracy': 0.7472297549247742, 'train/loss': 1.0725810527801514, 'validation/accuracy': 0.6823199987411499, 'validation/loss': 1.363050103187561, 'validation/num_examples': 50000, 'test/accuracy': 0.5624000430107117, 'test/loss': 2.0069241523742676, 'test/num_examples': 10000, 'score': 38310.2585234642, 'total_duration': 39770.780359745026, 'accumulated_submission_time': 38310.2585234642, 'accumulated_eval_time': 1452.9339735507965, 'accumulated_logging_time': 3.8331050872802734}
I0127 06:06:16.956897 139656666527488 logging_writer.py:48] [112221] accumulated_eval_time=1452.933974, accumulated_logging_time=3.833105, accumulated_submission_time=38310.258523, global_step=112221, preemption_count=0, score=38310.258523, test/accuracy=0.562400, test/loss=2.006924, test/num_examples=10000, total_duration=39770.780360, train/accuracy=0.747230, train/loss=1.072581, validation/accuracy=0.682320, validation/loss=1.363050, validation/num_examples=50000
I0127 06:06:44.179044 139656691705600 logging_writer.py:48] [112300] global_step=112300, grad_norm=4.523848056793213, loss=2.42116117477417
I0127 06:07:18.210029 139656666527488 logging_writer.py:48] [112400] global_step=112400, grad_norm=4.189570426940918, loss=2.4481425285339355
I0127 06:07:52.262216 139656691705600 logging_writer.py:48] [112500] global_step=112500, grad_norm=4.793610095977783, loss=2.4404337406158447
I0127 06:08:26.416745 139656666527488 logging_writer.py:48] [112600] global_step=112600, grad_norm=4.5024566650390625, loss=2.4310812950134277
I0127 06:09:00.490503 139656691705600 logging_writer.py:48] [112700] global_step=112700, grad_norm=4.316176414489746, loss=2.4369144439697266
I0127 06:09:34.555660 139656666527488 logging_writer.py:48] [112800] global_step=112800, grad_norm=3.938795566558838, loss=2.420515775680542
I0127 06:10:08.612894 139656691705600 logging_writer.py:48] [112900] global_step=112900, grad_norm=4.408012866973877, loss=2.4044582843780518
I0127 06:10:42.670564 139656666527488 logging_writer.py:48] [113000] global_step=113000, grad_norm=4.299966812133789, loss=2.371782064437866
I0127 06:11:16.753021 139656691705600 logging_writer.py:48] [113100] global_step=113100, grad_norm=4.0314040184021, loss=2.382697582244873
I0127 06:11:50.820616 139656666527488 logging_writer.py:48] [113200] global_step=113200, grad_norm=4.2151031494140625, loss=2.3276569843292236
I0127 06:12:24.862235 139656691705600 logging_writer.py:48] [113300] global_step=113300, grad_norm=3.9839398860931396, loss=2.3347675800323486
I0127 06:12:58.903491 139656666527488 logging_writer.py:48] [113400] global_step=113400, grad_norm=4.5842509269714355, loss=2.413116931915283
I0127 06:13:32.944941 139656691705600 logging_writer.py:48] [113500] global_step=113500, grad_norm=4.345894813537598, loss=2.4510104656219482
I0127 06:14:07.013727 139656666527488 logging_writer.py:48] [113600] global_step=113600, grad_norm=4.0314788818359375, loss=2.3772330284118652
I0127 06:14:41.179084 139656691705600 logging_writer.py:48] [113700] global_step=113700, grad_norm=4.907131671905518, loss=2.3378701210021973
I0127 06:14:47.112412 139822745589568 spec.py:321] Evaluating on the training split.
I0127 06:14:53.347706 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 06:15:02.009408 139822745589568 spec.py:349] Evaluating on the test split.
I0127 06:15:04.273367 139822745589568 submission_runner.py:408] Time since start: 40298.13s, 	Step: 113719, 	{'train/accuracy': 0.7479073405265808, 'train/loss': 1.0708001852035522, 'validation/accuracy': 0.6868199706077576, 'validation/loss': 1.3397966623306274, 'validation/num_examples': 50000, 'test/accuracy': 0.5601000189781189, 'test/loss': 2.0163300037384033, 'test/num_examples': 10000, 'score': 38820.35129117966, 'total_duration': 40298.13475847244, 'accumulated_submission_time': 38820.35129117966, 'accumulated_eval_time': 1470.0948634147644, 'accumulated_logging_time': 3.881237268447876}
I0127 06:15:04.309268 139656297445120 logging_writer.py:48] [113719] accumulated_eval_time=1470.094863, accumulated_logging_time=3.881237, accumulated_submission_time=38820.351291, global_step=113719, preemption_count=0, score=38820.351291, test/accuracy=0.560100, test/loss=2.016330, test/num_examples=10000, total_duration=40298.134758, train/accuracy=0.747907, train/loss=1.070800, validation/accuracy=0.686820, validation/loss=1.339797, validation/num_examples=50000
I0127 06:15:32.219606 139656649742080 logging_writer.py:48] [113800] global_step=113800, grad_norm=4.122278213500977, loss=2.3999853134155273
I0127 06:16:06.260153 139656297445120 logging_writer.py:48] [113900] global_step=113900, grad_norm=4.341429710388184, loss=2.391488552093506
I0127 06:16:40.328515 139656649742080 logging_writer.py:48] [114000] global_step=114000, grad_norm=3.9474899768829346, loss=2.456319570541382
I0127 06:17:14.404295 139656297445120 logging_writer.py:48] [114100] global_step=114100, grad_norm=4.645936965942383, loss=2.4088478088378906
I0127 06:17:48.489936 139656649742080 logging_writer.py:48] [114200] global_step=114200, grad_norm=3.8022148609161377, loss=2.365572214126587
I0127 06:18:22.550005 139656297445120 logging_writer.py:48] [114300] global_step=114300, grad_norm=3.7527389526367188, loss=2.3504679203033447
I0127 06:18:56.624202 139656649742080 logging_writer.py:48] [114400] global_step=114400, grad_norm=4.6241021156311035, loss=2.4173390865325928
I0127 06:19:30.708526 139656297445120 logging_writer.py:48] [114500] global_step=114500, grad_norm=4.034279823303223, loss=2.4243361949920654
I0127 06:20:04.779197 139656649742080 logging_writer.py:48] [114600] global_step=114600, grad_norm=4.251708984375, loss=2.4976115226745605
I0127 06:20:38.891822 139656297445120 logging_writer.py:48] [114700] global_step=114700, grad_norm=4.621498107910156, loss=2.4587130546569824
I0127 06:21:12.961796 139656649742080 logging_writer.py:48] [114800] global_step=114800, grad_norm=4.370200157165527, loss=2.446904182434082
I0127 06:21:47.060051 139656297445120 logging_writer.py:48] [114900] global_step=114900, grad_norm=4.198333263397217, loss=2.4462151527404785
I0127 06:22:21.129239 139656649742080 logging_writer.py:48] [115000] global_step=115000, grad_norm=4.135072231292725, loss=2.415404796600342
I0127 06:22:55.206835 139656297445120 logging_writer.py:48] [115100] global_step=115100, grad_norm=4.166737079620361, loss=2.2952003479003906
I0127 06:23:29.238016 139656649742080 logging_writer.py:48] [115200] global_step=115200, grad_norm=4.041244029998779, loss=2.3591196537017822
I0127 06:23:34.496642 139822745589568 spec.py:321] Evaluating on the training split.
I0127 06:23:40.742017 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 06:23:49.414939 139822745589568 spec.py:349] Evaluating on the test split.
I0127 06:23:51.719413 139822745589568 submission_runner.py:408] Time since start: 40825.58s, 	Step: 115217, 	{'train/accuracy': 0.7872887253761292, 'train/loss': 0.9008607268333435, 'validation/accuracy': 0.6908800005912781, 'validation/loss': 1.3157756328582764, 'validation/num_examples': 50000, 'test/accuracy': 0.5639000535011292, 'test/loss': 1.9897371530532837, 'test/num_examples': 10000, 'score': 39330.478276491165, 'total_duration': 40825.580815792084, 'accumulated_submission_time': 39330.478276491165, 'accumulated_eval_time': 1487.3175942897797, 'accumulated_logging_time': 3.9274373054504395}
I0127 06:23:51.758928 139656691705600 logging_writer.py:48] [115217] accumulated_eval_time=1487.317594, accumulated_logging_time=3.927437, accumulated_submission_time=39330.478276, global_step=115217, preemption_count=0, score=39330.478276, test/accuracy=0.563900, test/loss=1.989737, test/num_examples=10000, total_duration=40825.580816, train/accuracy=0.787289, train/loss=0.900861, validation/accuracy=0.690880, validation/loss=1.315776, validation/num_examples=50000
I0127 06:24:20.336871 139656700098304 logging_writer.py:48] [115300] global_step=115300, grad_norm=4.788458347320557, loss=2.3613431453704834
I0127 06:24:54.354535 139656691705600 logging_writer.py:48] [115400] global_step=115400, grad_norm=4.507634162902832, loss=2.3493332862854004
I0127 06:25:28.391600 139656700098304 logging_writer.py:48] [115500] global_step=115500, grad_norm=4.089771747589111, loss=2.3028035163879395
I0127 06:26:02.448154 139656691705600 logging_writer.py:48] [115600] global_step=115600, grad_norm=4.845076084136963, loss=2.437774181365967
I0127 06:26:36.577175 139656700098304 logging_writer.py:48] [115700] global_step=115700, grad_norm=4.454658031463623, loss=2.3098278045654297
I0127 06:27:10.604186 139656691705600 logging_writer.py:48] [115800] global_step=115800, grad_norm=3.9331254959106445, loss=2.358731985092163
I0127 06:27:44.632645 139656700098304 logging_writer.py:48] [115900] global_step=115900, grad_norm=4.633487224578857, loss=2.2993698120117188
I0127 06:28:18.665212 139656691705600 logging_writer.py:48] [116000] global_step=116000, grad_norm=4.296331882476807, loss=2.371722459793091
I0127 06:28:52.729170 139656700098304 logging_writer.py:48] [116100] global_step=116100, grad_norm=3.9429714679718018, loss=2.397087335586548
I0127 06:29:26.780843 139656691705600 logging_writer.py:48] [116200] global_step=116200, grad_norm=4.36553955078125, loss=2.360159397125244
I0127 06:30:00.825691 139656700098304 logging_writer.py:48] [116300] global_step=116300, grad_norm=4.073462963104248, loss=2.412905216217041
I0127 06:30:34.833694 139656691705600 logging_writer.py:48] [116400] global_step=116400, grad_norm=4.05638313293457, loss=2.3666300773620605
I0127 06:31:08.866841 139656700098304 logging_writer.py:48] [116500] global_step=116500, grad_norm=4.982603073120117, loss=2.3405284881591797
I0127 06:31:42.946632 139656691705600 logging_writer.py:48] [116600] global_step=116600, grad_norm=4.3704938888549805, loss=2.4202890396118164
I0127 06:32:17.007525 139656700098304 logging_writer.py:48] [116700] global_step=116700, grad_norm=4.90178108215332, loss=2.3526992797851562
I0127 06:32:21.918089 139822745589568 spec.py:321] Evaluating on the training split.
I0127 06:32:28.241766 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 06:32:36.902994 139822745589568 spec.py:349] Evaluating on the test split.
I0127 06:32:39.206679 139822745589568 submission_runner.py:408] Time since start: 41353.07s, 	Step: 116716, 	{'train/accuracy': 0.7737165093421936, 'train/loss': 0.961524486541748, 'validation/accuracy': 0.6925399899482727, 'validation/loss': 1.3126429319381714, 'validation/num_examples': 50000, 'test/accuracy': 0.5701000094413757, 'test/loss': 1.9820610284805298, 'test/num_examples': 10000, 'score': 39840.576077222824, 'total_duration': 41353.06807017326, 'accumulated_submission_time': 39840.576077222824, 'accumulated_eval_time': 1504.6061329841614, 'accumulated_logging_time': 3.9768893718719482}
I0127 06:32:39.250331 139656658134784 logging_writer.py:48] [116716] accumulated_eval_time=1504.606133, accumulated_logging_time=3.976889, accumulated_submission_time=39840.576077, global_step=116716, preemption_count=0, score=39840.576077, test/accuracy=0.570100, test/loss=1.982061, test/num_examples=10000, total_duration=41353.068070, train/accuracy=0.773717, train/loss=0.961524, validation/accuracy=0.692540, validation/loss=1.312643, validation/num_examples=50000
I0127 06:33:08.230934 139656666527488 logging_writer.py:48] [116800] global_step=116800, grad_norm=4.746757507324219, loss=2.4081075191497803
I0127 06:33:42.262085 139656658134784 logging_writer.py:48] [116900] global_step=116900, grad_norm=4.53244161605835, loss=2.43160343170166
I0127 06:34:16.304353 139656666527488 logging_writer.py:48] [117000] global_step=117000, grad_norm=4.199419021606445, loss=2.3326151371002197
I0127 06:34:50.365028 139656658134784 logging_writer.py:48] [117100] global_step=117100, grad_norm=4.806034088134766, loss=2.2840161323547363
I0127 06:35:24.421686 139656666527488 logging_writer.py:48] [117200] global_step=117200, grad_norm=4.500570297241211, loss=2.3328795433044434
I0127 06:35:58.494118 139656658134784 logging_writer.py:48] [117300] global_step=117300, grad_norm=4.277984619140625, loss=2.2854676246643066
I0127 06:36:32.536665 139656666527488 logging_writer.py:48] [117400] global_step=117400, grad_norm=4.617767333984375, loss=2.340790033340454
I0127 06:37:06.600845 139656658134784 logging_writer.py:48] [117500] global_step=117500, grad_norm=4.46066951751709, loss=2.378574848175049
I0127 06:37:40.651075 139656666527488 logging_writer.py:48] [117600] global_step=117600, grad_norm=4.403536796569824, loss=2.4103479385375977
I0127 06:38:14.699571 139656658134784 logging_writer.py:48] [117700] global_step=117700, grad_norm=4.2187604904174805, loss=2.382256031036377
I0127 06:38:48.749372 139656666527488 logging_writer.py:48] [117800] global_step=117800, grad_norm=4.669776439666748, loss=2.3588690757751465
I0127 06:39:22.891464 139656658134784 logging_writer.py:48] [117900] global_step=117900, grad_norm=4.336557865142822, loss=2.3356850147247314
I0127 06:39:56.980733 139656666527488 logging_writer.py:48] [118000] global_step=118000, grad_norm=4.403461456298828, loss=2.326512336730957
I0127 06:40:31.046883 139656658134784 logging_writer.py:48] [118100] global_step=118100, grad_norm=4.477179050445557, loss=2.3031251430511475
I0127 06:41:05.141197 139656666527488 logging_writer.py:48] [118200] global_step=118200, grad_norm=4.039165019989014, loss=2.377417802810669
I0127 06:41:09.378750 139822745589568 spec.py:321] Evaluating on the training split.
I0127 06:41:15.687520 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 06:41:24.310788 139822745589568 spec.py:349] Evaluating on the test split.
I0127 06:41:26.608953 139822745589568 submission_runner.py:408] Time since start: 41880.47s, 	Step: 118214, 	{'train/accuracy': 0.7669204473495483, 'train/loss': 1.010676383972168, 'validation/accuracy': 0.6954599618911743, 'validation/loss': 1.3212053775787354, 'validation/num_examples': 50000, 'test/accuracy': 0.5715000033378601, 'test/loss': 1.9586877822875977, 'test/num_examples': 10000, 'score': 40350.64465737343, 'total_duration': 41880.47035765648, 'accumulated_submission_time': 40350.64465737343, 'accumulated_eval_time': 1521.836299419403, 'accumulated_logging_time': 4.030749559402466}
I0127 06:41:26.649173 139656658134784 logging_writer.py:48] [118214] accumulated_eval_time=1521.836299, accumulated_logging_time=4.030750, accumulated_submission_time=40350.644657, global_step=118214, preemption_count=0, score=40350.644657, test/accuracy=0.571500, test/loss=1.958688, test/num_examples=10000, total_duration=41880.470358, train/accuracy=0.766920, train/loss=1.010676, validation/accuracy=0.695460, validation/loss=1.321205, validation/num_examples=50000
I0127 06:41:56.267946 139656666527488 logging_writer.py:48] [118300] global_step=118300, grad_norm=4.908730983734131, loss=2.448477268218994
I0127 06:42:30.315666 139656658134784 logging_writer.py:48] [118400] global_step=118400, grad_norm=4.324087142944336, loss=2.33849835395813
I0127 06:43:04.364130 139656666527488 logging_writer.py:48] [118500] global_step=118500, grad_norm=4.316549301147461, loss=2.338848114013672
I0127 06:43:38.436957 139656658134784 logging_writer.py:48] [118600] global_step=118600, grad_norm=4.27282190322876, loss=2.4007840156555176
I0127 06:44:12.525161 139656666527488 logging_writer.py:48] [118700] global_step=118700, grad_norm=5.287309169769287, loss=2.3823766708374023
I0127 06:44:46.584132 139656658134784 logging_writer.py:48] [118800] global_step=118800, grad_norm=4.642270565032959, loss=2.394958019256592
I0127 06:45:20.692465 139656666527488 logging_writer.py:48] [118900] global_step=118900, grad_norm=4.361474990844727, loss=2.3713388442993164
I0127 06:45:54.736905 139656658134784 logging_writer.py:48] [119000] global_step=119000, grad_norm=4.147728443145752, loss=2.339244842529297
I0127 06:46:28.799306 139656666527488 logging_writer.py:48] [119100] global_step=119100, grad_norm=4.783782958984375, loss=2.388916492462158
I0127 06:47:02.874732 139656658134784 logging_writer.py:48] [119200] global_step=119200, grad_norm=4.751754283905029, loss=2.348435878753662
I0127 06:47:36.943706 139656666527488 logging_writer.py:48] [119300] global_step=119300, grad_norm=4.517587184906006, loss=2.4328439235687256
I0127 06:48:11.021897 139656658134784 logging_writer.py:48] [119400] global_step=119400, grad_norm=4.633920669555664, loss=2.4408562183380127
I0127 06:48:45.070263 139656666527488 logging_writer.py:48] [119500] global_step=119500, grad_norm=4.62427282333374, loss=2.345376491546631
I0127 06:49:19.119070 139656658134784 logging_writer.py:48] [119600] global_step=119600, grad_norm=4.475279331207275, loss=2.308924436569214
I0127 06:49:53.178303 139656666527488 logging_writer.py:48] [119700] global_step=119700, grad_norm=4.513437271118164, loss=2.2963931560516357
I0127 06:49:56.721077 139822745589568 spec.py:321] Evaluating on the training split.
I0127 06:50:03.214067 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 06:50:11.828316 139822745589568 spec.py:349] Evaluating on the test split.
I0127 06:50:14.122164 139822745589568 submission_runner.py:408] Time since start: 42407.98s, 	Step: 119712, 	{'train/accuracy': 0.7684550285339355, 'train/loss': 1.0029422044754028, 'validation/accuracy': 0.6985799670219421, 'validation/loss': 1.310947299003601, 'validation/num_examples': 50000, 'test/accuracy': 0.5733000040054321, 'test/loss': 1.962609052658081, 'test/num_examples': 10000, 'score': 40860.65657663345, 'total_duration': 42407.98349690437, 'accumulated_submission_time': 40860.65657663345, 'accumulated_eval_time': 1539.2372624874115, 'accumulated_logging_time': 4.080804824829102}
I0127 06:50:14.159179 139656649742080 logging_writer.py:48] [119712] accumulated_eval_time=1539.237262, accumulated_logging_time=4.080805, accumulated_submission_time=40860.656577, global_step=119712, preemption_count=0, score=40860.656577, test/accuracy=0.573300, test/loss=1.962609, test/num_examples=10000, total_duration=42407.983497, train/accuracy=0.768455, train/loss=1.002942, validation/accuracy=0.698580, validation/loss=1.310947, validation/num_examples=50000
I0127 06:50:44.434081 139656674920192 logging_writer.py:48] [119800] global_step=119800, grad_norm=4.8434739112854, loss=2.4268856048583984
I0127 06:51:18.483896 139656649742080 logging_writer.py:48] [119900] global_step=119900, grad_norm=4.8573455810546875, loss=2.369048833847046
I0127 06:51:52.540340 139656674920192 logging_writer.py:48] [120000] global_step=120000, grad_norm=4.3523268699646, loss=2.347257614135742
I0127 06:52:26.588530 139656649742080 logging_writer.py:48] [120100] global_step=120100, grad_norm=4.0476250648498535, loss=2.3580873012542725
I0127 06:53:00.654822 139656674920192 logging_writer.py:48] [120200] global_step=120200, grad_norm=4.424360752105713, loss=2.302009344100952
I0127 06:53:34.685222 139656649742080 logging_writer.py:48] [120300] global_step=120300, grad_norm=4.474461078643799, loss=2.365318775177002
I0127 06:54:08.760402 139656674920192 logging_writer.py:48] [120400] global_step=120400, grad_norm=4.58250617980957, loss=2.4156253337860107
I0127 06:54:42.833753 139656649742080 logging_writer.py:48] [120500] global_step=120500, grad_norm=4.686264991760254, loss=2.480353355407715
I0127 06:55:16.902562 139656674920192 logging_writer.py:48] [120600] global_step=120600, grad_norm=4.294918537139893, loss=2.39312481880188
I0127 06:55:50.993800 139656649742080 logging_writer.py:48] [120700] global_step=120700, grad_norm=4.2471513748168945, loss=2.32338809967041
I0127 06:56:25.036062 139656674920192 logging_writer.py:48] [120800] global_step=120800, grad_norm=4.312180519104004, loss=2.3383002281188965
I0127 06:56:59.094512 139656649742080 logging_writer.py:48] [120900] global_step=120900, grad_norm=4.516228675842285, loss=2.3192451000213623
I0127 06:57:33.160310 139656674920192 logging_writer.py:48] [121000] global_step=121000, grad_norm=4.551607608795166, loss=2.3031301498413086
I0127 06:58:07.196249 139656649742080 logging_writer.py:48] [121100] global_step=121100, grad_norm=5.262760639190674, loss=2.3151540756225586
I0127 06:58:41.271250 139656674920192 logging_writer.py:48] [121200] global_step=121200, grad_norm=4.786339282989502, loss=2.329486131668091
I0127 06:58:44.137768 139822745589568 spec.py:321] Evaluating on the training split.
I0127 06:58:50.411523 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 06:58:59.174618 139822745589568 spec.py:349] Evaluating on the test split.
I0127 06:59:01.459844 139822745589568 submission_runner.py:408] Time since start: 42935.32s, 	Step: 121210, 	{'train/accuracy': 0.7650868892669678, 'train/loss': 0.9947850704193115, 'validation/accuracy': 0.6988399624824524, 'validation/loss': 1.2936506271362305, 'validation/num_examples': 50000, 'test/accuracy': 0.5770000219345093, 'test/loss': 1.9499543905258179, 'test/num_examples': 10000, 'score': 41370.574806690216, 'total_duration': 42935.32121896744, 'accumulated_submission_time': 41370.574806690216, 'accumulated_eval_time': 1556.559273481369, 'accumulated_logging_time': 4.127295732498169}
I0127 06:59:01.515321 139656658134784 logging_writer.py:48] [121210] accumulated_eval_time=1556.559273, accumulated_logging_time=4.127296, accumulated_submission_time=41370.574807, global_step=121210, preemption_count=0, score=41370.574807, test/accuracy=0.577000, test/loss=1.949954, test/num_examples=10000, total_duration=42935.321219, train/accuracy=0.765087, train/loss=0.994785, validation/accuracy=0.698840, validation/loss=1.293651, validation/num_examples=50000
I0127 06:59:32.514596 139656666527488 logging_writer.py:48] [121300] global_step=121300, grad_norm=3.9610352516174316, loss=2.258399486541748
I0127 07:00:06.559659 139656658134784 logging_writer.py:48] [121400] global_step=121400, grad_norm=5.2853240966796875, loss=2.455406904220581
I0127 07:00:40.599419 139656666527488 logging_writer.py:48] [121500] global_step=121500, grad_norm=4.653036117553711, loss=2.3059678077697754
I0127 07:01:14.685785 139656658134784 logging_writer.py:48] [121600] global_step=121600, grad_norm=4.379976272583008, loss=2.308368444442749
I0127 07:01:48.755326 139656666527488 logging_writer.py:48] [121700] global_step=121700, grad_norm=4.156039714813232, loss=2.3750510215759277
I0127 07:02:22.791809 139656658134784 logging_writer.py:48] [121800] global_step=121800, grad_norm=4.47255802154541, loss=2.411984920501709
I0127 07:02:56.834543 139656666527488 logging_writer.py:48] [121900] global_step=121900, grad_norm=4.981988906860352, loss=2.3546628952026367
I0127 07:03:31.060297 139656658134784 logging_writer.py:48] [122000] global_step=122000, grad_norm=4.359712600708008, loss=2.3532161712646484
I0127 07:04:05.106366 139656666527488 logging_writer.py:48] [122100] global_step=122100, grad_norm=4.533781051635742, loss=2.28682279586792
I0127 07:04:39.153463 139656658134784 logging_writer.py:48] [122200] global_step=122200, grad_norm=4.651174545288086, loss=2.2954931259155273
I0127 07:05:13.204763 139656666527488 logging_writer.py:48] [122300] global_step=122300, grad_norm=4.933935642242432, loss=2.4040842056274414
I0127 07:05:47.240937 139656658134784 logging_writer.py:48] [122400] global_step=122400, grad_norm=4.5199666023254395, loss=2.3102736473083496
I0127 07:06:21.276485 139656666527488 logging_writer.py:48] [122500] global_step=122500, grad_norm=5.603389739990234, loss=2.3634629249572754
I0127 07:06:55.301191 139656658134784 logging_writer.py:48] [122600] global_step=122600, grad_norm=4.975396156311035, loss=2.3539042472839355
I0127 07:07:29.355958 139656666527488 logging_writer.py:48] [122700] global_step=122700, grad_norm=4.61071252822876, loss=2.393193244934082
I0127 07:07:31.555659 139822745589568 spec.py:321] Evaluating on the training split.
I0127 07:07:37.743690 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 07:07:46.396942 139822745589568 spec.py:349] Evaluating on the test split.
I0127 07:07:48.667526 139822745589568 submission_runner.py:408] Time since start: 43462.53s, 	Step: 122708, 	{'train/accuracy': 0.7655652165412903, 'train/loss': 1.012738823890686, 'validation/accuracy': 0.6997199654579163, 'validation/loss': 1.3031195402145386, 'validation/num_examples': 50000, 'test/accuracy': 0.5733000040054321, 'test/loss': 1.966423749923706, 'test/num_examples': 10000, 'score': 41880.55111408234, 'total_duration': 43462.52871155739, 'accumulated_submission_time': 41880.55111408234, 'accumulated_eval_time': 1573.6708698272705, 'accumulated_logging_time': 4.196326732635498}
I0127 07:07:48.708729 139656297445120 logging_writer.py:48] [122708] accumulated_eval_time=1573.670870, accumulated_logging_time=4.196327, accumulated_submission_time=41880.551114, global_step=122708, preemption_count=0, score=41880.551114, test/accuracy=0.573300, test/loss=1.966424, test/num_examples=10000, total_duration=43462.528712, train/accuracy=0.765565, train/loss=1.012739, validation/accuracy=0.699720, validation/loss=1.303120, validation/num_examples=50000
I0127 07:08:20.313689 139656649742080 logging_writer.py:48] [122800] global_step=122800, grad_norm=4.966195583343506, loss=2.287738084793091
I0127 07:08:54.343294 139656297445120 logging_writer.py:48] [122900] global_step=122900, grad_norm=4.300537586212158, loss=2.3607523441314697
I0127 07:09:28.418511 139656649742080 logging_writer.py:48] [123000] global_step=123000, grad_norm=4.828794002532959, loss=2.4045584201812744
I0127 07:10:02.596934 139656297445120 logging_writer.py:48] [123100] global_step=123100, grad_norm=4.621440887451172, loss=2.383388042449951
I0127 07:10:36.688156 139656649742080 logging_writer.py:48] [123200] global_step=123200, grad_norm=4.943218231201172, loss=2.323659896850586
I0127 07:11:10.751317 139656297445120 logging_writer.py:48] [123300] global_step=123300, grad_norm=4.551092624664307, loss=2.2320516109466553
I0127 07:11:44.819902 139656649742080 logging_writer.py:48] [123400] global_step=123400, grad_norm=4.615607738494873, loss=2.3414719104766846
I0127 07:12:18.911597 139656297445120 logging_writer.py:48] [123500] global_step=123500, grad_norm=4.418788909912109, loss=2.300729513168335
I0127 07:12:52.996993 139656649742080 logging_writer.py:48] [123600] global_step=123600, grad_norm=4.457913398742676, loss=2.340150833129883
I0127 07:13:27.074506 139656297445120 logging_writer.py:48] [123700] global_step=123700, grad_norm=4.809876441955566, loss=2.378401756286621
I0127 07:14:01.155032 139656649742080 logging_writer.py:48] [123800] global_step=123800, grad_norm=4.831469535827637, loss=2.374704360961914
I0127 07:14:35.218626 139656297445120 logging_writer.py:48] [123900] global_step=123900, grad_norm=4.460507392883301, loss=2.3199703693389893
I0127 07:15:09.289601 139656649742080 logging_writer.py:48] [124000] global_step=124000, grad_norm=5.100496292114258, loss=2.3118419647216797
I0127 07:15:43.346607 139656297445120 logging_writer.py:48] [124100] global_step=124100, grad_norm=4.5716729164123535, loss=2.34708833694458
I0127 07:16:17.464998 139656649742080 logging_writer.py:48] [124200] global_step=124200, grad_norm=4.55540132522583, loss=2.2926082611083984
I0127 07:16:18.972628 139822745589568 spec.py:321] Evaluating on the training split.
I0127 07:16:25.243804 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 07:16:33.746332 139822745589568 spec.py:349] Evaluating on the test split.
I0127 07:16:36.038610 139822745589568 submission_runner.py:408] Time since start: 43989.90s, 	Step: 124206, 	{'train/accuracy': 0.7978515625, 'train/loss': 0.8683147430419922, 'validation/accuracy': 0.7073799967765808, 'validation/loss': 1.261612057685852, 'validation/num_examples': 50000, 'test/accuracy': 0.5795000195503235, 'test/loss': 1.902595043182373, 'test/num_examples': 10000, 'score': 42390.75530362129, 'total_duration': 43989.899998903275, 'accumulated_submission_time': 42390.75530362129, 'accumulated_eval_time': 1590.7367997169495, 'accumulated_logging_time': 4.2473227977752686}
I0127 07:16:36.074616 139656674920192 logging_writer.py:48] [124206] accumulated_eval_time=1590.736800, accumulated_logging_time=4.247323, accumulated_submission_time=42390.755304, global_step=124206, preemption_count=0, score=42390.755304, test/accuracy=0.579500, test/loss=1.902595, test/num_examples=10000, total_duration=43989.899999, train/accuracy=0.797852, train/loss=0.868315, validation/accuracy=0.707380, validation/loss=1.261612, validation/num_examples=50000
I0127 07:17:08.378438 139656683312896 logging_writer.py:48] [124300] global_step=124300, grad_norm=4.428791522979736, loss=2.2692790031433105
I0127 07:17:42.399289 139656674920192 logging_writer.py:48] [124400] global_step=124400, grad_norm=4.570363998413086, loss=2.310314416885376
I0127 07:18:16.461878 139656683312896 logging_writer.py:48] [124500] global_step=124500, grad_norm=4.485098361968994, loss=2.2577579021453857
I0127 07:18:50.521598 139656674920192 logging_writer.py:48] [124600] global_step=124600, grad_norm=4.624546527862549, loss=2.2653462886810303
I0127 07:19:24.567338 139656683312896 logging_writer.py:48] [124700] global_step=124700, grad_norm=4.879073143005371, loss=2.2738425731658936
I0127 07:19:58.579037 139656674920192 logging_writer.py:48] [124800] global_step=124800, grad_norm=4.7360124588012695, loss=2.177335023880005
I0127 07:20:32.642723 139656683312896 logging_writer.py:48] [124900] global_step=124900, grad_norm=4.587413787841797, loss=2.2608225345611572
I0127 07:21:06.695891 139656674920192 logging_writer.py:48] [125000] global_step=125000, grad_norm=4.660889625549316, loss=2.227254867553711
I0127 07:21:40.732789 139656683312896 logging_writer.py:48] [125100] global_step=125100, grad_norm=4.51668643951416, loss=2.2933034896850586
I0127 07:22:14.854795 139656674920192 logging_writer.py:48] [125200] global_step=125200, grad_norm=4.541111469268799, loss=2.3664050102233887
I0127 07:22:48.916806 139656683312896 logging_writer.py:48] [125300] global_step=125300, grad_norm=5.195541858673096, loss=2.273568868637085
I0127 07:23:22.940955 139656674920192 logging_writer.py:48] [125400] global_step=125400, grad_norm=4.592690944671631, loss=2.3777518272399902
I0127 07:23:56.993538 139656683312896 logging_writer.py:48] [125500] global_step=125500, grad_norm=4.532978057861328, loss=2.425093650817871
I0127 07:24:31.066405 139656674920192 logging_writer.py:48] [125600] global_step=125600, grad_norm=5.568973541259766, loss=2.3440873622894287
I0127 07:25:05.113075 139656683312896 logging_writer.py:48] [125700] global_step=125700, grad_norm=4.682182312011719, loss=2.2219462394714355
I0127 07:25:06.273469 139822745589568 spec.py:321] Evaluating on the training split.
I0127 07:25:12.612828 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 07:25:21.169987 139822745589568 spec.py:349] Evaluating on the test split.
I0127 07:25:23.490570 139822745589568 submission_runner.py:408] Time since start: 44517.35s, 	Step: 125705, 	{'train/accuracy': 0.7971938848495483, 'train/loss': 0.8802825808525085, 'validation/accuracy': 0.7064599990844727, 'validation/loss': 1.266579031944275, 'validation/num_examples': 50000, 'test/accuracy': 0.5808000564575195, 'test/loss': 1.9092512130737305, 'test/num_examples': 10000, 'score': 42900.89259982109, 'total_duration': 44517.35197234154, 'accumulated_submission_time': 42900.89259982109, 'accumulated_eval_time': 1607.9538469314575, 'accumulated_logging_time': 4.293523788452148}
I0127 07:25:23.532215 139656658134784 logging_writer.py:48] [125705] accumulated_eval_time=1607.953847, accumulated_logging_time=4.293524, accumulated_submission_time=42900.892600, global_step=125705, preemption_count=0, score=42900.892600, test/accuracy=0.580800, test/loss=1.909251, test/num_examples=10000, total_duration=44517.351972, train/accuracy=0.797194, train/loss=0.880283, validation/accuracy=0.706460, validation/loss=1.266579, validation/num_examples=50000
I0127 07:25:56.220427 139656666527488 logging_writer.py:48] [125800] global_step=125800, grad_norm=4.989776611328125, loss=2.3911631107330322
I0127 07:26:30.248550 139656658134784 logging_writer.py:48] [125900] global_step=125900, grad_norm=4.922996520996094, loss=2.3070333003997803
I0127 07:27:04.317193 139656666527488 logging_writer.py:48] [126000] global_step=126000, grad_norm=5.053068161010742, loss=2.3719048500061035
I0127 07:27:38.373979 139656658134784 logging_writer.py:48] [126100] global_step=126100, grad_norm=5.2718400955200195, loss=2.2653608322143555
I0127 07:28:12.441659 139656666527488 logging_writer.py:48] [126200] global_step=126200, grad_norm=4.728599548339844, loss=2.330791711807251
I0127 07:28:46.543012 139656658134784 logging_writer.py:48] [126300] global_step=126300, grad_norm=5.021413803100586, loss=2.3782429695129395
I0127 07:29:20.603317 139656666527488 logging_writer.py:48] [126400] global_step=126400, grad_norm=5.5132155418396, loss=2.3001883029937744
I0127 07:29:54.644753 139656658134784 logging_writer.py:48] [126500] global_step=126500, grad_norm=4.698856830596924, loss=2.21533203125
I0127 07:30:28.717071 139656666527488 logging_writer.py:48] [126600] global_step=126600, grad_norm=4.756938457489014, loss=2.355597496032715
I0127 07:31:02.753468 139656658134784 logging_writer.py:48] [126700] global_step=126700, grad_norm=5.112481117248535, loss=2.307311534881592
I0127 07:31:36.810639 139656666527488 logging_writer.py:48] [126800] global_step=126800, grad_norm=6.259881496429443, loss=2.315018653869629
I0127 07:32:10.867602 139656658134784 logging_writer.py:48] [126900] global_step=126900, grad_norm=4.820547103881836, loss=2.3821046352386475
I0127 07:32:44.921750 139656666527488 logging_writer.py:48] [127000] global_step=127000, grad_norm=4.556483745574951, loss=2.257913589477539
I0127 07:33:18.999848 139656658134784 logging_writer.py:48] [127100] global_step=127100, grad_norm=4.807771682739258, loss=2.3369147777557373
I0127 07:33:53.331524 139656666527488 logging_writer.py:48] [127200] global_step=127200, grad_norm=5.31980562210083, loss=2.265066146850586
I0127 07:33:53.491987 139822745589568 spec.py:321] Evaluating on the training split.
I0127 07:33:59.867149 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 07:34:08.583886 139822745589568 spec.py:349] Evaluating on the test split.
I0127 07:34:10.869948 139822745589568 submission_runner.py:408] Time since start: 45044.73s, 	Step: 127202, 	{'train/accuracy': 0.7934072017669678, 'train/loss': 0.8894890546798706, 'validation/accuracy': 0.7084199786186218, 'validation/loss': 1.2445156574249268, 'validation/num_examples': 50000, 'test/accuracy': 0.5782000422477722, 'test/loss': 1.8972203731536865, 'test/num_examples': 10000, 'score': 43410.791429281235, 'total_duration': 45044.73135638237, 'accumulated_submission_time': 43410.791429281235, 'accumulated_eval_time': 1625.3317823410034, 'accumulated_logging_time': 4.347227096557617}
I0127 07:34:10.912269 139656649742080 logging_writer.py:48] [127202] accumulated_eval_time=1625.331782, accumulated_logging_time=4.347227, accumulated_submission_time=43410.791429, global_step=127202, preemption_count=0, score=43410.791429, test/accuracy=0.578200, test/loss=1.897220, test/num_examples=10000, total_duration=45044.731356, train/accuracy=0.793407, train/loss=0.889489, validation/accuracy=0.708420, validation/loss=1.244516, validation/num_examples=50000
I0127 07:34:44.771971 139656683312896 logging_writer.py:48] [127300] global_step=127300, grad_norm=4.880733966827393, loss=2.299818754196167
I0127 07:35:18.818891 139656649742080 logging_writer.py:48] [127400] global_step=127400, grad_norm=4.941796779632568, loss=2.249269723892212
I0127 07:35:52.881861 139656683312896 logging_writer.py:48] [127500] global_step=127500, grad_norm=4.776216983795166, loss=2.157834529876709
I0127 07:36:26.966908 139656649742080 logging_writer.py:48] [127600] global_step=127600, grad_norm=4.668805122375488, loss=2.3128011226654053
I0127 07:37:01.040815 139656683312896 logging_writer.py:48] [127700] global_step=127700, grad_norm=5.101533889770508, loss=2.338101387023926
I0127 07:37:35.094680 139656649742080 logging_writer.py:48] [127800] global_step=127800, grad_norm=4.772897720336914, loss=2.3862099647521973
I0127 07:38:09.148329 139656683312896 logging_writer.py:48] [127900] global_step=127900, grad_norm=4.823097229003906, loss=2.2403688430786133
I0127 07:38:43.200810 139656649742080 logging_writer.py:48] [128000] global_step=128000, grad_norm=4.768134593963623, loss=2.3315184116363525
I0127 07:39:17.275715 139656683312896 logging_writer.py:48] [128100] global_step=128100, grad_norm=4.669795513153076, loss=2.2887680530548096
I0127 07:39:51.345061 139656649742080 logging_writer.py:48] [128200] global_step=128200, grad_norm=4.850890636444092, loss=2.226905584335327
I0127 07:40:25.433446 139656683312896 logging_writer.py:48] [128300] global_step=128300, grad_norm=5.146143913269043, loss=2.2791929244995117
I0127 07:40:59.610815 139656649742080 logging_writer.py:48] [128400] global_step=128400, grad_norm=4.987081527709961, loss=2.2523393630981445
I0127 07:41:33.691103 139656683312896 logging_writer.py:48] [128500] global_step=128500, grad_norm=4.784512042999268, loss=2.254145383834839
I0127 07:42:07.754505 139656649742080 logging_writer.py:48] [128600] global_step=128600, grad_norm=4.95505428314209, loss=2.2466230392456055
I0127 07:42:40.946917 139822745589568 spec.py:321] Evaluating on the training split.
I0127 07:42:47.163791 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 07:42:55.649889 139822745589568 spec.py:349] Evaluating on the test split.
I0127 07:42:57.954935 139822745589568 submission_runner.py:408] Time since start: 45571.82s, 	Step: 128699, 	{'train/accuracy': 0.7883649468421936, 'train/loss': 0.9085213541984558, 'validation/accuracy': 0.7102800011634827, 'validation/loss': 1.2549796104431152, 'validation/num_examples': 50000, 'test/accuracy': 0.5886000394821167, 'test/loss': 1.8822438716888428, 'test/num_examples': 10000, 'score': 43920.76516246796, 'total_duration': 45571.81634020805, 'accumulated_submission_time': 43920.76516246796, 'accumulated_eval_time': 1642.3397538661957, 'accumulated_logging_time': 4.400299549102783}
I0127 07:42:57.995441 139656666527488 logging_writer.py:48] [128699] accumulated_eval_time=1642.339754, accumulated_logging_time=4.400300, accumulated_submission_time=43920.765162, global_step=128699, preemption_count=0, score=43920.765162, test/accuracy=0.588600, test/loss=1.882244, test/num_examples=10000, total_duration=45571.816340, train/accuracy=0.788365, train/loss=0.908521, validation/accuracy=0.710280, validation/loss=1.254980, validation/num_examples=50000
I0127 07:42:58.686618 139656674920192 logging_writer.py:48] [128700] global_step=128700, grad_norm=4.831655502319336, loss=2.216115951538086
I0127 07:43:32.704897 139656666527488 logging_writer.py:48] [128800] global_step=128800, grad_norm=5.5753889083862305, loss=2.3182318210601807
I0127 07:44:06.740490 139656674920192 logging_writer.py:48] [128900] global_step=128900, grad_norm=4.562689781188965, loss=2.187609910964966
I0127 07:44:40.809161 139656666527488 logging_writer.py:48] [129000] global_step=129000, grad_norm=4.8138933181762695, loss=2.2322211265563965
I0127 07:45:14.902159 139656674920192 logging_writer.py:48] [129100] global_step=129100, grad_norm=5.712711334228516, loss=2.3418948650360107
I0127 07:45:48.987091 139656666527488 logging_writer.py:48] [129200] global_step=129200, grad_norm=4.879324913024902, loss=2.2674286365509033
I0127 07:46:23.081436 139656674920192 logging_writer.py:48] [129300] global_step=129300, grad_norm=5.1656904220581055, loss=2.2634382247924805
I0127 07:46:57.212428 139656666527488 logging_writer.py:48] [129400] global_step=129400, grad_norm=5.757946491241455, loss=2.3191964626312256
I0127 07:47:31.297730 139656674920192 logging_writer.py:48] [129500] global_step=129500, grad_norm=5.224537372589111, loss=2.226566791534424
I0127 07:48:05.342083 139656666527488 logging_writer.py:48] [129600] global_step=129600, grad_norm=5.161491870880127, loss=2.3018956184387207
I0127 07:48:39.424875 139656674920192 logging_writer.py:48] [129700] global_step=129700, grad_norm=5.0046162605285645, loss=2.221071720123291
I0127 07:49:13.463564 139656666527488 logging_writer.py:48] [129800] global_step=129800, grad_norm=4.821075916290283, loss=2.2934675216674805
I0127 07:49:47.504419 139656674920192 logging_writer.py:48] [129900] global_step=129900, grad_norm=4.8339152336120605, loss=2.2233328819274902
I0127 07:50:21.577230 139656666527488 logging_writer.py:48] [130000] global_step=130000, grad_norm=5.529191970825195, loss=2.3031721115112305
I0127 07:50:55.631358 139656674920192 logging_writer.py:48] [130100] global_step=130100, grad_norm=4.941043853759766, loss=2.301534414291382
I0127 07:51:28.121343 139822745589568 spec.py:321] Evaluating on the training split.
I0127 07:51:34.449458 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 07:51:43.077277 139822745589568 spec.py:349] Evaluating on the test split.
I0127 07:51:45.394186 139822745589568 submission_runner.py:408] Time since start: 46099.26s, 	Step: 130197, 	{'train/accuracy': 0.7904177308082581, 'train/loss': 0.8950925469398499, 'validation/accuracy': 0.7142800092697144, 'validation/loss': 1.2327156066894531, 'validation/num_examples': 50000, 'test/accuracy': 0.5909000039100647, 'test/loss': 1.8738075494766235, 'test/num_examples': 10000, 'score': 44430.832174539566, 'total_duration': 46099.25558972359, 'accumulated_submission_time': 44430.832174539566, 'accumulated_eval_time': 1659.6125497817993, 'accumulated_logging_time': 4.4501917362213135}
I0127 07:51:45.434154 139656700098304 logging_writer.py:48] [130197] accumulated_eval_time=1659.612550, accumulated_logging_time=4.450192, accumulated_submission_time=44430.832175, global_step=130197, preemption_count=0, score=44430.832175, test/accuracy=0.590900, test/loss=1.873808, test/num_examples=10000, total_duration=46099.255590, train/accuracy=0.790418, train/loss=0.895093, validation/accuracy=0.714280, validation/loss=1.232716, validation/num_examples=50000
I0127 07:51:46.800770 139658730145536 logging_writer.py:48] [130200] global_step=130200, grad_norm=4.558204174041748, loss=2.2173798084259033
I0127 07:52:20.814915 139656700098304 logging_writer.py:48] [130300] global_step=130300, grad_norm=5.035272121429443, loss=2.1832945346832275
I0127 07:52:54.880684 139658730145536 logging_writer.py:48] [130400] global_step=130400, grad_norm=4.861362934112549, loss=2.2646329402923584
I0127 07:53:28.973489 139656700098304 logging_writer.py:48] [130500] global_step=130500, grad_norm=5.056133270263672, loss=2.2174506187438965
I0127 07:54:03.064854 139658730145536 logging_writer.py:48] [130600] global_step=130600, grad_norm=4.485347270965576, loss=2.2015674114227295
I0127 07:54:37.094227 139656700098304 logging_writer.py:48] [130700] global_step=130700, grad_norm=4.960247993469238, loss=2.2969586849212646
I0127 07:55:11.130249 139658730145536 logging_writer.py:48] [130800] global_step=130800, grad_norm=4.913477420806885, loss=2.312488079071045
I0127 07:55:45.146158 139656700098304 logging_writer.py:48] [130900] global_step=130900, grad_norm=4.660576820373535, loss=2.2799322605133057
I0127 07:56:19.200510 139658730145536 logging_writer.py:48] [131000] global_step=131000, grad_norm=4.876357078552246, loss=2.1956899166107178
I0127 07:56:53.247550 139656700098304 logging_writer.py:48] [131100] global_step=131100, grad_norm=5.206632614135742, loss=2.1910932064056396
I0127 07:57:27.308724 139658730145536 logging_writer.py:48] [131200] global_step=131200, grad_norm=5.654382705688477, loss=2.275919198989868
I0127 07:58:01.363660 139656700098304 logging_writer.py:48] [131300] global_step=131300, grad_norm=4.940571308135986, loss=2.2637877464294434
I0127 07:58:35.450792 139658730145536 logging_writer.py:48] [131400] global_step=131400, grad_norm=5.452591896057129, loss=2.256075620651245
I0127 07:59:09.530726 139656700098304 logging_writer.py:48] [131500] global_step=131500, grad_norm=5.171507358551025, loss=2.2254209518432617
I0127 07:59:43.777045 139658730145536 logging_writer.py:48] [131600] global_step=131600, grad_norm=5.125094413757324, loss=2.171462059020996
I0127 08:00:15.589199 139822745589568 spec.py:321] Evaluating on the training split.
I0127 08:00:21.831777 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 08:00:30.362224 139822745589568 spec.py:349] Evaluating on the test split.
I0127 08:00:32.620164 139822745589568 submission_runner.py:408] Time since start: 46626.48s, 	Step: 131695, 	{'train/accuracy': 0.7917131781578064, 'train/loss': 0.8998433351516724, 'validation/accuracy': 0.7148999571800232, 'validation/loss': 1.22651207447052, 'validation/num_examples': 50000, 'test/accuracy': 0.5855000019073486, 'test/loss': 1.8798346519470215, 'test/num_examples': 10000, 'score': 44940.92831659317, 'total_duration': 46626.481568574905, 'accumulated_submission_time': 44940.92831659317, 'accumulated_eval_time': 1676.6434774398804, 'accumulated_logging_time': 4.499432563781738}
I0127 08:00:32.663012 139656649742080 logging_writer.py:48] [131695] accumulated_eval_time=1676.643477, accumulated_logging_time=4.499433, accumulated_submission_time=44940.928317, global_step=131695, preemption_count=0, score=44940.928317, test/accuracy=0.585500, test/loss=1.879835, test/num_examples=10000, total_duration=46626.481569, train/accuracy=0.791713, train/loss=0.899843, validation/accuracy=0.714900, validation/loss=1.226512, validation/num_examples=50000
I0127 08:00:34.705952 139656658134784 logging_writer.py:48] [131700] global_step=131700, grad_norm=5.22751522064209, loss=2.337717294692993
I0127 08:01:08.713556 139656649742080 logging_writer.py:48] [131800] global_step=131800, grad_norm=5.0036797523498535, loss=2.1985275745391846
I0127 08:01:42.739652 139656658134784 logging_writer.py:48] [131900] global_step=131900, grad_norm=5.197237968444824, loss=2.2237741947174072
I0127 08:02:16.772163 139656649742080 logging_writer.py:48] [132000] global_step=132000, grad_norm=5.127045631408691, loss=2.302154064178467
I0127 08:02:50.805622 139656658134784 logging_writer.py:48] [132100] global_step=132100, grad_norm=5.337079048156738, loss=2.226431369781494
I0127 08:03:24.840348 139656649742080 logging_writer.py:48] [132200] global_step=132200, grad_norm=5.106262683868408, loss=2.1179628372192383
I0127 08:03:58.868037 139656658134784 logging_writer.py:48] [132300] global_step=132300, grad_norm=5.113419532775879, loss=2.13334321975708
I0127 08:04:32.926914 139656649742080 logging_writer.py:48] [132400] global_step=132400, grad_norm=5.104918956756592, loss=2.1724274158477783
I0127 08:05:07.015773 139656658134784 logging_writer.py:48] [132500] global_step=132500, grad_norm=5.470212936401367, loss=2.239868640899658
I0127 08:05:41.174498 139656649742080 logging_writer.py:48] [132600] global_step=132600, grad_norm=5.035411834716797, loss=2.30148983001709
I0127 08:06:15.232211 139656658134784 logging_writer.py:48] [132700] global_step=132700, grad_norm=5.526988506317139, loss=2.2766010761260986
I0127 08:06:49.312981 139656649742080 logging_writer.py:48] [132800] global_step=132800, grad_norm=5.308632850646973, loss=2.3013393878936768
I0127 08:07:23.376409 139656658134784 logging_writer.py:48] [132900] global_step=132900, grad_norm=5.5446248054504395, loss=2.194768190383911
I0127 08:07:57.461913 139656649742080 logging_writer.py:48] [133000] global_step=133000, grad_norm=6.092517852783203, loss=2.395329475402832
I0127 08:08:31.534470 139656658134784 logging_writer.py:48] [133100] global_step=133100, grad_norm=5.060395240783691, loss=2.2334117889404297
I0127 08:09:02.662924 139822745589568 spec.py:321] Evaluating on the training split.
I0127 08:09:08.967307 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 08:09:17.595634 139822745589568 spec.py:349] Evaluating on the test split.
I0127 08:09:19.864345 139822745589568 submission_runner.py:408] Time since start: 47153.73s, 	Step: 133193, 	{'train/accuracy': 0.7970942258834839, 'train/loss': 0.8763123750686646, 'validation/accuracy': 0.7187199592590332, 'validation/loss': 1.2121546268463135, 'validation/num_examples': 50000, 'test/accuracy': 0.5919000506401062, 'test/loss': 1.845463752746582, 'test/num_examples': 10000, 'score': 45450.86598086357, 'total_duration': 47153.72575092316, 'accumulated_submission_time': 45450.86598086357, 'accumulated_eval_time': 1693.8448660373688, 'accumulated_logging_time': 4.552513122558594}
I0127 08:09:19.903965 139656297445120 logging_writer.py:48] [133193] accumulated_eval_time=1693.844866, accumulated_logging_time=4.552513, accumulated_submission_time=45450.865981, global_step=133193, preemption_count=0, score=45450.865981, test/accuracy=0.591900, test/loss=1.845464, test/num_examples=10000, total_duration=47153.725751, train/accuracy=0.797094, train/loss=0.876312, validation/accuracy=0.718720, validation/loss=1.212155, validation/num_examples=50000
I0127 08:09:22.635666 139656649742080 logging_writer.py:48] [133200] global_step=133200, grad_norm=4.83221435546875, loss=2.2559597492218018
I0127 08:09:56.657068 139656297445120 logging_writer.py:48] [133300] global_step=133300, grad_norm=5.145772457122803, loss=2.270139694213867
I0127 08:10:30.676679 139656649742080 logging_writer.py:48] [133400] global_step=133400, grad_norm=5.367403030395508, loss=2.1209988594055176
I0127 08:11:04.726985 139656297445120 logging_writer.py:48] [133500] global_step=133500, grad_norm=4.843679904937744, loss=2.1895394325256348
I0127 08:11:38.762557 139656649742080 logging_writer.py:48] [133600] global_step=133600, grad_norm=5.315275192260742, loss=2.2334647178649902
I0127 08:12:12.862326 139656297445120 logging_writer.py:48] [133700] global_step=133700, grad_norm=4.598202228546143, loss=2.19390606880188
I0127 08:12:46.874119 139656649742080 logging_writer.py:48] [133800] global_step=133800, grad_norm=4.792841911315918, loss=2.1519155502319336
I0127 08:13:20.906317 139656297445120 logging_writer.py:48] [133900] global_step=133900, grad_norm=5.282729148864746, loss=2.2940852642059326
I0127 08:13:54.967883 139656649742080 logging_writer.py:48] [134000] global_step=134000, grad_norm=5.138461112976074, loss=2.2040865421295166
I0127 08:14:28.972887 139656297445120 logging_writer.py:48] [134100] global_step=134100, grad_norm=5.412135601043701, loss=2.2330546379089355
I0127 08:15:03.028772 139656649742080 logging_writer.py:48] [134200] global_step=134200, grad_norm=5.133729934692383, loss=2.1453325748443604
I0127 08:15:37.076784 139656297445120 logging_writer.py:48] [134300] global_step=134300, grad_norm=5.688992023468018, loss=2.2043728828430176
I0127 08:16:11.137727 139656649742080 logging_writer.py:48] [134400] global_step=134400, grad_norm=4.901976108551025, loss=2.2461588382720947
I0127 08:16:45.198062 139656297445120 logging_writer.py:48] [134500] global_step=134500, grad_norm=5.0600666999816895, loss=2.2585668563842773
I0127 08:17:19.250977 139656649742080 logging_writer.py:48] [134600] global_step=134600, grad_norm=6.0991058349609375, loss=2.2310712337493896
I0127 08:17:50.105624 139822745589568 spec.py:321] Evaluating on the training split.
I0127 08:17:56.277802 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 08:18:05.005842 139822745589568 spec.py:349] Evaluating on the test split.
I0127 08:18:07.290674 139822745589568 submission_runner.py:408] Time since start: 47681.15s, 	Step: 134692, 	{'train/accuracy': 0.8205516338348389, 'train/loss': 0.7716889381408691, 'validation/accuracy': 0.7160199880599976, 'validation/loss': 1.2104519605636597, 'validation/num_examples': 50000, 'test/accuracy': 0.5961000323295593, 'test/loss': 1.841491937637329, 'test/num_examples': 10000, 'score': 45961.00407743454, 'total_duration': 47681.152082681656, 'accumulated_submission_time': 45961.00407743454, 'accumulated_eval_time': 1711.0298657417297, 'accumulated_logging_time': 4.6041131019592285}
I0127 08:18:07.332007 139656683312896 logging_writer.py:48] [134692] accumulated_eval_time=1711.029866, accumulated_logging_time=4.604113, accumulated_submission_time=45961.004077, global_step=134692, preemption_count=0, score=45961.004077, test/accuracy=0.596100, test/loss=1.841492, test/num_examples=10000, total_duration=47681.152083, train/accuracy=0.820552, train/loss=0.771689, validation/accuracy=0.716020, validation/loss=1.210452, validation/num_examples=50000
I0127 08:18:10.397047 139656691705600 logging_writer.py:48] [134700] global_step=134700, grad_norm=5.262111186981201, loss=2.3016390800476074
I0127 08:18:44.432525 139656683312896 logging_writer.py:48] [134800] global_step=134800, grad_norm=4.872912406921387, loss=2.2035112380981445
I0127 08:19:18.491729 139656691705600 logging_writer.py:48] [134900] global_step=134900, grad_norm=5.7010297775268555, loss=2.2053143978118896
I0127 08:19:52.562388 139656683312896 logging_writer.py:48] [135000] global_step=135000, grad_norm=5.993860721588135, loss=2.229952573776245
I0127 08:20:26.591885 139656691705600 logging_writer.py:48] [135100] global_step=135100, grad_norm=6.082630157470703, loss=2.151608467102051
I0127 08:21:00.619639 139656683312896 logging_writer.py:48] [135200] global_step=135200, grad_norm=5.188694477081299, loss=2.1696343421936035
I0127 08:21:34.691637 139656691705600 logging_writer.py:48] [135300] global_step=135300, grad_norm=5.072629451751709, loss=2.2001216411590576
I0127 08:22:08.749319 139656683312896 logging_writer.py:48] [135400] global_step=135400, grad_norm=4.790010452270508, loss=2.212649345397949
I0127 08:22:42.795665 139656691705600 logging_writer.py:48] [135500] global_step=135500, grad_norm=5.933803081512451, loss=2.1669564247131348
I0127 08:23:16.850747 139656683312896 logging_writer.py:48] [135600] global_step=135600, grad_norm=5.195247650146484, loss=2.1241891384124756
I0127 08:23:50.928922 139656691705600 logging_writer.py:48] [135700] global_step=135700, grad_norm=5.501157283782959, loss=2.1985645294189453
I0127 08:24:25.036721 139656683312896 logging_writer.py:48] [135800] global_step=135800, grad_norm=5.352468967437744, loss=2.1639223098754883
I0127 08:24:59.106620 139656691705600 logging_writer.py:48] [135900] global_step=135900, grad_norm=4.956638336181641, loss=2.1951866149902344
I0127 08:25:33.177171 139656683312896 logging_writer.py:48] [136000] global_step=136000, grad_norm=5.160712718963623, loss=2.0690860748291016
I0127 08:26:07.223096 139656691705600 logging_writer.py:48] [136100] global_step=136100, grad_norm=5.574253082275391, loss=2.2340292930603027
I0127 08:26:37.310474 139822745589568 spec.py:321] Evaluating on the training split.
I0127 08:26:43.579135 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 08:26:52.225554 139822745589568 spec.py:349] Evaluating on the test split.
I0127 08:26:54.532487 139822745589568 submission_runner.py:408] Time since start: 48208.39s, 	Step: 136190, 	{'train/accuracy': 0.8130978941917419, 'train/loss': 0.8022621870040894, 'validation/accuracy': 0.7203399538993835, 'validation/loss': 1.197677493095398, 'validation/num_examples': 50000, 'test/accuracy': 0.5961000323295593, 'test/loss': 1.8225889205932617, 'test/num_examples': 10000, 'score': 46470.92022943497, 'total_duration': 48208.3938832283, 'accumulated_submission_time': 46470.92022943497, 'accumulated_eval_time': 1728.2518351078033, 'accumulated_logging_time': 4.656829357147217}
I0127 08:26:54.576558 139656658134784 logging_writer.py:48] [136190] accumulated_eval_time=1728.251835, accumulated_logging_time=4.656829, accumulated_submission_time=46470.920229, global_step=136190, preemption_count=0, score=46470.920229, test/accuracy=0.596100, test/loss=1.822589, test/num_examples=10000, total_duration=48208.393883, train/accuracy=0.813098, train/loss=0.802262, validation/accuracy=0.720340, validation/loss=1.197677, validation/num_examples=50000
I0127 08:26:58.328314 139656666527488 logging_writer.py:48] [136200] global_step=136200, grad_norm=5.5603928565979, loss=2.1952128410339355
I0127 08:27:32.309291 139656658134784 logging_writer.py:48] [136300] global_step=136300, grad_norm=5.292867183685303, loss=2.162376880645752
I0127 08:28:06.305918 139656666527488 logging_writer.py:48] [136400] global_step=136400, grad_norm=5.122153282165527, loss=2.187922239303589
I0127 08:28:40.326445 139656658134784 logging_writer.py:48] [136500] global_step=136500, grad_norm=5.305112838745117, loss=2.176675796508789
I0127 08:29:14.343039 139656666527488 logging_writer.py:48] [136600] global_step=136600, grad_norm=5.422002792358398, loss=2.2815470695495605
I0127 08:29:48.365103 139656658134784 logging_writer.py:48] [136700] global_step=136700, grad_norm=5.382843494415283, loss=2.1626229286193848
I0127 08:30:22.488913 139656666527488 logging_writer.py:48] [136800] global_step=136800, grad_norm=5.595367908477783, loss=2.1819725036621094
I0127 08:30:56.519683 139656658134784 logging_writer.py:48] [136900] global_step=136900, grad_norm=5.472495079040527, loss=2.2184295654296875
I0127 08:31:30.583779 139656666527488 logging_writer.py:48] [137000] global_step=137000, grad_norm=5.010657787322998, loss=2.185319185256958
I0127 08:32:04.625248 139656658134784 logging_writer.py:48] [137100] global_step=137100, grad_norm=5.337948799133301, loss=2.2014150619506836
I0127 08:32:38.676016 139656666527488 logging_writer.py:48] [137200] global_step=137200, grad_norm=5.175022125244141, loss=2.1437978744506836
I0127 08:33:12.744973 139656658134784 logging_writer.py:48] [137300] global_step=137300, grad_norm=5.794987201690674, loss=2.2000961303710938
I0127 08:33:46.802938 139656666527488 logging_writer.py:48] [137400] global_step=137400, grad_norm=5.128594875335693, loss=2.163529872894287
I0127 08:34:20.852985 139656658134784 logging_writer.py:48] [137500] global_step=137500, grad_norm=5.543664932250977, loss=2.1179325580596924
I0127 08:34:54.886051 139656666527488 logging_writer.py:48] [137600] global_step=137600, grad_norm=5.026400566101074, loss=2.138139247894287
I0127 08:35:24.655156 139822745589568 spec.py:321] Evaluating on the training split.
I0127 08:35:30.855014 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 08:35:39.296650 139822745589568 spec.py:349] Evaluating on the test split.
I0127 08:35:41.596198 139822745589568 submission_runner.py:408] Time since start: 48735.46s, 	Step: 137689, 	{'train/accuracy': 0.810566782951355, 'train/loss': 0.8220297694206238, 'validation/accuracy': 0.722819983959198, 'validation/loss': 1.2077710628509521, 'validation/num_examples': 50000, 'test/accuracy': 0.5981000065803528, 'test/loss': 1.852874755859375, 'test/num_examples': 10000, 'score': 46980.9376642704, 'total_duration': 48735.45760130882, 'accumulated_submission_time': 46980.9376642704, 'accumulated_eval_time': 1745.192828655243, 'accumulated_logging_time': 4.712372779846191}
I0127 08:35:41.645248 139656658134784 logging_writer.py:48] [137689] accumulated_eval_time=1745.192829, accumulated_logging_time=4.712373, accumulated_submission_time=46980.937664, global_step=137689, preemption_count=0, score=46980.937664, test/accuracy=0.598100, test/loss=1.852875, test/num_examples=10000, total_duration=48735.457601, train/accuracy=0.810567, train/loss=0.822030, validation/accuracy=0.722820, validation/loss=1.207771, validation/num_examples=50000
I0127 08:35:45.724611 139656691705600 logging_writer.py:48] [137700] global_step=137700, grad_norm=5.6172404289245605, loss=2.283503293991089
I0127 08:36:19.727144 139656658134784 logging_writer.py:48] [137800] global_step=137800, grad_norm=5.337101936340332, loss=2.3004822731018066
I0127 08:36:53.810593 139656691705600 logging_writer.py:48] [137900] global_step=137900, grad_norm=5.530771255493164, loss=2.3248870372772217
I0127 08:37:27.839476 139656658134784 logging_writer.py:48] [138000] global_step=138000, grad_norm=5.817080974578857, loss=2.201914072036743
I0127 08:38:01.887680 139656691705600 logging_writer.py:48] [138100] global_step=138100, grad_norm=5.578945159912109, loss=2.167417526245117
I0127 08:38:35.943725 139656658134784 logging_writer.py:48] [138200] global_step=138200, grad_norm=5.235252857208252, loss=2.2027056217193604
I0127 08:39:09.983107 139656691705600 logging_writer.py:48] [138300] global_step=138300, grad_norm=5.323089599609375, loss=2.1719329357147217
I0127 08:39:44.024317 139656658134784 logging_writer.py:48] [138400] global_step=138400, grad_norm=5.51754093170166, loss=2.2567954063415527
I0127 08:40:18.056869 139656691705600 logging_writer.py:48] [138500] global_step=138500, grad_norm=5.850749969482422, loss=2.2498347759246826
I0127 08:40:52.120403 139656658134784 logging_writer.py:48] [138600] global_step=138600, grad_norm=5.025703430175781, loss=2.1387226581573486
I0127 08:41:26.173385 139656691705600 logging_writer.py:48] [138700] global_step=138700, grad_norm=5.533751964569092, loss=2.1793601512908936
I0127 08:42:00.199682 139656658134784 logging_writer.py:48] [138800] global_step=138800, grad_norm=5.4644269943237305, loss=2.1849780082702637
I0127 08:42:34.429172 139656691705600 logging_writer.py:48] [138900] global_step=138900, grad_norm=5.376611232757568, loss=2.234623908996582
I0127 08:43:08.497983 139656658134784 logging_writer.py:48] [139000] global_step=139000, grad_norm=5.363826274871826, loss=2.136013984680176
I0127 08:43:42.582741 139656691705600 logging_writer.py:48] [139100] global_step=139100, grad_norm=5.976862907409668, loss=2.232179880142212
I0127 08:44:11.701080 139822745589568 spec.py:321] Evaluating on the training split.
I0127 08:44:17.959053 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 08:44:26.575103 139822745589568 spec.py:349] Evaluating on the test split.
I0127 08:44:28.853097 139822745589568 submission_runner.py:408] Time since start: 49262.71s, 	Step: 139187, 	{'train/accuracy': 0.8121013641357422, 'train/loss': 0.8041132688522339, 'validation/accuracy': 0.7247599959373474, 'validation/loss': 1.1859569549560547, 'validation/num_examples': 50000, 'test/accuracy': 0.5973000526428223, 'test/loss': 1.8258440494537354, 'test/num_examples': 10000, 'score': 47490.93303322792, 'total_duration': 49262.71445965767, 'accumulated_submission_time': 47490.93303322792, 'accumulated_eval_time': 1762.3447728157043, 'accumulated_logging_time': 4.771524906158447}
I0127 08:44:28.896457 139656658134784 logging_writer.py:48] [139187] accumulated_eval_time=1762.344773, accumulated_logging_time=4.771525, accumulated_submission_time=47490.933033, global_step=139187, preemption_count=0, score=47490.933033, test/accuracy=0.597300, test/loss=1.825844, test/num_examples=10000, total_duration=49262.714460, train/accuracy=0.812101, train/loss=0.804113, validation/accuracy=0.724760, validation/loss=1.185957, validation/num_examples=50000
I0127 08:44:33.672598 139656666527488 logging_writer.py:48] [139200] global_step=139200, grad_norm=5.425967693328857, loss=2.144153594970703
I0127 08:45:07.700400 139656658134784 logging_writer.py:48] [139300] global_step=139300, grad_norm=5.408171653747559, loss=2.2087960243225098
I0127 08:45:41.732198 139656666527488 logging_writer.py:48] [139400] global_step=139400, grad_norm=5.733327865600586, loss=2.254631280899048
I0127 08:46:15.801271 139656658134784 logging_writer.py:48] [139500] global_step=139500, grad_norm=6.130597114562988, loss=2.146602153778076
I0127 08:46:49.880620 139656666527488 logging_writer.py:48] [139600] global_step=139600, grad_norm=5.132674217224121, loss=2.114062786102295
I0127 08:47:23.952906 139656658134784 logging_writer.py:48] [139700] global_step=139700, grad_norm=5.405245780944824, loss=2.177579164505005
I0127 08:47:58.036937 139656666527488 logging_writer.py:48] [139800] global_step=139800, grad_norm=5.442493915557861, loss=2.127387046813965
I0127 08:48:32.102910 139656658134784 logging_writer.py:48] [139900] global_step=139900, grad_norm=5.4598164558410645, loss=2.109480381011963
I0127 08:49:06.285183 139656666527488 logging_writer.py:48] [140000] global_step=140000, grad_norm=5.935908794403076, loss=2.1564881801605225
I0127 08:49:40.370540 139656658134784 logging_writer.py:48] [140100] global_step=140100, grad_norm=5.269863605499268, loss=2.1140804290771484
I0127 08:50:14.485277 139656666527488 logging_writer.py:48] [140200] global_step=140200, grad_norm=5.457488059997559, loss=2.1493489742279053
I0127 08:50:48.547456 139656658134784 logging_writer.py:48] [140300] global_step=140300, grad_norm=6.1696858406066895, loss=2.1632192134857178
I0127 08:51:22.651032 139656666527488 logging_writer.py:48] [140400] global_step=140400, grad_norm=5.2892584800720215, loss=2.1147522926330566
I0127 08:51:56.719655 139656658134784 logging_writer.py:48] [140500] global_step=140500, grad_norm=5.684648036956787, loss=2.194418430328369
I0127 08:52:30.790780 139656666527488 logging_writer.py:48] [140600] global_step=140600, grad_norm=5.476222991943359, loss=2.167191505432129
I0127 08:52:58.891934 139822745589568 spec.py:321] Evaluating on the training split.
I0127 08:53:05.212168 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 08:53:13.811782 139822745589568 spec.py:349] Evaluating on the test split.
I0127 08:53:16.080568 139822745589568 submission_runner.py:408] Time since start: 49789.94s, 	Step: 140684, 	{'train/accuracy': 0.8148915767669678, 'train/loss': 0.7869499921798706, 'validation/accuracy': 0.7278199791908264, 'validation/loss': 1.1672886610031128, 'validation/num_examples': 50000, 'test/accuracy': 0.6026000380516052, 'test/loss': 1.7988563776016235, 'test/num_examples': 10000, 'score': 48000.869512319565, 'total_duration': 49789.94197225571, 'accumulated_submission_time': 48000.869512319565, 'accumulated_eval_time': 1779.533354997635, 'accumulated_logging_time': 4.8248069286346436}
I0127 08:53:16.120478 139656658134784 logging_writer.py:48] [140684] accumulated_eval_time=1779.533355, accumulated_logging_time=4.824807, accumulated_submission_time=48000.869512, global_step=140684, preemption_count=0, score=48000.869512, test/accuracy=0.602600, test/loss=1.798856, test/num_examples=10000, total_duration=49789.941972, train/accuracy=0.814892, train/loss=0.786950, validation/accuracy=0.727820, validation/loss=1.167289, validation/num_examples=50000
I0127 08:53:21.908696 139656666527488 logging_writer.py:48] [140700] global_step=140700, grad_norm=5.565162181854248, loss=2.147901773452759
I0127 08:53:55.952228 139656658134784 logging_writer.py:48] [140800] global_step=140800, grad_norm=5.770327091217041, loss=2.179597854614258
I0127 08:54:30.002485 139656666527488 logging_writer.py:48] [140900] global_step=140900, grad_norm=5.472463130950928, loss=2.1290078163146973
I0127 08:55:04.073509 139656658134784 logging_writer.py:48] [141000] global_step=141000, grad_norm=6.04252815246582, loss=2.1276156902313232
I0127 08:55:38.143143 139656666527488 logging_writer.py:48] [141100] global_step=141100, grad_norm=5.552147388458252, loss=2.130953788757324
I0127 08:56:12.183211 139656658134784 logging_writer.py:48] [141200] global_step=141200, grad_norm=5.877923488616943, loss=2.1343297958374023
I0127 08:56:46.214241 139656666527488 logging_writer.py:48] [141300] global_step=141300, grad_norm=5.447758674621582, loss=2.172794818878174
I0127 08:57:20.304941 139656658134784 logging_writer.py:48] [141400] global_step=141400, grad_norm=6.079076290130615, loss=2.11616849899292
I0127 08:57:54.386041 139656666527488 logging_writer.py:48] [141500] global_step=141500, grad_norm=5.717397212982178, loss=2.1789193153381348
I0127 08:58:28.446249 139656658134784 logging_writer.py:48] [141600] global_step=141600, grad_norm=5.859950542449951, loss=2.1868395805358887
I0127 08:59:02.534091 139656666527488 logging_writer.py:48] [141700] global_step=141700, grad_norm=5.358554363250732, loss=2.110196828842163
I0127 08:59:36.599387 139656658134784 logging_writer.py:48] [141800] global_step=141800, grad_norm=5.504388809204102, loss=2.0940446853637695
I0127 09:00:10.682809 139656666527488 logging_writer.py:48] [141900] global_step=141900, grad_norm=5.702259063720703, loss=2.1360080242156982
I0127 09:00:44.763405 139656658134784 logging_writer.py:48] [142000] global_step=142000, grad_norm=5.382730960845947, loss=2.1155920028686523
I0127 09:01:18.900672 139656666527488 logging_writer.py:48] [142100] global_step=142100, grad_norm=6.536045551300049, loss=2.2129569053649902
I0127 09:01:46.292524 139822745589568 spec.py:321] Evaluating on the training split.
I0127 09:01:52.535681 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 09:02:01.126654 139822745589568 spec.py:349] Evaluating on the test split.
I0127 09:02:03.772219 139822745589568 submission_runner.py:408] Time since start: 50317.63s, 	Step: 142182, 	{'train/accuracy': 0.8167450428009033, 'train/loss': 0.7779141664505005, 'validation/accuracy': 0.7300800085067749, 'validation/loss': 1.1511822938919067, 'validation/num_examples': 50000, 'test/accuracy': 0.6078000068664551, 'test/loss': 1.7888727188110352, 'test/num_examples': 10000, 'score': 48510.98008084297, 'total_duration': 50317.633615255356, 'accumulated_submission_time': 48510.98008084297, 'accumulated_eval_time': 1797.0130088329315, 'accumulated_logging_time': 4.874320030212402}
I0127 09:02:03.810072 139656649742080 logging_writer.py:48] [142182] accumulated_eval_time=1797.013009, accumulated_logging_time=4.874320, accumulated_submission_time=48510.980081, global_step=142182, preemption_count=0, score=48510.980081, test/accuracy=0.607800, test/loss=1.788873, test/num_examples=10000, total_duration=50317.633615, train/accuracy=0.816745, train/loss=0.777914, validation/accuracy=0.730080, validation/loss=1.151182, validation/num_examples=50000
I0127 09:02:10.287584 139656674920192 logging_writer.py:48] [142200] global_step=142200, grad_norm=5.9049224853515625, loss=2.2164812088012695
I0127 09:02:44.297752 139656649742080 logging_writer.py:48] [142300] global_step=142300, grad_norm=5.830219745635986, loss=2.2071118354797363
I0127 09:03:18.314319 139656674920192 logging_writer.py:48] [142400] global_step=142400, grad_norm=5.713235855102539, loss=2.164658308029175
I0127 09:03:52.333083 139656649742080 logging_writer.py:48] [142500] global_step=142500, grad_norm=6.063742160797119, loss=2.121251106262207
I0127 09:04:26.362441 139656674920192 logging_writer.py:48] [142600] global_step=142600, grad_norm=6.068058490753174, loss=2.153376579284668
I0127 09:05:00.409981 139656649742080 logging_writer.py:48] [142700] global_step=142700, grad_norm=5.435762882232666, loss=2.167024612426758
I0127 09:05:34.475348 139656674920192 logging_writer.py:48] [142800] global_step=142800, grad_norm=5.768033027648926, loss=2.1300501823425293
I0127 09:06:08.529736 139656649742080 logging_writer.py:48] [142900] global_step=142900, grad_norm=5.720813751220703, loss=2.0712764263153076
I0127 09:06:42.617600 139656674920192 logging_writer.py:48] [143000] global_step=143000, grad_norm=6.24287223815918, loss=2.207573175430298
I0127 09:07:16.741307 139656649742080 logging_writer.py:48] [143100] global_step=143100, grad_norm=5.57327127456665, loss=2.1254310607910156
I0127 09:07:50.826980 139656674920192 logging_writer.py:48] [143200] global_step=143200, grad_norm=5.8022284507751465, loss=2.137101650238037
I0127 09:08:24.887506 139656649742080 logging_writer.py:48] [143300] global_step=143300, grad_norm=5.4836835861206055, loss=2.1310980319976807
I0127 09:08:58.952458 139656674920192 logging_writer.py:48] [143400] global_step=143400, grad_norm=5.838568210601807, loss=2.05725359916687
I0127 09:09:33.021306 139656649742080 logging_writer.py:48] [143500] global_step=143500, grad_norm=6.3566131591796875, loss=2.0973758697509766
I0127 09:10:07.084383 139656674920192 logging_writer.py:48] [143600] global_step=143600, grad_norm=6.052523612976074, loss=2.1866700649261475
I0127 09:10:34.113919 139822745589568 spec.py:321] Evaluating on the training split.
I0127 09:10:40.393361 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 09:10:49.008386 139822745589568 spec.py:349] Evaluating on the test split.
I0127 09:10:51.300624 139822745589568 submission_runner.py:408] Time since start: 50845.16s, 	Step: 143681, 	{'train/accuracy': 0.8469786047935486, 'train/loss': 0.6639379262924194, 'validation/accuracy': 0.7300399541854858, 'validation/loss': 1.1502034664154053, 'validation/num_examples': 50000, 'test/accuracy': 0.6038000583648682, 'test/loss': 1.7933541536331177, 'test/num_examples': 10000, 'score': 49021.22454404831, 'total_duration': 50845.16202926636, 'accumulated_submission_time': 49021.22454404831, 'accumulated_eval_time': 1814.1996743679047, 'accumulated_logging_time': 4.921878337860107}
I0127 09:10:51.343986 139656658134784 logging_writer.py:48] [143681] accumulated_eval_time=1814.199674, accumulated_logging_time=4.921878, accumulated_submission_time=49021.224544, global_step=143681, preemption_count=0, score=49021.224544, test/accuracy=0.603800, test/loss=1.793354, test/num_examples=10000, total_duration=50845.162029, train/accuracy=0.846979, train/loss=0.663938, validation/accuracy=0.730040, validation/loss=1.150203, validation/num_examples=50000
I0127 09:10:58.167797 139656691705600 logging_writer.py:48] [143700] global_step=143700, grad_norm=6.182267189025879, loss=2.173365592956543
I0127 09:11:32.201692 139656658134784 logging_writer.py:48] [143800] global_step=143800, grad_norm=5.945494651794434, loss=2.1075968742370605
I0127 09:12:06.236187 139656691705600 logging_writer.py:48] [143900] global_step=143900, grad_norm=5.667519569396973, loss=2.1341302394866943
I0127 09:12:40.285708 139656658134784 logging_writer.py:48] [144000] global_step=144000, grad_norm=5.749283790588379, loss=2.1464807987213135
I0127 09:13:14.335773 139656691705600 logging_writer.py:48] [144100] global_step=144100, grad_norm=5.353987693786621, loss=2.0530357360839844
I0127 09:13:48.462661 139656658134784 logging_writer.py:48] [144200] global_step=144200, grad_norm=5.769210338592529, loss=2.1269519329071045
I0127 09:14:22.551248 139656691705600 logging_writer.py:48] [144300] global_step=144300, grad_norm=6.289358615875244, loss=2.0708625316619873
I0127 09:14:56.608246 139656658134784 logging_writer.py:48] [144400] global_step=144400, grad_norm=5.51625919342041, loss=2.0344927310943604
I0127 09:15:30.665640 139656691705600 logging_writer.py:48] [144500] global_step=144500, grad_norm=6.61850118637085, loss=2.1315512657165527
I0127 09:16:04.708267 139656658134784 logging_writer.py:48] [144600] global_step=144600, grad_norm=6.178774833679199, loss=2.1173558235168457
I0127 09:16:38.751095 139656691705600 logging_writer.py:48] [144700] global_step=144700, grad_norm=6.0221967697143555, loss=2.2018563747406006
I0127 09:17:12.829991 139656658134784 logging_writer.py:48] [144800] global_step=144800, grad_norm=6.208476543426514, loss=2.1717593669891357
I0127 09:17:46.848075 139656691705600 logging_writer.py:48] [144900] global_step=144900, grad_norm=6.09896183013916, loss=2.0755434036254883
I0127 09:18:20.897964 139656658134784 logging_writer.py:48] [145000] global_step=145000, grad_norm=5.263915538787842, loss=2.0597193241119385
I0127 09:18:54.949661 139656691705600 logging_writer.py:48] [145100] global_step=145100, grad_norm=6.359673023223877, loss=2.060947895050049
I0127 09:19:21.321046 139822745589568 spec.py:321] Evaluating on the training split.
I0127 09:19:27.583372 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 09:19:36.312158 139822745589568 spec.py:349] Evaluating on the test split.
I0127 09:19:38.578120 139822745589568 submission_runner.py:408] Time since start: 51372.44s, 	Step: 145179, 	{'train/accuracy': 0.8422552347183228, 'train/loss': 0.703779399394989, 'validation/accuracy': 0.7349399924278259, 'validation/loss': 1.1473641395568848, 'validation/num_examples': 50000, 'test/accuracy': 0.6086000204086304, 'test/loss': 1.7797400951385498, 'test/num_examples': 10000, 'score': 49531.14088320732, 'total_duration': 51372.43950200081, 'accumulated_submission_time': 49531.14088320732, 'accumulated_eval_time': 1831.4566979408264, 'accumulated_logging_time': 4.974995851516724}
I0127 09:19:38.622066 139656297445120 logging_writer.py:48] [145179] accumulated_eval_time=1831.456698, accumulated_logging_time=4.974996, accumulated_submission_time=49531.140883, global_step=145179, preemption_count=0, score=49531.140883, test/accuracy=0.608600, test/loss=1.779740, test/num_examples=10000, total_duration=51372.439502, train/accuracy=0.842255, train/loss=0.703779, validation/accuracy=0.734940, validation/loss=1.147364, validation/num_examples=50000
I0127 09:19:46.128596 139656649742080 logging_writer.py:48] [145200] global_step=145200, grad_norm=6.159019947052002, loss=2.092970371246338
I0127 09:20:20.121636 139656297445120 logging_writer.py:48] [145300] global_step=145300, grad_norm=6.350396156311035, loss=2.088581085205078
I0127 09:20:54.155023 139656649742080 logging_writer.py:48] [145400] global_step=145400, grad_norm=5.965935230255127, loss=2.0527520179748535
I0127 09:21:28.214482 139656297445120 logging_writer.py:48] [145500] global_step=145500, grad_norm=5.601358890533447, loss=2.1147706508636475
I0127 09:22:02.283193 139656649742080 logging_writer.py:48] [145600] global_step=145600, grad_norm=5.442656517028809, loss=2.072756290435791
I0127 09:22:36.369527 139656297445120 logging_writer.py:48] [145700] global_step=145700, grad_norm=5.976174354553223, loss=2.076828718185425
I0127 09:23:10.409543 139656649742080 logging_writer.py:48] [145800] global_step=145800, grad_norm=5.526252746582031, loss=2.0134449005126953
I0127 09:23:44.490139 139656297445120 logging_writer.py:48] [145900] global_step=145900, grad_norm=6.375508785247803, loss=2.210691452026367
I0127 09:24:18.573306 139656649742080 logging_writer.py:48] [146000] global_step=146000, grad_norm=6.038939952850342, loss=2.085589647293091
I0127 09:24:52.627482 139656297445120 logging_writer.py:48] [146100] global_step=146100, grad_norm=5.921387672424316, loss=2.1708812713623047
I0127 09:25:26.682993 139656649742080 logging_writer.py:48] [146200] global_step=146200, grad_norm=5.978551864624023, loss=2.0984978675842285
I0127 09:26:00.798711 139656297445120 logging_writer.py:48] [146300] global_step=146300, grad_norm=6.325105667114258, loss=2.0993852615356445
I0127 09:26:34.865009 139656649742080 logging_writer.py:48] [146400] global_step=146400, grad_norm=6.038743019104004, loss=2.1818630695343018
I0127 09:27:08.909276 139656297445120 logging_writer.py:48] [146500] global_step=146500, grad_norm=6.330296039581299, loss=2.066758871078491
I0127 09:27:42.933179 139656649742080 logging_writer.py:48] [146600] global_step=146600, grad_norm=5.578218936920166, loss=2.1046535968780518
I0127 09:28:08.628818 139822745589568 spec.py:321] Evaluating on the training split.
I0127 09:28:14.804462 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 09:28:23.373353 139822745589568 spec.py:349] Evaluating on the test split.
I0127 09:28:25.651854 139822745589568 submission_runner.py:408] Time since start: 51899.51s, 	Step: 146677, 	{'train/accuracy': 0.8409199714660645, 'train/loss': 0.7093674540519714, 'validation/accuracy': 0.7373799681663513, 'validation/loss': 1.1325105428695679, 'validation/num_examples': 50000, 'test/accuracy': 0.6091000437736511, 'test/loss': 1.7796084880828857, 'test/num_examples': 10000, 'score': 50041.08595466614, 'total_duration': 51899.51325583458, 'accumulated_submission_time': 50041.08595466614, 'accumulated_eval_time': 1848.4796833992004, 'accumulated_logging_time': 5.031417369842529}
I0127 09:28:25.695098 139656649742080 logging_writer.py:48] [146677] accumulated_eval_time=1848.479683, accumulated_logging_time=5.031417, accumulated_submission_time=50041.085955, global_step=146677, preemption_count=0, score=50041.085955, test/accuracy=0.609100, test/loss=1.779608, test/num_examples=10000, total_duration=51899.513256, train/accuracy=0.840920, train/loss=0.709367, validation/accuracy=0.737380, validation/loss=1.132511, validation/num_examples=50000
I0127 09:28:33.865729 139656683312896 logging_writer.py:48] [146700] global_step=146700, grad_norm=5.6941609382629395, loss=2.1007542610168457
I0127 09:29:07.871925 139656649742080 logging_writer.py:48] [146800] global_step=146800, grad_norm=6.046243190765381, loss=2.1129331588745117
I0127 09:29:41.871416 139656683312896 logging_writer.py:48] [146900] global_step=146900, grad_norm=5.890792369842529, loss=2.0361485481262207
I0127 09:30:15.924687 139656649742080 logging_writer.py:48] [147000] global_step=147000, grad_norm=6.211577415466309, loss=2.129049777984619
I0127 09:30:49.961574 139656683312896 logging_writer.py:48] [147100] global_step=147100, grad_norm=5.813403129577637, loss=2.130082607269287
I0127 09:31:24.030365 139656649742080 logging_writer.py:48] [147200] global_step=147200, grad_norm=5.480018615722656, loss=2.0416157245635986
I0127 09:31:58.131427 139656683312896 logging_writer.py:48] [147300] global_step=147300, grad_norm=6.6115498542785645, loss=2.158430814743042
I0127 09:32:32.212490 139656649742080 logging_writer.py:48] [147400] global_step=147400, grad_norm=5.480737686157227, loss=2.036557197570801
I0127 09:33:06.253320 139656683312896 logging_writer.py:48] [147500] global_step=147500, grad_norm=6.361913204193115, loss=2.010775327682495
I0127 09:33:40.326152 139656649742080 logging_writer.py:48] [147600] global_step=147600, grad_norm=6.706103801727295, loss=2.137033462524414
I0127 09:34:14.417584 139656683312896 logging_writer.py:48] [147700] global_step=147700, grad_norm=6.298678398132324, loss=2.1108357906341553
I0127 09:34:48.481797 139656649742080 logging_writer.py:48] [147800] global_step=147800, grad_norm=5.9514265060424805, loss=2.0836615562438965
I0127 09:35:22.543241 139656683312896 logging_writer.py:48] [147900] global_step=147900, grad_norm=6.451528549194336, loss=2.143537998199463
I0127 09:35:56.610940 139656649742080 logging_writer.py:48] [148000] global_step=148000, grad_norm=6.164422988891602, loss=2.0635669231414795
I0127 09:36:30.662251 139656683312896 logging_writer.py:48] [148100] global_step=148100, grad_norm=7.175451755523682, loss=2.138890266418457
I0127 09:36:55.687611 139822745589568 spec.py:321] Evaluating on the training split.
I0127 09:37:01.964377 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 09:37:10.594375 139822745589568 spec.py:349] Evaluating on the test split.
I0127 09:37:12.925130 139822745589568 submission_runner.py:408] Time since start: 52426.79s, 	Step: 148175, 	{'train/accuracy': 0.8373923897743225, 'train/loss': 0.7035523056983948, 'validation/accuracy': 0.737280011177063, 'validation/loss': 1.1355136632919312, 'validation/num_examples': 50000, 'test/accuracy': 0.6115000247955322, 'test/loss': 1.7595330476760864, 'test/num_examples': 10000, 'score': 50551.017679452896, 'total_duration': 52426.786516427994, 'accumulated_submission_time': 50551.017679452896, 'accumulated_eval_time': 1865.7171568870544, 'accumulated_logging_time': 5.085627317428589}
I0127 09:37:12.981296 139656297445120 logging_writer.py:48] [148175] accumulated_eval_time=1865.717157, accumulated_logging_time=5.085627, accumulated_submission_time=50551.017679, global_step=148175, preemption_count=0, score=50551.017679, test/accuracy=0.611500, test/loss=1.759533, test/num_examples=10000, total_duration=52426.786516, train/accuracy=0.837392, train/loss=0.703552, validation/accuracy=0.737280, validation/loss=1.135514, validation/num_examples=50000
I0127 09:37:21.844726 139656649742080 logging_writer.py:48] [148200] global_step=148200, grad_norm=6.245387554168701, loss=2.0132412910461426
I0127 09:37:55.872737 139656297445120 logging_writer.py:48] [148300] global_step=148300, grad_norm=6.094043254852295, loss=2.0188534259796143
I0127 09:38:29.955255 139656649742080 logging_writer.py:48] [148400] global_step=148400, grad_norm=6.206563472747803, loss=2.0836734771728516
I0127 09:39:04.005409 139656297445120 logging_writer.py:48] [148500] global_step=148500, grad_norm=5.877196311950684, loss=2.053950309753418
I0127 09:39:38.090243 139656649742080 logging_writer.py:48] [148600] global_step=148600, grad_norm=6.234416484832764, loss=2.05615496635437
I0127 09:40:12.159240 139656297445120 logging_writer.py:48] [148700] global_step=148700, grad_norm=6.18675422668457, loss=2.1096622943878174
I0127 09:40:46.185289 139656649742080 logging_writer.py:48] [148800] global_step=148800, grad_norm=6.015292167663574, loss=2.022503137588501
I0127 09:41:20.253581 139656297445120 logging_writer.py:48] [148900] global_step=148900, grad_norm=6.342886447906494, loss=2.099212408065796
I0127 09:41:54.304011 139656649742080 logging_writer.py:48] [149000] global_step=149000, grad_norm=6.067733287811279, loss=2.0450310707092285
I0127 09:42:28.343391 139656297445120 logging_writer.py:48] [149100] global_step=149100, grad_norm=6.825209140777588, loss=2.1027681827545166
I0127 09:43:02.403669 139656649742080 logging_writer.py:48] [149200] global_step=149200, grad_norm=6.647561550140381, loss=2.089754104614258
I0127 09:43:36.434797 139656297445120 logging_writer.py:48] [149300] global_step=149300, grad_norm=6.506582260131836, loss=2.059051990509033
I0127 09:44:10.490776 139656649742080 logging_writer.py:48] [149400] global_step=149400, grad_norm=5.962479114532471, loss=2.0227162837982178
I0127 09:44:44.589427 139656297445120 logging_writer.py:48] [149500] global_step=149500, grad_norm=6.180421352386475, loss=2.1469693183898926
I0127 09:45:18.618377 139656649742080 logging_writer.py:48] [149600] global_step=149600, grad_norm=5.696653842926025, loss=2.003260850906372
I0127 09:45:42.932426 139822745589568 spec.py:321] Evaluating on the training split.
I0127 09:45:49.245086 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 09:45:57.857459 139822745589568 spec.py:349] Evaluating on the test split.
I0127 09:46:00.146862 139822745589568 submission_runner.py:408] Time since start: 52954.01s, 	Step: 149673, 	{'train/accuracy': 0.8453842401504517, 'train/loss': 0.686999499797821, 'validation/accuracy': 0.7429400086402893, 'validation/loss': 1.107838749885559, 'validation/num_examples': 50000, 'test/accuracy': 0.6195000410079956, 'test/loss': 1.7411073446273804, 'test/num_examples': 10000, 'score': 51060.90836381912, 'total_duration': 52954.008268117905, 'accumulated_submission_time': 51060.90836381912, 'accumulated_eval_time': 1882.9315605163574, 'accumulated_logging_time': 5.151835203170776}
I0127 09:46:00.193270 139656700098304 logging_writer.py:48] [149673] accumulated_eval_time=1882.931561, accumulated_logging_time=5.151835, accumulated_submission_time=51060.908364, global_step=149673, preemption_count=0, score=51060.908364, test/accuracy=0.619500, test/loss=1.741107, test/num_examples=10000, total_duration=52954.008268, train/accuracy=0.845384, train/loss=0.686999, validation/accuracy=0.742940, validation/loss=1.107839, validation/num_examples=50000
I0127 09:46:09.742412 139658730145536 logging_writer.py:48] [149700] global_step=149700, grad_norm=7.068960189819336, loss=2.0823967456817627
I0127 09:46:43.744006 139656700098304 logging_writer.py:48] [149800] global_step=149800, grad_norm=6.556229114532471, loss=2.0679879188537598
I0127 09:47:17.783869 139658730145536 logging_writer.py:48] [149900] global_step=149900, grad_norm=5.4589338302612305, loss=1.9885653257369995
I0127 09:47:51.857402 139656700098304 logging_writer.py:48] [150000] global_step=150000, grad_norm=6.5698957443237305, loss=2.047006130218506
I0127 09:48:25.939016 139658730145536 logging_writer.py:48] [150100] global_step=150100, grad_norm=5.9076948165893555, loss=2.004936695098877
I0127 09:48:59.989274 139656700098304 logging_writer.py:48] [150200] global_step=150200, grad_norm=6.223443984985352, loss=2.077427864074707
I0127 09:49:34.081586 139658730145536 logging_writer.py:48] [150300] global_step=150300, grad_norm=6.289438247680664, loss=2.0913095474243164
I0127 09:50:08.159445 139656700098304 logging_writer.py:48] [150400] global_step=150400, grad_norm=6.398135185241699, loss=2.065406322479248
I0127 09:50:42.294766 139658730145536 logging_writer.py:48] [150500] global_step=150500, grad_norm=6.849734306335449, loss=2.0557122230529785
I0127 09:51:16.341717 139656700098304 logging_writer.py:48] [150600] global_step=150600, grad_norm=6.036269187927246, loss=2.011875867843628
I0127 09:51:50.408176 139658730145536 logging_writer.py:48] [150700] global_step=150700, grad_norm=6.481838703155518, loss=2.0058300495147705
I0127 09:52:24.458369 139656700098304 logging_writer.py:48] [150800] global_step=150800, grad_norm=6.3822197914123535, loss=2.005448579788208
I0127 09:52:58.526133 139658730145536 logging_writer.py:48] [150900] global_step=150900, grad_norm=6.643211841583252, loss=2.0119359493255615
I0127 09:53:32.615314 139656700098304 logging_writer.py:48] [151000] global_step=151000, grad_norm=6.62588357925415, loss=2.0385031700134277
I0127 09:54:06.694654 139658730145536 logging_writer.py:48] [151100] global_step=151100, grad_norm=6.250577449798584, loss=2.0437393188476562
I0127 09:54:30.354547 139822745589568 spec.py:321] Evaluating on the training split.
I0127 09:54:37.212263 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 09:54:45.793664 139822745589568 spec.py:349] Evaluating on the test split.
I0127 09:54:48.063820 139822745589568 submission_runner.py:408] Time since start: 53481.93s, 	Step: 151171, 	{'train/accuracy': 0.8451052308082581, 'train/loss': 0.675420880317688, 'validation/accuracy': 0.7450799942016602, 'validation/loss': 1.1006088256835938, 'validation/num_examples': 50000, 'test/accuracy': 0.6212000250816345, 'test/loss': 1.727745532989502, 'test/num_examples': 10000, 'score': 51571.01009392738, 'total_duration': 53481.925209999084, 'accumulated_submission_time': 51571.01009392738, 'accumulated_eval_time': 1900.640768289566, 'accumulated_logging_time': 5.20801043510437}
I0127 09:54:48.106781 139656658134784 logging_writer.py:48] [151171] accumulated_eval_time=1900.640768, accumulated_logging_time=5.208010, accumulated_submission_time=51571.010094, global_step=151171, preemption_count=0, score=51571.010094, test/accuracy=0.621200, test/loss=1.727746, test/num_examples=10000, total_duration=53481.925210, train/accuracy=0.845105, train/loss=0.675421, validation/accuracy=0.745080, validation/loss=1.100609, validation/num_examples=50000
I0127 09:54:58.333565 139656666527488 logging_writer.py:48] [151200] global_step=151200, grad_norm=6.563230991363525, loss=2.031888246536255
I0127 09:55:32.337284 139656658134784 logging_writer.py:48] [151300] global_step=151300, grad_norm=6.559092044830322, loss=2.061737537384033
I0127 09:56:06.379292 139656666527488 logging_writer.py:48] [151400] global_step=151400, grad_norm=6.758108139038086, loss=2.0369555950164795
I0127 09:56:40.589399 139656658134784 logging_writer.py:48] [151500] global_step=151500, grad_norm=6.645466327667236, loss=2.0464699268341064
I0127 09:57:14.639270 139656666527488 logging_writer.py:48] [151600] global_step=151600, grad_norm=5.762533187866211, loss=1.9912363290786743
I0127 09:57:48.655642 139656658134784 logging_writer.py:48] [151700] global_step=151700, grad_norm=6.982354640960693, loss=2.1081972122192383
I0127 09:58:22.728648 139656666527488 logging_writer.py:48] [151800] global_step=151800, grad_norm=6.418267726898193, loss=2.134103775024414
I0127 09:58:56.772933 139656658134784 logging_writer.py:48] [151900] global_step=151900, grad_norm=7.061034202575684, loss=2.040098190307617
I0127 09:59:30.831251 139656666527488 logging_writer.py:48] [152000] global_step=152000, grad_norm=6.487306118011475, loss=1.9312183856964111
I0127 10:00:04.865850 139656658134784 logging_writer.py:48] [152100] global_step=152100, grad_norm=6.248157978057861, loss=2.0754473209381104
I0127 10:00:38.935895 139656666527488 logging_writer.py:48] [152200] global_step=152200, grad_norm=6.222494125366211, loss=2.0641672611236572
I0127 10:01:12.969107 139656658134784 logging_writer.py:48] [152300] global_step=152300, grad_norm=6.508444786071777, loss=2.0144219398498535
I0127 10:01:47.014636 139656666527488 logging_writer.py:48] [152400] global_step=152400, grad_norm=7.398867607116699, loss=2.080402374267578
I0127 10:02:21.082628 139656658134784 logging_writer.py:48] [152500] global_step=152500, grad_norm=6.00759744644165, loss=1.9589807987213135
I0127 10:02:55.223916 139656666527488 logging_writer.py:48] [152600] global_step=152600, grad_norm=6.524019241333008, loss=1.9942493438720703
I0127 10:03:18.202667 139822745589568 spec.py:321] Evaluating on the training split.
I0127 10:03:24.582325 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 10:03:32.956016 139822745589568 spec.py:349] Evaluating on the test split.
I0127 10:03:35.259692 139822745589568 submission_runner.py:408] Time since start: 54009.12s, 	Step: 152669, 	{'train/accuracy': 0.8661710619926453, 'train/loss': 0.6043965220451355, 'validation/accuracy': 0.7461599707603455, 'validation/loss': 1.0976543426513672, 'validation/num_examples': 50000, 'test/accuracy': 0.6201000213623047, 'test/loss': 1.735266089439392, 'test/num_examples': 10000, 'score': 52081.04586791992, 'total_duration': 54009.12109827995, 'accumulated_submission_time': 52081.04586791992, 'accumulated_eval_time': 1917.6977479457855, 'accumulated_logging_time': 5.260669708251953}
I0127 10:03:35.308334 139656297445120 logging_writer.py:48] [152669] accumulated_eval_time=1917.697748, accumulated_logging_time=5.260670, accumulated_submission_time=52081.045868, global_step=152669, preemption_count=0, score=52081.045868, test/accuracy=0.620100, test/loss=1.735266, test/num_examples=10000, total_duration=54009.121098, train/accuracy=0.866171, train/loss=0.604397, validation/accuracy=0.746160, validation/loss=1.097654, validation/num_examples=50000
I0127 10:03:46.205510 139656649742080 logging_writer.py:48] [152700] global_step=152700, grad_norm=6.81374979019165, loss=1.9961044788360596
I0127 10:04:20.236706 139656297445120 logging_writer.py:48] [152800] global_step=152800, grad_norm=6.321934223175049, loss=1.9300750494003296
I0127 10:04:54.306554 139656649742080 logging_writer.py:48] [152900] global_step=152900, grad_norm=6.339902400970459, loss=1.982964038848877
I0127 10:05:28.393037 139656297445120 logging_writer.py:48] [153000] global_step=153000, grad_norm=6.249255180358887, loss=2.0544559955596924
I0127 10:06:02.459636 139656649742080 logging_writer.py:48] [153100] global_step=153100, grad_norm=6.8076491355896, loss=1.9456377029418945
I0127 10:06:36.515613 139656297445120 logging_writer.py:48] [153200] global_step=153200, grad_norm=6.598353862762451, loss=1.9414019584655762
I0127 10:07:10.569087 139656649742080 logging_writer.py:48] [153300] global_step=153300, grad_norm=6.390610218048096, loss=1.9778168201446533
I0127 10:07:44.606813 139656297445120 logging_writer.py:48] [153400] global_step=153400, grad_norm=6.201834201812744, loss=2.017489194869995
I0127 10:08:18.677701 139656649742080 logging_writer.py:48] [153500] global_step=153500, grad_norm=6.40447473526001, loss=2.076885938644409
I0127 10:08:52.723060 139656297445120 logging_writer.py:48] [153600] global_step=153600, grad_norm=7.225463390350342, loss=2.035382032394409
I0127 10:09:26.839002 139656649742080 logging_writer.py:48] [153700] global_step=153700, grad_norm=6.669705867767334, loss=2.124329090118408
I0127 10:10:00.919613 139656297445120 logging_writer.py:48] [153800] global_step=153800, grad_norm=6.629127025604248, loss=1.9715700149536133
I0127 10:10:35.016584 139656649742080 logging_writer.py:48] [153900] global_step=153900, grad_norm=6.624584674835205, loss=1.972151279449463
I0127 10:11:09.086960 139656297445120 logging_writer.py:48] [154000] global_step=154000, grad_norm=6.611621379852295, loss=2.0548112392425537
I0127 10:11:43.166738 139656649742080 logging_writer.py:48] [154100] global_step=154100, grad_norm=6.343257427215576, loss=2.014685869216919
I0127 10:12:05.442104 139822745589568 spec.py:321] Evaluating on the training split.
I0127 10:12:11.674530 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 10:12:20.410693 139822745589568 spec.py:349] Evaluating on the test split.
I0127 10:12:22.645026 139822745589568 submission_runner.py:408] Time since start: 54536.51s, 	Step: 154167, 	{'train/accuracy': 0.8671875, 'train/loss': 0.5794281959533691, 'validation/accuracy': 0.7481399774551392, 'validation/loss': 1.0786703824996948, 'validation/num_examples': 50000, 'test/accuracy': 0.6278000473976135, 'test/loss': 1.6983375549316406, 'test/num_examples': 10000, 'score': 52591.11575245857, 'total_duration': 54536.50642514229, 'accumulated_submission_time': 52591.11575245857, 'accumulated_eval_time': 1934.90061545372, 'accumulated_logging_time': 5.321510553359985}
I0127 10:12:22.687615 139656700098304 logging_writer.py:48] [154167] accumulated_eval_time=1934.900615, accumulated_logging_time=5.321511, accumulated_submission_time=52591.115752, global_step=154167, preemption_count=0, score=52591.115752, test/accuracy=0.627800, test/loss=1.698338, test/num_examples=10000, total_duration=54536.506425, train/accuracy=0.867188, train/loss=0.579428, validation/accuracy=0.748140, validation/loss=1.078670, validation/num_examples=50000
I0127 10:12:34.255251 139658730145536 logging_writer.py:48] [154200] global_step=154200, grad_norm=6.819832801818848, loss=2.0411105155944824
I0127 10:13:08.296128 139656700098304 logging_writer.py:48] [154300] global_step=154300, grad_norm=6.468550205230713, loss=2.0120761394500732
I0127 10:13:42.342858 139658730145536 logging_writer.py:48] [154400] global_step=154400, grad_norm=7.048882007598877, loss=2.0393640995025635
I0127 10:14:16.387235 139656700098304 logging_writer.py:48] [154500] global_step=154500, grad_norm=6.788444995880127, loss=2.007847309112549
I0127 10:14:50.422591 139658730145536 logging_writer.py:48] [154600] global_step=154600, grad_norm=6.542410850524902, loss=2.0538408756256104
I0127 10:15:24.555624 139656700098304 logging_writer.py:48] [154700] global_step=154700, grad_norm=5.820356845855713, loss=1.9491724967956543
I0127 10:15:58.633117 139658730145536 logging_writer.py:48] [154800] global_step=154800, grad_norm=6.0272932052612305, loss=1.9961578845977783
I0127 10:16:32.712449 139656700098304 logging_writer.py:48] [154900] global_step=154900, grad_norm=6.850303649902344, loss=1.9662227630615234
I0127 10:17:06.776141 139658730145536 logging_writer.py:48] [155000] global_step=155000, grad_norm=6.836638927459717, loss=2.0658912658691406
I0127 10:17:40.825896 139656700098304 logging_writer.py:48] [155100] global_step=155100, grad_norm=6.699887275695801, loss=1.9974884986877441
I0127 10:18:14.904971 139658730145536 logging_writer.py:48] [155200] global_step=155200, grad_norm=6.838086128234863, loss=1.9320217370986938
I0127 10:18:48.962156 139656700098304 logging_writer.py:48] [155300] global_step=155300, grad_norm=6.358556747436523, loss=2.0365025997161865
I0127 10:19:23.023378 139658730145536 logging_writer.py:48] [155400] global_step=155400, grad_norm=6.530910968780518, loss=2.008600950241089
I0127 10:19:57.070515 139656700098304 logging_writer.py:48] [155500] global_step=155500, grad_norm=6.446916580200195, loss=1.9534751176834106
I0127 10:20:31.188205 139658730145536 logging_writer.py:48] [155600] global_step=155600, grad_norm=6.9442853927612305, loss=2.0163521766662598
I0127 10:20:52.772627 139822745589568 spec.py:321] Evaluating on the training split.
I0127 10:20:59.028650 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 10:21:07.746067 139822745589568 spec.py:349] Evaluating on the test split.
I0127 10:21:10.074207 139822745589568 submission_runner.py:408] Time since start: 55063.94s, 	Step: 155665, 	{'train/accuracy': 0.8687419891357422, 'train/loss': 0.5882298946380615, 'validation/accuracy': 0.7505999803543091, 'validation/loss': 1.0769861936569214, 'validation/num_examples': 50000, 'test/accuracy': 0.6246000528335571, 'test/loss': 1.6996747255325317, 'test/num_examples': 10000, 'score': 53101.14227557182, 'total_duration': 55063.9355969429, 'accumulated_submission_time': 53101.14227557182, 'accumulated_eval_time': 1952.2021520137787, 'accumulated_logging_time': 5.373865604400635}
I0127 10:21:10.117373 139656666527488 logging_writer.py:48] [155665] accumulated_eval_time=1952.202152, accumulated_logging_time=5.373866, accumulated_submission_time=53101.142276, global_step=155665, preemption_count=0, score=53101.142276, test/accuracy=0.624600, test/loss=1.699675, test/num_examples=10000, total_duration=55063.935597, train/accuracy=0.868742, train/loss=0.588230, validation/accuracy=0.750600, validation/loss=1.076986, validation/num_examples=50000
I0127 10:21:22.522891 139656674920192 logging_writer.py:48] [155700] global_step=155700, grad_norm=6.019148826599121, loss=1.9599721431732178
I0127 10:21:56.565521 139656666527488 logging_writer.py:48] [155800] global_step=155800, grad_norm=6.761764049530029, loss=1.9093303680419922
I0127 10:22:30.647010 139656674920192 logging_writer.py:48] [155900] global_step=155900, grad_norm=7.401124954223633, loss=2.0071182250976562
I0127 10:23:04.713108 139656666527488 logging_writer.py:48] [156000] global_step=156000, grad_norm=5.9088358879089355, loss=1.9961973428726196
I0127 10:23:38.772234 139656674920192 logging_writer.py:48] [156100] global_step=156100, grad_norm=6.383271217346191, loss=1.9604932069778442
I0127 10:24:12.853522 139656666527488 logging_writer.py:48] [156200] global_step=156200, grad_norm=6.326725006103516, loss=2.029505968093872
I0127 10:24:46.915560 139656674920192 logging_writer.py:48] [156300] global_step=156300, grad_norm=7.126922130584717, loss=1.9923138618469238
I0127 10:25:20.988717 139656666527488 logging_writer.py:48] [156400] global_step=156400, grad_norm=6.618186950683594, loss=1.9693348407745361
I0127 10:25:55.045350 139656674920192 logging_writer.py:48] [156500] global_step=156500, grad_norm=7.443127155303955, loss=1.952004075050354
I0127 10:26:29.131669 139656666527488 logging_writer.py:48] [156600] global_step=156600, grad_norm=6.755917072296143, loss=1.911545991897583
I0127 10:27:03.185941 139656674920192 logging_writer.py:48] [156700] global_step=156700, grad_norm=6.550538539886475, loss=1.9798933267593384
I0127 10:27:37.364490 139656666527488 logging_writer.py:48] [156800] global_step=156800, grad_norm=7.073777198791504, loss=1.9395625591278076
I0127 10:28:11.379919 139656674920192 logging_writer.py:48] [156900] global_step=156900, grad_norm=6.748880863189697, loss=1.9524801969528198
I0127 10:28:45.433512 139656666527488 logging_writer.py:48] [157000] global_step=157000, grad_norm=6.237751007080078, loss=1.8760395050048828
I0127 10:29:19.506971 139656674920192 logging_writer.py:48] [157100] global_step=157100, grad_norm=6.800388336181641, loss=1.9917402267456055
I0127 10:29:40.098598 139822745589568 spec.py:321] Evaluating on the training split.
I0127 10:29:46.273720 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 10:29:54.749842 139822745589568 spec.py:349] Evaluating on the test split.
I0127 10:29:57.026615 139822745589568 submission_runner.py:408] Time since start: 55590.89s, 	Step: 157162, 	{'train/accuracy': 0.8694794178009033, 'train/loss': 0.580014169216156, 'validation/accuracy': 0.7535199522972107, 'validation/loss': 1.0628749132156372, 'validation/num_examples': 50000, 'test/accuracy': 0.6239000558853149, 'test/loss': 1.6967737674713135, 'test/num_examples': 10000, 'score': 53611.06381011009, 'total_duration': 55590.8880238533, 'accumulated_submission_time': 53611.06381011009, 'accumulated_eval_time': 1969.1301229000092, 'accumulated_logging_time': 5.426816463470459}
I0127 10:29:57.074461 139656297445120 logging_writer.py:48] [157162] accumulated_eval_time=1969.130123, accumulated_logging_time=5.426816, accumulated_submission_time=53611.063810, global_step=157162, preemption_count=0, score=53611.063810, test/accuracy=0.623900, test/loss=1.696774, test/num_examples=10000, total_duration=55590.888024, train/accuracy=0.869479, train/loss=0.580014, validation/accuracy=0.753520, validation/loss=1.062875, validation/num_examples=50000
I0127 10:30:10.361772 139656649742080 logging_writer.py:48] [157200] global_step=157200, grad_norm=6.866216659545898, loss=1.971502423286438
I0127 10:30:44.399097 139656297445120 logging_writer.py:48] [157300] global_step=157300, grad_norm=6.416652679443359, loss=1.882001519203186
I0127 10:31:18.457415 139656649742080 logging_writer.py:48] [157400] global_step=157400, grad_norm=7.605220794677734, loss=1.9836174249649048
I0127 10:31:52.520527 139656297445120 logging_writer.py:48] [157500] global_step=157500, grad_norm=6.87501859664917, loss=1.9190301895141602
I0127 10:32:26.606467 139656649742080 logging_writer.py:48] [157600] global_step=157600, grad_norm=6.28400993347168, loss=1.9176658391952515
I0127 10:33:00.661377 139656297445120 logging_writer.py:48] [157700] global_step=157700, grad_norm=6.798328876495361, loss=1.890507698059082
I0127 10:33:34.738533 139656649742080 logging_writer.py:48] [157800] global_step=157800, grad_norm=6.027482032775879, loss=1.8933123350143433
I0127 10:34:08.864529 139656297445120 logging_writer.py:48] [157900] global_step=157900, grad_norm=6.761315822601318, loss=1.9854631423950195
I0127 10:34:42.935559 139656649742080 logging_writer.py:48] [158000] global_step=158000, grad_norm=6.48811149597168, loss=1.9596734046936035
I0127 10:35:17.020259 139656297445120 logging_writer.py:48] [158100] global_step=158100, grad_norm=6.457066535949707, loss=1.983842134475708
I0127 10:35:51.105274 139656649742080 logging_writer.py:48] [158200] global_step=158200, grad_norm=6.492990016937256, loss=1.9620773792266846
I0127 10:36:25.201700 139656297445120 logging_writer.py:48] [158300] global_step=158300, grad_norm=7.625866889953613, loss=2.0529439449310303
I0127 10:36:59.273509 139656649742080 logging_writer.py:48] [158400] global_step=158400, grad_norm=6.929214000701904, loss=2.0177626609802246
I0127 10:37:33.338812 139656297445120 logging_writer.py:48] [158500] global_step=158500, grad_norm=7.102456092834473, loss=1.9522628784179688
I0127 10:38:07.432694 139656649742080 logging_writer.py:48] [158600] global_step=158600, grad_norm=6.569952011108398, loss=1.903954029083252
I0127 10:38:27.320867 139822745589568 spec.py:321] Evaluating on the training split.
I0127 10:38:33.529004 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 10:38:42.064617 139822745589568 spec.py:349] Evaluating on the test split.
I0127 10:38:44.333046 139822745589568 submission_runner.py:408] Time since start: 56118.19s, 	Step: 158660, 	{'train/accuracy': 0.8698182106018066, 'train/loss': 0.5817106366157532, 'validation/accuracy': 0.7541399598121643, 'validation/loss': 1.064910888671875, 'validation/num_examples': 50000, 'test/accuracy': 0.6336000561714172, 'test/loss': 1.6853803396224976, 'test/num_examples': 10000, 'score': 54121.251658678055, 'total_duration': 56118.194451093674, 'accumulated_submission_time': 54121.251658678055, 'accumulated_eval_time': 1986.1422533988953, 'accumulated_logging_time': 5.484708070755005}
I0127 10:38:44.379899 139656297445120 logging_writer.py:48] [158660] accumulated_eval_time=1986.142253, accumulated_logging_time=5.484708, accumulated_submission_time=54121.251659, global_step=158660, preemption_count=0, score=54121.251659, test/accuracy=0.633600, test/loss=1.685380, test/num_examples=10000, total_duration=56118.194451, train/accuracy=0.869818, train/loss=0.581711, validation/accuracy=0.754140, validation/loss=1.064911, validation/num_examples=50000
I0127 10:38:58.342837 139656683312896 logging_writer.py:48] [158700] global_step=158700, grad_norm=7.071068286895752, loss=1.987366795539856
I0127 10:39:32.348382 139656297445120 logging_writer.py:48] [158800] global_step=158800, grad_norm=7.151721000671387, loss=1.9861371517181396
I0127 10:40:06.449189 139656683312896 logging_writer.py:48] [158900] global_step=158900, grad_norm=6.897851943969727, loss=1.9235848188400269
I0127 10:40:40.518797 139656297445120 logging_writer.py:48] [159000] global_step=159000, grad_norm=7.240267753601074, loss=1.978503942489624
I0127 10:41:14.597147 139656683312896 logging_writer.py:48] [159100] global_step=159100, grad_norm=6.718160629272461, loss=2.016672372817993
I0127 10:41:48.674946 139656297445120 logging_writer.py:48] [159200] global_step=159200, grad_norm=6.86095666885376, loss=1.9648480415344238
I0127 10:42:22.763795 139656683312896 logging_writer.py:48] [159300] global_step=159300, grad_norm=6.683438777923584, loss=1.9407864809036255
I0127 10:42:56.845244 139656297445120 logging_writer.py:48] [159400] global_step=159400, grad_norm=6.65812349319458, loss=2.0529098510742188
I0127 10:43:30.921219 139656683312896 logging_writer.py:48] [159500] global_step=159500, grad_norm=6.822144508361816, loss=1.9544867277145386
I0127 10:44:04.989845 139656297445120 logging_writer.py:48] [159600] global_step=159600, grad_norm=6.931162357330322, loss=1.8707976341247559
I0127 10:44:39.083349 139656683312896 logging_writer.py:48] [159700] global_step=159700, grad_norm=7.051519393920898, loss=1.940134048461914
I0127 10:45:13.132782 139656297445120 logging_writer.py:48] [159800] global_step=159800, grad_norm=6.738631248474121, loss=1.905753254890442
I0127 10:45:47.167911 139656683312896 logging_writer.py:48] [159900] global_step=159900, grad_norm=6.915086269378662, loss=1.974352478981018
I0127 10:46:21.371981 139656297445120 logging_writer.py:48] [160000] global_step=160000, grad_norm=7.03000545501709, loss=1.863073706626892
I0127 10:46:55.424669 139656683312896 logging_writer.py:48] [160100] global_step=160100, grad_norm=7.008747100830078, loss=1.9876585006713867
I0127 10:47:14.631141 139822745589568 spec.py:321] Evaluating on the training split.
I0127 10:47:20.838336 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 10:47:29.587499 139822745589568 spec.py:349] Evaluating on the test split.
I0127 10:47:31.859588 139822745589568 submission_runner.py:408] Time since start: 56645.72s, 	Step: 160158, 	{'train/accuracy': 0.875418484210968, 'train/loss': 0.5687929391860962, 'validation/accuracy': 0.756879985332489, 'validation/loss': 1.0507851839065552, 'validation/num_examples': 50000, 'test/accuracy': 0.6309000253677368, 'test/loss': 1.6826121807098389, 'test/num_examples': 10000, 'score': 54631.442410707474, 'total_duration': 56645.72098970413, 'accumulated_submission_time': 54631.442410707474, 'accumulated_eval_time': 2003.3706483840942, 'accumulated_logging_time': 5.54168963432312}
I0127 10:47:31.907263 139656700098304 logging_writer.py:48] [160158] accumulated_eval_time=2003.370648, accumulated_logging_time=5.541690, accumulated_submission_time=54631.442411, global_step=160158, preemption_count=0, score=54631.442411, test/accuracy=0.630900, test/loss=1.682612, test/num_examples=10000, total_duration=56645.720990, train/accuracy=0.875418, train/loss=0.568793, validation/accuracy=0.756880, validation/loss=1.050785, validation/num_examples=50000
I0127 10:47:46.538035 139658730145536 logging_writer.py:48] [160200] global_step=160200, grad_norm=6.767639636993408, loss=1.981287956237793
I0127 10:48:20.523724 139656700098304 logging_writer.py:48] [160300] global_step=160300, grad_norm=7.190932273864746, loss=1.9586620330810547
I0127 10:48:54.574597 139658730145536 logging_writer.py:48] [160400] global_step=160400, grad_norm=7.60753059387207, loss=1.9253575801849365
I0127 10:49:28.641188 139656700098304 logging_writer.py:48] [160500] global_step=160500, grad_norm=6.713435173034668, loss=1.9660834074020386
I0127 10:50:02.702867 139658730145536 logging_writer.py:48] [160600] global_step=160600, grad_norm=7.791473865509033, loss=2.0241317749023438
I0127 10:50:36.707129 139656700098304 logging_writer.py:48] [160700] global_step=160700, grad_norm=7.846524715423584, loss=1.9857653379440308
I0127 10:51:10.761079 139658730145536 logging_writer.py:48] [160800] global_step=160800, grad_norm=7.4109416007995605, loss=1.9822380542755127
I0127 10:51:44.811304 139656700098304 logging_writer.py:48] [160900] global_step=160900, grad_norm=7.11441707611084, loss=1.982327938079834
I0127 10:52:18.928522 139658730145536 logging_writer.py:48] [161000] global_step=161000, grad_norm=6.762549877166748, loss=1.8751577138900757
I0127 10:52:52.992505 139656700098304 logging_writer.py:48] [161100] global_step=161100, grad_norm=6.912176132202148, loss=1.941861629486084
I0127 10:53:27.050418 139658730145536 logging_writer.py:48] [161200] global_step=161200, grad_norm=6.750073432922363, loss=1.9091558456420898
I0127 10:54:01.143950 139656700098304 logging_writer.py:48] [161300] global_step=161300, grad_norm=7.112112045288086, loss=2.0154037475585938
I0127 10:54:35.218400 139658730145536 logging_writer.py:48] [161400] global_step=161400, grad_norm=6.944118022918701, loss=1.8761383295059204
I0127 10:55:09.294775 139656700098304 logging_writer.py:48] [161500] global_step=161500, grad_norm=7.814725399017334, loss=1.978884220123291
I0127 10:55:43.370326 139658730145536 logging_writer.py:48] [161600] global_step=161600, grad_norm=7.589658260345459, loss=1.9160418510437012
I0127 10:56:01.905369 139822745589568 spec.py:321] Evaluating on the training split.
I0127 10:56:08.146619 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 10:56:16.829737 139822745589568 spec.py:349] Evaluating on the test split.
I0127 10:56:19.183698 139822745589568 submission_runner.py:408] Time since start: 57173.04s, 	Step: 161656, 	{'train/accuracy': 0.8792450428009033, 'train/loss': 0.5455688238143921, 'validation/accuracy': 0.7571799755096436, 'validation/loss': 1.0448676347732544, 'validation/num_examples': 50000, 'test/accuracy': 0.6301000118255615, 'test/loss': 1.673742413520813, 'test/num_examples': 10000, 'score': 55141.38109588623, 'total_duration': 57173.04496335983, 'accumulated_submission_time': 55141.38109588623, 'accumulated_eval_time': 2020.6487910747528, 'accumulated_logging_time': 5.598784685134888}
I0127 10:56:19.230642 139656649742080 logging_writer.py:48] [161656] accumulated_eval_time=2020.648791, accumulated_logging_time=5.598785, accumulated_submission_time=55141.381096, global_step=161656, preemption_count=0, score=55141.381096, test/accuracy=0.630100, test/loss=1.673742, test/num_examples=10000, total_duration=57173.044963, train/accuracy=0.879245, train/loss=0.545569, validation/accuracy=0.757180, validation/loss=1.044868, validation/num_examples=50000
I0127 10:56:34.553523 139656658134784 logging_writer.py:48] [161700] global_step=161700, grad_norm=6.863358020782471, loss=1.847379207611084
I0127 10:57:08.554060 139656649742080 logging_writer.py:48] [161800] global_step=161800, grad_norm=6.941333770751953, loss=1.8652057647705078
I0127 10:57:42.580072 139656658134784 logging_writer.py:48] [161900] global_step=161900, grad_norm=6.881478786468506, loss=1.9090662002563477
I0127 10:58:16.606235 139656649742080 logging_writer.py:48] [162000] global_step=162000, grad_norm=8.423663139343262, loss=1.957594633102417
I0127 10:58:50.742118 139656658134784 logging_writer.py:48] [162100] global_step=162100, grad_norm=7.784857749938965, loss=1.9020006656646729
I0127 10:59:24.770820 139656649742080 logging_writer.py:48] [162200] global_step=162200, grad_norm=7.115670204162598, loss=1.9323983192443848
I0127 10:59:58.825127 139656658134784 logging_writer.py:48] [162300] global_step=162300, grad_norm=7.4120073318481445, loss=1.9759886264801025
I0127 11:00:32.883704 139656649742080 logging_writer.py:48] [162400] global_step=162400, grad_norm=7.254137992858887, loss=1.8939217329025269
I0127 11:01:06.913461 139656658134784 logging_writer.py:48] [162500] global_step=162500, grad_norm=7.579111576080322, loss=1.9252972602844238
I0127 11:01:40.983980 139656649742080 logging_writer.py:48] [162600] global_step=162600, grad_norm=7.150161266326904, loss=1.884204387664795
I0127 11:02:15.067652 139656658134784 logging_writer.py:48] [162700] global_step=162700, grad_norm=6.846484184265137, loss=1.8951903581619263
I0127 11:02:49.139699 139656649742080 logging_writer.py:48] [162800] global_step=162800, grad_norm=7.039794445037842, loss=1.897978663444519
I0127 11:03:23.188402 139656658134784 logging_writer.py:48] [162900] global_step=162900, grad_norm=7.030216693878174, loss=1.846066951751709
I0127 11:03:57.209825 139656649742080 logging_writer.py:48] [163000] global_step=163000, grad_norm=7.546666145324707, loss=1.8900489807128906
I0127 11:04:31.285617 139656658134784 logging_writer.py:48] [163100] global_step=163100, grad_norm=7.369758129119873, loss=1.916512370109558
I0127 11:04:49.196041 139822745589568 spec.py:321] Evaluating on the training split.
I0127 11:04:55.401469 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 11:05:03.944731 139822745589568 spec.py:349] Evaluating on the test split.
I0127 11:05:06.228939 139822745589568 submission_runner.py:408] Time since start: 57700.09s, 	Step: 163154, 	{'train/accuracy': 0.8960259556770325, 'train/loss': 0.49101361632347107, 'validation/accuracy': 0.7576000094413757, 'validation/loss': 1.0471340417861938, 'validation/num_examples': 50000, 'test/accuracy': 0.6349000334739685, 'test/loss': 1.6725289821624756, 'test/num_examples': 10000, 'score': 55651.28565096855, 'total_duration': 57700.09034585953, 'accumulated_submission_time': 55651.28565096855, 'accumulated_eval_time': 2037.681653022766, 'accumulated_logging_time': 5.65578556060791}
I0127 11:05:06.279332 139656649742080 logging_writer.py:48] [163154] accumulated_eval_time=2037.681653, accumulated_logging_time=5.655786, accumulated_submission_time=55651.285651, global_step=163154, preemption_count=0, score=55651.285651, test/accuracy=0.634900, test/loss=1.672529, test/num_examples=10000, total_duration=57700.090346, train/accuracy=0.896026, train/loss=0.491014, validation/accuracy=0.757600, validation/loss=1.047134, validation/num_examples=50000
I0127 11:05:22.275450 139656658134784 logging_writer.py:48] [163200] global_step=163200, grad_norm=6.97080135345459, loss=1.9640452861785889
I0127 11:05:56.294385 139656649742080 logging_writer.py:48] [163300] global_step=163300, grad_norm=7.6353936195373535, loss=1.9375419616699219
I0127 11:06:30.305757 139656658134784 logging_writer.py:48] [163400] global_step=163400, grad_norm=7.152023792266846, loss=1.9123570919036865
I0127 11:07:04.345507 139656649742080 logging_writer.py:48] [163500] global_step=163500, grad_norm=7.062088489532471, loss=1.898873209953308
I0127 11:07:38.390558 139656658134784 logging_writer.py:48] [163600] global_step=163600, grad_norm=7.284323215484619, loss=1.8575881719589233
I0127 11:08:12.466717 139656649742080 logging_writer.py:48] [163700] global_step=163700, grad_norm=7.3311967849731445, loss=1.9017729759216309
I0127 11:08:46.525051 139656658134784 logging_writer.py:48] [163800] global_step=163800, grad_norm=6.85099458694458, loss=1.8465428352355957
I0127 11:09:20.598725 139656649742080 logging_writer.py:48] [163900] global_step=163900, grad_norm=6.976068496704102, loss=1.8741095066070557
I0127 11:09:54.676803 139656658134784 logging_writer.py:48] [164000] global_step=164000, grad_norm=7.202106475830078, loss=1.9319677352905273
I0127 11:10:28.736651 139656649742080 logging_writer.py:48] [164100] global_step=164100, grad_norm=7.884264945983887, loss=1.9086763858795166
I0127 11:11:02.858691 139656658134784 logging_writer.py:48] [164200] global_step=164200, grad_norm=6.998528003692627, loss=1.8637292385101318
I0127 11:11:36.928113 139656649742080 logging_writer.py:48] [164300] global_step=164300, grad_norm=7.4660844802856445, loss=1.9189233779907227
I0127 11:12:10.984599 139656658134784 logging_writer.py:48] [164400] global_step=164400, grad_norm=7.5344977378845215, loss=1.9604296684265137
I0127 11:12:45.037000 139656649742080 logging_writer.py:48] [164500] global_step=164500, grad_norm=6.885470390319824, loss=1.865761637687683
I0127 11:13:19.083863 139656658134784 logging_writer.py:48] [164600] global_step=164600, grad_norm=7.3127899169921875, loss=1.9188028573989868
I0127 11:13:36.244275 139822745589568 spec.py:321] Evaluating on the training split.
I0127 11:13:42.513952 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 11:13:51.026689 139822745589568 spec.py:349] Evaluating on the test split.
I0127 11:13:53.333628 139822745589568 submission_runner.py:408] Time since start: 58227.20s, 	Step: 164652, 	{'train/accuracy': 0.8957070708274841, 'train/loss': 0.49083128571510315, 'validation/accuracy': 0.7618599534034729, 'validation/loss': 1.0314052104949951, 'validation/num_examples': 50000, 'test/accuracy': 0.6367000341415405, 'test/loss': 1.661550521850586, 'test/num_examples': 10000, 'score': 56161.19146823883, 'total_duration': 58227.19503569603, 'accumulated_submission_time': 56161.19146823883, 'accumulated_eval_time': 2054.770972251892, 'accumulated_logging_time': 5.715636491775513}
I0127 11:13:53.381065 139656691705600 logging_writer.py:48] [164652] accumulated_eval_time=2054.770972, accumulated_logging_time=5.715636, accumulated_submission_time=56161.191468, global_step=164652, preemption_count=0, score=56161.191468, test/accuracy=0.636700, test/loss=1.661551, test/num_examples=10000, total_duration=58227.195036, train/accuracy=0.895707, train/loss=0.490831, validation/accuracy=0.761860, validation/loss=1.031405, validation/num_examples=50000
I0127 11:14:10.070729 139656700098304 logging_writer.py:48] [164700] global_step=164700, grad_norm=7.613019943237305, loss=1.8779493570327759
I0127 11:14:44.094127 139656691705600 logging_writer.py:48] [164800] global_step=164800, grad_norm=7.164655685424805, loss=1.8982073068618774
I0127 11:15:18.153776 139656700098304 logging_writer.py:48] [164900] global_step=164900, grad_norm=7.772175312042236, loss=1.9007244110107422
I0127 11:15:52.224597 139656691705600 logging_writer.py:48] [165000] global_step=165000, grad_norm=7.39153528213501, loss=1.8929648399353027
I0127 11:16:26.267169 139656700098304 logging_writer.py:48] [165100] global_step=165100, grad_norm=7.8989338874816895, loss=1.9007008075714111
I0127 11:17:00.482759 139656691705600 logging_writer.py:48] [165200] global_step=165200, grad_norm=7.592467784881592, loss=1.898017168045044
I0127 11:17:34.552452 139656700098304 logging_writer.py:48] [165300] global_step=165300, grad_norm=7.469330787658691, loss=1.8673003911972046
I0127 11:18:08.633071 139656691705600 logging_writer.py:48] [165400] global_step=165400, grad_norm=7.665046215057373, loss=1.8941235542297363
I0127 11:18:42.707761 139656700098304 logging_writer.py:48] [165500] global_step=165500, grad_norm=7.447930335998535, loss=1.903446078300476
I0127 11:19:16.768612 139656691705600 logging_writer.py:48] [165600] global_step=165600, grad_norm=7.657830238342285, loss=1.9055296182632446
I0127 11:19:50.825245 139656700098304 logging_writer.py:48] [165700] global_step=165700, grad_norm=6.858541488647461, loss=1.8737514019012451
I0127 11:20:24.884343 139656691705600 logging_writer.py:48] [165800] global_step=165800, grad_norm=7.222423076629639, loss=1.898916244506836
I0127 11:20:58.975973 139656700098304 logging_writer.py:48] [165900] global_step=165900, grad_norm=6.97603178024292, loss=1.8414678573608398
I0127 11:21:33.027740 139656691705600 logging_writer.py:48] [166000] global_step=166000, grad_norm=8.522652626037598, loss=1.8938896656036377
I0127 11:22:07.040027 139656700098304 logging_writer.py:48] [166100] global_step=166100, grad_norm=7.597463607788086, loss=1.8642364740371704
I0127 11:22:23.527770 139822745589568 spec.py:321] Evaluating on the training split.
I0127 11:22:29.820905 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 11:22:38.320121 139822745589568 spec.py:349] Evaluating on the test split.
I0127 11:22:40.597582 139822745589568 submission_runner.py:408] Time since start: 58754.46s, 	Step: 166150, 	{'train/accuracy': 0.8951490521430969, 'train/loss': 0.48919644951820374, 'validation/accuracy': 0.7637199759483337, 'validation/loss': 1.026726245880127, 'validation/num_examples': 50000, 'test/accuracy': 0.6398000121116638, 'test/loss': 1.6573363542556763, 'test/num_examples': 10000, 'score': 56671.27508664131, 'total_duration': 58754.45888543129, 'accumulated_submission_time': 56671.27508664131, 'accumulated_eval_time': 2071.8406381607056, 'accumulated_logging_time': 5.775366306304932}
I0127 11:22:40.645878 139656658134784 logging_writer.py:48] [166150] accumulated_eval_time=2071.840638, accumulated_logging_time=5.775366, accumulated_submission_time=56671.275087, global_step=166150, preemption_count=0, score=56671.275087, test/accuracy=0.639800, test/loss=1.657336, test/num_examples=10000, total_duration=58754.458885, train/accuracy=0.895149, train/loss=0.489196, validation/accuracy=0.763720, validation/loss=1.026726, validation/num_examples=50000
I0127 11:22:57.989062 139656666527488 logging_writer.py:48] [166200] global_step=166200, grad_norm=7.166867256164551, loss=1.8568720817565918
I0127 11:23:32.114347 139656658134784 logging_writer.py:48] [166300] global_step=166300, grad_norm=7.561208724975586, loss=1.866544246673584
I0127 11:24:06.140893 139656666527488 logging_writer.py:48] [166400] global_step=166400, grad_norm=8.197269439697266, loss=1.9154174327850342
I0127 11:24:40.211379 139656658134784 logging_writer.py:48] [166500] global_step=166500, grad_norm=7.657230377197266, loss=1.8664547204971313
I0127 11:25:14.239380 139656666527488 logging_writer.py:48] [166600] global_step=166600, grad_norm=7.127018451690674, loss=1.8304595947265625
I0127 11:25:48.288359 139656658134784 logging_writer.py:48] [166700] global_step=166700, grad_norm=7.319458484649658, loss=1.9306654930114746
I0127 11:26:22.356227 139656666527488 logging_writer.py:48] [166800] global_step=166800, grad_norm=7.399407386779785, loss=1.8318208456039429
I0127 11:26:56.405657 139656658134784 logging_writer.py:48] [166900] global_step=166900, grad_norm=7.595518112182617, loss=1.8594567775726318
I0127 11:27:30.485051 139656666527488 logging_writer.py:48] [167000] global_step=167000, grad_norm=6.684193134307861, loss=1.8623223304748535
I0127 11:28:04.542737 139656658134784 logging_writer.py:48] [167100] global_step=167100, grad_norm=8.106978416442871, loss=1.9365304708480835
I0127 11:28:38.602481 139656666527488 logging_writer.py:48] [167200] global_step=167200, grad_norm=6.8460845947265625, loss=1.817178726196289
I0127 11:29:12.688670 139656658134784 logging_writer.py:48] [167300] global_step=167300, grad_norm=7.172441005706787, loss=1.8486437797546387
I0127 11:29:46.814594 139656666527488 logging_writer.py:48] [167400] global_step=167400, grad_norm=7.411465644836426, loss=1.8716672658920288
I0127 11:30:20.880734 139656658134784 logging_writer.py:48] [167500] global_step=167500, grad_norm=7.671167373657227, loss=1.8545249700546265
I0127 11:30:54.946979 139656666527488 logging_writer.py:48] [167600] global_step=167600, grad_norm=7.2683539390563965, loss=1.833404779434204
I0127 11:31:10.748669 139822745589568 spec.py:321] Evaluating on the training split.
I0127 11:31:17.055532 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 11:31:25.632624 139822745589568 spec.py:349] Evaluating on the test split.
I0127 11:31:27.888029 139822745589568 submission_runner.py:408] Time since start: 59281.75s, 	Step: 167648, 	{'train/accuracy': 0.8947305083274841, 'train/loss': 0.4829561114311218, 'validation/accuracy': 0.7655199766159058, 'validation/loss': 1.0176388025283813, 'validation/num_examples': 50000, 'test/accuracy': 0.6388000249862671, 'test/loss': 1.6397991180419922, 'test/num_examples': 10000, 'score': 57181.31681752205, 'total_duration': 59281.749415397644, 'accumulated_submission_time': 57181.31681752205, 'accumulated_eval_time': 2088.97993516922, 'accumulated_logging_time': 5.834702968597412}
I0127 11:31:27.932468 139656297445120 logging_writer.py:48] [167648] accumulated_eval_time=2088.979935, accumulated_logging_time=5.834703, accumulated_submission_time=57181.316818, global_step=167648, preemption_count=0, score=57181.316818, test/accuracy=0.638800, test/loss=1.639799, test/num_examples=10000, total_duration=59281.749415, train/accuracy=0.894731, train/loss=0.482956, validation/accuracy=0.765520, validation/loss=1.017639, validation/num_examples=50000
I0127 11:31:45.963595 139656649742080 logging_writer.py:48] [167700] global_step=167700, grad_norm=7.472301006317139, loss=1.8219246864318848
I0127 11:32:20.005016 139656297445120 logging_writer.py:48] [167800] global_step=167800, grad_norm=7.741883754730225, loss=1.8812265396118164
I0127 11:32:54.056214 139656649742080 logging_writer.py:48] [167900] global_step=167900, grad_norm=7.513864040374756, loss=1.8169751167297363
I0127 11:33:28.121599 139656297445120 logging_writer.py:48] [168000] global_step=168000, grad_norm=6.789621353149414, loss=1.7863799333572388
I0127 11:34:02.182727 139656649742080 logging_writer.py:48] [168100] global_step=168100, grad_norm=8.298690795898438, loss=1.8733258247375488
I0127 11:34:36.248668 139656297445120 logging_writer.py:48] [168200] global_step=168200, grad_norm=7.7360734939575195, loss=1.8951231241226196
I0127 11:35:10.296686 139656649742080 logging_writer.py:48] [168300] global_step=168300, grad_norm=6.848623752593994, loss=1.817291021347046
I0127 11:35:44.452996 139656297445120 logging_writer.py:48] [168400] global_step=168400, grad_norm=7.765068531036377, loss=1.7909977436065674
I0127 11:36:18.504722 139656649742080 logging_writer.py:48] [168500] global_step=168500, grad_norm=7.9511518478393555, loss=1.8934657573699951
I0127 11:36:52.580806 139656297445120 logging_writer.py:48] [168600] global_step=168600, grad_norm=7.954692363739014, loss=1.889431357383728
I0127 11:37:26.614559 139656649742080 logging_writer.py:48] [168700] global_step=168700, grad_norm=7.83146858215332, loss=1.8827999830245972
I0127 11:38:00.671496 139656297445120 logging_writer.py:48] [168800] global_step=168800, grad_norm=7.381887435913086, loss=1.8240045309066772
I0127 11:38:34.768710 139656649742080 logging_writer.py:48] [168900] global_step=168900, grad_norm=7.55044412612915, loss=1.8891096115112305
I0127 11:39:08.823117 139656297445120 logging_writer.py:48] [169000] global_step=169000, grad_norm=7.602762222290039, loss=1.877569317817688
I0127 11:39:42.924110 139656649742080 logging_writer.py:48] [169100] global_step=169100, grad_norm=8.357992172241211, loss=1.8588942289352417
I0127 11:39:58.078812 139822745589568 spec.py:321] Evaluating on the training split.
I0127 11:40:04.488185 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 11:40:12.954905 139822745589568 spec.py:349] Evaluating on the test split.
I0127 11:40:15.238095 139822745589568 submission_runner.py:408] Time since start: 59809.10s, 	Step: 169146, 	{'train/accuracy': 0.9001514315605164, 'train/loss': 0.47772398591041565, 'validation/accuracy': 0.7644000053405762, 'validation/loss': 1.0248445272445679, 'validation/num_examples': 50000, 'test/accuracy': 0.6415000557899475, 'test/loss': 1.6550037860870361, 'test/num_examples': 10000, 'score': 57691.400824546814, 'total_duration': 59809.09949350357, 'accumulated_submission_time': 57691.400824546814, 'accumulated_eval_time': 2106.1391632556915, 'accumulated_logging_time': 5.890669107437134}
I0127 11:40:15.285647 139656649742080 logging_writer.py:48] [169146] accumulated_eval_time=2106.139163, accumulated_logging_time=5.890669, accumulated_submission_time=57691.400825, global_step=169146, preemption_count=0, score=57691.400825, test/accuracy=0.641500, test/loss=1.655004, test/num_examples=10000, total_duration=59809.099494, train/accuracy=0.900151, train/loss=0.477724, validation/accuracy=0.764400, validation/loss=1.024845, validation/num_examples=50000
I0127 11:40:34.007242 139656658134784 logging_writer.py:48] [169200] global_step=169200, grad_norm=7.913017749786377, loss=1.795767068862915
I0127 11:41:08.032756 139656649742080 logging_writer.py:48] [169300] global_step=169300, grad_norm=7.799386024475098, loss=1.9264740943908691
I0127 11:41:42.089059 139656658134784 logging_writer.py:48] [169400] global_step=169400, grad_norm=7.304279804229736, loss=1.8152042627334595
I0127 11:42:16.225201 139656649742080 logging_writer.py:48] [169500] global_step=169500, grad_norm=7.689044952392578, loss=1.8423562049865723
I0127 11:42:50.283789 139656658134784 logging_writer.py:48] [169600] global_step=169600, grad_norm=7.055341720581055, loss=1.8376911878585815
I0127 11:43:24.309812 139656649742080 logging_writer.py:48] [169700] global_step=169700, grad_norm=7.430266380310059, loss=1.8454848527908325
I0127 11:43:58.362106 139656658134784 logging_writer.py:48] [169800] global_step=169800, grad_norm=7.435370445251465, loss=1.8735566139221191
I0127 11:44:32.423694 139656649742080 logging_writer.py:48] [169900] global_step=169900, grad_norm=7.503000259399414, loss=1.8133900165557861
I0127 11:45:06.473118 139656658134784 logging_writer.py:48] [170000] global_step=170000, grad_norm=8.304471015930176, loss=1.919670820236206
I0127 11:45:40.500891 139656649742080 logging_writer.py:48] [170100] global_step=170100, grad_norm=7.131316661834717, loss=1.7963966131210327
I0127 11:46:14.554280 139656658134784 logging_writer.py:48] [170200] global_step=170200, grad_norm=8.111696243286133, loss=1.837497353553772
I0127 11:46:48.624608 139656649742080 logging_writer.py:48] [170300] global_step=170300, grad_norm=6.795229911804199, loss=1.8140078783035278
I0127 11:47:22.692929 139656658134784 logging_writer.py:48] [170400] global_step=170400, grad_norm=7.286991596221924, loss=1.8505431413650513
I0127 11:47:56.831756 139656649742080 logging_writer.py:48] [170500] global_step=170500, grad_norm=7.8410139083862305, loss=1.8569204807281494
I0127 11:48:30.900094 139656658134784 logging_writer.py:48] [170600] global_step=170600, grad_norm=7.703794479370117, loss=1.8842644691467285
I0127 11:48:45.349885 139822745589568 spec.py:321] Evaluating on the training split.
I0127 11:48:51.561318 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 11:49:00.139238 139822745589568 spec.py:349] Evaluating on the test split.
I0127 11:49:02.454140 139822745589568 submission_runner.py:408] Time since start: 60336.32s, 	Step: 170644, 	{'train/accuracy': 0.9061303734779358, 'train/loss': 0.46180209517478943, 'validation/accuracy': 0.7671999931335449, 'validation/loss': 1.0098974704742432, 'validation/num_examples': 50000, 'test/accuracy': 0.6467000246047974, 'test/loss': 1.6335299015045166, 'test/num_examples': 10000, 'score': 58201.406227350235, 'total_duration': 60336.315509557724, 'accumulated_submission_time': 58201.406227350235, 'accumulated_eval_time': 2123.243331670761, 'accumulated_logging_time': 5.947777032852173}
I0127 11:49:02.516924 139656649742080 logging_writer.py:48] [170644] accumulated_eval_time=2123.243332, accumulated_logging_time=5.947777, accumulated_submission_time=58201.406227, global_step=170644, preemption_count=0, score=58201.406227, test/accuracy=0.646700, test/loss=1.633530, test/num_examples=10000, total_duration=60336.315510, train/accuracy=0.906130, train/loss=0.461802, validation/accuracy=0.767200, validation/loss=1.009897, validation/num_examples=50000
I0127 11:49:21.926589 139656658134784 logging_writer.py:48] [170700] global_step=170700, grad_norm=7.7952423095703125, loss=1.8623762130737305
I0127 11:49:55.971598 139656649742080 logging_writer.py:48] [170800] global_step=170800, grad_norm=7.68181037902832, loss=1.8568123579025269
I0127 11:50:30.045920 139656658134784 logging_writer.py:48] [170900] global_step=170900, grad_norm=7.847300052642822, loss=1.9145896434783936
I0127 11:51:04.094962 139656649742080 logging_writer.py:48] [171000] global_step=171000, grad_norm=8.170610427856445, loss=1.8488527536392212
I0127 11:51:38.154320 139656658134784 logging_writer.py:48] [171100] global_step=171100, grad_norm=7.170144081115723, loss=1.8310049772262573
I0127 11:52:12.217261 139656649742080 logging_writer.py:48] [171200] global_step=171200, grad_norm=7.413736343383789, loss=1.8000152111053467
I0127 11:52:46.301207 139656658134784 logging_writer.py:48] [171300] global_step=171300, grad_norm=7.389390468597412, loss=1.846359372138977
I0127 11:53:20.367001 139656649742080 logging_writer.py:48] [171400] global_step=171400, grad_norm=8.088552474975586, loss=1.870707631111145
I0127 11:53:54.435759 139656658134784 logging_writer.py:48] [171500] global_step=171500, grad_norm=7.2592692375183105, loss=1.7663984298706055
I0127 11:54:28.542475 139656649742080 logging_writer.py:48] [171600] global_step=171600, grad_norm=7.302468299865723, loss=1.8382340669631958
I0127 11:55:02.589417 139656658134784 logging_writer.py:48] [171700] global_step=171700, grad_norm=7.9249420166015625, loss=1.9058502912521362
I0127 11:55:36.640688 139656649742080 logging_writer.py:48] [171800] global_step=171800, grad_norm=7.960822582244873, loss=1.7565122842788696
I0127 11:56:10.692538 139656658134784 logging_writer.py:48] [171900] global_step=171900, grad_norm=7.797467231750488, loss=1.7667635679244995
I0127 11:56:44.757812 139656649742080 logging_writer.py:48] [172000] global_step=172000, grad_norm=7.343762397766113, loss=1.8294484615325928
I0127 11:57:18.816548 139656658134784 logging_writer.py:48] [172100] global_step=172100, grad_norm=7.9104390144348145, loss=1.8983649015426636
I0127 11:57:32.583173 139822745589568 spec.py:321] Evaluating on the training split.
I0127 11:57:38.821017 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 11:57:47.483045 139822745589568 spec.py:349] Evaluating on the test split.
I0127 11:57:49.779227 139822745589568 submission_runner.py:408] Time since start: 60863.64s, 	Step: 172142, 	{'train/accuracy': 0.9163145422935486, 'train/loss': 0.4251190423965454, 'validation/accuracy': 0.7689799666404724, 'validation/loss': 1.0125985145568848, 'validation/num_examples': 50000, 'test/accuracy': 0.6457000374794006, 'test/loss': 1.6340036392211914, 'test/num_examples': 10000, 'score': 58711.40957093239, 'total_duration': 60863.64063549042, 'accumulated_submission_time': 58711.40957093239, 'accumulated_eval_time': 2140.4393548965454, 'accumulated_logging_time': 6.021910905838013}
I0127 11:57:49.825856 139656658134784 logging_writer.py:48] [172142] accumulated_eval_time=2140.439355, accumulated_logging_time=6.021911, accumulated_submission_time=58711.409571, global_step=172142, preemption_count=0, score=58711.409571, test/accuracy=0.645700, test/loss=1.634004, test/num_examples=10000, total_duration=60863.640635, train/accuracy=0.916315, train/loss=0.425119, validation/accuracy=0.768980, validation/loss=1.012599, validation/num_examples=50000
I0127 11:58:09.932923 139656674920192 logging_writer.py:48] [172200] global_step=172200, grad_norm=7.84019660949707, loss=1.8084964752197266
I0127 11:58:43.945263 139656658134784 logging_writer.py:48] [172300] global_step=172300, grad_norm=8.248984336853027, loss=1.8817201852798462
I0127 11:59:17.996768 139656674920192 logging_writer.py:48] [172400] global_step=172400, grad_norm=8.400514602661133, loss=1.8457974195480347
I0127 11:59:52.042855 139656658134784 logging_writer.py:48] [172500] global_step=172500, grad_norm=7.578484535217285, loss=1.817626714706421
I0127 12:00:26.166216 139656674920192 logging_writer.py:48] [172600] global_step=172600, grad_norm=7.266501426696777, loss=1.8129560947418213
I0127 12:01:00.228978 139656658134784 logging_writer.py:48] [172700] global_step=172700, grad_norm=7.980962753295898, loss=1.852461576461792
I0127 12:01:34.298370 139656674920192 logging_writer.py:48] [172800] global_step=172800, grad_norm=8.233977317810059, loss=1.851407766342163
I0127 12:02:08.364926 139656658134784 logging_writer.py:48] [172900] global_step=172900, grad_norm=7.314218521118164, loss=1.8022791147232056
I0127 12:02:42.439163 139656674920192 logging_writer.py:48] [173000] global_step=173000, grad_norm=7.5841779708862305, loss=1.7921357154846191
I0127 12:03:16.500330 139656658134784 logging_writer.py:48] [173100] global_step=173100, grad_norm=8.354087829589844, loss=1.7931621074676514
I0127 12:03:50.539338 139656674920192 logging_writer.py:48] [173200] global_step=173200, grad_norm=7.22705602645874, loss=1.820854902267456
I0127 12:04:24.578307 139656658134784 logging_writer.py:48] [173300] global_step=173300, grad_norm=8.324790954589844, loss=1.7990623712539673
I0127 12:04:58.626240 139656674920192 logging_writer.py:48] [173400] global_step=173400, grad_norm=6.981098175048828, loss=1.7339080572128296
I0127 12:05:32.712937 139656658134784 logging_writer.py:48] [173500] global_step=173500, grad_norm=7.5173468589782715, loss=1.7844780683517456
I0127 12:06:06.783475 139656674920192 logging_writer.py:48] [173600] global_step=173600, grad_norm=7.3516645431518555, loss=1.7835453748703003
I0127 12:06:19.879377 139822745589568 spec.py:321] Evaluating on the training split.
I0127 12:06:26.246270 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 12:06:34.794176 139822745589568 spec.py:349] Evaluating on the test split.
I0127 12:06:37.072102 139822745589568 submission_runner.py:408] Time since start: 61390.93s, 	Step: 173640, 	{'train/accuracy': 0.9162746667861938, 'train/loss': 0.41596558690071106, 'validation/accuracy': 0.7691599726676941, 'validation/loss': 1.0013891458511353, 'validation/num_examples': 50000, 'test/accuracy': 0.6492000222206116, 'test/loss': 1.6199817657470703, 'test/num_examples': 10000, 'score': 59221.40473794937, 'total_duration': 61390.933445453644, 'accumulated_submission_time': 59221.40473794937, 'accumulated_eval_time': 2157.631984949112, 'accumulated_logging_time': 6.077967643737793}
I0127 12:06:37.118092 139656658134784 logging_writer.py:48] [173640] accumulated_eval_time=2157.631985, accumulated_logging_time=6.077968, accumulated_submission_time=59221.404738, global_step=173640, preemption_count=0, score=59221.404738, test/accuracy=0.649200, test/loss=1.619982, test/num_examples=10000, total_duration=61390.933445, train/accuracy=0.916275, train/loss=0.415966, validation/accuracy=0.769160, validation/loss=1.001389, validation/num_examples=50000
I0127 12:06:57.919537 139656666527488 logging_writer.py:48] [173700] global_step=173700, grad_norm=8.342727661132812, loss=1.8604660034179688
I0127 12:07:31.947872 139656658134784 logging_writer.py:48] [173800] global_step=173800, grad_norm=7.335314750671387, loss=1.8003923892974854
I0127 12:08:06.033628 139656666527488 logging_writer.py:48] [173900] global_step=173900, grad_norm=7.75761604309082, loss=1.773582935333252
I0127 12:08:40.106652 139656658134784 logging_writer.py:48] [174000] global_step=174000, grad_norm=7.276310443878174, loss=1.7507904767990112
I0127 12:09:14.177125 139656666527488 logging_writer.py:48] [174100] global_step=174100, grad_norm=7.882648944854736, loss=1.8602259159088135
I0127 12:09:48.252918 139656658134784 logging_writer.py:48] [174200] global_step=174200, grad_norm=8.290373802185059, loss=1.9004998207092285
I0127 12:10:22.348962 139656666527488 logging_writer.py:48] [174300] global_step=174300, grad_norm=7.678131103515625, loss=1.8335082530975342
I0127 12:10:56.419192 139656658134784 logging_writer.py:48] [174400] global_step=174400, grad_norm=7.431962490081787, loss=1.7835826873779297
I0127 12:11:30.495529 139656666527488 logging_writer.py:48] [174500] global_step=174500, grad_norm=8.132320404052734, loss=1.856391429901123
I0127 12:12:04.570632 139656658134784 logging_writer.py:48] [174600] global_step=174600, grad_norm=8.048707962036133, loss=1.7755370140075684
I0127 12:12:38.644333 139656666527488 logging_writer.py:48] [174700] global_step=174700, grad_norm=8.543827056884766, loss=1.8309698104858398
I0127 12:13:12.771598 139656658134784 logging_writer.py:48] [174800] global_step=174800, grad_norm=8.02140998840332, loss=1.8092842102050781
I0127 12:13:46.842679 139656666527488 logging_writer.py:48] [174900] global_step=174900, grad_norm=8.235479354858398, loss=1.8198416233062744
I0127 12:14:20.904023 139656658134784 logging_writer.py:48] [175000] global_step=175000, grad_norm=7.7132439613342285, loss=1.8619638681411743
I0127 12:14:54.956427 139656666527488 logging_writer.py:48] [175100] global_step=175100, grad_norm=8.358220100402832, loss=1.872189998626709
I0127 12:15:07.363232 139822745589568 spec.py:321] Evaluating on the training split.
I0127 12:15:13.587004 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 12:15:21.984923 139822745589568 spec.py:349] Evaluating on the test split.
I0127 12:15:24.245064 139822745589568 submission_runner.py:408] Time since start: 61918.11s, 	Step: 175138, 	{'train/accuracy': 0.9125677347183228, 'train/loss': 0.42805802822113037, 'validation/accuracy': 0.7706199884414673, 'validation/loss': 1.000607967376709, 'validation/num_examples': 50000, 'test/accuracy': 0.6484000086784363, 'test/loss': 1.6202576160430908, 'test/num_examples': 10000, 'score': 59731.5888364315, 'total_duration': 61918.106459379196, 'accumulated_submission_time': 59731.5888364315, 'accumulated_eval_time': 2174.5137605667114, 'accumulated_logging_time': 6.134857654571533}
I0127 12:15:24.290667 139656683312896 logging_writer.py:48] [175138] accumulated_eval_time=2174.513761, accumulated_logging_time=6.134858, accumulated_submission_time=59731.588836, global_step=175138, preemption_count=0, score=59731.588836, test/accuracy=0.648400, test/loss=1.620258, test/num_examples=10000, total_duration=61918.106459, train/accuracy=0.912568, train/loss=0.428058, validation/accuracy=0.770620, validation/loss=1.000608, validation/num_examples=50000
I0127 12:15:45.729887 139658730145536 logging_writer.py:48] [175200] global_step=175200, grad_norm=7.34602689743042, loss=1.8782222270965576
I0127 12:16:19.742885 139656683312896 logging_writer.py:48] [175300] global_step=175300, grad_norm=8.107487678527832, loss=1.7760488986968994
I0127 12:16:53.791778 139658730145536 logging_writer.py:48] [175400] global_step=175400, grad_norm=7.798691749572754, loss=1.8074826002120972
I0127 12:17:27.862439 139656683312896 logging_writer.py:48] [175500] global_step=175500, grad_norm=7.3235182762146, loss=1.81569242477417
I0127 12:18:01.938319 139658730145536 logging_writer.py:48] [175600] global_step=175600, grad_norm=8.29931354522705, loss=1.784331202507019
I0127 12:18:35.996938 139656683312896 logging_writer.py:48] [175700] global_step=175700, grad_norm=7.423405170440674, loss=1.7736868858337402
I0127 12:19:10.164328 139658730145536 logging_writer.py:48] [175800] global_step=175800, grad_norm=7.667361736297607, loss=1.8319261074066162
I0127 12:19:44.245826 139656683312896 logging_writer.py:48] [175900] global_step=175900, grad_norm=7.574192047119141, loss=1.8020204305648804
I0127 12:20:18.306571 139658730145536 logging_writer.py:48] [176000] global_step=176000, grad_norm=7.582773685455322, loss=1.9056514501571655
I0127 12:20:52.394662 139656683312896 logging_writer.py:48] [176100] global_step=176100, grad_norm=7.461560249328613, loss=1.7665290832519531
I0127 12:21:26.460711 139658730145536 logging_writer.py:48] [176200] global_step=176200, grad_norm=7.679832935333252, loss=1.7318187952041626
I0127 12:22:00.538732 139656683312896 logging_writer.py:48] [176300] global_step=176300, grad_norm=8.446913719177246, loss=1.8590962886810303
I0127 12:22:34.587777 139658730145536 logging_writer.py:48] [176400] global_step=176400, grad_norm=8.534965515136719, loss=1.8054938316345215
I0127 12:23:08.672087 139656683312896 logging_writer.py:48] [176500] global_step=176500, grad_norm=7.899834156036377, loss=1.782257080078125
I0127 12:23:42.743412 139658730145536 logging_writer.py:48] [176600] global_step=176600, grad_norm=7.07899284362793, loss=1.7840557098388672
I0127 12:23:54.471207 139822745589568 spec.py:321] Evaluating on the training split.
I0127 12:24:00.742836 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 12:24:09.458132 139822745589568 spec.py:349] Evaluating on the test split.
I0127 12:24:11.742857 139822745589568 submission_runner.py:408] Time since start: 62445.60s, 	Step: 176636, 	{'train/accuracy': 0.9162148833274841, 'train/loss': 0.4126388728618622, 'validation/accuracy': 0.7721799612045288, 'validation/loss': 0.9947059154510498, 'validation/num_examples': 50000, 'test/accuracy': 0.6476000547409058, 'test/loss': 1.617110252380371, 'test/num_examples': 10000, 'score': 60241.7090446949, 'total_duration': 62445.60426354408, 'accumulated_submission_time': 60241.7090446949, 'accumulated_eval_time': 2191.7853965759277, 'accumulated_logging_time': 6.190088748931885}
I0127 12:24:11.789951 139656649742080 logging_writer.py:48] [176636] accumulated_eval_time=2191.785397, accumulated_logging_time=6.190089, accumulated_submission_time=60241.709045, global_step=176636, preemption_count=0, score=60241.709045, test/accuracy=0.647600, test/loss=1.617110, test/num_examples=10000, total_duration=62445.604264, train/accuracy=0.916215, train/loss=0.412639, validation/accuracy=0.772180, validation/loss=0.994706, validation/num_examples=50000
I0127 12:24:33.896370 139656658134784 logging_writer.py:48] [176700] global_step=176700, grad_norm=8.60556697845459, loss=1.7212109565734863
I0127 12:25:08.177247 139656649742080 logging_writer.py:48] [176800] global_step=176800, grad_norm=8.374929428100586, loss=1.841444492340088
I0127 12:25:42.267744 139656658134784 logging_writer.py:48] [176900] global_step=176900, grad_norm=8.045920372009277, loss=1.733245611190796
I0127 12:26:16.312241 139656649742080 logging_writer.py:48] [177000] global_step=177000, grad_norm=7.390342712402344, loss=1.7627604007720947
I0127 12:26:50.378497 139656658134784 logging_writer.py:48] [177100] global_step=177100, grad_norm=7.360203266143799, loss=1.7390060424804688
I0127 12:27:24.442712 139656649742080 logging_writer.py:48] [177200] global_step=177200, grad_norm=9.401291847229004, loss=1.8594993352890015
I0127 12:27:58.509814 139656658134784 logging_writer.py:48] [177300] global_step=177300, grad_norm=8.352800369262695, loss=1.8116397857666016
I0127 12:28:32.592990 139656649742080 logging_writer.py:48] [177400] global_step=177400, grad_norm=7.100555896759033, loss=1.7078509330749512
I0127 12:29:06.653221 139656658134784 logging_writer.py:48] [177500] global_step=177500, grad_norm=7.823148727416992, loss=1.7927114963531494
I0127 12:29:40.739839 139656649742080 logging_writer.py:48] [177600] global_step=177600, grad_norm=7.992130756378174, loss=1.8321877717971802
I0127 12:30:14.795854 139656658134784 logging_writer.py:48] [177700] global_step=177700, grad_norm=7.648635387420654, loss=1.7751744985580444
I0127 12:30:48.860784 139656649742080 logging_writer.py:48] [177800] global_step=177800, grad_norm=8.146597862243652, loss=1.7738184928894043
I0127 12:31:22.966277 139656658134784 logging_writer.py:48] [177900] global_step=177900, grad_norm=7.438827037811279, loss=1.7130826711654663
I0127 12:31:56.996495 139656649742080 logging_writer.py:48] [178000] global_step=178000, grad_norm=7.556307315826416, loss=1.7624579668045044
I0127 12:32:31.051946 139656658134784 logging_writer.py:48] [178100] global_step=178100, grad_norm=7.437049388885498, loss=1.7071468830108643
I0127 12:32:41.743850 139822745589568 spec.py:321] Evaluating on the training split.
I0127 12:32:47.944324 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 12:32:56.363186 139822745589568 spec.py:349] Evaluating on the test split.
I0127 12:32:58.646395 139822745589568 submission_runner.py:408] Time since start: 62972.51s, 	Step: 178133, 	{'train/accuracy': 0.9172711968421936, 'train/loss': 0.4106919467449188, 'validation/accuracy': 0.7718799710273743, 'validation/loss': 0.9955499172210693, 'validation/num_examples': 50000, 'test/accuracy': 0.6497000455856323, 'test/loss': 1.6151281595230103, 'test/num_examples': 10000, 'score': 60751.60144472122, 'total_duration': 62972.50779604912, 'accumulated_submission_time': 60751.60144472122, 'accumulated_eval_time': 2208.687886953354, 'accumulated_logging_time': 6.247087478637695}
I0127 12:32:58.698125 139656700098304 logging_writer.py:48] [178133] accumulated_eval_time=2208.687887, accumulated_logging_time=6.247087, accumulated_submission_time=60751.601445, global_step=178133, preemption_count=0, score=60751.601445, test/accuracy=0.649700, test/loss=1.615128, test/num_examples=10000, total_duration=62972.507796, train/accuracy=0.917271, train/loss=0.410692, validation/accuracy=0.771880, validation/loss=0.995550, validation/num_examples=50000
I0127 12:33:21.854778 139658730145536 logging_writer.py:48] [178200] global_step=178200, grad_norm=7.056251525878906, loss=1.7146422863006592
I0127 12:33:55.871685 139656700098304 logging_writer.py:48] [178300] global_step=178300, grad_norm=7.907595634460449, loss=1.8208321332931519
I0127 12:34:29.917671 139658730145536 logging_writer.py:48] [178400] global_step=178400, grad_norm=8.335959434509277, loss=1.8027973175048828
I0127 12:35:03.988152 139656700098304 logging_writer.py:48] [178500] global_step=178500, grad_norm=7.689291477203369, loss=1.7869172096252441
I0127 12:35:38.064884 139658730145536 logging_writer.py:48] [178600] global_step=178600, grad_norm=8.310741424560547, loss=1.8203768730163574
I0127 12:36:12.119109 139656700098304 logging_writer.py:48] [178700] global_step=178700, grad_norm=8.611040115356445, loss=1.769747018814087
I0127 12:36:46.183772 139658730145536 logging_writer.py:48] [178800] global_step=178800, grad_norm=8.016061782836914, loss=1.7818351984024048
I0127 12:37:20.243998 139656700098304 logging_writer.py:48] [178900] global_step=178900, grad_norm=7.306567192077637, loss=1.7771697044372559
I0127 12:37:54.460220 139658730145536 logging_writer.py:48] [179000] global_step=179000, grad_norm=7.950360298156738, loss=1.7687876224517822
I0127 12:38:28.548346 139656700098304 logging_writer.py:48] [179100] global_step=179100, grad_norm=8.0205078125, loss=1.7087671756744385
I0127 12:39:02.604331 139658730145536 logging_writer.py:48] [179200] global_step=179200, grad_norm=8.542672157287598, loss=1.8209781646728516
I0127 12:39:36.669814 139656700098304 logging_writer.py:48] [179300] global_step=179300, grad_norm=7.469345569610596, loss=1.7460048198699951
I0127 12:40:10.755510 139658730145536 logging_writer.py:48] [179400] global_step=179400, grad_norm=8.27099895477295, loss=1.7813172340393066
I0127 12:40:44.836829 139656700098304 logging_writer.py:48] [179500] global_step=179500, grad_norm=7.736551761627197, loss=1.8220202922821045
I0127 12:41:18.907052 139658730145536 logging_writer.py:48] [179600] global_step=179600, grad_norm=8.174453735351562, loss=1.8089098930358887
I0127 12:41:28.927021 139822745589568 spec.py:321] Evaluating on the training split.
I0127 12:41:35.241086 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 12:41:43.870414 139822745589568 spec.py:349] Evaluating on the test split.
I0127 12:41:46.202996 139822745589568 submission_runner.py:408] Time since start: 63500.06s, 	Step: 179631, 	{'train/accuracy': 0.918965220451355, 'train/loss': 0.40768128633499146, 'validation/accuracy': 0.7737799882888794, 'validation/loss': 0.989608645439148, 'validation/num_examples': 50000, 'test/accuracy': 0.65010005235672, 'test/loss': 1.6078531742095947, 'test/num_examples': 10000, 'score': 61261.76955938339, 'total_duration': 63500.064401865005, 'accumulated_submission_time': 61261.76955938339, 'accumulated_eval_time': 2225.9638142585754, 'accumulated_logging_time': 6.308993816375732}
I0127 12:41:46.252321 139656649742080 logging_writer.py:48] [179631] accumulated_eval_time=2225.963814, accumulated_logging_time=6.308994, accumulated_submission_time=61261.769559, global_step=179631, preemption_count=0, score=61261.769559, test/accuracy=0.650100, test/loss=1.607853, test/num_examples=10000, total_duration=63500.064402, train/accuracy=0.918965, train/loss=0.407681, validation/accuracy=0.773780, validation/loss=0.989609, validation/num_examples=50000
I0127 12:42:10.096880 139656666527488 logging_writer.py:48] [179700] global_step=179700, grad_norm=7.021506309509277, loss=1.7449523210525513
I0127 12:42:44.115928 139656649742080 logging_writer.py:48] [179800] global_step=179800, grad_norm=8.187490463256836, loss=1.7888586521148682
I0127 12:43:18.185222 139656666527488 logging_writer.py:48] [179900] global_step=179900, grad_norm=8.485644340515137, loss=1.7819877862930298
I0127 12:43:52.399724 139656649742080 logging_writer.py:48] [180000] global_step=180000, grad_norm=8.568745613098145, loss=1.837404727935791
I0127 12:44:26.448407 139656666527488 logging_writer.py:48] [180100] global_step=180100, grad_norm=7.835805416107178, loss=1.8353599309921265
I0127 12:45:00.510956 139656649742080 logging_writer.py:48] [180200] global_step=180200, grad_norm=8.186034202575684, loss=1.760388731956482
I0127 12:45:34.588599 139656666527488 logging_writer.py:48] [180300] global_step=180300, grad_norm=8.12079906463623, loss=1.777895212173462
I0127 12:46:08.643651 139656649742080 logging_writer.py:48] [180400] global_step=180400, grad_norm=8.373198509216309, loss=1.7770404815673828
I0127 12:46:42.693984 139656666527488 logging_writer.py:48] [180500] global_step=180500, grad_norm=8.179557800292969, loss=1.7910399436950684
I0127 12:47:16.766043 139656649742080 logging_writer.py:48] [180600] global_step=180600, grad_norm=8.703112602233887, loss=1.7523698806762695
I0127 12:47:50.847627 139656666527488 logging_writer.py:48] [180700] global_step=180700, grad_norm=7.841158866882324, loss=1.7623779773712158
I0127 12:48:24.919892 139656649742080 logging_writer.py:48] [180800] global_step=180800, grad_norm=7.617069244384766, loss=1.7535539865493774
I0127 12:48:59.003176 139656666527488 logging_writer.py:48] [180900] global_step=180900, grad_norm=7.367071151733398, loss=1.7263303995132446
I0127 12:49:33.073656 139656649742080 logging_writer.py:48] [181000] global_step=181000, grad_norm=8.128121376037598, loss=1.7975717782974243
I0127 12:50:07.246217 139656666527488 logging_writer.py:48] [181100] global_step=181100, grad_norm=7.779513359069824, loss=1.7695715427398682
I0127 12:50:16.260260 139822745589568 spec.py:321] Evaluating on the training split.
I0127 12:50:22.521177 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 12:50:31.157876 139822745589568 spec.py:349] Evaluating on the test split.
I0127 12:50:33.523183 139822745589568 submission_runner.py:408] Time since start: 64027.38s, 	Step: 181128, 	{'train/accuracy': 0.9202008843421936, 'train/loss': 0.40204086899757385, 'validation/accuracy': 0.7730000019073486, 'validation/loss': 0.9929125308990479, 'validation/num_examples': 50000, 'test/accuracy': 0.6499000191688538, 'test/loss': 1.609398603439331, 'test/num_examples': 10000, 'score': 61771.71801686287, 'total_duration': 64027.38457632065, 'accumulated_submission_time': 61771.71801686287, 'accumulated_eval_time': 2243.2266731262207, 'accumulated_logging_time': 6.367881774902344}
I0127 12:50:33.576701 139656649742080 logging_writer.py:48] [181128] accumulated_eval_time=2243.226673, accumulated_logging_time=6.367882, accumulated_submission_time=61771.718017, global_step=181128, preemption_count=0, score=61771.718017, test/accuracy=0.649900, test/loss=1.609399, test/num_examples=10000, total_duration=64027.384576, train/accuracy=0.920201, train/loss=0.402041, validation/accuracy=0.773000, validation/loss=0.992913, validation/num_examples=50000
I0127 12:50:58.448273 139656691705600 logging_writer.py:48] [181200] global_step=181200, grad_norm=7.491727352142334, loss=1.7443312406539917
I0127 12:51:32.470909 139656649742080 logging_writer.py:48] [181300] global_step=181300, grad_norm=7.770501613616943, loss=1.7746129035949707
I0127 12:52:06.531448 139656691705600 logging_writer.py:48] [181400] global_step=181400, grad_norm=7.4551239013671875, loss=1.8057142496109009
I0127 12:52:40.595562 139656649742080 logging_writer.py:48] [181500] global_step=181500, grad_norm=8.346161842346191, loss=1.7965654134750366
I0127 12:53:14.668394 139656691705600 logging_writer.py:48] [181600] global_step=181600, grad_norm=7.8426833152771, loss=1.7508398294448853
I0127 12:53:48.757283 139656649742080 logging_writer.py:48] [181700] global_step=181700, grad_norm=7.312554836273193, loss=1.6872804164886475
I0127 12:54:22.828972 139656691705600 logging_writer.py:48] [181800] global_step=181800, grad_norm=7.640635013580322, loss=1.79413902759552
I0127 12:54:56.865073 139656649742080 logging_writer.py:48] [181900] global_step=181900, grad_norm=7.897892475128174, loss=1.7835359573364258
I0127 12:55:30.910080 139656691705600 logging_writer.py:48] [182000] global_step=182000, grad_norm=8.227750778198242, loss=1.8133224248886108
I0127 12:56:05.149425 139656649742080 logging_writer.py:48] [182100] global_step=182100, grad_norm=8.06665325164795, loss=1.7884588241577148
I0127 12:56:39.229075 139656691705600 logging_writer.py:48] [182200] global_step=182200, grad_norm=6.9639201164245605, loss=1.729684591293335
I0127 12:57:13.315799 139656649742080 logging_writer.py:48] [182300] global_step=182300, grad_norm=8.980992317199707, loss=1.8347615003585815
I0127 12:57:47.383642 139656691705600 logging_writer.py:48] [182400] global_step=182400, grad_norm=7.743030071258545, loss=1.7797417640686035
I0127 12:58:21.457715 139656649742080 logging_writer.py:48] [182500] global_step=182500, grad_norm=7.567518711090088, loss=1.7544059753417969
I0127 12:58:55.540454 139656691705600 logging_writer.py:48] [182600] global_step=182600, grad_norm=8.04770278930664, loss=1.7600791454315186
I0127 12:59:03.850670 139822745589568 spec.py:321] Evaluating on the training split.
I0127 12:59:10.042772 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 12:59:18.767954 139822745589568 spec.py:349] Evaluating on the test split.
I0127 12:59:21.042950 139822745589568 submission_runner.py:408] Time since start: 64554.90s, 	Step: 182626, 	{'train/accuracy': 0.9197225570678711, 'train/loss': 0.40420398116111755, 'validation/accuracy': 0.7734400033950806, 'validation/loss': 0.9904934167861938, 'validation/num_examples': 50000, 'test/accuracy': 0.6509000062942505, 'test/loss': 1.606026530265808, 'test/num_examples': 10000, 'score': 62281.93049740791, 'total_duration': 64554.90435361862, 'accumulated_submission_time': 62281.93049740791, 'accumulated_eval_time': 2260.4189026355743, 'accumulated_logging_time': 6.43097186088562}
I0127 12:59:21.094897 139656674920192 logging_writer.py:48] [182626] accumulated_eval_time=2260.418903, accumulated_logging_time=6.430972, accumulated_submission_time=62281.930497, global_step=182626, preemption_count=0, score=62281.930497, test/accuracy=0.650900, test/loss=1.606027, test/num_examples=10000, total_duration=64554.904354, train/accuracy=0.919723, train/loss=0.404204, validation/accuracy=0.773440, validation/loss=0.990493, validation/num_examples=50000
I0127 12:59:46.650929 139656683312896 logging_writer.py:48] [182700] global_step=182700, grad_norm=8.254622459411621, loss=1.793489694595337
I0127 13:00:20.679900 139656674920192 logging_writer.py:48] [182800] global_step=182800, grad_norm=7.634106159210205, loss=1.773728609085083
I0127 13:00:54.734869 139656683312896 logging_writer.py:48] [182900] global_step=182900, grad_norm=7.399010181427002, loss=1.724836826324463
I0127 13:01:28.817703 139656674920192 logging_writer.py:48] [183000] global_step=183000, grad_norm=8.030330657958984, loss=1.769928216934204
I0127 13:02:02.877951 139656683312896 logging_writer.py:48] [183100] global_step=183100, grad_norm=7.760592460632324, loss=1.7349671125411987
I0127 13:02:37.046548 139656674920192 logging_writer.py:48] [183200] global_step=183200, grad_norm=7.79033899307251, loss=1.7562813758850098
I0127 13:03:11.103426 139656683312896 logging_writer.py:48] [183300] global_step=183300, grad_norm=8.04244613647461, loss=1.8160220384597778
I0127 13:03:45.195253 139656674920192 logging_writer.py:48] [183400] global_step=183400, grad_norm=8.670110702514648, loss=1.6959471702575684
I0127 13:04:19.257441 139656683312896 logging_writer.py:48] [183500] global_step=183500, grad_norm=7.652594566345215, loss=1.7252888679504395
I0127 13:04:53.318549 139656674920192 logging_writer.py:48] [183600] global_step=183600, grad_norm=7.482848644256592, loss=1.7524536848068237
I0127 13:05:27.384340 139656683312896 logging_writer.py:48] [183700] global_step=183700, grad_norm=7.816490650177002, loss=1.7614402770996094
I0127 13:06:01.424138 139656674920192 logging_writer.py:48] [183800] global_step=183800, grad_norm=7.936823844909668, loss=1.8566813468933105
I0127 13:06:35.495555 139656683312896 logging_writer.py:48] [183900] global_step=183900, grad_norm=7.508184909820557, loss=1.8018357753753662
I0127 13:07:09.535916 139656674920192 logging_writer.py:48] [184000] global_step=184000, grad_norm=7.418323040008545, loss=1.743202805519104
I0127 13:07:43.579749 139656683312896 logging_writer.py:48] [184100] global_step=184100, grad_norm=8.434906959533691, loss=1.7560902833938599
I0127 13:07:51.210833 139822745589568 spec.py:321] Evaluating on the training split.
I0127 13:07:57.618274 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 13:08:06.205026 139822745589568 spec.py:349] Evaluating on the test split.
I0127 13:08:08.551028 139822745589568 submission_runner.py:408] Time since start: 65082.41s, 	Step: 184124, 	{'train/accuracy': 0.9210976958274841, 'train/loss': 0.3999505639076233, 'validation/accuracy': 0.7736999988555908, 'validation/loss': 0.9918732643127441, 'validation/num_examples': 50000, 'test/accuracy': 0.6513000130653381, 'test/loss': 1.6081324815750122, 'test/num_examples': 10000, 'score': 62791.98581242561, 'total_duration': 65082.41243624687, 'accumulated_submission_time': 62791.98581242561, 'accumulated_eval_time': 2277.7590713500977, 'accumulated_logging_time': 6.492360353469849}
I0127 13:08:08.602122 139656297445120 logging_writer.py:48] [184124] accumulated_eval_time=2277.759071, accumulated_logging_time=6.492360, accumulated_submission_time=62791.985812, global_step=184124, preemption_count=0, score=62791.985812, test/accuracy=0.651300, test/loss=1.608132, test/num_examples=10000, total_duration=65082.412436, train/accuracy=0.921098, train/loss=0.399951, validation/accuracy=0.773700, validation/loss=0.991873, validation/num_examples=50000
I0127 13:08:34.828162 139656666527488 logging_writer.py:48] [184200] global_step=184200, grad_norm=8.133047103881836, loss=1.791072964668274
I0127 13:09:08.842964 139656297445120 logging_writer.py:48] [184300] global_step=184300, grad_norm=7.210139751434326, loss=1.7144209146499634
I0127 13:09:42.873177 139656666527488 logging_writer.py:48] [184400] global_step=184400, grad_norm=7.718225479125977, loss=1.7867999076843262
I0127 13:10:16.933613 139656297445120 logging_writer.py:48] [184500] global_step=184500, grad_norm=8.152205467224121, loss=1.791560411453247
I0127 13:10:50.997050 139656666527488 logging_writer.py:48] [184600] global_step=184600, grad_norm=7.3695831298828125, loss=1.778641700744629
I0127 13:11:25.047686 139656297445120 logging_writer.py:48] [184700] global_step=184700, grad_norm=8.087138175964355, loss=1.7929248809814453
I0127 13:11:44.957028 139656666527488 logging_writer.py:48] [184760] global_step=184760, preemption_count=0, score=63008.280600
I0127 13:11:45.412885 139822745589568 checkpoints.py:490] Saving checkpoint at step: 184760
I0127 13:11:46.506479 139822745589568 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_1/imagenet_resnet_jax/trial_1/checkpoint_184760
I0127 13:11:46.536885 139822745589568 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_1/imagenet_resnet_jax/trial_1/checkpoint_184760.
I0127 13:11:47.343107 139822745589568 submission_runner.py:583] Tuning trial 1/5
I0127 13:11:47.343317 139822745589568 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.1, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0127 13:11:47.347256 139822745589568 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.000996492337435484, 'train/loss': 6.9111857414245605, 'validation/accuracy': 0.001019999966956675, 'validation/loss': 6.91091251373291, 'validation/num_examples': 50000, 'test/accuracy': 0.0012000000569969416, 'test/loss': 6.910790920257568, 'test/num_examples': 10000, 'score': 56.31122303009033, 'total_duration': 93.1584370136261, 'accumulated_submission_time': 56.31122303009033, 'accumulated_eval_time': 36.84709548950195, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1491, {'train/accuracy': 0.0701729878783226, 'train/loss': 5.387164115905762, 'validation/accuracy': 0.065420001745224, 'validation/loss': 5.45823860168457, 'validation/num_examples': 50000, 'test/accuracy': 0.04310000315308571, 'test/loss': 5.679708957672119, 'test/num_examples': 10000, 'score': 566.3269417285919, 'total_duration': 621.0424935817719, 'accumulated_submission_time': 566.3269417285919, 'accumulated_eval_time': 54.637348890304565, 'accumulated_logging_time': 0.02869892120361328, 'global_step': 1491, 'preemption_count': 0}), (2979, {'train/accuracy': 0.17562180757522583, 'train/loss': 4.2726922035217285, 'validation/accuracy': 0.15737999975681305, 'validation/loss': 4.3756303787231445, 'validation/num_examples': 50000, 'test/accuracy': 0.11840000748634338, 'test/loss': 4.798498630523682, 'test/num_examples': 10000, 'score': 1076.2741117477417, 'total_duration': 1149.0670676231384, 'accumulated_submission_time': 1076.2741117477417, 'accumulated_eval_time': 72.63711643218994, 'accumulated_logging_time': 0.05626201629638672, 'global_step': 2979, 'preemption_count': 0}), (4468, {'train/accuracy': 0.2839604616165161, 'train/loss': 3.509944200515747, 'validation/accuracy': 0.2582399845123291, 'validation/loss': 3.6567907333374023, 'validation/num_examples': 50000, 'test/accuracy': 0.19550001621246338, 'test/loss': 4.1657586097717285, 'test/num_examples': 10000, 'score': 1586.424509525299, 'total_duration': 1677.3552947044373, 'accumulated_submission_time': 1586.424509525299, 'accumulated_eval_time': 90.69883179664612, 'accumulated_logging_time': 0.08253097534179688, 'global_step': 4468, 'preemption_count': 0}), (5958, {'train/accuracy': 0.37035635113716125, 'train/loss': 2.9588210582733154, 'validation/accuracy': 0.34237998723983765, 'validation/loss': 3.106684684753418, 'validation/num_examples': 50000, 'test/accuracy': 0.26080000400543213, 'test/loss': 3.706625461578369, 'test/num_examples': 10000, 'score': 2096.635529756546, 'total_duration': 2205.71702504158, 'accumulated_submission_time': 2096.635529756546, 'accumulated_eval_time': 108.77395868301392, 'accumulated_logging_time': 0.10939264297485352, 'global_step': 5958, 'preemption_count': 0}), (7447, {'train/accuracy': 0.437519907951355, 'train/loss': 2.5694799423217773, 'validation/accuracy': 0.407260000705719, 'validation/loss': 2.7285168170928955, 'validation/num_examples': 50000, 'test/accuracy': 0.314300000667572, 'test/loss': 3.3685951232910156, 'test/num_examples': 10000, 'score': 2606.764097929001, 'total_duration': 2733.9635617733, 'accumulated_submission_time': 2606.764097929001, 'accumulated_eval_time': 126.81416702270508, 'accumulated_logging_time': 0.13728642463684082, 'global_step': 7447, 'preemption_count': 0}), (8937, {'train/accuracy': 0.49557557702064514, 'train/loss': 2.2581822872161865, 'validation/accuracy': 0.46515998244285583, 'validation/loss': 2.4037249088287354, 'validation/num_examples': 50000, 'test/accuracy': 0.3574000298976898, 'test/loss': 3.0720314979553223, 'test/num_examples': 10000, 'score': 3116.7672028541565, 'total_duration': 3262.1387367248535, 'accumulated_submission_time': 3116.7672028541565, 'accumulated_eval_time': 144.91092801094055, 'accumulated_logging_time': 0.16506457328796387, 'global_step': 8937, 'preemption_count': 0}), (10428, {'train/accuracy': 0.5627790093421936, 'train/loss': 1.9515559673309326, 'validation/accuracy': 0.4867599904537201, 'validation/loss': 2.320361614227295, 'validation/num_examples': 50000, 'test/accuracy': 0.37290000915527344, 'test/loss': 2.9771018028259277, 'test/num_examples': 10000, 'score': 3626.985067844391, 'total_duration': 3792.1785800457, 'accumulated_submission_time': 3626.985067844391, 'accumulated_eval_time': 164.654226064682, 'accumulated_logging_time': 0.1945652961730957, 'global_step': 10428, 'preemption_count': 0}), (11920, {'train/accuracy': 0.5782246589660645, 'train/loss': 1.846057415008545, 'validation/accuracy': 0.52947998046875, 'validation/loss': 2.106097936630249, 'validation/num_examples': 50000, 'test/accuracy': 0.4123000204563141, 'test/loss': 2.767740249633789, 'test/num_examples': 10000, 'score': 4137.04333615303, 'total_duration': 4325.421687364578, 'accumulated_submission_time': 4137.04333615303, 'accumulated_eval_time': 187.7570457458496, 'accumulated_logging_time': 0.22736477851867676, 'global_step': 11920, 'preemption_count': 0}), (13413, {'train/accuracy': 0.5894252061843872, 'train/loss': 1.8045969009399414, 'validation/accuracy': 0.5416799783706665, 'validation/loss': 2.0362939834594727, 'validation/num_examples': 50000, 'test/accuracy': 0.4132000207901001, 'test/loss': 2.7203760147094727, 'test/num_examples': 10000, 'score': 4647.025430679321, 'total_duration': 4857.068476676941, 'accumulated_submission_time': 4647.025430679321, 'accumulated_eval_time': 209.34475588798523, 'accumulated_logging_time': 0.25449466705322266, 'global_step': 13413, 'preemption_count': 0}), (14907, {'train/accuracy': 0.5977359414100647, 'train/loss': 1.8043289184570312, 'validation/accuracy': 0.5502399802207947, 'validation/loss': 2.0121490955352783, 'validation/num_examples': 50000, 'test/accuracy': 0.428600013256073, 'test/loss': 2.705935001373291, 'test/num_examples': 10000, 'score': 5157.151046037674, 'total_duration': 5389.574882030487, 'accumulated_submission_time': 5157.151046037674, 'accumulated_eval_time': 231.6502606868744, 'accumulated_logging_time': 0.28139376640319824, 'global_step': 14907, 'preemption_count': 0}), (16400, {'train/accuracy': 0.5902822017669678, 'train/loss': 1.8064643144607544, 'validation/accuracy': 0.5517199635505676, 'validation/loss': 1.9904471635818481, 'validation/num_examples': 50000, 'test/accuracy': 0.43080002069473267, 'test/loss': 2.7112877368927, 'test/num_examples': 10000, 'score': 5667.149292945862, 'total_duration': 5920.640057086945, 'accumulated_submission_time': 5667.149292945862, 'accumulated_eval_time': 252.6213595867157, 'accumulated_logging_time': 0.3276073932647705, 'global_step': 16400, 'preemption_count': 0}), (17895, {'train/accuracy': 0.5936902165412903, 'train/loss': 1.7567952871322632, 'validation/accuracy': 0.5552600026130676, 'validation/loss': 1.939975380897522, 'validation/num_examples': 50000, 'test/accuracy': 0.4360000193119049, 'test/loss': 2.634429931640625, 'test/num_examples': 10000, 'score': 6177.358016967773, 'total_duration': 6454.508526086807, 'accumulated_submission_time': 6177.358016967773, 'accumulated_eval_time': 276.202490568161, 'accumulated_logging_time': 0.35756540298461914, 'global_step': 17895, 'preemption_count': 0}), (19389, {'train/accuracy': 0.6022600531578064, 'train/loss': 1.7681397199630737, 'validation/accuracy': 0.5593400001525879, 'validation/loss': 1.9684282541275024, 'validation/num_examples': 50000, 'test/accuracy': 0.4310000240802765, 'test/loss': 2.670055866241455, 'test/num_examples': 10000, 'score': 6687.38344168663, 'total_duration': 6987.305696725845, 'accumulated_submission_time': 6687.38344168663, 'accumulated_eval_time': 298.885041475296, 'accumulated_logging_time': 0.39752960205078125, 'global_step': 19389, 'preemption_count': 0}), (20884, {'train/accuracy': 0.6381337642669678, 'train/loss': 1.5879325866699219, 'validation/accuracy': 0.5699599981307983, 'validation/loss': 1.8987358808517456, 'validation/num_examples': 50000, 'test/accuracy': 0.4415000081062317, 'test/loss': 2.5937812328338623, 'test/num_examples': 10000, 'score': 7197.566004276276, 'total_duration': 7521.352801322937, 'accumulated_submission_time': 7197.566004276276, 'accumulated_eval_time': 322.6722848415375, 'accumulated_logging_time': 0.42600297927856445, 'global_step': 20884, 'preemption_count': 0}), (22379, {'train/accuracy': 0.6238042116165161, 'train/loss': 1.6296091079711914, 'validation/accuracy': 0.5750600099563599, 'validation/loss': 1.8640074729919434, 'validation/num_examples': 50000, 'test/accuracy': 0.4531000256538391, 'test/loss': 2.565328359603882, 'test/num_examples': 10000, 'score': 7707.807578086853, 'total_duration': 8054.712727546692, 'accumulated_submission_time': 7707.807578086853, 'accumulated_eval_time': 345.71156549453735, 'accumulated_logging_time': 0.45580077171325684, 'global_step': 22379, 'preemption_count': 0}), (23874, {'train/accuracy': 0.6375358700752258, 'train/loss': 1.5796483755111694, 'validation/accuracy': 0.586899995803833, 'validation/loss': 1.8150631189346313, 'validation/num_examples': 50000, 'test/accuracy': 0.4646000266075134, 'test/loss': 2.491010904312134, 'test/num_examples': 10000, 'score': 8217.97511434555, 'total_duration': 8587.158815145493, 'accumulated_submission_time': 8217.97511434555, 'accumulated_eval_time': 367.9157955646515, 'accumulated_logging_time': 0.4832274913787842, 'global_step': 23874, 'preemption_count': 0}), (25369, {'train/accuracy': 0.6294642686843872, 'train/loss': 1.597659945487976, 'validation/accuracy': 0.584119975566864, 'validation/loss': 1.8133976459503174, 'validation/num_examples': 50000, 'test/accuracy': 0.4585000276565552, 'test/loss': 2.4793083667755127, 'test/num_examples': 10000, 'score': 8727.92223238945, 'total_duration': 9119.464904785156, 'accumulated_submission_time': 8727.92223238945, 'accumulated_eval_time': 390.196186542511, 'accumulated_logging_time': 0.512209415435791, 'global_step': 25369, 'preemption_count': 0}), (26864, {'train/accuracy': 0.6300422549247742, 'train/loss': 1.63661527633667, 'validation/accuracy': 0.5836600065231323, 'validation/loss': 1.8408207893371582, 'validation/num_examples': 50000, 'test/accuracy': 0.46410003304481506, 'test/loss': 2.510469913482666, 'test/num_examples': 10000, 'score': 9237.966356039047, 'total_duration': 9651.175999879837, 'accumulated_submission_time': 9237.966356039047, 'accumulated_eval_time': 411.78082633018494, 'accumulated_logging_time': 0.5465049743652344, 'global_step': 26864, 'preemption_count': 0}), (28358, {'train/accuracy': 0.6334103941917419, 'train/loss': 1.5800617933273315, 'validation/accuracy': 0.5934999585151672, 'validation/loss': 1.7827062606811523, 'validation/num_examples': 50000, 'test/accuracy': 0.4733000099658966, 'test/loss': 2.4336440563201904, 'test/num_examples': 10000, 'score': 9747.298719406128, 'total_duration': 10183.808728456497, 'accumulated_submission_time': 9747.298719406128, 'accumulated_eval_time': 434.0755660533905, 'accumulated_logging_time': 1.5038611888885498, 'global_step': 28358, 'preemption_count': 0}), (29854, {'train/accuracy': 0.6729910373687744, 'train/loss': 1.4220002889633179, 'validation/accuracy': 0.5934999585151672, 'validation/loss': 1.7916085720062256, 'validation/num_examples': 50000, 'test/accuracy': 0.4643000364303589, 'test/loss': 2.4862213134765625, 'test/num_examples': 10000, 'score': 10257.261818170547, 'total_duration': 10718.622612953186, 'accumulated_submission_time': 10257.261818170547, 'accumulated_eval_time': 458.8446247577667, 'accumulated_logging_time': 1.535851240158081, 'global_step': 29854, 'preemption_count': 0}), (31351, {'train/accuracy': 0.6510881781578064, 'train/loss': 1.5065069198608398, 'validation/accuracy': 0.5972599983215332, 'validation/loss': 1.7575511932373047, 'validation/num_examples': 50000, 'test/accuracy': 0.46950003504753113, 'test/loss': 2.46608304977417, 'test/num_examples': 10000, 'score': 10767.481489896774, 'total_duration': 11253.432497739792, 'accumulated_submission_time': 10767.481489896774, 'accumulated_eval_time': 483.3538534641266, 'accumulated_logging_time': 1.5658612251281738, 'global_step': 31351, 'preemption_count': 0}), (32848, {'train/accuracy': 0.6462252736091614, 'train/loss': 1.5267188549041748, 'validation/accuracy': 0.6002399921417236, 'validation/loss': 1.758679986000061, 'validation/num_examples': 50000, 'test/accuracy': 0.47610002756118774, 'test/loss': 2.4272708892822266, 'test/num_examples': 10000, 'score': 11277.717984676361, 'total_duration': 11786.628136873245, 'accumulated_submission_time': 11277.717984676361, 'accumulated_eval_time': 506.23028230667114, 'accumulated_logging_time': 1.5994904041290283, 'global_step': 32848, 'preemption_count': 0}), (34344, {'train/accuracy': 0.6496133208274841, 'train/loss': 1.536085605621338, 'validation/accuracy': 0.6037399768829346, 'validation/loss': 1.748511791229248, 'validation/num_examples': 50000, 'test/accuracy': 0.47700002789497375, 'test/loss': 2.4422802925109863, 'test/num_examples': 10000, 'score': 11787.83618426323, 'total_duration': 12318.89468216896, 'accumulated_submission_time': 11787.83618426323, 'accumulated_eval_time': 528.2995517253876, 'accumulated_logging_time': 1.6302180290222168, 'global_step': 34344, 'preemption_count': 0}), (35840, {'train/accuracy': 0.645926296710968, 'train/loss': 1.52766752243042, 'validation/accuracy': 0.596560001373291, 'validation/loss': 1.7534399032592773, 'validation/num_examples': 50000, 'test/accuracy': 0.4759000241756439, 'test/loss': 2.4189414978027344, 'test/num_examples': 10000, 'score': 12297.890921831131, 'total_duration': 12852.479150056839, 'accumulated_submission_time': 12297.890921831131, 'accumulated_eval_time': 551.7431771755219, 'accumulated_logging_time': 1.6666617393493652, 'global_step': 35840, 'preemption_count': 0}), (37336, {'train/accuracy': 0.6403061151504517, 'train/loss': 1.5559779405593872, 'validation/accuracy': 0.6036199927330017, 'validation/loss': 1.7466239929199219, 'validation/num_examples': 50000, 'test/accuracy': 0.4829000234603882, 'test/loss': 2.4181418418884277, 'test/num_examples': 10000, 'score': 12807.821355819702, 'total_duration': 13384.028679132462, 'accumulated_submission_time': 12807.821355819702, 'accumulated_eval_time': 573.2801125049591, 'accumulated_logging_time': 1.6983842849731445, 'global_step': 37336, 'preemption_count': 0}), (38832, {'train/accuracy': 0.6729711294174194, 'train/loss': 1.4390978813171387, 'validation/accuracy': 0.6108199954032898, 'validation/loss': 1.7267426252365112, 'validation/num_examples': 50000, 'test/accuracy': 0.49170002341270447, 'test/loss': 2.3655688762664795, 'test/num_examples': 10000, 'score': 13317.77873301506, 'total_duration': 13917.43752002716, 'accumulated_submission_time': 13317.77873301506, 'accumulated_eval_time': 596.6457276344299, 'accumulated_logging_time': 1.734304666519165, 'global_step': 38832, 'preemption_count': 0}), (40329, {'train/accuracy': 0.6744459271430969, 'train/loss': 1.4112752676010132, 'validation/accuracy': 0.610260009765625, 'validation/loss': 1.698612928390503, 'validation/num_examples': 50000, 'test/accuracy': 0.49410003423690796, 'test/loss': 2.351206064224243, 'test/num_examples': 10000, 'score': 13827.9108877182, 'total_duration': 14448.347055912018, 'accumulated_submission_time': 13827.9108877182, 'accumulated_eval_time': 617.3422248363495, 'accumulated_logging_time': 1.7646510601043701, 'global_step': 40329, 'preemption_count': 0}), (41825, {'train/accuracy': 0.6544762253761292, 'train/loss': 1.509508728981018, 'validation/accuracy': 0.6057199835777283, 'validation/loss': 1.743627667427063, 'validation/num_examples': 50000, 'test/accuracy': 0.4783000349998474, 'test/loss': 2.419309377670288, 'test/num_examples': 10000, 'score': 14337.827102661133, 'total_duration': 14978.894652605057, 'accumulated_submission_time': 14337.827102661133, 'accumulated_eval_time': 637.8876869678497, 'accumulated_logging_time': 1.7998626232147217, 'global_step': 41825, 'preemption_count': 0}), (43322, {'train/accuracy': 0.6491350531578064, 'train/loss': 1.5379236936569214, 'validation/accuracy': 0.6043199896812439, 'validation/loss': 1.748526930809021, 'validation/num_examples': 50000, 'test/accuracy': 0.4749000370502472, 'test/loss': 2.452561378479004, 'test/num_examples': 10000, 'score': 14847.952889919281, 'total_duration': 15509.131530761719, 'accumulated_submission_time': 14847.952889919281, 'accumulated_eval_time': 657.9168131351471, 'accumulated_logging_time': 1.832062005996704, 'global_step': 43322, 'preemption_count': 0}), (44819, {'train/accuracy': 0.6519451141357422, 'train/loss': 1.5086034536361694, 'validation/accuracy': 0.6087799668312073, 'validation/loss': 1.7206631898880005, 'validation/num_examples': 50000, 'test/accuracy': 0.48440003395080566, 'test/loss': 2.402400016784668, 'test/num_examples': 10000, 'score': 15358.028193473816, 'total_duration': 16037.553413391113, 'accumulated_submission_time': 15358.028193473816, 'accumulated_eval_time': 676.1758737564087, 'accumulated_logging_time': 1.8695974349975586, 'global_step': 44819, 'preemption_count': 0}), (46315, {'train/accuracy': 0.6457070708274841, 'train/loss': 1.543419361114502, 'validation/accuracy': 0.6055799722671509, 'validation/loss': 1.7364466190338135, 'validation/num_examples': 50000, 'test/accuracy': 0.47050002217292786, 'test/loss': 2.4317517280578613, 'test/num_examples': 10000, 'score': 15867.962439060211, 'total_duration': 16565.30499601364, 'accumulated_submission_time': 15867.962439060211, 'accumulated_eval_time': 693.9103631973267, 'accumulated_logging_time': 1.903987169265747, 'global_step': 46315, 'preemption_count': 0}), (47812, {'train/accuracy': 0.6532405614852905, 'train/loss': 1.498455286026001, 'validation/accuracy': 0.6109600067138672, 'validation/loss': 1.7112398147583008, 'validation/num_examples': 50000, 'test/accuracy': 0.4796000123023987, 'test/loss': 2.4064650535583496, 'test/num_examples': 10000, 'score': 16378.023522853851, 'total_duration': 17093.762252807617, 'accumulated_submission_time': 16378.023522853851, 'accumulated_eval_time': 712.2113356590271, 'accumulated_logging_time': 1.949737787246704, 'global_step': 47812, 'preemption_count': 0}), (49308, {'train/accuracy': 0.6916453838348389, 'train/loss': 1.3557281494140625, 'validation/accuracy': 0.6181399822235107, 'validation/loss': 1.6922627687454224, 'validation/num_examples': 50000, 'test/accuracy': 0.48600003123283386, 'test/loss': 2.362870216369629, 'test/num_examples': 10000, 'score': 16887.94068956375, 'total_duration': 17621.080164194107, 'accumulated_submission_time': 16887.94068956375, 'accumulated_eval_time': 729.52081990242, 'accumulated_logging_time': 1.9927701950073242, 'global_step': 49308, 'preemption_count': 0}), (50807, {'train/accuracy': 0.671297013759613, 'train/loss': 1.4186804294586182, 'validation/accuracy': 0.6159999966621399, 'validation/loss': 1.6773052215576172, 'validation/num_examples': 50000, 'test/accuracy': 0.4921000301837921, 'test/loss': 2.351040840148926, 'test/num_examples': 10000, 'score': 17398.12221980095, 'total_duration': 18148.496007680893, 'accumulated_submission_time': 17398.12221980095, 'accumulated_eval_time': 746.666835308075, 'accumulated_logging_time': 2.0300121307373047, 'global_step': 50807, 'preemption_count': 0}), (52304, {'train/accuracy': 0.6713567972183228, 'train/loss': 1.4391323328018188, 'validation/accuracy': 0.6169399619102478, 'validation/loss': 1.6796952486038208, 'validation/num_examples': 50000, 'test/accuracy': 0.4946000277996063, 'test/loss': 2.344132661819458, 'test/num_examples': 10000, 'score': 17908.237648248672, 'total_duration': 18675.987852096558, 'accumulated_submission_time': 17908.237648248672, 'accumulated_eval_time': 763.9545395374298, 'accumulated_logging_time': 2.067458152770996, 'global_step': 52304, 'preemption_count': 0}), (53801, {'train/accuracy': 0.6638432741165161, 'train/loss': 1.4525445699691772, 'validation/accuracy': 0.6154999732971191, 'validation/loss': 1.6743123531341553, 'validation/num_examples': 50000, 'test/accuracy': 0.49660003185272217, 'test/loss': 2.3256804943084717, 'test/num_examples': 10000, 'score': 18418.46981525421, 'total_duration': 19203.51691007614, 'accumulated_submission_time': 18418.46981525421, 'accumulated_eval_time': 781.1611652374268, 'accumulated_logging_time': 2.1074891090393066, 'global_step': 53801, 'preemption_count': 0}), (55299, {'train/accuracy': 0.6633649468421936, 'train/loss': 1.4799798727035522, 'validation/accuracy': 0.6179400086402893, 'validation/loss': 1.6885169744491577, 'validation/num_examples': 50000, 'test/accuracy': 0.49790000915527344, 'test/loss': 2.3356235027313232, 'test/num_examples': 10000, 'score': 18928.68518781662, 'total_duration': 19731.023154973984, 'accumulated_submission_time': 18928.68518781662, 'accumulated_eval_time': 798.3581395149231, 'accumulated_logging_time': 2.149481773376465, 'global_step': 55299, 'preemption_count': 0}), (56797, {'train/accuracy': 0.6719148755073547, 'train/loss': 1.425616979598999, 'validation/accuracy': 0.6231799721717834, 'validation/loss': 1.6395264863967896, 'validation/num_examples': 50000, 'test/accuracy': 0.508400022983551, 'test/loss': 2.2914958000183105, 'test/num_examples': 10000, 'score': 19438.76026201248, 'total_duration': 20258.592869758606, 'accumulated_submission_time': 19438.76026201248, 'accumulated_eval_time': 815.757345199585, 'accumulated_logging_time': 2.195958137512207, 'global_step': 56797, 'preemption_count': 0}), (58294, {'train/accuracy': 0.7206233739852905, 'train/loss': 1.2154070138931274, 'validation/accuracy': 0.6320399641990662, 'validation/loss': 1.6034917831420898, 'validation/num_examples': 50000, 'test/accuracy': 0.5078000426292419, 'test/loss': 2.266110897064209, 'test/num_examples': 10000, 'score': 19948.852291822433, 'total_duration': 20786.057732582092, 'accumulated_submission_time': 19948.852291822433, 'accumulated_eval_time': 833.042439699173, 'accumulated_logging_time': 2.2322604656219482, 'global_step': 58294, 'preemption_count': 0}), (59792, {'train/accuracy': 0.6831552982330322, 'train/loss': 1.3804254531860352, 'validation/accuracy': 0.6195999979972839, 'validation/loss': 1.6663074493408203, 'validation/num_examples': 50000, 'test/accuracy': 0.49150002002716064, 'test/loss': 2.3457841873168945, 'test/num_examples': 10000, 'score': 20458.783344745636, 'total_duration': 21313.644002199173, 'accumulated_submission_time': 20458.783344745636, 'accumulated_eval_time': 850.6091375350952, 'accumulated_logging_time': 2.270244836807251, 'global_step': 59792, 'preemption_count': 0}), (61289, {'train/accuracy': 0.6767179369926453, 'train/loss': 1.392195224761963, 'validation/accuracy': 0.6308199763298035, 'validation/loss': 1.6244053840637207, 'validation/num_examples': 50000, 'test/accuracy': 0.5031999945640564, 'test/loss': 2.3163297176361084, 'test/num_examples': 10000, 'score': 20968.790618896484, 'total_duration': 21840.92485141754, 'accumulated_submission_time': 20968.790618896484, 'accumulated_eval_time': 867.795756816864, 'accumulated_logging_time': 2.307173728942871, 'global_step': 61289, 'preemption_count': 0}), (62787, {'train/accuracy': 0.682637095451355, 'train/loss': 1.37348473072052, 'validation/accuracy': 0.6302399635314941, 'validation/loss': 1.626953125, 'validation/num_examples': 50000, 'test/accuracy': 0.5047000050544739, 'test/loss': 2.289322853088379, 'test/num_examples': 10000, 'score': 21478.93054676056, 'total_duration': 22368.341687202454, 'accumulated_submission_time': 21478.93054676056, 'accumulated_eval_time': 884.985392332077, 'accumulated_logging_time': 2.344024419784546, 'global_step': 62787, 'preemption_count': 0}), (64284, {'train/accuracy': 0.6745256781578064, 'train/loss': 1.4175406694412231, 'validation/accuracy': 0.6255800127983093, 'validation/loss': 1.6421574354171753, 'validation/num_examples': 50000, 'test/accuracy': 0.5065000057220459, 'test/loss': 2.2937440872192383, 'test/num_examples': 10000, 'score': 21988.930440187454, 'total_duration': 22895.69519519806, 'accumulated_submission_time': 21988.930440187454, 'accumulated_eval_time': 902.2461650371552, 'accumulated_logging_time': 2.3862104415893555, 'global_step': 64284, 'preemption_count': 0}), (65782, {'train/accuracy': 0.6729512214660645, 'train/loss': 1.4147884845733643, 'validation/accuracy': 0.6275599598884583, 'validation/loss': 1.630405068397522, 'validation/num_examples': 50000, 'test/accuracy': 0.504800021648407, 'test/loss': 2.2867746353149414, 'test/num_examples': 10000, 'score': 22499.075800418854, 'total_duration': 23423.206660985947, 'accumulated_submission_time': 22499.075800418854, 'accumulated_eval_time': 919.5094072818756, 'accumulated_logging_time': 2.438976287841797, 'global_step': 65782, 'preemption_count': 0}), (67279, {'train/accuracy': 0.6883569955825806, 'train/loss': 1.3431607484817505, 'validation/accuracy': 0.6266799569129944, 'validation/loss': 1.618585467338562, 'validation/num_examples': 50000, 'test/accuracy': 0.5004000067710876, 'test/loss': 2.3088037967681885, 'test/num_examples': 10000, 'score': 23009.149444818497, 'total_duration': 23950.570457935333, 'accumulated_submission_time': 23009.149444818497, 'accumulated_eval_time': 936.7078473567963, 'accumulated_logging_time': 2.4785714149475098, 'global_step': 67279, 'preemption_count': 0}), (68777, {'train/accuracy': 0.7035036683082581, 'train/loss': 1.274147391319275, 'validation/accuracy': 0.6340799927711487, 'validation/loss': 1.591722011566162, 'validation/num_examples': 50000, 'test/accuracy': 0.5062000155448914, 'test/loss': 2.276012897491455, 'test/num_examples': 10000, 'score': 23519.179130077362, 'total_duration': 24477.8970515728, 'accumulated_submission_time': 23519.179130077362, 'accumulated_eval_time': 953.9149236679077, 'accumulated_logging_time': 2.5172877311706543, 'global_step': 68777, 'preemption_count': 0}), (70275, {'train/accuracy': 0.6885961294174194, 'train/loss': 1.343567132949829, 'validation/accuracy': 0.6320799589157104, 'validation/loss': 1.6132029294967651, 'validation/num_examples': 50000, 'test/accuracy': 0.5059000253677368, 'test/loss': 2.2884953022003174, 'test/num_examples': 10000, 'score': 24029.257930278778, 'total_duration': 25005.752648115158, 'accumulated_submission_time': 24029.257930278778, 'accumulated_eval_time': 971.5965132713318, 'accumulated_logging_time': 2.5620453357696533, 'global_step': 70275, 'preemption_count': 0}), (71773, {'train/accuracy': 0.6815010905265808, 'train/loss': 1.3888893127441406, 'validation/accuracy': 0.626259982585907, 'validation/loss': 1.6451306343078613, 'validation/num_examples': 50000, 'test/accuracy': 0.5006000399589539, 'test/loss': 2.317592144012451, 'test/num_examples': 10000, 'score': 24539.311646461487, 'total_duration': 25533.141258955002, 'accumulated_submission_time': 24539.311646461487, 'accumulated_eval_time': 988.8352327346802, 'accumulated_logging_time': 2.6068990230560303, 'global_step': 71773, 'preemption_count': 0}), (73271, {'train/accuracy': 0.6846500039100647, 'train/loss': 1.3535865545272827, 'validation/accuracy': 0.6345799565315247, 'validation/loss': 1.5897456407546997, 'validation/num_examples': 50000, 'test/accuracy': 0.5053000450134277, 'test/loss': 2.248891592025757, 'test/num_examples': 10000, 'score': 25049.316918611526, 'total_duration': 26060.775750637054, 'accumulated_submission_time': 25049.316918611526, 'accumulated_eval_time': 1006.3702020645142, 'accumulated_logging_time': 2.651460647583008, 'global_step': 73271, 'preemption_count': 0}), (74769, {'train/accuracy': 0.6935586333274841, 'train/loss': 1.3076993227005005, 'validation/accuracy': 0.6421399712562561, 'validation/loss': 1.5479406118392944, 'validation/num_examples': 50000, 'test/accuracy': 0.5216000080108643, 'test/loss': 2.2021830081939697, 'test/num_examples': 10000, 'score': 25559.289101839066, 'total_duration': 26588.49595093727, 'accumulated_submission_time': 25559.289101839066, 'accumulated_eval_time': 1024.0254747867584, 'accumulated_logging_time': 2.6931560039520264, 'global_step': 74769, 'preemption_count': 0}), (76267, {'train/accuracy': 0.6836535334587097, 'train/loss': 1.3648991584777832, 'validation/accuracy': 0.6360799670219421, 'validation/loss': 1.5922093391418457, 'validation/num_examples': 50000, 'test/accuracy': 0.5103999972343445, 'test/loss': 2.274660348892212, 'test/num_examples': 10000, 'score': 26069.32606625557, 'total_duration': 27115.904922246933, 'accumulated_submission_time': 26069.32606625557, 'accumulated_eval_time': 1041.3056933879852, 'accumulated_logging_time': 2.7349095344543457, 'global_step': 76267, 'preemption_count': 0}), (77765, {'train/accuracy': 0.7120934128761292, 'train/loss': 1.2422109842300415, 'validation/accuracy': 0.638700008392334, 'validation/loss': 1.5625419616699219, 'validation/num_examples': 50000, 'test/accuracy': 0.5115000009536743, 'test/loss': 2.2503161430358887, 'test/num_examples': 10000, 'score': 26579.532474040985, 'total_duration': 27643.506959199905, 'accumulated_submission_time': 26579.532474040985, 'accumulated_eval_time': 1058.6034083366394, 'accumulated_logging_time': 2.780672311782837, 'global_step': 77765, 'preemption_count': 0}), (79263, {'train/accuracy': 0.7058752775192261, 'train/loss': 1.2854481935501099, 'validation/accuracy': 0.6415599584579468, 'validation/loss': 1.569705605506897, 'validation/num_examples': 50000, 'test/accuracy': 0.5182999968528748, 'test/loss': 2.221109390258789, 'test/num_examples': 10000, 'score': 27089.686541080475, 'total_duration': 28170.947011709213, 'accumulated_submission_time': 27089.686541080475, 'accumulated_eval_time': 1075.7997291088104, 'accumulated_logging_time': 2.820523977279663, 'global_step': 79263, 'preemption_count': 0}), (80762, {'train/accuracy': 0.7010523080825806, 'train/loss': 1.3063050508499146, 'validation/accuracy': 0.648140013217926, 'validation/loss': 1.5532654523849487, 'validation/num_examples': 50000, 'test/accuracy': 0.5182999968528748, 'test/loss': 2.2196383476257324, 'test/num_examples': 10000, 'score': 27599.767220258713, 'total_duration': 28698.262020349503, 'accumulated_submission_time': 27599.767220258713, 'accumulated_eval_time': 1092.9376814365387, 'accumulated_logging_time': 2.8669273853302, 'global_step': 80762, 'preemption_count': 0}), (82260, {'train/accuracy': 0.6965281963348389, 'train/loss': 1.2849880456924438, 'validation/accuracy': 0.64656001329422, 'validation/loss': 1.5245449542999268, 'validation/num_examples': 50000, 'test/accuracy': 0.5174000263214111, 'test/loss': 2.2001779079437256, 'test/num_examples': 10000, 'score': 28109.76361966133, 'total_duration': 29225.500024795532, 'accumulated_submission_time': 28109.76361966133, 'accumulated_eval_time': 1110.088816165924, 'accumulated_logging_time': 2.9066665172576904, 'global_step': 82260, 'preemption_count': 0}), (83758, {'train/accuracy': 0.7009526491165161, 'train/loss': 1.2806954383850098, 'validation/accuracy': 0.6505799889564514, 'validation/loss': 1.5104814767837524, 'validation/num_examples': 50000, 'test/accuracy': 0.5253000259399414, 'test/loss': 2.1855990886688232, 'test/num_examples': 10000, 'score': 28619.767454862595, 'total_duration': 29752.664216518402, 'accumulated_submission_time': 28619.767454862595, 'accumulated_eval_time': 1127.1571650505066, 'accumulated_logging_time': 2.946876287460327, 'global_step': 83758, 'preemption_count': 0}), (85257, {'train/accuracy': 0.7001753449440002, 'train/loss': 1.2842198610305786, 'validation/accuracy': 0.6523399949073792, 'validation/loss': 1.5107489824295044, 'validation/num_examples': 50000, 'test/accuracy': 0.5213000178337097, 'test/loss': 2.2066726684570312, 'test/num_examples': 10000, 'score': 29129.908967256546, 'total_duration': 30280.324359178543, 'accumulated_submission_time': 29129.908967256546, 'accumulated_eval_time': 1144.5813839435577, 'accumulated_logging_time': 2.9912562370300293, 'global_step': 85257, 'preemption_count': 0}), (86755, {'train/accuracy': 0.7459940910339355, 'train/loss': 1.084036946296692, 'validation/accuracy': 0.6534799933433533, 'validation/loss': 1.4911538362503052, 'validation/num_examples': 50000, 'test/accuracy': 0.5329000353813171, 'test/loss': 2.1359615325927734, 'test/num_examples': 10000, 'score': 29639.830031633377, 'total_duration': 30807.24199461937, 'accumulated_submission_time': 29639.830031633377, 'accumulated_eval_time': 1161.4845032691956, 'accumulated_logging_time': 3.0346953868865967, 'global_step': 86755, 'preemption_count': 0}), (88253, {'train/accuracy': 0.7254464030265808, 'train/loss': 1.1623930931091309, 'validation/accuracy': 0.6578199863433838, 'validation/loss': 1.4649604558944702, 'validation/num_examples': 50000, 'test/accuracy': 0.5353000164031982, 'test/loss': 2.1265015602111816, 'test/num_examples': 10000, 'score': 30150.005770921707, 'total_duration': 31334.62922167778, 'accumulated_submission_time': 30150.005770921707, 'accumulated_eval_time': 1178.5891389846802, 'accumulated_logging_time': 3.0897841453552246, 'global_step': 88253, 'preemption_count': 0}), (89751, {'train/accuracy': 0.7215999364852905, 'train/loss': 1.1721854209899902, 'validation/accuracy': 0.6617000102996826, 'validation/loss': 1.4494667053222656, 'validation/num_examples': 50000, 'test/accuracy': 0.5382000207901001, 'test/loss': 2.107398748397827, 'test/num_examples': 10000, 'score': 30660.091092586517, 'total_duration': 31862.09580183029, 'accumulated_submission_time': 30660.091092586517, 'accumulated_eval_time': 1195.8721101284027, 'accumulated_logging_time': 3.1363139152526855, 'global_step': 89751, 'preemption_count': 0}), (91249, {'train/accuracy': 0.7166972160339355, 'train/loss': 1.2218282222747803, 'validation/accuracy': 0.6592199802398682, 'validation/loss': 1.4793634414672852, 'validation/num_examples': 50000, 'test/accuracy': 0.5379000306129456, 'test/loss': 2.1280786991119385, 'test/num_examples': 10000, 'score': 31170.067274332047, 'total_duration': 32389.319952964783, 'accumulated_submission_time': 31170.067274332047, 'accumulated_eval_time': 1213.0246744155884, 'accumulated_logging_time': 3.181325912475586, 'global_step': 91249, 'preemption_count': 0}), (92747, {'train/accuracy': 0.7115951776504517, 'train/loss': 1.2422090768814087, 'validation/accuracy': 0.6555399894714355, 'validation/loss': 1.498248815536499, 'validation/num_examples': 50000, 'test/accuracy': 0.5285000205039978, 'test/loss': 2.168752908706665, 'test/num_examples': 10000, 'score': 31679.979499578476, 'total_duration': 32916.61490535736, 'accumulated_submission_time': 31679.979499578476, 'accumulated_eval_time': 1230.3129467964172, 'accumulated_logging_time': 3.225436210632324, 'global_step': 92747, 'preemption_count': 0}), (94245, {'train/accuracy': 0.7129902839660645, 'train/loss': 1.244142770767212, 'validation/accuracy': 0.6612399816513062, 'validation/loss': 1.4770818948745728, 'validation/num_examples': 50000, 'test/accuracy': 0.5364000201225281, 'test/loss': 2.1361281871795654, 'test/num_examples': 10000, 'score': 32189.998003721237, 'total_duration': 33443.94494795799, 'accumulated_submission_time': 32189.998003721237, 'accumulated_eval_time': 1247.537139415741, 'accumulated_logging_time': 3.262047529220581, 'global_step': 94245, 'preemption_count': 0}), (95743, {'train/accuracy': 0.742586076259613, 'train/loss': 1.1307847499847412, 'validation/accuracy': 0.669219970703125, 'validation/loss': 1.4510703086853027, 'validation/num_examples': 50000, 'test/accuracy': 0.5428000092506409, 'test/loss': 2.1002559661865234, 'test/num_examples': 10000, 'score': 32699.949419021606, 'total_duration': 33971.354709625244, 'accumulated_submission_time': 32699.949419021606, 'accumulated_eval_time': 1264.8968999385834, 'accumulated_logging_time': 3.308539628982544, 'global_step': 95743, 'preemption_count': 0}), (97241, {'train/accuracy': 0.7416892647743225, 'train/loss': 1.120581030845642, 'validation/accuracy': 0.6673399806022644, 'validation/loss': 1.463154911994934, 'validation/num_examples': 50000, 'test/accuracy': 0.5376999974250793, 'test/loss': 2.125854253768921, 'test/num_examples': 10000, 'score': 33210.114094257355, 'total_duration': 34498.70940423012, 'accumulated_submission_time': 33210.114094257355, 'accumulated_eval_time': 1281.9905200004578, 'accumulated_logging_time': 3.3550431728363037, 'global_step': 97241, 'preemption_count': 0}), (98739, {'train/accuracy': 0.7276586294174194, 'train/loss': 1.1531447172164917, 'validation/accuracy': 0.666920006275177, 'validation/loss': 1.4372801780700684, 'validation/num_examples': 50000, 'test/accuracy': 0.5424000024795532, 'test/loss': 2.1051735877990723, 'test/num_examples': 10000, 'score': 33720.06629896164, 'total_duration': 35025.87611365318, 'accumulated_submission_time': 33720.06629896164, 'accumulated_eval_time': 1299.1062581539154, 'accumulated_logging_time': 3.401531219482422, 'global_step': 98739, 'preemption_count': 0}), (100237, {'train/accuracy': 0.7302096486091614, 'train/loss': 1.1675571203231812, 'validation/accuracy': 0.6653199791908264, 'validation/loss': 1.4513657093048096, 'validation/num_examples': 50000, 'test/accuracy': 0.5361000299453735, 'test/loss': 2.1154842376708984, 'test/num_examples': 10000, 'score': 34229.99035668373, 'total_duration': 35552.84500479698, 'accumulated_submission_time': 34229.99035668373, 'accumulated_eval_time': 1316.049479007721, 'accumulated_logging_time': 3.4516913890838623, 'global_step': 100237, 'preemption_count': 0}), (101735, {'train/accuracy': 0.7242107391357422, 'train/loss': 1.1790367364883423, 'validation/accuracy': 0.6686800122261047, 'validation/loss': 1.4343260526657104, 'validation/num_examples': 50000, 'test/accuracy': 0.5398000478744507, 'test/loss': 2.1134259700775146, 'test/num_examples': 10000, 'score': 34739.97491002083, 'total_duration': 36080.01940727234, 'accumulated_submission_time': 34739.97491002083, 'accumulated_eval_time': 1333.1427223682404, 'accumulated_logging_time': 3.4981353282928467, 'global_step': 101735, 'preemption_count': 0}), (103233, {'train/accuracy': 0.7329798936843872, 'train/loss': 1.1520893573760986, 'validation/accuracy': 0.676859974861145, 'validation/loss': 1.4166473150253296, 'validation/num_examples': 50000, 'test/accuracy': 0.5520000457763672, 'test/loss': 2.0522189140319824, 'test/num_examples': 10000, 'score': 35250.05642032623, 'total_duration': 36607.308309316635, 'accumulated_submission_time': 35250.05642032623, 'accumulated_eval_time': 1350.2484858036041, 'accumulated_logging_time': 3.5491249561309814, 'global_step': 103233, 'preemption_count': 0}), (104731, {'train/accuracy': 0.7315847873687744, 'train/loss': 1.1411164999008179, 'validation/accuracy': 0.6702199578285217, 'validation/loss': 1.417891502380371, 'validation/num_examples': 50000, 'test/accuracy': 0.5373000502586365, 'test/loss': 2.095768928527832, 'test/num_examples': 10000, 'score': 35759.99800825119, 'total_duration': 37134.51687049866, 'accumulated_submission_time': 35759.99800825119, 'accumulated_eval_time': 1367.418380498886, 'accumulated_logging_time': 3.5954058170318604, 'global_step': 104731, 'preemption_count': 0}), (106229, {'train/accuracy': 0.7540457248687744, 'train/loss': 1.072227954864502, 'validation/accuracy': 0.6693599820137024, 'validation/loss': 1.442069172859192, 'validation/num_examples': 50000, 'test/accuracy': 0.5469000339508057, 'test/loss': 2.085392951965332, 'test/num_examples': 10000, 'score': 36270.085739851, 'total_duration': 37661.89115190506, 'accumulated_submission_time': 36270.085739851, 'accumulated_eval_time': 1384.609278678894, 'accumulated_logging_time': 3.6411328315734863, 'global_step': 106229, 'preemption_count': 0}), (107727, {'train/accuracy': 0.751973032951355, 'train/loss': 1.068299651145935, 'validation/accuracy': 0.6773999929428101, 'validation/loss': 1.4052557945251465, 'validation/num_examples': 50000, 'test/accuracy': 0.5508000254631042, 'test/loss': 2.0545578002929688, 'test/num_examples': 10000, 'score': 36780.10208392143, 'total_duration': 38188.91479277611, 'accumulated_submission_time': 36780.10208392143, 'accumulated_eval_time': 1401.521045923233, 'accumulated_logging_time': 3.6874375343322754, 'global_step': 107727, 'preemption_count': 0}), (109225, {'train/accuracy': 0.7469507455825806, 'train/loss': 1.086888074874878, 'validation/accuracy': 0.6785999536514282, 'validation/loss': 1.3956547975540161, 'validation/num_examples': 50000, 'test/accuracy': 0.5574000477790833, 'test/loss': 2.0311355590820312, 'test/num_examples': 10000, 'score': 37290.17399263382, 'total_duration': 38716.25345778465, 'accumulated_submission_time': 37290.17399263382, 'accumulated_eval_time': 1418.6850700378418, 'accumulated_logging_time': 3.740187406539917, 'global_step': 109225, 'preemption_count': 0}), (110723, {'train/accuracy': 0.7480069994926453, 'train/loss': 1.080083966255188, 'validation/accuracy': 0.6832799911499023, 'validation/loss': 1.3639898300170898, 'validation/num_examples': 50000, 'test/accuracy': 0.5574000477790833, 'test/loss': 2.017634391784668, 'test/num_examples': 10000, 'score': 37800.17114710808, 'total_duration': 39243.452078580856, 'accumulated_submission_time': 37800.17114710808, 'accumulated_eval_time': 1435.787403345108, 'accumulated_logging_time': 3.788019895553589, 'global_step': 110723, 'preemption_count': 0}), (112221, {'train/accuracy': 0.7472297549247742, 'train/loss': 1.0725810527801514, 'validation/accuracy': 0.6823199987411499, 'validation/loss': 1.363050103187561, 'validation/num_examples': 50000, 'test/accuracy': 0.5624000430107117, 'test/loss': 2.0069241523742676, 'test/num_examples': 10000, 'score': 38310.2585234642, 'total_duration': 39770.780359745026, 'accumulated_submission_time': 38310.2585234642, 'accumulated_eval_time': 1452.9339735507965, 'accumulated_logging_time': 3.8331050872802734, 'global_step': 112221, 'preemption_count': 0}), (113719, {'train/accuracy': 0.7479073405265808, 'train/loss': 1.0708001852035522, 'validation/accuracy': 0.6868199706077576, 'validation/loss': 1.3397966623306274, 'validation/num_examples': 50000, 'test/accuracy': 0.5601000189781189, 'test/loss': 2.0163300037384033, 'test/num_examples': 10000, 'score': 38820.35129117966, 'total_duration': 40298.13475847244, 'accumulated_submission_time': 38820.35129117966, 'accumulated_eval_time': 1470.0948634147644, 'accumulated_logging_time': 3.881237268447876, 'global_step': 113719, 'preemption_count': 0}), (115217, {'train/accuracy': 0.7872887253761292, 'train/loss': 0.9008607268333435, 'validation/accuracy': 0.6908800005912781, 'validation/loss': 1.3157756328582764, 'validation/num_examples': 50000, 'test/accuracy': 0.5639000535011292, 'test/loss': 1.9897371530532837, 'test/num_examples': 10000, 'score': 39330.478276491165, 'total_duration': 40825.580815792084, 'accumulated_submission_time': 39330.478276491165, 'accumulated_eval_time': 1487.3175942897797, 'accumulated_logging_time': 3.9274373054504395, 'global_step': 115217, 'preemption_count': 0}), (116716, {'train/accuracy': 0.7737165093421936, 'train/loss': 0.961524486541748, 'validation/accuracy': 0.6925399899482727, 'validation/loss': 1.3126429319381714, 'validation/num_examples': 50000, 'test/accuracy': 0.5701000094413757, 'test/loss': 1.9820610284805298, 'test/num_examples': 10000, 'score': 39840.576077222824, 'total_duration': 41353.06807017326, 'accumulated_submission_time': 39840.576077222824, 'accumulated_eval_time': 1504.6061329841614, 'accumulated_logging_time': 3.9768893718719482, 'global_step': 116716, 'preemption_count': 0}), (118214, {'train/accuracy': 0.7669204473495483, 'train/loss': 1.010676383972168, 'validation/accuracy': 0.6954599618911743, 'validation/loss': 1.3212053775787354, 'validation/num_examples': 50000, 'test/accuracy': 0.5715000033378601, 'test/loss': 1.9586877822875977, 'test/num_examples': 10000, 'score': 40350.64465737343, 'total_duration': 41880.47035765648, 'accumulated_submission_time': 40350.64465737343, 'accumulated_eval_time': 1521.836299419403, 'accumulated_logging_time': 4.030749559402466, 'global_step': 118214, 'preemption_count': 0}), (119712, {'train/accuracy': 0.7684550285339355, 'train/loss': 1.0029422044754028, 'validation/accuracy': 0.6985799670219421, 'validation/loss': 1.310947299003601, 'validation/num_examples': 50000, 'test/accuracy': 0.5733000040054321, 'test/loss': 1.962609052658081, 'test/num_examples': 10000, 'score': 40860.65657663345, 'total_duration': 42407.98349690437, 'accumulated_submission_time': 40860.65657663345, 'accumulated_eval_time': 1539.2372624874115, 'accumulated_logging_time': 4.080804824829102, 'global_step': 119712, 'preemption_count': 0}), (121210, {'train/accuracy': 0.7650868892669678, 'train/loss': 0.9947850704193115, 'validation/accuracy': 0.6988399624824524, 'validation/loss': 1.2936506271362305, 'validation/num_examples': 50000, 'test/accuracy': 0.5770000219345093, 'test/loss': 1.9499543905258179, 'test/num_examples': 10000, 'score': 41370.574806690216, 'total_duration': 42935.32121896744, 'accumulated_submission_time': 41370.574806690216, 'accumulated_eval_time': 1556.559273481369, 'accumulated_logging_time': 4.127295732498169, 'global_step': 121210, 'preemption_count': 0}), (122708, {'train/accuracy': 0.7655652165412903, 'train/loss': 1.012738823890686, 'validation/accuracy': 0.6997199654579163, 'validation/loss': 1.3031195402145386, 'validation/num_examples': 50000, 'test/accuracy': 0.5733000040054321, 'test/loss': 1.966423749923706, 'test/num_examples': 10000, 'score': 41880.55111408234, 'total_duration': 43462.52871155739, 'accumulated_submission_time': 41880.55111408234, 'accumulated_eval_time': 1573.6708698272705, 'accumulated_logging_time': 4.196326732635498, 'global_step': 122708, 'preemption_count': 0}), (124206, {'train/accuracy': 0.7978515625, 'train/loss': 0.8683147430419922, 'validation/accuracy': 0.7073799967765808, 'validation/loss': 1.261612057685852, 'validation/num_examples': 50000, 'test/accuracy': 0.5795000195503235, 'test/loss': 1.902595043182373, 'test/num_examples': 10000, 'score': 42390.75530362129, 'total_duration': 43989.899998903275, 'accumulated_submission_time': 42390.75530362129, 'accumulated_eval_time': 1590.7367997169495, 'accumulated_logging_time': 4.2473227977752686, 'global_step': 124206, 'preemption_count': 0}), (125705, {'train/accuracy': 0.7971938848495483, 'train/loss': 0.8802825808525085, 'validation/accuracy': 0.7064599990844727, 'validation/loss': 1.266579031944275, 'validation/num_examples': 50000, 'test/accuracy': 0.5808000564575195, 'test/loss': 1.9092512130737305, 'test/num_examples': 10000, 'score': 42900.89259982109, 'total_duration': 44517.35197234154, 'accumulated_submission_time': 42900.89259982109, 'accumulated_eval_time': 1607.9538469314575, 'accumulated_logging_time': 4.293523788452148, 'global_step': 125705, 'preemption_count': 0}), (127202, {'train/accuracy': 0.7934072017669678, 'train/loss': 0.8894890546798706, 'validation/accuracy': 0.7084199786186218, 'validation/loss': 1.2445156574249268, 'validation/num_examples': 50000, 'test/accuracy': 0.5782000422477722, 'test/loss': 1.8972203731536865, 'test/num_examples': 10000, 'score': 43410.791429281235, 'total_duration': 45044.73135638237, 'accumulated_submission_time': 43410.791429281235, 'accumulated_eval_time': 1625.3317823410034, 'accumulated_logging_time': 4.347227096557617, 'global_step': 127202, 'preemption_count': 0}), (128699, {'train/accuracy': 0.7883649468421936, 'train/loss': 0.9085213541984558, 'validation/accuracy': 0.7102800011634827, 'validation/loss': 1.2549796104431152, 'validation/num_examples': 50000, 'test/accuracy': 0.5886000394821167, 'test/loss': 1.8822438716888428, 'test/num_examples': 10000, 'score': 43920.76516246796, 'total_duration': 45571.81634020805, 'accumulated_submission_time': 43920.76516246796, 'accumulated_eval_time': 1642.3397538661957, 'accumulated_logging_time': 4.400299549102783, 'global_step': 128699, 'preemption_count': 0}), (130197, {'train/accuracy': 0.7904177308082581, 'train/loss': 0.8950925469398499, 'validation/accuracy': 0.7142800092697144, 'validation/loss': 1.2327156066894531, 'validation/num_examples': 50000, 'test/accuracy': 0.5909000039100647, 'test/loss': 1.8738075494766235, 'test/num_examples': 10000, 'score': 44430.832174539566, 'total_duration': 46099.25558972359, 'accumulated_submission_time': 44430.832174539566, 'accumulated_eval_time': 1659.6125497817993, 'accumulated_logging_time': 4.4501917362213135, 'global_step': 130197, 'preemption_count': 0}), (131695, {'train/accuracy': 0.7917131781578064, 'train/loss': 0.8998433351516724, 'validation/accuracy': 0.7148999571800232, 'validation/loss': 1.22651207447052, 'validation/num_examples': 50000, 'test/accuracy': 0.5855000019073486, 'test/loss': 1.8798346519470215, 'test/num_examples': 10000, 'score': 44940.92831659317, 'total_duration': 46626.481568574905, 'accumulated_submission_time': 44940.92831659317, 'accumulated_eval_time': 1676.6434774398804, 'accumulated_logging_time': 4.499432563781738, 'global_step': 131695, 'preemption_count': 0}), (133193, {'train/accuracy': 0.7970942258834839, 'train/loss': 0.8763123750686646, 'validation/accuracy': 0.7187199592590332, 'validation/loss': 1.2121546268463135, 'validation/num_examples': 50000, 'test/accuracy': 0.5919000506401062, 'test/loss': 1.845463752746582, 'test/num_examples': 10000, 'score': 45450.86598086357, 'total_duration': 47153.72575092316, 'accumulated_submission_time': 45450.86598086357, 'accumulated_eval_time': 1693.8448660373688, 'accumulated_logging_time': 4.552513122558594, 'global_step': 133193, 'preemption_count': 0}), (134692, {'train/accuracy': 0.8205516338348389, 'train/loss': 0.7716889381408691, 'validation/accuracy': 0.7160199880599976, 'validation/loss': 1.2104519605636597, 'validation/num_examples': 50000, 'test/accuracy': 0.5961000323295593, 'test/loss': 1.841491937637329, 'test/num_examples': 10000, 'score': 45961.00407743454, 'total_duration': 47681.152082681656, 'accumulated_submission_time': 45961.00407743454, 'accumulated_eval_time': 1711.0298657417297, 'accumulated_logging_time': 4.6041131019592285, 'global_step': 134692, 'preemption_count': 0}), (136190, {'train/accuracy': 0.8130978941917419, 'train/loss': 0.8022621870040894, 'validation/accuracy': 0.7203399538993835, 'validation/loss': 1.197677493095398, 'validation/num_examples': 50000, 'test/accuracy': 0.5961000323295593, 'test/loss': 1.8225889205932617, 'test/num_examples': 10000, 'score': 46470.92022943497, 'total_duration': 48208.3938832283, 'accumulated_submission_time': 46470.92022943497, 'accumulated_eval_time': 1728.2518351078033, 'accumulated_logging_time': 4.656829357147217, 'global_step': 136190, 'preemption_count': 0}), (137689, {'train/accuracy': 0.810566782951355, 'train/loss': 0.8220297694206238, 'validation/accuracy': 0.722819983959198, 'validation/loss': 1.2077710628509521, 'validation/num_examples': 50000, 'test/accuracy': 0.5981000065803528, 'test/loss': 1.852874755859375, 'test/num_examples': 10000, 'score': 46980.9376642704, 'total_duration': 48735.45760130882, 'accumulated_submission_time': 46980.9376642704, 'accumulated_eval_time': 1745.192828655243, 'accumulated_logging_time': 4.712372779846191, 'global_step': 137689, 'preemption_count': 0}), (139187, {'train/accuracy': 0.8121013641357422, 'train/loss': 0.8041132688522339, 'validation/accuracy': 0.7247599959373474, 'validation/loss': 1.1859569549560547, 'validation/num_examples': 50000, 'test/accuracy': 0.5973000526428223, 'test/loss': 1.8258440494537354, 'test/num_examples': 10000, 'score': 47490.93303322792, 'total_duration': 49262.71445965767, 'accumulated_submission_time': 47490.93303322792, 'accumulated_eval_time': 1762.3447728157043, 'accumulated_logging_time': 4.771524906158447, 'global_step': 139187, 'preemption_count': 0}), (140684, {'train/accuracy': 0.8148915767669678, 'train/loss': 0.7869499921798706, 'validation/accuracy': 0.7278199791908264, 'validation/loss': 1.1672886610031128, 'validation/num_examples': 50000, 'test/accuracy': 0.6026000380516052, 'test/loss': 1.7988563776016235, 'test/num_examples': 10000, 'score': 48000.869512319565, 'total_duration': 49789.94197225571, 'accumulated_submission_time': 48000.869512319565, 'accumulated_eval_time': 1779.533354997635, 'accumulated_logging_time': 4.8248069286346436, 'global_step': 140684, 'preemption_count': 0}), (142182, {'train/accuracy': 0.8167450428009033, 'train/loss': 0.7779141664505005, 'validation/accuracy': 0.7300800085067749, 'validation/loss': 1.1511822938919067, 'validation/num_examples': 50000, 'test/accuracy': 0.6078000068664551, 'test/loss': 1.7888727188110352, 'test/num_examples': 10000, 'score': 48510.98008084297, 'total_duration': 50317.633615255356, 'accumulated_submission_time': 48510.98008084297, 'accumulated_eval_time': 1797.0130088329315, 'accumulated_logging_time': 4.874320030212402, 'global_step': 142182, 'preemption_count': 0}), (143681, {'train/accuracy': 0.8469786047935486, 'train/loss': 0.6639379262924194, 'validation/accuracy': 0.7300399541854858, 'validation/loss': 1.1502034664154053, 'validation/num_examples': 50000, 'test/accuracy': 0.6038000583648682, 'test/loss': 1.7933541536331177, 'test/num_examples': 10000, 'score': 49021.22454404831, 'total_duration': 50845.16202926636, 'accumulated_submission_time': 49021.22454404831, 'accumulated_eval_time': 1814.1996743679047, 'accumulated_logging_time': 4.921878337860107, 'global_step': 143681, 'preemption_count': 0}), (145179, {'train/accuracy': 0.8422552347183228, 'train/loss': 0.703779399394989, 'validation/accuracy': 0.7349399924278259, 'validation/loss': 1.1473641395568848, 'validation/num_examples': 50000, 'test/accuracy': 0.6086000204086304, 'test/loss': 1.7797400951385498, 'test/num_examples': 10000, 'score': 49531.14088320732, 'total_duration': 51372.43950200081, 'accumulated_submission_time': 49531.14088320732, 'accumulated_eval_time': 1831.4566979408264, 'accumulated_logging_time': 4.974995851516724, 'global_step': 145179, 'preemption_count': 0}), (146677, {'train/accuracy': 0.8409199714660645, 'train/loss': 0.7093674540519714, 'validation/accuracy': 0.7373799681663513, 'validation/loss': 1.1325105428695679, 'validation/num_examples': 50000, 'test/accuracy': 0.6091000437736511, 'test/loss': 1.7796084880828857, 'test/num_examples': 10000, 'score': 50041.08595466614, 'total_duration': 51899.51325583458, 'accumulated_submission_time': 50041.08595466614, 'accumulated_eval_time': 1848.4796833992004, 'accumulated_logging_time': 5.031417369842529, 'global_step': 146677, 'preemption_count': 0}), (148175, {'train/accuracy': 0.8373923897743225, 'train/loss': 0.7035523056983948, 'validation/accuracy': 0.737280011177063, 'validation/loss': 1.1355136632919312, 'validation/num_examples': 50000, 'test/accuracy': 0.6115000247955322, 'test/loss': 1.7595330476760864, 'test/num_examples': 10000, 'score': 50551.017679452896, 'total_duration': 52426.786516427994, 'accumulated_submission_time': 50551.017679452896, 'accumulated_eval_time': 1865.7171568870544, 'accumulated_logging_time': 5.085627317428589, 'global_step': 148175, 'preemption_count': 0}), (149673, {'train/accuracy': 0.8453842401504517, 'train/loss': 0.686999499797821, 'validation/accuracy': 0.7429400086402893, 'validation/loss': 1.107838749885559, 'validation/num_examples': 50000, 'test/accuracy': 0.6195000410079956, 'test/loss': 1.7411073446273804, 'test/num_examples': 10000, 'score': 51060.90836381912, 'total_duration': 52954.008268117905, 'accumulated_submission_time': 51060.90836381912, 'accumulated_eval_time': 1882.9315605163574, 'accumulated_logging_time': 5.151835203170776, 'global_step': 149673, 'preemption_count': 0}), (151171, {'train/accuracy': 0.8451052308082581, 'train/loss': 0.675420880317688, 'validation/accuracy': 0.7450799942016602, 'validation/loss': 1.1006088256835938, 'validation/num_examples': 50000, 'test/accuracy': 0.6212000250816345, 'test/loss': 1.727745532989502, 'test/num_examples': 10000, 'score': 51571.01009392738, 'total_duration': 53481.925209999084, 'accumulated_submission_time': 51571.01009392738, 'accumulated_eval_time': 1900.640768289566, 'accumulated_logging_time': 5.20801043510437, 'global_step': 151171, 'preemption_count': 0}), (152669, {'train/accuracy': 0.8661710619926453, 'train/loss': 0.6043965220451355, 'validation/accuracy': 0.7461599707603455, 'validation/loss': 1.0976543426513672, 'validation/num_examples': 50000, 'test/accuracy': 0.6201000213623047, 'test/loss': 1.735266089439392, 'test/num_examples': 10000, 'score': 52081.04586791992, 'total_duration': 54009.12109827995, 'accumulated_submission_time': 52081.04586791992, 'accumulated_eval_time': 1917.6977479457855, 'accumulated_logging_time': 5.260669708251953, 'global_step': 152669, 'preemption_count': 0}), (154167, {'train/accuracy': 0.8671875, 'train/loss': 0.5794281959533691, 'validation/accuracy': 0.7481399774551392, 'validation/loss': 1.0786703824996948, 'validation/num_examples': 50000, 'test/accuracy': 0.6278000473976135, 'test/loss': 1.6983375549316406, 'test/num_examples': 10000, 'score': 52591.11575245857, 'total_duration': 54536.50642514229, 'accumulated_submission_time': 52591.11575245857, 'accumulated_eval_time': 1934.90061545372, 'accumulated_logging_time': 5.321510553359985, 'global_step': 154167, 'preemption_count': 0}), (155665, {'train/accuracy': 0.8687419891357422, 'train/loss': 0.5882298946380615, 'validation/accuracy': 0.7505999803543091, 'validation/loss': 1.0769861936569214, 'validation/num_examples': 50000, 'test/accuracy': 0.6246000528335571, 'test/loss': 1.6996747255325317, 'test/num_examples': 10000, 'score': 53101.14227557182, 'total_duration': 55063.9355969429, 'accumulated_submission_time': 53101.14227557182, 'accumulated_eval_time': 1952.2021520137787, 'accumulated_logging_time': 5.373865604400635, 'global_step': 155665, 'preemption_count': 0}), (157162, {'train/accuracy': 0.8694794178009033, 'train/loss': 0.580014169216156, 'validation/accuracy': 0.7535199522972107, 'validation/loss': 1.0628749132156372, 'validation/num_examples': 50000, 'test/accuracy': 0.6239000558853149, 'test/loss': 1.6967737674713135, 'test/num_examples': 10000, 'score': 53611.06381011009, 'total_duration': 55590.8880238533, 'accumulated_submission_time': 53611.06381011009, 'accumulated_eval_time': 1969.1301229000092, 'accumulated_logging_time': 5.426816463470459, 'global_step': 157162, 'preemption_count': 0}), (158660, {'train/accuracy': 0.8698182106018066, 'train/loss': 0.5817106366157532, 'validation/accuracy': 0.7541399598121643, 'validation/loss': 1.064910888671875, 'validation/num_examples': 50000, 'test/accuracy': 0.6336000561714172, 'test/loss': 1.6853803396224976, 'test/num_examples': 10000, 'score': 54121.251658678055, 'total_duration': 56118.194451093674, 'accumulated_submission_time': 54121.251658678055, 'accumulated_eval_time': 1986.1422533988953, 'accumulated_logging_time': 5.484708070755005, 'global_step': 158660, 'preemption_count': 0}), (160158, {'train/accuracy': 0.875418484210968, 'train/loss': 0.5687929391860962, 'validation/accuracy': 0.756879985332489, 'validation/loss': 1.0507851839065552, 'validation/num_examples': 50000, 'test/accuracy': 0.6309000253677368, 'test/loss': 1.6826121807098389, 'test/num_examples': 10000, 'score': 54631.442410707474, 'total_duration': 56645.72098970413, 'accumulated_submission_time': 54631.442410707474, 'accumulated_eval_time': 2003.3706483840942, 'accumulated_logging_time': 5.54168963432312, 'global_step': 160158, 'preemption_count': 0}), (161656, {'train/accuracy': 0.8792450428009033, 'train/loss': 0.5455688238143921, 'validation/accuracy': 0.7571799755096436, 'validation/loss': 1.0448676347732544, 'validation/num_examples': 50000, 'test/accuracy': 0.6301000118255615, 'test/loss': 1.673742413520813, 'test/num_examples': 10000, 'score': 55141.38109588623, 'total_duration': 57173.04496335983, 'accumulated_submission_time': 55141.38109588623, 'accumulated_eval_time': 2020.6487910747528, 'accumulated_logging_time': 5.598784685134888, 'global_step': 161656, 'preemption_count': 0}), (163154, {'train/accuracy': 0.8960259556770325, 'train/loss': 0.49101361632347107, 'validation/accuracy': 0.7576000094413757, 'validation/loss': 1.0471340417861938, 'validation/num_examples': 50000, 'test/accuracy': 0.6349000334739685, 'test/loss': 1.6725289821624756, 'test/num_examples': 10000, 'score': 55651.28565096855, 'total_duration': 57700.09034585953, 'accumulated_submission_time': 55651.28565096855, 'accumulated_eval_time': 2037.681653022766, 'accumulated_logging_time': 5.65578556060791, 'global_step': 163154, 'preemption_count': 0}), (164652, {'train/accuracy': 0.8957070708274841, 'train/loss': 0.49083128571510315, 'validation/accuracy': 0.7618599534034729, 'validation/loss': 1.0314052104949951, 'validation/num_examples': 50000, 'test/accuracy': 0.6367000341415405, 'test/loss': 1.661550521850586, 'test/num_examples': 10000, 'score': 56161.19146823883, 'total_duration': 58227.19503569603, 'accumulated_submission_time': 56161.19146823883, 'accumulated_eval_time': 2054.770972251892, 'accumulated_logging_time': 5.715636491775513, 'global_step': 164652, 'preemption_count': 0}), (166150, {'train/accuracy': 0.8951490521430969, 'train/loss': 0.48919644951820374, 'validation/accuracy': 0.7637199759483337, 'validation/loss': 1.026726245880127, 'validation/num_examples': 50000, 'test/accuracy': 0.6398000121116638, 'test/loss': 1.6573363542556763, 'test/num_examples': 10000, 'score': 56671.27508664131, 'total_duration': 58754.45888543129, 'accumulated_submission_time': 56671.27508664131, 'accumulated_eval_time': 2071.8406381607056, 'accumulated_logging_time': 5.775366306304932, 'global_step': 166150, 'preemption_count': 0}), (167648, {'train/accuracy': 0.8947305083274841, 'train/loss': 0.4829561114311218, 'validation/accuracy': 0.7655199766159058, 'validation/loss': 1.0176388025283813, 'validation/num_examples': 50000, 'test/accuracy': 0.6388000249862671, 'test/loss': 1.6397991180419922, 'test/num_examples': 10000, 'score': 57181.31681752205, 'total_duration': 59281.749415397644, 'accumulated_submission_time': 57181.31681752205, 'accumulated_eval_time': 2088.97993516922, 'accumulated_logging_time': 5.834702968597412, 'global_step': 167648, 'preemption_count': 0}), (169146, {'train/accuracy': 0.9001514315605164, 'train/loss': 0.47772398591041565, 'validation/accuracy': 0.7644000053405762, 'validation/loss': 1.0248445272445679, 'validation/num_examples': 50000, 'test/accuracy': 0.6415000557899475, 'test/loss': 1.6550037860870361, 'test/num_examples': 10000, 'score': 57691.400824546814, 'total_duration': 59809.09949350357, 'accumulated_submission_time': 57691.400824546814, 'accumulated_eval_time': 2106.1391632556915, 'accumulated_logging_time': 5.890669107437134, 'global_step': 169146, 'preemption_count': 0}), (170644, {'train/accuracy': 0.9061303734779358, 'train/loss': 0.46180209517478943, 'validation/accuracy': 0.7671999931335449, 'validation/loss': 1.0098974704742432, 'validation/num_examples': 50000, 'test/accuracy': 0.6467000246047974, 'test/loss': 1.6335299015045166, 'test/num_examples': 10000, 'score': 58201.406227350235, 'total_duration': 60336.315509557724, 'accumulated_submission_time': 58201.406227350235, 'accumulated_eval_time': 2123.243331670761, 'accumulated_logging_time': 5.947777032852173, 'global_step': 170644, 'preemption_count': 0}), (172142, {'train/accuracy': 0.9163145422935486, 'train/loss': 0.4251190423965454, 'validation/accuracy': 0.7689799666404724, 'validation/loss': 1.0125985145568848, 'validation/num_examples': 50000, 'test/accuracy': 0.6457000374794006, 'test/loss': 1.6340036392211914, 'test/num_examples': 10000, 'score': 58711.40957093239, 'total_duration': 60863.64063549042, 'accumulated_submission_time': 58711.40957093239, 'accumulated_eval_time': 2140.4393548965454, 'accumulated_logging_time': 6.021910905838013, 'global_step': 172142, 'preemption_count': 0}), (173640, {'train/accuracy': 0.9162746667861938, 'train/loss': 0.41596558690071106, 'validation/accuracy': 0.7691599726676941, 'validation/loss': 1.0013891458511353, 'validation/num_examples': 50000, 'test/accuracy': 0.6492000222206116, 'test/loss': 1.6199817657470703, 'test/num_examples': 10000, 'score': 59221.40473794937, 'total_duration': 61390.933445453644, 'accumulated_submission_time': 59221.40473794937, 'accumulated_eval_time': 2157.631984949112, 'accumulated_logging_time': 6.077967643737793, 'global_step': 173640, 'preemption_count': 0}), (175138, {'train/accuracy': 0.9125677347183228, 'train/loss': 0.42805802822113037, 'validation/accuracy': 0.7706199884414673, 'validation/loss': 1.000607967376709, 'validation/num_examples': 50000, 'test/accuracy': 0.6484000086784363, 'test/loss': 1.6202576160430908, 'test/num_examples': 10000, 'score': 59731.5888364315, 'total_duration': 61918.106459379196, 'accumulated_submission_time': 59731.5888364315, 'accumulated_eval_time': 2174.5137605667114, 'accumulated_logging_time': 6.134857654571533, 'global_step': 175138, 'preemption_count': 0}), (176636, {'train/accuracy': 0.9162148833274841, 'train/loss': 0.4126388728618622, 'validation/accuracy': 0.7721799612045288, 'validation/loss': 0.9947059154510498, 'validation/num_examples': 50000, 'test/accuracy': 0.6476000547409058, 'test/loss': 1.617110252380371, 'test/num_examples': 10000, 'score': 60241.7090446949, 'total_duration': 62445.60426354408, 'accumulated_submission_time': 60241.7090446949, 'accumulated_eval_time': 2191.7853965759277, 'accumulated_logging_time': 6.190088748931885, 'global_step': 176636, 'preemption_count': 0}), (178133, {'train/accuracy': 0.9172711968421936, 'train/loss': 0.4106919467449188, 'validation/accuracy': 0.7718799710273743, 'validation/loss': 0.9955499172210693, 'validation/num_examples': 50000, 'test/accuracy': 0.6497000455856323, 'test/loss': 1.6151281595230103, 'test/num_examples': 10000, 'score': 60751.60144472122, 'total_duration': 62972.50779604912, 'accumulated_submission_time': 60751.60144472122, 'accumulated_eval_time': 2208.687886953354, 'accumulated_logging_time': 6.247087478637695, 'global_step': 178133, 'preemption_count': 0}), (179631, {'train/accuracy': 0.918965220451355, 'train/loss': 0.40768128633499146, 'validation/accuracy': 0.7737799882888794, 'validation/loss': 0.989608645439148, 'validation/num_examples': 50000, 'test/accuracy': 0.65010005235672, 'test/loss': 1.6078531742095947, 'test/num_examples': 10000, 'score': 61261.76955938339, 'total_duration': 63500.064401865005, 'accumulated_submission_time': 61261.76955938339, 'accumulated_eval_time': 2225.9638142585754, 'accumulated_logging_time': 6.308993816375732, 'global_step': 179631, 'preemption_count': 0}), (181128, {'train/accuracy': 0.9202008843421936, 'train/loss': 0.40204086899757385, 'validation/accuracy': 0.7730000019073486, 'validation/loss': 0.9929125308990479, 'validation/num_examples': 50000, 'test/accuracy': 0.6499000191688538, 'test/loss': 1.609398603439331, 'test/num_examples': 10000, 'score': 61771.71801686287, 'total_duration': 64027.38457632065, 'accumulated_submission_time': 61771.71801686287, 'accumulated_eval_time': 2243.2266731262207, 'accumulated_logging_time': 6.367881774902344, 'global_step': 181128, 'preemption_count': 0}), (182626, {'train/accuracy': 0.9197225570678711, 'train/loss': 0.40420398116111755, 'validation/accuracy': 0.7734400033950806, 'validation/loss': 0.9904934167861938, 'validation/num_examples': 50000, 'test/accuracy': 0.6509000062942505, 'test/loss': 1.606026530265808, 'test/num_examples': 10000, 'score': 62281.93049740791, 'total_duration': 64554.90435361862, 'accumulated_submission_time': 62281.93049740791, 'accumulated_eval_time': 2260.4189026355743, 'accumulated_logging_time': 6.43097186088562, 'global_step': 182626, 'preemption_count': 0}), (184124, {'train/accuracy': 0.9210976958274841, 'train/loss': 0.3999505639076233, 'validation/accuracy': 0.7736999988555908, 'validation/loss': 0.9918732643127441, 'validation/num_examples': 50000, 'test/accuracy': 0.6513000130653381, 'test/loss': 1.6081324815750122, 'test/num_examples': 10000, 'score': 62791.98581242561, 'total_duration': 65082.41243624687, 'accumulated_submission_time': 62791.98581242561, 'accumulated_eval_time': 2277.7590713500977, 'accumulated_logging_time': 6.492360353469849, 'global_step': 184124, 'preemption_count': 0})], 'global_step': 184760}
I0127 13:11:47.347551 139822745589568 submission_runner.py:586] Timing: 63008.28060030937
I0127 13:11:47.347623 139822745589568 submission_runner.py:588] Total number of evals: 124
I0127 13:11:47.347664 139822745589568 submission_runner.py:589] ====================
I0127 13:11:47.347709 139822745589568 submission_runner.py:542] Using RNG seed 916031063
I0127 13:11:47.348949 139822745589568 submission_runner.py:551] --- Tuning run 2/5 ---
I0127 13:11:47.349058 139822745589568 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_1/imagenet_resnet_jax/trial_2.
I0127 13:11:47.356238 139822745589568 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_1/imagenet_resnet_jax/trial_2/hparams.json.
I0127 13:11:47.357723 139822745589568 submission_runner.py:206] Initializing dataset.
I0127 13:11:47.369123 139822745589568 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0127 13:11:47.378827 139822745589568 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0127 13:11:47.595465 139822745589568 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0127 13:11:47.900973 139822745589568 submission_runner.py:213] Initializing model.
I0127 13:11:53.347067 139822745589568 submission_runner.py:255] Initializing optimizer.
I0127 13:11:53.717853 139822745589568 submission_runner.py:262] Initializing metrics bundle.
I0127 13:11:53.717997 139822745589568 submission_runner.py:280] Initializing checkpoint and logger.
I0127 13:11:53.818319 139822745589568 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_1/imagenet_resnet_jax/trial_2 with prefix checkpoint_
I0127 13:11:53.818424 139822745589568 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_1/imagenet_resnet_jax/trial_2/meta_data_0.json.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0127 13:12:05.453952 139822745589568 logger_utils.py:220] Unable to record git information. Continuing without it.
I0127 13:12:16.603064 139822745589568 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_1/imagenet_resnet_jax/trial_2/flags_0.json.
I0127 13:12:16.615355 139822745589568 submission_runner.py:314] Starting training loop.
I0127 13:12:48.879495 139656792352512 logging_writer.py:48] [0] global_step=0, grad_norm=0.5260404348373413, loss=6.928783416748047
I0127 13:12:48.888521 139822745589568 spec.py:321] Evaluating on the training split.
I0127 13:12:55.116141 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 13:13:03.810598 139822745589568 spec.py:349] Evaluating on the test split.
I0127 13:13:06.290725 139822745589568 submission_runner.py:408] Time since start: 49.68s, 	Step: 1, 	{'train/accuracy': 0.0010363520123064518, 'train/loss': 6.910816669464111, 'validation/accuracy': 0.001019999966956675, 'validation/loss': 6.91091251373291, 'validation/num_examples': 50000, 'test/accuracy': 0.0012000000569969416, 'test/loss': 6.910790920257568, 'test/num_examples': 10000, 'score': 32.273086071014404, 'total_duration': 49.67532181739807, 'accumulated_submission_time': 32.273086071014404, 'accumulated_eval_time': 17.40215516090393, 'accumulated_logging_time': 0}
I0127 13:13:06.301201 139656825923328 logging_writer.py:48] [1] accumulated_eval_time=17.402155, accumulated_logging_time=0, accumulated_submission_time=32.273086, global_step=1, preemption_count=0, score=32.273086, test/accuracy=0.001200, test/loss=6.910791, test/num_examples=10000, total_duration=49.675322, train/accuracy=0.001036, train/loss=6.910817, validation/accuracy=0.001020, validation/loss=6.910913, validation/num_examples=50000
I0127 13:13:40.427783 139656834316032 logging_writer.py:48] [100] global_step=100, grad_norm=0.5132727026939392, loss=6.897595405578613
I0127 13:14:14.618527 139656825923328 logging_writer.py:48] [200] global_step=200, grad_norm=0.5499270558357239, loss=6.8603363037109375
I0127 13:14:48.845908 139656834316032 logging_writer.py:48] [300] global_step=300, grad_norm=0.5763813257217407, loss=6.761523246765137
I0127 13:15:23.096183 139656825923328 logging_writer.py:48] [400] global_step=400, grad_norm=0.6288668513298035, loss=6.686418056488037
I0127 13:15:57.345682 139656834316032 logging_writer.py:48] [500] global_step=500, grad_norm=0.6432526111602783, loss=6.612051010131836
I0127 13:16:31.603704 139656825923328 logging_writer.py:48] [600] global_step=600, grad_norm=0.6557278633117676, loss=6.529646873474121
I0127 13:17:05.838849 139656834316032 logging_writer.py:48] [700] global_step=700, grad_norm=0.997707724571228, loss=6.436459541320801
I0127 13:17:40.088807 139656825923328 logging_writer.py:48] [800] global_step=800, grad_norm=1.2202073335647583, loss=6.32804012298584
I0127 13:18:14.349898 139656834316032 logging_writer.py:48] [900] global_step=900, grad_norm=1.18399178981781, loss=6.275004863739014
I0127 13:18:48.687059 139656825923328 logging_writer.py:48] [1000] global_step=1000, grad_norm=2.4436397552490234, loss=6.221038818359375
I0127 13:19:22.903546 139656834316032 logging_writer.py:48] [1100] global_step=1100, grad_norm=1.6328164339065552, loss=6.1786932945251465
I0127 13:19:57.146403 139656825923328 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.4590516090393066, loss=6.078861236572266
I0127 13:20:31.424967 139656834316032 logging_writer.py:48] [1300] global_step=1300, grad_norm=1.7700080871582031, loss=6.032663822174072
I0127 13:21:05.667872 139656825923328 logging_writer.py:48] [1400] global_step=1400, grad_norm=1.5763370990753174, loss=5.999239921569824
I0127 13:21:36.294911 139822745589568 spec.py:321] Evaluating on the training split.
I0127 13:21:42.555602 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 13:21:51.275516 139822745589568 spec.py:349] Evaluating on the test split.
I0127 13:21:53.735141 139822745589568 submission_runner.py:408] Time since start: 577.12s, 	Step: 1491, 	{'train/accuracy': 0.08322703838348389, 'train/loss': 5.25524377822876, 'validation/accuracy': 0.07711999863386154, 'validation/loss': 5.3119893074035645, 'validation/num_examples': 50000, 'test/accuracy': 0.05530000105500221, 'test/loss': 5.544482231140137, 'test/num_examples': 10000, 'score': 542.2085037231445, 'total_duration': 577.119710445404, 'accumulated_submission_time': 542.2085037231445, 'accumulated_eval_time': 34.842326641082764, 'accumulated_logging_time': 0.01926255226135254}
I0127 13:21:53.756383 139656800745216 logging_writer.py:48] [1491] accumulated_eval_time=34.842327, accumulated_logging_time=0.019263, accumulated_submission_time=542.208504, global_step=1491, preemption_count=0, score=542.208504, test/accuracy=0.055300, test/loss=5.544482, test/num_examples=10000, total_duration=577.119710, train/accuracy=0.083227, train/loss=5.255244, validation/accuracy=0.077120, validation/loss=5.311989, validation/num_examples=50000
I0127 13:21:57.187814 139656809137920 logging_writer.py:48] [1500] global_step=1500, grad_norm=2.3492813110351562, loss=5.939683437347412
I0127 13:22:31.405723 139656800745216 logging_writer.py:48] [1600] global_step=1600, grad_norm=3.098443031311035, loss=5.776700973510742
I0127 13:23:05.629491 139656809137920 logging_writer.py:48] [1700] global_step=1700, grad_norm=2.8277385234832764, loss=5.761767387390137
I0127 13:23:39.874923 139656800745216 logging_writer.py:48] [1800] global_step=1800, grad_norm=2.6708626747131348, loss=5.768612861633301
I0127 13:24:14.146605 139656809137920 logging_writer.py:48] [1900] global_step=1900, grad_norm=3.1922178268432617, loss=5.660062789916992
I0127 13:24:48.392707 139656800745216 logging_writer.py:48] [2000] global_step=2000, grad_norm=2.048149347305298, loss=5.711209297180176
I0127 13:25:22.714714 139656809137920 logging_writer.py:48] [2100] global_step=2100, grad_norm=2.80983567237854, loss=5.646862030029297
I0127 13:25:56.976152 139656800745216 logging_writer.py:48] [2200] global_step=2200, grad_norm=2.273000717163086, loss=5.599942207336426
I0127 13:26:31.222090 139656809137920 logging_writer.py:48] [2300] global_step=2300, grad_norm=3.192206621170044, loss=5.539929389953613
I0127 13:27:05.471772 139656800745216 logging_writer.py:48] [2400] global_step=2400, grad_norm=2.8404626846313477, loss=5.515602111816406
I0127 13:27:39.714146 139656809137920 logging_writer.py:48] [2500] global_step=2500, grad_norm=3.048316240310669, loss=5.406522750854492
I0127 13:28:13.978791 139656800745216 logging_writer.py:48] [2600] global_step=2600, grad_norm=2.7332074642181396, loss=5.4106292724609375
I0127 13:28:48.227887 139656809137920 logging_writer.py:48] [2700] global_step=2700, grad_norm=5.69890832901001, loss=5.405145645141602
I0127 13:29:22.498256 139656800745216 logging_writer.py:48] [2800] global_step=2800, grad_norm=2.421459913253784, loss=5.29909610748291
I0127 13:29:56.763895 139656809137920 logging_writer.py:48] [2900] global_step=2900, grad_norm=2.994582176208496, loss=5.333071708679199
I0127 13:30:23.979982 139822745589568 spec.py:321] Evaluating on the training split.
I0127 13:30:30.206010 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 13:30:38.988424 139822745589568 spec.py:349] Evaluating on the test split.
I0127 13:30:41.426283 139822745589568 submission_runner.py:408] Time since start: 1104.81s, 	Step: 2981, 	{'train/accuracy': 0.19897958636283875, 'train/loss': 4.21004581451416, 'validation/accuracy': 0.17909999191761017, 'validation/loss': 4.317634105682373, 'validation/num_examples': 50000, 'test/accuracy': 0.13120000064373016, 'test/loss': 4.706563472747803, 'test/num_examples': 10000, 'score': 1052.3712661266327, 'total_duration': 1104.8108565807343, 'accumulated_submission_time': 1052.3712661266327, 'accumulated_eval_time': 52.28858208656311, 'accumulated_logging_time': 0.05031228065490723}
I0127 13:30:41.443528 139656700098304 logging_writer.py:48] [2981] accumulated_eval_time=52.288582, accumulated_logging_time=0.050312, accumulated_submission_time=1052.371266, global_step=2981, preemption_count=0, score=1052.371266, test/accuracy=0.131200, test/loss=4.706563, test/num_examples=10000, total_duration=1104.810857, train/accuracy=0.198980, train/loss=4.210046, validation/accuracy=0.179100, validation/loss=4.317634, validation/num_examples=50000
I0127 13:30:48.295368 139656783959808 logging_writer.py:48] [3000] global_step=3000, grad_norm=4.614821434020996, loss=5.203945159912109
I0127 13:31:22.558026 139656700098304 logging_writer.py:48] [3100] global_step=3100, grad_norm=3.6489672660827637, loss=5.182989120483398
I0127 13:31:56.773695 139656783959808 logging_writer.py:48] [3200] global_step=3200, grad_norm=2.354602813720703, loss=5.193289279937744
I0127 13:32:31.015321 139656700098304 logging_writer.py:48] [3300] global_step=3300, grad_norm=3.2097036838531494, loss=5.0747504234313965
I0127 13:33:05.256832 139656783959808 logging_writer.py:48] [3400] global_step=3400, grad_norm=4.7958455085754395, loss=5.0713958740234375
I0127 13:33:39.530163 139656700098304 logging_writer.py:48] [3500] global_step=3500, grad_norm=3.3314671516418457, loss=5.042407512664795
I0127 13:34:13.798977 139656783959808 logging_writer.py:48] [3600] global_step=3600, grad_norm=3.629373788833618, loss=5.041146755218506
I0127 13:34:48.075342 139656700098304 logging_writer.py:48] [3700] global_step=3700, grad_norm=3.2358031272888184, loss=4.951326370239258
I0127 13:35:22.310520 139656783959808 logging_writer.py:48] [3800] global_step=3800, grad_norm=3.9724860191345215, loss=4.945929527282715
I0127 13:35:56.572374 139656700098304 logging_writer.py:48] [3900] global_step=3900, grad_norm=5.543783664703369, loss=4.930159568786621
I0127 13:36:30.831854 139656783959808 logging_writer.py:48] [4000] global_step=4000, grad_norm=4.977532863616943, loss=5.004155158996582
I0127 13:37:05.107191 139656700098304 logging_writer.py:48] [4100] global_step=4100, grad_norm=3.9627318382263184, loss=4.828800678253174
I0127 13:37:39.454432 139656783959808 logging_writer.py:48] [4200] global_step=4200, grad_norm=3.518937349319458, loss=4.879948616027832
I0127 13:38:13.724600 139656700098304 logging_writer.py:48] [4300] global_step=4300, grad_norm=4.853044509887695, loss=4.763123512268066
I0127 13:38:47.995798 139656783959808 logging_writer.py:48] [4400] global_step=4400, grad_norm=3.165189504623413, loss=4.686741352081299
I0127 13:39:11.429263 139822745589568 spec.py:321] Evaluating on the training split.
I0127 13:39:18.302335 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 13:39:27.115258 139822745589568 spec.py:349] Evaluating on the test split.
I0127 13:39:29.586721 139822745589568 submission_runner.py:408] Time since start: 1632.97s, 	Step: 4470, 	{'train/accuracy': 0.2876076102256775, 'train/loss': 3.585117816925049, 'validation/accuracy': 0.26396000385284424, 'validation/loss': 3.7350914478302, 'validation/num_examples': 50000, 'test/accuracy': 0.19200000166893005, 'test/loss': 4.257549285888672, 'test/num_examples': 10000, 'score': 1562.2961611747742, 'total_duration': 1632.9713141918182, 'accumulated_submission_time': 1562.2961611747742, 'accumulated_eval_time': 70.44600534439087, 'accumulated_logging_time': 0.07889699935913086}
I0127 13:39:29.604793 139656700098304 logging_writer.py:48] [4470] accumulated_eval_time=70.446005, accumulated_logging_time=0.078897, accumulated_submission_time=1562.296161, global_step=4470, preemption_count=0, score=1562.296161, test/accuracy=0.192000, test/loss=4.257549, test/num_examples=10000, total_duration=1632.971314, train/accuracy=0.287608, train/loss=3.585118, validation/accuracy=0.263960, validation/loss=3.735091, validation/num_examples=50000
I0127 13:39:40.228290 139656809137920 logging_writer.py:48] [4500] global_step=4500, grad_norm=3.9264638423919678, loss=4.749931335449219
I0127 13:40:14.455468 139656700098304 logging_writer.py:48] [4600] global_step=4600, grad_norm=4.1325225830078125, loss=4.694253921508789
I0127 13:40:48.707237 139656809137920 logging_writer.py:48] [4700] global_step=4700, grad_norm=3.6340816020965576, loss=4.588937282562256
I0127 13:41:22.923982 139656700098304 logging_writer.py:48] [4800] global_step=4800, grad_norm=4.4641642570495605, loss=4.814888954162598
I0127 13:41:57.177653 139656809137920 logging_writer.py:48] [4900] global_step=4900, grad_norm=4.161268711090088, loss=4.568187713623047
I0127 13:42:31.419586 139656700098304 logging_writer.py:48] [5000] global_step=5000, grad_norm=3.695040702819824, loss=4.623300075531006
I0127 13:43:05.678267 139656809137920 logging_writer.py:48] [5100] global_step=5100, grad_norm=5.304862976074219, loss=4.502310752868652
I0127 13:43:39.958949 139656700098304 logging_writer.py:48] [5200] global_step=5200, grad_norm=4.2696685791015625, loss=4.44597053527832
I0127 13:44:14.281023 139656809137920 logging_writer.py:48] [5300] global_step=5300, grad_norm=4.38092565536499, loss=4.475561141967773
I0127 13:44:48.538295 139656700098304 logging_writer.py:48] [5400] global_step=5400, grad_norm=4.822208404541016, loss=4.445606231689453
I0127 13:45:22.785447 139656809137920 logging_writer.py:48] [5500] global_step=5500, grad_norm=2.4932093620300293, loss=4.411398410797119
I0127 13:45:57.057373 139656700098304 logging_writer.py:48] [5600] global_step=5600, grad_norm=3.0648224353790283, loss=4.505697250366211
I0127 13:46:31.326944 139656809137920 logging_writer.py:48] [5700] global_step=5700, grad_norm=3.647437334060669, loss=4.35003662109375
I0127 13:47:05.583532 139656700098304 logging_writer.py:48] [5800] global_step=5800, grad_norm=2.26932954788208, loss=4.386188983917236
I0127 13:47:39.845320 139656809137920 logging_writer.py:48] [5900] global_step=5900, grad_norm=2.615665912628174, loss=4.428244590759277
I0127 13:47:59.840968 139822745589568 spec.py:321] Evaluating on the training split.
I0127 13:48:06.271939 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 13:48:14.963986 139822745589568 spec.py:349] Evaluating on the test split.
I0127 13:48:17.385129 139822745589568 submission_runner.py:408] Time since start: 2160.77s, 	Step: 5960, 	{'train/accuracy': 0.38807398080825806, 'train/loss': 2.992854356765747, 'validation/accuracy': 0.3614400029182434, 'validation/loss': 3.130856990814209, 'validation/num_examples': 50000, 'test/accuracy': 0.27470001578330994, 'test/loss': 3.713503360748291, 'test/num_examples': 10000, 'score': 2072.4718992710114, 'total_duration': 2160.7697203159332, 'accumulated_submission_time': 2072.4718992710114, 'accumulated_eval_time': 87.99014830589294, 'accumulated_logging_time': 0.10644721984863281}
I0127 13:48:17.404611 139656792352512 logging_writer.py:48] [5960] accumulated_eval_time=87.990148, accumulated_logging_time=0.106447, accumulated_submission_time=2072.471899, global_step=5960, preemption_count=0, score=2072.471899, test/accuracy=0.274700, test/loss=3.713503, test/num_examples=10000, total_duration=2160.769720, train/accuracy=0.388074, train/loss=2.992854, validation/accuracy=0.361440, validation/loss=3.130857, validation/num_examples=50000
I0127 13:48:31.442687 139656800745216 logging_writer.py:48] [6000] global_step=6000, grad_norm=3.394573926925659, loss=4.369294166564941
I0127 13:49:05.640268 139656792352512 logging_writer.py:48] [6100] global_step=6100, grad_norm=3.1815602779388428, loss=4.3764801025390625
I0127 13:49:39.862044 139656800745216 logging_writer.py:48] [6200] global_step=6200, grad_norm=4.397026062011719, loss=4.369484901428223
I0127 13:50:14.201299 139656792352512 logging_writer.py:48] [6300] global_step=6300, grad_norm=2.6565535068511963, loss=4.272408485412598
I0127 13:50:48.463193 139656800745216 logging_writer.py:48] [6400] global_step=6400, grad_norm=2.6842594146728516, loss=4.327356338500977
I0127 13:51:22.704074 139656792352512 logging_writer.py:48] [6500] global_step=6500, grad_norm=3.697829246520996, loss=4.270969867706299
I0127 13:51:56.948612 139656800745216 logging_writer.py:48] [6600] global_step=6600, grad_norm=2.790440559387207, loss=4.291597366333008
I0127 13:52:31.210400 139656792352512 logging_writer.py:48] [6700] global_step=6700, grad_norm=3.03544020652771, loss=4.208102703094482
I0127 13:53:05.434095 139656800745216 logging_writer.py:48] [6800] global_step=6800, grad_norm=2.918612241744995, loss=4.144194602966309
I0127 13:53:39.674453 139656792352512 logging_writer.py:48] [6900] global_step=6900, grad_norm=2.5946781635284424, loss=4.215635299682617
I0127 13:54:13.899974 139656800745216 logging_writer.py:48] [7000] global_step=7000, grad_norm=5.69196081161499, loss=4.197232246398926
I0127 13:54:48.164226 139656792352512 logging_writer.py:48] [7100] global_step=7100, grad_norm=3.084275245666504, loss=4.192256927490234
I0127 13:55:22.386107 139656800745216 logging_writer.py:48] [7200] global_step=7200, grad_norm=2.7176365852355957, loss=4.191255569458008
I0127 13:55:56.631064 139656792352512 logging_writer.py:48] [7300] global_step=7300, grad_norm=2.40932035446167, loss=4.197519779205322
I0127 13:56:30.932546 139656800745216 logging_writer.py:48] [7400] global_step=7400, grad_norm=2.6255667209625244, loss=4.092894077301025
I0127 13:56:47.496865 139822745589568 spec.py:321] Evaluating on the training split.
I0127 13:56:53.761727 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 13:57:02.487017 139822745589568 spec.py:349] Evaluating on the test split.
I0127 13:57:05.152934 139822745589568 submission_runner.py:408] Time since start: 2688.54s, 	Step: 7450, 	{'train/accuracy': 0.45238760113716125, 'train/loss': 2.617628335952759, 'validation/accuracy': 0.4184799790382385, 'validation/loss': 2.7827277183532715, 'validation/num_examples': 50000, 'test/accuracy': 0.31640002131462097, 'test/loss': 3.388169765472412, 'test/num_examples': 10000, 'score': 2582.5007004737854, 'total_duration': 2688.537534236908, 'accumulated_submission_time': 2582.5007004737854, 'accumulated_eval_time': 105.64619159698486, 'accumulated_logging_time': 0.1386871337890625}
I0127 13:57:05.171908 139656783959808 logging_writer.py:48] [7450] accumulated_eval_time=105.646192, accumulated_logging_time=0.138687, accumulated_submission_time=2582.500700, global_step=7450, preemption_count=0, score=2582.500700, test/accuracy=0.316400, test/loss=3.388170, test/num_examples=10000, total_duration=2688.537534, train/accuracy=0.452388, train/loss=2.617628, validation/accuracy=0.418480, validation/loss=2.782728, validation/num_examples=50000
I0127 13:57:22.593397 139656809137920 logging_writer.py:48] [7500] global_step=7500, grad_norm=2.7610421180725098, loss=4.1496806144714355
I0127 13:57:56.753334 139656783959808 logging_writer.py:48] [7600] global_step=7600, grad_norm=3.301668882369995, loss=4.1818318367004395
I0127 13:58:30.964170 139656809137920 logging_writer.py:48] [7700] global_step=7700, grad_norm=2.919328451156616, loss=4.162764549255371
I0127 13:59:05.160848 139656783959808 logging_writer.py:48] [7800] global_step=7800, grad_norm=2.3164258003234863, loss=4.0146050453186035
I0127 13:59:39.396466 139656809137920 logging_writer.py:48] [7900] global_step=7900, grad_norm=2.852562427520752, loss=4.145022392272949
I0127 14:00:13.634922 139656783959808 logging_writer.py:48] [8000] global_step=8000, grad_norm=2.821713924407959, loss=4.068336486816406
I0127 14:00:47.851880 139656809137920 logging_writer.py:48] [8100] global_step=8100, grad_norm=2.8109543323516846, loss=4.037350177764893
I0127 14:01:22.083813 139656783959808 logging_writer.py:48] [8200] global_step=8200, grad_norm=3.7176713943481445, loss=4.073974609375
I0127 14:01:56.315171 139656809137920 logging_writer.py:48] [8300] global_step=8300, grad_norm=2.578059196472168, loss=4.027705192565918
I0127 14:02:30.512294 139656783959808 logging_writer.py:48] [8400] global_step=8400, grad_norm=2.381072521209717, loss=3.969026565551758
I0127 14:03:04.769706 139656809137920 logging_writer.py:48] [8500] global_step=8500, grad_norm=2.3440394401550293, loss=4.076456069946289
I0127 14:03:38.970724 139656783959808 logging_writer.py:48] [8600] global_step=8600, grad_norm=2.2941935062408447, loss=4.023224830627441
I0127 14:04:13.190074 139656809137920 logging_writer.py:48] [8700] global_step=8700, grad_norm=2.534900665283203, loss=3.9213955402374268
I0127 14:04:47.390266 139656783959808 logging_writer.py:48] [8800] global_step=8800, grad_norm=2.6973061561584473, loss=4.001725673675537
I0127 14:05:21.645885 139656809137920 logging_writer.py:48] [8900] global_step=8900, grad_norm=2.360788106918335, loss=3.973994731903076
I0127 14:05:35.492026 139822745589568 spec.py:321] Evaluating on the training split.
I0127 14:05:41.702798 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 14:05:50.428585 139822745589568 spec.py:349] Evaluating on the test split.
I0127 14:05:52.822211 139822745589568 submission_runner.py:408] Time since start: 3216.21s, 	Step: 8942, 	{'train/accuracy': 0.5526944994926453, 'train/loss': 2.124098539352417, 'validation/accuracy': 0.4767199754714966, 'validation/loss': 2.479666233062744, 'validation/num_examples': 50000, 'test/accuracy': 0.3680000305175781, 'test/loss': 3.117877244949341, 'test/num_examples': 10000, 'score': 3092.7613253593445, 'total_duration': 3216.206754922867, 'accumulated_submission_time': 3092.7613253593445, 'accumulated_eval_time': 122.9762909412384, 'accumulated_logging_time': 0.16669344902038574}
I0127 14:05:52.841214 139656834316032 logging_writer.py:48] [8942] accumulated_eval_time=122.976291, accumulated_logging_time=0.166693, accumulated_submission_time=3092.761325, global_step=8942, preemption_count=0, score=3092.761325, test/accuracy=0.368000, test/loss=3.117877, test/num_examples=10000, total_duration=3216.206755, train/accuracy=0.552694, train/loss=2.124099, validation/accuracy=0.476720, validation/loss=2.479666, validation/num_examples=50000
I0127 14:06:12.993746 139658730145536 logging_writer.py:48] [9000] global_step=9000, grad_norm=1.8861031532287598, loss=3.986846446990967
I0127 14:06:47.145812 139656834316032 logging_writer.py:48] [9100] global_step=9100, grad_norm=2.7171361446380615, loss=3.9276821613311768
I0127 14:07:21.362143 139658730145536 logging_writer.py:48] [9200] global_step=9200, grad_norm=2.0662152767181396, loss=3.9314093589782715
I0127 14:07:55.582927 139656834316032 logging_writer.py:48] [9300] global_step=9300, grad_norm=2.1298787593841553, loss=3.903815746307373
I0127 14:08:29.809457 139658730145536 logging_writer.py:48] [9400] global_step=9400, grad_norm=2.1367883682250977, loss=3.8518640995025635
I0127 14:09:04.030740 139656834316032 logging_writer.py:48] [9500] global_step=9500, grad_norm=1.8775678873062134, loss=3.9588141441345215
I0127 14:09:38.325245 139658730145536 logging_writer.py:48] [9600] global_step=9600, grad_norm=1.7085258960723877, loss=3.896555185317993
I0127 14:10:12.533004 139656834316032 logging_writer.py:48] [9700] global_step=9700, grad_norm=1.950567364692688, loss=3.8708977699279785
I0127 14:10:46.748310 139658730145536 logging_writer.py:48] [9800] global_step=9800, grad_norm=1.6441973447799683, loss=3.8577938079833984
I0127 14:11:20.960976 139656834316032 logging_writer.py:48] [9900] global_step=9900, grad_norm=3.3862621784210205, loss=3.898024082183838
I0127 14:11:55.174717 139658730145536 logging_writer.py:48] [10000] global_step=10000, grad_norm=2.908263921737671, loss=3.903822898864746
I0127 14:12:29.372011 139656834316032 logging_writer.py:48] [10100] global_step=10100, grad_norm=2.3785648345947266, loss=3.8313300609588623
I0127 14:13:03.601513 139658730145536 logging_writer.py:48] [10200] global_step=10200, grad_norm=2.2572669982910156, loss=3.822772979736328
I0127 14:13:37.805689 139656834316032 logging_writer.py:48] [10300] global_step=10300, grad_norm=1.5767171382904053, loss=3.7632923126220703
I0127 14:14:11.979502 139658730145536 logging_writer.py:48] [10400] global_step=10400, grad_norm=1.6474746465682983, loss=3.81524395942688
I0127 14:14:23.069448 139822745589568 spec.py:321] Evaluating on the training split.
I0127 14:14:29.430890 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 14:14:37.963411 139822745589568 spec.py:349] Evaluating on the test split.
I0127 14:14:40.428809 139822745589568 submission_runner.py:408] Time since start: 3743.81s, 	Step: 10434, 	{'train/accuracy': 0.5689373016357422, 'train/loss': 2.0224695205688477, 'validation/accuracy': 0.522159993648529, 'validation/loss': 2.2517900466918945, 'validation/num_examples': 50000, 'test/accuracy': 0.4036000072956085, 'test/loss': 2.9123005867004395, 'test/num_examples': 10000, 'score': 3602.929986476898, 'total_duration': 3743.8133985996246, 'accumulated_submission_time': 3602.929986476898, 'accumulated_eval_time': 140.33562183380127, 'accumulated_logging_time': 0.19498658180236816}
I0127 14:14:40.448349 139656800745216 logging_writer.py:48] [10434] accumulated_eval_time=140.335622, accumulated_logging_time=0.194987, accumulated_submission_time=3602.929986, global_step=10434, preemption_count=0, score=3602.929986, test/accuracy=0.403600, test/loss=2.912301, test/num_examples=10000, total_duration=3743.813399, train/accuracy=0.568937, train/loss=2.022470, validation/accuracy=0.522160, validation/loss=2.251790, validation/num_examples=50000
I0127 14:15:03.317001 139656809137920 logging_writer.py:48] [10500] global_step=10500, grad_norm=2.0135271549224854, loss=3.7800025939941406
I0127 14:15:37.658759 139656800745216 logging_writer.py:48] [10600] global_step=10600, grad_norm=2.5066192150115967, loss=3.742924690246582
I0127 14:16:11.833417 139656809137920 logging_writer.py:48] [10700] global_step=10700, grad_norm=2.548977851867676, loss=3.7523927688598633
I0127 14:16:46.015573 139656800745216 logging_writer.py:48] [10800] global_step=10800, grad_norm=1.5452916622161865, loss=3.772097110748291
I0127 14:17:20.211590 139656809137920 logging_writer.py:48] [10900] global_step=10900, grad_norm=1.676933765411377, loss=3.7083730697631836
I0127 14:17:54.382623 139656800745216 logging_writer.py:48] [11000] global_step=11000, grad_norm=1.2551499605178833, loss=3.6809659004211426
I0127 14:18:28.587029 139656809137920 logging_writer.py:48] [11100] global_step=11100, grad_norm=1.78423011302948, loss=3.799408197402954
I0127 14:19:02.743091 139656800745216 logging_writer.py:48] [11200] global_step=11200, grad_norm=1.7971210479736328, loss=3.773310422897339
I0127 14:19:36.947234 139656809137920 logging_writer.py:48] [11300] global_step=11300, grad_norm=1.9642707109451294, loss=3.711430072784424
I0127 14:20:11.127243 139656800745216 logging_writer.py:48] [11400] global_step=11400, grad_norm=2.0475432872772217, loss=3.708865165710449
I0127 14:20:45.282770 139656809137920 logging_writer.py:48] [11500] global_step=11500, grad_norm=2.20268177986145, loss=3.7105934619903564
I0127 14:21:19.507584 139656800745216 logging_writer.py:48] [11600] global_step=11600, grad_norm=2.0440845489501953, loss=3.7170770168304443
I0127 14:21:53.798150 139656809137920 logging_writer.py:48] [11700] global_step=11700, grad_norm=2.7116756439208984, loss=3.5993194580078125
I0127 14:22:28.001555 139656800745216 logging_writer.py:48] [11800] global_step=11800, grad_norm=1.7417134046554565, loss=3.6514344215393066
I0127 14:23:02.178513 139656809137920 logging_writer.py:48] [11900] global_step=11900, grad_norm=1.4012058973312378, loss=3.654392957687378
I0127 14:23:10.532814 139822745589568 spec.py:321] Evaluating on the training split.
I0127 14:23:16.737614 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 14:23:25.256047 139822745589568 spec.py:349] Evaluating on the test split.
I0127 14:23:27.795783 139822745589568 submission_runner.py:408] Time since start: 4271.18s, 	Step: 11926, 	{'train/accuracy': 0.5983737111091614, 'train/loss': 1.9081037044525146, 'validation/accuracy': 0.5486199855804443, 'validation/loss': 2.138624906539917, 'validation/num_examples': 50000, 'test/accuracy': 0.4262000322341919, 'test/loss': 2.801759719848633, 'test/num_examples': 10000, 'score': 4112.95587015152, 'total_duration': 4271.180375099182, 'accumulated_submission_time': 4112.95587015152, 'accumulated_eval_time': 157.598552942276, 'accumulated_logging_time': 0.22365856170654297}
I0127 14:23:27.815083 139656817530624 logging_writer.py:48] [11926] accumulated_eval_time=157.598553, accumulated_logging_time=0.223659, accumulated_submission_time=4112.955870, global_step=11926, preemption_count=0, score=4112.955870, test/accuracy=0.426200, test/loss=2.801760, test/num_examples=10000, total_duration=4271.180375, train/accuracy=0.598374, train/loss=1.908104, validation/accuracy=0.548620, validation/loss=2.138625, validation/num_examples=50000
I0127 14:23:53.421266 139656825923328 logging_writer.py:48] [12000] global_step=12000, grad_norm=1.9777753353118896, loss=3.686513900756836
I0127 14:24:27.576808 139656817530624 logging_writer.py:48] [12100] global_step=12100, grad_norm=2.1708292961120605, loss=3.6948695182800293
I0127 14:25:01.754469 139656825923328 logging_writer.py:48] [12200] global_step=12200, grad_norm=1.8644548654556274, loss=3.692368507385254
I0127 14:25:35.967217 139656817530624 logging_writer.py:48] [12300] global_step=12300, grad_norm=1.4750901460647583, loss=3.607011318206787
I0127 14:26:10.154648 139656825923328 logging_writer.py:48] [12400] global_step=12400, grad_norm=1.5242966413497925, loss=3.646465301513672
I0127 14:26:44.355467 139656817530624 logging_writer.py:48] [12500] global_step=12500, grad_norm=2.0239672660827637, loss=3.6548352241516113
I0127 14:27:18.565122 139656825923328 logging_writer.py:48] [12600] global_step=12600, grad_norm=1.4339596033096313, loss=3.707909107208252
I0127 14:27:52.747931 139656817530624 logging_writer.py:48] [12700] global_step=12700, grad_norm=1.5371718406677246, loss=3.590320110321045
I0127 14:28:26.997078 139656825923328 logging_writer.py:48] [12800] global_step=12800, grad_norm=1.8614614009857178, loss=3.6393778324127197
I0127 14:29:01.177911 139656817530624 logging_writer.py:48] [12900] global_step=12900, grad_norm=1.8664847612380981, loss=3.634951114654541
I0127 14:29:35.358993 139656825923328 logging_writer.py:48] [13000] global_step=13000, grad_norm=1.5827912092208862, loss=3.6198978424072266
I0127 14:30:09.537096 139656817530624 logging_writer.py:48] [13100] global_step=13100, grad_norm=1.7732857465744019, loss=3.674342632293701
I0127 14:30:43.719978 139656825923328 logging_writer.py:48] [13200] global_step=13200, grad_norm=2.4016358852386475, loss=3.6599037647247314
I0127 14:31:17.886115 139656817530624 logging_writer.py:48] [13300] global_step=13300, grad_norm=1.557605266571045, loss=3.6040687561035156
I0127 14:31:52.081582 139656825923328 logging_writer.py:48] [13400] global_step=13400, grad_norm=1.5519107580184937, loss=3.7022452354431152
I0127 14:31:58.024091 139822745589568 spec.py:321] Evaluating on the training split.
I0127 14:32:04.368089 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 14:32:12.993592 139822745589568 spec.py:349] Evaluating on the test split.
I0127 14:32:15.480121 139822745589568 submission_runner.py:408] Time since start: 4798.86s, 	Step: 13419, 	{'train/accuracy': 0.6163105964660645, 'train/loss': 1.7988837957382202, 'validation/accuracy': 0.5671799778938293, 'validation/loss': 2.038691282272339, 'validation/num_examples': 50000, 'test/accuracy': 0.4473000168800354, 'test/loss': 2.6989235877990723, 'test/num_examples': 10000, 'score': 4623.103399991989, 'total_duration': 4798.86471414566, 'accumulated_submission_time': 4623.103399991989, 'accumulated_eval_time': 175.05454540252686, 'accumulated_logging_time': 0.25371837615966797}
I0127 14:32:15.500373 139656783959808 logging_writer.py:48] [13419] accumulated_eval_time=175.054545, accumulated_logging_time=0.253718, accumulated_submission_time=4623.103400, global_step=13419, preemption_count=0, score=4623.103400, test/accuracy=0.447300, test/loss=2.698924, test/num_examples=10000, total_duration=4798.864714, train/accuracy=0.616311, train/loss=1.798884, validation/accuracy=0.567180, validation/loss=2.038691, validation/num_examples=50000
I0127 14:32:43.517075 139656792352512 logging_writer.py:48] [13500] global_step=13500, grad_norm=1.9962000846862793, loss=3.6561577320098877
I0127 14:33:17.676464 139656783959808 logging_writer.py:48] [13600] global_step=13600, grad_norm=1.8292577266693115, loss=3.602017402648926
I0127 14:33:51.825598 139656792352512 logging_writer.py:48] [13700] global_step=13700, grad_norm=1.8049037456512451, loss=3.534407138824463
I0127 14:34:26.146134 139656783959808 logging_writer.py:48] [13800] global_step=13800, grad_norm=1.714444875717163, loss=3.6551170349121094
I0127 14:35:00.329441 139656792352512 logging_writer.py:48] [13900] global_step=13900, grad_norm=1.9235635995864868, loss=3.5352365970611572
I0127 14:35:34.486863 139656783959808 logging_writer.py:48] [14000] global_step=14000, grad_norm=1.313801646232605, loss=3.6563146114349365
I0127 14:36:08.656204 139656792352512 logging_writer.py:48] [14100] global_step=14100, grad_norm=1.6726884841918945, loss=3.515786647796631
I0127 14:36:42.809486 139656783959808 logging_writer.py:48] [14200] global_step=14200, grad_norm=1.4524465799331665, loss=3.539785385131836
I0127 14:37:16.972027 139656792352512 logging_writer.py:48] [14300] global_step=14300, grad_norm=1.5512129068374634, loss=3.5666725635528564
I0127 14:37:51.163347 139656783959808 logging_writer.py:48] [14400] global_step=14400, grad_norm=1.4467054605484009, loss=3.534240245819092
I0127 14:38:25.355520 139656792352512 logging_writer.py:48] [14500] global_step=14500, grad_norm=1.8467347621917725, loss=3.575437545776367
I0127 14:38:59.538525 139656783959808 logging_writer.py:48] [14600] global_step=14600, grad_norm=1.6871942281723022, loss=3.5281152725219727
I0127 14:39:33.708687 139656792352512 logging_writer.py:48] [14700] global_step=14700, grad_norm=1.7528247833251953, loss=3.587367534637451
I0127 14:40:07.924996 139656783959808 logging_writer.py:48] [14800] global_step=14800, grad_norm=1.4072810411453247, loss=3.519462823867798
I0127 14:40:42.213285 139656792352512 logging_writer.py:48] [14900] global_step=14900, grad_norm=1.6648222208023071, loss=3.4844682216644287
I0127 14:40:45.769719 139822745589568 spec.py:321] Evaluating on the training split.
I0127 14:40:51.954584 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 14:41:00.563264 139822745589568 spec.py:349] Evaluating on the test split.
I0127 14:41:03.052203 139822745589568 submission_runner.py:408] Time since start: 5326.44s, 	Step: 14912, 	{'train/accuracy': 0.6312180757522583, 'train/loss': 1.7623796463012695, 'validation/accuracy': 0.5798799991607666, 'validation/loss': 1.994551658630371, 'validation/num_examples': 50000, 'test/accuracy': 0.45900002121925354, 'test/loss': 2.626842975616455, 'test/num_examples': 10000, 'score': 5133.311586380005, 'total_duration': 5326.436780452728, 'accumulated_submission_time': 5133.311586380005, 'accumulated_eval_time': 192.3369791507721, 'accumulated_logging_time': 0.2829594612121582}
I0127 14:41:03.077571 139656783959808 logging_writer.py:48] [14912] accumulated_eval_time=192.336979, accumulated_logging_time=0.282959, accumulated_submission_time=5133.311586, global_step=14912, preemption_count=0, score=5133.311586, test/accuracy=0.459000, test/loss=2.626843, test/num_examples=10000, total_duration=5326.436780, train/accuracy=0.631218, train/loss=1.762380, validation/accuracy=0.579880, validation/loss=1.994552, validation/num_examples=50000
I0127 14:41:33.471028 139656817530624 logging_writer.py:48] [15000] global_step=15000, grad_norm=2.0832109451293945, loss=3.482032060623169
I0127 14:42:07.595667 139656783959808 logging_writer.py:48] [15100] global_step=15100, grad_norm=1.660980224609375, loss=3.5560526847839355
I0127 14:42:41.771722 139656817530624 logging_writer.py:48] [15200] global_step=15200, grad_norm=2.0711092948913574, loss=3.5746328830718994
I0127 14:43:15.940180 139656783959808 logging_writer.py:48] [15300] global_step=15300, grad_norm=1.3657968044281006, loss=3.4808216094970703
I0127 14:43:50.120087 139656817530624 logging_writer.py:48] [15400] global_step=15400, grad_norm=1.8903616666793823, loss=3.5278139114379883
I0127 14:44:24.279906 139656783959808 logging_writer.py:48] [15500] global_step=15500, grad_norm=1.4435220956802368, loss=3.493591785430908
I0127 14:44:58.450766 139656817530624 logging_writer.py:48] [15600] global_step=15600, grad_norm=1.3558968305587769, loss=3.589468240737915
I0127 14:45:32.614062 139656783959808 logging_writer.py:48] [15700] global_step=15700, grad_norm=1.6818888187408447, loss=3.5987837314605713
I0127 14:46:06.849464 139656817530624 logging_writer.py:48] [15800] global_step=15800, grad_norm=1.1881011724472046, loss=3.4386003017425537
I0127 14:46:41.034076 139656783959808 logging_writer.py:48] [15900] global_step=15900, grad_norm=1.2972290515899658, loss=3.4884841442108154
I0127 14:47:15.297013 139656817530624 logging_writer.py:48] [16000] global_step=16000, grad_norm=1.3290033340454102, loss=3.5597035884857178
I0127 14:47:49.480127 139656783959808 logging_writer.py:48] [16100] global_step=16100, grad_norm=1.2100082635879517, loss=3.410749912261963
I0127 14:48:23.675548 139656817530624 logging_writer.py:48] [16200] global_step=16200, grad_norm=2.132685661315918, loss=3.4872331619262695
I0127 14:48:57.844594 139656783959808 logging_writer.py:48] [16300] global_step=16300, grad_norm=1.3004655838012695, loss=3.397536277770996
I0127 14:49:32.030134 139656817530624 logging_writer.py:48] [16400] global_step=16400, grad_norm=1.4321553707122803, loss=3.453362226486206
I0127 14:49:33.194858 139822745589568 spec.py:321] Evaluating on the training split.
I0127 14:49:39.429248 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 14:49:47.996828 139822745589568 spec.py:349] Evaluating on the test split.
I0127 14:49:50.464503 139822745589568 submission_runner.py:408] Time since start: 5853.85s, 	Step: 16405, 	{'train/accuracy': 0.6322146058082581, 'train/loss': 1.7425355911254883, 'validation/accuracy': 0.5858799815177917, 'validation/loss': 1.9565809965133667, 'validation/num_examples': 50000, 'test/accuracy': 0.4595000147819519, 'test/loss': 2.652688980102539, 'test/num_examples': 10000, 'score': 5643.3654227256775, 'total_duration': 5853.849092960358, 'accumulated_submission_time': 5643.3654227256775, 'accumulated_eval_time': 209.60658407211304, 'accumulated_logging_time': 0.32151174545288086}
I0127 14:49:50.488296 139656809137920 logging_writer.py:48] [16405] accumulated_eval_time=209.606584, accumulated_logging_time=0.321512, accumulated_submission_time=5643.365423, global_step=16405, preemption_count=0, score=5643.365423, test/accuracy=0.459500, test/loss=2.652689, test/num_examples=10000, total_duration=5853.849093, train/accuracy=0.632215, train/loss=1.742536, validation/accuracy=0.585880, validation/loss=1.956581, validation/num_examples=50000
I0127 14:50:23.254956 139656834316032 logging_writer.py:48] [16500] global_step=16500, grad_norm=1.7109750509262085, loss=3.4629573822021484
I0127 14:50:57.402261 139656809137920 logging_writer.py:48] [16600] global_step=16600, grad_norm=1.6099798679351807, loss=3.4315035343170166
I0127 14:51:31.541065 139656834316032 logging_writer.py:48] [16700] global_step=16700, grad_norm=1.639878511428833, loss=3.510727643966675
I0127 14:52:05.741996 139656809137920 logging_writer.py:48] [16800] global_step=16800, grad_norm=1.3786060810089111, loss=3.450871706008911
I0127 14:52:39.883449 139656834316032 logging_writer.py:48] [16900] global_step=16900, grad_norm=1.5608302354812622, loss=3.475581169128418
I0127 14:53:14.112101 139656809137920 logging_writer.py:48] [17000] global_step=17000, grad_norm=1.2835731506347656, loss=3.437990665435791
I0127 14:53:48.251100 139656834316032 logging_writer.py:48] [17100] global_step=17100, grad_norm=1.465757131576538, loss=3.4555299282073975
I0127 14:54:22.399505 139656809137920 logging_writer.py:48] [17200] global_step=17200, grad_norm=1.601436972618103, loss=3.426173210144043
I0127 14:54:56.540279 139656834316032 logging_writer.py:48] [17300] global_step=17300, grad_norm=1.3000892400741577, loss=3.403167247772217
I0127 14:55:30.714106 139656809137920 logging_writer.py:48] [17400] global_step=17400, grad_norm=1.9597474336624146, loss=3.408236503601074
I0127 14:56:04.868980 139656834316032 logging_writer.py:48] [17500] global_step=17500, grad_norm=1.377403736114502, loss=3.44217586517334
I0127 14:56:39.016508 139656809137920 logging_writer.py:48] [17600] global_step=17600, grad_norm=1.3855170011520386, loss=3.4748294353485107
I0127 14:57:13.162723 139656834316032 logging_writer.py:48] [17700] global_step=17700, grad_norm=1.6447854042053223, loss=3.522618293762207
I0127 14:57:47.321490 139656809137920 logging_writer.py:48] [17800] global_step=17800, grad_norm=1.5599106550216675, loss=3.4605472087860107
I0127 14:58:20.590323 139822745589568 spec.py:321] Evaluating on the training split.
I0127 14:58:26.996500 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 14:58:35.673031 139822745589568 spec.py:349] Evaluating on the test split.
I0127 14:58:38.128635 139822745589568 submission_runner.py:408] Time since start: 6381.51s, 	Step: 17899, 	{'train/accuracy': 0.6486168503761292, 'train/loss': 1.6495143175125122, 'validation/accuracy': 0.5984199643135071, 'validation/loss': 1.8850512504577637, 'validation/num_examples': 50000, 'test/accuracy': 0.4756000339984894, 'test/loss': 2.5317037105560303, 'test/num_examples': 10000, 'score': 6153.40634727478, 'total_duration': 6381.513226747513, 'accumulated_submission_time': 6153.40634727478, 'accumulated_eval_time': 227.14486122131348, 'accumulated_logging_time': 0.35701680183410645}
I0127 14:58:38.151526 139656792352512 logging_writer.py:48] [17899] accumulated_eval_time=227.144861, accumulated_logging_time=0.357017, accumulated_submission_time=6153.406347, global_step=17899, preemption_count=0, score=6153.406347, test/accuracy=0.475600, test/loss=2.531704, test/num_examples=10000, total_duration=6381.513227, train/accuracy=0.648617, train/loss=1.649514, validation/accuracy=0.598420, validation/loss=1.885051, validation/num_examples=50000
I0127 14:58:38.841078 139656800745216 logging_writer.py:48] [17900] global_step=17900, grad_norm=1.5565714836120605, loss=3.4791135787963867
I0127 14:59:12.998898 139656792352512 logging_writer.py:48] [18000] global_step=18000, grad_norm=1.645308017730713, loss=3.534755229949951
I0127 14:59:47.203013 139656800745216 logging_writer.py:48] [18100] global_step=18100, grad_norm=1.500544548034668, loss=3.4053211212158203
I0127 15:00:21.354642 139656792352512 logging_writer.py:48] [18200] global_step=18200, grad_norm=1.606542944908142, loss=3.4217021465301514
I0127 15:00:55.537795 139656800745216 logging_writer.py:48] [18300] global_step=18300, grad_norm=1.380440354347229, loss=3.4131524562835693
I0127 15:01:29.718673 139656792352512 logging_writer.py:48] [18400] global_step=18400, grad_norm=1.4195438623428345, loss=3.4729604721069336
I0127 15:02:03.914438 139656800745216 logging_writer.py:48] [18500] global_step=18500, grad_norm=1.1540014743804932, loss=3.380411386489868
I0127 15:02:38.083076 139656792352512 logging_writer.py:48] [18600] global_step=18600, grad_norm=1.1430730819702148, loss=3.41453218460083
I0127 15:03:12.234234 139656800745216 logging_writer.py:48] [18700] global_step=18700, grad_norm=1.4373174905776978, loss=3.382422685623169
I0127 15:03:46.402636 139656792352512 logging_writer.py:48] [18800] global_step=18800, grad_norm=1.5070195198059082, loss=3.45133376121521
I0127 15:04:20.536083 139656800745216 logging_writer.py:48] [18900] global_step=18900, grad_norm=1.2451577186584473, loss=3.4212868213653564
I0127 15:04:54.701487 139656792352512 logging_writer.py:48] [19000] global_step=19000, grad_norm=1.7549337148666382, loss=3.5016045570373535
I0127 15:05:28.863228 139656800745216 logging_writer.py:48] [19100] global_step=19100, grad_norm=1.4828450679779053, loss=3.457864284515381
I0127 15:06:03.103281 139656792352512 logging_writer.py:48] [19200] global_step=19200, grad_norm=1.7790526151657104, loss=3.4966623783111572
I0127 15:06:37.290594 139656800745216 logging_writer.py:48] [19300] global_step=19300, grad_norm=1.669355034828186, loss=3.4000344276428223
I0127 15:07:08.194038 139822745589568 spec.py:321] Evaluating on the training split.
I0127 15:07:14.440678 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 15:07:23.300859 139822745589568 spec.py:349] Evaluating on the test split.
I0127 15:07:25.698673 139822745589568 submission_runner.py:408] Time since start: 6909.08s, 	Step: 19392, 	{'train/accuracy': 0.6943359375, 'train/loss': 1.483161449432373, 'validation/accuracy': 0.615619957447052, 'validation/loss': 1.8391530513763428, 'validation/num_examples': 50000, 'test/accuracy': 0.49080002307891846, 'test/loss': 2.465847969055176, 'test/num_examples': 10000, 'score': 6663.387640237808, 'total_duration': 6909.0832579135895, 'accumulated_submission_time': 6663.387640237808, 'accumulated_eval_time': 244.6494562625885, 'accumulated_logging_time': 0.38909482955932617}
I0127 15:07:25.719979 139656825923328 logging_writer.py:48] [19392] accumulated_eval_time=244.649456, accumulated_logging_time=0.389095, accumulated_submission_time=6663.387640, global_step=19392, preemption_count=0, score=6663.387640, test/accuracy=0.490800, test/loss=2.465848, test/num_examples=10000, total_duration=6909.083258, train/accuracy=0.694336, train/loss=1.483161, validation/accuracy=0.615620, validation/loss=1.839153, validation/num_examples=50000
I0127 15:07:28.790763 139656834316032 logging_writer.py:48] [19400] global_step=19400, grad_norm=1.1973689794540405, loss=3.5160694122314453
I0127 15:08:02.912706 139656825923328 logging_writer.py:48] [19500] global_step=19500, grad_norm=1.8456295728683472, loss=3.447873592376709
I0127 15:08:37.064094 139656834316032 logging_writer.py:48] [19600] global_step=19600, grad_norm=1.490852952003479, loss=3.4147162437438965
I0127 15:09:11.197430 139656825923328 logging_writer.py:48] [19700] global_step=19700, grad_norm=1.5207738876342773, loss=3.4405839443206787
I0127 15:09:45.357704 139656834316032 logging_writer.py:48] [19800] global_step=19800, grad_norm=1.3347734212875366, loss=3.388953685760498
I0127 15:10:19.527952 139656825923328 logging_writer.py:48] [19900] global_step=19900, grad_norm=1.351510763168335, loss=3.46248197555542
I0127 15:10:53.686850 139656834316032 logging_writer.py:48] [20000] global_step=20000, grad_norm=1.2648532390594482, loss=3.3849194049835205
I0127 15:11:27.865039 139656825923328 logging_writer.py:48] [20100] global_step=20100, grad_norm=1.2695720195770264, loss=3.3529083728790283
I0127 15:12:02.214485 139656834316032 logging_writer.py:48] [20200] global_step=20200, grad_norm=1.5281058549880981, loss=3.368218421936035
I0127 15:12:36.384080 139656825923328 logging_writer.py:48] [20300] global_step=20300, grad_norm=1.320264458656311, loss=3.474421501159668
I0127 15:13:10.541188 139656834316032 logging_writer.py:48] [20400] global_step=20400, grad_norm=1.1321638822555542, loss=3.3514933586120605
I0127 15:13:44.718979 139656825923328 logging_writer.py:48] [20500] global_step=20500, grad_norm=1.547481894493103, loss=3.373764991760254
I0127 15:14:18.887651 139656834316032 logging_writer.py:48] [20600] global_step=20600, grad_norm=1.3315120935440063, loss=3.3285927772521973
I0127 15:14:53.081970 139656825923328 logging_writer.py:48] [20700] global_step=20700, grad_norm=1.288142204284668, loss=3.3789596557617188
I0127 15:15:27.245473 139656834316032 logging_writer.py:48] [20800] global_step=20800, grad_norm=1.2493773698806763, loss=3.364640474319458
I0127 15:15:55.776229 139822745589568 spec.py:321] Evaluating on the training split.
I0127 15:16:02.171777 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 15:16:10.870306 139822745589568 spec.py:349] Evaluating on the test split.
I0127 15:16:13.359522 139822745589568 submission_runner.py:408] Time since start: 7436.74s, 	Step: 20885, 	{'train/accuracy': 0.6927216053009033, 'train/loss': 1.4884588718414307, 'validation/accuracy': 0.6250799894332886, 'validation/loss': 1.790726661682129, 'validation/num_examples': 50000, 'test/accuracy': 0.49330002069473267, 'test/loss': 2.453991174697876, 'test/num_examples': 10000, 'score': 7173.381569385529, 'total_duration': 7436.744100093842, 'accumulated_submission_time': 7173.381569385529, 'accumulated_eval_time': 262.23272013664246, 'accumulated_logging_time': 0.42079997062683105}
I0127 15:16:13.384286 139656700098304 logging_writer.py:48] [20885] accumulated_eval_time=262.232720, accumulated_logging_time=0.420800, accumulated_submission_time=7173.381569, global_step=20885, preemption_count=0, score=7173.381569, test/accuracy=0.493300, test/loss=2.453991, test/num_examples=10000, total_duration=7436.744100, train/accuracy=0.692722, train/loss=1.488459, validation/accuracy=0.625080, validation/loss=1.790727, validation/num_examples=50000
I0127 15:16:18.841403 139656783959808 logging_writer.py:48] [20900] global_step=20900, grad_norm=1.3334370851516724, loss=3.489345073699951
I0127 15:16:52.980892 139656700098304 logging_writer.py:48] [21000] global_step=21000, grad_norm=1.2202258110046387, loss=3.4113616943359375
I0127 15:17:27.133888 139656783959808 logging_writer.py:48] [21100] global_step=21100, grad_norm=1.793773889541626, loss=3.437591791152954
I0127 15:18:01.290274 139656700098304 logging_writer.py:48] [21200] global_step=21200, grad_norm=1.2602837085723877, loss=3.406386613845825
I0127 15:18:35.557979 139656783959808 logging_writer.py:48] [21300] global_step=21300, grad_norm=1.5180096626281738, loss=3.4094347953796387
I0127 15:19:09.721215 139656700098304 logging_writer.py:48] [21400] global_step=21400, grad_norm=1.174302101135254, loss=3.2939276695251465
I0127 15:19:43.853678 139656783959808 logging_writer.py:48] [21500] global_step=21500, grad_norm=1.129695177078247, loss=3.39725399017334
I0127 15:20:18.016062 139656700098304 logging_writer.py:48] [21600] global_step=21600, grad_norm=1.2411655187606812, loss=3.420696973800659
I0127 15:20:52.191947 139656783959808 logging_writer.py:48] [21700] global_step=21700, grad_norm=1.2127928733825684, loss=3.3168070316314697
I0127 15:21:26.377858 139656700098304 logging_writer.py:48] [21800] global_step=21800, grad_norm=1.607245922088623, loss=3.3778812885284424
I0127 15:22:00.539898 139656783959808 logging_writer.py:48] [21900] global_step=21900, grad_norm=1.168168544769287, loss=3.4353504180908203
I0127 15:22:34.723648 139656700098304 logging_writer.py:48] [22000] global_step=22000, grad_norm=1.1818382740020752, loss=3.350968360900879
I0127 15:23:08.869390 139656783959808 logging_writer.py:48] [22100] global_step=22100, grad_norm=1.274039626121521, loss=3.3304898738861084
I0127 15:23:43.050530 139656700098304 logging_writer.py:48] [22200] global_step=22200, grad_norm=1.5177191495895386, loss=3.308241128921509
I0127 15:24:17.201524 139656783959808 logging_writer.py:48] [22300] global_step=22300, grad_norm=1.6417571306228638, loss=3.3634121417999268
I0127 15:24:43.474249 139822745589568 spec.py:321] Evaluating on the training split.
I0127 15:24:49.764713 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 15:24:58.513545 139822745589568 spec.py:349] Evaluating on the test split.
I0127 15:25:00.934070 139822745589568 submission_runner.py:408] Time since start: 7964.32s, 	Step: 22378, 	{'train/accuracy': 0.6850087642669678, 'train/loss': 1.4967600107192993, 'validation/accuracy': 0.6200599670410156, 'validation/loss': 1.7901036739349365, 'validation/num_examples': 50000, 'test/accuracy': 0.4984000325202942, 'test/loss': 2.4333972930908203, 'test/num_examples': 10000, 'score': 7683.411033153534, 'total_duration': 7964.318654060364, 'accumulated_submission_time': 7683.411033153534, 'accumulated_eval_time': 279.69251894950867, 'accumulated_logging_time': 0.4559054374694824}
I0127 15:25:00.955309 139656834316032 logging_writer.py:48] [22378] accumulated_eval_time=279.692519, accumulated_logging_time=0.455905, accumulated_submission_time=7683.411033, global_step=22378, preemption_count=0, score=7683.411033, test/accuracy=0.498400, test/loss=2.433397, test/num_examples=10000, total_duration=7964.318654, train/accuracy=0.685009, train/loss=1.496760, validation/accuracy=0.620060, validation/loss=1.790104, validation/num_examples=50000
I0127 15:25:08.795265 139658730145536 logging_writer.py:48] [22400] global_step=22400, grad_norm=1.3992921113967896, loss=3.3340210914611816
I0127 15:25:42.897883 139656834316032 logging_writer.py:48] [22500] global_step=22500, grad_norm=1.3769116401672363, loss=3.373396873474121
I0127 15:26:17.023559 139658730145536 logging_writer.py:48] [22600] global_step=22600, grad_norm=1.3297115564346313, loss=3.313530921936035
I0127 15:26:51.158736 139656834316032 logging_writer.py:48] [22700] global_step=22700, grad_norm=1.3780076503753662, loss=3.3138532638549805
I0127 15:27:25.292399 139658730145536 logging_writer.py:48] [22800] global_step=22800, grad_norm=1.454576849937439, loss=3.339381694793701
I0127 15:27:59.443855 139656834316032 logging_writer.py:48] [22900] global_step=22900, grad_norm=1.2737696170806885, loss=3.2963500022888184
I0127 15:28:33.588007 139658730145536 logging_writer.py:48] [23000] global_step=23000, grad_norm=1.397861361503601, loss=3.354233503341675
I0127 15:29:07.728959 139656834316032 logging_writer.py:48] [23100] global_step=23100, grad_norm=1.351709246635437, loss=3.367344379425049
I0127 15:29:41.908098 139658730145536 logging_writer.py:48] [23200] global_step=23200, grad_norm=1.3530818223953247, loss=3.3489773273468018
I0127 15:30:16.024890 139656834316032 logging_writer.py:48] [23300] global_step=23300, grad_norm=1.2355926036834717, loss=3.332277297973633
I0127 15:30:50.292339 139658730145536 logging_writer.py:48] [23400] global_step=23400, grad_norm=1.754528522491455, loss=3.4098353385925293
I0127 15:31:24.444893 139656834316032 logging_writer.py:48] [23500] global_step=23500, grad_norm=1.344741702079773, loss=3.372746229171753
I0127 15:31:58.605751 139658730145536 logging_writer.py:48] [23600] global_step=23600, grad_norm=1.275875449180603, loss=3.3659398555755615
I0127 15:32:32.771890 139656834316032 logging_writer.py:48] [23700] global_step=23700, grad_norm=1.5715440511703491, loss=3.3303306102752686
I0127 15:33:06.956517 139658730145536 logging_writer.py:48] [23800] global_step=23800, grad_norm=1.0625749826431274, loss=3.3217806816101074
I0127 15:33:31.005664 139822745589568 spec.py:321] Evaluating on the training split.
I0127 15:33:37.261469 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 15:33:45.826563 139822745589568 spec.py:349] Evaluating on the test split.
I0127 15:33:48.300725 139822745589568 submission_runner.py:408] Time since start: 8491.69s, 	Step: 23872, 	{'train/accuracy': 0.6826769709587097, 'train/loss': 1.5350451469421387, 'validation/accuracy': 0.6202600002288818, 'validation/loss': 1.8089746236801147, 'validation/num_examples': 50000, 'test/accuracy': 0.4937000274658203, 'test/loss': 2.4656965732574463, 'test/num_examples': 10000, 'score': 8193.398950099945, 'total_duration': 8491.685319185257, 'accumulated_submission_time': 8193.398950099945, 'accumulated_eval_time': 296.98755836486816, 'accumulated_logging_time': 0.4871697425842285}
I0127 15:33:48.322175 139656792352512 logging_writer.py:48] [23872] accumulated_eval_time=296.987558, accumulated_logging_time=0.487170, accumulated_submission_time=8193.398950, global_step=23872, preemption_count=0, score=8193.398950, test/accuracy=0.493700, test/loss=2.465697, test/num_examples=10000, total_duration=8491.685319, train/accuracy=0.682677, train/loss=1.535045, validation/accuracy=0.620260, validation/loss=1.808975, validation/num_examples=50000
I0127 15:33:58.223113 139656800745216 logging_writer.py:48] [23900] global_step=23900, grad_norm=1.2980395555496216, loss=3.304302930831909
I0127 15:34:32.335893 139656792352512 logging_writer.py:48] [24000] global_step=24000, grad_norm=1.4061942100524902, loss=3.4148406982421875
I0127 15:35:06.435972 139656800745216 logging_writer.py:48] [24100] global_step=24100, grad_norm=1.6145341396331787, loss=3.3642096519470215
I0127 15:35:40.571208 139656792352512 logging_writer.py:48] [24200] global_step=24200, grad_norm=1.2524949312210083, loss=3.3477606773376465
I0127 15:36:14.685097 139656800745216 logging_writer.py:48] [24300] global_step=24300, grad_norm=1.224152684211731, loss=3.340736150741577
I0127 15:36:48.840700 139656792352512 logging_writer.py:48] [24400] global_step=24400, grad_norm=1.2690635919570923, loss=3.3499467372894287
I0127 15:37:23.068120 139656800745216 logging_writer.py:48] [24500] global_step=24500, grad_norm=1.2204917669296265, loss=3.302280902862549
I0127 15:37:57.227822 139656792352512 logging_writer.py:48] [24600] global_step=24600, grad_norm=1.256430983543396, loss=3.309513568878174
I0127 15:38:31.349736 139656800745216 logging_writer.py:48] [24700] global_step=24700, grad_norm=1.107781171798706, loss=3.273869276046753
I0127 15:39:05.494943 139656792352512 logging_writer.py:48] [24800] global_step=24800, grad_norm=1.1801085472106934, loss=3.3032352924346924
I0127 15:39:39.627209 139656800745216 logging_writer.py:48] [24900] global_step=24900, grad_norm=1.2083386182785034, loss=3.4435362815856934
I0127 15:40:13.773494 139656792352512 logging_writer.py:48] [25000] global_step=25000, grad_norm=1.458791732788086, loss=3.308133125305176
I0127 15:40:47.901798 139656800745216 logging_writer.py:48] [25100] global_step=25100, grad_norm=1.4729994535446167, loss=3.306713104248047
I0127 15:41:22.048724 139656792352512 logging_writer.py:48] [25200] global_step=25200, grad_norm=1.2363280057907104, loss=3.250985860824585
I0127 15:41:56.173693 139656800745216 logging_writer.py:48] [25300] global_step=25300, grad_norm=1.3341528177261353, loss=3.309267997741699
I0127 15:42:18.501016 139822745589568 spec.py:321] Evaluating on the training split.
I0127 15:42:24.713646 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 15:42:33.628737 139822745589568 spec.py:349] Evaluating on the test split.
I0127 15:42:36.041551 139822745589568 submission_runner.py:408] Time since start: 9019.43s, 	Step: 25367, 	{'train/accuracy': 0.7011120915412903, 'train/loss': 1.4206559658050537, 'validation/accuracy': 0.6384599804878235, 'validation/loss': 1.7001821994781494, 'validation/num_examples': 50000, 'test/accuracy': 0.5014000535011292, 'test/loss': 2.3606183528900146, 'test/num_examples': 10000, 'score': 8703.517453432083, 'total_duration': 9019.426125764847, 'accumulated_submission_time': 8703.517453432083, 'accumulated_eval_time': 314.5280523300171, 'accumulated_logging_time': 0.5178220272064209}
I0127 15:42:36.064523 139656792352512 logging_writer.py:48] [25367] accumulated_eval_time=314.528052, accumulated_logging_time=0.517822, accumulated_submission_time=8703.517453, global_step=25367, preemption_count=0, score=8703.517453, test/accuracy=0.501400, test/loss=2.360618, test/num_examples=10000, total_duration=9019.426126, train/accuracy=0.701112, train/loss=1.420656, validation/accuracy=0.638460, validation/loss=1.700182, validation/num_examples=50000
I0127 15:42:47.669171 139656825923328 logging_writer.py:48] [25400] global_step=25400, grad_norm=1.6003319025039673, loss=3.416139841079712
I0127 15:43:21.835406 139656792352512 logging_writer.py:48] [25500] global_step=25500, grad_norm=1.6252416372299194, loss=3.29647159576416
I0127 15:43:55.946945 139656825923328 logging_writer.py:48] [25600] global_step=25600, grad_norm=1.5501766204833984, loss=3.3241775035858154
I0127 15:44:30.091561 139656792352512 logging_writer.py:48] [25700] global_step=25700, grad_norm=1.1895077228546143, loss=3.35996675491333
I0127 15:45:04.222335 139656825923328 logging_writer.py:48] [25800] global_step=25800, grad_norm=1.153140664100647, loss=3.299531936645508
I0127 15:45:38.369472 139656792352512 logging_writer.py:48] [25900] global_step=25900, grad_norm=1.2547374963760376, loss=3.410922050476074
I0127 15:46:12.505355 139656825923328 logging_writer.py:48] [26000] global_step=26000, grad_norm=1.2106982469558716, loss=3.234564781188965
I0127 15:46:46.650841 139656792352512 logging_writer.py:48] [26100] global_step=26100, grad_norm=1.5124038457870483, loss=3.3217689990997314
I0127 15:47:20.807462 139656825923328 logging_writer.py:48] [26200] global_step=26200, grad_norm=1.4461979866027832, loss=3.3494009971618652
I0127 15:47:54.983756 139656792352512 logging_writer.py:48] [26300] global_step=26300, grad_norm=1.2508254051208496, loss=3.267251491546631
I0127 15:48:29.129304 139656825923328 logging_writer.py:48] [26400] global_step=26400, grad_norm=1.2821755409240723, loss=3.3725218772888184
I0127 15:49:03.288554 139656792352512 logging_writer.py:48] [26500] global_step=26500, grad_norm=1.1149402856826782, loss=3.2779436111450195
I0127 15:49:37.497893 139656825923328 logging_writer.py:48] [26600] global_step=26600, grad_norm=1.3435269594192505, loss=3.2295312881469727
I0127 15:50:11.639349 139656792352512 logging_writer.py:48] [26700] global_step=26700, grad_norm=1.4522852897644043, loss=3.2396812438964844
I0127 15:50:45.764804 139656825923328 logging_writer.py:48] [26800] global_step=26800, grad_norm=1.302322506904602, loss=3.3313393592834473
I0127 15:51:06.383725 139822745589568 spec.py:321] Evaluating on the training split.
I0127 15:51:12.622071 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 15:51:21.416173 139822745589568 spec.py:349] Evaluating on the test split.
I0127 15:51:23.893256 139822745589568 submission_runner.py:408] Time since start: 9547.28s, 	Step: 26862, 	{'train/accuracy': 0.6947743892669678, 'train/loss': 1.4827384948730469, 'validation/accuracy': 0.6357600092887878, 'validation/loss': 1.7430270910263062, 'validation/num_examples': 50000, 'test/accuracy': 0.5125000476837158, 'test/loss': 2.392122507095337, 'test/num_examples': 10000, 'score': 9213.776574373245, 'total_duration': 9547.277846574783, 'accumulated_submission_time': 9213.776574373245, 'accumulated_eval_time': 332.03754591941833, 'accumulated_logging_time': 0.5506632328033447}
I0127 15:51:23.916608 139656800745216 logging_writer.py:48] [26862] accumulated_eval_time=332.037546, accumulated_logging_time=0.550663, accumulated_submission_time=9213.776574, global_step=26862, preemption_count=0, score=9213.776574, test/accuracy=0.512500, test/loss=2.392123, test/num_examples=10000, total_duration=9547.277847, train/accuracy=0.694774, train/loss=1.482738, validation/accuracy=0.635760, validation/loss=1.743027, validation/num_examples=50000
I0127 15:51:37.240215 139656809137920 logging_writer.py:48] [26900] global_step=26900, grad_norm=1.2104517221450806, loss=3.2771031856536865
I0127 15:52:11.364154 139656800745216 logging_writer.py:48] [27000] global_step=27000, grad_norm=1.1364368200302124, loss=3.4041709899902344
I0127 15:52:45.494303 139656809137920 logging_writer.py:48] [27100] global_step=27100, grad_norm=1.2725765705108643, loss=3.4014108180999756
I0127 15:53:19.597906 139656800745216 logging_writer.py:48] [27200] global_step=27200, grad_norm=1.1795316934585571, loss=3.3314146995544434
I0127 15:53:53.739425 139656809137920 logging_writer.py:48] [27300] global_step=27300, grad_norm=1.7179430723190308, loss=3.2426633834838867
I0127 15:54:27.877599 139656800745216 logging_writer.py:48] [27400] global_step=27400, grad_norm=1.3259382247924805, loss=3.231356143951416
I0127 15:55:02.013636 139656809137920 logging_writer.py:48] [27500] global_step=27500, grad_norm=1.446845531463623, loss=3.264267683029175
I0127 15:55:36.132263 139656800745216 logging_writer.py:48] [27600] global_step=27600, grad_norm=1.3662385940551758, loss=3.2753584384918213
I0127 15:56:10.319027 139656809137920 logging_writer.py:48] [27700] global_step=27700, grad_norm=1.2922866344451904, loss=3.2561376094818115
I0127 15:56:44.487585 139656800745216 logging_writer.py:48] [27800] global_step=27800, grad_norm=1.3418842554092407, loss=3.2818498611450195
I0127 15:57:18.631149 139656809137920 logging_writer.py:48] [27900] global_step=27900, grad_norm=1.4042823314666748, loss=3.3420298099517822
I0127 15:57:52.792386 139656800745216 logging_writer.py:48] [28000] global_step=28000, grad_norm=1.3739111423492432, loss=3.216641902923584
I0127 15:58:26.927252 139656809137920 logging_writer.py:48] [28100] global_step=28100, grad_norm=1.3775080442428589, loss=3.2852303981781006
I0127 15:59:01.108496 139656800745216 logging_writer.py:48] [28200] global_step=28200, grad_norm=1.4766323566436768, loss=3.2555699348449707
I0127 15:59:35.243140 139656809137920 logging_writer.py:48] [28300] global_step=28300, grad_norm=1.8297734260559082, loss=3.2144722938537598
I0127 15:59:54.180627 139822745589568 spec.py:321] Evaluating on the training split.
I0127 16:00:00.352590 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 16:00:09.116700 139822745589568 spec.py:349] Evaluating on the test split.
I0127 16:00:11.596225 139822745589568 submission_runner.py:408] Time since start: 10074.98s, 	Step: 28357, 	{'train/accuracy': 0.7403140664100647, 'train/loss': 1.251942753791809, 'validation/accuracy': 0.6389600038528442, 'validation/loss': 1.6875686645507812, 'validation/num_examples': 50000, 'test/accuracy': 0.515500009059906, 'test/loss': 2.3387906551361084, 'test/num_examples': 10000, 'score': 9723.97894001007, 'total_duration': 10074.980818033218, 'accumulated_submission_time': 9723.97894001007, 'accumulated_eval_time': 349.45311164855957, 'accumulated_logging_time': 0.5828866958618164}
I0127 16:00:11.620619 139656783959808 logging_writer.py:48] [28357] accumulated_eval_time=349.453112, accumulated_logging_time=0.582887, accumulated_submission_time=9723.978940, global_step=28357, preemption_count=0, score=9723.978940, test/accuracy=0.515500, test/loss=2.338791, test/num_examples=10000, total_duration=10074.980818, train/accuracy=0.740314, train/loss=1.251943, validation/accuracy=0.638960, validation/loss=1.687569, validation/num_examples=50000
I0127 16:00:26.627072 139656792352512 logging_writer.py:48] [28400] global_step=28400, grad_norm=1.5668917894363403, loss=3.2980458736419678
I0127 16:01:00.726313 139656783959808 logging_writer.py:48] [28500] global_step=28500, grad_norm=1.6098365783691406, loss=3.256988048553467
I0127 16:01:34.858062 139656792352512 logging_writer.py:48] [28600] global_step=28600, grad_norm=1.2691905498504639, loss=3.2522387504577637
I0127 16:02:09.173158 139656783959808 logging_writer.py:48] [28700] global_step=28700, grad_norm=1.3791669607162476, loss=3.2692699432373047
I0127 16:02:43.332378 139656792352512 logging_writer.py:48] [28800] global_step=28800, grad_norm=1.177744746208191, loss=3.2115917205810547
I0127 16:03:17.499250 139656783959808 logging_writer.py:48] [28900] global_step=28900, grad_norm=1.4920860528945923, loss=3.346322536468506
I0127 16:03:51.646605 139656792352512 logging_writer.py:48] [29000] global_step=29000, grad_norm=1.5103360414505005, loss=3.3406870365142822
I0127 16:04:25.808520 139656783959808 logging_writer.py:48] [29100] global_step=29100, grad_norm=1.199141502380371, loss=3.162562608718872
I0127 16:04:59.970315 139656792352512 logging_writer.py:48] [29200] global_step=29200, grad_norm=1.176306962966919, loss=3.254303216934204
I0127 16:05:34.112275 139656783959808 logging_writer.py:48] [29300] global_step=29300, grad_norm=1.1453644037246704, loss=3.2742068767547607
I0127 16:06:08.251429 139656792352512 logging_writer.py:48] [29400] global_step=29400, grad_norm=1.3958269357681274, loss=3.292656421661377
I0127 16:06:42.398495 139656783959808 logging_writer.py:48] [29500] global_step=29500, grad_norm=1.259394884109497, loss=3.329251289367676
I0127 16:07:16.575790 139656792352512 logging_writer.py:48] [29600] global_step=29600, grad_norm=1.2669514417648315, loss=3.285081624984741
I0127 16:07:50.738922 139656783959808 logging_writer.py:48] [29700] global_step=29700, grad_norm=1.3322778940200806, loss=3.2022721767425537
I0127 16:08:24.974522 139656792352512 logging_writer.py:48] [29800] global_step=29800, grad_norm=1.48466157913208, loss=3.2532753944396973
I0127 16:08:41.861485 139822745589568 spec.py:321] Evaluating on the training split.
I0127 16:08:48.080970 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 16:08:56.817669 139822745589568 spec.py:349] Evaluating on the test split.
I0127 16:08:59.318078 139822745589568 submission_runner.py:408] Time since start: 10602.70s, 	Step: 29851, 	{'train/accuracy': 0.725984513759613, 'train/loss': 1.2846310138702393, 'validation/accuracy': 0.6454600095748901, 'validation/loss': 1.647182583808899, 'validation/num_examples': 50000, 'test/accuracy': 0.5200999975204468, 'test/loss': 2.3046329021453857, 'test/num_examples': 10000, 'score': 10234.157279968262, 'total_duration': 10602.70266866684, 'accumulated_submission_time': 10234.157279968262, 'accumulated_eval_time': 366.90966606140137, 'accumulated_logging_time': 0.6168107986450195}
I0127 16:08:59.340528 139656700098304 logging_writer.py:48] [29851] accumulated_eval_time=366.909666, accumulated_logging_time=0.616811, accumulated_submission_time=10234.157280, global_step=29851, preemption_count=0, score=10234.157280, test/accuracy=0.520100, test/loss=2.304633, test/num_examples=10000, total_duration=10602.702669, train/accuracy=0.725985, train/loss=1.284631, validation/accuracy=0.645460, validation/loss=1.647183, validation/num_examples=50000
I0127 16:09:16.393997 139656783959808 logging_writer.py:48] [29900] global_step=29900, grad_norm=1.6181883811950684, loss=3.3226308822631836
I0127 16:09:50.493442 139656700098304 logging_writer.py:48] [30000] global_step=30000, grad_norm=1.302101731300354, loss=3.2379322052001953
I0127 16:10:24.625655 139656783959808 logging_writer.py:48] [30100] global_step=30100, grad_norm=1.4342026710510254, loss=3.2418577671051025
I0127 16:10:58.776609 139656700098304 logging_writer.py:48] [30200] global_step=30200, grad_norm=1.2653979063034058, loss=3.2866196632385254
I0127 16:11:32.955901 139656783959808 logging_writer.py:48] [30300] global_step=30300, grad_norm=1.3253589868545532, loss=3.176820755004883
I0127 16:12:07.105935 139656700098304 logging_writer.py:48] [30400] global_step=30400, grad_norm=1.2618303298950195, loss=3.282711982727051
I0127 16:12:41.268939 139656783959808 logging_writer.py:48] [30500] global_step=30500, grad_norm=1.2444878816604614, loss=3.254225254058838
I0127 16:13:15.440068 139656700098304 logging_writer.py:48] [30600] global_step=30600, grad_norm=1.409279704093933, loss=3.298722982406616
I0127 16:13:49.610384 139656783959808 logging_writer.py:48] [30700] global_step=30700, grad_norm=1.3076742887496948, loss=3.291977643966675
I0127 16:14:23.741227 139656700098304 logging_writer.py:48] [30800] global_step=30800, grad_norm=1.531012773513794, loss=3.2804646492004395
I0127 16:14:57.971537 139656783959808 logging_writer.py:48] [30900] global_step=30900, grad_norm=1.2382643222808838, loss=3.1901164054870605
I0127 16:15:32.105775 139656700098304 logging_writer.py:48] [31000] global_step=31000, grad_norm=1.5332436561584473, loss=3.2490947246551514
I0127 16:16:06.268778 139656783959808 logging_writer.py:48] [31100] global_step=31100, grad_norm=1.6563562154769897, loss=3.276366710662842
I0127 16:16:40.426065 139656700098304 logging_writer.py:48] [31200] global_step=31200, grad_norm=1.346393346786499, loss=3.2349801063537598
I0127 16:17:14.599794 139656783959808 logging_writer.py:48] [31300] global_step=31300, grad_norm=1.278771996498108, loss=3.296576499938965
I0127 16:17:29.439077 139822745589568 spec.py:321] Evaluating on the training split.
I0127 16:17:35.729789 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 16:17:44.394946 139822745589568 spec.py:349] Evaluating on the test split.
I0127 16:17:46.885348 139822745589568 submission_runner.py:408] Time since start: 11130.27s, 	Step: 31345, 	{'train/accuracy': 0.7231544852256775, 'train/loss': 1.3217514753341675, 'validation/accuracy': 0.6491000056266785, 'validation/loss': 1.644545078277588, 'validation/num_examples': 50000, 'test/accuracy': 0.5267000198364258, 'test/loss': 2.2946465015411377, 'test/num_examples': 10000, 'score': 10744.193604707718, 'total_duration': 11130.2699239254, 'accumulated_submission_time': 10744.193604707718, 'accumulated_eval_time': 384.35588550567627, 'accumulated_logging_time': 0.6501708030700684}
I0127 16:17:46.908573 139656783959808 logging_writer.py:48] [31345] accumulated_eval_time=384.355886, accumulated_logging_time=0.650171, accumulated_submission_time=10744.193605, global_step=31345, preemption_count=0, score=10744.193605, test/accuracy=0.526700, test/loss=2.294647, test/num_examples=10000, total_duration=11130.269924, train/accuracy=0.723154, train/loss=1.321751, validation/accuracy=0.649100, validation/loss=1.644545, validation/num_examples=50000
I0127 16:18:06.018544 139656809137920 logging_writer.py:48] [31400] global_step=31400, grad_norm=1.2589538097381592, loss=3.2727818489074707
I0127 16:18:40.103445 139656783959808 logging_writer.py:48] [31500] global_step=31500, grad_norm=1.4031484127044678, loss=3.3323116302490234
I0127 16:19:14.226526 139656809137920 logging_writer.py:48] [31600] global_step=31600, grad_norm=1.3198745250701904, loss=3.3227221965789795
I0127 16:19:48.356201 139656783959808 logging_writer.py:48] [31700] global_step=31700, grad_norm=1.369520902633667, loss=3.2415943145751953
I0127 16:20:22.530327 139656809137920 logging_writer.py:48] [31800] global_step=31800, grad_norm=1.3939478397369385, loss=3.262298107147217
I0127 16:20:56.730489 139656783959808 logging_writer.py:48] [31900] global_step=31900, grad_norm=1.2880065441131592, loss=3.23299503326416
I0127 16:21:30.873357 139656809137920 logging_writer.py:48] [32000] global_step=32000, grad_norm=1.5369127988815308, loss=3.2841105461120605
I0127 16:22:05.030109 139656783959808 logging_writer.py:48] [32100] global_step=32100, grad_norm=1.2993067502975464, loss=3.2377469539642334
I0127 16:22:39.174729 139656809137920 logging_writer.py:48] [32200] global_step=32200, grad_norm=1.399839997291565, loss=3.2575459480285645
I0127 16:23:13.318621 139656783959808 logging_writer.py:48] [32300] global_step=32300, grad_norm=1.2887461185455322, loss=3.245105743408203
I0127 16:23:47.467700 139656809137920 logging_writer.py:48] [32400] global_step=32400, grad_norm=1.428923487663269, loss=3.265139102935791
I0127 16:24:21.588657 139656783959808 logging_writer.py:48] [32500] global_step=32500, grad_norm=1.3823206424713135, loss=3.233417510986328
I0127 16:24:55.726965 139656809137920 logging_writer.py:48] [32600] global_step=32600, grad_norm=1.2675211429595947, loss=3.2198479175567627
I0127 16:25:29.856308 139656783959808 logging_writer.py:48] [32700] global_step=32700, grad_norm=1.3769524097442627, loss=3.2502307891845703
I0127 16:26:04.483463 139656809137920 logging_writer.py:48] [32800] global_step=32800, grad_norm=1.5003474950790405, loss=3.211329460144043
I0127 16:26:16.907083 139822745589568 spec.py:321] Evaluating on the training split.
I0127 16:26:23.150922 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 16:26:32.020426 139822745589568 spec.py:349] Evaluating on the test split.
I0127 16:26:34.525518 139822745589568 submission_runner.py:408] Time since start: 11657.91s, 	Step: 32838, 	{'train/accuracy': 0.7084861397743225, 'train/loss': 1.379503846168518, 'validation/accuracy': 0.64055997133255, 'validation/loss': 1.693148136138916, 'validation/num_examples': 50000, 'test/accuracy': 0.5159000158309937, 'test/loss': 2.3436386585235596, 'test/num_examples': 10000, 'score': 11254.132838010788, 'total_duration': 11657.91011095047, 'accumulated_submission_time': 11254.132838010788, 'accumulated_eval_time': 401.97428369522095, 'accumulated_logging_time': 0.6830503940582275}
I0127 16:26:34.551915 139656800745216 logging_writer.py:48] [32838] accumulated_eval_time=401.974284, accumulated_logging_time=0.683050, accumulated_submission_time=11254.132838, global_step=32838, preemption_count=0, score=11254.132838, test/accuracy=0.515900, test/loss=2.343639, test/num_examples=10000, total_duration=11657.910111, train/accuracy=0.708486, train/loss=1.379504, validation/accuracy=0.640560, validation/loss=1.693148, validation/num_examples=50000
I0127 16:26:56.035604 139656817530624 logging_writer.py:48] [32900] global_step=32900, grad_norm=1.4314054250717163, loss=3.246641159057617
I0127 16:27:30.197228 139656800745216 logging_writer.py:48] [33000] global_step=33000, grad_norm=1.3712489604949951, loss=3.2278404235839844
I0127 16:28:04.291521 139656817530624 logging_writer.py:48] [33100] global_step=33100, grad_norm=1.3211876153945923, loss=3.148711681365967
I0127 16:28:38.400957 139656800745216 logging_writer.py:48] [33200] global_step=33200, grad_norm=1.4738017320632935, loss=3.231955051422119
I0127 16:29:12.538787 139656817530624 logging_writer.py:48] [33300] global_step=33300, grad_norm=1.3394685983657837, loss=3.2503514289855957
I0127 16:29:46.696845 139656800745216 logging_writer.py:48] [33400] global_step=33400, grad_norm=1.5157082080841064, loss=3.1815645694732666
I0127 16:30:20.814022 139656817530624 logging_writer.py:48] [33500] global_step=33500, grad_norm=1.465667963027954, loss=3.2679710388183594
I0127 16:30:54.951525 139656800745216 logging_writer.py:48] [33600] global_step=33600, grad_norm=1.510642170906067, loss=3.2926628589630127
I0127 16:31:29.096832 139656817530624 logging_writer.py:48] [33700] global_step=33700, grad_norm=1.4347493648529053, loss=3.249678373336792
I0127 16:32:03.248286 139656800745216 logging_writer.py:48] [33800] global_step=33800, grad_norm=1.428682565689087, loss=3.274296998977661
I0127 16:32:37.394841 139656817530624 logging_writer.py:48] [33900] global_step=33900, grad_norm=1.2928173542022705, loss=3.308615207672119
I0127 16:33:11.541577 139656800745216 logging_writer.py:48] [34000] global_step=34000, grad_norm=1.3468440771102905, loss=3.2350387573242188
I0127 16:33:45.764696 139656817530624 logging_writer.py:48] [34100] global_step=34100, grad_norm=1.349401593208313, loss=3.3231348991394043
I0127 16:34:19.901187 139656800745216 logging_writer.py:48] [34200] global_step=34200, grad_norm=1.4500210285186768, loss=3.2717275619506836
I0127 16:34:54.046621 139656817530624 logging_writer.py:48] [34300] global_step=34300, grad_norm=1.3976267576217651, loss=3.2389721870422363
I0127 16:35:04.768128 139822745589568 spec.py:321] Evaluating on the training split.
I0127 16:35:11.057140 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 16:35:19.633337 139822745589568 spec.py:349] Evaluating on the test split.
I0127 16:35:22.154360 139822745589568 submission_runner.py:408] Time since start: 12185.54s, 	Step: 34333, 	{'train/accuracy': 0.7168765664100647, 'train/loss': 1.3567454814910889, 'validation/accuracy': 0.6507399678230286, 'validation/loss': 1.6542654037475586, 'validation/num_examples': 50000, 'test/accuracy': 0.524399995803833, 'test/loss': 2.2950656414031982, 'test/num_examples': 10000, 'score': 11764.288525104523, 'total_duration': 12185.538932323456, 'accumulated_submission_time': 11764.288525104523, 'accumulated_eval_time': 419.36046075820923, 'accumulated_logging_time': 0.7189726829528809}
I0127 16:35:22.179396 139656700098304 logging_writer.py:48] [34333] accumulated_eval_time=419.360461, accumulated_logging_time=0.718973, accumulated_submission_time=11764.288525, global_step=34333, preemption_count=0, score=11764.288525, test/accuracy=0.524400, test/loss=2.295066, test/num_examples=10000, total_duration=12185.538932, train/accuracy=0.716877, train/loss=1.356745, validation/accuracy=0.650740, validation/loss=1.654265, validation/num_examples=50000
I0127 16:35:45.378721 139656783959808 logging_writer.py:48] [34400] global_step=34400, grad_norm=1.407876968383789, loss=3.211667776107788
I0127 16:36:19.464554 139656700098304 logging_writer.py:48] [34500] global_step=34500, grad_norm=1.323137879371643, loss=3.213332414627075
I0127 16:36:53.577661 139656783959808 logging_writer.py:48] [34600] global_step=34600, grad_norm=1.244890570640564, loss=3.1738905906677246
I0127 16:37:27.703105 139656700098304 logging_writer.py:48] [34700] global_step=34700, grad_norm=1.7460483312606812, loss=3.2781646251678467
I0127 16:38:01.855089 139656783959808 logging_writer.py:48] [34800] global_step=34800, grad_norm=1.389430046081543, loss=3.2836151123046875
I0127 16:38:36.008742 139656700098304 logging_writer.py:48] [34900] global_step=34900, grad_norm=1.4648195505142212, loss=3.239157199859619
I0127 16:39:10.163126 139656783959808 logging_writer.py:48] [35000] global_step=35000, grad_norm=1.3781579732894897, loss=3.130434274673462
I0127 16:39:44.303360 139656700098304 logging_writer.py:48] [35100] global_step=35100, grad_norm=1.3012982606887817, loss=3.177875280380249
I0127 16:40:18.504599 139656783959808 logging_writer.py:48] [35200] global_step=35200, grad_norm=1.2480156421661377, loss=3.214625358581543
I0127 16:40:52.651000 139656700098304 logging_writer.py:48] [35300] global_step=35300, grad_norm=1.5157092809677124, loss=3.1812055110931396
I0127 16:41:26.809660 139656783959808 logging_writer.py:48] [35400] global_step=35400, grad_norm=1.3824315071105957, loss=3.2097866535186768
I0127 16:42:00.939788 139656700098304 logging_writer.py:48] [35500] global_step=35500, grad_norm=1.5691484212875366, loss=3.2352707386016846
I0127 16:42:35.059097 139656783959808 logging_writer.py:48] [35600] global_step=35600, grad_norm=1.4011917114257812, loss=3.247718095779419
I0127 16:43:09.193942 139656700098304 logging_writer.py:48] [35700] global_step=35700, grad_norm=1.307437539100647, loss=3.249600887298584
I0127 16:43:43.291924 139656783959808 logging_writer.py:48] [35800] global_step=35800, grad_norm=1.5796111822128296, loss=3.2895405292510986
I0127 16:43:52.310566 139822745589568 spec.py:321] Evaluating on the training split.
I0127 16:43:58.685051 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 16:44:07.282139 139822745589568 spec.py:349] Evaluating on the test split.
I0127 16:44:09.759301 139822745589568 submission_runner.py:408] Time since start: 12713.14s, 	Step: 35828, 	{'train/accuracy': 0.6993981003761292, 'train/loss': 1.4522290229797363, 'validation/accuracy': 0.6362599730491638, 'validation/loss': 1.7302823066711426, 'validation/num_examples': 50000, 'test/accuracy': 0.5097000002861023, 'test/loss': 2.3925395011901855, 'test/num_examples': 10000, 'score': 12274.358264684677, 'total_duration': 12713.143894910812, 'accumulated_submission_time': 12274.358264684677, 'accumulated_eval_time': 436.809175491333, 'accumulated_logging_time': 0.7555732727050781}
I0127 16:44:09.784747 139656783959808 logging_writer.py:48] [35828] accumulated_eval_time=436.809175, accumulated_logging_time=0.755573, accumulated_submission_time=12274.358265, global_step=35828, preemption_count=0, score=12274.358265, test/accuracy=0.509700, test/loss=2.392540, test/num_examples=10000, total_duration=12713.143895, train/accuracy=0.699398, train/loss=1.452229, validation/accuracy=0.636260, validation/loss=1.730282, validation/num_examples=50000
I0127 16:44:34.673765 139656817530624 logging_writer.py:48] [35900] global_step=35900, grad_norm=1.3434780836105347, loss=3.2084407806396484
I0127 16:45:08.819840 139656783959808 logging_writer.py:48] [36000] global_step=36000, grad_norm=1.3709211349487305, loss=3.269160032272339
I0127 16:45:42.932559 139656817530624 logging_writer.py:48] [36100] global_step=36100, grad_norm=1.3300926685333252, loss=3.2383668422698975
I0127 16:46:17.125076 139656783959808 logging_writer.py:48] [36200] global_step=36200, grad_norm=1.5176844596862793, loss=3.119849443435669
I0127 16:46:51.247981 139656817530624 logging_writer.py:48] [36300] global_step=36300, grad_norm=1.4303597211837769, loss=3.1726953983306885
I0127 16:47:25.375099 139656783959808 logging_writer.py:48] [36400] global_step=36400, grad_norm=1.463297963142395, loss=3.2549691200256348
I0127 16:47:59.501189 139656817530624 logging_writer.py:48] [36500] global_step=36500, grad_norm=1.4438048601150513, loss=3.238457202911377
I0127 16:48:33.647218 139656783959808 logging_writer.py:48] [36600] global_step=36600, grad_norm=1.4525933265686035, loss=3.321420192718506
I0127 16:49:07.793110 139656817530624 logging_writer.py:48] [36700] global_step=36700, grad_norm=1.3316209316253662, loss=3.1616690158843994
I0127 16:49:41.954942 139656783959808 logging_writer.py:48] [36800] global_step=36800, grad_norm=1.427685260772705, loss=3.109130620956421
I0127 16:50:16.085118 139656817530624 logging_writer.py:48] [36900] global_step=36900, grad_norm=1.6203639507293701, loss=3.168881893157959
I0127 16:50:50.231953 139656783959808 logging_writer.py:48] [37000] global_step=37000, grad_norm=1.396273136138916, loss=3.230781316757202
I0127 16:51:24.358514 139656817530624 logging_writer.py:48] [37100] global_step=37100, grad_norm=1.5148884057998657, loss=3.234947443008423
I0127 16:51:58.514241 139656783959808 logging_writer.py:48] [37200] global_step=37200, grad_norm=1.4193096160888672, loss=3.16231632232666
I0127 16:52:32.704076 139656817530624 logging_writer.py:48] [37300] global_step=37300, grad_norm=1.5527657270431519, loss=3.2628214359283447
I0127 16:52:40.040036 139822745589568 spec.py:321] Evaluating on the training split.
I0127 16:52:46.285017 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 16:52:54.967349 139822745589568 spec.py:349] Evaluating on the test split.
I0127 16:52:57.390967 139822745589568 submission_runner.py:408] Time since start: 13240.78s, 	Step: 37323, 	{'train/accuracy': 0.7249680757522583, 'train/loss': 1.295501708984375, 'validation/accuracy': 0.6560800075531006, 'validation/loss': 1.6071767807006836, 'validation/num_examples': 50000, 'test/accuracy': 0.5236000418663025, 'test/loss': 2.2822113037109375, 'test/num_examples': 10000, 'score': 12784.553744077682, 'total_duration': 13240.775558948517, 'accumulated_submission_time': 12784.553744077682, 'accumulated_eval_time': 454.1600670814514, 'accumulated_logging_time': 0.7909941673278809}
I0127 16:52:57.416221 139656800745216 logging_writer.py:48] [37323] accumulated_eval_time=454.160067, accumulated_logging_time=0.790994, accumulated_submission_time=12784.553744, global_step=37323, preemption_count=0, score=12784.553744, test/accuracy=0.523600, test/loss=2.282211, test/num_examples=10000, total_duration=13240.775559, train/accuracy=0.724968, train/loss=1.295502, validation/accuracy=0.656080, validation/loss=1.607177, validation/num_examples=50000
I0127 16:53:24.018459 139656809137920 logging_writer.py:48] [37400] global_step=37400, grad_norm=1.5480728149414062, loss=3.1315507888793945
I0127 16:53:58.127536 139656800745216 logging_writer.py:48] [37500] global_step=37500, grad_norm=1.3559224605560303, loss=3.2634761333465576
I0127 16:54:32.262368 139656809137920 logging_writer.py:48] [37600] global_step=37600, grad_norm=1.4762336015701294, loss=3.1958975791931152
I0127 16:55:06.392838 139656800745216 logging_writer.py:48] [37700] global_step=37700, grad_norm=1.8913774490356445, loss=3.277517318725586
I0127 16:55:40.510069 139656809137920 logging_writer.py:48] [37800] global_step=37800, grad_norm=1.5693981647491455, loss=3.2122318744659424
I0127 16:56:14.634877 139656800745216 logging_writer.py:48] [37900] global_step=37900, grad_norm=1.5420396327972412, loss=3.2161448001861572
I0127 16:56:48.776379 139656809137920 logging_writer.py:48] [38000] global_step=38000, grad_norm=1.7272573709487915, loss=3.1904170513153076
I0127 16:57:22.905503 139656800745216 logging_writer.py:48] [38100] global_step=38100, grad_norm=1.5534393787384033, loss=3.1880550384521484
I0127 16:57:57.007460 139656809137920 logging_writer.py:48] [38200] global_step=38200, grad_norm=1.7802850008010864, loss=3.2173783779144287
I0127 16:58:31.134927 139656800745216 logging_writer.py:48] [38300] global_step=38300, grad_norm=1.6755955219268799, loss=3.1777408123016357
I0127 16:59:05.336409 139656809137920 logging_writer.py:48] [38400] global_step=38400, grad_norm=1.4856423139572144, loss=3.156980514526367
I0127 16:59:39.470632 139656800745216 logging_writer.py:48] [38500] global_step=38500, grad_norm=1.927307367324829, loss=3.174624443054199
I0127 17:00:13.570228 139656809137920 logging_writer.py:48] [38600] global_step=38600, grad_norm=1.5318189859390259, loss=3.214456081390381
I0127 17:00:47.706951 139656800745216 logging_writer.py:48] [38700] global_step=38700, grad_norm=1.3613799810409546, loss=3.197897434234619
I0127 17:01:21.840710 139656809137920 logging_writer.py:48] [38800] global_step=38800, grad_norm=1.584417700767517, loss=3.194648265838623
I0127 17:01:27.434140 139822745589568 spec.py:321] Evaluating on the training split.
I0127 17:01:33.805364 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 17:01:42.528614 139822745589568 spec.py:349] Evaluating on the test split.
I0127 17:01:44.964128 139822745589568 submission_runner.py:408] Time since start: 13768.35s, 	Step: 38818, 	{'train/accuracy': 0.7355508208274841, 'train/loss': 1.286131739616394, 'validation/accuracy': 0.6459800004959106, 'validation/loss': 1.6857054233551025, 'validation/num_examples': 50000, 'test/accuracy': 0.5186000466346741, 'test/loss': 2.3471288681030273, 'test/num_examples': 10000, 'score': 13294.51098227501, 'total_duration': 13768.348722219467, 'accumulated_submission_time': 13294.51098227501, 'accumulated_eval_time': 471.69002628326416, 'accumulated_logging_time': 0.8257701396942139}
I0127 17:01:44.991181 139656817530624 logging_writer.py:48] [38818] accumulated_eval_time=471.690026, accumulated_logging_time=0.825770, accumulated_submission_time=13294.510982, global_step=38818, preemption_count=0, score=13294.510982, test/accuracy=0.518600, test/loss=2.347129, test/num_examples=10000, total_duration=13768.348722, train/accuracy=0.735551, train/loss=1.286132, validation/accuracy=0.645980, validation/loss=1.685705, validation/num_examples=50000
I0127 17:02:13.267715 139656825923328 logging_writer.py:48] [38900] global_step=38900, grad_norm=1.5439727306365967, loss=3.159846782684326
I0127 17:02:47.361392 139656817530624 logging_writer.py:48] [39000] global_step=39000, grad_norm=1.400235891342163, loss=3.2420146465301514
I0127 17:03:21.495564 139656825923328 logging_writer.py:48] [39100] global_step=39100, grad_norm=1.5429097414016724, loss=3.1353604793548584
I0127 17:03:55.634925 139656817530624 logging_writer.py:48] [39200] global_step=39200, grad_norm=1.5904499292373657, loss=3.2191994190216064
I0127 17:04:29.764902 139656825923328 logging_writer.py:48] [39300] global_step=39300, grad_norm=1.5910214185714722, loss=3.242945671081543
I0127 17:05:03.891294 139656817530624 logging_writer.py:48] [39400] global_step=39400, grad_norm=1.469586968421936, loss=3.2463812828063965
I0127 17:05:38.096789 139656825923328 logging_writer.py:48] [39500] global_step=39500, grad_norm=1.5434410572052002, loss=3.141214609146118
I0127 17:06:12.235070 139656817530624 logging_writer.py:48] [39600] global_step=39600, grad_norm=1.38941490650177, loss=3.1897482872009277
I0127 17:06:46.362426 139656825923328 logging_writer.py:48] [39700] global_step=39700, grad_norm=1.563559889793396, loss=3.154858112335205
I0127 17:07:20.477248 139656817530624 logging_writer.py:48] [39800] global_step=39800, grad_norm=1.5279983282089233, loss=3.2168216705322266
I0127 17:07:54.636162 139656825923328 logging_writer.py:48] [39900] global_step=39900, grad_norm=1.480292558670044, loss=3.173609495162964
I0127 17:08:28.791094 139656817530624 logging_writer.py:48] [40000] global_step=40000, grad_norm=1.5224459171295166, loss=3.171912670135498
I0127 17:09:02.926855 139656825923328 logging_writer.py:48] [40100] global_step=40100, grad_norm=1.5518718957901, loss=3.2159061431884766
I0127 17:09:37.082136 139656817530624 logging_writer.py:48] [40200] global_step=40200, grad_norm=1.5761151313781738, loss=3.1790883541107178
I0127 17:10:11.213221 139656825923328 logging_writer.py:48] [40300] global_step=40300, grad_norm=1.4684937000274658, loss=3.1938300132751465
I0127 17:10:15.121657 139822745589568 spec.py:321] Evaluating on the training split.
I0127 17:10:21.525876 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 17:10:30.195915 139822745589568 spec.py:349] Evaluating on the test split.
I0127 17:10:32.634617 139822745589568 submission_runner.py:408] Time since start: 14296.02s, 	Step: 40313, 	{'train/accuracy': 0.7302096486091614, 'train/loss': 1.310068964958191, 'validation/accuracy': 0.6537799835205078, 'validation/loss': 1.6593209505081177, 'validation/num_examples': 50000, 'test/accuracy': 0.5261000394821167, 'test/loss': 2.311683416366577, 'test/num_examples': 10000, 'score': 13804.579131126404, 'total_duration': 14296.019186019897, 'accumulated_submission_time': 13804.579131126404, 'accumulated_eval_time': 489.20293831825256, 'accumulated_logging_time': 0.8625240325927734}
I0127 17:10:32.660077 139656792352512 logging_writer.py:48] [40313] accumulated_eval_time=489.202938, accumulated_logging_time=0.862524, accumulated_submission_time=13804.579131, global_step=40313, preemption_count=0, score=13804.579131, test/accuracy=0.526100, test/loss=2.311683, test/num_examples=10000, total_duration=14296.019186, train/accuracy=0.730210, train/loss=1.310069, validation/accuracy=0.653780, validation/loss=1.659321, validation/num_examples=50000
I0127 17:11:02.646873 139656800745216 logging_writer.py:48] [40400] global_step=40400, grad_norm=1.59285306930542, loss=3.1989684104919434
I0127 17:11:36.773490 139656792352512 logging_writer.py:48] [40500] global_step=40500, grad_norm=1.5976731777191162, loss=3.233126401901245
I0127 17:12:10.883125 139656800745216 logging_writer.py:48] [40600] global_step=40600, grad_norm=1.6464570760726929, loss=3.282888174057007
I0127 17:12:45.025876 139656792352512 logging_writer.py:48] [40700] global_step=40700, grad_norm=1.545514702796936, loss=3.193397045135498
I0127 17:13:19.180162 139656800745216 logging_writer.py:48] [40800] global_step=40800, grad_norm=1.7257349491119385, loss=3.1425130367279053
I0127 17:13:53.310664 139656792352512 logging_writer.py:48] [40900] global_step=40900, grad_norm=1.6014328002929688, loss=3.188197135925293
I0127 17:14:27.433410 139656800745216 logging_writer.py:48] [41000] global_step=41000, grad_norm=1.5205693244934082, loss=3.1073083877563477
I0127 17:15:01.582304 139656792352512 logging_writer.py:48] [41100] global_step=41100, grad_norm=1.6276249885559082, loss=3.165894031524658
I0127 17:15:35.730467 139656800745216 logging_writer.py:48] [41200] global_step=41200, grad_norm=1.5961908102035522, loss=3.194507122039795
I0127 17:16:09.856803 139656792352512 logging_writer.py:48] [41300] global_step=41300, grad_norm=1.8023930788040161, loss=3.2262394428253174
I0127 17:16:43.978901 139656800745216 logging_writer.py:48] [41400] global_step=41400, grad_norm=1.4224717617034912, loss=3.1540451049804688
I0127 17:17:18.095108 139656792352512 logging_writer.py:48] [41500] global_step=41500, grad_norm=1.6471490859985352, loss=3.2841217517852783
I0127 17:17:52.330822 139656800745216 logging_writer.py:48] [41600] global_step=41600, grad_norm=1.6347506046295166, loss=3.2854819297790527
I0127 17:18:26.452596 139656792352512 logging_writer.py:48] [41700] global_step=41700, grad_norm=1.6009399890899658, loss=3.2613677978515625
I0127 17:19:00.592207 139656800745216 logging_writer.py:48] [41800] global_step=41800, grad_norm=1.5268676280975342, loss=3.1961615085601807
I0127 17:19:02.772563 139822745589568 spec.py:321] Evaluating on the training split.
I0127 17:19:09.653115 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 17:19:18.188406 139822745589568 spec.py:349] Evaluating on the test split.
I0127 17:19:20.630171 139822745589568 submission_runner.py:408] Time since start: 14824.01s, 	Step: 41808, 	{'train/accuracy': 0.7320830821990967, 'train/loss': 1.2917817831039429, 'validation/accuracy': 0.6575599908828735, 'validation/loss': 1.6185765266418457, 'validation/num_examples': 50000, 'test/accuracy': 0.5362000465393066, 'test/loss': 2.260235071182251, 'test/num_examples': 10000, 'score': 14314.632142066956, 'total_duration': 14824.014763116837, 'accumulated_submission_time': 14314.632142066956, 'accumulated_eval_time': 507.06050848960876, 'accumulated_logging_time': 0.8970699310302734}
I0127 17:19:20.658989 139656792352512 logging_writer.py:48] [41808] accumulated_eval_time=507.060508, accumulated_logging_time=0.897070, accumulated_submission_time=14314.632142, global_step=41808, preemption_count=0, score=14314.632142, test/accuracy=0.536200, test/loss=2.260235, test/num_examples=10000, total_duration=14824.014763, train/accuracy=0.732083, train/loss=1.291782, validation/accuracy=0.657560, validation/loss=1.618577, validation/num_examples=50000
I0127 17:19:52.398060 139656800745216 logging_writer.py:48] [41900] global_step=41900, grad_norm=1.698123574256897, loss=3.2664270401000977
I0127 17:20:26.446263 139656792352512 logging_writer.py:48] [42000] global_step=42000, grad_norm=1.5535897016525269, loss=3.174254894256592
I0127 17:21:00.584206 139656800745216 logging_writer.py:48] [42100] global_step=42100, grad_norm=1.584839105606079, loss=3.1096105575561523
I0127 17:21:34.683997 139656792352512 logging_writer.py:48] [42200] global_step=42200, grad_norm=1.6021262407302856, loss=3.2479918003082275
I0127 17:22:08.816242 139656800745216 logging_writer.py:48] [42300] global_step=42300, grad_norm=1.7071082592010498, loss=3.163881778717041
I0127 17:22:42.940909 139656792352512 logging_writer.py:48] [42400] global_step=42400, grad_norm=1.5708824396133423, loss=3.2173783779144287
I0127 17:23:17.079072 139656800745216 logging_writer.py:48] [42500] global_step=42500, grad_norm=1.5313698053359985, loss=3.1670029163360596
I0127 17:23:51.183110 139656792352512 logging_writer.py:48] [42600] global_step=42600, grad_norm=1.7263426780700684, loss=3.1496787071228027
I0127 17:24:25.356830 139656800745216 logging_writer.py:48] [42700] global_step=42700, grad_norm=1.5973235368728638, loss=3.1532695293426514
I0127 17:24:59.484096 139656792352512 logging_writer.py:48] [42800] global_step=42800, grad_norm=1.5488669872283936, loss=3.161705732345581
I0127 17:25:33.612496 139656800745216 logging_writer.py:48] [42900] global_step=42900, grad_norm=1.6574618816375732, loss=3.2399446964263916
I0127 17:26:07.729403 139656792352512 logging_writer.py:48] [43000] global_step=43000, grad_norm=1.6362537145614624, loss=3.169865369796753
I0127 17:26:41.830529 139656800745216 logging_writer.py:48] [43100] global_step=43100, grad_norm=1.781696081161499, loss=3.3185393810272217
I0127 17:27:15.943709 139656792352512 logging_writer.py:48] [43200] global_step=43200, grad_norm=1.5797392129898071, loss=3.155759572982788
I0127 17:27:50.094108 139656800745216 logging_writer.py:48] [43300] global_step=43300, grad_norm=1.7044346332550049, loss=3.203373432159424
I0127 17:27:50.917380 139822745589568 spec.py:321] Evaluating on the training split.
I0127 17:27:57.138748 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 17:28:05.842537 139822745589568 spec.py:349] Evaluating on the test split.
I0127 17:28:08.272402 139822745589568 submission_runner.py:408] Time since start: 15351.66s, 	Step: 43304, 	{'train/accuracy': 0.7353116869926453, 'train/loss': 1.2636276483535767, 'validation/accuracy': 0.6688199639320374, 'validation/loss': 1.5687376260757446, 'validation/num_examples': 50000, 'test/accuracy': 0.5380000472068787, 'test/loss': 2.22286057472229, 'test/num_examples': 10000, 'score': 14824.831728935242, 'total_duration': 15351.65698647499, 'accumulated_submission_time': 14824.831728935242, 'accumulated_eval_time': 524.4154839515686, 'accumulated_logging_time': 0.935309648513794}
I0127 17:28:08.297536 139656792352512 logging_writer.py:48] [43304] accumulated_eval_time=524.415484, accumulated_logging_time=0.935310, accumulated_submission_time=14824.831729, global_step=43304, preemption_count=0, score=14824.831729, test/accuracy=0.538000, test/loss=2.222861, test/num_examples=10000, total_duration=15351.656986, train/accuracy=0.735312, train/loss=1.263628, validation/accuracy=0.668820, validation/loss=1.568738, validation/num_examples=50000
I0127 17:28:41.358781 139656809137920 logging_writer.py:48] [43400] global_step=43400, grad_norm=1.5705474615097046, loss=3.228854179382324
I0127 17:29:15.472157 139656792352512 logging_writer.py:48] [43500] global_step=43500, grad_norm=1.581742286682129, loss=3.1299595832824707
I0127 17:29:49.587463 139656809137920 logging_writer.py:48] [43600] global_step=43600, grad_norm=1.9837028980255127, loss=3.2354965209960938
I0127 17:30:23.793629 139656792352512 logging_writer.py:48] [43700] global_step=43700, grad_norm=1.4838749170303345, loss=3.1002871990203857
I0127 17:30:57.920742 139656809137920 logging_writer.py:48] [43800] global_step=43800, grad_norm=1.639714241027832, loss=3.202821731567383
I0127 17:31:32.034174 139656792352512 logging_writer.py:48] [43900] global_step=43900, grad_norm=1.5096269845962524, loss=3.127200126647949
I0127 17:32:06.162000 139656809137920 logging_writer.py:48] [44000] global_step=44000, grad_norm=1.683759331703186, loss=3.196227788925171
I0127 17:32:40.299279 139656792352512 logging_writer.py:48] [44100] global_step=44100, grad_norm=1.6009390354156494, loss=3.1970248222351074
I0127 17:33:14.449318 139656809137920 logging_writer.py:48] [44200] global_step=44200, grad_norm=1.7260825634002686, loss=3.2858963012695312
I0127 17:33:48.610849 139656792352512 logging_writer.py:48] [44300] global_step=44300, grad_norm=1.605163812637329, loss=3.0783395767211914
I0127 17:34:22.752314 139656809137920 logging_writer.py:48] [44400] global_step=44400, grad_norm=1.523519515991211, loss=3.168058395385742
I0127 17:34:56.877710 139656792352512 logging_writer.py:48] [44500] global_step=44500, grad_norm=1.6682149171829224, loss=3.1842215061187744
I0127 17:35:31.007117 139656809137920 logging_writer.py:48] [44600] global_step=44600, grad_norm=1.6270699501037598, loss=3.1562137603759766
I0127 17:36:05.178625 139656792352512 logging_writer.py:48] [44700] global_step=44700, grad_norm=1.6371376514434814, loss=3.1503536701202393
I0127 17:36:38.481382 139822745589568 spec.py:321] Evaluating on the training split.
I0127 17:36:44.686138 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 17:36:53.422173 139822745589568 spec.py:349] Evaluating on the test split.
I0127 17:36:55.877940 139822745589568 submission_runner.py:408] Time since start: 15879.26s, 	Step: 44799, 	{'train/accuracy': 0.7269411683082581, 'train/loss': 1.2829989194869995, 'validation/accuracy': 0.6580599546432495, 'validation/loss': 1.5886094570159912, 'validation/num_examples': 50000, 'test/accuracy': 0.5301000475883484, 'test/loss': 2.2401185035705566, 'test/num_examples': 10000, 'score': 15334.955714941025, 'total_duration': 15879.262532949448, 'accumulated_submission_time': 15334.955714941025, 'accumulated_eval_time': 541.8120038509369, 'accumulated_logging_time': 0.9696609973907471}
I0127 17:36:55.903399 139656700098304 logging_writer.py:48] [44799] accumulated_eval_time=541.812004, accumulated_logging_time=0.969661, accumulated_submission_time=15334.955715, global_step=44799, preemption_count=0, score=15334.955715, test/accuracy=0.530100, test/loss=2.240119, test/num_examples=10000, total_duration=15879.262533, train/accuracy=0.726941, train/loss=1.282999, validation/accuracy=0.658060, validation/loss=1.588609, validation/num_examples=50000
I0127 17:36:56.589010 139656783959808 logging_writer.py:48] [44800] global_step=44800, grad_norm=1.5531469583511353, loss=3.152222156524658
I0127 17:37:30.700195 139656700098304 logging_writer.py:48] [44900] global_step=44900, grad_norm=2.0018718242645264, loss=3.150789976119995
I0127 17:38:04.815361 139656783959808 logging_writer.py:48] [45000] global_step=45000, grad_norm=1.6555238962173462, loss=3.150113582611084
I0127 17:38:38.936291 139656700098304 logging_writer.py:48] [45100] global_step=45100, grad_norm=1.8272961378097534, loss=3.1624138355255127
I0127 17:39:13.075632 139656783959808 logging_writer.py:48] [45200] global_step=45200, grad_norm=1.689326524734497, loss=3.156277656555176
I0127 17:39:47.211525 139656700098304 logging_writer.py:48] [45300] global_step=45300, grad_norm=1.6349581480026245, loss=3.243800640106201
I0127 17:40:21.369771 139656783959808 logging_writer.py:48] [45400] global_step=45400, grad_norm=1.7822132110595703, loss=3.1570987701416016
I0127 17:40:55.516511 139656700098304 logging_writer.py:48] [45500] global_step=45500, grad_norm=1.762169599533081, loss=3.2385005950927734
I0127 17:41:29.637761 139656783959808 logging_writer.py:48] [45600] global_step=45600, grad_norm=1.5759532451629639, loss=3.154808282852173
I0127 17:42:03.791228 139656700098304 logging_writer.py:48] [45700] global_step=45700, grad_norm=1.5874654054641724, loss=3.147690773010254
I0127 17:42:37.937778 139656783959808 logging_writer.py:48] [45800] global_step=45800, grad_norm=1.6123809814453125, loss=3.1871092319488525
I0127 17:43:12.092903 139656700098304 logging_writer.py:48] [45900] global_step=45900, grad_norm=1.7938930988311768, loss=3.239210605621338
I0127 17:43:46.232858 139656783959808 logging_writer.py:48] [46000] global_step=46000, grad_norm=1.75040602684021, loss=3.1753344535827637
I0127 17:44:20.370006 139656700098304 logging_writer.py:48] [46100] global_step=46100, grad_norm=1.6355514526367188, loss=3.146275758743286
I0127 17:44:54.490445 139656783959808 logging_writer.py:48] [46200] global_step=46200, grad_norm=1.5989148616790771, loss=3.208573818206787
I0127 17:45:26.035814 139822745589568 spec.py:321] Evaluating on the training split.
I0127 17:45:32.218303 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 17:45:40.831524 139822745589568 spec.py:349] Evaluating on the test split.
I0127 17:45:43.295384 139822745589568 submission_runner.py:408] Time since start: 16406.68s, 	Step: 46294, 	{'train/accuracy': 0.7327606678009033, 'train/loss': 1.280967116355896, 'validation/accuracy': 0.6660400032997131, 'validation/loss': 1.5779304504394531, 'validation/num_examples': 50000, 'test/accuracy': 0.5383000373840332, 'test/loss': 2.2129969596862793, 'test/num_examples': 10000, 'score': 15845.028705358505, 'total_duration': 16406.679964780807, 'accumulated_submission_time': 15845.028705358505, 'accumulated_eval_time': 559.0715284347534, 'accumulated_logging_time': 1.003936529159546}
I0127 17:45:43.320003 139656834316032 logging_writer.py:48] [46294] accumulated_eval_time=559.071528, accumulated_logging_time=1.003937, accumulated_submission_time=15845.028705, global_step=46294, preemption_count=0, score=15845.028705, test/accuracy=0.538300, test/loss=2.212997, test/num_examples=10000, total_duration=16406.679965, train/accuracy=0.732761, train/loss=1.280967, validation/accuracy=0.666040, validation/loss=1.577930, validation/num_examples=50000
I0127 17:45:45.708785 139658730145536 logging_writer.py:48] [46300] global_step=46300, grad_norm=1.9025517702102661, loss=3.1005077362060547
I0127 17:46:19.774651 139656834316032 logging_writer.py:48] [46400] global_step=46400, grad_norm=1.5495694875717163, loss=3.096125602722168
I0127 17:46:53.854062 139658730145536 logging_writer.py:48] [46500] global_step=46500, grad_norm=1.6453819274902344, loss=3.1281485557556152
I0127 17:47:27.975549 139656834316032 logging_writer.py:48] [46600] global_step=46600, grad_norm=1.8327959775924683, loss=3.154872179031372
I0127 17:48:02.104317 139658730145536 logging_writer.py:48] [46700] global_step=46700, grad_norm=1.8630821704864502, loss=3.1841013431549072
I0127 17:48:36.234947 139656834316032 logging_writer.py:48] [46800] global_step=46800, grad_norm=1.5902637243270874, loss=3.1160728931427
I0127 17:49:10.409020 139658730145536 logging_writer.py:48] [46900] global_step=46900, grad_norm=1.6108373403549194, loss=3.142550230026245
I0127 17:49:44.558104 139656834316032 logging_writer.py:48] [47000] global_step=47000, grad_norm=1.8433756828308105, loss=3.2220094203948975
I0127 17:50:18.693996 139658730145536 logging_writer.py:48] [47100] global_step=47100, grad_norm=1.7149285078048706, loss=3.0868735313415527
I0127 17:50:52.836192 139656834316032 logging_writer.py:48] [47200] global_step=47200, grad_norm=1.772030234336853, loss=3.1639199256896973
I0127 17:51:26.970131 139658730145536 logging_writer.py:48] [47300] global_step=47300, grad_norm=1.6873130798339844, loss=3.1033899784088135
I0127 17:52:01.105017 139656834316032 logging_writer.py:48] [47400] global_step=47400, grad_norm=1.7350331544876099, loss=3.1486244201660156
I0127 17:52:35.239646 139658730145536 logging_writer.py:48] [47500] global_step=47500, grad_norm=1.6998168230056763, loss=3.1978561878204346
I0127 17:53:09.387273 139656834316032 logging_writer.py:48] [47600] global_step=47600, grad_norm=1.7311789989471436, loss=3.209177017211914
I0127 17:53:43.519170 139658730145536 logging_writer.py:48] [47700] global_step=47700, grad_norm=1.6313642263412476, loss=3.18196177482605
I0127 17:54:13.361345 139822745589568 spec.py:321] Evaluating on the training split.
I0127 17:54:19.690696 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 17:54:28.061519 139822745589568 spec.py:349] Evaluating on the test split.
I0127 17:54:30.491948 139822745589568 submission_runner.py:408] Time since start: 16933.88s, 	Step: 47789, 	{'train/accuracy': 0.7629344463348389, 'train/loss': 1.1510975360870361, 'validation/accuracy': 0.6612600088119507, 'validation/loss': 1.5798641443252563, 'validation/num_examples': 50000, 'test/accuracy': 0.5379000306129456, 'test/loss': 2.240457057952881, 'test/num_examples': 10000, 'score': 16355.009400129318, 'total_duration': 16933.876542568207, 'accumulated_submission_time': 16355.009400129318, 'accumulated_eval_time': 576.2020993232727, 'accumulated_logging_time': 1.0376520156860352}
I0127 17:54:30.518501 139656783959808 logging_writer.py:48] [47789] accumulated_eval_time=576.202099, accumulated_logging_time=1.037652, accumulated_submission_time=16355.009400, global_step=47789, preemption_count=0, score=16355.009400, test/accuracy=0.537900, test/loss=2.240457, test/num_examples=10000, total_duration=16933.876543, train/accuracy=0.762934, train/loss=1.151098, validation/accuracy=0.661260, validation/loss=1.579864, validation/num_examples=50000
I0127 17:54:34.614419 139656800745216 logging_writer.py:48] [47800] global_step=47800, grad_norm=1.9912831783294678, loss=3.190336227416992
I0127 17:55:08.697442 139656783959808 logging_writer.py:48] [47900] global_step=47900, grad_norm=1.7774354219436646, loss=3.175938129425049
I0127 17:55:42.985031 139656800745216 logging_writer.py:48] [48000] global_step=48000, grad_norm=1.5945191383361816, loss=3.111466884613037
I0127 17:56:17.081420 139656783959808 logging_writer.py:48] [48100] global_step=48100, grad_norm=1.8976008892059326, loss=3.1063318252563477
I0127 17:56:51.239320 139656800745216 logging_writer.py:48] [48200] global_step=48200, grad_norm=1.6645958423614502, loss=3.135174036026001
I0127 17:57:25.374072 139656783959808 logging_writer.py:48] [48300] global_step=48300, grad_norm=1.7266496419906616, loss=3.1982905864715576
I0127 17:57:59.518388 139656800745216 logging_writer.py:48] [48400] global_step=48400, grad_norm=1.6249191761016846, loss=3.1286911964416504
I0127 17:58:33.635915 139656783959808 logging_writer.py:48] [48500] global_step=48500, grad_norm=1.7178034782409668, loss=3.1526029109954834
I0127 17:59:07.768853 139656800745216 logging_writer.py:48] [48600] global_step=48600, grad_norm=1.7492557764053345, loss=3.19726824760437
I0127 17:59:41.899221 139656783959808 logging_writer.py:48] [48700] global_step=48700, grad_norm=1.7428056001663208, loss=3.080310344696045
I0127 18:00:16.043191 139656800745216 logging_writer.py:48] [48800] global_step=48800, grad_norm=1.8528954982757568, loss=3.183640480041504
I0127 18:00:50.156404 139656783959808 logging_writer.py:48] [48900] global_step=48900, grad_norm=1.6376014947891235, loss=3.1414575576782227
I0127 18:01:24.296711 139656800745216 logging_writer.py:48] [49000] global_step=49000, grad_norm=1.7346179485321045, loss=3.168776512145996
I0127 18:01:58.526752 139656783959808 logging_writer.py:48] [49100] global_step=49100, grad_norm=1.7466154098510742, loss=3.2160210609436035
I0127 18:02:32.619456 139656800745216 logging_writer.py:48] [49200] global_step=49200, grad_norm=1.6106456518173218, loss=3.142188787460327
I0127 18:03:00.719205 139822745589568 spec.py:321] Evaluating on the training split.
I0127 18:03:07.035477 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 18:03:15.650152 139822745589568 spec.py:349] Evaluating on the test split.
I0127 18:03:18.104722 139822745589568 submission_runner.py:408] Time since start: 17461.49s, 	Step: 49284, 	{'train/accuracy': 0.7473692297935486, 'train/loss': 1.239794373512268, 'validation/accuracy': 0.6687399744987488, 'validation/loss': 1.5972586870193481, 'validation/num_examples': 50000, 'test/accuracy': 0.5371000170707703, 'test/loss': 2.2499301433563232, 'test/num_examples': 10000, 'score': 16865.14865398407, 'total_duration': 17461.489314556122, 'accumulated_submission_time': 16865.14865398407, 'accumulated_eval_time': 593.5875813961029, 'accumulated_logging_time': 1.0741991996765137}
I0127 18:03:18.133895 139656834316032 logging_writer.py:48] [49284] accumulated_eval_time=593.587581, accumulated_logging_time=1.074199, accumulated_submission_time=16865.148654, global_step=49284, preemption_count=0, score=16865.148654, test/accuracy=0.537100, test/loss=2.249930, test/num_examples=10000, total_duration=17461.489315, train/accuracy=0.747369, train/loss=1.239794, validation/accuracy=0.668740, validation/loss=1.597259, validation/num_examples=50000
I0127 18:03:23.931951 139658730145536 logging_writer.py:48] [49300] global_step=49300, grad_norm=1.6827796697616577, loss=3.072826862335205
I0127 18:03:58.024447 139656834316032 logging_writer.py:48] [49400] global_step=49400, grad_norm=1.8334808349609375, loss=3.1949148178100586
I0127 18:04:32.105788 139658730145536 logging_writer.py:48] [49500] global_step=49500, grad_norm=1.6484713554382324, loss=3.1616132259368896
I0127 18:05:06.210447 139656834316032 logging_writer.py:48] [49600] global_step=49600, grad_norm=1.6833038330078125, loss=3.184760808944702
I0127 18:05:40.341014 139658730145536 logging_writer.py:48] [49700] global_step=49700, grad_norm=1.6259965896606445, loss=3.207441568374634
I0127 18:06:14.483046 139656834316032 logging_writer.py:48] [49800] global_step=49800, grad_norm=1.593298077583313, loss=3.069838285446167
I0127 18:06:48.600276 139658730145536 logging_writer.py:48] [49900] global_step=49900, grad_norm=1.7168035507202148, loss=3.1674821376800537
I0127 18:07:22.705113 139656834316032 logging_writer.py:48] [50000] global_step=50000, grad_norm=1.7671760320663452, loss=3.086310625076294
I0127 18:07:56.842130 139658730145536 logging_writer.py:48] [50100] global_step=50100, grad_norm=1.6814264059066772, loss=3.212150812149048
I0127 18:08:31.068147 139656834316032 logging_writer.py:48] [50200] global_step=50200, grad_norm=1.7403029203414917, loss=3.157773733139038
I0127 18:09:05.193881 139658730145536 logging_writer.py:48] [50300] global_step=50300, grad_norm=1.6861721277236938, loss=3.1775736808776855
I0127 18:09:39.321401 139656834316032 logging_writer.py:48] [50400] global_step=50400, grad_norm=1.6519160270690918, loss=3.024245023727417
I0127 18:10:13.450407 139658730145536 logging_writer.py:48] [50500] global_step=50500, grad_norm=2.0088958740234375, loss=3.198843002319336
I0127 18:10:47.580684 139656834316032 logging_writer.py:48] [50600] global_step=50600, grad_norm=1.8050729036331177, loss=3.1445350646972656
I0127 18:11:21.690467 139658730145536 logging_writer.py:48] [50700] global_step=50700, grad_norm=1.6948118209838867, loss=3.1426191329956055
I0127 18:11:48.107033 139822745589568 spec.py:321] Evaluating on the training split.
I0127 18:11:54.387903 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 18:12:03.193759 139822745589568 spec.py:349] Evaluating on the test split.
I0127 18:12:06.324349 139822745589568 submission_runner.py:408] Time since start: 17989.71s, 	Step: 50779, 	{'train/accuracy': 0.7400350570678711, 'train/loss': 1.2261372804641724, 'validation/accuracy': 0.6668199896812439, 'validation/loss': 1.554896354675293, 'validation/num_examples': 50000, 'test/accuracy': 0.5362000465393066, 'test/loss': 2.243440628051758, 'test/num_examples': 10000, 'score': 17375.061665296555, 'total_duration': 17989.708940029144, 'accumulated_submission_time': 17375.061665296555, 'accumulated_eval_time': 611.8048617839813, 'accumulated_logging_time': 1.1133863925933838}
I0127 18:12:06.349913 139656800745216 logging_writer.py:48] [50779] accumulated_eval_time=611.804862, accumulated_logging_time=1.113386, accumulated_submission_time=17375.061665, global_step=50779, preemption_count=0, score=17375.061665, test/accuracy=0.536200, test/loss=2.243441, test/num_examples=10000, total_duration=17989.708940, train/accuracy=0.740035, train/loss=1.226137, validation/accuracy=0.666820, validation/loss=1.554896, validation/num_examples=50000
I0127 18:12:13.829233 139656809137920 logging_writer.py:48] [50800] global_step=50800, grad_norm=1.6873255968093872, loss=3.1243174076080322
I0127 18:12:47.906073 139656800745216 logging_writer.py:48] [50900] global_step=50900, grad_norm=1.7820295095443726, loss=3.166802406311035
I0127 18:13:21.990907 139656809137920 logging_writer.py:48] [51000] global_step=51000, grad_norm=1.7964552640914917, loss=3.1192054748535156
I0127 18:13:56.118773 139656800745216 logging_writer.py:48] [51100] global_step=51100, grad_norm=1.775617241859436, loss=3.0968315601348877
I0127 18:14:30.291084 139656809137920 logging_writer.py:48] [51200] global_step=51200, grad_norm=1.8094818592071533, loss=3.152508020401001
I0127 18:15:04.431224 139656800745216 logging_writer.py:48] [51300] global_step=51300, grad_norm=1.8610005378723145, loss=3.141674518585205
I0127 18:15:38.520087 139656809137920 logging_writer.py:48] [51400] global_step=51400, grad_norm=1.6585241556167603, loss=3.1185498237609863
I0127 18:16:12.658606 139656800745216 logging_writer.py:48] [51500] global_step=51500, grad_norm=1.7639667987823486, loss=3.207359552383423
I0127 18:16:46.776670 139656809137920 logging_writer.py:48] [51600] global_step=51600, grad_norm=1.6889326572418213, loss=3.141873598098755
I0127 18:17:20.907287 139656800745216 logging_writer.py:48] [51700] global_step=51700, grad_norm=1.8461928367614746, loss=3.1763627529144287
I0127 18:17:55.009085 139656809137920 logging_writer.py:48] [51800] global_step=51800, grad_norm=1.685408353805542, loss=3.1879677772521973
I0127 18:18:29.143226 139656800745216 logging_writer.py:48] [51900] global_step=51900, grad_norm=1.792209267616272, loss=3.102437973022461
I0127 18:19:03.267925 139656809137920 logging_writer.py:48] [52000] global_step=52000, grad_norm=1.7064716815948486, loss=3.0831704139709473
I0127 18:19:37.412340 139656800745216 logging_writer.py:48] [52100] global_step=52100, grad_norm=1.7765072584152222, loss=3.167444944381714
I0127 18:20:11.516879 139656809137920 logging_writer.py:48] [52200] global_step=52200, grad_norm=1.7253254652023315, loss=3.181871175765991
I0127 18:20:36.639342 139822745589568 spec.py:321] Evaluating on the training split.
I0127 18:20:42.843612 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 18:20:51.442481 139822745589568 spec.py:349] Evaluating on the test split.
I0127 18:20:53.910690 139822745589568 submission_runner.py:408] Time since start: 18517.30s, 	Step: 52275, 	{'train/accuracy': 0.7399553656578064, 'train/loss': 1.244918942451477, 'validation/accuracy': 0.6628599762916565, 'validation/loss': 1.5738775730133057, 'validation/num_examples': 50000, 'test/accuracy': 0.5449000000953674, 'test/loss': 2.2030839920043945, 'test/num_examples': 10000, 'score': 17885.29244852066, 'total_duration': 18517.295283079147, 'accumulated_submission_time': 17885.29244852066, 'accumulated_eval_time': 629.0761721134186, 'accumulated_logging_time': 1.1479730606079102}
I0127 18:20:53.940209 139656783959808 logging_writer.py:48] [52275] accumulated_eval_time=629.076172, accumulated_logging_time=1.147973, accumulated_submission_time=17885.292449, global_step=52275, preemption_count=0, score=17885.292449, test/accuracy=0.544900, test/loss=2.203084, test/num_examples=10000, total_duration=18517.295283, train/accuracy=0.739955, train/loss=1.244919, validation/accuracy=0.662860, validation/loss=1.573878, validation/num_examples=50000
I0127 18:21:02.792502 139656792352512 logging_writer.py:48] [52300] global_step=52300, grad_norm=1.937638759613037, loss=3.1462790966033936
I0127 18:21:36.868911 139656783959808 logging_writer.py:48] [52400] global_step=52400, grad_norm=1.9225702285766602, loss=3.150192975997925
I0127 18:22:10.971171 139656792352512 logging_writer.py:48] [52500] global_step=52500, grad_norm=1.8624904155731201, loss=3.1045353412628174
I0127 18:22:45.096429 139656783959808 logging_writer.py:48] [52600] global_step=52600, grad_norm=1.7034120559692383, loss=3.2300543785095215
I0127 18:23:19.198608 139656792352512 logging_writer.py:48] [52700] global_step=52700, grad_norm=1.7667726278305054, loss=3.0565929412841797
I0127 18:23:53.317535 139656783959808 logging_writer.py:48] [52800] global_step=52800, grad_norm=1.8673112392425537, loss=3.157740592956543
I0127 18:24:27.437747 139656792352512 logging_writer.py:48] [52900] global_step=52900, grad_norm=1.77932608127594, loss=3.1272165775299072
I0127 18:25:01.553572 139656783959808 logging_writer.py:48] [53000] global_step=53000, grad_norm=1.6651747226715088, loss=3.1354868412017822
I0127 18:25:35.673311 139656792352512 logging_writer.py:48] [53100] global_step=53100, grad_norm=1.7076473236083984, loss=3.1489803791046143
I0127 18:26:09.788390 139656783959808 logging_writer.py:48] [53200] global_step=53200, grad_norm=1.7742897272109985, loss=3.1124980449676514
I0127 18:26:43.914051 139656792352512 logging_writer.py:48] [53300] global_step=53300, grad_norm=1.7998406887054443, loss=3.1799752712249756
I0127 18:27:18.127711 139656783959808 logging_writer.py:48] [53400] global_step=53400, grad_norm=1.8637820482254028, loss=3.1766912937164307
I0127 18:27:52.255343 139656792352512 logging_writer.py:48] [53500] global_step=53500, grad_norm=1.799159049987793, loss=3.065795660018921
I0127 18:28:26.390909 139656783959808 logging_writer.py:48] [53600] global_step=53600, grad_norm=1.9444557428359985, loss=3.1512222290039062
I0127 18:29:00.487866 139656792352512 logging_writer.py:48] [53700] global_step=53700, grad_norm=1.7778518199920654, loss=3.154574394226074
I0127 18:29:24.193987 139822745589568 spec.py:321] Evaluating on the training split.
I0127 18:29:30.460947 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 18:29:38.947972 139822745589568 spec.py:349] Evaluating on the test split.
I0127 18:29:41.374434 139822745589568 submission_runner.py:408] Time since start: 19044.76s, 	Step: 53771, 	{'train/accuracy': 0.7357900142669678, 'train/loss': 1.252927303314209, 'validation/accuracy': 0.6644200086593628, 'validation/loss': 1.56941819190979, 'validation/num_examples': 50000, 'test/accuracy': 0.5424000024795532, 'test/loss': 2.2293903827667236, 'test/num_examples': 10000, 'score': 18395.48710179329, 'total_duration': 19044.75901412964, 'accumulated_submission_time': 18395.48710179329, 'accumulated_eval_time': 646.256591796875, 'accumulated_logging_time': 1.1864268779754639}
I0127 18:29:41.406308 139656783959808 logging_writer.py:48] [53771] accumulated_eval_time=646.256592, accumulated_logging_time=1.186427, accumulated_submission_time=18395.487102, global_step=53771, preemption_count=0, score=18395.487102, test/accuracy=0.542400, test/loss=2.229390, test/num_examples=10000, total_duration=19044.759014, train/accuracy=0.735790, train/loss=1.252927, validation/accuracy=0.664420, validation/loss=1.569418, validation/num_examples=50000
I0127 18:29:51.634393 139656809137920 logging_writer.py:48] [53800] global_step=53800, grad_norm=1.799554467201233, loss=3.116727352142334
I0127 18:30:25.748530 139656783959808 logging_writer.py:48] [53900] global_step=53900, grad_norm=1.87522292137146, loss=3.154946804046631
I0127 18:30:59.852893 139656809137920 logging_writer.py:48] [54000] global_step=54000, grad_norm=1.7905025482177734, loss=3.106959342956543
I0127 18:31:33.953919 139656783959808 logging_writer.py:48] [54100] global_step=54100, grad_norm=1.8677802085876465, loss=3.2169618606567383
I0127 18:32:08.078729 139656809137920 logging_writer.py:48] [54200] global_step=54200, grad_norm=1.9468352794647217, loss=3.168755292892456
I0127 18:32:42.208820 139656783959808 logging_writer.py:48] [54300] global_step=54300, grad_norm=1.7636007070541382, loss=3.133786678314209
I0127 18:33:16.482742 139656809137920 logging_writer.py:48] [54400] global_step=54400, grad_norm=2.1185202598571777, loss=3.1681652069091797
I0127 18:33:50.615169 139656783959808 logging_writer.py:48] [54500] global_step=54500, grad_norm=1.8816159963607788, loss=3.2211084365844727
I0127 18:34:24.724267 139656809137920 logging_writer.py:48] [54600] global_step=54600, grad_norm=1.9111779928207397, loss=3.1485626697540283
I0127 18:34:58.828208 139656783959808 logging_writer.py:48] [54700] global_step=54700, grad_norm=2.1241583824157715, loss=3.1749467849731445
I0127 18:35:32.992296 139656809137920 logging_writer.py:48] [54800] global_step=54800, grad_norm=1.9331895112991333, loss=3.156707525253296
I0127 18:36:07.161024 139656783959808 logging_writer.py:48] [54900] global_step=54900, grad_norm=1.80103600025177, loss=3.1153435707092285
I0127 18:36:41.281591 139656809137920 logging_writer.py:48] [55000] global_step=55000, grad_norm=1.9161845445632935, loss=3.1646652221679688
I0127 18:37:15.421701 139656783959808 logging_writer.py:48] [55100] global_step=55100, grad_norm=1.790846586227417, loss=3.121173143386841
I0127 18:37:49.512329 139656809137920 logging_writer.py:48] [55200] global_step=55200, grad_norm=2.1030497550964355, loss=3.1570541858673096
I0127 18:38:11.491716 139822745589568 spec.py:321] Evaluating on the training split.
I0127 18:38:17.733960 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 18:38:26.377429 139822745589568 spec.py:349] Evaluating on the test split.
I0127 18:38:28.824092 139822745589568 submission_runner.py:408] Time since start: 19572.21s, 	Step: 55266, 	{'train/accuracy': 0.7410913705825806, 'train/loss': 1.232458233833313, 'validation/accuracy': 0.6733999848365784, 'validation/loss': 1.5392425060272217, 'validation/num_examples': 50000, 'test/accuracy': 0.5421000123023987, 'test/loss': 2.2132768630981445, 'test/num_examples': 10000, 'score': 18905.508989572525, 'total_duration': 19572.208678483963, 'accumulated_submission_time': 18905.508989572525, 'accumulated_eval_time': 663.5889291763306, 'accumulated_logging_time': 1.2300894260406494}
I0127 18:38:28.851980 139656792352512 logging_writer.py:48] [55266] accumulated_eval_time=663.588929, accumulated_logging_time=1.230089, accumulated_submission_time=18905.508990, global_step=55266, preemption_count=0, score=18905.508990, test/accuracy=0.542100, test/loss=2.213277, test/num_examples=10000, total_duration=19572.208678, train/accuracy=0.741091, train/loss=1.232458, validation/accuracy=0.673400, validation/loss=1.539243, validation/num_examples=50000
I0127 18:38:40.797054 139656800745216 logging_writer.py:48] [55300] global_step=55300, grad_norm=2.0520248413085938, loss=3.1570751667022705
I0127 18:39:14.840994 139656792352512 logging_writer.py:48] [55400] global_step=55400, grad_norm=1.8035626411437988, loss=3.1502127647399902
I0127 18:39:49.049993 139656800745216 logging_writer.py:48] [55500] global_step=55500, grad_norm=1.8776901960372925, loss=3.163682460784912
I0127 18:40:23.174182 139656792352512 logging_writer.py:48] [55600] global_step=55600, grad_norm=1.8228273391723633, loss=3.1347076892852783
I0127 18:40:57.308529 139656800745216 logging_writer.py:48] [55700] global_step=55700, grad_norm=1.877457857131958, loss=3.1823601722717285
I0127 18:41:31.458622 139656792352512 logging_writer.py:48] [55800] global_step=55800, grad_norm=1.8017135858535767, loss=3.162280797958374
I0127 18:42:05.600971 139656800745216 logging_writer.py:48] [55900] global_step=55900, grad_norm=1.9243849515914917, loss=3.194092273712158
I0127 18:42:39.728330 139656792352512 logging_writer.py:48] [56000] global_step=56000, grad_norm=1.912752628326416, loss=3.19673490524292
I0127 18:43:13.856454 139656800745216 logging_writer.py:48] [56100] global_step=56100, grad_norm=1.8931397199630737, loss=3.0863664150238037
I0127 18:43:47.982808 139656792352512 logging_writer.py:48] [56200] global_step=56200, grad_norm=1.7658483982086182, loss=3.1339824199676514
I0127 18:44:22.097012 139656800745216 logging_writer.py:48] [56300] global_step=56300, grad_norm=1.7372609376907349, loss=3.1537721157073975
I0127 18:44:56.224492 139656792352512 logging_writer.py:48] [56400] global_step=56400, grad_norm=1.859984040260315, loss=3.109820604324341
I0127 18:45:30.355685 139656800745216 logging_writer.py:48] [56500] global_step=56500, grad_norm=1.8092049360275269, loss=3.104658365249634
I0127 18:46:04.539564 139656792352512 logging_writer.py:48] [56600] global_step=56600, grad_norm=1.8327317237854004, loss=3.0730197429656982
I0127 18:46:38.677081 139656800745216 logging_writer.py:48] [56700] global_step=56700, grad_norm=1.9575366973876953, loss=3.2066102027893066
I0127 18:46:58.951962 139822745589568 spec.py:321] Evaluating on the training split.
I0127 18:47:05.273386 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 18:47:13.826698 139822745589568 spec.py:349] Evaluating on the test split.
I0127 18:47:16.250050 139822745589568 submission_runner.py:408] Time since start: 20099.63s, 	Step: 56761, 	{'train/accuracy': 0.7707669138908386, 'train/loss': 1.1692932844161987, 'validation/accuracy': 0.6709399819374084, 'validation/loss': 1.5907434225082397, 'validation/num_examples': 50000, 'test/accuracy': 0.5403000116348267, 'test/loss': 2.260906457901001, 'test/num_examples': 10000, 'score': 19415.54997587204, 'total_duration': 20099.63463830948, 'accumulated_submission_time': 19415.54997587204, 'accumulated_eval_time': 680.8869771957397, 'accumulated_logging_time': 1.26725435256958}
I0127 18:47:16.278205 139656817530624 logging_writer.py:48] [56761] accumulated_eval_time=680.886977, accumulated_logging_time=1.267254, accumulated_submission_time=19415.549976, global_step=56761, preemption_count=0, score=19415.549976, test/accuracy=0.540300, test/loss=2.260906, test/num_examples=10000, total_duration=20099.634638, train/accuracy=0.770767, train/loss=1.169293, validation/accuracy=0.670940, validation/loss=1.590743, validation/num_examples=50000
I0127 18:47:29.915742 139656834316032 logging_writer.py:48] [56800] global_step=56800, grad_norm=1.9610239267349243, loss=3.100475311279297
I0127 18:48:03.996589 139656817530624 logging_writer.py:48] [56900] global_step=56900, grad_norm=1.8773728609085083, loss=3.1235082149505615
I0127 18:48:38.135196 139656834316032 logging_writer.py:48] [57000] global_step=57000, grad_norm=1.8855531215667725, loss=3.162367582321167
I0127 18:49:12.269210 139656817530624 logging_writer.py:48] [57100] global_step=57100, grad_norm=1.8301349878311157, loss=3.0202689170837402
I0127 18:49:46.388653 139656834316032 logging_writer.py:48] [57200] global_step=57200, grad_norm=1.8203634023666382, loss=3.0969886779785156
I0127 18:50:20.511010 139656817530624 logging_writer.py:48] [57300] global_step=57300, grad_norm=1.818526029586792, loss=3.143672227859497
I0127 18:50:54.653719 139656834316032 logging_writer.py:48] [57400] global_step=57400, grad_norm=1.8330963850021362, loss=3.0383572578430176
I0127 18:51:28.780794 139656817530624 logging_writer.py:48] [57500] global_step=57500, grad_norm=1.7366284132003784, loss=3.0941693782806396
I0127 18:52:03.109400 139656834316032 logging_writer.py:48] [57600] global_step=57600, grad_norm=1.8945938348770142, loss=3.120194673538208
I0127 18:52:37.217144 139656817530624 logging_writer.py:48] [57700] global_step=57700, grad_norm=1.810231328010559, loss=3.1055455207824707
I0127 18:53:11.352496 139656834316032 logging_writer.py:48] [57800] global_step=57800, grad_norm=1.8901866674423218, loss=3.1694326400756836
I0127 18:53:45.459021 139656817530624 logging_writer.py:48] [57900] global_step=57900, grad_norm=1.9304721355438232, loss=3.0749683380126953
I0127 18:54:19.565423 139656834316032 logging_writer.py:48] [58000] global_step=58000, grad_norm=1.8228447437286377, loss=3.109436511993408
I0127 18:54:53.689118 139656817530624 logging_writer.py:48] [58100] global_step=58100, grad_norm=1.9213223457336426, loss=3.170196056365967
I0127 18:55:27.838631 139656834316032 logging_writer.py:48] [58200] global_step=58200, grad_norm=1.9862728118896484, loss=3.0882315635681152
I0127 18:55:46.399942 139822745589568 spec.py:321] Evaluating on the training split.
I0127 18:55:52.676587 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 18:56:01.196660 139822745589568 spec.py:349] Evaluating on the test split.
I0127 18:56:03.816121 139822745589568 submission_runner.py:408] Time since start: 20627.20s, 	Step: 58256, 	{'train/accuracy': 0.7694514989852905, 'train/loss': 1.1050457954406738, 'validation/accuracy': 0.6767799854278564, 'validation/loss': 1.4937835931777954, 'validation/num_examples': 50000, 'test/accuracy': 0.5485000014305115, 'test/loss': 2.1709296703338623, 'test/num_examples': 10000, 'score': 19925.609867811203, 'total_duration': 20627.20070528984, 'accumulated_submission_time': 19925.609867811203, 'accumulated_eval_time': 698.3031196594238, 'accumulated_logging_time': 1.3048710823059082}
I0127 18:56:03.846344 139656809137920 logging_writer.py:48] [58256] accumulated_eval_time=698.303120, accumulated_logging_time=1.304871, accumulated_submission_time=19925.609868, global_step=58256, preemption_count=0, score=19925.609868, test/accuracy=0.548500, test/loss=2.170930, test/num_examples=10000, total_duration=20627.200705, train/accuracy=0.769451, train/loss=1.105046, validation/accuracy=0.676780, validation/loss=1.493784, validation/num_examples=50000
I0127 18:56:19.174283 139656825923328 logging_writer.py:48] [58300] global_step=58300, grad_norm=1.8638046979904175, loss=3.0712616443634033
I0127 18:56:53.263168 139656809137920 logging_writer.py:48] [58400] global_step=58400, grad_norm=2.004619598388672, loss=3.163449764251709
I0127 18:57:27.349416 139656825923328 logging_writer.py:48] [58500] global_step=58500, grad_norm=2.020603895187378, loss=3.0757601261138916
I0127 18:58:01.463802 139656809137920 logging_writer.py:48] [58600] global_step=58600, grad_norm=1.8344186544418335, loss=3.1282906532287598
I0127 18:58:35.683341 139656825923328 logging_writer.py:48] [58700] global_step=58700, grad_norm=1.8275682926177979, loss=3.1802868843078613
I0127 18:59:09.797159 139656809137920 logging_writer.py:48] [58800] global_step=58800, grad_norm=1.9803611040115356, loss=3.0682473182678223
I0127 18:59:43.939747 139656825923328 logging_writer.py:48] [58900] global_step=58900, grad_norm=1.8406703472137451, loss=3.043764591217041
I0127 19:00:18.032945 139656809137920 logging_writer.py:48] [59000] global_step=59000, grad_norm=1.9169150590896606, loss=3.1156532764434814
I0127 19:00:52.142001 139656825923328 logging_writer.py:48] [59100] global_step=59100, grad_norm=1.922296166419983, loss=2.999711036682129
I0127 19:01:26.252368 139656809137920 logging_writer.py:48] [59200] global_step=59200, grad_norm=1.9015722274780273, loss=3.159548759460449
I0127 19:02:00.356820 139656825923328 logging_writer.py:48] [59300] global_step=59300, grad_norm=1.8018913269042969, loss=3.0496034622192383
I0127 19:02:34.484080 139656809137920 logging_writer.py:48] [59400] global_step=59400, grad_norm=1.8841164112091064, loss=2.9997854232788086
I0127 19:03:08.610353 139656825923328 logging_writer.py:48] [59500] global_step=59500, grad_norm=1.993973731994629, loss=3.1393771171569824
I0127 19:03:42.711450 139656809137920 logging_writer.py:48] [59600] global_step=59600, grad_norm=1.8665647506713867, loss=3.0911171436309814
I0127 19:04:16.830392 139656825923328 logging_writer.py:48] [59700] global_step=59700, grad_norm=1.8738234043121338, loss=3.0742883682250977
I0127 19:04:34.101952 139822745589568 spec.py:321] Evaluating on the training split.
I0127 19:04:40.349056 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 19:04:49.065206 139822745589568 spec.py:349] Evaluating on the test split.
I0127 19:04:51.457054 139822745589568 submission_runner.py:408] Time since start: 21154.84s, 	Step: 59752, 	{'train/accuracy': 0.7449377775192261, 'train/loss': 1.1936075687408447, 'validation/accuracy': 0.666979968547821, 'validation/loss': 1.5506370067596436, 'validation/num_examples': 50000, 'test/accuracy': 0.5371000170707703, 'test/loss': 2.2205097675323486, 'test/num_examples': 10000, 'score': 20435.80241703987, 'total_duration': 21154.84163069725, 'accumulated_submission_time': 20435.80241703987, 'accumulated_eval_time': 715.6581652164459, 'accumulated_logging_time': 1.3460206985473633}
I0127 19:04:51.490364 139656800745216 logging_writer.py:48] [59752] accumulated_eval_time=715.658165, accumulated_logging_time=1.346021, accumulated_submission_time=20435.802417, global_step=59752, preemption_count=0, score=20435.802417, test/accuracy=0.537100, test/loss=2.220510, test/num_examples=10000, total_duration=21154.841631, train/accuracy=0.744938, train/loss=1.193608, validation/accuracy=0.666980, validation/loss=1.550637, validation/num_examples=50000
I0127 19:05:08.196825 139656817530624 logging_writer.py:48] [59800] global_step=59800, grad_norm=2.0033528804779053, loss=3.074334144592285
I0127 19:05:42.269869 139656800745216 logging_writer.py:48] [59900] global_step=59900, grad_norm=1.9968760013580322, loss=3.107104778289795
I0127 19:06:16.367165 139656817530624 logging_writer.py:48] [60000] global_step=60000, grad_norm=1.833229899406433, loss=3.074002981185913
I0127 19:06:50.494798 139656800745216 logging_writer.py:48] [60100] global_step=60100, grad_norm=1.911216378211975, loss=3.1134727001190186
I0127 19:07:24.634288 139656817530624 logging_writer.py:48] [60200] global_step=60200, grad_norm=1.8601523637771606, loss=3.097883701324463
I0127 19:07:58.760836 139656800745216 logging_writer.py:48] [60300] global_step=60300, grad_norm=1.9641033411026, loss=3.172987937927246
I0127 19:08:32.891703 139656817530624 logging_writer.py:48] [60400] global_step=60400, grad_norm=1.9706224203109741, loss=3.008460760116577
I0127 19:09:07.027674 139656800745216 logging_writer.py:48] [60500] global_step=60500, grad_norm=1.8672854900360107, loss=3.1381494998931885
I0127 19:09:41.148401 139656817530624 logging_writer.py:48] [60600] global_step=60600, grad_norm=1.9345303773880005, loss=3.095069646835327
I0127 19:10:15.255502 139656800745216 logging_writer.py:48] [60700] global_step=60700, grad_norm=2.001704216003418, loss=3.05656099319458
I0127 19:10:49.448526 139656817530624 logging_writer.py:48] [60800] global_step=60800, grad_norm=2.0139739513397217, loss=3.110335350036621
I0127 19:11:23.544667 139656800745216 logging_writer.py:48] [60900] global_step=60900, grad_norm=1.9488551616668701, loss=3.10270094871521
I0127 19:11:57.690925 139656817530624 logging_writer.py:48] [61000] global_step=61000, grad_norm=1.8946924209594727, loss=3.1230661869049072
I0127 19:12:31.787483 139656800745216 logging_writer.py:48] [61100] global_step=61100, grad_norm=2.001865863800049, loss=3.1448869705200195
I0127 19:13:05.918155 139656817530624 logging_writer.py:48] [61200] global_step=61200, grad_norm=1.9730987548828125, loss=3.243126392364502
I0127 19:13:21.762924 139822745589568 spec.py:321] Evaluating on the training split.
I0127 19:13:27.914399 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 19:13:36.324864 139822745589568 spec.py:349] Evaluating on the test split.
I0127 19:13:38.726866 139822745589568 submission_runner.py:408] Time since start: 21682.11s, 	Step: 61248, 	{'train/accuracy': 0.7509366869926453, 'train/loss': 1.1979851722717285, 'validation/accuracy': 0.6717399954795837, 'validation/loss': 1.540035605430603, 'validation/num_examples': 50000, 'test/accuracy': 0.5407000184059143, 'test/loss': 2.2069180011749268, 'test/num_examples': 10000, 'score': 20946.015642881393, 'total_duration': 21682.111456632614, 'accumulated_submission_time': 20946.015642881393, 'accumulated_eval_time': 732.6220688819885, 'accumulated_logging_time': 1.3891007900238037}
I0127 19:13:38.755461 139656800745216 logging_writer.py:48] [61248] accumulated_eval_time=732.622069, accumulated_logging_time=1.389101, accumulated_submission_time=20946.015643, global_step=61248, preemption_count=0, score=20946.015643, test/accuracy=0.540700, test/loss=2.206918, test/num_examples=10000, total_duration=21682.111457, train/accuracy=0.750937, train/loss=1.197985, validation/accuracy=0.671740, validation/loss=1.540036, validation/num_examples=50000
I0127 19:13:56.818664 139656809137920 logging_writer.py:48] [61300] global_step=61300, grad_norm=1.903564214706421, loss=3.117501974105835
I0127 19:14:30.879265 139656800745216 logging_writer.py:48] [61400] global_step=61400, grad_norm=1.9233931303024292, loss=3.113290309906006
I0127 19:15:04.998490 139656809137920 logging_writer.py:48] [61500] global_step=61500, grad_norm=1.8809658288955688, loss=3.0898077487945557
I0127 19:15:39.110509 139656800745216 logging_writer.py:48] [61600] global_step=61600, grad_norm=2.0331809520721436, loss=3.0489189624786377
I0127 19:16:13.230995 139656809137920 logging_writer.py:48] [61700] global_step=61700, grad_norm=2.015509843826294, loss=3.112765312194824
I0127 19:16:47.356818 139656800745216 logging_writer.py:48] [61800] global_step=61800, grad_norm=1.9257904291152954, loss=3.0180468559265137
I0127 19:17:21.533043 139656809137920 logging_writer.py:48] [61900] global_step=61900, grad_norm=1.994390845298767, loss=3.1352486610412598
I0127 19:17:55.643987 139656800745216 logging_writer.py:48] [62000] global_step=62000, grad_norm=2.0774402618408203, loss=3.0552620887756348
I0127 19:18:29.769000 139656809137920 logging_writer.py:48] [62100] global_step=62100, grad_norm=2.073976993560791, loss=3.1266071796417236
I0127 19:19:03.911279 139656800745216 logging_writer.py:48] [62200] global_step=62200, grad_norm=1.8430689573287964, loss=3.0969982147216797
I0127 19:19:38.048375 139656809137920 logging_writer.py:48] [62300] global_step=62300, grad_norm=1.9679371118545532, loss=3.0721209049224854
I0127 19:20:12.169272 139656800745216 logging_writer.py:48] [62400] global_step=62400, grad_norm=2.1276421546936035, loss=3.1120052337646484
I0127 19:20:46.301527 139656809137920 logging_writer.py:48] [62500] global_step=62500, grad_norm=2.0508902072906494, loss=3.158736228942871
I0127 19:21:20.446238 139656800745216 logging_writer.py:48] [62600] global_step=62600, grad_norm=1.9704581499099731, loss=3.0806453227996826
I0127 19:21:54.579049 139656809137920 logging_writer.py:48] [62700] global_step=62700, grad_norm=2.0981831550598145, loss=3.177788734436035
I0127 19:22:09.063506 139822745589568 spec.py:321] Evaluating on the training split.
I0127 19:22:15.233312 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 19:22:23.772702 139822745589568 spec.py:349] Evaluating on the test split.
I0127 19:22:26.141099 139822745589568 submission_runner.py:408] Time since start: 22209.53s, 	Step: 62744, 	{'train/accuracy': 0.7419483065605164, 'train/loss': 1.2212562561035156, 'validation/accuracy': 0.6708199977874756, 'validation/loss': 1.5463603734970093, 'validation/num_examples': 50000, 'test/accuracy': 0.5323000550270081, 'test/loss': 2.2445785999298096, 'test/num_examples': 10000, 'score': 21456.262974262238, 'total_duration': 22209.52569293976, 'accumulated_submission_time': 21456.262974262238, 'accumulated_eval_time': 749.6996276378632, 'accumulated_logging_time': 1.426433801651001}
I0127 19:22:26.170155 139656783959808 logging_writer.py:48] [62744] accumulated_eval_time=749.699628, accumulated_logging_time=1.426434, accumulated_submission_time=21456.262974, global_step=62744, preemption_count=0, score=21456.262974, test/accuracy=0.532300, test/loss=2.244579, test/num_examples=10000, total_duration=22209.525693, train/accuracy=0.741948, train/loss=1.221256, validation/accuracy=0.670820, validation/loss=1.546360, validation/num_examples=50000
I0127 19:22:45.610960 139656792352512 logging_writer.py:48] [62800] global_step=62800, grad_norm=2.218339443206787, loss=3.0869414806365967
I0127 19:23:19.763134 139656783959808 logging_writer.py:48] [62900] global_step=62900, grad_norm=2.0210416316986084, loss=3.079397678375244
I0127 19:23:53.866145 139656792352512 logging_writer.py:48] [63000] global_step=63000, grad_norm=1.9158278703689575, loss=3.046445369720459
I0127 19:24:28.004128 139656783959808 logging_writer.py:48] [63100] global_step=63100, grad_norm=1.9226560592651367, loss=3.1303930282592773
I0127 19:25:02.127000 139656792352512 logging_writer.py:48] [63200] global_step=63200, grad_norm=1.9689525365829468, loss=3.024529457092285
I0127 19:25:36.231328 139656783959808 logging_writer.py:48] [63300] global_step=63300, grad_norm=2.0402379035949707, loss=3.2095108032226562
I0127 19:26:10.320645 139656792352512 logging_writer.py:48] [63400] global_step=63400, grad_norm=2.0428433418273926, loss=3.132901430130005
I0127 19:26:44.443286 139656783959808 logging_writer.py:48] [63500] global_step=63500, grad_norm=2.034332036972046, loss=3.131056785583496
I0127 19:27:18.586087 139656792352512 logging_writer.py:48] [63600] global_step=63600, grad_norm=1.996696949005127, loss=3.0933268070220947
I0127 19:27:52.736729 139656783959808 logging_writer.py:48] [63700] global_step=63700, grad_norm=1.933030128479004, loss=3.112631320953369
I0127 19:28:26.897652 139656792352512 logging_writer.py:48] [63800] global_step=63800, grad_norm=1.9472296237945557, loss=3.093877077102661
I0127 19:29:01.033181 139656783959808 logging_writer.py:48] [63900] global_step=63900, grad_norm=1.9048807621002197, loss=3.042478322982788
I0127 19:29:35.240862 139656792352512 logging_writer.py:48] [64000] global_step=64000, grad_norm=2.0851941108703613, loss=3.0870895385742188
I0127 19:30:09.379227 139656783959808 logging_writer.py:48] [64100] global_step=64100, grad_norm=1.9008393287658691, loss=3.042613983154297
I0127 19:30:43.534533 139656792352512 logging_writer.py:48] [64200] global_step=64200, grad_norm=1.8393781185150146, loss=3.08293080329895
I0127 19:30:56.323511 139822745589568 spec.py:321] Evaluating on the training split.
I0127 19:31:02.629104 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 19:31:11.159321 139822745589568 spec.py:349] Evaluating on the test split.
I0127 19:31:13.640962 139822745589568 submission_runner.py:408] Time since start: 22737.03s, 	Step: 64239, 	{'train/accuracy': 0.7436822056770325, 'train/loss': 1.2639172077178955, 'validation/accuracy': 0.6767199635505676, 'validation/loss': 1.5742335319519043, 'validation/num_examples': 50000, 'test/accuracy': 0.5462000370025635, 'test/loss': 2.255506992340088, 'test/num_examples': 10000, 'score': 21966.35502266884, 'total_duration': 22737.02538251877, 'accumulated_submission_time': 21966.35502266884, 'accumulated_eval_time': 767.0168855190277, 'accumulated_logging_time': 1.464949369430542}
I0127 19:31:13.673479 139656817530624 logging_writer.py:48] [64239] accumulated_eval_time=767.016886, accumulated_logging_time=1.464949, accumulated_submission_time=21966.355023, global_step=64239, preemption_count=0, score=21966.355023, test/accuracy=0.546200, test/loss=2.255507, test/num_examples=10000, total_duration=22737.025383, train/accuracy=0.743682, train/loss=1.263917, validation/accuracy=0.676720, validation/loss=1.574234, validation/num_examples=50000
I0127 19:31:34.824028 139656825923328 logging_writer.py:48] [64300] global_step=64300, grad_norm=1.9437613487243652, loss=3.0813448429107666
I0127 19:32:08.919573 139656817530624 logging_writer.py:48] [64400] global_step=64400, grad_norm=2.003387451171875, loss=3.121295213699341
I0127 19:32:43.027637 139656825923328 logging_writer.py:48] [64500] global_step=64500, grad_norm=1.965919852256775, loss=3.0825870037078857
I0127 19:33:17.167919 139656817530624 logging_writer.py:48] [64600] global_step=64600, grad_norm=1.888684630393982, loss=3.059709072113037
I0127 19:33:51.287878 139656825923328 logging_writer.py:48] [64700] global_step=64700, grad_norm=1.9168024063110352, loss=3.0804431438446045
I0127 19:34:25.412617 139656817530624 logging_writer.py:48] [64800] global_step=64800, grad_norm=1.8721965551376343, loss=3.1170501708984375
I0127 19:34:59.521347 139656825923328 logging_writer.py:48] [64900] global_step=64900, grad_norm=1.9981807470321655, loss=3.053499698638916
I0127 19:35:33.663734 139656817530624 logging_writer.py:48] [65000] global_step=65000, grad_norm=2.018282413482666, loss=3.047027587890625
I0127 19:36:07.818238 139656825923328 logging_writer.py:48] [65100] global_step=65100, grad_norm=2.0305440425872803, loss=3.0414347648620605
I0127 19:36:41.922695 139656817530624 logging_writer.py:48] [65200] global_step=65200, grad_norm=1.932484745979309, loss=3.030182361602783
I0127 19:37:16.042605 139656825923328 logging_writer.py:48] [65300] global_step=65300, grad_norm=2.0603067874908447, loss=3.072746515274048
I0127 19:37:50.184932 139656817530624 logging_writer.py:48] [65400] global_step=65400, grad_norm=2.034135341644287, loss=3.1040632724761963
I0127 19:38:24.279265 139656825923328 logging_writer.py:48] [65500] global_step=65500, grad_norm=1.9981319904327393, loss=3.029716968536377
I0127 19:38:58.418243 139656817530624 logging_writer.py:48] [65600] global_step=65600, grad_norm=2.112332344055176, loss=3.0770182609558105
I0127 19:39:32.509285 139656825923328 logging_writer.py:48] [65700] global_step=65700, grad_norm=2.118736505508423, loss=3.1062545776367188
I0127 19:39:43.888439 139822745589568 spec.py:321] Evaluating on the training split.
I0127 19:39:50.067507 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 19:39:58.590032 139822745589568 spec.py:349] Evaluating on the test split.
I0127 19:40:00.982842 139822745589568 submission_runner.py:408] Time since start: 23264.37s, 	Step: 65735, 	{'train/accuracy': 0.7479472160339355, 'train/loss': 1.2383085489273071, 'validation/accuracy': 0.6757599711418152, 'validation/loss': 1.5526641607284546, 'validation/num_examples': 50000, 'test/accuracy': 0.5468000173568726, 'test/loss': 2.2133982181549072, 'test/num_examples': 10000, 'score': 22476.510162115097, 'total_duration': 23264.367428541183, 'accumulated_submission_time': 22476.510162115097, 'accumulated_eval_time': 784.1112470626831, 'accumulated_logging_time': 1.5077550411224365}
I0127 19:40:01.011029 139656700098304 logging_writer.py:48] [65735] accumulated_eval_time=784.111247, accumulated_logging_time=1.507755, accumulated_submission_time=22476.510162, global_step=65735, preemption_count=0, score=22476.510162, test/accuracy=0.546800, test/loss=2.213398, test/num_examples=10000, total_duration=23264.367429, train/accuracy=0.747947, train/loss=1.238309, validation/accuracy=0.675760, validation/loss=1.552664, validation/num_examples=50000
I0127 19:40:23.495629 139656783959808 logging_writer.py:48] [65800] global_step=65800, grad_norm=2.0032424926757812, loss=2.988132953643799
I0127 19:40:57.568246 139656700098304 logging_writer.py:48] [65900] global_step=65900, grad_norm=1.8568114042282104, loss=3.173266649246216
I0127 19:41:31.686564 139656783959808 logging_writer.py:48] [66000] global_step=66000, grad_norm=1.936809778213501, loss=3.134739398956299
I0127 19:42:05.810806 139656700098304 logging_writer.py:48] [66100] global_step=66100, grad_norm=2.0652291774749756, loss=3.133833169937134
I0127 19:42:40.114259 139656783959808 logging_writer.py:48] [66200] global_step=66200, grad_norm=2.016110897064209, loss=3.1151459217071533
I0127 19:43:14.252190 139656700098304 logging_writer.py:48] [66300] global_step=66300, grad_norm=2.063628911972046, loss=3.111772060394287
I0127 19:43:48.378641 139656783959808 logging_writer.py:48] [66400] global_step=66400, grad_norm=2.1047897338867188, loss=3.149808645248413
I0127 19:44:22.511612 139656700098304 logging_writer.py:48] [66500] global_step=66500, grad_norm=1.9516749382019043, loss=3.0762522220611572
I0127 19:44:56.672590 139656783959808 logging_writer.py:48] [66600] global_step=66600, grad_norm=2.2846972942352295, loss=3.0658340454101562
I0127 19:45:30.807322 139656700098304 logging_writer.py:48] [66700] global_step=66700, grad_norm=2.187952995300293, loss=3.115664482116699
I0127 19:46:04.950643 139656783959808 logging_writer.py:48] [66800] global_step=66800, grad_norm=1.9631550312042236, loss=3.039928674697876
I0127 19:46:39.071058 139656700098304 logging_writer.py:48] [66900] global_step=66900, grad_norm=2.142552375793457, loss=3.1188628673553467
I0127 19:47:13.208146 139656783959808 logging_writer.py:48] [67000] global_step=67000, grad_norm=2.0733273029327393, loss=3.1005442142486572
I0127 19:47:47.341207 139656700098304 logging_writer.py:48] [67100] global_step=67100, grad_norm=2.025806427001953, loss=3.172727584838867
I0127 19:48:21.548390 139656783959808 logging_writer.py:48] [67200] global_step=67200, grad_norm=1.931132197380066, loss=3.054349899291992
I0127 19:48:31.257438 139822745589568 spec.py:321] Evaluating on the training split.
I0127 19:48:37.416131 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 19:48:45.780089 139822745589568 spec.py:349] Evaluating on the test split.
I0127 19:48:48.142921 139822745589568 submission_runner.py:408] Time since start: 23791.53s, 	Step: 67230, 	{'train/accuracy': 0.7716238498687744, 'train/loss': 1.069993019104004, 'validation/accuracy': 0.6693199872970581, 'validation/loss': 1.5192725658416748, 'validation/num_examples': 50000, 'test/accuracy': 0.5466000437736511, 'test/loss': 2.168231725692749, 'test/num_examples': 10000, 'score': 22986.69782590866, 'total_duration': 23791.52751684189, 'accumulated_submission_time': 22986.69782590866, 'accumulated_eval_time': 800.9966917037964, 'accumulated_logging_time': 1.5449728965759277}
I0127 19:48:48.171340 139656817530624 logging_writer.py:48] [67230] accumulated_eval_time=800.996692, accumulated_logging_time=1.544973, accumulated_submission_time=22986.697826, global_step=67230, preemption_count=0, score=22986.697826, test/accuracy=0.546600, test/loss=2.168232, test/num_examples=10000, total_duration=23791.527517, train/accuracy=0.771624, train/loss=1.069993, validation/accuracy=0.669320, validation/loss=1.519273, validation/num_examples=50000
I0127 19:49:12.353968 139656825923328 logging_writer.py:48] [67300] global_step=67300, grad_norm=1.961029052734375, loss=3.0509119033813477
I0127 19:49:46.394366 139656817530624 logging_writer.py:48] [67400] global_step=67400, grad_norm=2.1707403659820557, loss=3.110384464263916
I0127 19:50:20.495308 139656825923328 logging_writer.py:48] [67500] global_step=67500, grad_norm=1.983994483947754, loss=3.0184240341186523
I0127 19:50:54.620797 139656817530624 logging_writer.py:48] [67600] global_step=67600, grad_norm=2.266990900039673, loss=3.119218587875366
I0127 19:51:28.717846 139656825923328 logging_writer.py:48] [67700] global_step=67700, grad_norm=2.0947039127349854, loss=3.0745089054107666
I0127 19:52:02.849838 139656817530624 logging_writer.py:48] [67800] global_step=67800, grad_norm=1.986611008644104, loss=3.041067600250244
I0127 19:52:36.968909 139656825923328 logging_writer.py:48] [67900] global_step=67900, grad_norm=1.882585883140564, loss=2.9844162464141846
I0127 19:53:11.094288 139656817530624 logging_writer.py:48] [68000] global_step=68000, grad_norm=1.9898909330368042, loss=3.0688023567199707
I0127 19:53:45.224277 139656825923328 logging_writer.py:48] [68100] global_step=68100, grad_norm=1.9677143096923828, loss=3.135169506072998
I0127 19:54:19.357710 139656817530624 logging_writer.py:48] [68200] global_step=68200, grad_norm=2.0139806270599365, loss=3.11226487159729
I0127 19:54:53.527610 139656825923328 logging_writer.py:48] [68300] global_step=68300, grad_norm=2.0317676067352295, loss=3.089733600616455
I0127 19:55:27.651037 139656817530624 logging_writer.py:48] [68400] global_step=68400, grad_norm=2.0232958793640137, loss=3.078108549118042
I0127 19:56:01.779154 139656825923328 logging_writer.py:48] [68500] global_step=68500, grad_norm=1.988088846206665, loss=3.026189088821411
I0127 19:56:35.887098 139656817530624 logging_writer.py:48] [68600] global_step=68600, grad_norm=1.9967312812805176, loss=3.0229599475860596
I0127 19:57:09.996650 139656825923328 logging_writer.py:48] [68700] global_step=68700, grad_norm=1.9188337326049805, loss=3.0394582748413086
I0127 19:57:18.339664 139822745589568 spec.py:321] Evaluating on the training split.
I0127 19:57:24.489776 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 19:57:33.150688 139822745589568 spec.py:349] Evaluating on the test split.
I0127 19:57:35.465858 139822745589568 submission_runner.py:408] Time since start: 24318.85s, 	Step: 68726, 	{'train/accuracy': 0.7636120915412903, 'train/loss': 1.1651519536972046, 'validation/accuracy': 0.6775799989700317, 'validation/loss': 1.5437074899673462, 'validation/num_examples': 50000, 'test/accuracy': 0.5540000200271606, 'test/loss': 2.1980412006378174, 'test/num_examples': 10000, 'score': 23496.80472302437, 'total_duration': 24318.850444078445, 'accumulated_submission_time': 23496.80472302437, 'accumulated_eval_time': 818.1228411197662, 'accumulated_logging_time': 1.5834143161773682}
I0127 19:57:35.497462 139656800745216 logging_writer.py:48] [68726] accumulated_eval_time=818.122841, accumulated_logging_time=1.583414, accumulated_submission_time=23496.804723, global_step=68726, preemption_count=0, score=23496.804723, test/accuracy=0.554000, test/loss=2.198041, test/num_examples=10000, total_duration=24318.850444, train/accuracy=0.763612, train/loss=1.165152, validation/accuracy=0.677580, validation/loss=1.543707, validation/num_examples=50000
I0127 19:58:01.089289 139656809137920 logging_writer.py:48] [68800] global_step=68800, grad_norm=2.34318470954895, loss=3.1054558753967285
I0127 19:58:35.157594 139656800745216 logging_writer.py:48] [68900] global_step=68900, grad_norm=2.053014039993286, loss=3.014965534210205
I0127 19:59:09.255813 139656809137920 logging_writer.py:48] [69000] global_step=69000, grad_norm=2.162991762161255, loss=3.0830180644989014
I0127 19:59:43.405039 139656800745216 logging_writer.py:48] [69100] global_step=69100, grad_norm=2.1110787391662598, loss=3.053471088409424
I0127 20:00:17.528132 139656809137920 logging_writer.py:48] [69200] global_step=69200, grad_norm=2.0497007369995117, loss=3.0981740951538086
I0127 20:00:51.725131 139656800745216 logging_writer.py:48] [69300] global_step=69300, grad_norm=1.9647903442382812, loss=3.031559467315674
I0127 20:01:25.849579 139656809137920 logging_writer.py:48] [69400] global_step=69400, grad_norm=2.0850894451141357, loss=3.051574230194092
I0127 20:01:59.989921 139656800745216 logging_writer.py:48] [69500] global_step=69500, grad_norm=2.2648913860321045, loss=3.142241954803467
I0127 20:02:34.131582 139656809137920 logging_writer.py:48] [69600] global_step=69600, grad_norm=2.0389671325683594, loss=3.0391132831573486
I0127 20:03:08.265475 139656800745216 logging_writer.py:48] [69700] global_step=69700, grad_norm=2.0957682132720947, loss=3.0449585914611816
I0127 20:03:42.402063 139656809137920 logging_writer.py:48] [69800] global_step=69800, grad_norm=2.0785202980041504, loss=3.0006706714630127
I0127 20:04:16.551199 139656800745216 logging_writer.py:48] [69900] global_step=69900, grad_norm=2.2198081016540527, loss=3.0842792987823486
I0127 20:04:50.699525 139656809137920 logging_writer.py:48] [70000] global_step=70000, grad_norm=2.0287086963653564, loss=3.109344005584717
I0127 20:05:24.826618 139656800745216 logging_writer.py:48] [70100] global_step=70100, grad_norm=2.14494252204895, loss=3.105905055999756
I0127 20:05:58.931447 139656809137920 logging_writer.py:48] [70200] global_step=70200, grad_norm=2.253664970397949, loss=3.0879223346710205
I0127 20:06:05.562280 139822745589568 spec.py:321] Evaluating on the training split.
I0127 20:06:11.730746 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 20:06:20.386986 139822745589568 spec.py:349] Evaluating on the test split.
I0127 20:06:22.764790 139822745589568 submission_runner.py:408] Time since start: 24846.15s, 	Step: 70221, 	{'train/accuracy': 0.7656847834587097, 'train/loss': 1.155431866645813, 'validation/accuracy': 0.68367999792099, 'validation/loss': 1.5203362703323364, 'validation/num_examples': 50000, 'test/accuracy': 0.5615000128746033, 'test/loss': 2.1426637172698975, 'test/num_examples': 10000, 'score': 24006.80624294281, 'total_duration': 24846.14938569069, 'accumulated_submission_time': 24006.80624294281, 'accumulated_eval_time': 835.3253185749054, 'accumulated_logging_time': 1.6262106895446777}
I0127 20:06:22.794430 139656700098304 logging_writer.py:48] [70221] accumulated_eval_time=835.325319, accumulated_logging_time=1.626211, accumulated_submission_time=24006.806243, global_step=70221, preemption_count=0, score=24006.806243, test/accuracy=0.561500, test/loss=2.142664, test/num_examples=10000, total_duration=24846.149386, train/accuracy=0.765685, train/loss=1.155432, validation/accuracy=0.683680, validation/loss=1.520336, validation/num_examples=50000
I0127 20:06:50.061754 139656783959808 logging_writer.py:48] [70300] global_step=70300, grad_norm=2.0760459899902344, loss=3.0597498416900635
I0127 20:07:24.184143 139656700098304 logging_writer.py:48] [70400] global_step=70400, grad_norm=2.000460147857666, loss=3.0734446048736572
I0127 20:07:58.258772 139656783959808 logging_writer.py:48] [70500] global_step=70500, grad_norm=2.168123960494995, loss=3.026198387145996
I0127 20:08:32.380913 139656700098304 logging_writer.py:48] [70600] global_step=70600, grad_norm=2.0048282146453857, loss=3.05176043510437
I0127 20:09:06.523208 139656783959808 logging_writer.py:48] [70700] global_step=70700, grad_norm=2.0459108352661133, loss=3.133460521697998
I0127 20:09:40.639909 139656700098304 logging_writer.py:48] [70800] global_step=70800, grad_norm=2.0709261894226074, loss=3.119650363922119
I0127 20:10:14.756076 139656783959808 logging_writer.py:48] [70900] global_step=70900, grad_norm=2.023124933242798, loss=3.045048236846924
I0127 20:10:48.839784 139656700098304 logging_writer.py:48] [71000] global_step=71000, grad_norm=2.051177740097046, loss=3.119927406311035
I0127 20:11:22.958552 139656783959808 logging_writer.py:48] [71100] global_step=71100, grad_norm=2.172423839569092, loss=3.0196409225463867
I0127 20:11:57.066096 139656700098304 logging_writer.py:48] [71200] global_step=71200, grad_norm=2.0347788333892822, loss=3.070835828781128
I0127 20:12:31.187544 139656783959808 logging_writer.py:48] [71300] global_step=71300, grad_norm=2.299517869949341, loss=3.0699422359466553
I0127 20:13:05.283825 139656700098304 logging_writer.py:48] [71400] global_step=71400, grad_norm=2.1150004863739014, loss=3.0375561714172363
I0127 20:13:39.427891 139656783959808 logging_writer.py:48] [71500] global_step=71500, grad_norm=2.127300262451172, loss=3.0486035346984863
I0127 20:14:13.534416 139656700098304 logging_writer.py:48] [71600] global_step=71600, grad_norm=2.0697274208068848, loss=3.0438971519470215
I0127 20:14:47.644723 139656783959808 logging_writer.py:48] [71700] global_step=71700, grad_norm=1.9997990131378174, loss=3.083369493484497
I0127 20:14:52.910038 139822745589568 spec.py:321] Evaluating on the training split.
I0127 20:14:59.066477 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 20:15:07.774916 139822745589568 spec.py:349] Evaluating on the test split.
I0127 20:15:10.019581 139822745589568 submission_runner.py:408] Time since start: 25373.40s, 	Step: 71717, 	{'train/accuracy': 0.770527720451355, 'train/loss': 1.1102607250213623, 'validation/accuracy': 0.6875999569892883, 'validation/loss': 1.4715756177902222, 'validation/num_examples': 50000, 'test/accuracy': 0.55840003490448, 'test/loss': 2.1221847534179688, 'test/num_examples': 10000, 'score': 24516.862579345703, 'total_duration': 25373.404136896133, 'accumulated_submission_time': 24516.862579345703, 'accumulated_eval_time': 852.434784412384, 'accumulated_logging_time': 1.6647412776947021}
I0127 20:15:10.057637 139656792352512 logging_writer.py:48] [71717] accumulated_eval_time=852.434784, accumulated_logging_time=1.664741, accumulated_submission_time=24516.862579, global_step=71717, preemption_count=0, score=24516.862579, test/accuracy=0.558400, test/loss=2.122185, test/num_examples=10000, total_duration=25373.404137, train/accuracy=0.770528, train/loss=1.110261, validation/accuracy=0.687600, validation/loss=1.471576, validation/num_examples=50000
I0127 20:15:38.689664 139656825923328 logging_writer.py:48] [71800] global_step=71800, grad_norm=2.097339153289795, loss=3.0056345462799072
I0127 20:16:12.779516 139656792352512 logging_writer.py:48] [71900] global_step=71900, grad_norm=2.0409629344940186, loss=2.9855282306671143
I0127 20:16:46.886650 139656825923328 logging_writer.py:48] [72000] global_step=72000, grad_norm=2.194138526916504, loss=3.076545476913452
I0127 20:17:21.019025 139656792352512 logging_writer.py:48] [72100] global_step=72100, grad_norm=1.9700677394866943, loss=3.0484509468078613
I0127 20:17:55.150203 139656825923328 logging_writer.py:48] [72200] global_step=72200, grad_norm=2.084613800048828, loss=3.01396107673645
I0127 20:18:29.266733 139656792352512 logging_writer.py:48] [72300] global_step=72300, grad_norm=2.2354648113250732, loss=3.128765344619751
I0127 20:19:03.399619 139656825923328 logging_writer.py:48] [72400] global_step=72400, grad_norm=2.1878066062927246, loss=3.128446340560913
I0127 20:19:37.511426 139656792352512 logging_writer.py:48] [72500] global_step=72500, grad_norm=2.047342300415039, loss=3.0932984352111816
I0127 20:20:11.687748 139656825923328 logging_writer.py:48] [72600] global_step=72600, grad_norm=1.9975651502609253, loss=3.047443389892578
I0127 20:20:45.805623 139656792352512 logging_writer.py:48] [72700] global_step=72700, grad_norm=2.1778273582458496, loss=3.103670597076416
I0127 20:21:19.932633 139656825923328 logging_writer.py:48] [72800] global_step=72800, grad_norm=2.1701290607452393, loss=3.0371878147125244
I0127 20:21:54.057514 139656792352512 logging_writer.py:48] [72900] global_step=72900, grad_norm=2.0781869888305664, loss=3.0748729705810547
I0127 20:22:28.190464 139656825923328 logging_writer.py:48] [73000] global_step=73000, grad_norm=2.1614654064178467, loss=3.0445618629455566
I0127 20:23:02.337987 139656792352512 logging_writer.py:48] [73100] global_step=73100, grad_norm=2.1577086448669434, loss=3.1347975730895996
I0127 20:23:36.479497 139656825923328 logging_writer.py:48] [73200] global_step=73200, grad_norm=2.2839033603668213, loss=3.1223580837249756
I0127 20:23:40.031888 139822745589568 spec.py:321] Evaluating on the training split.
I0127 20:23:46.329281 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 20:23:54.775753 139822745589568 spec.py:349] Evaluating on the test split.
I0127 20:23:57.134033 139822745589568 submission_runner.py:408] Time since start: 25900.52s, 	Step: 73212, 	{'train/accuracy': 0.7600645422935486, 'train/loss': 1.1490471363067627, 'validation/accuracy': 0.6841599941253662, 'validation/loss': 1.4841029644012451, 'validation/num_examples': 50000, 'test/accuracy': 0.5588000416755676, 'test/loss': 2.1481716632843018, 'test/num_examples': 10000, 'score': 25026.77587389946, 'total_duration': 25900.518624067307, 'accumulated_submission_time': 25026.77587389946, 'accumulated_eval_time': 869.5368921756744, 'accumulated_logging_time': 1.7121210098266602}
I0127 20:23:57.163857 139656817530624 logging_writer.py:48] [73212] accumulated_eval_time=869.536892, accumulated_logging_time=1.712121, accumulated_submission_time=25026.775874, global_step=73212, preemption_count=0, score=25026.775874, test/accuracy=0.558800, test/loss=2.148172, test/num_examples=10000, total_duration=25900.518624, train/accuracy=0.760065, train/loss=1.149047, validation/accuracy=0.684160, validation/loss=1.484103, validation/num_examples=50000
I0127 20:24:27.495982 139658730145536 logging_writer.py:48] [73300] global_step=73300, grad_norm=2.342409610748291, loss=3.097275972366333
I0127 20:25:01.573762 139656817530624 logging_writer.py:48] [73400] global_step=73400, grad_norm=2.1048457622528076, loss=3.001441717147827
I0127 20:25:35.661530 139658730145536 logging_writer.py:48] [73500] global_step=73500, grad_norm=2.1699860095977783, loss=3.095756769180298
I0127 20:26:09.832206 139656817530624 logging_writer.py:48] [73600] global_step=73600, grad_norm=2.1954798698425293, loss=2.9625632762908936
I0127 20:26:43.932103 139658730145536 logging_writer.py:48] [73700] global_step=73700, grad_norm=2.0381851196289062, loss=3.0192065238952637
I0127 20:27:18.061791 139656817530624 logging_writer.py:48] [73800] global_step=73800, grad_norm=1.924801230430603, loss=3.031777858734131
I0127 20:27:52.173401 139658730145536 logging_writer.py:48] [73900] global_step=73900, grad_norm=2.168567419052124, loss=3.0684516429901123
I0127 20:28:26.269748 139656817530624 logging_writer.py:48] [74000] global_step=74000, grad_norm=2.505513906478882, loss=3.02882719039917
I0127 20:29:00.389062 139658730145536 logging_writer.py:48] [74100] global_step=74100, grad_norm=2.110987424850464, loss=3.0065560340881348
I0127 20:29:34.493155 139656817530624 logging_writer.py:48] [74200] global_step=74200, grad_norm=2.039469003677368, loss=2.9477365016937256
I0127 20:30:08.611235 139658730145536 logging_writer.py:48] [74300] global_step=74300, grad_norm=2.174318790435791, loss=3.0496723651885986
I0127 20:30:42.738814 139656817530624 logging_writer.py:48] [74400] global_step=74400, grad_norm=2.2065682411193848, loss=3.007622480392456
I0127 20:31:16.843854 139658730145536 logging_writer.py:48] [74500] global_step=74500, grad_norm=2.1742570400238037, loss=3.037276268005371
I0127 20:31:50.970649 139656817530624 logging_writer.py:48] [74600] global_step=74600, grad_norm=2.1175243854522705, loss=3.0836308002471924
I0127 20:32:25.159959 139658730145536 logging_writer.py:48] [74700] global_step=74700, grad_norm=2.1757094860076904, loss=3.0352091789245605
I0127 20:32:27.343741 139822745589568 spec.py:321] Evaluating on the training split.
I0127 20:32:33.612520 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 20:32:42.065012 139822745589568 spec.py:349] Evaluating on the test split.
I0127 20:32:44.442137 139822745589568 submission_runner.py:408] Time since start: 26427.83s, 	Step: 74708, 	{'train/accuracy': 0.7657246589660645, 'train/loss': 1.1162415742874146, 'validation/accuracy': 0.6844599843025208, 'validation/loss': 1.4583039283752441, 'validation/num_examples': 50000, 'test/accuracy': 0.5622000098228455, 'test/loss': 2.1055116653442383, 'test/num_examples': 10000, 'score': 25536.89725470543, 'total_duration': 26427.82671189308, 'accumulated_submission_time': 25536.89725470543, 'accumulated_eval_time': 886.6352317333221, 'accumulated_logging_time': 1.7510671615600586}
I0127 20:32:44.474651 139656834316032 logging_writer.py:48] [74708] accumulated_eval_time=886.635232, accumulated_logging_time=1.751067, accumulated_submission_time=25536.897255, global_step=74708, preemption_count=0, score=25536.897255, test/accuracy=0.562200, test/loss=2.105512, test/num_examples=10000, total_duration=26427.826712, train/accuracy=0.765725, train/loss=1.116242, validation/accuracy=0.684460, validation/loss=1.458304, validation/num_examples=50000
I0127 20:33:16.184107 139658730145536 logging_writer.py:48] [74800] global_step=74800, grad_norm=2.0709712505340576, loss=3.087409496307373
I0127 20:33:50.276812 139656834316032 logging_writer.py:48] [74900] global_step=74900, grad_norm=2.2324399948120117, loss=3.0093142986297607
I0127 20:34:24.414427 139658730145536 logging_writer.py:48] [75000] global_step=75000, grad_norm=2.1881494522094727, loss=3.0117907524108887
I0127 20:34:58.555369 139656834316032 logging_writer.py:48] [75100] global_step=75100, grad_norm=2.229867935180664, loss=3.057769298553467
I0127 20:35:32.677102 139658730145536 logging_writer.py:48] [75200] global_step=75200, grad_norm=2.208946943283081, loss=3.058206081390381
I0127 20:36:06.794709 139656834316032 logging_writer.py:48] [75300] global_step=75300, grad_norm=2.215353012084961, loss=3.0580828189849854
I0127 20:36:40.897285 139658730145536 logging_writer.py:48] [75400] global_step=75400, grad_norm=2.0055806636810303, loss=3.0429158210754395
I0127 20:37:15.013729 139656834316032 logging_writer.py:48] [75500] global_step=75500, grad_norm=2.079333543777466, loss=3.091191053390503
I0127 20:37:49.144474 139658730145536 logging_writer.py:48] [75600] global_step=75600, grad_norm=2.2356889247894287, loss=3.152592182159424
I0127 20:38:23.283241 139656834316032 logging_writer.py:48] [75700] global_step=75700, grad_norm=2.3052449226379395, loss=3.0640344619750977
I0127 20:38:57.498404 139658730145536 logging_writer.py:48] [75800] global_step=75800, grad_norm=2.2165684700012207, loss=3.067512035369873
I0127 20:39:31.627068 139656834316032 logging_writer.py:48] [75900] global_step=75900, grad_norm=2.0376195907592773, loss=3.060209035873413
I0127 20:40:05.789737 139658730145536 logging_writer.py:48] [76000] global_step=76000, grad_norm=2.122774124145508, loss=3.0560829639434814
I0127 20:40:39.914473 139656834316032 logging_writer.py:48] [76100] global_step=76100, grad_norm=2.013829469680786, loss=3.013666868209839
I0127 20:41:14.059720 139658730145536 logging_writer.py:48] [76200] global_step=76200, grad_norm=1.979475498199463, loss=3.0251410007476807
I0127 20:41:14.545993 139822745589568 spec.py:321] Evaluating on the training split.
I0127 20:41:20.724198 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 20:41:29.135707 139822745589568 spec.py:349] Evaluating on the test split.
I0127 20:41:31.545436 139822745589568 submission_runner.py:408] Time since start: 26954.93s, 	Step: 76203, 	{'train/accuracy': 0.8078364133834839, 'train/loss': 1.0016721487045288, 'validation/accuracy': 0.6889399886131287, 'validation/loss': 1.5002516508102417, 'validation/num_examples': 50000, 'test/accuracy': 0.5706000328063965, 'test/loss': 2.115853786468506, 'test/num_examples': 10000, 'score': 26046.908839941025, 'total_duration': 26954.93002486229, 'accumulated_submission_time': 26046.908839941025, 'accumulated_eval_time': 903.6346333026886, 'accumulated_logging_time': 1.7926886081695557}
I0127 20:41:31.576889 139656792352512 logging_writer.py:48] [76203] accumulated_eval_time=903.634633, accumulated_logging_time=1.792689, accumulated_submission_time=26046.908840, global_step=76203, preemption_count=0, score=26046.908840, test/accuracy=0.570600, test/loss=2.115854, test/num_examples=10000, total_duration=26954.930025, train/accuracy=0.807836, train/loss=1.001672, validation/accuracy=0.688940, validation/loss=1.500252, validation/num_examples=50000
I0127 20:42:04.984905 139656800745216 logging_writer.py:48] [76300] global_step=76300, grad_norm=2.0439271926879883, loss=3.021747589111328
I0127 20:42:39.091578 139656792352512 logging_writer.py:48] [76400] global_step=76400, grad_norm=2.129549264907837, loss=3.0804615020751953
I0127 20:43:13.202956 139656800745216 logging_writer.py:48] [76500] global_step=76500, grad_norm=2.1344878673553467, loss=2.992643117904663
I0127 20:43:47.280196 139656792352512 logging_writer.py:48] [76600] global_step=76600, grad_norm=2.2201194763183594, loss=3.0373270511627197
I0127 20:44:21.407148 139656800745216 logging_writer.py:48] [76700] global_step=76700, grad_norm=2.2566936016082764, loss=3.082550048828125
I0127 20:44:55.577340 139656792352512 logging_writer.py:48] [76800] global_step=76800, grad_norm=1.9582933187484741, loss=3.006200075149536
I0127 20:45:29.710832 139656800745216 logging_writer.py:48] [76900] global_step=76900, grad_norm=2.3163790702819824, loss=3.07804012298584
I0127 20:46:03.795426 139656792352512 logging_writer.py:48] [77000] global_step=77000, grad_norm=2.1867752075195312, loss=3.035053253173828
I0127 20:46:37.935236 139656800745216 logging_writer.py:48] [77100] global_step=77100, grad_norm=2.15716290473938, loss=3.071279525756836
I0127 20:47:12.071501 139656792352512 logging_writer.py:48] [77200] global_step=77200, grad_norm=2.261834144592285, loss=3.0962514877319336
I0127 20:47:46.196863 139656800745216 logging_writer.py:48] [77300] global_step=77300, grad_norm=2.5317599773406982, loss=3.043206214904785
I0127 20:48:20.329264 139656792352512 logging_writer.py:48] [77400] global_step=77400, grad_norm=2.1826999187469482, loss=3.0786542892456055
I0127 20:48:54.455544 139656800745216 logging_writer.py:48] [77500] global_step=77500, grad_norm=2.135572671890259, loss=3.001080274581909
I0127 20:49:28.616057 139656792352512 logging_writer.py:48] [77600] global_step=77600, grad_norm=2.0568959712982178, loss=3.0137479305267334
I0127 20:50:01.870465 139822745589568 spec.py:321] Evaluating on the training split.
I0127 20:50:08.038809 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 20:50:16.666702 139822745589568 spec.py:349] Evaluating on the test split.
I0127 20:50:18.947675 139822745589568 submission_runner.py:408] Time since start: 27482.33s, 	Step: 77699, 	{'train/accuracy': 0.786551296710968, 'train/loss': 1.0268144607543945, 'validation/accuracy': 0.6879599690437317, 'validation/loss': 1.454639196395874, 'validation/num_examples': 50000, 'test/accuracy': 0.5588000416755676, 'test/loss': 2.096848964691162, 'test/num_examples': 10000, 'score': 26557.141659975052, 'total_duration': 27482.33225798607, 'accumulated_submission_time': 26557.141659975052, 'accumulated_eval_time': 920.7117989063263, 'accumulated_logging_time': 1.8335063457489014}
I0127 20:50:18.979295 139656817530624 logging_writer.py:48] [77699] accumulated_eval_time=920.711799, accumulated_logging_time=1.833506, accumulated_submission_time=26557.141660, global_step=77699, preemption_count=0, score=26557.141660, test/accuracy=0.558800, test/loss=2.096849, test/num_examples=10000, total_duration=27482.332258, train/accuracy=0.786551, train/loss=1.026814, validation/accuracy=0.687960, validation/loss=1.454639, validation/num_examples=50000
I0127 20:50:19.690921 139656825923328 logging_writer.py:48] [77700] global_step=77700, grad_norm=2.2722508907318115, loss=3.090298652648926
I0127 20:50:53.778367 139656817530624 logging_writer.py:48] [77800] global_step=77800, grad_norm=2.276233673095703, loss=3.057694911956787
I0127 20:51:28.056455 139656825923328 logging_writer.py:48] [77900] global_step=77900, grad_norm=2.1183009147644043, loss=3.0554745197296143
I0127 20:52:02.146019 139656817530624 logging_writer.py:48] [78000] global_step=78000, grad_norm=2.1671226024627686, loss=3.0678741931915283
I0127 20:52:36.251963 139656825923328 logging_writer.py:48] [78100] global_step=78100, grad_norm=2.132488250732422, loss=3.0046029090881348
I0127 20:53:10.378849 139656817530624 logging_writer.py:48] [78200] global_step=78200, grad_norm=2.159903049468994, loss=2.985572338104248
I0127 20:53:44.474629 139656825923328 logging_writer.py:48] [78300] global_step=78300, grad_norm=2.0548408031463623, loss=2.980461597442627
I0127 20:54:18.583225 139656817530624 logging_writer.py:48] [78400] global_step=78400, grad_norm=2.2754647731781006, loss=3.0538759231567383
I0127 20:54:52.705442 139656825923328 logging_writer.py:48] [78500] global_step=78500, grad_norm=2.2134854793548584, loss=3.0754122734069824
I0127 20:55:26.812761 139656817530624 logging_writer.py:48] [78600] global_step=78600, grad_norm=2.2461774349212646, loss=3.0416455268859863
I0127 20:56:00.934408 139656825923328 logging_writer.py:48] [78700] global_step=78700, grad_norm=2.1361935138702393, loss=2.939992904663086
I0127 20:56:35.041902 139656817530624 logging_writer.py:48] [78800] global_step=78800, grad_norm=2.2577269077301025, loss=3.038163661956787
I0127 20:57:09.147002 139656825923328 logging_writer.py:48] [78900] global_step=78900, grad_norm=2.4100561141967773, loss=3.037432909011841
I0127 20:57:43.356478 139656817530624 logging_writer.py:48] [79000] global_step=79000, grad_norm=2.196030378341675, loss=3.103600025177002
I0127 20:58:17.475994 139656825923328 logging_writer.py:48] [79100] global_step=79100, grad_norm=2.3981716632843018, loss=3.034104585647583
I0127 20:58:48.980174 139822745589568 spec.py:321] Evaluating on the training split.
I0127 20:58:55.203743 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 20:59:03.849192 139822745589568 spec.py:349] Evaluating on the test split.
I0127 20:59:06.341204 139822745589568 submission_runner.py:408] Time since start: 28009.73s, 	Step: 79194, 	{'train/accuracy': 0.7811902165412903, 'train/loss': 1.058087706565857, 'validation/accuracy': 0.6916999816894531, 'validation/loss': 1.4431703090667725, 'validation/num_examples': 50000, 'test/accuracy': 0.5654000043869019, 'test/loss': 2.102102756500244, 'test/num_examples': 10000, 'score': 27067.081208705902, 'total_duration': 28009.725796461105, 'accumulated_submission_time': 27067.081208705902, 'accumulated_eval_time': 938.072808265686, 'accumulated_logging_time': 1.8744986057281494}
I0127 20:59:06.375318 139656800745216 logging_writer.py:48] [79194] accumulated_eval_time=938.072808, accumulated_logging_time=1.874499, accumulated_submission_time=27067.081209, global_step=79194, preemption_count=0, score=27067.081209, test/accuracy=0.565400, test/loss=2.102103, test/num_examples=10000, total_duration=28009.725796, train/accuracy=0.781190, train/loss=1.058088, validation/accuracy=0.691700, validation/loss=1.443170, validation/num_examples=50000
I0127 20:59:08.773053 139656809137920 logging_writer.py:48] [79200] global_step=79200, grad_norm=2.2563157081604004, loss=3.1095681190490723
I0127 20:59:42.841400 139656800745216 logging_writer.py:48] [79300] global_step=79300, grad_norm=2.3365581035614014, loss=3.062612295150757
I0127 21:00:16.919909 139656809137920 logging_writer.py:48] [79400] global_step=79400, grad_norm=2.24703311920166, loss=2.9946699142456055
I0127 21:00:51.036034 139656800745216 logging_writer.py:48] [79500] global_step=79500, grad_norm=2.2262227535247803, loss=3.0164103507995605
I0127 21:01:25.150085 139656809137920 logging_writer.py:48] [79600] global_step=79600, grad_norm=2.1586365699768066, loss=3.0112757682800293
I0127 21:01:59.255520 139656800745216 logging_writer.py:48] [79700] global_step=79700, grad_norm=2.083219528198242, loss=2.967262029647827
I0127 21:02:33.362250 139656809137920 logging_writer.py:48] [79800] global_step=79800, grad_norm=2.1897146701812744, loss=2.996061325073242
I0127 21:03:07.453753 139656800745216 logging_writer.py:48] [79900] global_step=79900, grad_norm=2.064491033554077, loss=2.9516425132751465
I0127 21:03:41.623285 139656809137920 logging_writer.py:48] [80000] global_step=80000, grad_norm=2.3613080978393555, loss=3.0620648860931396
I0127 21:04:15.748157 139656800745216 logging_writer.py:48] [80100] global_step=80100, grad_norm=2.220876932144165, loss=3.102846622467041
I0127 21:04:49.864441 139656809137920 logging_writer.py:48] [80200] global_step=80200, grad_norm=2.1393771171569824, loss=3.011768102645874
I0127 21:05:23.992781 139656800745216 logging_writer.py:48] [80300] global_step=80300, grad_norm=2.349653482437134, loss=3.0583016872406006
I0127 21:05:58.127442 139656809137920 logging_writer.py:48] [80400] global_step=80400, grad_norm=2.1858882904052734, loss=2.973914623260498
I0127 21:06:32.264410 139656800745216 logging_writer.py:48] [80500] global_step=80500, grad_norm=2.196094274520874, loss=3.061922550201416
I0127 21:07:06.393110 139656809137920 logging_writer.py:48] [80600] global_step=80600, grad_norm=2.179563045501709, loss=3.031093120574951
I0127 21:07:36.567466 139822745589568 spec.py:321] Evaluating on the training split.
I0127 21:07:43.479759 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 21:07:52.114010 139822745589568 spec.py:349] Evaluating on the test split.
I0127 21:07:54.489800 139822745589568 submission_runner.py:408] Time since start: 28537.87s, 	Step: 80690, 	{'train/accuracy': 0.7735171914100647, 'train/loss': 1.107993483543396, 'validation/accuracy': 0.6871599555015564, 'validation/loss': 1.4867323637008667, 'validation/num_examples': 50000, 'test/accuracy': 0.5612000226974487, 'test/loss': 2.126382827758789, 'test/num_examples': 10000, 'score': 27577.212859630585, 'total_duration': 28537.874395132065, 'accumulated_submission_time': 27577.212859630585, 'accumulated_eval_time': 955.9951276779175, 'accumulated_logging_time': 1.9179785251617432}
I0127 21:07:54.523106 139656792352512 logging_writer.py:48] [80690] accumulated_eval_time=955.995128, accumulated_logging_time=1.917979, accumulated_submission_time=27577.212860, global_step=80690, preemption_count=0, score=27577.212860, test/accuracy=0.561200, test/loss=2.126383, test/num_examples=10000, total_duration=28537.874395, train/accuracy=0.773517, train/loss=1.107993, validation/accuracy=0.687160, validation/loss=1.486732, validation/num_examples=50000
I0127 21:07:58.282671 139656817530624 logging_writer.py:48] [80700] global_step=80700, grad_norm=2.2221100330352783, loss=2.9413652420043945
I0127 21:08:32.366461 139656792352512 logging_writer.py:48] [80800] global_step=80800, grad_norm=2.2979238033294678, loss=2.9419748783111572
I0127 21:09:06.430400 139656817530624 logging_writer.py:48] [80900] global_step=80900, grad_norm=2.172417402267456, loss=2.994792938232422
I0127 21:09:40.548651 139656792352512 logging_writer.py:48] [81000] global_step=81000, grad_norm=2.2780988216400146, loss=3.075589895248413
I0127 21:10:14.717097 139656817530624 logging_writer.py:48] [81100] global_step=81100, grad_norm=2.2146894931793213, loss=2.9634900093078613
I0127 21:10:48.822724 139656792352512 logging_writer.py:48] [81200] global_step=81200, grad_norm=2.0868935585021973, loss=2.946991205215454
I0127 21:11:22.916404 139656817530624 logging_writer.py:48] [81300] global_step=81300, grad_norm=2.2784202098846436, loss=3.048203468322754
I0127 21:11:57.067761 139656792352512 logging_writer.py:48] [81400] global_step=81400, grad_norm=2.240457057952881, loss=3.0602970123291016
I0127 21:12:31.201045 139656817530624 logging_writer.py:48] [81500] global_step=81500, grad_norm=2.380159378051758, loss=3.1011903285980225
I0127 21:13:05.307416 139656792352512 logging_writer.py:48] [81600] global_step=81600, grad_norm=2.108952283859253, loss=3.0097029209136963
I0127 21:13:39.410063 139656817530624 logging_writer.py:48] [81700] global_step=81700, grad_norm=2.3379175662994385, loss=3.016634225845337
I0127 21:14:13.551066 139656792352512 logging_writer.py:48] [81800] global_step=81800, grad_norm=2.3784890174865723, loss=3.0813350677490234
I0127 21:14:47.674652 139656817530624 logging_writer.py:48] [81900] global_step=81900, grad_norm=2.2649481296539307, loss=3.0702810287475586
I0127 21:15:21.785476 139656792352512 logging_writer.py:48] [82000] global_step=82000, grad_norm=2.314021110534668, loss=2.981933832168579
I0127 21:15:55.903558 139656817530624 logging_writer.py:48] [82100] global_step=82100, grad_norm=2.3017096519470215, loss=2.9971797466278076
I0127 21:16:24.717867 139822745589568 spec.py:321] Evaluating on the training split.
I0127 21:16:30.922410 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 21:16:39.526412 139822745589568 spec.py:349] Evaluating on the test split.
I0127 21:16:41.915431 139822745589568 submission_runner.py:408] Time since start: 29065.30s, 	Step: 82186, 	{'train/accuracy': 0.7631337642669678, 'train/loss': 1.1587564945220947, 'validation/accuracy': 0.6825199723243713, 'validation/loss': 1.5233628749847412, 'validation/num_examples': 50000, 'test/accuracy': 0.5517000555992126, 'test/loss': 2.1907894611358643, 'test/num_examples': 10000, 'score': 28087.347998142242, 'total_duration': 29065.300013542175, 'accumulated_submission_time': 28087.347998142242, 'accumulated_eval_time': 973.1926457881927, 'accumulated_logging_time': 1.9603497982025146}
I0127 21:16:41.949597 139656783959808 logging_writer.py:48] [82186] accumulated_eval_time=973.192646, accumulated_logging_time=1.960350, accumulated_submission_time=28087.347998, global_step=82186, preemption_count=0, score=28087.347998, test/accuracy=0.551700, test/loss=2.190789, test/num_examples=10000, total_duration=29065.300014, train/accuracy=0.763134, train/loss=1.158756, validation/accuracy=0.682520, validation/loss=1.523363, validation/num_examples=50000
I0127 21:16:47.067209 139656800745216 logging_writer.py:48] [82200] global_step=82200, grad_norm=2.454279661178589, loss=3.0220601558685303
I0127 21:17:21.114254 139656783959808 logging_writer.py:48] [82300] global_step=82300, grad_norm=2.1951687335968018, loss=2.995896816253662
I0127 21:17:55.187997 139656800745216 logging_writer.py:48] [82400] global_step=82400, grad_norm=2.148292064666748, loss=2.9962568283081055
I0127 21:18:29.711744 139656783959808 logging_writer.py:48] [82500] global_step=82500, grad_norm=2.2392590045928955, loss=3.04085111618042
I0127 21:19:03.802188 139656800745216 logging_writer.py:48] [82600] global_step=82600, grad_norm=2.273491859436035, loss=3.0052030086517334
I0127 21:19:37.901664 139656783959808 logging_writer.py:48] [82700] global_step=82700, grad_norm=2.261216163635254, loss=3.0501694679260254
I0127 21:20:12.007811 139656800745216 logging_writer.py:48] [82800] global_step=82800, grad_norm=2.193540096282959, loss=2.955106258392334
I0127 21:20:46.138708 139656783959808 logging_writer.py:48] [82900] global_step=82900, grad_norm=2.210747718811035, loss=3.052335023880005
I0127 21:21:20.284801 139656800745216 logging_writer.py:48] [83000] global_step=83000, grad_norm=2.244823455810547, loss=3.03317928314209
I0127 21:21:54.414683 139656783959808 logging_writer.py:48] [83100] global_step=83100, grad_norm=2.1521592140197754, loss=3.0067896842956543
I0127 21:22:28.688601 139656800745216 logging_writer.py:48] [83200] global_step=83200, grad_norm=2.1349270343780518, loss=2.9198522567749023
I0127 21:23:02.824759 139656783959808 logging_writer.py:48] [83300] global_step=83300, grad_norm=2.431199073791504, loss=3.078742027282715
I0127 21:23:36.925719 139656800745216 logging_writer.py:48] [83400] global_step=83400, grad_norm=2.416246175765991, loss=3.0206847190856934
I0127 21:24:11.047303 139656783959808 logging_writer.py:48] [83500] global_step=83500, grad_norm=2.3549890518188477, loss=2.998638868331909
I0127 21:24:45.182308 139656800745216 logging_writer.py:48] [83600] global_step=83600, grad_norm=2.420275926589966, loss=3.0155930519104004
I0127 21:25:11.925215 139822745589568 spec.py:321] Evaluating on the training split.
I0127 21:25:18.193168 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 21:25:26.700632 139822745589568 spec.py:349] Evaluating on the test split.
I0127 21:25:29.093055 139822745589568 submission_runner.py:408] Time since start: 29592.48s, 	Step: 83680, 	{'train/accuracy': 0.7662228941917419, 'train/loss': 1.1387059688568115, 'validation/accuracy': 0.6846799850463867, 'validation/loss': 1.4886494874954224, 'validation/num_examples': 50000, 'test/accuracy': 0.5542000532150269, 'test/loss': 2.152329683303833, 'test/num_examples': 10000, 'score': 28597.258448839188, 'total_duration': 29592.47763466835, 'accumulated_submission_time': 28597.258448839188, 'accumulated_eval_time': 990.360454082489, 'accumulated_logging_time': 2.00658917427063}
I0127 21:25:29.125122 139656817530624 logging_writer.py:48] [83680] accumulated_eval_time=990.360454, accumulated_logging_time=2.006589, accumulated_submission_time=28597.258449, global_step=83680, preemption_count=0, score=28597.258449, test/accuracy=0.554200, test/loss=2.152330, test/num_examples=10000, total_duration=29592.477635, train/accuracy=0.766223, train/loss=1.138706, validation/accuracy=0.684680, validation/loss=1.488649, validation/num_examples=50000
I0127 21:25:36.294682 139656825923328 logging_writer.py:48] [83700] global_step=83700, grad_norm=2.2413854598999023, loss=3.1118476390838623
I0127 21:26:10.367641 139656817530624 logging_writer.py:48] [83800] global_step=83800, grad_norm=2.3226587772369385, loss=3.107858657836914
I0127 21:26:44.468051 139656825923328 logging_writer.py:48] [83900] global_step=83900, grad_norm=2.320249080657959, loss=3.010801076889038
I0127 21:27:18.588531 139656817530624 logging_writer.py:48] [84000] global_step=84000, grad_norm=2.3074142932891846, loss=3.0560786724090576
I0127 21:27:52.731654 139656825923328 logging_writer.py:48] [84100] global_step=84100, grad_norm=2.1795589923858643, loss=3.101062297821045
I0127 21:28:26.846050 139656817530624 logging_writer.py:48] [84200] global_step=84200, grad_norm=2.4116671085357666, loss=3.0236010551452637
I0127 21:29:01.084926 139656825923328 logging_writer.py:48] [84300] global_step=84300, grad_norm=2.3322649002075195, loss=2.9820404052734375
I0127 21:29:35.191822 139656817530624 logging_writer.py:48] [84400] global_step=84400, grad_norm=2.340970277786255, loss=3.0396785736083984
I0127 21:30:09.308921 139656825923328 logging_writer.py:48] [84500] global_step=84500, grad_norm=2.3121843338012695, loss=3.021977663040161
I0127 21:30:43.432147 139656817530624 logging_writer.py:48] [84600] global_step=84600, grad_norm=2.4530463218688965, loss=3.0029072761535645
I0127 21:31:17.537893 139656825923328 logging_writer.py:48] [84700] global_step=84700, grad_norm=2.3284051418304443, loss=3.0455033779144287
I0127 21:31:51.657410 139656817530624 logging_writer.py:48] [84800] global_step=84800, grad_norm=2.3325135707855225, loss=2.9967594146728516
I0127 21:32:25.764731 139656825923328 logging_writer.py:48] [84900] global_step=84900, grad_norm=2.2562403678894043, loss=3.0263607501983643
I0127 21:32:59.875652 139656817530624 logging_writer.py:48] [85000] global_step=85000, grad_norm=2.255089044570923, loss=3.024834394454956
I0127 21:33:33.995696 139656825923328 logging_writer.py:48] [85100] global_step=85100, grad_norm=2.3560147285461426, loss=3.071263074874878
I0127 21:33:59.392077 139822745589568 spec.py:321] Evaluating on the training split.
I0127 21:34:05.740009 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 21:34:14.303873 139822745589568 spec.py:349] Evaluating on the test split.
I0127 21:34:16.756535 139822745589568 submission_runner.py:408] Time since start: 30120.14s, 	Step: 85176, 	{'train/accuracy': 0.7777224183082581, 'train/loss': 1.1008814573287964, 'validation/accuracy': 0.6924399733543396, 'validation/loss': 1.4702638387680054, 'validation/num_examples': 50000, 'test/accuracy': 0.5660000443458557, 'test/loss': 2.104079246520996, 'test/num_examples': 10000, 'score': 29107.465238809586, 'total_duration': 30120.14112353325, 'accumulated_submission_time': 29107.465238809586, 'accumulated_eval_time': 1007.7248740196228, 'accumulated_logging_time': 2.047886610031128}
I0127 21:34:16.791048 139656800745216 logging_writer.py:48] [85176] accumulated_eval_time=1007.724874, accumulated_logging_time=2.047887, accumulated_submission_time=29107.465239, global_step=85176, preemption_count=0, score=29107.465239, test/accuracy=0.566000, test/loss=2.104079, test/num_examples=10000, total_duration=30120.141124, train/accuracy=0.777722, train/loss=1.100881, validation/accuracy=0.692440, validation/loss=1.470264, validation/num_examples=50000
I0127 21:34:25.310621 139656809137920 logging_writer.py:48] [85200] global_step=85200, grad_norm=2.243151903152466, loss=2.9985132217407227
I0127 21:34:59.428163 139656800745216 logging_writer.py:48] [85300] global_step=85300, grad_norm=2.313507556915283, loss=3.010852098464966
I0127 21:35:33.512701 139656809137920 logging_writer.py:48] [85400] global_step=85400, grad_norm=2.347661256790161, loss=2.977545976638794
I0127 21:36:07.637152 139656800745216 logging_writer.py:48] [85500] global_step=85500, grad_norm=2.2394328117370605, loss=2.9386777877807617
I0127 21:36:41.743211 139656809137920 logging_writer.py:48] [85600] global_step=85600, grad_norm=2.214501142501831, loss=2.9255542755126953
I0127 21:37:15.830422 139656800745216 logging_writer.py:48] [85700] global_step=85700, grad_norm=2.2229669094085693, loss=2.9747202396392822
I0127 21:37:49.930986 139656809137920 logging_writer.py:48] [85800] global_step=85800, grad_norm=2.2080001831054688, loss=3.0238149166107178
I0127 21:38:24.054667 139656800745216 logging_writer.py:48] [85900] global_step=85900, grad_norm=2.2987992763519287, loss=3.0141940116882324
I0127 21:38:58.161227 139656809137920 logging_writer.py:48] [86000] global_step=86000, grad_norm=2.298107147216797, loss=2.9920687675476074
I0127 21:39:32.303894 139656800745216 logging_writer.py:48] [86100] global_step=86100, grad_norm=2.201545238494873, loss=3.029681444168091
I0127 21:40:06.396925 139656809137920 logging_writer.py:48] [86200] global_step=86200, grad_norm=2.3685495853424072, loss=3.056645393371582
I0127 21:40:40.513171 139656800745216 logging_writer.py:48] [86300] global_step=86300, grad_norm=2.338301420211792, loss=2.955796003341675
I0127 21:41:14.678269 139656809137920 logging_writer.py:48] [86400] global_step=86400, grad_norm=2.2444465160369873, loss=2.9892916679382324
I0127 21:41:48.806427 139656800745216 logging_writer.py:48] [86500] global_step=86500, grad_norm=2.1225502490997314, loss=2.957394599914551
I0127 21:42:22.922585 139656809137920 logging_writer.py:48] [86600] global_step=86600, grad_norm=2.5425171852111816, loss=3.0172462463378906
I0127 21:42:46.955170 139822745589568 spec.py:321] Evaluating on the training split.
I0127 21:42:53.161988 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 21:43:01.729549 139822745589568 spec.py:349] Evaluating on the test split.
I0127 21:43:04.194558 139822745589568 submission_runner.py:408] Time since start: 30647.58s, 	Step: 86672, 	{'train/accuracy': 0.7954002022743225, 'train/loss': 1.0093271732330322, 'validation/accuracy': 0.6930800080299377, 'validation/loss': 1.4499201774597168, 'validation/num_examples': 50000, 'test/accuracy': 0.5685000419616699, 'test/loss': 2.0924246311187744, 'test/num_examples': 10000, 'score': 29617.56979894638, 'total_duration': 30647.5791118145, 'accumulated_submission_time': 29617.56979894638, 'accumulated_eval_time': 1024.9641880989075, 'accumulated_logging_time': 2.0917553901672363}
I0127 21:43:04.243909 139656825923328 logging_writer.py:48] [86672] accumulated_eval_time=1024.964188, accumulated_logging_time=2.091755, accumulated_submission_time=29617.569799, global_step=86672, preemption_count=0, score=29617.569799, test/accuracy=0.568500, test/loss=2.092425, test/num_examples=10000, total_duration=30647.579112, train/accuracy=0.795400, train/loss=1.009327, validation/accuracy=0.693080, validation/loss=1.449920, validation/num_examples=50000
I0127 21:43:14.154828 139656834316032 logging_writer.py:48] [86700] global_step=86700, grad_norm=2.422391891479492, loss=3.0881662368774414
I0127 21:43:48.256529 139656825923328 logging_writer.py:48] [86800] global_step=86800, grad_norm=2.180192708969116, loss=2.943073272705078
I0127 21:44:22.363179 139656834316032 logging_writer.py:48] [86900] global_step=86900, grad_norm=2.2242445945739746, loss=3.0023579597473145
I0127 21:44:56.500931 139656825923328 logging_writer.py:48] [87000] global_step=87000, grad_norm=2.2494149208068848, loss=2.9930973052978516
I0127 21:45:30.613864 139656834316032 logging_writer.py:48] [87100] global_step=87100, grad_norm=2.550879716873169, loss=2.9867382049560547
I0127 21:46:04.752998 139656825923328 logging_writer.py:48] [87200] global_step=87200, grad_norm=2.3786821365356445, loss=2.9534740447998047
I0127 21:46:38.903652 139656834316032 logging_writer.py:48] [87300] global_step=87300, grad_norm=2.526879072189331, loss=2.931896448135376
I0127 21:47:13.043442 139656825923328 logging_writer.py:48] [87400] global_step=87400, grad_norm=2.336740493774414, loss=3.0442872047424316
I0127 21:47:47.215414 139656834316032 logging_writer.py:48] [87500] global_step=87500, grad_norm=2.301753044128418, loss=2.957268476486206
I0127 21:48:21.361651 139656825923328 logging_writer.py:48] [87600] global_step=87600, grad_norm=2.3030929565429688, loss=3.0060112476348877
I0127 21:48:55.489655 139656834316032 logging_writer.py:48] [87700] global_step=87700, grad_norm=2.5217435359954834, loss=3.000093698501587
I0127 21:49:29.616543 139656825923328 logging_writer.py:48] [87800] global_step=87800, grad_norm=2.250718355178833, loss=2.9465813636779785
I0127 21:50:03.747335 139656834316032 logging_writer.py:48] [87900] global_step=87900, grad_norm=2.244016647338867, loss=2.935375928878784
I0127 21:50:37.864662 139656825923328 logging_writer.py:48] [88000] global_step=88000, grad_norm=2.351966619491577, loss=2.9135074615478516
I0127 21:51:11.996327 139656834316032 logging_writer.py:48] [88100] global_step=88100, grad_norm=2.3273661136627197, loss=2.9511165618896484
I0127 21:51:34.303143 139822745589568 spec.py:321] Evaluating on the training split.
I0127 21:51:40.563997 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 21:51:49.272603 139822745589568 spec.py:349] Evaluating on the test split.
I0127 21:51:51.709564 139822745589568 submission_runner.py:408] Time since start: 31175.09s, 	Step: 88167, 	{'train/accuracy': 0.7859733700752258, 'train/loss': 1.0578300952911377, 'validation/accuracy': 0.693399965763092, 'validation/loss': 1.460898995399475, 'validation/num_examples': 50000, 'test/accuracy': 0.5648000240325928, 'test/loss': 2.1210591793060303, 'test/num_examples': 10000, 'score': 30127.56591463089, 'total_duration': 31175.094157218933, 'accumulated_submission_time': 30127.56591463089, 'accumulated_eval_time': 1042.3706114292145, 'accumulated_logging_time': 2.1539254188537598}
I0127 21:51:51.744967 139656800745216 logging_writer.py:48] [88167] accumulated_eval_time=1042.370611, accumulated_logging_time=2.153925, accumulated_submission_time=30127.565915, global_step=88167, preemption_count=0, score=30127.565915, test/accuracy=0.564800, test/loss=2.121059, test/num_examples=10000, total_duration=31175.094157, train/accuracy=0.785973, train/loss=1.057830, validation/accuracy=0.693400, validation/loss=1.460899, validation/num_examples=50000
I0127 21:52:03.342526 139656809137920 logging_writer.py:48] [88200] global_step=88200, grad_norm=2.426680326461792, loss=3.001553535461426
I0127 21:52:37.399089 139656800745216 logging_writer.py:48] [88300] global_step=88300, grad_norm=2.3337881565093994, loss=2.9773995876312256
I0127 21:53:11.504953 139656809137920 logging_writer.py:48] [88400] global_step=88400, grad_norm=2.1897242069244385, loss=3.0200040340423584
I0127 21:53:45.617223 139656800745216 logging_writer.py:48] [88500] global_step=88500, grad_norm=2.3779702186584473, loss=3.0041024684906006
I0127 21:54:19.820295 139656809137920 logging_writer.py:48] [88600] global_step=88600, grad_norm=2.3771512508392334, loss=2.991297721862793
I0127 21:54:53.922628 139656800745216 logging_writer.py:48] [88700] global_step=88700, grad_norm=2.3683598041534424, loss=2.980870246887207
I0127 21:55:28.048544 139656809137920 logging_writer.py:48] [88800] global_step=88800, grad_norm=2.2657511234283447, loss=2.8708972930908203
I0127 21:56:02.165565 139656800745216 logging_writer.py:48] [88900] global_step=88900, grad_norm=2.2473623752593994, loss=3.031315565109253
I0127 21:56:36.296670 139656809137920 logging_writer.py:48] [89000] global_step=89000, grad_norm=2.4852375984191895, loss=2.9789493083953857
I0127 21:57:10.434195 139656800745216 logging_writer.py:48] [89100] global_step=89100, grad_norm=2.3439829349517822, loss=2.9435269832611084
I0127 21:57:44.561778 139656809137920 logging_writer.py:48] [89200] global_step=89200, grad_norm=2.219747304916382, loss=2.9799320697784424
I0127 21:58:18.649070 139656800745216 logging_writer.py:48] [89300] global_step=89300, grad_norm=2.4122321605682373, loss=2.998812437057495
I0127 21:58:52.772235 139656809137920 logging_writer.py:48] [89400] global_step=89400, grad_norm=2.1906280517578125, loss=2.883997678756714
I0127 21:59:26.886481 139656800745216 logging_writer.py:48] [89500] global_step=89500, grad_norm=2.1654419898986816, loss=2.9292945861816406
I0127 22:00:01.014837 139656809137920 logging_writer.py:48] [89600] global_step=89600, grad_norm=2.306490182876587, loss=2.969024419784546
I0127 22:00:21.813574 139822745589568 spec.py:321] Evaluating on the training split.
I0127 22:00:28.156230 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 22:00:36.623969 139822745589568 spec.py:349] Evaluating on the test split.
I0127 22:00:39.035059 139822745589568 submission_runner.py:408] Time since start: 31702.42s, 	Step: 89662, 	{'train/accuracy': 0.7878866195678711, 'train/loss': 1.0814951658248901, 'validation/accuracy': 0.6979999542236328, 'validation/loss': 1.4614171981811523, 'validation/num_examples': 50000, 'test/accuracy': 0.5674000382423401, 'test/loss': 2.1174557209014893, 'test/num_examples': 10000, 'score': 30637.575980186462, 'total_duration': 31702.41963338852, 'accumulated_submission_time': 30637.575980186462, 'accumulated_eval_time': 1059.5920572280884, 'accumulated_logging_time': 2.1985137462615967}
I0127 22:00:39.076849 139656825923328 logging_writer.py:48] [89662] accumulated_eval_time=1059.592057, accumulated_logging_time=2.198514, accumulated_submission_time=30637.575980, global_step=89662, preemption_count=0, score=30637.575980, test/accuracy=0.567400, test/loss=2.117456, test/num_examples=10000, total_duration=31702.419633, train/accuracy=0.787887, train/loss=1.081495, validation/accuracy=0.698000, validation/loss=1.461417, validation/num_examples=50000
I0127 22:00:52.389709 139656834316032 logging_writer.py:48] [89700] global_step=89700, grad_norm=2.4175055027008057, loss=2.9372949600219727
I0127 22:01:26.483818 139656825923328 logging_writer.py:48] [89800] global_step=89800, grad_norm=2.3074944019317627, loss=2.954777240753174
I0127 22:02:00.578690 139656834316032 logging_writer.py:48] [89900] global_step=89900, grad_norm=2.552105665206909, loss=3.043593406677246
I0127 22:02:34.697673 139656825923328 logging_writer.py:48] [90000] global_step=90000, grad_norm=2.260868549346924, loss=2.947413682937622
I0127 22:03:08.824134 139656834316032 logging_writer.py:48] [90100] global_step=90100, grad_norm=2.3039228916168213, loss=3.0385897159576416
I0127 22:03:42.948186 139656825923328 logging_writer.py:48] [90200] global_step=90200, grad_norm=2.415917158126831, loss=2.984410047531128
I0127 22:04:17.081951 139656834316032 logging_writer.py:48] [90300] global_step=90300, grad_norm=2.4610819816589355, loss=2.9894485473632812
I0127 22:04:51.191333 139656825923328 logging_writer.py:48] [90400] global_step=90400, grad_norm=2.559142827987671, loss=2.9771766662597656
I0127 22:05:25.332896 139656834316032 logging_writer.py:48] [90500] global_step=90500, grad_norm=2.315885305404663, loss=2.937856674194336
I0127 22:05:59.476532 139656825923328 logging_writer.py:48] [90600] global_step=90600, grad_norm=2.2963242530822754, loss=2.942483901977539
I0127 22:06:33.694876 139656834316032 logging_writer.py:48] [90700] global_step=90700, grad_norm=2.376408815383911, loss=3.075564384460449
I0127 22:07:07.822996 139656825923328 logging_writer.py:48] [90800] global_step=90800, grad_norm=2.5583980083465576, loss=3.001136541366577
I0127 22:07:41.922252 139656834316032 logging_writer.py:48] [90900] global_step=90900, grad_norm=2.3230433464050293, loss=2.959242582321167
I0127 22:08:16.025145 139656825923328 logging_writer.py:48] [91000] global_step=91000, grad_norm=2.6043436527252197, loss=3.058032989501953
I0127 22:08:50.121221 139656834316032 logging_writer.py:48] [91100] global_step=91100, grad_norm=2.2631959915161133, loss=2.9825682640075684
I0127 22:09:09.369878 139822745589568 spec.py:321] Evaluating on the training split.
I0127 22:09:15.602681 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 22:09:24.143250 139822745589568 spec.py:349] Evaluating on the test split.
I0127 22:09:26.578836 139822745589568 submission_runner.py:408] Time since start: 32229.96s, 	Step: 91158, 	{'train/accuracy': 0.7898397445678711, 'train/loss': 1.0304040908813477, 'validation/accuracy': 0.6977599859237671, 'validation/loss': 1.4194663763046265, 'validation/num_examples': 50000, 'test/accuracy': 0.5745000243186951, 'test/loss': 2.0515968799591064, 'test/num_examples': 10000, 'score': 31147.80820083618, 'total_duration': 32229.963414669037, 'accumulated_submission_time': 31147.80820083618, 'accumulated_eval_time': 1076.8009662628174, 'accumulated_logging_time': 2.2499001026153564}
I0127 22:09:26.615345 139656700098304 logging_writer.py:48] [91158] accumulated_eval_time=1076.800966, accumulated_logging_time=2.249900, accumulated_submission_time=31147.808201, global_step=91158, preemption_count=0, score=31147.808201, test/accuracy=0.574500, test/loss=2.051597, test/num_examples=10000, total_duration=32229.963415, train/accuracy=0.789840, train/loss=1.030404, validation/accuracy=0.697760, validation/loss=1.419466, validation/num_examples=50000
I0127 22:09:41.258652 139656783959808 logging_writer.py:48] [91200] global_step=91200, grad_norm=2.419673204421997, loss=2.9514894485473633
I0127 22:10:15.308927 139656700098304 logging_writer.py:48] [91300] global_step=91300, grad_norm=2.3459980487823486, loss=2.96187162399292
I0127 22:10:49.423567 139656783959808 logging_writer.py:48] [91400] global_step=91400, grad_norm=2.4882941246032715, loss=2.9813685417175293
I0127 22:11:23.517545 139656700098304 logging_writer.py:48] [91500] global_step=91500, grad_norm=2.222649574279785, loss=2.899237871170044
I0127 22:11:57.680571 139656783959808 logging_writer.py:48] [91600] global_step=91600, grad_norm=2.3311543464660645, loss=2.976891040802002
I0127 22:12:31.788826 139656700098304 logging_writer.py:48] [91700] global_step=91700, grad_norm=2.5898451805114746, loss=2.9639689922332764
I0127 22:13:05.981038 139656783959808 logging_writer.py:48] [91800] global_step=91800, grad_norm=2.4116873741149902, loss=3.0329601764678955
I0127 22:13:40.117831 139656700098304 logging_writer.py:48] [91900] global_step=91900, grad_norm=2.3294036388397217, loss=2.953261375427246
I0127 22:14:14.203116 139656783959808 logging_writer.py:48] [92000] global_step=92000, grad_norm=2.346205472946167, loss=2.9930450916290283
I0127 22:14:48.340601 139656700098304 logging_writer.py:48] [92100] global_step=92100, grad_norm=2.581357002258301, loss=3.0694332122802734
I0127 22:15:22.433640 139656783959808 logging_writer.py:48] [92200] global_step=92200, grad_norm=2.28790020942688, loss=2.9678754806518555
I0127 22:15:56.533744 139656700098304 logging_writer.py:48] [92300] global_step=92300, grad_norm=2.399737596511841, loss=2.9016215801239014
I0127 22:16:30.652776 139656783959808 logging_writer.py:48] [92400] global_step=92400, grad_norm=2.2925188541412354, loss=2.9549598693847656
I0127 22:17:04.757258 139656700098304 logging_writer.py:48] [92500] global_step=92500, grad_norm=2.312804937362671, loss=2.962246894836426
I0127 22:17:38.873411 139656783959808 logging_writer.py:48] [92600] global_step=92600, grad_norm=2.3888773918151855, loss=2.9853358268737793
I0127 22:17:56.775310 139822745589568 spec.py:321] Evaluating on the training split.
I0127 22:18:03.096953 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 22:18:11.560071 139822745589568 spec.py:349] Evaluating on the test split.
I0127 22:18:13.976566 139822745589568 submission_runner.py:408] Time since start: 32757.36s, 	Step: 92654, 	{'train/accuracy': 0.7855349183082581, 'train/loss': 1.0370293855667114, 'validation/accuracy': 0.6988399624824524, 'validation/loss': 1.4155194759368896, 'validation/num_examples': 50000, 'test/accuracy': 0.5738000273704529, 'test/loss': 2.0675852298736572, 'test/num_examples': 10000, 'score': 31657.907905578613, 'total_duration': 32757.361146211624, 'accumulated_submission_time': 31657.907905578613, 'accumulated_eval_time': 1094.0021858215332, 'accumulated_logging_time': 2.295071840286255}
I0127 22:18:14.011592 139656825923328 logging_writer.py:48] [92654] accumulated_eval_time=1094.002186, accumulated_logging_time=2.295072, accumulated_submission_time=31657.907906, global_step=92654, preemption_count=0, score=31657.907906, test/accuracy=0.573800, test/loss=2.067585, test/num_examples=10000, total_duration=32757.361146, train/accuracy=0.785535, train/loss=1.037029, validation/accuracy=0.698840, validation/loss=1.415519, validation/num_examples=50000
I0127 22:18:30.059491 139656834316032 logging_writer.py:48] [92700] global_step=92700, grad_norm=2.4000935554504395, loss=2.9291183948516846
I0127 22:19:04.151058 139656825923328 logging_writer.py:48] [92800] global_step=92800, grad_norm=2.3494105339050293, loss=2.9976754188537598
I0127 22:19:38.294260 139656834316032 logging_writer.py:48] [92900] global_step=92900, grad_norm=2.3778927326202393, loss=2.955901622772217
I0127 22:20:12.409696 139656825923328 logging_writer.py:48] [93000] global_step=93000, grad_norm=2.4374568462371826, loss=3.0038838386535645
I0127 22:20:46.493962 139656834316032 logging_writer.py:48] [93100] global_step=93100, grad_norm=2.3717005252838135, loss=3.028775691986084
I0127 22:21:20.621589 139656825923328 logging_writer.py:48] [93200] global_step=93200, grad_norm=2.446699380874634, loss=2.9387989044189453
I0127 22:21:54.727323 139656834316032 logging_writer.py:48] [93300] global_step=93300, grad_norm=2.239837169647217, loss=2.958984613418579
I0127 22:22:28.849208 139656825923328 logging_writer.py:48] [93400] global_step=93400, grad_norm=2.2857182025909424, loss=2.8839433193206787
I0127 22:23:02.975273 139656834316032 logging_writer.py:48] [93500] global_step=93500, grad_norm=2.4803884029388428, loss=2.979052782058716
I0127 22:23:37.098841 139656825923328 logging_writer.py:48] [93600] global_step=93600, grad_norm=2.3478610515594482, loss=2.9496586322784424
I0127 22:24:11.233764 139656834316032 logging_writer.py:48] [93700] global_step=93700, grad_norm=2.5450620651245117, loss=2.9929747581481934
I0127 22:24:45.350155 139656825923328 logging_writer.py:48] [93800] global_step=93800, grad_norm=2.4481499195098877, loss=3.038722276687622
I0127 22:25:19.479342 139656834316032 logging_writer.py:48] [93900] global_step=93900, grad_norm=2.4859907627105713, loss=2.927238702774048
I0127 22:25:53.624099 139656825923328 logging_writer.py:48] [94000] global_step=94000, grad_norm=2.537215232849121, loss=3.005537986755371
I0127 22:26:27.772198 139656834316032 logging_writer.py:48] [94100] global_step=94100, grad_norm=2.3226754665374756, loss=2.955658435821533
I0127 22:26:44.308355 139822745589568 spec.py:321] Evaluating on the training split.
I0127 22:26:50.485025 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 22:26:59.007566 139822745589568 spec.py:349] Evaluating on the test split.
I0127 22:27:01.375915 139822745589568 submission_runner.py:408] Time since start: 33284.76s, 	Step: 94150, 	{'train/accuracy': 0.7915935516357422, 'train/loss': 1.011091709136963, 'validation/accuracy': 0.7041199803352356, 'validation/loss': 1.3878302574157715, 'validation/num_examples': 50000, 'test/accuracy': 0.5834000110626221, 'test/loss': 2.04312801361084, 'test/num_examples': 10000, 'score': 32168.143102407455, 'total_duration': 33284.76044559479, 'accumulated_submission_time': 32168.143102407455, 'accumulated_eval_time': 1111.0696558952332, 'accumulated_logging_time': 2.34027099609375}
I0127 22:27:01.431683 139656792352512 logging_writer.py:48] [94150] accumulated_eval_time=1111.069656, accumulated_logging_time=2.340271, accumulated_submission_time=32168.143102, global_step=94150, preemption_count=0, score=32168.143102, test/accuracy=0.583400, test/loss=2.043128, test/num_examples=10000, total_duration=33284.760446, train/accuracy=0.791594, train/loss=1.011092, validation/accuracy=0.704120, validation/loss=1.387830, validation/num_examples=50000
I0127 22:27:18.818087 139656800745216 logging_writer.py:48] [94200] global_step=94200, grad_norm=2.427219867706299, loss=3.0404534339904785
I0127 22:27:52.877732 139656792352512 logging_writer.py:48] [94300] global_step=94300, grad_norm=2.4800455570220947, loss=3.023263454437256
I0127 22:28:26.989708 139656800745216 logging_writer.py:48] [94400] global_step=94400, grad_norm=2.1767771244049072, loss=2.881214141845703
I0127 22:29:01.077661 139656792352512 logging_writer.py:48] [94500] global_step=94500, grad_norm=2.5012760162353516, loss=3.0194854736328125
I0127 22:29:35.185421 139656800745216 logging_writer.py:48] [94600] global_step=94600, grad_norm=2.438297986984253, loss=2.9623584747314453
I0127 22:30:09.297314 139656792352512 logging_writer.py:48] [94700] global_step=94700, grad_norm=2.4869325160980225, loss=2.9283649921417236
I0127 22:30:43.410728 139656800745216 logging_writer.py:48] [94800] global_step=94800, grad_norm=2.5719261169433594, loss=2.971989631652832
I0127 22:31:17.537897 139656792352512 logging_writer.py:48] [94900] global_step=94900, grad_norm=2.6333305835723877, loss=3.0132322311401367
I0127 22:31:51.799711 139656800745216 logging_writer.py:48] [95000] global_step=95000, grad_norm=2.34783673286438, loss=2.974320411682129
I0127 22:32:25.880890 139656792352512 logging_writer.py:48] [95100] global_step=95100, grad_norm=2.616711378097534, loss=2.9155118465423584
I0127 22:33:00.003855 139656800745216 logging_writer.py:48] [95200] global_step=95200, grad_norm=2.7908573150634766, loss=2.9437766075134277
I0127 22:33:34.113923 139656792352512 logging_writer.py:48] [95300] global_step=95300, grad_norm=2.596017837524414, loss=2.920545816421509
I0127 22:34:08.246478 139656800745216 logging_writer.py:48] [95400] global_step=95400, grad_norm=2.3481388092041016, loss=2.9620633125305176
I0127 22:34:42.371869 139656792352512 logging_writer.py:48] [95500] global_step=95500, grad_norm=2.4908530712127686, loss=3.0452983379364014
I0127 22:35:16.491738 139656800745216 logging_writer.py:48] [95600] global_step=95600, grad_norm=2.3498127460479736, loss=2.9592201709747314
I0127 22:35:31.650863 139822745589568 spec.py:321] Evaluating on the training split.
I0127 22:35:37.859774 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 22:35:46.417594 139822745589568 spec.py:349] Evaluating on the test split.
I0127 22:35:48.902563 139822745589568 submission_runner.py:408] Time since start: 33812.29s, 	Step: 95646, 	{'train/accuracy': 0.8151108026504517, 'train/loss': 0.9238508343696594, 'validation/accuracy': 0.7001399993896484, 'validation/loss': 1.4183787107467651, 'validation/num_examples': 50000, 'test/accuracy': 0.570900022983551, 'test/loss': 2.061049699783325, 'test/num_examples': 10000, 'score': 32678.294413089752, 'total_duration': 33812.28715801239, 'accumulated_submission_time': 32678.294413089752, 'accumulated_eval_time': 1128.321323633194, 'accumulated_logging_time': 2.4101314544677734}
I0127 22:35:48.936230 139656825923328 logging_writer.py:48] [95646] accumulated_eval_time=1128.321324, accumulated_logging_time=2.410131, accumulated_submission_time=32678.294413, global_step=95646, preemption_count=0, score=32678.294413, test/accuracy=0.570900, test/loss=2.061050, test/num_examples=10000, total_duration=33812.287158, train/accuracy=0.815111, train/loss=0.923851, validation/accuracy=0.700140, validation/loss=1.418379, validation/num_examples=50000
I0127 22:36:07.683309 139656834316032 logging_writer.py:48] [95700] global_step=95700, grad_norm=2.3392298221588135, loss=2.9419312477111816
I0127 22:36:41.771161 139656825923328 logging_writer.py:48] [95800] global_step=95800, grad_norm=2.4794199466705322, loss=2.9318666458129883
I0127 22:37:15.872812 139656834316032 logging_writer.py:48] [95900] global_step=95900, grad_norm=2.545522689819336, loss=3.0066280364990234
I0127 22:37:49.975569 139656825923328 logging_writer.py:48] [96000] global_step=96000, grad_norm=2.674665689468384, loss=2.981044292449951
I0127 22:38:24.235486 139656834316032 logging_writer.py:48] [96100] global_step=96100, grad_norm=2.5109176635742188, loss=2.893846035003662
I0127 22:38:58.356287 139656825923328 logging_writer.py:48] [96200] global_step=96200, grad_norm=2.6348795890808105, loss=2.9664077758789062
I0127 22:39:32.474449 139656834316032 logging_writer.py:48] [96300] global_step=96300, grad_norm=2.43892240524292, loss=2.9768457412719727
I0127 22:40:06.571670 139656825923328 logging_writer.py:48] [96400] global_step=96400, grad_norm=2.2876131534576416, loss=2.892118453979492
I0127 22:40:40.701371 139656834316032 logging_writer.py:48] [96500] global_step=96500, grad_norm=2.469265937805176, loss=2.9270942211151123
I0127 22:41:14.810937 139656825923328 logging_writer.py:48] [96600] global_step=96600, grad_norm=2.6955406665802, loss=3.0024898052215576
I0127 22:41:48.944470 139656834316032 logging_writer.py:48] [96700] global_step=96700, grad_norm=2.3983514308929443, loss=2.933492660522461
I0127 22:42:23.050608 139656825923328 logging_writer.py:48] [96800] global_step=96800, grad_norm=2.5050716400146484, loss=2.9521360397338867
I0127 22:42:57.174010 139656834316032 logging_writer.py:48] [96900] global_step=96900, grad_norm=2.3765573501586914, loss=2.961325168609619
I0127 22:43:31.320501 139656825923328 logging_writer.py:48] [97000] global_step=97000, grad_norm=2.4323651790618896, loss=2.93058443069458
I0127 22:44:05.439060 139656834316032 logging_writer.py:48] [97100] global_step=97100, grad_norm=2.519895076751709, loss=2.980403423309326
I0127 22:44:18.962056 139822745589568 spec.py:321] Evaluating on the training split.
I0127 22:44:25.166964 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 22:44:33.646295 139822745589568 spec.py:349] Evaluating on the test split.
I0127 22:44:35.978692 139822745589568 submission_runner.py:408] Time since start: 34339.36s, 	Step: 97141, 	{'train/accuracy': 0.8090322017669678, 'train/loss': 0.9489533305168152, 'validation/accuracy': 0.7054199576377869, 'validation/loss': 1.3887578248977661, 'validation/num_examples': 50000, 'test/accuracy': 0.5785000324249268, 'test/loss': 2.047945022583008, 'test/num_examples': 10000, 'score': 33188.26053261757, 'total_duration': 34339.3632774353, 'accumulated_submission_time': 33188.26053261757, 'accumulated_eval_time': 1145.3379135131836, 'accumulated_logging_time': 2.4528286457061768}
I0127 22:44:36.020214 139656792352512 logging_writer.py:48] [97141] accumulated_eval_time=1145.337914, accumulated_logging_time=2.452829, accumulated_submission_time=33188.260533, global_step=97141, preemption_count=0, score=33188.260533, test/accuracy=0.578500, test/loss=2.047945, test/num_examples=10000, total_duration=34339.363277, train/accuracy=0.809032, train/loss=0.948953, validation/accuracy=0.705420, validation/loss=1.388758, validation/num_examples=50000
I0127 22:44:56.486373 139656800745216 logging_writer.py:48] [97200] global_step=97200, grad_norm=2.5895121097564697, loss=3.0217607021331787
I0127 22:45:30.558609 139656792352512 logging_writer.py:48] [97300] global_step=97300, grad_norm=2.390796422958374, loss=2.9418907165527344
I0127 22:46:04.650469 139656800745216 logging_writer.py:48] [97400] global_step=97400, grad_norm=2.727626323699951, loss=2.951432228088379
I0127 22:46:38.757816 139656792352512 logging_writer.py:48] [97500] global_step=97500, grad_norm=2.477236747741699, loss=2.8771860599517822
I0127 22:47:12.903785 139656800745216 logging_writer.py:48] [97600] global_step=97600, grad_norm=2.3247148990631104, loss=2.8851852416992188
I0127 22:47:46.994702 139656792352512 logging_writer.py:48] [97700] global_step=97700, grad_norm=2.415494918823242, loss=2.8838906288146973
I0127 22:48:21.116465 139656800745216 logging_writer.py:48] [97800] global_step=97800, grad_norm=2.597785234451294, loss=3.023245334625244
I0127 22:48:55.198819 139656792352512 logging_writer.py:48] [97900] global_step=97900, grad_norm=2.3164634704589844, loss=2.939126968383789
I0127 22:49:29.307884 139656800745216 logging_writer.py:48] [98000] global_step=98000, grad_norm=2.508756160736084, loss=2.96396541595459
I0127 22:50:03.429715 139656792352512 logging_writer.py:48] [98100] global_step=98100, grad_norm=2.4382946491241455, loss=2.953517198562622
I0127 22:50:37.531208 139656800745216 logging_writer.py:48] [98200] global_step=98200, grad_norm=2.433858871459961, loss=2.893439531326294
I0127 22:51:11.844071 139656792352512 logging_writer.py:48] [98300] global_step=98300, grad_norm=2.3439152240753174, loss=2.9061880111694336
I0127 22:51:45.951787 139656800745216 logging_writer.py:48] [98400] global_step=98400, grad_norm=2.541189432144165, loss=3.021315097808838
I0127 22:52:20.081610 139656792352512 logging_writer.py:48] [98500] global_step=98500, grad_norm=2.3804996013641357, loss=2.934135913848877
I0127 22:52:54.179813 139656800745216 logging_writer.py:48] [98600] global_step=98600, grad_norm=2.440295696258545, loss=2.959728240966797
I0127 22:53:06.274139 139822745589568 spec.py:321] Evaluating on the training split.
I0127 22:53:12.549721 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 22:53:21.249820 139822745589568 spec.py:349] Evaluating on the test split.
I0127 22:53:23.678048 139822745589568 submission_runner.py:408] Time since start: 34867.06s, 	Step: 98637, 	{'train/accuracy': 0.8087332248687744, 'train/loss': 0.9831731915473938, 'validation/accuracy': 0.7090199589729309, 'validation/loss': 1.404129147529602, 'validation/num_examples': 50000, 'test/accuracy': 0.5819000005722046, 'test/loss': 2.0445165634155273, 'test/num_examples': 10000, 'score': 33698.45007276535, 'total_duration': 34867.06263709068, 'accumulated_submission_time': 33698.45007276535, 'accumulated_eval_time': 1162.7417826652527, 'accumulated_logging_time': 2.5040063858032227}
I0127 22:53:23.715752 139656783959808 logging_writer.py:48] [98637] accumulated_eval_time=1162.741783, accumulated_logging_time=2.504006, accumulated_submission_time=33698.450073, global_step=98637, preemption_count=0, score=33698.450073, test/accuracy=0.581900, test/loss=2.044517, test/num_examples=10000, total_duration=34867.062637, train/accuracy=0.808733, train/loss=0.983173, validation/accuracy=0.709020, validation/loss=1.404129, validation/num_examples=50000
I0127 22:53:45.542321 139656792352512 logging_writer.py:48] [98700] global_step=98700, grad_norm=2.6240928173065186, loss=2.998969316482544
I0127 22:54:19.627496 139656783959808 logging_writer.py:48] [98800] global_step=98800, grad_norm=2.6109414100646973, loss=2.9687442779541016
I0127 22:54:53.738820 139656792352512 logging_writer.py:48] [98900] global_step=98900, grad_norm=2.4597129821777344, loss=2.926856517791748
I0127 22:55:27.850291 139656783959808 logging_writer.py:48] [99000] global_step=99000, grad_norm=2.552095651626587, loss=3.00327467918396
I0127 22:56:01.980467 139656792352512 logging_writer.py:48] [99100] global_step=99100, grad_norm=2.425079345703125, loss=2.858971118927002
I0127 22:56:36.096214 139656783959808 logging_writer.py:48] [99200] global_step=99200, grad_norm=2.585123062133789, loss=3.007863759994507
I0127 22:57:10.410064 139656792352512 logging_writer.py:48] [99300] global_step=99300, grad_norm=2.6191253662109375, loss=2.952866554260254
I0127 22:57:44.544984 139656783959808 logging_writer.py:48] [99400] global_step=99400, grad_norm=2.488697052001953, loss=2.904622793197632
I0127 22:58:18.674981 139656792352512 logging_writer.py:48] [99500] global_step=99500, grad_norm=2.3639438152313232, loss=2.973292589187622
I0127 22:58:52.799891 139656783959808 logging_writer.py:48] [99600] global_step=99600, grad_norm=2.571010112762451, loss=2.9519050121307373
I0127 22:59:26.919009 139656792352512 logging_writer.py:48] [99700] global_step=99700, grad_norm=2.5218260288238525, loss=2.910109758377075
I0127 23:00:01.072974 139656783959808 logging_writer.py:48] [99800] global_step=99800, grad_norm=2.445373773574829, loss=2.8503148555755615
I0127 23:00:35.182691 139656792352512 logging_writer.py:48] [99900] global_step=99900, grad_norm=2.59895396232605, loss=3.0055699348449707
I0127 23:01:09.315237 139656783959808 logging_writer.py:48] [100000] global_step=100000, grad_norm=2.3823397159576416, loss=2.882094383239746
I0127 23:01:43.388618 139656792352512 logging_writer.py:48] [100100] global_step=100100, grad_norm=2.4865856170654297, loss=2.9458727836608887
I0127 23:01:53.775549 139822745589568 spec.py:321] Evaluating on the training split.
I0127 23:02:00.039550 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 23:02:08.781868 139822745589568 spec.py:349] Evaluating on the test split.
I0127 23:02:11.208840 139822745589568 submission_runner.py:408] Time since start: 35394.59s, 	Step: 100132, 	{'train/accuracy': 0.7820471525192261, 'train/loss': 1.040468454360962, 'validation/accuracy': 0.687720000743866, 'validation/loss': 1.4538909196853638, 'validation/num_examples': 50000, 'test/accuracy': 0.5609000325202942, 'test/loss': 2.109252691268921, 'test/num_examples': 10000, 'score': 34208.44733428955, 'total_duration': 35394.59343075752, 'accumulated_submission_time': 34208.44733428955, 'accumulated_eval_time': 1180.1750495433807, 'accumulated_logging_time': 2.553105354309082}
I0127 23:02:11.246389 139656783959808 logging_writer.py:48] [100132] accumulated_eval_time=1180.175050, accumulated_logging_time=2.553105, accumulated_submission_time=34208.447334, global_step=100132, preemption_count=0, score=34208.447334, test/accuracy=0.560900, test/loss=2.109253, test/num_examples=10000, total_duration=35394.593431, train/accuracy=0.782047, train/loss=1.040468, validation/accuracy=0.687720, validation/loss=1.453891, validation/num_examples=50000
I0127 23:02:34.742043 139656792352512 logging_writer.py:48] [100200] global_step=100200, grad_norm=2.4047939777374268, loss=2.8412258625030518
I0127 23:03:08.812733 139656783959808 logging_writer.py:48] [100300] global_step=100300, grad_norm=2.614166736602783, loss=3.0252890586853027
I0127 23:03:43.005595 139656792352512 logging_writer.py:48] [100400] global_step=100400, grad_norm=2.4349749088287354, loss=2.9199843406677246
I0127 23:04:17.086255 139656783959808 logging_writer.py:48] [100500] global_step=100500, grad_norm=2.5480849742889404, loss=2.9544591903686523
I0127 23:04:51.198758 139656792352512 logging_writer.py:48] [100600] global_step=100600, grad_norm=2.5132153034210205, loss=2.8899483680725098
I0127 23:05:25.313022 139656783959808 logging_writer.py:48] [100700] global_step=100700, grad_norm=2.641126871109009, loss=2.993252754211426
I0127 23:05:59.421398 139656792352512 logging_writer.py:48] [100800] global_step=100800, grad_norm=2.5529415607452393, loss=2.8695051670074463
I0127 23:06:33.535550 139656783959808 logging_writer.py:48] [100900] global_step=100900, grad_norm=2.66941499710083, loss=2.962231159210205
I0127 23:07:07.666720 139656792352512 logging_writer.py:48] [101000] global_step=101000, grad_norm=2.5721349716186523, loss=3.0160951614379883
I0127 23:07:41.786616 139656783959808 logging_writer.py:48] [101100] global_step=101100, grad_norm=2.6282122135162354, loss=2.9103729724884033
I0127 23:08:15.894909 139656792352512 logging_writer.py:48] [101200] global_step=101200, grad_norm=2.445924758911133, loss=2.9132533073425293
I0127 23:08:50.013903 139656783959808 logging_writer.py:48] [101300] global_step=101300, grad_norm=2.684884786605835, loss=2.8900017738342285
I0127 23:09:24.133459 139656792352512 logging_writer.py:48] [101400] global_step=101400, grad_norm=2.5300357341766357, loss=2.9348556995391846
I0127 23:09:58.328605 139656783959808 logging_writer.py:48] [101500] global_step=101500, grad_norm=2.5254955291748047, loss=2.9032886028289795
I0127 23:10:32.448366 139656792352512 logging_writer.py:48] [101600] global_step=101600, grad_norm=2.537940502166748, loss=2.9257636070251465
I0127 23:10:41.480230 139822745589568 spec.py:321] Evaluating on the training split.
I0127 23:10:47.665845 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 23:10:56.354405 139822745589568 spec.py:349] Evaluating on the test split.
I0127 23:10:58.771241 139822745589568 submission_runner.py:408] Time since start: 35922.16s, 	Step: 101628, 	{'train/accuracy': 0.8019172549247742, 'train/loss': 0.9545334577560425, 'validation/accuracy': 0.7048199772834778, 'validation/loss': 1.3737319707870483, 'validation/num_examples': 50000, 'test/accuracy': 0.5788000226020813, 'test/loss': 2.02254056930542, 'test/num_examples': 10000, 'score': 34718.620924949646, 'total_duration': 35922.155831575394, 'accumulated_submission_time': 34718.620924949646, 'accumulated_eval_time': 1197.4660267829895, 'accumulated_logging_time': 2.60019588470459}
I0127 23:10:58.810078 139656825923328 logging_writer.py:48] [101628] accumulated_eval_time=1197.466027, accumulated_logging_time=2.600196, accumulated_submission_time=34718.620925, global_step=101628, preemption_count=0, score=34718.620925, test/accuracy=0.578800, test/loss=2.022541, test/num_examples=10000, total_duration=35922.155832, train/accuracy=0.801917, train/loss=0.954533, validation/accuracy=0.704820, validation/loss=1.373732, validation/num_examples=50000
I0127 23:11:23.681284 139656834316032 logging_writer.py:48] [101700] global_step=101700, grad_norm=2.688809871673584, loss=2.9128873348236084
I0127 23:11:57.769151 139656825923328 logging_writer.py:48] [101800] global_step=101800, grad_norm=2.517686128616333, loss=2.9065957069396973
I0127 23:12:31.843883 139656834316032 logging_writer.py:48] [101900] global_step=101900, grad_norm=2.7031660079956055, loss=3.0164902210235596
I0127 23:13:05.977928 139656825923328 logging_writer.py:48] [102000] global_step=102000, grad_norm=2.6471612453460693, loss=2.9520256519317627
I0127 23:13:40.092057 139656834316032 logging_writer.py:48] [102100] global_step=102100, grad_norm=2.614748239517212, loss=2.916564702987671
I0127 23:14:14.221478 139656825923328 logging_writer.py:48] [102200] global_step=102200, grad_norm=2.3602356910705566, loss=2.883577585220337
I0127 23:14:48.343719 139656834316032 logging_writer.py:48] [102300] global_step=102300, grad_norm=2.5534236431121826, loss=2.857668161392212
I0127 23:15:22.453787 139656825923328 logging_writer.py:48] [102400] global_step=102400, grad_norm=2.625580072402954, loss=2.915031671524048
I0127 23:15:56.621167 139656834316032 logging_writer.py:48] [102500] global_step=102500, grad_norm=2.5977652072906494, loss=2.943673849105835
I0127 23:16:30.756273 139656825923328 logging_writer.py:48] [102600] global_step=102600, grad_norm=2.51524019241333, loss=2.97702956199646
I0127 23:17:04.884697 139656834316032 logging_writer.py:48] [102700] global_step=102700, grad_norm=2.4215619564056396, loss=2.8479361534118652
I0127 23:17:39.003000 139656825923328 logging_writer.py:48] [102800] global_step=102800, grad_norm=2.5653254985809326, loss=2.894657850265503
I0127 23:18:13.138280 139656834316032 logging_writer.py:48] [102900] global_step=102900, grad_norm=2.660784959793091, loss=2.990483045578003
I0127 23:18:47.248583 139656825923328 logging_writer.py:48] [103000] global_step=103000, grad_norm=2.7031733989715576, loss=2.994372844696045
I0127 23:19:21.373238 139656834316032 logging_writer.py:48] [103100] global_step=103100, grad_norm=2.732961654663086, loss=2.91549015045166
I0127 23:19:29.017184 139822745589568 spec.py:321] Evaluating on the training split.
I0127 23:19:35.200008 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 23:19:43.928615 139822745589568 spec.py:349] Evaluating on the test split.
I0127 23:19:46.344428 139822745589568 submission_runner.py:408] Time since start: 36449.73s, 	Step: 103124, 	{'train/accuracy': 0.8040497303009033, 'train/loss': 0.9926905035972595, 'validation/accuracy': 0.7096999883651733, 'validation/loss': 1.3941349983215332, 'validation/num_examples': 50000, 'test/accuracy': 0.5879000425338745, 'test/loss': 2.022136926651001, 'test/num_examples': 10000, 'score': 35228.76759457588, 'total_duration': 36449.7290225029, 'accumulated_submission_time': 35228.76759457588, 'accumulated_eval_time': 1214.7932357788086, 'accumulated_logging_time': 2.648162364959717}
I0127 23:19:46.379827 139656783959808 logging_writer.py:48] [103124] accumulated_eval_time=1214.793236, accumulated_logging_time=2.648162, accumulated_submission_time=35228.767595, global_step=103124, preemption_count=0, score=35228.767595, test/accuracy=0.587900, test/loss=2.022137, test/num_examples=10000, total_duration=36449.729023, train/accuracy=0.804050, train/loss=0.992691, validation/accuracy=0.709700, validation/loss=1.394135, validation/num_examples=50000
I0127 23:20:12.622447 139656792352512 logging_writer.py:48] [103200] global_step=103200, grad_norm=2.6611850261688232, loss=2.9455275535583496
I0127 23:20:46.733618 139656783959808 logging_writer.py:48] [103300] global_step=103300, grad_norm=2.6039042472839355, loss=2.9816226959228516
I0127 23:21:20.823467 139656792352512 logging_writer.py:48] [103400] global_step=103400, grad_norm=2.56787371635437, loss=2.913238286972046
I0127 23:21:54.944384 139656783959808 logging_writer.py:48] [103500] global_step=103500, grad_norm=2.534914970397949, loss=2.902996063232422
I0127 23:22:29.111968 139656792352512 logging_writer.py:48] [103600] global_step=103600, grad_norm=2.779865026473999, loss=3.0063247680664062
I0127 23:23:03.221919 139656783959808 logging_writer.py:48] [103700] global_step=103700, grad_norm=2.435746192932129, loss=2.894744873046875
I0127 23:23:37.322869 139656792352512 logging_writer.py:48] [103800] global_step=103800, grad_norm=2.590459108352661, loss=2.905332326889038
I0127 23:24:11.428527 139656783959808 logging_writer.py:48] [103900] global_step=103900, grad_norm=2.6331369876861572, loss=2.8766233921051025
I0127 23:24:45.546643 139656792352512 logging_writer.py:48] [104000] global_step=104000, grad_norm=2.68448543548584, loss=2.911771297454834
I0127 23:25:19.679769 139656783959808 logging_writer.py:48] [104100] global_step=104100, grad_norm=2.539806604385376, loss=2.9254989624023438
I0127 23:25:53.770526 139656792352512 logging_writer.py:48] [104200] global_step=104200, grad_norm=2.6653363704681396, loss=2.8844988346099854
I0127 23:26:27.888769 139656783959808 logging_writer.py:48] [104300] global_step=104300, grad_norm=2.534294605255127, loss=2.884861946105957
I0127 23:27:01.991836 139656792352512 logging_writer.py:48] [104400] global_step=104400, grad_norm=2.8946187496185303, loss=2.9125943183898926
I0127 23:27:36.128676 139656783959808 logging_writer.py:48] [104500] global_step=104500, grad_norm=2.573263168334961, loss=2.962282180786133
I0127 23:28:10.249327 139656792352512 logging_writer.py:48] [104600] global_step=104600, grad_norm=2.623507261276245, loss=2.95166015625
I0127 23:28:16.537435 139822745589568 spec.py:321] Evaluating on the training split.
I0127 23:28:22.887704 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 23:28:31.491565 139822745589568 spec.py:349] Evaluating on the test split.
I0127 23:28:34.276444 139822745589568 submission_runner.py:408] Time since start: 36977.66s, 	Step: 104620, 	{'train/accuracy': 0.8269292116165161, 'train/loss': 0.9147905707359314, 'validation/accuracy': 0.7056399583816528, 'validation/loss': 1.4107757806777954, 'validation/num_examples': 50000, 'test/accuracy': 0.5788000226020813, 'test/loss': 2.058584213256836, 'test/num_examples': 10000, 'score': 35738.865753889084, 'total_duration': 36977.661039590836, 'accumulated_submission_time': 35738.865753889084, 'accumulated_eval_time': 1232.5322148799896, 'accumulated_logging_time': 2.6931216716766357}
I0127 23:28:34.316020 139656825923328 logging_writer.py:48] [104620] accumulated_eval_time=1232.532215, accumulated_logging_time=2.693122, accumulated_submission_time=35738.865754, global_step=104620, preemption_count=0, score=35738.865754, test/accuracy=0.578800, test/loss=2.058584, test/num_examples=10000, total_duration=36977.661040, train/accuracy=0.826929, train/loss=0.914791, validation/accuracy=0.705640, validation/loss=1.410776, validation/num_examples=50000
I0127 23:29:01.937534 139656834316032 logging_writer.py:48] [104700] global_step=104700, grad_norm=2.641453266143799, loss=2.9481472969055176
I0127 23:29:36.017986 139656825923328 logging_writer.py:48] [104800] global_step=104800, grad_norm=2.6047210693359375, loss=2.9492526054382324
I0127 23:30:10.156325 139656834316032 logging_writer.py:48] [104900] global_step=104900, grad_norm=2.717128276824951, loss=2.904395341873169
I0127 23:30:44.291522 139656825923328 logging_writer.py:48] [105000] global_step=105000, grad_norm=2.736629009246826, loss=2.889415979385376
I0127 23:31:18.426208 139656834316032 logging_writer.py:48] [105100] global_step=105100, grad_norm=2.6688313484191895, loss=2.9188010692596436
I0127 23:31:52.548299 139656825923328 logging_writer.py:48] [105200] global_step=105200, grad_norm=2.5238046646118164, loss=2.841923236846924
I0127 23:32:26.631984 139656834316032 logging_writer.py:48] [105300] global_step=105300, grad_norm=2.446337938308716, loss=2.8873088359832764
I0127 23:33:00.782497 139656825923328 logging_writer.py:48] [105400] global_step=105400, grad_norm=2.7347052097320557, loss=2.9446563720703125
I0127 23:33:34.897124 139656834316032 logging_writer.py:48] [105500] global_step=105500, grad_norm=2.8255884647369385, loss=2.936891555786133
I0127 23:34:09.019662 139656825923328 logging_writer.py:48] [105600] global_step=105600, grad_norm=2.3619608879089355, loss=2.859923839569092
I0127 23:34:43.130769 139656834316032 logging_writer.py:48] [105700] global_step=105700, grad_norm=2.582130193710327, loss=2.9879631996154785
I0127 23:35:17.311398 139656825923328 logging_writer.py:48] [105800] global_step=105800, grad_norm=2.7746057510375977, loss=2.9106836318969727
I0127 23:35:51.424663 139656834316032 logging_writer.py:48] [105900] global_step=105900, grad_norm=2.730822801589966, loss=2.8984413146972656
I0127 23:36:25.538101 139656825923328 logging_writer.py:48] [106000] global_step=106000, grad_norm=2.531826972961426, loss=2.882929563522339
I0127 23:36:59.654844 139656834316032 logging_writer.py:48] [106100] global_step=106100, grad_norm=2.488922595977783, loss=2.93411922454834
I0127 23:37:04.581022 139822745589568 spec.py:321] Evaluating on the training split.
I0127 23:37:10.825588 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 23:37:19.269434 139822745589568 spec.py:349] Evaluating on the test split.
I0127 23:37:21.749842 139822745589568 submission_runner.py:408] Time since start: 37505.13s, 	Step: 106116, 	{'train/accuracy': 0.8301976919174194, 'train/loss': 0.8830342292785645, 'validation/accuracy': 0.711359977722168, 'validation/loss': 1.3829030990600586, 'validation/num_examples': 50000, 'test/accuracy': 0.5855000019073486, 'test/loss': 2.002440929412842, 'test/num_examples': 10000, 'score': 36249.07039260864, 'total_duration': 37505.13443374634, 'accumulated_submission_time': 36249.07039260864, 'accumulated_eval_time': 1249.7009994983673, 'accumulated_logging_time': 2.7423999309539795}
I0127 23:37:21.785080 139656783959808 logging_writer.py:48] [106116] accumulated_eval_time=1249.700999, accumulated_logging_time=2.742400, accumulated_submission_time=36249.070393, global_step=106116, preemption_count=0, score=36249.070393, test/accuracy=0.585500, test/loss=2.002441, test/num_examples=10000, total_duration=37505.134434, train/accuracy=0.830198, train/loss=0.883034, validation/accuracy=0.711360, validation/loss=1.382903, validation/num_examples=50000
I0127 23:37:50.742997 139656792352512 logging_writer.py:48] [106200] global_step=106200, grad_norm=2.6353676319122314, loss=2.9757745265960693
I0127 23:38:24.815977 139656783959808 logging_writer.py:48] [106300] global_step=106300, grad_norm=2.675788640975952, loss=2.9466402530670166
I0127 23:38:58.893600 139656792352512 logging_writer.py:48] [106400] global_step=106400, grad_norm=2.6919703483581543, loss=2.963585615158081
I0127 23:39:33.009028 139656783959808 logging_writer.py:48] [106500] global_step=106500, grad_norm=2.6671931743621826, loss=2.913748264312744
I0127 23:40:07.124714 139656792352512 logging_writer.py:48] [106600] global_step=106600, grad_norm=2.6905272006988525, loss=2.9530839920043945
I0127 23:40:41.253384 139656783959808 logging_writer.py:48] [106700] global_step=106700, grad_norm=2.6373684406280518, loss=2.8574275970458984
I0127 23:41:15.422570 139656792352512 logging_writer.py:48] [106800] global_step=106800, grad_norm=2.630889892578125, loss=2.947366237640381
I0127 23:41:49.543960 139656783959808 logging_writer.py:48] [106900] global_step=106900, grad_norm=2.567340135574341, loss=2.8300278186798096
I0127 23:42:23.646841 139656792352512 logging_writer.py:48] [107000] global_step=107000, grad_norm=2.7961344718933105, loss=2.962789297103882
I0127 23:42:57.779293 139656783959808 logging_writer.py:48] [107100] global_step=107100, grad_norm=2.7672157287597656, loss=2.8998358249664307
I0127 23:43:31.910494 139656792352512 logging_writer.py:48] [107200] global_step=107200, grad_norm=2.522815465927124, loss=2.8679099082946777
I0127 23:44:06.035509 139656783959808 logging_writer.py:48] [107300] global_step=107300, grad_norm=2.6571266651153564, loss=2.94793701171875
I0127 23:44:40.142248 139656792352512 logging_writer.py:48] [107400] global_step=107400, grad_norm=2.5771398544311523, loss=2.87662410736084
I0127 23:45:14.247991 139656783959808 logging_writer.py:48] [107500] global_step=107500, grad_norm=2.4786183834075928, loss=2.8817288875579834
I0127 23:45:48.365076 139656792352512 logging_writer.py:48] [107600] global_step=107600, grad_norm=2.801391363143921, loss=2.977909564971924
I0127 23:45:51.924226 139822745589568 spec.py:321] Evaluating on the training split.
I0127 23:45:58.122989 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 23:46:06.764822 139822745589568 spec.py:349] Evaluating on the test split.
I0127 23:46:09.203687 139822745589568 submission_runner.py:408] Time since start: 38032.59s, 	Step: 107612, 	{'train/accuracy': 0.8184390664100647, 'train/loss': 0.9219950437545776, 'validation/accuracy': 0.7115199565887451, 'validation/loss': 1.3807446956634521, 'validation/num_examples': 50000, 'test/accuracy': 0.5803000330924988, 'test/loss': 2.026761054992676, 'test/num_examples': 10000, 'score': 36759.14828467369, 'total_duration': 38032.58826184273, 'accumulated_submission_time': 36759.14828467369, 'accumulated_eval_time': 1266.9804100990295, 'accumulated_logging_time': 2.787186861038208}
I0127 23:46:09.241166 139656825923328 logging_writer.py:48] [107612] accumulated_eval_time=1266.980410, accumulated_logging_time=2.787187, accumulated_submission_time=36759.148285, global_step=107612, preemption_count=0, score=36759.148285, test/accuracy=0.580300, test/loss=2.026761, test/num_examples=10000, total_duration=38032.588262, train/accuracy=0.818439, train/loss=0.921995, validation/accuracy=0.711520, validation/loss=1.380745, validation/num_examples=50000
I0127 23:46:39.566224 139656834316032 logging_writer.py:48] [107700] global_step=107700, grad_norm=2.6152713298797607, loss=2.8449649810791016
I0127 23:47:13.643049 139656825923328 logging_writer.py:48] [107800] global_step=107800, grad_norm=2.5585010051727295, loss=2.883591651916504
I0127 23:47:47.930862 139656834316032 logging_writer.py:48] [107900] global_step=107900, grad_norm=2.5336499214172363, loss=2.8829736709594727
I0127 23:48:22.055566 139656825923328 logging_writer.py:48] [108000] global_step=108000, grad_norm=2.8094096183776855, loss=2.986020803451538
I0127 23:48:56.175029 139656834316032 logging_writer.py:48] [108100] global_step=108100, grad_norm=2.5216023921966553, loss=2.841418981552124
I0127 23:49:30.322269 139656825923328 logging_writer.py:48] [108200] global_step=108200, grad_norm=2.8608994483947754, loss=2.9299936294555664
I0127 23:50:04.431649 139656834316032 logging_writer.py:48] [108300] global_step=108300, grad_norm=2.814481735229492, loss=2.902430534362793
I0127 23:50:38.562752 139656825923328 logging_writer.py:48] [108400] global_step=108400, grad_norm=2.812239646911621, loss=2.9140143394470215
I0127 23:51:12.695049 139656834316032 logging_writer.py:48] [108500] global_step=108500, grad_norm=2.9267642498016357, loss=2.9533329010009766
I0127 23:51:46.810295 139656825923328 logging_writer.py:48] [108600] global_step=108600, grad_norm=2.805185556411743, loss=2.8567492961883545
I0127 23:52:20.943643 139656834316032 logging_writer.py:48] [108700] global_step=108700, grad_norm=2.721940517425537, loss=2.8875441551208496
I0127 23:52:55.042667 139656825923328 logging_writer.py:48] [108800] global_step=108800, grad_norm=2.6957600116729736, loss=2.908620595932007
I0127 23:53:29.169550 139656834316032 logging_writer.py:48] [108900] global_step=108900, grad_norm=2.7992100715637207, loss=2.9408891201019287
I0127 23:54:03.431888 139656825923328 logging_writer.py:48] [109000] global_step=109000, grad_norm=2.7611584663391113, loss=2.8920881748199463
I0127 23:54:37.555437 139656834316032 logging_writer.py:48] [109100] global_step=109100, grad_norm=2.701007127761841, loss=2.8712682723999023
I0127 23:54:39.403628 139822745589568 spec.py:321] Evaluating on the training split.
I0127 23:54:45.602092 139822745589568 spec.py:333] Evaluating on the validation split.
I0127 23:54:54.178104 139822745589568 spec.py:349] Evaluating on the test split.
I0127 23:54:56.596572 139822745589568 submission_runner.py:408] Time since start: 38559.98s, 	Step: 109107, 	{'train/accuracy': 0.8204121589660645, 'train/loss': 0.8763977885246277, 'validation/accuracy': 0.7163999676704407, 'validation/loss': 1.3261253833770752, 'validation/num_examples': 50000, 'test/accuracy': 0.5889000296592712, 'test/loss': 1.9659696817398071, 'test/num_examples': 10000, 'score': 37269.2513358593, 'total_duration': 38559.98116540909, 'accumulated_submission_time': 37269.2513358593, 'accumulated_eval_time': 1284.1733181476593, 'accumulated_logging_time': 2.833984613418579}
I0127 23:54:56.634592 139656800745216 logging_writer.py:48] [109107] accumulated_eval_time=1284.173318, accumulated_logging_time=2.833985, accumulated_submission_time=37269.251336, global_step=109107, preemption_count=0, score=37269.251336, test/accuracy=0.588900, test/loss=1.965970, test/num_examples=10000, total_duration=38559.981165, train/accuracy=0.820412, train/loss=0.876398, validation/accuracy=0.716400, validation/loss=1.326125, validation/num_examples=50000
I0127 23:55:28.670400 139656809137920 logging_writer.py:48] [109200] global_step=109200, grad_norm=2.6533007621765137, loss=2.846811294555664
I0127 23:56:02.734147 139656800745216 logging_writer.py:48] [109300] global_step=109300, grad_norm=2.6804933547973633, loss=2.8223037719726562
I0127 23:56:36.850050 139656809137920 logging_writer.py:48] [109400] global_step=109400, grad_norm=2.7651376724243164, loss=2.851062774658203
I0127 23:57:10.947522 139656800745216 logging_writer.py:48] [109500] global_step=109500, grad_norm=2.801353931427002, loss=2.9085755348205566
I0127 23:57:45.067990 139656809137920 logging_writer.py:48] [109600] global_step=109600, grad_norm=2.9823079109191895, loss=2.8906853199005127
I0127 23:58:19.139655 139656800745216 logging_writer.py:48] [109700] global_step=109700, grad_norm=2.7683629989624023, loss=2.914722442626953
I0127 23:58:53.255095 139656809137920 logging_writer.py:48] [109800] global_step=109800, grad_norm=2.714648962020874, loss=2.8478002548217773
I0127 23:59:27.336166 139656800745216 logging_writer.py:48] [109900] global_step=109900, grad_norm=2.7108685970306396, loss=2.884427070617676
I0128 00:00:01.490658 139656809137920 logging_writer.py:48] [110000] global_step=110000, grad_norm=2.6370716094970703, loss=2.867518663406372
I0128 00:00:35.616585 139656800745216 logging_writer.py:48] [110100] global_step=110100, grad_norm=2.876903533935547, loss=2.8879315853118896
I0128 00:01:09.711215 139656809137920 logging_writer.py:48] [110200] global_step=110200, grad_norm=2.7198801040649414, loss=2.910471200942993
I0128 00:01:43.850119 139656800745216 logging_writer.py:48] [110300] global_step=110300, grad_norm=2.740039348602295, loss=2.958714485168457
I0128 00:02:17.970141 139656809137920 logging_writer.py:48] [110400] global_step=110400, grad_norm=2.809434413909912, loss=2.9399375915527344
I0128 00:02:52.087028 139656800745216 logging_writer.py:48] [110500] global_step=110500, grad_norm=3.0036847591400146, loss=2.911113977432251
I0128 00:03:26.236995 139656809137920 logging_writer.py:48] [110600] global_step=110600, grad_norm=2.697512626647949, loss=2.822627305984497
I0128 00:03:26.729672 139822745589568 spec.py:321] Evaluating on the training split.
I0128 00:03:32.962618 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 00:03:41.636319 139822745589568 spec.py:349] Evaluating on the test split.
I0128 00:03:44.076828 139822745589568 submission_runner.py:408] Time since start: 39087.46s, 	Step: 110603, 	{'train/accuracy': 0.8202527165412903, 'train/loss': 0.907325804233551, 'validation/accuracy': 0.7150799632072449, 'validation/loss': 1.3554683923721313, 'validation/num_examples': 50000, 'test/accuracy': 0.5920000076293945, 'test/loss': 1.9924134016036987, 'test/num_examples': 10000, 'score': 37779.28477668762, 'total_duration': 39087.46141552925, 'accumulated_submission_time': 37779.28477668762, 'accumulated_eval_time': 1301.520435810089, 'accumulated_logging_time': 2.8837478160858154}
I0128 00:03:44.117419 139656700098304 logging_writer.py:48] [110603] accumulated_eval_time=1301.520436, accumulated_logging_time=2.883748, accumulated_submission_time=37779.284777, global_step=110603, preemption_count=0, score=37779.284777, test/accuracy=0.592000, test/loss=1.992413, test/num_examples=10000, total_duration=39087.461416, train/accuracy=0.820253, train/loss=0.907326, validation/accuracy=0.715080, validation/loss=1.355468, validation/num_examples=50000
I0128 00:04:17.519207 139656792352512 logging_writer.py:48] [110700] global_step=110700, grad_norm=2.595677137374878, loss=2.909813165664673
I0128 00:04:51.600122 139656700098304 logging_writer.py:48] [110800] global_step=110800, grad_norm=2.7891578674316406, loss=2.8523380756378174
I0128 00:05:25.705180 139656792352512 logging_writer.py:48] [110900] global_step=110900, grad_norm=2.6980326175689697, loss=2.90334415435791
I0128 00:05:59.831033 139656700098304 logging_writer.py:48] [111000] global_step=111000, grad_norm=3.0220084190368652, loss=2.978489637374878
I0128 00:06:33.986624 139656792352512 logging_writer.py:48] [111100] global_step=111100, grad_norm=2.7670161724090576, loss=2.9008898735046387
I0128 00:07:08.103487 139656700098304 logging_writer.py:48] [111200] global_step=111200, grad_norm=2.673927068710327, loss=2.855255365371704
I0128 00:07:42.238351 139656792352512 logging_writer.py:48] [111300] global_step=111300, grad_norm=2.7512760162353516, loss=2.941904067993164
I0128 00:08:16.372310 139656700098304 logging_writer.py:48] [111400] global_step=111400, grad_norm=2.728116273880005, loss=2.8451075553894043
I0128 00:08:50.499939 139656792352512 logging_writer.py:48] [111500] global_step=111500, grad_norm=2.7781121730804443, loss=2.862572193145752
I0128 00:09:24.624374 139656700098304 logging_writer.py:48] [111600] global_step=111600, grad_norm=2.7911078929901123, loss=2.9096038341522217
I0128 00:09:58.750735 139656792352512 logging_writer.py:48] [111700] global_step=111700, grad_norm=2.7683303356170654, loss=2.8706412315368652
I0128 00:10:32.896929 139656700098304 logging_writer.py:48] [111800] global_step=111800, grad_norm=2.7306320667266846, loss=2.8319344520568848
I0128 00:11:07.052358 139656792352512 logging_writer.py:48] [111900] global_step=111900, grad_norm=2.864245653152466, loss=2.8333468437194824
I0128 00:11:41.163023 139656700098304 logging_writer.py:48] [112000] global_step=112000, grad_norm=2.6950299739837646, loss=2.875908613204956
I0128 00:12:14.408049 139822745589568 spec.py:321] Evaluating on the training split.
I0128 00:12:20.792350 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 00:12:29.228115 139822745589568 spec.py:349] Evaluating on the test split.
I0128 00:12:31.642859 139822745589568 submission_runner.py:408] Time since start: 39615.03s, 	Step: 112099, 	{'train/accuracy': 0.8202327489852905, 'train/loss': 0.9242339134216309, 'validation/accuracy': 0.7185199856758118, 'validation/loss': 1.3499921560287476, 'validation/num_examples': 50000, 'test/accuracy': 0.5906000137329102, 'test/loss': 2.007988452911377, 'test/num_examples': 10000, 'score': 38289.51672291756, 'total_duration': 39615.02745246887, 'accumulated_submission_time': 38289.51672291756, 'accumulated_eval_time': 1318.7552318572998, 'accumulated_logging_time': 2.933572769165039}
I0128 00:12:31.683454 139656783959808 logging_writer.py:48] [112099] accumulated_eval_time=1318.755232, accumulated_logging_time=2.933573, accumulated_submission_time=38289.516723, global_step=112099, preemption_count=0, score=38289.516723, test/accuracy=0.590600, test/loss=2.007988, test/num_examples=10000, total_duration=39615.027452, train/accuracy=0.820233, train/loss=0.924234, validation/accuracy=0.718520, validation/loss=1.349992, validation/num_examples=50000
I0128 00:12:32.377984 139656809137920 logging_writer.py:48] [112100] global_step=112100, grad_norm=2.746631383895874, loss=2.865363597869873
I0128 00:13:06.506676 139656783959808 logging_writer.py:48] [112200] global_step=112200, grad_norm=2.75541615486145, loss=2.822686195373535
I0128 00:13:40.592494 139656809137920 logging_writer.py:48] [112300] global_step=112300, grad_norm=2.78048038482666, loss=2.9032304286956787
I0128 00:14:14.698555 139656783959808 logging_writer.py:48] [112400] global_step=112400, grad_norm=2.736907958984375, loss=2.9311017990112305
I0128 00:14:48.820243 139656809137920 logging_writer.py:48] [112500] global_step=112500, grad_norm=2.7723915576934814, loss=2.8830959796905518
I0128 00:15:22.944822 139656783959808 logging_writer.py:48] [112600] global_step=112600, grad_norm=2.8657052516937256, loss=2.9059596061706543
I0128 00:15:57.091244 139656809137920 logging_writer.py:48] [112700] global_step=112700, grad_norm=2.8417623043060303, loss=2.8801627159118652
I0128 00:16:31.227121 139656783959808 logging_writer.py:48] [112800] global_step=112800, grad_norm=2.8236849308013916, loss=2.8584632873535156
I0128 00:17:05.335746 139656809137920 logging_writer.py:48] [112900] global_step=112900, grad_norm=2.8699309825897217, loss=2.888982057571411
I0128 00:17:39.453011 139656783959808 logging_writer.py:48] [113000] global_step=113000, grad_norm=2.9052469730377197, loss=2.8721885681152344
I0128 00:18:13.588622 139656809137920 logging_writer.py:48] [113100] global_step=113100, grad_norm=2.6840004920959473, loss=2.8543903827667236
I0128 00:18:47.700228 139656783959808 logging_writer.py:48] [113200] global_step=113200, grad_norm=2.671677350997925, loss=2.8125154972076416
I0128 00:19:21.897254 139656809137920 logging_writer.py:48] [113300] global_step=113300, grad_norm=2.683213233947754, loss=2.812941074371338
I0128 00:19:56.018361 139656783959808 logging_writer.py:48] [113400] global_step=113400, grad_norm=2.7516894340515137, loss=2.8565428256988525
I0128 00:20:30.133480 139656809137920 logging_writer.py:48] [113500] global_step=113500, grad_norm=2.7901289463043213, loss=2.9111852645874023
I0128 00:21:01.724342 139822745589568 spec.py:321] Evaluating on the training split.
I0128 00:21:07.934962 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 00:21:16.525950 139822745589568 spec.py:349] Evaluating on the test split.
I0128 00:21:18.954231 139822745589568 submission_runner.py:408] Time since start: 40142.34s, 	Step: 113594, 	{'train/accuracy': 0.8160673975944519, 'train/loss': 0.9513839483261108, 'validation/accuracy': 0.7136200070381165, 'validation/loss': 1.3845340013504028, 'validation/num_examples': 50000, 'test/accuracy': 0.5892000198364258, 'test/loss': 2.0263185501098633, 'test/num_examples': 10000, 'score': 38799.4973552227, 'total_duration': 40142.33880519867, 'accumulated_submission_time': 38799.4973552227, 'accumulated_eval_time': 1335.9850897789001, 'accumulated_logging_time': 2.9843294620513916}
I0128 00:21:18.991773 139656800745216 logging_writer.py:48] [113594] accumulated_eval_time=1335.985090, accumulated_logging_time=2.984329, accumulated_submission_time=38799.497355, global_step=113594, preemption_count=0, score=38799.497355, test/accuracy=0.589200, test/loss=2.026319, test/num_examples=10000, total_duration=40142.338805, train/accuracy=0.816067, train/loss=0.951384, validation/accuracy=0.713620, validation/loss=1.384534, validation/num_examples=50000
I0128 00:21:21.392604 139656825923328 logging_writer.py:48] [113600] global_step=113600, grad_norm=2.718231678009033, loss=2.8375625610351562
I0128 00:21:55.462983 139656800745216 logging_writer.py:48] [113700] global_step=113700, grad_norm=2.806260824203491, loss=2.786142110824585
I0128 00:22:29.585394 139656825923328 logging_writer.py:48] [113800] global_step=113800, grad_norm=2.698500156402588, loss=2.8236358165740967
I0128 00:23:03.683871 139656800745216 logging_writer.py:48] [113900] global_step=113900, grad_norm=2.786525011062622, loss=2.842006206512451
I0128 00:23:37.826966 139656825923328 logging_writer.py:48] [114000] global_step=114000, grad_norm=3.032776355743408, loss=2.922229290008545
I0128 00:24:11.964922 139656800745216 logging_writer.py:48] [114100] global_step=114100, grad_norm=3.0342984199523926, loss=2.859323501586914
I0128 00:24:46.110043 139656825923328 logging_writer.py:48] [114200] global_step=114200, grad_norm=2.7179067134857178, loss=2.807229995727539
I0128 00:25:20.252304 139656800745216 logging_writer.py:48] [114300] global_step=114300, grad_norm=2.7692909240722656, loss=2.8379430770874023
I0128 00:25:54.362792 139656825923328 logging_writer.py:48] [114400] global_step=114400, grad_norm=2.854360580444336, loss=2.8425188064575195
I0128 00:26:28.462467 139656800745216 logging_writer.py:48] [114500] global_step=114500, grad_norm=2.8173880577087402, loss=2.8988559246063232
I0128 00:27:02.625050 139656825923328 logging_writer.py:48] [114600] global_step=114600, grad_norm=2.963008403778076, loss=2.9299612045288086
I0128 00:27:36.759049 139656800745216 logging_writer.py:48] [114700] global_step=114700, grad_norm=2.872020721435547, loss=2.8970370292663574
I0128 00:28:10.878286 139656825923328 logging_writer.py:48] [114800] global_step=114800, grad_norm=2.6873056888580322, loss=2.9218456745147705
I0128 00:28:45.011608 139656800745216 logging_writer.py:48] [114900] global_step=114900, grad_norm=2.824483871459961, loss=2.873023748397827
I0128 00:29:19.133567 139656825923328 logging_writer.py:48] [115000] global_step=115000, grad_norm=2.694514036178589, loss=2.919574022293091
I0128 00:29:49.275882 139822745589568 spec.py:321] Evaluating on the training split.
I0128 00:29:55.482295 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 00:30:04.238490 139822745589568 spec.py:349] Evaluating on the test split.
I0128 00:30:06.694973 139822745589568 submission_runner.py:408] Time since start: 40670.08s, 	Step: 115090, 	{'train/accuracy': 0.8474569320678711, 'train/loss': 0.7873832583427429, 'validation/accuracy': 0.7181199789047241, 'validation/loss': 1.3268829584121704, 'validation/num_examples': 50000, 'test/accuracy': 0.5927000045776367, 'test/loss': 1.9718732833862305, 'test/num_examples': 10000, 'score': 39309.721982717514, 'total_duration': 40670.07956337929, 'accumulated_submission_time': 39309.721982717514, 'accumulated_eval_time': 1353.4041435718536, 'accumulated_logging_time': 3.0307705402374268}
I0128 00:30:06.732853 139656800745216 logging_writer.py:48] [115090] accumulated_eval_time=1353.404144, accumulated_logging_time=3.030771, accumulated_submission_time=39309.721983, global_step=115090, preemption_count=0, score=39309.721983, test/accuracy=0.592700, test/loss=1.971873, test/num_examples=10000, total_duration=40670.079563, train/accuracy=0.847457, train/loss=0.787383, validation/accuracy=0.718120, validation/loss=1.326883, validation/num_examples=50000
I0128 00:30:10.488671 139656809137920 logging_writer.py:48] [115100] global_step=115100, grad_norm=2.8978843688964844, loss=2.8081130981445312
I0128 00:30:44.549406 139656800745216 logging_writer.py:48] [115200] global_step=115200, grad_norm=2.8647265434265137, loss=2.8424718379974365
I0128 00:31:18.620894 139656809137920 logging_writer.py:48] [115300] global_step=115300, grad_norm=2.8394198417663574, loss=2.8313658237457275
I0128 00:31:52.816851 139656800745216 logging_writer.py:48] [115400] global_step=115400, grad_norm=2.854020595550537, loss=2.846921682357788
I0128 00:32:26.933771 139656809137920 logging_writer.py:48] [115500] global_step=115500, grad_norm=2.938277244567871, loss=2.838825225830078
I0128 00:33:01.039380 139656800745216 logging_writer.py:48] [115600] global_step=115600, grad_norm=2.935908317565918, loss=2.84302020072937
I0128 00:33:35.137259 139656809137920 logging_writer.py:48] [115700] global_step=115700, grad_norm=2.7265799045562744, loss=2.8202037811279297
I0128 00:34:09.278439 139656800745216 logging_writer.py:48] [115800] global_step=115800, grad_norm=2.7332537174224854, loss=2.835700750350952
I0128 00:34:43.398318 139656809137920 logging_writer.py:48] [115900] global_step=115900, grad_norm=2.925814628601074, loss=2.811371088027954
I0128 00:35:17.503379 139656800745216 logging_writer.py:48] [116000] global_step=116000, grad_norm=2.8799233436584473, loss=2.8177404403686523
I0128 00:35:51.624445 139656809137920 logging_writer.py:48] [116100] global_step=116100, grad_norm=2.7942934036254883, loss=2.8403666019439697
I0128 00:36:25.737437 139656800745216 logging_writer.py:48] [116200] global_step=116200, grad_norm=3.1023738384246826, loss=2.8396403789520264
I0128 00:36:59.872099 139656809137920 logging_writer.py:48] [116300] global_step=116300, grad_norm=2.78922963142395, loss=2.846702814102173
I0128 00:37:34.002617 139656800745216 logging_writer.py:48] [116400] global_step=116400, grad_norm=2.6794331073760986, loss=2.80230712890625
I0128 00:38:08.187730 139656809137920 logging_writer.py:48] [116500] global_step=116500, grad_norm=2.6527957916259766, loss=2.823641538619995
I0128 00:38:36.969676 139822745589568 spec.py:321] Evaluating on the training split.
I0128 00:38:43.351909 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 00:38:51.887087 139822745589568 spec.py:349] Evaluating on the test split.
I0128 00:38:54.314112 139822745589568 submission_runner.py:408] Time since start: 41197.70s, 	Step: 116586, 	{'train/accuracy': 0.8415776491165161, 'train/loss': 0.8181595206260681, 'validation/accuracy': 0.7233999967575073, 'validation/loss': 1.3208779096603394, 'validation/num_examples': 50000, 'test/accuracy': 0.5944000482559204, 'test/loss': 1.96568763256073, 'test/num_examples': 10000, 'score': 39819.89853024483, 'total_duration': 41197.698704242706, 'accumulated_submission_time': 39819.89853024483, 'accumulated_eval_time': 1370.7485435009003, 'accumulated_logging_time': 3.077587127685547}
I0128 00:38:54.353982 139656792352512 logging_writer.py:48] [116586] accumulated_eval_time=1370.748544, accumulated_logging_time=3.077587, accumulated_submission_time=39819.898530, global_step=116586, preemption_count=0, score=39819.898530, test/accuracy=0.594400, test/loss=1.965688, test/num_examples=10000, total_duration=41197.698704, train/accuracy=0.841578, train/loss=0.818160, validation/accuracy=0.723400, validation/loss=1.320878, validation/num_examples=50000
I0128 00:38:59.475952 139656834316032 logging_writer.py:48] [116600] global_step=116600, grad_norm=2.9045064449310303, loss=2.8898606300354004
I0128 00:39:33.533893 139656792352512 logging_writer.py:48] [116700] global_step=116700, grad_norm=2.9892995357513428, loss=2.8404781818389893
I0128 00:40:07.636842 139656834316032 logging_writer.py:48] [116800] global_step=116800, grad_norm=3.232607364654541, loss=2.8843624591827393
I0128 00:40:41.758568 139656792352512 logging_writer.py:48] [116900] global_step=116900, grad_norm=2.979220390319824, loss=2.8814148902893066
I0128 00:41:15.869843 139656834316032 logging_writer.py:48] [117000] global_step=117000, grad_norm=2.7338526248931885, loss=2.7917542457580566
I0128 00:41:50.005931 139656792352512 logging_writer.py:48] [117100] global_step=117100, grad_norm=3.032127857208252, loss=2.814060688018799
I0128 00:42:24.144604 139656834316032 logging_writer.py:48] [117200] global_step=117200, grad_norm=2.8069422245025635, loss=2.7929577827453613
I0128 00:42:58.272168 139656792352512 logging_writer.py:48] [117300] global_step=117300, grad_norm=2.9555575847625732, loss=2.809635639190674
I0128 00:43:32.389774 139656834316032 logging_writer.py:48] [117400] global_step=117400, grad_norm=2.8465561866760254, loss=2.8622727394104004
I0128 00:44:06.591831 139656792352512 logging_writer.py:48] [117500] global_step=117500, grad_norm=2.953489065170288, loss=2.8425121307373047
I0128 00:44:40.705877 139656834316032 logging_writer.py:48] [117600] global_step=117600, grad_norm=3.3185477256774902, loss=2.8931164741516113
I0128 00:45:14.844789 139656792352512 logging_writer.py:48] [117700] global_step=117700, grad_norm=2.721867799758911, loss=2.8347506523132324
I0128 00:45:48.978871 139656834316032 logging_writer.py:48] [117800] global_step=117800, grad_norm=2.8418633937835693, loss=2.840435028076172
I0128 00:46:23.101505 139656792352512 logging_writer.py:48] [117900] global_step=117900, grad_norm=2.8710365295410156, loss=2.8778953552246094
I0128 00:46:57.250857 139656834316032 logging_writer.py:48] [118000] global_step=118000, grad_norm=2.8630874156951904, loss=2.8078863620758057
I0128 00:47:24.350222 139822745589568 spec.py:321] Evaluating on the training split.
I0128 00:47:31.204749 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 00:47:39.695837 139822745589568 spec.py:349] Evaluating on the test split.
I0128 00:47:42.121540 139822745589568 submission_runner.py:408] Time since start: 41725.51s, 	Step: 118081, 	{'train/accuracy': 0.8370934128761292, 'train/loss': 0.8302208185195923, 'validation/accuracy': 0.7227999567985535, 'validation/loss': 1.316156268119812, 'validation/num_examples': 50000, 'test/accuracy': 0.5999000072479248, 'test/loss': 1.953519582748413, 'test/num_examples': 10000, 'score': 40329.83609485626, 'total_duration': 41725.5061340332, 'accumulated_submission_time': 40329.83609485626, 'accumulated_eval_time': 1388.5198497772217, 'accumulated_logging_time': 3.126605749130249}
I0128 00:47:42.159100 139656783959808 logging_writer.py:48] [118081] accumulated_eval_time=1388.519850, accumulated_logging_time=3.126606, accumulated_submission_time=40329.836095, global_step=118081, preemption_count=0, score=40329.836095, test/accuracy=0.599900, test/loss=1.953520, test/num_examples=10000, total_duration=41725.506134, train/accuracy=0.837093, train/loss=0.830221, validation/accuracy=0.722800, validation/loss=1.316156, validation/num_examples=50000
I0128 00:47:48.996817 139656800745216 logging_writer.py:48] [118100] global_step=118100, grad_norm=2.788877010345459, loss=2.782379388809204
I0128 00:48:23.076197 139656783959808 logging_writer.py:48] [118200] global_step=118200, grad_norm=3.0589187145233154, loss=2.8741891384124756
I0128 00:48:57.146100 139656800745216 logging_writer.py:48] [118300] global_step=118300, grad_norm=2.798569917678833, loss=2.8962788581848145
I0128 00:49:31.259775 139656783959808 logging_writer.py:48] [118400] global_step=118400, grad_norm=3.139369487762451, loss=2.808816432952881
I0128 00:50:05.364323 139656800745216 logging_writer.py:48] [118500] global_step=118500, grad_norm=3.022991180419922, loss=2.8213977813720703
I0128 00:50:39.513598 139656783959808 logging_writer.py:48] [118600] global_step=118600, grad_norm=2.7948427200317383, loss=2.8389768600463867
I0128 00:51:13.591202 139656800745216 logging_writer.py:48] [118700] global_step=118700, grad_norm=2.885573625564575, loss=2.8423805236816406
I0128 00:51:47.698776 139656783959808 logging_writer.py:48] [118800] global_step=118800, grad_norm=2.9992620944976807, loss=2.8345768451690674
I0128 00:52:21.797636 139656800745216 logging_writer.py:48] [118900] global_step=118900, grad_norm=3.2262144088745117, loss=2.898531913757324
I0128 00:52:55.910722 139656783959808 logging_writer.py:48] [119000] global_step=119000, grad_norm=2.8344216346740723, loss=2.811283826828003
I0128 00:53:29.977213 139656800745216 logging_writer.py:48] [119100] global_step=119100, grad_norm=2.86130690574646, loss=2.8566179275512695
I0128 00:54:04.085055 139656783959808 logging_writer.py:48] [119200] global_step=119200, grad_norm=2.749319314956665, loss=2.8123788833618164
I0128 00:54:38.184236 139656800745216 logging_writer.py:48] [119300] global_step=119300, grad_norm=2.9865901470184326, loss=2.8902761936187744
I0128 00:55:12.313157 139656783959808 logging_writer.py:48] [119400] global_step=119400, grad_norm=3.00504469871521, loss=2.917372465133667
I0128 00:55:46.443117 139656800745216 logging_writer.py:48] [119500] global_step=119500, grad_norm=2.870866537094116, loss=2.829192638397217
I0128 00:56:12.164327 139822745589568 spec.py:321] Evaluating on the training split.
I0128 00:56:18.390490 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 00:56:27.023373 139822745589568 spec.py:349] Evaluating on the test split.
I0128 00:56:29.507229 139822745589568 submission_runner.py:408] Time since start: 42252.89s, 	Step: 119577, 	{'train/accuracy': 0.8384885191917419, 'train/loss': 0.8248199224472046, 'validation/accuracy': 0.7274799942970276, 'validation/loss': 1.3024871349334717, 'validation/num_examples': 50000, 'test/accuracy': 0.5933000445365906, 'test/loss': 1.958518624305725, 'test/num_examples': 10000, 'score': 40839.775539159775, 'total_duration': 42252.89182281494, 'accumulated_submission_time': 40839.775539159775, 'accumulated_eval_time': 1405.8627269268036, 'accumulated_logging_time': 3.179180860519409}
I0128 00:56:29.545011 139656792352512 logging_writer.py:48] [119577] accumulated_eval_time=1405.862727, accumulated_logging_time=3.179181, accumulated_submission_time=40839.775539, global_step=119577, preemption_count=0, score=40839.775539, test/accuracy=0.593300, test/loss=1.958519, test/num_examples=10000, total_duration=42252.891823, train/accuracy=0.838489, train/loss=0.824820, validation/accuracy=0.727480, validation/loss=1.302487, validation/num_examples=50000
I0128 00:56:37.730489 139656825923328 logging_writer.py:48] [119600] global_step=119600, grad_norm=2.8629150390625, loss=2.771312713623047
I0128 00:57:11.891290 139656792352512 logging_writer.py:48] [119700] global_step=119700, grad_norm=2.8510615825653076, loss=2.7738699913024902
I0128 00:57:45.987543 139656825923328 logging_writer.py:48] [119800] global_step=119800, grad_norm=2.9889612197875977, loss=2.889784097671509
I0128 00:58:20.097193 139656792352512 logging_writer.py:48] [119900] global_step=119900, grad_norm=3.5048959255218506, loss=2.8169445991516113
I0128 00:58:54.191525 139656825923328 logging_writer.py:48] [120000] global_step=120000, grad_norm=3.095151901245117, loss=2.8207099437713623
I0128 00:59:28.272397 139656792352512 logging_writer.py:48] [120100] global_step=120100, grad_norm=2.945706844329834, loss=2.831493854522705
I0128 01:00:02.398399 139656825923328 logging_writer.py:48] [120200] global_step=120200, grad_norm=2.9762144088745117, loss=2.7927119731903076
I0128 01:00:36.519705 139656792352512 logging_writer.py:48] [120300] global_step=120300, grad_norm=2.9557621479034424, loss=2.8219051361083984
I0128 01:01:10.654931 139656825923328 logging_writer.py:48] [120400] global_step=120400, grad_norm=3.070868492126465, loss=2.8818039894104004
I0128 01:01:44.766652 139656792352512 logging_writer.py:48] [120500] global_step=120500, grad_norm=3.0354530811309814, loss=2.9065849781036377
I0128 01:02:18.911437 139656825923328 logging_writer.py:48] [120600] global_step=120600, grad_norm=3.115206480026245, loss=2.836371898651123
I0128 01:02:53.085144 139656792352512 logging_writer.py:48] [120700] global_step=120700, grad_norm=2.9912590980529785, loss=2.81868839263916
I0128 01:03:27.221645 139656825923328 logging_writer.py:48] [120800] global_step=120800, grad_norm=2.922940254211426, loss=2.799353837966919
I0128 01:04:01.350602 139656792352512 logging_writer.py:48] [120900] global_step=120900, grad_norm=2.9967901706695557, loss=2.7966511249542236
I0128 01:04:35.468426 139656825923328 logging_writer.py:48] [121000] global_step=121000, grad_norm=2.9069101810455322, loss=2.7851574420928955
I0128 01:04:59.838994 139822745589568 spec.py:321] Evaluating on the training split.
I0128 01:05:06.147474 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 01:05:14.640244 139822745589568 spec.py:349] Evaluating on the test split.
I0128 01:05:17.063123 139822745589568 submission_runner.py:408] Time since start: 42780.45s, 	Step: 121073, 	{'train/accuracy': 0.8438097834587097, 'train/loss': 0.8003804683685303, 'validation/accuracy': 0.7287399768829346, 'validation/loss': 1.283615231513977, 'validation/num_examples': 50000, 'test/accuracy': 0.6027000546455383, 'test/loss': 1.9129961729049683, 'test/num_examples': 10000, 'score': 41350.00688409805, 'total_duration': 42780.44771409035, 'accumulated_submission_time': 41350.00688409805, 'accumulated_eval_time': 1423.086817741394, 'accumulated_logging_time': 3.2281734943389893}
I0128 01:05:17.106885 139656800745216 logging_writer.py:48] [121073] accumulated_eval_time=1423.086818, accumulated_logging_time=3.228173, accumulated_submission_time=41350.006884, global_step=121073, preemption_count=0, score=41350.006884, test/accuracy=0.602700, test/loss=1.912996, test/num_examples=10000, total_duration=42780.447714, train/accuracy=0.843810, train/loss=0.800380, validation/accuracy=0.728740, validation/loss=1.283615, validation/num_examples=50000
I0128 01:05:26.667051 139656809137920 logging_writer.py:48] [121100] global_step=121100, grad_norm=3.2613494396209717, loss=2.8028383255004883
I0128 01:06:00.746845 139656800745216 logging_writer.py:48] [121200] global_step=121200, grad_norm=2.9622929096221924, loss=2.8012685775756836
I0128 01:06:34.814559 139656809137920 logging_writer.py:48] [121300] global_step=121300, grad_norm=2.993661403656006, loss=2.7565689086914062
I0128 01:07:08.909214 139656800745216 logging_writer.py:48] [121400] global_step=121400, grad_norm=2.822256326675415, loss=2.877100944519043
I0128 01:07:42.998540 139656809137920 logging_writer.py:48] [121500] global_step=121500, grad_norm=3.112030506134033, loss=2.8139262199401855
I0128 01:08:17.103494 139656800745216 logging_writer.py:48] [121600] global_step=121600, grad_norm=2.952350378036499, loss=2.817917823791504
I0128 01:08:51.204243 139656809137920 logging_writer.py:48] [121700] global_step=121700, grad_norm=2.8927953243255615, loss=2.839315414428711
I0128 01:09:25.358940 139656800745216 logging_writer.py:48] [121800] global_step=121800, grad_norm=3.0057592391967773, loss=2.897538185119629
I0128 01:09:59.471907 139656809137920 logging_writer.py:48] [121900] global_step=121900, grad_norm=3.0300161838531494, loss=2.8443822860717773
I0128 01:10:33.591081 139656800745216 logging_writer.py:48] [122000] global_step=122000, grad_norm=3.1314098834991455, loss=2.8390560150146484
I0128 01:11:07.700569 139656809137920 logging_writer.py:48] [122100] global_step=122100, grad_norm=3.0130386352539062, loss=2.7984442710876465
I0128 01:11:41.834697 139656800745216 logging_writer.py:48] [122200] global_step=122200, grad_norm=3.0486950874328613, loss=2.7843270301818848
I0128 01:12:15.937978 139656809137920 logging_writer.py:48] [122300] global_step=122300, grad_norm=3.025815963745117, loss=2.8567023277282715
I0128 01:12:50.066587 139656800745216 logging_writer.py:48] [122400] global_step=122400, grad_norm=3.0579159259796143, loss=2.78328013420105
I0128 01:13:24.190270 139656809137920 logging_writer.py:48] [122500] global_step=122500, grad_norm=3.2101709842681885, loss=2.8199000358581543
I0128 01:13:47.197754 139822745589568 spec.py:321] Evaluating on the training split.
I0128 01:13:53.515948 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 01:14:01.854217 139822745589568 spec.py:349] Evaluating on the test split.
I0128 01:14:04.356095 139822745589568 submission_runner.py:408] Time since start: 43307.74s, 	Step: 122569, 	{'train/accuracy': 0.832051157951355, 'train/loss': 0.8912011384963989, 'validation/accuracy': 0.7217599749565125, 'validation/loss': 1.3543952703475952, 'validation/num_examples': 50000, 'test/accuracy': 0.5974000096321106, 'test/loss': 1.9827086925506592, 'test/num_examples': 10000, 'score': 41860.036199092865, 'total_duration': 43307.74064588547, 'accumulated_submission_time': 41860.036199092865, 'accumulated_eval_time': 1440.245083808899, 'accumulated_logging_time': 3.281466007232666}
I0128 01:14:04.408100 139656700098304 logging_writer.py:48] [122569] accumulated_eval_time=1440.245084, accumulated_logging_time=3.281466, accumulated_submission_time=41860.036199, global_step=122569, preemption_count=0, score=41860.036199, test/accuracy=0.597400, test/loss=1.982709, test/num_examples=10000, total_duration=43307.740646, train/accuracy=0.832051, train/loss=0.891201, validation/accuracy=0.721760, validation/loss=1.354395, validation/num_examples=50000
I0128 01:14:15.324726 139656792352512 logging_writer.py:48] [122600] global_step=122600, grad_norm=2.7885003089904785, loss=2.8149585723876953
I0128 01:14:49.403206 139656700098304 logging_writer.py:48] [122700] global_step=122700, grad_norm=3.0171151161193848, loss=2.8591883182525635
I0128 01:15:23.511145 139656792352512 logging_writer.py:48] [122800] global_step=122800, grad_norm=2.9568123817443848, loss=2.7977559566497803
I0128 01:15:57.695602 139656700098304 logging_writer.py:48] [122900] global_step=122900, grad_norm=3.0572729110717773, loss=2.8327972888946533
I0128 01:16:31.819281 139656792352512 logging_writer.py:48] [123000] global_step=123000, grad_norm=3.0075783729553223, loss=2.817141532897949
I0128 01:17:05.940175 139656700098304 logging_writer.py:48] [123100] global_step=123100, grad_norm=2.9844260215759277, loss=2.8533172607421875
I0128 01:17:40.060145 139656792352512 logging_writer.py:48] [123200] global_step=123200, grad_norm=3.0879647731781006, loss=2.7650628089904785
I0128 01:18:14.184431 139656700098304 logging_writer.py:48] [123300] global_step=123300, grad_norm=2.9196441173553467, loss=2.752897262573242
I0128 01:18:48.278198 139656792352512 logging_writer.py:48] [123400] global_step=123400, grad_norm=3.014922857284546, loss=2.8265514373779297
I0128 01:19:22.382879 139656700098304 logging_writer.py:48] [123500] global_step=123500, grad_norm=3.1485536098480225, loss=2.7980525493621826
I0128 01:19:56.495472 139656792352512 logging_writer.py:48] [123600] global_step=123600, grad_norm=3.030505895614624, loss=2.8037123680114746
I0128 01:20:30.616652 139656700098304 logging_writer.py:48] [123700] global_step=123700, grad_norm=2.961641788482666, loss=2.839885711669922
I0128 01:21:04.726279 139656792352512 logging_writer.py:48] [123800] global_step=123800, grad_norm=3.2101545333862305, loss=2.8309404850006104
I0128 01:21:38.863317 139656700098304 logging_writer.py:48] [123900] global_step=123900, grad_norm=3.055004596710205, loss=2.8184990882873535
I0128 01:22:13.021291 139656792352512 logging_writer.py:48] [124000] global_step=124000, grad_norm=2.966341018676758, loss=2.7990970611572266
I0128 01:22:34.675349 139822745589568 spec.py:321] Evaluating on the training split.
I0128 01:22:40.903381 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 01:22:49.417627 139822745589568 spec.py:349] Evaluating on the test split.
I0128 01:22:51.870687 139822745589568 submission_runner.py:408] Time since start: 43835.26s, 	Step: 124065, 	{'train/accuracy': 0.8768334984779358, 'train/loss': 0.6922958493232727, 'validation/accuracy': 0.7325199842453003, 'validation/loss': 1.2779021263122559, 'validation/num_examples': 50000, 'test/accuracy': 0.6080000400543213, 'test/loss': 1.9164098501205444, 'test/num_examples': 10000, 'score': 42370.23780179024, 'total_duration': 43835.2552819252, 'accumulated_submission_time': 42370.23780179024, 'accumulated_eval_time': 1457.4404020309448, 'accumulated_logging_time': 3.34682035446167}
I0128 01:22:51.913057 139656700098304 logging_writer.py:48] [124065] accumulated_eval_time=1457.440402, accumulated_logging_time=3.346820, accumulated_submission_time=42370.237802, global_step=124065, preemption_count=0, score=42370.237802, test/accuracy=0.608000, test/loss=1.916410, test/num_examples=10000, total_duration=43835.255282, train/accuracy=0.876833, train/loss=0.692296, validation/accuracy=0.732520, validation/loss=1.277902, validation/num_examples=50000
I0128 01:23:04.200266 139656783959808 logging_writer.py:48] [124100] global_step=124100, grad_norm=3.048389434814453, loss=2.838472366333008
I0128 01:23:38.281992 139656700098304 logging_writer.py:48] [124200] global_step=124200, grad_norm=2.870059013366699, loss=2.744727849960327
I0128 01:24:12.371275 139656783959808 logging_writer.py:48] [124300] global_step=124300, grad_norm=3.0220394134521484, loss=2.746304988861084
I0128 01:24:46.481075 139656700098304 logging_writer.py:48] [124400] global_step=124400, grad_norm=2.904695510864258, loss=2.8107447624206543
I0128 01:25:20.596768 139656783959808 logging_writer.py:48] [124500] global_step=124500, grad_norm=3.0249595642089844, loss=2.745997905731201
I0128 01:25:54.688801 139656700098304 logging_writer.py:48] [124600] global_step=124600, grad_norm=3.0450994968414307, loss=2.774461507797241
I0128 01:26:28.804585 139656783959808 logging_writer.py:48] [124700] global_step=124700, grad_norm=3.0019662380218506, loss=2.7479944229125977
I0128 01:27:02.899241 139656700098304 logging_writer.py:48] [124800] global_step=124800, grad_norm=2.9371144771575928, loss=2.7240660190582275
I0128 01:27:37.045125 139656783959808 logging_writer.py:48] [124900] global_step=124900, grad_norm=2.8745503425598145, loss=2.7129077911376953
I0128 01:28:11.219528 139656700098304 logging_writer.py:48] [125000] global_step=125000, grad_norm=2.8209683895111084, loss=2.7368104457855225
I0128 01:28:45.339911 139656783959808 logging_writer.py:48] [125100] global_step=125100, grad_norm=3.008347511291504, loss=2.809282064437866
I0128 01:29:19.444677 139656700098304 logging_writer.py:48] [125200] global_step=125200, grad_norm=3.079613208770752, loss=2.80844783782959
I0128 01:29:53.527240 139656783959808 logging_writer.py:48] [125300] global_step=125300, grad_norm=3.093100070953369, loss=2.7427189350128174
I0128 01:30:27.647856 139656700098304 logging_writer.py:48] [125400] global_step=125400, grad_norm=3.0988080501556396, loss=2.8076844215393066
I0128 01:31:01.757595 139656783959808 logging_writer.py:48] [125500] global_step=125500, grad_norm=3.1928012371063232, loss=2.8783087730407715
I0128 01:31:22.041710 139822745589568 spec.py:321] Evaluating on the training split.
I0128 01:31:28.183664 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 01:31:36.946532 139822745589568 spec.py:349] Evaluating on the test split.
I0128 01:31:39.458986 139822745589568 submission_runner.py:408] Time since start: 44362.84s, 	Step: 125561, 	{'train/accuracy': 0.857421875, 'train/loss': 0.7503759264945984, 'validation/accuracy': 0.7276999950408936, 'validation/loss': 1.2880154848098755, 'validation/num_examples': 50000, 'test/accuracy': 0.6013000011444092, 'test/loss': 1.927111268043518, 'test/num_examples': 10000, 'score': 42880.30663514137, 'total_duration': 44362.843577861786, 'accumulated_submission_time': 42880.30663514137, 'accumulated_eval_time': 1474.8576436042786, 'accumulated_logging_time': 3.398554563522339}
I0128 01:31:39.497778 139656783959808 logging_writer.py:48] [125561] accumulated_eval_time=1474.857644, accumulated_logging_time=3.398555, accumulated_submission_time=42880.306635, global_step=125561, preemption_count=0, score=42880.306635, test/accuracy=0.601300, test/loss=1.927111, test/num_examples=10000, total_duration=44362.843578, train/accuracy=0.857422, train/loss=0.750376, validation/accuracy=0.727700, validation/loss=1.288015, validation/num_examples=50000
I0128 01:31:53.136926 139656817530624 logging_writer.py:48] [125600] global_step=125600, grad_norm=3.130815029144287, loss=2.8236002922058105
I0128 01:32:27.206646 139656783959808 logging_writer.py:48] [125700] global_step=125700, grad_norm=2.9800150394439697, loss=2.722858428955078
I0128 01:33:01.302572 139656817530624 logging_writer.py:48] [125800] global_step=125800, grad_norm=3.2268319129943848, loss=2.8403189182281494
I0128 01:33:35.426286 139656783959808 logging_writer.py:48] [125900] global_step=125900, grad_norm=3.2830708026885986, loss=2.7835612297058105
I0128 01:34:09.551034 139656817530624 logging_writer.py:48] [126000] global_step=126000, grad_norm=3.126220703125, loss=2.8052444458007812
I0128 01:34:43.737143 139656783959808 logging_writer.py:48] [126100] global_step=126100, grad_norm=3.068559408187866, loss=2.7559494972229004
I0128 01:35:17.878079 139656817530624 logging_writer.py:48] [126200] global_step=126200, grad_norm=3.298126459121704, loss=2.8372416496276855
I0128 01:35:51.997199 139656783959808 logging_writer.py:48] [126300] global_step=126300, grad_norm=3.1455109119415283, loss=2.8456296920776367
I0128 01:36:26.149383 139656817530624 logging_writer.py:48] [126400] global_step=126400, grad_norm=3.4241714477539062, loss=2.805906295776367
I0128 01:37:00.241299 139656783959808 logging_writer.py:48] [126500] global_step=126500, grad_norm=3.0005953311920166, loss=2.704300880432129
I0128 01:37:34.359161 139656817530624 logging_writer.py:48] [126600] global_step=126600, grad_norm=3.0335583686828613, loss=2.812300443649292
I0128 01:38:08.468947 139656783959808 logging_writer.py:48] [126700] global_step=126700, grad_norm=3.041965961456299, loss=2.768392562866211
I0128 01:38:42.604150 139656817530624 logging_writer.py:48] [126800] global_step=126800, grad_norm=3.1832687854766846, loss=2.7898991107940674
I0128 01:39:16.706238 139656783959808 logging_writer.py:48] [126900] global_step=126900, grad_norm=3.1650702953338623, loss=2.841127634048462
I0128 01:39:50.804525 139656817530624 logging_writer.py:48] [127000] global_step=127000, grad_norm=3.058680772781372, loss=2.7724077701568604
I0128 01:40:09.720487 139822745589568 spec.py:321] Evaluating on the training split.
I0128 01:40:15.913246 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 01:40:24.559328 139822745589568 spec.py:349] Evaluating on the test split.
I0128 01:40:26.976416 139822745589568 submission_runner.py:408] Time since start: 44890.36s, 	Step: 127057, 	{'train/accuracy': 0.8565648794174194, 'train/loss': 0.7832421064376831, 'validation/accuracy': 0.7305200099945068, 'validation/loss': 1.3154152631759644, 'validation/num_examples': 50000, 'test/accuracy': 0.6014000177383423, 'test/loss': 1.9597022533416748, 'test/num_examples': 10000, 'score': 43390.468705654144, 'total_duration': 44890.360988378525, 'accumulated_submission_time': 43390.468705654144, 'accumulated_eval_time': 1492.1135189533234, 'accumulated_logging_time': 3.44738507270813}
I0128 01:40:27.015125 139656783959808 logging_writer.py:48] [127057] accumulated_eval_time=1492.113519, accumulated_logging_time=3.447385, accumulated_submission_time=43390.468706, global_step=127057, preemption_count=0, score=43390.468706, test/accuracy=0.601400, test/loss=1.959702, test/num_examples=10000, total_duration=44890.360988, train/accuracy=0.856565, train/loss=0.783242, validation/accuracy=0.730520, validation/loss=1.315415, validation/num_examples=50000
I0128 01:40:42.032442 139656792352512 logging_writer.py:48] [127100] global_step=127100, grad_norm=3.097266912460327, loss=2.81833553314209
I0128 01:41:16.162638 139656783959808 logging_writer.py:48] [127200] global_step=127200, grad_norm=3.25557541847229, loss=2.779197931289673
I0128 01:41:50.253880 139656792352512 logging_writer.py:48] [127300] global_step=127300, grad_norm=3.2877979278564453, loss=2.7737770080566406
I0128 01:42:24.356399 139656783959808 logging_writer.py:48] [127400] global_step=127400, grad_norm=3.1577701568603516, loss=2.757030487060547
I0128 01:42:58.443500 139656792352512 logging_writer.py:48] [127500] global_step=127500, grad_norm=3.012349843978882, loss=2.6772823333740234
I0128 01:43:32.556761 139656783959808 logging_writer.py:48] [127600] global_step=127600, grad_norm=3.120321273803711, loss=2.779697895050049
I0128 01:44:06.647355 139656792352512 logging_writer.py:48] [127700] global_step=127700, grad_norm=3.2902393341064453, loss=2.839860439300537
I0128 01:44:40.774421 139656783959808 logging_writer.py:48] [127800] global_step=127800, grad_norm=3.2018253803253174, loss=2.8279709815979004
I0128 01:45:14.915587 139656792352512 logging_writer.py:48] [127900] global_step=127900, grad_norm=3.2766077518463135, loss=2.7142093181610107
I0128 01:45:49.041590 139656783959808 logging_writer.py:48] [128000] global_step=128000, grad_norm=3.4338879585266113, loss=2.8234822750091553
I0128 01:46:23.156556 139656792352512 logging_writer.py:48] [128100] global_step=128100, grad_norm=3.2432117462158203, loss=2.807006597518921
I0128 01:46:57.244472 139656783959808 logging_writer.py:48] [128200] global_step=128200, grad_norm=3.14245343208313, loss=2.7202348709106445
I0128 01:47:31.440467 139656792352512 logging_writer.py:48] [128300] global_step=128300, grad_norm=3.461317539215088, loss=2.794910430908203
I0128 01:48:05.536067 139656783959808 logging_writer.py:48] [128400] global_step=128400, grad_norm=3.242255449295044, loss=2.7497634887695312
I0128 01:48:39.676414 139656792352512 logging_writer.py:48] [128500] global_step=128500, grad_norm=3.1897237300872803, loss=2.774932861328125
I0128 01:48:57.211145 139822745589568 spec.py:321] Evaluating on the training split.
I0128 01:49:03.644961 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 01:49:12.322149 139822745589568 spec.py:349] Evaluating on the test split.
I0128 01:49:14.739295 139822745589568 submission_runner.py:408] Time since start: 45418.12s, 	Step: 128553, 	{'train/accuracy': 0.8598732352256775, 'train/loss': 0.7385032773017883, 'validation/accuracy': 0.7359399795532227, 'validation/loss': 1.2580225467681885, 'validation/num_examples': 50000, 'test/accuracy': 0.6163000464439392, 'test/loss': 1.8852897882461548, 'test/num_examples': 10000, 'score': 43900.603063344955, 'total_duration': 45418.12388706207, 'accumulated_submission_time': 43900.603063344955, 'accumulated_eval_time': 1509.6416466236115, 'accumulated_logging_time': 3.495047092437744}
I0128 01:49:14.778851 139656817530624 logging_writer.py:48] [128553] accumulated_eval_time=1509.641647, accumulated_logging_time=3.495047, accumulated_submission_time=43900.603063, global_step=128553, preemption_count=0, score=43900.603063, test/accuracy=0.616300, test/loss=1.885290, test/num_examples=10000, total_duration=45418.123887, train/accuracy=0.859873, train/loss=0.738503, validation/accuracy=0.735940, validation/loss=1.258023, validation/num_examples=50000
I0128 01:49:31.151932 139656825923328 logging_writer.py:48] [128600] global_step=128600, grad_norm=3.319688320159912, loss=2.759599447250366
I0128 01:50:05.237895 139656817530624 logging_writer.py:48] [128700] global_step=128700, grad_norm=2.9650399684906006, loss=2.7132344245910645
I0128 01:50:39.335639 139656825923328 logging_writer.py:48] [128800] global_step=128800, grad_norm=3.0988106727600098, loss=2.7812159061431885
I0128 01:51:13.437042 139656817530624 logging_writer.py:48] [128900] global_step=128900, grad_norm=3.105832576751709, loss=2.6756508350372314
I0128 01:51:47.552518 139656825923328 logging_writer.py:48] [129000] global_step=129000, grad_norm=3.2424399852752686, loss=2.737408399581909
I0128 01:52:21.712294 139656817530624 logging_writer.py:48] [129100] global_step=129100, grad_norm=3.1777753829956055, loss=2.8229424953460693
I0128 01:52:55.825557 139656825923328 logging_writer.py:48] [129200] global_step=129200, grad_norm=2.942211151123047, loss=2.765890121459961
I0128 01:53:30.105975 139656817530624 logging_writer.py:48] [129300] global_step=129300, grad_norm=3.2856192588806152, loss=2.784048318862915
I0128 01:54:04.234339 139656825923328 logging_writer.py:48] [129400] global_step=129400, grad_norm=3.140599012374878, loss=2.827819347381592
I0128 01:54:38.370268 139656817530624 logging_writer.py:48] [129500] global_step=129500, grad_norm=3.1955995559692383, loss=2.7428598403930664
I0128 01:55:12.504114 139656825923328 logging_writer.py:48] [129600] global_step=129600, grad_norm=3.1110148429870605, loss=2.7988457679748535
I0128 01:55:46.629160 139656817530624 logging_writer.py:48] [129700] global_step=129700, grad_norm=3.1498990058898926, loss=2.7221195697784424
I0128 01:56:20.759320 139656825923328 logging_writer.py:48] [129800] global_step=129800, grad_norm=3.1007888317108154, loss=2.802175998687744
I0128 01:56:54.889776 139656817530624 logging_writer.py:48] [129900] global_step=129900, grad_norm=3.1792454719543457, loss=2.739588499069214
I0128 01:57:29.022100 139656825923328 logging_writer.py:48] [130000] global_step=130000, grad_norm=3.102755308151245, loss=2.803223133087158
I0128 01:57:44.863693 139822745589568 spec.py:321] Evaluating on the training split.
I0128 01:57:51.126518 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 01:57:59.838898 139822745589568 spec.py:349] Evaluating on the test split.
I0128 01:58:02.177144 139822745589568 submission_runner.py:408] Time since start: 45945.56s, 	Step: 130048, 	{'train/accuracy': 0.8598732352256775, 'train/loss': 0.7397308349609375, 'validation/accuracy': 0.7367599606513977, 'validation/loss': 1.2535072565078735, 'validation/num_examples': 50000, 'test/accuracy': 0.6109000444412231, 'test/loss': 1.88594388961792, 'test/num_examples': 10000, 'score': 44410.62669610977, 'total_duration': 45945.561729192734, 'accumulated_submission_time': 44410.62669610977, 'accumulated_eval_time': 1526.9550709724426, 'accumulated_logging_time': 3.543776512145996}
I0128 01:58:02.217130 139656792352512 logging_writer.py:48] [130048] accumulated_eval_time=1526.955071, accumulated_logging_time=3.543777, accumulated_submission_time=44410.626696, global_step=130048, preemption_count=0, score=44410.626696, test/accuracy=0.610900, test/loss=1.885944, test/num_examples=10000, total_duration=45945.561729, train/accuracy=0.859873, train/loss=0.739731, validation/accuracy=0.736760, validation/loss=1.253507, validation/num_examples=50000
I0128 01:58:20.293383 139656800745216 logging_writer.py:48] [130100] global_step=130100, grad_norm=3.2556376457214355, loss=2.7900304794311523
I0128 01:58:54.383186 139656792352512 logging_writer.py:48] [130200] global_step=130200, grad_norm=3.2570905685424805, loss=2.738645553588867
I0128 01:59:28.486222 139656800745216 logging_writer.py:48] [130300] global_step=130300, grad_norm=2.9765830039978027, loss=2.697744369506836
I0128 02:00:02.708283 139656792352512 logging_writer.py:48] [130400] global_step=130400, grad_norm=2.990068197250366, loss=2.755913257598877
I0128 02:00:36.831144 139656800745216 logging_writer.py:48] [130500] global_step=130500, grad_norm=3.3523247241973877, loss=2.750089168548584
I0128 02:01:10.975677 139656792352512 logging_writer.py:48] [130600] global_step=130600, grad_norm=3.172846555709839, loss=2.705827236175537
I0128 02:01:45.111729 139656800745216 logging_writer.py:48] [130700] global_step=130700, grad_norm=3.29116153717041, loss=2.7691071033477783
I0128 02:02:19.216018 139656792352512 logging_writer.py:48] [130800] global_step=130800, grad_norm=3.360157012939453, loss=2.8269901275634766
I0128 02:02:53.329453 139656800745216 logging_writer.py:48] [130900] global_step=130900, grad_norm=3.3236963748931885, loss=2.777686834335327
I0128 02:03:27.457994 139656792352512 logging_writer.py:48] [131000] global_step=131000, grad_norm=3.1509976387023926, loss=2.697190523147583
I0128 02:04:01.598480 139656800745216 logging_writer.py:48] [131100] global_step=131100, grad_norm=3.551622152328491, loss=2.7132368087768555
I0128 02:04:35.716346 139656792352512 logging_writer.py:48] [131200] global_step=131200, grad_norm=3.385563373565674, loss=2.751617431640625
I0128 02:05:09.816993 139656800745216 logging_writer.py:48] [131300] global_step=131300, grad_norm=3.1905200481414795, loss=2.753981828689575
I0128 02:05:44.021083 139656792352512 logging_writer.py:48] [131400] global_step=131400, grad_norm=3.265930652618408, loss=2.7469258308410645
I0128 02:06:18.119670 139656800745216 logging_writer.py:48] [131500] global_step=131500, grad_norm=3.184680223464966, loss=2.73301362991333
I0128 02:06:32.263540 139822745589568 spec.py:321] Evaluating on the training split.
I0128 02:06:38.445215 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 02:06:47.017886 139822745589568 spec.py:349] Evaluating on the test split.
I0128 02:06:49.402981 139822745589568 submission_runner.py:408] Time since start: 46472.79s, 	Step: 131543, 	{'train/accuracy': 0.8572624325752258, 'train/loss': 0.7386617660522461, 'validation/accuracy': 0.7386400103569031, 'validation/loss': 1.2487965822219849, 'validation/num_examples': 50000, 'test/accuracy': 0.613800048828125, 'test/loss': 1.8877145051956177, 'test/num_examples': 10000, 'score': 44920.61176490784, 'total_duration': 46472.78757691383, 'accumulated_submission_time': 44920.61176490784, 'accumulated_eval_time': 1544.0944755077362, 'accumulated_logging_time': 3.5926764011383057}
I0128 02:06:49.445562 139656825923328 logging_writer.py:48] [131543] accumulated_eval_time=1544.094476, accumulated_logging_time=3.592676, accumulated_submission_time=44920.611765, global_step=131543, preemption_count=0, score=44920.611765, test/accuracy=0.613800, test/loss=1.887715, test/num_examples=10000, total_duration=46472.787577, train/accuracy=0.857262, train/loss=0.738662, validation/accuracy=0.738640, validation/loss=1.248797, validation/num_examples=50000
I0128 02:07:09.225244 139656834316032 logging_writer.py:48] [131600] global_step=131600, grad_norm=3.192049741744995, loss=2.6929056644439697
I0128 02:07:43.329643 139656825923328 logging_writer.py:48] [131700] global_step=131700, grad_norm=3.3708274364471436, loss=2.802088737487793
I0128 02:08:17.429944 139656834316032 logging_writer.py:48] [131800] global_step=131800, grad_norm=3.347999095916748, loss=2.7493810653686523
I0128 02:08:51.556668 139656825923328 logging_writer.py:48] [131900] global_step=131900, grad_norm=3.105520248413086, loss=2.722755193710327
I0128 02:09:25.648700 139656834316032 logging_writer.py:48] [132000] global_step=132000, grad_norm=3.3504464626312256, loss=2.7501401901245117
I0128 02:09:59.762392 139656825923328 logging_writer.py:48] [132100] global_step=132100, grad_norm=3.6215877532958984, loss=2.7567801475524902
I0128 02:10:33.895621 139656834316032 logging_writer.py:48] [132200] global_step=132200, grad_norm=3.499150037765503, loss=2.6993138790130615
I0128 02:11:07.998383 139656825923328 logging_writer.py:48] [132300] global_step=132300, grad_norm=3.168565511703491, loss=2.6949174404144287
I0128 02:11:42.126343 139656834316032 logging_writer.py:48] [132400] global_step=132400, grad_norm=3.149975061416626, loss=2.7051753997802734
I0128 02:12:16.309862 139656825923328 logging_writer.py:48] [132500] global_step=132500, grad_norm=3.3929667472839355, loss=2.772639036178589
I0128 02:12:50.849426 139656834316032 logging_writer.py:48] [132600] global_step=132600, grad_norm=3.2813334465026855, loss=2.782747745513916
I0128 02:13:24.959264 139656825923328 logging_writer.py:48] [132700] global_step=132700, grad_norm=3.3492093086242676, loss=2.767273187637329
I0128 02:13:59.082043 139656834316032 logging_writer.py:48] [132800] global_step=132800, grad_norm=3.3593616485595703, loss=2.787811756134033
I0128 02:14:33.222054 139656825923328 logging_writer.py:48] [132900] global_step=132900, grad_norm=3.4648849964141846, loss=2.718027114868164
I0128 02:15:07.336402 139656834316032 logging_writer.py:48] [133000] global_step=133000, grad_norm=3.483858585357666, loss=2.825066328048706
I0128 02:15:19.419334 139822745589568 spec.py:321] Evaluating on the training split.
I0128 02:15:25.666726 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 02:15:34.088460 139822745589568 spec.py:349] Evaluating on the test split.
I0128 02:15:36.539770 139822745589568 submission_runner.py:408] Time since start: 46999.92s, 	Step: 133037, 	{'train/accuracy': 0.866609513759613, 'train/loss': 0.7195022106170654, 'validation/accuracy': 0.73881995677948, 'validation/loss': 1.249531865119934, 'validation/num_examples': 50000, 'test/accuracy': 0.613800048828125, 'test/loss': 1.8898125886917114, 'test/num_examples': 10000, 'score': 45430.52412056923, 'total_duration': 46999.9243516922, 'accumulated_submission_time': 45430.52412056923, 'accumulated_eval_time': 1561.2148640155792, 'accumulated_logging_time': 3.644584894180298}
I0128 02:15:36.590232 139656783959808 logging_writer.py:48] [133037] accumulated_eval_time=1561.214864, accumulated_logging_time=3.644585, accumulated_submission_time=45430.524121, global_step=133037, preemption_count=0, score=45430.524121, test/accuracy=0.613800, test/loss=1.889813, test/num_examples=10000, total_duration=46999.924352, train/accuracy=0.866610, train/loss=0.719502, validation/accuracy=0.738820, validation/loss=1.249532, validation/num_examples=50000
I0128 02:15:58.384043 139656800745216 logging_writer.py:48] [133100] global_step=133100, grad_norm=3.37849497795105, loss=2.7313575744628906
I0128 02:16:32.447454 139656783959808 logging_writer.py:48] [133200] global_step=133200, grad_norm=3.654536247253418, loss=2.774266242980957
I0128 02:17:06.539630 139656800745216 logging_writer.py:48] [133300] global_step=133300, grad_norm=3.365764856338501, loss=2.7618606090545654
I0128 02:17:40.667779 139656783959808 logging_writer.py:48] [133400] global_step=133400, grad_norm=3.5393431186676025, loss=2.7109756469726562
I0128 02:18:14.770182 139656800745216 logging_writer.py:48] [133500] global_step=133500, grad_norm=3.3440661430358887, loss=2.721644401550293
I0128 02:18:48.965011 139656783959808 logging_writer.py:48] [133600] global_step=133600, grad_norm=3.335538864135742, loss=2.712175130844116
I0128 02:19:23.089871 139656800745216 logging_writer.py:48] [133700] global_step=133700, grad_norm=3.3646745681762695, loss=2.736279010772705
I0128 02:19:57.227725 139656783959808 logging_writer.py:48] [133800] global_step=133800, grad_norm=3.11545467376709, loss=2.679844856262207
I0128 02:20:31.335287 139656800745216 logging_writer.py:48] [133900] global_step=133900, grad_norm=3.3043696880340576, loss=2.8006224632263184
I0128 02:21:05.466325 139656783959808 logging_writer.py:48] [134000] global_step=134000, grad_norm=3.280759811401367, loss=2.7522213459014893
I0128 02:21:39.597133 139656800745216 logging_writer.py:48] [134100] global_step=134100, grad_norm=3.657123565673828, loss=2.7664570808410645
I0128 02:22:13.715044 139656783959808 logging_writer.py:48] [134200] global_step=134200, grad_norm=3.369415521621704, loss=2.7024238109588623
I0128 02:22:47.833268 139656800745216 logging_writer.py:48] [134300] global_step=134300, grad_norm=3.3462841510772705, loss=2.733819007873535
I0128 02:23:21.960415 139656783959808 logging_writer.py:48] [134400] global_step=134400, grad_norm=3.1059417724609375, loss=2.7386951446533203
I0128 02:23:56.066929 139656800745216 logging_writer.py:48] [134500] global_step=134500, grad_norm=3.2594618797302246, loss=2.769752264022827
I0128 02:24:06.788886 139822745589568 spec.py:321] Evaluating on the training split.
I0128 02:24:12.941993 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 02:24:21.737397 139822745589568 spec.py:349] Evaluating on the test split.
I0128 02:24:24.047324 139822745589568 submission_runner.py:408] Time since start: 47527.43s, 	Step: 134533, 	{'train/accuracy': 0.8819156289100647, 'train/loss': 0.64863520860672, 'validation/accuracy': 0.7436599731445312, 'validation/loss': 1.2403019666671753, 'validation/num_examples': 50000, 'test/accuracy': 0.6176000237464905, 'test/loss': 1.8791606426239014, 'test/num_examples': 10000, 'score': 45940.66190671921, 'total_duration': 47527.43191242218, 'accumulated_submission_time': 45940.66190671921, 'accumulated_eval_time': 1578.4732689857483, 'accumulated_logging_time': 3.7044129371643066}
I0128 02:24:24.087588 139656825923328 logging_writer.py:48] [134533] accumulated_eval_time=1578.473269, accumulated_logging_time=3.704413, accumulated_submission_time=45940.661907, global_step=134533, preemption_count=0, score=45940.661907, test/accuracy=0.617600, test/loss=1.879161, test/num_examples=10000, total_duration=47527.431912, train/accuracy=0.881916, train/loss=0.648635, validation/accuracy=0.743660, validation/loss=1.240302, validation/num_examples=50000
I0128 02:24:47.365564 139656834316032 logging_writer.py:48] [134600] global_step=134600, grad_norm=3.1863176822662354, loss=2.724771499633789
I0128 02:25:21.442207 139656825923328 logging_writer.py:48] [134700] global_step=134700, grad_norm=3.353057622909546, loss=2.796409845352173
I0128 02:25:55.555510 139656834316032 logging_writer.py:48] [134800] global_step=134800, grad_norm=3.2827723026275635, loss=2.738416910171509
I0128 02:26:29.680262 139656825923328 logging_writer.py:48] [134900] global_step=134900, grad_norm=3.1469457149505615, loss=2.7127461433410645
I0128 02:27:03.816452 139656834316032 logging_writer.py:48] [135000] global_step=135000, grad_norm=3.7210936546325684, loss=2.71315336227417
I0128 02:27:37.952303 139656825923328 logging_writer.py:48] [135100] global_step=135100, grad_norm=3.459918260574341, loss=2.708904504776001
I0128 02:28:12.064663 139656834316032 logging_writer.py:48] [135200] global_step=135200, grad_norm=3.3801112174987793, loss=2.687093734741211
I0128 02:28:46.206491 139656825923328 logging_writer.py:48] [135300] global_step=135300, grad_norm=3.238961935043335, loss=2.7156877517700195
I0128 02:29:20.339267 139656834316032 logging_writer.py:48] [135400] global_step=135400, grad_norm=3.1953372955322266, loss=2.7088851928710938
I0128 02:29:54.416026 139656825923328 logging_writer.py:48] [135500] global_step=135500, grad_norm=3.1417641639709473, loss=2.653993606567383
I0128 02:30:28.532708 139656834316032 logging_writer.py:48] [135600] global_step=135600, grad_norm=3.379741907119751, loss=2.640085458755493
I0128 02:31:02.704077 139656825923328 logging_writer.py:48] [135700] global_step=135700, grad_norm=3.3762850761413574, loss=2.72658371925354
I0128 02:31:36.805821 139656834316032 logging_writer.py:48] [135800] global_step=135800, grad_norm=3.4177193641662598, loss=2.6960387229919434
I0128 02:32:10.891625 139656825923328 logging_writer.py:48] [135900] global_step=135900, grad_norm=3.4101943969726562, loss=2.726578712463379
I0128 02:32:45.009907 139656834316032 logging_writer.py:48] [136000] global_step=136000, grad_norm=3.091801881790161, loss=2.6325955390930176
I0128 02:32:54.369069 139822745589568 spec.py:321] Evaluating on the training split.
I0128 02:33:00.631219 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 02:33:09.175988 139822745589568 spec.py:349] Evaluating on the test split.
I0128 02:33:11.600091 139822745589568 submission_runner.py:408] Time since start: 48054.98s, 	Step: 136029, 	{'train/accuracy': 0.8819355964660645, 'train/loss': 0.6802816987037659, 'validation/accuracy': 0.7430999875068665, 'validation/loss': 1.2458999156951904, 'validation/num_examples': 50000, 'test/accuracy': 0.6157000064849854, 'test/loss': 1.8828948736190796, 'test/num_examples': 10000, 'score': 46450.88255214691, 'total_duration': 48054.984679460526, 'accumulated_submission_time': 46450.88255214691, 'accumulated_eval_time': 1595.7042515277863, 'accumulated_logging_time': 3.7541310787200928}
I0128 02:33:11.646224 139656783959808 logging_writer.py:48] [136029] accumulated_eval_time=1595.704252, accumulated_logging_time=3.754131, accumulated_submission_time=46450.882552, global_step=136029, preemption_count=0, score=46450.882552, test/accuracy=0.615700, test/loss=1.882895, test/num_examples=10000, total_duration=48054.984679, train/accuracy=0.881936, train/loss=0.680282, validation/accuracy=0.743100, validation/loss=1.245900, validation/num_examples=50000
I0128 02:33:36.179440 139656792352512 logging_writer.py:48] [136100] global_step=136100, grad_norm=3.1707217693328857, loss=2.7267677783966064
I0128 02:34:10.241061 139656783959808 logging_writer.py:48] [136200] global_step=136200, grad_norm=3.202425956726074, loss=2.660682201385498
I0128 02:34:44.329632 139656792352512 logging_writer.py:48] [136300] global_step=136300, grad_norm=3.345409870147705, loss=2.6893160343170166
I0128 02:35:18.482956 139656783959808 logging_writer.py:48] [136400] global_step=136400, grad_norm=3.5081660747528076, loss=2.710233688354492
I0128 02:35:52.599736 139656792352512 logging_writer.py:48] [136500] global_step=136500, grad_norm=3.5430119037628174, loss=2.6861932277679443
I0128 02:36:26.697047 139656783959808 logging_writer.py:48] [136600] global_step=136600, grad_norm=3.232792854309082, loss=2.7539138793945312
I0128 02:37:00.836448 139656792352512 logging_writer.py:48] [136700] global_step=136700, grad_norm=3.368227243423462, loss=2.708347797393799
I0128 02:37:35.082879 139656783959808 logging_writer.py:48] [136800] global_step=136800, grad_norm=3.16477108001709, loss=2.696152448654175
I0128 02:38:09.206134 139656792352512 logging_writer.py:48] [136900] global_step=136900, grad_norm=3.5345685482025146, loss=2.7275047302246094
I0128 02:38:43.316578 139656783959808 logging_writer.py:48] [137000] global_step=137000, grad_norm=3.152503490447998, loss=2.6746129989624023
I0128 02:39:17.412201 139656792352512 logging_writer.py:48] [137100] global_step=137100, grad_norm=3.46309494972229, loss=2.7391715049743652
I0128 02:39:51.532776 139656783959808 logging_writer.py:48] [137200] global_step=137200, grad_norm=3.2257184982299805, loss=2.6713669300079346
I0128 02:40:25.634255 139656792352512 logging_writer.py:48] [137300] global_step=137300, grad_norm=3.4861462116241455, loss=2.734562635421753
I0128 02:40:59.757570 139656783959808 logging_writer.py:48] [137400] global_step=137400, grad_norm=3.5216522216796875, loss=2.710960865020752
I0128 02:41:33.878938 139656792352512 logging_writer.py:48] [137500] global_step=137500, grad_norm=3.4267735481262207, loss=2.671194553375244
I0128 02:41:41.878686 139822745589568 spec.py:321] Evaluating on the training split.
I0128 02:41:48.133494 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 02:41:56.624279 139822745589568 spec.py:349] Evaluating on the test split.
I0128 02:41:59.054131 139822745589568 submission_runner.py:408] Time since start: 48582.44s, 	Step: 137525, 	{'train/accuracy': 0.8787667155265808, 'train/loss': 0.682250440120697, 'validation/accuracy': 0.7416799664497375, 'validation/loss': 1.249807357788086, 'validation/num_examples': 50000, 'test/accuracy': 0.6220000386238098, 'test/loss': 1.8697481155395508, 'test/num_examples': 10000, 'score': 46961.05339837074, 'total_duration': 48582.43872284889, 'accumulated_submission_time': 46961.05339837074, 'accumulated_eval_time': 1612.8796932697296, 'accumulated_logging_time': 3.8113210201263428}
I0128 02:41:59.096924 139656817530624 logging_writer.py:48] [137525] accumulated_eval_time=1612.879693, accumulated_logging_time=3.811321, accumulated_submission_time=46961.053398, global_step=137525, preemption_count=0, score=46961.053398, test/accuracy=0.622000, test/loss=1.869748, test/num_examples=10000, total_duration=48582.438723, train/accuracy=0.878767, train/loss=0.682250, validation/accuracy=0.741680, validation/loss=1.249807, validation/num_examples=50000
I0128 02:42:25.011687 139656825923328 logging_writer.py:48] [137600] global_step=137600, grad_norm=3.343552827835083, loss=2.6954751014709473
I0128 02:42:59.100169 139656817530624 logging_writer.py:48] [137700] global_step=137700, grad_norm=3.315988302230835, loss=2.799086809158325
I0128 02:43:33.177974 139656825923328 logging_writer.py:48] [137800] global_step=137800, grad_norm=3.5579099655151367, loss=2.8360495567321777
I0128 02:44:07.393972 139656817530624 logging_writer.py:48] [137900] global_step=137900, grad_norm=3.670219898223877, loss=2.787047863006592
I0128 02:44:41.501267 139656825923328 logging_writer.py:48] [138000] global_step=138000, grad_norm=3.4746854305267334, loss=2.6986021995544434
I0128 02:45:15.670745 139656817530624 logging_writer.py:48] [138100] global_step=138100, grad_norm=3.436444044113159, loss=2.697406768798828
I0128 02:45:49.800572 139656825923328 logging_writer.py:48] [138200] global_step=138200, grad_norm=3.2293801307678223, loss=2.689657211303711
I0128 02:46:23.891809 139656817530624 logging_writer.py:48] [138300] global_step=138300, grad_norm=3.5173919200897217, loss=2.7478482723236084
I0128 02:46:58.014348 139656825923328 logging_writer.py:48] [138400] global_step=138400, grad_norm=3.7586076259613037, loss=2.79746675491333
I0128 02:47:32.130770 139656817530624 logging_writer.py:48] [138500] global_step=138500, grad_norm=3.388097047805786, loss=2.7619569301605225
I0128 02:48:06.237115 139656825923328 logging_writer.py:48] [138600] global_step=138600, grad_norm=3.136669158935547, loss=2.693302869796753
I0128 02:48:40.382617 139656817530624 logging_writer.py:48] [138700] global_step=138700, grad_norm=3.344719409942627, loss=2.6905221939086914
I0128 02:49:14.508891 139656825923328 logging_writer.py:48] [138800] global_step=138800, grad_norm=3.451941728591919, loss=2.7026257514953613
I0128 02:49:48.632450 139656817530624 logging_writer.py:48] [138900] global_step=138900, grad_norm=3.414247751235962, loss=2.7505922317504883
I0128 02:50:22.849354 139656825923328 logging_writer.py:48] [139000] global_step=139000, grad_norm=3.2717766761779785, loss=2.6374282836914062
I0128 02:50:29.129137 139822745589568 spec.py:321] Evaluating on the training split.
I0128 02:50:35.335186 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 02:50:43.813642 139822745589568 spec.py:349] Evaluating on the test split.
I0128 02:50:46.272310 139822745589568 submission_runner.py:408] Time since start: 49109.66s, 	Step: 139020, 	{'train/accuracy': 0.8798628449440002, 'train/loss': 0.6878957748413086, 'validation/accuracy': 0.7449600100517273, 'validation/loss': 1.2519656419754028, 'validation/num_examples': 50000, 'test/accuracy': 0.6232000589370728, 'test/loss': 1.8762387037277222, 'test/num_examples': 10000, 'score': 47471.025155067444, 'total_duration': 49109.65690302849, 'accumulated_submission_time': 47471.025155067444, 'accumulated_eval_time': 1630.022828578949, 'accumulated_logging_time': 3.8632400035858154}
I0128 02:50:46.313197 139656800745216 logging_writer.py:48] [139020] accumulated_eval_time=1630.022829, accumulated_logging_time=3.863240, accumulated_submission_time=47471.025155, global_step=139020, preemption_count=0, score=47471.025155, test/accuracy=0.623200, test/loss=1.876239, test/num_examples=10000, total_duration=49109.656903, train/accuracy=0.879863, train/loss=0.687896, validation/accuracy=0.744960, validation/loss=1.251966, validation/num_examples=50000
I0128 02:51:13.915756 139656809137920 logging_writer.py:48] [139100] global_step=139100, grad_norm=3.6490097045898438, loss=2.768205404281616
I0128 02:51:47.996819 139656800745216 logging_writer.py:48] [139200] global_step=139200, grad_norm=3.3281280994415283, loss=2.6697895526885986
I0128 02:52:22.103127 139656809137920 logging_writer.py:48] [139300] global_step=139300, grad_norm=3.58599853515625, loss=2.7609283924102783
I0128 02:52:56.222517 139656800745216 logging_writer.py:48] [139400] global_step=139400, grad_norm=3.683985710144043, loss=2.766174793243408
I0128 02:53:30.332365 139656809137920 logging_writer.py:48] [139500] global_step=139500, grad_norm=3.41041898727417, loss=2.686790704727173
I0128 02:54:04.455685 139656800745216 logging_writer.py:48] [139600] global_step=139600, grad_norm=3.1115102767944336, loss=2.6560773849487305
I0128 02:54:38.600517 139656809137920 logging_writer.py:48] [139700] global_step=139700, grad_norm=3.356213331222534, loss=2.7240333557128906
I0128 02:55:12.703043 139656800745216 logging_writer.py:48] [139800] global_step=139800, grad_norm=3.4478375911712646, loss=2.7018861770629883
I0128 02:55:46.816607 139656809137920 logging_writer.py:48] [139900] global_step=139900, grad_norm=3.6884865760803223, loss=2.6780757904052734
I0128 02:56:21.005991 139656800745216 logging_writer.py:48] [140000] global_step=140000, grad_norm=3.648301362991333, loss=2.6923444271087646
I0128 02:56:55.131079 139656809137920 logging_writer.py:48] [140100] global_step=140100, grad_norm=3.344627618789673, loss=2.6531569957733154
I0128 02:57:29.253321 139656800745216 logging_writer.py:48] [140200] global_step=140200, grad_norm=3.3272674083709717, loss=2.6829748153686523
I0128 02:58:03.388436 139656809137920 logging_writer.py:48] [140300] global_step=140300, grad_norm=3.423513174057007, loss=2.671726942062378
I0128 02:58:37.502866 139656800745216 logging_writer.py:48] [140400] global_step=140400, grad_norm=3.475923538208008, loss=2.6643383502960205
I0128 02:59:11.632205 139656809137920 logging_writer.py:48] [140500] global_step=140500, grad_norm=3.404076337814331, loss=2.7368123531341553
I0128 02:59:16.542920 139822745589568 spec.py:321] Evaluating on the training split.
I0128 02:59:22.721120 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 02:59:31.409486 139822745589568 spec.py:349] Evaluating on the test split.
I0128 02:59:33.818711 139822745589568 submission_runner.py:408] Time since start: 49637.20s, 	Step: 140516, 	{'train/accuracy': 0.8801219463348389, 'train/loss': 0.6773303151130676, 'validation/accuracy': 0.742859959602356, 'validation/loss': 1.2454878091812134, 'validation/num_examples': 50000, 'test/accuracy': 0.6203000545501709, 'test/loss': 1.8631223440170288, 'test/num_examples': 10000, 'score': 47981.19206619263, 'total_duration': 49637.203291893005, 'accumulated_submission_time': 47981.19206619263, 'accumulated_eval_time': 1647.2985713481903, 'accumulated_logging_time': 3.9131062030792236}
I0128 02:59:33.864751 139656783959808 logging_writer.py:48] [140516] accumulated_eval_time=1647.298571, accumulated_logging_time=3.913106, accumulated_submission_time=47981.192066, global_step=140516, preemption_count=0, score=47981.192066, test/accuracy=0.620300, test/loss=1.863122, test/num_examples=10000, total_duration=49637.203292, train/accuracy=0.880122, train/loss=0.677330, validation/accuracy=0.742860, validation/loss=1.245488, validation/num_examples=50000
I0128 03:00:02.815968 139656792352512 logging_writer.py:48] [140600] global_step=140600, grad_norm=3.70512056350708, loss=2.719705104827881
I0128 03:00:36.893824 139656783959808 logging_writer.py:48] [140700] global_step=140700, grad_norm=3.4520139694213867, loss=2.6756269931793213
I0128 03:01:11.009444 139656792352512 logging_writer.py:48] [140800] global_step=140800, grad_norm=3.9580485820770264, loss=2.6904685497283936
I0128 03:01:45.109373 139656783959808 logging_writer.py:48] [140900] global_step=140900, grad_norm=3.360701084136963, loss=2.695500135421753
I0128 03:02:19.235796 139656792352512 logging_writer.py:48] [141000] global_step=141000, grad_norm=3.424647092819214, loss=2.658895492553711
I0128 03:02:53.430909 139656783959808 logging_writer.py:48] [141100] global_step=141100, grad_norm=3.5410492420196533, loss=2.7109951972961426
I0128 03:03:27.555558 139656792352512 logging_writer.py:48] [141200] global_step=141200, grad_norm=3.373504161834717, loss=2.668055534362793
I0128 03:04:01.690324 139656783959808 logging_writer.py:48] [141300] global_step=141300, grad_norm=3.4174046516418457, loss=2.7332100868225098
I0128 03:04:35.843421 139656792352512 logging_writer.py:48] [141400] global_step=141400, grad_norm=3.518388271331787, loss=2.6451056003570557
I0128 03:05:09.976171 139656783959808 logging_writer.py:48] [141500] global_step=141500, grad_norm=3.9006335735321045, loss=2.694941997528076
I0128 03:05:44.109447 139656792352512 logging_writer.py:48] [141600] global_step=141600, grad_norm=3.5130839347839355, loss=2.716338872909546
I0128 03:06:18.249558 139656783959808 logging_writer.py:48] [141700] global_step=141700, grad_norm=3.4042112827301025, loss=2.6526124477386475
I0128 03:06:52.386976 139656792352512 logging_writer.py:48] [141800] global_step=141800, grad_norm=3.480030059814453, loss=2.6810221672058105
I0128 03:07:26.509366 139656783959808 logging_writer.py:48] [141900] global_step=141900, grad_norm=3.5459799766540527, loss=2.671811580657959
I0128 03:08:00.645828 139656792352512 logging_writer.py:48] [142000] global_step=142000, grad_norm=3.259911060333252, loss=2.6751208305358887
I0128 03:08:03.868427 139822745589568 spec.py:321] Evaluating on the training split.
I0128 03:08:10.073587 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 03:08:18.611059 139822745589568 spec.py:349] Evaluating on the test split.
I0128 03:08:21.054241 139822745589568 submission_runner.py:408] Time since start: 50164.44s, 	Step: 142011, 	{'train/accuracy': 0.8788862824440002, 'train/loss': 0.6838304996490479, 'validation/accuracy': 0.7432599663734436, 'validation/loss': 1.2508834600448608, 'validation/num_examples': 50000, 'test/accuracy': 0.6175000071525574, 'test/loss': 1.8836888074874878, 'test/num_examples': 10000, 'score': 48491.136139154434, 'total_duration': 50164.43881726265, 'accumulated_submission_time': 48491.136139154434, 'accumulated_eval_time': 1664.4843318462372, 'accumulated_logging_time': 3.967681646347046}
I0128 03:08:21.098549 139656834316032 logging_writer.py:48] [142011] accumulated_eval_time=1664.484332, accumulated_logging_time=3.967682, accumulated_submission_time=48491.136139, global_step=142011, preemption_count=0, score=48491.136139, test/accuracy=0.617500, test/loss=1.883689, test/num_examples=10000, total_duration=50164.438817, train/accuracy=0.878886, train/loss=0.683830, validation/accuracy=0.743260, validation/loss=1.250883, validation/num_examples=50000
I0128 03:08:51.814808 139658730145536 logging_writer.py:48] [142100] global_step=142100, grad_norm=3.4942471981048584, loss=2.7433342933654785
I0128 03:09:25.930668 139656834316032 logging_writer.py:48] [142200] global_step=142200, grad_norm=3.7330589294433594, loss=2.7261738777160645
I0128 03:10:00.003440 139658730145536 logging_writer.py:48] [142300] global_step=142300, grad_norm=3.576425075531006, loss=2.737708568572998
I0128 03:10:34.110910 139656834316032 logging_writer.py:48] [142400] global_step=142400, grad_norm=3.651977777481079, loss=2.70401668548584
I0128 03:11:08.211045 139658730145536 logging_writer.py:48] [142500] global_step=142500, grad_norm=3.5157101154327393, loss=2.6887662410736084
I0128 03:11:42.330770 139656834316032 logging_writer.py:48] [142600] global_step=142600, grad_norm=3.549365758895874, loss=2.694243907928467
I0128 03:12:16.432765 139658730145536 logging_writer.py:48] [142700] global_step=142700, grad_norm=3.4646847248077393, loss=2.703765869140625
I0128 03:12:50.523803 139656834316032 logging_writer.py:48] [142800] global_step=142800, grad_norm=3.483690023422241, loss=2.6436517238616943
I0128 03:13:24.629156 139658730145536 logging_writer.py:48] [142900] global_step=142900, grad_norm=3.3885016441345215, loss=2.630035161972046
I0128 03:13:58.741376 139656834316032 logging_writer.py:48] [143000] global_step=143000, grad_norm=3.7835090160369873, loss=2.7316434383392334
I0128 03:14:32.872425 139658730145536 logging_writer.py:48] [143100] global_step=143100, grad_norm=3.564786434173584, loss=2.684229612350464
I0128 03:15:07.011089 139656834316032 logging_writer.py:48] [143200] global_step=143200, grad_norm=3.6074435710906982, loss=2.7208139896392822
I0128 03:15:41.148527 139658730145536 logging_writer.py:48] [143300] global_step=143300, grad_norm=3.5959115028381348, loss=2.704040288925171
I0128 03:16:15.284859 139656834316032 logging_writer.py:48] [143400] global_step=143400, grad_norm=3.501688003540039, loss=2.6170263290405273
I0128 03:16:49.428289 139658730145536 logging_writer.py:48] [143500] global_step=143500, grad_norm=3.5065152645111084, loss=2.626225471496582
I0128 03:16:51.272605 139822745589568 spec.py:321] Evaluating on the training split.
I0128 03:16:57.493581 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 03:17:06.053018 139822745589568 spec.py:349] Evaluating on the test split.
I0128 03:17:08.469832 139822745589568 submission_runner.py:408] Time since start: 50691.85s, 	Step: 143507, 	{'train/accuracy': 0.9057118892669678, 'train/loss': 0.5698558688163757, 'validation/accuracy': 0.7466599941253662, 'validation/loss': 1.2103779315948486, 'validation/num_examples': 50000, 'test/accuracy': 0.6228000521659851, 'test/loss': 1.8483272790908813, 'test/num_examples': 10000, 'score': 49001.24908995628, 'total_duration': 50691.854425907135, 'accumulated_submission_time': 49001.24908995628, 'accumulated_eval_time': 1681.6815202236176, 'accumulated_logging_time': 4.021223306655884}
I0128 03:17:08.514643 139656800745216 logging_writer.py:48] [143507] accumulated_eval_time=1681.681520, accumulated_logging_time=4.021223, accumulated_submission_time=49001.249090, global_step=143507, preemption_count=0, score=49001.249090, test/accuracy=0.622800, test/loss=1.848327, test/num_examples=10000, total_duration=50691.854426, train/accuracy=0.905712, train/loss=0.569856, validation/accuracy=0.746660, validation/loss=1.210378, validation/num_examples=50000
I0128 03:17:40.533254 139656809137920 logging_writer.py:48] [143600] global_step=143600, grad_norm=3.584200143814087, loss=2.7173681259155273
I0128 03:18:14.618297 139656800745216 logging_writer.py:48] [143700] global_step=143700, grad_norm=3.8418314456939697, loss=2.7148337364196777
I0128 03:18:48.758521 139656809137920 logging_writer.py:48] [143800] global_step=143800, grad_norm=3.4390547275543213, loss=2.653799057006836
I0128 03:19:22.893999 139656800745216 logging_writer.py:48] [143900] global_step=143900, grad_norm=3.6759932041168213, loss=2.6902809143066406
I0128 03:19:57.039424 139656809137920 logging_writer.py:48] [144000] global_step=144000, grad_norm=3.530388832092285, loss=2.674750566482544
I0128 03:20:31.183159 139656800745216 logging_writer.py:48] [144100] global_step=144100, grad_norm=3.526272773742676, loss=2.6281557083129883
I0128 03:21:05.317323 139656809137920 logging_writer.py:48] [144200] global_step=144200, grad_norm=3.6709184646606445, loss=2.681030035018921
I0128 03:21:39.497487 139656800745216 logging_writer.py:48] [144300] global_step=144300, grad_norm=3.8154451847076416, loss=2.619170904159546
I0128 03:22:13.610111 139656809137920 logging_writer.py:48] [144400] global_step=144400, grad_norm=3.4095964431762695, loss=2.610156774520874
I0128 03:22:47.764995 139656800745216 logging_writer.py:48] [144500] global_step=144500, grad_norm=3.4976963996887207, loss=2.6571767330169678
I0128 03:23:21.863280 139656809137920 logging_writer.py:48] [144600] global_step=144600, grad_norm=3.593501567840576, loss=2.6642661094665527
I0128 03:23:56.003242 139656800745216 logging_writer.py:48] [144700] global_step=144700, grad_norm=3.9765865802764893, loss=2.7496554851531982
I0128 03:24:30.136239 139656809137920 logging_writer.py:48] [144800] global_step=144800, grad_norm=3.5217947959899902, loss=2.688563346862793
I0128 03:25:04.281417 139656800745216 logging_writer.py:48] [144900] global_step=144900, grad_norm=3.518723964691162, loss=2.6380252838134766
I0128 03:25:38.406314 139656809137920 logging_writer.py:48] [145000] global_step=145000, grad_norm=3.4058687686920166, loss=2.6420791149139404
I0128 03:25:38.563893 139822745589568 spec.py:321] Evaluating on the training split.
I0128 03:25:44.865372 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 03:25:53.559706 139822745589568 spec.py:349] Evaluating on the test split.
I0128 03:25:55.987369 139822745589568 submission_runner.py:408] Time since start: 51219.37s, 	Step: 145002, 	{'train/accuracy': 0.8981983065605164, 'train/loss': 0.6036162972450256, 'validation/accuracy': 0.7472599744796753, 'validation/loss': 1.228808045387268, 'validation/num_examples': 50000, 'test/accuracy': 0.6213000416755676, 'test/loss': 1.8598668575286865, 'test/num_examples': 10000, 'score': 49511.237023591995, 'total_duration': 51219.37195444107, 'accumulated_submission_time': 49511.237023591995, 'accumulated_eval_time': 1699.1049826145172, 'accumulated_logging_time': 4.07612681388855}
I0128 03:25:56.031703 139656792352512 logging_writer.py:48] [145002] accumulated_eval_time=1699.104983, accumulated_logging_time=4.076127, accumulated_submission_time=49511.237024, global_step=145002, preemption_count=0, score=49511.237024, test/accuracy=0.621300, test/loss=1.859867, test/num_examples=10000, total_duration=51219.371954, train/accuracy=0.898198, train/loss=0.603616, validation/accuracy=0.747260, validation/loss=1.228808, validation/num_examples=50000
I0128 03:26:29.782757 139656825923328 logging_writer.py:48] [145100] global_step=145100, grad_norm=3.636214256286621, loss=2.6125943660736084
I0128 03:27:03.862721 139656792352512 logging_writer.py:48] [145200] global_step=145200, grad_norm=3.4746127128601074, loss=2.6738224029541016
I0128 03:27:38.020370 139656825923328 logging_writer.py:48] [145300] global_step=145300, grad_norm=3.342895746231079, loss=2.618957757949829
I0128 03:28:12.117233 139656792352512 logging_writer.py:48] [145400] global_step=145400, grad_norm=3.485257863998413, loss=2.59810733795166
I0128 03:28:46.233331 139656825923328 logging_writer.py:48] [145500] global_step=145500, grad_norm=3.7484326362609863, loss=2.6535284519195557
I0128 03:29:20.341961 139656792352512 logging_writer.py:48] [145600] global_step=145600, grad_norm=3.616128921508789, loss=2.637255907058716
I0128 03:29:54.440777 139656825923328 logging_writer.py:48] [145700] global_step=145700, grad_norm=3.618380546569824, loss=2.63928484916687
I0128 03:30:28.558669 139656792352512 logging_writer.py:48] [145800] global_step=145800, grad_norm=3.8216254711151123, loss=2.6896703243255615
I0128 03:31:02.713055 139656825923328 logging_writer.py:48] [145900] global_step=145900, grad_norm=3.4440760612487793, loss=2.720327377319336
I0128 03:31:36.836257 139656792352512 logging_writer.py:48] [146000] global_step=146000, grad_norm=3.7578582763671875, loss=2.6622071266174316
I0128 03:32:10.979160 139656825923328 logging_writer.py:48] [146100] global_step=146100, grad_norm=3.4211814403533936, loss=2.7184395790100098
I0128 03:32:45.107152 139656792352512 logging_writer.py:48] [146200] global_step=146200, grad_norm=3.762017011642456, loss=2.6932051181793213
I0128 03:33:19.241546 139656825923328 logging_writer.py:48] [146300] global_step=146300, grad_norm=3.5199806690216064, loss=2.6620500087738037
I0128 03:33:53.437352 139656792352512 logging_writer.py:48] [146400] global_step=146400, grad_norm=3.9288763999938965, loss=2.692765235900879
I0128 03:34:26.327896 139822745589568 spec.py:321] Evaluating on the training split.
I0128 03:34:32.524544 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 03:34:41.065979 139822745589568 spec.py:349] Evaluating on the test split.
I0128 03:34:43.475452 139822745589568 submission_runner.py:408] Time since start: 51746.86s, 	Step: 146498, 	{'train/accuracy': 0.8981983065605164, 'train/loss': 0.5911901593208313, 'validation/accuracy': 0.7471599578857422, 'validation/loss': 1.2115702629089355, 'validation/num_examples': 50000, 'test/accuracy': 0.6213000416755676, 'test/loss': 1.8459560871124268, 'test/num_examples': 10000, 'score': 50021.47194981575, 'total_duration': 51746.86004567146, 'accumulated_submission_time': 50021.47194981575, 'accumulated_eval_time': 1716.2525057792664, 'accumulated_logging_time': 4.129936933517456}
I0128 03:34:43.521817 139656783959808 logging_writer.py:48] [146498] accumulated_eval_time=1716.252506, accumulated_logging_time=4.129937, accumulated_submission_time=50021.471950, global_step=146498, preemption_count=0, score=50021.471950, test/accuracy=0.621300, test/loss=1.845956, test/num_examples=10000, total_duration=51746.860046, train/accuracy=0.898198, train/loss=0.591190, validation/accuracy=0.747160, validation/loss=1.211570, validation/num_examples=50000
I0128 03:34:44.554597 139656792352512 logging_writer.py:48] [146500] global_step=146500, grad_norm=3.6709039211273193, loss=2.661224126815796
I0128 03:35:18.611982 139656783959808 logging_writer.py:48] [146600] global_step=146600, grad_norm=3.7409708499908447, loss=2.68042254447937
I0128 03:35:52.716832 139656792352512 logging_writer.py:48] [146700] global_step=146700, grad_norm=3.5708367824554443, loss=2.670686721801758
I0128 03:36:26.830954 139656783959808 logging_writer.py:48] [146800] global_step=146800, grad_norm=3.85986065864563, loss=2.6368212699890137
I0128 03:37:00.973526 139656792352512 logging_writer.py:48] [146900] global_step=146900, grad_norm=3.7677886486053467, loss=2.6151678562164307
I0128 03:37:35.104597 139656783959808 logging_writer.py:48] [147000] global_step=147000, grad_norm=3.9531521797180176, loss=2.6548237800598145
I0128 03:38:09.252472 139656792352512 logging_writer.py:48] [147100] global_step=147100, grad_norm=3.754004955291748, loss=2.7085275650024414
I0128 03:38:43.377420 139656783959808 logging_writer.py:48] [147200] global_step=147200, grad_norm=3.676490545272827, loss=2.6495351791381836
I0128 03:39:17.521609 139656792352512 logging_writer.py:48] [147300] global_step=147300, grad_norm=3.771425724029541, loss=2.680748462677002
I0128 03:39:51.647536 139656783959808 logging_writer.py:48] [147400] global_step=147400, grad_norm=3.5136096477508545, loss=2.622493267059326
I0128 03:40:25.870975 139656792352512 logging_writer.py:48] [147500] global_step=147500, grad_norm=3.454897403717041, loss=2.574411392211914
I0128 03:40:59.963491 139656783959808 logging_writer.py:48] [147600] global_step=147600, grad_norm=3.52071475982666, loss=2.673065662384033
I0128 03:41:34.102686 139656792352512 logging_writer.py:48] [147700] global_step=147700, grad_norm=3.723764657974243, loss=2.6764373779296875
I0128 03:42:08.245460 139656783959808 logging_writer.py:48] [147800] global_step=147800, grad_norm=3.549666404724121, loss=2.6128783226013184
I0128 03:42:42.411280 139656792352512 logging_writer.py:48] [147900] global_step=147900, grad_norm=3.7190115451812744, loss=2.6738853454589844
I0128 03:43:13.596456 139822745589568 spec.py:321] Evaluating on the training split.
I0128 03:43:19.821178 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 03:43:28.398832 139822745589568 spec.py:349] Evaluating on the test split.
I0128 03:43:30.969498 139822745589568 submission_runner.py:408] Time since start: 52274.35s, 	Step: 147993, 	{'train/accuracy': 0.8964046239852905, 'train/loss': 0.6049222350120544, 'validation/accuracy': 0.7515599727630615, 'validation/loss': 1.208533525466919, 'validation/num_examples': 50000, 'test/accuracy': 0.6234000325202942, 'test/loss': 1.849199652671814, 'test/num_examples': 10000, 'score': 50531.48566865921, 'total_duration': 52274.354078531265, 'accumulated_submission_time': 50531.48566865921, 'accumulated_eval_time': 1733.6255042552948, 'accumulated_logging_time': 4.185802459716797}
I0128 03:43:31.017579 139656825923328 logging_writer.py:48] [147993] accumulated_eval_time=1733.625504, accumulated_logging_time=4.185802, accumulated_submission_time=50531.485669, global_step=147993, preemption_count=0, score=50531.485669, test/accuracy=0.623400, test/loss=1.849200, test/num_examples=10000, total_duration=52274.354079, train/accuracy=0.896405, train/loss=0.604922, validation/accuracy=0.751560, validation/loss=1.208534, validation/num_examples=50000
I0128 03:43:33.750796 139656834316032 logging_writer.py:48] [148000] global_step=148000, grad_norm=3.827714681625366, loss=2.6593477725982666
I0128 03:44:07.821574 139656825923328 logging_writer.py:48] [148100] global_step=148100, grad_norm=3.6111819744110107, loss=2.6756985187530518
I0128 03:44:41.862049 139656834316032 logging_writer.py:48] [148200] global_step=148200, grad_norm=3.478208541870117, loss=2.61388897895813
I0128 03:45:15.975221 139656825923328 logging_writer.py:48] [148300] global_step=148300, grad_norm=3.5399093627929688, loss=2.618424415588379
I0128 03:45:50.054694 139656834316032 logging_writer.py:48] [148400] global_step=148400, grad_norm=3.4508774280548096, loss=2.6305856704711914
I0128 03:46:24.177908 139656825923328 logging_writer.py:48] [148500] global_step=148500, grad_norm=3.5550334453582764, loss=2.636812448501587
I0128 03:46:58.333259 139656834316032 logging_writer.py:48] [148600] global_step=148600, grad_norm=3.5847115516662598, loss=2.6684651374816895
I0128 03:47:32.445061 139656825923328 logging_writer.py:48] [148700] global_step=148700, grad_norm=3.5356223583221436, loss=2.6775879859924316
I0128 03:48:06.558240 139656834316032 logging_writer.py:48] [148800] global_step=148800, grad_norm=3.683741331100464, loss=2.6135575771331787
I0128 03:48:40.692356 139656825923328 logging_writer.py:48] [148900] global_step=148900, grad_norm=3.7123024463653564, loss=2.6776087284088135
I0128 03:49:14.806340 139656834316032 logging_writer.py:48] [149000] global_step=149000, grad_norm=3.6337594985961914, loss=2.6334192752838135
I0128 03:49:48.910318 139656825923328 logging_writer.py:48] [149100] global_step=149100, grad_norm=3.7208666801452637, loss=2.6522152423858643
I0128 03:50:23.005144 139656834316032 logging_writer.py:48] [149200] global_step=149200, grad_norm=3.8038675785064697, loss=2.6745784282684326
I0128 03:50:57.105840 139656825923328 logging_writer.py:48] [149300] global_step=149300, grad_norm=3.759363889694214, loss=2.6351065635681152
I0128 03:51:31.206066 139656834316032 logging_writer.py:48] [149400] global_step=149400, grad_norm=3.575025796890259, loss=2.581301212310791
I0128 03:52:01.023143 139822745589568 spec.py:321] Evaluating on the training split.
I0128 03:52:07.432786 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 03:52:16.057952 139822745589568 spec.py:349] Evaluating on the test split.
I0128 03:52:18.501240 139822745589568 submission_runner.py:408] Time since start: 52801.89s, 	Step: 149489, 	{'train/accuracy': 0.899832546710968, 'train/loss': 0.6207277774810791, 'validation/accuracy': 0.7516599893569946, 'validation/loss': 1.2279235124588013, 'validation/num_examples': 50000, 'test/accuracy': 0.6240000128746033, 'test/loss': 1.8540517091751099, 'test/num_examples': 10000, 'score': 51041.4313583374, 'total_duration': 52801.885820388794, 'accumulated_submission_time': 51041.4313583374, 'accumulated_eval_time': 1751.1035561561584, 'accumulated_logging_time': 4.243193626403809}
I0128 03:52:18.547086 139656783959808 logging_writer.py:48] [149489] accumulated_eval_time=1751.103556, accumulated_logging_time=4.243194, accumulated_submission_time=51041.431358, global_step=149489, preemption_count=0, score=51041.431358, test/accuracy=0.624000, test/loss=1.854052, test/num_examples=10000, total_duration=52801.885820, train/accuracy=0.899833, train/loss=0.620728, validation/accuracy=0.751660, validation/loss=1.227924, validation/num_examples=50000
I0128 03:52:22.659985 139656800745216 logging_writer.py:48] [149500] global_step=149500, grad_norm=3.698103427886963, loss=2.693131446838379
I0128 03:52:56.826246 139656783959808 logging_writer.py:48] [149600] global_step=149600, grad_norm=3.703230381011963, loss=2.620121479034424
I0128 03:53:30.957596 139656800745216 logging_writer.py:48] [149700] global_step=149700, grad_norm=3.8753468990325928, loss=2.627049446105957
I0128 03:54:05.078234 139656783959808 logging_writer.py:48] [149800] global_step=149800, grad_norm=3.5462779998779297, loss=2.622981071472168
I0128 03:54:39.203618 139656800745216 logging_writer.py:48] [149900] global_step=149900, grad_norm=3.49849796295166, loss=2.6019859313964844
I0128 03:55:13.346067 139656783959808 logging_writer.py:48] [150000] global_step=150000, grad_norm=3.740020513534546, loss=2.6213958263397217
I0128 03:55:47.475698 139656800745216 logging_writer.py:48] [150100] global_step=150100, grad_norm=3.7508559226989746, loss=2.6110966205596924
I0128 03:56:21.608302 139656783959808 logging_writer.py:48] [150200] global_step=150200, grad_norm=3.565638303756714, loss=2.6263604164123535
I0128 03:56:55.714145 139656800745216 logging_writer.py:48] [150300] global_step=150300, grad_norm=3.645570755004883, loss=2.627617359161377
I0128 03:57:29.842830 139656783959808 logging_writer.py:48] [150400] global_step=150400, grad_norm=3.762960195541382, loss=2.634441614151001
I0128 03:58:03.964530 139656800745216 logging_writer.py:48] [150500] global_step=150500, grad_norm=3.872227668762207, loss=2.6405014991760254
I0128 03:58:38.083636 139656783959808 logging_writer.py:48] [150600] global_step=150600, grad_norm=3.6730175018310547, loss=2.6484155654907227
I0128 03:59:12.217851 139656800745216 logging_writer.py:48] [150700] global_step=150700, grad_norm=3.824998140335083, loss=2.5876708030700684
I0128 03:59:46.337020 139656783959808 logging_writer.py:48] [150800] global_step=150800, grad_norm=3.783679485321045, loss=2.5960092544555664
I0128 04:00:20.499292 139656800745216 logging_writer.py:48] [150900] global_step=150900, grad_norm=3.6456494331359863, loss=2.61289381980896
I0128 04:00:48.615478 139822745589568 spec.py:321] Evaluating on the training split.
I0128 04:00:54.750255 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 04:01:03.721980 139822745589568 spec.py:349] Evaluating on the test split.
I0128 04:01:06.765725 139822745589568 submission_runner.py:408] Time since start: 53330.15s, 	Step: 150984, 	{'train/accuracy': 0.9048349857330322, 'train/loss': 0.5874025821685791, 'validation/accuracy': 0.7528600096702576, 'validation/loss': 1.2086260318756104, 'validation/num_examples': 50000, 'test/accuracy': 0.6312000155448914, 'test/loss': 1.8340696096420288, 'test/num_examples': 10000, 'score': 51551.43528985977, 'total_duration': 53330.150327920914, 'accumulated_submission_time': 51551.43528985977, 'accumulated_eval_time': 1769.2537994384766, 'accumulated_logging_time': 4.300515413284302}
I0128 04:01:06.802027 139656792352512 logging_writer.py:48] [150984] accumulated_eval_time=1769.253799, accumulated_logging_time=4.300515, accumulated_submission_time=51551.435290, global_step=150984, preemption_count=0, score=51551.435290, test/accuracy=0.631200, test/loss=1.834070, test/num_examples=10000, total_duration=53330.150328, train/accuracy=0.904835, train/loss=0.587403, validation/accuracy=0.752860, validation/loss=1.208626, validation/num_examples=50000
I0128 04:01:12.596699 139656800745216 logging_writer.py:48] [151000] global_step=151000, grad_norm=3.871086359024048, loss=2.6265950202941895
I0128 04:01:46.641812 139656792352512 logging_writer.py:48] [151100] global_step=151100, grad_norm=3.60884165763855, loss=2.6525306701660156
I0128 04:02:20.717834 139656800745216 logging_writer.py:48] [151200] global_step=151200, grad_norm=3.798269510269165, loss=2.641568660736084
I0128 04:02:54.806292 139656792352512 logging_writer.py:48] [151300] global_step=151300, grad_norm=3.8544704914093018, loss=2.6474251747131348
I0128 04:03:28.945007 139656800745216 logging_writer.py:48] [151400] global_step=151400, grad_norm=3.8857359886169434, loss=2.6010446548461914
I0128 04:04:03.081506 139656792352512 logging_writer.py:48] [151500] global_step=151500, grad_norm=3.6919775009155273, loss=2.617116689682007
I0128 04:04:37.195807 139656800745216 logging_writer.py:48] [151600] global_step=151600, grad_norm=3.711890697479248, loss=2.606092929840088
I0128 04:05:11.311827 139656792352512 logging_writer.py:48] [151700] global_step=151700, grad_norm=3.8846864700317383, loss=2.675908088684082
I0128 04:05:45.469796 139656800745216 logging_writer.py:48] [151800] global_step=151800, grad_norm=3.7885398864746094, loss=2.697347640991211
I0128 04:06:19.608421 139656792352512 logging_writer.py:48] [151900] global_step=151900, grad_norm=4.004288673400879, loss=2.6273610591888428
I0128 04:06:53.732593 139656800745216 logging_writer.py:48] [152000] global_step=152000, grad_norm=3.688739776611328, loss=2.571444034576416
I0128 04:07:27.842587 139656792352512 logging_writer.py:48] [152100] global_step=152100, grad_norm=3.657271146774292, loss=2.6446382999420166
I0128 04:08:01.975792 139656800745216 logging_writer.py:48] [152200] global_step=152200, grad_norm=3.667306423187256, loss=2.6570119857788086
I0128 04:08:36.068068 139656792352512 logging_writer.py:48] [152300] global_step=152300, grad_norm=3.7288596630096436, loss=2.600735902786255
I0128 04:09:10.173642 139656800745216 logging_writer.py:48] [152400] global_step=152400, grad_norm=3.84944486618042, loss=2.637375831604004
I0128 04:09:36.935069 139822745589568 spec.py:321] Evaluating on the training split.
I0128 04:09:42.994641 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 04:09:51.778947 139822745589568 spec.py:349] Evaluating on the test split.
I0128 04:09:54.199395 139822745589568 submission_runner.py:408] Time since start: 53857.58s, 	Step: 152480, 	{'train/accuracy': 0.9223333597183228, 'train/loss': 0.5322718024253845, 'validation/accuracy': 0.7513200044631958, 'validation/loss': 1.214342713356018, 'validation/num_examples': 50000, 'test/accuracy': 0.6295000314712524, 'test/loss': 1.8381295204162598, 'test/num_examples': 10000, 'score': 52061.50852203369, 'total_duration': 53857.58398604393, 'accumulated_submission_time': 52061.50852203369, 'accumulated_eval_time': 1786.5181086063385, 'accumulated_logging_time': 4.345062255859375}
I0128 04:09:54.247897 139656817530624 logging_writer.py:48] [152480] accumulated_eval_time=1786.518109, accumulated_logging_time=4.345062, accumulated_submission_time=52061.508522, global_step=152480, preemption_count=0, score=52061.508522, test/accuracy=0.629500, test/loss=1.838130, test/num_examples=10000, total_duration=53857.583986, train/accuracy=0.922333, train/loss=0.532272, validation/accuracy=0.751320, validation/loss=1.214343, validation/num_examples=50000
I0128 04:10:01.430767 139656834316032 logging_writer.py:48] [152500] global_step=152500, grad_norm=3.449354887008667, loss=2.5679516792297363
I0128 04:10:35.481831 139656817530624 logging_writer.py:48] [152600] global_step=152600, grad_norm=3.6832640171051025, loss=2.5910704135894775
I0128 04:11:09.577987 139656834316032 logging_writer.py:48] [152700] global_step=152700, grad_norm=3.6824169158935547, loss=2.5992178916931152
I0128 04:11:43.658694 139656817530624 logging_writer.py:48] [152800] global_step=152800, grad_norm=3.697942018508911, loss=2.535712957382202
I0128 04:12:17.846797 139656834316032 logging_writer.py:48] [152900] global_step=152900, grad_norm=3.661829710006714, loss=2.580888032913208
I0128 04:12:51.959861 139656817530624 logging_writer.py:48] [153000] global_step=153000, grad_norm=4.085297107696533, loss=2.6361305713653564
I0128 04:13:26.113233 139656834316032 logging_writer.py:48] [153100] global_step=153100, grad_norm=3.6783859729766846, loss=2.559732437133789
I0128 04:14:00.235372 139656817530624 logging_writer.py:48] [153200] global_step=153200, grad_norm=3.8438823223114014, loss=2.576810598373413
I0128 04:14:34.361832 139656834316032 logging_writer.py:48] [153300] global_step=153300, grad_norm=3.702497959136963, loss=2.6300148963928223
I0128 04:15:08.480695 139656817530624 logging_writer.py:48] [153400] global_step=153400, grad_norm=3.745535373687744, loss=2.647364854812622
I0128 04:15:42.587036 139656834316032 logging_writer.py:48] [153500] global_step=153500, grad_norm=3.98618745803833, loss=2.6456410884857178
I0128 04:16:16.725069 139656817530624 logging_writer.py:48] [153600] global_step=153600, grad_norm=4.007263660430908, loss=2.6362802982330322
I0128 04:16:50.849516 139656834316032 logging_writer.py:48] [153700] global_step=153700, grad_norm=3.9942660331726074, loss=2.722644329071045
I0128 04:17:24.969918 139656817530624 logging_writer.py:48] [153800] global_step=153800, grad_norm=3.73282790184021, loss=2.5990071296691895
I0128 04:17:59.107649 139656834316032 logging_writer.py:48] [153900] global_step=153900, grad_norm=3.6475181579589844, loss=2.5766844749450684
I0128 04:18:24.209115 139822745589568 spec.py:321] Evaluating on the training split.
I0128 04:18:30.227730 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 04:18:39.075803 139822745589568 spec.py:349] Evaluating on the test split.
I0128 04:18:41.513461 139822745589568 submission_runner.py:408] Time since start: 54384.90s, 	Step: 153975, 	{'train/accuracy': 0.9167131781578064, 'train/loss': 0.5371339917182922, 'validation/accuracy': 0.7554799914360046, 'validation/loss': 1.2029668092727661, 'validation/num_examples': 50000, 'test/accuracy': 0.6319000124931335, 'test/loss': 1.8418346643447876, 'test/num_examples': 10000, 'score': 52571.40790128708, 'total_duration': 54384.898052453995, 'accumulated_submission_time': 52571.40790128708, 'accumulated_eval_time': 1803.822417974472, 'accumulated_logging_time': 4.4028167724609375}
I0128 04:18:41.558071 139656800745216 logging_writer.py:48] [153975] accumulated_eval_time=1803.822418, accumulated_logging_time=4.402817, accumulated_submission_time=52571.407901, global_step=153975, preemption_count=0, score=52571.407901, test/accuracy=0.631900, test/loss=1.841835, test/num_examples=10000, total_duration=54384.898052, train/accuracy=0.916713, train/loss=0.537134, validation/accuracy=0.755480, validation/loss=1.202967, validation/num_examples=50000
I0128 04:18:50.451925 139656809137920 logging_writer.py:48] [154000] global_step=154000, grad_norm=3.787461996078491, loss=2.629124402999878
I0128 04:19:24.500871 139656800745216 logging_writer.py:48] [154100] global_step=154100, grad_norm=3.9246673583984375, loss=2.593489170074463
I0128 04:19:58.612298 139656809137920 logging_writer.py:48] [154200] global_step=154200, grad_norm=3.8042173385620117, loss=2.640536308288574
I0128 04:20:32.688273 139656800745216 logging_writer.py:48] [154300] global_step=154300, grad_norm=3.6717872619628906, loss=2.632706880569458
I0128 04:21:06.808480 139656809137920 logging_writer.py:48] [154400] global_step=154400, grad_norm=4.131359577178955, loss=2.64017391204834
I0128 04:21:40.895138 139656800745216 logging_writer.py:48] [154500] global_step=154500, grad_norm=3.6416029930114746, loss=2.5912389755249023
I0128 04:22:15.003654 139656809137920 logging_writer.py:48] [154600] global_step=154600, grad_norm=4.088022708892822, loss=2.662356376647949
I0128 04:22:49.104302 139656800745216 logging_writer.py:48] [154700] global_step=154700, grad_norm=3.638378381729126, loss=2.563776969909668
I0128 04:23:23.210164 139656809137920 logging_writer.py:48] [154800] global_step=154800, grad_norm=3.8571105003356934, loss=2.610534191131592
I0128 04:23:57.320305 139656800745216 logging_writer.py:48] [154900] global_step=154900, grad_norm=3.7797884941101074, loss=2.5866568088531494
I0128 04:24:31.497181 139656809137920 logging_writer.py:48] [155000] global_step=155000, grad_norm=4.0370073318481445, loss=2.6566529273986816
I0128 04:25:05.627728 139656800745216 logging_writer.py:48] [155100] global_step=155100, grad_norm=3.815509080886841, loss=2.599026918411255
I0128 04:25:39.753617 139656809137920 logging_writer.py:48] [155200] global_step=155200, grad_norm=4.337448596954346, loss=2.5598578453063965
I0128 04:26:13.852618 139656800745216 logging_writer.py:48] [155300] global_step=155300, grad_norm=3.9787609577178955, loss=2.654892921447754
I0128 04:26:47.974260 139656809137920 logging_writer.py:48] [155400] global_step=155400, grad_norm=3.9145076274871826, loss=2.655233383178711
I0128 04:27:11.663802 139822745589568 spec.py:321] Evaluating on the training split.
I0128 04:27:17.783728 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 04:27:26.512477 139822745589568 spec.py:349] Evaluating on the test split.
I0128 04:27:29.053813 139822745589568 submission_runner.py:408] Time since start: 54912.44s, 	Step: 155471, 	{'train/accuracy': 0.9189253449440002, 'train/loss': 0.5368630886077881, 'validation/accuracy': 0.7572599649429321, 'validation/loss': 1.1976443529129028, 'validation/num_examples': 50000, 'test/accuracy': 0.6337000131607056, 'test/loss': 1.8293724060058594, 'test/num_examples': 10000, 'score': 53081.45244860649, 'total_duration': 54912.43840265274, 'accumulated_submission_time': 53081.45244860649, 'accumulated_eval_time': 1821.2124042510986, 'accumulated_logging_time': 4.457838535308838}
I0128 04:27:29.101429 139656792352512 logging_writer.py:48] [155471] accumulated_eval_time=1821.212404, accumulated_logging_time=4.457839, accumulated_submission_time=53081.452449, global_step=155471, preemption_count=0, score=53081.452449, test/accuracy=0.633700, test/loss=1.829372, test/num_examples=10000, total_duration=54912.438403, train/accuracy=0.918925, train/loss=0.536863, validation/accuracy=0.757260, validation/loss=1.197644, validation/num_examples=50000
I0128 04:27:39.329543 139656800745216 logging_writer.py:48] [155500] global_step=155500, grad_norm=4.093766212463379, loss=2.5769546031951904
I0128 04:28:13.387753 139656792352512 logging_writer.py:48] [155600] global_step=155600, grad_norm=3.983828067779541, loss=2.6360549926757812
I0128 04:28:47.478265 139656800745216 logging_writer.py:48] [155700] global_step=155700, grad_norm=3.9185328483581543, loss=2.5670254230499268
I0128 04:29:21.601900 139656792352512 logging_writer.py:48] [155800] global_step=155800, grad_norm=3.5810110569000244, loss=2.546116352081299
I0128 04:29:55.720358 139656800745216 logging_writer.py:48] [155900] global_step=155900, grad_norm=3.7838938236236572, loss=2.6184158325195312
I0128 04:30:29.836868 139656792352512 logging_writer.py:48] [156000] global_step=156000, grad_norm=3.5592339038848877, loss=2.5835485458374023
I0128 04:31:03.985124 139656800745216 logging_writer.py:48] [156100] global_step=156100, grad_norm=4.055835723876953, loss=2.5936408042907715
I0128 04:31:38.106581 139656792352512 logging_writer.py:48] [156200] global_step=156200, grad_norm=3.725904703140259, loss=2.6248388290405273
I0128 04:32:12.215513 139656800745216 logging_writer.py:48] [156300] global_step=156300, grad_norm=4.021242141723633, loss=2.6314001083374023
I0128 04:32:46.354804 139656792352512 logging_writer.py:48] [156400] global_step=156400, grad_norm=3.7596311569213867, loss=2.577887535095215
I0128 04:33:20.469351 139656800745216 logging_writer.py:48] [156500] global_step=156500, grad_norm=3.8650379180908203, loss=2.5722973346710205
I0128 04:33:54.602204 139656792352512 logging_writer.py:48] [156600] global_step=156600, grad_norm=3.5865519046783447, loss=2.5519464015960693
I0128 04:34:28.744328 139656800745216 logging_writer.py:48] [156700] global_step=156700, grad_norm=3.689124584197998, loss=2.6000194549560547
I0128 04:35:02.870541 139656792352512 logging_writer.py:48] [156800] global_step=156800, grad_norm=3.6372790336608887, loss=2.551366090774536
I0128 04:35:37.009118 139656800745216 logging_writer.py:48] [156900] global_step=156900, grad_norm=3.6914286613464355, loss=2.582437515258789
I0128 04:35:59.337767 139822745589568 spec.py:321] Evaluating on the training split.
I0128 04:36:06.116405 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 04:36:14.770921 139822745589568 spec.py:349] Evaluating on the test split.
I0128 04:36:17.260379 139822745589568 submission_runner.py:408] Time since start: 55440.64s, 	Step: 156967, 	{'train/accuracy': 0.9168327450752258, 'train/loss': 0.5451948642730713, 'validation/accuracy': 0.7570799589157104, 'validation/loss': 1.195743203163147, 'validation/num_examples': 50000, 'test/accuracy': 0.6295000314712524, 'test/loss': 1.8315805196762085, 'test/num_examples': 10000, 'score': 53591.627490758896, 'total_duration': 55440.64496970177, 'accumulated_submission_time': 53591.627490758896, 'accumulated_eval_time': 1839.1349787712097, 'accumulated_logging_time': 4.514558553695679}
I0128 04:36:17.306162 139656792352512 logging_writer.py:48] [156967] accumulated_eval_time=1839.134979, accumulated_logging_time=4.514559, accumulated_submission_time=53591.627491, global_step=156967, preemption_count=0, score=53591.627491, test/accuracy=0.629500, test/loss=1.831581, test/num_examples=10000, total_duration=55440.644970, train/accuracy=0.916833, train/loss=0.545195, validation/accuracy=0.757080, validation/loss=1.195743, validation/num_examples=50000
I0128 04:36:28.905707 139656809137920 logging_writer.py:48] [157000] global_step=157000, grad_norm=3.7443461418151855, loss=2.549309015274048
I0128 04:37:03.040041 139656792352512 logging_writer.py:48] [157100] global_step=157100, grad_norm=3.758920192718506, loss=2.623197555541992
I0128 04:37:37.143850 139656809137920 logging_writer.py:48] [157200] global_step=157200, grad_norm=4.158996105194092, loss=2.597261428833008
I0128 04:38:11.247853 139656792352512 logging_writer.py:48] [157300] global_step=157300, grad_norm=3.791649103164673, loss=2.531714677810669
I0128 04:38:45.351299 139656809137920 logging_writer.py:48] [157400] global_step=157400, grad_norm=3.9976305961608887, loss=2.590564012527466
I0128 04:39:19.466501 139656792352512 logging_writer.py:48] [157500] global_step=157500, grad_norm=3.861828088760376, loss=2.545931816101074
I0128 04:39:53.586795 139656809137920 logging_writer.py:48] [157600] global_step=157600, grad_norm=3.653275966644287, loss=2.567558765411377
I0128 04:40:27.702108 139656792352512 logging_writer.py:48] [157700] global_step=157700, grad_norm=3.8584423065185547, loss=2.566652297973633
I0128 04:41:01.797426 139656809137920 logging_writer.py:48] [157800] global_step=157800, grad_norm=3.4993693828582764, loss=2.545891046524048
I0128 04:41:35.918789 139656792352512 logging_writer.py:48] [157900] global_step=157900, grad_norm=4.0158586502075195, loss=2.5839526653289795
I0128 04:42:10.024872 139656809137920 logging_writer.py:48] [158000] global_step=158000, grad_norm=3.8244469165802, loss=2.564331293106079
I0128 04:42:44.156707 139656792352512 logging_writer.py:48] [158100] global_step=158100, grad_norm=3.9483113288879395, loss=2.644458293914795
I0128 04:43:18.442725 139656809137920 logging_writer.py:48] [158200] global_step=158200, grad_norm=3.632744312286377, loss=2.5855982303619385
I0128 04:43:52.559076 139656792352512 logging_writer.py:48] [158300] global_step=158300, grad_norm=3.9771440029144287, loss=2.657672643661499
I0128 04:44:26.679917 139656809137920 logging_writer.py:48] [158400] global_step=158400, grad_norm=4.215747833251953, loss=2.616893768310547
I0128 04:44:47.321068 139822745589568 spec.py:321] Evaluating on the training split.
I0128 04:44:53.396740 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 04:45:02.143100 139822745589568 spec.py:349] Evaluating on the test split.
I0128 04:45:04.576687 139822745589568 submission_runner.py:408] Time since start: 55967.96s, 	Step: 158462, 	{'train/accuracy': 0.9191047549247742, 'train/loss': 0.5344241857528687, 'validation/accuracy': 0.7557399868965149, 'validation/loss': 1.1946972608566284, 'validation/num_examples': 50000, 'test/accuracy': 0.6314000487327576, 'test/loss': 1.8226438760757446, 'test/num_examples': 10000, 'score': 54101.57953572273, 'total_duration': 55967.96126079559, 'accumulated_submission_time': 54101.57953572273, 'accumulated_eval_time': 1856.390554189682, 'accumulated_logging_time': 4.57092547416687}
I0128 04:45:04.627385 139656825923328 logging_writer.py:48] [158462] accumulated_eval_time=1856.390554, accumulated_logging_time=4.570925, accumulated_submission_time=54101.579536, global_step=158462, preemption_count=0, score=54101.579536, test/accuracy=0.631400, test/loss=1.822644, test/num_examples=10000, total_duration=55967.961261, train/accuracy=0.919105, train/loss=0.534424, validation/accuracy=0.755740, validation/loss=1.194697, validation/num_examples=50000
I0128 04:45:17.935936 139656834316032 logging_writer.py:48] [158500] global_step=158500, grad_norm=3.5647425651550293, loss=2.5521609783172607
I0128 04:45:52.029724 139656825923328 logging_writer.py:48] [158600] global_step=158600, grad_norm=3.806011438369751, loss=2.5348265171051025
I0128 04:46:26.134491 139656834316032 logging_writer.py:48] [158700] global_step=158700, grad_norm=3.991422176361084, loss=2.6128838062286377
I0128 04:47:00.247162 139656825923328 logging_writer.py:48] [158800] global_step=158800, grad_norm=3.7437500953674316, loss=2.5850799083709717
I0128 04:47:34.381866 139656834316032 logging_writer.py:48] [158900] global_step=158900, grad_norm=3.9281580448150635, loss=2.587195873260498
I0128 04:48:08.522849 139656825923328 logging_writer.py:48] [159000] global_step=159000, grad_norm=3.8385469913482666, loss=2.593843460083008
I0128 04:48:42.661804 139656834316032 logging_writer.py:48] [159100] global_step=159100, grad_norm=4.034730434417725, loss=2.6449074745178223
I0128 04:49:16.806654 139656825923328 logging_writer.py:48] [159200] global_step=159200, grad_norm=3.9709155559539795, loss=2.6133298873901367
I0128 04:49:50.992670 139656834316032 logging_writer.py:48] [159300] global_step=159300, grad_norm=3.7790870666503906, loss=2.584023952484131
I0128 04:50:25.111888 139656825923328 logging_writer.py:48] [159400] global_step=159400, grad_norm=3.995306968688965, loss=2.6462526321411133
I0128 04:50:59.208907 139656834316032 logging_writer.py:48] [159500] global_step=159500, grad_norm=3.709568500518799, loss=2.5687735080718994
I0128 04:51:33.321411 139656825923328 logging_writer.py:48] [159600] global_step=159600, grad_norm=3.8405518531799316, loss=2.5404865741729736
I0128 04:52:07.417670 139656834316032 logging_writer.py:48] [159700] global_step=159700, grad_norm=3.7733500003814697, loss=2.572645664215088
I0128 04:52:41.548047 139656825923328 logging_writer.py:48] [159800] global_step=159800, grad_norm=3.744166851043701, loss=2.5661253929138184
I0128 04:53:15.685344 139656834316032 logging_writer.py:48] [159900] global_step=159900, grad_norm=4.209158897399902, loss=2.6094131469726562
I0128 04:53:34.597553 139822745589568 spec.py:321] Evaluating on the training split.
I0128 04:53:40.772879 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 04:53:49.536570 139822745589568 spec.py:349] Evaluating on the test split.
I0128 04:53:51.969971 139822745589568 submission_runner.py:408] Time since start: 56495.35s, 	Step: 159957, 	{'train/accuracy': 0.9191445708274841, 'train/loss': 0.5355068445205688, 'validation/accuracy': 0.7576599717140198, 'validation/loss': 1.2013722658157349, 'validation/num_examples': 50000, 'test/accuracy': 0.6325000524520874, 'test/loss': 1.8268934488296509, 'test/num_examples': 10000, 'score': 54611.48469758034, 'total_duration': 56495.354562044144, 'accumulated_submission_time': 54611.48469758034, 'accumulated_eval_time': 1873.7629334926605, 'accumulated_logging_time': 4.634932041168213}
I0128 04:53:52.023400 139656783959808 logging_writer.py:48] [159957] accumulated_eval_time=1873.762933, accumulated_logging_time=4.634932, accumulated_submission_time=54611.484698, global_step=159957, preemption_count=0, score=54611.484698, test/accuracy=0.632500, test/loss=1.826893, test/num_examples=10000, total_duration=56495.354562, train/accuracy=0.919145, train/loss=0.535507, validation/accuracy=0.757660, validation/loss=1.201372, validation/num_examples=50000
I0128 04:54:07.007026 139656792352512 logging_writer.py:48] [160000] global_step=160000, grad_norm=3.702331304550171, loss=2.529519557952881
I0128 04:54:41.087606 139656783959808 logging_writer.py:48] [160100] global_step=160100, grad_norm=3.8603579998016357, loss=2.624713897705078
I0128 04:55:15.187049 139656792352512 logging_writer.py:48] [160200] global_step=160200, grad_norm=4.0243449211120605, loss=2.6184816360473633
I0128 04:55:49.259497 139656783959808 logging_writer.py:48] [160300] global_step=160300, grad_norm=4.0079803466796875, loss=2.578503131866455
I0128 04:56:23.466554 139656792352512 logging_writer.py:48] [160400] global_step=160400, grad_norm=4.160809516906738, loss=2.5936999320983887
I0128 04:56:57.581569 139656783959808 logging_writer.py:48] [160500] global_step=160500, grad_norm=3.6997148990631104, loss=2.591912031173706
I0128 04:57:31.695571 139656792352512 logging_writer.py:48] [160600] global_step=160600, grad_norm=4.019063472747803, loss=2.6190669536590576
I0128 04:58:05.792695 139656783959808 logging_writer.py:48] [160700] global_step=160700, grad_norm=3.972529888153076, loss=2.635204553604126
I0128 04:58:39.929595 139656792352512 logging_writer.py:48] [160800] global_step=160800, grad_norm=4.109278678894043, loss=2.582242012023926
I0128 04:59:14.042898 139656783959808 logging_writer.py:48] [160900] global_step=160900, grad_norm=3.75803542137146, loss=2.613770008087158
I0128 04:59:48.160751 139656792352512 logging_writer.py:48] [161000] global_step=161000, grad_norm=4.162162780761719, loss=2.509213924407959
I0128 05:00:22.315405 139656783959808 logging_writer.py:48] [161100] global_step=161100, grad_norm=3.8426644802093506, loss=2.581313133239746
I0128 05:00:56.408910 139656792352512 logging_writer.py:48] [161200] global_step=161200, grad_norm=4.094521999359131, loss=2.534420967102051
I0128 05:01:30.516633 139656783959808 logging_writer.py:48] [161300] global_step=161300, grad_norm=4.072257995605469, loss=2.6370749473571777
I0128 05:02:04.639238 139656792352512 logging_writer.py:48] [161400] global_step=161400, grad_norm=3.7880218029022217, loss=2.528972625732422
I0128 05:02:22.216520 139822745589568 spec.py:321] Evaluating on the training split.
I0128 05:02:28.327900 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 05:02:37.210115 139822745589568 spec.py:349] Evaluating on the test split.
I0128 05:02:39.660326 139822745589568 submission_runner.py:408] Time since start: 57023.04s, 	Step: 161453, 	{'train/accuracy': 0.9230110049247742, 'train/loss': 0.5109298229217529, 'validation/accuracy': 0.7595599889755249, 'validation/loss': 1.1808478832244873, 'validation/num_examples': 50000, 'test/accuracy': 0.6340000033378601, 'test/loss': 1.8122520446777344, 'test/num_examples': 10000, 'score': 55121.617911338806, 'total_duration': 57023.04491233826, 'accumulated_submission_time': 55121.617911338806, 'accumulated_eval_time': 1891.2067058086395, 'accumulated_logging_time': 4.697674751281738}
I0128 05:02:39.705459 139656783959808 logging_writer.py:48] [161453] accumulated_eval_time=1891.206706, accumulated_logging_time=4.697675, accumulated_submission_time=55121.617911, global_step=161453, preemption_count=0, score=55121.617911, test/accuracy=0.634000, test/loss=1.812252, test/num_examples=10000, total_duration=57023.044912, train/accuracy=0.923011, train/loss=0.510930, validation/accuracy=0.759560, validation/loss=1.180848, validation/num_examples=50000
I0128 05:02:56.077096 139656817530624 logging_writer.py:48] [161500] global_step=161500, grad_norm=4.436865329742432, loss=2.61189603805542
I0128 05:03:30.153315 139656783959808 logging_writer.py:48] [161600] global_step=161600, grad_norm=3.99853777885437, loss=2.595167875289917
I0128 05:04:04.255383 139656817530624 logging_writer.py:48] [161700] global_step=161700, grad_norm=3.9330525398254395, loss=2.515742778778076
I0128 05:04:38.384135 139656783959808 logging_writer.py:48] [161800] global_step=161800, grad_norm=3.774777412414551, loss=2.524162769317627
I0128 05:05:12.480151 139656817530624 logging_writer.py:48] [161900] global_step=161900, grad_norm=3.9659833908081055, loss=2.5604825019836426
I0128 05:05:46.610637 139656783959808 logging_writer.py:48] [162000] global_step=162000, grad_norm=4.115168571472168, loss=2.5866200923919678
I0128 05:06:20.733504 139656817530624 logging_writer.py:48] [162100] global_step=162100, grad_norm=3.9048831462860107, loss=2.559654951095581
I0128 05:06:54.871828 139656783959808 logging_writer.py:48] [162200] global_step=162200, grad_norm=4.005956649780273, loss=2.5812458992004395
I0128 05:07:29.000821 139656817530624 logging_writer.py:48] [162300] global_step=162300, grad_norm=3.9401261806488037, loss=2.6231045722961426
I0128 05:08:03.131803 139656783959808 logging_writer.py:48] [162400] global_step=162400, grad_norm=3.6335883140563965, loss=2.529979705810547
I0128 05:08:37.306094 139656817530624 logging_writer.py:48] [162500] global_step=162500, grad_norm=3.9780449867248535, loss=2.566204071044922
I0128 05:09:11.432446 139656783959808 logging_writer.py:48] [162600] global_step=162600, grad_norm=3.899963617324829, loss=2.5242929458618164
I0128 05:09:45.575723 139656817530624 logging_writer.py:48] [162700] global_step=162700, grad_norm=3.9965431690216064, loss=2.5672109127044678
I0128 05:10:19.720144 139656783959808 logging_writer.py:48] [162800] global_step=162800, grad_norm=3.9292151927948, loss=2.5499138832092285
I0128 05:10:53.863573 139656817530624 logging_writer.py:48] [162900] global_step=162900, grad_norm=3.884204626083374, loss=2.5208687782287598
I0128 05:11:09.695964 139822745589568 spec.py:321] Evaluating on the training split.
I0128 05:11:15.768262 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 05:11:24.493959 139822745589568 spec.py:349] Evaluating on the test split.
I0128 05:11:26.971965 139822745589568 submission_runner.py:408] Time since start: 57550.36s, 	Step: 162948, 	{'train/accuracy': 0.9344706535339355, 'train/loss': 0.48090997338294983, 'validation/accuracy': 0.7597000002861023, 'validation/loss': 1.189298152923584, 'validation/num_examples': 50000, 'test/accuracy': 0.6338000297546387, 'test/loss': 1.813969612121582, 'test/num_examples': 10000, 'score': 55631.54829573631, 'total_duration': 57550.35655713081, 'accumulated_submission_time': 55631.54829573631, 'accumulated_eval_time': 1908.4826707839966, 'accumulated_logging_time': 4.7523229122161865}
I0128 05:11:27.019268 139656783959808 logging_writer.py:48] [162948] accumulated_eval_time=1908.482671, accumulated_logging_time=4.752323, accumulated_submission_time=55631.548296, global_step=162948, preemption_count=0, score=55631.548296, test/accuracy=0.633800, test/loss=1.813970, test/num_examples=10000, total_duration=57550.356557, train/accuracy=0.934471, train/loss=0.480910, validation/accuracy=0.759700, validation/loss=1.189298, validation/num_examples=50000
I0128 05:11:45.081849 139656809137920 logging_writer.py:48] [163000] global_step=163000, grad_norm=3.8029723167419434, loss=2.560927391052246
I0128 05:12:19.149658 139656783959808 logging_writer.py:48] [163100] global_step=163100, grad_norm=4.015542507171631, loss=2.570723056793213
I0128 05:12:53.252156 139656809137920 logging_writer.py:48] [163200] global_step=163200, grad_norm=3.877285957336426, loss=2.613577365875244
I0128 05:13:27.351804 139656783959808 logging_writer.py:48] [163300] global_step=163300, grad_norm=4.331058502197266, loss=2.6055660247802734
I0128 05:14:01.460177 139656809137920 logging_writer.py:48] [163400] global_step=163400, grad_norm=3.825796365737915, loss=2.5629124641418457
I0128 05:14:35.574151 139656783959808 logging_writer.py:48] [163500] global_step=163500, grad_norm=3.939053773880005, loss=2.5697684288024902
I0128 05:15:09.760390 139656809137920 logging_writer.py:48] [163600] global_step=163600, grad_norm=4.331782341003418, loss=2.5400173664093018
I0128 05:15:43.869754 139656783959808 logging_writer.py:48] [163700] global_step=163700, grad_norm=3.7842025756835938, loss=2.5477757453918457
I0128 05:16:17.968327 139656809137920 logging_writer.py:48] [163800] global_step=163800, grad_norm=4.075033664703369, loss=2.520627498626709
I0128 05:16:52.085765 139656783959808 logging_writer.py:48] [163900] global_step=163900, grad_norm=3.9311351776123047, loss=2.534031867980957
I0128 05:17:26.207514 139656809137920 logging_writer.py:48] [164000] global_step=164000, grad_norm=4.046697616577148, loss=2.5798730850219727
I0128 05:18:00.340609 139656783959808 logging_writer.py:48] [164100] global_step=164100, grad_norm=3.8476107120513916, loss=2.5376198291778564
I0128 05:18:34.472247 139656809137920 logging_writer.py:48] [164200] global_step=164200, grad_norm=4.183990478515625, loss=2.532318115234375
I0128 05:19:08.583179 139656783959808 logging_writer.py:48] [164300] global_step=164300, grad_norm=4.016987323760986, loss=2.5778260231018066
I0128 05:19:42.713031 139656809137920 logging_writer.py:48] [164400] global_step=164400, grad_norm=3.8930556774139404, loss=2.614696979522705
I0128 05:19:57.176511 139822745589568 spec.py:321] Evaluating on the training split.
I0128 05:20:03.353353 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 05:20:12.131371 139822745589568 spec.py:349] Evaluating on the test split.
I0128 05:20:14.553776 139822745589568 submission_runner.py:408] Time since start: 58077.94s, 	Step: 164444, 	{'train/accuracy': 0.9301458597183228, 'train/loss': 0.48501822352409363, 'validation/accuracy': 0.7604199647903442, 'validation/loss': 1.181099534034729, 'validation/num_examples': 50000, 'test/accuracy': 0.6353000402450562, 'test/loss': 1.8088539838790894, 'test/num_examples': 10000, 'score': 56141.64248919487, 'total_duration': 58077.93835878372, 'accumulated_submission_time': 56141.64248919487, 'accumulated_eval_time': 1925.8598954677582, 'accumulated_logging_time': 4.809988737106323}
I0128 05:20:14.599729 139656834316032 logging_writer.py:48] [164444] accumulated_eval_time=1925.859895, accumulated_logging_time=4.809989, accumulated_submission_time=56141.642489, global_step=164444, preemption_count=0, score=56141.642489, test/accuracy=0.635300, test/loss=1.808854, test/num_examples=10000, total_duration=58077.938359, train/accuracy=0.930146, train/loss=0.485018, validation/accuracy=0.760420, validation/loss=1.181100, validation/num_examples=50000
I0128 05:20:34.013810 139658730145536 logging_writer.py:48] [164500] global_step=164500, grad_norm=3.814593553543091, loss=2.5383846759796143
I0128 05:21:08.121790 139656834316032 logging_writer.py:48] [164600] global_step=164600, grad_norm=3.862431764602661, loss=2.5653727054595947
I0128 05:21:42.224690 139658730145536 logging_writer.py:48] [164700] global_step=164700, grad_norm=4.033200740814209, loss=2.5624194145202637
I0128 05:22:16.341982 139656834316032 logging_writer.py:48] [164800] global_step=164800, grad_norm=3.931074857711792, loss=2.561628818511963
I0128 05:22:50.509370 139658730145536 logging_writer.py:48] [164900] global_step=164900, grad_norm=4.0045037269592285, loss=2.579683780670166
I0128 05:23:24.627816 139656834316032 logging_writer.py:48] [165000] global_step=165000, grad_norm=3.6672353744506836, loss=2.5579466819763184
I0128 05:23:58.743915 139658730145536 logging_writer.py:48] [165100] global_step=165100, grad_norm=3.8892338275909424, loss=2.5743587017059326
I0128 05:24:32.841190 139656834316032 logging_writer.py:48] [165200] global_step=165200, grad_norm=3.8361690044403076, loss=2.5770437717437744
I0128 05:25:06.960139 139658730145536 logging_writer.py:48] [165300] global_step=165300, grad_norm=4.293868064880371, loss=2.563068151473999
I0128 05:25:41.104196 139656834316032 logging_writer.py:48] [165400] global_step=165400, grad_norm=4.010149002075195, loss=2.569330930709839
I0128 05:26:15.219869 139658730145536 logging_writer.py:48] [165500] global_step=165500, grad_norm=4.151147842407227, loss=2.5612223148345947
I0128 05:26:49.359597 139656834316032 logging_writer.py:48] [165600] global_step=165600, grad_norm=3.877743721008301, loss=2.5559632778167725
I0128 05:27:23.520005 139658730145536 logging_writer.py:48] [165700] global_step=165700, grad_norm=4.004972457885742, loss=2.556401491165161
I0128 05:27:57.673767 139656834316032 logging_writer.py:48] [165800] global_step=165800, grad_norm=4.183874607086182, loss=2.570164203643799
I0128 05:28:31.782940 139658730145536 logging_writer.py:48] [165900] global_step=165900, grad_norm=3.924429178237915, loss=2.55610990524292
I0128 05:28:44.559855 139822745589568 spec.py:321] Evaluating on the training split.
I0128 05:28:50.635519 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 05:28:59.334541 139822745589568 spec.py:349] Evaluating on the test split.
I0128 05:29:01.868485 139822745589568 submission_runner.py:408] Time since start: 58605.25s, 	Step: 165939, 	{'train/accuracy': 0.9299465417861938, 'train/loss': 0.4834108352661133, 'validation/accuracy': 0.7608599662780762, 'validation/loss': 1.1775710582733154, 'validation/num_examples': 50000, 'test/accuracy': 0.6385000348091125, 'test/loss': 1.806591510772705, 'test/num_examples': 10000, 'score': 56651.543511390686, 'total_duration': 58605.25307846069, 'accumulated_submission_time': 56651.543511390686, 'accumulated_eval_time': 1943.168488740921, 'accumulated_logging_time': 4.8650195598602295}
I0128 05:29:01.920289 139656800745216 logging_writer.py:48] [165939] accumulated_eval_time=1943.168489, accumulated_logging_time=4.865020, accumulated_submission_time=56651.543511, global_step=165939, preemption_count=0, score=56651.543511, test/accuracy=0.638500, test/loss=1.806592, test/num_examples=10000, total_duration=58605.253078, train/accuracy=0.929947, train/loss=0.483411, validation/accuracy=0.760860, validation/loss=1.177571, validation/num_examples=50000
I0128 05:29:23.050079 139656809137920 logging_writer.py:48] [166000] global_step=166000, grad_norm=3.768653631210327, loss=2.5326175689697266
I0128 05:29:57.099682 139656800745216 logging_writer.py:48] [166100] global_step=166100, grad_norm=3.7108283042907715, loss=2.530142307281494
I0128 05:30:31.200272 139656809137920 logging_writer.py:48] [166200] global_step=166200, grad_norm=3.87597393989563, loss=2.5700812339782715
I0128 05:31:05.273168 139656800745216 logging_writer.py:48] [166300] global_step=166300, grad_norm=4.1513190269470215, loss=2.5707359313964844
I0128 05:31:39.396330 139656809137920 logging_writer.py:48] [166400] global_step=166400, grad_norm=4.190212726593018, loss=2.570749521255493
I0128 05:32:13.536840 139656800745216 logging_writer.py:48] [166500] global_step=166500, grad_norm=4.115581512451172, loss=2.547848701477051
I0128 05:32:47.648990 139656809137920 logging_writer.py:48] [166600] global_step=166600, grad_norm=3.9471025466918945, loss=2.5079731941223145
I0128 05:33:21.770795 139656800745216 logging_writer.py:48] [166700] global_step=166700, grad_norm=4.101850986480713, loss=2.601919412612915
I0128 05:33:55.963285 139656809137920 logging_writer.py:48] [166800] global_step=166800, grad_norm=4.369396686553955, loss=2.529752492904663
I0128 05:34:30.101171 139656800745216 logging_writer.py:48] [166900] global_step=166900, grad_norm=4.106139183044434, loss=2.5316362380981445
I0128 05:35:04.230490 139656809137920 logging_writer.py:48] [167000] global_step=167000, grad_norm=3.9474103450775146, loss=2.5488100051879883
I0128 05:35:38.336104 139656800745216 logging_writer.py:48] [167100] global_step=167100, grad_norm=4.282256126403809, loss=2.610884189605713
I0128 05:36:12.431452 139656809137920 logging_writer.py:48] [167200] global_step=167200, grad_norm=3.6865227222442627, loss=2.5305986404418945
I0128 05:36:46.564548 139656800745216 logging_writer.py:48] [167300] global_step=167300, grad_norm=3.690476894378662, loss=2.539565086364746
I0128 05:37:20.684308 139656809137920 logging_writer.py:48] [167400] global_step=167400, grad_norm=3.8530075550079346, loss=2.541599750518799
I0128 05:37:32.074398 139822745589568 spec.py:321] Evaluating on the training split.
I0128 05:37:38.161259 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 05:37:46.772451 139822745589568 spec.py:349] Evaluating on the test split.
I0128 05:37:49.199028 139822745589568 submission_runner.py:408] Time since start: 59132.58s, 	Step: 167435, 	{'train/accuracy': 0.9329559803009033, 'train/loss': 0.47671154141426086, 'validation/accuracy': 0.7617599964141846, 'validation/loss': 1.1800211668014526, 'validation/num_examples': 50000, 'test/accuracy': 0.6402000188827515, 'test/loss': 1.8032078742980957, 'test/num_examples': 10000, 'score': 57161.636585474014, 'total_duration': 59132.583621263504, 'accumulated_submission_time': 57161.636585474014, 'accumulated_eval_time': 1960.2930953502655, 'accumulated_logging_time': 4.926531791687012}
I0128 05:37:49.248208 139656817530624 logging_writer.py:48] [167435] accumulated_eval_time=1960.293095, accumulated_logging_time=4.926532, accumulated_submission_time=57161.636585, global_step=167435, preemption_count=0, score=57161.636585, test/accuracy=0.640200, test/loss=1.803208, test/num_examples=10000, total_duration=59132.583621, train/accuracy=0.932956, train/loss=0.476712, validation/accuracy=0.761760, validation/loss=1.180021, validation/num_examples=50000
I0128 05:38:11.752752 139656825923328 logging_writer.py:48] [167500] global_step=167500, grad_norm=3.7485926151275635, loss=2.5146431922912598
I0128 05:38:45.849406 139656817530624 logging_writer.py:48] [167600] global_step=167600, grad_norm=3.9998769760131836, loss=2.5303690433502197
I0128 05:39:19.981635 139656825923328 logging_writer.py:48] [167700] global_step=167700, grad_norm=4.121006965637207, loss=2.537991523742676
I0128 05:39:54.094574 139656817530624 logging_writer.py:48] [167800] global_step=167800, grad_norm=4.009284973144531, loss=2.569307327270508
I0128 05:40:28.264865 139656825923328 logging_writer.py:48] [167900] global_step=167900, grad_norm=3.835599660873413, loss=2.532212257385254
I0128 05:41:02.382397 139656817530624 logging_writer.py:48] [168000] global_step=168000, grad_norm=3.6744871139526367, loss=2.4819483757019043
I0128 05:41:36.516548 139656825923328 logging_writer.py:48] [168100] global_step=168100, grad_norm=4.234210968017578, loss=2.566204309463501
I0128 05:42:10.612722 139656817530624 logging_writer.py:48] [168200] global_step=168200, grad_norm=4.232669353485107, loss=2.5745089054107666
I0128 05:42:44.723110 139656825923328 logging_writer.py:48] [168300] global_step=168300, grad_norm=3.852560043334961, loss=2.5300345420837402
I0128 05:43:18.831895 139656817530624 logging_writer.py:48] [168400] global_step=168400, grad_norm=3.590904712677002, loss=2.521848678588867
I0128 05:43:52.936294 139656825923328 logging_writer.py:48] [168500] global_step=168500, grad_norm=3.8355696201324463, loss=2.565901041030884
I0128 05:44:27.056891 139656817530624 logging_writer.py:48] [168600] global_step=168600, grad_norm=4.18519926071167, loss=2.5858428478240967
I0128 05:45:01.170648 139656825923328 logging_writer.py:48] [168700] global_step=168700, grad_norm=3.8962771892547607, loss=2.5686349868774414
I0128 05:45:35.295232 139656817530624 logging_writer.py:48] [168800] global_step=168800, grad_norm=4.100794315338135, loss=2.514857769012451
I0128 05:46:09.438363 139656825923328 logging_writer.py:48] [168900] global_step=168900, grad_norm=4.142076015472412, loss=2.5890958309173584
I0128 05:46:19.477754 139822745589568 spec.py:321] Evaluating on the training split.
I0128 05:46:25.759460 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 05:46:34.497803 139822745589568 spec.py:349] Evaluating on the test split.
I0128 05:46:36.816010 139822745589568 submission_runner.py:408] Time since start: 59660.20s, 	Step: 168931, 	{'train/accuracy': 0.9340720176696777, 'train/loss': 0.4837055206298828, 'validation/accuracy': 0.7615999579429626, 'validation/loss': 1.1844817399978638, 'validation/num_examples': 50000, 'test/accuracy': 0.6378000378608704, 'test/loss': 1.8050771951675415, 'test/num_examples': 10000, 'score': 57671.803205251694, 'total_duration': 59660.20059251785, 'accumulated_submission_time': 57671.803205251694, 'accumulated_eval_time': 1977.6313047409058, 'accumulated_logging_time': 4.987208604812622}
I0128 05:46:36.867970 139656792352512 logging_writer.py:48] [168931] accumulated_eval_time=1977.631305, accumulated_logging_time=4.987209, accumulated_submission_time=57671.803205, global_step=168931, preemption_count=0, score=57671.803205, test/accuracy=0.637800, test/loss=1.805077, test/num_examples=10000, total_duration=59660.200593, train/accuracy=0.934072, train/loss=0.483706, validation/accuracy=0.761600, validation/loss=1.184482, validation/num_examples=50000
I0128 05:47:00.705223 139656800745216 logging_writer.py:48] [169000] global_step=169000, grad_norm=4.136316299438477, loss=2.568269968032837
I0128 05:47:34.786754 139656792352512 logging_writer.py:48] [169100] global_step=169100, grad_norm=3.841461420059204, loss=2.538048267364502
I0128 05:48:08.871113 139656800745216 logging_writer.py:48] [169200] global_step=169200, grad_norm=4.090688228607178, loss=2.5025229454040527
I0128 05:48:42.992217 139656792352512 logging_writer.py:48] [169300] global_step=169300, grad_norm=4.068251609802246, loss=2.6096465587615967
I0128 05:49:17.080057 139656800745216 logging_writer.py:48] [169400] global_step=169400, grad_norm=4.004903793334961, loss=2.5139424800872803
I0128 05:49:51.191764 139656792352512 logging_writer.py:48] [169500] global_step=169500, grad_norm=4.140435695648193, loss=2.518798589706421
I0128 05:50:25.314302 139656800745216 logging_writer.py:48] [169600] global_step=169600, grad_norm=3.859081506729126, loss=2.5455188751220703
I0128 05:50:59.427253 139656792352512 logging_writer.py:48] [169700] global_step=169700, grad_norm=4.107006072998047, loss=2.5360331535339355
I0128 05:51:33.561527 139656800745216 logging_writer.py:48] [169800] global_step=169800, grad_norm=4.069728374481201, loss=2.5735623836517334
I0128 05:52:07.679594 139656792352512 logging_writer.py:48] [169900] global_step=169900, grad_norm=3.9833614826202393, loss=2.4973502159118652
I0128 05:52:41.860336 139656800745216 logging_writer.py:48] [170000] global_step=170000, grad_norm=4.4042158126831055, loss=2.611574649810791
I0128 05:53:15.991123 139656792352512 logging_writer.py:48] [170100] global_step=170100, grad_norm=3.9478986263275146, loss=2.5156302452087402
I0128 05:53:50.084987 139656800745216 logging_writer.py:48] [170200] global_step=170200, grad_norm=4.09096097946167, loss=2.515362501144409
I0128 05:54:24.210515 139656792352512 logging_writer.py:48] [170300] global_step=170300, grad_norm=3.7894845008850098, loss=2.5182504653930664
I0128 05:54:58.344920 139656800745216 logging_writer.py:48] [170400] global_step=170400, grad_norm=3.8781471252441406, loss=2.552807331085205
I0128 05:55:07.025765 139822745589568 spec.py:321] Evaluating on the training split.
I0128 05:55:13.187812 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 05:55:21.734686 139822745589568 spec.py:349] Evaluating on the test split.
I0128 05:55:24.139152 139822745589568 submission_runner.py:408] Time since start: 60187.52s, 	Step: 170427, 	{'train/accuracy': 0.9329758882522583, 'train/loss': 0.47846075892448425, 'validation/accuracy': 0.7625399827957153, 'validation/loss': 1.1756603717803955, 'validation/num_examples': 50000, 'test/accuracy': 0.6420000195503235, 'test/loss': 1.7998629808425903, 'test/num_examples': 10000, 'score': 58181.900406360626, 'total_duration': 60187.52374267578, 'accumulated_submission_time': 58181.900406360626, 'accumulated_eval_time': 1994.7446548938751, 'accumulated_logging_time': 5.04884672164917}
I0128 05:55:24.190006 139656700098304 logging_writer.py:48] [170427] accumulated_eval_time=1994.744655, accumulated_logging_time=5.048847, accumulated_submission_time=58181.900406, global_step=170427, preemption_count=0, score=58181.900406, test/accuracy=0.642000, test/loss=1.799863, test/num_examples=10000, total_duration=60187.523743, train/accuracy=0.932976, train/loss=0.478461, validation/accuracy=0.762540, validation/loss=1.175660, validation/num_examples=50000
I0128 05:55:49.402276 139656783959808 logging_writer.py:48] [170500] global_step=170500, grad_norm=4.221706390380859, loss=2.539123773574829
I0128 05:56:23.489907 139656700098304 logging_writer.py:48] [170600] global_step=170600, grad_norm=4.2044291496276855, loss=2.561544895172119
I0128 05:56:57.602072 139656783959808 logging_writer.py:48] [170700] global_step=170700, grad_norm=4.0945820808410645, loss=2.563669443130493
I0128 05:57:31.715245 139656700098304 logging_writer.py:48] [170800] global_step=170800, grad_norm=4.145262718200684, loss=2.5511233806610107
I0128 05:58:05.856868 139656783959808 logging_writer.py:48] [170900] global_step=170900, grad_norm=4.231925964355469, loss=2.5981693267822266
I0128 05:58:39.980895 139656700098304 logging_writer.py:48] [171000] global_step=171000, grad_norm=4.009326934814453, loss=2.566253185272217
I0128 05:59:14.174782 139656783959808 logging_writer.py:48] [171100] global_step=171100, grad_norm=4.154445648193359, loss=2.524049758911133
I0128 05:59:48.292841 139656700098304 logging_writer.py:48] [171200] global_step=171200, grad_norm=3.965080499649048, loss=2.5179240703582764
I0128 06:00:22.395362 139656783959808 logging_writer.py:48] [171300] global_step=171300, grad_norm=4.084906101226807, loss=2.546177864074707
I0128 06:00:56.542351 139656700098304 logging_writer.py:48] [171400] global_step=171400, grad_norm=4.162672519683838, loss=2.568476676940918
I0128 06:01:30.658023 139656783959808 logging_writer.py:48] [171500] global_step=171500, grad_norm=3.913766860961914, loss=2.497445821762085
I0128 06:02:04.778009 139656700098304 logging_writer.py:48] [171600] global_step=171600, grad_norm=4.301403522491455, loss=2.4972574710845947
I0128 06:02:38.901897 139656783959808 logging_writer.py:48] [171700] global_step=171700, grad_norm=4.00697660446167, loss=2.5843725204467773
I0128 06:03:13.034622 139656700098304 logging_writer.py:48] [171800] global_step=171800, grad_norm=3.995375633239746, loss=2.501422882080078
I0128 06:03:47.133940 139656783959808 logging_writer.py:48] [171900] global_step=171900, grad_norm=3.8129947185516357, loss=2.4970977306365967
I0128 06:03:54.444844 139822745589568 spec.py:321] Evaluating on the training split.
I0128 06:04:00.461544 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 06:04:09.359609 139822745589568 spec.py:349] Evaluating on the test split.
I0128 06:04:11.693168 139822745589568 submission_runner.py:408] Time since start: 60715.08s, 	Step: 171923, 	{'train/accuracy': 0.9389548301696777, 'train/loss': 0.46069061756134033, 'validation/accuracy': 0.7628600001335144, 'validation/loss': 1.1755813360214233, 'validation/num_examples': 50000, 'test/accuracy': 0.6399000287055969, 'test/loss': 1.7999354600906372, 'test/num_examples': 10000, 'score': 58692.09399271011, 'total_duration': 60715.07775473595, 'accumulated_submission_time': 58692.09399271011, 'accumulated_eval_time': 2011.9929358959198, 'accumulated_logging_time': 5.10937762260437}
I0128 06:04:11.744330 139656809137920 logging_writer.py:48] [171923] accumulated_eval_time=2011.992936, accumulated_logging_time=5.109378, accumulated_submission_time=58692.093993, global_step=171923, preemption_count=0, score=58692.093993, test/accuracy=0.639900, test/loss=1.799935, test/num_examples=10000, total_duration=60715.077755, train/accuracy=0.938955, train/loss=0.460691, validation/accuracy=0.762860, validation/loss=1.175581, validation/num_examples=50000
I0128 06:04:38.321779 139656817530624 logging_writer.py:48] [172000] global_step=172000, grad_norm=3.953526496887207, loss=2.5324134826660156
I0128 06:05:12.406693 139656809137920 logging_writer.py:48] [172100] global_step=172100, grad_norm=3.9098315238952637, loss=2.560220718383789
I0128 06:05:46.585375 139656817530624 logging_writer.py:48] [172200] global_step=172200, grad_norm=4.128584384918213, loss=2.5359411239624023
I0128 06:06:20.683531 139656809137920 logging_writer.py:48] [172300] global_step=172300, grad_norm=4.16328239440918, loss=2.5811760425567627
I0128 06:06:54.814047 139656817530624 logging_writer.py:48] [172400] global_step=172400, grad_norm=4.261655330657959, loss=2.566279411315918
I0128 06:07:28.933045 139656809137920 logging_writer.py:48] [172500] global_step=172500, grad_norm=4.072198867797852, loss=2.541647434234619
I0128 06:08:03.070713 139656817530624 logging_writer.py:48] [172600] global_step=172600, grad_norm=4.0614333152771, loss=2.522721290588379
I0128 06:08:37.187400 139656809137920 logging_writer.py:48] [172700] global_step=172700, grad_norm=4.233314514160156, loss=2.5654900074005127
I0128 06:09:11.337713 139656817530624 logging_writer.py:48] [172800] global_step=172800, grad_norm=3.8926801681518555, loss=2.5504989624023438
I0128 06:09:45.445732 139656809137920 logging_writer.py:48] [172900] global_step=172900, grad_norm=3.9811630249023438, loss=2.5361533164978027
I0128 06:10:19.589381 139656817530624 logging_writer.py:48] [173000] global_step=173000, grad_norm=4.008810997009277, loss=2.5076866149902344
I0128 06:10:53.728906 139656809137920 logging_writer.py:48] [173100] global_step=173100, grad_norm=4.022998332977295, loss=2.516857624053955
I0128 06:11:27.844300 139656817530624 logging_writer.py:48] [173200] global_step=173200, grad_norm=4.059172630310059, loss=2.526484966278076
I0128 06:12:02.056469 139656809137920 logging_writer.py:48] [173300] global_step=173300, grad_norm=3.9142560958862305, loss=2.521641969680786
I0128 06:12:36.167616 139656817530624 logging_writer.py:48] [173400] global_step=173400, grad_norm=3.992429494857788, loss=2.468367576599121
I0128 06:12:41.788083 139822745589568 spec.py:321] Evaluating on the training split.
I0128 06:12:47.841019 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 06:12:56.547449 139822745589568 spec.py:349] Evaluating on the test split.
I0128 06:12:59.051229 139822745589568 submission_runner.py:408] Time since start: 61242.44s, 	Step: 173418, 	{'train/accuracy': 0.9387754797935486, 'train/loss': 0.46282458305358887, 'validation/accuracy': 0.761900007724762, 'validation/loss': 1.1791095733642578, 'validation/num_examples': 50000, 'test/accuracy': 0.64000004529953, 'test/loss': 1.8056416511535645, 'test/num_examples': 10000, 'score': 59202.0762693882, 'total_duration': 61242.43579864502, 'accumulated_submission_time': 59202.0762693882, 'accumulated_eval_time': 2029.256034374237, 'accumulated_logging_time': 5.169875860214233}
I0128 06:12:59.104036 139656783959808 logging_writer.py:48] [173418] accumulated_eval_time=2029.256034, accumulated_logging_time=5.169876, accumulated_submission_time=59202.076269, global_step=173418, preemption_count=0, score=59202.076269, test/accuracy=0.640000, test/loss=1.805642, test/num_examples=10000, total_duration=61242.435799, train/accuracy=0.938775, train/loss=0.462825, validation/accuracy=0.761900, validation/loss=1.179110, validation/num_examples=50000
I0128 06:13:27.396066 139656792352512 logging_writer.py:48] [173500] global_step=173500, grad_norm=4.1261396408081055, loss=2.522702932357788
I0128 06:14:01.469383 139656783959808 logging_writer.py:48] [173600] global_step=173600, grad_norm=3.9878575801849365, loss=2.5294151306152344
I0128 06:14:35.604853 139656792352512 logging_writer.py:48] [173700] global_step=173700, grad_norm=4.423538684844971, loss=2.581799030303955
I0128 06:15:09.733331 139656783959808 logging_writer.py:48] [173800] global_step=173800, grad_norm=3.993457555770874, loss=2.5225818157196045
I0128 06:15:43.844125 139656792352512 logging_writer.py:48] [173900] global_step=173900, grad_norm=3.8631041049957275, loss=2.4802560806274414
I0128 06:16:17.968409 139656783959808 logging_writer.py:48] [174000] global_step=174000, grad_norm=4.0220627784729, loss=2.4859652519226074
I0128 06:16:52.095190 139656792352512 logging_writer.py:48] [174100] global_step=174100, grad_norm=4.199225902557373, loss=2.541576623916626
I0128 06:17:26.209124 139656783959808 logging_writer.py:48] [174200] global_step=174200, grad_norm=3.988417863845825, loss=2.596665859222412
I0128 06:18:00.502416 139656792352512 logging_writer.py:48] [174300] global_step=174300, grad_norm=4.046979904174805, loss=2.5186495780944824
I0128 06:18:34.601820 139656783959808 logging_writer.py:48] [174400] global_step=174400, grad_norm=4.265388488769531, loss=2.496187686920166
I0128 06:19:08.710379 139656792352512 logging_writer.py:48] [174500] global_step=174500, grad_norm=4.071064472198486, loss=2.57283616065979
I0128 06:19:42.830895 139656783959808 logging_writer.py:48] [174600] global_step=174600, grad_norm=3.944201707839966, loss=2.4853811264038086
I0128 06:20:16.952602 139656792352512 logging_writer.py:48] [174700] global_step=174700, grad_norm=4.043544292449951, loss=2.5478291511535645
I0128 06:20:51.071446 139656783959808 logging_writer.py:48] [174800] global_step=174800, grad_norm=3.8684659004211426, loss=2.5500071048736572
I0128 06:21:25.205414 139656792352512 logging_writer.py:48] [174900] global_step=174900, grad_norm=3.976274251937866, loss=2.5184431076049805
I0128 06:21:29.098163 139822745589568 spec.py:321] Evaluating on the training split.
I0128 06:21:35.285087 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 06:21:44.082172 139822745589568 spec.py:349] Evaluating on the test split.
I0128 06:21:46.509905 139822745589568 submission_runner.py:408] Time since start: 61769.89s, 	Step: 174913, 	{'train/accuracy': 0.9377790093421936, 'train/loss': 0.4617302119731903, 'validation/accuracy': 0.7626799941062927, 'validation/loss': 1.1732220649719238, 'validation/num_examples': 50000, 'test/accuracy': 0.6394000053405762, 'test/loss': 1.7951186895370483, 'test/num_examples': 10000, 'score': 59712.00906252861, 'total_duration': 61769.894496679306, 'accumulated_submission_time': 59712.00906252861, 'accumulated_eval_time': 2046.667736530304, 'accumulated_logging_time': 5.231820344924927}
I0128 06:21:46.557115 139656700098304 logging_writer.py:48] [174913] accumulated_eval_time=2046.667737, accumulated_logging_time=5.231820, accumulated_submission_time=59712.009063, global_step=174913, preemption_count=0, score=59712.009063, test/accuracy=0.639400, test/loss=1.795119, test/num_examples=10000, total_duration=61769.894497, train/accuracy=0.937779, train/loss=0.461730, validation/accuracy=0.762680, validation/loss=1.173222, validation/num_examples=50000
I0128 06:22:16.549486 139656783959808 logging_writer.py:48] [175000] global_step=175000, grad_norm=4.0133585929870605, loss=2.5457723140716553
I0128 06:22:50.639398 139656700098304 logging_writer.py:48] [175100] global_step=175100, grad_norm=4.238616943359375, loss=2.583588123321533
I0128 06:23:24.727618 139656783959808 logging_writer.py:48] [175200] global_step=175200, grad_norm=4.015614986419678, loss=2.580798387527466
I0128 06:23:58.844071 139656700098304 logging_writer.py:48] [175300] global_step=175300, grad_norm=4.10077428817749, loss=2.5204567909240723
I0128 06:24:33.104278 139656783959808 logging_writer.py:48] [175400] global_step=175400, grad_norm=3.9304821491241455, loss=2.550137996673584
I0128 06:25:07.201842 139656700098304 logging_writer.py:48] [175500] global_step=175500, grad_norm=3.8991286754608154, loss=2.5379576683044434
I0128 06:25:41.326500 139656783959808 logging_writer.py:48] [175600] global_step=175600, grad_norm=4.029608726501465, loss=2.513854503631592
I0128 06:26:15.427566 139656700098304 logging_writer.py:48] [175700] global_step=175700, grad_norm=4.028932094573975, loss=2.5116214752197266
I0128 06:26:49.559203 139656783959808 logging_writer.py:48] [175800] global_step=175800, grad_norm=3.9369730949401855, loss=2.543304204940796
I0128 06:27:23.667360 139656700098304 logging_writer.py:48] [175900] global_step=175900, grad_norm=4.088376998901367, loss=2.5260825157165527
I0128 06:27:57.783724 139656783959808 logging_writer.py:48] [176000] global_step=176000, grad_norm=4.1069512367248535, loss=2.6157469749450684
I0128 06:28:31.939273 139656700098304 logging_writer.py:48] [176100] global_step=176100, grad_norm=4.090583801269531, loss=2.506309747695923
I0128 06:29:06.062781 139656783959808 logging_writer.py:48] [176200] global_step=176200, grad_norm=3.9528021812438965, loss=2.475402355194092
I0128 06:29:40.176614 139656700098304 logging_writer.py:48] [176300] global_step=176300, grad_norm=4.250314235687256, loss=2.573610782623291
I0128 06:30:14.308714 139656783959808 logging_writer.py:48] [176400] global_step=176400, grad_norm=4.194356441497803, loss=2.5192039012908936
I0128 06:30:16.845465 139822745589568 spec.py:321] Evaluating on the training split.
I0128 06:30:22.874408 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 06:30:31.553604 139822745589568 spec.py:349] Evaluating on the test split.
I0128 06:30:33.990525 139822745589568 submission_runner.py:408] Time since start: 62297.38s, 	Step: 176409, 	{'train/accuracy': 0.93949294090271, 'train/loss': 0.4578624963760376, 'validation/accuracy': 0.76419997215271, 'validation/loss': 1.1710017919540405, 'validation/num_examples': 50000, 'test/accuracy': 0.64410001039505, 'test/loss': 1.790612816810608, 'test/num_examples': 10000, 'score': 60222.23619198799, 'total_duration': 62297.375115156174, 'accumulated_submission_time': 60222.23619198799, 'accumulated_eval_time': 2063.812755346298, 'accumulated_logging_time': 5.288837909698486}
I0128 06:30:34.041359 139656809137920 logging_writer.py:48] [176409] accumulated_eval_time=2063.812755, accumulated_logging_time=5.288838, accumulated_submission_time=60222.236192, global_step=176409, preemption_count=0, score=60222.236192, test/accuracy=0.644100, test/loss=1.790613, test/num_examples=10000, total_duration=62297.375115, train/accuracy=0.939493, train/loss=0.457862, validation/accuracy=0.764200, validation/loss=1.171002, validation/num_examples=50000
I0128 06:31:05.399376 139656825923328 logging_writer.py:48] [176500] global_step=176500, grad_norm=4.061716556549072, loss=2.4894986152648926
I0128 06:31:39.511727 139656809137920 logging_writer.py:48] [176600] global_step=176600, grad_norm=3.9355227947235107, loss=2.497084379196167
I0128 06:32:13.669335 139656825923328 logging_writer.py:48] [176700] global_step=176700, grad_norm=4.063774585723877, loss=2.4622294902801514
I0128 06:32:47.764125 139656809137920 logging_writer.py:48] [176800] global_step=176800, grad_norm=3.9948227405548096, loss=2.547149181365967
I0128 06:33:21.896699 139656825923328 logging_writer.py:48] [176900] global_step=176900, grad_norm=3.9251229763031006, loss=2.4496994018554688
I0128 06:33:56.003948 139656809137920 logging_writer.py:48] [177000] global_step=177000, grad_norm=4.000762939453125, loss=2.500425338745117
I0128 06:34:30.099313 139656825923328 logging_writer.py:48] [177100] global_step=177100, grad_norm=4.111804008483887, loss=2.454998254776001
I0128 06:35:04.256445 139656809137920 logging_writer.py:48] [177200] global_step=177200, grad_norm=4.402994632720947, loss=2.556044101715088
I0128 06:35:38.394309 139656825923328 logging_writer.py:48] [177300] global_step=177300, grad_norm=4.248279094696045, loss=2.5273149013519287
I0128 06:36:12.518110 139656809137920 logging_writer.py:48] [177400] global_step=177400, grad_norm=4.00034236907959, loss=2.4635982513427734
I0128 06:36:46.724555 139656825923328 logging_writer.py:48] [177500] global_step=177500, grad_norm=4.1734795570373535, loss=2.5173423290252686
I0128 06:37:20.837054 139656809137920 logging_writer.py:48] [177600] global_step=177600, grad_norm=4.198391437530518, loss=2.550581932067871
I0128 06:37:54.990308 139656825923328 logging_writer.py:48] [177700] global_step=177700, grad_norm=4.066330909729004, loss=2.5183255672454834
I0128 06:38:29.118976 139656809137920 logging_writer.py:48] [177800] global_step=177800, grad_norm=4.20108699798584, loss=2.5322976112365723
I0128 06:39:03.269483 139656825923328 logging_writer.py:48] [177900] global_step=177900, grad_norm=3.8909683227539062, loss=2.4612925052642822
I0128 06:39:04.091061 139822745589568 spec.py:321] Evaluating on the training split.
I0128 06:39:10.121212 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 06:39:18.807761 139822745589568 spec.py:349] Evaluating on the test split.
I0128 06:39:21.227293 139822745589568 submission_runner.py:408] Time since start: 62824.61s, 	Step: 177904, 	{'train/accuracy': 0.9384167790412903, 'train/loss': 0.45472314953804016, 'validation/accuracy': 0.7644400000572205, 'validation/loss': 1.167048454284668, 'validation/num_examples': 50000, 'test/accuracy': 0.6421000361442566, 'test/loss': 1.7899819612503052, 'test/num_examples': 10000, 'score': 60732.223915576935, 'total_duration': 62824.61188578606, 'accumulated_submission_time': 60732.223915576935, 'accumulated_eval_time': 2080.94895029068, 'accumulated_logging_time': 5.348676443099976}
I0128 06:39:21.280217 139656700098304 logging_writer.py:48] [177904] accumulated_eval_time=2080.948950, accumulated_logging_time=5.348676, accumulated_submission_time=60732.223916, global_step=177904, preemption_count=0, score=60732.223916, test/accuracy=0.642100, test/loss=1.789982, test/num_examples=10000, total_duration=62824.611886, train/accuracy=0.938417, train/loss=0.454723, validation/accuracy=0.764440, validation/loss=1.167048, validation/num_examples=50000
I0128 06:39:54.360243 139656783959808 logging_writer.py:48] [178000] global_step=178000, grad_norm=3.9221408367156982, loss=2.488004207611084
I0128 06:40:28.452621 139656700098304 logging_writer.py:48] [178100] global_step=178100, grad_norm=3.690135955810547, loss=2.4754624366760254
I0128 06:41:02.585691 139656783959808 logging_writer.py:48] [178200] global_step=178200, grad_norm=3.7527122497558594, loss=2.460627555847168
I0128 06:41:36.722618 139656700098304 logging_writer.py:48] [178300] global_step=178300, grad_norm=4.356651306152344, loss=2.5777177810668945
I0128 06:42:10.818079 139656783959808 logging_writer.py:48] [178400] global_step=178400, grad_norm=4.068392753601074, loss=2.52063250541687
I0128 06:42:44.921974 139656700098304 logging_writer.py:48] [178500] global_step=178500, grad_norm=4.126748561859131, loss=2.514293670654297
I0128 06:43:19.098110 139656783959808 logging_writer.py:48] [178600] global_step=178600, grad_norm=4.216477870941162, loss=2.5199155807495117
I0128 06:43:53.222943 139656700098304 logging_writer.py:48] [178700] global_step=178700, grad_norm=3.949828863143921, loss=2.4872231483459473
I0128 06:44:27.356461 139656783959808 logging_writer.py:48] [178800] global_step=178800, grad_norm=4.141541957855225, loss=2.5313010215759277
I0128 06:45:01.488696 139656700098304 logging_writer.py:48] [178900] global_step=178900, grad_norm=4.105281829833984, loss=2.502685070037842
I0128 06:45:35.621381 139656783959808 logging_writer.py:48] [179000] global_step=179000, grad_norm=4.085905075073242, loss=2.487971782684326
I0128 06:46:09.736433 139656700098304 logging_writer.py:48] [179100] global_step=179100, grad_norm=3.836940050125122, loss=2.482161045074463
I0128 06:46:43.856889 139656783959808 logging_writer.py:48] [179200] global_step=179200, grad_norm=3.961524248123169, loss=2.540631055831909
I0128 06:47:17.991852 139656700098304 logging_writer.py:48] [179300] global_step=179300, grad_norm=3.8424954414367676, loss=2.486246109008789
I0128 06:47:51.567356 139822745589568 spec.py:321] Evaluating on the training split.
I0128 06:47:57.667336 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 06:48:06.437223 139822745589568 spec.py:349] Evaluating on the test split.
I0128 06:48:08.871304 139822745589568 submission_runner.py:408] Time since start: 63352.26s, 	Step: 179400, 	{'train/accuracy': 0.9395527839660645, 'train/loss': 0.4537563621997833, 'validation/accuracy': 0.7645599842071533, 'validation/loss': 1.1670894622802734, 'validation/num_examples': 50000, 'test/accuracy': 0.6438000202178955, 'test/loss': 1.7903746366500854, 'test/num_examples': 10000, 'score': 61242.448419332504, 'total_duration': 63352.25589585304, 'accumulated_submission_time': 61242.448419332504, 'accumulated_eval_time': 2098.2528672218323, 'accumulated_logging_time': 5.410964012145996}
I0128 06:48:08.925253 139656783959808 logging_writer.py:48] [179400] accumulated_eval_time=2098.252867, accumulated_logging_time=5.410964, accumulated_submission_time=61242.448419, global_step=179400, preemption_count=0, score=61242.448419, test/accuracy=0.643800, test/loss=1.790375, test/num_examples=10000, total_duration=63352.255896, train/accuracy=0.939553, train/loss=0.453756, validation/accuracy=0.764560, validation/loss=1.167089, validation/num_examples=50000
I0128 06:48:09.281241 139656809137920 logging_writer.py:48] [179400] global_step=179400, grad_norm=4.2586283683776855, loss=2.539843797683716
I0128 06:48:43.363566 139656783959808 logging_writer.py:48] [179500] global_step=179500, grad_norm=4.03125524520874, loss=2.5511162281036377
I0128 06:49:17.532025 139656809137920 logging_writer.py:48] [179600] global_step=179600, grad_norm=3.8319365978240967, loss=2.5139894485473633
I0128 06:49:51.640669 139656783959808 logging_writer.py:48] [179700] global_step=179700, grad_norm=3.9859793186187744, loss=2.4828684329986572
I0128 06:50:25.777756 139656809137920 logging_writer.py:48] [179800] global_step=179800, grad_norm=4.255047798156738, loss=2.528827667236328
I0128 06:50:59.917655 139656783959808 logging_writer.py:48] [179900] global_step=179900, grad_norm=4.00574254989624, loss=2.5146689414978027
I0128 06:51:34.041534 139656809137920 logging_writer.py:48] [180000] global_step=180000, grad_norm=4.015293598175049, loss=2.5274171829223633
I0128 06:52:08.169204 139656783959808 logging_writer.py:48] [180100] global_step=180100, grad_norm=3.9604594707489014, loss=2.5259804725646973
I0128 06:52:42.271571 139656809137920 logging_writer.py:48] [180200] global_step=180200, grad_norm=4.265509128570557, loss=2.4887478351593018
I0128 06:53:16.382989 139656783959808 logging_writer.py:48] [180300] global_step=180300, grad_norm=3.9107279777526855, loss=2.5094759464263916
I0128 06:53:50.518148 139656809137920 logging_writer.py:48] [180400] global_step=180400, grad_norm=4.010951042175293, loss=2.5131888389587402
I0128 06:54:24.656145 139656783959808 logging_writer.py:48] [180500] global_step=180500, grad_norm=3.9976348876953125, loss=2.557004690170288
I0128 06:54:58.777787 139656809137920 logging_writer.py:48] [180600] global_step=180600, grad_norm=4.136631011962891, loss=2.5022928714752197
I0128 06:55:32.967902 139656783959808 logging_writer.py:48] [180700] global_step=180700, grad_norm=4.1565470695495605, loss=2.487736940383911
I0128 06:56:07.099134 139656809137920 logging_writer.py:48] [180800] global_step=180800, grad_norm=3.6565608978271484, loss=2.494478702545166
I0128 06:56:38.967980 139822745589568 spec.py:321] Evaluating on the training split.
I0128 06:56:45.001914 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 06:56:53.790087 139822745589568 spec.py:349] Evaluating on the test split.
I0128 06:56:56.224471 139822745589568 submission_runner.py:408] Time since start: 63879.61s, 	Step: 180895, 	{'train/accuracy': 0.9403898119926453, 'train/loss': 0.4528295397758484, 'validation/accuracy': 0.7649999856948853, 'validation/loss': 1.168945074081421, 'validation/num_examples': 50000, 'test/accuracy': 0.6429000496864319, 'test/loss': 1.7909166812896729, 'test/num_examples': 10000, 'score': 61752.42921423912, 'total_duration': 63879.60906338692, 'accumulated_submission_time': 61752.42921423912, 'accumulated_eval_time': 2115.509332180023, 'accumulated_logging_time': 5.475275278091431}
I0128 06:56:56.281434 139656792352512 logging_writer.py:48] [180895] accumulated_eval_time=2115.509332, accumulated_logging_time=5.475275, accumulated_submission_time=61752.429214, global_step=180895, preemption_count=0, score=61752.429214, test/accuracy=0.642900, test/loss=1.790917, test/num_examples=10000, total_duration=63879.609063, train/accuracy=0.940390, train/loss=0.452830, validation/accuracy=0.765000, validation/loss=1.168945, validation/num_examples=50000
I0128 06:56:58.336197 139656800745216 logging_writer.py:48] [180900] global_step=180900, grad_norm=4.214674949645996, loss=2.4889235496520996
I0128 06:57:32.394798 139656792352512 logging_writer.py:48] [181000] global_step=181000, grad_norm=4.391696929931641, loss=2.5468292236328125
I0128 06:58:06.478773 139656800745216 logging_writer.py:48] [181100] global_step=181100, grad_norm=4.0172858238220215, loss=2.5160577297210693
I0128 06:58:40.617991 139656792352512 logging_writer.py:48] [181200] global_step=181200, grad_norm=3.942196846008301, loss=2.488537311553955
I0128 06:59:14.733259 139656800745216 logging_writer.py:48] [181300] global_step=181300, grad_norm=3.9067814350128174, loss=2.516082763671875
I0128 06:59:48.857188 139656792352512 logging_writer.py:48] [181400] global_step=181400, grad_norm=3.829814910888672, loss=2.507089138031006
I0128 07:00:22.966276 139656800745216 logging_writer.py:48] [181500] global_step=181500, grad_norm=4.058095932006836, loss=2.5292861461639404
I0128 07:00:57.077473 139656792352512 logging_writer.py:48] [181600] global_step=181600, grad_norm=4.2535600662231445, loss=2.4970223903656006
I0128 07:01:31.184385 139656800745216 logging_writer.py:48] [181700] global_step=181700, grad_norm=4.215383529663086, loss=2.440753698348999
I0128 07:02:05.395069 139656792352512 logging_writer.py:48] [181800] global_step=181800, grad_norm=3.9576027393341064, loss=2.531651496887207
I0128 07:02:39.520948 139656800745216 logging_writer.py:48] [181900] global_step=181900, grad_norm=4.077906131744385, loss=2.5376579761505127
I0128 07:03:13.638047 139656792352512 logging_writer.py:48] [182000] global_step=182000, grad_norm=4.128261566162109, loss=2.5554182529449463
I0128 07:03:47.746879 139656800745216 logging_writer.py:48] [182100] global_step=182100, grad_norm=4.140591144561768, loss=2.519256591796875
I0128 07:04:21.864627 139656792352512 logging_writer.py:48] [182200] global_step=182200, grad_norm=3.805692195892334, loss=2.488706588745117
I0128 07:04:55.988653 139656800745216 logging_writer.py:48] [182300] global_step=182300, grad_norm=4.2717108726501465, loss=2.5711491107940674
I0128 07:05:26.467177 139822745589568 spec.py:321] Evaluating on the training split.
I0128 07:05:32.569973 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 07:05:41.327899 139822745589568 spec.py:349] Evaluating on the test split.
I0128 07:05:43.781316 139822745589568 submission_runner.py:408] Time since start: 64407.17s, 	Step: 182391, 	{'train/accuracy': 0.9399114847183228, 'train/loss': 0.4492985010147095, 'validation/accuracy': 0.7644000053405762, 'validation/loss': 1.1697618961334229, 'validation/num_examples': 50000, 'test/accuracy': 0.6430000066757202, 'test/loss': 1.7924065589904785, 'test/num_examples': 10000, 'score': 62262.553658008575, 'total_duration': 64407.16589832306, 'accumulated_submission_time': 62262.553658008575, 'accumulated_eval_time': 2132.8234283924103, 'accumulated_logging_time': 5.542778015136719}
I0128 07:05:43.830391 139656817530624 logging_writer.py:48] [182391] accumulated_eval_time=2132.823428, accumulated_logging_time=5.542778, accumulated_submission_time=62262.553658, global_step=182391, preemption_count=0, score=62262.553658, test/accuracy=0.643000, test/loss=1.792407, test/num_examples=10000, total_duration=64407.165898, train/accuracy=0.939911, train/loss=0.449299, validation/accuracy=0.764400, validation/loss=1.169762, validation/num_examples=50000
I0128 07:05:48.400732 139656825923328 logging_writer.py:48] [182400] global_step=182400, grad_norm=4.240524768829346, loss=2.516634464263916
I0128 07:06:22.460903 139656817530624 logging_writer.py:48] [182500] global_step=182500, grad_norm=4.305940628051758, loss=2.496309995651245
I0128 07:06:56.534927 139656825923328 logging_writer.py:48] [182600] global_step=182600, grad_norm=3.9322164058685303, loss=2.491304874420166
I0128 07:07:30.647317 139656817530624 logging_writer.py:48] [182700] global_step=182700, grad_norm=4.005276679992676, loss=2.505733013153076
I0128 07:08:04.754345 139656825923328 logging_writer.py:48] [182800] global_step=182800, grad_norm=3.9589297771453857, loss=2.516005516052246
I0128 07:08:38.952656 139656817530624 logging_writer.py:48] [182900] global_step=182900, grad_norm=3.958090305328369, loss=2.4823641777038574
I0128 07:09:13.074028 139656825923328 logging_writer.py:48] [183000] global_step=183000, grad_norm=4.098106384277344, loss=2.51747989654541
I0128 07:09:47.200663 139656817530624 logging_writer.py:48] [183100] global_step=183100, grad_norm=4.58051061630249, loss=2.4868123531341553
I0128 07:10:21.336434 139656825923328 logging_writer.py:48] [183200] global_step=183200, grad_norm=3.913198709487915, loss=2.492532253265381
I0128 07:10:55.484779 139656817530624 logging_writer.py:48] [183300] global_step=183300, grad_norm=4.049259185791016, loss=2.5504937171936035
I0128 07:11:29.596098 139656825923328 logging_writer.py:48] [183400] global_step=183400, grad_norm=3.783999443054199, loss=2.4669992923736572
I0128 07:12:03.734132 139656817530624 logging_writer.py:48] [183500] global_step=183500, grad_norm=3.8249239921569824, loss=2.471203565597534
I0128 07:12:37.869452 139656825923328 logging_writer.py:48] [183600] global_step=183600, grad_norm=3.8774499893188477, loss=2.4756648540496826
I0128 07:13:12.022284 139656817530624 logging_writer.py:48] [183700] global_step=183700, grad_norm=3.9951069355010986, loss=2.524056911468506
I0128 07:13:46.164892 139656825923328 logging_writer.py:48] [183800] global_step=183800, grad_norm=4.241805076599121, loss=2.575638771057129
I0128 07:14:13.960234 139822745589568 spec.py:321] Evaluating on the training split.
I0128 07:14:19.992372 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 07:14:28.839010 139822745589568 spec.py:349] Evaluating on the test split.
I0128 07:14:31.253895 139822745589568 submission_runner.py:408] Time since start: 64934.64s, 	Step: 183883, 	{'train/accuracy': 0.9393534660339355, 'train/loss': 0.4564688205718994, 'validation/accuracy': 0.7648400068283081, 'validation/loss': 1.1722710132598877, 'validation/num_examples': 50000, 'test/accuracy': 0.6425000429153442, 'test/loss': 1.794602870941162, 'test/num_examples': 10000, 'score': 62771.477128744125, 'total_duration': 64934.63848924637, 'accumulated_submission_time': 62771.477128744125, 'accumulated_eval_time': 2150.1170587539673, 'accumulated_logging_time': 6.747524976730347}
I0128 07:14:31.303735 139656700098304 logging_writer.py:48] [183883] accumulated_eval_time=2150.117059, accumulated_logging_time=6.747525, accumulated_submission_time=62771.477129, global_step=183883, preemption_count=0, score=62771.477129, test/accuracy=0.642500, test/loss=1.794603, test/num_examples=10000, total_duration=64934.638489, train/accuracy=0.939353, train/loss=0.456469, validation/accuracy=0.764840, validation/loss=1.172271, validation/num_examples=50000
I0128 07:14:37.520163 139656783959808 logging_writer.py:48] [183900] global_step=183900, grad_norm=4.007784366607666, loss=2.5225396156311035
I0128 07:15:11.603338 139656700098304 logging_writer.py:48] [184000] global_step=184000, grad_norm=3.8488521575927734, loss=2.494616985321045
I0128 07:15:45.702650 139656783959808 logging_writer.py:48] [184100] global_step=184100, grad_norm=4.292133808135986, loss=2.4912102222442627
I0128 07:16:19.828777 139656700098304 logging_writer.py:48] [184200] global_step=184200, grad_norm=3.9732015132904053, loss=2.5175647735595703
I0128 07:16:53.981397 139656783959808 logging_writer.py:48] [184300] global_step=184300, grad_norm=4.0144734382629395, loss=2.469670295715332
I0128 07:17:28.106860 139656700098304 logging_writer.py:48] [184400] global_step=184400, grad_norm=3.90775203704834, loss=2.525583267211914
I0128 07:18:02.235123 139656783959808 logging_writer.py:48] [184500] global_step=184500, grad_norm=4.076757431030273, loss=2.551504611968994
I0128 07:18:28.011258 139656700098304 logging_writer.py:48] [184577] global_step=184577, preemption_count=0, score=63008.119927
I0128 07:18:28.480375 139822745589568 checkpoints.py:490] Saving checkpoint at step: 184577
I0128 07:18:29.586073 139822745589568 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_1/imagenet_resnet_jax/trial_2/checkpoint_184577
I0128 07:18:29.610244 139822745589568 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_1/imagenet_resnet_jax/trial_2/checkpoint_184577.
I0128 07:18:30.358557 139822745589568 submission_runner.py:583] Tuning trial 2/5
I0128 07:18:30.358777 139822745589568 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.2, learning_rate=0.0008445074561975979, one_minus_beta1=0.11042418465, beta2=0.9978504782314613, weight_decay=0.08135402759553023, warmup_factor=0.05)
I0128 07:18:30.364359 139822745589568 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0010363520123064518, 'train/loss': 6.910816669464111, 'validation/accuracy': 0.001019999966956675, 'validation/loss': 6.91091251373291, 'validation/num_examples': 50000, 'test/accuracy': 0.0012000000569969416, 'test/loss': 6.910790920257568, 'test/num_examples': 10000, 'score': 32.273086071014404, 'total_duration': 49.67532181739807, 'accumulated_submission_time': 32.273086071014404, 'accumulated_eval_time': 17.40215516090393, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1491, {'train/accuracy': 0.08322703838348389, 'train/loss': 5.25524377822876, 'validation/accuracy': 0.07711999863386154, 'validation/loss': 5.3119893074035645, 'validation/num_examples': 50000, 'test/accuracy': 0.05530000105500221, 'test/loss': 5.544482231140137, 'test/num_examples': 10000, 'score': 542.2085037231445, 'total_duration': 577.119710445404, 'accumulated_submission_time': 542.2085037231445, 'accumulated_eval_time': 34.842326641082764, 'accumulated_logging_time': 0.01926255226135254, 'global_step': 1491, 'preemption_count': 0}), (2981, {'train/accuracy': 0.19897958636283875, 'train/loss': 4.21004581451416, 'validation/accuracy': 0.17909999191761017, 'validation/loss': 4.317634105682373, 'validation/num_examples': 50000, 'test/accuracy': 0.13120000064373016, 'test/loss': 4.706563472747803, 'test/num_examples': 10000, 'score': 1052.3712661266327, 'total_duration': 1104.8108565807343, 'accumulated_submission_time': 1052.3712661266327, 'accumulated_eval_time': 52.28858208656311, 'accumulated_logging_time': 0.05031228065490723, 'global_step': 2981, 'preemption_count': 0}), (4470, {'train/accuracy': 0.2876076102256775, 'train/loss': 3.585117816925049, 'validation/accuracy': 0.26396000385284424, 'validation/loss': 3.7350914478302, 'validation/num_examples': 50000, 'test/accuracy': 0.19200000166893005, 'test/loss': 4.257549285888672, 'test/num_examples': 10000, 'score': 1562.2961611747742, 'total_duration': 1632.9713141918182, 'accumulated_submission_time': 1562.2961611747742, 'accumulated_eval_time': 70.44600534439087, 'accumulated_logging_time': 0.07889699935913086, 'global_step': 4470, 'preemption_count': 0}), (5960, {'train/accuracy': 0.38807398080825806, 'train/loss': 2.992854356765747, 'validation/accuracy': 0.3614400029182434, 'validation/loss': 3.130856990814209, 'validation/num_examples': 50000, 'test/accuracy': 0.27470001578330994, 'test/loss': 3.713503360748291, 'test/num_examples': 10000, 'score': 2072.4718992710114, 'total_duration': 2160.7697203159332, 'accumulated_submission_time': 2072.4718992710114, 'accumulated_eval_time': 87.99014830589294, 'accumulated_logging_time': 0.10644721984863281, 'global_step': 5960, 'preemption_count': 0}), (7450, {'train/accuracy': 0.45238760113716125, 'train/loss': 2.617628335952759, 'validation/accuracy': 0.4184799790382385, 'validation/loss': 2.7827277183532715, 'validation/num_examples': 50000, 'test/accuracy': 0.31640002131462097, 'test/loss': 3.388169765472412, 'test/num_examples': 10000, 'score': 2582.5007004737854, 'total_duration': 2688.537534236908, 'accumulated_submission_time': 2582.5007004737854, 'accumulated_eval_time': 105.64619159698486, 'accumulated_logging_time': 0.1386871337890625, 'global_step': 7450, 'preemption_count': 0}), (8942, {'train/accuracy': 0.5526944994926453, 'train/loss': 2.124098539352417, 'validation/accuracy': 0.4767199754714966, 'validation/loss': 2.479666233062744, 'validation/num_examples': 50000, 'test/accuracy': 0.3680000305175781, 'test/loss': 3.117877244949341, 'test/num_examples': 10000, 'score': 3092.7613253593445, 'total_duration': 3216.206754922867, 'accumulated_submission_time': 3092.7613253593445, 'accumulated_eval_time': 122.9762909412384, 'accumulated_logging_time': 0.16669344902038574, 'global_step': 8942, 'preemption_count': 0}), (10434, {'train/accuracy': 0.5689373016357422, 'train/loss': 2.0224695205688477, 'validation/accuracy': 0.522159993648529, 'validation/loss': 2.2517900466918945, 'validation/num_examples': 50000, 'test/accuracy': 0.4036000072956085, 'test/loss': 2.9123005867004395, 'test/num_examples': 10000, 'score': 3602.929986476898, 'total_duration': 3743.8133985996246, 'accumulated_submission_time': 3602.929986476898, 'accumulated_eval_time': 140.33562183380127, 'accumulated_logging_time': 0.19498658180236816, 'global_step': 10434, 'preemption_count': 0}), (11926, {'train/accuracy': 0.5983737111091614, 'train/loss': 1.9081037044525146, 'validation/accuracy': 0.5486199855804443, 'validation/loss': 2.138624906539917, 'validation/num_examples': 50000, 'test/accuracy': 0.4262000322341919, 'test/loss': 2.801759719848633, 'test/num_examples': 10000, 'score': 4112.95587015152, 'total_duration': 4271.180375099182, 'accumulated_submission_time': 4112.95587015152, 'accumulated_eval_time': 157.598552942276, 'accumulated_logging_time': 0.22365856170654297, 'global_step': 11926, 'preemption_count': 0}), (13419, {'train/accuracy': 0.6163105964660645, 'train/loss': 1.7988837957382202, 'validation/accuracy': 0.5671799778938293, 'validation/loss': 2.038691282272339, 'validation/num_examples': 50000, 'test/accuracy': 0.4473000168800354, 'test/loss': 2.6989235877990723, 'test/num_examples': 10000, 'score': 4623.103399991989, 'total_duration': 4798.86471414566, 'accumulated_submission_time': 4623.103399991989, 'accumulated_eval_time': 175.05454540252686, 'accumulated_logging_time': 0.25371837615966797, 'global_step': 13419, 'preemption_count': 0}), (14912, {'train/accuracy': 0.6312180757522583, 'train/loss': 1.7623796463012695, 'validation/accuracy': 0.5798799991607666, 'validation/loss': 1.994551658630371, 'validation/num_examples': 50000, 'test/accuracy': 0.45900002121925354, 'test/loss': 2.626842975616455, 'test/num_examples': 10000, 'score': 5133.311586380005, 'total_duration': 5326.436780452728, 'accumulated_submission_time': 5133.311586380005, 'accumulated_eval_time': 192.3369791507721, 'accumulated_logging_time': 0.2829594612121582, 'global_step': 14912, 'preemption_count': 0}), (16405, {'train/accuracy': 0.6322146058082581, 'train/loss': 1.7425355911254883, 'validation/accuracy': 0.5858799815177917, 'validation/loss': 1.9565809965133667, 'validation/num_examples': 50000, 'test/accuracy': 0.4595000147819519, 'test/loss': 2.652688980102539, 'test/num_examples': 10000, 'score': 5643.3654227256775, 'total_duration': 5853.849092960358, 'accumulated_submission_time': 5643.3654227256775, 'accumulated_eval_time': 209.60658407211304, 'accumulated_logging_time': 0.32151174545288086, 'global_step': 16405, 'preemption_count': 0}), (17899, {'train/accuracy': 0.6486168503761292, 'train/loss': 1.6495143175125122, 'validation/accuracy': 0.5984199643135071, 'validation/loss': 1.8850512504577637, 'validation/num_examples': 50000, 'test/accuracy': 0.4756000339984894, 'test/loss': 2.5317037105560303, 'test/num_examples': 10000, 'score': 6153.40634727478, 'total_duration': 6381.513226747513, 'accumulated_submission_time': 6153.40634727478, 'accumulated_eval_time': 227.14486122131348, 'accumulated_logging_time': 0.35701680183410645, 'global_step': 17899, 'preemption_count': 0}), (19392, {'train/accuracy': 0.6943359375, 'train/loss': 1.483161449432373, 'validation/accuracy': 0.615619957447052, 'validation/loss': 1.8391530513763428, 'validation/num_examples': 50000, 'test/accuracy': 0.49080002307891846, 'test/loss': 2.465847969055176, 'test/num_examples': 10000, 'score': 6663.387640237808, 'total_duration': 6909.0832579135895, 'accumulated_submission_time': 6663.387640237808, 'accumulated_eval_time': 244.6494562625885, 'accumulated_logging_time': 0.38909482955932617, 'global_step': 19392, 'preemption_count': 0}), (20885, {'train/accuracy': 0.6927216053009033, 'train/loss': 1.4884588718414307, 'validation/accuracy': 0.6250799894332886, 'validation/loss': 1.790726661682129, 'validation/num_examples': 50000, 'test/accuracy': 0.49330002069473267, 'test/loss': 2.453991174697876, 'test/num_examples': 10000, 'score': 7173.381569385529, 'total_duration': 7436.744100093842, 'accumulated_submission_time': 7173.381569385529, 'accumulated_eval_time': 262.23272013664246, 'accumulated_logging_time': 0.42079997062683105, 'global_step': 20885, 'preemption_count': 0}), (22378, {'train/accuracy': 0.6850087642669678, 'train/loss': 1.4967600107192993, 'validation/accuracy': 0.6200599670410156, 'validation/loss': 1.7901036739349365, 'validation/num_examples': 50000, 'test/accuracy': 0.4984000325202942, 'test/loss': 2.4333972930908203, 'test/num_examples': 10000, 'score': 7683.411033153534, 'total_duration': 7964.318654060364, 'accumulated_submission_time': 7683.411033153534, 'accumulated_eval_time': 279.69251894950867, 'accumulated_logging_time': 0.4559054374694824, 'global_step': 22378, 'preemption_count': 0}), (23872, {'train/accuracy': 0.6826769709587097, 'train/loss': 1.5350451469421387, 'validation/accuracy': 0.6202600002288818, 'validation/loss': 1.8089746236801147, 'validation/num_examples': 50000, 'test/accuracy': 0.4937000274658203, 'test/loss': 2.4656965732574463, 'test/num_examples': 10000, 'score': 8193.398950099945, 'total_duration': 8491.685319185257, 'accumulated_submission_time': 8193.398950099945, 'accumulated_eval_time': 296.98755836486816, 'accumulated_logging_time': 0.4871697425842285, 'global_step': 23872, 'preemption_count': 0}), (25367, {'train/accuracy': 0.7011120915412903, 'train/loss': 1.4206559658050537, 'validation/accuracy': 0.6384599804878235, 'validation/loss': 1.7001821994781494, 'validation/num_examples': 50000, 'test/accuracy': 0.5014000535011292, 'test/loss': 2.3606183528900146, 'test/num_examples': 10000, 'score': 8703.517453432083, 'total_duration': 9019.426125764847, 'accumulated_submission_time': 8703.517453432083, 'accumulated_eval_time': 314.5280523300171, 'accumulated_logging_time': 0.5178220272064209, 'global_step': 25367, 'preemption_count': 0}), (26862, {'train/accuracy': 0.6947743892669678, 'train/loss': 1.4827384948730469, 'validation/accuracy': 0.6357600092887878, 'validation/loss': 1.7430270910263062, 'validation/num_examples': 50000, 'test/accuracy': 0.5125000476837158, 'test/loss': 2.392122507095337, 'test/num_examples': 10000, 'score': 9213.776574373245, 'total_duration': 9547.277846574783, 'accumulated_submission_time': 9213.776574373245, 'accumulated_eval_time': 332.03754591941833, 'accumulated_logging_time': 0.5506632328033447, 'global_step': 26862, 'preemption_count': 0}), (28357, {'train/accuracy': 0.7403140664100647, 'train/loss': 1.251942753791809, 'validation/accuracy': 0.6389600038528442, 'validation/loss': 1.6875686645507812, 'validation/num_examples': 50000, 'test/accuracy': 0.515500009059906, 'test/loss': 2.3387906551361084, 'test/num_examples': 10000, 'score': 9723.97894001007, 'total_duration': 10074.980818033218, 'accumulated_submission_time': 9723.97894001007, 'accumulated_eval_time': 349.45311164855957, 'accumulated_logging_time': 0.5828866958618164, 'global_step': 28357, 'preemption_count': 0}), (29851, {'train/accuracy': 0.725984513759613, 'train/loss': 1.2846310138702393, 'validation/accuracy': 0.6454600095748901, 'validation/loss': 1.647182583808899, 'validation/num_examples': 50000, 'test/accuracy': 0.5200999975204468, 'test/loss': 2.3046329021453857, 'test/num_examples': 10000, 'score': 10234.157279968262, 'total_duration': 10602.70266866684, 'accumulated_submission_time': 10234.157279968262, 'accumulated_eval_time': 366.90966606140137, 'accumulated_logging_time': 0.6168107986450195, 'global_step': 29851, 'preemption_count': 0}), (31345, {'train/accuracy': 0.7231544852256775, 'train/loss': 1.3217514753341675, 'validation/accuracy': 0.6491000056266785, 'validation/loss': 1.644545078277588, 'validation/num_examples': 50000, 'test/accuracy': 0.5267000198364258, 'test/loss': 2.2946465015411377, 'test/num_examples': 10000, 'score': 10744.193604707718, 'total_duration': 11130.2699239254, 'accumulated_submission_time': 10744.193604707718, 'accumulated_eval_time': 384.35588550567627, 'accumulated_logging_time': 0.6501708030700684, 'global_step': 31345, 'preemption_count': 0}), (32838, {'train/accuracy': 0.7084861397743225, 'train/loss': 1.379503846168518, 'validation/accuracy': 0.64055997133255, 'validation/loss': 1.693148136138916, 'validation/num_examples': 50000, 'test/accuracy': 0.5159000158309937, 'test/loss': 2.3436386585235596, 'test/num_examples': 10000, 'score': 11254.132838010788, 'total_duration': 11657.91011095047, 'accumulated_submission_time': 11254.132838010788, 'accumulated_eval_time': 401.97428369522095, 'accumulated_logging_time': 0.6830503940582275, 'global_step': 32838, 'preemption_count': 0}), (34333, {'train/accuracy': 0.7168765664100647, 'train/loss': 1.3567454814910889, 'validation/accuracy': 0.6507399678230286, 'validation/loss': 1.6542654037475586, 'validation/num_examples': 50000, 'test/accuracy': 0.524399995803833, 'test/loss': 2.2950656414031982, 'test/num_examples': 10000, 'score': 11764.288525104523, 'total_duration': 12185.538932323456, 'accumulated_submission_time': 11764.288525104523, 'accumulated_eval_time': 419.36046075820923, 'accumulated_logging_time': 0.7189726829528809, 'global_step': 34333, 'preemption_count': 0}), (35828, {'train/accuracy': 0.6993981003761292, 'train/loss': 1.4522290229797363, 'validation/accuracy': 0.6362599730491638, 'validation/loss': 1.7302823066711426, 'validation/num_examples': 50000, 'test/accuracy': 0.5097000002861023, 'test/loss': 2.3925395011901855, 'test/num_examples': 10000, 'score': 12274.358264684677, 'total_duration': 12713.143894910812, 'accumulated_submission_time': 12274.358264684677, 'accumulated_eval_time': 436.809175491333, 'accumulated_logging_time': 0.7555732727050781, 'global_step': 35828, 'preemption_count': 0}), (37323, {'train/accuracy': 0.7249680757522583, 'train/loss': 1.295501708984375, 'validation/accuracy': 0.6560800075531006, 'validation/loss': 1.6071767807006836, 'validation/num_examples': 50000, 'test/accuracy': 0.5236000418663025, 'test/loss': 2.2822113037109375, 'test/num_examples': 10000, 'score': 12784.553744077682, 'total_duration': 13240.775558948517, 'accumulated_submission_time': 12784.553744077682, 'accumulated_eval_time': 454.1600670814514, 'accumulated_logging_time': 0.7909941673278809, 'global_step': 37323, 'preemption_count': 0}), (38818, {'train/accuracy': 0.7355508208274841, 'train/loss': 1.286131739616394, 'validation/accuracy': 0.6459800004959106, 'validation/loss': 1.6857054233551025, 'validation/num_examples': 50000, 'test/accuracy': 0.5186000466346741, 'test/loss': 2.3471288681030273, 'test/num_examples': 10000, 'score': 13294.51098227501, 'total_duration': 13768.348722219467, 'accumulated_submission_time': 13294.51098227501, 'accumulated_eval_time': 471.69002628326416, 'accumulated_logging_time': 0.8257701396942139, 'global_step': 38818, 'preemption_count': 0}), (40313, {'train/accuracy': 0.7302096486091614, 'train/loss': 1.310068964958191, 'validation/accuracy': 0.6537799835205078, 'validation/loss': 1.6593209505081177, 'validation/num_examples': 50000, 'test/accuracy': 0.5261000394821167, 'test/loss': 2.311683416366577, 'test/num_examples': 10000, 'score': 13804.579131126404, 'total_duration': 14296.019186019897, 'accumulated_submission_time': 13804.579131126404, 'accumulated_eval_time': 489.20293831825256, 'accumulated_logging_time': 0.8625240325927734, 'global_step': 40313, 'preemption_count': 0}), (41808, {'train/accuracy': 0.7320830821990967, 'train/loss': 1.2917817831039429, 'validation/accuracy': 0.6575599908828735, 'validation/loss': 1.6185765266418457, 'validation/num_examples': 50000, 'test/accuracy': 0.5362000465393066, 'test/loss': 2.260235071182251, 'test/num_examples': 10000, 'score': 14314.632142066956, 'total_duration': 14824.014763116837, 'accumulated_submission_time': 14314.632142066956, 'accumulated_eval_time': 507.06050848960876, 'accumulated_logging_time': 0.8970699310302734, 'global_step': 41808, 'preemption_count': 0}), (43304, {'train/accuracy': 0.7353116869926453, 'train/loss': 1.2636276483535767, 'validation/accuracy': 0.6688199639320374, 'validation/loss': 1.5687376260757446, 'validation/num_examples': 50000, 'test/accuracy': 0.5380000472068787, 'test/loss': 2.22286057472229, 'test/num_examples': 10000, 'score': 14824.831728935242, 'total_duration': 15351.65698647499, 'accumulated_submission_time': 14824.831728935242, 'accumulated_eval_time': 524.4154839515686, 'accumulated_logging_time': 0.935309648513794, 'global_step': 43304, 'preemption_count': 0}), (44799, {'train/accuracy': 0.7269411683082581, 'train/loss': 1.2829989194869995, 'validation/accuracy': 0.6580599546432495, 'validation/loss': 1.5886094570159912, 'validation/num_examples': 50000, 'test/accuracy': 0.5301000475883484, 'test/loss': 2.2401185035705566, 'test/num_examples': 10000, 'score': 15334.955714941025, 'total_duration': 15879.262532949448, 'accumulated_submission_time': 15334.955714941025, 'accumulated_eval_time': 541.8120038509369, 'accumulated_logging_time': 0.9696609973907471, 'global_step': 44799, 'preemption_count': 0}), (46294, {'train/accuracy': 0.7327606678009033, 'train/loss': 1.280967116355896, 'validation/accuracy': 0.6660400032997131, 'validation/loss': 1.5779304504394531, 'validation/num_examples': 50000, 'test/accuracy': 0.5383000373840332, 'test/loss': 2.2129969596862793, 'test/num_examples': 10000, 'score': 15845.028705358505, 'total_duration': 16406.679964780807, 'accumulated_submission_time': 15845.028705358505, 'accumulated_eval_time': 559.0715284347534, 'accumulated_logging_time': 1.003936529159546, 'global_step': 46294, 'preemption_count': 0}), (47789, {'train/accuracy': 0.7629344463348389, 'train/loss': 1.1510975360870361, 'validation/accuracy': 0.6612600088119507, 'validation/loss': 1.5798641443252563, 'validation/num_examples': 50000, 'test/accuracy': 0.5379000306129456, 'test/loss': 2.240457057952881, 'test/num_examples': 10000, 'score': 16355.009400129318, 'total_duration': 16933.876542568207, 'accumulated_submission_time': 16355.009400129318, 'accumulated_eval_time': 576.2020993232727, 'accumulated_logging_time': 1.0376520156860352, 'global_step': 47789, 'preemption_count': 0}), (49284, {'train/accuracy': 0.7473692297935486, 'train/loss': 1.239794373512268, 'validation/accuracy': 0.6687399744987488, 'validation/loss': 1.5972586870193481, 'validation/num_examples': 50000, 'test/accuracy': 0.5371000170707703, 'test/loss': 2.2499301433563232, 'test/num_examples': 10000, 'score': 16865.14865398407, 'total_duration': 17461.489314556122, 'accumulated_submission_time': 16865.14865398407, 'accumulated_eval_time': 593.5875813961029, 'accumulated_logging_time': 1.0741991996765137, 'global_step': 49284, 'preemption_count': 0}), (50779, {'train/accuracy': 0.7400350570678711, 'train/loss': 1.2261372804641724, 'validation/accuracy': 0.6668199896812439, 'validation/loss': 1.554896354675293, 'validation/num_examples': 50000, 'test/accuracy': 0.5362000465393066, 'test/loss': 2.243440628051758, 'test/num_examples': 10000, 'score': 17375.061665296555, 'total_duration': 17989.708940029144, 'accumulated_submission_time': 17375.061665296555, 'accumulated_eval_time': 611.8048617839813, 'accumulated_logging_time': 1.1133863925933838, 'global_step': 50779, 'preemption_count': 0}), (52275, {'train/accuracy': 0.7399553656578064, 'train/loss': 1.244918942451477, 'validation/accuracy': 0.6628599762916565, 'validation/loss': 1.5738775730133057, 'validation/num_examples': 50000, 'test/accuracy': 0.5449000000953674, 'test/loss': 2.2030839920043945, 'test/num_examples': 10000, 'score': 17885.29244852066, 'total_duration': 18517.295283079147, 'accumulated_submission_time': 17885.29244852066, 'accumulated_eval_time': 629.0761721134186, 'accumulated_logging_time': 1.1479730606079102, 'global_step': 52275, 'preemption_count': 0}), (53771, {'train/accuracy': 0.7357900142669678, 'train/loss': 1.252927303314209, 'validation/accuracy': 0.6644200086593628, 'validation/loss': 1.56941819190979, 'validation/num_examples': 50000, 'test/accuracy': 0.5424000024795532, 'test/loss': 2.2293903827667236, 'test/num_examples': 10000, 'score': 18395.48710179329, 'total_duration': 19044.75901412964, 'accumulated_submission_time': 18395.48710179329, 'accumulated_eval_time': 646.256591796875, 'accumulated_logging_time': 1.1864268779754639, 'global_step': 53771, 'preemption_count': 0}), (55266, {'train/accuracy': 0.7410913705825806, 'train/loss': 1.232458233833313, 'validation/accuracy': 0.6733999848365784, 'validation/loss': 1.5392425060272217, 'validation/num_examples': 50000, 'test/accuracy': 0.5421000123023987, 'test/loss': 2.2132768630981445, 'test/num_examples': 10000, 'score': 18905.508989572525, 'total_duration': 19572.208678483963, 'accumulated_submission_time': 18905.508989572525, 'accumulated_eval_time': 663.5889291763306, 'accumulated_logging_time': 1.2300894260406494, 'global_step': 55266, 'preemption_count': 0}), (56761, {'train/accuracy': 0.7707669138908386, 'train/loss': 1.1692932844161987, 'validation/accuracy': 0.6709399819374084, 'validation/loss': 1.5907434225082397, 'validation/num_examples': 50000, 'test/accuracy': 0.5403000116348267, 'test/loss': 2.260906457901001, 'test/num_examples': 10000, 'score': 19415.54997587204, 'total_duration': 20099.63463830948, 'accumulated_submission_time': 19415.54997587204, 'accumulated_eval_time': 680.8869771957397, 'accumulated_logging_time': 1.26725435256958, 'global_step': 56761, 'preemption_count': 0}), (58256, {'train/accuracy': 0.7694514989852905, 'train/loss': 1.1050457954406738, 'validation/accuracy': 0.6767799854278564, 'validation/loss': 1.4937835931777954, 'validation/num_examples': 50000, 'test/accuracy': 0.5485000014305115, 'test/loss': 2.1709296703338623, 'test/num_examples': 10000, 'score': 19925.609867811203, 'total_duration': 20627.20070528984, 'accumulated_submission_time': 19925.609867811203, 'accumulated_eval_time': 698.3031196594238, 'accumulated_logging_time': 1.3048710823059082, 'global_step': 58256, 'preemption_count': 0}), (59752, {'train/accuracy': 0.7449377775192261, 'train/loss': 1.1936075687408447, 'validation/accuracy': 0.666979968547821, 'validation/loss': 1.5506370067596436, 'validation/num_examples': 50000, 'test/accuracy': 0.5371000170707703, 'test/loss': 2.2205097675323486, 'test/num_examples': 10000, 'score': 20435.80241703987, 'total_duration': 21154.84163069725, 'accumulated_submission_time': 20435.80241703987, 'accumulated_eval_time': 715.6581652164459, 'accumulated_logging_time': 1.3460206985473633, 'global_step': 59752, 'preemption_count': 0}), (61248, {'train/accuracy': 0.7509366869926453, 'train/loss': 1.1979851722717285, 'validation/accuracy': 0.6717399954795837, 'validation/loss': 1.540035605430603, 'validation/num_examples': 50000, 'test/accuracy': 0.5407000184059143, 'test/loss': 2.2069180011749268, 'test/num_examples': 10000, 'score': 20946.015642881393, 'total_duration': 21682.111456632614, 'accumulated_submission_time': 20946.015642881393, 'accumulated_eval_time': 732.6220688819885, 'accumulated_logging_time': 1.3891007900238037, 'global_step': 61248, 'preemption_count': 0}), (62744, {'train/accuracy': 0.7419483065605164, 'train/loss': 1.2212562561035156, 'validation/accuracy': 0.6708199977874756, 'validation/loss': 1.5463603734970093, 'validation/num_examples': 50000, 'test/accuracy': 0.5323000550270081, 'test/loss': 2.2445785999298096, 'test/num_examples': 10000, 'score': 21456.262974262238, 'total_duration': 22209.52569293976, 'accumulated_submission_time': 21456.262974262238, 'accumulated_eval_time': 749.6996276378632, 'accumulated_logging_time': 1.426433801651001, 'global_step': 62744, 'preemption_count': 0}), (64239, {'train/accuracy': 0.7436822056770325, 'train/loss': 1.2639172077178955, 'validation/accuracy': 0.6767199635505676, 'validation/loss': 1.5742335319519043, 'validation/num_examples': 50000, 'test/accuracy': 0.5462000370025635, 'test/loss': 2.255506992340088, 'test/num_examples': 10000, 'score': 21966.35502266884, 'total_duration': 22737.02538251877, 'accumulated_submission_time': 21966.35502266884, 'accumulated_eval_time': 767.0168855190277, 'accumulated_logging_time': 1.464949369430542, 'global_step': 64239, 'preemption_count': 0}), (65735, {'train/accuracy': 0.7479472160339355, 'train/loss': 1.2383085489273071, 'validation/accuracy': 0.6757599711418152, 'validation/loss': 1.5526641607284546, 'validation/num_examples': 50000, 'test/accuracy': 0.5468000173568726, 'test/loss': 2.2133982181549072, 'test/num_examples': 10000, 'score': 22476.510162115097, 'total_duration': 23264.367428541183, 'accumulated_submission_time': 22476.510162115097, 'accumulated_eval_time': 784.1112470626831, 'accumulated_logging_time': 1.5077550411224365, 'global_step': 65735, 'preemption_count': 0}), (67230, {'train/accuracy': 0.7716238498687744, 'train/loss': 1.069993019104004, 'validation/accuracy': 0.6693199872970581, 'validation/loss': 1.5192725658416748, 'validation/num_examples': 50000, 'test/accuracy': 0.5466000437736511, 'test/loss': 2.168231725692749, 'test/num_examples': 10000, 'score': 22986.69782590866, 'total_duration': 23791.52751684189, 'accumulated_submission_time': 22986.69782590866, 'accumulated_eval_time': 800.9966917037964, 'accumulated_logging_time': 1.5449728965759277, 'global_step': 67230, 'preemption_count': 0}), (68726, {'train/accuracy': 0.7636120915412903, 'train/loss': 1.1651519536972046, 'validation/accuracy': 0.6775799989700317, 'validation/loss': 1.5437074899673462, 'validation/num_examples': 50000, 'test/accuracy': 0.5540000200271606, 'test/loss': 2.1980412006378174, 'test/num_examples': 10000, 'score': 23496.80472302437, 'total_duration': 24318.850444078445, 'accumulated_submission_time': 23496.80472302437, 'accumulated_eval_time': 818.1228411197662, 'accumulated_logging_time': 1.5834143161773682, 'global_step': 68726, 'preemption_count': 0}), (70221, {'train/accuracy': 0.7656847834587097, 'train/loss': 1.155431866645813, 'validation/accuracy': 0.68367999792099, 'validation/loss': 1.5203362703323364, 'validation/num_examples': 50000, 'test/accuracy': 0.5615000128746033, 'test/loss': 2.1426637172698975, 'test/num_examples': 10000, 'score': 24006.80624294281, 'total_duration': 24846.14938569069, 'accumulated_submission_time': 24006.80624294281, 'accumulated_eval_time': 835.3253185749054, 'accumulated_logging_time': 1.6262106895446777, 'global_step': 70221, 'preemption_count': 0}), (71717, {'train/accuracy': 0.770527720451355, 'train/loss': 1.1102607250213623, 'validation/accuracy': 0.6875999569892883, 'validation/loss': 1.4715756177902222, 'validation/num_examples': 50000, 'test/accuracy': 0.55840003490448, 'test/loss': 2.1221847534179688, 'test/num_examples': 10000, 'score': 24516.862579345703, 'total_duration': 25373.404136896133, 'accumulated_submission_time': 24516.862579345703, 'accumulated_eval_time': 852.434784412384, 'accumulated_logging_time': 1.6647412776947021, 'global_step': 71717, 'preemption_count': 0}), (73212, {'train/accuracy': 0.7600645422935486, 'train/loss': 1.1490471363067627, 'validation/accuracy': 0.6841599941253662, 'validation/loss': 1.4841029644012451, 'validation/num_examples': 50000, 'test/accuracy': 0.5588000416755676, 'test/loss': 2.1481716632843018, 'test/num_examples': 10000, 'score': 25026.77587389946, 'total_duration': 25900.518624067307, 'accumulated_submission_time': 25026.77587389946, 'accumulated_eval_time': 869.5368921756744, 'accumulated_logging_time': 1.7121210098266602, 'global_step': 73212, 'preemption_count': 0}), (74708, {'train/accuracy': 0.7657246589660645, 'train/loss': 1.1162415742874146, 'validation/accuracy': 0.6844599843025208, 'validation/loss': 1.4583039283752441, 'validation/num_examples': 50000, 'test/accuracy': 0.5622000098228455, 'test/loss': 2.1055116653442383, 'test/num_examples': 10000, 'score': 25536.89725470543, 'total_duration': 26427.82671189308, 'accumulated_submission_time': 25536.89725470543, 'accumulated_eval_time': 886.6352317333221, 'accumulated_logging_time': 1.7510671615600586, 'global_step': 74708, 'preemption_count': 0}), (76203, {'train/accuracy': 0.8078364133834839, 'train/loss': 1.0016721487045288, 'validation/accuracy': 0.6889399886131287, 'validation/loss': 1.5002516508102417, 'validation/num_examples': 50000, 'test/accuracy': 0.5706000328063965, 'test/loss': 2.115853786468506, 'test/num_examples': 10000, 'score': 26046.908839941025, 'total_duration': 26954.93002486229, 'accumulated_submission_time': 26046.908839941025, 'accumulated_eval_time': 903.6346333026886, 'accumulated_logging_time': 1.7926886081695557, 'global_step': 76203, 'preemption_count': 0}), (77699, {'train/accuracy': 0.786551296710968, 'train/loss': 1.0268144607543945, 'validation/accuracy': 0.6879599690437317, 'validation/loss': 1.454639196395874, 'validation/num_examples': 50000, 'test/accuracy': 0.5588000416755676, 'test/loss': 2.096848964691162, 'test/num_examples': 10000, 'score': 26557.141659975052, 'total_duration': 27482.33225798607, 'accumulated_submission_time': 26557.141659975052, 'accumulated_eval_time': 920.7117989063263, 'accumulated_logging_time': 1.8335063457489014, 'global_step': 77699, 'preemption_count': 0}), (79194, {'train/accuracy': 0.7811902165412903, 'train/loss': 1.058087706565857, 'validation/accuracy': 0.6916999816894531, 'validation/loss': 1.4431703090667725, 'validation/num_examples': 50000, 'test/accuracy': 0.5654000043869019, 'test/loss': 2.102102756500244, 'test/num_examples': 10000, 'score': 27067.081208705902, 'total_duration': 28009.725796461105, 'accumulated_submission_time': 27067.081208705902, 'accumulated_eval_time': 938.072808265686, 'accumulated_logging_time': 1.8744986057281494, 'global_step': 79194, 'preemption_count': 0}), (80690, {'train/accuracy': 0.7735171914100647, 'train/loss': 1.107993483543396, 'validation/accuracy': 0.6871599555015564, 'validation/loss': 1.4867323637008667, 'validation/num_examples': 50000, 'test/accuracy': 0.5612000226974487, 'test/loss': 2.126382827758789, 'test/num_examples': 10000, 'score': 27577.212859630585, 'total_duration': 28537.874395132065, 'accumulated_submission_time': 27577.212859630585, 'accumulated_eval_time': 955.9951276779175, 'accumulated_logging_time': 1.9179785251617432, 'global_step': 80690, 'preemption_count': 0}), (82186, {'train/accuracy': 0.7631337642669678, 'train/loss': 1.1587564945220947, 'validation/accuracy': 0.6825199723243713, 'validation/loss': 1.5233628749847412, 'validation/num_examples': 50000, 'test/accuracy': 0.5517000555992126, 'test/loss': 2.1907894611358643, 'test/num_examples': 10000, 'score': 28087.347998142242, 'total_duration': 29065.300013542175, 'accumulated_submission_time': 28087.347998142242, 'accumulated_eval_time': 973.1926457881927, 'accumulated_logging_time': 1.9603497982025146, 'global_step': 82186, 'preemption_count': 0}), (83680, {'train/accuracy': 0.7662228941917419, 'train/loss': 1.1387059688568115, 'validation/accuracy': 0.6846799850463867, 'validation/loss': 1.4886494874954224, 'validation/num_examples': 50000, 'test/accuracy': 0.5542000532150269, 'test/loss': 2.152329683303833, 'test/num_examples': 10000, 'score': 28597.258448839188, 'total_duration': 29592.47763466835, 'accumulated_submission_time': 28597.258448839188, 'accumulated_eval_time': 990.360454082489, 'accumulated_logging_time': 2.00658917427063, 'global_step': 83680, 'preemption_count': 0}), (85176, {'train/accuracy': 0.7777224183082581, 'train/loss': 1.1008814573287964, 'validation/accuracy': 0.6924399733543396, 'validation/loss': 1.4702638387680054, 'validation/num_examples': 50000, 'test/accuracy': 0.5660000443458557, 'test/loss': 2.104079246520996, 'test/num_examples': 10000, 'score': 29107.465238809586, 'total_duration': 30120.14112353325, 'accumulated_submission_time': 29107.465238809586, 'accumulated_eval_time': 1007.7248740196228, 'accumulated_logging_time': 2.047886610031128, 'global_step': 85176, 'preemption_count': 0}), (86672, {'train/accuracy': 0.7954002022743225, 'train/loss': 1.0093271732330322, 'validation/accuracy': 0.6930800080299377, 'validation/loss': 1.4499201774597168, 'validation/num_examples': 50000, 'test/accuracy': 0.5685000419616699, 'test/loss': 2.0924246311187744, 'test/num_examples': 10000, 'score': 29617.56979894638, 'total_duration': 30647.5791118145, 'accumulated_submission_time': 29617.56979894638, 'accumulated_eval_time': 1024.9641880989075, 'accumulated_logging_time': 2.0917553901672363, 'global_step': 86672, 'preemption_count': 0}), (88167, {'train/accuracy': 0.7859733700752258, 'train/loss': 1.0578300952911377, 'validation/accuracy': 0.693399965763092, 'validation/loss': 1.460898995399475, 'validation/num_examples': 50000, 'test/accuracy': 0.5648000240325928, 'test/loss': 2.1210591793060303, 'test/num_examples': 10000, 'score': 30127.56591463089, 'total_duration': 31175.094157218933, 'accumulated_submission_time': 30127.56591463089, 'accumulated_eval_time': 1042.3706114292145, 'accumulated_logging_time': 2.1539254188537598, 'global_step': 88167, 'preemption_count': 0}), (89662, {'train/accuracy': 0.7878866195678711, 'train/loss': 1.0814951658248901, 'validation/accuracy': 0.6979999542236328, 'validation/loss': 1.4614171981811523, 'validation/num_examples': 50000, 'test/accuracy': 0.5674000382423401, 'test/loss': 2.1174557209014893, 'test/num_examples': 10000, 'score': 30637.575980186462, 'total_duration': 31702.41963338852, 'accumulated_submission_time': 30637.575980186462, 'accumulated_eval_time': 1059.5920572280884, 'accumulated_logging_time': 2.1985137462615967, 'global_step': 89662, 'preemption_count': 0}), (91158, {'train/accuracy': 0.7898397445678711, 'train/loss': 1.0304040908813477, 'validation/accuracy': 0.6977599859237671, 'validation/loss': 1.4194663763046265, 'validation/num_examples': 50000, 'test/accuracy': 0.5745000243186951, 'test/loss': 2.0515968799591064, 'test/num_examples': 10000, 'score': 31147.80820083618, 'total_duration': 32229.963414669037, 'accumulated_submission_time': 31147.80820083618, 'accumulated_eval_time': 1076.8009662628174, 'accumulated_logging_time': 2.2499001026153564, 'global_step': 91158, 'preemption_count': 0}), (92654, {'train/accuracy': 0.7855349183082581, 'train/loss': 1.0370293855667114, 'validation/accuracy': 0.6988399624824524, 'validation/loss': 1.4155194759368896, 'validation/num_examples': 50000, 'test/accuracy': 0.5738000273704529, 'test/loss': 2.0675852298736572, 'test/num_examples': 10000, 'score': 31657.907905578613, 'total_duration': 32757.361146211624, 'accumulated_submission_time': 31657.907905578613, 'accumulated_eval_time': 1094.0021858215332, 'accumulated_logging_time': 2.295071840286255, 'global_step': 92654, 'preemption_count': 0}), (94150, {'train/accuracy': 0.7915935516357422, 'train/loss': 1.011091709136963, 'validation/accuracy': 0.7041199803352356, 'validation/loss': 1.3878302574157715, 'validation/num_examples': 50000, 'test/accuracy': 0.5834000110626221, 'test/loss': 2.04312801361084, 'test/num_examples': 10000, 'score': 32168.143102407455, 'total_duration': 33284.76044559479, 'accumulated_submission_time': 32168.143102407455, 'accumulated_eval_time': 1111.0696558952332, 'accumulated_logging_time': 2.34027099609375, 'global_step': 94150, 'preemption_count': 0}), (95646, {'train/accuracy': 0.8151108026504517, 'train/loss': 0.9238508343696594, 'validation/accuracy': 0.7001399993896484, 'validation/loss': 1.4183787107467651, 'validation/num_examples': 50000, 'test/accuracy': 0.570900022983551, 'test/loss': 2.061049699783325, 'test/num_examples': 10000, 'score': 32678.294413089752, 'total_duration': 33812.28715801239, 'accumulated_submission_time': 32678.294413089752, 'accumulated_eval_time': 1128.321323633194, 'accumulated_logging_time': 2.4101314544677734, 'global_step': 95646, 'preemption_count': 0}), (97141, {'train/accuracy': 0.8090322017669678, 'train/loss': 0.9489533305168152, 'validation/accuracy': 0.7054199576377869, 'validation/loss': 1.3887578248977661, 'validation/num_examples': 50000, 'test/accuracy': 0.5785000324249268, 'test/loss': 2.047945022583008, 'test/num_examples': 10000, 'score': 33188.26053261757, 'total_duration': 34339.3632774353, 'accumulated_submission_time': 33188.26053261757, 'accumulated_eval_time': 1145.3379135131836, 'accumulated_logging_time': 2.4528286457061768, 'global_step': 97141, 'preemption_count': 0}), (98637, {'train/accuracy': 0.8087332248687744, 'train/loss': 0.9831731915473938, 'validation/accuracy': 0.7090199589729309, 'validation/loss': 1.404129147529602, 'validation/num_examples': 50000, 'test/accuracy': 0.5819000005722046, 'test/loss': 2.0445165634155273, 'test/num_examples': 10000, 'score': 33698.45007276535, 'total_duration': 34867.06263709068, 'accumulated_submission_time': 33698.45007276535, 'accumulated_eval_time': 1162.7417826652527, 'accumulated_logging_time': 2.5040063858032227, 'global_step': 98637, 'preemption_count': 0}), (100132, {'train/accuracy': 0.7820471525192261, 'train/loss': 1.040468454360962, 'validation/accuracy': 0.687720000743866, 'validation/loss': 1.4538909196853638, 'validation/num_examples': 50000, 'test/accuracy': 0.5609000325202942, 'test/loss': 2.109252691268921, 'test/num_examples': 10000, 'score': 34208.44733428955, 'total_duration': 35394.59343075752, 'accumulated_submission_time': 34208.44733428955, 'accumulated_eval_time': 1180.1750495433807, 'accumulated_logging_time': 2.553105354309082, 'global_step': 100132, 'preemption_count': 0}), (101628, {'train/accuracy': 0.8019172549247742, 'train/loss': 0.9545334577560425, 'validation/accuracy': 0.7048199772834778, 'validation/loss': 1.3737319707870483, 'validation/num_examples': 50000, 'test/accuracy': 0.5788000226020813, 'test/loss': 2.02254056930542, 'test/num_examples': 10000, 'score': 34718.620924949646, 'total_duration': 35922.155831575394, 'accumulated_submission_time': 34718.620924949646, 'accumulated_eval_time': 1197.4660267829895, 'accumulated_logging_time': 2.60019588470459, 'global_step': 101628, 'preemption_count': 0}), (103124, {'train/accuracy': 0.8040497303009033, 'train/loss': 0.9926905035972595, 'validation/accuracy': 0.7096999883651733, 'validation/loss': 1.3941349983215332, 'validation/num_examples': 50000, 'test/accuracy': 0.5879000425338745, 'test/loss': 2.022136926651001, 'test/num_examples': 10000, 'score': 35228.76759457588, 'total_duration': 36449.7290225029, 'accumulated_submission_time': 35228.76759457588, 'accumulated_eval_time': 1214.7932357788086, 'accumulated_logging_time': 2.648162364959717, 'global_step': 103124, 'preemption_count': 0}), (104620, {'train/accuracy': 0.8269292116165161, 'train/loss': 0.9147905707359314, 'validation/accuracy': 0.7056399583816528, 'validation/loss': 1.4107757806777954, 'validation/num_examples': 50000, 'test/accuracy': 0.5788000226020813, 'test/loss': 2.058584213256836, 'test/num_examples': 10000, 'score': 35738.865753889084, 'total_duration': 36977.661039590836, 'accumulated_submission_time': 35738.865753889084, 'accumulated_eval_time': 1232.5322148799896, 'accumulated_logging_time': 2.6931216716766357, 'global_step': 104620, 'preemption_count': 0}), (106116, {'train/accuracy': 0.8301976919174194, 'train/loss': 0.8830342292785645, 'validation/accuracy': 0.711359977722168, 'validation/loss': 1.3829030990600586, 'validation/num_examples': 50000, 'test/accuracy': 0.5855000019073486, 'test/loss': 2.002440929412842, 'test/num_examples': 10000, 'score': 36249.07039260864, 'total_duration': 37505.13443374634, 'accumulated_submission_time': 36249.07039260864, 'accumulated_eval_time': 1249.7009994983673, 'accumulated_logging_time': 2.7423999309539795, 'global_step': 106116, 'preemption_count': 0}), (107612, {'train/accuracy': 0.8184390664100647, 'train/loss': 0.9219950437545776, 'validation/accuracy': 0.7115199565887451, 'validation/loss': 1.3807446956634521, 'validation/num_examples': 50000, 'test/accuracy': 0.5803000330924988, 'test/loss': 2.026761054992676, 'test/num_examples': 10000, 'score': 36759.14828467369, 'total_duration': 38032.58826184273, 'accumulated_submission_time': 36759.14828467369, 'accumulated_eval_time': 1266.9804100990295, 'accumulated_logging_time': 2.787186861038208, 'global_step': 107612, 'preemption_count': 0}), (109107, {'train/accuracy': 0.8204121589660645, 'train/loss': 0.8763977885246277, 'validation/accuracy': 0.7163999676704407, 'validation/loss': 1.3261253833770752, 'validation/num_examples': 50000, 'test/accuracy': 0.5889000296592712, 'test/loss': 1.9659696817398071, 'test/num_examples': 10000, 'score': 37269.2513358593, 'total_duration': 38559.98116540909, 'accumulated_submission_time': 37269.2513358593, 'accumulated_eval_time': 1284.1733181476593, 'accumulated_logging_time': 2.833984613418579, 'global_step': 109107, 'preemption_count': 0}), (110603, {'train/accuracy': 0.8202527165412903, 'train/loss': 0.907325804233551, 'validation/accuracy': 0.7150799632072449, 'validation/loss': 1.3554683923721313, 'validation/num_examples': 50000, 'test/accuracy': 0.5920000076293945, 'test/loss': 1.9924134016036987, 'test/num_examples': 10000, 'score': 37779.28477668762, 'total_duration': 39087.46141552925, 'accumulated_submission_time': 37779.28477668762, 'accumulated_eval_time': 1301.520435810089, 'accumulated_logging_time': 2.8837478160858154, 'global_step': 110603, 'preemption_count': 0}), (112099, {'train/accuracy': 0.8202327489852905, 'train/loss': 0.9242339134216309, 'validation/accuracy': 0.7185199856758118, 'validation/loss': 1.3499921560287476, 'validation/num_examples': 50000, 'test/accuracy': 0.5906000137329102, 'test/loss': 2.007988452911377, 'test/num_examples': 10000, 'score': 38289.51672291756, 'total_duration': 39615.02745246887, 'accumulated_submission_time': 38289.51672291756, 'accumulated_eval_time': 1318.7552318572998, 'accumulated_logging_time': 2.933572769165039, 'global_step': 112099, 'preemption_count': 0}), (113594, {'train/accuracy': 0.8160673975944519, 'train/loss': 0.9513839483261108, 'validation/accuracy': 0.7136200070381165, 'validation/loss': 1.3845340013504028, 'validation/num_examples': 50000, 'test/accuracy': 0.5892000198364258, 'test/loss': 2.0263185501098633, 'test/num_examples': 10000, 'score': 38799.4973552227, 'total_duration': 40142.33880519867, 'accumulated_submission_time': 38799.4973552227, 'accumulated_eval_time': 1335.9850897789001, 'accumulated_logging_time': 2.9843294620513916, 'global_step': 113594, 'preemption_count': 0}), (115090, {'train/accuracy': 0.8474569320678711, 'train/loss': 0.7873832583427429, 'validation/accuracy': 0.7181199789047241, 'validation/loss': 1.3268829584121704, 'validation/num_examples': 50000, 'test/accuracy': 0.5927000045776367, 'test/loss': 1.9718732833862305, 'test/num_examples': 10000, 'score': 39309.721982717514, 'total_duration': 40670.07956337929, 'accumulated_submission_time': 39309.721982717514, 'accumulated_eval_time': 1353.4041435718536, 'accumulated_logging_time': 3.0307705402374268, 'global_step': 115090, 'preemption_count': 0}), (116586, {'train/accuracy': 0.8415776491165161, 'train/loss': 0.8181595206260681, 'validation/accuracy': 0.7233999967575073, 'validation/loss': 1.3208779096603394, 'validation/num_examples': 50000, 'test/accuracy': 0.5944000482559204, 'test/loss': 1.96568763256073, 'test/num_examples': 10000, 'score': 39819.89853024483, 'total_duration': 41197.698704242706, 'accumulated_submission_time': 39819.89853024483, 'accumulated_eval_time': 1370.7485435009003, 'accumulated_logging_time': 3.077587127685547, 'global_step': 116586, 'preemption_count': 0}), (118081, {'train/accuracy': 0.8370934128761292, 'train/loss': 0.8302208185195923, 'validation/accuracy': 0.7227999567985535, 'validation/loss': 1.316156268119812, 'validation/num_examples': 50000, 'test/accuracy': 0.5999000072479248, 'test/loss': 1.953519582748413, 'test/num_examples': 10000, 'score': 40329.83609485626, 'total_duration': 41725.5061340332, 'accumulated_submission_time': 40329.83609485626, 'accumulated_eval_time': 1388.5198497772217, 'accumulated_logging_time': 3.126605749130249, 'global_step': 118081, 'preemption_count': 0}), (119577, {'train/accuracy': 0.8384885191917419, 'train/loss': 0.8248199224472046, 'validation/accuracy': 0.7274799942970276, 'validation/loss': 1.3024871349334717, 'validation/num_examples': 50000, 'test/accuracy': 0.5933000445365906, 'test/loss': 1.958518624305725, 'test/num_examples': 10000, 'score': 40839.775539159775, 'total_duration': 42252.89182281494, 'accumulated_submission_time': 40839.775539159775, 'accumulated_eval_time': 1405.8627269268036, 'accumulated_logging_time': 3.179180860519409, 'global_step': 119577, 'preemption_count': 0}), (121073, {'train/accuracy': 0.8438097834587097, 'train/loss': 0.8003804683685303, 'validation/accuracy': 0.7287399768829346, 'validation/loss': 1.283615231513977, 'validation/num_examples': 50000, 'test/accuracy': 0.6027000546455383, 'test/loss': 1.9129961729049683, 'test/num_examples': 10000, 'score': 41350.00688409805, 'total_duration': 42780.44771409035, 'accumulated_submission_time': 41350.00688409805, 'accumulated_eval_time': 1423.086817741394, 'accumulated_logging_time': 3.2281734943389893, 'global_step': 121073, 'preemption_count': 0}), (122569, {'train/accuracy': 0.832051157951355, 'train/loss': 0.8912011384963989, 'validation/accuracy': 0.7217599749565125, 'validation/loss': 1.3543952703475952, 'validation/num_examples': 50000, 'test/accuracy': 0.5974000096321106, 'test/loss': 1.9827086925506592, 'test/num_examples': 10000, 'score': 41860.036199092865, 'total_duration': 43307.74064588547, 'accumulated_submission_time': 41860.036199092865, 'accumulated_eval_time': 1440.245083808899, 'accumulated_logging_time': 3.281466007232666, 'global_step': 122569, 'preemption_count': 0}), (124065, {'train/accuracy': 0.8768334984779358, 'train/loss': 0.6922958493232727, 'validation/accuracy': 0.7325199842453003, 'validation/loss': 1.2779021263122559, 'validation/num_examples': 50000, 'test/accuracy': 0.6080000400543213, 'test/loss': 1.9164098501205444, 'test/num_examples': 10000, 'score': 42370.23780179024, 'total_duration': 43835.2552819252, 'accumulated_submission_time': 42370.23780179024, 'accumulated_eval_time': 1457.4404020309448, 'accumulated_logging_time': 3.34682035446167, 'global_step': 124065, 'preemption_count': 0}), (125561, {'train/accuracy': 0.857421875, 'train/loss': 0.7503759264945984, 'validation/accuracy': 0.7276999950408936, 'validation/loss': 1.2880154848098755, 'validation/num_examples': 50000, 'test/accuracy': 0.6013000011444092, 'test/loss': 1.927111268043518, 'test/num_examples': 10000, 'score': 42880.30663514137, 'total_duration': 44362.843577861786, 'accumulated_submission_time': 42880.30663514137, 'accumulated_eval_time': 1474.8576436042786, 'accumulated_logging_time': 3.398554563522339, 'global_step': 125561, 'preemption_count': 0}), (127057, {'train/accuracy': 0.8565648794174194, 'train/loss': 0.7832421064376831, 'validation/accuracy': 0.7305200099945068, 'validation/loss': 1.3154152631759644, 'validation/num_examples': 50000, 'test/accuracy': 0.6014000177383423, 'test/loss': 1.9597022533416748, 'test/num_examples': 10000, 'score': 43390.468705654144, 'total_duration': 44890.360988378525, 'accumulated_submission_time': 43390.468705654144, 'accumulated_eval_time': 1492.1135189533234, 'accumulated_logging_time': 3.44738507270813, 'global_step': 127057, 'preemption_count': 0}), (128553, {'train/accuracy': 0.8598732352256775, 'train/loss': 0.7385032773017883, 'validation/accuracy': 0.7359399795532227, 'validation/loss': 1.2580225467681885, 'validation/num_examples': 50000, 'test/accuracy': 0.6163000464439392, 'test/loss': 1.8852897882461548, 'test/num_examples': 10000, 'score': 43900.603063344955, 'total_duration': 45418.12388706207, 'accumulated_submission_time': 43900.603063344955, 'accumulated_eval_time': 1509.6416466236115, 'accumulated_logging_time': 3.495047092437744, 'global_step': 128553, 'preemption_count': 0}), (130048, {'train/accuracy': 0.8598732352256775, 'train/loss': 0.7397308349609375, 'validation/accuracy': 0.7367599606513977, 'validation/loss': 1.2535072565078735, 'validation/num_examples': 50000, 'test/accuracy': 0.6109000444412231, 'test/loss': 1.88594388961792, 'test/num_examples': 10000, 'score': 44410.62669610977, 'total_duration': 45945.561729192734, 'accumulated_submission_time': 44410.62669610977, 'accumulated_eval_time': 1526.9550709724426, 'accumulated_logging_time': 3.543776512145996, 'global_step': 130048, 'preemption_count': 0}), (131543, {'train/accuracy': 0.8572624325752258, 'train/loss': 0.7386617660522461, 'validation/accuracy': 0.7386400103569031, 'validation/loss': 1.2487965822219849, 'validation/num_examples': 50000, 'test/accuracy': 0.613800048828125, 'test/loss': 1.8877145051956177, 'test/num_examples': 10000, 'score': 44920.61176490784, 'total_duration': 46472.78757691383, 'accumulated_submission_time': 44920.61176490784, 'accumulated_eval_time': 1544.0944755077362, 'accumulated_logging_time': 3.5926764011383057, 'global_step': 131543, 'preemption_count': 0}), (133037, {'train/accuracy': 0.866609513759613, 'train/loss': 0.7195022106170654, 'validation/accuracy': 0.73881995677948, 'validation/loss': 1.249531865119934, 'validation/num_examples': 50000, 'test/accuracy': 0.613800048828125, 'test/loss': 1.8898125886917114, 'test/num_examples': 10000, 'score': 45430.52412056923, 'total_duration': 46999.9243516922, 'accumulated_submission_time': 45430.52412056923, 'accumulated_eval_time': 1561.2148640155792, 'accumulated_logging_time': 3.644584894180298, 'global_step': 133037, 'preemption_count': 0}), (134533, {'train/accuracy': 0.8819156289100647, 'train/loss': 0.64863520860672, 'validation/accuracy': 0.7436599731445312, 'validation/loss': 1.2403019666671753, 'validation/num_examples': 50000, 'test/accuracy': 0.6176000237464905, 'test/loss': 1.8791606426239014, 'test/num_examples': 10000, 'score': 45940.66190671921, 'total_duration': 47527.43191242218, 'accumulated_submission_time': 45940.66190671921, 'accumulated_eval_time': 1578.4732689857483, 'accumulated_logging_time': 3.7044129371643066, 'global_step': 134533, 'preemption_count': 0}), (136029, {'train/accuracy': 0.8819355964660645, 'train/loss': 0.6802816987037659, 'validation/accuracy': 0.7430999875068665, 'validation/loss': 1.2458999156951904, 'validation/num_examples': 50000, 'test/accuracy': 0.6157000064849854, 'test/loss': 1.8828948736190796, 'test/num_examples': 10000, 'score': 46450.88255214691, 'total_duration': 48054.984679460526, 'accumulated_submission_time': 46450.88255214691, 'accumulated_eval_time': 1595.7042515277863, 'accumulated_logging_time': 3.7541310787200928, 'global_step': 136029, 'preemption_count': 0}), (137525, {'train/accuracy': 0.8787667155265808, 'train/loss': 0.682250440120697, 'validation/accuracy': 0.7416799664497375, 'validation/loss': 1.249807357788086, 'validation/num_examples': 50000, 'test/accuracy': 0.6220000386238098, 'test/loss': 1.8697481155395508, 'test/num_examples': 10000, 'score': 46961.05339837074, 'total_duration': 48582.43872284889, 'accumulated_submission_time': 46961.05339837074, 'accumulated_eval_time': 1612.8796932697296, 'accumulated_logging_time': 3.8113210201263428, 'global_step': 137525, 'preemption_count': 0}), (139020, {'train/accuracy': 0.8798628449440002, 'train/loss': 0.6878957748413086, 'validation/accuracy': 0.7449600100517273, 'validation/loss': 1.2519656419754028, 'validation/num_examples': 50000, 'test/accuracy': 0.6232000589370728, 'test/loss': 1.8762387037277222, 'test/num_examples': 10000, 'score': 47471.025155067444, 'total_duration': 49109.65690302849, 'accumulated_submission_time': 47471.025155067444, 'accumulated_eval_time': 1630.022828578949, 'accumulated_logging_time': 3.8632400035858154, 'global_step': 139020, 'preemption_count': 0}), (140516, {'train/accuracy': 0.8801219463348389, 'train/loss': 0.6773303151130676, 'validation/accuracy': 0.742859959602356, 'validation/loss': 1.2454878091812134, 'validation/num_examples': 50000, 'test/accuracy': 0.6203000545501709, 'test/loss': 1.8631223440170288, 'test/num_examples': 10000, 'score': 47981.19206619263, 'total_duration': 49637.203291893005, 'accumulated_submission_time': 47981.19206619263, 'accumulated_eval_time': 1647.2985713481903, 'accumulated_logging_time': 3.9131062030792236, 'global_step': 140516, 'preemption_count': 0}), (142011, {'train/accuracy': 0.8788862824440002, 'train/loss': 0.6838304996490479, 'validation/accuracy': 0.7432599663734436, 'validation/loss': 1.2508834600448608, 'validation/num_examples': 50000, 'test/accuracy': 0.6175000071525574, 'test/loss': 1.8836888074874878, 'test/num_examples': 10000, 'score': 48491.136139154434, 'total_duration': 50164.43881726265, 'accumulated_submission_time': 48491.136139154434, 'accumulated_eval_time': 1664.4843318462372, 'accumulated_logging_time': 3.967681646347046, 'global_step': 142011, 'preemption_count': 0}), (143507, {'train/accuracy': 0.9057118892669678, 'train/loss': 0.5698558688163757, 'validation/accuracy': 0.7466599941253662, 'validation/loss': 1.2103779315948486, 'validation/num_examples': 50000, 'test/accuracy': 0.6228000521659851, 'test/loss': 1.8483272790908813, 'test/num_examples': 10000, 'score': 49001.24908995628, 'total_duration': 50691.854425907135, 'accumulated_submission_time': 49001.24908995628, 'accumulated_eval_time': 1681.6815202236176, 'accumulated_logging_time': 4.021223306655884, 'global_step': 143507, 'preemption_count': 0}), (145002, {'train/accuracy': 0.8981983065605164, 'train/loss': 0.6036162972450256, 'validation/accuracy': 0.7472599744796753, 'validation/loss': 1.228808045387268, 'validation/num_examples': 50000, 'test/accuracy': 0.6213000416755676, 'test/loss': 1.8598668575286865, 'test/num_examples': 10000, 'score': 49511.237023591995, 'total_duration': 51219.37195444107, 'accumulated_submission_time': 49511.237023591995, 'accumulated_eval_time': 1699.1049826145172, 'accumulated_logging_time': 4.07612681388855, 'global_step': 145002, 'preemption_count': 0}), (146498, {'train/accuracy': 0.8981983065605164, 'train/loss': 0.5911901593208313, 'validation/accuracy': 0.7471599578857422, 'validation/loss': 1.2115702629089355, 'validation/num_examples': 50000, 'test/accuracy': 0.6213000416755676, 'test/loss': 1.8459560871124268, 'test/num_examples': 10000, 'score': 50021.47194981575, 'total_duration': 51746.86004567146, 'accumulated_submission_time': 50021.47194981575, 'accumulated_eval_time': 1716.2525057792664, 'accumulated_logging_time': 4.129936933517456, 'global_step': 146498, 'preemption_count': 0}), (147993, {'train/accuracy': 0.8964046239852905, 'train/loss': 0.6049222350120544, 'validation/accuracy': 0.7515599727630615, 'validation/loss': 1.208533525466919, 'validation/num_examples': 50000, 'test/accuracy': 0.6234000325202942, 'test/loss': 1.849199652671814, 'test/num_examples': 10000, 'score': 50531.48566865921, 'total_duration': 52274.354078531265, 'accumulated_submission_time': 50531.48566865921, 'accumulated_eval_time': 1733.6255042552948, 'accumulated_logging_time': 4.185802459716797, 'global_step': 147993, 'preemption_count': 0}), (149489, {'train/accuracy': 0.899832546710968, 'train/loss': 0.6207277774810791, 'validation/accuracy': 0.7516599893569946, 'validation/loss': 1.2279235124588013, 'validation/num_examples': 50000, 'test/accuracy': 0.6240000128746033, 'test/loss': 1.8540517091751099, 'test/num_examples': 10000, 'score': 51041.4313583374, 'total_duration': 52801.885820388794, 'accumulated_submission_time': 51041.4313583374, 'accumulated_eval_time': 1751.1035561561584, 'accumulated_logging_time': 4.243193626403809, 'global_step': 149489, 'preemption_count': 0}), (150984, {'train/accuracy': 0.9048349857330322, 'train/loss': 0.5874025821685791, 'validation/accuracy': 0.7528600096702576, 'validation/loss': 1.2086260318756104, 'validation/num_examples': 50000, 'test/accuracy': 0.6312000155448914, 'test/loss': 1.8340696096420288, 'test/num_examples': 10000, 'score': 51551.43528985977, 'total_duration': 53330.150327920914, 'accumulated_submission_time': 51551.43528985977, 'accumulated_eval_time': 1769.2537994384766, 'accumulated_logging_time': 4.300515413284302, 'global_step': 150984, 'preemption_count': 0}), (152480, {'train/accuracy': 0.9223333597183228, 'train/loss': 0.5322718024253845, 'validation/accuracy': 0.7513200044631958, 'validation/loss': 1.214342713356018, 'validation/num_examples': 50000, 'test/accuracy': 0.6295000314712524, 'test/loss': 1.8381295204162598, 'test/num_examples': 10000, 'score': 52061.50852203369, 'total_duration': 53857.58398604393, 'accumulated_submission_time': 52061.50852203369, 'accumulated_eval_time': 1786.5181086063385, 'accumulated_logging_time': 4.345062255859375, 'global_step': 152480, 'preemption_count': 0}), (153975, {'train/accuracy': 0.9167131781578064, 'train/loss': 0.5371339917182922, 'validation/accuracy': 0.7554799914360046, 'validation/loss': 1.2029668092727661, 'validation/num_examples': 50000, 'test/accuracy': 0.6319000124931335, 'test/loss': 1.8418346643447876, 'test/num_examples': 10000, 'score': 52571.40790128708, 'total_duration': 54384.898052453995, 'accumulated_submission_time': 52571.40790128708, 'accumulated_eval_time': 1803.822417974472, 'accumulated_logging_time': 4.4028167724609375, 'global_step': 153975, 'preemption_count': 0}), (155471, {'train/accuracy': 0.9189253449440002, 'train/loss': 0.5368630886077881, 'validation/accuracy': 0.7572599649429321, 'validation/loss': 1.1976443529129028, 'validation/num_examples': 50000, 'test/accuracy': 0.6337000131607056, 'test/loss': 1.8293724060058594, 'test/num_examples': 10000, 'score': 53081.45244860649, 'total_duration': 54912.43840265274, 'accumulated_submission_time': 53081.45244860649, 'accumulated_eval_time': 1821.2124042510986, 'accumulated_logging_time': 4.457838535308838, 'global_step': 155471, 'preemption_count': 0}), (156967, {'train/accuracy': 0.9168327450752258, 'train/loss': 0.5451948642730713, 'validation/accuracy': 0.7570799589157104, 'validation/loss': 1.195743203163147, 'validation/num_examples': 50000, 'test/accuracy': 0.6295000314712524, 'test/loss': 1.8315805196762085, 'test/num_examples': 10000, 'score': 53591.627490758896, 'total_duration': 55440.64496970177, 'accumulated_submission_time': 53591.627490758896, 'accumulated_eval_time': 1839.1349787712097, 'accumulated_logging_time': 4.514558553695679, 'global_step': 156967, 'preemption_count': 0}), (158462, {'train/accuracy': 0.9191047549247742, 'train/loss': 0.5344241857528687, 'validation/accuracy': 0.7557399868965149, 'validation/loss': 1.1946972608566284, 'validation/num_examples': 50000, 'test/accuracy': 0.6314000487327576, 'test/loss': 1.8226438760757446, 'test/num_examples': 10000, 'score': 54101.57953572273, 'total_duration': 55967.96126079559, 'accumulated_submission_time': 54101.57953572273, 'accumulated_eval_time': 1856.390554189682, 'accumulated_logging_time': 4.57092547416687, 'global_step': 158462, 'preemption_count': 0}), (159957, {'train/accuracy': 0.9191445708274841, 'train/loss': 0.5355068445205688, 'validation/accuracy': 0.7576599717140198, 'validation/loss': 1.2013722658157349, 'validation/num_examples': 50000, 'test/accuracy': 0.6325000524520874, 'test/loss': 1.8268934488296509, 'test/num_examples': 10000, 'score': 54611.48469758034, 'total_duration': 56495.354562044144, 'accumulated_submission_time': 54611.48469758034, 'accumulated_eval_time': 1873.7629334926605, 'accumulated_logging_time': 4.634932041168213, 'global_step': 159957, 'preemption_count': 0}), (161453, {'train/accuracy': 0.9230110049247742, 'train/loss': 0.5109298229217529, 'validation/accuracy': 0.7595599889755249, 'validation/loss': 1.1808478832244873, 'validation/num_examples': 50000, 'test/accuracy': 0.6340000033378601, 'test/loss': 1.8122520446777344, 'test/num_examples': 10000, 'score': 55121.617911338806, 'total_duration': 57023.04491233826, 'accumulated_submission_time': 55121.617911338806, 'accumulated_eval_time': 1891.2067058086395, 'accumulated_logging_time': 4.697674751281738, 'global_step': 161453, 'preemption_count': 0}), (162948, {'train/accuracy': 0.9344706535339355, 'train/loss': 0.48090997338294983, 'validation/accuracy': 0.7597000002861023, 'validation/loss': 1.189298152923584, 'validation/num_examples': 50000, 'test/accuracy': 0.6338000297546387, 'test/loss': 1.813969612121582, 'test/num_examples': 10000, 'score': 55631.54829573631, 'total_duration': 57550.35655713081, 'accumulated_submission_time': 55631.54829573631, 'accumulated_eval_time': 1908.4826707839966, 'accumulated_logging_time': 4.7523229122161865, 'global_step': 162948, 'preemption_count': 0}), (164444, {'train/accuracy': 0.9301458597183228, 'train/loss': 0.48501822352409363, 'validation/accuracy': 0.7604199647903442, 'validation/loss': 1.181099534034729, 'validation/num_examples': 50000, 'test/accuracy': 0.6353000402450562, 'test/loss': 1.8088539838790894, 'test/num_examples': 10000, 'score': 56141.64248919487, 'total_duration': 58077.93835878372, 'accumulated_submission_time': 56141.64248919487, 'accumulated_eval_time': 1925.8598954677582, 'accumulated_logging_time': 4.809988737106323, 'global_step': 164444, 'preemption_count': 0}), (165939, {'train/accuracy': 0.9299465417861938, 'train/loss': 0.4834108352661133, 'validation/accuracy': 0.7608599662780762, 'validation/loss': 1.1775710582733154, 'validation/num_examples': 50000, 'test/accuracy': 0.6385000348091125, 'test/loss': 1.806591510772705, 'test/num_examples': 10000, 'score': 56651.543511390686, 'total_duration': 58605.25307846069, 'accumulated_submission_time': 56651.543511390686, 'accumulated_eval_time': 1943.168488740921, 'accumulated_logging_time': 4.8650195598602295, 'global_step': 165939, 'preemption_count': 0}), (167435, {'train/accuracy': 0.9329559803009033, 'train/loss': 0.47671154141426086, 'validation/accuracy': 0.7617599964141846, 'validation/loss': 1.1800211668014526, 'validation/num_examples': 50000, 'test/accuracy': 0.6402000188827515, 'test/loss': 1.8032078742980957, 'test/num_examples': 10000, 'score': 57161.636585474014, 'total_duration': 59132.583621263504, 'accumulated_submission_time': 57161.636585474014, 'accumulated_eval_time': 1960.2930953502655, 'accumulated_logging_time': 4.926531791687012, 'global_step': 167435, 'preemption_count': 0}), (168931, {'train/accuracy': 0.9340720176696777, 'train/loss': 0.4837055206298828, 'validation/accuracy': 0.7615999579429626, 'validation/loss': 1.1844817399978638, 'validation/num_examples': 50000, 'test/accuracy': 0.6378000378608704, 'test/loss': 1.8050771951675415, 'test/num_examples': 10000, 'score': 57671.803205251694, 'total_duration': 59660.20059251785, 'accumulated_submission_time': 57671.803205251694, 'accumulated_eval_time': 1977.6313047409058, 'accumulated_logging_time': 4.987208604812622, 'global_step': 168931, 'preemption_count': 0}), (170427, {'train/accuracy': 0.9329758882522583, 'train/loss': 0.47846075892448425, 'validation/accuracy': 0.7625399827957153, 'validation/loss': 1.1756603717803955, 'validation/num_examples': 50000, 'test/accuracy': 0.6420000195503235, 'test/loss': 1.7998629808425903, 'test/num_examples': 10000, 'score': 58181.900406360626, 'total_duration': 60187.52374267578, 'accumulated_submission_time': 58181.900406360626, 'accumulated_eval_time': 1994.7446548938751, 'accumulated_logging_time': 5.04884672164917, 'global_step': 170427, 'preemption_count': 0}), (171923, {'train/accuracy': 0.9389548301696777, 'train/loss': 0.46069061756134033, 'validation/accuracy': 0.7628600001335144, 'validation/loss': 1.1755813360214233, 'validation/num_examples': 50000, 'test/accuracy': 0.6399000287055969, 'test/loss': 1.7999354600906372, 'test/num_examples': 10000, 'score': 58692.09399271011, 'total_duration': 60715.07775473595, 'accumulated_submission_time': 58692.09399271011, 'accumulated_eval_time': 2011.9929358959198, 'accumulated_logging_time': 5.10937762260437, 'global_step': 171923, 'preemption_count': 0}), (173418, {'train/accuracy': 0.9387754797935486, 'train/loss': 0.46282458305358887, 'validation/accuracy': 0.761900007724762, 'validation/loss': 1.1791095733642578, 'validation/num_examples': 50000, 'test/accuracy': 0.64000004529953, 'test/loss': 1.8056416511535645, 'test/num_examples': 10000, 'score': 59202.0762693882, 'total_duration': 61242.43579864502, 'accumulated_submission_time': 59202.0762693882, 'accumulated_eval_time': 2029.256034374237, 'accumulated_logging_time': 5.169875860214233, 'global_step': 173418, 'preemption_count': 0}), (174913, {'train/accuracy': 0.9377790093421936, 'train/loss': 0.4617302119731903, 'validation/accuracy': 0.7626799941062927, 'validation/loss': 1.1732220649719238, 'validation/num_examples': 50000, 'test/accuracy': 0.6394000053405762, 'test/loss': 1.7951186895370483, 'test/num_examples': 10000, 'score': 59712.00906252861, 'total_duration': 61769.894496679306, 'accumulated_submission_time': 59712.00906252861, 'accumulated_eval_time': 2046.667736530304, 'accumulated_logging_time': 5.231820344924927, 'global_step': 174913, 'preemption_count': 0}), (176409, {'train/accuracy': 0.93949294090271, 'train/loss': 0.4578624963760376, 'validation/accuracy': 0.76419997215271, 'validation/loss': 1.1710017919540405, 'validation/num_examples': 50000, 'test/accuracy': 0.64410001039505, 'test/loss': 1.790612816810608, 'test/num_examples': 10000, 'score': 60222.23619198799, 'total_duration': 62297.375115156174, 'accumulated_submission_time': 60222.23619198799, 'accumulated_eval_time': 2063.812755346298, 'accumulated_logging_time': 5.288837909698486, 'global_step': 176409, 'preemption_count': 0}), (177904, {'train/accuracy': 0.9384167790412903, 'train/loss': 0.45472314953804016, 'validation/accuracy': 0.7644400000572205, 'validation/loss': 1.167048454284668, 'validation/num_examples': 50000, 'test/accuracy': 0.6421000361442566, 'test/loss': 1.7899819612503052, 'test/num_examples': 10000, 'score': 60732.223915576935, 'total_duration': 62824.61188578606, 'accumulated_submission_time': 60732.223915576935, 'accumulated_eval_time': 2080.94895029068, 'accumulated_logging_time': 5.348676443099976, 'global_step': 177904, 'preemption_count': 0}), (179400, {'train/accuracy': 0.9395527839660645, 'train/loss': 0.4537563621997833, 'validation/accuracy': 0.7645599842071533, 'validation/loss': 1.1670894622802734, 'validation/num_examples': 50000, 'test/accuracy': 0.6438000202178955, 'test/loss': 1.7903746366500854, 'test/num_examples': 10000, 'score': 61242.448419332504, 'total_duration': 63352.25589585304, 'accumulated_submission_time': 61242.448419332504, 'accumulated_eval_time': 2098.2528672218323, 'accumulated_logging_time': 5.410964012145996, 'global_step': 179400, 'preemption_count': 0}), (180895, {'train/accuracy': 0.9403898119926453, 'train/loss': 0.4528295397758484, 'validation/accuracy': 0.7649999856948853, 'validation/loss': 1.168945074081421, 'validation/num_examples': 50000, 'test/accuracy': 0.6429000496864319, 'test/loss': 1.7909166812896729, 'test/num_examples': 10000, 'score': 61752.42921423912, 'total_duration': 63879.60906338692, 'accumulated_submission_time': 61752.42921423912, 'accumulated_eval_time': 2115.509332180023, 'accumulated_logging_time': 5.475275278091431, 'global_step': 180895, 'preemption_count': 0}), (182391, {'train/accuracy': 0.9399114847183228, 'train/loss': 0.4492985010147095, 'validation/accuracy': 0.7644000053405762, 'validation/loss': 1.1697618961334229, 'validation/num_examples': 50000, 'test/accuracy': 0.6430000066757202, 'test/loss': 1.7924065589904785, 'test/num_examples': 10000, 'score': 62262.553658008575, 'total_duration': 64407.16589832306, 'accumulated_submission_time': 62262.553658008575, 'accumulated_eval_time': 2132.8234283924103, 'accumulated_logging_time': 5.542778015136719, 'global_step': 182391, 'preemption_count': 0}), (183883, {'train/accuracy': 0.9393534660339355, 'train/loss': 0.4564688205718994, 'validation/accuracy': 0.7648400068283081, 'validation/loss': 1.1722710132598877, 'validation/num_examples': 50000, 'test/accuracy': 0.6425000429153442, 'test/loss': 1.794602870941162, 'test/num_examples': 10000, 'score': 62771.477128744125, 'total_duration': 64934.63848924637, 'accumulated_submission_time': 62771.477128744125, 'accumulated_eval_time': 2150.1170587539673, 'accumulated_logging_time': 6.747524976730347, 'global_step': 183883, 'preemption_count': 0})], 'global_step': 184577}
I0128 07:18:30.364676 139822745589568 submission_runner.py:586] Timing: 63008.11992740631
I0128 07:18:30.364757 139822745589568 submission_runner.py:588] Total number of evals: 124
I0128 07:18:30.364799 139822745589568 submission_runner.py:589] ====================
I0128 07:18:30.364844 139822745589568 submission_runner.py:542] Using RNG seed 916031063
I0128 07:18:30.366116 139822745589568 submission_runner.py:551] --- Tuning run 3/5 ---
I0128 07:18:30.366237 139822745589568 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_1/imagenet_resnet_jax/trial_3.
I0128 07:18:30.367909 139822745589568 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_1/imagenet_resnet_jax/trial_3/hparams.json.
I0128 07:18:30.368622 139822745589568 submission_runner.py:206] Initializing dataset.
I0128 07:18:30.377869 139822745589568 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0128 07:18:30.388002 139822745589568 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0128 07:18:30.571050 139822745589568 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0128 07:18:30.774861 139822745589568 submission_runner.py:213] Initializing model.
I0128 07:18:36.242094 139822745589568 submission_runner.py:255] Initializing optimizer.
I0128 07:18:36.625786 139822745589568 submission_runner.py:262] Initializing metrics bundle.
I0128 07:18:36.625950 139822745589568 submission_runner.py:280] Initializing checkpoint and logger.
I0128 07:18:36.640850 139822745589568 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_1/imagenet_resnet_jax/trial_3 with prefix checkpoint_
I0128 07:18:36.640957 139822745589568 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_1/imagenet_resnet_jax/trial_3/meta_data_0.json.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0128 07:18:48.583922 139822745589568 logger_utils.py:220] Unable to record git information. Continuing without it.
I0128 07:19:00.276649 139822745589568 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_1/imagenet_resnet_jax/trial_3/flags_0.json.
I0128 07:19:00.281803 139822745589568 submission_runner.py:314] Starting training loop.
I0128 07:19:31.766481 139656783959808 logging_writer.py:48] [0] global_step=0, grad_norm=0.6531614065170288, loss=6.929165840148926
I0128 07:19:31.779072 139822745589568 spec.py:321] Evaluating on the training split.
I0128 07:19:37.982478 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 07:19:46.687819 139822745589568 spec.py:349] Evaluating on the test split.
I0128 07:19:49.209755 139822745589568 submission_runner.py:408] Time since start: 48.93s, 	Step: 1, 	{'train/accuracy': 0.0011758609907701612, 'train/loss': 6.911500930786133, 'validation/accuracy': 0.001019999966956675, 'validation/loss': 6.91091251373291, 'validation/num_examples': 50000, 'test/accuracy': 0.0012000000569969416, 'test/loss': 6.910790920257568, 'test/num_examples': 10000, 'score': 31.49718737602234, 'total_duration': 48.92790198326111, 'accumulated_submission_time': 31.49718737602234, 'accumulated_eval_time': 17.43062710762024, 'accumulated_logging_time': 0}
I0128 07:19:49.219187 139656666543872 logging_writer.py:48] [1] accumulated_eval_time=17.430627, accumulated_logging_time=0, accumulated_submission_time=31.497187, global_step=1, preemption_count=0, score=31.497187, test/accuracy=0.001200, test/loss=6.910791, test/num_examples=10000, total_duration=48.927902, train/accuracy=0.001176, train/loss=6.911501, validation/accuracy=0.001020, validation/loss=6.910913, validation/num_examples=50000
I0128 07:20:23.335537 139656817530624 logging_writer.py:48] [100] global_step=100, grad_norm=0.6412794589996338, loss=6.897230625152588
I0128 07:20:57.532606 139656666543872 logging_writer.py:48] [200] global_step=200, grad_norm=0.6799598336219788, loss=6.85982608795166
I0128 07:21:31.770139 139656817530624 logging_writer.py:48] [300] global_step=300, grad_norm=0.6973274946212769, loss=6.773915767669678
I0128 07:22:05.987801 139656666543872 logging_writer.py:48] [400] global_step=400, grad_norm=0.7566278576850891, loss=6.669538974761963
I0128 07:22:40.200640 139656817530624 logging_writer.py:48] [500] global_step=500, grad_norm=0.7913644909858704, loss=6.55587911605835
I0128 07:23:14.453226 139656666543872 logging_writer.py:48] [600] global_step=600, grad_norm=0.8003682494163513, loss=6.460474014282227
I0128 07:23:48.688637 139656817530624 logging_writer.py:48] [700] global_step=700, grad_norm=0.8571093678474426, loss=6.348482131958008
I0128 07:24:22.963928 139656666543872 logging_writer.py:48] [800] global_step=800, grad_norm=0.9600871205329895, loss=6.19419002532959
I0128 07:24:57.202176 139656817530624 logging_writer.py:48] [900] global_step=900, grad_norm=1.702767014503479, loss=6.090267181396484
I0128 07:25:31.525110 139656666543872 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.59502375125885, loss=5.990011692047119
I0128 07:26:05.756916 139656817530624 logging_writer.py:48] [1100] global_step=1100, grad_norm=2.398313283920288, loss=5.880382061004639
I0128 07:26:40.039843 139656666543872 logging_writer.py:48] [1200] global_step=1200, grad_norm=1.788598656654358, loss=5.762966156005859
I0128 07:27:14.269778 139656817530624 logging_writer.py:48] [1300] global_step=1300, grad_norm=2.237032175064087, loss=5.700747966766357
I0128 07:27:48.516481 139656666543872 logging_writer.py:48] [1400] global_step=1400, grad_norm=3.085442543029785, loss=5.633295059204102
I0128 07:28:19.474087 139822745589568 spec.py:321] Evaluating on the training split.
I0128 07:28:25.638005 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 07:28:34.347146 139822745589568 spec.py:349] Evaluating on the test split.
I0128 07:28:36.764001 139822745589568 submission_runner.py:408] Time since start: 576.48s, 	Step: 1492, 	{'train/accuracy': 0.06754224747419357, 'train/loss': 5.367956638336182, 'validation/accuracy': 0.06469999998807907, 'validation/loss': 5.413669109344482, 'validation/num_examples': 50000, 'test/accuracy': 0.044600002467632294, 'test/loss': 5.659720420837402, 'test/num_examples': 10000, 'score': 541.692113161087, 'total_duration': 576.4821372032166, 'accumulated_submission_time': 541.692113161087, 'accumulated_eval_time': 34.72050094604492, 'accumulated_logging_time': 0.018468618392944336}
I0128 07:28:36.785445 139656800745216 logging_writer.py:48] [1492] accumulated_eval_time=34.720501, accumulated_logging_time=0.018469, accumulated_submission_time=541.692113, global_step=1492, preemption_count=0, score=541.692113, test/accuracy=0.044600, test/loss=5.659720, test/num_examples=10000, total_duration=576.482137, train/accuracy=0.067542, train/loss=5.367957, validation/accuracy=0.064700, validation/loss=5.413669, validation/num_examples=50000
I0128 07:28:39.857492 139656809137920 logging_writer.py:48] [1500] global_step=1500, grad_norm=2.9605603218078613, loss=5.520704746246338
I0128 07:29:14.073400 139656800745216 logging_writer.py:48] [1600] global_step=1600, grad_norm=3.826650381088257, loss=5.328136920928955
I0128 07:29:48.314335 139656809137920 logging_writer.py:48] [1700] global_step=1700, grad_norm=3.3383522033691406, loss=5.289833068847656
I0128 07:30:22.571225 139656800745216 logging_writer.py:48] [1800] global_step=1800, grad_norm=3.207928419113159, loss=5.299045562744141
I0128 07:30:56.834281 139656809137920 logging_writer.py:48] [1900] global_step=1900, grad_norm=3.61492919921875, loss=5.128779411315918
I0128 07:31:31.081289 139656800745216 logging_writer.py:48] [2000] global_step=2000, grad_norm=6.2552080154418945, loss=5.238395690917969
I0128 07:32:05.404695 139656809137920 logging_writer.py:48] [2100] global_step=2100, grad_norm=4.658733367919922, loss=5.093015193939209
I0128 07:32:39.643254 139656800745216 logging_writer.py:48] [2200] global_step=2200, grad_norm=5.846602916717529, loss=5.072201728820801
I0128 07:33:13.891328 139656809137920 logging_writer.py:48] [2300] global_step=2300, grad_norm=5.265655517578125, loss=4.968798637390137
I0128 07:33:48.147776 139656800745216 logging_writer.py:48] [2400] global_step=2400, grad_norm=6.776378631591797, loss=4.948673725128174
I0128 07:34:22.396399 139656809137920 logging_writer.py:48] [2500] global_step=2500, grad_norm=6.923156261444092, loss=4.84798526763916
I0128 07:34:56.638518 139656800745216 logging_writer.py:48] [2600] global_step=2600, grad_norm=6.155092716217041, loss=4.80086088180542
I0128 07:35:30.900090 139656809137920 logging_writer.py:48] [2700] global_step=2700, grad_norm=6.477617263793945, loss=4.7461676597595215
I0128 07:36:05.172403 139656800745216 logging_writer.py:48] [2800] global_step=2800, grad_norm=4.652783393859863, loss=4.65935754776001
I0128 07:36:39.431143 139656809137920 logging_writer.py:48] [2900] global_step=2900, grad_norm=4.800449848175049, loss=4.70090913772583
I0128 07:37:07.001426 139822745589568 spec.py:321] Evaluating on the training split.
I0128 07:37:13.135879 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 07:37:21.868687 139822745589568 spec.py:349] Evaluating on the test split.
I0128 07:37:24.414428 139822745589568 submission_runner.py:408] Time since start: 1104.13s, 	Step: 2982, 	{'train/accuracy': 0.16820789873600006, 'train/loss': 4.278195858001709, 'validation/accuracy': 0.1507599949836731, 'validation/loss': 4.392276287078857, 'validation/num_examples': 50000, 'test/accuracy': 0.10750000178813934, 'test/loss': 4.850800514221191, 'test/num_examples': 10000, 'score': 1051.8486168384552, 'total_duration': 1104.1325709819794, 'accumulated_submission_time': 1051.8486168384552, 'accumulated_eval_time': 52.13346600532532, 'accumulated_logging_time': 0.04932880401611328}
I0128 07:37:24.432841 139656792352512 logging_writer.py:48] [2982] accumulated_eval_time=52.133466, accumulated_logging_time=0.049329, accumulated_submission_time=1051.848617, global_step=2982, preemption_count=0, score=1051.848617, test/accuracy=0.107500, test/loss=4.850801, test/num_examples=10000, total_duration=1104.132571, train/accuracy=0.168208, train/loss=4.278196, validation/accuracy=0.150760, validation/loss=4.392276, validation/num_examples=50000
I0128 07:37:30.937006 139656800745216 logging_writer.py:48] [3000] global_step=3000, grad_norm=4.697734832763672, loss=4.576807498931885
I0128 07:38:05.129161 139656792352512 logging_writer.py:48] [3100] global_step=3100, grad_norm=6.239461421966553, loss=4.44228458404541
I0128 07:38:39.425502 139656800745216 logging_writer.py:48] [3200] global_step=3200, grad_norm=6.460658550262451, loss=4.567251205444336
I0128 07:39:13.696171 139656792352512 logging_writer.py:48] [3300] global_step=3300, grad_norm=8.499980926513672, loss=4.3946332931518555
I0128 07:39:47.942261 139656800745216 logging_writer.py:48] [3400] global_step=3400, grad_norm=5.917148590087891, loss=4.34876012802124
I0128 07:40:22.190388 139656792352512 logging_writer.py:48] [3500] global_step=3500, grad_norm=4.157984256744385, loss=4.338830471038818
I0128 07:40:56.463592 139656800745216 logging_writer.py:48] [3600] global_step=3600, grad_norm=7.48245096206665, loss=4.338810443878174
I0128 07:41:30.738984 139656792352512 logging_writer.py:48] [3700] global_step=3700, grad_norm=6.7462239265441895, loss=4.186159610748291
I0128 07:42:05.000199 139656800745216 logging_writer.py:48] [3800] global_step=3800, grad_norm=6.489109039306641, loss=4.16029167175293
I0128 07:42:39.253672 139656792352512 logging_writer.py:48] [3900] global_step=3900, grad_norm=9.935474395751953, loss=4.1545515060424805
I0128 07:43:13.521496 139656800745216 logging_writer.py:48] [4000] global_step=4000, grad_norm=4.799978733062744, loss=4.261081695556641
I0128 07:43:47.810308 139656792352512 logging_writer.py:48] [4100] global_step=4100, grad_norm=9.96945571899414, loss=4.047626495361328
I0128 07:44:22.078511 139656800745216 logging_writer.py:48] [4200] global_step=4200, grad_norm=4.496142864227295, loss=4.09165096282959
I0128 07:44:56.415548 139656792352512 logging_writer.py:48] [4300] global_step=4300, grad_norm=5.526951789855957, loss=3.9434683322906494
I0128 07:45:30.693471 139656800745216 logging_writer.py:48] [4400] global_step=4400, grad_norm=11.474291801452637, loss=3.8425164222717285
I0128 07:45:54.478851 139822745589568 spec.py:321] Evaluating on the training split.
I0128 07:46:00.732136 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 07:46:09.582552 139822745589568 spec.py:349] Evaluating on the test split.
I0128 07:46:12.119774 139822745589568 submission_runner.py:408] Time since start: 1631.84s, 	Step: 4471, 	{'train/accuracy': 0.26907286047935486, 'train/loss': 3.525336265563965, 'validation/accuracy': 0.24647998809814453, 'validation/loss': 3.675424337387085, 'validation/num_examples': 50000, 'test/accuracy': 0.18060000240802765, 'test/loss': 4.246772289276123, 'test/num_examples': 10000, 'score': 1561.8328936100006, 'total_duration': 1631.8379180431366, 'accumulated_submission_time': 1561.8328936100006, 'accumulated_eval_time': 69.77436900138855, 'accumulated_logging_time': 0.07820892333984375}
I0128 07:46:12.140331 139656800745216 logging_writer.py:48] [4471] accumulated_eval_time=69.774369, accumulated_logging_time=0.078209, accumulated_submission_time=1561.832894, global_step=4471, preemption_count=0, score=1561.832894, test/accuracy=0.180600, test/loss=4.246772, test/num_examples=10000, total_duration=1631.837918, train/accuracy=0.269073, train/loss=3.525336, validation/accuracy=0.246480, validation/loss=3.675424, validation/num_examples=50000
I0128 07:46:22.420963 139656809137920 logging_writer.py:48] [4500] global_step=4500, grad_norm=5.988036632537842, loss=3.9365546703338623
I0128 07:46:56.630090 139656800745216 logging_writer.py:48] [4600] global_step=4600, grad_norm=8.63184642791748, loss=3.8402609825134277
I0128 07:47:30.868189 139656809137920 logging_writer.py:48] [4700] global_step=4700, grad_norm=6.070985317230225, loss=3.7075672149658203
I0128 07:48:05.127607 139656800745216 logging_writer.py:48] [4800] global_step=4800, grad_norm=5.932716369628906, loss=3.959139347076416
I0128 07:48:39.381338 139656809137920 logging_writer.py:48] [4900] global_step=4900, grad_norm=6.458346366882324, loss=3.6821980476379395
I0128 07:49:13.624821 139656800745216 logging_writer.py:48] [5000] global_step=5000, grad_norm=6.286330699920654, loss=3.6956849098205566
I0128 07:49:47.886724 139656809137920 logging_writer.py:48] [5100] global_step=5100, grad_norm=9.771048545837402, loss=3.558682441711426
I0128 07:50:22.128834 139656800745216 logging_writer.py:48] [5200] global_step=5200, grad_norm=7.730517387390137, loss=3.54634165763855
I0128 07:50:56.374633 139656809137920 logging_writer.py:48] [5300] global_step=5300, grad_norm=4.490062236785889, loss=3.5081770420074463
I0128 07:51:30.842337 139656800745216 logging_writer.py:48] [5400] global_step=5400, grad_norm=5.375096797943115, loss=3.484433650970459
I0128 07:52:05.103986 139656809137920 logging_writer.py:48] [5500] global_step=5500, grad_norm=8.266056060791016, loss=3.4681811332702637
I0128 07:52:39.326201 139656800745216 logging_writer.py:48] [5600] global_step=5600, grad_norm=11.157161712646484, loss=3.6044530868530273
I0128 07:53:13.556255 139656809137920 logging_writer.py:48] [5700] global_step=5700, grad_norm=11.814643859863281, loss=3.39202618598938
I0128 07:53:47.795623 139656800745216 logging_writer.py:48] [5800] global_step=5800, grad_norm=6.987729549407959, loss=3.3984222412109375
I0128 07:54:22.046440 139656809137920 logging_writer.py:48] [5900] global_step=5900, grad_norm=7.417304515838623, loss=3.5087156295776367
I0128 07:54:42.420341 139822745589568 spec.py:321] Evaluating on the training split.
I0128 07:54:48.643338 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 07:54:57.427260 139822745589568 spec.py:349] Evaluating on the test split.
I0128 07:54:59.941668 139822745589568 submission_runner.py:408] Time since start: 2159.66s, 	Step: 5961, 	{'train/accuracy': 0.35855787992477417, 'train/loss': 2.934255361557007, 'validation/accuracy': 0.33243998885154724, 'validation/loss': 3.0907390117645264, 'validation/num_examples': 50000, 'test/accuracy': 0.2507000267505646, 'test/loss': 3.733696699142456, 'test/num_examples': 10000, 'score': 2072.0533249378204, 'total_duration': 2159.659814119339, 'accumulated_submission_time': 2072.0533249378204, 'accumulated_eval_time': 87.29565978050232, 'accumulated_logging_time': 0.10798144340515137}
I0128 07:54:59.960716 139656825923328 logging_writer.py:48] [5961] accumulated_eval_time=87.295660, accumulated_logging_time=0.107981, accumulated_submission_time=2072.053325, global_step=5961, preemption_count=0, score=2072.053325, test/accuracy=0.250700, test/loss=3.733697, test/num_examples=10000, total_duration=2159.659814, train/accuracy=0.358558, train/loss=2.934255, validation/accuracy=0.332440, validation/loss=3.090739, validation/num_examples=50000
I0128 07:55:13.638635 139656834316032 logging_writer.py:48] [6000] global_step=6000, grad_norm=7.486086368560791, loss=3.3999080657958984
I0128 07:55:47.824415 139656825923328 logging_writer.py:48] [6100] global_step=6100, grad_norm=4.667637348175049, loss=3.356271982192993
I0128 07:56:22.062823 139656834316032 logging_writer.py:48] [6200] global_step=6200, grad_norm=13.58862590789795, loss=3.3176050186157227
I0128 07:56:56.311363 139656825923328 logging_writer.py:48] [6300] global_step=6300, grad_norm=8.585403442382812, loss=3.221231460571289
I0128 07:57:30.664808 139656834316032 logging_writer.py:48] [6400] global_step=6400, grad_norm=11.541687965393066, loss=3.325993537902832
I0128 07:58:04.891666 139656825923328 logging_writer.py:48] [6500] global_step=6500, grad_norm=5.545323848724365, loss=3.1652302742004395
I0128 07:58:39.165548 139656834316032 logging_writer.py:48] [6600] global_step=6600, grad_norm=7.616775989532471, loss=3.2708442211151123
I0128 07:59:13.408209 139656825923328 logging_writer.py:48] [6700] global_step=6700, grad_norm=9.479379653930664, loss=3.209880828857422
I0128 07:59:47.648221 139656834316032 logging_writer.py:48] [6800] global_step=6800, grad_norm=9.855964660644531, loss=3.058619976043701
I0128 08:00:21.901585 139656825923328 logging_writer.py:48] [6900] global_step=6900, grad_norm=7.5155768394470215, loss=3.13073468208313
I0128 08:00:56.136603 139656834316032 logging_writer.py:48] [7000] global_step=7000, grad_norm=7.136222839355469, loss=3.0476701259613037
I0128 08:01:30.385964 139656825923328 logging_writer.py:48] [7100] global_step=7100, grad_norm=8.60036849975586, loss=3.0856449604034424
I0128 08:02:04.630405 139656834316032 logging_writer.py:48] [7200] global_step=7200, grad_norm=6.855020523071289, loss=3.085505247116089
I0128 08:02:38.880642 139656825923328 logging_writer.py:48] [7300] global_step=7300, grad_norm=7.991335391998291, loss=3.0711843967437744
I0128 08:03:13.131322 139656834316032 logging_writer.py:48] [7400] global_step=7400, grad_norm=3.3652184009552, loss=2.9838924407958984
I0128 08:03:30.052608 139822745589568 spec.py:321] Evaluating on the training split.
I0128 08:03:36.181223 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 08:03:45.076246 139822745589568 spec.py:349] Evaluating on the test split.
I0128 08:03:47.635337 139822745589568 submission_runner.py:408] Time since start: 2687.35s, 	Step: 7451, 	{'train/accuracy': 0.4449138939380646, 'train/loss': 2.4179675579071045, 'validation/accuracy': 0.38711997866630554, 'validation/loss': 2.7826380729675293, 'validation/num_examples': 50000, 'test/accuracy': 0.29590001702308655, 'test/loss': 3.4668314456939697, 'test/num_examples': 10000, 'score': 2582.084417819977, 'total_duration': 2687.353482246399, 'accumulated_submission_time': 2582.084417819977, 'accumulated_eval_time': 104.87834978103638, 'accumulated_logging_time': 0.136627197265625}
I0128 08:03:47.653617 139656800745216 logging_writer.py:48] [7451] accumulated_eval_time=104.878350, accumulated_logging_time=0.136627, accumulated_submission_time=2582.084418, global_step=7451, preemption_count=0, score=2582.084418, test/accuracy=0.295900, test/loss=3.466831, test/num_examples=10000, total_duration=2687.353482, train/accuracy=0.444914, train/loss=2.417968, validation/accuracy=0.387120, validation/loss=2.782638, validation/num_examples=50000
I0128 08:04:04.934084 139656809137920 logging_writer.py:48] [7500] global_step=7500, grad_norm=6.520136833190918, loss=2.991384267807007
I0128 08:04:39.165148 139656800745216 logging_writer.py:48] [7600] global_step=7600, grad_norm=13.6153564453125, loss=3.0907704830169678
I0128 08:05:13.358191 139656809137920 logging_writer.py:48] [7700] global_step=7700, grad_norm=6.765888214111328, loss=3.0877299308776855
I0128 08:05:47.595849 139656800745216 logging_writer.py:48] [7800] global_step=7800, grad_norm=7.9507951736450195, loss=2.8267440795898438
I0128 08:06:21.830154 139656809137920 logging_writer.py:48] [7900] global_step=7900, grad_norm=8.027349472045898, loss=2.9943368434906006
I0128 08:06:56.077808 139656800745216 logging_writer.py:48] [8000] global_step=8000, grad_norm=8.891744613647461, loss=2.9182944297790527
I0128 08:07:30.333611 139656809137920 logging_writer.py:48] [8100] global_step=8100, grad_norm=5.710428237915039, loss=2.8285036087036133
I0128 08:08:04.564555 139656800745216 logging_writer.py:48] [8200] global_step=8200, grad_norm=6.698364734649658, loss=2.8900704383850098
I0128 08:08:38.814109 139656809137920 logging_writer.py:48] [8300] global_step=8300, grad_norm=6.652004241943359, loss=2.8385744094848633
I0128 08:09:13.023857 139656800745216 logging_writer.py:48] [8400] global_step=8400, grad_norm=5.772018909454346, loss=2.7856686115264893
I0128 08:09:47.278201 139656809137920 logging_writer.py:48] [8500] global_step=8500, grad_norm=7.0622758865356445, loss=2.916858673095703
I0128 08:10:21.587007 139656800745216 logging_writer.py:48] [8600] global_step=8600, grad_norm=8.90964126586914, loss=2.8210744857788086
I0128 08:10:55.841381 139656809137920 logging_writer.py:48] [8700] global_step=8700, grad_norm=7.4092817306518555, loss=2.6941473484039307
I0128 08:11:30.053575 139656800745216 logging_writer.py:48] [8800] global_step=8800, grad_norm=8.341492652893066, loss=2.786844491958618
I0128 08:12:04.302463 139656809137920 logging_writer.py:48] [8900] global_step=8900, grad_norm=5.922513961791992, loss=2.7305049896240234
I0128 08:12:17.790462 139822745589568 spec.py:321] Evaluating on the training split.
I0128 08:12:24.656483 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 08:12:33.423283 139822745589568 spec.py:349] Evaluating on the test split.
I0128 08:12:35.950916 139822745589568 submission_runner.py:408] Time since start: 3215.67s, 	Step: 8941, 	{'train/accuracy': 0.5053611397743225, 'train/loss': 2.105006217956543, 'validation/accuracy': 0.4567599892616272, 'validation/loss': 2.381918430328369, 'validation/num_examples': 50000, 'test/accuracy': 0.3477000296115875, 'test/loss': 3.1159727573394775, 'test/num_examples': 10000, 'score': 3092.1619415283203, 'total_duration': 3215.6690373420715, 'accumulated_submission_time': 3092.1619415283203, 'accumulated_eval_time': 123.03875970840454, 'accumulated_logging_time': 0.16377711296081543}
I0128 08:12:35.969930 139656783959808 logging_writer.py:48] [8941] accumulated_eval_time=123.038760, accumulated_logging_time=0.163777, accumulated_submission_time=3092.161942, global_step=8941, preemption_count=0, score=3092.161942, test/accuracy=0.347700, test/loss=3.115973, test/num_examples=10000, total_duration=3215.669037, train/accuracy=0.505361, train/loss=2.105006, validation/accuracy=0.456760, validation/loss=2.381918, validation/num_examples=50000
I0128 08:12:56.509922 139656792352512 logging_writer.py:48] [9000] global_step=9000, grad_norm=6.155279159545898, loss=2.7679314613342285
I0128 08:13:30.694115 139656783959808 logging_writer.py:48] [9100] global_step=9100, grad_norm=5.796136379241943, loss=2.697232723236084
I0128 08:14:04.904762 139656792352512 logging_writer.py:48] [9200] global_step=9200, grad_norm=4.604428291320801, loss=2.719510555267334
I0128 08:14:39.124432 139656783959808 logging_writer.py:48] [9300] global_step=9300, grad_norm=7.443772315979004, loss=2.71673846244812
I0128 08:15:13.345153 139656792352512 logging_writer.py:48] [9400] global_step=9400, grad_norm=7.655348300933838, loss=2.5625791549682617
I0128 08:15:47.546159 139656783959808 logging_writer.py:48] [9500] global_step=9500, grad_norm=6.269818305969238, loss=2.738226890563965
I0128 08:16:21.746006 139656792352512 logging_writer.py:48] [9600] global_step=9600, grad_norm=8.230375289916992, loss=2.654365062713623
I0128 08:16:56.024487 139656783959808 logging_writer.py:48] [9700] global_step=9700, grad_norm=4.631346225738525, loss=2.577613592147827
I0128 08:17:30.258852 139656792352512 logging_writer.py:48] [9800] global_step=9800, grad_norm=6.503378391265869, loss=2.6872594356536865
I0128 08:18:04.491183 139656783959808 logging_writer.py:48] [9900] global_step=9900, grad_norm=8.103902816772461, loss=2.6765494346618652
I0128 08:18:38.683418 139656792352512 logging_writer.py:48] [10000] global_step=10000, grad_norm=7.361148357391357, loss=2.6772711277008057
I0128 08:19:12.897063 139656783959808 logging_writer.py:48] [10100] global_step=10100, grad_norm=6.702253341674805, loss=2.6024088859558105
I0128 08:19:47.117537 139656792352512 logging_writer.py:48] [10200] global_step=10200, grad_norm=5.763084888458252, loss=2.556476593017578
I0128 08:20:21.325997 139656783959808 logging_writer.py:48] [10300] global_step=10300, grad_norm=6.0204548835754395, loss=2.4878389835357666
I0128 08:20:55.530653 139656792352512 logging_writer.py:48] [10400] global_step=10400, grad_norm=6.763820648193359, loss=2.6335158348083496
I0128 08:21:06.284707 139822745589568 spec.py:321] Evaluating on the training split.
I0128 08:21:12.406152 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 08:21:21.183844 139822745589568 spec.py:349] Evaluating on the test split.
I0128 08:21:23.757168 139822745589568 submission_runner.py:408] Time since start: 3743.48s, 	Step: 10433, 	{'train/accuracy': 0.5231783986091614, 'train/loss': 2.0281193256378174, 'validation/accuracy': 0.48311999440193176, 'validation/loss': 2.2610156536102295, 'validation/num_examples': 50000, 'test/accuracy': 0.37310001254081726, 'test/loss': 2.9467318058013916, 'test/num_examples': 10000, 'score': 3602.4153928756714, 'total_duration': 3743.4753136634827, 'accumulated_submission_time': 3602.4153928756714, 'accumulated_eval_time': 140.51118230819702, 'accumulated_logging_time': 0.19390320777893066}
I0128 08:21:23.775918 139656834316032 logging_writer.py:48] [10433] accumulated_eval_time=140.511182, accumulated_logging_time=0.193903, accumulated_submission_time=3602.415393, global_step=10433, preemption_count=0, score=3602.415393, test/accuracy=0.373100, test/loss=2.946732, test/num_examples=10000, total_duration=3743.475314, train/accuracy=0.523178, train/loss=2.028119, validation/accuracy=0.483120, validation/loss=2.261016, validation/num_examples=50000
I0128 08:21:47.050326 139658730145536 logging_writer.py:48] [10500] global_step=10500, grad_norm=4.667538642883301, loss=2.495375394821167
I0128 08:22:21.208075 139656834316032 logging_writer.py:48] [10600] global_step=10600, grad_norm=7.257315635681152, loss=2.4306211471557617
I0128 08:22:55.386576 139658730145536 logging_writer.py:48] [10700] global_step=10700, grad_norm=6.336897850036621, loss=2.472597122192383
I0128 08:23:29.661053 139656834316032 logging_writer.py:48] [10800] global_step=10800, grad_norm=5.619204521179199, loss=2.546509265899658
I0128 08:24:03.852748 139658730145536 logging_writer.py:48] [10900] global_step=10900, grad_norm=7.062539100646973, loss=2.352020502090454
I0128 08:24:38.045634 139656834316032 logging_writer.py:48] [11000] global_step=11000, grad_norm=5.101799964904785, loss=2.3609609603881836
I0128 08:25:12.229596 139658730145536 logging_writer.py:48] [11100] global_step=11100, grad_norm=10.432120323181152, loss=2.5848097801208496
I0128 08:25:46.432279 139656834316032 logging_writer.py:48] [11200] global_step=11200, grad_norm=8.329623222351074, loss=2.5434658527374268
I0128 08:26:20.627309 139658730145536 logging_writer.py:48] [11300] global_step=11300, grad_norm=5.005059242248535, loss=2.4493045806884766
I0128 08:26:54.818974 139656834316032 logging_writer.py:48] [11400] global_step=11400, grad_norm=7.830517768859863, loss=2.4265923500061035
I0128 08:27:29.018930 139658730145536 logging_writer.py:48] [11500] global_step=11500, grad_norm=11.222108840942383, loss=2.4745216369628906
I0128 08:28:03.189659 139656834316032 logging_writer.py:48] [11600] global_step=11600, grad_norm=7.385280132293701, loss=2.4085094928741455
I0128 08:28:37.384632 139658730145536 logging_writer.py:48] [11700] global_step=11700, grad_norm=5.145910263061523, loss=2.3712949752807617
I0128 08:29:11.588425 139656834316032 logging_writer.py:48] [11800] global_step=11800, grad_norm=11.736947059631348, loss=2.4461965560913086
I0128 08:29:45.960900 139658730145536 logging_writer.py:48] [11900] global_step=11900, grad_norm=6.851274490356445, loss=2.4037137031555176
I0128 08:29:53.975648 139822745589568 spec.py:321] Evaluating on the training split.
I0128 08:30:00.132091 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 08:30:09.064999 139822745589568 spec.py:349] Evaluating on the test split.
I0128 08:30:11.483923 139822745589568 submission_runner.py:408] Time since start: 4271.20s, 	Step: 11925, 	{'train/accuracy': 0.5595105290412903, 'train/loss': 1.8536113500595093, 'validation/accuracy': 0.5171599984169006, 'validation/loss': 2.09015154838562, 'validation/num_examples': 50000, 'test/accuracy': 0.4001000225543976, 'test/loss': 2.7808094024658203, 'test/num_examples': 10000, 'score': 4112.552248477936, 'total_duration': 4271.202058792114, 'accumulated_submission_time': 4112.552248477936, 'accumulated_eval_time': 158.01940941810608, 'accumulated_logging_time': 0.22330927848815918}
I0128 08:30:11.503171 139656800745216 logging_writer.py:48] [11925] accumulated_eval_time=158.019409, accumulated_logging_time=0.223309, accumulated_submission_time=4112.552248, global_step=11925, preemption_count=0, score=4112.552248, test/accuracy=0.400100, test/loss=2.780809, test/num_examples=10000, total_duration=4271.202059, train/accuracy=0.559511, train/loss=1.853611, validation/accuracy=0.517160, validation/loss=2.090152, validation/num_examples=50000
I0128 08:30:37.457666 139656809137920 logging_writer.py:48] [12000] global_step=12000, grad_norm=6.898433685302734, loss=2.3879494667053223
I0128 08:31:11.600146 139656800745216 logging_writer.py:48] [12100] global_step=12100, grad_norm=5.840748310089111, loss=2.4170994758605957
I0128 08:31:45.760593 139656809137920 logging_writer.py:48] [12200] global_step=12200, grad_norm=6.483694076538086, loss=2.4201931953430176
I0128 08:32:19.947863 139656800745216 logging_writer.py:48] [12300] global_step=12300, grad_norm=4.956389904022217, loss=2.2957210540771484
I0128 08:32:54.156240 139656809137920 logging_writer.py:48] [12400] global_step=12400, grad_norm=6.7909159660339355, loss=2.376783609390259
I0128 08:33:28.354461 139656800745216 logging_writer.py:48] [12500] global_step=12500, grad_norm=5.915409564971924, loss=2.4166440963745117
I0128 08:34:02.544001 139656809137920 logging_writer.py:48] [12600] global_step=12600, grad_norm=5.845141887664795, loss=2.4372661113739014
I0128 08:34:36.726066 139656800745216 logging_writer.py:48] [12700] global_step=12700, grad_norm=3.6055448055267334, loss=2.303292751312256
I0128 08:35:10.934419 139656809137920 logging_writer.py:48] [12800] global_step=12800, grad_norm=9.087491989135742, loss=2.3664636611938477
I0128 08:35:45.114472 139656800745216 logging_writer.py:48] [12900] global_step=12900, grad_norm=7.617410182952881, loss=2.39406156539917
I0128 08:36:19.408453 139656809137920 logging_writer.py:48] [13000] global_step=13000, grad_norm=4.235058784484863, loss=2.3311269283294678
I0128 08:36:53.594502 139656800745216 logging_writer.py:48] [13100] global_step=13100, grad_norm=6.119426250457764, loss=2.348336935043335
I0128 08:37:27.786185 139656809137920 logging_writer.py:48] [13200] global_step=13200, grad_norm=5.236505508422852, loss=2.3541646003723145
I0128 08:38:01.951300 139656800745216 logging_writer.py:48] [13300] global_step=13300, grad_norm=6.93541145324707, loss=2.336855411529541
I0128 08:38:36.155225 139656809137920 logging_writer.py:48] [13400] global_step=13400, grad_norm=5.910555839538574, loss=2.4127278327941895
I0128 08:38:41.770474 139822745589568 spec.py:321] Evaluating on the training split.
I0128 08:38:47.906424 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 08:38:56.595794 139822745589568 spec.py:349] Evaluating on the test split.
I0128 08:38:59.142566 139822745589568 submission_runner.py:408] Time since start: 4798.86s, 	Step: 13418, 	{'train/accuracy': 0.5712292790412903, 'train/loss': 1.7940986156463623, 'validation/accuracy': 0.5276399850845337, 'validation/loss': 2.0156517028808594, 'validation/num_examples': 50000, 'test/accuracy': 0.4075000286102295, 'test/loss': 2.762702703475952, 'test/num_examples': 10000, 'score': 4622.760374307632, 'total_duration': 4798.860710382462, 'accumulated_submission_time': 4622.760374307632, 'accumulated_eval_time': 175.39147543907166, 'accumulated_logging_time': 0.2514383792877197}
I0128 08:38:59.162417 139656666543872 logging_writer.py:48] [13418] accumulated_eval_time=175.391475, accumulated_logging_time=0.251438, accumulated_submission_time=4622.760374, global_step=13418, preemption_count=0, score=4622.760374, test/accuracy=0.407500, test/loss=2.762703, test/num_examples=10000, total_duration=4798.860710, train/accuracy=0.571229, train/loss=1.794099, validation/accuracy=0.527640, validation/loss=2.015652, validation/num_examples=50000
I0128 08:39:27.519760 139656783959808 logging_writer.py:48] [13500] global_step=13500, grad_norm=8.474976539611816, loss=2.395789384841919
I0128 08:40:01.648863 139656666543872 logging_writer.py:48] [13600] global_step=13600, grad_norm=5.421871662139893, loss=2.283383369445801
I0128 08:40:35.846401 139656783959808 logging_writer.py:48] [13700] global_step=13700, grad_norm=5.513626575469971, loss=2.1732208728790283
I0128 08:41:10.001035 139656666543872 logging_writer.py:48] [13800] global_step=13800, grad_norm=5.525580406188965, loss=2.354947328567505
I0128 08:41:44.183446 139656783959808 logging_writer.py:48] [13900] global_step=13900, grad_norm=5.965480804443359, loss=2.2112810611724854
I0128 08:42:18.361563 139656666543872 logging_writer.py:48] [14000] global_step=14000, grad_norm=4.606672763824463, loss=2.3964662551879883
I0128 08:42:52.715136 139656783959808 logging_writer.py:48] [14100] global_step=14100, grad_norm=5.4186625480651855, loss=2.297008514404297
I0128 08:43:26.901194 139656666543872 logging_writer.py:48] [14200] global_step=14200, grad_norm=8.955366134643555, loss=2.1943390369415283
I0128 08:44:01.071383 139656783959808 logging_writer.py:48] [14300] global_step=14300, grad_norm=5.039338111877441, loss=2.3108325004577637
I0128 08:44:35.245634 139656666543872 logging_writer.py:48] [14400] global_step=14400, grad_norm=5.623256206512451, loss=2.30503511428833
I0128 08:45:09.442162 139656783959808 logging_writer.py:48] [14500] global_step=14500, grad_norm=5.358025550842285, loss=2.2978498935699463
I0128 08:45:43.622986 139656666543872 logging_writer.py:48] [14600] global_step=14600, grad_norm=5.272122383117676, loss=2.181643009185791
I0128 08:46:17.759890 139656783959808 logging_writer.py:48] [14700] global_step=14700, grad_norm=5.89533805847168, loss=2.370474338531494
I0128 08:46:51.905158 139656666543872 logging_writer.py:48] [14800] global_step=14800, grad_norm=4.531749725341797, loss=2.236605167388916
I0128 08:47:26.068152 139656783959808 logging_writer.py:48] [14900] global_step=14900, grad_norm=5.206332206726074, loss=2.1305763721466064
I0128 08:47:29.296315 139822745589568 spec.py:321] Evaluating on the training split.
I0128 08:47:35.407209 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 08:47:43.991458 139822745589568 spec.py:349] Evaluating on the test split.
I0128 08:47:46.516481 139822745589568 submission_runner.py:408] Time since start: 5326.23s, 	Step: 14911, 	{'train/accuracy': 0.5833864808082581, 'train/loss': 1.730528473854065, 'validation/accuracy': 0.5450999736785889, 'validation/loss': 1.9298218488693237, 'validation/num_examples': 50000, 'test/accuracy': 0.426800012588501, 'test/loss': 2.671276807785034, 'test/num_examples': 10000, 'score': 5132.832676887512, 'total_duration': 5326.234619617462, 'accumulated_submission_time': 5132.832676887512, 'accumulated_eval_time': 192.61159753799438, 'accumulated_logging_time': 0.2807481288909912}
I0128 08:47:46.536832 139656817530624 logging_writer.py:48] [14911] accumulated_eval_time=192.611598, accumulated_logging_time=0.280748, accumulated_submission_time=5132.832677, global_step=14911, preemption_count=0, score=5132.832677, test/accuracy=0.426800, test/loss=2.671277, test/num_examples=10000, total_duration=5326.234620, train/accuracy=0.583386, train/loss=1.730528, validation/accuracy=0.545100, validation/loss=1.929822, validation/num_examples=50000
I0128 08:48:17.265480 139656825923328 logging_writer.py:48] [15000] global_step=15000, grad_norm=7.583055019378662, loss=2.1161742210388184
I0128 08:48:51.381257 139656817530624 logging_writer.py:48] [15100] global_step=15100, grad_norm=5.098238945007324, loss=2.3294427394866943
I0128 08:49:25.650155 139656825923328 logging_writer.py:48] [15200] global_step=15200, grad_norm=6.573176383972168, loss=2.2283763885498047
I0128 08:49:59.799535 139656817530624 logging_writer.py:48] [15300] global_step=15300, grad_norm=4.8482279777526855, loss=2.184328079223633
I0128 08:50:33.952669 139656825923328 logging_writer.py:48] [15400] global_step=15400, grad_norm=4.811984539031982, loss=2.2718067169189453
I0128 08:51:08.072421 139656817530624 logging_writer.py:48] [15500] global_step=15500, grad_norm=7.050044536590576, loss=2.207038402557373
I0128 08:51:42.257796 139656825923328 logging_writer.py:48] [15600] global_step=15600, grad_norm=6.442436695098877, loss=2.369597911834717
I0128 08:52:16.412629 139656817530624 logging_writer.py:48] [15700] global_step=15700, grad_norm=8.362507820129395, loss=2.3462421894073486
I0128 08:52:50.613894 139656825923328 logging_writer.py:48] [15800] global_step=15800, grad_norm=5.0974040031433105, loss=2.2127389907836914
I0128 08:53:24.756090 139656817530624 logging_writer.py:48] [15900] global_step=15900, grad_norm=5.440890789031982, loss=2.205328941345215
I0128 08:53:58.946208 139656825923328 logging_writer.py:48] [16000] global_step=16000, grad_norm=6.6394453048706055, loss=2.2935657501220703
I0128 08:54:33.113116 139656817530624 logging_writer.py:48] [16100] global_step=16100, grad_norm=6.708950519561768, loss=2.114903211593628
I0128 08:55:07.271276 139656825923328 logging_writer.py:48] [16200] global_step=16200, grad_norm=5.515059471130371, loss=2.2210822105407715
I0128 08:55:41.492862 139656817530624 logging_writer.py:48] [16300] global_step=16300, grad_norm=5.072677135467529, loss=2.055246591567993
I0128 08:56:15.652890 139656825923328 logging_writer.py:48] [16400] global_step=16400, grad_norm=3.260488271713257, loss=2.1729862689971924
I0128 08:56:16.829183 139822745589568 spec.py:321] Evaluating on the training split.
I0128 08:56:22.961951 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 08:56:31.626424 139822745589568 spec.py:349] Evaluating on the test split.
I0128 08:56:34.151541 139822745589568 submission_runner.py:408] Time since start: 5853.87s, 	Step: 16405, 	{'train/accuracy': 0.5859175324440002, 'train/loss': 1.707316517829895, 'validation/accuracy': 0.549780011177063, 'validation/loss': 1.9031552076339722, 'validation/num_examples': 50000, 'test/accuracy': 0.42250001430511475, 'test/loss': 2.6825528144836426, 'test/num_examples': 10000, 'score': 5643.064661502838, 'total_duration': 5853.869685411453, 'accumulated_submission_time': 5643.064661502838, 'accumulated_eval_time': 209.93391489982605, 'accumulated_logging_time': 0.3114583492279053}
I0128 08:56:34.171343 139656809137920 logging_writer.py:48] [16405] accumulated_eval_time=209.933915, accumulated_logging_time=0.311458, accumulated_submission_time=5643.064662, global_step=16405, preemption_count=0, score=5643.064662, test/accuracy=0.422500, test/loss=2.682553, test/num_examples=10000, total_duration=5853.869685, train/accuracy=0.585918, train/loss=1.707317, validation/accuracy=0.549780, validation/loss=1.903155, validation/num_examples=50000
I0128 08:57:06.903870 139656825923328 logging_writer.py:48] [16500] global_step=16500, grad_norm=7.480515480041504, loss=2.2117505073547363
I0128 08:57:41.013966 139656809137920 logging_writer.py:48] [16600] global_step=16600, grad_norm=5.642242431640625, loss=2.158238172531128
I0128 08:58:15.135467 139656825923328 logging_writer.py:48] [16700] global_step=16700, grad_norm=6.607875823974609, loss=2.2806875705718994
I0128 08:58:49.264836 139656809137920 logging_writer.py:48] [16800] global_step=16800, grad_norm=4.457146644592285, loss=2.1800270080566406
I0128 08:59:23.381514 139656825923328 logging_writer.py:48] [16900] global_step=16900, grad_norm=4.853541374206543, loss=2.1990933418273926
I0128 08:59:57.527866 139656809137920 logging_writer.py:48] [17000] global_step=17000, grad_norm=7.32468318939209, loss=2.2395377159118652
I0128 09:00:31.670244 139656825923328 logging_writer.py:48] [17100] global_step=17100, grad_norm=5.4669976234436035, loss=2.2032103538513184
I0128 09:01:05.808095 139656809137920 logging_writer.py:48] [17200] global_step=17200, grad_norm=4.8575873374938965, loss=2.1605758666992188
I0128 09:01:39.959727 139656825923328 logging_writer.py:48] [17300] global_step=17300, grad_norm=4.937193393707275, loss=2.1121468544006348
I0128 09:02:14.314900 139656809137920 logging_writer.py:48] [17400] global_step=17400, grad_norm=5.510050296783447, loss=2.1415915489196777
I0128 09:02:48.475066 139656825923328 logging_writer.py:48] [17500] global_step=17500, grad_norm=5.610598087310791, loss=2.2084450721740723
I0128 09:03:22.624383 139656809137920 logging_writer.py:48] [17600] global_step=17600, grad_norm=5.932989120483398, loss=2.213322877883911
I0128 09:03:56.786841 139656825923328 logging_writer.py:48] [17700] global_step=17700, grad_norm=5.890010356903076, loss=2.288952112197876
I0128 09:04:30.919530 139656809137920 logging_writer.py:48] [17800] global_step=17800, grad_norm=4.697917461395264, loss=2.2490437030792236
I0128 09:05:04.183181 139822745589568 spec.py:321] Evaluating on the training split.
I0128 09:05:10.370090 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 09:05:19.114730 139822745589568 spec.py:349] Evaluating on the test split.
I0128 09:05:21.649741 139822745589568 submission_runner.py:408] Time since start: 6381.37s, 	Step: 17899, 	{'train/accuracy': 0.6173269748687744, 'train/loss': 1.5603954792022705, 'validation/accuracy': 0.5484799742698669, 'validation/loss': 1.9227455854415894, 'validation/num_examples': 50000, 'test/accuracy': 0.42500001192092896, 'test/loss': 2.6512632369995117, 'test/num_examples': 10000, 'score': 6153.015790462494, 'total_duration': 6381.367884874344, 'accumulated_submission_time': 6153.015790462494, 'accumulated_eval_time': 227.40043759346008, 'accumulated_logging_time': 0.3407416343688965}
I0128 09:05:21.670801 139656792352512 logging_writer.py:48] [17899] accumulated_eval_time=227.400438, accumulated_logging_time=0.340742, accumulated_submission_time=6153.015790, global_step=17899, preemption_count=0, score=6153.015790, test/accuracy=0.425000, test/loss=2.651263, test/num_examples=10000, total_duration=6381.367885, train/accuracy=0.617327, train/loss=1.560395, validation/accuracy=0.548480, validation/loss=1.922746, validation/num_examples=50000
I0128 09:05:22.359155 139656800745216 logging_writer.py:48] [17900] global_step=17900, grad_norm=6.790358543395996, loss=2.243136405944824
I0128 09:05:56.475389 139656792352512 logging_writer.py:48] [18000] global_step=18000, grad_norm=5.250722408294678, loss=2.342008590698242
I0128 09:06:30.579984 139656800745216 logging_writer.py:48] [18100] global_step=18100, grad_norm=4.710892677307129, loss=2.180773973464966
I0128 09:07:04.709162 139656792352512 logging_writer.py:48] [18200] global_step=18200, grad_norm=4.116799831390381, loss=2.0595457553863525
I0128 09:07:38.854271 139656800745216 logging_writer.py:48] [18300] global_step=18300, grad_norm=5.419302940368652, loss=2.1264455318450928
I0128 09:08:13.094726 139656792352512 logging_writer.py:48] [18400] global_step=18400, grad_norm=3.796600341796875, loss=2.3020026683807373
I0128 09:08:47.215362 139656800745216 logging_writer.py:48] [18500] global_step=18500, grad_norm=3.5668437480926514, loss=2.1644489765167236
I0128 09:09:21.355807 139656792352512 logging_writer.py:48] [18600] global_step=18600, grad_norm=4.437433242797852, loss=2.1695854663848877
I0128 09:09:55.466341 139656800745216 logging_writer.py:48] [18700] global_step=18700, grad_norm=4.144028186798096, loss=2.146353244781494
I0128 09:10:29.624071 139656792352512 logging_writer.py:48] [18800] global_step=18800, grad_norm=4.0591254234313965, loss=2.254737615585327
I0128 09:11:03.775674 139656800745216 logging_writer.py:48] [18900] global_step=18900, grad_norm=4.704333782196045, loss=2.21802020072937
I0128 09:11:37.920926 139656792352512 logging_writer.py:48] [19000] global_step=19000, grad_norm=3.912240982055664, loss=2.2157931327819824
I0128 09:12:12.037280 139656800745216 logging_writer.py:48] [19100] global_step=19100, grad_norm=4.322015285491943, loss=2.161395788192749
I0128 09:12:46.191565 139656792352512 logging_writer.py:48] [19200] global_step=19200, grad_norm=4.89362096786499, loss=2.3063080310821533
I0128 09:13:20.315242 139656800745216 logging_writer.py:48] [19300] global_step=19300, grad_norm=4.4428486824035645, loss=2.122498035430908
I0128 09:13:51.854304 139822745589568 spec.py:321] Evaluating on the training split.
I0128 09:13:58.033483 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 09:14:06.910579 139822745589568 spec.py:349] Evaluating on the test split.
I0128 09:14:09.453405 139822745589568 submission_runner.py:408] Time since start: 6909.17s, 	Step: 19394, 	{'train/accuracy': 0.6105906963348389, 'train/loss': 1.5887254476547241, 'validation/accuracy': 0.5607399940490723, 'validation/loss': 1.864424467086792, 'validation/num_examples': 50000, 'test/accuracy': 0.44380003213882446, 'test/loss': 2.5865235328674316, 'test/num_examples': 10000, 'score': 6663.139073133469, 'total_duration': 6909.171550512314, 'accumulated_submission_time': 6663.139073133469, 'accumulated_eval_time': 244.99951553344727, 'accumulated_logging_time': 0.37102794647216797}
I0128 09:14:09.475101 139656792352512 logging_writer.py:48] [19394] accumulated_eval_time=244.999516, accumulated_logging_time=0.371028, accumulated_submission_time=6663.139073, global_step=19394, preemption_count=0, score=6663.139073, test/accuracy=0.443800, test/loss=2.586524, test/num_examples=10000, total_duration=6909.171551, train/accuracy=0.610591, train/loss=1.588725, validation/accuracy=0.560740, validation/loss=1.864424, validation/num_examples=50000
I0128 09:14:11.859285 139656809137920 logging_writer.py:48] [19400] global_step=19400, grad_norm=4.28978157043457, loss=2.3451619148254395
I0128 09:14:46.016283 139656792352512 logging_writer.py:48] [19500] global_step=19500, grad_norm=4.992381572723389, loss=2.235286235809326
I0128 09:15:20.129049 139656809137920 logging_writer.py:48] [19600] global_step=19600, grad_norm=3.7280538082122803, loss=2.217961311340332
I0128 09:15:54.243756 139656792352512 logging_writer.py:48] [19700] global_step=19700, grad_norm=4.257726669311523, loss=2.1860742568969727
I0128 09:16:28.414165 139656809137920 logging_writer.py:48] [19800] global_step=19800, grad_norm=5.349264621734619, loss=2.145618200302124
I0128 09:17:02.537109 139656792352512 logging_writer.py:48] [19900] global_step=19900, grad_norm=4.0991129875183105, loss=2.2087135314941406
I0128 09:17:36.683357 139656809137920 logging_writer.py:48] [20000] global_step=20000, grad_norm=3.473097801208496, loss=2.0817031860351562
I0128 09:18:10.795794 139656792352512 logging_writer.py:48] [20100] global_step=20100, grad_norm=3.870835781097412, loss=2.0986852645874023
I0128 09:18:44.936697 139656809137920 logging_writer.py:48] [20200] global_step=20200, grad_norm=4.011689186096191, loss=2.134253740310669
I0128 09:19:19.066453 139656792352512 logging_writer.py:48] [20300] global_step=20300, grad_norm=3.2998666763305664, loss=2.220126152038574
I0128 09:19:53.204779 139656809137920 logging_writer.py:48] [20400] global_step=20400, grad_norm=4.6903533935546875, loss=2.090378522872925
I0128 09:20:27.343355 139656792352512 logging_writer.py:48] [20500] global_step=20500, grad_norm=4.2390241622924805, loss=2.1118898391723633
I0128 09:21:01.563214 139656809137920 logging_writer.py:48] [20600] global_step=20600, grad_norm=3.598391532897949, loss=2.0446105003356934
I0128 09:21:35.736451 139656792352512 logging_writer.py:48] [20700] global_step=20700, grad_norm=3.5448482036590576, loss=2.080096483230591
I0128 09:22:09.880233 139656809137920 logging_writer.py:48] [20800] global_step=20800, grad_norm=3.5857014656066895, loss=2.0911498069763184
I0128 09:22:39.710148 139822745589568 spec.py:321] Evaluating on the training split.
I0128 09:22:45.840702 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 09:22:54.472219 139822745589568 spec.py:349] Evaluating on the test split.
I0128 09:22:57.090076 139822745589568 submission_runner.py:408] Time since start: 7436.81s, 	Step: 20889, 	{'train/accuracy': 0.6055883169174194, 'train/loss': 1.5978363752365112, 'validation/accuracy': 0.5553199648857117, 'validation/loss': 1.8586797714233398, 'validation/num_examples': 50000, 'test/accuracy': 0.42980003356933594, 'test/loss': 2.6199140548706055, 'test/num_examples': 10000, 'score': 7173.313796281815, 'total_duration': 7436.8082230091095, 'accumulated_submission_time': 7173.313796281815, 'accumulated_eval_time': 262.37942600250244, 'accumulated_logging_time': 0.40203213691711426}
I0128 09:22:57.111120 139656800745216 logging_writer.py:48] [20889] accumulated_eval_time=262.379426, accumulated_logging_time=0.402032, accumulated_submission_time=7173.313796, global_step=20889, preemption_count=0, score=7173.313796, test/accuracy=0.429800, test/loss=2.619914, test/num_examples=10000, total_duration=7436.808223, train/accuracy=0.605588, train/loss=1.597836, validation/accuracy=0.555320, validation/loss=1.858680, validation/num_examples=50000
I0128 09:23:01.227601 139656817530624 logging_writer.py:48] [20900] global_step=20900, grad_norm=3.980895757675171, loss=2.3309226036071777
I0128 09:23:35.319038 139656800745216 logging_writer.py:48] [21000] global_step=21000, grad_norm=3.2780613899230957, loss=2.1394894123077393
I0128 09:24:09.406717 139656817530624 logging_writer.py:48] [21100] global_step=21100, grad_norm=3.9363772869110107, loss=2.186354398727417
I0128 09:24:43.532130 139656800745216 logging_writer.py:48] [21200] global_step=21200, grad_norm=3.2226600646972656, loss=2.135749101638794
I0128 09:25:17.679655 139656817530624 logging_writer.py:48] [21300] global_step=21300, grad_norm=3.1464033126831055, loss=2.1280527114868164
I0128 09:25:51.789169 139656800745216 logging_writer.py:48] [21400] global_step=21400, grad_norm=3.7156033515930176, loss=2.00689959526062
I0128 09:26:25.909561 139656817530624 logging_writer.py:48] [21500] global_step=21500, grad_norm=3.7050514221191406, loss=2.1362521648406982
I0128 09:27:00.045960 139656800745216 logging_writer.py:48] [21600] global_step=21600, grad_norm=3.6888959407806396, loss=2.2347710132598877
I0128 09:27:34.240527 139656817530624 logging_writer.py:48] [21700] global_step=21700, grad_norm=3.5928094387054443, loss=2.0096428394317627
I0128 09:28:08.375336 139656800745216 logging_writer.py:48] [21800] global_step=21800, grad_norm=3.489849328994751, loss=2.169626235961914
I0128 09:28:42.518465 139656817530624 logging_writer.py:48] [21900] global_step=21900, grad_norm=3.925825357437134, loss=2.1812870502471924
I0128 09:29:16.669593 139656800745216 logging_writer.py:48] [22000] global_step=22000, grad_norm=3.319929599761963, loss=2.0831081867218018
I0128 09:29:50.801467 139656817530624 logging_writer.py:48] [22100] global_step=22100, grad_norm=3.542132616043091, loss=2.0504207611083984
I0128 09:30:24.945576 139656800745216 logging_writer.py:48] [22200] global_step=22200, grad_norm=3.9815499782562256, loss=2.0418810844421387
I0128 09:30:59.062235 139656817530624 logging_writer.py:48] [22300] global_step=22300, grad_norm=4.307428359985352, loss=2.1096913814544678
I0128 09:31:27.206333 139822745589568 spec.py:321] Evaluating on the training split.
I0128 09:31:33.414673 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 09:31:42.225373 139822745589568 spec.py:349] Evaluating on the test split.
I0128 09:31:44.731022 139822745589568 submission_runner.py:408] Time since start: 7964.45s, 	Step: 22384, 	{'train/accuracy': 0.6145368218421936, 'train/loss': 1.5811792612075806, 'validation/accuracy': 0.5697999596595764, 'validation/loss': 1.8240339756011963, 'validation/num_examples': 50000, 'test/accuracy': 0.44620001316070557, 'test/loss': 2.5530011653900146, 'test/num_examples': 10000, 'score': 7683.348705768585, 'total_duration': 7964.449161529541, 'accumulated_submission_time': 7683.348705768585, 'accumulated_eval_time': 279.90408277511597, 'accumulated_logging_time': 0.4324047565460205}
I0128 09:31:44.753154 139656783959808 logging_writer.py:48] [22384] accumulated_eval_time=279.904083, accumulated_logging_time=0.432405, accumulated_submission_time=7683.348706, global_step=22384, preemption_count=0, score=7683.348706, test/accuracy=0.446200, test/loss=2.553001, test/num_examples=10000, total_duration=7964.449162, train/accuracy=0.614537, train/loss=1.581179, validation/accuracy=0.569800, validation/loss=1.824034, validation/num_examples=50000
I0128 09:31:50.573917 139656792352512 logging_writer.py:48] [22400] global_step=22400, grad_norm=3.6286532878875732, loss=2.0709235668182373
I0128 09:32:24.631341 139656783959808 logging_writer.py:48] [22500] global_step=22500, grad_norm=3.3623504638671875, loss=2.1194467544555664
I0128 09:32:58.732983 139656792352512 logging_writer.py:48] [22600] global_step=22600, grad_norm=2.9203014373779297, loss=1.9985145330429077
I0128 09:33:32.842632 139656783959808 logging_writer.py:48] [22700] global_step=22700, grad_norm=3.2090001106262207, loss=1.9901039600372314
I0128 09:34:07.027702 139656792352512 logging_writer.py:48] [22800] global_step=22800, grad_norm=3.909102439880371, loss=2.0453648567199707
I0128 09:34:41.156733 139656783959808 logging_writer.py:48] [22900] global_step=22900, grad_norm=3.5614967346191406, loss=1.9897927045822144
I0128 09:35:15.294354 139656792352512 logging_writer.py:48] [23000] global_step=23000, grad_norm=4.589352130889893, loss=2.099679946899414
I0128 09:35:49.457652 139656783959808 logging_writer.py:48] [23100] global_step=23100, grad_norm=4.322702407836914, loss=2.124751091003418
I0128 09:36:23.583268 139656792352512 logging_writer.py:48] [23200] global_step=23200, grad_norm=3.3385274410247803, loss=2.0849082469940186
I0128 09:36:57.725586 139656783959808 logging_writer.py:48] [23300] global_step=23300, grad_norm=3.584540843963623, loss=2.036350965499878
I0128 09:37:31.847676 139656792352512 logging_writer.py:48] [23400] global_step=23400, grad_norm=4.703360557556152, loss=2.220350980758667
I0128 09:38:05.982099 139656783959808 logging_writer.py:48] [23500] global_step=23500, grad_norm=3.242475748062134, loss=2.180704355239868
I0128 09:38:40.101253 139656792352512 logging_writer.py:48] [23600] global_step=23600, grad_norm=5.001430034637451, loss=2.058309316635132
I0128 09:39:14.207414 139656783959808 logging_writer.py:48] [23700] global_step=23700, grad_norm=5.1049485206604, loss=2.0349676609039307
I0128 09:39:48.322957 139656792352512 logging_writer.py:48] [23800] global_step=23800, grad_norm=3.547499418258667, loss=2.055262327194214
I0128 09:40:14.822886 139822745589568 spec.py:321] Evaluating on the training split.
I0128 09:40:21.064404 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 09:40:29.970409 139822745589568 spec.py:349] Evaluating on the test split.
I0128 09:40:32.637511 139822745589568 submission_runner.py:408] Time since start: 8492.36s, 	Step: 23879, 	{'train/accuracy': 0.6197385191917419, 'train/loss': 1.5652210712432861, 'validation/accuracy': 0.5730199813842773, 'validation/loss': 1.7761461734771729, 'validation/num_examples': 50000, 'test/accuracy': 0.45190003514289856, 'test/loss': 2.501939296722412, 'test/num_examples': 10000, 'score': 8193.355751514435, 'total_duration': 8492.355654716492, 'accumulated_submission_time': 8193.355751514435, 'accumulated_eval_time': 297.7186679840088, 'accumulated_logging_time': 0.4656527042388916}
I0128 09:40:32.659211 139656783959808 logging_writer.py:48] [23879] accumulated_eval_time=297.718668, accumulated_logging_time=0.465653, accumulated_submission_time=8193.355752, global_step=23879, preemption_count=0, score=8193.355752, test/accuracy=0.451900, test/loss=2.501939, test/num_examples=10000, total_duration=8492.355655, train/accuracy=0.619739, train/loss=1.565221, validation/accuracy=0.573020, validation/loss=1.776146, validation/num_examples=50000
I0128 09:40:40.165085 139656817530624 logging_writer.py:48] [23900] global_step=23900, grad_norm=3.6969144344329834, loss=1.988755702972412
I0128 09:41:14.250149 139656783959808 logging_writer.py:48] [24000] global_step=24000, grad_norm=4.424036502838135, loss=2.132577419281006
I0128 09:41:48.333591 139656817530624 logging_writer.py:48] [24100] global_step=24100, grad_norm=3.757244825363159, loss=2.0891289710998535
I0128 09:42:22.458952 139656783959808 logging_writer.py:48] [24200] global_step=24200, grad_norm=3.013021230697632, loss=2.025339126586914
I0128 09:42:56.559253 139656817530624 logging_writer.py:48] [24300] global_step=24300, grad_norm=3.587928295135498, loss=2.065915584564209
I0128 09:43:30.682385 139656783959808 logging_writer.py:48] [24400] global_step=24400, grad_norm=3.7344961166381836, loss=2.099428176879883
I0128 09:44:04.810632 139656817530624 logging_writer.py:48] [24500] global_step=24500, grad_norm=3.5145719051361084, loss=2.0426411628723145
I0128 09:44:38.940080 139656783959808 logging_writer.py:48] [24600] global_step=24600, grad_norm=3.0555801391601562, loss=2.0001380443573
I0128 09:45:13.088456 139656817530624 logging_writer.py:48] [24700] global_step=24700, grad_norm=3.710123062133789, loss=1.9946242570877075
I0128 09:45:47.191748 139656783959808 logging_writer.py:48] [24800] global_step=24800, grad_norm=3.7271902561187744, loss=1.9924216270446777
I0128 09:46:21.338118 139656817530624 logging_writer.py:48] [24900] global_step=24900, grad_norm=3.7647247314453125, loss=2.162210702896118
I0128 09:46:55.479988 139656783959808 logging_writer.py:48] [25000] global_step=25000, grad_norm=3.522122383117676, loss=2.0115175247192383
I0128 09:47:29.596723 139656817530624 logging_writer.py:48] [25100] global_step=25100, grad_norm=4.632003307342529, loss=2.0391528606414795
I0128 09:48:03.716076 139656783959808 logging_writer.py:48] [25200] global_step=25200, grad_norm=3.3687143325805664, loss=1.9632349014282227
I0128 09:48:37.824941 139656817530624 logging_writer.py:48] [25300] global_step=25300, grad_norm=3.5425832271575928, loss=2.0260872840881348
I0128 09:49:02.870441 139822745589568 spec.py:321] Evaluating on the training split.
I0128 09:49:08.986612 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 09:49:17.690701 139822745589568 spec.py:349] Evaluating on the test split.
I0128 09:49:20.267307 139822745589568 submission_runner.py:408] Time since start: 9019.99s, 	Step: 25375, 	{'train/accuracy': 0.6237244606018066, 'train/loss': 1.5361076593399048, 'validation/accuracy': 0.5832799673080444, 'validation/loss': 1.745568037033081, 'validation/num_examples': 50000, 'test/accuracy': 0.4537000358104706, 'test/loss': 2.5031628608703613, 'test/num_examples': 10000, 'score': 8703.507536411285, 'total_duration': 9019.985451698303, 'accumulated_submission_time': 8703.507536411285, 'accumulated_eval_time': 315.1154990196228, 'accumulated_logging_time': 0.49759435653686523}
I0128 09:49:20.289307 139656792352512 logging_writer.py:48] [25375] accumulated_eval_time=315.115499, accumulated_logging_time=0.497594, accumulated_submission_time=8703.507536, global_step=25375, preemption_count=0, score=8703.507536, test/accuracy=0.453700, test/loss=2.503163, test/num_examples=10000, total_duration=9019.985452, train/accuracy=0.623724, train/loss=1.536108, validation/accuracy=0.583280, validation/loss=1.745568, validation/num_examples=50000
I0128 09:49:29.155388 139656800745216 logging_writer.py:48] [25400] global_step=25400, grad_norm=4.231727600097656, loss=2.126791477203369
I0128 09:50:03.183264 139656792352512 logging_writer.py:48] [25500] global_step=25500, grad_norm=4.1683855056762695, loss=1.984833836555481
I0128 09:50:37.235475 139656800745216 logging_writer.py:48] [25600] global_step=25600, grad_norm=3.5635485649108887, loss=2.090425491333008
I0128 09:51:11.341668 139656792352512 logging_writer.py:48] [25700] global_step=25700, grad_norm=3.4002766609191895, loss=2.079876661300659
I0128 09:51:45.474082 139656800745216 logging_writer.py:48] [25800] global_step=25800, grad_norm=4.081578254699707, loss=2.027005672454834
I0128 09:52:19.611274 139656792352512 logging_writer.py:48] [25900] global_step=25900, grad_norm=3.4388272762298584, loss=2.1844005584716797
I0128 09:52:53.725499 139656800745216 logging_writer.py:48] [26000] global_step=26000, grad_norm=3.694301128387451, loss=1.9346882104873657
I0128 09:53:27.916716 139656792352512 logging_writer.py:48] [26100] global_step=26100, grad_norm=3.3683841228485107, loss=2.007511615753174
I0128 09:54:02.047777 139656800745216 logging_writer.py:48] [26200] global_step=26200, grad_norm=3.633113145828247, loss=2.0980560779571533
I0128 09:54:36.154342 139656792352512 logging_writer.py:48] [26300] global_step=26300, grad_norm=3.3656253814697266, loss=2.034623622894287
I0128 09:55:10.275034 139656800745216 logging_writer.py:48] [26400] global_step=26400, grad_norm=4.167001724243164, loss=2.103163242340088
I0128 09:55:44.405451 139656792352512 logging_writer.py:48] [26500] global_step=26500, grad_norm=3.7367308139801025, loss=1.976133942604065
I0128 09:56:18.513514 139656800745216 logging_writer.py:48] [26600] global_step=26600, grad_norm=3.7516911029815674, loss=1.942697525024414
I0128 09:56:52.604880 139656792352512 logging_writer.py:48] [26700] global_step=26700, grad_norm=3.0721774101257324, loss=1.8910434246063232
I0128 09:57:26.720816 139656800745216 logging_writer.py:48] [26800] global_step=26800, grad_norm=3.6739203929901123, loss=2.0506863594055176
I0128 09:57:50.391648 139822745589568 spec.py:321] Evaluating on the training split.
I0128 09:57:56.534700 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 09:58:05.497513 139822745589568 spec.py:349] Evaluating on the test split.
I0128 09:58:08.057274 139822745589568 submission_runner.py:408] Time since start: 9547.78s, 	Step: 26871, 	{'train/accuracy': 0.6704002022743225, 'train/loss': 1.311537265777588, 'validation/accuracy': 0.5862399935722351, 'validation/loss': 1.7210910320281982, 'validation/num_examples': 50000, 'test/accuracy': 0.4626000225543976, 'test/loss': 2.476860761642456, 'test/num_examples': 10000, 'score': 9213.550162315369, 'total_duration': 9547.775417804718, 'accumulated_submission_time': 9213.550162315369, 'accumulated_eval_time': 332.7810888290405, 'accumulated_logging_time': 0.5300009250640869}
I0128 09:58:08.082219 139656783959808 logging_writer.py:48] [26871] accumulated_eval_time=332.781089, accumulated_logging_time=0.530001, accumulated_submission_time=9213.550162, global_step=26871, preemption_count=0, score=9213.550162, test/accuracy=0.462600, test/loss=2.476861, test/num_examples=10000, total_duration=9547.775418, train/accuracy=0.670400, train/loss=1.311537, validation/accuracy=0.586240, validation/loss=1.721091, validation/num_examples=50000
I0128 09:58:18.329986 139656825923328 logging_writer.py:48] [26900] global_step=26900, grad_norm=2.9297587871551514, loss=1.9940361976623535
I0128 09:58:52.391415 139656783959808 logging_writer.py:48] [27000] global_step=27000, grad_norm=3.6235368251800537, loss=2.198258399963379
I0128 09:59:26.662289 139656825923328 logging_writer.py:48] [27100] global_step=27100, grad_norm=4.198694705963135, loss=2.1348514556884766
I0128 10:00:00.775183 139656783959808 logging_writer.py:48] [27200] global_step=27200, grad_norm=3.3445918560028076, loss=2.1056056022644043
I0128 10:00:34.898691 139656825923328 logging_writer.py:48] [27300] global_step=27300, grad_norm=3.3852720260620117, loss=1.9217197895050049
I0128 10:01:09.018990 139656783959808 logging_writer.py:48] [27400] global_step=27400, grad_norm=3.672245502471924, loss=1.9819996356964111
I0128 10:01:43.142953 139656825923328 logging_writer.py:48] [27500] global_step=27500, grad_norm=3.5153274536132812, loss=1.9413172006607056
I0128 10:02:17.268021 139656783959808 logging_writer.py:48] [27600] global_step=27600, grad_norm=3.1000936031341553, loss=2.0199906826019287
I0128 10:02:51.390878 139656825923328 logging_writer.py:48] [27700] global_step=27700, grad_norm=3.882503032684326, loss=1.9560543298721313
I0128 10:03:25.490174 139656783959808 logging_writer.py:48] [27800] global_step=27800, grad_norm=3.4839677810668945, loss=1.9955127239227295
I0128 10:03:59.625207 139656825923328 logging_writer.py:48] [27900] global_step=27900, grad_norm=3.6596500873565674, loss=2.037447452545166
I0128 10:04:33.729528 139656783959808 logging_writer.py:48] [28000] global_step=28000, grad_norm=3.2777976989746094, loss=1.9337917566299438
I0128 10:05:07.833600 139656825923328 logging_writer.py:48] [28100] global_step=28100, grad_norm=3.6736955642700195, loss=2.0024595260620117
I0128 10:05:41.961556 139656783959808 logging_writer.py:48] [28200] global_step=28200, grad_norm=3.3196372985839844, loss=1.9157031774520874
I0128 10:06:16.185113 139656825923328 logging_writer.py:48] [28300] global_step=28300, grad_norm=4.482511043548584, loss=1.9140663146972656
I0128 10:06:38.157973 139822745589568 spec.py:321] Evaluating on the training split.
I0128 10:06:44.360868 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 10:06:53.172873 139822745589568 spec.py:349] Evaluating on the test split.
I0128 10:06:55.805714 139822745589568 submission_runner.py:408] Time since start: 10075.52s, 	Step: 28366, 	{'train/accuracy': 0.6376155614852905, 'train/loss': 1.4694420099258423, 'validation/accuracy': 0.57669997215271, 'validation/loss': 1.7748409509658813, 'validation/num_examples': 50000, 'test/accuracy': 0.453900009393692, 'test/loss': 2.4946587085723877, 'test/num_examples': 10000, 'score': 9723.562857627869, 'total_duration': 10075.523859977722, 'accumulated_submission_time': 9723.562857627869, 'accumulated_eval_time': 350.4288082122803, 'accumulated_logging_time': 0.5664412975311279}
I0128 10:06:55.836242 139656809137920 logging_writer.py:48] [28366] accumulated_eval_time=350.428808, accumulated_logging_time=0.566441, accumulated_submission_time=9723.562858, global_step=28366, preemption_count=0, score=9723.562858, test/accuracy=0.453900, test/loss=2.494659, test/num_examples=10000, total_duration=10075.523860, train/accuracy=0.637616, train/loss=1.469442, validation/accuracy=0.576700, validation/loss=1.774841, validation/num_examples=50000
I0128 10:07:07.767910 139656817530624 logging_writer.py:48] [28400] global_step=28400, grad_norm=2.9584875106811523, loss=2.031172513961792
I0128 10:07:41.806686 139656809137920 logging_writer.py:48] [28500] global_step=28500, grad_norm=4.113638877868652, loss=1.9704850912094116
I0128 10:08:15.903482 139656817530624 logging_writer.py:48] [28600] global_step=28600, grad_norm=3.745180130004883, loss=1.9844458103179932
I0128 10:08:50.029470 139656809137920 logging_writer.py:48] [28700] global_step=28700, grad_norm=3.579720973968506, loss=2.004782199859619
I0128 10:09:24.115200 139656817530624 logging_writer.py:48] [28800] global_step=28800, grad_norm=3.6092307567596436, loss=1.9262616634368896
I0128 10:09:58.233783 139656809137920 logging_writer.py:48] [28900] global_step=28900, grad_norm=3.688826084136963, loss=2.09222149848938
I0128 10:10:32.307643 139656817530624 logging_writer.py:48] [29000] global_step=29000, grad_norm=3.8257598876953125, loss=2.065450429916382
I0128 10:11:06.399267 139656809137920 logging_writer.py:48] [29100] global_step=29100, grad_norm=3.4443655014038086, loss=1.7649844884872437
I0128 10:11:40.482107 139656817530624 logging_writer.py:48] [29200] global_step=29200, grad_norm=3.6837713718414307, loss=2.0189976692199707
I0128 10:12:14.653197 139656809137920 logging_writer.py:48] [29300] global_step=29300, grad_norm=3.64742112159729, loss=2.040229082107544
I0128 10:12:48.766264 139656817530624 logging_writer.py:48] [29400] global_step=29400, grad_norm=3.291064977645874, loss=2.0567102432250977
I0128 10:13:22.857698 139656809137920 logging_writer.py:48] [29500] global_step=29500, grad_norm=3.8735790252685547, loss=2.038210391998291
I0128 10:13:56.956734 139656817530624 logging_writer.py:48] [29600] global_step=29600, grad_norm=3.3491363525390625, loss=2.0268726348876953
I0128 10:14:31.062880 139656809137920 logging_writer.py:48] [29700] global_step=29700, grad_norm=4.07275390625, loss=1.9248210191726685
I0128 10:15:05.171323 139656817530624 logging_writer.py:48] [29800] global_step=29800, grad_norm=3.1331028938293457, loss=1.933010458946228
I0128 10:15:26.123794 139822745589568 spec.py:321] Evaluating on the training split.
I0128 10:15:32.376801 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 10:15:40.931351 139822745589568 spec.py:349] Evaluating on the test split.
I0128 10:15:43.467534 139822745589568 submission_runner.py:408] Time since start: 10603.19s, 	Step: 29863, 	{'train/accuracy': 0.6395886540412903, 'train/loss': 1.4568294286727905, 'validation/accuracy': 0.586080014705658, 'validation/loss': 1.7302289009094238, 'validation/num_examples': 50000, 'test/accuracy': 0.4571000337600708, 'test/loss': 2.4968318939208984, 'test/num_examples': 10000, 'score': 10233.790237903595, 'total_duration': 10603.185664176941, 'accumulated_submission_time': 10233.790237903595, 'accumulated_eval_time': 367.77251267433167, 'accumulated_logging_time': 0.6059026718139648}
I0128 10:15:43.490552 139656800745216 logging_writer.py:48] [29863] accumulated_eval_time=367.772513, accumulated_logging_time=0.605903, accumulated_submission_time=10233.790238, global_step=29863, preemption_count=0, score=10233.790238, test/accuracy=0.457100, test/loss=2.496832, test/num_examples=10000, total_duration=10603.185664, train/accuracy=0.639589, train/loss=1.456829, validation/accuracy=0.586080, validation/loss=1.730229, validation/num_examples=50000
I0128 10:15:56.454949 139656825923328 logging_writer.py:48] [29900] global_step=29900, grad_norm=5.356261253356934, loss=2.04528546333313
I0128 10:16:30.535246 139656800745216 logging_writer.py:48] [30000] global_step=30000, grad_norm=3.6931228637695312, loss=1.9871201515197754
I0128 10:17:04.591166 139656825923328 logging_writer.py:48] [30100] global_step=30100, grad_norm=4.131118297576904, loss=1.992297887802124
I0128 10:17:38.701594 139656800745216 logging_writer.py:48] [30200] global_step=30200, grad_norm=3.6085753440856934, loss=2.029008626937866
I0128 10:18:12.802171 139656825923328 logging_writer.py:48] [30300] global_step=30300, grad_norm=3.3592689037323, loss=1.8916187286376953
I0128 10:18:46.970312 139656800745216 logging_writer.py:48] [30400] global_step=30400, grad_norm=4.288479328155518, loss=1.9881446361541748
I0128 10:19:21.057440 139656825923328 logging_writer.py:48] [30500] global_step=30500, grad_norm=3.9350428581237793, loss=1.9771686792373657
I0128 10:19:55.185358 139656800745216 logging_writer.py:48] [30600] global_step=30600, grad_norm=3.559250593185425, loss=2.025022506713867
I0128 10:20:29.269452 139656825923328 logging_writer.py:48] [30700] global_step=30700, grad_norm=3.764962911605835, loss=2.0010721683502197
I0128 10:21:03.386564 139656800745216 logging_writer.py:48] [30800] global_step=30800, grad_norm=3.7731471061706543, loss=1.9499212503433228
I0128 10:21:37.490342 139656825923328 logging_writer.py:48] [30900] global_step=30900, grad_norm=3.5632925033569336, loss=1.8931556940078735
I0128 10:22:11.591340 139656800745216 logging_writer.py:48] [31000] global_step=31000, grad_norm=3.3725273609161377, loss=1.9884233474731445
I0128 10:22:45.699254 139656825923328 logging_writer.py:48] [31100] global_step=31100, grad_norm=3.5035510063171387, loss=1.9863784313201904
I0128 10:23:19.798393 139656800745216 logging_writer.py:48] [31200] global_step=31200, grad_norm=3.098665237426758, loss=1.9535455703735352
I0128 10:23:53.880197 139656825923328 logging_writer.py:48] [31300] global_step=31300, grad_norm=3.70162034034729, loss=1.9926865100860596
I0128 10:24:13.472024 139822745589568 spec.py:321] Evaluating on the training split.
I0128 10:24:19.590651 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 10:24:28.197559 139822745589568 spec.py:349] Evaluating on the test split.
I0128 10:24:30.708643 139822745589568 submission_runner.py:408] Time since start: 11130.43s, 	Step: 31359, 	{'train/accuracy': 0.638671875, 'train/loss': 1.4534977674484253, 'validation/accuracy': 0.5884400010108948, 'validation/loss': 1.7131558656692505, 'validation/num_examples': 50000, 'test/accuracy': 0.4750000238418579, 'test/loss': 2.414470672607422, 'test/num_examples': 10000, 'score': 10743.71028470993, 'total_duration': 11130.426788568497, 'accumulated_submission_time': 10743.71028470993, 'accumulated_eval_time': 385.00909519195557, 'accumulated_logging_time': 0.6394505500793457}
I0128 10:24:30.731379 139656783959808 logging_writer.py:48] [31359] accumulated_eval_time=385.009095, accumulated_logging_time=0.639451, accumulated_submission_time=10743.710285, global_step=31359, preemption_count=0, score=10743.710285, test/accuracy=0.475000, test/loss=2.414471, test/num_examples=10000, total_duration=11130.426789, train/accuracy=0.638672, train/loss=1.453498, validation/accuracy=0.588440, validation/loss=1.713156, validation/num_examples=50000
I0128 10:24:45.028374 139656809137920 logging_writer.py:48] [31400] global_step=31400, grad_norm=3.807488441467285, loss=2.011794090270996
I0128 10:25:19.145067 139656783959808 logging_writer.py:48] [31500] global_step=31500, grad_norm=3.4829764366149902, loss=2.062107801437378
I0128 10:25:53.218645 139656809137920 logging_writer.py:48] [31600] global_step=31600, grad_norm=3.7206573486328125, loss=2.0326480865478516
I0128 10:26:27.323736 139656783959808 logging_writer.py:48] [31700] global_step=31700, grad_norm=3.4887008666992188, loss=1.9294970035552979
I0128 10:27:01.401759 139656809137920 logging_writer.py:48] [31800] global_step=31800, grad_norm=3.5929486751556396, loss=2.048201084136963
I0128 10:27:35.492082 139656783959808 logging_writer.py:48] [31900] global_step=31900, grad_norm=3.292896032333374, loss=1.961832046508789
I0128 10:28:09.614302 139656809137920 logging_writer.py:48] [32000] global_step=32000, grad_norm=3.536351203918457, loss=2.0305182933807373
I0128 10:28:43.707731 139656783959808 logging_writer.py:48] [32100] global_step=32100, grad_norm=3.6358392238616943, loss=1.9515173435211182
I0128 10:29:17.816387 139656809137920 logging_writer.py:48] [32200] global_step=32200, grad_norm=3.7624382972717285, loss=2.01556396484375
I0128 10:29:51.925905 139656783959808 logging_writer.py:48] [32300] global_step=32300, grad_norm=4.337088108062744, loss=2.0012412071228027
I0128 10:30:25.997885 139656809137920 logging_writer.py:48] [32400] global_step=32400, grad_norm=3.5115702152252197, loss=1.9614192247390747
I0128 10:31:00.123009 139656783959808 logging_writer.py:48] [32500] global_step=32500, grad_norm=3.4422144889831543, loss=1.9565744400024414
I0128 10:31:34.302622 139656809137920 logging_writer.py:48] [32600] global_step=32600, grad_norm=3.197197198867798, loss=1.916927695274353
I0128 10:32:08.409860 139656783959808 logging_writer.py:48] [32700] global_step=32700, grad_norm=3.4722423553466797, loss=1.9806197881698608
I0128 10:32:42.507579 139656809137920 logging_writer.py:48] [32800] global_step=32800, grad_norm=3.9281790256500244, loss=1.937496542930603
I0128 10:33:00.728421 139822745589568 spec.py:321] Evaluating on the training split.
I0128 10:33:06.963080 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 10:33:15.834622 139822745589568 spec.py:349] Evaluating on the test split.
I0128 10:33:18.403496 139822745589568 submission_runner.py:408] Time since start: 11658.12s, 	Step: 32855, 	{'train/accuracy': 0.6384924650192261, 'train/loss': 1.4499725103378296, 'validation/accuracy': 0.5962799787521362, 'validation/loss': 1.6743437051773071, 'validation/num_examples': 50000, 'test/accuracy': 0.4702000319957733, 'test/loss': 2.409991502761841, 'test/num_examples': 10000, 'score': 11253.648388624191, 'total_duration': 11658.121633052826, 'accumulated_submission_time': 11253.648388624191, 'accumulated_eval_time': 402.6841251850128, 'accumulated_logging_time': 0.6713178157806396}
I0128 10:33:18.426811 139656800745216 logging_writer.py:48] [32855] accumulated_eval_time=402.684125, accumulated_logging_time=0.671318, accumulated_submission_time=11253.648389, global_step=32855, preemption_count=0, score=11253.648389, test/accuracy=0.470200, test/loss=2.409992, test/num_examples=10000, total_duration=11658.121633, train/accuracy=0.638492, train/loss=1.449973, validation/accuracy=0.596280, validation/loss=1.674344, validation/num_examples=50000
I0128 10:33:34.098318 139656825923328 logging_writer.py:48] [32900] global_step=32900, grad_norm=3.351867914199829, loss=1.9884910583496094
I0128 10:34:08.143873 139656800745216 logging_writer.py:48] [33000] global_step=33000, grad_norm=3.317591428756714, loss=1.9522591829299927
I0128 10:34:42.237168 139656825923328 logging_writer.py:48] [33100] global_step=33100, grad_norm=2.950669765472412, loss=1.844102382659912
I0128 10:35:16.329454 139656800745216 logging_writer.py:48] [33200] global_step=33200, grad_norm=3.739673137664795, loss=1.9606921672821045
I0128 10:35:50.433995 139656825923328 logging_writer.py:48] [33300] global_step=33300, grad_norm=3.606766700744629, loss=1.9751825332641602
I0128 10:36:24.523567 139656800745216 logging_writer.py:48] [33400] global_step=33400, grad_norm=4.031517505645752, loss=1.927196741104126
I0128 10:36:58.610109 139656825923328 logging_writer.py:48] [33500] global_step=33500, grad_norm=4.156129837036133, loss=2.063610792160034
I0128 10:37:32.720338 139656800745216 logging_writer.py:48] [33600] global_step=33600, grad_norm=3.321715831756592, loss=2.0680129528045654
I0128 10:38:06.898387 139656825923328 logging_writer.py:48] [33700] global_step=33700, grad_norm=3.9218411445617676, loss=1.9566031694412231
I0128 10:38:40.985230 139656800745216 logging_writer.py:48] [33800] global_step=33800, grad_norm=3.7475223541259766, loss=1.9837554693222046
I0128 10:39:15.084654 139656825923328 logging_writer.py:48] [33900] global_step=33900, grad_norm=3.9106810092926025, loss=2.095256805419922
I0128 10:39:49.173298 139656800745216 logging_writer.py:48] [34000] global_step=34000, grad_norm=3.671143054962158, loss=1.9753046035766602
I0128 10:40:23.277948 139656825923328 logging_writer.py:48] [34100] global_step=34100, grad_norm=3.6820075511932373, loss=2.056154727935791
I0128 10:40:57.372349 139656800745216 logging_writer.py:48] [34200] global_step=34200, grad_norm=3.8644838333129883, loss=1.9856228828430176
I0128 10:41:31.469300 139656825923328 logging_writer.py:48] [34300] global_step=34300, grad_norm=3.3187475204467773, loss=1.9523184299468994
I0128 10:41:48.649775 139822745589568 spec.py:321] Evaluating on the training split.
I0128 10:41:54.824762 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 10:42:03.732729 139822745589568 spec.py:349] Evaluating on the test split.
I0128 10:42:06.343490 139822745589568 submission_runner.py:408] Time since start: 12186.06s, 	Step: 34352, 	{'train/accuracy': 0.6353236436843872, 'train/loss': 1.4725189208984375, 'validation/accuracy': 0.5895400047302246, 'validation/loss': 1.7027602195739746, 'validation/num_examples': 50000, 'test/accuracy': 0.4675000309944153, 'test/loss': 2.4392247200012207, 'test/num_examples': 10000, 'score': 11763.811690092087, 'total_duration': 12186.06164097786, 'accumulated_submission_time': 11763.811690092087, 'accumulated_eval_time': 420.37781167030334, 'accumulated_logging_time': 0.7036874294281006}
I0128 10:42:06.364162 139656817530624 logging_writer.py:48] [34352] accumulated_eval_time=420.377812, accumulated_logging_time=0.703687, accumulated_submission_time=11763.811690, global_step=34352, preemption_count=0, score=11763.811690, test/accuracy=0.467500, test/loss=2.439225, test/num_examples=10000, total_duration=12186.061641, train/accuracy=0.635324, train/loss=1.472519, validation/accuracy=0.589540, validation/loss=1.702760, validation/num_examples=50000
I0128 10:42:23.039293 139656825923328 logging_writer.py:48] [34400] global_step=34400, grad_norm=3.1004178524017334, loss=1.9190057516098022
I0128 10:42:57.093019 139656817530624 logging_writer.py:48] [34500] global_step=34500, grad_norm=3.523195505142212, loss=1.967820644378662
I0128 10:43:31.187447 139656825923328 logging_writer.py:48] [34600] global_step=34600, grad_norm=3.334042549133301, loss=1.9197733402252197
I0128 10:44:05.279484 139656817530624 logging_writer.py:48] [34700] global_step=34700, grad_norm=3.848799705505371, loss=2.013115406036377
I0128 10:44:39.420241 139656825923328 logging_writer.py:48] [34800] global_step=34800, grad_norm=3.7937636375427246, loss=2.0490479469299316
I0128 10:45:13.494147 139656817530624 logging_writer.py:48] [34900] global_step=34900, grad_norm=3.7560112476348877, loss=1.9377074241638184
I0128 10:45:47.584231 139656825923328 logging_writer.py:48] [35000] global_step=35000, grad_norm=3.3509578704833984, loss=1.8643336296081543
I0128 10:46:21.679640 139656817530624 logging_writer.py:48] [35100] global_step=35100, grad_norm=3.2527809143066406, loss=1.9233222007751465
I0128 10:46:55.770171 139656825923328 logging_writer.py:48] [35200] global_step=35200, grad_norm=3.0540435314178467, loss=1.8797252178192139
I0128 10:47:29.875967 139656817530624 logging_writer.py:48] [35300] global_step=35300, grad_norm=3.326956272125244, loss=1.8590292930603027
I0128 10:48:03.950795 139656825923328 logging_writer.py:48] [35400] global_step=35400, grad_norm=3.5960843563079834, loss=1.9183077812194824
I0128 10:48:38.045955 139656817530624 logging_writer.py:48] [35500] global_step=35500, grad_norm=3.5178732872009277, loss=1.9478412866592407
I0128 10:49:12.120605 139656825923328 logging_writer.py:48] [35600] global_step=35600, grad_norm=3.5269272327423096, loss=1.954427719116211
I0128 10:49:46.213027 139656817530624 logging_writer.py:48] [35700] global_step=35700, grad_norm=3.2279133796691895, loss=2.012760639190674
I0128 10:50:20.311288 139656825923328 logging_writer.py:48] [35800] global_step=35800, grad_norm=3.9723973274230957, loss=1.9881190061569214
I0128 10:50:36.541297 139822745589568 spec.py:321] Evaluating on the training split.
I0128 10:50:42.736813 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 10:50:51.383461 139822745589568 spec.py:349] Evaluating on the test split.
I0128 10:50:53.818367 139822745589568 submission_runner.py:408] Time since start: 12713.54s, 	Step: 35849, 	{'train/accuracy': 0.652762234210968, 'train/loss': 1.4022929668426514, 'validation/accuracy': 0.5992199778556824, 'validation/loss': 1.6665433645248413, 'validation/num_examples': 50000, 'test/accuracy': 0.4771000146865845, 'test/loss': 2.394937753677368, 'test/num_examples': 10000, 'score': 12273.92913389206, 'total_duration': 12713.53648853302, 'accumulated_submission_time': 12273.92913389206, 'accumulated_eval_time': 437.6548173427582, 'accumulated_logging_time': 0.7323830127716064}
I0128 10:50:53.843971 139656783959808 logging_writer.py:48] [35849] accumulated_eval_time=437.654817, accumulated_logging_time=0.732383, accumulated_submission_time=12273.929134, global_step=35849, preemption_count=0, score=12273.929134, test/accuracy=0.477100, test/loss=2.394938, test/num_examples=10000, total_duration=12713.536489, train/accuracy=0.652762, train/loss=1.402293, validation/accuracy=0.599220, validation/loss=1.666543, validation/num_examples=50000
I0128 10:51:11.562546 139656800745216 logging_writer.py:48] [35900] global_step=35900, grad_norm=3.967355728149414, loss=1.9406969547271729
I0128 10:51:45.644288 139656783959808 logging_writer.py:48] [36000] global_step=36000, grad_norm=3.778923988342285, loss=2.0241076946258545
I0128 10:52:19.700997 139656800745216 logging_writer.py:48] [36100] global_step=36100, grad_norm=3.2265350818634033, loss=1.955193281173706
I0128 10:52:53.803760 139656783959808 logging_writer.py:48] [36200] global_step=36200, grad_norm=3.4521687030792236, loss=1.8072222471237183
I0128 10:53:27.883435 139656800745216 logging_writer.py:48] [36300] global_step=36300, grad_norm=3.3703930377960205, loss=1.8536664247512817
I0128 10:54:01.998657 139656783959808 logging_writer.py:48] [36400] global_step=36400, grad_norm=3.9994661808013916, loss=2.0032835006713867
I0128 10:54:36.076730 139656800745216 logging_writer.py:48] [36500] global_step=36500, grad_norm=3.6522512435913086, loss=1.9831830263137817
I0128 10:55:10.190790 139656783959808 logging_writer.py:48] [36600] global_step=36600, grad_norm=3.8221828937530518, loss=2.106675863265991
I0128 10:55:44.247572 139656800745216 logging_writer.py:48] [36700] global_step=36700, grad_norm=3.2128589153289795, loss=1.81857430934906
I0128 10:56:18.366666 139656783959808 logging_writer.py:48] [36800] global_step=36800, grad_norm=3.8706133365631104, loss=1.729191780090332
I0128 10:56:52.452329 139656800745216 logging_writer.py:48] [36900] global_step=36900, grad_norm=3.6791958808898926, loss=1.870198369026184
I0128 10:57:26.632345 139656783959808 logging_writer.py:48] [37000] global_step=37000, grad_norm=3.2426960468292236, loss=1.925613522529602
I0128 10:58:00.726925 139656800745216 logging_writer.py:48] [37100] global_step=37100, grad_norm=3.621364116668701, loss=1.9526965618133545
I0128 10:58:34.851864 139656783959808 logging_writer.py:48] [37200] global_step=37200, grad_norm=3.0890605449676514, loss=1.8579657077789307
I0128 10:59:08.945720 139656800745216 logging_writer.py:48] [37300] global_step=37300, grad_norm=3.4541618824005127, loss=1.9667980670928955
I0128 10:59:24.081351 139822745589568 spec.py:321] Evaluating on the training split.
I0128 10:59:30.352475 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 10:59:39.223098 139822745589568 spec.py:349] Evaluating on the test split.
I0128 10:59:41.720512 139822745589568 submission_runner.py:408] Time since start: 13241.44s, 	Step: 37346, 	{'train/accuracy': 0.6575254797935486, 'train/loss': 1.3663238286972046, 'validation/accuracy': 0.5941999554634094, 'validation/loss': 1.6988669633865356, 'validation/num_examples': 50000, 'test/accuracy': 0.47440001368522644, 'test/loss': 2.4055161476135254, 'test/num_examples': 10000, 'score': 12784.103419065475, 'total_duration': 13241.438656330109, 'accumulated_submission_time': 12784.103419065475, 'accumulated_eval_time': 455.29395627975464, 'accumulated_logging_time': 0.7698986530303955}
I0128 10:59:41.747655 139656666543872 logging_writer.py:48] [37346] accumulated_eval_time=455.293956, accumulated_logging_time=0.769899, accumulated_submission_time=12784.103419, global_step=37346, preemption_count=0, score=12784.103419, test/accuracy=0.474400, test/loss=2.405516, test/num_examples=10000, total_duration=13241.438656, train/accuracy=0.657525, train/loss=1.366324, validation/accuracy=0.594200, validation/loss=1.698867, validation/num_examples=50000
I0128 11:00:00.474910 139656792352512 logging_writer.py:48] [37400] global_step=37400, grad_norm=3.2597739696502686, loss=1.8521068096160889
I0128 11:00:34.512491 139656666543872 logging_writer.py:48] [37500] global_step=37500, grad_norm=3.7056705951690674, loss=2.0107240676879883
I0128 11:01:08.616882 139656792352512 logging_writer.py:48] [37600] global_step=37600, grad_norm=3.5944325923919678, loss=1.9122333526611328
I0128 11:01:42.677205 139656666543872 logging_writer.py:48] [37700] global_step=37700, grad_norm=3.926299571990967, loss=2.061610698699951
I0128 11:02:16.750932 139656792352512 logging_writer.py:48] [37800] global_step=37800, grad_norm=4.002978324890137, loss=1.9370348453521729
I0128 11:02:50.797961 139656666543872 logging_writer.py:48] [37900] global_step=37900, grad_norm=3.2793281078338623, loss=1.9498181343078613
I0128 11:03:24.902370 139656792352512 logging_writer.py:48] [38000] global_step=38000, grad_norm=3.338648557662964, loss=1.8767635822296143
I0128 11:03:59.161406 139656666543872 logging_writer.py:48] [38100] global_step=38100, grad_norm=4.2225446701049805, loss=1.9260282516479492
I0128 11:04:33.261266 139656792352512 logging_writer.py:48] [38200] global_step=38200, grad_norm=3.733790159225464, loss=1.9856667518615723
I0128 11:05:07.345175 139656666543872 logging_writer.py:48] [38300] global_step=38300, grad_norm=3.33817720413208, loss=1.9042434692382812
I0128 11:05:41.436832 139656792352512 logging_writer.py:48] [38400] global_step=38400, grad_norm=3.5897865295410156, loss=1.8760823011398315
I0128 11:06:16.127309 139656666543872 logging_writer.py:48] [38500] global_step=38500, grad_norm=3.6751813888549805, loss=1.89986252784729
I0128 11:06:50.197365 139656792352512 logging_writer.py:48] [38600] global_step=38600, grad_norm=3.518592119216919, loss=1.9824295043945312
I0128 11:07:24.263983 139656666543872 logging_writer.py:48] [38700] global_step=38700, grad_norm=2.8767621517181396, loss=1.8745280504226685
I0128 11:07:58.359628 139656792352512 logging_writer.py:48] [38800] global_step=38800, grad_norm=3.1643426418304443, loss=1.9119648933410645
I0128 11:08:11.785265 139822745589568 spec.py:321] Evaluating on the training split.
I0128 11:08:17.972567 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 11:08:26.511471 139822745589568 spec.py:349] Evaluating on the test split.
I0128 11:08:29.013720 139822745589568 submission_runner.py:408] Time since start: 13768.73s, 	Step: 38841, 	{'train/accuracy': 0.6528818607330322, 'train/loss': 1.384704828262329, 'validation/accuracy': 0.5984399914741516, 'validation/loss': 1.6746433973312378, 'validation/num_examples': 50000, 'test/accuracy': 0.48280003666877747, 'test/loss': 2.4071831703186035, 'test/num_examples': 10000, 'score': 13294.079423427582, 'total_duration': 13768.731862545013, 'accumulated_submission_time': 13294.079423427582, 'accumulated_eval_time': 472.52238607406616, 'accumulated_logging_time': 0.8064682483673096}
I0128 11:08:29.041619 139656666543872 logging_writer.py:48] [38841] accumulated_eval_time=472.522386, accumulated_logging_time=0.806468, accumulated_submission_time=13294.079423, global_step=38841, preemption_count=0, score=13294.079423, test/accuracy=0.482800, test/loss=2.407183, test/num_examples=10000, total_duration=13768.731863, train/accuracy=0.652882, train/loss=1.384705, validation/accuracy=0.598440, validation/loss=1.674643, validation/num_examples=50000
I0128 11:08:49.493465 139656783959808 logging_writer.py:48] [38900] global_step=38900, grad_norm=3.854522228240967, loss=1.932166337966919
I0128 11:09:23.558316 139656666543872 logging_writer.py:48] [39000] global_step=39000, grad_norm=3.1908280849456787, loss=1.9668800830841064
I0128 11:09:57.727372 139656783959808 logging_writer.py:48] [39100] global_step=39100, grad_norm=3.561175584793091, loss=1.8145817518234253
I0128 11:10:31.786043 139656666543872 logging_writer.py:48] [39200] global_step=39200, grad_norm=3.592552900314331, loss=1.9894709587097168
I0128 11:11:05.854765 139656783959808 logging_writer.py:48] [39300] global_step=39300, grad_norm=3.9429736137390137, loss=1.99773371219635
I0128 11:11:39.941770 139656666543872 logging_writer.py:48] [39400] global_step=39400, grad_norm=3.412648916244507, loss=2.0011284351348877
I0128 11:12:14.022397 139656783959808 logging_writer.py:48] [39500] global_step=39500, grad_norm=3.851693868637085, loss=1.8195139169692993
I0128 11:12:48.113982 139656666543872 logging_writer.py:48] [39600] global_step=39600, grad_norm=3.06474232673645, loss=1.8874799013137817
I0128 11:13:22.184101 139656783959808 logging_writer.py:48] [39700] global_step=39700, grad_norm=3.438201427459717, loss=1.8688819408416748
I0128 11:13:56.272217 139656666543872 logging_writer.py:48] [39800] global_step=39800, grad_norm=3.638200521469116, loss=1.9626102447509766
I0128 11:14:30.365882 139656783959808 logging_writer.py:48] [39900] global_step=39900, grad_norm=3.911964178085327, loss=1.9094387292861938
I0128 11:15:04.451250 139656666543872 logging_writer.py:48] [40000] global_step=40000, grad_norm=3.0787618160247803, loss=1.8354299068450928
I0128 11:15:38.541619 139656783959808 logging_writer.py:48] [40100] global_step=40100, grad_norm=4.731303691864014, loss=1.8905112743377686
I0128 11:16:12.648597 139656666543872 logging_writer.py:48] [40200] global_step=40200, grad_norm=3.373418092727661, loss=1.9167927503585815
I0128 11:16:46.784373 139656783959808 logging_writer.py:48] [40300] global_step=40300, grad_norm=3.4908392429351807, loss=1.9416966438293457
I0128 11:16:59.194753 139822745589568 spec.py:321] Evaluating on the training split.
I0128 11:17:05.438818 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 11:17:14.254621 139822745589568 spec.py:349] Evaluating on the test split.
I0128 11:17:16.762120 139822745589568 submission_runner.py:408] Time since start: 14296.48s, 	Step: 40338, 	{'train/accuracy': 0.6393694281578064, 'train/loss': 1.4571224451065063, 'validation/accuracy': 0.5904799699783325, 'validation/loss': 1.712005615234375, 'validation/num_examples': 50000, 'test/accuracy': 0.4669000208377838, 'test/loss': 2.4362292289733887, 'test/num_examples': 10000, 'score': 13804.172444343567, 'total_duration': 14296.480261325836, 'accumulated_submission_time': 13804.172444343567, 'accumulated_eval_time': 490.08971118927, 'accumulated_logging_time': 0.845099687576294}
I0128 11:17:16.789180 139656783959808 logging_writer.py:48] [40338] accumulated_eval_time=490.089711, accumulated_logging_time=0.845100, accumulated_submission_time=13804.172444, global_step=40338, preemption_count=0, score=13804.172444, test/accuracy=0.466900, test/loss=2.436229, test/num_examples=10000, total_duration=14296.480261, train/accuracy=0.639369, train/loss=1.457122, validation/accuracy=0.590480, validation/loss=1.712006, validation/num_examples=50000
I0128 11:17:38.250507 139656825923328 logging_writer.py:48] [40400] global_step=40400, grad_norm=3.3468058109283447, loss=1.916579008102417
I0128 11:18:12.286473 139656783959808 logging_writer.py:48] [40500] global_step=40500, grad_norm=3.483621597290039, loss=1.9683094024658203
I0128 11:18:46.359840 139656825923328 logging_writer.py:48] [40600] global_step=40600, grad_norm=4.030672550201416, loss=2.0854363441467285
I0128 11:19:20.446281 139656783959808 logging_writer.py:48] [40700] global_step=40700, grad_norm=3.5564932823181152, loss=1.8656370639801025
I0128 11:19:54.534615 139656825923328 logging_writer.py:48] [40800] global_step=40800, grad_norm=3.6574771404266357, loss=1.8322497606277466
I0128 11:20:28.634319 139656783959808 logging_writer.py:48] [40900] global_step=40900, grad_norm=3.5848331451416016, loss=1.8523123264312744
I0128 11:21:02.698725 139656825923328 logging_writer.py:48] [41000] global_step=41000, grad_norm=3.1871776580810547, loss=1.7901710271835327
I0128 11:21:36.801327 139656783959808 logging_writer.py:48] [41100] global_step=41100, grad_norm=3.168811559677124, loss=1.8531379699707031
I0128 11:22:10.871537 139656825923328 logging_writer.py:48] [41200] global_step=41200, grad_norm=3.3043460845947266, loss=1.8629564046859741
I0128 11:22:45.031230 139656783959808 logging_writer.py:48] [41300] global_step=41300, grad_norm=3.804332733154297, loss=1.9473841190338135
I0128 11:23:19.107254 139656825923328 logging_writer.py:48] [41400] global_step=41400, grad_norm=3.591662883758545, loss=1.8325660228729248
I0128 11:23:53.178102 139656783959808 logging_writer.py:48] [41500] global_step=41500, grad_norm=3.8738484382629395, loss=2.0734105110168457
I0128 11:24:27.258618 139656825923328 logging_writer.py:48] [41600] global_step=41600, grad_norm=3.476010799407959, loss=2.0186426639556885
I0128 11:25:01.349612 139656783959808 logging_writer.py:48] [41700] global_step=41700, grad_norm=3.730818748474121, loss=2.0577404499053955
I0128 11:25:35.432572 139656825923328 logging_writer.py:48] [41800] global_step=41800, grad_norm=3.5273094177246094, loss=1.8630688190460205
I0128 11:25:46.823219 139822745589568 spec.py:321] Evaluating on the training split.
I0128 11:25:53.049992 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 11:26:01.858158 139822745589568 spec.py:349] Evaluating on the test split.
I0128 11:26:04.357777 139822745589568 submission_runner.py:408] Time since start: 14824.08s, 	Step: 41835, 	{'train/accuracy': 0.6407644748687744, 'train/loss': 1.4308786392211914, 'validation/accuracy': 0.5977599620819092, 'validation/loss': 1.671799659729004, 'validation/num_examples': 50000, 'test/accuracy': 0.48110002279281616, 'test/loss': 2.351424217224121, 'test/num_examples': 10000, 'score': 14314.146545886993, 'total_duration': 14824.075912237167, 'accumulated_submission_time': 14314.146545886993, 'accumulated_eval_time': 507.6242277622223, 'accumulated_logging_time': 0.8813223838806152}
I0128 11:26:04.382353 139656792352512 logging_writer.py:48] [41835] accumulated_eval_time=507.624228, accumulated_logging_time=0.881322, accumulated_submission_time=14314.146546, global_step=41835, preemption_count=0, score=14314.146546, test/accuracy=0.481100, test/loss=2.351424, test/num_examples=10000, total_duration=14824.075912, train/accuracy=0.640764, train/loss=1.430879, validation/accuracy=0.597760, validation/loss=1.671800, validation/num_examples=50000
I0128 11:26:26.854339 139656800745216 logging_writer.py:48] [41900] global_step=41900, grad_norm=4.235613822937012, loss=2.0293796062469482
I0128 11:27:00.883284 139656792352512 logging_writer.py:48] [42000] global_step=42000, grad_norm=4.065656661987305, loss=1.890453815460205
I0128 11:27:34.914479 139656800745216 logging_writer.py:48] [42100] global_step=42100, grad_norm=3.603025197982788, loss=1.8187180757522583
I0128 11:28:09.017949 139656792352512 logging_writer.py:48] [42200] global_step=42200, grad_norm=4.4766411781311035, loss=2.0494134426116943
I0128 11:28:43.069337 139656800745216 logging_writer.py:48] [42300] global_step=42300, grad_norm=4.133124828338623, loss=1.896265983581543
I0128 11:29:17.396426 139656792352512 logging_writer.py:48] [42400] global_step=42400, grad_norm=3.3432254791259766, loss=1.9640533924102783
I0128 11:29:51.462764 139656800745216 logging_writer.py:48] [42500] global_step=42500, grad_norm=3.32401180267334, loss=1.997046947479248
I0128 11:30:25.562820 139656792352512 logging_writer.py:48] [42600] global_step=42600, grad_norm=3.3867368698120117, loss=1.8615702390670776
I0128 11:30:59.618601 139656800745216 logging_writer.py:48] [42700] global_step=42700, grad_norm=3.2303547859191895, loss=1.877875566482544
I0128 11:31:33.694276 139656792352512 logging_writer.py:48] [42800] global_step=42800, grad_norm=3.6363465785980225, loss=1.8498963117599487
I0128 11:32:07.770601 139656800745216 logging_writer.py:48] [42900] global_step=42900, grad_norm=3.1945061683654785, loss=1.9837754964828491
I0128 11:32:41.828010 139656792352512 logging_writer.py:48] [43000] global_step=43000, grad_norm=3.540618658065796, loss=1.887159824371338
I0128 11:33:15.922329 139656800745216 logging_writer.py:48] [43100] global_step=43100, grad_norm=2.9358112812042236, loss=2.0356814861297607
I0128 11:33:50.000880 139656792352512 logging_writer.py:48] [43200] global_step=43200, grad_norm=3.179319381713867, loss=1.8875840902328491
I0128 11:34:24.100411 139656800745216 logging_writer.py:48] [43300] global_step=43300, grad_norm=3.637212038040161, loss=1.9667068719863892
I0128 11:34:34.477176 139822745589568 spec.py:321] Evaluating on the training split.
I0128 11:34:40.633034 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 11:34:49.340875 139822745589568 spec.py:349] Evaluating on the test split.
I0128 11:34:51.790734 139822745589568 submission_runner.py:408] Time since start: 15351.51s, 	Step: 43332, 	{'train/accuracy': 0.6420599222183228, 'train/loss': 1.4258434772491455, 'validation/accuracy': 0.5988199710845947, 'validation/loss': 1.6653971672058105, 'validation/num_examples': 50000, 'test/accuracy': 0.47440001368522644, 'test/loss': 2.4011220932006836, 'test/num_examples': 10000, 'score': 14824.182245731354, 'total_duration': 15351.50887298584, 'accumulated_submission_time': 14824.182245731354, 'accumulated_eval_time': 524.9377455711365, 'accumulated_logging_time': 0.915226936340332}
I0128 11:34:51.816206 139656792352512 logging_writer.py:48] [43332] accumulated_eval_time=524.937746, accumulated_logging_time=0.915227, accumulated_submission_time=14824.182246, global_step=43332, preemption_count=0, score=14824.182246, test/accuracy=0.474400, test/loss=2.401122, test/num_examples=10000, total_duration=15351.508873, train/accuracy=0.642060, train/loss=1.425843, validation/accuracy=0.598820, validation/loss=1.665397, validation/num_examples=50000
I0128 11:35:15.324067 139656800745216 logging_writer.py:48] [43400] global_step=43400, grad_norm=3.3602302074432373, loss=1.999730110168457
I0128 11:35:49.459808 139656792352512 logging_writer.py:48] [43500] global_step=43500, grad_norm=3.9310100078582764, loss=1.860765814781189
I0128 11:36:23.531685 139656800745216 logging_writer.py:48] [43600] global_step=43600, grad_norm=3.2222893238067627, loss=1.922860860824585
I0128 11:36:57.622045 139656792352512 logging_writer.py:48] [43700] global_step=43700, grad_norm=3.317946195602417, loss=1.8146274089813232
I0128 11:37:31.694001 139656800745216 logging_writer.py:48] [43800] global_step=43800, grad_norm=3.376901626586914, loss=1.9162641763687134
I0128 11:38:05.777096 139656792352512 logging_writer.py:48] [43900] global_step=43900, grad_norm=3.4469611644744873, loss=1.7941157817840576
I0128 11:38:39.872957 139656800745216 logging_writer.py:48] [44000] global_step=44000, grad_norm=3.6462185382843018, loss=1.944566249847412
I0128 11:39:13.980835 139656792352512 logging_writer.py:48] [44100] global_step=44100, grad_norm=3.688436508178711, loss=1.908328652381897
I0128 11:39:48.054507 139656800745216 logging_writer.py:48] [44200] global_step=44200, grad_norm=3.818906545639038, loss=2.035593032836914
I0128 11:40:22.150356 139656792352512 logging_writer.py:48] [44300] global_step=44300, grad_norm=3.0574769973754883, loss=1.7638967037200928
I0128 11:40:56.238884 139656800745216 logging_writer.py:48] [44400] global_step=44400, grad_norm=3.655881643295288, loss=1.8843719959259033
I0128 11:41:30.332082 139656792352512 logging_writer.py:48] [44500] global_step=44500, grad_norm=3.9598042964935303, loss=1.9563140869140625
I0128 11:42:04.477320 139656800745216 logging_writer.py:48] [44600] global_step=44600, grad_norm=3.327409029006958, loss=1.8668766021728516
I0128 11:42:38.562411 139656792352512 logging_writer.py:48] [44700] global_step=44700, grad_norm=3.6386590003967285, loss=1.9060461521148682
I0128 11:43:12.632987 139656800745216 logging_writer.py:48] [44800] global_step=44800, grad_norm=3.3801586627960205, loss=1.9181709289550781
I0128 11:43:21.972360 139822745589568 spec.py:321] Evaluating on the training split.
I0128 11:43:28.102595 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 11:43:36.732260 139822745589568 spec.py:349] Evaluating on the test split.
I0128 11:43:39.258317 139822745589568 submission_runner.py:408] Time since start: 15878.98s, 	Step: 44829, 	{'train/accuracy': 0.6495137214660645, 'train/loss': 1.4016178846359253, 'validation/accuracy': 0.6055200099945068, 'validation/loss': 1.6298383474349976, 'validation/num_examples': 50000, 'test/accuracy': 0.4788000285625458, 'test/loss': 2.33974552154541, 'test/num_examples': 10000, 'score': 15334.278590202332, 'total_duration': 15878.976461172104, 'accumulated_submission_time': 15334.278590202332, 'accumulated_eval_time': 542.2236630916595, 'accumulated_logging_time': 0.9501383304595947}
I0128 11:43:39.286080 139656817530624 logging_writer.py:48] [44829] accumulated_eval_time=542.223663, accumulated_logging_time=0.950138, accumulated_submission_time=15334.278590, global_step=44829, preemption_count=0, score=15334.278590, test/accuracy=0.478800, test/loss=2.339746, test/num_examples=10000, total_duration=15878.976461, train/accuracy=0.649514, train/loss=1.401618, validation/accuracy=0.605520, validation/loss=1.629838, validation/num_examples=50000
I0128 11:44:03.798448 139656834316032 logging_writer.py:48] [44900] global_step=44900, grad_norm=3.688734769821167, loss=1.8910719156265259
I0128 11:44:37.818982 139656817530624 logging_writer.py:48] [45000] global_step=45000, grad_norm=3.348259449005127, loss=1.7782211303710938
I0128 11:45:11.869522 139656834316032 logging_writer.py:48] [45100] global_step=45100, grad_norm=3.8220067024230957, loss=1.8690025806427002
I0128 11:45:45.918606 139656817530624 logging_writer.py:48] [45200] global_step=45200, grad_norm=3.0118613243103027, loss=1.875939130783081
I0128 11:46:19.993344 139656834316032 logging_writer.py:48] [45300] global_step=45300, grad_norm=3.6331465244293213, loss=1.937516212463379
I0128 11:46:54.058741 139656817530624 logging_writer.py:48] [45400] global_step=45400, grad_norm=3.406496524810791, loss=1.7848358154296875
I0128 11:47:28.130640 139656834316032 logging_writer.py:48] [45500] global_step=45500, grad_norm=3.635715961456299, loss=2.0193474292755127
I0128 11:48:02.197900 139656817530624 logging_writer.py:48] [45600] global_step=45600, grad_norm=3.0965399742126465, loss=1.8975011110305786
I0128 11:48:36.411884 139656834316032 logging_writer.py:48] [45700] global_step=45700, grad_norm=3.4542272090911865, loss=1.87446928024292
I0128 11:49:10.478770 139656817530624 logging_writer.py:48] [45800] global_step=45800, grad_norm=3.7630057334899902, loss=1.9475600719451904
I0128 11:49:44.539021 139656834316032 logging_writer.py:48] [45900] global_step=45900, grad_norm=3.649064540863037, loss=2.024603843688965
I0128 11:50:18.611615 139656817530624 logging_writer.py:48] [46000] global_step=46000, grad_norm=3.2442691326141357, loss=1.904948353767395
I0128 11:50:52.702162 139656834316032 logging_writer.py:48] [46100] global_step=46100, grad_norm=3.3513858318328857, loss=1.8975311517715454
I0128 11:51:26.776453 139656817530624 logging_writer.py:48] [46200] global_step=46200, grad_norm=4.510783672332764, loss=1.9526605606079102
I0128 11:52:00.863555 139656834316032 logging_writer.py:48] [46300] global_step=46300, grad_norm=4.400283336639404, loss=1.8434908390045166
I0128 11:52:09.547378 139822745589568 spec.py:321] Evaluating on the training split.
I0128 11:52:15.733470 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 11:52:24.552725 139822745589568 spec.py:349] Evaluating on the test split.
I0128 11:52:27.070477 139822745589568 submission_runner.py:408] Time since start: 16406.79s, 	Step: 46327, 	{'train/accuracy': 0.6932198405265808, 'train/loss': 1.2089905738830566, 'validation/accuracy': 0.6118999719619751, 'validation/loss': 1.6064671277999878, 'validation/num_examples': 50000, 'test/accuracy': 0.49000000953674316, 'test/loss': 2.314314126968384, 'test/num_examples': 10000, 'score': 15844.478893518448, 'total_duration': 16406.788619995117, 'accumulated_submission_time': 15844.478893518448, 'accumulated_eval_time': 559.746725320816, 'accumulated_logging_time': 0.9882421493530273}
I0128 11:52:27.099414 139656800745216 logging_writer.py:48] [46327] accumulated_eval_time=559.746725, accumulated_logging_time=0.988242, accumulated_submission_time=15844.478894, global_step=46327, preemption_count=0, score=15844.478894, test/accuracy=0.490000, test/loss=2.314314, test/num_examples=10000, total_duration=16406.788620, train/accuracy=0.693220, train/loss=1.208991, validation/accuracy=0.611900, validation/loss=1.606467, validation/num_examples=50000
I0128 11:52:52.293309 139656809137920 logging_writer.py:48] [46400] global_step=46400, grad_norm=3.3449060916900635, loss=1.807930827140808
I0128 11:53:26.288215 139656800745216 logging_writer.py:48] [46500] global_step=46500, grad_norm=3.3479514122009277, loss=1.8850822448730469
I0128 11:54:00.327447 139656809137920 logging_writer.py:48] [46600] global_step=46600, grad_norm=3.527278423309326, loss=1.881821632385254
I0128 11:54:34.387401 139656800745216 logging_writer.py:48] [46700] global_step=46700, grad_norm=3.471937894821167, loss=1.9191021919250488
I0128 11:55:08.643176 139656809137920 logging_writer.py:48] [46800] global_step=46800, grad_norm=3.7442612648010254, loss=1.9135907888412476
I0128 11:55:42.700582 139656800745216 logging_writer.py:48] [46900] global_step=46900, grad_norm=3.243089199066162, loss=1.882462501525879
I0128 11:56:16.772076 139656809137920 logging_writer.py:48] [47000] global_step=47000, grad_norm=3.2183444499969482, loss=1.973860263824463
I0128 11:56:50.854785 139656800745216 logging_writer.py:48] [47100] global_step=47100, grad_norm=3.7985522747039795, loss=1.809114933013916
I0128 11:57:24.941565 139656809137920 logging_writer.py:48] [47200] global_step=47200, grad_norm=3.882575273513794, loss=1.8764135837554932
I0128 11:57:59.014667 139656800745216 logging_writer.py:48] [47300] global_step=47300, grad_norm=3.4987733364105225, loss=1.8662078380584717
I0128 11:58:33.070153 139656809137920 logging_writer.py:48] [47400] global_step=47400, grad_norm=3.9991328716278076, loss=1.893513560295105
I0128 11:59:07.119896 139656800745216 logging_writer.py:48] [47500] global_step=47500, grad_norm=4.008906364440918, loss=1.980862021446228
I0128 11:59:41.183505 139656809137920 logging_writer.py:48] [47600] global_step=47600, grad_norm=3.44388747215271, loss=1.9615581035614014
I0128 12:00:15.222188 139656800745216 logging_writer.py:48] [47700] global_step=47700, grad_norm=3.3173611164093018, loss=1.899661898612976
I0128 12:00:49.293096 139656809137920 logging_writer.py:48] [47800] global_step=47800, grad_norm=3.6622114181518555, loss=1.9324767589569092
I0128 12:00:57.285417 139822745589568 spec.py:321] Evaluating on the training split.
I0128 12:01:04.280378 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 12:01:13.080501 139822745589568 spec.py:349] Evaluating on the test split.
I0128 12:01:15.477134 139822745589568 submission_runner.py:408] Time since start: 16935.20s, 	Step: 47825, 	{'train/accuracy': 0.6660555005073547, 'train/loss': 1.3233352899551392, 'validation/accuracy': 0.604699969291687, 'validation/loss': 1.6360735893249512, 'validation/num_examples': 50000, 'test/accuracy': 0.4797000288963318, 'test/loss': 2.354118585586548, 'test/num_examples': 10000, 'score': 16354.602724313736, 'total_duration': 16935.195270061493, 'accumulated_submission_time': 16354.602724313736, 'accumulated_eval_time': 577.9383962154388, 'accumulated_logging_time': 1.026573657989502}
I0128 12:01:15.505264 139656792352512 logging_writer.py:48] [47825] accumulated_eval_time=577.938396, accumulated_logging_time=1.026574, accumulated_submission_time=16354.602724, global_step=47825, preemption_count=0, score=16354.602724, test/accuracy=0.479700, test/loss=2.354119, test/num_examples=10000, total_duration=16935.195270, train/accuracy=0.666056, train/loss=1.323335, validation/accuracy=0.604700, validation/loss=1.636074, validation/num_examples=50000
I0128 12:01:41.360508 139656817530624 logging_writer.py:48] [47900] global_step=47900, grad_norm=3.8377957344055176, loss=1.8677809238433838
I0128 12:02:15.387452 139656792352512 logging_writer.py:48] [48000] global_step=48000, grad_norm=3.119868278503418, loss=1.8345996141433716
I0128 12:02:49.444850 139656817530624 logging_writer.py:48] [48100] global_step=48100, grad_norm=3.8016018867492676, loss=1.8152928352355957
I0128 12:03:23.502721 139656792352512 logging_writer.py:48] [48200] global_step=48200, grad_norm=3.9958815574645996, loss=1.8164960145950317
I0128 12:03:57.534686 139656817530624 logging_writer.py:48] [48300] global_step=48300, grad_norm=3.2084853649139404, loss=1.9263875484466553
I0128 12:04:31.617813 139656792352512 logging_writer.py:48] [48400] global_step=48400, grad_norm=3.684464454650879, loss=1.831010103225708
I0128 12:05:05.668414 139656817530624 logging_writer.py:48] [48500] global_step=48500, grad_norm=3.7118136882781982, loss=1.9130747318267822
I0128 12:05:39.756911 139656792352512 logging_writer.py:48] [48600] global_step=48600, grad_norm=3.7324507236480713, loss=1.9834177494049072
I0128 12:06:13.825579 139656817530624 logging_writer.py:48] [48700] global_step=48700, grad_norm=4.356682777404785, loss=1.8423198461532593
I0128 12:06:47.923008 139656792352512 logging_writer.py:48] [48800] global_step=48800, grad_norm=4.048375606536865, loss=1.8878873586654663
I0128 12:07:21.985906 139656817530624 logging_writer.py:48] [48900] global_step=48900, grad_norm=4.009975433349609, loss=1.853670597076416
I0128 12:07:56.126486 139656792352512 logging_writer.py:48] [49000] global_step=49000, grad_norm=3.780148983001709, loss=1.8563474416732788
I0128 12:08:30.198272 139656817530624 logging_writer.py:48] [49100] global_step=49100, grad_norm=3.4120538234710693, loss=1.9271093606948853
I0128 12:09:04.258163 139656792352512 logging_writer.py:48] [49200] global_step=49200, grad_norm=3.3542354106903076, loss=1.9198020696640015
I0128 12:09:38.330937 139656817530624 logging_writer.py:48] [49300] global_step=49300, grad_norm=3.318222761154175, loss=1.7893558740615845
I0128 12:09:45.634263 139822745589568 spec.py:321] Evaluating on the training split.
I0128 12:09:51.750298 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 12:10:00.564460 139822745589568 spec.py:349] Evaluating on the test split.
I0128 12:10:03.175192 139822745589568 submission_runner.py:408] Time since start: 17462.89s, 	Step: 49323, 	{'train/accuracy': 0.6553133130073547, 'train/loss': 1.3799877166748047, 'validation/accuracy': 0.6030399799346924, 'validation/loss': 1.6445624828338623, 'validation/num_examples': 50000, 'test/accuracy': 0.4821000099182129, 'test/loss': 2.3614590167999268, 'test/num_examples': 10000, 'score': 16864.671887874603, 'total_duration': 17462.89333844185, 'accumulated_submission_time': 16864.671887874603, 'accumulated_eval_time': 595.4793081283569, 'accumulated_logging_time': 1.063713788986206}
I0128 12:10:03.201875 139656809137920 logging_writer.py:48] [49323] accumulated_eval_time=595.479308, accumulated_logging_time=1.063714, accumulated_submission_time=16864.671888, global_step=49323, preemption_count=0, score=16864.671888, test/accuracy=0.482100, test/loss=2.361459, test/num_examples=10000, total_duration=17462.893338, train/accuracy=0.655313, train/loss=1.379988, validation/accuracy=0.603040, validation/loss=1.644562, validation/num_examples=50000
I0128 12:10:29.753548 139656825923328 logging_writer.py:48] [49400] global_step=49400, grad_norm=3.629974365234375, loss=1.919452428817749
I0128 12:11:03.771180 139656809137920 logging_writer.py:48] [49500] global_step=49500, grad_norm=3.099296808242798, loss=1.878541350364685
I0128 12:11:37.810022 139656825923328 logging_writer.py:48] [49600] global_step=49600, grad_norm=3.507570266723633, loss=1.887829303741455
I0128 12:12:11.871887 139656809137920 logging_writer.py:48] [49700] global_step=49700, grad_norm=4.024905204772949, loss=2.048330307006836
I0128 12:12:45.935458 139656825923328 logging_writer.py:48] [49800] global_step=49800, grad_norm=3.3588621616363525, loss=1.7326781749725342
I0128 12:13:19.967730 139656809137920 logging_writer.py:48] [49900] global_step=49900, grad_norm=3.352884292602539, loss=2.0002989768981934
I0128 12:13:54.001148 139656825923328 logging_writer.py:48] [50000] global_step=50000, grad_norm=4.083990097045898, loss=1.8263390064239502
I0128 12:14:28.143289 139656809137920 logging_writer.py:48] [50100] global_step=50100, grad_norm=3.2827751636505127, loss=2.0051486492156982
I0128 12:15:02.175660 139656825923328 logging_writer.py:48] [50200] global_step=50200, grad_norm=4.15786600112915, loss=1.911439299583435
I0128 12:15:36.250407 139656809137920 logging_writer.py:48] [50300] global_step=50300, grad_norm=4.2083353996276855, loss=1.8980414867401123
I0128 12:16:10.295797 139656825923328 logging_writer.py:48] [50400] global_step=50400, grad_norm=3.660871982574463, loss=1.6831656694412231
I0128 12:16:44.374267 139656809137920 logging_writer.py:48] [50500] global_step=50500, grad_norm=3.7101902961730957, loss=1.9477697610855103
I0128 12:17:18.441349 139656825923328 logging_writer.py:48] [50600] global_step=50600, grad_norm=3.7288951873779297, loss=1.8850287199020386
I0128 12:17:52.479083 139656809137920 logging_writer.py:48] [50700] global_step=50700, grad_norm=3.3834080696105957, loss=1.8568329811096191
I0128 12:18:26.546741 139656825923328 logging_writer.py:48] [50800] global_step=50800, grad_norm=3.0567214488983154, loss=1.832981824874878
I0128 12:18:33.513775 139822745589568 spec.py:321] Evaluating on the training split.
I0128 12:18:39.637734 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 12:18:48.445676 139822745589568 spec.py:349] Evaluating on the test split.
I0128 12:18:51.009296 139822745589568 submission_runner.py:408] Time since start: 17990.73s, 	Step: 50822, 	{'train/accuracy': 0.6630061864852905, 'train/loss': 1.3523014783859253, 'validation/accuracy': 0.6115399599075317, 'validation/loss': 1.6137793064117432, 'validation/num_examples': 50000, 'test/accuracy': 0.48830002546310425, 'test/loss': 2.343726396560669, 'test/num_examples': 10000, 'score': 17374.922873973846, 'total_duration': 17990.727430582047, 'accumulated_submission_time': 17374.922873973846, 'accumulated_eval_time': 612.9747793674469, 'accumulated_logging_time': 1.0999596118927002}
I0128 12:18:51.038121 139656783959808 logging_writer.py:48] [50822] accumulated_eval_time=612.974779, accumulated_logging_time=1.099960, accumulated_submission_time=17374.922874, global_step=50822, preemption_count=0, score=17374.922874, test/accuracy=0.488300, test/loss=2.343726, test/num_examples=10000, total_duration=17990.727431, train/accuracy=0.663006, train/loss=1.352301, validation/accuracy=0.611540, validation/loss=1.613779, validation/num_examples=50000
I0128 12:19:17.911110 139656792352512 logging_writer.py:48] [50900] global_step=50900, grad_norm=3.6794164180755615, loss=1.9457035064697266
I0128 12:19:51.935293 139656783959808 logging_writer.py:48] [51000] global_step=51000, grad_norm=3.976698398590088, loss=1.8864424228668213
I0128 12:20:26.168996 139656792352512 logging_writer.py:48] [51100] global_step=51100, grad_norm=3.603023052215576, loss=1.7926348447799683
I0128 12:21:00.228568 139656783959808 logging_writer.py:48] [51200] global_step=51200, grad_norm=4.166807174682617, loss=1.8418357372283936
I0128 12:21:34.306821 139656792352512 logging_writer.py:48] [51300] global_step=51300, grad_norm=3.989895820617676, loss=1.8492319583892822
I0128 12:22:08.382257 139656783959808 logging_writer.py:48] [51400] global_step=51400, grad_norm=3.6478052139282227, loss=1.839797854423523
I0128 12:22:42.461407 139656792352512 logging_writer.py:48] [51500] global_step=51500, grad_norm=3.644249439239502, loss=1.9591395854949951
I0128 12:23:16.522730 139656783959808 logging_writer.py:48] [51600] global_step=51600, grad_norm=3.379638910293579, loss=1.8919271230697632
I0128 12:23:50.587539 139656792352512 logging_writer.py:48] [51700] global_step=51700, grad_norm=3.532031536102295, loss=1.9353173971176147
I0128 12:24:24.637477 139656783959808 logging_writer.py:48] [51800] global_step=51800, grad_norm=3.6422617435455322, loss=1.9197137355804443
I0128 12:24:58.692638 139656792352512 logging_writer.py:48] [51900] global_step=51900, grad_norm=3.699341297149658, loss=1.8186193704605103
I0128 12:25:32.761857 139656783959808 logging_writer.py:48] [52000] global_step=52000, grad_norm=3.4428539276123047, loss=1.8040050268173218
I0128 12:26:06.837099 139656792352512 logging_writer.py:48] [52100] global_step=52100, grad_norm=3.534529685974121, loss=1.8850504159927368
I0128 12:26:40.895947 139656783959808 logging_writer.py:48] [52200] global_step=52200, grad_norm=3.48260760307312, loss=2.003169059753418
I0128 12:27:15.036916 139656792352512 logging_writer.py:48] [52300] global_step=52300, grad_norm=3.748680830001831, loss=1.8657605648040771
I0128 12:27:21.318697 139822745589568 spec.py:321] Evaluating on the training split.
I0128 12:27:27.487939 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 12:27:36.185812 139822745589568 spec.py:349] Evaluating on the test split.
I0128 12:27:38.564646 139822745589568 submission_runner.py:408] Time since start: 18518.28s, 	Step: 52320, 	{'train/accuracy': 0.6526426672935486, 'train/loss': 1.3899118900299072, 'validation/accuracy': 0.6061800122261047, 'validation/loss': 1.6280734539031982, 'validation/num_examples': 50000, 'test/accuracy': 0.4897000193595886, 'test/loss': 2.339770793914795, 'test/num_examples': 10000, 'score': 17885.14226746559, 'total_duration': 18518.282783985138, 'accumulated_submission_time': 17885.14226746559, 'accumulated_eval_time': 630.2206964492798, 'accumulated_logging_time': 1.1384408473968506}
I0128 12:27:38.592036 139656809137920 logging_writer.py:48] [52320] accumulated_eval_time=630.220696, accumulated_logging_time=1.138441, accumulated_submission_time=17885.142267, global_step=52320, preemption_count=0, score=17885.142267, test/accuracy=0.489700, test/loss=2.339771, test/num_examples=10000, total_duration=18518.282784, train/accuracy=0.652643, train/loss=1.389912, validation/accuracy=0.606180, validation/loss=1.628073, validation/num_examples=50000
I0128 12:28:06.147310 139656825923328 logging_writer.py:48] [52400] global_step=52400, grad_norm=3.4090445041656494, loss=1.8526214361190796
I0128 12:28:40.173497 139656809137920 logging_writer.py:48] [52500] global_step=52500, grad_norm=3.8934268951416016, loss=1.7689921855926514
I0128 12:29:14.175774 139656825923328 logging_writer.py:48] [52600] global_step=52600, grad_norm=3.8031959533691406, loss=2.008368492126465
I0128 12:29:48.237637 139656809137920 logging_writer.py:48] [52700] global_step=52700, grad_norm=4.124594688415527, loss=1.758406162261963
I0128 12:30:22.306478 139656825923328 logging_writer.py:48] [52800] global_step=52800, grad_norm=3.408552408218384, loss=1.8194600343704224
I0128 12:30:56.375910 139656809137920 logging_writer.py:48] [52900] global_step=52900, grad_norm=4.268754959106445, loss=1.7896987199783325
I0128 12:31:30.455080 139656825923328 logging_writer.py:48] [53000] global_step=53000, grad_norm=3.452650785446167, loss=1.8087148666381836
I0128 12:32:04.528815 139656809137920 logging_writer.py:48] [53100] global_step=53100, grad_norm=3.5433273315429688, loss=1.928739309310913
I0128 12:32:38.582272 139656825923328 logging_writer.py:48] [53200] global_step=53200, grad_norm=3.843994617462158, loss=1.8408052921295166
I0128 12:33:12.827582 139656809137920 logging_writer.py:48] [53300] global_step=53300, grad_norm=3.5731041431427, loss=1.9833025932312012
I0128 12:33:46.876596 139656825923328 logging_writer.py:48] [53400] global_step=53400, grad_norm=4.0756144523620605, loss=1.9176456928253174
I0128 12:34:20.936213 139656809137920 logging_writer.py:48] [53500] global_step=53500, grad_norm=3.7767131328582764, loss=1.7694456577301025
I0128 12:34:55.005970 139656825923328 logging_writer.py:48] [53600] global_step=53600, grad_norm=3.0752980709075928, loss=1.8833332061767578
I0128 12:35:29.063612 139656809137920 logging_writer.py:48] [53700] global_step=53700, grad_norm=3.232276201248169, loss=1.8773866891860962
I0128 12:36:03.129062 139656825923328 logging_writer.py:48] [53800] global_step=53800, grad_norm=3.330585241317749, loss=1.8292396068572998
I0128 12:36:08.733027 139822745589568 spec.py:321] Evaluating on the training split.
I0128 12:36:15.082890 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 12:36:23.507010 139822745589568 spec.py:349] Evaluating on the test split.
I0128 12:36:25.994909 139822745589568 submission_runner.py:408] Time since start: 19045.71s, 	Step: 53818, 	{'train/accuracy': 0.6602359414100647, 'train/loss': 1.361589789390564, 'validation/accuracy': 0.613379955291748, 'validation/loss': 1.606788992881775, 'validation/num_examples': 50000, 'test/accuracy': 0.4889000356197357, 'test/loss': 2.3146893978118896, 'test/num_examples': 10000, 'score': 18395.222403526306, 'total_duration': 19045.71305155754, 'accumulated_submission_time': 18395.222403526306, 'accumulated_eval_time': 647.4825391769409, 'accumulated_logging_time': 1.1758079528808594}
I0128 12:36:26.023828 139656800745216 logging_writer.py:48] [53818] accumulated_eval_time=647.482539, accumulated_logging_time=1.175808, accumulated_submission_time=18395.222404, global_step=53818, preemption_count=0, score=18395.222404, test/accuracy=0.488900, test/loss=2.314689, test/num_examples=10000, total_duration=19045.713052, train/accuracy=0.660236, train/loss=1.361590, validation/accuracy=0.613380, validation/loss=1.606789, validation/num_examples=50000
I0128 12:36:54.287606 139656817530624 logging_writer.py:48] [53900] global_step=53900, grad_norm=3.8343987464904785, loss=1.9660706520080566
I0128 12:37:28.314785 139656800745216 logging_writer.py:48] [54000] global_step=54000, grad_norm=3.3217902183532715, loss=1.8564631938934326
I0128 12:38:02.344233 139656817530624 logging_writer.py:48] [54100] global_step=54100, grad_norm=3.675630569458008, loss=1.9848777055740356
I0128 12:38:36.404530 139656800745216 logging_writer.py:48] [54200] global_step=54200, grad_norm=3.374561309814453, loss=1.9653313159942627
I0128 12:39:10.469336 139656817530624 logging_writer.py:48] [54300] global_step=54300, grad_norm=4.0829548835754395, loss=1.8591300249099731
I0128 12:39:44.638123 139656800745216 logging_writer.py:48] [54400] global_step=54400, grad_norm=3.5074634552001953, loss=1.9175381660461426
I0128 12:40:18.687552 139656817530624 logging_writer.py:48] [54500] global_step=54500, grad_norm=4.553888320922852, loss=1.935280680656433
I0128 12:40:52.770444 139656800745216 logging_writer.py:48] [54600] global_step=54600, grad_norm=3.801579475402832, loss=1.8969011306762695
I0128 12:41:26.823345 139656817530624 logging_writer.py:48] [54700] global_step=54700, grad_norm=3.662182569503784, loss=1.925645112991333
I0128 12:42:00.914705 139656800745216 logging_writer.py:48] [54800] global_step=54800, grad_norm=4.335498332977295, loss=1.9284498691558838
I0128 12:42:34.963443 139656817530624 logging_writer.py:48] [54900] global_step=54900, grad_norm=4.287529468536377, loss=1.8261595964431763
I0128 12:43:09.049806 139656800745216 logging_writer.py:48] [55000] global_step=55000, grad_norm=3.529362201690674, loss=1.888903021812439
I0128 12:43:43.092545 139656817530624 logging_writer.py:48] [55100] global_step=55100, grad_norm=3.728386640548706, loss=1.8697389364242554
I0128 12:44:17.190301 139656800745216 logging_writer.py:48] [55200] global_step=55200, grad_norm=3.2884907722473145, loss=1.907670259475708
I0128 12:44:51.224847 139656817530624 logging_writer.py:48] [55300] global_step=55300, grad_norm=3.9958386421203613, loss=1.8574490547180176
I0128 12:44:56.133275 139822745589568 spec.py:321] Evaluating on the training split.
I0128 12:45:02.368193 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 12:45:11.077354 139822745589568 spec.py:349] Evaluating on the test split.
I0128 12:45:13.498258 139822745589568 submission_runner.py:408] Time since start: 19573.22s, 	Step: 55316, 	{'train/accuracy': 0.699238657951355, 'train/loss': 1.1795507669448853, 'validation/accuracy': 0.6133399605751038, 'validation/loss': 1.5974823236465454, 'validation/num_examples': 50000, 'test/accuracy': 0.4910000264644623, 'test/loss': 2.2842953205108643, 'test/num_examples': 10000, 'score': 18905.270992279053, 'total_duration': 19573.216374635696, 'accumulated_submission_time': 18905.270992279053, 'accumulated_eval_time': 664.8474590778351, 'accumulated_logging_time': 1.2139925956726074}
I0128 12:45:13.526547 139656800745216 logging_writer.py:48] [55316] accumulated_eval_time=664.847459, accumulated_logging_time=1.213993, accumulated_submission_time=18905.270992, global_step=55316, preemption_count=0, score=18905.270992, test/accuracy=0.491000, test/loss=2.284295, test/num_examples=10000, total_duration=19573.216375, train/accuracy=0.699239, train/loss=1.179551, validation/accuracy=0.613340, validation/loss=1.597482, validation/num_examples=50000
I0128 12:45:42.433266 139656809137920 logging_writer.py:48] [55400] global_step=55400, grad_norm=3.4570252895355225, loss=1.8309235572814941
I0128 12:46:16.540614 139656800745216 logging_writer.py:48] [55500] global_step=55500, grad_norm=3.745941400527954, loss=1.9338784217834473
I0128 12:46:50.581271 139656809137920 logging_writer.py:48] [55600] global_step=55600, grad_norm=3.186826229095459, loss=1.8536217212677002
I0128 12:47:24.628971 139656800745216 logging_writer.py:48] [55700] global_step=55700, grad_norm=4.114998817443848, loss=1.899714708328247
I0128 12:47:58.679167 139656809137920 logging_writer.py:48] [55800] global_step=55800, grad_norm=3.42128586769104, loss=1.8748042583465576
I0128 12:48:32.731289 139656800745216 logging_writer.py:48] [55900] global_step=55900, grad_norm=3.850667953491211, loss=1.9879634380340576
I0128 12:49:06.789134 139656809137920 logging_writer.py:48] [56000] global_step=56000, grad_norm=3.9544482231140137, loss=1.9570279121398926
I0128 12:49:40.856098 139656800745216 logging_writer.py:48] [56100] global_step=56100, grad_norm=3.609529733657837, loss=1.8075979948043823
I0128 12:50:14.927059 139656809137920 logging_writer.py:48] [56200] global_step=56200, grad_norm=3.5124127864837646, loss=1.917876124382019
I0128 12:50:48.972794 139656800745216 logging_writer.py:48] [56300] global_step=56300, grad_norm=3.4484946727752686, loss=1.9083552360534668
I0128 12:51:23.009120 139656809137920 logging_writer.py:48] [56400] global_step=56400, grad_norm=3.5693719387054443, loss=1.8212122917175293
I0128 12:51:57.036468 139656800745216 logging_writer.py:48] [56500] global_step=56500, grad_norm=3.4409358501434326, loss=1.8043421506881714
I0128 12:52:31.278762 139656809137920 logging_writer.py:48] [56600] global_step=56600, grad_norm=3.9476940631866455, loss=1.801764726638794
I0128 12:53:05.329693 139656800745216 logging_writer.py:48] [56700] global_step=56700, grad_norm=3.4395949840545654, loss=2.008626937866211
I0128 12:53:39.375872 139656809137920 logging_writer.py:48] [56800] global_step=56800, grad_norm=4.4088006019592285, loss=1.8162615299224854
I0128 12:53:43.613169 139822745589568 spec.py:321] Evaluating on the training split.
I0128 12:53:49.774330 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 12:53:58.553274 139822745589568 spec.py:349] Evaluating on the test split.
I0128 12:54:01.069853 139822745589568 submission_runner.py:408] Time since start: 20100.79s, 	Step: 56814, 	{'train/accuracy': 0.685965359210968, 'train/loss': 1.2393864393234253, 'validation/accuracy': 0.6195799708366394, 'validation/loss': 1.5674865245819092, 'validation/num_examples': 50000, 'test/accuracy': 0.49500003457069397, 'test/loss': 2.303964853286743, 'test/num_examples': 10000, 'score': 19415.29590034485, 'total_duration': 20100.787999868393, 'accumulated_submission_time': 19415.29590034485, 'accumulated_eval_time': 682.3041090965271, 'accumulated_logging_time': 1.252126932144165}
I0128 12:54:01.106314 139656834316032 logging_writer.py:48] [56814] accumulated_eval_time=682.304109, accumulated_logging_time=1.252127, accumulated_submission_time=19415.295900, global_step=56814, preemption_count=0, score=19415.295900, test/accuracy=0.495000, test/loss=2.303965, test/num_examples=10000, total_duration=20100.788000, train/accuracy=0.685965, train/loss=1.239386, validation/accuracy=0.619580, validation/loss=1.567487, validation/num_examples=50000
I0128 12:54:30.716865 139658730145536 logging_writer.py:48] [56900] global_step=56900, grad_norm=4.045173645019531, loss=1.8169817924499512
I0128 12:55:04.757187 139656834316032 logging_writer.py:48] [57000] global_step=57000, grad_norm=3.748932123184204, loss=1.9158611297607422
I0128 12:55:38.810805 139658730145536 logging_writer.py:48] [57100] global_step=57100, grad_norm=3.0204684734344482, loss=1.6231135129928589
I0128 12:56:12.852221 139656834316032 logging_writer.py:48] [57200] global_step=57200, grad_norm=3.4142086505889893, loss=1.8074934482574463
I0128 12:56:46.911901 139658730145536 logging_writer.py:48] [57300] global_step=57300, grad_norm=3.828162670135498, loss=1.8615206480026245
I0128 12:57:20.972348 139656834316032 logging_writer.py:48] [57400] global_step=57400, grad_norm=3.618501901626587, loss=1.7154184579849243
I0128 12:57:55.070006 139658730145536 logging_writer.py:48] [57500] global_step=57500, grad_norm=4.169082164764404, loss=1.805701732635498
I0128 12:58:29.096313 139656834316032 logging_writer.py:48] [57600] global_step=57600, grad_norm=3.926813840866089, loss=1.897357702255249
I0128 12:59:03.260651 139658730145536 logging_writer.py:48] [57700] global_step=57700, grad_norm=3.9902870655059814, loss=1.7997163534164429
I0128 12:59:37.302326 139656834316032 logging_writer.py:48] [57800] global_step=57800, grad_norm=3.5894980430603027, loss=1.9544105529785156
I0128 13:00:11.350560 139658730145536 logging_writer.py:48] [57900] global_step=57900, grad_norm=4.530187129974365, loss=1.8247082233428955
I0128 13:00:45.389026 139656834316032 logging_writer.py:48] [58000] global_step=58000, grad_norm=3.384746551513672, loss=1.7892765998840332
I0128 13:01:19.453435 139658730145536 logging_writer.py:48] [58100] global_step=58100, grad_norm=3.827507734298706, loss=1.9367787837982178
I0128 13:01:53.518740 139656834316032 logging_writer.py:48] [58200] global_step=58200, grad_norm=3.6730847358703613, loss=1.834593653678894
I0128 13:02:27.562697 139658730145536 logging_writer.py:48] [58300] global_step=58300, grad_norm=4.111893177032471, loss=1.7814182043075562
I0128 13:02:31.112352 139822745589568 spec.py:321] Evaluating on the training split.
I0128 13:02:37.263094 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 13:02:45.987309 139822745589568 spec.py:349] Evaluating on the test split.
I0128 13:02:48.548615 139822745589568 submission_runner.py:408] Time since start: 20628.27s, 	Step: 58312, 	{'train/accuracy': 0.6573660373687744, 'train/loss': 1.369436264038086, 'validation/accuracy': 0.6055799722671509, 'validation/loss': 1.6424763202667236, 'validation/num_examples': 50000, 'test/accuracy': 0.484000027179718, 'test/loss': 2.3704142570495605, 'test/num_examples': 10000, 'score': 19925.24132657051, 'total_duration': 20628.266762018204, 'accumulated_submission_time': 19925.24132657051, 'accumulated_eval_time': 699.7403359413147, 'accumulated_logging_time': 1.298933506011963}
I0128 13:02:48.579405 139655609571072 logging_writer.py:48] [58312] accumulated_eval_time=699.740336, accumulated_logging_time=1.298934, accumulated_submission_time=19925.241327, global_step=58312, preemption_count=0, score=19925.241327, test/accuracy=0.484000, test/loss=2.370414, test/num_examples=10000, total_duration=20628.266762, train/accuracy=0.657366, train/loss=1.369436, validation/accuracy=0.605580, validation/loss=1.642476, validation/num_examples=50000
I0128 13:03:18.839301 139655617963776 logging_writer.py:48] [58400] global_step=58400, grad_norm=3.601675510406494, loss=1.9179095029830933
I0128 13:03:52.869851 139655609571072 logging_writer.py:48] [58500] global_step=58500, grad_norm=3.3294711112976074, loss=1.744352102279663
I0128 13:04:26.915048 139655617963776 logging_writer.py:48] [58600] global_step=58600, grad_norm=3.5725483894348145, loss=1.8952163457870483
I0128 13:05:00.994231 139655609571072 logging_writer.py:48] [58700] global_step=58700, grad_norm=3.172834634780884, loss=1.8844271898269653
I0128 13:05:35.124159 139655617963776 logging_writer.py:48] [58800] global_step=58800, grad_norm=4.039165019989014, loss=1.77779221534729
I0128 13:06:09.176392 139655609571072 logging_writer.py:48] [58900] global_step=58900, grad_norm=3.734724998474121, loss=1.7558215856552124
I0128 13:06:43.263426 139655617963776 logging_writer.py:48] [59000] global_step=59000, grad_norm=3.344580888748169, loss=1.8390663862228394
I0128 13:07:17.308851 139655609571072 logging_writer.py:48] [59100] global_step=59100, grad_norm=3.357027053833008, loss=1.7316361665725708
I0128 13:07:51.382499 139655617963776 logging_writer.py:48] [59200] global_step=59200, grad_norm=3.46280574798584, loss=1.9163239002227783
I0128 13:08:25.409724 139655609571072 logging_writer.py:48] [59300] global_step=59300, grad_norm=3.4312903881073, loss=1.7630261182785034
I0128 13:08:59.447107 139655617963776 logging_writer.py:48] [59400] global_step=59400, grad_norm=3.7820544242858887, loss=1.7247246503829956
I0128 13:09:33.465365 139655609571072 logging_writer.py:48] [59500] global_step=59500, grad_norm=3.794677257537842, loss=1.860285997390747
I0128 13:10:07.509011 139655617963776 logging_writer.py:48] [59600] global_step=59600, grad_norm=3.325570583343506, loss=1.8222142457962036
I0128 13:10:41.560130 139655609571072 logging_writer.py:48] [59700] global_step=59700, grad_norm=3.187715530395508, loss=1.7945092916488647
I0128 13:11:15.621157 139655617963776 logging_writer.py:48] [59800] global_step=59800, grad_norm=4.1070237159729, loss=1.8573079109191895
I0128 13:11:18.823765 139822745589568 spec.py:321] Evaluating on the training split.
I0128 13:11:24.925573 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 13:11:33.562346 139822745589568 spec.py:349] Evaluating on the test split.
I0128 13:11:36.100554 139822745589568 submission_runner.py:408] Time since start: 21155.82s, 	Step: 59811, 	{'train/accuracy': 0.6717155575752258, 'train/loss': 1.299277424812317, 'validation/accuracy': 0.6213600039482117, 'validation/loss': 1.5658512115478516, 'validation/num_examples': 50000, 'test/accuracy': 0.49710002541542053, 'test/loss': 2.3210856914520264, 'test/num_examples': 10000, 'score': 20435.426945209503, 'total_duration': 21155.81867671013, 'accumulated_submission_time': 20435.426945209503, 'accumulated_eval_time': 717.0170676708221, 'accumulated_logging_time': 1.3389678001403809}
I0128 13:11:36.130804 139656666543872 logging_writer.py:48] [59811] accumulated_eval_time=717.017068, accumulated_logging_time=1.338968, accumulated_submission_time=20435.426945, global_step=59811, preemption_count=0, score=20435.426945, test/accuracy=0.497100, test/loss=2.321086, test/num_examples=10000, total_duration=21155.818677, train/accuracy=0.671716, train/loss=1.299277, validation/accuracy=0.621360, validation/loss=1.565851, validation/num_examples=50000
I0128 13:12:06.839414 139656834316032 logging_writer.py:48] [59900] global_step=59900, grad_norm=3.4083569049835205, loss=1.8029999732971191
I0128 13:12:40.846768 139656666543872 logging_writer.py:48] [60000] global_step=60000, grad_norm=3.6687545776367188, loss=1.7645654678344727
I0128 13:13:14.896570 139656834316032 logging_writer.py:48] [60100] global_step=60100, grad_norm=3.738889217376709, loss=1.8787016868591309
I0128 13:13:48.932617 139656666543872 logging_writer.py:48] [60200] global_step=60200, grad_norm=3.679673671722412, loss=1.7295228242874146
I0128 13:14:22.961982 139656834316032 logging_writer.py:48] [60300] global_step=60300, grad_norm=3.8952548503875732, loss=1.8846766948699951
I0128 13:14:57.013365 139656666543872 logging_writer.py:48] [60400] global_step=60400, grad_norm=3.910991668701172, loss=1.6290366649627686
I0128 13:15:31.057858 139656834316032 logging_writer.py:48] [60500] global_step=60500, grad_norm=3.717129945755005, loss=1.8355692625045776
I0128 13:16:05.091964 139656666543872 logging_writer.py:48] [60600] global_step=60600, grad_norm=3.5847983360290527, loss=1.810585618019104
I0128 13:16:39.149273 139656834316032 logging_writer.py:48] [60700] global_step=60700, grad_norm=3.9905662536621094, loss=1.737931251525879
I0128 13:17:13.215241 139656666543872 logging_writer.py:48] [60800] global_step=60800, grad_norm=3.194753646850586, loss=1.7990561723709106
I0128 13:17:47.267675 139656834316032 logging_writer.py:48] [60900] global_step=60900, grad_norm=3.9743454456329346, loss=1.8724950551986694
I0128 13:18:21.373049 139656666543872 logging_writer.py:48] [61000] global_step=61000, grad_norm=3.91609525680542, loss=1.8349589109420776
I0128 13:18:55.422094 139656834316032 logging_writer.py:48] [61100] global_step=61100, grad_norm=3.480617046356201, loss=1.824556827545166
I0128 13:19:29.490392 139656666543872 logging_writer.py:48] [61200] global_step=61200, grad_norm=4.101248741149902, loss=1.990896463394165
I0128 13:20:03.538502 139656834316032 logging_writer.py:48] [61300] global_step=61300, grad_norm=3.391857624053955, loss=1.868997573852539
I0128 13:20:06.399102 139822745589568 spec.py:321] Evaluating on the training split.
I0128 13:20:12.497142 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 13:20:21.258293 139822745589568 spec.py:349] Evaluating on the test split.
I0128 13:20:23.797694 139822745589568 submission_runner.py:408] Time since start: 21683.52s, 	Step: 61310, 	{'train/accuracy': 0.6558912396430969, 'train/loss': 1.3650057315826416, 'validation/accuracy': 0.6165199875831604, 'validation/loss': 1.608451247215271, 'validation/num_examples': 50000, 'test/accuracy': 0.4895000159740448, 'test/loss': 2.3405680656433105, 'test/num_examples': 10000, 'score': 20945.637128591537, 'total_duration': 21683.5158367157, 'accumulated_submission_time': 20945.637128591537, 'accumulated_eval_time': 734.4156177043915, 'accumulated_logging_time': 1.3787786960601807}
I0128 13:20:23.825714 139655609571072 logging_writer.py:48] [61310] accumulated_eval_time=734.415618, accumulated_logging_time=1.378779, accumulated_submission_time=20945.637129, global_step=61310, preemption_count=0, score=20945.637129, test/accuracy=0.489500, test/loss=2.340568, test/num_examples=10000, total_duration=21683.515837, train/accuracy=0.655891, train/loss=1.365006, validation/accuracy=0.616520, validation/loss=1.608451, validation/num_examples=50000
I0128 13:20:54.792487 139655617963776 logging_writer.py:48] [61400] global_step=61400, grad_norm=3.5611913204193115, loss=1.8779268264770508
I0128 13:21:28.813654 139655609571072 logging_writer.py:48] [61500] global_step=61500, grad_norm=3.934920072555542, loss=1.7690387964248657
I0128 13:22:02.897949 139655617963776 logging_writer.py:48] [61600] global_step=61600, grad_norm=4.1365647315979, loss=1.7622050046920776
I0128 13:22:36.962361 139655609571072 logging_writer.py:48] [61700] global_step=61700, grad_norm=3.7189841270446777, loss=1.8612070083618164
I0128 13:23:11.001765 139655617963776 logging_writer.py:48] [61800] global_step=61800, grad_norm=3.6032752990722656, loss=1.7561007738113403
I0128 13:23:45.046085 139655609571072 logging_writer.py:48] [61900] global_step=61900, grad_norm=3.9119462966918945, loss=1.9192349910736084
I0128 13:24:19.091047 139655617963776 logging_writer.py:48] [62000] global_step=62000, grad_norm=3.7365221977233887, loss=1.7424582242965698
I0128 13:24:53.321673 139655609571072 logging_writer.py:48] [62100] global_step=62100, grad_norm=3.7723629474639893, loss=1.8358749151229858
I0128 13:25:27.373927 139655617963776 logging_writer.py:48] [62200] global_step=62200, grad_norm=3.326936721801758, loss=1.784908413887024
I0128 13:26:01.407617 139655609571072 logging_writer.py:48] [62300] global_step=62300, grad_norm=3.7454190254211426, loss=1.766788363456726
I0128 13:26:35.460657 139655617963776 logging_writer.py:48] [62400] global_step=62400, grad_norm=3.8596549034118652, loss=1.8270924091339111
I0128 13:27:09.510341 139655609571072 logging_writer.py:48] [62500] global_step=62500, grad_norm=3.6499404907226562, loss=1.9256097078323364
I0128 13:27:43.568168 139655617963776 logging_writer.py:48] [62600] global_step=62600, grad_norm=3.6696932315826416, loss=1.7150317430496216
I0128 13:28:17.629565 139655609571072 logging_writer.py:48] [62700] global_step=62700, grad_norm=3.7004363536834717, loss=1.9034761190414429
I0128 13:28:51.673900 139655617963776 logging_writer.py:48] [62800] global_step=62800, grad_norm=4.123714447021484, loss=1.8591110706329346
I0128 13:28:53.863032 139822745589568 spec.py:321] Evaluating on the training split.
I0128 13:28:59.985651 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 13:29:08.753674 139822745589568 spec.py:349] Evaluating on the test split.
I0128 13:29:11.282099 139822745589568 submission_runner.py:408] Time since start: 22211.00s, 	Step: 62808, 	{'train/accuracy': 0.6518056392669678, 'train/loss': 1.3800477981567383, 'validation/accuracy': 0.6077399849891663, 'validation/loss': 1.6119192838668823, 'validation/num_examples': 50000, 'test/accuracy': 0.4837000370025635, 'test/loss': 2.364459991455078, 'test/num_examples': 10000, 'score': 21455.61483645439, 'total_duration': 22211.000241994858, 'accumulated_submission_time': 21455.61483645439, 'accumulated_eval_time': 751.8346419334412, 'accumulated_logging_time': 1.415900468826294}
I0128 13:29:11.312266 139656666543872 logging_writer.py:48] [62808] accumulated_eval_time=751.834642, accumulated_logging_time=1.415900, accumulated_submission_time=21455.614836, global_step=62808, preemption_count=0, score=21455.614836, test/accuracy=0.483700, test/loss=2.364460, test/num_examples=10000, total_duration=22211.000242, train/accuracy=0.651806, train/loss=1.380048, validation/accuracy=0.607740, validation/loss=1.611919, validation/num_examples=50000
I0128 13:29:42.958890 139656834316032 logging_writer.py:48] [62900] global_step=62900, grad_norm=3.973144054412842, loss=1.773931860923767
I0128 13:30:16.941534 139656666543872 logging_writer.py:48] [63000] global_step=63000, grad_norm=4.151851654052734, loss=1.752487063407898
I0128 13:30:50.990352 139656834316032 logging_writer.py:48] [63100] global_step=63100, grad_norm=3.7702834606170654, loss=1.8589415550231934
I0128 13:31:25.172819 139656666543872 logging_writer.py:48] [63200] global_step=63200, grad_norm=3.9592671394348145, loss=1.7815356254577637
I0128 13:31:59.223704 139656834316032 logging_writer.py:48] [63300] global_step=63300, grad_norm=4.305785655975342, loss=2.000391721725464
I0128 13:32:33.293731 139656666543872 logging_writer.py:48] [63400] global_step=63400, grad_norm=4.800726890563965, loss=1.917569875717163
I0128 13:33:07.374993 139656834316032 logging_writer.py:48] [63500] global_step=63500, grad_norm=3.877544641494751, loss=1.8782386779785156
I0128 13:33:41.434159 139656666543872 logging_writer.py:48] [63600] global_step=63600, grad_norm=4.07448673248291, loss=1.7999868392944336
I0128 13:34:15.496604 139656834316032 logging_writer.py:48] [63700] global_step=63700, grad_norm=4.274529457092285, loss=1.8891948461532593
I0128 13:34:49.583671 139656666543872 logging_writer.py:48] [63800] global_step=63800, grad_norm=4.111441612243652, loss=1.7962267398834229
I0128 13:35:23.634089 139656834316032 logging_writer.py:48] [63900] global_step=63900, grad_norm=3.770927667617798, loss=1.7754944562911987
I0128 13:35:57.700722 139656666543872 logging_writer.py:48] [64000] global_step=64000, grad_norm=3.696678638458252, loss=1.7873332500457764
I0128 13:36:31.771635 139656834316032 logging_writer.py:48] [64100] global_step=64100, grad_norm=3.6938416957855225, loss=1.765123724937439
I0128 13:37:05.791254 139656666543872 logging_writer.py:48] [64200] global_step=64200, grad_norm=3.5370311737060547, loss=1.8643989562988281
I0128 13:37:39.916337 139656834316032 logging_writer.py:48] [64300] global_step=64300, grad_norm=3.428406238555908, loss=1.773816704750061
I0128 13:37:41.420050 139822745589568 spec.py:321] Evaluating on the training split.
I0128 13:37:47.601680 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 13:37:56.335830 139822745589568 spec.py:349] Evaluating on the test split.
I0128 13:37:58.847715 139822745589568 submission_runner.py:408] Time since start: 22738.57s, 	Step: 64306, 	{'train/accuracy': 0.6535594463348389, 'train/loss': 1.3952399492263794, 'validation/accuracy': 0.6021400094032288, 'validation/loss': 1.6519254446029663, 'validation/num_examples': 50000, 'test/accuracy': 0.48030000925064087, 'test/loss': 2.3926892280578613, 'test/num_examples': 10000, 'score': 21965.65988755226, 'total_duration': 22738.56585907936, 'accumulated_submission_time': 21965.65988755226, 'accumulated_eval_time': 769.2622895240784, 'accumulated_logging_time': 1.4578070640563965}
I0128 13:37:58.879398 139655617963776 logging_writer.py:48] [64306] accumulated_eval_time=769.262290, accumulated_logging_time=1.457807, accumulated_submission_time=21965.659888, global_step=64306, preemption_count=0, score=21965.659888, test/accuracy=0.480300, test/loss=2.392689, test/num_examples=10000, total_duration=22738.565859, train/accuracy=0.653559, train/loss=1.395240, validation/accuracy=0.602140, validation/loss=1.651925, validation/num_examples=50000
I0128 13:38:31.208096 139655626356480 logging_writer.py:48] [64400] global_step=64400, grad_norm=3.5561673641204834, loss=1.8126722574234009
I0128 13:39:05.212324 139655617963776 logging_writer.py:48] [64500] global_step=64500, grad_norm=3.574343204498291, loss=1.80195951461792
I0128 13:39:39.279373 139655626356480 logging_writer.py:48] [64600] global_step=64600, grad_norm=3.8912200927734375, loss=1.685373067855835
I0128 13:40:13.343846 139655617963776 logging_writer.py:48] [64700] global_step=64700, grad_norm=4.250892639160156, loss=1.7936363220214844
I0128 13:40:47.401724 139655626356480 logging_writer.py:48] [64800] global_step=64800, grad_norm=3.684296131134033, loss=1.8379586935043335
I0128 13:41:21.453640 139655617963776 logging_writer.py:48] [64900] global_step=64900, grad_norm=4.303371429443359, loss=1.6980212926864624
I0128 13:41:55.500422 139655626356480 logging_writer.py:48] [65000] global_step=65000, grad_norm=3.7148377895355225, loss=1.7818292379379272
I0128 13:42:29.554653 139655617963776 logging_writer.py:48] [65100] global_step=65100, grad_norm=3.809771776199341, loss=1.7514208555221558
I0128 13:43:03.609292 139655626356480 logging_writer.py:48] [65200] global_step=65200, grad_norm=3.755047559738159, loss=1.724304437637329
I0128 13:43:37.658360 139655617963776 logging_writer.py:48] [65300] global_step=65300, grad_norm=3.6985154151916504, loss=1.8197810649871826
I0128 13:44:11.755610 139655626356480 logging_writer.py:48] [65400] global_step=65400, grad_norm=3.5442752838134766, loss=1.7386054992675781
I0128 13:44:45.801985 139655617963776 logging_writer.py:48] [65500] global_step=65500, grad_norm=3.3568453788757324, loss=1.7138278484344482
I0128 13:45:19.852289 139655626356480 logging_writer.py:48] [65600] global_step=65600, grad_norm=3.370866298675537, loss=1.8021605014801025
I0128 13:45:53.898888 139655617963776 logging_writer.py:48] [65700] global_step=65700, grad_norm=3.643867254257202, loss=1.8665854930877686
I0128 13:46:27.945150 139655626356480 logging_writer.py:48] [65800] global_step=65800, grad_norm=3.623755693435669, loss=1.6647379398345947
I0128 13:46:29.111458 139822745589568 spec.py:321] Evaluating on the training split.
I0128 13:46:35.275796 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 13:46:43.981592 139822745589568 spec.py:349] Evaluating on the test split.
I0128 13:46:46.532152 139822745589568 submission_runner.py:408] Time since start: 23266.25s, 	Step: 65805, 	{'train/accuracy': 0.6921834945678711, 'train/loss': 1.207922101020813, 'validation/accuracy': 0.6225999593734741, 'validation/loss': 1.554673671722412, 'validation/num_examples': 50000, 'test/accuracy': 0.49970000982284546, 'test/loss': 2.25830078125, 'test/num_examples': 10000, 'score': 22475.830349206924, 'total_duration': 23266.250294208527, 'accumulated_submission_time': 22475.830349206924, 'accumulated_eval_time': 786.6829445362091, 'accumulated_logging_time': 1.5008704662322998}
I0128 13:46:46.566536 139656666543872 logging_writer.py:48] [65805] accumulated_eval_time=786.682945, accumulated_logging_time=1.500870, accumulated_submission_time=22475.830349, global_step=65805, preemption_count=0, score=22475.830349, test/accuracy=0.499700, test/loss=2.258301, test/num_examples=10000, total_duration=23266.250294, train/accuracy=0.692183, train/loss=1.207922, validation/accuracy=0.622600, validation/loss=1.554674, validation/num_examples=50000
I0128 13:47:19.234369 139656834316032 logging_writer.py:48] [65900] global_step=65900, grad_norm=3.791715383529663, loss=1.939565896987915
I0128 13:47:53.228101 139656666543872 logging_writer.py:48] [66000] global_step=66000, grad_norm=3.742295503616333, loss=1.8789691925048828
I0128 13:48:27.269559 139656834316032 logging_writer.py:48] [66100] global_step=66100, grad_norm=4.195257186889648, loss=1.8323386907577515
I0128 13:49:01.308165 139656666543872 logging_writer.py:48] [66200] global_step=66200, grad_norm=4.431745529174805, loss=1.9205116033554077
I0128 13:49:35.373908 139656834316032 logging_writer.py:48] [66300] global_step=66300, grad_norm=3.565906286239624, loss=1.8554259538650513
I0128 13:50:09.497737 139656666543872 logging_writer.py:48] [66400] global_step=66400, grad_norm=3.559424638748169, loss=1.8679323196411133
I0128 13:50:43.550974 139656834316032 logging_writer.py:48] [66500] global_step=66500, grad_norm=4.339645862579346, loss=1.8352744579315186
I0128 13:51:17.588282 139656666543872 logging_writer.py:48] [66600] global_step=66600, grad_norm=4.000571250915527, loss=1.7101027965545654
I0128 13:51:51.640570 139656834316032 logging_writer.py:48] [66700] global_step=66700, grad_norm=4.097347259521484, loss=1.806369662284851
I0128 13:52:25.689217 139656666543872 logging_writer.py:48] [66800] global_step=66800, grad_norm=3.521221399307251, loss=1.7224597930908203
I0128 13:52:59.744091 139656834316032 logging_writer.py:48] [66900] global_step=66900, grad_norm=3.5529110431671143, loss=1.8497058153152466
I0128 13:53:33.788174 139656666543872 logging_writer.py:48] [67000] global_step=67000, grad_norm=3.9234611988067627, loss=1.887231707572937
I0128 13:54:07.851865 139656834316032 logging_writer.py:48] [67100] global_step=67100, grad_norm=3.68349289894104, loss=1.9426182508468628
I0128 13:54:41.895533 139656666543872 logging_writer.py:48] [67200] global_step=67200, grad_norm=3.6625912189483643, loss=1.6872504949569702
I0128 13:55:15.964099 139656834316032 logging_writer.py:48] [67300] global_step=67300, grad_norm=4.058361053466797, loss=1.7243082523345947
I0128 13:55:16.790096 139822745589568 spec.py:321] Evaluating on the training split.
I0128 13:55:22.917150 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 13:55:31.452577 139822745589568 spec.py:349] Evaluating on the test split.
I0128 13:55:34.009910 139822745589568 submission_runner.py:408] Time since start: 23793.73s, 	Step: 67304, 	{'train/accuracy': 0.6910474896430969, 'train/loss': 1.2102947235107422, 'validation/accuracy': 0.6284999847412109, 'validation/loss': 1.5212945938110352, 'validation/num_examples': 50000, 'test/accuracy': 0.5111000537872314, 'test/loss': 2.2321712970733643, 'test/num_examples': 10000, 'score': 22985.990831136703, 'total_duration': 23793.728055477142, 'accumulated_submission_time': 22985.990831136703, 'accumulated_eval_time': 803.9027199745178, 'accumulated_logging_time': 1.547590732574463}
I0128 13:55:34.039208 139655617963776 logging_writer.py:48] [67304] accumulated_eval_time=803.902720, accumulated_logging_time=1.547591, accumulated_submission_time=22985.990831, global_step=67304, preemption_count=0, score=22985.990831, test/accuracy=0.511100, test/loss=2.232171, test/num_examples=10000, total_duration=23793.728055, train/accuracy=0.691047, train/loss=1.210295, validation/accuracy=0.628500, validation/loss=1.521295, validation/num_examples=50000
I0128 13:56:07.002844 139655626356480 logging_writer.py:48] [67400] global_step=67400, grad_norm=4.482993125915527, loss=1.9011425971984863
I0128 13:56:41.102311 139655617963776 logging_writer.py:48] [67500] global_step=67500, grad_norm=3.020871877670288, loss=1.7199070453643799
I0128 13:57:15.107501 139655626356480 logging_writer.py:48] [67600] global_step=67600, grad_norm=3.43330979347229, loss=1.8951746225357056
I0128 13:57:49.149114 139655617963776 logging_writer.py:48] [67700] global_step=67700, grad_norm=3.696491003036499, loss=1.8144515752792358
I0128 13:58:23.180929 139655626356480 logging_writer.py:48] [67800] global_step=67800, grad_norm=3.7178452014923096, loss=1.7787983417510986
I0128 13:58:57.220870 139655617963776 logging_writer.py:48] [67900] global_step=67900, grad_norm=3.679513692855835, loss=1.617113709449768
I0128 13:59:31.258873 139655626356480 logging_writer.py:48] [68000] global_step=68000, grad_norm=4.319189071655273, loss=1.7894402742385864
I0128 14:00:05.328905 139655617963776 logging_writer.py:48] [68100] global_step=68100, grad_norm=3.564666986465454, loss=1.8588720560073853
I0128 14:00:39.373669 139655626356480 logging_writer.py:48] [68200] global_step=68200, grad_norm=4.801330089569092, loss=1.9028346538543701
I0128 14:01:13.421095 139655617963776 logging_writer.py:48] [68300] global_step=68300, grad_norm=3.7474310398101807, loss=1.8213014602661133
I0128 14:01:47.467339 139655626356480 logging_writer.py:48] [68400] global_step=68400, grad_norm=4.316654205322266, loss=1.7589248418807983
I0128 14:02:21.521314 139655617963776 logging_writer.py:48] [68500] global_step=68500, grad_norm=3.660247802734375, loss=1.7699841260910034
I0128 14:02:55.633494 139655626356480 logging_writer.py:48] [68600] global_step=68600, grad_norm=3.284846067428589, loss=1.7100292444229126
I0128 14:03:29.680203 139655617963776 logging_writer.py:48] [68700] global_step=68700, grad_norm=3.493614912033081, loss=1.7101787328720093
I0128 14:04:03.743734 139655626356480 logging_writer.py:48] [68800] global_step=68800, grad_norm=4.650036811828613, loss=1.8160016536712646
I0128 14:04:04.236422 139822745589568 spec.py:321] Evaluating on the training split.
I0128 14:04:10.449174 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 14:04:19.206527 139822745589568 spec.py:349] Evaluating on the test split.
I0128 14:04:21.758141 139822745589568 submission_runner.py:408] Time since start: 24321.48s, 	Step: 68803, 	{'train/accuracy': 0.6715561151504517, 'train/loss': 1.3061954975128174, 'validation/accuracy': 0.614579975605011, 'validation/loss': 1.594861626625061, 'validation/num_examples': 50000, 'test/accuracy': 0.4935000240802765, 'test/loss': 2.3209471702575684, 'test/num_examples': 10000, 'score': 23496.128660917282, 'total_duration': 24321.476287841797, 'accumulated_submission_time': 23496.128660917282, 'accumulated_eval_time': 821.4244170188904, 'accumulated_logging_time': 1.5859100818634033}
I0128 14:04:21.790710 139655626356480 logging_writer.py:48] [68803] accumulated_eval_time=821.424417, accumulated_logging_time=1.585910, accumulated_submission_time=23496.128661, global_step=68803, preemption_count=0, score=23496.128661, test/accuracy=0.493500, test/loss=2.320947, test/num_examples=10000, total_duration=24321.476288, train/accuracy=0.671556, train/loss=1.306195, validation/accuracy=0.614580, validation/loss=1.594862, validation/num_examples=50000
I0128 14:04:55.143789 139656658151168 logging_writer.py:48] [68900] global_step=68900, grad_norm=3.7726643085479736, loss=1.7312004566192627
I0128 14:05:29.174259 139655626356480 logging_writer.py:48] [69000] global_step=69000, grad_norm=4.265146255493164, loss=1.835306167602539
I0128 14:06:03.226836 139656658151168 logging_writer.py:48] [69100] global_step=69100, grad_norm=4.046197414398193, loss=1.7687958478927612
I0128 14:06:37.246871 139655626356480 logging_writer.py:48] [69200] global_step=69200, grad_norm=3.7800161838531494, loss=1.845116376876831
I0128 14:07:11.287713 139656658151168 logging_writer.py:48] [69300] global_step=69300, grad_norm=3.4581615924835205, loss=1.7757927179336548
I0128 14:07:45.365297 139655626356480 logging_writer.py:48] [69400] global_step=69400, grad_norm=3.732640027999878, loss=1.794600486755371
I0128 14:08:19.409482 139656658151168 logging_writer.py:48] [69500] global_step=69500, grad_norm=5.936307430267334, loss=1.8511269092559814
I0128 14:08:53.436663 139655626356480 logging_writer.py:48] [69600] global_step=69600, grad_norm=3.8495829105377197, loss=1.7137762308120728
I0128 14:09:27.571017 139656658151168 logging_writer.py:48] [69700] global_step=69700, grad_norm=4.10405158996582, loss=1.7397873401641846
I0128 14:10:01.608692 139655626356480 logging_writer.py:48] [69800] global_step=69800, grad_norm=4.096768856048584, loss=1.6618397235870361
I0128 14:10:35.642367 139656658151168 logging_writer.py:48] [69900] global_step=69900, grad_norm=3.887305736541748, loss=1.8065698146820068
I0128 14:11:09.674115 139655626356480 logging_writer.py:48] [70000] global_step=70000, grad_norm=3.7581565380096436, loss=1.8016552925109863
I0128 14:11:43.728367 139656658151168 logging_writer.py:48] [70100] global_step=70100, grad_norm=3.9639816284179688, loss=1.8182638883590698
I0128 14:12:17.802815 139655626356480 logging_writer.py:48] [70200] global_step=70200, grad_norm=3.9772095680236816, loss=1.7603384256362915
I0128 14:12:51.848020 139656658151168 logging_writer.py:48] [70300] global_step=70300, grad_norm=4.803086280822754, loss=1.7394120693206787
I0128 14:12:51.855378 139822745589568 spec.py:321] Evaluating on the training split.
I0128 14:12:57.987858 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 14:13:07.097675 139822745589568 spec.py:349] Evaluating on the test split.
I0128 14:13:09.517535 139822745589568 submission_runner.py:408] Time since start: 24849.24s, 	Step: 70301, 	{'train/accuracy': 0.6846699714660645, 'train/loss': 1.244409203529358, 'validation/accuracy': 0.6282599568367004, 'validation/loss': 1.5239821672439575, 'validation/num_examples': 50000, 'test/accuracy': 0.5094000101089478, 'test/loss': 2.2113044261932373, 'test/num_examples': 10000, 'score': 24006.130012512207, 'total_duration': 24849.235661029816, 'accumulated_submission_time': 24006.130012512207, 'accumulated_eval_time': 839.0864970684052, 'accumulated_logging_time': 1.6295363903045654}
I0128 14:13:09.552425 139656649758464 logging_writer.py:48] [70301] accumulated_eval_time=839.086497, accumulated_logging_time=1.629536, accumulated_submission_time=24006.130013, global_step=70301, preemption_count=0, score=24006.130013, test/accuracy=0.509400, test/loss=2.211304, test/num_examples=10000, total_duration=24849.235661, train/accuracy=0.684670, train/loss=1.244409, validation/accuracy=0.628260, validation/loss=1.523982, validation/num_examples=50000
I0128 14:13:43.558714 139658730145536 logging_writer.py:48] [70400] global_step=70400, grad_norm=3.567124366760254, loss=1.7747559547424316
I0128 14:14:17.580724 139656649758464 logging_writer.py:48] [70500] global_step=70500, grad_norm=3.6009044647216797, loss=1.6860177516937256
I0128 14:14:51.643803 139658730145536 logging_writer.py:48] [70600] global_step=70600, grad_norm=3.996190309524536, loss=1.7887970209121704
I0128 14:15:25.685704 139656649758464 logging_writer.py:48] [70700] global_step=70700, grad_norm=3.923405408859253, loss=1.8634339570999146
I0128 14:15:59.785272 139658730145536 logging_writer.py:48] [70800] global_step=70800, grad_norm=4.04398250579834, loss=1.8591536283493042
I0128 14:16:33.816675 139656649758464 logging_writer.py:48] [70900] global_step=70900, grad_norm=3.8161652088165283, loss=1.7148752212524414
I0128 14:17:07.857343 139658730145536 logging_writer.py:48] [71000] global_step=71000, grad_norm=4.704802989959717, loss=1.856360673904419
I0128 14:17:41.896130 139656649758464 logging_writer.py:48] [71100] global_step=71100, grad_norm=3.4795444011688232, loss=1.657461404800415
I0128 14:18:15.944718 139658730145536 logging_writer.py:48] [71200] global_step=71200, grad_norm=4.5293192863464355, loss=1.750855565071106
I0128 14:18:49.996833 139656649758464 logging_writer.py:48] [71300] global_step=71300, grad_norm=4.45884895324707, loss=1.7646863460540771
I0128 14:19:24.045147 139658730145536 logging_writer.py:48] [71400] global_step=71400, grad_norm=3.7094905376434326, loss=1.7447826862335205
I0128 14:19:58.071211 139656649758464 logging_writer.py:48] [71500] global_step=71500, grad_norm=3.627858877182007, loss=1.8601813316345215
I0128 14:20:32.142288 139658730145536 logging_writer.py:48] [71600] global_step=71600, grad_norm=4.97130012512207, loss=1.7152647972106934
I0128 14:21:06.167706 139656649758464 logging_writer.py:48] [71700] global_step=71700, grad_norm=4.031789779663086, loss=1.8320293426513672
I0128 14:21:39.696716 139822745589568 spec.py:321] Evaluating on the training split.
I0128 14:21:45.877902 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 14:21:54.531479 139822745589568 spec.py:349] Evaluating on the test split.
I0128 14:21:57.048967 139822745589568 submission_runner.py:408] Time since start: 25376.77s, 	Step: 71800, 	{'train/accuracy': 0.6765784025192261, 'train/loss': 1.2757912874221802, 'validation/accuracy': 0.6304799914360046, 'validation/loss': 1.5133068561553955, 'validation/num_examples': 50000, 'test/accuracy': 0.510200023651123, 'test/loss': 2.217399835586548, 'test/num_examples': 10000, 'score': 24516.21312022209, 'total_duration': 25376.767111063004, 'accumulated_submission_time': 24516.21312022209, 'accumulated_eval_time': 856.4387171268463, 'accumulated_logging_time': 1.6738028526306152}
I0128 14:21:57.079583 139655609571072 logging_writer.py:48] [71800] accumulated_eval_time=856.438717, accumulated_logging_time=1.673803, accumulated_submission_time=24516.213120, global_step=71800, preemption_count=0, score=24516.213120, test/accuracy=0.510200, test/loss=2.217400, test/num_examples=10000, total_duration=25376.767111, train/accuracy=0.676578, train/loss=1.275791, validation/accuracy=0.630480, validation/loss=1.513307, validation/num_examples=50000
I0128 14:21:57.421468 139655617963776 logging_writer.py:48] [71800] global_step=71800, grad_norm=3.9714865684509277, loss=1.7049328088760376
I0128 14:22:31.497342 139655609571072 logging_writer.py:48] [71900] global_step=71900, grad_norm=3.9371862411499023, loss=1.6925344467163086
I0128 14:23:05.505341 139655617963776 logging_writer.py:48] [72000] global_step=72000, grad_norm=4.429511070251465, loss=1.782861590385437
I0128 14:23:39.553824 139655609571072 logging_writer.py:48] [72100] global_step=72100, grad_norm=4.082669734954834, loss=1.7354865074157715
I0128 14:24:13.583427 139655617963776 logging_writer.py:48] [72200] global_step=72200, grad_norm=3.2600250244140625, loss=1.7076722383499146
I0128 14:24:47.620384 139655609571072 logging_writer.py:48] [72300] global_step=72300, grad_norm=4.162722110748291, loss=1.8187750577926636
I0128 14:25:21.646464 139655617963776 logging_writer.py:48] [72400] global_step=72400, grad_norm=3.856039524078369, loss=1.9223376512527466
I0128 14:25:55.715138 139655609571072 logging_writer.py:48] [72500] global_step=72500, grad_norm=4.059991836547852, loss=1.782370924949646
I0128 14:26:29.787382 139655617963776 logging_writer.py:48] [72600] global_step=72600, grad_norm=3.853893280029297, loss=1.749875545501709
I0128 14:27:03.838018 139655609571072 logging_writer.py:48] [72700] global_step=72700, grad_norm=3.6270225048065186, loss=1.7861857414245605
I0128 14:27:37.897244 139655617963776 logging_writer.py:48] [72800] global_step=72800, grad_norm=3.9666357040405273, loss=1.7286417484283447
I0128 14:28:11.976946 139655609571072 logging_writer.py:48] [72900] global_step=72900, grad_norm=4.656034469604492, loss=1.7849621772766113
I0128 14:28:46.189409 139655617963776 logging_writer.py:48] [73000] global_step=73000, grad_norm=4.165320873260498, loss=1.7454991340637207
I0128 14:29:20.227279 139655609571072 logging_writer.py:48] [73100] global_step=73100, grad_norm=3.7293615341186523, loss=1.8735471963882446
I0128 14:29:54.276381 139655617963776 logging_writer.py:48] [73200] global_step=73200, grad_norm=3.994292974472046, loss=1.86943519115448
I0128 14:30:27.082612 139822745589568 spec.py:321] Evaluating on the training split.
I0128 14:30:33.323444 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 14:30:41.937160 139822745589568 spec.py:349] Evaluating on the test split.
I0128 14:30:44.454567 139822745589568 submission_runner.py:408] Time since start: 25904.17s, 	Step: 73298, 	{'train/accuracy': 0.6873405575752258, 'train/loss': 1.2287601232528687, 'validation/accuracy': 0.6338199973106384, 'validation/loss': 1.491397738456726, 'validation/num_examples': 50000, 'test/accuracy': 0.5108000040054321, 'test/loss': 2.190986156463623, 'test/num_examples': 10000, 'score': 25026.155697584152, 'total_duration': 25904.172393083572, 'accumulated_submission_time': 25026.155697584152, 'accumulated_eval_time': 873.810319185257, 'accumulated_logging_time': 1.7138514518737793}
I0128 14:30:44.486208 139655617963776 logging_writer.py:48] [73298] accumulated_eval_time=873.810319, accumulated_logging_time=1.713851, accumulated_submission_time=25026.155698, global_step=73298, preemption_count=0, score=25026.155698, test/accuracy=0.510800, test/loss=2.190986, test/num_examples=10000, total_duration=25904.172393, train/accuracy=0.687341, train/loss=1.228760, validation/accuracy=0.633820, validation/loss=1.491398, validation/num_examples=50000
I0128 14:30:45.526648 139656649758464 logging_writer.py:48] [73300] global_step=73300, grad_norm=3.827148199081421, loss=1.896099328994751
I0128 14:31:19.532001 139655617963776 logging_writer.py:48] [73400] global_step=73400, grad_norm=4.145694732666016, loss=1.6629551649093628
I0128 14:31:53.547735 139656649758464 logging_writer.py:48] [73500] global_step=73500, grad_norm=4.35047721862793, loss=1.843466877937317
I0128 14:32:27.600080 139655617963776 logging_writer.py:48] [73600] global_step=73600, grad_norm=3.950011968612671, loss=1.602766752243042
I0128 14:33:01.659467 139656649758464 logging_writer.py:48] [73700] global_step=73700, grad_norm=4.780004501342773, loss=1.7190824747085571
I0128 14:33:35.714500 139655617963776 logging_writer.py:48] [73800] global_step=73800, grad_norm=3.561905860900879, loss=1.711179494857788
I0128 14:34:09.774103 139656649758464 logging_writer.py:48] [73900] global_step=73900, grad_norm=3.798964262008667, loss=1.7851369380950928
I0128 14:34:43.807048 139655617963776 logging_writer.py:48] [74000] global_step=74000, grad_norm=3.6647446155548096, loss=1.7491583824157715
I0128 14:35:17.957601 139656649758464 logging_writer.py:48] [74100] global_step=74100, grad_norm=3.4590885639190674, loss=1.6413220167160034
I0128 14:35:51.970443 139655617963776 logging_writer.py:48] [74200] global_step=74200, grad_norm=4.115981101989746, loss=1.6033087968826294
I0128 14:36:26.023433 139656649758464 logging_writer.py:48] [74300] global_step=74300, grad_norm=3.9790403842926025, loss=1.7666009664535522
I0128 14:37:00.063835 139655617963776 logging_writer.py:48] [74400] global_step=74400, grad_norm=3.9620046615600586, loss=1.71293306350708
I0128 14:37:34.096760 139656649758464 logging_writer.py:48] [74500] global_step=74500, grad_norm=4.271871089935303, loss=1.70565927028656
I0128 14:38:08.160284 139655617963776 logging_writer.py:48] [74600] global_step=74600, grad_norm=3.811089038848877, loss=1.8381152153015137
I0128 14:38:42.236697 139656649758464 logging_writer.py:48] [74700] global_step=74700, grad_norm=4.445122718811035, loss=1.7215338945388794
I0128 14:39:14.728397 139822745589568 spec.py:321] Evaluating on the training split.
I0128 14:39:20.824063 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 14:39:29.633816 139822745589568 spec.py:349] Evaluating on the test split.
I0128 14:39:32.161872 139822745589568 submission_runner.py:408] Time since start: 26431.88s, 	Step: 74797, 	{'train/accuracy': 0.6872010231018066, 'train/loss': 1.2051925659179688, 'validation/accuracy': 0.6161800026893616, 'validation/loss': 1.587920069694519, 'validation/num_examples': 50000, 'test/accuracy': 0.5003000497817993, 'test/loss': 2.303654909133911, 'test/num_examples': 10000, 'score': 25536.339210748672, 'total_duration': 26431.880017757416, 'accumulated_submission_time': 25536.339210748672, 'accumulated_eval_time': 891.2437620162964, 'accumulated_logging_time': 1.7546777725219727}
I0128 14:39:32.192285 139655617963776 logging_writer.py:48] [74797] accumulated_eval_time=891.243762, accumulated_logging_time=1.754678, accumulated_submission_time=25536.339211, global_step=74797, preemption_count=0, score=25536.339211, test/accuracy=0.500300, test/loss=2.303655, test/num_examples=10000, total_duration=26431.880018, train/accuracy=0.687201, train/loss=1.205193, validation/accuracy=0.616180, validation/loss=1.587920, validation/num_examples=50000
I0128 14:39:33.563219 139656297445120 logging_writer.py:48] [74800] global_step=74800, grad_norm=4.030892372131348, loss=1.85933518409729
I0128 14:40:07.566206 139655617963776 logging_writer.py:48] [74900] global_step=74900, grad_norm=4.144493579864502, loss=1.7080379724502563
I0128 14:40:41.598751 139656297445120 logging_writer.py:48] [75000] global_step=75000, grad_norm=3.6205849647521973, loss=1.7077977657318115
I0128 14:41:15.641804 139655617963776 logging_writer.py:48] [75100] global_step=75100, grad_norm=3.7497661113739014, loss=1.7524937391281128
I0128 14:41:49.788406 139656297445120 logging_writer.py:48] [75200] global_step=75200, grad_norm=4.729228496551514, loss=1.747025966644287
I0128 14:42:23.828750 139655617963776 logging_writer.py:48] [75300] global_step=75300, grad_norm=3.919412136077881, loss=1.7887367010116577
I0128 14:42:57.850797 139656297445120 logging_writer.py:48] [75400] global_step=75400, grad_norm=3.883546829223633, loss=1.7637759447097778
I0128 14:43:31.915559 139655617963776 logging_writer.py:48] [75500] global_step=75500, grad_norm=3.8463709354400635, loss=1.810031771659851
I0128 14:44:05.994140 139656297445120 logging_writer.py:48] [75600] global_step=75600, grad_norm=3.7208073139190674, loss=1.870640754699707
I0128 14:44:40.041657 139655617963776 logging_writer.py:48] [75700] global_step=75700, grad_norm=3.5679807662963867, loss=1.762966275215149
I0128 14:45:14.090464 139656297445120 logging_writer.py:48] [75800] global_step=75800, grad_norm=4.6172075271606445, loss=1.7647850513458252
I0128 14:45:48.122938 139655617963776 logging_writer.py:48] [75900] global_step=75900, grad_norm=3.7015511989593506, loss=1.7401162385940552
I0128 14:46:22.165287 139656297445120 logging_writer.py:48] [76000] global_step=76000, grad_norm=4.161664009094238, loss=1.7671144008636475
I0128 14:46:56.227368 139655617963776 logging_writer.py:48] [76100] global_step=76100, grad_norm=3.822671413421631, loss=1.7255383729934692
I0128 14:47:30.242675 139656297445120 logging_writer.py:48] [76200] global_step=76200, grad_norm=3.4084677696228027, loss=1.7251640558242798
I0128 14:48:02.455206 139822745589568 spec.py:321] Evaluating on the training split.
I0128 14:48:08.626439 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 14:48:17.211058 139822745589568 spec.py:349] Evaluating on the test split.
I0128 14:48:19.747258 139822745589568 submission_runner.py:408] Time since start: 26959.47s, 	Step: 76296, 	{'train/accuracy': 0.6835737824440002, 'train/loss': 1.253710389137268, 'validation/accuracy': 0.6217799782752991, 'validation/loss': 1.5734446048736572, 'validation/num_examples': 50000, 'test/accuracy': 0.4935000240802765, 'test/loss': 2.3314461708068848, 'test/num_examples': 10000, 'score': 26046.54245686531, 'total_duration': 26959.465401887894, 'accumulated_submission_time': 26046.54245686531, 'accumulated_eval_time': 908.53577709198, 'accumulated_logging_time': 1.7942428588867188}
I0128 14:48:19.777760 139655626356480 logging_writer.py:48] [76296] accumulated_eval_time=908.535777, accumulated_logging_time=1.794243, accumulated_submission_time=26046.542457, global_step=76296, preemption_count=0, score=26046.542457, test/accuracy=0.493500, test/loss=2.331446, test/num_examples=10000, total_duration=26959.465402, train/accuracy=0.683574, train/loss=1.253710, validation/accuracy=0.621780, validation/loss=1.573445, validation/num_examples=50000
I0128 14:48:21.479379 139656649758464 logging_writer.py:48] [76300] global_step=76300, grad_norm=5.109127998352051, loss=1.7127493619918823
I0128 14:48:55.473919 139655626356480 logging_writer.py:48] [76400] global_step=76400, grad_norm=3.831993579864502, loss=1.8284603357315063
I0128 14:49:29.490442 139656649758464 logging_writer.py:48] [76500] global_step=76500, grad_norm=4.158608436584473, loss=1.680681824684143
I0128 14:50:03.534746 139655626356480 logging_writer.py:48] [76600] global_step=76600, grad_norm=3.738774061203003, loss=1.7549179792404175
I0128 14:50:37.598039 139656649758464 logging_writer.py:48] [76700] global_step=76700, grad_norm=3.9175779819488525, loss=1.7949211597442627
I0128 14:51:11.629736 139655626356480 logging_writer.py:48] [76800] global_step=76800, grad_norm=3.7539443969726562, loss=1.6998448371887207
I0128 14:51:45.676195 139656649758464 logging_writer.py:48] [76900] global_step=76900, grad_norm=3.9352364540100098, loss=1.7986444234848022
I0128 14:52:19.715478 139655626356480 logging_writer.py:48] [77000] global_step=77000, grad_norm=4.141637802124023, loss=1.7547154426574707
I0128 14:52:53.782010 139656649758464 logging_writer.py:48] [77100] global_step=77100, grad_norm=3.554527997970581, loss=1.789154291152954
I0128 14:53:27.840819 139655626356480 logging_writer.py:48] [77200] global_step=77200, grad_norm=3.913987398147583, loss=1.845594048500061
I0128 14:54:01.859949 139656649758464 logging_writer.py:48] [77300] global_step=77300, grad_norm=4.28369140625, loss=1.7480180263519287
I0128 14:54:35.959575 139655626356480 logging_writer.py:48] [77400] global_step=77400, grad_norm=4.2858476638793945, loss=1.7543120384216309
I0128 14:55:09.989961 139656649758464 logging_writer.py:48] [77500] global_step=77500, grad_norm=3.517991781234741, loss=1.7267026901245117
I0128 14:55:48.144728 139655626356480 logging_writer.py:48] [77600] global_step=77600, grad_norm=3.6173529624938965, loss=1.6631569862365723
I0128 14:56:22.218237 139656649758464 logging_writer.py:48] [77700] global_step=77700, grad_norm=4.427647590637207, loss=1.8320426940917969
I0128 14:56:49.935957 139822745589568 spec.py:321] Evaluating on the training split.
I0128 14:56:56.183483 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 14:57:04.968393 139822745589568 spec.py:349] Evaluating on the test split.
I0128 14:57:08.250957 139822745589568 submission_runner.py:408] Time since start: 27487.97s, 	Step: 77783, 	{'train/accuracy': 0.6978435516357422, 'train/loss': 1.2021422386169434, 'validation/accuracy': 0.6412799954414368, 'validation/loss': 1.4752492904663086, 'validation/num_examples': 50000, 'test/accuracy': 0.5133000016212463, 'test/loss': 2.162278652191162, 'test/num_examples': 10000, 'score': 26556.63892173767, 'total_duration': 27487.96911430359, 'accumulated_submission_time': 26556.63892173767, 'accumulated_eval_time': 926.8507552146912, 'accumulated_logging_time': 1.83475923538208}
I0128 14:57:08.279854 139656658151168 logging_writer.py:48] [77783] accumulated_eval_time=926.850755, accumulated_logging_time=1.834759, accumulated_submission_time=26556.638922, global_step=77783, preemption_count=0, score=26556.638922, test/accuracy=0.513300, test/loss=2.162279, test/num_examples=10000, total_duration=27487.969114, train/accuracy=0.697844, train/loss=1.202142, validation/accuracy=0.641280, validation/loss=1.475249, validation/num_examples=50000
I0128 14:57:14.409844 139656834316032 logging_writer.py:48] [77800] global_step=77800, grad_norm=4.05733585357666, loss=1.7975784540176392
I0128 14:57:48.420827 139656658151168 logging_writer.py:48] [77900] global_step=77900, grad_norm=4.109833240509033, loss=1.8195390701293945
I0128 14:58:22.435865 139656834316032 logging_writer.py:48] [78000] global_step=78000, grad_norm=3.524751901626587, loss=1.7721741199493408
I0128 14:58:56.470092 139656658151168 logging_writer.py:48] [78100] global_step=78100, grad_norm=3.8731446266174316, loss=1.7263031005859375
I0128 14:59:30.511613 139656834316032 logging_writer.py:48] [78200] global_step=78200, grad_norm=3.7867138385772705, loss=1.6461477279663086
I0128 15:00:04.582480 139656658151168 logging_writer.py:48] [78300] global_step=78300, grad_norm=3.6722850799560547, loss=1.649145245552063
I0128 15:00:38.640837 139656834316032 logging_writer.py:48] [78400] global_step=78400, grad_norm=4.243447303771973, loss=1.813307762145996
I0128 15:01:12.767066 139656658151168 logging_writer.py:48] [78500] global_step=78500, grad_norm=3.786245346069336, loss=1.7492756843566895
I0128 15:01:46.807578 139656834316032 logging_writer.py:48] [78600] global_step=78600, grad_norm=3.575455665588379, loss=1.7553644180297852
I0128 15:02:20.872256 139656658151168 logging_writer.py:48] [78700] global_step=78700, grad_norm=3.929666042327881, loss=1.5487629175186157
I0128 15:02:54.916582 139656834316032 logging_writer.py:48] [78800] global_step=78800, grad_norm=4.447516918182373, loss=1.7647933959960938
I0128 15:03:28.964631 139656658151168 logging_writer.py:48] [78900] global_step=78900, grad_norm=4.113376140594482, loss=1.8094422817230225
I0128 15:04:03.014627 139656834316032 logging_writer.py:48] [79000] global_step=79000, grad_norm=4.106769561767578, loss=1.7831354141235352
I0128 15:04:37.073417 139656658151168 logging_writer.py:48] [79100] global_step=79100, grad_norm=3.851987361907959, loss=1.6888129711151123
I0128 15:05:11.099093 139656834316032 logging_writer.py:48] [79200] global_step=79200, grad_norm=4.746864318847656, loss=1.948233723640442
I0128 15:05:38.482675 139822745589568 spec.py:321] Evaluating on the training split.
I0128 15:05:44.583302 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 15:05:53.440501 139822745589568 spec.py:349] Evaluating on the test split.
I0128 15:05:55.957102 139822745589568 submission_runner.py:408] Time since start: 28015.68s, 	Step: 79282, 	{'train/accuracy': 0.6960698366165161, 'train/loss': 1.1828243732452393, 'validation/accuracy': 0.6363999843597412, 'validation/loss': 1.4793654680252075, 'validation/num_examples': 50000, 'test/accuracy': 0.5126000046730042, 'test/loss': 2.2020113468170166, 'test/num_examples': 10000, 'score': 27066.782158851624, 'total_duration': 28015.67524456978, 'accumulated_submission_time': 27066.782158851624, 'accumulated_eval_time': 944.3251445293427, 'accumulated_logging_time': 1.8716328144073486}
I0128 15:05:55.991409 139655617963776 logging_writer.py:48] [79282] accumulated_eval_time=944.325145, accumulated_logging_time=1.871633, accumulated_submission_time=27066.782159, global_step=79282, preemption_count=0, score=27066.782159, test/accuracy=0.512600, test/loss=2.202011, test/num_examples=10000, total_duration=28015.675245, train/accuracy=0.696070, train/loss=1.182824, validation/accuracy=0.636400, validation/loss=1.479365, validation/num_examples=50000
I0128 15:06:02.479061 139655626356480 logging_writer.py:48] [79300] global_step=79300, grad_norm=4.077689170837402, loss=1.8026905059814453
I0128 15:06:36.447364 139655617963776 logging_writer.py:48] [79400] global_step=79400, grad_norm=4.152100563049316, loss=1.6206140518188477
I0128 15:07:10.472209 139655626356480 logging_writer.py:48] [79500] global_step=79500, grad_norm=4.396329879760742, loss=1.7445731163024902
I0128 15:07:44.572361 139655617963776 logging_writer.py:48] [79600] global_step=79600, grad_norm=3.95735502243042, loss=1.6589720249176025
I0128 15:08:18.640546 139655626356480 logging_writer.py:48] [79700] global_step=79700, grad_norm=3.8372268676757812, loss=1.6137690544128418
I0128 15:08:52.691556 139655617963776 logging_writer.py:48] [79800] global_step=79800, grad_norm=4.561295986175537, loss=1.7029550075531006
I0128 15:09:26.729728 139655626356480 logging_writer.py:48] [79900] global_step=79900, grad_norm=4.353876113891602, loss=1.6224336624145508
I0128 15:10:00.785119 139655617963776 logging_writer.py:48] [80000] global_step=80000, grad_norm=3.8904173374176025, loss=1.691765546798706
I0128 15:10:34.831784 139655626356480 logging_writer.py:48] [80100] global_step=80100, grad_norm=3.8574438095092773, loss=1.8351719379425049
I0128 15:11:08.894605 139655617963776 logging_writer.py:48] [80200] global_step=80200, grad_norm=4.003726482391357, loss=1.7383590936660767
I0128 15:11:42.936614 139655626356480 logging_writer.py:48] [80300] global_step=80300, grad_norm=3.9210686683654785, loss=1.795919418334961
I0128 15:12:16.985555 139655617963776 logging_writer.py:48] [80400] global_step=80400, grad_norm=4.092197895050049, loss=1.638035774230957
I0128 15:12:51.010382 139655626356480 logging_writer.py:48] [80500] global_step=80500, grad_norm=4.350946426391602, loss=1.7578741312026978
I0128 15:13:25.082042 139655617963776 logging_writer.py:48] [80600] global_step=80600, grad_norm=3.9474868774414062, loss=1.7271852493286133
I0128 15:13:59.160276 139655626356480 logging_writer.py:48] [80700] global_step=80700, grad_norm=4.287719249725342, loss=1.6654952764511108
I0128 15:14:26.221059 139822745589568 spec.py:321] Evaluating on the training split.
I0128 15:14:32.407819 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 15:14:41.116145 139822745589568 spec.py:349] Evaluating on the test split.
I0128 15:14:43.645105 139822745589568 submission_runner.py:408] Time since start: 28543.36s, 	Step: 80781, 	{'train/accuracy': 0.6879384517669678, 'train/loss': 1.2187047004699707, 'validation/accuracy': 0.6388999819755554, 'validation/loss': 1.4817255735397339, 'validation/num_examples': 50000, 'test/accuracy': 0.5117000341415405, 'test/loss': 2.2074272632598877, 'test/num_examples': 10000, 'score': 27576.946058273315, 'total_duration': 28543.36324763298, 'accumulated_submission_time': 27576.946058273315, 'accumulated_eval_time': 961.7491703033447, 'accumulated_logging_time': 1.9183545112609863}
I0128 15:14:43.678120 139655609571072 logging_writer.py:48] [80781] accumulated_eval_time=961.749170, accumulated_logging_time=1.918355, accumulated_submission_time=27576.946058, global_step=80781, preemption_count=0, score=27576.946058, test/accuracy=0.511700, test/loss=2.207427, test/num_examples=10000, total_duration=28543.363248, train/accuracy=0.687938, train/loss=1.218705, validation/accuracy=0.638900, validation/loss=1.481726, validation/num_examples=50000
I0128 15:14:50.464752 139655617963776 logging_writer.py:48] [80800] global_step=80800, grad_norm=4.379898548126221, loss=1.6146876811981201
I0128 15:15:24.466256 139655609571072 logging_writer.py:48] [80900] global_step=80900, grad_norm=4.612380504608154, loss=1.7193413972854614
I0128 15:15:58.459792 139655617963776 logging_writer.py:48] [81000] global_step=81000, grad_norm=3.8280553817749023, loss=1.7414612770080566
I0128 15:16:32.470007 139655609571072 logging_writer.py:48] [81100] global_step=81100, grad_norm=4.113564968109131, loss=1.5633816719055176
I0128 15:17:06.514455 139655617963776 logging_writer.py:48] [81200] global_step=81200, grad_norm=4.365695953369141, loss=1.5977833271026611
I0128 15:17:40.553682 139655609571072 logging_writer.py:48] [81300] global_step=81300, grad_norm=3.9345178604125977, loss=1.717548131942749
I0128 15:18:14.588248 139655617963776 logging_writer.py:48] [81400] global_step=81400, grad_norm=3.7849061489105225, loss=1.7783892154693604
I0128 15:18:48.650300 139655609571072 logging_writer.py:48] [81500] global_step=81500, grad_norm=3.665607452392578, loss=1.8799970149993896
I0128 15:19:22.687332 139655617963776 logging_writer.py:48] [81600] global_step=81600, grad_norm=4.0324387550354, loss=1.7478070259094238
I0128 15:19:56.762103 139655609571072 logging_writer.py:48] [81700] global_step=81700, grad_norm=4.449368476867676, loss=1.676328420639038
I0128 15:20:30.865453 139655617963776 logging_writer.py:48] [81800] global_step=81800, grad_norm=4.035181522369385, loss=1.7778221368789673
I0128 15:21:04.958642 139655609571072 logging_writer.py:48] [81900] global_step=81900, grad_norm=3.912889003753662, loss=1.7346172332763672
I0128 15:21:39.008773 139655617963776 logging_writer.py:48] [82000] global_step=82000, grad_norm=4.186103343963623, loss=1.7072277069091797
I0128 15:22:13.073004 139655609571072 logging_writer.py:48] [82100] global_step=82100, grad_norm=4.210862159729004, loss=1.71475088596344
I0128 15:22:47.123365 139655617963776 logging_writer.py:48] [82200] global_step=82200, grad_norm=3.942857265472412, loss=1.7214024066925049
I0128 15:23:13.812567 139822745589568 spec.py:321] Evaluating on the training split.
I0128 15:23:19.928863 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 15:23:28.571774 139822745589568 spec.py:349] Evaluating on the test split.
I0128 15:23:31.155428 139822745589568 submission_runner.py:408] Time since start: 29070.87s, 	Step: 82280, 	{'train/accuracy': 0.6951131820678711, 'train/loss': 1.1844233274459839, 'validation/accuracy': 0.6406799554824829, 'validation/loss': 1.4681005477905273, 'validation/num_examples': 50000, 'test/accuracy': 0.5145000219345093, 'test/loss': 2.1780965328216553, 'test/num_examples': 10000, 'score': 28087.02109003067, 'total_duration': 29070.87357234955, 'accumulated_submission_time': 28087.02109003067, 'accumulated_eval_time': 979.0919954776764, 'accumulated_logging_time': 1.960249662399292}
I0128 15:23:31.187239 139656834316032 logging_writer.py:48] [82280] accumulated_eval_time=979.091995, accumulated_logging_time=1.960250, accumulated_submission_time=28087.021090, global_step=82280, preemption_count=0, score=28087.021090, test/accuracy=0.514500, test/loss=2.178097, test/num_examples=10000, total_duration=29070.873572, train/accuracy=0.695113, train/loss=1.184423, validation/accuracy=0.640680, validation/loss=1.468101, validation/num_examples=50000
I0128 15:23:38.339146 139658730145536 logging_writer.py:48] [82300] global_step=82300, grad_norm=3.823158025741577, loss=1.727541446685791
I0128 15:24:12.351650 139656834316032 logging_writer.py:48] [82400] global_step=82400, grad_norm=3.939570903778076, loss=1.6705836057662964
I0128 15:24:46.343670 139658730145536 logging_writer.py:48] [82500] global_step=82500, grad_norm=3.892648220062256, loss=1.7602636814117432
I0128 15:25:20.363907 139656834316032 logging_writer.py:48] [82600] global_step=82600, grad_norm=4.168801784515381, loss=1.6920571327209473
I0128 15:25:54.406073 139658730145536 logging_writer.py:48] [82700] global_step=82700, grad_norm=3.6957478523254395, loss=1.7665835618972778
I0128 15:26:28.458915 139656834316032 logging_writer.py:48] [82800] global_step=82800, grad_norm=5.578182697296143, loss=1.6037160158157349
I0128 15:27:02.564483 139658730145536 logging_writer.py:48] [82900] global_step=82900, grad_norm=4.602062702178955, loss=1.7900002002716064
I0128 15:27:36.611440 139656834316032 logging_writer.py:48] [83000] global_step=83000, grad_norm=4.328262805938721, loss=1.735360860824585
I0128 15:28:10.651740 139658730145536 logging_writer.py:48] [83100] global_step=83100, grad_norm=4.057531833648682, loss=1.6919975280761719
I0128 15:28:44.707965 139656834316032 logging_writer.py:48] [83200] global_step=83200, grad_norm=3.797283411026001, loss=1.6128807067871094
I0128 15:29:18.749198 139658730145536 logging_writer.py:48] [83300] global_step=83300, grad_norm=4.393401622772217, loss=1.8270115852355957
I0128 15:29:52.772627 139656834316032 logging_writer.py:48] [83400] global_step=83400, grad_norm=4.387860298156738, loss=1.6723425388336182
I0128 15:30:26.812199 139658730145536 logging_writer.py:48] [83500] global_step=83500, grad_norm=4.150897979736328, loss=1.6595871448516846
I0128 15:31:00.843559 139656834316032 logging_writer.py:48] [83600] global_step=83600, grad_norm=3.961592674255371, loss=1.7058182954788208
I0128 15:31:34.870942 139658730145536 logging_writer.py:48] [83700] global_step=83700, grad_norm=5.789330959320068, loss=1.886620044708252
I0128 15:32:01.211939 139822745589568 spec.py:321] Evaluating on the training split.
I0128 15:32:07.540220 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 15:32:16.082108 139822745589568 spec.py:349] Evaluating on the test split.
I0128 15:32:18.565420 139822745589568 submission_runner.py:408] Time since start: 29598.28s, 	Step: 83779, 	{'train/accuracy': 0.7388990521430969, 'train/loss': 1.0035078525543213, 'validation/accuracy': 0.6438999772071838, 'validation/loss': 1.44945228099823, 'validation/num_examples': 50000, 'test/accuracy': 0.5115000009536743, 'test/loss': 2.1780617237091064, 'test/num_examples': 10000, 'score': 28596.984345436096, 'total_duration': 29598.28356528282, 'accumulated_submission_time': 28596.984345436096, 'accumulated_eval_time': 996.4454569816589, 'accumulated_logging_time': 2.001596212387085}
I0128 15:32:18.598352 139655626356480 logging_writer.py:48] [83779] accumulated_eval_time=996.445457, accumulated_logging_time=2.001596, accumulated_submission_time=28596.984345, global_step=83779, preemption_count=0, score=28596.984345, test/accuracy=0.511500, test/loss=2.178062, test/num_examples=10000, total_duration=29598.283565, train/accuracy=0.738899, train/loss=1.003508, validation/accuracy=0.643900, validation/loss=1.449452, validation/num_examples=50000
I0128 15:32:26.093170 139656297445120 logging_writer.py:48] [83800] global_step=83800, grad_norm=3.620529890060425, loss=1.8174046277999878
I0128 15:33:00.248549 139655626356480 logging_writer.py:48] [83900] global_step=83900, grad_norm=4.038114070892334, loss=1.7534983158111572
I0128 15:33:34.239764 139656297445120 logging_writer.py:48] [84000] global_step=84000, grad_norm=4.260257720947266, loss=1.791468858718872
I0128 15:34:08.286653 139655626356480 logging_writer.py:48] [84100] global_step=84100, grad_norm=4.3193817138671875, loss=1.7653372287750244
I0128 15:34:42.314218 139656297445120 logging_writer.py:48] [84200] global_step=84200, grad_norm=3.9410648345947266, loss=1.7282148599624634
I0128 15:35:16.390710 139655626356480 logging_writer.py:48] [84300] global_step=84300, grad_norm=4.298847198486328, loss=1.7379858493804932
I0128 15:35:50.417965 139656297445120 logging_writer.py:48] [84400] global_step=84400, grad_norm=4.075752258300781, loss=1.762373924255371
I0128 15:36:24.488070 139655626356480 logging_writer.py:48] [84500] global_step=84500, grad_norm=4.206105709075928, loss=1.7952392101287842
I0128 15:36:58.542668 139656297445120 logging_writer.py:48] [84600] global_step=84600, grad_norm=4.05502986907959, loss=1.7564386129379272
I0128 15:37:32.590409 139655626356480 logging_writer.py:48] [84700] global_step=84700, grad_norm=4.09551477432251, loss=1.7745261192321777
I0128 15:38:06.662923 139656297445120 logging_writer.py:48] [84800] global_step=84800, grad_norm=3.9950878620147705, loss=1.6513800621032715
I0128 15:38:40.682645 139655626356480 logging_writer.py:48] [84900] global_step=84900, grad_norm=3.7024905681610107, loss=1.7036420106887817
I0128 15:39:14.740225 139656297445120 logging_writer.py:48] [85000] global_step=85000, grad_norm=3.9255106449127197, loss=1.7276986837387085
I0128 15:39:48.950411 139655626356480 logging_writer.py:48] [85100] global_step=85100, grad_norm=3.884852647781372, loss=1.7476483583450317
I0128 15:40:23.000846 139656297445120 logging_writer.py:48] [85200] global_step=85200, grad_norm=4.731960296630859, loss=1.746814489364624
I0128 15:40:48.684768 139822745589568 spec.py:321] Evaluating on the training split.
I0128 15:40:54.813261 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 15:41:04.004263 139822745589568 spec.py:349] Evaluating on the test split.
I0128 15:41:06.605392 139822745589568 submission_runner.py:408] Time since start: 30126.32s, 	Step: 85277, 	{'train/accuracy': 0.7179328799247742, 'train/loss': 1.081685185432434, 'validation/accuracy': 0.6489399671554565, 'validation/loss': 1.4340031147003174, 'validation/num_examples': 50000, 'test/accuracy': 0.522599995136261, 'test/loss': 2.1304209232330322, 'test/num_examples': 10000, 'score': 29107.010613918304, 'total_duration': 30126.32354569435, 'accumulated_submission_time': 29107.010613918304, 'accumulated_eval_time': 1014.3660583496094, 'accumulated_logging_time': 2.0440704822540283}
I0128 15:41:06.633900 139656834316032 logging_writer.py:48] [85277] accumulated_eval_time=1014.366058, accumulated_logging_time=2.044070, accumulated_submission_time=29107.010614, global_step=85277, preemption_count=0, score=29107.010614, test/accuracy=0.522600, test/loss=2.130421, test/num_examples=10000, total_duration=30126.323546, train/accuracy=0.717933, train/loss=1.081685, validation/accuracy=0.648940, validation/loss=1.434003, validation/num_examples=50000
I0128 15:41:14.802150 139658730145536 logging_writer.py:48] [85300] global_step=85300, grad_norm=3.667832136154175, loss=1.6823079586029053
I0128 15:41:48.795661 139656834316032 logging_writer.py:48] [85400] global_step=85400, grad_norm=4.010569095611572, loss=1.602442741394043
I0128 15:42:22.830487 139658730145536 logging_writer.py:48] [85500] global_step=85500, grad_norm=4.141944408416748, loss=1.5874674320220947
I0128 15:42:56.842430 139656834316032 logging_writer.py:48] [85600] global_step=85600, grad_norm=4.510951995849609, loss=1.5989137887954712
I0128 15:43:30.882411 139658730145536 logging_writer.py:48] [85700] global_step=85700, grad_norm=4.234880447387695, loss=1.6855707168579102
I0128 15:44:04.899780 139656834316032 logging_writer.py:48] [85800] global_step=85800, grad_norm=3.688617706298828, loss=1.7031075954437256
I0128 15:44:38.926195 139658730145536 logging_writer.py:48] [85900] global_step=85900, grad_norm=3.9752161502838135, loss=1.7181334495544434
I0128 15:45:12.940905 139656834316032 logging_writer.py:48] [86000] global_step=86000, grad_norm=4.747817516326904, loss=1.7205804586410522
I0128 15:45:46.955146 139658730145536 logging_writer.py:48] [86100] global_step=86100, grad_norm=3.9255177974700928, loss=1.7579660415649414
I0128 15:46:21.208458 139656834316032 logging_writer.py:48] [86200] global_step=86200, grad_norm=4.043106555938721, loss=1.7703839540481567
I0128 15:46:55.243152 139658730145536 logging_writer.py:48] [86300] global_step=86300, grad_norm=4.256375312805176, loss=1.5834652185440063
I0128 15:47:29.298841 139656834316032 logging_writer.py:48] [86400] global_step=86400, grad_norm=4.387894630432129, loss=1.7193443775177002
I0128 15:48:03.343975 139658730145536 logging_writer.py:48] [86500] global_step=86500, grad_norm=4.094200134277344, loss=1.7144204378128052
I0128 15:48:37.379907 139656834316032 logging_writer.py:48] [86600] global_step=86600, grad_norm=4.304083824157715, loss=1.6871209144592285
I0128 15:49:11.441392 139658730145536 logging_writer.py:48] [86700] global_step=86700, grad_norm=4.425259113311768, loss=1.7952698469161987
I0128 15:49:36.771631 139822745589568 spec.py:321] Evaluating on the training split.
I0128 15:49:42.904614 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 15:49:51.706967 139822745589568 spec.py:349] Evaluating on the test split.
I0128 15:49:54.263353 139822745589568 submission_runner.py:408] Time since start: 30653.98s, 	Step: 86776, 	{'train/accuracy': 0.7063137888908386, 'train/loss': 1.1358715295791626, 'validation/accuracy': 0.6468200087547302, 'validation/loss': 1.4428404569625854, 'validation/num_examples': 50000, 'test/accuracy': 0.5206000208854675, 'test/loss': 2.138397455215454, 'test/num_examples': 10000, 'score': 29617.09086871147, 'total_duration': 30653.98149752617, 'accumulated_submission_time': 29617.09086871147, 'accumulated_eval_time': 1031.8577575683594, 'accumulated_logging_time': 2.080768346786499}
I0128 15:49:54.295392 139655626356480 logging_writer.py:48] [86776] accumulated_eval_time=1031.857758, accumulated_logging_time=2.080768, accumulated_submission_time=29617.090869, global_step=86776, preemption_count=0, score=29617.090869, test/accuracy=0.520600, test/loss=2.138397, test/num_examples=10000, total_duration=30653.981498, train/accuracy=0.706314, train/loss=1.135872, validation/accuracy=0.646820, validation/loss=1.442840, validation/num_examples=50000
I0128 15:50:02.808894 139656297445120 logging_writer.py:48] [86800] global_step=86800, grad_norm=4.596951484680176, loss=1.6185352802276611
I0128 15:50:36.785369 139655626356480 logging_writer.py:48] [86900] global_step=86900, grad_norm=3.8801279067993164, loss=1.7380948066711426
I0128 15:51:10.788818 139656297445120 logging_writer.py:48] [87000] global_step=87000, grad_norm=3.889101982116699, loss=1.6540838479995728
I0128 15:51:44.798714 139655626356480 logging_writer.py:48] [87100] global_step=87100, grad_norm=3.965646266937256, loss=1.6465697288513184
I0128 15:52:18.980561 139656297445120 logging_writer.py:48] [87200] global_step=87200, grad_norm=4.221691131591797, loss=1.6135563850402832
I0128 15:52:53.017604 139655626356480 logging_writer.py:48] [87300] global_step=87300, grad_norm=4.4840006828308105, loss=1.5948885679244995
I0128 15:53:27.071967 139656297445120 logging_writer.py:48] [87400] global_step=87400, grad_norm=5.113937854766846, loss=1.7057305574417114
I0128 15:54:01.121195 139655626356480 logging_writer.py:48] [87500] global_step=87500, grad_norm=4.155773639678955, loss=1.5976454019546509
I0128 15:54:35.169485 139656297445120 logging_writer.py:48] [87600] global_step=87600, grad_norm=4.1403422355651855, loss=1.703680396080017
I0128 15:55:09.229657 139655626356480 logging_writer.py:48] [87700] global_step=87700, grad_norm=5.0962042808532715, loss=1.674059271812439
I0128 15:55:43.268943 139656297445120 logging_writer.py:48] [87800] global_step=87800, grad_norm=4.305117607116699, loss=1.6090781688690186
I0128 15:56:17.301039 139655626356480 logging_writer.py:48] [87900] global_step=87900, grad_norm=4.0409698486328125, loss=1.6151679754257202
I0128 15:56:51.336302 139656297445120 logging_writer.py:48] [88000] global_step=88000, grad_norm=3.9859111309051514, loss=1.5540293455123901
I0128 15:57:25.398681 139655626356480 logging_writer.py:48] [88100] global_step=88100, grad_norm=3.911986827850342, loss=1.6258480548858643
I0128 15:57:59.443071 139656297445120 logging_writer.py:48] [88200] global_step=88200, grad_norm=4.2473859786987305, loss=1.6678963899612427
I0128 15:58:24.446989 139822745589568 spec.py:321] Evaluating on the training split.
I0128 15:58:30.701695 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 15:58:39.329220 139822745589568 spec.py:349] Evaluating on the test split.
I0128 15:58:42.086411 139822745589568 submission_runner.py:408] Time since start: 31181.80s, 	Step: 88275, 	{'train/accuracy': 0.7068120241165161, 'train/loss': 1.13710355758667, 'validation/accuracy': 0.6526600122451782, 'validation/loss': 1.4295099973678589, 'validation/num_examples': 50000, 'test/accuracy': 0.5166000127792358, 'test/loss': 2.154176950454712, 'test/num_examples': 10000, 'score': 30127.182915449142, 'total_duration': 31181.804544210434, 'accumulated_submission_time': 30127.182915449142, 'accumulated_eval_time': 1049.4971315860748, 'accumulated_logging_time': 2.121476888656616}
I0128 15:58:42.118002 139655617963776 logging_writer.py:48] [88275] accumulated_eval_time=1049.497132, accumulated_logging_time=2.121477, accumulated_submission_time=30127.182915, global_step=88275, preemption_count=0, score=30127.182915, test/accuracy=0.516600, test/loss=2.154177, test/num_examples=10000, total_duration=31181.804544, train/accuracy=0.706812, train/loss=1.137104, validation/accuracy=0.652660, validation/loss=1.429510, validation/num_examples=50000
I0128 15:58:52.025777 139655626356480 logging_writer.py:48] [88300] global_step=88300, grad_norm=4.10804557800293, loss=1.6245009899139404
I0128 15:59:25.971831 139655617963776 logging_writer.py:48] [88400] global_step=88400, grad_norm=4.289387226104736, loss=1.7571218013763428
I0128 15:59:59.968016 139655626356480 logging_writer.py:48] [88500] global_step=88500, grad_norm=4.081479549407959, loss=1.693618893623352
I0128 16:00:33.984533 139655617963776 logging_writer.py:48] [88600] global_step=88600, grad_norm=4.411355972290039, loss=1.7422730922698975
I0128 16:01:08.042448 139655626356480 logging_writer.py:48] [88700] global_step=88700, grad_norm=4.301963806152344, loss=1.703658938407898
I0128 16:01:42.060036 139655617963776 logging_writer.py:48] [88800] global_step=88800, grad_norm=4.337989807128906, loss=1.5092196464538574
I0128 16:02:16.101850 139655626356480 logging_writer.py:48] [88900] global_step=88900, grad_norm=4.435481071472168, loss=1.7211108207702637
I0128 16:02:50.149014 139655617963776 logging_writer.py:48] [89000] global_step=89000, grad_norm=4.231410026550293, loss=1.6065924167633057
I0128 16:03:24.205749 139655626356480 logging_writer.py:48] [89100] global_step=89100, grad_norm=3.706017017364502, loss=1.5990217924118042
I0128 16:03:58.277570 139655617963776 logging_writer.py:48] [89200] global_step=89200, grad_norm=3.9778547286987305, loss=1.6826382875442505
I0128 16:04:32.312549 139655626356480 logging_writer.py:48] [89300] global_step=89300, grad_norm=3.9664785861968994, loss=1.7185590267181396
I0128 16:05:06.478353 139655617963776 logging_writer.py:48] [89400] global_step=89400, grad_norm=3.9639344215393066, loss=1.581666111946106
I0128 16:05:40.522368 139655626356480 logging_writer.py:48] [89500] global_step=89500, grad_norm=4.466045379638672, loss=1.580402135848999
I0128 16:06:14.578088 139655617963776 logging_writer.py:48] [89600] global_step=89600, grad_norm=4.2790703773498535, loss=1.635096549987793
I0128 16:06:48.614475 139655626356480 logging_writer.py:48] [89700] global_step=89700, grad_norm=3.8714444637298584, loss=1.53524911403656
I0128 16:07:12.259944 139822745589568 spec.py:321] Evaluating on the training split.
I0128 16:07:18.383704 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 16:07:27.010685 139822745589568 spec.py:349] Evaluating on the test split.
I0128 16:07:29.626429 139822745589568 submission_runner.py:408] Time since start: 31709.34s, 	Step: 89771, 	{'train/accuracy': 0.6945750713348389, 'train/loss': 1.1966772079467773, 'validation/accuracy': 0.6418799757957458, 'validation/loss': 1.463532567024231, 'validation/num_examples': 50000, 'test/accuracy': 0.5118000507354736, 'test/loss': 2.200810432434082, 'test/num_examples': 10000, 'score': 30636.219479560852, 'total_duration': 31709.34457540512, 'accumulated_submission_time': 30636.219479560852, 'accumulated_eval_time': 1066.8635828495026, 'accumulated_logging_time': 3.207822322845459}
I0128 16:07:29.662033 139656297445120 logging_writer.py:48] [89771] accumulated_eval_time=1066.863583, accumulated_logging_time=3.207822, accumulated_submission_time=30636.219480, global_step=89771, preemption_count=0, score=30636.219480, test/accuracy=0.511800, test/loss=2.200810, test/num_examples=10000, total_duration=31709.344575, train/accuracy=0.694575, train/loss=1.196677, validation/accuracy=0.641880, validation/loss=1.463533, validation/num_examples=50000
I0128 16:07:39.879382 139656649758464 logging_writer.py:48] [89800] global_step=89800, grad_norm=5.005990982055664, loss=1.5897037982940674
I0128 16:08:13.850235 139656297445120 logging_writer.py:48] [89900] global_step=89900, grad_norm=5.105868339538574, loss=1.76854407787323
I0128 16:08:47.876761 139656649758464 logging_writer.py:48] [90000] global_step=90000, grad_norm=4.0795578956604, loss=1.642654299736023
I0128 16:09:21.899500 139656297445120 logging_writer.py:48] [90100] global_step=90100, grad_norm=4.2062153816223145, loss=1.742599606513977
I0128 16:09:55.947499 139656649758464 logging_writer.py:48] [90200] global_step=90200, grad_norm=4.13734245300293, loss=1.6574103832244873
I0128 16:10:29.996417 139656297445120 logging_writer.py:48] [90300] global_step=90300, grad_norm=4.146205425262451, loss=1.6721315383911133
I0128 16:11:04.027035 139656649758464 logging_writer.py:48] [90400] global_step=90400, grad_norm=4.606571197509766, loss=1.6527937650680542
I0128 16:11:38.252646 139656297445120 logging_writer.py:48] [90500] global_step=90500, grad_norm=4.133448600769043, loss=1.6525647640228271
I0128 16:12:12.274431 139656649758464 logging_writer.py:48] [90600] global_step=90600, grad_norm=4.69500732421875, loss=1.6410207748413086
I0128 16:12:46.324436 139656297445120 logging_writer.py:48] [90700] global_step=90700, grad_norm=3.9545600414276123, loss=1.8208740949630737
I0128 16:13:20.365506 139656649758464 logging_writer.py:48] [90800] global_step=90800, grad_norm=4.4742584228515625, loss=1.700442910194397
I0128 16:13:54.420717 139656297445120 logging_writer.py:48] [90900] global_step=90900, grad_norm=3.940800189971924, loss=1.6136261224746704
I0128 16:14:28.453386 139656649758464 logging_writer.py:48] [91000] global_step=91000, grad_norm=4.127523422241211, loss=1.7426624298095703
I0128 16:15:02.499889 139656297445120 logging_writer.py:48] [91100] global_step=91100, grad_norm=4.01750373840332, loss=1.7004988193511963
I0128 16:15:36.549880 139656649758464 logging_writer.py:48] [91200] global_step=91200, grad_norm=3.891484260559082, loss=1.6231285333633423
I0128 16:15:59.853601 139822745589568 spec.py:321] Evaluating on the training split.
I0128 16:16:06.156920 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 16:16:14.649894 139822745589568 spec.py:349] Evaluating on the test split.
I0128 16:16:17.146178 139822745589568 submission_runner.py:408] Time since start: 32236.86s, 	Step: 91270, 	{'train/accuracy': 0.7092235088348389, 'train/loss': 1.1346157789230347, 'validation/accuracy': 0.6538000106811523, 'validation/loss': 1.4101835489273071, 'validation/num_examples': 50000, 'test/accuracy': 0.5267000198364258, 'test/loss': 2.1272940635681152, 'test/num_examples': 10000, 'score': 31146.34582209587, 'total_duration': 32236.864321231842, 'accumulated_submission_time': 31146.34582209587, 'accumulated_eval_time': 1084.156126499176, 'accumulated_logging_time': 3.255371332168579}
I0128 16:16:17.182539 139655609571072 logging_writer.py:48] [91270] accumulated_eval_time=1084.156126, accumulated_logging_time=3.255371, accumulated_submission_time=31146.345822, global_step=91270, preemption_count=0, score=31146.345822, test/accuracy=0.526700, test/loss=2.127294, test/num_examples=10000, total_duration=32236.864321, train/accuracy=0.709224, train/loss=1.134616, validation/accuracy=0.653800, validation/loss=1.410184, validation/num_examples=50000
I0128 16:16:27.738222 139655617963776 logging_writer.py:48] [91300] global_step=91300, grad_norm=3.751058340072632, loss=1.582838535308838
I0128 16:17:01.756621 139655609571072 logging_writer.py:48] [91400] global_step=91400, grad_norm=4.190531253814697, loss=1.6234688758850098
I0128 16:17:35.748564 139655617963776 logging_writer.py:48] [91500] global_step=91500, grad_norm=3.70082426071167, loss=1.5438199043273926
I0128 16:18:09.959003 139655609571072 logging_writer.py:48] [91600] global_step=91600, grad_norm=4.194886684417725, loss=1.6905299425125122
I0128 16:18:44.001240 139655617963776 logging_writer.py:48] [91700] global_step=91700, grad_norm=4.811161994934082, loss=1.6194000244140625
I0128 16:19:18.022775 139655609571072 logging_writer.py:48] [91800] global_step=91800, grad_norm=4.167595386505127, loss=1.7482768297195435
I0128 16:19:52.084143 139655617963776 logging_writer.py:48] [91900] global_step=91900, grad_norm=4.088532447814941, loss=1.6459102630615234
I0128 16:20:26.101803 139655609571072 logging_writer.py:48] [92000] global_step=92000, grad_norm=4.110213279724121, loss=1.658157229423523
I0128 16:21:00.155942 139655617963776 logging_writer.py:48] [92100] global_step=92100, grad_norm=4.314942359924316, loss=1.8353116512298584
I0128 16:21:34.198719 139655609571072 logging_writer.py:48] [92200] global_step=92200, grad_norm=3.9364893436431885, loss=1.6318066120147705
I0128 16:22:08.259162 139655617963776 logging_writer.py:48] [92300] global_step=92300, grad_norm=4.321209907531738, loss=1.5239512920379639
I0128 16:22:42.329954 139655609571072 logging_writer.py:48] [92400] global_step=92400, grad_norm=4.74972677230835, loss=1.6387417316436768
I0128 16:23:16.365230 139655617963776 logging_writer.py:48] [92500] global_step=92500, grad_norm=3.8494467735290527, loss=1.631600022315979
I0128 16:23:50.389780 139655609571072 logging_writer.py:48] [92600] global_step=92600, grad_norm=3.825061559677124, loss=1.660271167755127
I0128 16:24:24.542621 139655617963776 logging_writer.py:48] [92700] global_step=92700, grad_norm=4.01922082901001, loss=1.5658336877822876
I0128 16:24:47.154444 139822745589568 spec.py:321] Evaluating on the training split.
I0128 16:24:53.297992 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 16:25:01.871355 139822745589568 spec.py:349] Evaluating on the test split.
I0128 16:25:04.474174 139822745589568 submission_runner.py:408] Time since start: 32764.19s, 	Step: 92768, 	{'train/accuracy': 0.7113958597183228, 'train/loss': 1.1191548109054565, 'validation/accuracy': 0.6482200026512146, 'validation/loss': 1.4321719408035278, 'validation/num_examples': 50000, 'test/accuracy': 0.5174000263214111, 'test/loss': 2.1670353412628174, 'test/num_examples': 10000, 'score': 31656.254772424698, 'total_duration': 32764.192311525345, 'accumulated_submission_time': 31656.254772424698, 'accumulated_eval_time': 1101.4758217334747, 'accumulated_logging_time': 3.3038907051086426}
I0128 16:25:04.512461 139655626356480 logging_writer.py:48] [92768] accumulated_eval_time=1101.475822, accumulated_logging_time=3.303891, accumulated_submission_time=31656.254772, global_step=92768, preemption_count=0, score=31656.254772, test/accuracy=0.517400, test/loss=2.167035, test/num_examples=10000, total_duration=32764.192312, train/accuracy=0.711396, train/loss=1.119155, validation/accuracy=0.648220, validation/loss=1.432172, validation/num_examples=50000
I0128 16:25:15.726845 139656658151168 logging_writer.py:48] [92800] global_step=92800, grad_norm=4.332424640655518, loss=1.6823334693908691
I0128 16:25:49.726989 139655626356480 logging_writer.py:48] [92900] global_step=92900, grad_norm=4.914697647094727, loss=1.6028920412063599
I0128 16:26:23.759488 139656658151168 logging_writer.py:48] [93000] global_step=93000, grad_norm=4.353906154632568, loss=1.6577203273773193
I0128 16:26:57.758846 139655626356480 logging_writer.py:48] [93100] global_step=93100, grad_norm=4.588626384735107, loss=1.679532527923584
I0128 16:27:31.805220 139656658151168 logging_writer.py:48] [93200] global_step=93200, grad_norm=4.184805393218994, loss=1.6263327598571777
I0128 16:28:05.843268 139655626356480 logging_writer.py:48] [93300] global_step=93300, grad_norm=4.415202617645264, loss=1.6465684175491333
I0128 16:28:39.864297 139656658151168 logging_writer.py:48] [93400] global_step=93400, grad_norm=3.975768804550171, loss=1.5126254558563232
I0128 16:29:13.900519 139655626356480 logging_writer.py:48] [93500] global_step=93500, grad_norm=4.032371997833252, loss=1.7059439420700073
I0128 16:29:47.944271 139656658151168 logging_writer.py:48] [93600] global_step=93600, grad_norm=4.26828145980835, loss=1.5750278234481812
I0128 16:30:21.962197 139655626356480 logging_writer.py:48] [93700] global_step=93700, grad_norm=4.307137489318848, loss=1.6910932064056396
I0128 16:30:56.055665 139656658151168 logging_writer.py:48] [93800] global_step=93800, grad_norm=4.573404312133789, loss=1.7404659986495972
I0128 16:31:30.106467 139655626356480 logging_writer.py:48] [93900] global_step=93900, grad_norm=3.8450546264648438, loss=1.558551549911499
I0128 16:32:04.161185 139656658151168 logging_writer.py:48] [94000] global_step=94000, grad_norm=4.750588893890381, loss=1.6503887176513672
I0128 16:32:38.218782 139655626356480 logging_writer.py:48] [94100] global_step=94100, grad_norm=3.932004451751709, loss=1.6208853721618652
I0128 16:33:12.266500 139656658151168 logging_writer.py:48] [94200] global_step=94200, grad_norm=4.8938493728637695, loss=1.6927342414855957
I0128 16:33:34.560731 139822745589568 spec.py:321] Evaluating on the training split.
I0128 16:33:40.668010 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 16:33:49.496998 139822745589568 spec.py:349] Evaluating on the test split.
I0128 16:33:52.034259 139822745589568 submission_runner.py:408] Time since start: 33291.75s, 	Step: 94267, 	{'train/accuracy': 0.7361487150192261, 'train/loss': 1.0138471126556396, 'validation/accuracy': 0.659500002861023, 'validation/loss': 1.3887938261032104, 'validation/num_examples': 50000, 'test/accuracy': 0.5303000211715698, 'test/loss': 2.0873241424560547, 'test/num_examples': 10000, 'score': 32166.24189376831, 'total_duration': 33291.75237035751, 'accumulated_submission_time': 32166.24189376831, 'accumulated_eval_time': 1118.9492797851562, 'accumulated_logging_time': 3.351661443710327}
I0128 16:33:52.072348 139655626356480 logging_writer.py:48] [94267] accumulated_eval_time=1118.949280, accumulated_logging_time=3.351661, accumulated_submission_time=32166.241894, global_step=94267, preemption_count=0, score=32166.241894, test/accuracy=0.530300, test/loss=2.087324, test/num_examples=10000, total_duration=33291.752370, train/accuracy=0.736149, train/loss=1.013847, validation/accuracy=0.659500, validation/loss=1.388794, validation/num_examples=50000
I0128 16:34:03.644596 139656297445120 logging_writer.py:48] [94300] global_step=94300, grad_norm=4.259779453277588, loss=1.6616673469543457
I0128 16:34:37.648613 139655626356480 logging_writer.py:48] [94400] global_step=94400, grad_norm=4.153964996337891, loss=1.53486168384552
I0128 16:35:11.670059 139656297445120 logging_writer.py:48] [94500] global_step=94500, grad_norm=4.093407154083252, loss=1.734392523765564
I0128 16:35:45.706813 139655626356480 logging_writer.py:48] [94600] global_step=94600, grad_norm=4.223355770111084, loss=1.6566574573516846
I0128 16:36:19.763731 139656297445120 logging_writer.py:48] [94700] global_step=94700, grad_norm=4.671474456787109, loss=1.5776957273483276
I0128 16:36:53.783047 139655626356480 logging_writer.py:48] [94800] global_step=94800, grad_norm=4.20737886428833, loss=1.6408175230026245
I0128 16:37:27.908024 139656297445120 logging_writer.py:48] [94900] global_step=94900, grad_norm=4.0229010581970215, loss=1.6462584733963013
I0128 16:38:01.928638 139655626356480 logging_writer.py:48] [95000] global_step=95000, grad_norm=4.406037330627441, loss=1.6491485834121704
I0128 16:38:35.992815 139656297445120 logging_writer.py:48] [95100] global_step=95100, grad_norm=4.334139347076416, loss=1.6012420654296875
I0128 16:39:10.013046 139655626356480 logging_writer.py:48] [95200] global_step=95200, grad_norm=4.230351448059082, loss=1.5692410469055176
I0128 16:39:44.061378 139656297445120 logging_writer.py:48] [95300] global_step=95300, grad_norm=4.432152271270752, loss=1.543838620185852
I0128 16:40:18.082010 139655626356480 logging_writer.py:48] [95400] global_step=95400, grad_norm=4.379080772399902, loss=1.680285096168518
I0128 16:40:52.101906 139656297445120 logging_writer.py:48] [95500] global_step=95500, grad_norm=4.561418056488037, loss=1.7779357433319092
I0128 16:41:26.167450 139655626356480 logging_writer.py:48] [95600] global_step=95600, grad_norm=4.777940273284912, loss=1.6266865730285645
I0128 16:42:00.192037 139656297445120 logging_writer.py:48] [95700] global_step=95700, grad_norm=4.152402400970459, loss=1.6097232103347778
I0128 16:42:22.122165 139822745589568 spec.py:321] Evaluating on the training split.
I0128 16:42:28.321658 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 16:42:36.785997 139822745589568 spec.py:349] Evaluating on the test split.
I0128 16:42:39.302636 139822745589568 submission_runner.py:408] Time since start: 33819.02s, 	Step: 95766, 	{'train/accuracy': 0.7229153513908386, 'train/loss': 1.0641008615493774, 'validation/accuracy': 0.6566199660301208, 'validation/loss': 1.4057345390319824, 'validation/num_examples': 50000, 'test/accuracy': 0.5320000052452087, 'test/loss': 2.1097888946533203, 'test/num_examples': 10000, 'score': 32676.229960918427, 'total_duration': 33819.02078318596, 'accumulated_submission_time': 32676.229960918427, 'accumulated_eval_time': 1136.129715681076, 'accumulated_logging_time': 3.401322364807129}
I0128 16:42:39.338140 139655617963776 logging_writer.py:48] [95766] accumulated_eval_time=1136.129716, accumulated_logging_time=3.401322, accumulated_submission_time=32676.229961, global_step=95766, preemption_count=0, score=32676.229961, test/accuracy=0.532000, test/loss=2.109789, test/num_examples=10000, total_duration=33819.020783, train/accuracy=0.722915, train/loss=1.064101, validation/accuracy=0.656620, validation/loss=1.405735, validation/num_examples=50000
I0128 16:42:51.231196 139656666543872 logging_writer.py:48] [95800] global_step=95800, grad_norm=4.375743865966797, loss=1.6014747619628906
I0128 16:43:25.256211 139655617963776 logging_writer.py:48] [95900] global_step=95900, grad_norm=4.463104248046875, loss=1.7252240180969238
I0128 16:43:59.449213 139656666543872 logging_writer.py:48] [96000] global_step=96000, grad_norm=4.67605447769165, loss=1.662009358406067
I0128 16:44:33.482036 139655617963776 logging_writer.py:48] [96100] global_step=96100, grad_norm=4.874606132507324, loss=1.53806734085083
I0128 16:45:07.525564 139656666543872 logging_writer.py:48] [96200] global_step=96200, grad_norm=4.409528732299805, loss=1.6922857761383057
I0128 16:45:41.571235 139655617963776 logging_writer.py:48] [96300] global_step=96300, grad_norm=4.514437675476074, loss=1.6211637258529663
I0128 16:46:15.619587 139656666543872 logging_writer.py:48] [96400] global_step=96400, grad_norm=4.171038627624512, loss=1.5471930503845215
I0128 16:46:49.649061 139655617963776 logging_writer.py:48] [96500] global_step=96500, grad_norm=4.763638019561768, loss=1.628679633140564
I0128 16:47:23.693449 139656666543872 logging_writer.py:48] [96600] global_step=96600, grad_norm=4.104254245758057, loss=1.597434401512146
I0128 16:47:57.731050 139655617963776 logging_writer.py:48] [96700] global_step=96700, grad_norm=4.394445896148682, loss=1.6022132635116577
I0128 16:48:31.758475 139656666543872 logging_writer.py:48] [96800] global_step=96800, grad_norm=4.007443428039551, loss=1.5715906620025635
I0128 16:49:05.800162 139655617963776 logging_writer.py:48] [96900] global_step=96900, grad_norm=4.598202705383301, loss=1.6682649850845337
I0128 16:49:39.841191 139656666543872 logging_writer.py:48] [97000] global_step=97000, grad_norm=4.7533674240112305, loss=1.6061489582061768
I0128 16:50:13.957252 139655617963776 logging_writer.py:48] [97100] global_step=97100, grad_norm=3.9375357627868652, loss=1.6459308862686157
I0128 16:50:47.983204 139656666543872 logging_writer.py:48] [97200] global_step=97200, grad_norm=4.625740051269531, loss=1.7521657943725586
I0128 16:51:09.565135 139822745589568 spec.py:321] Evaluating on the training split.
I0128 16:51:15.673506 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 16:51:24.236322 139822745589568 spec.py:349] Evaluating on the test split.
I0128 16:51:26.760315 139822745589568 submission_runner.py:408] Time since start: 34346.48s, 	Step: 97265, 	{'train/accuracy': 0.7237324714660645, 'train/loss': 1.0708374977111816, 'validation/accuracy': 0.6580399870872498, 'validation/loss': 1.3982630968093872, 'validation/num_examples': 50000, 'test/accuracy': 0.5344000458717346, 'test/loss': 2.089726209640503, 'test/num_examples': 10000, 'score': 33186.39605593681, 'total_duration': 34346.4784386158, 'accumulated_submission_time': 33186.39605593681, 'accumulated_eval_time': 1153.3248386383057, 'accumulated_logging_time': 3.446504831314087}
I0128 16:51:26.795322 139656297445120 logging_writer.py:48] [97265] accumulated_eval_time=1153.324839, accumulated_logging_time=3.446505, accumulated_submission_time=33186.396056, global_step=97265, preemption_count=0, score=33186.396056, test/accuracy=0.534400, test/loss=2.089726, test/num_examples=10000, total_duration=34346.478439, train/accuracy=0.723732, train/loss=1.070837, validation/accuracy=0.658040, validation/loss=1.398263, validation/num_examples=50000
I0128 16:51:39.023101 139656649758464 logging_writer.py:48] [97300] global_step=97300, grad_norm=4.19032096862793, loss=1.5906528234481812
I0128 16:52:12.993222 139656297445120 logging_writer.py:48] [97400] global_step=97400, grad_norm=4.394495964050293, loss=1.5751878023147583
I0128 16:52:46.989290 139656649758464 logging_writer.py:48] [97500] global_step=97500, grad_norm=4.112525939941406, loss=1.567495346069336
I0128 16:53:21.023867 139656297445120 logging_writer.py:48] [97600] global_step=97600, grad_norm=4.04604959487915, loss=1.567393183708191
I0128 16:53:55.052384 139656649758464 logging_writer.py:48] [97700] global_step=97700, grad_norm=4.360733985900879, loss=1.6037489175796509
I0128 16:54:29.110284 139656297445120 logging_writer.py:48] [97800] global_step=97800, grad_norm=4.955499649047852, loss=1.6954050064086914
I0128 16:55:03.156667 139656649758464 logging_writer.py:48] [97900] global_step=97900, grad_norm=4.350665092468262, loss=1.606261968612671
I0128 16:55:37.171354 139656297445120 logging_writer.py:48] [98000] global_step=98000, grad_norm=4.354501247406006, loss=1.6613909006118774
I0128 16:56:11.221789 139656649758464 logging_writer.py:48] [98100] global_step=98100, grad_norm=4.228613376617432, loss=1.624460220336914
I0128 16:56:45.327193 139656297445120 logging_writer.py:48] [98200] global_step=98200, grad_norm=5.177719593048096, loss=1.535478949546814
I0128 16:57:19.362161 139656649758464 logging_writer.py:48] [98300] global_step=98300, grad_norm=4.318589210510254, loss=1.5660126209259033
I0128 16:57:53.398859 139656297445120 logging_writer.py:48] [98400] global_step=98400, grad_norm=4.735039710998535, loss=1.712446689605713
I0128 16:58:27.430207 139656649758464 logging_writer.py:48] [98500] global_step=98500, grad_norm=4.109688758850098, loss=1.5513288974761963
I0128 16:59:01.450132 139656297445120 logging_writer.py:48] [98600] global_step=98600, grad_norm=4.351833820343018, loss=1.6128169298171997
I0128 16:59:35.466572 139656649758464 logging_writer.py:48] [98700] global_step=98700, grad_norm=4.60843563079834, loss=1.679408311843872
I0128 16:59:57.081603 139822745589568 spec.py:321] Evaluating on the training split.
I0128 17:00:03.341059 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 17:00:12.133559 139822745589568 spec.py:349] Evaluating on the test split.
I0128 17:00:14.516422 139822745589568 submission_runner.py:408] Time since start: 34874.23s, 	Step: 98765, 	{'train/accuracy': 0.7150430083274841, 'train/loss': 1.1016902923583984, 'validation/accuracy': 0.6593799591064453, 'validation/loss': 1.3938559293746948, 'validation/num_examples': 50000, 'test/accuracy': 0.536300003528595, 'test/loss': 2.0971498489379883, 'test/num_examples': 10000, 'score': 33696.623683452606, 'total_duration': 34874.23455262184, 'accumulated_submission_time': 33696.623683452606, 'accumulated_eval_time': 1170.7596073150635, 'accumulated_logging_time': 3.4904332160949707}
I0128 17:00:14.552435 139656666543872 logging_writer.py:48] [98765] accumulated_eval_time=1170.759607, accumulated_logging_time=3.490433, accumulated_submission_time=33696.623683, global_step=98765, preemption_count=0, score=33696.623683, test/accuracy=0.536300, test/loss=2.097150, test/num_examples=10000, total_duration=34874.234553, train/accuracy=0.715043, train/loss=1.101690, validation/accuracy=0.659380, validation/loss=1.393856, validation/num_examples=50000
I0128 17:00:26.781100 139656834316032 logging_writer.py:48] [98800] global_step=98800, grad_norm=5.121597766876221, loss=1.598809003829956
I0128 17:01:00.772655 139656666543872 logging_writer.py:48] [98900] global_step=98900, grad_norm=4.270750522613525, loss=1.579646348953247
I0128 17:01:34.750652 139656834316032 logging_writer.py:48] [99000] global_step=99000, grad_norm=3.8376846313476562, loss=1.6382313966751099
I0128 17:02:08.795917 139656666543872 logging_writer.py:48] [99100] global_step=99100, grad_norm=4.2862043380737305, loss=1.4484199285507202
I0128 17:02:42.906334 139656834316032 logging_writer.py:48] [99200] global_step=99200, grad_norm=4.480778217315674, loss=1.6860017776489258
I0128 17:03:16.917137 139656666543872 logging_writer.py:48] [99300] global_step=99300, grad_norm=4.963086128234863, loss=1.6121301651000977
I0128 17:03:50.982085 139656834316032 logging_writer.py:48] [99400] global_step=99400, grad_norm=5.038342475891113, loss=1.5390490293502808
I0128 17:04:24.990688 139656666543872 logging_writer.py:48] [99500] global_step=99500, grad_norm=4.866220951080322, loss=1.6351990699768066
I0128 17:04:59.048255 139656834316032 logging_writer.py:48] [99600] global_step=99600, grad_norm=4.9276533126831055, loss=1.6237943172454834
I0128 17:05:33.102893 139656666543872 logging_writer.py:48] [99700] global_step=99700, grad_norm=4.437717437744141, loss=1.544344186782837
I0128 17:06:07.158797 139656834316032 logging_writer.py:48] [99800] global_step=99800, grad_norm=5.119948387145996, loss=1.4647835493087769
I0128 17:06:41.233870 139656666543872 logging_writer.py:48] [99900] global_step=99900, grad_norm=4.104864597320557, loss=1.696365237236023
I0128 17:07:15.259148 139656834316032 logging_writer.py:48] [100000] global_step=100000, grad_norm=4.044492244720459, loss=1.526608943939209
I0128 17:07:49.300441 139656666543872 logging_writer.py:48] [100100] global_step=100100, grad_norm=4.578060150146484, loss=1.6128480434417725
I0128 17:08:23.358532 139656834316032 logging_writer.py:48] [100200] global_step=100200, grad_norm=4.367800712585449, loss=1.42630934715271
I0128 17:08:44.629777 139822745589568 spec.py:321] Evaluating on the training split.
I0128 17:08:50.759086 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 17:08:59.592805 139822745589568 spec.py:349] Evaluating on the test split.
I0128 17:09:02.120132 139822745589568 submission_runner.py:408] Time since start: 35401.84s, 	Step: 100264, 	{'train/accuracy': 0.713309109210968, 'train/loss': 1.1122252941131592, 'validation/accuracy': 0.6600399613380432, 'validation/loss': 1.3887194395065308, 'validation/num_examples': 50000, 'test/accuracy': 0.5412000417709351, 'test/loss': 2.056500196456909, 'test/num_examples': 10000, 'score': 34206.6414039135, 'total_duration': 35401.838272333145, 'accumulated_submission_time': 34206.6414039135, 'accumulated_eval_time': 1188.2499401569366, 'accumulated_logging_time': 3.535299777984619}
I0128 17:09:02.159799 139656297445120 logging_writer.py:48] [100264] accumulated_eval_time=1188.249940, accumulated_logging_time=3.535300, accumulated_submission_time=34206.641404, global_step=100264, preemption_count=0, score=34206.641404, test/accuracy=0.541200, test/loss=2.056500, test/num_examples=10000, total_duration=35401.838272, train/accuracy=0.713309, train/loss=1.112225, validation/accuracy=0.660040, validation/loss=1.388719, validation/num_examples=50000
I0128 17:09:14.780139 139656649758464 logging_writer.py:48] [100300] global_step=100300, grad_norm=4.337638854980469, loss=1.6962783336639404
I0128 17:09:48.779438 139656297445120 logging_writer.py:48] [100400] global_step=100400, grad_norm=4.459133625030518, loss=1.564136266708374
I0128 17:10:22.781579 139656649758464 logging_writer.py:48] [100500] global_step=100500, grad_norm=4.1372904777526855, loss=1.6226061582565308
I0128 17:10:56.819683 139656297445120 logging_writer.py:48] [100600] global_step=100600, grad_norm=4.284760475158691, loss=1.5071934461593628
I0128 17:11:30.851024 139656649758464 logging_writer.py:48] [100700] global_step=100700, grad_norm=4.709329605102539, loss=1.6803091764450073
I0128 17:12:04.908875 139656297445120 logging_writer.py:48] [100800] global_step=100800, grad_norm=4.010499477386475, loss=1.4263379573822021
I0128 17:12:38.931941 139656649758464 logging_writer.py:48] [100900] global_step=100900, grad_norm=4.514830112457275, loss=1.6860764026641846
I0128 17:13:12.943417 139656297445120 logging_writer.py:48] [101000] global_step=101000, grad_norm=4.674889087677002, loss=1.7786319255828857
I0128 17:13:46.950083 139656649758464 logging_writer.py:48] [101100] global_step=101100, grad_norm=4.151904582977295, loss=1.484527349472046
I0128 17:14:20.982347 139656297445120 logging_writer.py:48] [101200] global_step=101200, grad_norm=4.768189907073975, loss=1.555243968963623
I0128 17:14:55.015922 139656649758464 logging_writer.py:48] [101300] global_step=101300, grad_norm=4.269552707672119, loss=1.5314139127731323
I0128 17:15:29.093247 139656297445120 logging_writer.py:48] [101400] global_step=101400, grad_norm=4.295862197875977, loss=1.5986729860305786
I0128 17:16:03.130239 139656649758464 logging_writer.py:48] [101500] global_step=101500, grad_norm=4.479156017303467, loss=1.612776517868042
I0128 17:16:37.139953 139656297445120 logging_writer.py:48] [101600] global_step=101600, grad_norm=4.438459873199463, loss=1.5549490451812744
I0128 17:17:11.177498 139656649758464 logging_writer.py:48] [101700] global_step=101700, grad_norm=4.336947917938232, loss=1.4641053676605225
I0128 17:17:32.428420 139822745589568 spec.py:321] Evaluating on the training split.
I0128 17:17:38.538840 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 17:17:46.978736 139822745589568 spec.py:349] Evaluating on the test split.
I0128 17:17:49.471459 139822745589568 submission_runner.py:408] Time since start: 35929.19s, 	Step: 101764, 	{'train/accuracy': 0.7233139276504517, 'train/loss': 1.072434902191162, 'validation/accuracy': 0.6643799543380737, 'validation/loss': 1.362235426902771, 'validation/num_examples': 50000, 'test/accuracy': 0.5402000546455383, 'test/loss': 2.057695150375366, 'test/num_examples': 10000, 'score': 34716.84902739525, 'total_duration': 35929.189604759216, 'accumulated_submission_time': 34716.84902739525, 'accumulated_eval_time': 1205.2929441928864, 'accumulated_logging_time': 3.586189031600952}
I0128 17:17:49.513534 139656666543872 logging_writer.py:48] [101764] accumulated_eval_time=1205.292944, accumulated_logging_time=3.586189, accumulated_submission_time=34716.849027, global_step=101764, preemption_count=0, score=34716.849027, test/accuracy=0.540200, test/loss=2.057695, test/num_examples=10000, total_duration=35929.189605, train/accuracy=0.723314, train/loss=1.072435, validation/accuracy=0.664380, validation/loss=1.362235, validation/num_examples=50000
I0128 17:18:02.100211 139656834316032 logging_writer.py:48] [101800] global_step=101800, grad_norm=4.2437744140625, loss=1.5222394466400146
I0128 17:18:36.102038 139656666543872 logging_writer.py:48] [101900] global_step=101900, grad_norm=5.070517539978027, loss=1.721010446548462
I0128 17:19:10.113642 139656834316032 logging_writer.py:48] [102000] global_step=102000, grad_norm=4.795856475830078, loss=1.6182469129562378
I0128 17:19:44.166024 139656666543872 logging_writer.py:48] [102100] global_step=102100, grad_norm=4.493093967437744, loss=1.587762475013733
I0128 17:20:18.203672 139656834316032 logging_writer.py:48] [102200] global_step=102200, grad_norm=4.033024311065674, loss=1.5145750045776367
I0128 17:20:52.230063 139656666543872 logging_writer.py:48] [102300] global_step=102300, grad_norm=4.313003063201904, loss=1.4491876363754272
I0128 17:21:26.255461 139656834316032 logging_writer.py:48] [102400] global_step=102400, grad_norm=4.127918243408203, loss=1.5240130424499512
I0128 17:22:00.344963 139656666543872 logging_writer.py:48] [102500] global_step=102500, grad_norm=4.3288092613220215, loss=1.6317150592803955
I0128 17:22:34.375890 139656834316032 logging_writer.py:48] [102600] global_step=102600, grad_norm=4.452430725097656, loss=1.627712607383728
I0128 17:23:08.436552 139656666543872 logging_writer.py:48] [102700] global_step=102700, grad_norm=4.6223602294921875, loss=1.4496164321899414
I0128 17:23:42.492330 139656834316032 logging_writer.py:48] [102800] global_step=102800, grad_norm=4.915090560913086, loss=1.5387780666351318
I0128 17:24:16.513657 139656666543872 logging_writer.py:48] [102900] global_step=102900, grad_norm=4.544457912445068, loss=1.6181528568267822
I0128 17:24:50.546831 139656834316032 logging_writer.py:48] [103000] global_step=103000, grad_norm=4.623828411102295, loss=1.6545836925506592
I0128 17:25:24.583103 139656666543872 logging_writer.py:48] [103100] global_step=103100, grad_norm=4.791272163391113, loss=1.5660324096679688
I0128 17:25:58.607614 139656834316032 logging_writer.py:48] [103200] global_step=103200, grad_norm=4.877057075500488, loss=1.6569241285324097
I0128 17:26:19.492577 139822745589568 spec.py:321] Evaluating on the training split.
I0128 17:26:25.613802 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 17:26:34.198032 139822745589568 spec.py:349] Evaluating on the test split.
I0128 17:26:36.804168 139822745589568 submission_runner.py:408] Time since start: 36456.52s, 	Step: 103263, 	{'train/accuracy': 0.75394606590271, 'train/loss': 0.9281212091445923, 'validation/accuracy': 0.6668999791145325, 'validation/loss': 1.3567508459091187, 'validation/num_examples': 50000, 'test/accuracy': 0.5405000448226929, 'test/loss': 2.041477918624878, 'test/num_examples': 10000, 'score': 35226.76603245735, 'total_duration': 36456.52231359482, 'accumulated_submission_time': 35226.76603245735, 'accumulated_eval_time': 1222.604502916336, 'accumulated_logging_time': 3.63908052444458}
I0128 17:26:36.844961 139655626356480 logging_writer.py:48] [103263] accumulated_eval_time=1222.604503, accumulated_logging_time=3.639081, accumulated_submission_time=35226.766032, global_step=103263, preemption_count=0, score=35226.766032, test/accuracy=0.540500, test/loss=2.041478, test/num_examples=10000, total_duration=36456.522314, train/accuracy=0.753946, train/loss=0.928121, validation/accuracy=0.666900, validation/loss=1.356751, validation/num_examples=50000
I0128 17:26:49.773904 139656297445120 logging_writer.py:48] [103300] global_step=103300, grad_norm=5.028841972351074, loss=1.6248714923858643
I0128 17:27:23.707904 139655626356480 logging_writer.py:48] [103400] global_step=103400, grad_norm=4.459053039550781, loss=1.5164083242416382
I0128 17:27:57.702559 139656297445120 logging_writer.py:48] [103500] global_step=103500, grad_norm=4.732516288757324, loss=1.5665206909179688
I0128 17:28:31.837228 139655626356480 logging_writer.py:48] [103600] global_step=103600, grad_norm=5.042705059051514, loss=1.6941964626312256
I0128 17:29:05.874063 139656297445120 logging_writer.py:48] [103700] global_step=103700, grad_norm=4.463792324066162, loss=1.5785610675811768
I0128 17:29:39.929363 139655626356480 logging_writer.py:48] [103800] global_step=103800, grad_norm=4.786386013031006, loss=1.5320167541503906
I0128 17:30:13.980055 139656297445120 logging_writer.py:48] [103900] global_step=103900, grad_norm=4.362640380859375, loss=1.4683319330215454
I0128 17:30:48.027897 139655626356480 logging_writer.py:48] [104000] global_step=104000, grad_norm=4.807556629180908, loss=1.621250867843628
I0128 17:31:22.096355 139656297445120 logging_writer.py:48] [104100] global_step=104100, grad_norm=4.695059299468994, loss=1.6699597835540771
I0128 17:31:56.123721 139655626356480 logging_writer.py:48] [104200] global_step=104200, grad_norm=4.088282585144043, loss=1.4942008256912231
I0128 17:32:30.157219 139656297445120 logging_writer.py:48] [104300] global_step=104300, grad_norm=4.995940208435059, loss=1.5168079137802124
I0128 17:33:04.184077 139655626356480 logging_writer.py:48] [104400] global_step=104400, grad_norm=4.67410135269165, loss=1.5184568166732788
I0128 17:33:38.217273 139656297445120 logging_writer.py:48] [104500] global_step=104500, grad_norm=4.477258205413818, loss=1.6020214557647705
I0128 17:34:12.263308 139655626356480 logging_writer.py:48] [104600] global_step=104600, grad_norm=4.577295780181885, loss=1.6060845851898193
I0128 17:34:46.352379 139656297445120 logging_writer.py:48] [104700] global_step=104700, grad_norm=4.433370590209961, loss=1.6194554567337036
I0128 17:35:06.897036 139822745589568 spec.py:321] Evaluating on the training split.
I0128 17:35:13.009062 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 17:35:21.627963 139822745589568 spec.py:349] Evaluating on the test split.
I0128 17:35:24.322063 139822745589568 submission_runner.py:408] Time since start: 36984.04s, 	Step: 104762, 	{'train/accuracy': 0.7365872263908386, 'train/loss': 1.0082699060440063, 'validation/accuracy': 0.6635199785232544, 'validation/loss': 1.3672338724136353, 'validation/num_examples': 50000, 'test/accuracy': 0.5410000085830688, 'test/loss': 2.045719623565674, 'test/num_examples': 10000, 'score': 35736.759813547134, 'total_duration': 36984.040206193924, 'accumulated_submission_time': 35736.759813547134, 'accumulated_eval_time': 1240.0294904708862, 'accumulated_logging_time': 3.6891348361968994}
I0128 17:35:24.359217 139656834316032 logging_writer.py:48] [104762] accumulated_eval_time=1240.029490, accumulated_logging_time=3.689135, accumulated_submission_time=35736.759814, global_step=104762, preemption_count=0, score=35736.759814, test/accuracy=0.541000, test/loss=2.045720, test/num_examples=10000, total_duration=36984.040206, train/accuracy=0.736587, train/loss=1.008270, validation/accuracy=0.663520, validation/loss=1.367234, validation/num_examples=50000
I0128 17:35:37.633633 139658730145536 logging_writer.py:48] [104800] global_step=104800, grad_norm=4.647905349731445, loss=1.6409480571746826
I0128 17:36:11.634255 139656834316032 logging_writer.py:48] [104900] global_step=104900, grad_norm=4.979074001312256, loss=1.4990884065628052
I0128 17:36:45.661540 139658730145536 logging_writer.py:48] [105000] global_step=105000, grad_norm=4.3275651931762695, loss=1.5558829307556152
I0128 17:37:19.674627 139656834316032 logging_writer.py:48] [105100] global_step=105100, grad_norm=4.808986186981201, loss=1.5581492185592651
I0128 17:37:53.691850 139658730145536 logging_writer.py:48] [105200] global_step=105200, grad_norm=4.385133743286133, loss=1.4705753326416016
I0128 17:38:27.727017 139656834316032 logging_writer.py:48] [105300] global_step=105300, grad_norm=4.488723278045654, loss=1.5826836824417114
I0128 17:39:01.750050 139658730145536 logging_writer.py:48] [105400] global_step=105400, grad_norm=5.548435688018799, loss=1.6374285221099854
I0128 17:39:35.784953 139656834316032 logging_writer.py:48] [105500] global_step=105500, grad_norm=4.723338603973389, loss=1.574272871017456
I0128 17:40:09.802139 139658730145536 logging_writer.py:48] [105600] global_step=105600, grad_norm=4.805322647094727, loss=1.529179334640503
I0128 17:40:43.887630 139656834316032 logging_writer.py:48] [105700] global_step=105700, grad_norm=5.008698463439941, loss=1.7039111852645874
I0128 17:41:18.000218 139658730145536 logging_writer.py:48] [105800] global_step=105800, grad_norm=5.149923801422119, loss=1.5609911680221558
I0128 17:41:52.025533 139656834316032 logging_writer.py:48] [105900] global_step=105900, grad_norm=4.391039848327637, loss=1.5501346588134766
I0128 17:42:26.061783 139658730145536 logging_writer.py:48] [106000] global_step=106000, grad_norm=5.388969421386719, loss=1.52768075466156
I0128 17:43:00.110352 139656834316032 logging_writer.py:48] [106100] global_step=106100, grad_norm=4.381659984588623, loss=1.5496597290039062
I0128 17:43:34.175755 139658730145536 logging_writer.py:48] [106200] global_step=106200, grad_norm=4.87239933013916, loss=1.6489686965942383
I0128 17:43:54.402424 139822745589568 spec.py:321] Evaluating on the training split.
I0128 17:44:00.631594 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 17:44:09.430249 139822745589568 spec.py:349] Evaluating on the test split.
I0128 17:44:11.895644 139822745589568 submission_runner.py:408] Time since start: 37511.61s, 	Step: 106261, 	{'train/accuracy': 0.7372249364852905, 'train/loss': 0.9990629553794861, 'validation/accuracy': 0.6670599579811096, 'validation/loss': 1.3503457307815552, 'validation/num_examples': 50000, 'test/accuracy': 0.5432000160217285, 'test/loss': 2.0669026374816895, 'test/num_examples': 10000, 'score': 36246.74147129059, 'total_duration': 37511.61378097534, 'accumulated_submission_time': 36246.74147129059, 'accumulated_eval_time': 1257.5226662158966, 'accumulated_logging_time': 3.7353804111480713}
I0128 17:44:11.932329 139655617963776 logging_writer.py:48] [106261] accumulated_eval_time=1257.522666, accumulated_logging_time=3.735380, accumulated_submission_time=36246.741471, global_step=106261, preemption_count=0, score=36246.741471, test/accuracy=0.543200, test/loss=2.066903, test/num_examples=10000, total_duration=37511.613781, train/accuracy=0.737225, train/loss=0.999063, validation/accuracy=0.667060, validation/loss=1.350346, validation/num_examples=50000
I0128 17:44:25.523386 139655626356480 logging_writer.py:48] [106300] global_step=106300, grad_norm=4.700468063354492, loss=1.6406807899475098
I0128 17:44:59.543067 139655617963776 logging_writer.py:48] [106400] global_step=106400, grad_norm=5.297008991241455, loss=1.6280012130737305
I0128 17:45:33.570420 139655626356480 logging_writer.py:48] [106500] global_step=106500, grad_norm=5.698566436767578, loss=1.5525305271148682
I0128 17:46:07.607191 139655617963776 logging_writer.py:48] [106600] global_step=106600, grad_norm=5.443656921386719, loss=1.6540404558181763
I0128 17:46:41.650766 139655626356480 logging_writer.py:48] [106700] global_step=106700, grad_norm=4.055311679840088, loss=1.4446078538894653
I0128 17:47:15.690385 139655617963776 logging_writer.py:48] [106800] global_step=106800, grad_norm=5.243873119354248, loss=1.5454896688461304
I0128 17:47:49.801937 139655626356480 logging_writer.py:48] [106900] global_step=106900, grad_norm=5.143488883972168, loss=1.4847849607467651
I0128 17:48:23.826225 139655617963776 logging_writer.py:48] [107000] global_step=107000, grad_norm=4.38639497756958, loss=1.6582392454147339
I0128 17:48:57.848207 139655626356480 logging_writer.py:48] [107100] global_step=107100, grad_norm=4.978689193725586, loss=1.6120550632476807
I0128 17:49:31.884766 139655617963776 logging_writer.py:48] [107200] global_step=107200, grad_norm=4.633290767669678, loss=1.5070265531539917
I0128 17:50:05.922951 139655626356480 logging_writer.py:48] [107300] global_step=107300, grad_norm=5.179641246795654, loss=1.620893955230713
I0128 17:50:39.959063 139655617963776 logging_writer.py:48] [107400] global_step=107400, grad_norm=4.743373870849609, loss=1.5701732635498047
I0128 17:51:13.990816 139655626356480 logging_writer.py:48] [107500] global_step=107500, grad_norm=4.557387828826904, loss=1.496695876121521
I0128 17:51:48.008150 139655617963776 logging_writer.py:48] [107600] global_step=107600, grad_norm=5.083144187927246, loss=1.6843860149383545
I0128 17:52:22.037875 139655626356480 logging_writer.py:48] [107700] global_step=107700, grad_norm=4.952807903289795, loss=1.4681916236877441
I0128 17:52:41.928323 139822745589568 spec.py:321] Evaluating on the training split.
I0128 17:52:48.038031 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 17:52:56.747446 139822745589568 spec.py:349] Evaluating on the test split.
I0128 17:52:59.320187 139822745589568 submission_runner.py:408] Time since start: 38039.04s, 	Step: 107760, 	{'train/accuracy': 0.7306082248687744, 'train/loss': 1.0350559949874878, 'validation/accuracy': 0.6622999906539917, 'validation/loss': 1.3632971048355103, 'validation/num_examples': 50000, 'test/accuracy': 0.5291000008583069, 'test/loss': 2.1044600009918213, 'test/num_examples': 10000, 'score': 36756.6770863533, 'total_duration': 38039.03833150864, 'accumulated_submission_time': 36756.6770863533, 'accumulated_eval_time': 1274.9144945144653, 'accumulated_logging_time': 3.7825989723205566}
I0128 17:52:59.359017 139656658151168 logging_writer.py:48] [107760] accumulated_eval_time=1274.914495, accumulated_logging_time=3.782599, accumulated_submission_time=36756.677086, global_step=107760, preemption_count=0, score=36756.677086, test/accuracy=0.529100, test/loss=2.104460, test/num_examples=10000, total_duration=38039.038332, train/accuracy=0.730608, train/loss=1.035056, validation/accuracy=0.662300, validation/loss=1.363297, validation/num_examples=50000
I0128 17:53:13.288124 139656666543872 logging_writer.py:48] [107800] global_step=107800, grad_norm=4.652145862579346, loss=1.5283949375152588
I0128 17:53:47.259239 139656658151168 logging_writer.py:48] [107900] global_step=107900, grad_norm=4.559628009796143, loss=1.5056394338607788
I0128 17:54:21.347990 139656666543872 logging_writer.py:48] [108000] global_step=108000, grad_norm=4.984641075134277, loss=1.6827502250671387
I0128 17:54:55.392794 139656658151168 logging_writer.py:48] [108100] global_step=108100, grad_norm=4.6310715675354, loss=1.4645277261734009
I0128 17:55:29.428671 139656666543872 logging_writer.py:48] [108200] global_step=108200, grad_norm=5.432178497314453, loss=1.528449535369873
I0128 17:56:03.450957 139656658151168 logging_writer.py:48] [108300] global_step=108300, grad_norm=5.044912815093994, loss=1.5033960342407227
I0128 17:56:37.472457 139656666543872 logging_writer.py:48] [108400] global_step=108400, grad_norm=5.269445419311523, loss=1.5990526676177979
I0128 17:57:11.510675 139656658151168 logging_writer.py:48] [108500] global_step=108500, grad_norm=4.74345588684082, loss=1.630588173866272
I0128 17:57:45.551856 139656666543872 logging_writer.py:48] [108600] global_step=108600, grad_norm=4.42182731628418, loss=1.489074945449829
I0128 17:58:19.601938 139656658151168 logging_writer.py:48] [108700] global_step=108700, grad_norm=4.782585144042969, loss=1.54323148727417
I0128 17:58:53.650889 139656666543872 logging_writer.py:48] [108800] global_step=108800, grad_norm=5.315977096557617, loss=1.5189207792282104
I0128 17:59:27.710800 139656658151168 logging_writer.py:48] [108900] global_step=108900, grad_norm=5.869244575500488, loss=1.607197880744934
I0128 18:00:01.754284 139656666543872 logging_writer.py:48] [109000] global_step=109000, grad_norm=5.220618724822998, loss=1.5434982776641846
I0128 18:00:35.991879 139656658151168 logging_writer.py:48] [109100] global_step=109100, grad_norm=4.9158124923706055, loss=1.5330551862716675
I0128 18:01:10.012567 139656666543872 logging_writer.py:48] [109200] global_step=109200, grad_norm=4.336617946624756, loss=1.4546334743499756
I0128 18:01:29.547767 139822745589568 spec.py:321] Evaluating on the training split.
I0128 18:01:35.740531 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 18:01:44.557248 139822745589568 spec.py:349] Evaluating on the test split.
I0128 18:01:47.084252 139822745589568 submission_runner.py:408] Time since start: 38566.80s, 	Step: 109259, 	{'train/accuracy': 0.7350525856018066, 'train/loss': 1.0115010738372803, 'validation/accuracy': 0.6735000014305115, 'validation/loss': 1.3214294910430908, 'validation/num_examples': 50000, 'test/accuracy': 0.549500048160553, 'test/loss': 2.004624605178833, 'test/num_examples': 10000, 'score': 37266.8054394722, 'total_duration': 38566.80239248276, 'accumulated_submission_time': 37266.8054394722, 'accumulated_eval_time': 1292.4509418010712, 'accumulated_logging_time': 3.830709218978882}
I0128 18:01:47.123482 139655609571072 logging_writer.py:48] [109259] accumulated_eval_time=1292.450942, accumulated_logging_time=3.830709, accumulated_submission_time=37266.805439, global_step=109259, preemption_count=0, score=37266.805439, test/accuracy=0.549500, test/loss=2.004625, test/num_examples=10000, total_duration=38566.802392, train/accuracy=0.735053, train/loss=1.011501, validation/accuracy=0.673500, validation/loss=1.321429, validation/num_examples=50000
I0128 18:02:01.395961 139655617963776 logging_writer.py:48] [109300] global_step=109300, grad_norm=4.6942315101623535, loss=1.4273957014083862
I0128 18:02:35.376229 139655609571072 logging_writer.py:48] [109400] global_step=109400, grad_norm=5.025783061981201, loss=1.485715389251709
I0128 18:03:09.375293 139655617963776 logging_writer.py:48] [109500] global_step=109500, grad_norm=4.358339786529541, loss=1.5659946203231812
I0128 18:03:43.383231 139655609571072 logging_writer.py:48] [109600] global_step=109600, grad_norm=5.4504241943359375, loss=1.5691293478012085
I0128 18:04:17.432822 139655617963776 logging_writer.py:48] [109700] global_step=109700, grad_norm=4.492526054382324, loss=1.4516657590866089
I0128 18:04:51.461900 139655609571072 logging_writer.py:48] [109800] global_step=109800, grad_norm=5.200987339019775, loss=1.5308042764663696
I0128 18:05:25.490691 139655617963776 logging_writer.py:48] [109900] global_step=109900, grad_norm=4.777785778045654, loss=1.5510424375534058
I0128 18:05:59.496710 139655609571072 logging_writer.py:48] [110000] global_step=110000, grad_norm=4.776839256286621, loss=1.5592052936553955
I0128 18:06:33.534278 139655617963776 logging_writer.py:48] [110100] global_step=110100, grad_norm=4.520969867706299, loss=1.5441442728042603
I0128 18:07:07.667159 139655609571072 logging_writer.py:48] [110200] global_step=110200, grad_norm=4.595527172088623, loss=1.5327038764953613
I0128 18:07:41.702433 139655617963776 logging_writer.py:48] [110300] global_step=110300, grad_norm=5.225363731384277, loss=1.587436318397522
I0128 18:08:15.750027 139655609571072 logging_writer.py:48] [110400] global_step=110400, grad_norm=6.454835891723633, loss=1.5970982313156128
I0128 18:08:49.808453 139655617963776 logging_writer.py:48] [110500] global_step=110500, grad_norm=5.383016109466553, loss=1.5635193586349487
I0128 18:09:23.850339 139655609571072 logging_writer.py:48] [110600] global_step=110600, grad_norm=4.517843723297119, loss=1.4577866792678833
I0128 18:09:57.900309 139655617963776 logging_writer.py:48] [110700] global_step=110700, grad_norm=5.019343376159668, loss=1.5734672546386719
I0128 18:10:17.109872 139822745589568 spec.py:321] Evaluating on the training split.
I0128 18:10:23.224950 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 18:10:32.122376 139822745589568 spec.py:349] Evaluating on the test split.
I0128 18:10:34.653654 139822745589568 submission_runner.py:408] Time since start: 39094.37s, 	Step: 110758, 	{'train/accuracy': 0.7382214665412903, 'train/loss': 0.9993380308151245, 'validation/accuracy': 0.6735000014305115, 'validation/loss': 1.3205397129058838, 'validation/num_examples': 50000, 'test/accuracy': 0.5402000546455383, 'test/loss': 2.009732484817505, 'test/num_examples': 10000, 'score': 37776.72888278961, 'total_duration': 39094.371799230576, 'accumulated_submission_time': 37776.72888278961, 'accumulated_eval_time': 1309.9946851730347, 'accumulated_logging_time': 3.8815505504608154}
I0128 18:10:34.692263 139656658151168 logging_writer.py:48] [110758] accumulated_eval_time=1309.994685, accumulated_logging_time=3.881551, accumulated_submission_time=37776.728883, global_step=110758, preemption_count=0, score=37776.728883, test/accuracy=0.540200, test/loss=2.009732, test/num_examples=10000, total_duration=39094.371799, train/accuracy=0.738221, train/loss=0.999338, validation/accuracy=0.673500, validation/loss=1.320540, validation/num_examples=50000
I0128 18:10:49.312548 139656666543872 logging_writer.py:48] [110800] global_step=110800, grad_norm=5.096390247344971, loss=1.5067448616027832
I0128 18:11:23.301541 139656658151168 logging_writer.py:48] [110900] global_step=110900, grad_norm=5.288843154907227, loss=1.4960421323776245
I0128 18:11:57.309763 139656666543872 logging_writer.py:48] [111000] global_step=111000, grad_norm=5.5721964836120605, loss=1.6374659538269043
I0128 18:12:31.339175 139656658151168 logging_writer.py:48] [111100] global_step=111100, grad_norm=5.220701217651367, loss=1.493069052696228
I0128 18:13:05.390450 139656666543872 logging_writer.py:48] [111200] global_step=111200, grad_norm=4.8119425773620605, loss=1.4803435802459717
I0128 18:13:39.510888 139656658151168 logging_writer.py:48] [111300] global_step=111300, grad_norm=4.753777503967285, loss=1.666782259941101
I0128 18:14:13.531769 139656666543872 logging_writer.py:48] [111400] global_step=111400, grad_norm=4.859457492828369, loss=1.5195469856262207
I0128 18:14:47.526300 139656658151168 logging_writer.py:48] [111500] global_step=111500, grad_norm=4.444562911987305, loss=1.4398434162139893
I0128 18:15:21.577158 139656666543872 logging_writer.py:48] [111600] global_step=111600, grad_norm=4.858429431915283, loss=1.4972442388534546
I0128 18:15:55.605757 139656658151168 logging_writer.py:48] [111700] global_step=111700, grad_norm=4.917857646942139, loss=1.5301513671875
I0128 18:16:29.649553 139656666543872 logging_writer.py:48] [111800] global_step=111800, grad_norm=5.21942138671875, loss=1.4759485721588135
I0128 18:17:03.686781 139656658151168 logging_writer.py:48] [111900] global_step=111900, grad_norm=5.352888584136963, loss=1.4548425674438477
I0128 18:17:37.714126 139656666543872 logging_writer.py:48] [112000] global_step=112000, grad_norm=4.595026016235352, loss=1.5106532573699951
I0128 18:18:11.756463 139656658151168 logging_writer.py:48] [112100] global_step=112100, grad_norm=4.996020793914795, loss=1.5081934928894043
I0128 18:18:45.802665 139656666543872 logging_writer.py:48] [112200] global_step=112200, grad_norm=5.21901798248291, loss=1.4266704320907593
I0128 18:19:04.679495 139822745589568 spec.py:321] Evaluating on the training split.
I0128 18:19:10.845385 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 18:19:19.561765 139822745589568 spec.py:349] Evaluating on the test split.
I0128 18:19:22.126857 139822745589568 submission_runner.py:408] Time since start: 39621.85s, 	Step: 112257, 	{'train/accuracy': 0.7806122303009033, 'train/loss': 0.819338858127594, 'validation/accuracy': 0.6816200017929077, 'validation/loss': 1.2890325784683228, 'validation/num_examples': 50000, 'test/accuracy': 0.5503000020980835, 'test/loss': 1.9960659742355347, 'test/num_examples': 10000, 'score': 38286.65507602692, 'total_duration': 39621.84500050545, 'accumulated_submission_time': 38286.65507602692, 'accumulated_eval_time': 1327.4420084953308, 'accumulated_logging_time': 3.929419755935669}
I0128 18:19:22.164241 139656649758464 logging_writer.py:48] [112257] accumulated_eval_time=1327.442008, accumulated_logging_time=3.929420, accumulated_submission_time=38286.655076, global_step=112257, preemption_count=0, score=38286.655076, test/accuracy=0.550300, test/loss=1.996066, test/num_examples=10000, total_duration=39621.845001, train/accuracy=0.780612, train/loss=0.819339, validation/accuracy=0.681620, validation/loss=1.289033, validation/num_examples=50000
I0128 18:19:37.108395 139656834316032 logging_writer.py:48] [112300] global_step=112300, grad_norm=5.568892955780029, loss=1.5063674449920654
I0128 18:20:11.190855 139656649758464 logging_writer.py:48] [112400] global_step=112400, grad_norm=4.790523052215576, loss=1.522662878036499
I0128 18:20:45.184921 139656834316032 logging_writer.py:48] [112500] global_step=112500, grad_norm=5.561687469482422, loss=1.5195708274841309
I0128 18:21:19.203964 139656649758464 logging_writer.py:48] [112600] global_step=112600, grad_norm=4.807353496551514, loss=1.552108883857727
I0128 18:21:53.229847 139656834316032 logging_writer.py:48] [112700] global_step=112700, grad_norm=4.791679859161377, loss=1.4987003803253174
I0128 18:22:27.235800 139656649758464 logging_writer.py:48] [112800] global_step=112800, grad_norm=4.787189483642578, loss=1.5011577606201172
I0128 18:23:01.276202 139656834316032 logging_writer.py:48] [112900] global_step=112900, grad_norm=5.187003135681152, loss=1.458844542503357
I0128 18:23:35.300505 139656649758464 logging_writer.py:48] [113000] global_step=113000, grad_norm=5.059937477111816, loss=1.440582275390625
I0128 18:24:09.336325 139656834316032 logging_writer.py:48] [113100] global_step=113100, grad_norm=6.17843770980835, loss=1.5086092948913574
I0128 18:24:43.385675 139656649758464 logging_writer.py:48] [113200] global_step=113200, grad_norm=4.797811985015869, loss=1.4176772832870483
I0128 18:25:17.428564 139656834316032 logging_writer.py:48] [113300] global_step=113300, grad_norm=4.447545051574707, loss=1.3866300582885742
I0128 18:25:51.491208 139656649758464 logging_writer.py:48] [113400] global_step=113400, grad_norm=5.223977088928223, loss=1.5189989805221558
I0128 18:26:25.708922 139656834316032 logging_writer.py:48] [113500] global_step=113500, grad_norm=4.993298530578613, loss=1.5176188945770264
I0128 18:26:59.754317 139656649758464 logging_writer.py:48] [113600] global_step=113600, grad_norm=5.159761905670166, loss=1.4335886240005493
I0128 18:27:33.785623 139656834316032 logging_writer.py:48] [113700] global_step=113700, grad_norm=5.080687999725342, loss=1.4066163301467896
I0128 18:27:52.304570 139822745589568 spec.py:321] Evaluating on the training split.
I0128 18:27:58.498567 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 18:28:07.450322 139822745589568 spec.py:349] Evaluating on the test split.
I0128 18:28:09.976089 139822745589568 submission_runner.py:408] Time since start: 40149.69s, 	Step: 113756, 	{'train/accuracy': 0.7663623690605164, 'train/loss': 0.8877369165420532, 'validation/accuracy': 0.6804400086402893, 'validation/loss': 1.2903692722320557, 'validation/num_examples': 50000, 'test/accuracy': 0.5544000267982483, 'test/loss': 1.976928472518921, 'test/num_examples': 10000, 'score': 38796.73586535454, 'total_duration': 40149.694214344025, 'accumulated_submission_time': 38796.73586535454, 'accumulated_eval_time': 1345.113474369049, 'accumulated_logging_time': 3.9760618209838867}
I0128 18:28:10.018226 139655609571072 logging_writer.py:48] [113756] accumulated_eval_time=1345.113474, accumulated_logging_time=3.976062, accumulated_submission_time=38796.735865, global_step=113756, preemption_count=0, score=38796.735865, test/accuracy=0.554400, test/loss=1.976928, test/num_examples=10000, total_duration=40149.694214, train/accuracy=0.766362, train/loss=0.887737, validation/accuracy=0.680440, validation/loss=1.290369, validation/num_examples=50000
I0128 18:28:25.300012 139655617963776 logging_writer.py:48] [113800] global_step=113800, grad_norm=5.116557598114014, loss=1.4877738952636719
I0128 18:28:59.281003 139655609571072 logging_writer.py:48] [113900] global_step=113900, grad_norm=5.146480083465576, loss=1.4668008089065552
I0128 18:29:33.263483 139655617963776 logging_writer.py:48] [114000] global_step=114000, grad_norm=4.936832427978516, loss=1.5420275926589966
I0128 18:30:07.288613 139655609571072 logging_writer.py:48] [114100] global_step=114100, grad_norm=5.309085369110107, loss=1.4910004138946533
I0128 18:30:41.310271 139655617963776 logging_writer.py:48] [114200] global_step=114200, grad_norm=5.003183364868164, loss=1.444756031036377
I0128 18:31:15.353272 139655609571072 logging_writer.py:48] [114300] global_step=114300, grad_norm=4.834023475646973, loss=1.4871208667755127
I0128 18:31:49.393199 139655617963776 logging_writer.py:48] [114400] global_step=114400, grad_norm=5.405201435089111, loss=1.4649025201797485
I0128 18:32:23.460179 139655609571072 logging_writer.py:48] [114500] global_step=114500, grad_norm=4.601928234100342, loss=1.504014015197754
I0128 18:32:57.618051 139655617963776 logging_writer.py:48] [114600] global_step=114600, grad_norm=4.754279613494873, loss=1.600067377090454
I0128 18:33:31.646295 139655609571072 logging_writer.py:48] [114700] global_step=114700, grad_norm=5.244227409362793, loss=1.5496411323547363
I0128 18:34:05.688354 139655617963776 logging_writer.py:48] [114800] global_step=114800, grad_norm=4.784327507019043, loss=1.4997856616973877
I0128 18:34:39.737124 139655609571072 logging_writer.py:48] [114900] global_step=114900, grad_norm=5.018565654754639, loss=1.513649821281433
I0128 18:35:13.787521 139655617963776 logging_writer.py:48] [115000] global_step=115000, grad_norm=5.075167179107666, loss=1.559462070465088
I0128 18:35:47.838266 139655609571072 logging_writer.py:48] [115100] global_step=115100, grad_norm=5.839085102081299, loss=1.393627405166626
I0128 18:36:21.862858 139655617963776 logging_writer.py:48] [115200] global_step=115200, grad_norm=5.875182628631592, loss=1.4508650302886963
I0128 18:36:40.033771 139822745589568 spec.py:321] Evaluating on the training split.
I0128 18:36:46.264325 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 18:36:55.054656 139822745589568 spec.py:349] Evaluating on the test split.
I0128 18:36:57.572492 139822745589568 submission_runner.py:408] Time since start: 40677.29s, 	Step: 115255, 	{'train/accuracy': 0.7538464665412903, 'train/loss': 0.9340843558311462, 'validation/accuracy': 0.6775199770927429, 'validation/loss': 1.3047815561294556, 'validation/num_examples': 50000, 'test/accuracy': 0.5565000176429749, 'test/loss': 1.98634934425354, 'test/num_examples': 10000, 'score': 39306.689522743225, 'total_duration': 40677.29035973549, 'accumulated_submission_time': 39306.689522743225, 'accumulated_eval_time': 1362.6518981456757, 'accumulated_logging_time': 4.028971195220947}
I0128 18:36:57.608987 139656834316032 logging_writer.py:48] [115255] accumulated_eval_time=1362.651898, accumulated_logging_time=4.028971, accumulated_submission_time=39306.689523, global_step=115255, preemption_count=0, score=39306.689523, test/accuracy=0.556500, test/loss=1.986349, test/num_examples=10000, total_duration=40677.290360, train/accuracy=0.753846, train/loss=0.934084, validation/accuracy=0.677520, validation/loss=1.304782, validation/num_examples=50000
I0128 18:37:13.265275 139658730145536 logging_writer.py:48] [115300] global_step=115300, grad_norm=4.785164833068848, loss=1.4652312994003296
I0128 18:37:47.281547 139656834316032 logging_writer.py:48] [115400] global_step=115400, grad_norm=5.017472743988037, loss=1.4171000719070435
I0128 18:38:21.308298 139658730145536 logging_writer.py:48] [115500] global_step=115500, grad_norm=4.517855167388916, loss=1.3861939907073975
I0128 18:38:55.338578 139656834316032 logging_writer.py:48] [115600] global_step=115600, grad_norm=4.890807628631592, loss=1.4943093061447144
I0128 18:39:29.459910 139658730145536 logging_writer.py:48] [115700] global_step=115700, grad_norm=5.291678428649902, loss=1.3647700548171997
I0128 18:40:03.492902 139656834316032 logging_writer.py:48] [115800] global_step=115800, grad_norm=4.430635929107666, loss=1.4701309204101562
I0128 18:40:37.523327 139658730145536 logging_writer.py:48] [115900] global_step=115900, grad_norm=4.45242166519165, loss=1.3656102418899536
I0128 18:41:11.592170 139656834316032 logging_writer.py:48] [116000] global_step=116000, grad_norm=5.994135856628418, loss=1.4886846542358398
I0128 18:41:45.656481 139658730145536 logging_writer.py:48] [116100] global_step=116100, grad_norm=5.4700117111206055, loss=1.497995138168335
I0128 18:42:19.693143 139656834316032 logging_writer.py:48] [116200] global_step=116200, grad_norm=5.609704494476318, loss=1.4395164251327515
I0128 18:42:53.735132 139658730145536 logging_writer.py:48] [116300] global_step=116300, grad_norm=5.0508527755737305, loss=1.4968457221984863
I0128 18:43:27.768600 139656834316032 logging_writer.py:48] [116400] global_step=116400, grad_norm=4.583052635192871, loss=1.4042733907699585
I0128 18:44:01.830152 139658730145536 logging_writer.py:48] [116500] global_step=116500, grad_norm=5.224019527435303, loss=1.4219467639923096
I0128 18:44:35.880320 139656834316032 logging_writer.py:48] [116600] global_step=116600, grad_norm=5.603640556335449, loss=1.5281685590744019
I0128 18:45:09.936193 139658730145536 logging_writer.py:48] [116700] global_step=116700, grad_norm=4.920283794403076, loss=1.4221715927124023
I0128 18:45:27.837464 139822745589568 spec.py:321] Evaluating on the training split.
I0128 18:45:33.988626 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 18:45:42.619817 139822745589568 spec.py:349] Evaluating on the test split.
I0128 18:45:45.145550 139822745589568 submission_runner.py:408] Time since start: 41204.86s, 	Step: 116754, 	{'train/accuracy': 0.7443000674247742, 'train/loss': 0.9577973484992981, 'validation/accuracy': 0.6729399561882019, 'validation/loss': 1.3240751028060913, 'validation/num_examples': 50000, 'test/accuracy': 0.5436000227928162, 'test/loss': 2.0469701290130615, 'test/num_examples': 10000, 'score': 39816.85724949837, 'total_duration': 41204.86368608475, 'accumulated_submission_time': 39816.85724949837, 'accumulated_eval_time': 1379.9599359035492, 'accumulated_logging_time': 4.0755980014801025}
I0128 18:45:45.183195 139655626356480 logging_writer.py:48] [116754] accumulated_eval_time=1379.959936, accumulated_logging_time=4.075598, accumulated_submission_time=39816.857249, global_step=116754, preemption_count=0, score=39816.857249, test/accuracy=0.543600, test/loss=2.046970, test/num_examples=10000, total_duration=41204.863686, train/accuracy=0.744300, train/loss=0.957797, validation/accuracy=0.672940, validation/loss=1.324075, validation/num_examples=50000
I0128 18:46:01.153913 139656297445120 logging_writer.py:48] [116800] global_step=116800, grad_norm=5.033792972564697, loss=1.4635884761810303
I0128 18:46:35.142105 139655626356480 logging_writer.py:48] [116900] global_step=116900, grad_norm=5.523260593414307, loss=1.503924012184143
I0128 18:47:09.155504 139656297445120 logging_writer.py:48] [117000] global_step=117000, grad_norm=5.348818302154541, loss=1.3567959070205688
I0128 18:47:43.164650 139655626356480 logging_writer.py:48] [117100] global_step=117100, grad_norm=5.843819618225098, loss=1.4091119766235352
I0128 18:48:17.205975 139656297445120 logging_writer.py:48] [117200] global_step=117200, grad_norm=4.973286151885986, loss=1.3795169591903687
I0128 18:48:51.229918 139655626356480 logging_writer.py:48] [117300] global_step=117300, grad_norm=5.076929092407227, loss=1.3857409954071045
I0128 18:49:25.243360 139656297445120 logging_writer.py:48] [117400] global_step=117400, grad_norm=4.755605220794678, loss=1.456924319267273
I0128 18:49:59.307379 139655626356480 logging_writer.py:48] [117500] global_step=117500, grad_norm=4.9051432609558105, loss=1.4583096504211426
I0128 18:50:33.347128 139656297445120 logging_writer.py:48] [117600] global_step=117600, grad_norm=5.2549920082092285, loss=1.5307921171188354
I0128 18:51:07.384463 139655626356480 logging_writer.py:48] [117700] global_step=117700, grad_norm=5.1630120277404785, loss=1.4807779788970947
I0128 18:51:41.417064 139656297445120 logging_writer.py:48] [117800] global_step=117800, grad_norm=5.084367752075195, loss=1.4630194902420044
I0128 18:52:15.517329 139655626356480 logging_writer.py:48] [117900] global_step=117900, grad_norm=4.955583572387695, loss=1.4435402154922485
I0128 18:52:49.571862 139656297445120 logging_writer.py:48] [118000] global_step=118000, grad_norm=5.479132652282715, loss=1.4028922319412231
I0128 18:53:23.606404 139655626356480 logging_writer.py:48] [118100] global_step=118100, grad_norm=4.722219467163086, loss=1.373748540878296
I0128 18:53:57.689673 139656297445120 logging_writer.py:48] [118200] global_step=118200, grad_norm=4.972330570220947, loss=1.4497172832489014
I0128 18:54:15.215929 139822745589568 spec.py:321] Evaluating on the training split.
I0128 18:54:21.747774 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 18:54:30.279032 139822745589568 spec.py:349] Evaluating on the test split.
I0128 18:54:32.812899 139822745589568 submission_runner.py:408] Time since start: 41732.53s, 	Step: 118253, 	{'train/accuracy': 0.7626155614852905, 'train/loss': 0.9064222574234009, 'validation/accuracy': 0.6890999674797058, 'validation/loss': 1.2482517957687378, 'validation/num_examples': 50000, 'test/accuracy': 0.5649000406265259, 'test/loss': 1.9236938953399658, 'test/num_examples': 10000, 'score': 40326.82902789116, 'total_duration': 41732.53103065491, 'accumulated_submission_time': 40326.82902789116, 'accumulated_eval_time': 1397.556854724884, 'accumulated_logging_time': 4.121983051300049}
I0128 18:54:32.853511 139655626356480 logging_writer.py:48] [118253] accumulated_eval_time=1397.556855, accumulated_logging_time=4.121983, accumulated_submission_time=40326.829028, global_step=118253, preemption_count=0, score=40326.829028, test/accuracy=0.564900, test/loss=1.923694, test/num_examples=10000, total_duration=41732.531031, train/accuracy=0.762616, train/loss=0.906422, validation/accuracy=0.689100, validation/loss=1.248252, validation/num_examples=50000
I0128 18:54:49.166010 139656297445120 logging_writer.py:48] [118300] global_step=118300, grad_norm=4.586151123046875, loss=1.5000523328781128
I0128 18:55:23.172082 139655626356480 logging_writer.py:48] [118400] global_step=118400, grad_norm=5.205132007598877, loss=1.4633163213729858
I0128 18:55:57.188441 139656297445120 logging_writer.py:48] [118500] global_step=118500, grad_norm=4.955036163330078, loss=1.3644893169403076
I0128 18:56:31.239934 139655626356480 logging_writer.py:48] [118600] global_step=118600, grad_norm=5.513144016265869, loss=1.470555305480957
I0128 18:57:05.251171 139656297445120 logging_writer.py:48] [118700] global_step=118700, grad_norm=5.328003883361816, loss=1.4126551151275635
I0128 18:57:39.287259 139655626356480 logging_writer.py:48] [118800] global_step=118800, grad_norm=5.204579830169678, loss=1.4892884492874146
I0128 18:58:13.329824 139656297445120 logging_writer.py:48] [118900] global_step=118900, grad_norm=5.590017795562744, loss=1.5241762399673462
I0128 18:58:47.458977 139655626356480 logging_writer.py:48] [119000] global_step=119000, grad_norm=5.236358642578125, loss=1.4424614906311035
I0128 18:59:21.496199 139656297445120 logging_writer.py:48] [119100] global_step=119100, grad_norm=5.374685764312744, loss=1.4728002548217773
I0128 18:59:55.507357 139655626356480 logging_writer.py:48] [119200] global_step=119200, grad_norm=5.239765167236328, loss=1.427070140838623
I0128 19:00:29.554804 139656297445120 logging_writer.py:48] [119300] global_step=119300, grad_norm=5.156480312347412, loss=1.5091383457183838
I0128 19:01:03.561871 139655626356480 logging_writer.py:48] [119400] global_step=119400, grad_norm=5.766445159912109, loss=1.5467501878738403
I0128 19:01:37.610965 139656297445120 logging_writer.py:48] [119500] global_step=119500, grad_norm=5.651208400726318, loss=1.4483826160430908
I0128 19:02:11.651273 139655626356480 logging_writer.py:48] [119600] global_step=119600, grad_norm=4.88442325592041, loss=1.3411287069320679
I0128 19:02:45.660921 139656297445120 logging_writer.py:48] [119700] global_step=119700, grad_norm=5.07614803314209, loss=1.3665213584899902
I0128 19:03:02.839079 139822745589568 spec.py:321] Evaluating on the training split.
I0128 19:03:09.011980 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 19:03:17.566207 139822745589568 spec.py:349] Evaluating on the test split.
I0128 19:03:20.083436 139822745589568 submission_runner.py:408] Time since start: 42259.80s, 	Step: 119752, 	{'train/accuracy': 0.7578921914100647, 'train/loss': 0.9132976531982422, 'validation/accuracy': 0.6890599727630615, 'validation/loss': 1.2581051588058472, 'validation/num_examples': 50000, 'test/accuracy': 0.5614000558853149, 'test/loss': 1.9510211944580078, 'test/num_examples': 10000, 'score': 40836.754336595535, 'total_duration': 42259.80155444145, 'accumulated_submission_time': 40836.754336595535, 'accumulated_eval_time': 1414.801174402237, 'accumulated_logging_time': 4.171825647354126}
I0128 19:03:20.124973 139656649758464 logging_writer.py:48] [119752] accumulated_eval_time=1414.801174, accumulated_logging_time=4.171826, accumulated_submission_time=40836.754337, global_step=119752, preemption_count=0, score=40836.754337, test/accuracy=0.561400, test/loss=1.951021, test/num_examples=10000, total_duration=42259.801554, train/accuracy=0.757892, train/loss=0.913298, validation/accuracy=0.689060, validation/loss=1.258105, validation/num_examples=50000
I0128 19:03:36.800514 139656834316032 logging_writer.py:48] [119800] global_step=119800, grad_norm=5.437103271484375, loss=1.4985737800598145
I0128 19:04:10.817005 139656649758464 logging_writer.py:48] [119900] global_step=119900, grad_norm=5.40001916885376, loss=1.478408932685852
I0128 19:04:44.923665 139656834316032 logging_writer.py:48] [120000] global_step=120000, grad_norm=5.510804653167725, loss=1.4168792963027954
I0128 19:05:18.955895 139656649758464 logging_writer.py:48] [120100] global_step=120100, grad_norm=5.256537437438965, loss=1.4124113321304321
I0128 19:05:52.994917 139656834316032 logging_writer.py:48] [120200] global_step=120200, grad_norm=5.315912246704102, loss=1.4122977256774902
I0128 19:06:27.074835 139656649758464 logging_writer.py:48] [120300] global_step=120300, grad_norm=5.661840915679932, loss=1.4267868995666504
I0128 19:07:01.117849 139656834316032 logging_writer.py:48] [120400] global_step=120400, grad_norm=5.895686626434326, loss=1.5460460186004639
I0128 19:07:35.176580 139656649758464 logging_writer.py:48] [120500] global_step=120500, grad_norm=5.006037712097168, loss=1.550092101097107
I0128 19:08:09.196234 139656834316032 logging_writer.py:48] [120600] global_step=120600, grad_norm=5.265702724456787, loss=1.4924840927124023
I0128 19:08:43.226316 139656649758464 logging_writer.py:48] [120700] global_step=120700, grad_norm=5.093387603759766, loss=1.3919754028320312
I0128 19:09:17.260145 139656834316032 logging_writer.py:48] [120800] global_step=120800, grad_norm=4.804447174072266, loss=1.3959324359893799
I0128 19:09:51.279469 139656649758464 logging_writer.py:48] [120900] global_step=120900, grad_norm=5.4144721031188965, loss=1.3770360946655273
I0128 19:10:25.313457 139656834316032 logging_writer.py:48] [121000] global_step=121000, grad_norm=5.183382034301758, loss=1.3872417211532593
I0128 19:10:59.439486 139656649758464 logging_writer.py:48] [121100] global_step=121100, grad_norm=5.150533676147461, loss=1.3567739725112915
I0128 19:11:33.468601 139656834316032 logging_writer.py:48] [121200] global_step=121200, grad_norm=4.849283218383789, loss=1.3701680898666382
I0128 19:11:50.305345 139822745589568 spec.py:321] Evaluating on the training split.
I0128 19:11:56.435904 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 19:12:05.159877 139822745589568 spec.py:349] Evaluating on the test split.
I0128 19:12:07.938178 139822745589568 submission_runner.py:408] Time since start: 42787.66s, 	Step: 121251, 	{'train/accuracy': 0.7889827489852905, 'train/loss': 0.7927571535110474, 'validation/accuracy': 0.6894400119781494, 'validation/loss': 1.2526060342788696, 'validation/num_examples': 50000, 'test/accuracy': 0.5688000321388245, 'test/loss': 1.9327174425125122, 'test/num_examples': 10000, 'score': 41346.87264943123, 'total_duration': 42787.65633249283, 'accumulated_submission_time': 41346.87264943123, 'accumulated_eval_time': 1432.4339890480042, 'accumulated_logging_time': 4.225170850753784}
I0128 19:12:07.973337 139655617963776 logging_writer.py:48] [121251] accumulated_eval_time=1432.433989, accumulated_logging_time=4.225171, accumulated_submission_time=41346.872649, global_step=121251, preemption_count=0, score=41346.872649, test/accuracy=0.568800, test/loss=1.932717, test/num_examples=10000, total_duration=42787.656332, train/accuracy=0.788983, train/loss=0.792757, validation/accuracy=0.689440, validation/loss=1.252606, validation/num_examples=50000
I0128 19:12:24.958273 139655626356480 logging_writer.py:48] [121300] global_step=121300, grad_norm=5.067939281463623, loss=1.32395339012146
I0128 19:12:58.928989 139655617963776 logging_writer.py:48] [121400] global_step=121400, grad_norm=5.922874450683594, loss=1.5501885414123535
I0128 19:13:32.933633 139655626356480 logging_writer.py:48] [121500] global_step=121500, grad_norm=5.637547492980957, loss=1.4191133975982666
I0128 19:14:06.952953 139655617963776 logging_writer.py:48] [121600] global_step=121600, grad_norm=5.15713357925415, loss=1.4044417142868042
I0128 19:14:40.970131 139655626356480 logging_writer.py:48] [121700] global_step=121700, grad_norm=5.120547294616699, loss=1.5091041326522827
I0128 19:15:14.991753 139655617963776 logging_writer.py:48] [121800] global_step=121800, grad_norm=5.466067314147949, loss=1.51527738571167
I0128 19:15:49.006920 139655626356480 logging_writer.py:48] [121900] global_step=121900, grad_norm=5.488563060760498, loss=1.4723252058029175
I0128 19:16:23.057182 139655617963776 logging_writer.py:48] [122000] global_step=122000, grad_norm=5.628477573394775, loss=1.4430270195007324
I0128 19:16:57.089146 139655626356480 logging_writer.py:48] [122100] global_step=122100, grad_norm=6.020265579223633, loss=1.347246527671814
I0128 19:17:31.175172 139655617963776 logging_writer.py:48] [122200] global_step=122200, grad_norm=6.173768520355225, loss=1.350121259689331
I0128 19:18:05.226182 139655626356480 logging_writer.py:48] [122300] global_step=122300, grad_norm=5.174737453460693, loss=1.4235198497772217
I0128 19:18:39.267472 139655617963776 logging_writer.py:48] [122400] global_step=122400, grad_norm=4.767386436462402, loss=1.3843235969543457
I0128 19:19:13.285836 139655626356480 logging_writer.py:48] [122500] global_step=122500, grad_norm=5.947170257568359, loss=1.4925618171691895
I0128 19:19:47.348406 139655617963776 logging_writer.py:48] [122600] global_step=122600, grad_norm=5.92656135559082, loss=1.4267303943634033
I0128 19:20:21.418042 139655626356480 logging_writer.py:48] [122700] global_step=122700, grad_norm=5.552057266235352, loss=1.448904275894165
I0128 19:20:38.246925 139822745589568 spec.py:321] Evaluating on the training split.
I0128 19:20:44.406324 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 19:20:53.227237 139822745589568 spec.py:349] Evaluating on the test split.
I0128 19:20:55.749177 139822745589568 submission_runner.py:408] Time since start: 43315.47s, 	Step: 122751, 	{'train/accuracy': 0.778738796710968, 'train/loss': 0.8235031962394714, 'validation/accuracy': 0.6881600022315979, 'validation/loss': 1.264472484588623, 'validation/num_examples': 50000, 'test/accuracy': 0.5613000392913818, 'test/loss': 1.9879837036132812, 'test/num_examples': 10000, 'score': 41857.08833384514, 'total_duration': 43315.46732521057, 'accumulated_submission_time': 41857.08833384514, 'accumulated_eval_time': 1449.9362061023712, 'accumulated_logging_time': 4.2682952880859375}
I0128 19:20:55.791148 139656649758464 logging_writer.py:48] [122751] accumulated_eval_time=1449.936206, accumulated_logging_time=4.268295, accumulated_submission_time=41857.088334, global_step=122751, preemption_count=0, score=41857.088334, test/accuracy=0.561300, test/loss=1.987984, test/num_examples=10000, total_duration=43315.467325, train/accuracy=0.778739, train/loss=0.823503, validation/accuracy=0.688160, validation/loss=1.264472, validation/num_examples=50000
I0128 19:21:12.801081 139656666543872 logging_writer.py:48] [122800] global_step=122800, grad_norm=5.3956193923950195, loss=1.3707083463668823
I0128 19:21:46.800963 139656649758464 logging_writer.py:48] [122900] global_step=122900, grad_norm=5.017759799957275, loss=1.4017058610916138
I0128 19:22:20.820740 139656666543872 logging_writer.py:48] [123000] global_step=123000, grad_norm=5.717126846313477, loss=1.4385769367218018
I0128 19:22:54.865113 139656649758464 logging_writer.py:48] [123100] global_step=123100, grad_norm=5.5987749099731445, loss=1.4677454233169556
I0128 19:23:28.924448 139656666543872 logging_writer.py:48] [123200] global_step=123200, grad_norm=5.616305828094482, loss=1.3454362154006958
I0128 19:24:03.036006 139656649758464 logging_writer.py:48] [123300] global_step=123300, grad_norm=5.249444484710693, loss=1.2892429828643799
I0128 19:24:37.072946 139656666543872 logging_writer.py:48] [123400] global_step=123400, grad_norm=5.2591705322265625, loss=1.4314870834350586
I0128 19:25:11.129710 139656649758464 logging_writer.py:48] [123500] global_step=123500, grad_norm=5.618427276611328, loss=1.3720703125
I0128 19:25:45.168419 139656666543872 logging_writer.py:48] [123600] global_step=123600, grad_norm=5.74359130859375, loss=1.399922490119934
I0128 19:26:19.191242 139656649758464 logging_writer.py:48] [123700] global_step=123700, grad_norm=5.4095540046691895, loss=1.4715358018875122
I0128 19:26:53.233988 139656666543872 logging_writer.py:48] [123800] global_step=123800, grad_norm=5.325403213500977, loss=1.4098610877990723
I0128 19:27:27.242295 139656649758464 logging_writer.py:48] [123900] global_step=123900, grad_norm=5.731842994689941, loss=1.4102147817611694
I0128 19:28:01.305554 139656666543872 logging_writer.py:48] [124000] global_step=124000, grad_norm=5.699957370758057, loss=1.4210939407348633
I0128 19:28:35.341932 139656649758464 logging_writer.py:48] [124100] global_step=124100, grad_norm=5.999598503112793, loss=1.4475445747375488
I0128 19:29:09.397502 139656666543872 logging_writer.py:48] [124200] global_step=124200, grad_norm=5.943859577178955, loss=1.3464086055755615
I0128 19:29:25.867754 139822745589568 spec.py:321] Evaluating on the training split.
I0128 19:29:32.709203 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 19:29:41.325384 139822745589568 spec.py:349] Evaluating on the test split.
I0128 19:29:43.845873 139822745589568 submission_runner.py:408] Time since start: 43843.56s, 	Step: 124250, 	{'train/accuracy': 0.782246470451355, 'train/loss': 0.8037508726119995, 'validation/accuracy': 0.6977399587631226, 'validation/loss': 1.2117669582366943, 'validation/num_examples': 50000, 'test/accuracy': 0.5701000094413757, 'test/loss': 1.924551010131836, 'test/num_examples': 10000, 'score': 42367.103356838226, 'total_duration': 43843.563719034195, 'accumulated_submission_time': 42367.103356838226, 'accumulated_eval_time': 1467.9139926433563, 'accumulated_logging_time': 4.320462465286255}
I0128 19:29:43.890999 139656297445120 logging_writer.py:48] [124250] accumulated_eval_time=1467.913993, accumulated_logging_time=4.320462, accumulated_submission_time=42367.103357, global_step=124250, preemption_count=0, score=42367.103357, test/accuracy=0.570100, test/loss=1.924551, test/num_examples=10000, total_duration=43843.563719, train/accuracy=0.782246, train/loss=0.803751, validation/accuracy=0.697740, validation/loss=1.211767, validation/num_examples=50000
I0128 19:30:01.224223 139656658151168 logging_writer.py:48] [124300] global_step=124300, grad_norm=4.902246475219727, loss=1.3070342540740967
I0128 19:30:35.404049 139656297445120 logging_writer.py:48] [124400] global_step=124400, grad_norm=5.122190475463867, loss=1.364315390586853
I0128 19:31:09.414002 139656658151168 logging_writer.py:48] [124500] global_step=124500, grad_norm=5.419183254241943, loss=1.352315068244934
I0128 19:31:43.455595 139656297445120 logging_writer.py:48] [124600] global_step=124600, grad_norm=5.809447288513184, loss=1.34280526638031
I0128 19:32:17.492997 139656658151168 logging_writer.py:48] [124700] global_step=124700, grad_norm=5.6658759117126465, loss=1.3204514980316162
I0128 19:32:51.538353 139656297445120 logging_writer.py:48] [124800] global_step=124800, grad_norm=5.858808517456055, loss=1.2624986171722412
I0128 19:33:25.598265 139656658151168 logging_writer.py:48] [124900] global_step=124900, grad_norm=5.658480644226074, loss=1.3447794914245605
I0128 19:33:59.609588 139656297445120 logging_writer.py:48] [125000] global_step=125000, grad_norm=5.373465538024902, loss=1.3069441318511963
I0128 19:34:33.658476 139656658151168 logging_writer.py:48] [125100] global_step=125100, grad_norm=5.832947731018066, loss=1.4227713346481323
I0128 19:35:07.693042 139656297445120 logging_writer.py:48] [125200] global_step=125200, grad_norm=5.781914710998535, loss=1.4084728956222534
I0128 19:35:41.755214 139656658151168 logging_writer.py:48] [125300] global_step=125300, grad_norm=5.666309833526611, loss=1.2914113998413086
I0128 19:36:15.786368 139656297445120 logging_writer.py:48] [125400] global_step=125400, grad_norm=6.754126071929932, loss=1.458406925201416
I0128 19:36:49.885622 139656658151168 logging_writer.py:48] [125500] global_step=125500, grad_norm=5.666439056396484, loss=1.499058485031128
I0128 19:37:23.955334 139656297445120 logging_writer.py:48] [125600] global_step=125600, grad_norm=5.6795806884765625, loss=1.4131022691726685
I0128 19:37:58.020939 139656658151168 logging_writer.py:48] [125700] global_step=125700, grad_norm=5.437555313110352, loss=1.2545762062072754
I0128 19:38:14.148759 139822745589568 spec.py:321] Evaluating on the training split.
I0128 19:38:20.275501 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 19:38:28.834388 139822745589568 spec.py:349] Evaluating on the test split.
I0128 19:38:31.356613 139822745589568 submission_runner.py:408] Time since start: 44371.07s, 	Step: 125749, 	{'train/accuracy': 0.7780413031578064, 'train/loss': 0.8233484625816345, 'validation/accuracy': 0.6994400024414062, 'validation/loss': 1.2192716598510742, 'validation/num_examples': 50000, 'test/accuracy': 0.5735000371932983, 'test/loss': 1.909732699394226, 'test/num_examples': 10000, 'score': 42877.30141162872, 'total_duration': 44371.0747590065, 'accumulated_submission_time': 42877.30141162872, 'accumulated_eval_time': 1485.1218111515045, 'accumulated_logging_time': 4.3753581047058105}
I0128 19:38:31.399077 139655617963776 logging_writer.py:48] [125749] accumulated_eval_time=1485.121811, accumulated_logging_time=4.375358, accumulated_submission_time=42877.301412, global_step=125749, preemption_count=0, score=42877.301412, test/accuracy=0.573500, test/loss=1.909733, test/num_examples=10000, total_duration=44371.074759, train/accuracy=0.778041, train/loss=0.823348, validation/accuracy=0.699440, validation/loss=1.219272, validation/num_examples=50000
I0128 19:38:49.091070 139655626356480 logging_writer.py:48] [125800] global_step=125800, grad_norm=5.6126484870910645, loss=1.4698379039764404
I0128 19:39:23.055562 139655617963776 logging_writer.py:48] [125900] global_step=125900, grad_norm=5.735001564025879, loss=1.3886232376098633
I0128 19:39:57.066345 139655626356480 logging_writer.py:48] [126000] global_step=126000, grad_norm=5.6575608253479, loss=1.474610447883606
I0128 19:40:31.102264 139655617963776 logging_writer.py:48] [126100] global_step=126100, grad_norm=6.094594478607178, loss=1.3179259300231934
I0128 19:41:05.148619 139655626356480 logging_writer.py:48] [126200] global_step=126200, grad_norm=5.301483154296875, loss=1.3698616027832031
I0128 19:41:39.232033 139655617963776 logging_writer.py:48] [126300] global_step=126300, grad_norm=6.5011749267578125, loss=1.461167812347412
I0128 19:42:13.270316 139655626356480 logging_writer.py:48] [126400] global_step=126400, grad_norm=5.4723334312438965, loss=1.3561955690383911
I0128 19:42:47.367879 139655617963776 logging_writer.py:48] [126500] global_step=126500, grad_norm=5.227745532989502, loss=1.2545223236083984
I0128 19:43:21.433924 139655626356480 logging_writer.py:48] [126600] global_step=126600, grad_norm=5.863739490509033, loss=1.414799451828003
I0128 19:43:55.472357 139655617963776 logging_writer.py:48] [126700] global_step=126700, grad_norm=5.6561598777771, loss=1.368167757987976
I0128 19:44:29.508177 139655626356480 logging_writer.py:48] [126800] global_step=126800, grad_norm=5.746796607971191, loss=1.3891632556915283
I0128 19:45:03.549654 139655617963776 logging_writer.py:48] [126900] global_step=126900, grad_norm=6.007411479949951, loss=1.4866204261779785
I0128 19:45:37.621629 139655626356480 logging_writer.py:48] [127000] global_step=127000, grad_norm=5.239055156707764, loss=1.2945507764816284
I0128 19:46:11.662027 139655617963776 logging_writer.py:48] [127100] global_step=127100, grad_norm=6.452236175537109, loss=1.4278960227966309
I0128 19:46:45.747025 139655626356480 logging_writer.py:48] [127200] global_step=127200, grad_norm=6.389657497406006, loss=1.3555786609649658
I0128 19:47:01.546098 139822745589568 spec.py:321] Evaluating on the training split.
I0128 19:47:07.788078 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 19:47:16.267078 139822745589568 spec.py:349] Evaluating on the test split.
I0128 19:47:18.795351 139822745589568 submission_runner.py:408] Time since start: 44898.51s, 	Step: 127248, 	{'train/accuracy': 0.7848173975944519, 'train/loss': 0.798477292060852, 'validation/accuracy': 0.6995799541473389, 'validation/loss': 1.2017760276794434, 'validation/num_examples': 50000, 'test/accuracy': 0.5724000334739685, 'test/loss': 1.9187508821487427, 'test/num_examples': 10000, 'score': 43387.38800287247, 'total_duration': 44898.51348400116, 'accumulated_submission_time': 43387.38800287247, 'accumulated_eval_time': 1502.3710179328918, 'accumulated_logging_time': 4.427471876144409}
I0128 19:47:18.838315 139656297445120 logging_writer.py:48] [127248] accumulated_eval_time=1502.371018, accumulated_logging_time=4.427472, accumulated_submission_time=43387.388003, global_step=127248, preemption_count=0, score=43387.388003, test/accuracy=0.572400, test/loss=1.918751, test/num_examples=10000, total_duration=44898.513484, train/accuracy=0.784817, train/loss=0.798477, validation/accuracy=0.699580, validation/loss=1.201776, validation/num_examples=50000
I0128 19:47:36.857528 139656658151168 logging_writer.py:48] [127300] global_step=127300, grad_norm=5.929317951202393, loss=1.3795523643493652
I0128 19:48:10.837942 139656297445120 logging_writer.py:48] [127400] global_step=127400, grad_norm=5.374867916107178, loss=1.3199219703674316
I0128 19:48:44.867674 139656658151168 logging_writer.py:48] [127500] global_step=127500, grad_norm=5.538387298583984, loss=1.1969783306121826
I0128 19:49:19.077159 139656297445120 logging_writer.py:48] [127600] global_step=127600, grad_norm=5.6738433837890625, loss=1.4086130857467651
I0128 19:49:53.122888 139656658151168 logging_writer.py:48] [127700] global_step=127700, grad_norm=5.744748115539551, loss=1.413108229637146
I0128 19:50:27.159603 139656297445120 logging_writer.py:48] [127800] global_step=127800, grad_norm=5.471384048461914, loss=1.4731059074401855
I0128 19:51:01.177379 139656658151168 logging_writer.py:48] [127900] global_step=127900, grad_norm=5.948435306549072, loss=1.2779152393341064
I0128 19:51:35.198299 139656297445120 logging_writer.py:48] [128000] global_step=128000, grad_norm=6.091439247131348, loss=1.4186798334121704
I0128 19:52:09.226070 139656658151168 logging_writer.py:48] [128100] global_step=128100, grad_norm=5.49107027053833, loss=1.4050929546356201
I0128 19:52:43.257550 139656297445120 logging_writer.py:48] [128200] global_step=128200, grad_norm=5.766926288604736, loss=1.2954057455062866
I0128 19:53:17.268339 139656658151168 logging_writer.py:48] [128300] global_step=128300, grad_norm=5.5791826248168945, loss=1.3979699611663818
I0128 19:53:51.277218 139656297445120 logging_writer.py:48] [128400] global_step=128400, grad_norm=5.909684181213379, loss=1.2744784355163574
I0128 19:54:25.329993 139656658151168 logging_writer.py:48] [128500] global_step=128500, grad_norm=5.360745429992676, loss=1.324141502380371
I0128 19:54:59.366255 139656297445120 logging_writer.py:48] [128600] global_step=128600, grad_norm=5.983165264129639, loss=1.3724687099456787
I0128 19:55:33.534618 139656658151168 logging_writer.py:48] [128700] global_step=128700, grad_norm=5.502110958099365, loss=1.2573010921478271
I0128 19:55:49.007355 139822745589568 spec.py:321] Evaluating on the training split.
I0128 19:55:55.128622 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 19:56:03.787986 139822745589568 spec.py:349] Evaluating on the test split.
I0128 19:56:06.343193 139822745589568 submission_runner.py:408] Time since start: 45426.06s, 	Step: 128747, 	{'train/accuracy': 0.7816087007522583, 'train/loss': 0.8127210736274719, 'validation/accuracy': 0.7039200067520142, 'validation/loss': 1.1963602304458618, 'validation/num_examples': 50000, 'test/accuracy': 0.5781000256538391, 'test/loss': 1.8842612504959106, 'test/num_examples': 10000, 'score': 43897.49772691727, 'total_duration': 45426.061324596405, 'accumulated_submission_time': 43897.49772691727, 'accumulated_eval_time': 1519.7068076133728, 'accumulated_logging_time': 4.479499578475952}
I0128 19:56:06.390991 139655626356480 logging_writer.py:48] [128747] accumulated_eval_time=1519.706808, accumulated_logging_time=4.479500, accumulated_submission_time=43897.497727, global_step=128747, preemption_count=0, score=43897.497727, test/accuracy=0.578100, test/loss=1.884261, test/num_examples=10000, total_duration=45426.061325, train/accuracy=0.781609, train/loss=0.812721, validation/accuracy=0.703920, validation/loss=1.196360, validation/num_examples=50000
I0128 19:56:24.778496 139656297445120 logging_writer.py:48] [128800] global_step=128800, grad_norm=5.567416191101074, loss=1.338877558708191
I0128 19:56:58.771980 139655626356480 logging_writer.py:48] [128900] global_step=128900, grad_norm=5.342006206512451, loss=1.2465887069702148
I0128 19:57:32.776510 139656297445120 logging_writer.py:48] [129000] global_step=129000, grad_norm=6.169078350067139, loss=1.2757731676101685
I0128 19:58:06.823713 139655626356480 logging_writer.py:48] [129100] global_step=129100, grad_norm=5.53005838394165, loss=1.4162126779556274
I0128 19:58:40.870780 139656297445120 logging_writer.py:48] [129200] global_step=129200, grad_norm=5.698071002960205, loss=1.3169878721237183
I0128 19:59:14.896078 139655626356480 logging_writer.py:48] [129300] global_step=129300, grad_norm=5.644674777984619, loss=1.3462504148483276
I0128 19:59:48.929089 139656297445120 logging_writer.py:48] [129400] global_step=129400, grad_norm=5.7968668937683105, loss=1.390877366065979
I0128 20:00:22.960203 139655626356480 logging_writer.py:48] [129500] global_step=129500, grad_norm=6.069299697875977, loss=1.3027969598770142
I0128 20:00:56.998000 139656297445120 logging_writer.py:48] [129600] global_step=129600, grad_norm=5.854202747344971, loss=1.4127097129821777
I0128 20:01:31.031204 139655626356480 logging_writer.py:48] [129700] global_step=129700, grad_norm=5.535501956939697, loss=1.244649887084961
I0128 20:02:05.133022 139656297445120 logging_writer.py:48] [129800] global_step=129800, grad_norm=6.405325889587402, loss=1.3965260982513428
I0128 20:02:39.161304 139655626356480 logging_writer.py:48] [129900] global_step=129900, grad_norm=5.2151384353637695, loss=1.2517889738082886
I0128 20:03:13.222044 139656297445120 logging_writer.py:48] [130000] global_step=130000, grad_norm=6.306448459625244, loss=1.3702492713928223
I0128 20:03:47.246682 139655626356480 logging_writer.py:48] [130100] global_step=130100, grad_norm=6.059328556060791, loss=1.401477336883545
I0128 20:04:21.296509 139656297445120 logging_writer.py:48] [130200] global_step=130200, grad_norm=6.201458930969238, loss=1.2598838806152344
I0128 20:04:36.438769 139822745589568 spec.py:321] Evaluating on the training split.
I0128 20:04:42.553960 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 20:04:51.266448 139822745589568 spec.py:349] Evaluating on the test split.
I0128 20:04:53.706493 139822745589568 submission_runner.py:408] Time since start: 45953.42s, 	Step: 130246, 	{'train/accuracy': 0.7840800285339355, 'train/loss': 0.8188005685806274, 'validation/accuracy': 0.7031799554824829, 'validation/loss': 1.1907312870025635, 'validation/num_examples': 50000, 'test/accuracy': 0.5733000040054321, 'test/loss': 1.8935832977294922, 'test/num_examples': 10000, 'score': 44407.48382616043, 'total_duration': 45953.42463064194, 'accumulated_submission_time': 44407.48382616043, 'accumulated_eval_time': 1536.9744882583618, 'accumulated_logging_time': 4.538180828094482}
I0128 20:04:53.749210 139656666543872 logging_writer.py:48] [130246] accumulated_eval_time=1536.974488, accumulated_logging_time=4.538181, accumulated_submission_time=44407.483826, global_step=130246, preemption_count=0, score=44407.483826, test/accuracy=0.573300, test/loss=1.893583, test/num_examples=10000, total_duration=45953.424631, train/accuracy=0.784080, train/loss=0.818801, validation/accuracy=0.703180, validation/loss=1.190731, validation/num_examples=50000
I0128 20:05:12.488524 139656834316032 logging_writer.py:48] [130300] global_step=130300, grad_norm=6.073007106781006, loss=1.2437607049942017
I0128 20:05:46.488290 139656666543872 logging_writer.py:48] [130400] global_step=130400, grad_norm=5.969916343688965, loss=1.345931887626648
I0128 20:06:20.509837 139656834316032 logging_writer.py:48] [130500] global_step=130500, grad_norm=5.3199462890625, loss=1.2890475988388062
I0128 20:06:54.556561 139656666543872 logging_writer.py:48] [130600] global_step=130600, grad_norm=4.896763324737549, loss=1.257534146308899
I0128 20:07:28.607541 139656834316032 logging_writer.py:48] [130700] global_step=130700, grad_norm=5.497650623321533, loss=1.3796793222427368
I0128 20:08:02.673730 139656666543872 logging_writer.py:48] [130800] global_step=130800, grad_norm=6.268631458282471, loss=1.3983392715454102
I0128 20:08:36.758839 139656834316032 logging_writer.py:48] [130900] global_step=130900, grad_norm=5.839191436767578, loss=1.378027081489563
I0128 20:09:10.777583 139656666543872 logging_writer.py:48] [131000] global_step=131000, grad_norm=5.772426128387451, loss=1.249878168106079
I0128 20:09:44.812777 139656834316032 logging_writer.py:48] [131100] global_step=131100, grad_norm=6.707455635070801, loss=1.2802400588989258
I0128 20:10:18.871540 139656666543872 logging_writer.py:48] [131200] global_step=131200, grad_norm=5.772222995758057, loss=1.2826285362243652
I0128 20:10:52.899268 139656834316032 logging_writer.py:48] [131300] global_step=131300, grad_norm=5.5643815994262695, loss=1.2830877304077148
I0128 20:11:26.922423 139656666543872 logging_writer.py:48] [131400] global_step=131400, grad_norm=5.919090270996094, loss=1.308938980102539
I0128 20:12:00.958695 139656834316032 logging_writer.py:48] [131500] global_step=131500, grad_norm=5.774734973907471, loss=1.2140032052993774
I0128 20:12:34.965839 139656666543872 logging_writer.py:48] [131600] global_step=131600, grad_norm=5.346297264099121, loss=1.2123194932937622
I0128 20:13:08.999877 139656834316032 logging_writer.py:48] [131700] global_step=131700, grad_norm=6.768967151641846, loss=1.424411416053772
I0128 20:13:23.762124 139822745589568 spec.py:321] Evaluating on the training split.
I0128 20:13:29.929978 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 20:13:38.732519 139822745589568 spec.py:349] Evaluating on the test split.
I0128 20:13:41.264300 139822745589568 submission_runner.py:408] Time since start: 46480.98s, 	Step: 131745, 	{'train/accuracy': 0.8127192258834839, 'train/loss': 0.6832287907600403, 'validation/accuracy': 0.7074599862098694, 'validation/loss': 1.1807633638381958, 'validation/num_examples': 50000, 'test/accuracy': 0.5805000066757202, 'test/loss': 1.8970988988876343, 'test/num_examples': 10000, 'score': 44917.434248924255, 'total_duration': 46480.9824347496, 'accumulated_submission_time': 44917.434248924255, 'accumulated_eval_time': 1554.4766201972961, 'accumulated_logging_time': 4.591028213500977}
I0128 20:13:41.303605 139656297445120 logging_writer.py:48] [131745] accumulated_eval_time=1554.476620, accumulated_logging_time=4.591028, accumulated_submission_time=44917.434249, global_step=131745, preemption_count=0, score=44917.434249, test/accuracy=0.580500, test/loss=1.897099, test/num_examples=10000, total_duration=46480.982435, train/accuracy=0.812719, train/loss=0.683229, validation/accuracy=0.707460, validation/loss=1.180763, validation/num_examples=50000
I0128 20:14:00.352445 139656649758464 logging_writer.py:48] [131800] global_step=131800, grad_norm=6.210976600646973, loss=1.2712420225143433
I0128 20:14:34.524946 139656297445120 logging_writer.py:48] [131900] global_step=131900, grad_norm=5.457627773284912, loss=1.2588077783584595
I0128 20:15:08.527769 139656649758464 logging_writer.py:48] [132000] global_step=132000, grad_norm=5.4844489097595215, loss=1.2971667051315308
I0128 20:15:42.591848 139656297445120 logging_writer.py:48] [132100] global_step=132100, grad_norm=5.73691987991333, loss=1.2854748964309692
I0128 20:16:16.634213 139656649758464 logging_writer.py:48] [132200] global_step=132200, grad_norm=5.800090789794922, loss=1.2026525735855103
I0128 20:16:50.673216 139656297445120 logging_writer.py:48] [132300] global_step=132300, grad_norm=6.039668560028076, loss=1.1840672492980957
I0128 20:17:24.694325 139656649758464 logging_writer.py:48] [132400] global_step=132400, grad_norm=5.8213324546813965, loss=1.2256319522857666
I0128 20:17:58.749145 139656297445120 logging_writer.py:48] [132500] global_step=132500, grad_norm=6.537838935852051, loss=1.3601086139678955
I0128 20:18:32.785614 139656649758464 logging_writer.py:48] [132600] global_step=132600, grad_norm=5.536269664764404, loss=1.3370463848114014
I0128 20:19:06.817924 139656297445120 logging_writer.py:48] [132700] global_step=132700, grad_norm=6.671743869781494, loss=1.361817479133606
I0128 20:19:40.879473 139656649758464 logging_writer.py:48] [132800] global_step=132800, grad_norm=5.579260349273682, loss=1.3469923734664917
I0128 20:20:14.948251 139656297445120 logging_writer.py:48] [132900] global_step=132900, grad_norm=7.0322184562683105, loss=1.2159032821655273
I0128 20:20:49.017363 139656649758464 logging_writer.py:48] [133000] global_step=133000, grad_norm=6.117637634277344, loss=1.4771766662597656
I0128 20:21:23.161883 139656297445120 logging_writer.py:48] [133100] global_step=133100, grad_norm=5.446235179901123, loss=1.222475528717041
I0128 20:21:57.215974 139656649758464 logging_writer.py:48] [133200] global_step=133200, grad_norm=6.087165832519531, loss=1.316019058227539
I0128 20:22:11.320041 139822745589568 spec.py:321] Evaluating on the training split.
I0128 20:22:17.422265 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 20:22:25.973852 139822745589568 spec.py:349] Evaluating on the test split.
I0128 20:22:28.533810 139822745589568 submission_runner.py:408] Time since start: 47008.25s, 	Step: 133243, 	{'train/accuracy': 0.79984450340271, 'train/loss': 0.7381070852279663, 'validation/accuracy': 0.7058199644088745, 'validation/loss': 1.1831684112548828, 'validation/num_examples': 50000, 'test/accuracy': 0.5821000337600708, 'test/loss': 1.8820722103118896, 'test/num_examples': 10000, 'score': 45427.39062857628, 'total_duration': 47008.251879930496, 'accumulated_submission_time': 45427.39062857628, 'accumulated_eval_time': 1571.6903052330017, 'accumulated_logging_time': 4.63919734954834}
I0128 20:22:28.576713 139655617963776 logging_writer.py:48] [133243] accumulated_eval_time=1571.690305, accumulated_logging_time=4.639197, accumulated_submission_time=45427.390629, global_step=133243, preemption_count=0, score=45427.390629, test/accuracy=0.582100, test/loss=1.882072, test/num_examples=10000, total_duration=47008.251880, train/accuracy=0.799845, train/loss=0.738107, validation/accuracy=0.705820, validation/loss=1.183168, validation/num_examples=50000
I0128 20:22:48.315709 139655626356480 logging_writer.py:48] [133300] global_step=133300, grad_norm=5.702800273895264, loss=1.3159829378128052
I0128 20:23:22.302516 139655617963776 logging_writer.py:48] [133400] global_step=133400, grad_norm=6.713501453399658, loss=1.2106069326400757
I0128 20:23:56.354298 139655626356480 logging_writer.py:48] [133500] global_step=133500, grad_norm=5.736091136932373, loss=1.2298223972320557
I0128 20:24:30.371990 139655617963776 logging_writer.py:48] [133600] global_step=133600, grad_norm=5.915574550628662, loss=1.274707317352295
I0128 20:25:04.412488 139655626356480 logging_writer.py:48] [133700] global_step=133700, grad_norm=5.506729602813721, loss=1.2717127799987793
I0128 20:25:38.453498 139655617963776 logging_writer.py:48] [133800] global_step=133800, grad_norm=5.872201442718506, loss=1.1813026666641235
I0128 20:26:12.501337 139655626356480 logging_writer.py:48] [133900] global_step=133900, grad_norm=6.089379787445068, loss=1.358303189277649
I0128 20:26:46.566647 139655617963776 logging_writer.py:48] [134000] global_step=134000, grad_norm=5.59021520614624, loss=1.2674684524536133
I0128 20:27:20.678027 139655626356480 logging_writer.py:48] [134100] global_step=134100, grad_norm=6.266578197479248, loss=1.3185008764266968
I0128 20:27:54.728518 139655617963776 logging_writer.py:48] [134200] global_step=134200, grad_norm=5.898715972900391, loss=1.233988642692566
I0128 20:28:28.770104 139655626356480 logging_writer.py:48] [134300] global_step=134300, grad_norm=6.342792510986328, loss=1.2523329257965088
I0128 20:29:02.827913 139655617963776 logging_writer.py:48] [134400] global_step=134400, grad_norm=5.826840877532959, loss=1.2990477085113525
I0128 20:29:36.872231 139655626356480 logging_writer.py:48] [134500] global_step=134500, grad_norm=6.4748616218566895, loss=1.3424152135849
I0128 20:30:10.909473 139655617963776 logging_writer.py:48] [134600] global_step=134600, grad_norm=5.674469470977783, loss=1.2318161725997925
I0128 20:30:44.953151 139655626356480 logging_writer.py:48] [134700] global_step=134700, grad_norm=6.381692886352539, loss=1.3512763977050781
I0128 20:30:58.702332 139822745589568 spec.py:321] Evaluating on the training split.
I0128 20:31:04.930295 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 20:31:13.672017 139822745589568 spec.py:349] Evaluating on the test split.
I0128 20:31:16.191341 139822745589568 submission_runner.py:408] Time since start: 47535.91s, 	Step: 134742, 	{'train/accuracy': 0.8057836294174194, 'train/loss': 0.7143407464027405, 'validation/accuracy': 0.7128799557685852, 'validation/loss': 1.146412968635559, 'validation/num_examples': 50000, 'test/accuracy': 0.5893000364303589, 'test/loss': 1.8250541687011719, 'test/num_examples': 10000, 'score': 45937.45642733574, 'total_duration': 47535.90947675705, 'accumulated_submission_time': 45937.45642733574, 'accumulated_eval_time': 1589.1792786121368, 'accumulated_logging_time': 4.692151308059692}
I0128 20:31:16.231379 139656649758464 logging_writer.py:48] [134742] accumulated_eval_time=1589.179279, accumulated_logging_time=4.692151, accumulated_submission_time=45937.456427, global_step=134742, preemption_count=0, score=45937.456427, test/accuracy=0.589300, test/loss=1.825054, test/num_examples=10000, total_duration=47535.909477, train/accuracy=0.805784, train/loss=0.714341, validation/accuracy=0.712880, validation/loss=1.146413, validation/num_examples=50000
I0128 20:31:36.300479 139656834316032 logging_writer.py:48] [134800] global_step=134800, grad_norm=5.796552658081055, loss=1.221394658088684
I0128 20:32:10.316233 139656649758464 logging_writer.py:48] [134900] global_step=134900, grad_norm=6.1139817237854, loss=1.2538484334945679
I0128 20:32:44.358123 139656834316032 logging_writer.py:48] [135000] global_step=135000, grad_norm=7.118714809417725, loss=1.2604939937591553
I0128 20:33:18.375428 139656649758464 logging_writer.py:48] [135100] global_step=135100, grad_norm=6.122687339782715, loss=1.222113847732544
I0128 20:33:52.470125 139656834316032 logging_writer.py:48] [135200] global_step=135200, grad_norm=6.153674125671387, loss=1.183259129524231
I0128 20:34:26.542570 139656649758464 logging_writer.py:48] [135300] global_step=135300, grad_norm=5.862117767333984, loss=1.2761813402175903
I0128 20:35:00.588706 139656834316032 logging_writer.py:48] [135400] global_step=135400, grad_norm=6.059973239898682, loss=1.239135980606079
I0128 20:35:34.631130 139656649758464 logging_writer.py:48] [135500] global_step=135500, grad_norm=5.866225242614746, loss=1.1749074459075928
I0128 20:36:08.683337 139656834316032 logging_writer.py:48] [135600] global_step=135600, grad_norm=7.136178493499756, loss=1.2146300077438354
I0128 20:36:42.712171 139656649758464 logging_writer.py:48] [135700] global_step=135700, grad_norm=7.117112159729004, loss=1.2596559524536133
I0128 20:37:16.768185 139656834316032 logging_writer.py:48] [135800] global_step=135800, grad_norm=7.050499439239502, loss=1.2601019144058228
I0128 20:37:50.802865 139656649758464 logging_writer.py:48] [135900] global_step=135900, grad_norm=6.162633419036865, loss=1.270609736442566
I0128 20:38:24.829403 139656834316032 logging_writer.py:48] [136000] global_step=136000, grad_norm=5.421011924743652, loss=1.123152732849121
I0128 20:38:58.872617 139656649758464 logging_writer.py:48] [136100] global_step=136100, grad_norm=6.7007012367248535, loss=1.2696765661239624
I0128 20:39:32.931696 139656834316032 logging_writer.py:48] [136200] global_step=136200, grad_norm=6.5482587814331055, loss=1.2560285329818726
I0128 20:39:46.344405 139822745589568 spec.py:321] Evaluating on the training split.
I0128 20:39:52.742042 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 20:40:01.509238 139822745589568 spec.py:349] Evaluating on the test split.
I0128 20:40:04.023159 139822745589568 submission_runner.py:408] Time since start: 48063.74s, 	Step: 136241, 	{'train/accuracy': 0.8033322691917419, 'train/loss': 0.7214902639389038, 'validation/accuracy': 0.7112399935722351, 'validation/loss': 1.1542009115219116, 'validation/num_examples': 50000, 'test/accuracy': 0.5920000076293945, 'test/loss': 1.826322317123413, 'test/num_examples': 10000, 'score': 46447.50892996788, 'total_duration': 48063.74123668671, 'accumulated_submission_time': 46447.50892996788, 'accumulated_eval_time': 1606.857929468155, 'accumulated_logging_time': 4.741678476333618}
I0128 20:40:04.063516 139655617963776 logging_writer.py:48] [136241] accumulated_eval_time=1606.857929, accumulated_logging_time=4.741678, accumulated_submission_time=46447.508930, global_step=136241, preemption_count=0, score=46447.508930, test/accuracy=0.592000, test/loss=1.826322, test/num_examples=10000, total_duration=48063.741237, train/accuracy=0.803332, train/loss=0.721490, validation/accuracy=0.711240, validation/loss=1.154201, validation/num_examples=50000
I0128 20:40:24.490279 139656297445120 logging_writer.py:48] [136300] global_step=136300, grad_norm=5.875438213348389, loss=1.1805940866470337
I0128 20:40:58.481117 139655617963776 logging_writer.py:48] [136400] global_step=136400, grad_norm=6.111236572265625, loss=1.2161787748336792
I0128 20:41:32.507056 139656297445120 logging_writer.py:48] [136500] global_step=136500, grad_norm=6.315309047698975, loss=1.207421064376831
I0128 20:42:06.530748 139655617963776 logging_writer.py:48] [136600] global_step=136600, grad_norm=6.0576910972595215, loss=1.3004062175750732
I0128 20:42:40.600815 139656297445120 logging_writer.py:48] [136700] global_step=136700, grad_norm=6.384385585784912, loss=1.1916779279708862
I0128 20:43:14.644881 139655617963776 logging_writer.py:48] [136800] global_step=136800, grad_norm=5.706701278686523, loss=1.211320400238037
I0128 20:43:48.683504 139656297445120 logging_writer.py:48] [136900] global_step=136900, grad_norm=5.866849899291992, loss=1.192602515220642
I0128 20:44:22.704324 139655617963776 logging_writer.py:48] [137000] global_step=137000, grad_norm=5.936450481414795, loss=1.2388759851455688
I0128 20:44:56.762351 139656297445120 logging_writer.py:48] [137100] global_step=137100, grad_norm=6.097863674163818, loss=1.26729154586792
I0128 20:45:30.773022 139655617963776 logging_writer.py:48] [137200] global_step=137200, grad_norm=5.963160514831543, loss=1.1886664628982544
I0128 20:46:04.829231 139656297445120 logging_writer.py:48] [137300] global_step=137300, grad_norm=6.691737651824951, loss=1.2452709674835205
I0128 20:46:39.031501 139655617963776 logging_writer.py:48] [137400] global_step=137400, grad_norm=6.151787281036377, loss=1.2455562353134155
I0128 20:47:13.094926 139656297445120 logging_writer.py:48] [137500] global_step=137500, grad_norm=6.1746649742126465, loss=1.1312764883041382
I0128 20:47:47.114666 139655617963776 logging_writer.py:48] [137600] global_step=137600, grad_norm=6.238767147064209, loss=1.1995155811309814
I0128 20:48:21.157970 139656297445120 logging_writer.py:48] [137700] global_step=137700, grad_norm=6.734447479248047, loss=1.3659870624542236
I0128 20:48:34.234018 139822745589568 spec.py:321] Evaluating on the training split.
I0128 20:48:40.387907 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 20:48:49.091331 139822745589568 spec.py:349] Evaluating on the test split.
I0128 20:48:51.643750 139822745589568 submission_runner.py:408] Time since start: 48591.36s, 	Step: 137740, 	{'train/accuracy': 0.805683970451355, 'train/loss': 0.7093267440795898, 'validation/accuracy': 0.7160399556159973, 'validation/loss': 1.1419600248336792, 'validation/num_examples': 50000, 'test/accuracy': 0.5933000445365906, 'test/loss': 1.8347601890563965, 'test/num_examples': 10000, 'score': 46957.61950492859, 'total_duration': 48591.361892938614, 'accumulated_submission_time': 46957.61950492859, 'accumulated_eval_time': 1624.2676224708557, 'accumulated_logging_time': 4.790990352630615}
I0128 20:48:51.685187 139655609571072 logging_writer.py:48] [137740] accumulated_eval_time=1624.267622, accumulated_logging_time=4.790990, accumulated_submission_time=46957.619505, global_step=137740, preemption_count=0, score=46957.619505, test/accuracy=0.593300, test/loss=1.834760, test/num_examples=10000, total_duration=48591.361893, train/accuracy=0.805684, train/loss=0.709327, validation/accuracy=0.716040, validation/loss=1.141960, validation/num_examples=50000
I0128 20:49:12.439223 139655617963776 logging_writer.py:48] [137800] global_step=137800, grad_norm=6.706700325012207, loss=1.39388906955719
I0128 20:49:46.411605 139655609571072 logging_writer.py:48] [137900] global_step=137900, grad_norm=6.7334980964660645, loss=1.3727099895477295
I0128 20:50:20.448992 139655617963776 logging_writer.py:48] [138000] global_step=138000, grad_norm=6.202234745025635, loss=1.2088561058044434
I0128 20:50:54.464435 139655609571072 logging_writer.py:48] [138100] global_step=138100, grad_norm=6.872551918029785, loss=1.2422096729278564
I0128 20:51:28.504634 139655617963776 logging_writer.py:48] [138200] global_step=138200, grad_norm=5.841340065002441, loss=1.2163121700286865
I0128 20:52:02.994036 139655609571072 logging_writer.py:48] [138300] global_step=138300, grad_norm=6.442925453186035, loss=1.2381982803344727
I0128 20:52:37.018974 139655617963776 logging_writer.py:48] [138400] global_step=138400, grad_norm=6.841901779174805, loss=1.3547855615615845
I0128 20:53:11.214091 139655609571072 logging_writer.py:48] [138500] global_step=138500, grad_norm=7.485294818878174, loss=1.3285351991653442
I0128 20:53:45.255895 139655617963776 logging_writer.py:48] [138600] global_step=138600, grad_norm=6.423520565032959, loss=1.246876835823059
I0128 20:54:19.292539 139655609571072 logging_writer.py:48] [138700] global_step=138700, grad_norm=6.705366134643555, loss=1.234883189201355
I0128 20:54:53.339509 139655617963776 logging_writer.py:48] [138800] global_step=138800, grad_norm=6.423606872558594, loss=1.2389371395111084
I0128 20:55:27.389119 139655609571072 logging_writer.py:48] [138900] global_step=138900, grad_norm=6.588061332702637, loss=1.2902202606201172
I0128 20:56:01.437948 139655617963776 logging_writer.py:48] [139000] global_step=139000, grad_norm=6.697692394256592, loss=1.1938868761062622
I0128 20:56:35.508101 139655609571072 logging_writer.py:48] [139100] global_step=139100, grad_norm=6.91071891784668, loss=1.298326849937439
I0128 20:57:09.575970 139655617963776 logging_writer.py:48] [139200] global_step=139200, grad_norm=7.1934332847595215, loss=1.1897482872009277
I0128 20:57:21.983672 139822745589568 spec.py:321] Evaluating on the training split.
I0128 20:57:28.083787 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 20:57:36.597606 139822745589568 spec.py:349] Evaluating on the test split.
I0128 20:57:39.140744 139822745589568 submission_runner.py:408] Time since start: 49118.86s, 	Step: 139238, 	{'train/accuracy': 0.8067402839660645, 'train/loss': 0.7009314298629761, 'validation/accuracy': 0.7160199880599976, 'validation/loss': 1.1319137811660767, 'validation/num_examples': 50000, 'test/accuracy': 0.5916000008583069, 'test/loss': 1.830636978149414, 'test/num_examples': 10000, 'score': 47467.85808753967, 'total_duration': 49118.858887672424, 'accumulated_submission_time': 47467.85808753967, 'accumulated_eval_time': 1641.4246740341187, 'accumulated_logging_time': 4.841644525527954}
I0128 20:57:39.184180 139656658151168 logging_writer.py:48] [139238] accumulated_eval_time=1641.424674, accumulated_logging_time=4.841645, accumulated_submission_time=47467.858088, global_step=139238, preemption_count=0, score=47467.858088, test/accuracy=0.591600, test/loss=1.830637, test/num_examples=10000, total_duration=49118.858888, train/accuracy=0.806740, train/loss=0.700931, validation/accuracy=0.716020, validation/loss=1.131914, validation/num_examples=50000
I0128 20:58:00.622360 139656834316032 logging_writer.py:48] [139300] global_step=139300, grad_norm=6.095730304718018, loss=1.3138675689697266
I0128 20:58:34.598293 139656658151168 logging_writer.py:48] [139400] global_step=139400, grad_norm=6.888119697570801, loss=1.3226226568222046
I0128 20:59:08.650210 139656834316032 logging_writer.py:48] [139500] global_step=139500, grad_norm=6.2430315017700195, loss=1.2092055082321167
I0128 20:59:42.813701 139656658151168 logging_writer.py:48] [139600] global_step=139600, grad_norm=5.870771884918213, loss=1.1744379997253418
I0128 21:00:16.855267 139656834316032 logging_writer.py:48] [139700] global_step=139700, grad_norm=6.053491115570068, loss=1.2270833253860474
I0128 21:00:50.915486 139656658151168 logging_writer.py:48] [139800] global_step=139800, grad_norm=6.49667501449585, loss=1.1358896493911743
I0128 21:01:24.942643 139656834316032 logging_writer.py:48] [139900] global_step=139900, grad_norm=6.460053443908691, loss=1.1928256750106812
I0128 21:01:59.014216 139656658151168 logging_writer.py:48] [140000] global_step=140000, grad_norm=6.406142711639404, loss=1.1458280086517334
I0128 21:02:33.054164 139656834316032 logging_writer.py:48] [140100] global_step=140100, grad_norm=6.457091331481934, loss=1.1536157131195068
I0128 21:03:07.126844 139656658151168 logging_writer.py:48] [140200] global_step=140200, grad_norm=6.554905414581299, loss=1.21366286277771
I0128 21:03:41.157655 139656834316032 logging_writer.py:48] [140300] global_step=140300, grad_norm=6.14448356628418, loss=1.1858471632003784
I0128 21:04:15.217965 139656658151168 logging_writer.py:48] [140400] global_step=140400, grad_norm=6.587380409240723, loss=1.1913790702819824
I0128 21:04:49.245447 139656834316032 logging_writer.py:48] [140500] global_step=140500, grad_norm=6.676740646362305, loss=1.2605443000793457
I0128 21:05:23.273681 139656658151168 logging_writer.py:48] [140600] global_step=140600, grad_norm=6.859194755554199, loss=1.232870101928711
I0128 21:05:57.385694 139656834316032 logging_writer.py:48] [140700] global_step=140700, grad_norm=6.592728614807129, loss=1.14146888256073
I0128 21:06:09.452849 139822745589568 spec.py:321] Evaluating on the training split.
I0128 21:06:15.634130 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 21:06:24.117611 139822745589568 spec.py:349] Evaluating on the test split.
I0128 21:06:26.653310 139822745589568 submission_runner.py:408] Time since start: 49646.37s, 	Step: 140737, 	{'train/accuracy': 0.8413185477256775, 'train/loss': 0.5797902345657349, 'validation/accuracy': 0.7160599827766418, 'validation/loss': 1.138371467590332, 'validation/num_examples': 50000, 'test/accuracy': 0.5914000272750854, 'test/loss': 1.8102748394012451, 'test/num_examples': 10000, 'score': 47978.06637239456, 'total_duration': 49646.37145638466, 'accumulated_submission_time': 47978.06637239456, 'accumulated_eval_time': 1658.625111579895, 'accumulated_logging_time': 4.895170450210571}
I0128 21:06:26.697295 139655617963776 logging_writer.py:48] [140737] accumulated_eval_time=1658.625112, accumulated_logging_time=4.895170, accumulated_submission_time=47978.066372, global_step=140737, preemption_count=0, score=47978.066372, test/accuracy=0.591400, test/loss=1.810275, test/num_examples=10000, total_duration=49646.371456, train/accuracy=0.841319, train/loss=0.579790, validation/accuracy=0.716060, validation/loss=1.138371, validation/num_examples=50000
I0128 21:06:48.449880 139655626356480 logging_writer.py:48] [140800] global_step=140800, grad_norm=6.912129878997803, loss=1.2057520151138306
I0128 21:07:22.463201 139655617963776 logging_writer.py:48] [140900] global_step=140900, grad_norm=6.357919216156006, loss=1.1912325620651245
I0128 21:07:56.486964 139655626356480 logging_writer.py:48] [141000] global_step=141000, grad_norm=6.6398138999938965, loss=1.162766933441162
I0128 21:08:30.540251 139655617963776 logging_writer.py:48] [141100] global_step=141100, grad_norm=6.397385597229004, loss=1.1684043407440186
I0128 21:09:04.568404 139655626356480 logging_writer.py:48] [141200] global_step=141200, grad_norm=6.702794075012207, loss=1.1925517320632935
I0128 21:09:38.621723 139655617963776 logging_writer.py:48] [141300] global_step=141300, grad_norm=6.623297691345215, loss=1.2575150728225708
I0128 21:10:12.657426 139655626356480 logging_writer.py:48] [141400] global_step=141400, grad_norm=7.28886604309082, loss=1.1362097263336182
I0128 21:10:46.730883 139655617963776 logging_writer.py:48] [141500] global_step=141500, grad_norm=7.0285258293151855, loss=1.2346291542053223
I0128 21:11:20.770371 139655626356480 logging_writer.py:48] [141600] global_step=141600, grad_norm=6.423446178436279, loss=1.236723780632019
I0128 21:11:54.832231 139655617963776 logging_writer.py:48] [141700] global_step=141700, grad_norm=6.078007698059082, loss=1.1438877582550049
I0128 21:12:29.030458 139655626356480 logging_writer.py:48] [141800] global_step=141800, grad_norm=6.962180137634277, loss=1.1245611906051636
I0128 21:13:03.096241 139655617963776 logging_writer.py:48] [141900] global_step=141900, grad_norm=6.98524284362793, loss=1.1812676191329956
I0128 21:13:37.132285 139655626356480 logging_writer.py:48] [142000] global_step=142000, grad_norm=6.4078450202941895, loss=1.1414427757263184
I0128 21:14:11.166444 139655617963776 logging_writer.py:48] [142100] global_step=142100, grad_norm=6.839639663696289, loss=1.2476346492767334
I0128 21:14:45.201462 139655626356480 logging_writer.py:48] [142200] global_step=142200, grad_norm=7.479702472686768, loss=1.2307956218719482
I0128 21:14:56.927654 139822745589568 spec.py:321] Evaluating on the training split.
I0128 21:15:03.119079 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 21:15:11.887912 139822745589568 spec.py:349] Evaluating on the test split.
I0128 21:15:14.482592 139822745589568 submission_runner.py:408] Time since start: 50174.20s, 	Step: 142236, 	{'train/accuracy': 0.8332070708274841, 'train/loss': 0.6023600101470947, 'validation/accuracy': 0.7219199538230896, 'validation/loss': 1.1166437864303589, 'validation/num_examples': 50000, 'test/accuracy': 0.5998000502586365, 'test/loss': 1.7817325592041016, 'test/num_examples': 10000, 'score': 48488.23559617996, 'total_duration': 50174.20073246956, 'accumulated_submission_time': 48488.23559617996, 'accumulated_eval_time': 1676.1800088882446, 'accumulated_logging_time': 4.949621677398682}
I0128 21:15:14.526885 139656666543872 logging_writer.py:48] [142236] accumulated_eval_time=1676.180009, accumulated_logging_time=4.949622, accumulated_submission_time=48488.235596, global_step=142236, preemption_count=0, score=48488.235596, test/accuracy=0.599800, test/loss=1.781733, test/num_examples=10000, total_duration=50174.200732, train/accuracy=0.833207, train/loss=0.602360, validation/accuracy=0.721920, validation/loss=1.116644, validation/num_examples=50000
I0128 21:15:36.603838 139656834316032 logging_writer.py:48] [142300] global_step=142300, grad_norm=7.693929672241211, loss=1.2396146059036255
I0128 21:16:10.600998 139656666543872 logging_writer.py:48] [142400] global_step=142400, grad_norm=6.405568599700928, loss=1.160435676574707
I0128 21:16:44.613764 139656834316032 logging_writer.py:48] [142500] global_step=142500, grad_norm=6.730010032653809, loss=1.1851873397827148
I0128 21:17:18.648189 139656666543872 logging_writer.py:48] [142600] global_step=142600, grad_norm=6.3791823387146, loss=1.1995331048965454
I0128 21:17:52.671711 139656834316032 logging_writer.py:48] [142700] global_step=142700, grad_norm=6.870651721954346, loss=1.197784185409546
I0128 21:18:26.702828 139656666543872 logging_writer.py:48] [142800] global_step=142800, grad_norm=7.302199363708496, loss=1.1474920511245728
I0128 21:19:00.844442 139656834316032 logging_writer.py:48] [142900] global_step=142900, grad_norm=6.1842193603515625, loss=1.0886969566345215
I0128 21:19:34.870350 139656666543872 logging_writer.py:48] [143000] global_step=143000, grad_norm=6.849264144897461, loss=1.235837459564209
I0128 21:20:08.910406 139656834316032 logging_writer.py:48] [143100] global_step=143100, grad_norm=7.028534412384033, loss=1.185747742652893
I0128 21:20:42.972382 139656666543872 logging_writer.py:48] [143200] global_step=143200, grad_norm=6.776230812072754, loss=1.1923977136611938
I0128 21:21:17.025167 139656834316032 logging_writer.py:48] [143300] global_step=143300, grad_norm=6.739951133728027, loss=1.2248013019561768
I0128 21:21:51.060461 139656666543872 logging_writer.py:48] [143400] global_step=143400, grad_norm=6.355172634124756, loss=1.0450000762939453
I0128 21:22:25.059578 139656834316032 logging_writer.py:48] [143500] global_step=143500, grad_norm=6.256063461303711, loss=1.0915918350219727
I0128 21:22:59.099178 139656666543872 logging_writer.py:48] [143600] global_step=143600, grad_norm=6.550120830535889, loss=1.26670241355896
I0128 21:23:33.146893 139656834316032 logging_writer.py:48] [143700] global_step=143700, grad_norm=6.850279331207275, loss=1.216485619544983
I0128 21:23:44.501403 139822745589568 spec.py:321] Evaluating on the training split.
I0128 21:23:50.576617 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 21:23:59.201975 139822745589568 spec.py:349] Evaluating on the test split.
I0128 21:24:01.722325 139822745589568 submission_runner.py:408] Time since start: 50701.44s, 	Step: 143735, 	{'train/accuracy': 0.8356783986091614, 'train/loss': 0.5958223938941956, 'validation/accuracy': 0.7263199687004089, 'validation/loss': 1.1001946926116943, 'validation/num_examples': 50000, 'test/accuracy': 0.6032000184059143, 'test/loss': 1.785300612449646, 'test/num_examples': 10000, 'score': 48998.14870905876, 'total_duration': 50701.440212488174, 'accumulated_submission_time': 48998.14870905876, 'accumulated_eval_time': 1693.400636434555, 'accumulated_logging_time': 5.0050599575042725}
I0128 21:24:01.792152 139655617963776 logging_writer.py:48] [143735] accumulated_eval_time=1693.400636, accumulated_logging_time=5.005060, accumulated_submission_time=48998.148709, global_step=143735, preemption_count=0, score=48998.148709, test/accuracy=0.603200, test/loss=1.785301, test/num_examples=10000, total_duration=50701.440212, train/accuracy=0.835678, train/loss=0.595822, validation/accuracy=0.726320, validation/loss=1.100195, validation/num_examples=50000
I0128 21:24:24.311456 139655626356480 logging_writer.py:48] [143800] global_step=143800, grad_norm=6.389349460601807, loss=1.115491509437561
I0128 21:24:58.509716 139655617963776 logging_writer.py:48] [143900] global_step=143900, grad_norm=7.291127681732178, loss=1.1624562740325928
I0128 21:25:32.510932 139655626356480 logging_writer.py:48] [144000] global_step=144000, grad_norm=6.76016902923584, loss=1.19557785987854
I0128 21:26:06.557353 139655617963776 logging_writer.py:48] [144100] global_step=144100, grad_norm=6.384415626525879, loss=1.0628299713134766
I0128 21:26:40.586494 139655626356480 logging_writer.py:48] [144200] global_step=144200, grad_norm=6.562222480773926, loss=1.1595699787139893
I0128 21:27:14.630876 139655617963776 logging_writer.py:48] [144300] global_step=144300, grad_norm=7.878013610839844, loss=1.1114428043365479
I0128 21:27:48.682761 139655626356480 logging_writer.py:48] [144400] global_step=144400, grad_norm=6.350196838378906, loss=1.1019701957702637
I0128 21:28:22.730611 139655617963776 logging_writer.py:48] [144500] global_step=144500, grad_norm=6.823889255523682, loss=1.1791727542877197
I0128 21:28:56.787354 139655626356480 logging_writer.py:48] [144600] global_step=144600, grad_norm=6.104464054107666, loss=1.1020549535751343
I0128 21:29:30.839846 139655617963776 logging_writer.py:48] [144700] global_step=144700, grad_norm=6.96761417388916, loss=1.2814087867736816
I0128 21:30:04.898186 139655626356480 logging_writer.py:48] [144800] global_step=144800, grad_norm=6.712403774261475, loss=1.188315987586975
I0128 21:30:38.933139 139655617963776 logging_writer.py:48] [144900] global_step=144900, grad_norm=7.782827377319336, loss=1.128252387046814
I0128 21:31:12.970055 139655626356480 logging_writer.py:48] [145000] global_step=145000, grad_norm=6.375181198120117, loss=1.0681354999542236
I0128 21:31:47.103444 139655617963776 logging_writer.py:48] [145100] global_step=145100, grad_norm=6.492147445678711, loss=1.047240972518921
I0128 21:32:21.136211 139655626356480 logging_writer.py:48] [145200] global_step=145200, grad_norm=6.8348870277404785, loss=1.1322166919708252
I0128 21:32:31.824063 139822745589568 spec.py:321] Evaluating on the training split.
I0128 21:32:37.965205 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 21:32:46.800029 139822745589568 spec.py:349] Evaluating on the test split.
I0128 21:32:49.275487 139822745589568 submission_runner.py:408] Time since start: 51228.99s, 	Step: 145233, 	{'train/accuracy': 0.8364556431770325, 'train/loss': 0.5910767316818237, 'validation/accuracy': 0.7298399806022644, 'validation/loss': 1.085693120956421, 'validation/num_examples': 50000, 'test/accuracy': 0.6122000217437744, 'test/loss': 1.7577836513519287, 'test/num_examples': 10000, 'score': 49508.116096019745, 'total_duration': 51228.993624448776, 'accumulated_submission_time': 49508.116096019745, 'accumulated_eval_time': 1710.852013349533, 'accumulated_logging_time': 5.087496757507324}
I0128 21:32:49.318691 139656658151168 logging_writer.py:48] [145233] accumulated_eval_time=1710.852013, accumulated_logging_time=5.087497, accumulated_submission_time=49508.116096, global_step=145233, preemption_count=0, score=49508.116096, test/accuracy=0.612200, test/loss=1.757784, test/num_examples=10000, total_duration=51228.993624, train/accuracy=0.836456, train/loss=0.591077, validation/accuracy=0.729840, validation/loss=1.085693, validation/num_examples=50000
I0128 21:33:12.451265 139656666543872 logging_writer.py:48] [145300] global_step=145300, grad_norm=6.635201454162598, loss=1.1073923110961914
I0128 21:33:46.465681 139656658151168 logging_writer.py:48] [145400] global_step=145400, grad_norm=6.85682487487793, loss=1.0387040376663208
I0128 21:34:20.511516 139656666543872 logging_writer.py:48] [145500] global_step=145500, grad_norm=7.422996520996094, loss=1.1880834102630615
I0128 21:34:54.561412 139656658151168 logging_writer.py:48] [145600] global_step=145600, grad_norm=6.968235492706299, loss=1.1039830446243286
I0128 21:35:28.636392 139656666543872 logging_writer.py:48] [145700] global_step=145700, grad_norm=6.943321228027344, loss=1.0867186784744263
I0128 21:36:02.688564 139656658151168 logging_writer.py:48] [145800] global_step=145800, grad_norm=6.412381172180176, loss=1.103331446647644
I0128 21:36:36.763576 139656666543872 logging_writer.py:48] [145900] global_step=145900, grad_norm=7.2941083908081055, loss=1.2447959184646606
I0128 21:37:10.799728 139656658151168 logging_writer.py:48] [146000] global_step=146000, grad_norm=6.632347106933594, loss=1.0967767238616943
I0128 21:37:44.870508 139656666543872 logging_writer.py:48] [146100] global_step=146100, grad_norm=6.6960272789001465, loss=1.2084922790527344
I0128 21:38:18.994570 139656658151168 logging_writer.py:48] [146200] global_step=146200, grad_norm=7.000945568084717, loss=1.171201229095459
I0128 21:38:53.040683 139656666543872 logging_writer.py:48] [146300] global_step=146300, grad_norm=6.487891674041748, loss=1.1175473928451538
I0128 21:39:27.055741 139656658151168 logging_writer.py:48] [146400] global_step=146400, grad_norm=8.152831077575684, loss=1.2266573905944824
I0128 21:40:01.103883 139656666543872 logging_writer.py:48] [146500] global_step=146500, grad_norm=7.099554061889648, loss=1.1246442794799805
I0128 21:40:35.147833 139656658151168 logging_writer.py:48] [146600] global_step=146600, grad_norm=7.49617338180542, loss=1.1811010837554932
I0128 21:41:09.181601 139656666543872 logging_writer.py:48] [146700] global_step=146700, grad_norm=6.550294399261475, loss=1.121168851852417
I0128 21:41:19.554488 139822745589568 spec.py:321] Evaluating on the training split.
I0128 21:41:25.683328 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 21:41:34.308536 139822745589568 spec.py:349] Evaluating on the test split.
I0128 21:41:36.708629 139822745589568 submission_runner.py:408] Time since start: 51756.43s, 	Step: 146732, 	{'train/accuracy': 0.83402419090271, 'train/loss': 0.6022161841392517, 'validation/accuracy': 0.729919970035553, 'validation/loss': 1.0840203762054443, 'validation/num_examples': 50000, 'test/accuracy': 0.6041000485420227, 'test/loss': 1.7766298055648804, 'test/num_examples': 10000, 'score': 50018.29201626778, 'total_duration': 51756.42676591873, 'accumulated_submission_time': 50018.29201626778, 'accumulated_eval_time': 1728.0061275959015, 'accumulated_logging_time': 5.139847993850708}
I0128 21:41:36.750304 139656297445120 logging_writer.py:48] [146732] accumulated_eval_time=1728.006128, accumulated_logging_time=5.139848, accumulated_submission_time=50018.292016, global_step=146732, preemption_count=0, score=50018.292016, test/accuracy=0.604100, test/loss=1.776630, test/num_examples=10000, total_duration=51756.426766, train/accuracy=0.834024, train/loss=0.602216, validation/accuracy=0.729920, validation/loss=1.084020, validation/num_examples=50000
I0128 21:42:00.231342 139656649758464 logging_writer.py:48] [146800] global_step=146800, grad_norm=6.773160934448242, loss=1.129417896270752
I0128 21:42:34.246786 139656297445120 logging_writer.py:48] [146900] global_step=146900, grad_norm=6.712085723876953, loss=1.0678985118865967
I0128 21:43:08.299490 139656649758464 logging_writer.py:48] [147000] global_step=147000, grad_norm=7.154675483703613, loss=1.1339613199234009
I0128 21:43:42.339223 139656297445120 logging_writer.py:48] [147100] global_step=147100, grad_norm=7.215452194213867, loss=1.174465537071228
I0128 21:44:16.473193 139656649758464 logging_writer.py:48] [147200] global_step=147200, grad_norm=6.532829761505127, loss=1.0854403972625732
I0128 21:44:50.510778 139656297445120 logging_writer.py:48] [147300] global_step=147300, grad_norm=6.9291300773620605, loss=1.162484884262085
I0128 21:45:24.558596 139656649758464 logging_writer.py:48] [147400] global_step=147400, grad_norm=6.544220447540283, loss=1.0773119926452637
I0128 21:45:58.589176 139656297445120 logging_writer.py:48] [147500] global_step=147500, grad_norm=6.30654239654541, loss=1.0016292333602905
I0128 21:46:32.630319 139656649758464 logging_writer.py:48] [147600] global_step=147600, grad_norm=7.006615161895752, loss=1.1703252792358398
I0128 21:47:06.658278 139656297445120 logging_writer.py:48] [147700] global_step=147700, grad_norm=7.128981590270996, loss=1.1271969079971313
I0128 21:47:40.704766 139656649758464 logging_writer.py:48] [147800] global_step=147800, grad_norm=6.826290130615234, loss=1.0841326713562012
I0128 21:48:14.726369 139656297445120 logging_writer.py:48] [147900] global_step=147900, grad_norm=7.034286022186279, loss=1.1874021291732788
I0128 21:48:48.771533 139656649758464 logging_writer.py:48] [148000] global_step=148000, grad_norm=7.129028797149658, loss=1.125619888305664
I0128 21:49:22.838884 139656297445120 logging_writer.py:48] [148100] global_step=148100, grad_norm=7.74822473526001, loss=1.0914690494537354
I0128 21:49:56.878342 139656649758464 logging_writer.py:48] [148200] global_step=148200, grad_norm=7.244772434234619, loss=1.0774205923080444
I0128 21:50:06.899369 139822745589568 spec.py:321] Evaluating on the training split.
I0128 21:50:13.051576 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 21:50:21.621412 139822745589568 spec.py:349] Evaluating on the test split.
I0128 21:50:24.134835 139822745589568 submission_runner.py:408] Time since start: 52283.85s, 	Step: 148231, 	{'train/accuracy': 0.8361965417861938, 'train/loss': 0.5804234743118286, 'validation/accuracy': 0.7318599820137024, 'validation/loss': 1.0679874420166016, 'validation/num_examples': 50000, 'test/accuracy': 0.6070000529289246, 'test/loss': 1.7447444200515747, 'test/num_examples': 10000, 'score': 50528.38182926178, 'total_duration': 52283.85293364525, 'accumulated_submission_time': 50528.38182926178, 'accumulated_eval_time': 1745.2415256500244, 'accumulated_logging_time': 5.190353631973267}
I0128 21:50:24.178136 139655626356480 logging_writer.py:48] [148231] accumulated_eval_time=1745.241526, accumulated_logging_time=5.190354, accumulated_submission_time=50528.381829, global_step=148231, preemption_count=0, score=50528.381829, test/accuracy=0.607000, test/loss=1.744744, test/num_examples=10000, total_duration=52283.852934, train/accuracy=0.836197, train/loss=0.580423, validation/accuracy=0.731860, validation/loss=1.067987, validation/num_examples=50000
I0128 21:50:48.075411 139656666543872 logging_writer.py:48] [148300] global_step=148300, grad_norm=7.222057819366455, loss=1.0589337348937988
I0128 21:51:22.087818 139655626356480 logging_writer.py:48] [148400] global_step=148400, grad_norm=7.361691951751709, loss=1.1145358085632324
I0128 21:51:56.104891 139656666543872 logging_writer.py:48] [148500] global_step=148500, grad_norm=6.963391304016113, loss=1.0695642232894897
I0128 21:52:30.121045 139655626356480 logging_writer.py:48] [148600] global_step=148600, grad_norm=6.228840351104736, loss=1.0729235410690308
I0128 21:53:04.152738 139656666543872 logging_writer.py:48] [148700] global_step=148700, grad_norm=7.246131420135498, loss=1.1140871047973633
I0128 21:53:38.184853 139655626356480 logging_writer.py:48] [148800] global_step=148800, grad_norm=6.865138530731201, loss=1.062525987625122
I0128 21:54:12.250623 139656666543872 logging_writer.py:48] [148900] global_step=148900, grad_norm=7.3137688636779785, loss=1.1248871088027954
I0128 21:54:46.310725 139655626356480 logging_writer.py:48] [149000] global_step=149000, grad_norm=7.030279159545898, loss=1.0621180534362793
I0128 21:55:20.335884 139656666543872 logging_writer.py:48] [149100] global_step=149100, grad_norm=7.100301742553711, loss=1.107037901878357
I0128 21:55:54.364475 139655626356480 logging_writer.py:48] [149200] global_step=149200, grad_norm=7.683491230010986, loss=1.1752562522888184
I0128 21:56:28.388029 139656666543872 logging_writer.py:48] [149300] global_step=149300, grad_norm=7.510843276977539, loss=1.0836845636367798
I0128 21:57:02.514314 139655626356480 logging_writer.py:48] [149400] global_step=149400, grad_norm=7.103618144989014, loss=1.0439682006835938
I0128 21:57:36.541600 139656666543872 logging_writer.py:48] [149500] global_step=149500, grad_norm=6.944694519042969, loss=1.095449447631836
I0128 21:58:10.583214 139655626356480 logging_writer.py:48] [149600] global_step=149600, grad_norm=7.964280128479004, loss=1.0393553972244263
I0128 21:58:44.657280 139656666543872 logging_writer.py:48] [149700] global_step=149700, grad_norm=7.719065189361572, loss=1.0854222774505615
I0128 21:58:54.335757 139822745589568 spec.py:321] Evaluating on the training split.
I0128 21:59:00.516033 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 21:59:09.360007 139822745589568 spec.py:349] Evaluating on the test split.
I0128 21:59:11.858051 139822745589568 submission_runner.py:408] Time since start: 52811.58s, 	Step: 149730, 	{'train/accuracy': 0.8720304369926453, 'train/loss': 0.4616715610027313, 'validation/accuracy': 0.7359199523925781, 'validation/loss': 1.06617271900177, 'validation/num_examples': 50000, 'test/accuracy': 0.6065000295639038, 'test/loss': 1.750016450881958, 'test/num_examples': 10000, 'score': 51038.4784321785, 'total_duration': 52811.57619476318, 'accumulated_submission_time': 51038.4784321785, 'accumulated_eval_time': 1762.763778924942, 'accumulated_logging_time': 5.244019508361816}
I0128 21:59:11.900106 139655617963776 logging_writer.py:48] [149730] accumulated_eval_time=1762.763779, accumulated_logging_time=5.244020, accumulated_submission_time=51038.478432, global_step=149730, preemption_count=0, score=51038.478432, test/accuracy=0.606500, test/loss=1.750016, test/num_examples=10000, total_duration=52811.576195, train/accuracy=0.872030, train/loss=0.461672, validation/accuracy=0.735920, validation/loss=1.066173, validation/num_examples=50000
I0128 21:59:36.047528 139656297445120 logging_writer.py:48] [149800] global_step=149800, grad_norm=7.276052951812744, loss=1.0715241432189941
I0128 22:00:10.059216 139655617963776 logging_writer.py:48] [149900] global_step=149900, grad_norm=6.718937873840332, loss=1.0336159467697144
I0128 22:00:44.124480 139656297445120 logging_writer.py:48] [150000] global_step=150000, grad_norm=7.366076469421387, loss=1.056221604347229
I0128 22:01:18.200857 139655617963776 logging_writer.py:48] [150100] global_step=150100, grad_norm=7.634733200073242, loss=1.0354490280151367
I0128 22:01:52.279964 139656297445120 logging_writer.py:48] [150200] global_step=150200, grad_norm=7.316063404083252, loss=1.0840169191360474
I0128 22:02:26.329062 139655617963776 logging_writer.py:48] [150300] global_step=150300, grad_norm=7.057040214538574, loss=1.0873267650604248
I0128 22:03:00.388828 139656297445120 logging_writer.py:48] [150400] global_step=150400, grad_norm=7.472329139709473, loss=1.1093409061431885
I0128 22:03:34.510799 139655617963776 logging_writer.py:48] [150500] global_step=150500, grad_norm=7.499305725097656, loss=1.0709983110427856
I0128 22:04:08.594420 139656297445120 logging_writer.py:48] [150600] global_step=150600, grad_norm=7.5506110191345215, loss=1.0724526643753052
I0128 22:04:42.653207 139655617963776 logging_writer.py:48] [150700] global_step=150700, grad_norm=7.335206985473633, loss=0.9977231025695801
I0128 22:05:16.700738 139656297445120 logging_writer.py:48] [150800] global_step=150800, grad_norm=6.243720054626465, loss=1.000124216079712
I0128 22:05:50.767107 139655617963776 logging_writer.py:48] [150900] global_step=150900, grad_norm=7.688564300537109, loss=1.031219244003296
I0128 22:06:24.802744 139656297445120 logging_writer.py:48] [151000] global_step=151000, grad_norm=7.373027801513672, loss=1.0229727029800415
I0128 22:06:58.825892 139655617963776 logging_writer.py:48] [151100] global_step=151100, grad_norm=7.446686267852783, loss=1.0685926675796509
I0128 22:07:32.871337 139656297445120 logging_writer.py:48] [151200] global_step=151200, grad_norm=7.164676666259766, loss=1.0653076171875
I0128 22:07:41.865310 139822745589568 spec.py:321] Evaluating on the training split.
I0128 22:07:48.064977 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 22:07:56.675307 139822745589568 spec.py:349] Evaluating on the test split.
I0128 22:07:59.185348 139822745589568 submission_runner.py:408] Time since start: 53338.90s, 	Step: 151228, 	{'train/accuracy': 0.8650151491165161, 'train/loss': 0.48393839597702026, 'validation/accuracy': 0.7351799607276917, 'validation/loss': 1.0641911029815674, 'validation/num_examples': 50000, 'test/accuracy': 0.6137000322341919, 'test/loss': 1.7262399196624756, 'test/num_examples': 10000, 'score': 51548.38336634636, 'total_duration': 53338.903485774994, 'accumulated_submission_time': 51548.38336634636, 'accumulated_eval_time': 1780.0837841033936, 'accumulated_logging_time': 5.296934366226196}
I0128 22:07:59.230908 139656666543872 logging_writer.py:48] [151228] accumulated_eval_time=1780.083784, accumulated_logging_time=5.296934, accumulated_submission_time=51548.383366, global_step=151228, preemption_count=0, score=51548.383366, test/accuracy=0.613700, test/loss=1.726240, test/num_examples=10000, total_duration=53338.903486, train/accuracy=0.865015, train/loss=0.483938, validation/accuracy=0.735180, validation/loss=1.064191, validation/num_examples=50000
I0128 22:08:24.046556 139656834316032 logging_writer.py:48] [151300] global_step=151300, grad_norm=7.942168712615967, loss=1.1040406227111816
I0128 22:08:58.040520 139656666543872 logging_writer.py:48] [151400] global_step=151400, grad_norm=7.076739311218262, loss=1.0113112926483154
I0128 22:09:32.074846 139656834316032 logging_writer.py:48] [151500] global_step=151500, grad_norm=7.624411582946777, loss=1.0622806549072266
I0128 22:10:06.189649 139656666543872 logging_writer.py:48] [151600] global_step=151600, grad_norm=7.173094272613525, loss=1.0254665613174438
I0128 22:10:40.243982 139656834316032 logging_writer.py:48] [151700] global_step=151700, grad_norm=7.918088436126709, loss=1.161316156387329
I0128 22:11:14.280582 139656666543872 logging_writer.py:48] [151800] global_step=151800, grad_norm=7.400777816772461, loss=1.1334095001220703
I0128 22:11:48.355564 139656834316032 logging_writer.py:48] [151900] global_step=151900, grad_norm=7.891779899597168, loss=1.0624547004699707
I0128 22:12:22.398409 139656666543872 logging_writer.py:48] [152000] global_step=152000, grad_norm=7.191218376159668, loss=0.9637895822525024
I0128 22:12:56.441595 139656834316032 logging_writer.py:48] [152100] global_step=152100, grad_norm=7.244046688079834, loss=1.0836738348007202
I0128 22:13:30.460610 139656666543872 logging_writer.py:48] [152200] global_step=152200, grad_norm=7.195831298828125, loss=1.1232726573944092
I0128 22:14:04.529439 139656834316032 logging_writer.py:48] [152300] global_step=152300, grad_norm=7.789840221405029, loss=1.0116922855377197
I0128 22:14:38.562636 139656666543872 logging_writer.py:48] [152400] global_step=152400, grad_norm=8.423128128051758, loss=1.0857350826263428
I0128 22:15:12.609326 139656834316032 logging_writer.py:48] [152500] global_step=152500, grad_norm=7.370634078979492, loss=0.9594842791557312
I0128 22:15:46.683630 139656666543872 logging_writer.py:48] [152600] global_step=152600, grad_norm=7.028827667236328, loss=1.0051732063293457
I0128 22:16:20.892470 139656834316032 logging_writer.py:48] [152700] global_step=152700, grad_norm=7.612254619598389, loss=0.9994921684265137
I0128 22:16:29.220042 139822745589568 spec.py:321] Evaluating on the training split.
I0128 22:16:35.413784 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 22:16:44.118605 139822745589568 spec.py:349] Evaluating on the test split.
I0128 22:16:46.638801 139822745589568 submission_runner.py:408] Time since start: 53866.36s, 	Step: 152726, 	{'train/accuracy': 0.8591955900192261, 'train/loss': 0.49269208312034607, 'validation/accuracy': 0.7340599894523621, 'validation/loss': 1.070358157157898, 'validation/num_examples': 50000, 'test/accuracy': 0.6089000105857849, 'test/loss': 1.7674813270568848, 'test/num_examples': 10000, 'score': 52058.31352877617, 'total_duration': 53866.35694384575, 'accumulated_submission_time': 52058.31352877617, 'accumulated_eval_time': 1797.5025045871735, 'accumulated_logging_time': 5.351749420166016}
I0128 22:16:46.685073 139655617963776 logging_writer.py:48] [152726] accumulated_eval_time=1797.502505, accumulated_logging_time=5.351749, accumulated_submission_time=52058.313529, global_step=152726, preemption_count=0, score=52058.313529, test/accuracy=0.608900, test/loss=1.767481, test/num_examples=10000, total_duration=53866.356944, train/accuracy=0.859196, train/loss=0.492692, validation/accuracy=0.734060, validation/loss=1.070358, validation/num_examples=50000
I0128 22:17:12.189526 139655626356480 logging_writer.py:48] [152800] global_step=152800, grad_norm=7.400534629821777, loss=0.9232098460197449
I0128 22:17:46.198440 139655617963776 logging_writer.py:48] [152900] global_step=152900, grad_norm=7.118778228759766, loss=0.9806933403015137
I0128 22:18:20.229530 139655626356480 logging_writer.py:48] [153000] global_step=153000, grad_norm=7.80684232711792, loss=1.0909758806228638
I0128 22:18:54.291835 139655617963776 logging_writer.py:48] [153100] global_step=153100, grad_norm=7.2270917892456055, loss=0.9686391353607178
I0128 22:19:28.335004 139655626356480 logging_writer.py:48] [153200] global_step=153200, grad_norm=6.867711544036865, loss=0.902566134929657
I0128 22:20:02.364908 139655617963776 logging_writer.py:48] [153300] global_step=153300, grad_norm=8.763038635253906, loss=0.9967402815818787
I0128 22:20:36.445001 139655626356480 logging_writer.py:48] [153400] global_step=153400, grad_norm=7.9134907722473145, loss=1.0313788652420044
I0128 22:21:10.498081 139655617963776 logging_writer.py:48] [153500] global_step=153500, grad_norm=7.690896987915039, loss=1.1050196886062622
I0128 22:21:44.542694 139655626356480 logging_writer.py:48] [153600] global_step=153600, grad_norm=8.333959579467773, loss=1.0670238733291626
I0128 22:22:18.586046 139655617963776 logging_writer.py:48] [153700] global_step=153700, grad_norm=7.597529411315918, loss=1.1762466430664062
I0128 22:22:52.746088 139655626356480 logging_writer.py:48] [153800] global_step=153800, grad_norm=7.179478168487549, loss=0.9908792972564697
I0128 22:23:26.781618 139655617963776 logging_writer.py:48] [153900] global_step=153900, grad_norm=7.713982105255127, loss=0.9940706491470337
I0128 22:24:00.846709 139655626356480 logging_writer.py:48] [154000] global_step=154000, grad_norm=8.179247856140137, loss=1.0401475429534912
I0128 22:24:34.886958 139655617963776 logging_writer.py:48] [154100] global_step=154100, grad_norm=7.3439788818359375, loss=0.9992272853851318
I0128 22:25:08.935962 139655626356480 logging_writer.py:48] [154200] global_step=154200, grad_norm=8.553646087646484, loss=1.0706850290298462
I0128 22:25:16.923383 139822745589568 spec.py:321] Evaluating on the training split.
I0128 22:25:23.048772 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 22:25:31.838928 139822745589568 spec.py:349] Evaluating on the test split.
I0128 22:25:34.267220 139822745589568 submission_runner.py:408] Time since start: 54393.99s, 	Step: 154225, 	{'train/accuracy': 0.8656927347183228, 'train/loss': 0.478325217962265, 'validation/accuracy': 0.7418199777603149, 'validation/loss': 1.0443856716156006, 'validation/num_examples': 50000, 'test/accuracy': 0.6195000410079956, 'test/loss': 1.7304673194885254, 'test/num_examples': 10000, 'score': 52568.48841428757, 'total_duration': 54393.9851975441, 'accumulated_submission_time': 52568.48841428757, 'accumulated_eval_time': 1814.8461351394653, 'accumulated_logging_time': 5.408911228179932}
I0128 22:25:34.313389 139656834316032 logging_writer.py:48] [154225] accumulated_eval_time=1814.846135, accumulated_logging_time=5.408911, accumulated_submission_time=52568.488414, global_step=154225, preemption_count=0, score=52568.488414, test/accuracy=0.619500, test/loss=1.730467, test/num_examples=10000, total_duration=54393.985198, train/accuracy=0.865693, train/loss=0.478325, validation/accuracy=0.741820, validation/loss=1.044386, validation/num_examples=50000
I0128 22:26:00.142210 139658730145536 logging_writer.py:48] [154300] global_step=154300, grad_norm=7.592066287994385, loss=1.0168769359588623
I0128 22:26:34.186401 139656834316032 logging_writer.py:48] [154400] global_step=154400, grad_norm=9.47368335723877, loss=1.0475432872772217
I0128 22:27:08.239188 139658730145536 logging_writer.py:48] [154500] global_step=154500, grad_norm=7.558047771453857, loss=0.9928381443023682
I0128 22:27:42.277280 139656834316032 logging_writer.py:48] [154600] global_step=154600, grad_norm=6.867910861968994, loss=1.058840274810791
I0128 22:28:16.339608 139658730145536 logging_writer.py:48] [154700] global_step=154700, grad_norm=7.2925705909729, loss=0.9683843851089478
I0128 22:28:50.377593 139656834316032 logging_writer.py:48] [154800] global_step=154800, grad_norm=7.093039512634277, loss=1.0420527458190918
I0128 22:29:24.504086 139658730145536 logging_writer.py:48] [154900] global_step=154900, grad_norm=7.524174690246582, loss=0.9701690673828125
I0128 22:29:58.531546 139656834316032 logging_writer.py:48] [155000] global_step=155000, grad_norm=9.271832466125488, loss=1.0693066120147705
I0128 22:30:32.592847 139658730145536 logging_writer.py:48] [155100] global_step=155100, grad_norm=7.77207612991333, loss=0.9869598746299744
I0128 22:31:06.635276 139656834316032 logging_writer.py:48] [155200] global_step=155200, grad_norm=7.6010661125183105, loss=0.9399771690368652
I0128 22:31:40.689926 139658730145536 logging_writer.py:48] [155300] global_step=155300, grad_norm=8.475343704223633, loss=1.0406124591827393
I0128 22:32:14.729388 139656834316032 logging_writer.py:48] [155400] global_step=155400, grad_norm=7.5078511238098145, loss=1.0278395414352417
I0128 22:32:48.779438 139658730145536 logging_writer.py:48] [155500] global_step=155500, grad_norm=7.764700412750244, loss=0.9506235718727112
I0128 22:33:22.821097 139656834316032 logging_writer.py:48] [155600] global_step=155600, grad_norm=9.011335372924805, loss=1.0542653799057007
I0128 22:33:56.872368 139658730145536 logging_writer.py:48] [155700] global_step=155700, grad_norm=8.30255126953125, loss=0.9369151592254639
I0128 22:34:04.513132 139822745589568 spec.py:321] Evaluating on the training split.
I0128 22:34:10.758729 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 22:34:19.538276 139822745589568 spec.py:349] Evaluating on the test split.
I0128 22:34:21.986031 139822745589568 submission_runner.py:408] Time since start: 54921.70s, 	Step: 155724, 	{'train/accuracy': 0.8673469424247742, 'train/loss': 0.4798938035964966, 'validation/accuracy': 0.7419399619102478, 'validation/loss': 1.0362374782562256, 'validation/num_examples': 50000, 'test/accuracy': 0.6204000115394592, 'test/loss': 1.7236957550048828, 'test/num_examples': 10000, 'score': 53078.627429008484, 'total_duration': 54921.7041721344, 'accumulated_submission_time': 53078.627429008484, 'accumulated_eval_time': 1832.3190059661865, 'accumulated_logging_time': 5.4648637771606445}
I0128 22:34:22.033528 139655626356480 logging_writer.py:48] [155724] accumulated_eval_time=1832.319006, accumulated_logging_time=5.464864, accumulated_submission_time=53078.627429, global_step=155724, preemption_count=0, score=53078.627429, test/accuracy=0.620400, test/loss=1.723696, test/num_examples=10000, total_duration=54921.704172, train/accuracy=0.867347, train/loss=0.479894, validation/accuracy=0.741940, validation/loss=1.036237, validation/num_examples=50000
I0128 22:34:48.249551 139656297445120 logging_writer.py:48] [155800] global_step=155800, grad_norm=7.285367965698242, loss=0.882993221282959
I0128 22:35:22.286855 139655626356480 logging_writer.py:48] [155900] global_step=155900, grad_norm=7.844075679779053, loss=1.0046006441116333
I0128 22:35:56.328696 139656297445120 logging_writer.py:48] [156000] global_step=156000, grad_norm=8.364680290222168, loss=1.001945972442627
I0128 22:36:30.348060 139655626356480 logging_writer.py:48] [156100] global_step=156100, grad_norm=7.484086990356445, loss=0.984977126121521
I0128 22:37:04.429023 139656297445120 logging_writer.py:48] [156200] global_step=156200, grad_norm=7.574686050415039, loss=1.0309675931930542
I0128 22:37:38.495728 139655626356480 logging_writer.py:48] [156300] global_step=156300, grad_norm=8.513795852661133, loss=1.013655662536621
I0128 22:38:12.553276 139656297445120 logging_writer.py:48] [156400] global_step=156400, grad_norm=8.039751052856445, loss=0.9552372694015503
I0128 22:38:46.576646 139655626356480 logging_writer.py:48] [156500] global_step=156500, grad_norm=8.064752578735352, loss=0.9308327436447144
I0128 22:39:20.633893 139656297445120 logging_writer.py:48] [156600] global_step=156600, grad_norm=8.258106231689453, loss=0.9379416108131409
I0128 22:39:54.688369 139655626356480 logging_writer.py:48] [156700] global_step=156700, grad_norm=7.797565460205078, loss=1.0143640041351318
I0128 22:40:28.784100 139656297445120 logging_writer.py:48] [156800] global_step=156800, grad_norm=7.648423671722412, loss=0.947346568107605
I0128 22:41:02.834467 139655626356480 logging_writer.py:48] [156900] global_step=156900, grad_norm=7.637884616851807, loss=0.9459567070007324
I0128 22:41:36.877661 139656297445120 logging_writer.py:48] [157000] global_step=157000, grad_norm=7.052810192108154, loss=0.859437882900238
I0128 22:42:11.004363 139655626356480 logging_writer.py:48] [157100] global_step=157100, grad_norm=7.563148498535156, loss=0.9837930202484131
I0128 22:42:45.037728 139656297445120 logging_writer.py:48] [157200] global_step=157200, grad_norm=8.743379592895508, loss=0.9913229942321777
I0128 22:42:52.009829 139822745589568 spec.py:321] Evaluating on the training split.
I0128 22:42:58.225913 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 22:43:07.104461 139822745589568 spec.py:349] Evaluating on the test split.
I0128 22:43:09.655009 139822745589568 submission_runner.py:408] Time since start: 55449.37s, 	Step: 157222, 	{'train/accuracy': 0.8683633208274841, 'train/loss': 0.46061205863952637, 'validation/accuracy': 0.7424399852752686, 'validation/loss': 1.0408451557159424, 'validation/num_examples': 50000, 'test/accuracy': 0.6229000091552734, 'test/loss': 1.7110683917999268, 'test/num_examples': 10000, 'score': 53588.541823387146, 'total_duration': 55449.373153209686, 'accumulated_submission_time': 53588.541823387146, 'accumulated_eval_time': 1849.9641468524933, 'accumulated_logging_time': 5.523918867111206}
I0128 22:43:09.698997 139655609571072 logging_writer.py:48] [157222] accumulated_eval_time=1849.964147, accumulated_logging_time=5.523919, accumulated_submission_time=53588.541823, global_step=157222, preemption_count=0, score=53588.541823, test/accuracy=0.622900, test/loss=1.711068, test/num_examples=10000, total_duration=55449.373153, train/accuracy=0.868363, train/loss=0.460612, validation/accuracy=0.742440, validation/loss=1.040845, validation/num_examples=50000
I0128 22:43:36.491245 139655617963776 logging_writer.py:48] [157300] global_step=157300, grad_norm=7.02705192565918, loss=0.8695535659790039
I0128 22:44:10.475076 139655609571072 logging_writer.py:48] [157400] global_step=157400, grad_norm=8.188291549682617, loss=1.0208687782287598
I0128 22:44:44.493163 139655617963776 logging_writer.py:48] [157500] global_step=157500, grad_norm=7.518199443817139, loss=0.8964821100234985
I0128 22:45:18.529589 139655609571072 logging_writer.py:48] [157600] global_step=157600, grad_norm=7.3056159019470215, loss=0.8915159106254578
I0128 22:45:52.548451 139655617963776 logging_writer.py:48] [157700] global_step=157700, grad_norm=7.376613140106201, loss=0.9453180432319641
I0128 22:46:26.604896 139655609571072 logging_writer.py:48] [157800] global_step=157800, grad_norm=7.205003261566162, loss=0.8998496532440186
I0128 22:47:00.665513 139655617963776 logging_writer.py:48] [157900] global_step=157900, grad_norm=8.956558227539062, loss=0.9814900159835815
I0128 22:47:34.711115 139655609571072 logging_writer.py:48] [158000] global_step=158000, grad_norm=8.240751266479492, loss=0.9681597352027893
I0128 22:48:08.836481 139655617963776 logging_writer.py:48] [158100] global_step=158100, grad_norm=7.764753818511963, loss=0.9712598323822021
I0128 22:48:42.889981 139655609571072 logging_writer.py:48] [158200] global_step=158200, grad_norm=7.35567045211792, loss=0.959426999092102
I0128 22:49:16.940291 139655617963776 logging_writer.py:48] [158300] global_step=158300, grad_norm=8.972332000732422, loss=1.0837855339050293
I0128 22:49:50.966084 139655609571072 logging_writer.py:48] [158400] global_step=158400, grad_norm=7.801868915557861, loss=1.014474630355835
I0128 22:50:25.004456 139655617963776 logging_writer.py:48] [158500] global_step=158500, grad_norm=8.2180814743042, loss=0.953148365020752
I0128 22:50:59.034617 139655609571072 logging_writer.py:48] [158600] global_step=158600, grad_norm=7.923691272735596, loss=0.9080793261528015
I0128 22:51:33.080717 139655617963776 logging_writer.py:48] [158700] global_step=158700, grad_norm=7.971466541290283, loss=1.0284297466278076
I0128 22:51:39.689413 139822745589568 spec.py:321] Evaluating on the training split.
I0128 22:51:45.846091 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 22:51:54.481459 139822745589568 spec.py:349] Evaluating on the test split.
I0128 22:51:56.997258 139822745589568 submission_runner.py:408] Time since start: 55976.72s, 	Step: 158721, 	{'train/accuracy': 0.8824138641357422, 'train/loss': 0.41793200373649597, 'validation/accuracy': 0.7461999654769897, 'validation/loss': 1.0250033140182495, 'validation/num_examples': 50000, 'test/accuracy': 0.6211000084877014, 'test/loss': 1.7047979831695557, 'test/num_examples': 10000, 'score': 54098.47330284119, 'total_duration': 55976.7153468132, 'accumulated_submission_time': 54098.47330284119, 'accumulated_eval_time': 1867.2719218730927, 'accumulated_logging_time': 5.576862573623657}
I0128 22:51:57.043279 139656649758464 logging_writer.py:48] [158721] accumulated_eval_time=1867.271922, accumulated_logging_time=5.576863, accumulated_submission_time=54098.473303, global_step=158721, preemption_count=0, score=54098.473303, test/accuracy=0.621100, test/loss=1.704798, test/num_examples=10000, total_duration=55976.715347, train/accuracy=0.882414, train/loss=0.417932, validation/accuracy=0.746200, validation/loss=1.025003, validation/num_examples=50000
I0128 22:52:24.233900 139656658151168 logging_writer.py:48] [158800] global_step=158800, grad_norm=7.458909034729004, loss=0.9670991897583008
I0128 22:52:58.223673 139656649758464 logging_writer.py:48] [158900] global_step=158900, grad_norm=8.709969520568848, loss=0.9569132924079895
I0128 22:53:32.272053 139656658151168 logging_writer.py:48] [159000] global_step=159000, grad_norm=8.208395004272461, loss=0.9722372889518738
I0128 22:54:06.315131 139656649758464 logging_writer.py:48] [159100] global_step=159100, grad_norm=7.930128574371338, loss=1.0009899139404297
I0128 22:54:40.428277 139656658151168 logging_writer.py:48] [159200] global_step=159200, grad_norm=7.8743438720703125, loss=0.9843619465827942
I0128 22:55:14.498753 139656649758464 logging_writer.py:48] [159300] global_step=159300, grad_norm=7.763704299926758, loss=0.9634206295013428
I0128 22:55:48.523770 139656658151168 logging_writer.py:48] [159400] global_step=159400, grad_norm=8.644874572753906, loss=1.0769777297973633
I0128 22:56:22.580336 139656649758464 logging_writer.py:48] [159500] global_step=159500, grad_norm=7.2956132888793945, loss=0.9256170392036438
I0128 22:56:56.649033 139656658151168 logging_writer.py:48] [159600] global_step=159600, grad_norm=8.380659103393555, loss=0.8750584721565247
I0128 22:57:30.727553 139656649758464 logging_writer.py:48] [159700] global_step=159700, grad_norm=9.449440956115723, loss=0.9491573572158813
I0128 22:58:04.781086 139656658151168 logging_writer.py:48] [159800] global_step=159800, grad_norm=7.395923614501953, loss=0.901005744934082
I0128 22:58:38.827696 139656649758464 logging_writer.py:48] [159900] global_step=159900, grad_norm=7.746912479400635, loss=0.9845820665359497
I0128 22:59:12.887653 139656658151168 logging_writer.py:48] [160000] global_step=160000, grad_norm=7.771626949310303, loss=0.8349066972732544
I0128 22:59:46.946770 139656649758464 logging_writer.py:48] [160100] global_step=160100, grad_norm=8.28927230834961, loss=0.9884354472160339
I0128 23:00:20.992108 139656658151168 logging_writer.py:48] [160200] global_step=160200, grad_norm=7.654020309448242, loss=0.9869176149368286
I0128 23:00:27.277584 139822745589568 spec.py:321] Evaluating on the training split.
I0128 23:00:33.496731 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 23:00:42.160282 139822745589568 spec.py:349] Evaluating on the test split.
I0128 23:00:44.599876 139822745589568 submission_runner.py:408] Time since start: 56504.32s, 	Step: 160220, 	{'train/accuracy': 0.8951291441917419, 'train/loss': 0.3667570948600769, 'validation/accuracy': 0.7464399933815002, 'validation/loss': 1.0255107879638672, 'validation/num_examples': 50000, 'test/accuracy': 0.6242000460624695, 'test/loss': 1.7131024599075317, 'test/num_examples': 10000, 'score': 54608.64770102501, 'total_duration': 56504.31801342964, 'accumulated_submission_time': 54608.64770102501, 'accumulated_eval_time': 1884.594167470932, 'accumulated_logging_time': 5.631555557250977}
I0128 23:00:44.644387 139656297445120 logging_writer.py:48] [160220] accumulated_eval_time=1884.594167, accumulated_logging_time=5.631556, accumulated_submission_time=54608.647701, global_step=160220, preemption_count=0, score=54608.647701, test/accuracy=0.624200, test/loss=1.713102, test/num_examples=10000, total_duration=56504.318013, train/accuracy=0.895129, train/loss=0.366757, validation/accuracy=0.746440, validation/loss=1.025511, validation/num_examples=50000
I0128 23:01:12.249000 139656666543872 logging_writer.py:48] [160300] global_step=160300, grad_norm=8.826720237731934, loss=0.9865090250968933
I0128 23:01:46.246355 139656297445120 logging_writer.py:48] [160400] global_step=160400, grad_norm=9.744026184082031, loss=0.9179711937904358
I0128 23:02:20.261704 139656666543872 logging_writer.py:48] [160500] global_step=160500, grad_norm=8.183252334594727, loss=0.924871563911438
I0128 23:02:54.293904 139656297445120 logging_writer.py:48] [160600] global_step=160600, grad_norm=7.886718273162842, loss=1.033942699432373
I0128 23:03:28.323936 139656666543872 logging_writer.py:48] [160700] global_step=160700, grad_norm=8.903860092163086, loss=1.038339614868164
I0128 23:04:02.368921 139656297445120 logging_writer.py:48] [160800] global_step=160800, grad_norm=9.345195770263672, loss=0.9446255564689636
I0128 23:04:36.421730 139656666543872 logging_writer.py:48] [160900] global_step=160900, grad_norm=7.672408103942871, loss=0.9932388067245483
I0128 23:05:10.447241 139656297445120 logging_writer.py:48] [161000] global_step=161000, grad_norm=8.700591087341309, loss=0.886818528175354
I0128 23:05:44.495083 139656666543872 logging_writer.py:48] [161100] global_step=161100, grad_norm=8.200881958007812, loss=0.9269615411758423
I0128 23:06:18.555835 139656297445120 logging_writer.py:48] [161200] global_step=161200, grad_norm=7.650393962860107, loss=0.9010964632034302
I0128 23:06:52.613681 139656666543872 logging_writer.py:48] [161300] global_step=161300, grad_norm=8.688401222229004, loss=1.012031078338623
I0128 23:07:26.727786 139656297445120 logging_writer.py:48] [161400] global_step=161400, grad_norm=8.088757514953613, loss=0.8647677302360535
I0128 23:08:00.798111 139656666543872 logging_writer.py:48] [161500] global_step=161500, grad_norm=7.9543914794921875, loss=0.9805610179901123
I0128 23:08:34.857681 139656297445120 logging_writer.py:48] [161600] global_step=161600, grad_norm=7.8526787757873535, loss=0.9145480394363403
I0128 23:09:08.887216 139656666543872 logging_writer.py:48] [161700] global_step=161700, grad_norm=8.807148933410645, loss=0.8302407264709473
I0128 23:09:14.818620 139822745589568 spec.py:321] Evaluating on the training split.
I0128 23:09:20.980746 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 23:09:30.001302 139822745589568 spec.py:349] Evaluating on the test split.
I0128 23:09:32.408544 139822745589568 submission_runner.py:408] Time since start: 57032.13s, 	Step: 161719, 	{'train/accuracy': 0.8964046239852905, 'train/loss': 0.3655628263950348, 'validation/accuracy': 0.7499399781227112, 'validation/loss': 1.0128942728042603, 'validation/num_examples': 50000, 'test/accuracy': 0.6291000247001648, 'test/loss': 1.6922518014907837, 'test/num_examples': 10000, 'score': 55118.76144886017, 'total_duration': 57032.126682281494, 'accumulated_submission_time': 55118.76144886017, 'accumulated_eval_time': 1902.1840479373932, 'accumulated_logging_time': 5.686068296432495}
I0128 23:09:32.456725 139656658151168 logging_writer.py:48] [161719] accumulated_eval_time=1902.184048, accumulated_logging_time=5.686068, accumulated_submission_time=55118.761449, global_step=161719, preemption_count=0, score=55118.761449, test/accuracy=0.629100, test/loss=1.692252, test/num_examples=10000, total_duration=57032.126682, train/accuracy=0.896405, train/loss=0.365563, validation/accuracy=0.749940, validation/loss=1.012894, validation/num_examples=50000
I0128 23:10:00.339868 139658730145536 logging_writer.py:48] [161800] global_step=161800, grad_norm=8.460509300231934, loss=0.8659959435462952
I0128 23:10:34.352807 139656658151168 logging_writer.py:48] [161900] global_step=161900, grad_norm=7.735274791717529, loss=0.8761578798294067
I0128 23:11:08.355059 139658730145536 logging_writer.py:48] [162000] global_step=162000, grad_norm=8.461573600769043, loss=0.9544675350189209
I0128 23:11:42.375415 139656658151168 logging_writer.py:48] [162100] global_step=162100, grad_norm=7.579866886138916, loss=0.8636280298233032
I0128 23:12:16.391132 139658730145536 logging_writer.py:48] [162200] global_step=162200, grad_norm=8.439942359924316, loss=0.9453099370002747
I0128 23:12:50.459616 139656658151168 logging_writer.py:48] [162300] global_step=162300, grad_norm=8.38117504119873, loss=0.9803603291511536
I0128 23:13:24.507396 139658730145536 logging_writer.py:48] [162400] global_step=162400, grad_norm=7.682738304138184, loss=0.8430218696594238
I0128 23:13:58.632848 139656658151168 logging_writer.py:48] [162500] global_step=162500, grad_norm=8.070216178894043, loss=0.8927141427993774
I0128 23:14:32.682742 139658730145536 logging_writer.py:48] [162600] global_step=162600, grad_norm=7.800783157348633, loss=0.8639634251594543
I0128 23:15:06.731746 139656658151168 logging_writer.py:48] [162700] global_step=162700, grad_norm=7.738929748535156, loss=0.8881754279136658
I0128 23:15:40.808787 139658730145536 logging_writer.py:48] [162800] global_step=162800, grad_norm=8.60079288482666, loss=0.9150009751319885
I0128 23:16:14.868824 139656658151168 logging_writer.py:48] [162900] global_step=162900, grad_norm=8.211556434631348, loss=0.844632089138031
I0128 23:16:48.906656 139658730145536 logging_writer.py:48] [163000] global_step=163000, grad_norm=7.423530578613281, loss=0.880616307258606
I0128 23:17:22.949985 139656658151168 logging_writer.py:48] [163100] global_step=163100, grad_norm=8.4678316116333, loss=0.8824769258499146
I0128 23:17:57.007159 139658730145536 logging_writer.py:48] [163200] global_step=163200, grad_norm=7.764517784118652, loss=0.926115870475769
I0128 23:18:02.589806 139822745589568 spec.py:321] Evaluating on the training split.
I0128 23:18:08.718115 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 23:18:17.397484 139822745589568 spec.py:349] Evaluating on the test split.
I0128 23:18:19.821518 139822745589568 submission_runner.py:408] Time since start: 57559.54s, 	Step: 163218, 	{'train/accuracy': 0.8958067297935486, 'train/loss': 0.3668033480644226, 'validation/accuracy': 0.7515599727630615, 'validation/loss': 1.0076985359191895, 'validation/num_examples': 50000, 'test/accuracy': 0.6283000111579895, 'test/loss': 1.6882383823394775, 'test/num_examples': 10000, 'score': 55628.83065366745, 'total_duration': 57559.539657115936, 'accumulated_submission_time': 55628.83065366745, 'accumulated_eval_time': 1919.415715456009, 'accumulated_logging_time': 5.746610403060913}
I0128 23:18:19.869510 139655626356480 logging_writer.py:48] [163218] accumulated_eval_time=1919.415715, accumulated_logging_time=5.746610, accumulated_submission_time=55628.830654, global_step=163218, preemption_count=0, score=55628.830654, test/accuracy=0.628300, test/loss=1.688238, test/num_examples=10000, total_duration=57559.539657, train/accuracy=0.895807, train/loss=0.366803, validation/accuracy=0.751560, validation/loss=1.007699, validation/num_examples=50000
I0128 23:18:48.096610 139656297445120 logging_writer.py:48] [163300] global_step=163300, grad_norm=8.575139045715332, loss=0.8939793705940247
I0128 23:19:22.122769 139655626356480 logging_writer.py:48] [163400] global_step=163400, grad_norm=8.257540702819824, loss=0.8751299977302551
I0128 23:19:56.174460 139656297445120 logging_writer.py:48] [163500] global_step=163500, grad_norm=7.929429054260254, loss=0.8654223680496216
I0128 23:20:30.297044 139655626356480 logging_writer.py:48] [163600] global_step=163600, grad_norm=8.200109481811523, loss=0.8569537997245789
I0128 23:21:04.356375 139656297445120 logging_writer.py:48] [163700] global_step=163700, grad_norm=8.076395988464355, loss=0.8962854146957397
I0128 23:21:38.400811 139655626356480 logging_writer.py:48] [163800] global_step=163800, grad_norm=7.811238765716553, loss=0.8156458735466003
I0128 23:22:12.456794 139656297445120 logging_writer.py:48] [163900] global_step=163900, grad_norm=8.849251747131348, loss=0.8956649303436279
I0128 23:22:46.504400 139655626356480 logging_writer.py:48] [164000] global_step=164000, grad_norm=9.266010284423828, loss=0.9215427041053772
I0128 23:23:20.555566 139656297445120 logging_writer.py:48] [164100] global_step=164100, grad_norm=8.531962394714355, loss=0.8647562861442566
I0128 23:23:54.594043 139655626356480 logging_writer.py:48] [164200] global_step=164200, grad_norm=8.042760848999023, loss=0.8363556265830994
I0128 23:24:28.639147 139656297445120 logging_writer.py:48] [164300] global_step=164300, grad_norm=8.418349266052246, loss=0.8906023502349854
I0128 23:25:02.680452 139655626356480 logging_writer.py:48] [164400] global_step=164400, grad_norm=8.006370544433594, loss=0.9648776650428772
I0128 23:25:36.698626 139656297445120 logging_writer.py:48] [164500] global_step=164500, grad_norm=8.575213432312012, loss=0.8709155917167664
I0128 23:26:10.726940 139655626356480 logging_writer.py:48] [164600] global_step=164600, grad_norm=9.00475025177002, loss=0.9072078466415405
I0128 23:26:44.833025 139656297445120 logging_writer.py:48] [164700] global_step=164700, grad_norm=9.55247974395752, loss=0.8971526622772217
I0128 23:26:50.085960 139822745589568 spec.py:321] Evaluating on the training split.
I0128 23:26:56.178297 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 23:27:05.051548 139822745589568 spec.py:349] Evaluating on the test split.
I0128 23:27:08.016309 139822745589568 submission_runner.py:408] Time since start: 58087.73s, 	Step: 164717, 	{'train/accuracy': 0.89652419090271, 'train/loss': 0.35903504490852356, 'validation/accuracy': 0.7538999915122986, 'validation/loss': 0.9958780407905579, 'validation/num_examples': 50000, 'test/accuracy': 0.6323000192642212, 'test/loss': 1.692557692527771, 'test/num_examples': 10000, 'score': 56138.98541164398, 'total_duration': 58087.73439216614, 'accumulated_submission_time': 56138.98541164398, 'accumulated_eval_time': 1937.3459751605988, 'accumulated_logging_time': 5.80523681640625}
I0128 23:27:08.053411 139656666543872 logging_writer.py:48] [164717] accumulated_eval_time=1937.345975, accumulated_logging_time=5.805237, accumulated_submission_time=56138.985412, global_step=164717, preemption_count=0, score=56138.985412, test/accuracy=0.632300, test/loss=1.692558, test/num_examples=10000, total_duration=58087.734392, train/accuracy=0.896524, train/loss=0.359035, validation/accuracy=0.753900, validation/loss=0.995878, validation/num_examples=50000
I0128 23:27:36.584982 139656834316032 logging_writer.py:48] [164800] global_step=164800, grad_norm=8.544816970825195, loss=0.8975200653076172
I0128 23:28:10.584548 139656666543872 logging_writer.py:48] [164900] global_step=164900, grad_norm=8.770723342895508, loss=0.8644841909408569
I0128 23:28:44.614561 139656834316032 logging_writer.py:48] [165000] global_step=165000, grad_norm=8.898483276367188, loss=0.8592944741249084
I0128 23:29:18.644453 139656666543872 logging_writer.py:48] [165100] global_step=165100, grad_norm=8.105460166931152, loss=0.8765895366668701
I0128 23:29:52.696260 139656834316032 logging_writer.py:48] [165200] global_step=165200, grad_norm=9.483420372009277, loss=0.8859480619430542
I0128 23:30:26.747252 139656666543872 logging_writer.py:48] [165300] global_step=165300, grad_norm=8.555269241333008, loss=0.8817508220672607
I0128 23:31:00.800658 139656834316032 logging_writer.py:48] [165400] global_step=165400, grad_norm=8.20093822479248, loss=0.8965656161308289
I0128 23:31:34.823356 139656666543872 logging_writer.py:48] [165500] global_step=165500, grad_norm=8.215216636657715, loss=0.8764196038246155
I0128 23:32:08.885560 139656834316032 logging_writer.py:48] [165600] global_step=165600, grad_norm=8.585511207580566, loss=0.8827106952667236
I0128 23:32:42.955508 139656666543872 logging_writer.py:48] [165700] global_step=165700, grad_norm=8.346412658691406, loss=0.829918622970581
I0128 23:33:17.084882 139656834316032 logging_writer.py:48] [165800] global_step=165800, grad_norm=8.573983192443848, loss=0.8753131628036499
I0128 23:33:51.135626 139656666543872 logging_writer.py:48] [165900] global_step=165900, grad_norm=7.796062469482422, loss=0.8317866921424866
I0128 23:34:25.176458 139656834316032 logging_writer.py:48] [166000] global_step=166000, grad_norm=8.874568939208984, loss=0.8374086618423462
I0128 23:34:59.225384 139656666543872 logging_writer.py:48] [166100] global_step=166100, grad_norm=7.719707489013672, loss=0.8315660357475281
I0128 23:35:33.304818 139656834316032 logging_writer.py:48] [166200] global_step=166200, grad_norm=7.701957702636719, loss=0.8145555853843689
I0128 23:35:38.209671 139822745589568 spec.py:321] Evaluating on the training split.
I0128 23:35:44.409087 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 23:35:53.227058 139822745589568 spec.py:349] Evaluating on the test split.
I0128 23:35:55.619797 139822745589568 submission_runner.py:408] Time since start: 58615.34s, 	Step: 166216, 	{'train/accuracy': 0.9032605290412903, 'train/loss': 0.3413633108139038, 'validation/accuracy': 0.7542799711227417, 'validation/loss': 0.9981317520141602, 'validation/num_examples': 50000, 'test/accuracy': 0.6308000087738037, 'test/loss': 1.68972647190094, 'test/num_examples': 10000, 'score': 56649.082023859024, 'total_duration': 58615.33793616295, 'accumulated_submission_time': 56649.082023859024, 'accumulated_eval_time': 1954.7560713291168, 'accumulated_logging_time': 5.8507981300354}
I0128 23:35:55.667398 139655617963776 logging_writer.py:48] [166216] accumulated_eval_time=1954.756071, accumulated_logging_time=5.850798, accumulated_submission_time=56649.082024, global_step=166216, preemption_count=0, score=56649.082024, test/accuracy=0.630800, test/loss=1.689726, test/num_examples=10000, total_duration=58615.337936, train/accuracy=0.903261, train/loss=0.341363, validation/accuracy=0.754280, validation/loss=0.998132, validation/num_examples=50000
I0128 23:36:24.513373 139655626356480 logging_writer.py:48] [166300] global_step=166300, grad_norm=8.395438194274902, loss=0.88311368227005
I0128 23:36:58.557523 139655617963776 logging_writer.py:48] [166400] global_step=166400, grad_norm=8.769155502319336, loss=0.929537832736969
I0128 23:37:32.593653 139655626356480 logging_writer.py:48] [166500] global_step=166500, grad_norm=8.59929084777832, loss=0.83056640625
I0128 23:38:06.635125 139655617963776 logging_writer.py:48] [166600] global_step=166600, grad_norm=8.412410736083984, loss=0.8022285103797913
I0128 23:38:40.696644 139655626356480 logging_writer.py:48] [166700] global_step=166700, grad_norm=8.653148651123047, loss=0.9645919799804688
I0128 23:39:14.741991 139655617963776 logging_writer.py:48] [166800] global_step=166800, grad_norm=8.135183334350586, loss=0.8318445682525635
I0128 23:39:48.866918 139655626356480 logging_writer.py:48] [166900] global_step=166900, grad_norm=9.48231029510498, loss=0.8427771329879761
I0128 23:40:22.898026 139655617963776 logging_writer.py:48] [167000] global_step=167000, grad_norm=7.808739185333252, loss=0.8385494351387024
I0128 23:40:56.923841 139655626356480 logging_writer.py:48] [167100] global_step=167100, grad_norm=8.667425155639648, loss=0.9704211950302124
I0128 23:41:31.002962 139655617963776 logging_writer.py:48] [167200] global_step=167200, grad_norm=8.157848358154297, loss=0.8048697113990784
I0128 23:42:05.069812 139655626356480 logging_writer.py:48] [167300] global_step=167300, grad_norm=8.45281982421875, loss=0.8327012062072754
I0128 23:42:39.146593 139655617963776 logging_writer.py:48] [167400] global_step=167400, grad_norm=8.302095413208008, loss=0.844272255897522
I0128 23:43:13.227494 139655626356480 logging_writer.py:48] [167500] global_step=167500, grad_norm=8.407637596130371, loss=0.791944682598114
I0128 23:43:47.267086 139655617963776 logging_writer.py:48] [167600] global_step=167600, grad_norm=8.129396438598633, loss=0.8388184309005737
I0128 23:44:21.330073 139655626356480 logging_writer.py:48] [167700] global_step=167700, grad_norm=8.029886245727539, loss=0.8072577714920044
I0128 23:44:25.900002 139822745589568 spec.py:321] Evaluating on the training split.
I0128 23:44:32.082747 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 23:44:40.700192 139822745589568 spec.py:349] Evaluating on the test split.
I0128 23:44:43.222724 139822745589568 submission_runner.py:408] Time since start: 59142.94s, 	Step: 167715, 	{'train/accuracy': 0.9057517647743225, 'train/loss': 0.3304113447666168, 'validation/accuracy': 0.755620002746582, 'validation/loss': 0.9929838180541992, 'validation/num_examples': 50000, 'test/accuracy': 0.6345000267028809, 'test/loss': 1.6859267950057983, 'test/num_examples': 10000, 'score': 57159.2556912899, 'total_duration': 59142.94081425667, 'accumulated_submission_time': 57159.2556912899, 'accumulated_eval_time': 1972.07869887352, 'accumulated_logging_time': 5.9076502323150635}
I0128 23:44:43.268251 139656658151168 logging_writer.py:48] [167715] accumulated_eval_time=1972.078699, accumulated_logging_time=5.907650, accumulated_submission_time=57159.255691, global_step=167715, preemption_count=0, score=57159.255691, test/accuracy=0.634500, test/loss=1.685927, test/num_examples=10000, total_duration=59142.940814, train/accuracy=0.905752, train/loss=0.330411, validation/accuracy=0.755620, validation/loss=0.992984, validation/num_examples=50000
I0128 23:45:12.535185 139656666543872 logging_writer.py:48] [167800] global_step=167800, grad_norm=8.753641128540039, loss=0.813527524471283
I0128 23:45:46.541150 139656658151168 logging_writer.py:48] [167900] global_step=167900, grad_norm=7.877997875213623, loss=0.7913660407066345
I0128 23:46:20.682877 139656666543872 logging_writer.py:48] [168000] global_step=168000, grad_norm=7.93604850769043, loss=0.7603898048400879
I0128 23:46:54.747478 139656658151168 logging_writer.py:48] [168100] global_step=168100, grad_norm=9.13278579711914, loss=0.857445240020752
I0128 23:47:28.806554 139656666543872 logging_writer.py:48] [168200] global_step=168200, grad_norm=9.489167213439941, loss=0.8801050186157227
I0128 23:48:02.865130 139656658151168 logging_writer.py:48] [168300] global_step=168300, grad_norm=9.127093315124512, loss=0.8389121294021606
I0128 23:48:36.926356 139656666543872 logging_writer.py:48] [168400] global_step=168400, grad_norm=7.944761753082275, loss=0.7982072830200195
I0128 23:49:10.986289 139656658151168 logging_writer.py:48] [168500] global_step=168500, grad_norm=9.064800262451172, loss=0.9018798470497131
I0128 23:49:45.038848 139656666543872 logging_writer.py:48] [168600] global_step=168600, grad_norm=8.276891708374023, loss=0.8745858073234558
I0128 23:50:19.080591 139656658151168 logging_writer.py:48] [168700] global_step=168700, grad_norm=8.732087135314941, loss=0.832831859588623
I0128 23:50:53.165020 139656666543872 logging_writer.py:48] [168800] global_step=168800, grad_norm=9.794562339782715, loss=0.8136002421379089
I0128 23:51:27.216653 139656658151168 logging_writer.py:48] [168900] global_step=168900, grad_norm=8.389379501342773, loss=0.845231831073761
I0128 23:52:01.257536 139656666543872 logging_writer.py:48] [169000] global_step=169000, grad_norm=8.813633918762207, loss=0.8730192184448242
I0128 23:52:35.357903 139656658151168 logging_writer.py:48] [169100] global_step=169100, grad_norm=9.077580451965332, loss=0.8402104377746582
I0128 23:53:09.432897 139656666543872 logging_writer.py:48] [169200] global_step=169200, grad_norm=9.012256622314453, loss=0.7457228302955627
I0128 23:53:13.341966 139822745589568 spec.py:321] Evaluating on the training split.
I0128 23:53:19.465131 139822745589568 spec.py:333] Evaluating on the validation split.
I0128 23:53:27.959818 139822745589568 spec.py:349] Evaluating on the test split.
I0128 23:53:30.495126 139822745589568 submission_runner.py:408] Time since start: 59670.21s, 	Step: 169213, 	{'train/accuracy': 0.920340359210968, 'train/loss': 0.28462982177734375, 'validation/accuracy': 0.7571199536323547, 'validation/loss': 0.9897215962409973, 'validation/num_examples': 50000, 'test/accuracy': 0.6319000124931335, 'test/loss': 1.6728354692459106, 'test/num_examples': 10000, 'score': 57669.26935172081, 'total_duration': 59670.21319055557, 'accumulated_submission_time': 57669.26935172081, 'accumulated_eval_time': 1989.2317397594452, 'accumulated_logging_time': 5.962466239929199}
I0128 23:53:30.541585 139658730145536 logging_writer.py:48] [169213] accumulated_eval_time=1989.231740, accumulated_logging_time=5.962466, accumulated_submission_time=57669.269352, global_step=169213, preemption_count=0, score=57669.269352, test/accuracy=0.631900, test/loss=1.672835, test/num_examples=10000, total_duration=59670.213191, train/accuracy=0.920340, train/loss=0.284630, validation/accuracy=0.757120, validation/loss=0.989722, validation/num_examples=50000
I0128 23:54:00.451451 139658738538240 logging_writer.py:48] [169300] global_step=169300, grad_norm=9.251334190368652, loss=0.917071521282196
I0128 23:54:34.457732 139658730145536 logging_writer.py:48] [169400] global_step=169400, grad_norm=8.277419090270996, loss=0.7566045522689819
I0128 23:55:08.475600 139658738538240 logging_writer.py:48] [169500] global_step=169500, grad_norm=10.30230712890625, loss=0.7918909788131714
I0128 23:55:42.522065 139658730145536 logging_writer.py:48] [169600] global_step=169600, grad_norm=8.099681854248047, loss=0.8046638369560242
I0128 23:56:16.589484 139658738538240 logging_writer.py:48] [169700] global_step=169700, grad_norm=8.53105640411377, loss=0.791780412197113
I0128 23:56:50.642314 139658730145536 logging_writer.py:48] [169800] global_step=169800, grad_norm=8.52704906463623, loss=0.8619351387023926
I0128 23:57:24.711492 139658738538240 logging_writer.py:48] [169900] global_step=169900, grad_norm=8.554065704345703, loss=0.7919081449508667
I0128 23:57:58.779525 139658730145536 logging_writer.py:48] [170000] global_step=170000, grad_norm=9.668274879455566, loss=0.9085802435874939
I0128 23:58:32.813497 139658738538240 logging_writer.py:48] [170100] global_step=170100, grad_norm=8.778905868530273, loss=0.7705512046813965
I0128 23:59:06.924325 139658730145536 logging_writer.py:48] [170200] global_step=170200, grad_norm=8.408553123474121, loss=0.7998974919319153
I0128 23:59:41.004450 139658738538240 logging_writer.py:48] [170300] global_step=170300, grad_norm=8.066211700439453, loss=0.792462170124054
I0129 00:00:15.096415 139658730145536 logging_writer.py:48] [170400] global_step=170400, grad_norm=8.712956428527832, loss=0.8410589694976807
I0129 00:00:49.147774 139658738538240 logging_writer.py:48] [170500] global_step=170500, grad_norm=8.479928970336914, loss=0.7915313243865967
I0129 00:01:23.208526 139658730145536 logging_writer.py:48] [170600] global_step=170600, grad_norm=9.201553344726562, loss=0.8456618785858154
I0129 00:01:57.245580 139658738538240 logging_writer.py:48] [170700] global_step=170700, grad_norm=9.3792085647583, loss=0.8846535086631775
I0129 00:02:00.788099 139822745589568 spec.py:321] Evaluating on the training split.
I0129 00:02:07.189873 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 00:02:15.786373 139822745589568 spec.py:349] Evaluating on the test split.
I0129 00:02:18.293073 139822745589568 submission_runner.py:408] Time since start: 60198.01s, 	Step: 170712, 	{'train/accuracy': 0.9207190275192261, 'train/loss': 0.2819788455963135, 'validation/accuracy': 0.7594199776649475, 'validation/loss': 0.9810996651649475, 'validation/num_examples': 50000, 'test/accuracy': 0.6370000243186951, 'test/loss': 1.660903811454773, 'test/num_examples': 10000, 'score': 58179.45411133766, 'total_duration': 60198.01121592522, 'accumulated_submission_time': 58179.45411133766, 'accumulated_eval_time': 2006.7366869449615, 'accumulated_logging_time': 6.018509387969971}
I0129 00:02:18.340712 139656297445120 logging_writer.py:48] [170712] accumulated_eval_time=2006.736687, accumulated_logging_time=6.018509, accumulated_submission_time=58179.454111, global_step=170712, preemption_count=0, score=58179.454111, test/accuracy=0.637000, test/loss=1.660904, test/num_examples=10000, total_duration=60198.011216, train/accuracy=0.920719, train/loss=0.281979, validation/accuracy=0.759420, validation/loss=0.981100, validation/num_examples=50000
I0129 00:02:48.620491 139656649758464 logging_writer.py:48] [170800] global_step=170800, grad_norm=8.548129081726074, loss=0.7962868213653564
I0129 00:03:22.610236 139656297445120 logging_writer.py:48] [170900] global_step=170900, grad_norm=9.468510627746582, loss=0.9060909748077393
I0129 00:03:56.665931 139656649758464 logging_writer.py:48] [171000] global_step=171000, grad_norm=8.112067222595215, loss=0.8146692514419556
I0129 00:04:30.709595 139656297445120 logging_writer.py:48] [171100] global_step=171100, grad_norm=10.371698379516602, loss=0.7633343935012817
I0129 00:05:04.771390 139656649758464 logging_writer.py:48] [171200] global_step=171200, grad_norm=8.888291358947754, loss=0.805869460105896
I0129 00:05:38.903188 139656297445120 logging_writer.py:48] [171300] global_step=171300, grad_norm=9.479395866394043, loss=0.800218403339386
I0129 00:06:12.951502 139656649758464 logging_writer.py:48] [171400] global_step=171400, grad_norm=8.730913162231445, loss=0.8149840235710144
I0129 00:06:47.014036 139656297445120 logging_writer.py:48] [171500] global_step=171500, grad_norm=8.24722957611084, loss=0.7389169931411743
I0129 00:07:21.074923 139656649758464 logging_writer.py:48] [171600] global_step=171600, grad_norm=8.498878479003906, loss=0.766209602355957
I0129 00:07:55.156660 139656297445120 logging_writer.py:48] [171700] global_step=171700, grad_norm=9.414422988891602, loss=0.8724315166473389
I0129 00:08:29.251219 139656649758464 logging_writer.py:48] [171800] global_step=171800, grad_norm=8.343008041381836, loss=0.7372027039527893
I0129 00:09:03.312660 139656297445120 logging_writer.py:48] [171900] global_step=171900, grad_norm=9.38560676574707, loss=0.7482372522354126
I0129 00:09:37.385322 139656649758464 logging_writer.py:48] [172000] global_step=172000, grad_norm=8.211173057556152, loss=0.783660888671875
I0129 00:10:11.454343 139656297445120 logging_writer.py:48] [172100] global_step=172100, grad_norm=8.799622535705566, loss=0.8623232841491699
I0129 00:10:45.468324 139656649758464 logging_writer.py:48] [172200] global_step=172200, grad_norm=8.884263038635254, loss=0.7750546336174011
I0129 00:10:48.333302 139822745589568 spec.py:321] Evaluating on the training split.
I0129 00:10:54.465891 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 00:11:03.295696 139822745589568 spec.py:349] Evaluating on the test split.
I0129 00:11:05.830953 139822745589568 submission_runner.py:408] Time since start: 60725.55s, 	Step: 172210, 	{'train/accuracy': 0.9206393361091614, 'train/loss': 0.2826157808303833, 'validation/accuracy': 0.7601400017738342, 'validation/loss': 0.9788408875465393, 'validation/num_examples': 50000, 'test/accuracy': 0.6363000273704529, 'test/loss': 1.666724443435669, 'test/num_examples': 10000, 'score': 58689.38737034798, 'total_duration': 60725.549094200134, 'accumulated_submission_time': 58689.38737034798, 'accumulated_eval_time': 2024.2342946529388, 'accumulated_logging_time': 6.075444459915161}
I0129 00:11:05.881980 139655626356480 logging_writer.py:48] [172210] accumulated_eval_time=2024.234295, accumulated_logging_time=6.075444, accumulated_submission_time=58689.387370, global_step=172210, preemption_count=0, score=58689.387370, test/accuracy=0.636300, test/loss=1.666724, test/num_examples=10000, total_duration=60725.549094, train/accuracy=0.920639, train/loss=0.282616, validation/accuracy=0.760140, validation/loss=0.978841, validation/num_examples=50000
I0129 00:11:36.826442 139656297445120 logging_writer.py:48] [172300] global_step=172300, grad_norm=8.997915267944336, loss=0.8484892845153809
I0129 00:12:10.926254 139655626356480 logging_writer.py:48] [172400] global_step=172400, grad_norm=8.51843547821045, loss=0.8156049847602844
I0129 00:12:44.954922 139656297445120 logging_writer.py:48] [172500] global_step=172500, grad_norm=9.509407043457031, loss=0.8174463510513306
I0129 00:13:19.025187 139655626356480 logging_writer.py:48] [172600] global_step=172600, grad_norm=8.378229141235352, loss=0.7730198502540588
I0129 00:13:53.071809 139656297445120 logging_writer.py:48] [172700] global_step=172700, grad_norm=8.872827529907227, loss=0.8097656965255737
I0129 00:14:27.112437 139655626356480 logging_writer.py:48] [172800] global_step=172800, grad_norm=8.779932975769043, loss=0.8224195241928101
I0129 00:15:01.151196 139656297445120 logging_writer.py:48] [172900] global_step=172900, grad_norm=9.099239349365234, loss=0.784546971321106
I0129 00:15:35.197321 139655626356480 logging_writer.py:48] [173000] global_step=173000, grad_norm=8.000041961669922, loss=0.7258105874061584
I0129 00:16:09.250987 139656297445120 logging_writer.py:48] [173100] global_step=173100, grad_norm=8.451568603515625, loss=0.7658132910728455
I0129 00:16:43.285940 139655626356480 logging_writer.py:48] [173200] global_step=173200, grad_norm=8.887639045715332, loss=0.753757894039154
I0129 00:17:17.363678 139656297445120 logging_writer.py:48] [173300] global_step=173300, grad_norm=8.511220932006836, loss=0.7670003175735474
I0129 00:17:51.421906 139655626356480 logging_writer.py:48] [173400] global_step=173400, grad_norm=7.831377029418945, loss=0.7237977981567383
I0129 00:18:25.552304 139656297445120 logging_writer.py:48] [173500] global_step=173500, grad_norm=8.175512313842773, loss=0.7441222071647644
I0129 00:18:59.601621 139655626356480 logging_writer.py:48] [173600] global_step=173600, grad_norm=8.852734565734863, loss=0.734645426273346
I0129 00:19:33.639545 139656297445120 logging_writer.py:48] [173700] global_step=173700, grad_norm=9.076178550720215, loss=0.8511148691177368
I0129 00:19:35.837025 139822745589568 spec.py:321] Evaluating on the training split.
I0129 00:19:41.940169 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 00:19:50.687189 139822745589568 spec.py:349] Evaluating on the test split.
I0129 00:19:53.199247 139822745589568 submission_runner.py:408] Time since start: 61252.92s, 	Step: 173708, 	{'train/accuracy': 0.9241868257522583, 'train/loss': 0.26722291111946106, 'validation/accuracy': 0.7613199949264526, 'validation/loss': 0.9772453904151917, 'validation/num_examples': 50000, 'test/accuracy': 0.6378000378608704, 'test/loss': 1.6611794233322144, 'test/num_examples': 10000, 'score': 59199.27856183052, 'total_duration': 61252.9173913002, 'accumulated_submission_time': 59199.27856183052, 'accumulated_eval_time': 2041.5964758396149, 'accumulated_logging_time': 6.139222145080566}
I0129 00:19:53.253668 139655617963776 logging_writer.py:48] [173708] accumulated_eval_time=2041.596476, accumulated_logging_time=6.139222, accumulated_submission_time=59199.278562, global_step=173708, preemption_count=0, score=59199.278562, test/accuracy=0.637800, test/loss=1.661179, test/num_examples=10000, total_duration=61252.917391, train/accuracy=0.924187, train/loss=0.267223, validation/accuracy=0.761320, validation/loss=0.977245, validation/num_examples=50000
I0129 00:20:24.892040 139655626356480 logging_writer.py:48] [173800] global_step=173800, grad_norm=8.884247779846191, loss=0.7578234672546387
I0129 00:20:58.897354 139655617963776 logging_writer.py:48] [173900] global_step=173900, grad_norm=8.080845832824707, loss=0.7385774850845337
I0129 00:21:32.930428 139655626356480 logging_writer.py:48] [174000] global_step=174000, grad_norm=7.885458469390869, loss=0.6840449571609497
I0129 00:22:06.982837 139655617963776 logging_writer.py:48] [174100] global_step=174100, grad_norm=8.700096130371094, loss=0.8160299062728882
I0129 00:22:41.024925 139655626356480 logging_writer.py:48] [174200] global_step=174200, grad_norm=9.968133926391602, loss=0.8534272313117981
I0129 00:23:15.065339 139655617963776 logging_writer.py:48] [174300] global_step=174300, grad_norm=9.131721496582031, loss=0.8049209713935852
I0129 00:23:49.126997 139655626356480 logging_writer.py:48] [174400] global_step=174400, grad_norm=8.974736213684082, loss=0.739486575126648
I0129 00:24:23.372924 139655617963776 logging_writer.py:48] [174500] global_step=174500, grad_norm=9.113736152648926, loss=0.8115224838256836
I0129 00:24:57.441923 139655626356480 logging_writer.py:48] [174600] global_step=174600, grad_norm=8.422086715698242, loss=0.7337873578071594
I0129 00:25:31.504633 139655617963776 logging_writer.py:48] [174700] global_step=174700, grad_norm=8.55196762084961, loss=0.7684617042541504
I0129 00:26:05.591216 139655626356480 logging_writer.py:48] [174800] global_step=174800, grad_norm=10.026044845581055, loss=0.7796980142593384
I0129 00:26:39.666802 139655617963776 logging_writer.py:48] [174900] global_step=174900, grad_norm=9.100625038146973, loss=0.7434541583061218
I0129 00:27:13.724238 139655626356480 logging_writer.py:48] [175000] global_step=175000, grad_norm=8.665565490722656, loss=0.7679186463356018
I0129 00:27:47.764893 139655617963776 logging_writer.py:48] [175100] global_step=175100, grad_norm=8.865154266357422, loss=0.8112906217575073
I0129 00:28:21.797852 139655626356480 logging_writer.py:48] [175200] global_step=175200, grad_norm=9.247648239135742, loss=0.8642931580543518
I0129 00:28:23.299116 139822745589568 spec.py:321] Evaluating on the training split.
I0129 00:28:29.413708 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 00:28:38.002831 139822745589568 spec.py:349] Evaluating on the test split.
I0129 00:28:40.527006 139822745589568 submission_runner.py:408] Time since start: 61780.25s, 	Step: 175206, 	{'train/accuracy': 0.9243263602256775, 'train/loss': 0.2708885669708252, 'validation/accuracy': 0.7618199586868286, 'validation/loss': 0.9700082540512085, 'validation/num_examples': 50000, 'test/accuracy': 0.6360000371932983, 'test/loss': 1.6560876369476318, 'test/num_examples': 10000, 'score': 59709.26147675514, 'total_duration': 61780.24515080452, 'accumulated_submission_time': 59709.26147675514, 'accumulated_eval_time': 2058.8243272304535, 'accumulated_logging_time': 6.205310344696045}
I0129 00:28:40.574393 139655617963776 logging_writer.py:48] [175206] accumulated_eval_time=2058.824327, accumulated_logging_time=6.205310, accumulated_submission_time=59709.261477, global_step=175206, preemption_count=0, score=59709.261477, test/accuracy=0.636000, test/loss=1.656088, test/num_examples=10000, total_duration=61780.245151, train/accuracy=0.924326, train/loss=0.270889, validation/accuracy=0.761820, validation/loss=0.970008, validation/num_examples=50000
I0129 00:29:12.892631 139656666543872 logging_writer.py:48] [175300] global_step=175300, grad_norm=9.326573371887207, loss=0.725461483001709
I0129 00:29:46.871635 139655617963776 logging_writer.py:48] [175400] global_step=175400, grad_norm=9.265033721923828, loss=0.7822943925857544
I0129 00:30:20.922606 139656666543872 logging_writer.py:48] [175500] global_step=175500, grad_norm=8.783951759338379, loss=0.7722601294517517
I0129 00:30:55.100112 139655617963776 logging_writer.py:48] [175600] global_step=175600, grad_norm=8.908084869384766, loss=0.7595693469047546
I0129 00:31:29.133079 139656666543872 logging_writer.py:48] [175700] global_step=175700, grad_norm=8.985969543457031, loss=0.8068276643753052
I0129 00:32:03.199558 139655617963776 logging_writer.py:48] [175800] global_step=175800, grad_norm=8.497406005859375, loss=0.7960275411605835
I0129 00:32:37.276452 139656666543872 logging_writer.py:48] [175900] global_step=175900, grad_norm=8.409782409667969, loss=0.7545074820518494
I0129 00:33:11.328104 139655617963776 logging_writer.py:48] [176000] global_step=176000, grad_norm=9.15645694732666, loss=0.8659552335739136
I0129 00:33:45.399643 139656666543872 logging_writer.py:48] [176100] global_step=176100, grad_norm=9.422137260437012, loss=0.7177127599716187
I0129 00:34:19.468842 139655617963776 logging_writer.py:48] [176200] global_step=176200, grad_norm=8.604384422302246, loss=0.6866855621337891
I0129 00:34:53.545387 139656666543872 logging_writer.py:48] [176300] global_step=176300, grad_norm=9.297608375549316, loss=0.8415467143058777
I0129 00:35:27.602128 139655617963776 logging_writer.py:48] [176400] global_step=176400, grad_norm=9.739091873168945, loss=0.7747294902801514
I0129 00:36:01.689069 139656666543872 logging_writer.py:48] [176500] global_step=176500, grad_norm=8.71811294555664, loss=0.7568672299385071
I0129 00:36:35.733922 139655617963776 logging_writer.py:48] [176600] global_step=176600, grad_norm=9.524384498596191, loss=0.7351323366165161
I0129 00:37:09.766392 139656666543872 logging_writer.py:48] [176700] global_step=176700, grad_norm=9.269796371459961, loss=0.6992510557174683
I0129 00:37:10.590567 139822745589568 spec.py:321] Evaluating on the training split.
I0129 00:37:16.996815 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 00:37:25.748026 139822745589568 spec.py:349] Evaluating on the test split.
I0129 00:37:28.245671 139822745589568 submission_runner.py:408] Time since start: 62307.96s, 	Step: 176704, 	{'train/accuracy': 0.9294283986091614, 'train/loss': 0.25611138343811035, 'validation/accuracy': 0.7628200054168701, 'validation/loss': 0.9697470664978027, 'validation/num_examples': 50000, 'test/accuracy': 0.6390000581741333, 'test/loss': 1.6528555154800415, 'test/num_examples': 10000, 'score': 60219.217061281204, 'total_duration': 62307.96381902695, 'accumulated_submission_time': 60219.217061281204, 'accumulated_eval_time': 2076.4793939590454, 'accumulated_logging_time': 6.263585090637207}
I0129 00:37:28.296929 139655617963776 logging_writer.py:48] [176704] accumulated_eval_time=2076.479394, accumulated_logging_time=6.263585, accumulated_submission_time=60219.217061, global_step=176704, preemption_count=0, score=60219.217061, test/accuracy=0.639000, test/loss=1.652856, test/num_examples=10000, total_duration=62307.963819, train/accuracy=0.929428, train/loss=0.256111, validation/accuracy=0.762820, validation/loss=0.969747, validation/num_examples=50000
I0129 00:38:01.284845 139656297445120 logging_writer.py:48] [176800] global_step=176800, grad_norm=9.040303230285645, loss=0.7948978543281555
I0129 00:38:35.278940 139655617963776 logging_writer.py:48] [176900] global_step=176900, grad_norm=8.914119720458984, loss=0.6756231784820557
I0129 00:39:09.322808 139656297445120 logging_writer.py:48] [177000] global_step=177000, grad_norm=8.690328598022461, loss=0.7265104055404663
I0129 00:39:43.376020 139655617963776 logging_writer.py:48] [177100] global_step=177100, grad_norm=8.77078914642334, loss=0.7468689680099487
I0129 00:40:17.441370 139656297445120 logging_writer.py:48] [177200] global_step=177200, grad_norm=9.442708969116211, loss=0.808870792388916
I0129 00:40:51.483110 139655617963776 logging_writer.py:48] [177300] global_step=177300, grad_norm=9.29609489440918, loss=0.7766305804252625
I0129 00:41:25.563963 139656297445120 logging_writer.py:48] [177400] global_step=177400, grad_norm=8.532333374023438, loss=0.6800838708877563
I0129 00:41:59.626676 139655617963776 logging_writer.py:48] [177500] global_step=177500, grad_norm=9.083874702453613, loss=0.7514718770980835
I0129 00:42:33.684372 139656297445120 logging_writer.py:48] [177600] global_step=177600, grad_norm=9.126970291137695, loss=0.7983721494674683
I0129 00:43:07.736031 139655617963776 logging_writer.py:48] [177700] global_step=177700, grad_norm=8.301839828491211, loss=0.6982364654541016
I0129 00:43:41.807510 139656297445120 logging_writer.py:48] [177800] global_step=177800, grad_norm=8.484597206115723, loss=0.7655021548271179
I0129 00:44:15.909992 139655617963776 logging_writer.py:48] [177900] global_step=177900, grad_norm=8.654340744018555, loss=0.6824562549591064
I0129 00:44:49.942017 139656297445120 logging_writer.py:48] [178000] global_step=178000, grad_norm=8.906122207641602, loss=0.7388402819633484
I0129 00:45:23.968847 139655617963776 logging_writer.py:48] [178100] global_step=178100, grad_norm=8.048670768737793, loss=0.6692380309104919
I0129 00:45:58.018229 139656297445120 logging_writer.py:48] [178200] global_step=178200, grad_norm=7.605349540710449, loss=0.6643733978271484
I0129 00:45:58.501808 139822745589568 spec.py:321] Evaluating on the training split.
I0129 00:46:04.710445 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 00:46:13.299024 139822745589568 spec.py:349] Evaluating on the test split.
I0129 00:46:15.870568 139822745589568 submission_runner.py:408] Time since start: 62835.59s, 	Step: 178203, 	{'train/accuracy': 0.9300462007522583, 'train/loss': 0.25256940722465515, 'validation/accuracy': 0.763759970664978, 'validation/loss': 0.9669691920280457, 'validation/num_examples': 50000, 'test/accuracy': 0.6402000188827515, 'test/loss': 1.652724266052246, 'test/num_examples': 10000, 'score': 60729.36058998108, 'total_duration': 62835.58871150017, 'accumulated_submission_time': 60729.36058998108, 'accumulated_eval_time': 2093.848112821579, 'accumulated_logging_time': 6.324261903762817}
I0129 00:46:15.922260 139656658151168 logging_writer.py:48] [178203] accumulated_eval_time=2093.848113, accumulated_logging_time=6.324262, accumulated_submission_time=60729.360590, global_step=178203, preemption_count=0, score=60729.360590, test/accuracy=0.640200, test/loss=1.652724, test/num_examples=10000, total_duration=62835.588712, train/accuracy=0.930046, train/loss=0.252569, validation/accuracy=0.763760, validation/loss=0.966969, validation/num_examples=50000
I0129 00:46:49.230713 139656666543872 logging_writer.py:48] [178300] global_step=178300, grad_norm=8.624274253845215, loss=0.8009259700775146
I0129 00:47:23.245595 139656658151168 logging_writer.py:48] [178400] global_step=178400, grad_norm=8.858892440795898, loss=0.7272199392318726
I0129 00:47:57.267712 139656666543872 logging_writer.py:48] [178500] global_step=178500, grad_norm=8.870950698852539, loss=0.740876317024231
I0129 00:48:31.316846 139656658151168 logging_writer.py:48] [178600] global_step=178600, grad_norm=9.96989631652832, loss=0.7660236954689026
I0129 00:49:05.359498 139656666543872 logging_writer.py:48] [178700] global_step=178700, grad_norm=8.904217720031738, loss=0.7119339108467102
I0129 00:49:39.392012 139656658151168 logging_writer.py:48] [178800] global_step=178800, grad_norm=9.588485717773438, loss=0.7699235081672668
I0129 00:50:13.490808 139656666543872 logging_writer.py:48] [178900] global_step=178900, grad_norm=8.848175048828125, loss=0.7197182774543762
I0129 00:50:47.533256 139656658151168 logging_writer.py:48] [179000] global_step=179000, grad_norm=8.069539070129395, loss=0.7018133401870728
I0129 00:51:21.581405 139656666543872 logging_writer.py:48] [179100] global_step=179100, grad_norm=8.84691047668457, loss=0.6466771364212036
I0129 00:51:55.650226 139656658151168 logging_writer.py:48] [179200] global_step=179200, grad_norm=8.78032398223877, loss=0.7835075259208679
I0129 00:52:29.729593 139656666543872 logging_writer.py:48] [179300] global_step=179300, grad_norm=8.912033081054688, loss=0.6753369569778442
I0129 00:53:03.804488 139656658151168 logging_writer.py:48] [179400] global_step=179400, grad_norm=8.833649635314941, loss=0.7806620597839355
I0129 00:53:37.890052 139656666543872 logging_writer.py:48] [179500] global_step=179500, grad_norm=9.545635223388672, loss=0.7617844343185425
I0129 00:54:11.953893 139656658151168 logging_writer.py:48] [179600] global_step=179600, grad_norm=8.945109367370605, loss=0.7367603778839111
I0129 00:54:45.969964 139656666543872 logging_writer.py:48] [179700] global_step=179700, grad_norm=8.618101119995117, loss=0.7154744863510132
I0129 00:54:45.977361 139822745589568 spec.py:321] Evaluating on the training split.
I0129 00:54:52.131014 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 00:55:00.684964 139822745589568 spec.py:349] Evaluating on the test split.
I0129 00:55:03.209200 139822745589568 submission_runner.py:408] Time since start: 63362.93s, 	Step: 179701, 	{'train/accuracy': 0.9334741234779358, 'train/loss': 0.24098944664001465, 'validation/accuracy': 0.7638599872589111, 'validation/loss': 0.9650241136550903, 'validation/num_examples': 50000, 'test/accuracy': 0.64410001039505, 'test/loss': 1.6483752727508545, 'test/num_examples': 10000, 'score': 61239.35639166832, 'total_duration': 63362.927340745926, 'accumulated_submission_time': 61239.35639166832, 'accumulated_eval_time': 2111.0798873901367, 'accumulated_logging_time': 6.385619401931763}
I0129 00:55:03.257982 139656297445120 logging_writer.py:48] [179701] accumulated_eval_time=2111.079887, accumulated_logging_time=6.385619, accumulated_submission_time=61239.356392, global_step=179701, preemption_count=0, score=61239.356392, test/accuracy=0.644100, test/loss=1.648375, test/num_examples=10000, total_duration=63362.927341, train/accuracy=0.933474, train/loss=0.240989, validation/accuracy=0.763860, validation/loss=0.965024, validation/num_examples=50000
I0129 00:55:37.247965 139656649758464 logging_writer.py:48] [179800] global_step=179800, grad_norm=8.908748626708984, loss=0.7395215034484863
I0129 00:56:11.276207 139656297445120 logging_writer.py:48] [179900] global_step=179900, grad_norm=9.007567405700684, loss=0.7389593124389648
I0129 00:56:45.394400 139656649758464 logging_writer.py:48] [180000] global_step=180000, grad_norm=9.785457611083984, loss=0.813349723815918
I0129 00:57:19.474482 139656297445120 logging_writer.py:48] [180100] global_step=180100, grad_norm=8.914037704467773, loss=0.7437841296195984
I0129 00:57:53.527482 139656649758464 logging_writer.py:48] [180200] global_step=180200, grad_norm=9.754379272460938, loss=0.6942251324653625
I0129 00:58:27.563374 139656297445120 logging_writer.py:48] [180300] global_step=180300, grad_norm=9.326227188110352, loss=0.7227213978767395
I0129 00:59:01.594160 139656649758464 logging_writer.py:48] [180400] global_step=180400, grad_norm=9.219756126403809, loss=0.7128734588623047
I0129 00:59:35.613605 139656297445120 logging_writer.py:48] [180500] global_step=180500, grad_norm=9.513994216918945, loss=0.800614595413208
I0129 01:00:09.681349 139656649758464 logging_writer.py:48] [180600] global_step=180600, grad_norm=9.754803657531738, loss=0.7318226099014282
I0129 01:00:43.738397 139656297445120 logging_writer.py:48] [180700] global_step=180700, grad_norm=9.387979507446289, loss=0.6882292032241821
I0129 01:01:17.800352 139656649758464 logging_writer.py:48] [180800] global_step=180800, grad_norm=8.355453491210938, loss=0.7351488471031189
I0129 01:01:51.856694 139656297445120 logging_writer.py:48] [180900] global_step=180900, grad_norm=9.506263732910156, loss=0.6995193362236023
I0129 01:02:25.913718 139656649758464 logging_writer.py:48] [181000] global_step=181000, grad_norm=9.697563171386719, loss=0.7797566056251526
I0129 01:03:00.050904 139656297445120 logging_writer.py:48] [181100] global_step=181100, grad_norm=8.827834129333496, loss=0.725455641746521
I0129 01:03:33.218623 139822745589568 spec.py:321] Evaluating on the training split.
I0129 01:03:39.383578 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 01:03:47.898835 139822745589568 spec.py:349] Evaluating on the test split.
I0129 01:03:50.456453 139822745589568 submission_runner.py:408] Time since start: 63890.17s, 	Step: 181199, 	{'train/accuracy': 0.9332548975944519, 'train/loss': 0.24204431474208832, 'validation/accuracy': 0.7646999955177307, 'validation/loss': 0.9629248380661011, 'validation/num_examples': 50000, 'test/accuracy': 0.6421000361442566, 'test/loss': 1.6437848806381226, 'test/num_examples': 10000, 'score': 61749.25453186035, 'total_duration': 63890.17459869385, 'accumulated_submission_time': 61749.25453186035, 'accumulated_eval_time': 2128.31769990921, 'accumulated_logging_time': 6.443438529968262}
I0129 01:03:50.504701 139656658151168 logging_writer.py:48] [181199] accumulated_eval_time=2128.317700, accumulated_logging_time=6.443439, accumulated_submission_time=61749.254532, global_step=181199, preemption_count=0, score=61749.254532, test/accuracy=0.642100, test/loss=1.643785, test/num_examples=10000, total_duration=63890.174599, train/accuracy=0.933255, train/loss=0.242044, validation/accuracy=0.764700, validation/loss=0.962925, validation/num_examples=50000
I0129 01:03:51.188048 139656666543872 logging_writer.py:48] [181200] global_step=181200, grad_norm=9.020593643188477, loss=0.7246903777122498
I0129 01:04:25.162265 139656658151168 logging_writer.py:48] [181300] global_step=181300, grad_norm=8.961963653564453, loss=0.7284920811653137
I0129 01:04:59.157793 139656666543872 logging_writer.py:48] [181400] global_step=181400, grad_norm=9.929523468017578, loss=0.7665588855743408
I0129 01:05:33.222130 139656658151168 logging_writer.py:48] [181500] global_step=181500, grad_norm=8.983811378479004, loss=0.7622708082199097
I0129 01:06:07.280386 139656666543872 logging_writer.py:48] [181600] global_step=181600, grad_norm=8.638952255249023, loss=0.7026052474975586
I0129 01:06:41.345987 139656658151168 logging_writer.py:48] [181700] global_step=181700, grad_norm=8.612564086914062, loss=0.6584433317184448
I0129 01:07:15.411714 139656666543872 logging_writer.py:48] [181800] global_step=181800, grad_norm=9.137724876403809, loss=0.7704523801803589
I0129 01:07:49.444069 139656658151168 logging_writer.py:48] [181900] global_step=181900, grad_norm=9.369949340820312, loss=0.7651013135910034
I0129 01:08:23.529222 139656666543872 logging_writer.py:48] [182000] global_step=182000, grad_norm=9.271891593933105, loss=0.7935430407524109
I0129 01:08:57.585500 139656658151168 logging_writer.py:48] [182100] global_step=182100, grad_norm=9.77294635772705, loss=0.7625654339790344
I0129 01:09:31.699111 139656666543872 logging_writer.py:48] [182200] global_step=182200, grad_norm=7.819105625152588, loss=0.6903767585754395
I0129 01:10:05.732421 139656658151168 logging_writer.py:48] [182300] global_step=182300, grad_norm=9.550581932067871, loss=0.7851045727729797
I0129 01:10:39.793102 139656666543872 logging_writer.py:48] [182400] global_step=182400, grad_norm=8.941452980041504, loss=0.7175107002258301
I0129 01:11:13.817367 139656658151168 logging_writer.py:48] [182500] global_step=182500, grad_norm=8.975196838378906, loss=0.6952476501464844
I0129 01:11:47.858731 139656666543872 logging_writer.py:48] [182600] global_step=182600, grad_norm=8.685888290405273, loss=0.6989941596984863
I0129 01:12:20.691001 139822745589568 spec.py:321] Evaluating on the training split.
I0129 01:12:26.793354 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 01:12:35.399612 139822745589568 spec.py:349] Evaluating on the test split.
I0129 01:12:37.907291 139822745589568 submission_runner.py:408] Time since start: 64417.63s, 	Step: 182698, 	{'train/accuracy': 0.9331353306770325, 'train/loss': 0.24270980060100555, 'validation/accuracy': 0.7647599577903748, 'validation/loss': 0.9616653919219971, 'validation/num_examples': 50000, 'test/accuracy': 0.6443000435829163, 'test/loss': 1.6459656953811646, 'test/num_examples': 10000, 'score': 62259.378823280334, 'total_duration': 64417.625435590744, 'accumulated_submission_time': 62259.378823280334, 'accumulated_eval_time': 2145.5339555740356, 'accumulated_logging_time': 6.500751495361328}
I0129 01:12:37.957622 139656297445120 logging_writer.py:48] [182698] accumulated_eval_time=2145.533956, accumulated_logging_time=6.500751, accumulated_submission_time=62259.378823, global_step=182698, preemption_count=0, score=62259.378823, test/accuracy=0.644300, test/loss=1.645966, test/num_examples=10000, total_duration=64417.625436, train/accuracy=0.933135, train/loss=0.242710, validation/accuracy=0.764760, validation/loss=0.961665, validation/num_examples=50000
I0129 01:12:38.982523 139656649758464 logging_writer.py:48] [182700] global_step=182700, grad_norm=8.658872604370117, loss=0.7526775598526001
I0129 01:13:12.958400 139656297445120 logging_writer.py:48] [182800] global_step=182800, grad_norm=9.628445625305176, loss=0.7532880306243896
I0129 01:13:46.960964 139656649758464 logging_writer.py:48] [182900] global_step=182900, grad_norm=8.817741394042969, loss=0.6747673153877258
I0129 01:14:21.014044 139656297445120 logging_writer.py:48] [183000] global_step=183000, grad_norm=8.836739540100098, loss=0.7619524598121643
I0129 01:14:55.082749 139656649758464 logging_writer.py:48] [183100] global_step=183100, grad_norm=8.426810264587402, loss=0.6717283129692078
I0129 01:15:29.121797 139656297445120 logging_writer.py:48] [183200] global_step=183200, grad_norm=8.78881549835205, loss=0.7220052480697632
I0129 01:16:03.244007 139656649758464 logging_writer.py:48] [183300] global_step=183300, grad_norm=8.75735855102539, loss=0.7820150852203369
I0129 01:16:37.275734 139656297445120 logging_writer.py:48] [183400] global_step=183400, grad_norm=8.99125862121582, loss=0.6838709115982056
I0129 01:17:11.310791 139656649758464 logging_writer.py:48] [183500] global_step=183500, grad_norm=8.70000171661377, loss=0.6989206075668335
I0129 01:17:45.389707 139656297445120 logging_writer.py:48] [183600] global_step=183600, grad_norm=8.711701393127441, loss=0.7073116302490234
I0129 01:18:19.433363 139656649758464 logging_writer.py:48] [183700] global_step=183700, grad_norm=9.172904014587402, loss=0.7119513750076294
I0129 01:18:53.497223 139656297445120 logging_writer.py:48] [183800] global_step=183800, grad_norm=8.822271347045898, loss=0.8126279711723328
I0129 01:19:27.575586 139656649758464 logging_writer.py:48] [183900] global_step=183900, grad_norm=8.813384056091309, loss=0.7580904960632324
I0129 01:20:01.637138 139656297445120 logging_writer.py:48] [184000] global_step=184000, grad_norm=8.42401123046875, loss=0.7045736908912659
I0129 01:20:35.689671 139656649758464 logging_writer.py:48] [184100] global_step=184100, grad_norm=9.683323860168457, loss=0.6798862218856812
I0129 01:21:08.186130 139822745589568 spec.py:321] Evaluating on the training split.
I0129 01:21:14.304419 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 01:21:22.821126 139822745589568 spec.py:349] Evaluating on the test split.
I0129 01:21:25.425052 139822745589568 submission_runner.py:408] Time since start: 64945.14s, 	Step: 184197, 	{'train/accuracy': 0.9338129758834839, 'train/loss': 0.24053217470645905, 'validation/accuracy': 0.7651599645614624, 'validation/loss': 0.9626170992851257, 'validation/num_examples': 50000, 'test/accuracy': 0.6438000202178955, 'test/loss': 1.6469836235046387, 'test/num_examples': 10000, 'score': 62769.5444791317, 'total_duration': 64945.14319252968, 'accumulated_submission_time': 62769.5444791317, 'accumulated_eval_time': 2162.7728395462036, 'accumulated_logging_time': 6.56220269203186}
I0129 01:21:25.474539 139655626356480 logging_writer.py:48] [184197] accumulated_eval_time=2162.772840, accumulated_logging_time=6.562203, accumulated_submission_time=62769.544479, global_step=184197, preemption_count=0, score=62769.544479, test/accuracy=0.643800, test/loss=1.646984, test/num_examples=10000, total_duration=64945.143193, train/accuracy=0.933813, train/loss=0.240532, validation/accuracy=0.765160, validation/loss=0.962617, validation/num_examples=50000
I0129 01:21:26.851150 139656297445120 logging_writer.py:48] [184200] global_step=184200, grad_norm=9.425275802612305, loss=0.7624165415763855
I0129 01:22:00.841707 139655626356480 logging_writer.py:48] [184300] global_step=184300, grad_norm=8.310904502868652, loss=0.6919412016868591
I0129 01:22:34.916664 139656297445120 logging_writer.py:48] [184400] global_step=184400, grad_norm=8.572884559631348, loss=0.7803954482078552
I0129 01:23:08.953992 139655626356480 logging_writer.py:48] [184500] global_step=184500, grad_norm=10.229056358337402, loss=0.7844905853271484
I0129 01:23:43.020656 139656297445120 logging_writer.py:48] [184600] global_step=184600, grad_norm=10.210765838623047, loss=0.7749599814414978
I0129 01:24:17.085783 139655626356480 logging_writer.py:48] [184700] global_step=184700, grad_norm=9.293251991271973, loss=0.7698839902877808
I0129 01:24:51.155484 139656297445120 logging_writer.py:48] [184800] global_step=184800, grad_norm=8.69773006439209, loss=0.7325857877731323
I0129 01:25:24.037607 139655626356480 logging_writer.py:48] [184898] global_step=184898, preemption_count=0, score=63008.043020
I0129 01:25:24.475775 139822745589568 checkpoints.py:490] Saving checkpoint at step: 184898
I0129 01:25:25.588018 139822745589568 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_1/imagenet_resnet_jax/trial_3/checkpoint_184898
I0129 01:25:25.613705 139822745589568 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_1/imagenet_resnet_jax/trial_3/checkpoint_184898.
I0129 01:25:26.377053 139822745589568 submission_runner.py:583] Tuning trial 3/5
I0129 01:25:26.377261 139822745589568 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0129 01:25:26.385345 139822745589568 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0011758609907701612, 'train/loss': 6.911500930786133, 'validation/accuracy': 0.001019999966956675, 'validation/loss': 6.91091251373291, 'validation/num_examples': 50000, 'test/accuracy': 0.0012000000569969416, 'test/loss': 6.910790920257568, 'test/num_examples': 10000, 'score': 31.49718737602234, 'total_duration': 48.92790198326111, 'accumulated_submission_time': 31.49718737602234, 'accumulated_eval_time': 17.43062710762024, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1492, {'train/accuracy': 0.06754224747419357, 'train/loss': 5.367956638336182, 'validation/accuracy': 0.06469999998807907, 'validation/loss': 5.413669109344482, 'validation/num_examples': 50000, 'test/accuracy': 0.044600002467632294, 'test/loss': 5.659720420837402, 'test/num_examples': 10000, 'score': 541.692113161087, 'total_duration': 576.4821372032166, 'accumulated_submission_time': 541.692113161087, 'accumulated_eval_time': 34.72050094604492, 'accumulated_logging_time': 0.018468618392944336, 'global_step': 1492, 'preemption_count': 0}), (2982, {'train/accuracy': 0.16820789873600006, 'train/loss': 4.278195858001709, 'validation/accuracy': 0.1507599949836731, 'validation/loss': 4.392276287078857, 'validation/num_examples': 50000, 'test/accuracy': 0.10750000178813934, 'test/loss': 4.850800514221191, 'test/num_examples': 10000, 'score': 1051.8486168384552, 'total_duration': 1104.1325709819794, 'accumulated_submission_time': 1051.8486168384552, 'accumulated_eval_time': 52.13346600532532, 'accumulated_logging_time': 0.04932880401611328, 'global_step': 2982, 'preemption_count': 0}), (4471, {'train/accuracy': 0.26907286047935486, 'train/loss': 3.525336265563965, 'validation/accuracy': 0.24647998809814453, 'validation/loss': 3.675424337387085, 'validation/num_examples': 50000, 'test/accuracy': 0.18060000240802765, 'test/loss': 4.246772289276123, 'test/num_examples': 10000, 'score': 1561.8328936100006, 'total_duration': 1631.8379180431366, 'accumulated_submission_time': 1561.8328936100006, 'accumulated_eval_time': 69.77436900138855, 'accumulated_logging_time': 0.07820892333984375, 'global_step': 4471, 'preemption_count': 0}), (5961, {'train/accuracy': 0.35855787992477417, 'train/loss': 2.934255361557007, 'validation/accuracy': 0.33243998885154724, 'validation/loss': 3.0907390117645264, 'validation/num_examples': 50000, 'test/accuracy': 0.2507000267505646, 'test/loss': 3.733696699142456, 'test/num_examples': 10000, 'score': 2072.0533249378204, 'total_duration': 2159.659814119339, 'accumulated_submission_time': 2072.0533249378204, 'accumulated_eval_time': 87.29565978050232, 'accumulated_logging_time': 0.10798144340515137, 'global_step': 5961, 'preemption_count': 0}), (7451, {'train/accuracy': 0.4449138939380646, 'train/loss': 2.4179675579071045, 'validation/accuracy': 0.38711997866630554, 'validation/loss': 2.7826380729675293, 'validation/num_examples': 50000, 'test/accuracy': 0.29590001702308655, 'test/loss': 3.4668314456939697, 'test/num_examples': 10000, 'score': 2582.084417819977, 'total_duration': 2687.353482246399, 'accumulated_submission_time': 2582.084417819977, 'accumulated_eval_time': 104.87834978103638, 'accumulated_logging_time': 0.136627197265625, 'global_step': 7451, 'preemption_count': 0}), (8941, {'train/accuracy': 0.5053611397743225, 'train/loss': 2.105006217956543, 'validation/accuracy': 0.4567599892616272, 'validation/loss': 2.381918430328369, 'validation/num_examples': 50000, 'test/accuracy': 0.3477000296115875, 'test/loss': 3.1159727573394775, 'test/num_examples': 10000, 'score': 3092.1619415283203, 'total_duration': 3215.6690373420715, 'accumulated_submission_time': 3092.1619415283203, 'accumulated_eval_time': 123.03875970840454, 'accumulated_logging_time': 0.16377711296081543, 'global_step': 8941, 'preemption_count': 0}), (10433, {'train/accuracy': 0.5231783986091614, 'train/loss': 2.0281193256378174, 'validation/accuracy': 0.48311999440193176, 'validation/loss': 2.2610156536102295, 'validation/num_examples': 50000, 'test/accuracy': 0.37310001254081726, 'test/loss': 2.9467318058013916, 'test/num_examples': 10000, 'score': 3602.4153928756714, 'total_duration': 3743.4753136634827, 'accumulated_submission_time': 3602.4153928756714, 'accumulated_eval_time': 140.51118230819702, 'accumulated_logging_time': 0.19390320777893066, 'global_step': 10433, 'preemption_count': 0}), (11925, {'train/accuracy': 0.5595105290412903, 'train/loss': 1.8536113500595093, 'validation/accuracy': 0.5171599984169006, 'validation/loss': 2.09015154838562, 'validation/num_examples': 50000, 'test/accuracy': 0.4001000225543976, 'test/loss': 2.7808094024658203, 'test/num_examples': 10000, 'score': 4112.552248477936, 'total_duration': 4271.202058792114, 'accumulated_submission_time': 4112.552248477936, 'accumulated_eval_time': 158.01940941810608, 'accumulated_logging_time': 0.22330927848815918, 'global_step': 11925, 'preemption_count': 0}), (13418, {'train/accuracy': 0.5712292790412903, 'train/loss': 1.7940986156463623, 'validation/accuracy': 0.5276399850845337, 'validation/loss': 2.0156517028808594, 'validation/num_examples': 50000, 'test/accuracy': 0.4075000286102295, 'test/loss': 2.762702703475952, 'test/num_examples': 10000, 'score': 4622.760374307632, 'total_duration': 4798.860710382462, 'accumulated_submission_time': 4622.760374307632, 'accumulated_eval_time': 175.39147543907166, 'accumulated_logging_time': 0.2514383792877197, 'global_step': 13418, 'preemption_count': 0}), (14911, {'train/accuracy': 0.5833864808082581, 'train/loss': 1.730528473854065, 'validation/accuracy': 0.5450999736785889, 'validation/loss': 1.9298218488693237, 'validation/num_examples': 50000, 'test/accuracy': 0.426800012588501, 'test/loss': 2.671276807785034, 'test/num_examples': 10000, 'score': 5132.832676887512, 'total_duration': 5326.234619617462, 'accumulated_submission_time': 5132.832676887512, 'accumulated_eval_time': 192.61159753799438, 'accumulated_logging_time': 0.2807481288909912, 'global_step': 14911, 'preemption_count': 0}), (16405, {'train/accuracy': 0.5859175324440002, 'train/loss': 1.707316517829895, 'validation/accuracy': 0.549780011177063, 'validation/loss': 1.9031552076339722, 'validation/num_examples': 50000, 'test/accuracy': 0.42250001430511475, 'test/loss': 2.6825528144836426, 'test/num_examples': 10000, 'score': 5643.064661502838, 'total_duration': 5853.869685411453, 'accumulated_submission_time': 5643.064661502838, 'accumulated_eval_time': 209.93391489982605, 'accumulated_logging_time': 0.3114583492279053, 'global_step': 16405, 'preemption_count': 0}), (17899, {'train/accuracy': 0.6173269748687744, 'train/loss': 1.5603954792022705, 'validation/accuracy': 0.5484799742698669, 'validation/loss': 1.9227455854415894, 'validation/num_examples': 50000, 'test/accuracy': 0.42500001192092896, 'test/loss': 2.6512632369995117, 'test/num_examples': 10000, 'score': 6153.015790462494, 'total_duration': 6381.367884874344, 'accumulated_submission_time': 6153.015790462494, 'accumulated_eval_time': 227.40043759346008, 'accumulated_logging_time': 0.3407416343688965, 'global_step': 17899, 'preemption_count': 0}), (19394, {'train/accuracy': 0.6105906963348389, 'train/loss': 1.5887254476547241, 'validation/accuracy': 0.5607399940490723, 'validation/loss': 1.864424467086792, 'validation/num_examples': 50000, 'test/accuracy': 0.44380003213882446, 'test/loss': 2.5865235328674316, 'test/num_examples': 10000, 'score': 6663.139073133469, 'total_duration': 6909.171550512314, 'accumulated_submission_time': 6663.139073133469, 'accumulated_eval_time': 244.99951553344727, 'accumulated_logging_time': 0.37102794647216797, 'global_step': 19394, 'preemption_count': 0}), (20889, {'train/accuracy': 0.6055883169174194, 'train/loss': 1.5978363752365112, 'validation/accuracy': 0.5553199648857117, 'validation/loss': 1.8586797714233398, 'validation/num_examples': 50000, 'test/accuracy': 0.42980003356933594, 'test/loss': 2.6199140548706055, 'test/num_examples': 10000, 'score': 7173.313796281815, 'total_duration': 7436.8082230091095, 'accumulated_submission_time': 7173.313796281815, 'accumulated_eval_time': 262.37942600250244, 'accumulated_logging_time': 0.40203213691711426, 'global_step': 20889, 'preemption_count': 0}), (22384, {'train/accuracy': 0.6145368218421936, 'train/loss': 1.5811792612075806, 'validation/accuracy': 0.5697999596595764, 'validation/loss': 1.8240339756011963, 'validation/num_examples': 50000, 'test/accuracy': 0.44620001316070557, 'test/loss': 2.5530011653900146, 'test/num_examples': 10000, 'score': 7683.348705768585, 'total_duration': 7964.449161529541, 'accumulated_submission_time': 7683.348705768585, 'accumulated_eval_time': 279.90408277511597, 'accumulated_logging_time': 0.4324047565460205, 'global_step': 22384, 'preemption_count': 0}), (23879, {'train/accuracy': 0.6197385191917419, 'train/loss': 1.5652210712432861, 'validation/accuracy': 0.5730199813842773, 'validation/loss': 1.7761461734771729, 'validation/num_examples': 50000, 'test/accuracy': 0.45190003514289856, 'test/loss': 2.501939296722412, 'test/num_examples': 10000, 'score': 8193.355751514435, 'total_duration': 8492.355654716492, 'accumulated_submission_time': 8193.355751514435, 'accumulated_eval_time': 297.7186679840088, 'accumulated_logging_time': 0.4656527042388916, 'global_step': 23879, 'preemption_count': 0}), (25375, {'train/accuracy': 0.6237244606018066, 'train/loss': 1.5361076593399048, 'validation/accuracy': 0.5832799673080444, 'validation/loss': 1.745568037033081, 'validation/num_examples': 50000, 'test/accuracy': 0.4537000358104706, 'test/loss': 2.5031628608703613, 'test/num_examples': 10000, 'score': 8703.507536411285, 'total_duration': 9019.985451698303, 'accumulated_submission_time': 8703.507536411285, 'accumulated_eval_time': 315.1154990196228, 'accumulated_logging_time': 0.49759435653686523, 'global_step': 25375, 'preemption_count': 0}), (26871, {'train/accuracy': 0.6704002022743225, 'train/loss': 1.311537265777588, 'validation/accuracy': 0.5862399935722351, 'validation/loss': 1.7210910320281982, 'validation/num_examples': 50000, 'test/accuracy': 0.4626000225543976, 'test/loss': 2.476860761642456, 'test/num_examples': 10000, 'score': 9213.550162315369, 'total_duration': 9547.775417804718, 'accumulated_submission_time': 9213.550162315369, 'accumulated_eval_time': 332.7810888290405, 'accumulated_logging_time': 0.5300009250640869, 'global_step': 26871, 'preemption_count': 0}), (28366, {'train/accuracy': 0.6376155614852905, 'train/loss': 1.4694420099258423, 'validation/accuracy': 0.57669997215271, 'validation/loss': 1.7748409509658813, 'validation/num_examples': 50000, 'test/accuracy': 0.453900009393692, 'test/loss': 2.4946587085723877, 'test/num_examples': 10000, 'score': 9723.562857627869, 'total_duration': 10075.523859977722, 'accumulated_submission_time': 9723.562857627869, 'accumulated_eval_time': 350.4288082122803, 'accumulated_logging_time': 0.5664412975311279, 'global_step': 28366, 'preemption_count': 0}), (29863, {'train/accuracy': 0.6395886540412903, 'train/loss': 1.4568294286727905, 'validation/accuracy': 0.586080014705658, 'validation/loss': 1.7302289009094238, 'validation/num_examples': 50000, 'test/accuracy': 0.4571000337600708, 'test/loss': 2.4968318939208984, 'test/num_examples': 10000, 'score': 10233.790237903595, 'total_duration': 10603.185664176941, 'accumulated_submission_time': 10233.790237903595, 'accumulated_eval_time': 367.77251267433167, 'accumulated_logging_time': 0.6059026718139648, 'global_step': 29863, 'preemption_count': 0}), (31359, {'train/accuracy': 0.638671875, 'train/loss': 1.4534977674484253, 'validation/accuracy': 0.5884400010108948, 'validation/loss': 1.7131558656692505, 'validation/num_examples': 50000, 'test/accuracy': 0.4750000238418579, 'test/loss': 2.414470672607422, 'test/num_examples': 10000, 'score': 10743.71028470993, 'total_duration': 11130.426788568497, 'accumulated_submission_time': 10743.71028470993, 'accumulated_eval_time': 385.00909519195557, 'accumulated_logging_time': 0.6394505500793457, 'global_step': 31359, 'preemption_count': 0}), (32855, {'train/accuracy': 0.6384924650192261, 'train/loss': 1.4499725103378296, 'validation/accuracy': 0.5962799787521362, 'validation/loss': 1.6743437051773071, 'validation/num_examples': 50000, 'test/accuracy': 0.4702000319957733, 'test/loss': 2.409991502761841, 'test/num_examples': 10000, 'score': 11253.648388624191, 'total_duration': 11658.121633052826, 'accumulated_submission_time': 11253.648388624191, 'accumulated_eval_time': 402.6841251850128, 'accumulated_logging_time': 0.6713178157806396, 'global_step': 32855, 'preemption_count': 0}), (34352, {'train/accuracy': 0.6353236436843872, 'train/loss': 1.4725189208984375, 'validation/accuracy': 0.5895400047302246, 'validation/loss': 1.7027602195739746, 'validation/num_examples': 50000, 'test/accuracy': 0.4675000309944153, 'test/loss': 2.4392247200012207, 'test/num_examples': 10000, 'score': 11763.811690092087, 'total_duration': 12186.06164097786, 'accumulated_submission_time': 11763.811690092087, 'accumulated_eval_time': 420.37781167030334, 'accumulated_logging_time': 0.7036874294281006, 'global_step': 34352, 'preemption_count': 0}), (35849, {'train/accuracy': 0.652762234210968, 'train/loss': 1.4022929668426514, 'validation/accuracy': 0.5992199778556824, 'validation/loss': 1.6665433645248413, 'validation/num_examples': 50000, 'test/accuracy': 0.4771000146865845, 'test/loss': 2.394937753677368, 'test/num_examples': 10000, 'score': 12273.92913389206, 'total_duration': 12713.53648853302, 'accumulated_submission_time': 12273.92913389206, 'accumulated_eval_time': 437.6548173427582, 'accumulated_logging_time': 0.7323830127716064, 'global_step': 35849, 'preemption_count': 0}), (37346, {'train/accuracy': 0.6575254797935486, 'train/loss': 1.3663238286972046, 'validation/accuracy': 0.5941999554634094, 'validation/loss': 1.6988669633865356, 'validation/num_examples': 50000, 'test/accuracy': 0.47440001368522644, 'test/loss': 2.4055161476135254, 'test/num_examples': 10000, 'score': 12784.103419065475, 'total_duration': 13241.438656330109, 'accumulated_submission_time': 12784.103419065475, 'accumulated_eval_time': 455.29395627975464, 'accumulated_logging_time': 0.7698986530303955, 'global_step': 37346, 'preemption_count': 0}), (38841, {'train/accuracy': 0.6528818607330322, 'train/loss': 1.384704828262329, 'validation/accuracy': 0.5984399914741516, 'validation/loss': 1.6746433973312378, 'validation/num_examples': 50000, 'test/accuracy': 0.48280003666877747, 'test/loss': 2.4071831703186035, 'test/num_examples': 10000, 'score': 13294.079423427582, 'total_duration': 13768.731862545013, 'accumulated_submission_time': 13294.079423427582, 'accumulated_eval_time': 472.52238607406616, 'accumulated_logging_time': 0.8064682483673096, 'global_step': 38841, 'preemption_count': 0}), (40338, {'train/accuracy': 0.6393694281578064, 'train/loss': 1.4571224451065063, 'validation/accuracy': 0.5904799699783325, 'validation/loss': 1.712005615234375, 'validation/num_examples': 50000, 'test/accuracy': 0.4669000208377838, 'test/loss': 2.4362292289733887, 'test/num_examples': 10000, 'score': 13804.172444343567, 'total_duration': 14296.480261325836, 'accumulated_submission_time': 13804.172444343567, 'accumulated_eval_time': 490.08971118927, 'accumulated_logging_time': 0.845099687576294, 'global_step': 40338, 'preemption_count': 0}), (41835, {'train/accuracy': 0.6407644748687744, 'train/loss': 1.4308786392211914, 'validation/accuracy': 0.5977599620819092, 'validation/loss': 1.671799659729004, 'validation/num_examples': 50000, 'test/accuracy': 0.48110002279281616, 'test/loss': 2.351424217224121, 'test/num_examples': 10000, 'score': 14314.146545886993, 'total_duration': 14824.075912237167, 'accumulated_submission_time': 14314.146545886993, 'accumulated_eval_time': 507.6242277622223, 'accumulated_logging_time': 0.8813223838806152, 'global_step': 41835, 'preemption_count': 0}), (43332, {'train/accuracy': 0.6420599222183228, 'train/loss': 1.4258434772491455, 'validation/accuracy': 0.5988199710845947, 'validation/loss': 1.6653971672058105, 'validation/num_examples': 50000, 'test/accuracy': 0.47440001368522644, 'test/loss': 2.4011220932006836, 'test/num_examples': 10000, 'score': 14824.182245731354, 'total_duration': 15351.50887298584, 'accumulated_submission_time': 14824.182245731354, 'accumulated_eval_time': 524.9377455711365, 'accumulated_logging_time': 0.915226936340332, 'global_step': 43332, 'preemption_count': 0}), (44829, {'train/accuracy': 0.6495137214660645, 'train/loss': 1.4016178846359253, 'validation/accuracy': 0.6055200099945068, 'validation/loss': 1.6298383474349976, 'validation/num_examples': 50000, 'test/accuracy': 0.4788000285625458, 'test/loss': 2.33974552154541, 'test/num_examples': 10000, 'score': 15334.278590202332, 'total_duration': 15878.976461172104, 'accumulated_submission_time': 15334.278590202332, 'accumulated_eval_time': 542.2236630916595, 'accumulated_logging_time': 0.9501383304595947, 'global_step': 44829, 'preemption_count': 0}), (46327, {'train/accuracy': 0.6932198405265808, 'train/loss': 1.2089905738830566, 'validation/accuracy': 0.6118999719619751, 'validation/loss': 1.6064671277999878, 'validation/num_examples': 50000, 'test/accuracy': 0.49000000953674316, 'test/loss': 2.314314126968384, 'test/num_examples': 10000, 'score': 15844.478893518448, 'total_duration': 16406.788619995117, 'accumulated_submission_time': 15844.478893518448, 'accumulated_eval_time': 559.746725320816, 'accumulated_logging_time': 0.9882421493530273, 'global_step': 46327, 'preemption_count': 0}), (47825, {'train/accuracy': 0.6660555005073547, 'train/loss': 1.3233352899551392, 'validation/accuracy': 0.604699969291687, 'validation/loss': 1.6360735893249512, 'validation/num_examples': 50000, 'test/accuracy': 0.4797000288963318, 'test/loss': 2.354118585586548, 'test/num_examples': 10000, 'score': 16354.602724313736, 'total_duration': 16935.195270061493, 'accumulated_submission_time': 16354.602724313736, 'accumulated_eval_time': 577.9383962154388, 'accumulated_logging_time': 1.026573657989502, 'global_step': 47825, 'preemption_count': 0}), (49323, {'train/accuracy': 0.6553133130073547, 'train/loss': 1.3799877166748047, 'validation/accuracy': 0.6030399799346924, 'validation/loss': 1.6445624828338623, 'validation/num_examples': 50000, 'test/accuracy': 0.4821000099182129, 'test/loss': 2.3614590167999268, 'test/num_examples': 10000, 'score': 16864.671887874603, 'total_duration': 17462.89333844185, 'accumulated_submission_time': 16864.671887874603, 'accumulated_eval_time': 595.4793081283569, 'accumulated_logging_time': 1.063713788986206, 'global_step': 49323, 'preemption_count': 0}), (50822, {'train/accuracy': 0.6630061864852905, 'train/loss': 1.3523014783859253, 'validation/accuracy': 0.6115399599075317, 'validation/loss': 1.6137793064117432, 'validation/num_examples': 50000, 'test/accuracy': 0.48830002546310425, 'test/loss': 2.343726396560669, 'test/num_examples': 10000, 'score': 17374.922873973846, 'total_duration': 17990.727430582047, 'accumulated_submission_time': 17374.922873973846, 'accumulated_eval_time': 612.9747793674469, 'accumulated_logging_time': 1.0999596118927002, 'global_step': 50822, 'preemption_count': 0}), (52320, {'train/accuracy': 0.6526426672935486, 'train/loss': 1.3899118900299072, 'validation/accuracy': 0.6061800122261047, 'validation/loss': 1.6280734539031982, 'validation/num_examples': 50000, 'test/accuracy': 0.4897000193595886, 'test/loss': 2.339770793914795, 'test/num_examples': 10000, 'score': 17885.14226746559, 'total_duration': 18518.282783985138, 'accumulated_submission_time': 17885.14226746559, 'accumulated_eval_time': 630.2206964492798, 'accumulated_logging_time': 1.1384408473968506, 'global_step': 52320, 'preemption_count': 0}), (53818, {'train/accuracy': 0.6602359414100647, 'train/loss': 1.361589789390564, 'validation/accuracy': 0.613379955291748, 'validation/loss': 1.606788992881775, 'validation/num_examples': 50000, 'test/accuracy': 0.4889000356197357, 'test/loss': 2.3146893978118896, 'test/num_examples': 10000, 'score': 18395.222403526306, 'total_duration': 19045.71305155754, 'accumulated_submission_time': 18395.222403526306, 'accumulated_eval_time': 647.4825391769409, 'accumulated_logging_time': 1.1758079528808594, 'global_step': 53818, 'preemption_count': 0}), (55316, {'train/accuracy': 0.699238657951355, 'train/loss': 1.1795507669448853, 'validation/accuracy': 0.6133399605751038, 'validation/loss': 1.5974823236465454, 'validation/num_examples': 50000, 'test/accuracy': 0.4910000264644623, 'test/loss': 2.2842953205108643, 'test/num_examples': 10000, 'score': 18905.270992279053, 'total_duration': 19573.216374635696, 'accumulated_submission_time': 18905.270992279053, 'accumulated_eval_time': 664.8474590778351, 'accumulated_logging_time': 1.2139925956726074, 'global_step': 55316, 'preemption_count': 0}), (56814, {'train/accuracy': 0.685965359210968, 'train/loss': 1.2393864393234253, 'validation/accuracy': 0.6195799708366394, 'validation/loss': 1.5674865245819092, 'validation/num_examples': 50000, 'test/accuracy': 0.49500003457069397, 'test/loss': 2.303964853286743, 'test/num_examples': 10000, 'score': 19415.29590034485, 'total_duration': 20100.787999868393, 'accumulated_submission_time': 19415.29590034485, 'accumulated_eval_time': 682.3041090965271, 'accumulated_logging_time': 1.252126932144165, 'global_step': 56814, 'preemption_count': 0}), (58312, {'train/accuracy': 0.6573660373687744, 'train/loss': 1.369436264038086, 'validation/accuracy': 0.6055799722671509, 'validation/loss': 1.6424763202667236, 'validation/num_examples': 50000, 'test/accuracy': 0.484000027179718, 'test/loss': 2.3704142570495605, 'test/num_examples': 10000, 'score': 19925.24132657051, 'total_duration': 20628.266762018204, 'accumulated_submission_time': 19925.24132657051, 'accumulated_eval_time': 699.7403359413147, 'accumulated_logging_time': 1.298933506011963, 'global_step': 58312, 'preemption_count': 0}), (59811, {'train/accuracy': 0.6717155575752258, 'train/loss': 1.299277424812317, 'validation/accuracy': 0.6213600039482117, 'validation/loss': 1.5658512115478516, 'validation/num_examples': 50000, 'test/accuracy': 0.49710002541542053, 'test/loss': 2.3210856914520264, 'test/num_examples': 10000, 'score': 20435.426945209503, 'total_duration': 21155.81867671013, 'accumulated_submission_time': 20435.426945209503, 'accumulated_eval_time': 717.0170676708221, 'accumulated_logging_time': 1.3389678001403809, 'global_step': 59811, 'preemption_count': 0}), (61310, {'train/accuracy': 0.6558912396430969, 'train/loss': 1.3650057315826416, 'validation/accuracy': 0.6165199875831604, 'validation/loss': 1.608451247215271, 'validation/num_examples': 50000, 'test/accuracy': 0.4895000159740448, 'test/loss': 2.3405680656433105, 'test/num_examples': 10000, 'score': 20945.637128591537, 'total_duration': 21683.5158367157, 'accumulated_submission_time': 20945.637128591537, 'accumulated_eval_time': 734.4156177043915, 'accumulated_logging_time': 1.3787786960601807, 'global_step': 61310, 'preemption_count': 0}), (62808, {'train/accuracy': 0.6518056392669678, 'train/loss': 1.3800477981567383, 'validation/accuracy': 0.6077399849891663, 'validation/loss': 1.6119192838668823, 'validation/num_examples': 50000, 'test/accuracy': 0.4837000370025635, 'test/loss': 2.364459991455078, 'test/num_examples': 10000, 'score': 21455.61483645439, 'total_duration': 22211.000241994858, 'accumulated_submission_time': 21455.61483645439, 'accumulated_eval_time': 751.8346419334412, 'accumulated_logging_time': 1.415900468826294, 'global_step': 62808, 'preemption_count': 0}), (64306, {'train/accuracy': 0.6535594463348389, 'train/loss': 1.3952399492263794, 'validation/accuracy': 0.6021400094032288, 'validation/loss': 1.6519254446029663, 'validation/num_examples': 50000, 'test/accuracy': 0.48030000925064087, 'test/loss': 2.3926892280578613, 'test/num_examples': 10000, 'score': 21965.65988755226, 'total_duration': 22738.56585907936, 'accumulated_submission_time': 21965.65988755226, 'accumulated_eval_time': 769.2622895240784, 'accumulated_logging_time': 1.4578070640563965, 'global_step': 64306, 'preemption_count': 0}), (65805, {'train/accuracy': 0.6921834945678711, 'train/loss': 1.207922101020813, 'validation/accuracy': 0.6225999593734741, 'validation/loss': 1.554673671722412, 'validation/num_examples': 50000, 'test/accuracy': 0.49970000982284546, 'test/loss': 2.25830078125, 'test/num_examples': 10000, 'score': 22475.830349206924, 'total_duration': 23266.250294208527, 'accumulated_submission_time': 22475.830349206924, 'accumulated_eval_time': 786.6829445362091, 'accumulated_logging_time': 1.5008704662322998, 'global_step': 65805, 'preemption_count': 0}), (67304, {'train/accuracy': 0.6910474896430969, 'train/loss': 1.2102947235107422, 'validation/accuracy': 0.6284999847412109, 'validation/loss': 1.5212945938110352, 'validation/num_examples': 50000, 'test/accuracy': 0.5111000537872314, 'test/loss': 2.2321712970733643, 'test/num_examples': 10000, 'score': 22985.990831136703, 'total_duration': 23793.728055477142, 'accumulated_submission_time': 22985.990831136703, 'accumulated_eval_time': 803.9027199745178, 'accumulated_logging_time': 1.547590732574463, 'global_step': 67304, 'preemption_count': 0}), (68803, {'train/accuracy': 0.6715561151504517, 'train/loss': 1.3061954975128174, 'validation/accuracy': 0.614579975605011, 'validation/loss': 1.594861626625061, 'validation/num_examples': 50000, 'test/accuracy': 0.4935000240802765, 'test/loss': 2.3209471702575684, 'test/num_examples': 10000, 'score': 23496.128660917282, 'total_duration': 24321.476287841797, 'accumulated_submission_time': 23496.128660917282, 'accumulated_eval_time': 821.4244170188904, 'accumulated_logging_time': 1.5859100818634033, 'global_step': 68803, 'preemption_count': 0}), (70301, {'train/accuracy': 0.6846699714660645, 'train/loss': 1.244409203529358, 'validation/accuracy': 0.6282599568367004, 'validation/loss': 1.5239821672439575, 'validation/num_examples': 50000, 'test/accuracy': 0.5094000101089478, 'test/loss': 2.2113044261932373, 'test/num_examples': 10000, 'score': 24006.130012512207, 'total_duration': 24849.235661029816, 'accumulated_submission_time': 24006.130012512207, 'accumulated_eval_time': 839.0864970684052, 'accumulated_logging_time': 1.6295363903045654, 'global_step': 70301, 'preemption_count': 0}), (71800, {'train/accuracy': 0.6765784025192261, 'train/loss': 1.2757912874221802, 'validation/accuracy': 0.6304799914360046, 'validation/loss': 1.5133068561553955, 'validation/num_examples': 50000, 'test/accuracy': 0.510200023651123, 'test/loss': 2.217399835586548, 'test/num_examples': 10000, 'score': 24516.21312022209, 'total_duration': 25376.767111063004, 'accumulated_submission_time': 24516.21312022209, 'accumulated_eval_time': 856.4387171268463, 'accumulated_logging_time': 1.6738028526306152, 'global_step': 71800, 'preemption_count': 0}), (73298, {'train/accuracy': 0.6873405575752258, 'train/loss': 1.2287601232528687, 'validation/accuracy': 0.6338199973106384, 'validation/loss': 1.491397738456726, 'validation/num_examples': 50000, 'test/accuracy': 0.5108000040054321, 'test/loss': 2.190986156463623, 'test/num_examples': 10000, 'score': 25026.155697584152, 'total_duration': 25904.172393083572, 'accumulated_submission_time': 25026.155697584152, 'accumulated_eval_time': 873.810319185257, 'accumulated_logging_time': 1.7138514518737793, 'global_step': 73298, 'preemption_count': 0}), (74797, {'train/accuracy': 0.6872010231018066, 'train/loss': 1.2051925659179688, 'validation/accuracy': 0.6161800026893616, 'validation/loss': 1.587920069694519, 'validation/num_examples': 50000, 'test/accuracy': 0.5003000497817993, 'test/loss': 2.303654909133911, 'test/num_examples': 10000, 'score': 25536.339210748672, 'total_duration': 26431.880017757416, 'accumulated_submission_time': 25536.339210748672, 'accumulated_eval_time': 891.2437620162964, 'accumulated_logging_time': 1.7546777725219727, 'global_step': 74797, 'preemption_count': 0}), (76296, {'train/accuracy': 0.6835737824440002, 'train/loss': 1.253710389137268, 'validation/accuracy': 0.6217799782752991, 'validation/loss': 1.5734446048736572, 'validation/num_examples': 50000, 'test/accuracy': 0.4935000240802765, 'test/loss': 2.3314461708068848, 'test/num_examples': 10000, 'score': 26046.54245686531, 'total_duration': 26959.465401887894, 'accumulated_submission_time': 26046.54245686531, 'accumulated_eval_time': 908.53577709198, 'accumulated_logging_time': 1.7942428588867188, 'global_step': 76296, 'preemption_count': 0}), (77783, {'train/accuracy': 0.6978435516357422, 'train/loss': 1.2021422386169434, 'validation/accuracy': 0.6412799954414368, 'validation/loss': 1.4752492904663086, 'validation/num_examples': 50000, 'test/accuracy': 0.5133000016212463, 'test/loss': 2.162278652191162, 'test/num_examples': 10000, 'score': 26556.63892173767, 'total_duration': 27487.96911430359, 'accumulated_submission_time': 26556.63892173767, 'accumulated_eval_time': 926.8507552146912, 'accumulated_logging_time': 1.83475923538208, 'global_step': 77783, 'preemption_count': 0}), (79282, {'train/accuracy': 0.6960698366165161, 'train/loss': 1.1828243732452393, 'validation/accuracy': 0.6363999843597412, 'validation/loss': 1.4793654680252075, 'validation/num_examples': 50000, 'test/accuracy': 0.5126000046730042, 'test/loss': 2.2020113468170166, 'test/num_examples': 10000, 'score': 27066.782158851624, 'total_duration': 28015.67524456978, 'accumulated_submission_time': 27066.782158851624, 'accumulated_eval_time': 944.3251445293427, 'accumulated_logging_time': 1.8716328144073486, 'global_step': 79282, 'preemption_count': 0}), (80781, {'train/accuracy': 0.6879384517669678, 'train/loss': 1.2187047004699707, 'validation/accuracy': 0.6388999819755554, 'validation/loss': 1.4817255735397339, 'validation/num_examples': 50000, 'test/accuracy': 0.5117000341415405, 'test/loss': 2.2074272632598877, 'test/num_examples': 10000, 'score': 27576.946058273315, 'total_duration': 28543.36324763298, 'accumulated_submission_time': 27576.946058273315, 'accumulated_eval_time': 961.7491703033447, 'accumulated_logging_time': 1.9183545112609863, 'global_step': 80781, 'preemption_count': 0}), (82280, {'train/accuracy': 0.6951131820678711, 'train/loss': 1.1844233274459839, 'validation/accuracy': 0.6406799554824829, 'validation/loss': 1.4681005477905273, 'validation/num_examples': 50000, 'test/accuracy': 0.5145000219345093, 'test/loss': 2.1780965328216553, 'test/num_examples': 10000, 'score': 28087.02109003067, 'total_duration': 29070.87357234955, 'accumulated_submission_time': 28087.02109003067, 'accumulated_eval_time': 979.0919954776764, 'accumulated_logging_time': 1.960249662399292, 'global_step': 82280, 'preemption_count': 0}), (83779, {'train/accuracy': 0.7388990521430969, 'train/loss': 1.0035078525543213, 'validation/accuracy': 0.6438999772071838, 'validation/loss': 1.44945228099823, 'validation/num_examples': 50000, 'test/accuracy': 0.5115000009536743, 'test/loss': 2.1780617237091064, 'test/num_examples': 10000, 'score': 28596.984345436096, 'total_duration': 29598.28356528282, 'accumulated_submission_time': 28596.984345436096, 'accumulated_eval_time': 996.4454569816589, 'accumulated_logging_time': 2.001596212387085, 'global_step': 83779, 'preemption_count': 0}), (85277, {'train/accuracy': 0.7179328799247742, 'train/loss': 1.081685185432434, 'validation/accuracy': 0.6489399671554565, 'validation/loss': 1.4340031147003174, 'validation/num_examples': 50000, 'test/accuracy': 0.522599995136261, 'test/loss': 2.1304209232330322, 'test/num_examples': 10000, 'score': 29107.010613918304, 'total_duration': 30126.32354569435, 'accumulated_submission_time': 29107.010613918304, 'accumulated_eval_time': 1014.3660583496094, 'accumulated_logging_time': 2.0440704822540283, 'global_step': 85277, 'preemption_count': 0}), (86776, {'train/accuracy': 0.7063137888908386, 'train/loss': 1.1358715295791626, 'validation/accuracy': 0.6468200087547302, 'validation/loss': 1.4428404569625854, 'validation/num_examples': 50000, 'test/accuracy': 0.5206000208854675, 'test/loss': 2.138397455215454, 'test/num_examples': 10000, 'score': 29617.09086871147, 'total_duration': 30653.98149752617, 'accumulated_submission_time': 29617.09086871147, 'accumulated_eval_time': 1031.8577575683594, 'accumulated_logging_time': 2.080768346786499, 'global_step': 86776, 'preemption_count': 0}), (88275, {'train/accuracy': 0.7068120241165161, 'train/loss': 1.13710355758667, 'validation/accuracy': 0.6526600122451782, 'validation/loss': 1.4295099973678589, 'validation/num_examples': 50000, 'test/accuracy': 0.5166000127792358, 'test/loss': 2.154176950454712, 'test/num_examples': 10000, 'score': 30127.182915449142, 'total_duration': 31181.804544210434, 'accumulated_submission_time': 30127.182915449142, 'accumulated_eval_time': 1049.4971315860748, 'accumulated_logging_time': 2.121476888656616, 'global_step': 88275, 'preemption_count': 0}), (89771, {'train/accuracy': 0.6945750713348389, 'train/loss': 1.1966772079467773, 'validation/accuracy': 0.6418799757957458, 'validation/loss': 1.463532567024231, 'validation/num_examples': 50000, 'test/accuracy': 0.5118000507354736, 'test/loss': 2.200810432434082, 'test/num_examples': 10000, 'score': 30636.219479560852, 'total_duration': 31709.34457540512, 'accumulated_submission_time': 30636.219479560852, 'accumulated_eval_time': 1066.8635828495026, 'accumulated_logging_time': 3.207822322845459, 'global_step': 89771, 'preemption_count': 0}), (91270, {'train/accuracy': 0.7092235088348389, 'train/loss': 1.1346157789230347, 'validation/accuracy': 0.6538000106811523, 'validation/loss': 1.4101835489273071, 'validation/num_examples': 50000, 'test/accuracy': 0.5267000198364258, 'test/loss': 2.1272940635681152, 'test/num_examples': 10000, 'score': 31146.34582209587, 'total_duration': 32236.864321231842, 'accumulated_submission_time': 31146.34582209587, 'accumulated_eval_time': 1084.156126499176, 'accumulated_logging_time': 3.255371332168579, 'global_step': 91270, 'preemption_count': 0}), (92768, {'train/accuracy': 0.7113958597183228, 'train/loss': 1.1191548109054565, 'validation/accuracy': 0.6482200026512146, 'validation/loss': 1.4321719408035278, 'validation/num_examples': 50000, 'test/accuracy': 0.5174000263214111, 'test/loss': 2.1670353412628174, 'test/num_examples': 10000, 'score': 31656.254772424698, 'total_duration': 32764.192311525345, 'accumulated_submission_time': 31656.254772424698, 'accumulated_eval_time': 1101.4758217334747, 'accumulated_logging_time': 3.3038907051086426, 'global_step': 92768, 'preemption_count': 0}), (94267, {'train/accuracy': 0.7361487150192261, 'train/loss': 1.0138471126556396, 'validation/accuracy': 0.659500002861023, 'validation/loss': 1.3887938261032104, 'validation/num_examples': 50000, 'test/accuracy': 0.5303000211715698, 'test/loss': 2.0873241424560547, 'test/num_examples': 10000, 'score': 32166.24189376831, 'total_duration': 33291.75237035751, 'accumulated_submission_time': 32166.24189376831, 'accumulated_eval_time': 1118.9492797851562, 'accumulated_logging_time': 3.351661443710327, 'global_step': 94267, 'preemption_count': 0}), (95766, {'train/accuracy': 0.7229153513908386, 'train/loss': 1.0641008615493774, 'validation/accuracy': 0.6566199660301208, 'validation/loss': 1.4057345390319824, 'validation/num_examples': 50000, 'test/accuracy': 0.5320000052452087, 'test/loss': 2.1097888946533203, 'test/num_examples': 10000, 'score': 32676.229960918427, 'total_duration': 33819.02078318596, 'accumulated_submission_time': 32676.229960918427, 'accumulated_eval_time': 1136.129715681076, 'accumulated_logging_time': 3.401322364807129, 'global_step': 95766, 'preemption_count': 0}), (97265, {'train/accuracy': 0.7237324714660645, 'train/loss': 1.0708374977111816, 'validation/accuracy': 0.6580399870872498, 'validation/loss': 1.3982630968093872, 'validation/num_examples': 50000, 'test/accuracy': 0.5344000458717346, 'test/loss': 2.089726209640503, 'test/num_examples': 10000, 'score': 33186.39605593681, 'total_duration': 34346.4784386158, 'accumulated_submission_time': 33186.39605593681, 'accumulated_eval_time': 1153.3248386383057, 'accumulated_logging_time': 3.446504831314087, 'global_step': 97265, 'preemption_count': 0}), (98765, {'train/accuracy': 0.7150430083274841, 'train/loss': 1.1016902923583984, 'validation/accuracy': 0.6593799591064453, 'validation/loss': 1.3938559293746948, 'validation/num_examples': 50000, 'test/accuracy': 0.536300003528595, 'test/loss': 2.0971498489379883, 'test/num_examples': 10000, 'score': 33696.623683452606, 'total_duration': 34874.23455262184, 'accumulated_submission_time': 33696.623683452606, 'accumulated_eval_time': 1170.7596073150635, 'accumulated_logging_time': 3.4904332160949707, 'global_step': 98765, 'preemption_count': 0}), (100264, {'train/accuracy': 0.713309109210968, 'train/loss': 1.1122252941131592, 'validation/accuracy': 0.6600399613380432, 'validation/loss': 1.3887194395065308, 'validation/num_examples': 50000, 'test/accuracy': 0.5412000417709351, 'test/loss': 2.056500196456909, 'test/num_examples': 10000, 'score': 34206.6414039135, 'total_duration': 35401.838272333145, 'accumulated_submission_time': 34206.6414039135, 'accumulated_eval_time': 1188.2499401569366, 'accumulated_logging_time': 3.535299777984619, 'global_step': 100264, 'preemption_count': 0}), (101764, {'train/accuracy': 0.7233139276504517, 'train/loss': 1.072434902191162, 'validation/accuracy': 0.6643799543380737, 'validation/loss': 1.362235426902771, 'validation/num_examples': 50000, 'test/accuracy': 0.5402000546455383, 'test/loss': 2.057695150375366, 'test/num_examples': 10000, 'score': 34716.84902739525, 'total_duration': 35929.189604759216, 'accumulated_submission_time': 34716.84902739525, 'accumulated_eval_time': 1205.2929441928864, 'accumulated_logging_time': 3.586189031600952, 'global_step': 101764, 'preemption_count': 0}), (103263, {'train/accuracy': 0.75394606590271, 'train/loss': 0.9281212091445923, 'validation/accuracy': 0.6668999791145325, 'validation/loss': 1.3567508459091187, 'validation/num_examples': 50000, 'test/accuracy': 0.5405000448226929, 'test/loss': 2.041477918624878, 'test/num_examples': 10000, 'score': 35226.76603245735, 'total_duration': 36456.52231359482, 'accumulated_submission_time': 35226.76603245735, 'accumulated_eval_time': 1222.604502916336, 'accumulated_logging_time': 3.63908052444458, 'global_step': 103263, 'preemption_count': 0}), (104762, {'train/accuracy': 0.7365872263908386, 'train/loss': 1.0082699060440063, 'validation/accuracy': 0.6635199785232544, 'validation/loss': 1.3672338724136353, 'validation/num_examples': 50000, 'test/accuracy': 0.5410000085830688, 'test/loss': 2.045719623565674, 'test/num_examples': 10000, 'score': 35736.759813547134, 'total_duration': 36984.040206193924, 'accumulated_submission_time': 35736.759813547134, 'accumulated_eval_time': 1240.0294904708862, 'accumulated_logging_time': 3.6891348361968994, 'global_step': 104762, 'preemption_count': 0}), (106261, {'train/accuracy': 0.7372249364852905, 'train/loss': 0.9990629553794861, 'validation/accuracy': 0.6670599579811096, 'validation/loss': 1.3503457307815552, 'validation/num_examples': 50000, 'test/accuracy': 0.5432000160217285, 'test/loss': 2.0669026374816895, 'test/num_examples': 10000, 'score': 36246.74147129059, 'total_duration': 37511.61378097534, 'accumulated_submission_time': 36246.74147129059, 'accumulated_eval_time': 1257.5226662158966, 'accumulated_logging_time': 3.7353804111480713, 'global_step': 106261, 'preemption_count': 0}), (107760, {'train/accuracy': 0.7306082248687744, 'train/loss': 1.0350559949874878, 'validation/accuracy': 0.6622999906539917, 'validation/loss': 1.3632971048355103, 'validation/num_examples': 50000, 'test/accuracy': 0.5291000008583069, 'test/loss': 2.1044600009918213, 'test/num_examples': 10000, 'score': 36756.6770863533, 'total_duration': 38039.03833150864, 'accumulated_submission_time': 36756.6770863533, 'accumulated_eval_time': 1274.9144945144653, 'accumulated_logging_time': 3.7825989723205566, 'global_step': 107760, 'preemption_count': 0}), (109259, {'train/accuracy': 0.7350525856018066, 'train/loss': 1.0115010738372803, 'validation/accuracy': 0.6735000014305115, 'validation/loss': 1.3214294910430908, 'validation/num_examples': 50000, 'test/accuracy': 0.549500048160553, 'test/loss': 2.004624605178833, 'test/num_examples': 10000, 'score': 37266.8054394722, 'total_duration': 38566.80239248276, 'accumulated_submission_time': 37266.8054394722, 'accumulated_eval_time': 1292.4509418010712, 'accumulated_logging_time': 3.830709218978882, 'global_step': 109259, 'preemption_count': 0}), (110758, {'train/accuracy': 0.7382214665412903, 'train/loss': 0.9993380308151245, 'validation/accuracy': 0.6735000014305115, 'validation/loss': 1.3205397129058838, 'validation/num_examples': 50000, 'test/accuracy': 0.5402000546455383, 'test/loss': 2.009732484817505, 'test/num_examples': 10000, 'score': 37776.72888278961, 'total_duration': 39094.371799230576, 'accumulated_submission_time': 37776.72888278961, 'accumulated_eval_time': 1309.9946851730347, 'accumulated_logging_time': 3.8815505504608154, 'global_step': 110758, 'preemption_count': 0}), (112257, {'train/accuracy': 0.7806122303009033, 'train/loss': 0.819338858127594, 'validation/accuracy': 0.6816200017929077, 'validation/loss': 1.2890325784683228, 'validation/num_examples': 50000, 'test/accuracy': 0.5503000020980835, 'test/loss': 1.9960659742355347, 'test/num_examples': 10000, 'score': 38286.65507602692, 'total_duration': 39621.84500050545, 'accumulated_submission_time': 38286.65507602692, 'accumulated_eval_time': 1327.4420084953308, 'accumulated_logging_time': 3.929419755935669, 'global_step': 112257, 'preemption_count': 0}), (113756, {'train/accuracy': 0.7663623690605164, 'train/loss': 0.8877369165420532, 'validation/accuracy': 0.6804400086402893, 'validation/loss': 1.2903692722320557, 'validation/num_examples': 50000, 'test/accuracy': 0.5544000267982483, 'test/loss': 1.976928472518921, 'test/num_examples': 10000, 'score': 38796.73586535454, 'total_duration': 40149.694214344025, 'accumulated_submission_time': 38796.73586535454, 'accumulated_eval_time': 1345.113474369049, 'accumulated_logging_time': 3.9760618209838867, 'global_step': 113756, 'preemption_count': 0}), (115255, {'train/accuracy': 0.7538464665412903, 'train/loss': 0.9340843558311462, 'validation/accuracy': 0.6775199770927429, 'validation/loss': 1.3047815561294556, 'validation/num_examples': 50000, 'test/accuracy': 0.5565000176429749, 'test/loss': 1.98634934425354, 'test/num_examples': 10000, 'score': 39306.689522743225, 'total_duration': 40677.29035973549, 'accumulated_submission_time': 39306.689522743225, 'accumulated_eval_time': 1362.6518981456757, 'accumulated_logging_time': 4.028971195220947, 'global_step': 115255, 'preemption_count': 0}), (116754, {'train/accuracy': 0.7443000674247742, 'train/loss': 0.9577973484992981, 'validation/accuracy': 0.6729399561882019, 'validation/loss': 1.3240751028060913, 'validation/num_examples': 50000, 'test/accuracy': 0.5436000227928162, 'test/loss': 2.0469701290130615, 'test/num_examples': 10000, 'score': 39816.85724949837, 'total_duration': 41204.86368608475, 'accumulated_submission_time': 39816.85724949837, 'accumulated_eval_time': 1379.9599359035492, 'accumulated_logging_time': 4.0755980014801025, 'global_step': 116754, 'preemption_count': 0}), (118253, {'train/accuracy': 0.7626155614852905, 'train/loss': 0.9064222574234009, 'validation/accuracy': 0.6890999674797058, 'validation/loss': 1.2482517957687378, 'validation/num_examples': 50000, 'test/accuracy': 0.5649000406265259, 'test/loss': 1.9236938953399658, 'test/num_examples': 10000, 'score': 40326.82902789116, 'total_duration': 41732.53103065491, 'accumulated_submission_time': 40326.82902789116, 'accumulated_eval_time': 1397.556854724884, 'accumulated_logging_time': 4.121983051300049, 'global_step': 118253, 'preemption_count': 0}), (119752, {'train/accuracy': 0.7578921914100647, 'train/loss': 0.9132976531982422, 'validation/accuracy': 0.6890599727630615, 'validation/loss': 1.2581051588058472, 'validation/num_examples': 50000, 'test/accuracy': 0.5614000558853149, 'test/loss': 1.9510211944580078, 'test/num_examples': 10000, 'score': 40836.754336595535, 'total_duration': 42259.80155444145, 'accumulated_submission_time': 40836.754336595535, 'accumulated_eval_time': 1414.801174402237, 'accumulated_logging_time': 4.171825647354126, 'global_step': 119752, 'preemption_count': 0}), (121251, {'train/accuracy': 0.7889827489852905, 'train/loss': 0.7927571535110474, 'validation/accuracy': 0.6894400119781494, 'validation/loss': 1.2526060342788696, 'validation/num_examples': 50000, 'test/accuracy': 0.5688000321388245, 'test/loss': 1.9327174425125122, 'test/num_examples': 10000, 'score': 41346.87264943123, 'total_duration': 42787.65633249283, 'accumulated_submission_time': 41346.87264943123, 'accumulated_eval_time': 1432.4339890480042, 'accumulated_logging_time': 4.225170850753784, 'global_step': 121251, 'preemption_count': 0}), (122751, {'train/accuracy': 0.778738796710968, 'train/loss': 0.8235031962394714, 'validation/accuracy': 0.6881600022315979, 'validation/loss': 1.264472484588623, 'validation/num_examples': 50000, 'test/accuracy': 0.5613000392913818, 'test/loss': 1.9879837036132812, 'test/num_examples': 10000, 'score': 41857.08833384514, 'total_duration': 43315.46732521057, 'accumulated_submission_time': 41857.08833384514, 'accumulated_eval_time': 1449.9362061023712, 'accumulated_logging_time': 4.2682952880859375, 'global_step': 122751, 'preemption_count': 0}), (124250, {'train/accuracy': 0.782246470451355, 'train/loss': 0.8037508726119995, 'validation/accuracy': 0.6977399587631226, 'validation/loss': 1.2117669582366943, 'validation/num_examples': 50000, 'test/accuracy': 0.5701000094413757, 'test/loss': 1.924551010131836, 'test/num_examples': 10000, 'score': 42367.103356838226, 'total_duration': 43843.563719034195, 'accumulated_submission_time': 42367.103356838226, 'accumulated_eval_time': 1467.9139926433563, 'accumulated_logging_time': 4.320462465286255, 'global_step': 124250, 'preemption_count': 0}), (125749, {'train/accuracy': 0.7780413031578064, 'train/loss': 0.8233484625816345, 'validation/accuracy': 0.6994400024414062, 'validation/loss': 1.2192716598510742, 'validation/num_examples': 50000, 'test/accuracy': 0.5735000371932983, 'test/loss': 1.909732699394226, 'test/num_examples': 10000, 'score': 42877.30141162872, 'total_duration': 44371.0747590065, 'accumulated_submission_time': 42877.30141162872, 'accumulated_eval_time': 1485.1218111515045, 'accumulated_logging_time': 4.3753581047058105, 'global_step': 125749, 'preemption_count': 0}), (127248, {'train/accuracy': 0.7848173975944519, 'train/loss': 0.798477292060852, 'validation/accuracy': 0.6995799541473389, 'validation/loss': 1.2017760276794434, 'validation/num_examples': 50000, 'test/accuracy': 0.5724000334739685, 'test/loss': 1.9187508821487427, 'test/num_examples': 10000, 'score': 43387.38800287247, 'total_duration': 44898.51348400116, 'accumulated_submission_time': 43387.38800287247, 'accumulated_eval_time': 1502.3710179328918, 'accumulated_logging_time': 4.427471876144409, 'global_step': 127248, 'preemption_count': 0}), (128747, {'train/accuracy': 0.7816087007522583, 'train/loss': 0.8127210736274719, 'validation/accuracy': 0.7039200067520142, 'validation/loss': 1.1963602304458618, 'validation/num_examples': 50000, 'test/accuracy': 0.5781000256538391, 'test/loss': 1.8842612504959106, 'test/num_examples': 10000, 'score': 43897.49772691727, 'total_duration': 45426.061324596405, 'accumulated_submission_time': 43897.49772691727, 'accumulated_eval_time': 1519.7068076133728, 'accumulated_logging_time': 4.479499578475952, 'global_step': 128747, 'preemption_count': 0}), (130246, {'train/accuracy': 0.7840800285339355, 'train/loss': 0.8188005685806274, 'validation/accuracy': 0.7031799554824829, 'validation/loss': 1.1907312870025635, 'validation/num_examples': 50000, 'test/accuracy': 0.5733000040054321, 'test/loss': 1.8935832977294922, 'test/num_examples': 10000, 'score': 44407.48382616043, 'total_duration': 45953.42463064194, 'accumulated_submission_time': 44407.48382616043, 'accumulated_eval_time': 1536.9744882583618, 'accumulated_logging_time': 4.538180828094482, 'global_step': 130246, 'preemption_count': 0}), (131745, {'train/accuracy': 0.8127192258834839, 'train/loss': 0.6832287907600403, 'validation/accuracy': 0.7074599862098694, 'validation/loss': 1.1807633638381958, 'validation/num_examples': 50000, 'test/accuracy': 0.5805000066757202, 'test/loss': 1.8970988988876343, 'test/num_examples': 10000, 'score': 44917.434248924255, 'total_duration': 46480.9824347496, 'accumulated_submission_time': 44917.434248924255, 'accumulated_eval_time': 1554.4766201972961, 'accumulated_logging_time': 4.591028213500977, 'global_step': 131745, 'preemption_count': 0}), (133243, {'train/accuracy': 0.79984450340271, 'train/loss': 0.7381070852279663, 'validation/accuracy': 0.7058199644088745, 'validation/loss': 1.1831684112548828, 'validation/num_examples': 50000, 'test/accuracy': 0.5821000337600708, 'test/loss': 1.8820722103118896, 'test/num_examples': 10000, 'score': 45427.39062857628, 'total_duration': 47008.251879930496, 'accumulated_submission_time': 45427.39062857628, 'accumulated_eval_time': 1571.6903052330017, 'accumulated_logging_time': 4.63919734954834, 'global_step': 133243, 'preemption_count': 0}), (134742, {'train/accuracy': 0.8057836294174194, 'train/loss': 0.7143407464027405, 'validation/accuracy': 0.7128799557685852, 'validation/loss': 1.146412968635559, 'validation/num_examples': 50000, 'test/accuracy': 0.5893000364303589, 'test/loss': 1.8250541687011719, 'test/num_examples': 10000, 'score': 45937.45642733574, 'total_duration': 47535.90947675705, 'accumulated_submission_time': 45937.45642733574, 'accumulated_eval_time': 1589.1792786121368, 'accumulated_logging_time': 4.692151308059692, 'global_step': 134742, 'preemption_count': 0}), (136241, {'train/accuracy': 0.8033322691917419, 'train/loss': 0.7214902639389038, 'validation/accuracy': 0.7112399935722351, 'validation/loss': 1.1542009115219116, 'validation/num_examples': 50000, 'test/accuracy': 0.5920000076293945, 'test/loss': 1.826322317123413, 'test/num_examples': 10000, 'score': 46447.50892996788, 'total_duration': 48063.74123668671, 'accumulated_submission_time': 46447.50892996788, 'accumulated_eval_time': 1606.857929468155, 'accumulated_logging_time': 4.741678476333618, 'global_step': 136241, 'preemption_count': 0}), (137740, {'train/accuracy': 0.805683970451355, 'train/loss': 0.7093267440795898, 'validation/accuracy': 0.7160399556159973, 'validation/loss': 1.1419600248336792, 'validation/num_examples': 50000, 'test/accuracy': 0.5933000445365906, 'test/loss': 1.8347601890563965, 'test/num_examples': 10000, 'score': 46957.61950492859, 'total_duration': 48591.361892938614, 'accumulated_submission_time': 46957.61950492859, 'accumulated_eval_time': 1624.2676224708557, 'accumulated_logging_time': 4.790990352630615, 'global_step': 137740, 'preemption_count': 0}), (139238, {'train/accuracy': 0.8067402839660645, 'train/loss': 0.7009314298629761, 'validation/accuracy': 0.7160199880599976, 'validation/loss': 1.1319137811660767, 'validation/num_examples': 50000, 'test/accuracy': 0.5916000008583069, 'test/loss': 1.830636978149414, 'test/num_examples': 10000, 'score': 47467.85808753967, 'total_duration': 49118.858887672424, 'accumulated_submission_time': 47467.85808753967, 'accumulated_eval_time': 1641.4246740341187, 'accumulated_logging_time': 4.841644525527954, 'global_step': 139238, 'preemption_count': 0}), (140737, {'train/accuracy': 0.8413185477256775, 'train/loss': 0.5797902345657349, 'validation/accuracy': 0.7160599827766418, 'validation/loss': 1.138371467590332, 'validation/num_examples': 50000, 'test/accuracy': 0.5914000272750854, 'test/loss': 1.8102748394012451, 'test/num_examples': 10000, 'score': 47978.06637239456, 'total_duration': 49646.37145638466, 'accumulated_submission_time': 47978.06637239456, 'accumulated_eval_time': 1658.625111579895, 'accumulated_logging_time': 4.895170450210571, 'global_step': 140737, 'preemption_count': 0}), (142236, {'train/accuracy': 0.8332070708274841, 'train/loss': 0.6023600101470947, 'validation/accuracy': 0.7219199538230896, 'validation/loss': 1.1166437864303589, 'validation/num_examples': 50000, 'test/accuracy': 0.5998000502586365, 'test/loss': 1.7817325592041016, 'test/num_examples': 10000, 'score': 48488.23559617996, 'total_duration': 50174.20073246956, 'accumulated_submission_time': 48488.23559617996, 'accumulated_eval_time': 1676.1800088882446, 'accumulated_logging_time': 4.949621677398682, 'global_step': 142236, 'preemption_count': 0}), (143735, {'train/accuracy': 0.8356783986091614, 'train/loss': 0.5958223938941956, 'validation/accuracy': 0.7263199687004089, 'validation/loss': 1.1001946926116943, 'validation/num_examples': 50000, 'test/accuracy': 0.6032000184059143, 'test/loss': 1.785300612449646, 'test/num_examples': 10000, 'score': 48998.14870905876, 'total_duration': 50701.440212488174, 'accumulated_submission_time': 48998.14870905876, 'accumulated_eval_time': 1693.400636434555, 'accumulated_logging_time': 5.0050599575042725, 'global_step': 143735, 'preemption_count': 0}), (145233, {'train/accuracy': 0.8364556431770325, 'train/loss': 0.5910767316818237, 'validation/accuracy': 0.7298399806022644, 'validation/loss': 1.085693120956421, 'validation/num_examples': 50000, 'test/accuracy': 0.6122000217437744, 'test/loss': 1.7577836513519287, 'test/num_examples': 10000, 'score': 49508.116096019745, 'total_duration': 51228.993624448776, 'accumulated_submission_time': 49508.116096019745, 'accumulated_eval_time': 1710.852013349533, 'accumulated_logging_time': 5.087496757507324, 'global_step': 145233, 'preemption_count': 0}), (146732, {'train/accuracy': 0.83402419090271, 'train/loss': 0.6022161841392517, 'validation/accuracy': 0.729919970035553, 'validation/loss': 1.0840203762054443, 'validation/num_examples': 50000, 'test/accuracy': 0.6041000485420227, 'test/loss': 1.7766298055648804, 'test/num_examples': 10000, 'score': 50018.29201626778, 'total_duration': 51756.42676591873, 'accumulated_submission_time': 50018.29201626778, 'accumulated_eval_time': 1728.0061275959015, 'accumulated_logging_time': 5.139847993850708, 'global_step': 146732, 'preemption_count': 0}), (148231, {'train/accuracy': 0.8361965417861938, 'train/loss': 0.5804234743118286, 'validation/accuracy': 0.7318599820137024, 'validation/loss': 1.0679874420166016, 'validation/num_examples': 50000, 'test/accuracy': 0.6070000529289246, 'test/loss': 1.7447444200515747, 'test/num_examples': 10000, 'score': 50528.38182926178, 'total_duration': 52283.85293364525, 'accumulated_submission_time': 50528.38182926178, 'accumulated_eval_time': 1745.2415256500244, 'accumulated_logging_time': 5.190353631973267, 'global_step': 148231, 'preemption_count': 0}), (149730, {'train/accuracy': 0.8720304369926453, 'train/loss': 0.4616715610027313, 'validation/accuracy': 0.7359199523925781, 'validation/loss': 1.06617271900177, 'validation/num_examples': 50000, 'test/accuracy': 0.6065000295639038, 'test/loss': 1.750016450881958, 'test/num_examples': 10000, 'score': 51038.4784321785, 'total_duration': 52811.57619476318, 'accumulated_submission_time': 51038.4784321785, 'accumulated_eval_time': 1762.763778924942, 'accumulated_logging_time': 5.244019508361816, 'global_step': 149730, 'preemption_count': 0}), (151228, {'train/accuracy': 0.8650151491165161, 'train/loss': 0.48393839597702026, 'validation/accuracy': 0.7351799607276917, 'validation/loss': 1.0641911029815674, 'validation/num_examples': 50000, 'test/accuracy': 0.6137000322341919, 'test/loss': 1.7262399196624756, 'test/num_examples': 10000, 'score': 51548.38336634636, 'total_duration': 53338.903485774994, 'accumulated_submission_time': 51548.38336634636, 'accumulated_eval_time': 1780.0837841033936, 'accumulated_logging_time': 5.296934366226196, 'global_step': 151228, 'preemption_count': 0}), (152726, {'train/accuracy': 0.8591955900192261, 'train/loss': 0.49269208312034607, 'validation/accuracy': 0.7340599894523621, 'validation/loss': 1.070358157157898, 'validation/num_examples': 50000, 'test/accuracy': 0.6089000105857849, 'test/loss': 1.7674813270568848, 'test/num_examples': 10000, 'score': 52058.31352877617, 'total_duration': 53866.35694384575, 'accumulated_submission_time': 52058.31352877617, 'accumulated_eval_time': 1797.5025045871735, 'accumulated_logging_time': 5.351749420166016, 'global_step': 152726, 'preemption_count': 0}), (154225, {'train/accuracy': 0.8656927347183228, 'train/loss': 0.478325217962265, 'validation/accuracy': 0.7418199777603149, 'validation/loss': 1.0443856716156006, 'validation/num_examples': 50000, 'test/accuracy': 0.6195000410079956, 'test/loss': 1.7304673194885254, 'test/num_examples': 10000, 'score': 52568.48841428757, 'total_duration': 54393.9851975441, 'accumulated_submission_time': 52568.48841428757, 'accumulated_eval_time': 1814.8461351394653, 'accumulated_logging_time': 5.408911228179932, 'global_step': 154225, 'preemption_count': 0}), (155724, {'train/accuracy': 0.8673469424247742, 'train/loss': 0.4798938035964966, 'validation/accuracy': 0.7419399619102478, 'validation/loss': 1.0362374782562256, 'validation/num_examples': 50000, 'test/accuracy': 0.6204000115394592, 'test/loss': 1.7236957550048828, 'test/num_examples': 10000, 'score': 53078.627429008484, 'total_duration': 54921.7041721344, 'accumulated_submission_time': 53078.627429008484, 'accumulated_eval_time': 1832.3190059661865, 'accumulated_logging_time': 5.4648637771606445, 'global_step': 155724, 'preemption_count': 0}), (157222, {'train/accuracy': 0.8683633208274841, 'train/loss': 0.46061205863952637, 'validation/accuracy': 0.7424399852752686, 'validation/loss': 1.0408451557159424, 'validation/num_examples': 50000, 'test/accuracy': 0.6229000091552734, 'test/loss': 1.7110683917999268, 'test/num_examples': 10000, 'score': 53588.541823387146, 'total_duration': 55449.373153209686, 'accumulated_submission_time': 53588.541823387146, 'accumulated_eval_time': 1849.9641468524933, 'accumulated_logging_time': 5.523918867111206, 'global_step': 157222, 'preemption_count': 0}), (158721, {'train/accuracy': 0.8824138641357422, 'train/loss': 0.41793200373649597, 'validation/accuracy': 0.7461999654769897, 'validation/loss': 1.0250033140182495, 'validation/num_examples': 50000, 'test/accuracy': 0.6211000084877014, 'test/loss': 1.7047979831695557, 'test/num_examples': 10000, 'score': 54098.47330284119, 'total_duration': 55976.7153468132, 'accumulated_submission_time': 54098.47330284119, 'accumulated_eval_time': 1867.2719218730927, 'accumulated_logging_time': 5.576862573623657, 'global_step': 158721, 'preemption_count': 0}), (160220, {'train/accuracy': 0.8951291441917419, 'train/loss': 0.3667570948600769, 'validation/accuracy': 0.7464399933815002, 'validation/loss': 1.0255107879638672, 'validation/num_examples': 50000, 'test/accuracy': 0.6242000460624695, 'test/loss': 1.7131024599075317, 'test/num_examples': 10000, 'score': 54608.64770102501, 'total_duration': 56504.31801342964, 'accumulated_submission_time': 54608.64770102501, 'accumulated_eval_time': 1884.594167470932, 'accumulated_logging_time': 5.631555557250977, 'global_step': 160220, 'preemption_count': 0}), (161719, {'train/accuracy': 0.8964046239852905, 'train/loss': 0.3655628263950348, 'validation/accuracy': 0.7499399781227112, 'validation/loss': 1.0128942728042603, 'validation/num_examples': 50000, 'test/accuracy': 0.6291000247001648, 'test/loss': 1.6922518014907837, 'test/num_examples': 10000, 'score': 55118.76144886017, 'total_duration': 57032.126682281494, 'accumulated_submission_time': 55118.76144886017, 'accumulated_eval_time': 1902.1840479373932, 'accumulated_logging_time': 5.686068296432495, 'global_step': 161719, 'preemption_count': 0}), (163218, {'train/accuracy': 0.8958067297935486, 'train/loss': 0.3668033480644226, 'validation/accuracy': 0.7515599727630615, 'validation/loss': 1.0076985359191895, 'validation/num_examples': 50000, 'test/accuracy': 0.6283000111579895, 'test/loss': 1.6882383823394775, 'test/num_examples': 10000, 'score': 55628.83065366745, 'total_duration': 57559.539657115936, 'accumulated_submission_time': 55628.83065366745, 'accumulated_eval_time': 1919.415715456009, 'accumulated_logging_time': 5.746610403060913, 'global_step': 163218, 'preemption_count': 0}), (164717, {'train/accuracy': 0.89652419090271, 'train/loss': 0.35903504490852356, 'validation/accuracy': 0.7538999915122986, 'validation/loss': 0.9958780407905579, 'validation/num_examples': 50000, 'test/accuracy': 0.6323000192642212, 'test/loss': 1.692557692527771, 'test/num_examples': 10000, 'score': 56138.98541164398, 'total_duration': 58087.73439216614, 'accumulated_submission_time': 56138.98541164398, 'accumulated_eval_time': 1937.3459751605988, 'accumulated_logging_time': 5.80523681640625, 'global_step': 164717, 'preemption_count': 0}), (166216, {'train/accuracy': 0.9032605290412903, 'train/loss': 0.3413633108139038, 'validation/accuracy': 0.7542799711227417, 'validation/loss': 0.9981317520141602, 'validation/num_examples': 50000, 'test/accuracy': 0.6308000087738037, 'test/loss': 1.68972647190094, 'test/num_examples': 10000, 'score': 56649.082023859024, 'total_duration': 58615.33793616295, 'accumulated_submission_time': 56649.082023859024, 'accumulated_eval_time': 1954.7560713291168, 'accumulated_logging_time': 5.8507981300354, 'global_step': 166216, 'preemption_count': 0}), (167715, {'train/accuracy': 0.9057517647743225, 'train/loss': 0.3304113447666168, 'validation/accuracy': 0.755620002746582, 'validation/loss': 0.9929838180541992, 'validation/num_examples': 50000, 'test/accuracy': 0.6345000267028809, 'test/loss': 1.6859267950057983, 'test/num_examples': 10000, 'score': 57159.2556912899, 'total_duration': 59142.94081425667, 'accumulated_submission_time': 57159.2556912899, 'accumulated_eval_time': 1972.07869887352, 'accumulated_logging_time': 5.9076502323150635, 'global_step': 167715, 'preemption_count': 0}), (169213, {'train/accuracy': 0.920340359210968, 'train/loss': 0.28462982177734375, 'validation/accuracy': 0.7571199536323547, 'validation/loss': 0.9897215962409973, 'validation/num_examples': 50000, 'test/accuracy': 0.6319000124931335, 'test/loss': 1.6728354692459106, 'test/num_examples': 10000, 'score': 57669.26935172081, 'total_duration': 59670.21319055557, 'accumulated_submission_time': 57669.26935172081, 'accumulated_eval_time': 1989.2317397594452, 'accumulated_logging_time': 5.962466239929199, 'global_step': 169213, 'preemption_count': 0}), (170712, {'train/accuracy': 0.9207190275192261, 'train/loss': 0.2819788455963135, 'validation/accuracy': 0.7594199776649475, 'validation/loss': 0.9810996651649475, 'validation/num_examples': 50000, 'test/accuracy': 0.6370000243186951, 'test/loss': 1.660903811454773, 'test/num_examples': 10000, 'score': 58179.45411133766, 'total_duration': 60198.01121592522, 'accumulated_submission_time': 58179.45411133766, 'accumulated_eval_time': 2006.7366869449615, 'accumulated_logging_time': 6.018509387969971, 'global_step': 170712, 'preemption_count': 0}), (172210, {'train/accuracy': 0.9206393361091614, 'train/loss': 0.2826157808303833, 'validation/accuracy': 0.7601400017738342, 'validation/loss': 0.9788408875465393, 'validation/num_examples': 50000, 'test/accuracy': 0.6363000273704529, 'test/loss': 1.666724443435669, 'test/num_examples': 10000, 'score': 58689.38737034798, 'total_duration': 60725.549094200134, 'accumulated_submission_time': 58689.38737034798, 'accumulated_eval_time': 2024.2342946529388, 'accumulated_logging_time': 6.075444459915161, 'global_step': 172210, 'preemption_count': 0}), (173708, {'train/accuracy': 0.9241868257522583, 'train/loss': 0.26722291111946106, 'validation/accuracy': 0.7613199949264526, 'validation/loss': 0.9772453904151917, 'validation/num_examples': 50000, 'test/accuracy': 0.6378000378608704, 'test/loss': 1.6611794233322144, 'test/num_examples': 10000, 'score': 59199.27856183052, 'total_duration': 61252.9173913002, 'accumulated_submission_time': 59199.27856183052, 'accumulated_eval_time': 2041.5964758396149, 'accumulated_logging_time': 6.139222145080566, 'global_step': 173708, 'preemption_count': 0}), (175206, {'train/accuracy': 0.9243263602256775, 'train/loss': 0.2708885669708252, 'validation/accuracy': 0.7618199586868286, 'validation/loss': 0.9700082540512085, 'validation/num_examples': 50000, 'test/accuracy': 0.6360000371932983, 'test/loss': 1.6560876369476318, 'test/num_examples': 10000, 'score': 59709.26147675514, 'total_duration': 61780.24515080452, 'accumulated_submission_time': 59709.26147675514, 'accumulated_eval_time': 2058.8243272304535, 'accumulated_logging_time': 6.205310344696045, 'global_step': 175206, 'preemption_count': 0}), (176704, {'train/accuracy': 0.9294283986091614, 'train/loss': 0.25611138343811035, 'validation/accuracy': 0.7628200054168701, 'validation/loss': 0.9697470664978027, 'validation/num_examples': 50000, 'test/accuracy': 0.6390000581741333, 'test/loss': 1.6528555154800415, 'test/num_examples': 10000, 'score': 60219.217061281204, 'total_duration': 62307.96381902695, 'accumulated_submission_time': 60219.217061281204, 'accumulated_eval_time': 2076.4793939590454, 'accumulated_logging_time': 6.263585090637207, 'global_step': 176704, 'preemption_count': 0}), (178203, {'train/accuracy': 0.9300462007522583, 'train/loss': 0.25256940722465515, 'validation/accuracy': 0.763759970664978, 'validation/loss': 0.9669691920280457, 'validation/num_examples': 50000, 'test/accuracy': 0.6402000188827515, 'test/loss': 1.652724266052246, 'test/num_examples': 10000, 'score': 60729.36058998108, 'total_duration': 62835.58871150017, 'accumulated_submission_time': 60729.36058998108, 'accumulated_eval_time': 2093.848112821579, 'accumulated_logging_time': 6.324261903762817, 'global_step': 178203, 'preemption_count': 0}), (179701, {'train/accuracy': 0.9334741234779358, 'train/loss': 0.24098944664001465, 'validation/accuracy': 0.7638599872589111, 'validation/loss': 0.9650241136550903, 'validation/num_examples': 50000, 'test/accuracy': 0.64410001039505, 'test/loss': 1.6483752727508545, 'test/num_examples': 10000, 'score': 61239.35639166832, 'total_duration': 63362.927340745926, 'accumulated_submission_time': 61239.35639166832, 'accumulated_eval_time': 2111.0798873901367, 'accumulated_logging_time': 6.385619401931763, 'global_step': 179701, 'preemption_count': 0}), (181199, {'train/accuracy': 0.9332548975944519, 'train/loss': 0.24204431474208832, 'validation/accuracy': 0.7646999955177307, 'validation/loss': 0.9629248380661011, 'validation/num_examples': 50000, 'test/accuracy': 0.6421000361442566, 'test/loss': 1.6437848806381226, 'test/num_examples': 10000, 'score': 61749.25453186035, 'total_duration': 63890.17459869385, 'accumulated_submission_time': 61749.25453186035, 'accumulated_eval_time': 2128.31769990921, 'accumulated_logging_time': 6.443438529968262, 'global_step': 181199, 'preemption_count': 0}), (182698, {'train/accuracy': 0.9331353306770325, 'train/loss': 0.24270980060100555, 'validation/accuracy': 0.7647599577903748, 'validation/loss': 0.9616653919219971, 'validation/num_examples': 50000, 'test/accuracy': 0.6443000435829163, 'test/loss': 1.6459656953811646, 'test/num_examples': 10000, 'score': 62259.378823280334, 'total_duration': 64417.625435590744, 'accumulated_submission_time': 62259.378823280334, 'accumulated_eval_time': 2145.5339555740356, 'accumulated_logging_time': 6.500751495361328, 'global_step': 182698, 'preemption_count': 0}), (184197, {'train/accuracy': 0.9338129758834839, 'train/loss': 0.24053217470645905, 'validation/accuracy': 0.7651599645614624, 'validation/loss': 0.9626170992851257, 'validation/num_examples': 50000, 'test/accuracy': 0.6438000202178955, 'test/loss': 1.6469836235046387, 'test/num_examples': 10000, 'score': 62769.5444791317, 'total_duration': 64945.14319252968, 'accumulated_submission_time': 62769.5444791317, 'accumulated_eval_time': 2162.7728395462036, 'accumulated_logging_time': 6.56220269203186, 'global_step': 184197, 'preemption_count': 0})], 'global_step': 184898}
I0129 01:25:26.385692 139822745589568 submission_runner.py:586] Timing: 63008.04301953316
I0129 01:25:26.385766 139822745589568 submission_runner.py:588] Total number of evals: 124
I0129 01:25:26.385808 139822745589568 submission_runner.py:589] ====================
I0129 01:25:26.385852 139822745589568 submission_runner.py:542] Using RNG seed 916031063
I0129 01:25:26.387195 139822745589568 submission_runner.py:551] --- Tuning run 4/5 ---
I0129 01:25:26.387293 139822745589568 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_1/imagenet_resnet_jax/trial_4.
I0129 01:25:26.388856 139822745589568 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_1/imagenet_resnet_jax/trial_4/hparams.json.
I0129 01:25:26.389632 139822745589568 submission_runner.py:206] Initializing dataset.
I0129 01:25:26.398321 139822745589568 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0129 01:25:26.407895 139822745589568 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0129 01:25:26.594009 139822745589568 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0129 01:25:27.472110 139822745589568 submission_runner.py:213] Initializing model.
I0129 01:25:33.232599 139822745589568 submission_runner.py:255] Initializing optimizer.
I0129 01:25:33.623706 139822745589568 submission_runner.py:262] Initializing metrics bundle.
I0129 01:25:33.623853 139822745589568 submission_runner.py:280] Initializing checkpoint and logger.
I0129 01:25:33.638644 139822745589568 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_1/imagenet_resnet_jax/trial_4 with prefix checkpoint_
I0129 01:25:33.638783 139822745589568 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_1/imagenet_resnet_jax/trial_4/meta_data_0.json.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0129 01:25:45.238051 139822745589568 logger_utils.py:220] Unable to record git information. Continuing without it.
I0129 01:25:56.638648 139822745589568 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_1/imagenet_resnet_jax/trial_4/flags_0.json.
I0129 01:25:56.643302 139822745589568 submission_runner.py:314] Starting training loop.
I0129 01:26:30.188364 139655626356480 logging_writer.py:48] [0] global_step=0, grad_norm=0.6531614065170288, loss=6.929165840148926
I0129 01:26:30.203024 139822745589568 spec.py:321] Evaluating on the training split.
I0129 01:26:36.364672 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 01:26:44.966994 139822745589568 spec.py:349] Evaluating on the test split.
I0129 01:26:47.561743 139822745589568 submission_runner.py:408] Time since start: 50.92s, 	Step: 1, 	{'train/accuracy': 0.0013352996902540326, 'train/loss': 6.91108512878418, 'validation/accuracy': 0.001019999966956675, 'validation/loss': 6.91091251373291, 'validation/num_examples': 50000, 'test/accuracy': 0.0012000000569969416, 'test/loss': 6.910790920257568, 'test/num_examples': 10000, 'score': 33.559484243392944, 'total_duration': 50.918386936187744, 'accumulated_submission_time': 33.559484243392944, 'accumulated_eval_time': 17.358678817749023, 'accumulated_logging_time': 0}
I0129 01:26:47.571433 139656666543872 logging_writer.py:48] [1] accumulated_eval_time=17.358679, accumulated_logging_time=0, accumulated_submission_time=33.559484, global_step=1, preemption_count=0, score=33.559484, test/accuracy=0.001200, test/loss=6.910791, test/num_examples=10000, total_duration=50.918387, train/accuracy=0.001335, train/loss=6.911085, validation/accuracy=0.001020, validation/loss=6.910913, validation/num_examples=50000
I0129 01:27:21.714459 139656834316032 logging_writer.py:48] [100] global_step=100, grad_norm=0.7672736048698425, loss=6.642425537109375
I0129 01:27:55.904365 139656666543872 logging_writer.py:48] [200] global_step=200, grad_norm=0.9619094133377075, loss=6.329228401184082
I0129 01:28:30.154170 139656834316032 logging_writer.py:48] [300] global_step=300, grad_norm=2.150749683380127, loss=5.940128326416016
I0129 01:29:04.368274 139656666543872 logging_writer.py:48] [400] global_step=400, grad_norm=2.4506921768188477, loss=5.6909708976745605
I0129 01:29:38.638772 139656834316032 logging_writer.py:48] [500] global_step=500, grad_norm=3.050664186477661, loss=5.570857048034668
I0129 01:30:12.879883 139656666543872 logging_writer.py:48] [600] global_step=600, grad_norm=2.929595470428467, loss=5.328566074371338
I0129 01:30:47.139679 139656834316032 logging_writer.py:48] [700] global_step=700, grad_norm=4.853056907653809, loss=5.263814449310303
I0129 01:31:21.405025 139656666543872 logging_writer.py:48] [800] global_step=800, grad_norm=2.3795197010040283, loss=5.0222249031066895
I0129 01:31:55.657105 139656834316032 logging_writer.py:48] [900] global_step=900, grad_norm=3.2171401977539062, loss=4.8368682861328125
I0129 01:32:29.908816 139656666543872 logging_writer.py:48] [1000] global_step=1000, grad_norm=3.671173334121704, loss=4.664506912231445
I0129 01:33:04.185177 139656834316032 logging_writer.py:48] [1100] global_step=1100, grad_norm=2.697385549545288, loss=4.601972579956055
I0129 01:33:38.387566 139656666543872 logging_writer.py:48] [1200] global_step=1200, grad_norm=3.6170825958251953, loss=4.3426513671875
I0129 01:34:12.598229 139656834316032 logging_writer.py:48] [1300] global_step=1300, grad_norm=3.189373016357422, loss=4.251278400421143
I0129 01:34:46.856238 139656666543872 logging_writer.py:48] [1400] global_step=1400, grad_norm=2.6716344356536865, loss=4.342503547668457
I0129 01:35:17.791576 139822745589568 spec.py:321] Evaluating on the training split.
I0129 01:35:24.045367 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 01:35:32.579481 139822745589568 spec.py:349] Evaluating on the test split.
I0129 01:35:35.146850 139822745589568 submission_runner.py:408] Time since start: 578.50s, 	Step: 1492, 	{'train/accuracy': 0.18839684128761292, 'train/loss': 4.257904052734375, 'validation/accuracy': 0.1703599989414215, 'validation/loss': 4.3842315673828125, 'validation/num_examples': 50000, 'test/accuracy': 0.1371000111103058, 'test/loss': 4.80158805847168, 'test/num_examples': 10000, 'score': 543.7184791564941, 'total_duration': 578.5034921169281, 'accumulated_submission_time': 543.7184791564941, 'accumulated_eval_time': 34.713916540145874, 'accumulated_logging_time': 0.019278764724731445}
I0129 01:35:35.167839 139655626356480 logging_writer.py:48] [1492] accumulated_eval_time=34.713917, accumulated_logging_time=0.019279, accumulated_submission_time=543.718479, global_step=1492, preemption_count=0, score=543.718479, test/accuracy=0.137100, test/loss=4.801588, test/num_examples=10000, total_duration=578.503492, train/accuracy=0.188397, train/loss=4.257904, validation/accuracy=0.170360, validation/loss=4.384232, validation/num_examples=50000
I0129 01:35:38.239295 139656297445120 logging_writer.py:48] [1500] global_step=1500, grad_norm=2.47636342048645, loss=4.028163909912109
I0129 01:36:12.406468 139655626356480 logging_writer.py:48] [1600] global_step=1600, grad_norm=3.1236650943756104, loss=3.778998851776123
I0129 01:36:46.581677 139656297445120 logging_writer.py:48] [1700] global_step=1700, grad_norm=3.5173118114471436, loss=3.856391429901123
I0129 01:37:20.790255 139655626356480 logging_writer.py:48] [1800] global_step=1800, grad_norm=2.0055863857269287, loss=3.7633652687072754
I0129 01:37:54.984968 139656297445120 logging_writer.py:48] [1900] global_step=1900, grad_norm=2.579998254776001, loss=3.5252137184143066
I0129 01:38:29.173577 139655626356480 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.2712091207504272, loss=3.6393353939056396
I0129 01:39:03.367078 139656297445120 logging_writer.py:48] [2100] global_step=2100, grad_norm=1.5412259101867676, loss=3.5908141136169434
I0129 01:39:37.652984 139655626356480 logging_writer.py:48] [2200] global_step=2200, grad_norm=1.7657289505004883, loss=3.4079437255859375
I0129 01:40:11.826376 139656297445120 logging_writer.py:48] [2300] global_step=2300, grad_norm=1.7299295663833618, loss=3.317142963409424
I0129 01:40:45.997972 139655626356480 logging_writer.py:48] [2400] global_step=2400, grad_norm=1.3506252765655518, loss=3.4638493061065674
I0129 01:41:20.157630 139656297445120 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.4858583211898804, loss=3.278376579284668
I0129 01:41:54.356258 139655626356480 logging_writer.py:48] [2600] global_step=2600, grad_norm=1.1777362823486328, loss=3.2358367443084717
I0129 01:42:28.518661 139656297445120 logging_writer.py:48] [2700] global_step=2700, grad_norm=1.3671923875808716, loss=3.292045831680298
I0129 01:43:02.677747 139655626356480 logging_writer.py:48] [2800] global_step=2800, grad_norm=1.2872809171676636, loss=3.145505428314209
I0129 01:43:36.829348 139656297445120 logging_writer.py:48] [2900] global_step=2900, grad_norm=1.0795953273773193, loss=3.1972219944000244
I0129 01:44:05.331654 139822745589568 spec.py:321] Evaluating on the training split.
I0129 01:44:11.520964 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 01:44:20.077938 139822745589568 spec.py:349] Evaluating on the test split.
I0129 01:44:22.555253 139822745589568 submission_runner.py:408] Time since start: 1105.91s, 	Step: 2985, 	{'train/accuracy': 0.35174185037612915, 'train/loss': 3.024991035461426, 'validation/accuracy': 0.32603999972343445, 'validation/loss': 3.193049192428589, 'validation/num_examples': 50000, 'test/accuracy': 0.24540001153945923, 'test/loss': 3.8982651233673096, 'test/num_examples': 10000, 'score': 1053.8195695877075, 'total_duration': 1105.9118909835815, 'accumulated_submission_time': 1053.8195695877075, 'accumulated_eval_time': 51.93747568130493, 'accumulated_logging_time': 0.04973602294921875}
I0129 01:44:22.572771 139656666543872 logging_writer.py:48] [2985] accumulated_eval_time=51.937476, accumulated_logging_time=0.049736, accumulated_submission_time=1053.819570, global_step=2985, preemption_count=0, score=1053.819570, test/accuracy=0.245400, test/loss=3.898265, test/num_examples=10000, total_duration=1105.911891, train/accuracy=0.351742, train/loss=3.024991, validation/accuracy=0.326040, validation/loss=3.193049, validation/num_examples=50000
I0129 01:44:28.048928 139656834316032 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.2225428819656372, loss=3.1042797565460205
I0129 01:45:02.166746 139656666543872 logging_writer.py:48] [3100] global_step=3100, grad_norm=1.1064202785491943, loss=2.9783363342285156
I0129 01:45:36.297181 139656834316032 logging_writer.py:48] [3200] global_step=3200, grad_norm=1.4637954235076904, loss=3.1736812591552734
I0129 01:46:10.488242 139656666543872 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.9831429719924927, loss=2.9504055976867676
I0129 01:46:44.638814 139656834316032 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.8614412546157837, loss=2.959700345993042
I0129 01:47:18.788309 139656666543872 logging_writer.py:48] [3500] global_step=3500, grad_norm=1.083771824836731, loss=2.9704477787017822
I0129 01:47:52.950238 139656834316032 logging_writer.py:48] [3600] global_step=3600, grad_norm=1.2643753290176392, loss=3.1491856575012207
I0129 01:48:27.123309 139656666543872 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.8330846428871155, loss=2.8697729110717773
I0129 01:49:01.268724 139656834316032 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.8952084183692932, loss=2.9641547203063965
I0129 01:49:35.452119 139656666543872 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.8604539036750793, loss=3.022597074508667
I0129 01:50:09.607491 139656834316032 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.9344420433044434, loss=3.072077512741089
I0129 01:50:43.742610 139656666543872 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.8714554309844971, loss=2.8348443508148193
I0129 01:51:17.895407 139656834316032 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.784740686416626, loss=2.8678436279296875
I0129 01:51:52.035467 139656666543872 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.7676903009414673, loss=2.809098243713379
I0129 01:52:26.187644 139656834316032 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.8886309862136841, loss=2.67452335357666
I0129 01:52:52.685638 139822745589568 spec.py:321] Evaluating on the training split.
I0129 01:52:58.974457 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 01:53:07.822935 139822745589568 spec.py:349] Evaluating on the test split.
I0129 01:53:10.383220 139822745589568 submission_runner.py:408] Time since start: 1633.74s, 	Step: 4479, 	{'train/accuracy': 0.409877210855484, 'train/loss': 2.6678028106689453, 'validation/accuracy': 0.3876599967479706, 'validation/loss': 2.80322003364563, 'validation/num_examples': 50000, 'test/accuracy': 0.2946000099182129, 'test/loss': 3.5155205726623535, 'test/num_examples': 10000, 'score': 1563.8707466125488, 'total_duration': 1633.739863872528, 'accumulated_submission_time': 1563.8707466125488, 'accumulated_eval_time': 69.63504076004028, 'accumulated_logging_time': 0.07636284828186035}
I0129 01:53:10.401450 139655617963776 logging_writer.py:48] [4479] accumulated_eval_time=69.635041, accumulated_logging_time=0.076363, accumulated_submission_time=1563.870747, global_step=4479, preemption_count=0, score=1563.870747, test/accuracy=0.294600, test/loss=3.515521, test/num_examples=10000, total_duration=1633.739864, train/accuracy=0.409877, train/loss=2.667803, validation/accuracy=0.387660, validation/loss=2.803220, validation/num_examples=50000
I0129 01:53:17.923000 139655626356480 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.800548255443573, loss=2.9177379608154297
I0129 01:53:52.008284 139655617963776 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.8445045948028564, loss=2.7644433975219727
I0129 01:54:26.102408 139655626356480 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.7999932765960693, loss=2.625089645385742
I0129 01:55:00.246891 139655617963776 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.8388117551803589, loss=3.0957019329071045
I0129 01:55:34.403485 139655626356480 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.7327355146408081, loss=2.586937427520752
I0129 01:56:08.530879 139655617963776 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.8791404366493225, loss=2.7048375606536865
I0129 01:56:42.642345 139655626356480 logging_writer.py:48] [5100] global_step=5100, grad_norm=1.2046188116073608, loss=2.692328453063965
I0129 01:57:16.753949 139655617963776 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.8189364075660706, loss=2.6396284103393555
I0129 01:57:50.892187 139655626356480 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.8335248231887817, loss=2.644787549972534
I0129 01:58:25.004199 139655617963776 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.8569058775901794, loss=2.6000189781188965
I0129 01:58:59.163655 139655626356480 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.9120422005653381, loss=2.5686326026916504
I0129 01:59:33.398429 139655617963776 logging_writer.py:48] [5600] global_step=5600, grad_norm=1.1774696111679077, loss=2.745835781097412
I0129 02:00:07.540787 139655626356480 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.8829196095466614, loss=2.5345828533172607
I0129 02:00:41.663171 139655617963776 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.7978886961936951, loss=2.60581111907959
I0129 02:01:15.791858 139655626356480 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.8083541393280029, loss=2.7050721645355225
I0129 02:01:40.497145 139822745589568 spec.py:321] Evaluating on the training split.
I0129 02:01:46.723392 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 02:01:55.488122 139822745589568 spec.py:349] Evaluating on the test split.
I0129 02:01:57.934650 139822745589568 submission_runner.py:408] Time since start: 2161.29s, 	Step: 5974, 	{'train/accuracy': 0.4144810140132904, 'train/loss': 2.693990468978882, 'validation/accuracy': 0.35273998975753784, 'validation/loss': 3.0620031356811523, 'validation/num_examples': 50000, 'test/accuracy': 0.27010002732276917, 'test/loss': 3.7617311477661133, 'test/num_examples': 10000, 'score': 2073.9059176445007, 'total_duration': 2161.2912888526917, 'accumulated_submission_time': 2073.9059176445007, 'accumulated_eval_time': 87.07250952720642, 'accumulated_logging_time': 0.10388541221618652}
I0129 02:01:57.953088 139656658151168 logging_writer.py:48] [5974] accumulated_eval_time=87.072510, accumulated_logging_time=0.103885, accumulated_submission_time=2073.905918, global_step=5974, preemption_count=0, score=2073.905918, test/accuracy=0.270100, test/loss=3.761731, test/num_examples=10000, total_duration=2161.291289, train/accuracy=0.414481, train/loss=2.693990, validation/accuracy=0.352740, validation/loss=3.062003, validation/num_examples=50000
I0129 02:02:07.160228 139656666543872 logging_writer.py:48] [6000] global_step=6000, grad_norm=1.2197915315628052, loss=2.644573211669922
I0129 02:02:41.238571 139656658151168 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.9673020243644714, loss=2.6515352725982666
I0129 02:03:15.336193 139656666543872 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.9506726264953613, loss=2.609985113143921
I0129 02:03:49.421725 139656658151168 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.8169944882392883, loss=2.564077377319336
I0129 02:04:23.525830 139656666543872 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.9130039811134338, loss=2.659442663192749
I0129 02:04:57.653028 139656658151168 logging_writer.py:48] [6500] global_step=6500, grad_norm=1.0228710174560547, loss=2.605381488800049
I0129 02:05:31.758587 139656666543872 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.9433238506317139, loss=2.6134986877441406
I0129 02:06:05.967942 139656658151168 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.8961299657821655, loss=2.5704476833343506
I0129 02:06:40.110610 139656666543872 logging_writer.py:48] [6800] global_step=6800, grad_norm=1.0850704908370972, loss=2.4734227657318115
I0129 02:07:14.211568 139656658151168 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.9202173948287964, loss=2.5919928550720215
I0129 02:07:48.325577 139656666543872 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.9810008406639099, loss=2.52160382270813
I0129 02:08:22.445052 139656658151168 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.9155332446098328, loss=2.6054890155792236
I0129 02:08:56.568616 139656666543872 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.9353386759757996, loss=2.608058452606201
I0129 02:09:30.678937 139656658151168 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.9809895157814026, loss=2.5992579460144043
I0129 02:10:04.809313 139656666543872 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.8069614768028259, loss=2.4749491214752197
I0129 02:10:28.143063 139822745589568 spec.py:321] Evaluating on the training split.
I0129 02:10:34.634630 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 02:10:43.093952 139822745589568 spec.py:349] Evaluating on the test split.
I0129 02:10:45.678493 139822745589568 submission_runner.py:408] Time since start: 2689.04s, 	Step: 7470, 	{'train/accuracy': 0.3179607689380646, 'train/loss': 3.236488103866577, 'validation/accuracy': 0.29165998101234436, 'validation/loss': 3.4889159202575684, 'validation/num_examples': 50000, 'test/accuracy': 0.2379000186920166, 'test/loss': 4.055367946624756, 'test/num_examples': 10000, 'score': 2584.0340342521667, 'total_duration': 2689.035103559494, 'accumulated_submission_time': 2584.0340342521667, 'accumulated_eval_time': 104.60787105560303, 'accumulated_logging_time': 0.1320948600769043}
I0129 02:10:45.699414 139655626356480 logging_writer.py:48] [7470] accumulated_eval_time=104.607871, accumulated_logging_time=0.132095, accumulated_submission_time=2584.034034, global_step=7470, preemption_count=0, score=2584.034034, test/accuracy=0.237900, test/loss=4.055368, test/num_examples=10000, total_duration=2689.035104, train/accuracy=0.317961, train/loss=3.236488, validation/accuracy=0.291660, validation/loss=3.488916, validation/num_examples=50000
I0129 02:10:56.261853 139656297445120 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.9715965390205383, loss=2.5671839714050293
I0129 02:11:30.303435 139655626356480 logging_writer.py:48] [7600] global_step=7600, grad_norm=1.032487154006958, loss=2.623070240020752
I0129 02:12:04.378093 139656297445120 logging_writer.py:48] [7700] global_step=7700, grad_norm=1.053249478340149, loss=2.6335599422454834
I0129 02:12:38.525230 139655626356480 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.9330551624298096, loss=2.391418218612671
I0129 02:13:12.618503 139656297445120 logging_writer.py:48] [7900] global_step=7900, grad_norm=1.2146415710449219, loss=2.6135165691375732
I0129 02:13:46.705846 139655626356480 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.9066014289855957, loss=2.5654964447021484
I0129 02:14:20.795272 139656297445120 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.9149792194366455, loss=2.525822639465332
I0129 02:14:54.871216 139655626356480 logging_writer.py:48] [8200] global_step=8200, grad_norm=1.1076830625534058, loss=2.5354552268981934
I0129 02:15:28.929155 139656297445120 logging_writer.py:48] [8300] global_step=8300, grad_norm=1.0641744136810303, loss=2.4826436042785645
I0129 02:16:03.018254 139655626356480 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.8552061319351196, loss=2.405477523803711
I0129 02:16:37.099600 139656297445120 logging_writer.py:48] [8500] global_step=8500, grad_norm=1.0009865760803223, loss=2.5888073444366455
I0129 02:17:11.192979 139655626356480 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.9973143935203552, loss=2.502913236618042
I0129 02:17:45.281805 139656297445120 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.8354661464691162, loss=2.410374641418457
I0129 02:18:19.324558 139655626356480 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.9320057034492493, loss=2.523291826248169
I0129 02:18:53.569979 139656297445120 logging_writer.py:48] [8900] global_step=8900, grad_norm=1.0251569747924805, loss=2.458500623703003
I0129 02:19:15.870598 139822745589568 spec.py:321] Evaluating on the training split.
I0129 02:19:22.053236 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 02:19:30.588179 139822745589568 spec.py:349] Evaluating on the test split.
I0129 02:19:33.181187 139822745589568 submission_runner.py:408] Time since start: 3216.54s, 	Step: 8967, 	{'train/accuracy': 0.3252750337123871, 'train/loss': 3.3958663940429688, 'validation/accuracy': 0.30667999386787415, 'validation/loss': 3.4880292415618896, 'validation/num_examples': 50000, 'test/accuracy': 0.2201000154018402, 'test/loss': 4.388706684112549, 'test/num_examples': 10000, 'score': 3094.1442699432373, 'total_duration': 3216.537830352783, 'accumulated_submission_time': 3094.1442699432373, 'accumulated_eval_time': 121.91842865943909, 'accumulated_logging_time': 0.16176819801330566}
I0129 02:19:33.199785 139655609571072 logging_writer.py:48] [8967] accumulated_eval_time=121.918429, accumulated_logging_time=0.161768, accumulated_submission_time=3094.144270, global_step=8967, preemption_count=0, score=3094.144270, test/accuracy=0.220100, test/loss=4.388707, test/num_examples=10000, total_duration=3216.537830, train/accuracy=0.325275, train/loss=3.395866, validation/accuracy=0.306680, validation/loss=3.488029, validation/num_examples=50000
I0129 02:19:44.799541 139655617963776 logging_writer.py:48] [9000] global_step=9000, grad_norm=1.0528734922409058, loss=2.605372667312622
I0129 02:20:18.814211 139655609571072 logging_writer.py:48] [9100] global_step=9100, grad_norm=1.0032912492752075, loss=2.4487853050231934
I0129 02:20:52.885565 139655617963776 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.9162325859069824, loss=2.4454054832458496
I0129 02:21:26.963644 139655609571072 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.9546389579772949, loss=2.4373531341552734
I0129 02:22:01.034924 139655617963776 logging_writer.py:48] [9400] global_step=9400, grad_norm=1.0700043439865112, loss=2.4126827716827393
I0129 02:22:35.154820 139655609571072 logging_writer.py:48] [9500] global_step=9500, grad_norm=1.054086446762085, loss=2.561164140701294
I0129 02:23:09.250712 139655617963776 logging_writer.py:48] [9600] global_step=9600, grad_norm=1.075449824333191, loss=2.459257125854492
I0129 02:23:43.343458 139655609571072 logging_writer.py:48] [9700] global_step=9700, grad_norm=1.0643420219421387, loss=2.480076551437378
I0129 02:24:17.405763 139655617963776 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.8825326561927795, loss=2.4512393474578857
I0129 02:24:51.463212 139655609571072 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.8696154952049255, loss=2.477778434753418
I0129 02:25:25.639155 139655617963776 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.9844834208488464, loss=2.473104476928711
I0129 02:25:59.699594 139655609571072 logging_writer.py:48] [10100] global_step=10100, grad_norm=1.1342425346374512, loss=2.5354065895080566
I0129 02:26:33.788180 139655617963776 logging_writer.py:48] [10200] global_step=10200, grad_norm=1.0168479681015015, loss=2.455665111541748
I0129 02:27:07.829939 139655609571072 logging_writer.py:48] [10300] global_step=10300, grad_norm=1.072449803352356, loss=2.4363412857055664
I0129 02:27:41.905581 139655617963776 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.9315239191055298, loss=2.4898643493652344
I0129 02:28:03.185645 139822745589568 spec.py:321] Evaluating on the training split.
I0129 02:28:09.345663 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 02:28:18.028521 139822745589568 spec.py:349] Evaluating on the test split.
I0129 02:28:20.584901 139822745589568 submission_runner.py:408] Time since start: 3743.94s, 	Step: 10464, 	{'train/accuracy': 0.22395168244838715, 'train/loss': 4.2264933586120605, 'validation/accuracy': 0.21407999098300934, 'validation/loss': 4.341734409332275, 'validation/num_examples': 50000, 'test/accuracy': 0.15730001032352448, 'test/loss': 5.09550666809082, 'test/num_examples': 10000, 'score': 3604.070210456848, 'total_duration': 3743.9415435791016, 'accumulated_submission_time': 3604.070210456848, 'accumulated_eval_time': 139.31764602661133, 'accumulated_logging_time': 0.18913674354553223}
I0129 02:28:20.603724 139656834316032 logging_writer.py:48] [10464] accumulated_eval_time=139.317646, accumulated_logging_time=0.189137, accumulated_submission_time=3604.070210, global_step=10464, preemption_count=0, score=3604.070210, test/accuracy=0.157300, test/loss=5.095507, test/num_examples=10000, total_duration=3743.941544, train/accuracy=0.223952, train/loss=4.226493, validation/accuracy=0.214080, validation/loss=4.341734, validation/num_examples=50000
I0129 02:28:33.186409 139658730145536 logging_writer.py:48] [10500] global_step=10500, grad_norm=1.1965742111206055, loss=2.405836582183838
I0129 02:29:07.201151 139656834316032 logging_writer.py:48] [10600] global_step=10600, grad_norm=1.034492015838623, loss=2.3867604732513428
I0129 02:29:41.203620 139658730145536 logging_writer.py:48] [10700] global_step=10700, grad_norm=1.1487444639205933, loss=2.401479482650757
I0129 02:30:15.264930 139656834316032 logging_writer.py:48] [10800] global_step=10800, grad_norm=1.0011661052703857, loss=2.495769739151001
I0129 02:30:49.313909 139658730145536 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.9147613048553467, loss=2.272968292236328
I0129 02:31:23.411383 139656834316032 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.8927260637283325, loss=2.3684890270233154
I0129 02:31:57.543830 139658730145536 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.9912310838699341, loss=2.526350975036621
I0129 02:32:31.602314 139656834316032 logging_writer.py:48] [11200] global_step=11200, grad_norm=1.0085499286651611, loss=2.4568347930908203
I0129 02:33:05.672726 139658730145536 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.9702430367469788, loss=2.397301197052002
I0129 02:33:39.704128 139656834316032 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.9992648959159851, loss=2.4047765731811523
I0129 02:34:13.771831 139658730145536 logging_writer.py:48] [11500] global_step=11500, grad_norm=1.2988390922546387, loss=2.458796262741089
I0129 02:34:47.821558 139656834316032 logging_writer.py:48] [11600] global_step=11600, grad_norm=1.0107858180999756, loss=2.4289608001708984
I0129 02:35:21.871220 139658730145536 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.9093748927116394, loss=2.278014659881592
I0129 02:35:55.948545 139656834316032 logging_writer.py:48] [11800] global_step=11800, grad_norm=1.0113859176635742, loss=2.3645107746124268
I0129 02:36:30.034075 139658730145536 logging_writer.py:48] [11900] global_step=11900, grad_norm=1.0279080867767334, loss=2.406562328338623
I0129 02:36:50.639835 139822745589568 spec.py:321] Evaluating on the training split.
I0129 02:36:56.795660 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 02:37:05.487287 139822745589568 spec.py:349] Evaluating on the test split.
I0129 02:37:08.677138 139822745589568 submission_runner.py:408] Time since start: 4272.03s, 	Step: 11962, 	{'train/accuracy': 0.3482939898967743, 'train/loss': 3.1312360763549805, 'validation/accuracy': 0.32655999064445496, 'validation/loss': 3.3243465423583984, 'validation/num_examples': 50000, 'test/accuracy': 0.24170000851154327, 'test/loss': 4.087606906890869, 'test/num_examples': 10000, 'score': 4114.044556617737, 'total_duration': 4272.033766746521, 'accumulated_submission_time': 4114.044556617737, 'accumulated_eval_time': 157.3548982143402, 'accumulated_logging_time': 0.2187190055847168}
I0129 02:37:08.698632 139655617963776 logging_writer.py:48] [11962] accumulated_eval_time=157.354898, accumulated_logging_time=0.218719, accumulated_submission_time=4114.044557, global_step=11962, preemption_count=0, score=4114.044557, test/accuracy=0.241700, test/loss=4.087607, test/num_examples=10000, total_duration=4272.033767, train/accuracy=0.348294, train/loss=3.131236, validation/accuracy=0.326560, validation/loss=3.324347, validation/num_examples=50000
I0129 02:37:21.961303 139655626356480 logging_writer.py:48] [12000] global_step=12000, grad_norm=1.0592892169952393, loss=2.434351921081543
I0129 02:37:55.930418 139655617963776 logging_writer.py:48] [12100] global_step=12100, grad_norm=1.03706693649292, loss=2.42621111869812
I0129 02:38:30.148552 139655626356480 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.9478345513343811, loss=2.402090072631836
I0129 02:39:04.180196 139655617963776 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.944742739200592, loss=2.2974376678466797
I0129 02:39:38.245757 139655626356480 logging_writer.py:48] [12400] global_step=12400, grad_norm=1.1287715435028076, loss=2.421292781829834
I0129 02:40:12.287408 139655617963776 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.9678049087524414, loss=2.3939144611358643
I0129 02:40:46.345650 139655626356480 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.9994649887084961, loss=2.5259175300598145
I0129 02:41:20.414120 139655617963776 logging_writer.py:48] [12700] global_step=12700, grad_norm=1.0671075582504272, loss=2.409135341644287
I0129 02:41:54.457591 139655626356480 logging_writer.py:48] [12800] global_step=12800, grad_norm=1.0698424577713013, loss=2.444866418838501
I0129 02:42:28.517194 139655617963776 logging_writer.py:48] [12900] global_step=12900, grad_norm=1.0720455646514893, loss=2.4024434089660645
I0129 02:43:02.552196 139655626356480 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.8873104453086853, loss=2.3755717277526855
I0129 02:43:36.607854 139655617963776 logging_writer.py:48] [13100] global_step=13100, grad_norm=1.0001521110534668, loss=2.489532709121704
I0129 02:44:10.648534 139655626356480 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.9317785501480103, loss=2.4379403591156006
I0129 02:44:44.872879 139655617963776 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.9785647392272949, loss=2.4127166271209717
I0129 02:45:18.922325 139655626356480 logging_writer.py:48] [13400] global_step=13400, grad_norm=1.0037270784378052, loss=2.549367904663086
I0129 02:45:38.806677 139822745589568 spec.py:321] Evaluating on the training split.
I0129 02:45:44.991917 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 02:45:53.755990 139822745589568 spec.py:349] Evaluating on the test split.
I0129 02:45:56.334639 139822745589568 submission_runner.py:408] Time since start: 4799.69s, 	Step: 13460, 	{'train/accuracy': 0.2033442258834839, 'train/loss': 4.297685623168945, 'validation/accuracy': 0.19129998981952667, 'validation/loss': 4.41984748840332, 'validation/num_examples': 50000, 'test/accuracy': 0.14250001311302185, 'test/loss': 5.075582027435303, 'test/num_examples': 10000, 'score': 4624.091902256012, 'total_duration': 4799.691284656525, 'accumulated_submission_time': 4624.091902256012, 'accumulated_eval_time': 174.88282465934753, 'accumulated_logging_time': 0.24899768829345703}
I0129 02:45:56.358047 139656658151168 logging_writer.py:48] [13460] accumulated_eval_time=174.882825, accumulated_logging_time=0.248998, accumulated_submission_time=4624.091902, global_step=13460, preemption_count=0, score=4624.091902, test/accuracy=0.142500, test/loss=5.075582, test/num_examples=10000, total_duration=4799.691285, train/accuracy=0.203344, train/loss=4.297686, validation/accuracy=0.191300, validation/loss=4.419847, validation/num_examples=50000
I0129 02:46:10.302573 139656666543872 logging_writer.py:48] [13500] global_step=13500, grad_norm=1.0806843042373657, loss=2.456024169921875
I0129 02:46:44.280759 139656658151168 logging_writer.py:48] [13600] global_step=13600, grad_norm=1.000922441482544, loss=2.41347336769104
I0129 02:47:18.291932 139656666543872 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.9814068675041199, loss=2.2824859619140625
I0129 02:47:52.323322 139656658151168 logging_writer.py:48] [13800] global_step=13800, grad_norm=1.0038565397262573, loss=2.58243465423584
I0129 02:48:26.370498 139656666543872 logging_writer.py:48] [13900] global_step=13900, grad_norm=1.0071910619735718, loss=2.38783860206604
I0129 02:49:00.435485 139656658151168 logging_writer.py:48] [14000] global_step=14000, grad_norm=1.1890943050384521, loss=2.594449996948242
I0129 02:49:34.470155 139656666543872 logging_writer.py:48] [14100] global_step=14100, grad_norm=1.1118932962417603, loss=2.3351476192474365
I0129 02:50:08.507217 139656658151168 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.9732443690299988, loss=2.2987918853759766
I0129 02:50:42.513119 139656666543872 logging_writer.py:48] [14300] global_step=14300, grad_norm=1.0722721815109253, loss=2.43656849861145
I0129 02:51:16.740714 139656658151168 logging_writer.py:48] [14400] global_step=14400, grad_norm=1.0747098922729492, loss=2.403930187225342
I0129 02:51:50.772185 139656666543872 logging_writer.py:48] [14500] global_step=14500, grad_norm=1.0625735521316528, loss=2.3909590244293213
I0129 02:52:24.818743 139656658151168 logging_writer.py:48] [14600] global_step=14600, grad_norm=1.0258264541625977, loss=2.3547420501708984
I0129 02:52:58.848368 139656666543872 logging_writer.py:48] [14700] global_step=14700, grad_norm=1.0457760095596313, loss=2.444521188735962
I0129 02:53:32.848943 139656658151168 logging_writer.py:48] [14800] global_step=14800, grad_norm=1.0250434875488281, loss=2.3767194747924805
I0129 02:54:06.888525 139656666543872 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.971957802772522, loss=2.2411906719207764
I0129 02:54:26.430393 139822745589568 spec.py:321] Evaluating on the training split.
I0129 02:54:33.393647 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 02:54:41.929885 139822745589568 spec.py:349] Evaluating on the test split.
I0129 02:54:44.511474 139822745589568 submission_runner.py:408] Time since start: 5327.87s, 	Step: 14959, 	{'train/accuracy': 0.2525111436843872, 'train/loss': 3.8521933555603027, 'validation/accuracy': 0.2320999950170517, 'validation/loss': 4.034037113189697, 'validation/num_examples': 50000, 'test/accuracy': 0.1761000156402588, 'test/loss': 4.722900867462158, 'test/num_examples': 10000, 'score': 5134.104543209076, 'total_duration': 5327.868116140366, 'accumulated_submission_time': 5134.104543209076, 'accumulated_eval_time': 192.96386647224426, 'accumulated_logging_time': 0.28130149841308594}
I0129 02:54:44.531311 139655609571072 logging_writer.py:48] [14959] accumulated_eval_time=192.963866, accumulated_logging_time=0.281301, accumulated_submission_time=5134.104543, global_step=14959, preemption_count=0, score=5134.104543, test/accuracy=0.176100, test/loss=4.722901, test/num_examples=10000, total_duration=5327.868116, train/accuracy=0.252511, train/loss=3.852193, validation/accuracy=0.232100, validation/loss=4.034037, validation/num_examples=50000
I0129 02:54:58.816245 139655617963776 logging_writer.py:48] [15000] global_step=15000, grad_norm=1.004421353340149, loss=2.3047306537628174
I0129 02:55:32.795817 139655609571072 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.9911173582077026, loss=2.4578840732574463
I0129 02:56:06.813989 139655617963776 logging_writer.py:48] [15200] global_step=15200, grad_norm=1.033298373222351, loss=2.3742101192474365
I0129 02:56:40.853085 139655609571072 logging_writer.py:48] [15300] global_step=15300, grad_norm=1.2072845697402954, loss=2.3100709915161133
I0129 02:57:14.895599 139655617963776 logging_writer.py:48] [15400] global_step=15400, grad_norm=1.0340392589569092, loss=2.4702324867248535
I0129 02:57:49.037211 139655609571072 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.9565480947494507, loss=2.358137845993042
I0129 02:58:23.088573 139655617963776 logging_writer.py:48] [15600] global_step=15600, grad_norm=1.0496479272842407, loss=2.5377297401428223
I0129 02:58:57.101953 139655609571072 logging_writer.py:48] [15700] global_step=15700, grad_norm=1.0753275156021118, loss=2.4842798709869385
I0129 02:59:31.122625 139655617963776 logging_writer.py:48] [15800] global_step=15800, grad_norm=1.0877748727798462, loss=2.338700532913208
I0129 03:00:05.133546 139655609571072 logging_writer.py:48] [15900] global_step=15900, grad_norm=1.0704282522201538, loss=2.3652713298797607
I0129 03:00:39.134503 139655617963776 logging_writer.py:48] [16000] global_step=16000, grad_norm=1.0435965061187744, loss=2.468935966491699
I0129 03:01:13.127313 139655609571072 logging_writer.py:48] [16100] global_step=16100, grad_norm=1.0229008197784424, loss=2.273110866546631
I0129 03:01:47.126302 139655617963776 logging_writer.py:48] [16200] global_step=16200, grad_norm=1.0083568096160889, loss=2.3592793941497803
I0129 03:02:21.142469 139655609571072 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.9368559122085571, loss=2.207991123199463
I0129 03:02:55.171677 139655617963776 logging_writer.py:48] [16400] global_step=16400, grad_norm=1.0245023965835571, loss=2.3782715797424316
I0129 03:03:14.719158 139822745589568 spec.py:321] Evaluating on the training split.
I0129 03:03:20.980237 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 03:03:29.507390 139822745589568 spec.py:349] Evaluating on the test split.
I0129 03:03:32.158902 139822745589568 submission_runner.py:408] Time since start: 5855.52s, 	Step: 16459, 	{'train/accuracy': 0.1354631632566452, 'train/loss': 5.622068405151367, 'validation/accuracy': 0.11997999995946884, 'validation/loss': 5.786570072174072, 'validation/num_examples': 50000, 'test/accuracy': 0.08390000462532043, 'test/loss': 6.400252819061279, 'test/num_examples': 10000, 'score': 5644.231992006302, 'total_duration': 5855.515547513962, 'accumulated_submission_time': 5644.231992006302, 'accumulated_eval_time': 210.40357780456543, 'accumulated_logging_time': 0.3103921413421631}
I0129 03:03:32.178981 139656658151168 logging_writer.py:48] [16459] accumulated_eval_time=210.403578, accumulated_logging_time=0.310392, accumulated_submission_time=5644.231992, global_step=16459, preemption_count=0, score=5644.231992, test/accuracy=0.083900, test/loss=6.400253, test/num_examples=10000, total_duration=5855.515548, train/accuracy=0.135463, train/loss=5.622068, validation/accuracy=0.119980, validation/loss=5.786570, validation/num_examples=50000
I0129 03:03:46.448638 139656666543872 logging_writer.py:48] [16500] global_step=16500, grad_norm=1.0553950071334839, loss=2.3758225440979004
I0129 03:04:20.492546 139656658151168 logging_writer.py:48] [16600] global_step=16600, grad_norm=1.082922101020813, loss=2.3259522914886475
I0129 03:04:54.480704 139656666543872 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.9667780995368958, loss=2.3858416080474854
I0129 03:05:28.493445 139656658151168 logging_writer.py:48] [16800] global_step=16800, grad_norm=1.0407605171203613, loss=2.386791706085205
I0129 03:06:02.507096 139656666543872 logging_writer.py:48] [16900] global_step=16900, grad_norm=1.0122158527374268, loss=2.2943029403686523
I0129 03:06:36.550299 139656658151168 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.9903905987739563, loss=2.3675947189331055
I0129 03:07:10.576274 139656666543872 logging_writer.py:48] [17100] global_step=17100, grad_norm=1.1077824831008911, loss=2.4027795791625977
I0129 03:07:44.618222 139656658151168 logging_writer.py:48] [17200] global_step=17200, grad_norm=1.0960785150527954, loss=2.334458112716675
I0129 03:08:18.660482 139656666543872 logging_writer.py:48] [17300] global_step=17300, grad_norm=1.076730489730835, loss=2.281259298324585
I0129 03:08:52.652875 139656658151168 logging_writer.py:48] [17400] global_step=17400, grad_norm=1.142188310623169, loss=2.335319757461548
I0129 03:09:26.674485 139656666543872 logging_writer.py:48] [17500] global_step=17500, grad_norm=1.028883695602417, loss=2.378342628479004
I0129 03:10:00.680037 139656658151168 logging_writer.py:48] [17600] global_step=17600, grad_norm=1.0030232667922974, loss=2.3200199604034424
I0129 03:10:34.723361 139656666543872 logging_writer.py:48] [17700] global_step=17700, grad_norm=1.2920172214508057, loss=2.450674057006836
I0129 03:11:08.829382 139656658151168 logging_writer.py:48] [17800] global_step=17800, grad_norm=1.0251208543777466, loss=2.3878509998321533
I0129 03:11:42.853549 139656666543872 logging_writer.py:48] [17900] global_step=17900, grad_norm=1.0975407361984253, loss=2.413851737976074
I0129 03:12:02.364899 139822745589568 spec.py:321] Evaluating on the training split.
I0129 03:12:08.624716 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 03:12:17.280581 139822745589568 spec.py:349] Evaluating on the test split.
I0129 03:12:19.849873 139822745589568 submission_runner.py:408] Time since start: 6383.21s, 	Step: 17959, 	{'train/accuracy': 0.3703164756298065, 'train/loss': 2.9259555339813232, 'validation/accuracy': 0.34158000349998474, 'validation/loss': 3.1281611919403076, 'validation/num_examples': 50000, 'test/accuracy': 0.254800021648407, 'test/loss': 3.834587574005127, 'test/num_examples': 10000, 'score': 6154.353933811188, 'total_duration': 6383.206515073776, 'accumulated_submission_time': 6154.353933811188, 'accumulated_eval_time': 227.88853096961975, 'accumulated_logging_time': 0.34072422981262207}
I0129 03:12:19.870667 139655626356480 logging_writer.py:48] [17959] accumulated_eval_time=227.888531, accumulated_logging_time=0.340724, accumulated_submission_time=6154.353934, global_step=17959, preemption_count=0, score=6154.353934, test/accuracy=0.254800, test/loss=3.834588, test/num_examples=10000, total_duration=6383.206515, train/accuracy=0.370316, train/loss=2.925956, validation/accuracy=0.341580, validation/loss=3.128161, validation/num_examples=50000
I0129 03:12:34.133165 139656297445120 logging_writer.py:48] [18000] global_step=18000, grad_norm=1.023582935333252, loss=2.4943974018096924
I0129 03:13:08.087414 139655626356480 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.9583504796028137, loss=2.300909996032715
I0129 03:13:42.076001 139656297445120 logging_writer.py:48] [18200] global_step=18200, grad_norm=1.0700734853744507, loss=2.314350128173828
I0129 03:14:16.057099 139655626356480 logging_writer.py:48] [18300] global_step=18300, grad_norm=1.2153666019439697, loss=2.3392813205718994
I0129 03:14:50.051865 139656297445120 logging_writer.py:48] [18400] global_step=18400, grad_norm=1.099434733390808, loss=2.4205482006073
I0129 03:15:24.048964 139655626356480 logging_writer.py:48] [18500] global_step=18500, grad_norm=1.18467116355896, loss=2.3489952087402344
I0129 03:15:58.030339 139656297445120 logging_writer.py:48] [18600] global_step=18600, grad_norm=1.2123093605041504, loss=2.3504583835601807
I0129 03:16:32.038310 139655626356480 logging_writer.py:48] [18700] global_step=18700, grad_norm=1.0074126720428467, loss=2.2672438621520996
I0129 03:17:06.048390 139656297445120 logging_writer.py:48] [18800] global_step=18800, grad_norm=1.0114104747772217, loss=2.4785494804382324
I0129 03:17:40.116602 139655626356480 logging_writer.py:48] [18900] global_step=18900, grad_norm=1.0030303001403809, loss=2.3643221855163574
I0129 03:18:14.140522 139656297445120 logging_writer.py:48] [19000] global_step=19000, grad_norm=1.0881717205047607, loss=2.460956573486328
I0129 03:18:48.155759 139655626356480 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.924034059047699, loss=2.359936237335205
I0129 03:19:22.169788 139656297445120 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.9831620454788208, loss=2.443601131439209
I0129 03:19:56.179576 139655626356480 logging_writer.py:48] [19300] global_step=19300, grad_norm=1.0484243631362915, loss=2.4014179706573486
I0129 03:20:30.203529 139656297445120 logging_writer.py:48] [19400] global_step=19400, grad_norm=1.0559896230697632, loss=2.5640861988067627
I0129 03:20:50.086995 139822745589568 spec.py:321] Evaluating on the training split.
I0129 03:20:56.236479 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 03:21:05.040189 139822745589568 spec.py:349] Evaluating on the test split.
I0129 03:21:07.753435 139822745589568 submission_runner.py:408] Time since start: 6911.11s, 	Step: 19460, 	{'train/accuracy': 0.24842554330825806, 'train/loss': 4.295435428619385, 'validation/accuracy': 0.22613999247550964, 'validation/loss': 4.466485023498535, 'validation/num_examples': 50000, 'test/accuracy': 0.1698000133037567, 'test/loss': 5.243460178375244, 'test/num_examples': 10000, 'score': 6664.5083796978, 'total_duration': 6911.110089302063, 'accumulated_submission_time': 6664.5083796978, 'accumulated_eval_time': 245.55494594573975, 'accumulated_logging_time': 0.3720059394836426}
I0129 03:21:07.771822 139656658151168 logging_writer.py:48] [19460] accumulated_eval_time=245.554946, accumulated_logging_time=0.372006, accumulated_submission_time=6664.508380, global_step=19460, preemption_count=0, score=6664.508380, test/accuracy=0.169800, test/loss=5.243460, test/num_examples=10000, total_duration=6911.110089, train/accuracy=0.248426, train/loss=4.295435, validation/accuracy=0.226140, validation/loss=4.466485, validation/num_examples=50000
I0129 03:21:21.707700 139656666543872 logging_writer.py:48] [19500] global_step=19500, grad_norm=1.0958189964294434, loss=2.4174880981445312
I0129 03:21:55.692642 139656658151168 logging_writer.py:48] [19600] global_step=19600, grad_norm=1.0848385095596313, loss=2.347168445587158
I0129 03:22:29.643027 139656666543872 logging_writer.py:48] [19700] global_step=19700, grad_norm=1.029252290725708, loss=2.3539323806762695
I0129 03:23:03.616920 139656658151168 logging_writer.py:48] [19800] global_step=19800, grad_norm=1.055609107017517, loss=2.3300914764404297
I0129 03:23:37.608121 139656666543872 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.9803504943847656, loss=2.3512370586395264
I0129 03:24:11.646724 139656658151168 logging_writer.py:48] [20000] global_step=20000, grad_norm=1.1103720664978027, loss=2.3591597080230713
I0129 03:24:45.628016 139656666543872 logging_writer.py:48] [20100] global_step=20100, grad_norm=1.0187923908233643, loss=2.275850534439087
I0129 03:25:19.661077 139656658151168 logging_writer.py:48] [20200] global_step=20200, grad_norm=1.0408917665481567, loss=2.332272529602051
I0129 03:25:53.659941 139656666543872 logging_writer.py:48] [20300] global_step=20300, grad_norm=1.1018445491790771, loss=2.4572813510894775
I0129 03:26:27.665328 139656658151168 logging_writer.py:48] [20400] global_step=20400, grad_norm=1.0460481643676758, loss=2.319687843322754
I0129 03:27:01.642096 139656666543872 logging_writer.py:48] [20500] global_step=20500, grad_norm=1.0076284408569336, loss=2.2576186656951904
I0129 03:27:35.663091 139656658151168 logging_writer.py:48] [20600] global_step=20600, grad_norm=1.1202338933944702, loss=2.2684226036071777
I0129 03:28:09.631559 139656666543872 logging_writer.py:48] [20700] global_step=20700, grad_norm=1.018344759941101, loss=2.2760262489318848
I0129 03:28:43.614765 139656658151168 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.9924654364585876, loss=2.265580415725708
I0129 03:29:17.572435 139656666543872 logging_writer.py:48] [20900] global_step=20900, grad_norm=1.2234810590744019, loss=2.566918134689331
I0129 03:29:37.769311 139822745589568 spec.py:321] Evaluating on the training split.
I0129 03:29:44.016276 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 03:29:52.516014 139822745589568 spec.py:349] Evaluating on the test split.
I0129 03:29:55.067297 139822745589568 submission_runner.py:408] Time since start: 7438.42s, 	Step: 20961, 	{'train/accuracy': 0.2622568607330322, 'train/loss': 3.897775888442993, 'validation/accuracy': 0.25540000200271606, 'validation/loss': 3.955528974533081, 'validation/num_examples': 50000, 'test/accuracy': 0.18300001323223114, 'test/loss': 4.831855773925781, 'test/num_examples': 10000, 'score': 7174.447612285614, 'total_duration': 7438.4239411354065, 'accumulated_submission_time': 7174.447612285614, 'accumulated_eval_time': 262.85289573669434, 'accumulated_logging_time': 0.3986082077026367}
I0129 03:29:55.088297 139656297445120 logging_writer.py:48] [20961] accumulated_eval_time=262.852896, accumulated_logging_time=0.398608, accumulated_submission_time=7174.447612, global_step=20961, preemption_count=0, score=7174.447612, test/accuracy=0.183000, test/loss=4.831856, test/num_examples=10000, total_duration=7438.423941, train/accuracy=0.262257, train/loss=3.897776, validation/accuracy=0.255400, validation/loss=3.955529, validation/num_examples=50000
I0129 03:30:08.667780 139656649758464 logging_writer.py:48] [21000] global_step=21000, grad_norm=1.0441683530807495, loss=2.3402962684631348
I0129 03:30:42.697028 139656297445120 logging_writer.py:48] [21100] global_step=21100, grad_norm=1.0869299173355103, loss=2.3962464332580566
I0129 03:31:16.650764 139656649758464 logging_writer.py:48] [21200] global_step=21200, grad_norm=1.0795344114303589, loss=2.333848476409912
I0129 03:31:50.645372 139656297445120 logging_writer.py:48] [21300] global_step=21300, grad_norm=1.0573488473892212, loss=2.340604305267334
I0129 03:32:24.636836 139656649758464 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.9691243767738342, loss=2.3108389377593994
I0129 03:32:58.648354 139656297445120 logging_writer.py:48] [21500] global_step=21500, grad_norm=1.0627607107162476, loss=2.358419895172119
I0129 03:33:32.626152 139656649758464 logging_writer.py:48] [21600] global_step=21600, grad_norm=1.11469304561615, loss=2.481616497039795
I0129 03:34:06.626507 139656297445120 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.8777720928192139, loss=2.2577548027038574
I0129 03:34:40.580345 139656649758464 logging_writer.py:48] [21800] global_step=21800, grad_norm=1.1197261810302734, loss=2.394287347793579
I0129 03:35:14.569650 139656297445120 logging_writer.py:48] [21900] global_step=21900, grad_norm=1.1426242589950562, loss=2.4454710483551025
I0129 03:35:48.568500 139656649758464 logging_writer.py:48] [22000] global_step=22000, grad_norm=1.0616854429244995, loss=2.309694290161133
I0129 03:36:22.581695 139656297445120 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.9609121680259705, loss=2.2773797512054443
I0129 03:36:56.625474 139656649758464 logging_writer.py:48] [22200] global_step=22200, grad_norm=1.101539134979248, loss=2.3234920501708984
I0129 03:37:30.637395 139656297445120 logging_writer.py:48] [22300] global_step=22300, grad_norm=1.0673030614852905, loss=2.2845585346221924
I0129 03:38:04.615688 139656649758464 logging_writer.py:48] [22400] global_step=22400, grad_norm=1.0177022218704224, loss=2.366109848022461
I0129 03:38:25.159414 139822745589568 spec.py:321] Evaluating on the training split.
I0129 03:38:31.317374 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 03:38:39.907023 139822745589568 spec.py:349] Evaluating on the test split.
I0129 03:38:42.465978 139822745589568 submission_runner.py:408] Time since start: 7965.82s, 	Step: 22462, 	{'train/accuracy': 0.12033641338348389, 'train/loss': 5.798086643218994, 'validation/accuracy': 0.10941999405622482, 'validation/loss': 5.968713760375977, 'validation/num_examples': 50000, 'test/accuracy': 0.0861000046133995, 'test/loss': 6.424266338348389, 'test/num_examples': 10000, 'score': 7684.4579641819, 'total_duration': 7965.822619438171, 'accumulated_submission_time': 7684.4579641819, 'accumulated_eval_time': 280.15942215919495, 'accumulated_logging_time': 0.43025684356689453}
I0129 03:38:42.487663 139655626356480 logging_writer.py:48] [22462] accumulated_eval_time=280.159422, accumulated_logging_time=0.430257, accumulated_submission_time=7684.457964, global_step=22462, preemption_count=0, score=7684.457964, test/accuracy=0.086100, test/loss=6.424266, test/num_examples=10000, total_duration=7965.822619, train/accuracy=0.120336, train/loss=5.798087, validation/accuracy=0.109420, validation/loss=5.968714, validation/num_examples=50000
I0129 03:38:55.747570 139656297445120 logging_writer.py:48] [22500] global_step=22500, grad_norm=1.1969265937805176, loss=2.419527530670166
I0129 03:39:29.712877 139655626356480 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.9920916557312012, loss=2.2850332260131836
I0129 03:40:03.682789 139656297445120 logging_writer.py:48] [22700] global_step=22700, grad_norm=1.0874899625778198, loss=2.2524755001068115
I0129 03:40:37.664566 139655626356480 logging_writer.py:48] [22800] global_step=22800, grad_norm=1.0784378051757812, loss=2.342259645462036
I0129 03:41:11.665782 139656297445120 logging_writer.py:48] [22900] global_step=22900, grad_norm=1.02392578125, loss=2.2144341468811035
I0129 03:41:45.668497 139655626356480 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.9899042248725891, loss=2.3199005126953125
I0129 03:42:19.650320 139656297445120 logging_writer.py:48] [23100] global_step=23100, grad_norm=1.10896635055542, loss=2.3488144874572754
I0129 03:42:53.652415 139655626356480 logging_writer.py:48] [23200] global_step=23200, grad_norm=1.078218698501587, loss=2.392838954925537
I0129 03:43:27.737274 139656297445120 logging_writer.py:48] [23300] global_step=23300, grad_norm=1.1327993869781494, loss=2.3384194374084473
I0129 03:44:01.741286 139655626356480 logging_writer.py:48] [23400] global_step=23400, grad_norm=1.0671920776367188, loss=2.4189612865448
I0129 03:44:35.735805 139656297445120 logging_writer.py:48] [23500] global_step=23500, grad_norm=1.018040418624878, loss=2.3796401023864746
I0129 03:45:09.739252 139655626356480 logging_writer.py:48] [23600] global_step=23600, grad_norm=1.0864591598510742, loss=2.3171472549438477
I0129 03:45:43.743399 139656297445120 logging_writer.py:48] [23700] global_step=23700, grad_norm=1.0610716342926025, loss=2.3193774223327637
I0129 03:46:17.753239 139655626356480 logging_writer.py:48] [23800] global_step=23800, grad_norm=1.141523003578186, loss=2.3494536876678467
I0129 03:46:51.778414 139656297445120 logging_writer.py:48] [23900] global_step=23900, grad_norm=1.034214973449707, loss=2.296653985977173
I0129 03:47:12.651926 139822745589568 spec.py:321] Evaluating on the training split.
I0129 03:47:18.824483 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 03:47:27.320302 139822745589568 spec.py:349] Evaluating on the test split.
I0129 03:47:29.910621 139822745589568 submission_runner.py:408] Time since start: 8493.27s, 	Step: 23963, 	{'train/accuracy': 0.08017776906490326, 'train/loss': 6.423879146575928, 'validation/accuracy': 0.07471999526023865, 'validation/loss': 6.4704060554504395, 'validation/num_examples': 50000, 'test/accuracy': 0.055500004440546036, 'test/loss': 6.987226963043213, 'test/num_examples': 10000, 'score': 8194.56183886528, 'total_duration': 8493.26726603508, 'accumulated_submission_time': 8194.56183886528, 'accumulated_eval_time': 297.41808342933655, 'accumulated_logging_time': 0.4613358974456787}
I0129 03:47:29.934670 139656658151168 logging_writer.py:48] [23963] accumulated_eval_time=297.418083, accumulated_logging_time=0.461336, accumulated_submission_time=8194.561839, global_step=23963, preemption_count=0, score=8194.561839, test/accuracy=0.055500, test/loss=6.987227, test/num_examples=10000, total_duration=8493.267266, train/accuracy=0.080178, train/loss=6.423879, validation/accuracy=0.074720, validation/loss=6.470406, validation/num_examples=50000
I0129 03:47:42.806399 139656666543872 logging_writer.py:48] [24000] global_step=24000, grad_norm=1.201103687286377, loss=2.52522349357605
I0129 03:48:16.742337 139656658151168 logging_writer.py:48] [24100] global_step=24100, grad_norm=1.0756179094314575, loss=2.3493802547454834
I0129 03:48:50.724149 139656666543872 logging_writer.py:48] [24200] global_step=24200, grad_norm=1.0473636388778687, loss=2.333195447921753
I0129 03:49:24.722128 139656658151168 logging_writer.py:48] [24300] global_step=24300, grad_norm=1.0482988357543945, loss=2.321639060974121
I0129 03:49:58.778299 139656666543872 logging_writer.py:48] [24400] global_step=24400, grad_norm=1.0462558269500732, loss=2.3068649768829346
I0129 03:50:32.770627 139656658151168 logging_writer.py:48] [24500] global_step=24500, grad_norm=1.0739896297454834, loss=2.2838754653930664
I0129 03:51:06.768170 139656666543872 logging_writer.py:48] [24600] global_step=24600, grad_norm=1.1148836612701416, loss=2.2848453521728516
I0129 03:51:40.772539 139656658151168 logging_writer.py:48] [24700] global_step=24700, grad_norm=1.0609183311462402, loss=2.287201404571533
I0129 03:52:14.781857 139656666543872 logging_writer.py:48] [24800] global_step=24800, grad_norm=1.039042353630066, loss=2.2613518238067627
I0129 03:52:48.771022 139656658151168 logging_writer.py:48] [24900] global_step=24900, grad_norm=1.0272499322891235, loss=2.466120719909668
I0129 03:53:22.751100 139656666543872 logging_writer.py:48] [25000] global_step=25000, grad_norm=1.068396806716919, loss=2.3171768188476562
I0129 03:53:56.727103 139656658151168 logging_writer.py:48] [25100] global_step=25100, grad_norm=1.1079593896865845, loss=2.308849334716797
I0129 03:54:30.690450 139656666543872 logging_writer.py:48] [25200] global_step=25200, grad_norm=1.1252233982086182, loss=2.2589526176452637
I0129 03:55:04.708941 139656658151168 logging_writer.py:48] [25300] global_step=25300, grad_norm=1.045525312423706, loss=2.337599992752075
I0129 03:55:38.688786 139656666543872 logging_writer.py:48] [25400] global_step=25400, grad_norm=1.0896215438842773, loss=2.4404115676879883
I0129 03:55:59.922001 139822745589568 spec.py:321] Evaluating on the training split.
I0129 03:56:06.282941 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 03:56:15.249517 139822745589568 spec.py:349] Evaluating on the test split.
I0129 03:56:17.716739 139822745589568 submission_runner.py:408] Time since start: 9021.07s, 	Step: 25464, 	{'train/accuracy': 0.06762196868658066, 'train/loss': 8.211507797241211, 'validation/accuracy': 0.05689999833703041, 'validation/loss': 8.37997817993164, 'validation/num_examples': 50000, 'test/accuracy': 0.040800001472234726, 'test/loss': 9.22226333618164, 'test/num_examples': 10000, 'score': 8704.487569570541, 'total_duration': 9021.073375463486, 'accumulated_submission_time': 8704.487569570541, 'accumulated_eval_time': 315.2127788066864, 'accumulated_logging_time': 0.494720458984375}
I0129 03:56:17.741976 139655626356480 logging_writer.py:48] [25464] accumulated_eval_time=315.212779, accumulated_logging_time=0.494720, accumulated_submission_time=8704.487570, global_step=25464, preemption_count=0, score=8704.487570, test/accuracy=0.040800, test/loss=9.222263, test/num_examples=10000, total_duration=9021.073375, train/accuracy=0.067622, train/loss=8.211508, validation/accuracy=0.056900, validation/loss=8.379978, validation/num_examples=50000
I0129 03:56:30.302245 139656297445120 logging_writer.py:48] [25500] global_step=25500, grad_norm=1.0429706573486328, loss=2.2393651008605957
I0129 03:57:04.244075 139655626356480 logging_writer.py:48] [25600] global_step=25600, grad_norm=1.0338160991668701, loss=2.3827714920043945
I0129 03:57:38.229520 139656297445120 logging_writer.py:48] [25700] global_step=25700, grad_norm=1.0657950639724731, loss=2.3631412982940674
I0129 03:58:12.217614 139655626356480 logging_writer.py:48] [25800] global_step=25800, grad_norm=1.138355016708374, loss=2.3083977699279785
I0129 03:58:46.206353 139656297445120 logging_writer.py:48] [25900] global_step=25900, grad_norm=1.1272019147872925, loss=2.5028884410858154
I0129 03:59:20.181860 139655626356480 logging_writer.py:48] [26000] global_step=26000, grad_norm=1.1274524927139282, loss=2.207937240600586
I0129 03:59:54.179900 139656297445120 logging_writer.py:48] [26100] global_step=26100, grad_norm=1.0108884572982788, loss=2.3485283851623535
I0129 04:00:28.169041 139655626356480 logging_writer.py:48] [26200] global_step=26200, grad_norm=1.0901038646697998, loss=2.37239408493042
I0129 04:01:02.159828 139656297445120 logging_writer.py:48] [26300] global_step=26300, grad_norm=1.1955363750457764, loss=2.348517894744873
I0129 04:01:36.117898 139655626356480 logging_writer.py:48] [26400] global_step=26400, grad_norm=1.0522304773330688, loss=2.406735420227051
I0129 04:02:10.087924 139656297445120 logging_writer.py:48] [26500] global_step=26500, grad_norm=1.044623851776123, loss=2.2671592235565186
I0129 04:02:44.221045 139655626356480 logging_writer.py:48] [26600] global_step=26600, grad_norm=1.177783727645874, loss=2.2046942710876465
I0129 04:03:18.202108 139656297445120 logging_writer.py:48] [26700] global_step=26700, grad_norm=1.0616899728775024, loss=2.2123403549194336
I0129 04:03:52.173976 139655626356480 logging_writer.py:48] [26800] global_step=26800, grad_norm=1.2047879695892334, loss=2.3891749382019043
I0129 04:04:26.142373 139656297445120 logging_writer.py:48] [26900] global_step=26900, grad_norm=1.1343433856964111, loss=2.296600580215454
I0129 04:04:47.722538 139822745589568 spec.py:321] Evaluating on the training split.
I0129 04:04:53.926308 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 04:05:02.869749 139822745589568 spec.py:349] Evaluating on the test split.
I0129 04:05:05.353661 139822745589568 submission_runner.py:408] Time since start: 9548.71s, 	Step: 26965, 	{'train/accuracy': 0.18293605744838715, 'train/loss': 4.7571492195129395, 'validation/accuracy': 0.17357999086380005, 'validation/loss': 4.842849254608154, 'validation/num_examples': 50000, 'test/accuracy': 0.11590000241994858, 'test/loss': 5.787625312805176, 'test/num_examples': 10000, 'score': 9214.40810918808, 'total_duration': 9548.710106372833, 'accumulated_submission_time': 9214.40810918808, 'accumulated_eval_time': 332.8436703681946, 'accumulated_logging_time': 0.5292325019836426}
I0129 04:05:05.375580 139655617963776 logging_writer.py:48] [26965] accumulated_eval_time=332.843670, accumulated_logging_time=0.529233, accumulated_submission_time=9214.408109, global_step=26965, preemption_count=0, score=9214.408109, test/accuracy=0.115900, test/loss=5.787625, test/num_examples=10000, total_duration=9548.710106, train/accuracy=0.182936, train/loss=4.757149, validation/accuracy=0.173580, validation/loss=4.842849, validation/num_examples=50000
I0129 04:05:17.572998 139655626356480 logging_writer.py:48] [27000] global_step=27000, grad_norm=1.1255238056182861, loss=2.5115833282470703
I0129 04:05:51.501389 139655617963776 logging_writer.py:48] [27100] global_step=27100, grad_norm=1.084218978881836, loss=2.414977788925171
I0129 04:06:25.437382 139655626356480 logging_writer.py:48] [27200] global_step=27200, grad_norm=1.1452478170394897, loss=2.435469150543213
I0129 04:06:59.404342 139655617963776 logging_writer.py:48] [27300] global_step=27300, grad_norm=1.0881290435791016, loss=2.224748134613037
I0129 04:07:33.387121 139655626356480 logging_writer.py:48] [27400] global_step=27400, grad_norm=1.1701163053512573, loss=2.2228317260742188
I0129 04:08:07.371015 139655617963776 logging_writer.py:48] [27500] global_step=27500, grad_norm=1.0418349504470825, loss=2.242295742034912
I0129 04:08:41.359951 139655626356480 logging_writer.py:48] [27600] global_step=27600, grad_norm=1.1107797622680664, loss=2.3031082153320312
I0129 04:09:15.435695 139655617963776 logging_writer.py:48] [27700] global_step=27700, grad_norm=1.2341089248657227, loss=2.2759363651275635
I0129 04:09:49.421807 139655626356480 logging_writer.py:48] [27800] global_step=27800, grad_norm=1.112113356590271, loss=2.352245330810547
I0129 04:10:23.377507 139655617963776 logging_writer.py:48] [27900] global_step=27900, grad_norm=1.0395011901855469, loss=2.3693504333496094
I0129 04:10:57.388751 139655626356480 logging_writer.py:48] [28000] global_step=28000, grad_norm=1.0961151123046875, loss=2.2818851470947266
I0129 04:11:31.345809 139655617963776 logging_writer.py:48] [28100] global_step=28100, grad_norm=1.1493507623672485, loss=2.2971749305725098
I0129 04:12:05.320706 139655626356480 logging_writer.py:48] [28200] global_step=28200, grad_norm=1.339797854423523, loss=2.235259532928467
I0129 04:12:39.286666 139655617963776 logging_writer.py:48] [28300] global_step=28300, grad_norm=1.1164963245391846, loss=2.24151873588562
I0129 04:13:13.284726 139655626356480 logging_writer.py:48] [28400] global_step=28400, grad_norm=1.209497332572937, loss=2.360618829727173
I0129 04:13:35.519571 139822745589568 spec.py:321] Evaluating on the training split.
I0129 04:13:41.706841 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 04:13:50.441748 139822745589568 spec.py:349] Evaluating on the test split.
I0129 04:13:52.988315 139822745589568 submission_runner.py:408] Time since start: 10076.34s, 	Step: 28467, 	{'train/accuracy': 0.1595384180545807, 'train/loss': 5.077648639678955, 'validation/accuracy': 0.14961999654769897, 'validation/loss': 5.2059807777404785, 'validation/num_examples': 50000, 'test/accuracy': 0.11750000715255737, 'test/loss': 5.716974258422852, 'test/num_examples': 10000, 'score': 9724.49069738388, 'total_duration': 10076.344948291779, 'accumulated_submission_time': 9724.49069738388, 'accumulated_eval_time': 350.31236839294434, 'accumulated_logging_time': 0.5601718425750732}
I0129 04:13:53.010455 139656658151168 logging_writer.py:48] [28467] accumulated_eval_time=350.312368, accumulated_logging_time=0.560172, accumulated_submission_time=9724.490697, global_step=28467, preemption_count=0, score=9724.490697, test/accuracy=0.117500, test/loss=5.716974, test/num_examples=10000, total_duration=10076.344948, train/accuracy=0.159538, train/loss=5.077649, validation/accuracy=0.149620, validation/loss=5.205981, validation/num_examples=50000
I0129 04:14:04.560374 139656834316032 logging_writer.py:48] [28500] global_step=28500, grad_norm=1.1729320287704468, loss=2.347578525543213
I0129 04:14:38.483791 139656658151168 logging_writer.py:48] [28600] global_step=28600, grad_norm=1.1692447662353516, loss=2.2219815254211426
I0129 04:15:12.420808 139656834316032 logging_writer.py:48] [28700] global_step=28700, grad_norm=1.0078279972076416, loss=2.2884578704833984
I0129 04:15:46.466748 139656658151168 logging_writer.py:48] [28800] global_step=28800, grad_norm=1.0887964963912964, loss=2.284714698791504
I0129 04:16:20.418589 139656834316032 logging_writer.py:48] [28900] global_step=28900, grad_norm=1.0713553428649902, loss=2.3631560802459717
I0129 04:16:54.342726 139656658151168 logging_writer.py:48] [29000] global_step=29000, grad_norm=1.401090145111084, loss=2.4258179664611816
I0129 04:17:28.303290 139656834316032 logging_writer.py:48] [29100] global_step=29100, grad_norm=1.0095912218093872, loss=2.1249468326568604
I0129 04:18:02.260405 139656658151168 logging_writer.py:48] [29200] global_step=29200, grad_norm=1.1331974267959595, loss=2.2567944526672363
I0129 04:18:36.208550 139656834316032 logging_writer.py:48] [29300] global_step=29300, grad_norm=1.0740883350372314, loss=2.359804391860962
I0129 04:19:10.163075 139656658151168 logging_writer.py:48] [29400] global_step=29400, grad_norm=1.1478283405303955, loss=2.369089126586914
I0129 04:19:44.127242 139656834316032 logging_writer.py:48] [29500] global_step=29500, grad_norm=1.1017369031906128, loss=2.3604044914245605
I0129 04:20:18.074748 139656658151168 logging_writer.py:48] [29600] global_step=29600, grad_norm=1.191780924797058, loss=2.288951873779297
I0129 04:20:52.057025 139656834316032 logging_writer.py:48] [29700] global_step=29700, grad_norm=1.3242781162261963, loss=2.282839059829712
I0129 04:21:26.044796 139656658151168 logging_writer.py:48] [29800] global_step=29800, grad_norm=1.2364552021026611, loss=2.2301716804504395
I0129 04:22:00.032481 139656834316032 logging_writer.py:48] [29900] global_step=29900, grad_norm=1.1356738805770874, loss=2.3158884048461914
I0129 04:22:23.005326 139822745589568 spec.py:321] Evaluating on the training split.
I0129 04:22:29.158865 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 04:22:37.769689 139822745589568 spec.py:349] Evaluating on the test split.
I0129 04:22:40.229591 139822745589568 submission_runner.py:408] Time since start: 10603.59s, 	Step: 29969, 	{'train/accuracy': 0.2542450428009033, 'train/loss': 3.912951707839966, 'validation/accuracy': 0.23863999545574188, 'validation/loss': 4.062727928161621, 'validation/num_examples': 50000, 'test/accuracy': 0.18040001392364502, 'test/loss': 4.745062351226807, 'test/num_examples': 10000, 'score': 10234.423271417618, 'total_duration': 10603.58622789383, 'accumulated_submission_time': 10234.423271417618, 'accumulated_eval_time': 367.536589384079, 'accumulated_logging_time': 0.5917963981628418}
I0129 04:22:40.253029 139656297445120 logging_writer.py:48] [29969] accumulated_eval_time=367.536589, accumulated_logging_time=0.591796, accumulated_submission_time=10234.423271, global_step=29969, preemption_count=0, score=10234.423271, test/accuracy=0.180400, test/loss=4.745062, test/num_examples=10000, total_duration=10603.586228, train/accuracy=0.254245, train/loss=3.912952, validation/accuracy=0.238640, validation/loss=4.062728, validation/num_examples=50000
I0129 04:22:51.133492 139656649758464 logging_writer.py:48] [30000] global_step=30000, grad_norm=1.132306694984436, loss=2.273892402648926
I0129 04:23:25.016997 139656297445120 logging_writer.py:48] [30100] global_step=30100, grad_norm=1.0881811380386353, loss=2.29125714302063
I0129 04:23:58.993110 139656649758464 logging_writer.py:48] [30200] global_step=30200, grad_norm=1.0647029876708984, loss=2.293773651123047
I0129 04:24:32.952481 139656297445120 logging_writer.py:48] [30300] global_step=30300, grad_norm=1.124870777130127, loss=2.227532386779785
I0129 04:25:06.919059 139656649758464 logging_writer.py:48] [30400] global_step=30400, grad_norm=1.0814003944396973, loss=2.2484288215637207
I0129 04:25:40.875389 139656297445120 logging_writer.py:48] [30500] global_step=30500, grad_norm=1.0965765714645386, loss=2.3326821327209473
I0129 04:26:14.854753 139656649758464 logging_writer.py:48] [30600] global_step=30600, grad_norm=1.0685057640075684, loss=2.3510050773620605
I0129 04:26:48.846608 139656297445120 logging_writer.py:48] [30700] global_step=30700, grad_norm=1.0224889516830444, loss=2.305633068084717
I0129 04:27:22.815089 139656649758464 logging_writer.py:48] [30800] global_step=30800, grad_norm=1.1891448497772217, loss=2.287428617477417
I0129 04:27:56.803150 139656297445120 logging_writer.py:48] [30900] global_step=30900, grad_norm=1.2230538129806519, loss=2.182257890701294
I0129 04:28:30.766112 139656649758464 logging_writer.py:48] [31000] global_step=31000, grad_norm=1.0610376596450806, loss=2.291520118713379
I0129 04:29:04.942691 139656297445120 logging_writer.py:48] [31100] global_step=31100, grad_norm=1.0747536420822144, loss=2.2953996658325195
I0129 04:29:38.886576 139656649758464 logging_writer.py:48] [31200] global_step=31200, grad_norm=1.1259403228759766, loss=2.2318294048309326
I0129 04:30:12.858781 139656297445120 logging_writer.py:48] [31300] global_step=31300, grad_norm=1.0851415395736694, loss=2.343545913696289
I0129 04:30:46.830431 139656649758464 logging_writer.py:48] [31400] global_step=31400, grad_norm=1.080151915550232, loss=2.328691244125366
I0129 04:31:10.409515 139822745589568 spec.py:321] Evaluating on the training split.
I0129 04:31:16.583184 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 04:31:25.072948 139822745589568 spec.py:349] Evaluating on the test split.
I0129 04:31:27.644003 139822745589568 submission_runner.py:408] Time since start: 11131.00s, 	Step: 31471, 	{'train/accuracy': 0.34287306666374207, 'train/loss': 3.108630418777466, 'validation/accuracy': 0.33239999413490295, 'validation/loss': 3.178079843521118, 'validation/num_examples': 50000, 'test/accuracy': 0.24860000610351562, 'test/loss': 3.8949763774871826, 'test/num_examples': 10000, 'score': 10744.5173869133, 'total_duration': 11131.000636100769, 'accumulated_submission_time': 10744.5173869133, 'accumulated_eval_time': 384.77102971076965, 'accumulated_logging_time': 0.6250383853912354}
I0129 04:31:27.667087 139655617963776 logging_writer.py:48] [31471] accumulated_eval_time=384.771030, accumulated_logging_time=0.625038, accumulated_submission_time=10744.517387, global_step=31471, preemption_count=0, score=10744.517387, test/accuracy=0.248600, test/loss=3.894976, test/num_examples=10000, total_duration=11131.000636, train/accuracy=0.342873, train/loss=3.108630, validation/accuracy=0.332400, validation/loss=3.178080, validation/num_examples=50000
I0129 04:31:37.825207 139655626356480 logging_writer.py:48] [31500] global_step=31500, grad_norm=1.194955825805664, loss=2.4207091331481934
I0129 04:32:11.735940 139655617963776 logging_writer.py:48] [31600] global_step=31600, grad_norm=1.2713922262191772, loss=2.4324238300323486
I0129 04:32:45.682031 139655626356480 logging_writer.py:48] [31700] global_step=31700, grad_norm=1.172061562538147, loss=2.3193483352661133
I0129 04:33:19.593215 139655617963776 logging_writer.py:48] [31800] global_step=31800, grad_norm=1.2224860191345215, loss=2.3728082180023193
I0129 04:33:53.567024 139655626356480 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.9686534404754639, loss=2.2828519344329834
I0129 04:34:27.478434 139655617963776 logging_writer.py:48] [32000] global_step=32000, grad_norm=1.136527419090271, loss=2.3910179138183594
I0129 04:35:01.441888 139655626356480 logging_writer.py:48] [32100] global_step=32100, grad_norm=1.0620472431182861, loss=2.2519307136535645
I0129 04:35:35.469361 139655617963776 logging_writer.py:48] [32200] global_step=32200, grad_norm=1.2064520120620728, loss=2.3559558391571045
I0129 04:36:09.429784 139655626356480 logging_writer.py:48] [32300] global_step=32300, grad_norm=1.215520977973938, loss=2.286057949066162
I0129 04:36:43.369849 139655617963776 logging_writer.py:48] [32400] global_step=32400, grad_norm=1.2585017681121826, loss=2.3353044986724854
I0129 04:37:17.323249 139655626356480 logging_writer.py:48] [32500] global_step=32500, grad_norm=1.1398805379867554, loss=2.261852264404297
I0129 04:37:51.280017 139655617963776 logging_writer.py:48] [32600] global_step=32600, grad_norm=1.2111108303070068, loss=2.2632670402526855
I0129 04:38:25.241750 139655626356480 logging_writer.py:48] [32700] global_step=32700, grad_norm=1.2244720458984375, loss=2.324138879776001
I0129 04:38:59.231394 139655617963776 logging_writer.py:48] [32800] global_step=32800, grad_norm=1.0493637323379517, loss=2.240015745162964
I0129 04:39:33.201305 139655626356480 logging_writer.py:48] [32900] global_step=32900, grad_norm=1.115354061126709, loss=2.32434344291687
I0129 04:39:57.794370 139822745589568 spec.py:321] Evaluating on the training split.
I0129 04:40:04.122271 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 04:40:12.722883 139822745589568 spec.py:349] Evaluating on the test split.
I0129 04:40:15.358726 139822745589568 submission_runner.py:408] Time since start: 11658.72s, 	Step: 32974, 	{'train/accuracy': 0.27313855290412903, 'train/loss': 4.065606594085693, 'validation/accuracy': 0.26225998997688293, 'validation/loss': 4.184144973754883, 'validation/num_examples': 50000, 'test/accuracy': 0.19510000944137573, 'test/loss': 5.094240665435791, 'test/num_examples': 10000, 'score': 11254.584214448929, 'total_duration': 11658.715369939804, 'accumulated_submission_time': 11254.584214448929, 'accumulated_eval_time': 402.33535385131836, 'accumulated_logging_time': 0.6580126285552979}
I0129 04:40:15.382215 139656658151168 logging_writer.py:48] [32974] accumulated_eval_time=402.335354, accumulated_logging_time=0.658013, accumulated_submission_time=11254.584214, global_step=32974, preemption_count=0, score=11254.584214, test/accuracy=0.195100, test/loss=5.094241, test/num_examples=10000, total_duration=11658.715370, train/accuracy=0.273139, train/loss=4.065607, validation/accuracy=0.262260, validation/loss=4.184145, validation/num_examples=50000
I0129 04:40:24.532081 139656666543872 logging_writer.py:48] [33000] global_step=33000, grad_norm=1.1065703630447388, loss=2.3107619285583496
I0129 04:40:58.443326 139656658151168 logging_writer.py:48] [33100] global_step=33100, grad_norm=1.073710322380066, loss=2.1663670539855957
I0129 04:41:32.394797 139656666543872 logging_writer.py:48] [33200] global_step=33200, grad_norm=1.2168488502502441, loss=2.255751609802246
I0129 04:42:06.452022 139656658151168 logging_writer.py:48] [33300] global_step=33300, grad_norm=1.1714587211608887, loss=2.3294930458068848
I0129 04:42:40.456371 139656666543872 logging_writer.py:48] [33400] global_step=33400, grad_norm=1.3667824268341064, loss=2.290468692779541
I0129 04:43:14.399800 139656658151168 logging_writer.py:48] [33500] global_step=33500, grad_norm=1.1177464723587036, loss=2.374390125274658
I0129 04:43:48.337755 139656666543872 logging_writer.py:48] [33600] global_step=33600, grad_norm=1.2016149759292603, loss=2.4573793411254883
I0129 04:44:22.311385 139656658151168 logging_writer.py:48] [33700] global_step=33700, grad_norm=1.283589482307434, loss=2.337158441543579
I0129 04:44:56.281186 139656666543872 logging_writer.py:48] [33800] global_step=33800, grad_norm=1.1096272468566895, loss=2.3137247562408447
I0129 04:45:30.213027 139656658151168 logging_writer.py:48] [33900] global_step=33900, grad_norm=1.1288291215896606, loss=2.3621602058410645
I0129 04:46:04.145978 139656666543872 logging_writer.py:48] [34000] global_step=34000, grad_norm=1.111343502998352, loss=2.316063404083252
I0129 04:46:38.129389 139656658151168 logging_writer.py:48] [34100] global_step=34100, grad_norm=1.1063379049301147, loss=2.4057302474975586
I0129 04:47:12.085543 139656666543872 logging_writer.py:48] [34200] global_step=34200, grad_norm=1.1510196924209595, loss=2.318462610244751
I0129 04:47:46.027625 139656658151168 logging_writer.py:48] [34300] global_step=34300, grad_norm=1.0558793544769287, loss=2.276076316833496
I0129 04:48:20.056924 139656666543872 logging_writer.py:48] [34400] global_step=34400, grad_norm=1.1195533275604248, loss=2.218066453933716
I0129 04:48:45.655686 139822745589568 spec.py:321] Evaluating on the training split.
I0129 04:48:51.877984 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 04:49:00.646192 139822745589568 spec.py:349] Evaluating on the test split.
I0129 04:49:03.241124 139822745589568 submission_runner.py:408] Time since start: 12186.60s, 	Step: 34477, 	{'train/accuracy': 0.1878587305545807, 'train/loss': 4.851592540740967, 'validation/accuracy': 0.17073999345302582, 'validation/loss': 5.058218955993652, 'validation/num_examples': 50000, 'test/accuracy': 0.11810000240802765, 'test/loss': 5.938706398010254, 'test/num_examples': 10000, 'score': 11764.796533107758, 'total_duration': 12186.59768295288, 'accumulated_submission_time': 11764.796533107758, 'accumulated_eval_time': 419.9206705093384, 'accumulated_logging_time': 0.6911814212799072}
I0129 04:49:03.267802 139655626356480 logging_writer.py:48] [34477] accumulated_eval_time=419.920671, accumulated_logging_time=0.691181, accumulated_submission_time=11764.796533, global_step=34477, preemption_count=0, score=11764.796533, test/accuracy=0.118100, test/loss=5.938706, test/num_examples=10000, total_duration=12186.597683, train/accuracy=0.187859, train/loss=4.851593, validation/accuracy=0.170740, validation/loss=5.058219, validation/num_examples=50000
I0129 04:49:11.422765 139656297445120 logging_writer.py:48] [34500] global_step=34500, grad_norm=1.2349207401275635, loss=2.2939658164978027
I0129 04:49:45.300681 139655626356480 logging_writer.py:48] [34600] global_step=34600, grad_norm=1.0807678699493408, loss=2.2342641353607178
I0129 04:50:19.216538 139656297445120 logging_writer.py:48] [34700] global_step=34700, grad_norm=1.239135503768921, loss=2.420342445373535
I0129 04:50:53.124456 139655626356480 logging_writer.py:48] [34800] global_step=34800, grad_norm=1.1398218870162964, loss=2.395862340927124
I0129 04:51:27.092612 139656297445120 logging_writer.py:48] [34900] global_step=34900, grad_norm=1.0702414512634277, loss=2.2546706199645996
I0129 04:52:00.994967 139655626356480 logging_writer.py:48] [35000] global_step=35000, grad_norm=1.117878794670105, loss=2.223398208618164
I0129 04:52:34.961403 139656297445120 logging_writer.py:48] [35100] global_step=35100, grad_norm=1.0851683616638184, loss=2.239427328109741
I0129 04:53:08.891197 139655626356480 logging_writer.py:48] [35200] global_step=35200, grad_norm=1.1717917919158936, loss=2.2329978942871094
I0129 04:53:42.855620 139656297445120 logging_writer.py:48] [35300] global_step=35300, grad_norm=1.1320147514343262, loss=2.2059950828552246
I0129 04:54:16.809497 139655626356480 logging_writer.py:48] [35400] global_step=35400, grad_norm=1.1641650199890137, loss=2.2094833850860596
I0129 04:54:50.837476 139656297445120 logging_writer.py:48] [35500] global_step=35500, grad_norm=1.2002850770950317, loss=2.318031072616577
I0129 04:55:24.783924 139655626356480 logging_writer.py:48] [35600] global_step=35600, grad_norm=1.363123893737793, loss=2.3571979999542236
I0129 04:55:58.751106 139656297445120 logging_writer.py:48] [35700] global_step=35700, grad_norm=1.1645705699920654, loss=2.2746329307556152
I0129 04:56:32.689549 139655626356480 logging_writer.py:48] [35800] global_step=35800, grad_norm=1.1014584302902222, loss=2.333866834640503
I0129 04:57:06.637588 139656297445120 logging_writer.py:48] [35900] global_step=35900, grad_norm=1.0807358026504517, loss=2.2847740650177
I0129 04:57:33.250867 139822745589568 spec.py:321] Evaluating on the training split.
I0129 04:57:39.539124 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 04:57:48.233131 139822745589568 spec.py:349] Evaluating on the test split.
I0129 04:57:50.727013 139822745589568 submission_runner.py:408] Time since start: 12714.08s, 	Step: 35980, 	{'train/accuracy': 0.24667170643806458, 'train/loss': 4.081879615783691, 'validation/accuracy': 0.231019988656044, 'validation/loss': 4.22337007522583, 'validation/num_examples': 50000, 'test/accuracy': 0.1688000112771988, 'test/loss': 4.942447185516357, 'test/num_examples': 10000, 'score': 12274.719363689423, 'total_duration': 12714.083587408066, 'accumulated_submission_time': 12274.719363689423, 'accumulated_eval_time': 437.3967123031616, 'accumulated_logging_time': 0.727304220199585}
I0129 04:57:50.754407 139655626356480 logging_writer.py:48] [35980] accumulated_eval_time=437.396712, accumulated_logging_time=0.727304, accumulated_submission_time=12274.719364, global_step=35980, preemption_count=0, score=12274.719364, test/accuracy=0.168800, test/loss=4.942447, test/num_examples=10000, total_duration=12714.083587, train/accuracy=0.246672, train/loss=4.081880, validation/accuracy=0.231020, validation/loss=4.223370, validation/num_examples=50000
I0129 04:57:57.880385 139656666543872 logging_writer.py:48] [36000] global_step=36000, grad_norm=1.2249493598937988, loss=2.3814873695373535
I0129 04:58:31.774813 139655626356480 logging_writer.py:48] [36100] global_step=36100, grad_norm=1.0725148916244507, loss=2.2955241203308105
I0129 04:59:05.714641 139656666543872 logging_writer.py:48] [36200] global_step=36200, grad_norm=1.1077066659927368, loss=2.111588954925537
I0129 04:59:39.650428 139655626356480 logging_writer.py:48] [36300] global_step=36300, grad_norm=1.1769611835479736, loss=2.2155215740203857
I0129 05:00:13.592353 139656666543872 logging_writer.py:48] [36400] global_step=36400, grad_norm=1.3114070892333984, loss=2.336484432220459
I0129 05:00:47.527391 139655626356480 logging_writer.py:48] [36500] global_step=36500, grad_norm=1.3198736906051636, loss=2.306014060974121
I0129 05:01:21.515000 139656666543872 logging_writer.py:48] [36600] global_step=36600, grad_norm=1.2778319120407104, loss=2.4906013011932373
I0129 05:01:55.430409 139655626356480 logging_writer.py:48] [36700] global_step=36700, grad_norm=1.2053712606430054, loss=2.2089414596557617
I0129 05:02:29.354106 139656666543872 logging_writer.py:48] [36800] global_step=36800, grad_norm=1.1391688585281372, loss=2.0756521224975586
I0129 05:03:03.327199 139655626356480 logging_writer.py:48] [36900] global_step=36900, grad_norm=1.1630254983901978, loss=2.254194736480713
I0129 05:03:37.246842 139656666543872 logging_writer.py:48] [37000] global_step=37000, grad_norm=1.1171175241470337, loss=2.315300941467285
I0129 05:04:11.180557 139655626356480 logging_writer.py:48] [37100] global_step=37100, grad_norm=1.1975460052490234, loss=2.2806127071380615
I0129 05:04:45.144720 139656666543872 logging_writer.py:48] [37200] global_step=37200, grad_norm=1.2846553325653076, loss=2.2355051040649414
I0129 05:05:19.123852 139655626356480 logging_writer.py:48] [37300] global_step=37300, grad_norm=1.070611596107483, loss=2.308310031890869
I0129 05:05:53.043346 139656666543872 logging_writer.py:48] [37400] global_step=37400, grad_norm=1.0833934545516968, loss=2.1478512287139893
I0129 05:06:21.013464 139822745589568 spec.py:321] Evaluating on the training split.
I0129 05:06:27.253673 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 05:06:35.695267 139822745589568 spec.py:349] Evaluating on the test split.
I0129 05:06:38.264643 139822745589568 submission_runner.py:408] Time since start: 13241.62s, 	Step: 37484, 	{'train/accuracy': 0.2384207546710968, 'train/loss': 4.461237907409668, 'validation/accuracy': 0.22425998747348785, 'validation/loss': 4.545414447784424, 'validation/num_examples': 50000, 'test/accuracy': 0.15650001168251038, 'test/loss': 5.578945159912109, 'test/num_examples': 10000, 'score': 12784.916873216629, 'total_duration': 13241.621287107468, 'accumulated_submission_time': 12784.916873216629, 'accumulated_eval_time': 454.6478660106659, 'accumulated_logging_time': 0.764479398727417}
I0129 05:06:38.288488 139656297445120 logging_writer.py:48] [37484] accumulated_eval_time=454.647866, accumulated_logging_time=0.764479, accumulated_submission_time=12784.916873, global_step=37484, preemption_count=0, score=12784.916873, test/accuracy=0.156500, test/loss=5.578945, test/num_examples=10000, total_duration=13241.621287, train/accuracy=0.238421, train/loss=4.461238, validation/accuracy=0.224260, validation/loss=4.545414, validation/num_examples=50000
I0129 05:06:44.086804 139656649758464 logging_writer.py:48] [37500] global_step=37500, grad_norm=1.1248854398727417, loss=2.375119686126709
I0129 05:07:17.998101 139656297445120 logging_writer.py:48] [37600] global_step=37600, grad_norm=1.0760724544525146, loss=2.209210157394409
I0129 05:07:52.078285 139656649758464 logging_writer.py:48] [37700] global_step=37700, grad_norm=1.190571904182434, loss=2.3360490798950195
I0129 05:08:26.006012 139656297445120 logging_writer.py:48] [37800] global_step=37800, grad_norm=1.1729539632797241, loss=2.3151233196258545
I0129 05:08:59.941290 139656649758464 logging_writer.py:48] [37900] global_step=37900, grad_norm=1.2380460500717163, loss=2.289921283721924
I0129 05:09:33.872647 139656297445120 logging_writer.py:48] [38000] global_step=38000, grad_norm=1.2050288915634155, loss=2.240745782852173
I0129 05:10:07.862098 139656649758464 logging_writer.py:48] [38100] global_step=38100, grad_norm=1.2979789972305298, loss=2.270528554916382
I0129 05:10:41.801072 139656297445120 logging_writer.py:48] [38200] global_step=38200, grad_norm=1.1921521425247192, loss=2.3449878692626953
I0129 05:11:15.727228 139656649758464 logging_writer.py:48] [38300] global_step=38300, grad_norm=1.2265872955322266, loss=2.2872166633605957
I0129 05:11:49.699258 139656297445120 logging_writer.py:48] [38400] global_step=38400, grad_norm=1.2802889347076416, loss=2.1712441444396973
I0129 05:12:23.658266 139656649758464 logging_writer.py:48] [38500] global_step=38500, grad_norm=1.262943983078003, loss=2.259671926498413
I0129 05:12:57.564407 139656297445120 logging_writer.py:48] [38600] global_step=38600, grad_norm=1.1814030408859253, loss=2.284486770629883
I0129 05:13:31.492574 139656649758464 logging_writer.py:48] [38700] global_step=38700, grad_norm=1.088413119316101, loss=2.228137969970703
I0129 05:14:05.525638 139656297445120 logging_writer.py:48] [38800] global_step=38800, grad_norm=1.108323574066162, loss=2.249533176422119
I0129 05:14:39.488269 139656649758464 logging_writer.py:48] [38900] global_step=38900, grad_norm=1.1632214784622192, loss=2.227062940597534
I0129 05:15:08.484774 139822745589568 spec.py:321] Evaluating on the training split.
I0129 05:15:14.629289 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 05:15:23.453444 139822745589568 spec.py:349] Evaluating on the test split.
I0129 05:15:26.018090 139822745589568 submission_runner.py:408] Time since start: 13769.37s, 	Step: 38987, 	{'train/accuracy': 0.2852160334587097, 'train/loss': 3.7270145416259766, 'validation/accuracy': 0.2743600010871887, 'validation/loss': 3.8535454273223877, 'validation/num_examples': 50000, 'test/accuracy': 0.19460001587867737, 'test/loss': 4.6907501220703125, 'test/num_examples': 10000, 'score': 13295.052711725235, 'total_duration': 13769.374715805054, 'accumulated_submission_time': 13295.052711725235, 'accumulated_eval_time': 472.18113017082214, 'accumulated_logging_time': 0.797905683517456}
I0129 05:15:26.043718 139655617963776 logging_writer.py:48] [38987] accumulated_eval_time=472.181130, accumulated_logging_time=0.797906, accumulated_submission_time=13295.052712, global_step=38987, preemption_count=0, score=13295.052712, test/accuracy=0.194600, test/loss=4.690750, test/num_examples=10000, total_duration=13769.374716, train/accuracy=0.285216, train/loss=3.727015, validation/accuracy=0.274360, validation/loss=3.853545, validation/num_examples=50000
I0129 05:15:30.821059 139655626356480 logging_writer.py:48] [39000] global_step=39000, grad_norm=1.157214641571045, loss=2.2945237159729004
I0129 05:16:04.705219 139655617963776 logging_writer.py:48] [39100] global_step=39100, grad_norm=1.1513694524765015, loss=2.1796412467956543
I0129 05:16:38.600048 139655626356480 logging_writer.py:48] [39200] global_step=39200, grad_norm=1.188546895980835, loss=2.3200292587280273
I0129 05:17:12.526314 139655617963776 logging_writer.py:48] [39300] global_step=39300, grad_norm=1.1512922048568726, loss=2.3394839763641357
I0129 05:17:46.457867 139655626356480 logging_writer.py:48] [39400] global_step=39400, grad_norm=1.1189136505126953, loss=2.3493103981018066
I0129 05:18:20.412126 139655617963776 logging_writer.py:48] [39500] global_step=39500, grad_norm=1.1445035934448242, loss=2.1389989852905273
I0129 05:18:54.340328 139655626356480 logging_writer.py:48] [39600] global_step=39600, grad_norm=1.1528044939041138, loss=2.194284439086914
I0129 05:19:28.289134 139655617963776 logging_writer.py:48] [39700] global_step=39700, grad_norm=1.0966742038726807, loss=2.2029757499694824
I0129 05:20:02.202089 139655626356480 logging_writer.py:48] [39800] global_step=39800, grad_norm=1.11305832862854, loss=2.288358449935913
I0129 05:20:36.225513 139655617963776 logging_writer.py:48] [39900] global_step=39900, grad_norm=1.0966756343841553, loss=2.2089385986328125
I0129 05:21:10.189599 139655626356480 logging_writer.py:48] [40000] global_step=40000, grad_norm=1.2585673332214355, loss=2.1970200538635254
I0129 05:21:44.137856 139655617963776 logging_writer.py:48] [40100] global_step=40100, grad_norm=1.1933609247207642, loss=2.2145276069641113
I0129 05:22:18.089238 139655626356480 logging_writer.py:48] [40200] global_step=40200, grad_norm=1.2320464849472046, loss=2.2360455989837646
I0129 05:22:52.055543 139655617963776 logging_writer.py:48] [40300] global_step=40300, grad_norm=1.2382086515426636, loss=2.287313938140869
I0129 05:23:26.012055 139655626356480 logging_writer.py:48] [40400] global_step=40400, grad_norm=1.1834540367126465, loss=2.216561794281006
I0129 05:23:56.030119 139822745589568 spec.py:321] Evaluating on the training split.
I0129 05:24:02.333494 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 05:24:10.818433 139822745589568 spec.py:349] Evaluating on the test split.
I0129 05:24:13.393322 139822745589568 submission_runner.py:408] Time since start: 14296.75s, 	Step: 40490, 	{'train/accuracy': 0.3294602930545807, 'train/loss': 3.2785985469818115, 'validation/accuracy': 0.30687999725341797, 'validation/loss': 3.4649877548217773, 'validation/num_examples': 50000, 'test/accuracy': 0.24260000884532928, 'test/loss': 4.134172439575195, 'test/num_examples': 10000, 'score': 13804.977420091629, 'total_duration': 14296.749955415726, 'accumulated_submission_time': 13804.977420091629, 'accumulated_eval_time': 489.544287443161, 'accumulated_logging_time': 0.8351831436157227}
I0129 05:24:13.417498 139656834316032 logging_writer.py:48] [40490] accumulated_eval_time=489.544287, accumulated_logging_time=0.835183, accumulated_submission_time=13804.977420, global_step=40490, preemption_count=0, score=13804.977420, test/accuracy=0.242600, test/loss=4.134172, test/num_examples=10000, total_duration=14296.749955, train/accuracy=0.329460, train/loss=3.278599, validation/accuracy=0.306880, validation/loss=3.464988, validation/num_examples=50000
I0129 05:24:17.174077 139658730145536 logging_writer.py:48] [40500] global_step=40500, grad_norm=1.1161667108535767, loss=2.318319082260132
I0129 05:24:51.073921 139656834316032 logging_writer.py:48] [40600] global_step=40600, grad_norm=1.2166800498962402, loss=2.4220972061157227
I0129 05:25:24.927334 139658730145536 logging_writer.py:48] [40700] global_step=40700, grad_norm=1.1695544719696045, loss=2.205659866333008
I0129 05:25:58.861318 139656834316032 logging_writer.py:48] [40800] global_step=40800, grad_norm=1.1337193250656128, loss=2.1706902980804443
I0129 05:26:32.798698 139658730145536 logging_writer.py:48] [40900] global_step=40900, grad_norm=1.2019702196121216, loss=2.2729415893554688
I0129 05:27:06.808871 139656834316032 logging_writer.py:48] [41000] global_step=41000, grad_norm=1.2283275127410889, loss=2.066352367401123
I0129 05:27:40.748094 139658730145536 logging_writer.py:48] [41100] global_step=41100, grad_norm=1.2344022989273071, loss=2.257568120956421
I0129 05:28:14.688797 139656834316032 logging_writer.py:48] [41200] global_step=41200, grad_norm=1.1351503133773804, loss=2.216386318206787
I0129 05:28:48.627946 139658730145536 logging_writer.py:48] [41300] global_step=41300, grad_norm=1.2493125200271606, loss=2.3221750259399414
I0129 05:29:22.571145 139656834316032 logging_writer.py:48] [41400] global_step=41400, grad_norm=1.2953583002090454, loss=2.1989998817443848
I0129 05:29:56.520956 139658730145536 logging_writer.py:48] [41500] global_step=41500, grad_norm=1.4504156112670898, loss=2.4526617527008057
I0129 05:30:30.428175 139656834316032 logging_writer.py:48] [41600] global_step=41600, grad_norm=1.120675802230835, loss=2.3808517456054688
I0129 05:31:04.361646 139658730145536 logging_writer.py:48] [41700] global_step=41700, grad_norm=1.1996917724609375, loss=2.3944780826568604
I0129 05:31:38.302467 139656834316032 logging_writer.py:48] [41800] global_step=41800, grad_norm=1.05732262134552, loss=2.2007880210876465
I0129 05:32:12.211907 139658730145536 logging_writer.py:48] [41900] global_step=41900, grad_norm=1.256563425064087, loss=2.342442512512207
I0129 05:32:43.593347 139822745589568 spec.py:321] Evaluating on the training split.
I0129 05:32:49.870800 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 05:32:58.323466 139822745589568 spec.py:349] Evaluating on the test split.
I0129 05:33:00.865600 139822745589568 submission_runner.py:408] Time since start: 14824.22s, 	Step: 41994, 	{'train/accuracy': 0.10829878598451614, 'train/loss': 6.09940767288208, 'validation/accuracy': 0.10523999482393265, 'validation/loss': 6.120722770690918, 'validation/num_examples': 50000, 'test/accuracy': 0.07490000128746033, 'test/loss': 6.715982437133789, 'test/num_examples': 10000, 'score': 14315.08835530281, 'total_duration': 14824.222242355347, 'accumulated_submission_time': 14315.08835530281, 'accumulated_eval_time': 506.81652092933655, 'accumulated_logging_time': 0.8721778392791748}
I0129 05:33:00.890606 139656297445120 logging_writer.py:48] [41994] accumulated_eval_time=506.816521, accumulated_logging_time=0.872178, accumulated_submission_time=14315.088355, global_step=41994, preemption_count=0, score=14315.088355, test/accuracy=0.074900, test/loss=6.715982, test/num_examples=10000, total_duration=14824.222242, train/accuracy=0.108299, train/loss=6.099408, validation/accuracy=0.105240, validation/loss=6.120723, validation/num_examples=50000
I0129 05:33:03.275355 139656649758464 logging_writer.py:48] [42000] global_step=42000, grad_norm=1.24425208568573, loss=2.1657767295837402
I0129 05:33:37.195685 139656297445120 logging_writer.py:48] [42100] global_step=42100, grad_norm=1.1723798513412476, loss=2.150766134262085
I0129 05:34:11.183532 139656649758464 logging_writer.py:48] [42200] global_step=42200, grad_norm=1.2330610752105713, loss=2.3820104598999023
I0129 05:34:45.134291 139656297445120 logging_writer.py:48] [42300] global_step=42300, grad_norm=1.1912009716033936, loss=2.2378990650177
I0129 05:35:19.096375 139656649758464 logging_writer.py:48] [42400] global_step=42400, grad_norm=1.2266120910644531, loss=2.3283333778381348
I0129 05:35:53.048108 139656297445120 logging_writer.py:48] [42500] global_step=42500, grad_norm=1.2276157140731812, loss=2.250459671020508
I0129 05:36:26.992037 139656649758464 logging_writer.py:48] [42600] global_step=42600, grad_norm=1.0627561807632446, loss=2.2107555866241455
I0129 05:37:00.950081 139656297445120 logging_writer.py:48] [42700] global_step=42700, grad_norm=1.2371805906295776, loss=2.215665817260742
I0129 05:37:34.886399 139656649758464 logging_writer.py:48] [42800] global_step=42800, grad_norm=1.2830432653427124, loss=2.2469279766082764
I0129 05:38:08.828595 139656297445120 logging_writer.py:48] [42900] global_step=42900, grad_norm=1.2750455141067505, loss=2.3446106910705566
I0129 05:38:42.778642 139656649758464 logging_writer.py:48] [43000] global_step=43000, grad_norm=1.3188811540603638, loss=2.2359554767608643
I0129 05:39:16.706849 139656297445120 logging_writer.py:48] [43100] global_step=43100, grad_norm=1.1561434268951416, loss=2.4259488582611084
I0129 05:39:50.641243 139656649758464 logging_writer.py:48] [43200] global_step=43200, grad_norm=1.1415245532989502, loss=2.177802801132202
I0129 05:40:24.665748 139656297445120 logging_writer.py:48] [43300] global_step=43300, grad_norm=1.0992069244384766, loss=2.2665982246398926
I0129 05:40:58.618615 139656649758464 logging_writer.py:48] [43400] global_step=43400, grad_norm=1.1955708265304565, loss=2.337472915649414
I0129 05:41:31.024148 139822745589568 spec.py:321] Evaluating on the training split.
I0129 05:41:37.376355 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 05:41:45.866215 139822745589568 spec.py:349] Evaluating on the test split.
I0129 05:41:48.448654 139822745589568 submission_runner.py:408] Time since start: 15351.81s, 	Step: 43497, 	{'train/accuracy': 0.13149712979793549, 'train/loss': 4.971019744873047, 'validation/accuracy': 0.11965999752283096, 'validation/loss': 5.1093292236328125, 'validation/num_examples': 50000, 'test/accuracy': 0.08090000599622726, 'test/loss': 5.751832008361816, 'test/num_examples': 10000, 'score': 14825.161313533783, 'total_duration': 15351.805294513702, 'accumulated_submission_time': 14825.161313533783, 'accumulated_eval_time': 524.241007566452, 'accumulated_logging_time': 0.9064865112304688}
I0129 05:41:48.476048 139656666543872 logging_writer.py:48] [43497] accumulated_eval_time=524.241008, accumulated_logging_time=0.906487, accumulated_submission_time=14825.161314, global_step=43497, preemption_count=0, score=14825.161314, test/accuracy=0.080900, test/loss=5.751832, test/num_examples=10000, total_duration=15351.805295, train/accuracy=0.131497, train/loss=4.971020, validation/accuracy=0.119660, validation/loss=5.109329, validation/num_examples=50000
I0129 05:41:49.841165 139656834316032 logging_writer.py:48] [43500] global_step=43500, grad_norm=1.2401800155639648, loss=2.1722967624664307
I0129 05:42:23.747625 139656666543872 logging_writer.py:48] [43600] global_step=43600, grad_norm=1.2824950218200684, loss=2.3258180618286133
I0129 05:42:57.667761 139656834316032 logging_writer.py:48] [43700] global_step=43700, grad_norm=1.0717061758041382, loss=2.133052110671997
I0129 05:43:31.593928 139656666543872 logging_writer.py:48] [43800] global_step=43800, grad_norm=1.2774099111557007, loss=2.2991995811462402
I0129 05:44:06.029452 139656834316032 logging_writer.py:48] [43900] global_step=43900, grad_norm=1.1922541856765747, loss=2.121546506881714
I0129 05:44:39.926230 139656666543872 logging_writer.py:48] [44000] global_step=44000, grad_norm=1.2543882131576538, loss=2.316192388534546
I0129 05:45:13.863498 139656834316032 logging_writer.py:48] [44100] global_step=44100, grad_norm=1.2390189170837402, loss=2.2570135593414307
I0129 05:45:47.799796 139656666543872 logging_writer.py:48] [44200] global_step=44200, grad_norm=1.2972882986068726, loss=2.386064291000366
I0129 05:46:21.750434 139656834316032 logging_writer.py:48] [44300] global_step=44300, grad_norm=1.3100935220718384, loss=2.1739494800567627
I0129 05:46:55.759767 139656666543872 logging_writer.py:48] [44400] global_step=44400, grad_norm=1.1650052070617676, loss=2.18402099609375
I0129 05:47:29.701115 139656834316032 logging_writer.py:48] [44500] global_step=44500, grad_norm=1.0734045505523682, loss=2.302966833114624
I0129 05:48:03.634713 139656666543872 logging_writer.py:48] [44600] global_step=44600, grad_norm=1.2321478128433228, loss=2.2264392375946045
I0129 05:48:37.589756 139656834316032 logging_writer.py:48] [44700] global_step=44700, grad_norm=1.3545162677764893, loss=2.3262641429901123
I0129 05:49:11.515215 139656666543872 logging_writer.py:48] [44800] global_step=44800, grad_norm=1.2129924297332764, loss=2.265458583831787
I0129 05:49:45.447208 139656834316032 logging_writer.py:48] [44900] global_step=44900, grad_norm=1.2010352611541748, loss=2.1721653938293457
I0129 05:50:18.482893 139822745589568 spec.py:321] Evaluating on the training split.
I0129 05:50:24.754509 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 05:50:33.192063 139822745589568 spec.py:349] Evaluating on the test split.
I0129 05:50:35.721819 139822745589568 submission_runner.py:408] Time since start: 15879.08s, 	Step: 44999, 	{'train/accuracy': 0.17918924987316132, 'train/loss': 5.077031135559082, 'validation/accuracy': 0.16359999775886536, 'validation/loss': 5.281920433044434, 'validation/num_examples': 50000, 'test/accuracy': 0.12550000846385956, 'test/loss': 5.8240647315979, 'test/num_examples': 10000, 'score': 15335.105654001236, 'total_duration': 15879.07846212387, 'accumulated_submission_time': 15335.105654001236, 'accumulated_eval_time': 541.479898929596, 'accumulated_logging_time': 0.9436588287353516}
I0129 05:50:35.747713 139655609571072 logging_writer.py:48] [44999] accumulated_eval_time=541.479899, accumulated_logging_time=0.943659, accumulated_submission_time=15335.105654, global_step=44999, preemption_count=0, score=15335.105654, test/accuracy=0.125500, test/loss=5.824065, test/num_examples=10000, total_duration=15879.078462, train/accuracy=0.179189, train/loss=5.077031, validation/accuracy=0.163600, validation/loss=5.281920, validation/num_examples=50000
I0129 05:50:36.440699 139655617963776 logging_writer.py:48] [45000] global_step=45000, grad_norm=1.2577837705612183, loss=2.1336259841918945
I0129 05:51:10.304363 139655609571072 logging_writer.py:48] [45100] global_step=45100, grad_norm=1.2253384590148926, loss=2.1708383560180664
I0129 05:51:44.188288 139655617963776 logging_writer.py:48] [45200] global_step=45200, grad_norm=1.2731683254241943, loss=2.260251522064209
I0129 05:52:18.112094 139655609571072 logging_writer.py:48] [45300] global_step=45300, grad_norm=1.2342644929885864, loss=2.3130221366882324
I0129 05:52:52.042385 139655617963776 logging_writer.py:48] [45400] global_step=45400, grad_norm=1.282935380935669, loss=2.2264037132263184
I0129 05:53:26.051729 139655609571072 logging_writer.py:48] [45500] global_step=45500, grad_norm=1.3858122825622559, loss=2.356132745742798
I0129 05:53:59.975160 139655617963776 logging_writer.py:48] [45600] global_step=45600, grad_norm=1.230342984199524, loss=2.245405435562134
I0129 05:54:33.920511 139655609571072 logging_writer.py:48] [45700] global_step=45700, grad_norm=1.2083957195281982, loss=2.216935157775879
I0129 05:55:07.855311 139655617963776 logging_writer.py:48] [45800] global_step=45800, grad_norm=1.1977969408035278, loss=2.2761895656585693
I0129 05:55:41.778027 139655609571072 logging_writer.py:48] [45900] global_step=45900, grad_norm=1.1564303636550903, loss=2.364668607711792
I0129 05:56:15.728188 139655617963776 logging_writer.py:48] [46000] global_step=46000, grad_norm=1.1521663665771484, loss=2.21085524559021
I0129 05:56:49.654588 139655609571072 logging_writer.py:48] [46100] global_step=46100, grad_norm=1.2242194414138794, loss=2.250478506088257
I0129 05:57:23.601848 139655617963776 logging_writer.py:48] [46200] global_step=46200, grad_norm=1.254475474357605, loss=2.3106844425201416
I0129 05:57:57.515300 139655609571072 logging_writer.py:48] [46300] global_step=46300, grad_norm=1.102828860282898, loss=2.164213180541992
I0129 05:58:31.451522 139655617963776 logging_writer.py:48] [46400] global_step=46400, grad_norm=1.168786644935608, loss=2.1581969261169434
I0129 05:59:05.373302 139655609571072 logging_writer.py:48] [46500] global_step=46500, grad_norm=1.1207889318466187, loss=2.1929492950439453
I0129 05:59:05.856944 139822745589568 spec.py:321] Evaluating on the training split.
I0129 05:59:12.026395 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 05:59:20.678494 139822745589568 spec.py:349] Evaluating on the test split.
I0129 05:59:23.241341 139822745589568 submission_runner.py:408] Time since start: 16406.60s, 	Step: 46503, 	{'train/accuracy': 0.3250558078289032, 'train/loss': 3.3820459842681885, 'validation/accuracy': 0.3132599890232086, 'validation/loss': 3.5227527618408203, 'validation/num_examples': 50000, 'test/accuracy': 0.23610001802444458, 'test/loss': 4.2942023277282715, 'test/num_examples': 10000, 'score': 15845.155004501343, 'total_duration': 16406.597977876663, 'accumulated_submission_time': 15845.155004501343, 'accumulated_eval_time': 558.8642518520355, 'accumulated_logging_time': 0.9791169166564941}
I0129 05:59:23.271429 139656658151168 logging_writer.py:48] [46503] accumulated_eval_time=558.864252, accumulated_logging_time=0.979117, accumulated_submission_time=15845.155005, global_step=46503, preemption_count=0, score=15845.155005, test/accuracy=0.236100, test/loss=4.294202, test/num_examples=10000, total_duration=16406.597978, train/accuracy=0.325056, train/loss=3.382046, validation/accuracy=0.313260, validation/loss=3.522753, validation/num_examples=50000
I0129 05:59:56.568662 139656666543872 logging_writer.py:48] [46600] global_step=46600, grad_norm=1.257521629333496, loss=2.248537540435791
I0129 06:00:30.484204 139656658151168 logging_writer.py:48] [46700] global_step=46700, grad_norm=1.3362181186676025, loss=2.3030576705932617
I0129 06:01:04.424175 139656666543872 logging_writer.py:48] [46800] global_step=46800, grad_norm=1.1247553825378418, loss=2.2129018306732178
I0129 06:01:38.319197 139656658151168 logging_writer.py:48] [46900] global_step=46900, grad_norm=1.3776341676712036, loss=2.2263221740722656
I0129 06:02:12.272698 139656666543872 logging_writer.py:48] [47000] global_step=47000, grad_norm=1.3942437171936035, loss=2.4127964973449707
I0129 06:02:46.178721 139656658151168 logging_writer.py:48] [47100] global_step=47100, grad_norm=1.2921420335769653, loss=2.174276828765869
I0129 06:03:20.118298 139656666543872 logging_writer.py:48] [47200] global_step=47200, grad_norm=1.3094062805175781, loss=2.2719058990478516
I0129 06:03:54.045615 139656658151168 logging_writer.py:48] [47300] global_step=47300, grad_norm=1.1896766424179077, loss=2.171376943588257
I0129 06:04:27.970345 139656666543872 logging_writer.py:48] [47400] global_step=47400, grad_norm=1.3160114288330078, loss=2.3043837547302246
I0129 06:05:01.902266 139656658151168 logging_writer.py:48] [47500] global_step=47500, grad_norm=1.2494477033615112, loss=2.296443223953247
I0129 06:05:35.808813 139656666543872 logging_writer.py:48] [47600] global_step=47600, grad_norm=1.1719707250595093, loss=2.3194198608398438
I0129 06:06:09.793522 139656658151168 logging_writer.py:48] [47700] global_step=47700, grad_norm=1.199168086051941, loss=2.2946701049804688
I0129 06:06:43.685993 139656666543872 logging_writer.py:48] [47800] global_step=47800, grad_norm=1.272741436958313, loss=2.268399477005005
I0129 06:07:17.610166 139656658151168 logging_writer.py:48] [47900] global_step=47900, grad_norm=1.2558287382125854, loss=2.2474794387817383
I0129 06:07:51.530122 139656666543872 logging_writer.py:48] [48000] global_step=48000, grad_norm=1.3017994165420532, loss=2.266847848892212
I0129 06:07:53.369197 139822745589568 spec.py:321] Evaluating on the training split.
I0129 06:07:59.508218 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 06:08:08.376835 139822745589568 spec.py:349] Evaluating on the test split.
I0129 06:08:10.962998 139822745589568 submission_runner.py:408] Time since start: 16934.32s, 	Step: 48007, 	{'train/accuracy': 0.3681640625, 'train/loss': 3.0340774059295654, 'validation/accuracy': 0.34505999088287354, 'validation/loss': 3.1679186820983887, 'validation/num_examples': 50000, 'test/accuracy': 0.26680001616477966, 'test/loss': 3.835937261581421, 'test/num_examples': 10000, 'score': 16355.190060853958, 'total_duration': 16934.3196310997, 'accumulated_submission_time': 16355.190060853958, 'accumulated_eval_time': 576.4580047130585, 'accumulated_logging_time': 1.020794153213501}
I0129 06:08:10.989308 139655626356480 logging_writer.py:48] [48007] accumulated_eval_time=576.458005, accumulated_logging_time=1.020794, accumulated_submission_time=16355.190061, global_step=48007, preemption_count=0, score=16355.190061, test/accuracy=0.266800, test/loss=3.835937, test/num_examples=10000, total_duration=16934.319631, train/accuracy=0.368164, train/loss=3.034077, validation/accuracy=0.345060, validation/loss=3.167919, validation/num_examples=50000
I0129 06:08:42.828284 139656297445120 logging_writer.py:48] [48100] global_step=48100, grad_norm=1.3040719032287598, loss=2.2192392349243164
I0129 06:09:16.732258 139655626356480 logging_writer.py:48] [48200] global_step=48200, grad_norm=1.1467247009277344, loss=2.191770553588867
I0129 06:09:50.663318 139656297445120 logging_writer.py:48] [48300] global_step=48300, grad_norm=1.1543573141098022, loss=2.2667551040649414
I0129 06:10:24.590069 139655626356480 logging_writer.py:48] [48400] global_step=48400, grad_norm=1.1327626705169678, loss=2.1505088806152344
I0129 06:10:58.536982 139656297445120 logging_writer.py:48] [48500] global_step=48500, grad_norm=1.2183489799499512, loss=2.312771797180176
I0129 06:11:32.470756 139655626356480 logging_writer.py:48] [48600] global_step=48600, grad_norm=1.2433803081512451, loss=2.2924599647521973
I0129 06:12:06.395254 139656297445120 logging_writer.py:48] [48700] global_step=48700, grad_norm=1.1391887664794922, loss=2.178729772567749
I0129 06:12:40.506641 139655626356480 logging_writer.py:48] [48800] global_step=48800, grad_norm=1.2022147178649902, loss=2.2879831790924072
I0129 06:13:14.436147 139656297445120 logging_writer.py:48] [48900] global_step=48900, grad_norm=1.2816250324249268, loss=2.191960096359253
I0129 06:13:48.357793 139655626356480 logging_writer.py:48] [49000] global_step=49000, grad_norm=1.3299942016601562, loss=2.2796390056610107
I0129 06:14:22.311913 139656297445120 logging_writer.py:48] [49100] global_step=49100, grad_norm=1.3324589729309082, loss=2.277614116668701
I0129 06:14:56.252725 139655626356480 logging_writer.py:48] [49200] global_step=49200, grad_norm=1.2030452489852905, loss=2.26344633102417
I0129 06:15:30.180135 139656297445120 logging_writer.py:48] [49300] global_step=49300, grad_norm=1.2948648929595947, loss=2.140151023864746
I0129 06:16:04.089666 139655626356480 logging_writer.py:48] [49400] global_step=49400, grad_norm=1.172861933708191, loss=2.2443909645080566
I0129 06:16:38.037120 139656297445120 logging_writer.py:48] [49500] global_step=49500, grad_norm=1.1296383142471313, loss=2.226600408554077
I0129 06:16:41.241901 139822745589568 spec.py:321] Evaluating on the training split.
I0129 06:16:47.545195 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 06:16:56.024090 139822745589568 spec.py:349] Evaluating on the test split.
I0129 06:16:58.614143 139822745589568 submission_runner.py:408] Time since start: 17461.97s, 	Step: 49511, 	{'train/accuracy': 0.2969347834587097, 'train/loss': 3.6690821647644043, 'validation/accuracy': 0.28205999732017517, 'validation/loss': 3.842806100845337, 'validation/num_examples': 50000, 'test/accuracy': 0.20390000939369202, 'test/loss': 4.691739082336426, 'test/num_examples': 10000, 'score': 16865.38143491745, 'total_duration': 17461.97078728676, 'accumulated_submission_time': 16865.38143491745, 'accumulated_eval_time': 593.8302228450775, 'accumulated_logging_time': 1.0560753345489502}
I0129 06:16:58.643330 139656297445120 logging_writer.py:48] [49511] accumulated_eval_time=593.830223, accumulated_logging_time=1.056075, accumulated_submission_time=16865.381435, global_step=49511, preemption_count=0, score=16865.381435, test/accuracy=0.203900, test/loss=4.691739, test/num_examples=10000, total_duration=17461.970787, train/accuracy=0.296935, train/loss=3.669082, validation/accuracy=0.282060, validation/loss=3.842806, validation/num_examples=50000
I0129 06:17:29.166761 139656666543872 logging_writer.py:48] [49600] global_step=49600, grad_norm=1.2344309091567993, loss=2.159193515777588
I0129 06:18:03.065904 139656297445120 logging_writer.py:48] [49700] global_step=49700, grad_norm=1.2399502992630005, loss=2.3842077255249023
I0129 06:18:37.000570 139656666543872 logging_writer.py:48] [49800] global_step=49800, grad_norm=1.1879884004592896, loss=2.127976894378662
I0129 06:19:11.056806 139656297445120 logging_writer.py:48] [49900] global_step=49900, grad_norm=1.3555676937103271, loss=2.2427408695220947
I0129 06:19:45.000673 139656666543872 logging_writer.py:48] [50000] global_step=50000, grad_norm=1.1188839673995972, loss=2.1302919387817383
I0129 06:20:18.935602 139656297445120 logging_writer.py:48] [50100] global_step=50100, grad_norm=1.2312943935394287, loss=2.338909149169922
I0129 06:20:52.872287 139656666543872 logging_writer.py:48] [50200] global_step=50200, grad_norm=1.2678337097167969, loss=2.274473190307617
I0129 06:21:26.810367 139656297445120 logging_writer.py:48] [50300] global_step=50300, grad_norm=1.4326783418655396, loss=2.19460129737854
I0129 06:22:00.757244 139656666543872 logging_writer.py:48] [50400] global_step=50400, grad_norm=1.1525497436523438, loss=1.979458212852478
I0129 06:22:34.686340 139656297445120 logging_writer.py:48] [50500] global_step=50500, grad_norm=1.2265188694000244, loss=2.31308650970459
I0129 06:23:08.635407 139656666543872 logging_writer.py:48] [50600] global_step=50600, grad_norm=1.1815252304077148, loss=2.2245805263519287
I0129 06:23:42.573599 139656297445120 logging_writer.py:48] [50700] global_step=50700, grad_norm=1.262562870979309, loss=2.2690913677215576
I0129 06:24:16.538547 139656666543872 logging_writer.py:48] [50800] global_step=50800, grad_norm=1.1881152391433716, loss=2.140773296356201
I0129 06:24:50.444223 139656297445120 logging_writer.py:48] [50900] global_step=50900, grad_norm=1.2103626728057861, loss=2.248107671737671
I0129 06:25:24.383832 139656666543872 logging_writer.py:48] [51000] global_step=51000, grad_norm=1.286257266998291, loss=2.165158987045288
I0129 06:25:28.619406 139822745589568 spec.py:321] Evaluating on the training split.
I0129 06:25:34.803027 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 06:25:43.518632 139822745589568 spec.py:349] Evaluating on the test split.
I0129 06:25:46.067543 139822745589568 submission_runner.py:408] Time since start: 17989.42s, 	Step: 51014, 	{'train/accuracy': 0.15383848547935486, 'train/loss': 4.8142242431640625, 'validation/accuracy': 0.14691999554634094, 'validation/loss': 4.868548393249512, 'validation/num_examples': 50000, 'test/accuracy': 0.10580000281333923, 'test/loss': 5.460964679718018, 'test/num_examples': 10000, 'score': 17375.29843711853, 'total_duration': 17989.424177646637, 'accumulated_submission_time': 17375.29843711853, 'accumulated_eval_time': 611.2783124446869, 'accumulated_logging_time': 1.094315767288208}
I0129 06:25:46.094997 139655617963776 logging_writer.py:48] [51014] accumulated_eval_time=611.278312, accumulated_logging_time=1.094316, accumulated_submission_time=17375.298437, global_step=51014, preemption_count=0, score=17375.298437, test/accuracy=0.105800, test/loss=5.460965, test/num_examples=10000, total_duration=17989.424178, train/accuracy=0.153838, train/loss=4.814224, validation/accuracy=0.146920, validation/loss=4.868548, validation/num_examples=50000
I0129 06:26:15.577715 139656649758464 logging_writer.py:48] [51100] global_step=51100, grad_norm=1.3175454139709473, loss=2.095421314239502
I0129 06:26:49.482472 139655617963776 logging_writer.py:48] [51200] global_step=51200, grad_norm=1.1938648223876953, loss=2.214489698410034
I0129 06:27:23.416415 139656649758464 logging_writer.py:48] [51300] global_step=51300, grad_norm=1.316962718963623, loss=2.188722610473633
I0129 06:27:57.322485 139655617963776 logging_writer.py:48] [51400] global_step=51400, grad_norm=1.0921661853790283, loss=2.1256349086761475
I0129 06:28:31.258231 139656649758464 logging_writer.py:48] [51500] global_step=51500, grad_norm=1.1402997970581055, loss=2.289874315261841
I0129 06:29:05.160036 139655617963776 logging_writer.py:48] [51600] global_step=51600, grad_norm=1.2388715744018555, loss=2.202315330505371
I0129 06:29:39.083964 139656649758464 logging_writer.py:48] [51700] global_step=51700, grad_norm=1.298038363456726, loss=2.3440496921539307
I0129 06:30:12.978305 139655617963776 logging_writer.py:48] [51800] global_step=51800, grad_norm=1.1959080696105957, loss=2.2679145336151123
I0129 06:30:46.883989 139656649758464 logging_writer.py:48] [51900] global_step=51900, grad_norm=1.2788517475128174, loss=2.2100768089294434
I0129 06:31:20.799336 139655617963776 logging_writer.py:48] [52000] global_step=52000, grad_norm=1.1278084516525269, loss=2.0988357067108154
I0129 06:31:54.723455 139656649758464 logging_writer.py:48] [52100] global_step=52100, grad_norm=1.180882453918457, loss=2.196138620376587
I0129 06:32:28.685610 139655617963776 logging_writer.py:48] [52200] global_step=52200, grad_norm=1.2602390050888062, loss=2.370992422103882
I0129 06:33:02.591984 139656649758464 logging_writer.py:48] [52300] global_step=52300, grad_norm=1.2250806093215942, loss=2.200380802154541
I0129 06:33:36.531447 139655617963776 logging_writer.py:48] [52400] global_step=52400, grad_norm=1.316928744316101, loss=2.318669080734253
I0129 06:34:10.463171 139656649758464 logging_writer.py:48] [52500] global_step=52500, grad_norm=1.1996982097625732, loss=2.0852277278900146
I0129 06:34:16.376895 139822745589568 spec.py:321] Evaluating on the training split.
I0129 06:34:22.659872 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 06:34:31.552090 139822745589568 spec.py:349] Evaluating on the test split.
I0129 06:34:34.140101 139822745589568 submission_runner.py:408] Time since start: 18517.50s, 	Step: 52519, 	{'train/accuracy': 0.45874521136283875, 'train/loss': 2.4128456115722656, 'validation/accuracy': 0.39997997879981995, 'validation/loss': 2.8159847259521484, 'validation/num_examples': 50000, 'test/accuracy': 0.30150002241134644, 'test/loss': 3.607529878616333, 'test/num_examples': 10000, 'score': 17885.518147945404, 'total_duration': 18517.496727705002, 'accumulated_submission_time': 17885.518147945404, 'accumulated_eval_time': 629.0414938926697, 'accumulated_logging_time': 1.1306431293487549}
I0129 06:34:34.166839 139655609571072 logging_writer.py:48] [52519] accumulated_eval_time=629.041494, accumulated_logging_time=1.130643, accumulated_submission_time=17885.518148, global_step=52519, preemption_count=0, score=17885.518148, test/accuracy=0.301500, test/loss=3.607530, test/num_examples=10000, total_duration=18517.496728, train/accuracy=0.458745, train/loss=2.412846, validation/accuracy=0.399980, validation/loss=2.815985, validation/num_examples=50000
I0129 06:35:01.975546 139656297445120 logging_writer.py:48] [52600] global_step=52600, grad_norm=1.1777498722076416, loss=2.326663017272949
I0129 06:35:35.863961 139655609571072 logging_writer.py:48] [52700] global_step=52700, grad_norm=1.2041128873825073, loss=2.050140857696533
I0129 06:36:09.794613 139656297445120 logging_writer.py:48] [52800] global_step=52800, grad_norm=1.341092824935913, loss=2.1954617500305176
I0129 06:36:43.726260 139655609571072 logging_writer.py:48] [52900] global_step=52900, grad_norm=1.2484387159347534, loss=2.1507568359375
I0129 06:37:17.638225 139656297445120 logging_writer.py:48] [53000] global_step=53000, grad_norm=1.3184645175933838, loss=2.167008638381958
I0129 06:37:51.573555 139655609571072 logging_writer.py:48] [53100] global_step=53100, grad_norm=1.2153314352035522, loss=2.2130980491638184
I0129 06:38:25.481764 139656297445120 logging_writer.py:48] [53200] global_step=53200, grad_norm=1.3236225843429565, loss=2.207075357437134
I0129 06:38:59.476049 139655609571072 logging_writer.py:48] [53300] global_step=53300, grad_norm=1.2379086017608643, loss=2.3338565826416016
I0129 06:39:33.405361 139656297445120 logging_writer.py:48] [53400] global_step=53400, grad_norm=1.3336329460144043, loss=2.244459390640259
I0129 06:40:07.340995 139655609571072 logging_writer.py:48] [53500] global_step=53500, grad_norm=1.2316192388534546, loss=2.139451742172241
I0129 06:40:41.278173 139656297445120 logging_writer.py:48] [53600] global_step=53600, grad_norm=1.1973176002502441, loss=2.2162578105926514
I0129 06:41:15.213779 139655609571072 logging_writer.py:48] [53700] global_step=53700, grad_norm=1.1154273748397827, loss=2.20241641998291
I0129 06:41:49.117509 139656297445120 logging_writer.py:48] [53800] global_step=53800, grad_norm=1.3293070793151855, loss=2.2269582748413086
I0129 06:42:23.023127 139655609571072 logging_writer.py:48] [53900] global_step=53900, grad_norm=1.3682948350906372, loss=2.326486110687256
I0129 06:42:56.948188 139656297445120 logging_writer.py:48] [54000] global_step=54000, grad_norm=1.1833864450454712, loss=2.214367628097534
I0129 06:43:04.211403 139822745589568 spec.py:321] Evaluating on the training split.
I0129 06:43:10.344823 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 06:43:18.881213 139822745589568 spec.py:349] Evaluating on the test split.
I0129 06:43:21.454614 139822745589568 submission_runner.py:408] Time since start: 19044.81s, 	Step: 54023, 	{'train/accuracy': 0.23064811527729034, 'train/loss': 4.139097213745117, 'validation/accuracy': 0.21379999816417694, 'validation/loss': 4.321166038513184, 'validation/num_examples': 50000, 'test/accuracy': 0.16940000653266907, 'test/loss': 4.901164531707764, 'test/num_examples': 10000, 'score': 18395.49843478203, 'total_duration': 19044.81125664711, 'accumulated_submission_time': 18395.49843478203, 'accumulated_eval_time': 646.2846746444702, 'accumulated_logging_time': 1.1675300598144531}
I0129 06:43:21.485838 139655626356480 logging_writer.py:48] [54023] accumulated_eval_time=646.284675, accumulated_logging_time=1.167530, accumulated_submission_time=18395.498435, global_step=54023, preemption_count=0, score=18395.498435, test/accuracy=0.169400, test/loss=4.901165, test/num_examples=10000, total_duration=19044.811257, train/accuracy=0.230648, train/loss=4.139097, validation/accuracy=0.213800, validation/loss=4.321166, validation/num_examples=50000
I0129 06:43:47.911018 139656649758464 logging_writer.py:48] [54100] global_step=54100, grad_norm=1.2731287479400635, loss=2.3493804931640625
I0129 06:44:21.805234 139655626356480 logging_writer.py:48] [54200] global_step=54200, grad_norm=1.3644872903823853, loss=2.3826560974121094
I0129 06:44:55.711556 139656649758464 logging_writer.py:48] [54300] global_step=54300, grad_norm=1.2661972045898438, loss=2.2140908241271973
I0129 06:45:29.696124 139655626356480 logging_writer.py:48] [54400] global_step=54400, grad_norm=1.3272963762283325, loss=2.2014904022216797
I0129 06:46:03.620647 139656649758464 logging_writer.py:48] [54500] global_step=54500, grad_norm=1.3027050495147705, loss=2.2525100708007812
I0129 06:46:37.555791 139655626356480 logging_writer.py:48] [54600] global_step=54600, grad_norm=1.2622078657150269, loss=2.2220733165740967
I0129 06:47:11.465225 139656649758464 logging_writer.py:48] [54700] global_step=54700, grad_norm=1.418680191040039, loss=2.2508864402770996
I0129 06:47:45.377202 139655626356480 logging_writer.py:48] [54800] global_step=54800, grad_norm=1.320837378501892, loss=2.274754285812378
I0129 06:48:19.282878 139656649758464 logging_writer.py:48] [54900] global_step=54900, grad_norm=1.171270489692688, loss=2.208547353744507
I0129 06:48:53.177696 139655626356480 logging_writer.py:48] [55000] global_step=55000, grad_norm=1.2000386714935303, loss=2.321428060531616
I0129 06:49:27.095864 139656649758464 logging_writer.py:48] [55100] global_step=55100, grad_norm=1.2519696950912476, loss=2.2229256629943848
I0129 06:50:01.002857 139655626356480 logging_writer.py:48] [55200] global_step=55200, grad_norm=1.3248945474624634, loss=2.2248687744140625
I0129 06:50:34.952411 139656649758464 logging_writer.py:48] [55300] global_step=55300, grad_norm=1.296007513999939, loss=2.2708513736724854
I0129 06:51:08.860895 139655626356480 logging_writer.py:48] [55400] global_step=55400, grad_norm=1.3699018955230713, loss=2.124171733856201
I0129 06:51:42.895736 139656649758464 logging_writer.py:48] [55500] global_step=55500, grad_norm=1.2266652584075928, loss=2.3031108379364014
I0129 06:51:51.526915 139822745589568 spec.py:321] Evaluating on the training split.
I0129 06:51:57.669520 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 06:52:06.389704 139822745589568 spec.py:349] Evaluating on the test split.
I0129 06:52:09.711664 139822745589568 submission_runner.py:408] Time since start: 19573.07s, 	Step: 55527, 	{'train/accuracy': 0.2505779564380646, 'train/loss': 3.9523448944091797, 'validation/accuracy': 0.23389999568462372, 'validation/loss': 4.098372936248779, 'validation/num_examples': 50000, 'test/accuracy': 0.172200009226799, 'test/loss': 4.754857063293457, 'test/num_examples': 10000, 'score': 18905.47820210457, 'total_duration': 19573.068316936493, 'accumulated_submission_time': 18905.47820210457, 'accumulated_eval_time': 664.4693939685822, 'accumulated_logging_time': 1.2078561782836914}
I0129 06:52:09.735747 139655626356480 logging_writer.py:48] [55527] accumulated_eval_time=664.469394, accumulated_logging_time=1.207856, accumulated_submission_time=18905.478202, global_step=55527, preemption_count=0, score=18905.478202, test/accuracy=0.172200, test/loss=4.754857, test/num_examples=10000, total_duration=19573.068317, train/accuracy=0.250578, train/loss=3.952345, validation/accuracy=0.233900, validation/loss=4.098373, validation/num_examples=50000
I0129 06:52:34.820311 139656297445120 logging_writer.py:48] [55600] global_step=55600, grad_norm=1.2193613052368164, loss=2.154665470123291
I0129 06:53:08.688794 139655626356480 logging_writer.py:48] [55700] global_step=55700, grad_norm=1.4798195362091064, loss=2.3025286197662354
I0129 06:53:42.599222 139656297445120 logging_writer.py:48] [55800] global_step=55800, grad_norm=1.2726997137069702, loss=2.2330305576324463
I0129 06:54:16.509998 139655626356480 logging_writer.py:48] [55900] global_step=55900, grad_norm=1.2458235025405884, loss=2.24271297454834
I0129 06:54:50.448821 139656297445120 logging_writer.py:48] [56000] global_step=56000, grad_norm=1.1602833271026611, loss=2.3188436031341553
I0129 06:55:24.361487 139655626356480 logging_writer.py:48] [56100] global_step=56100, grad_norm=1.246698260307312, loss=2.168701171875
I0129 06:55:58.300328 139656297445120 logging_writer.py:48] [56200] global_step=56200, grad_norm=1.3237342834472656, loss=2.258915901184082
I0129 06:56:32.205607 139655626356480 logging_writer.py:48] [56300] global_step=56300, grad_norm=1.1150575876235962, loss=2.221245527267456
I0129 06:57:06.136217 139656297445120 logging_writer.py:48] [56400] global_step=56400, grad_norm=1.2963179349899292, loss=2.1847705841064453
I0129 06:57:40.016171 139655626356480 logging_writer.py:48] [56500] global_step=56500, grad_norm=1.1438003778457642, loss=2.144862174987793
I0129 06:58:13.993838 139656297445120 logging_writer.py:48] [56600] global_step=56600, grad_norm=1.2941920757293701, loss=2.064225435256958
I0129 06:58:47.922624 139655626356480 logging_writer.py:48] [56700] global_step=56700, grad_norm=1.3085803985595703, loss=2.3906285762786865
I0129 06:59:21.808019 139656297445120 logging_writer.py:48] [56800] global_step=56800, grad_norm=1.2342995405197144, loss=2.1954760551452637
I0129 06:59:55.725714 139655626356480 logging_writer.py:48] [56900] global_step=56900, grad_norm=1.3921656608581543, loss=2.2118477821350098
I0129 07:00:29.618541 139656297445120 logging_writer.py:48] [57000] global_step=57000, grad_norm=1.211631178855896, loss=2.279456853866577
I0129 07:00:39.951586 139822745589568 spec.py:321] Evaluating on the training split.
I0129 07:00:46.158009 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 07:00:54.639543 139822745589568 spec.py:349] Evaluating on the test split.
I0129 07:00:57.212485 139822745589568 submission_runner.py:408] Time since start: 20100.57s, 	Step: 57032, 	{'train/accuracy': 0.12844786047935486, 'train/loss': 5.659578323364258, 'validation/accuracy': 0.12205999344587326, 'validation/loss': 5.704127788543701, 'validation/num_examples': 50000, 'test/accuracy': 0.0820000022649765, 'test/loss': 6.516331672668457, 'test/num_examples': 10000, 'score': 19415.634298086166, 'total_duration': 20100.56913280487, 'accumulated_submission_time': 19415.634298086166, 'accumulated_eval_time': 681.7302577495575, 'accumulated_logging_time': 1.2400367259979248}
I0129 07:00:57.242724 139655617963776 logging_writer.py:48] [57032] accumulated_eval_time=681.730258, accumulated_logging_time=1.240037, accumulated_submission_time=19415.634298, global_step=57032, preemption_count=0, score=19415.634298, test/accuracy=0.082000, test/loss=6.516332, test/num_examples=10000, total_duration=20100.569133, train/accuracy=0.128448, train/loss=5.659578, validation/accuracy=0.122060, validation/loss=5.704128, validation/num_examples=50000
I0129 07:01:20.595707 139655626356480 logging_writer.py:48] [57100] global_step=57100, grad_norm=1.2115147113800049, loss=2.028195858001709
I0129 07:01:54.473740 139655617963776 logging_writer.py:48] [57200] global_step=57200, grad_norm=1.2667551040649414, loss=2.126516342163086
I0129 07:02:28.378271 139655626356480 logging_writer.py:48] [57300] global_step=57300, grad_norm=1.2641772031784058, loss=2.275080680847168
I0129 07:03:02.272267 139655617963776 logging_writer.py:48] [57400] global_step=57400, grad_norm=1.302427053451538, loss=2.0667355060577393
I0129 07:03:36.201283 139655626356480 logging_writer.py:48] [57500] global_step=57500, grad_norm=1.4399986267089844, loss=2.20082688331604
I0129 07:04:10.126264 139655617963776 logging_writer.py:48] [57600] global_step=57600, grad_norm=1.2810558080673218, loss=2.205664873123169
I0129 07:04:44.157092 139655626356480 logging_writer.py:48] [57700] global_step=57700, grad_norm=1.460218906402588, loss=2.1535873413085938
I0129 07:05:18.061354 139655617963776 logging_writer.py:48] [57800] global_step=57800, grad_norm=1.4341521263122559, loss=2.2958216667175293
I0129 07:05:51.992149 139655626356480 logging_writer.py:48] [57900] global_step=57900, grad_norm=1.3605833053588867, loss=2.100950002670288
I0129 07:06:25.928290 139655617963776 logging_writer.py:48] [58000] global_step=58000, grad_norm=1.2602534294128418, loss=2.168363571166992
I0129 07:06:59.849121 139655626356480 logging_writer.py:48] [58100] global_step=58100, grad_norm=1.3228576183319092, loss=2.3118181228637695
I0129 07:07:33.772953 139655617963776 logging_writer.py:48] [58200] global_step=58200, grad_norm=1.2349823713302612, loss=2.1059725284576416
I0129 07:08:07.726645 139655626356480 logging_writer.py:48] [58300] global_step=58300, grad_norm=1.2436904907226562, loss=2.13838791847229
I0129 07:08:41.614719 139655617963776 logging_writer.py:48] [58400] global_step=58400, grad_norm=1.3068428039550781, loss=2.3055667877197266
I0129 07:09:15.543046 139655626356480 logging_writer.py:48] [58500] global_step=58500, grad_norm=1.1391983032226562, loss=2.081798553466797
I0129 07:09:27.542896 139822745589568 spec.py:321] Evaluating on the training split.
I0129 07:09:33.722064 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 07:09:42.213551 139822745589568 spec.py:349] Evaluating on the test split.
I0129 07:09:44.727011 139822745589568 submission_runner.py:408] Time since start: 20628.08s, 	Step: 58537, 	{'train/accuracy': 0.21765385568141937, 'train/loss': 4.364476680755615, 'validation/accuracy': 0.20535999536514282, 'validation/loss': 4.498104095458984, 'validation/num_examples': 50000, 'test/accuracy': 0.15230000019073486, 'test/loss': 5.230589866638184, 'test/num_examples': 10000, 'score': 19925.873183965683, 'total_duration': 20628.083652734756, 'accumulated_submission_time': 19925.873183965683, 'accumulated_eval_time': 698.9143342971802, 'accumulated_logging_time': 1.2803306579589844}
I0129 07:09:44.758144 139655609571072 logging_writer.py:48] [58537] accumulated_eval_time=698.914334, accumulated_logging_time=1.280331, accumulated_submission_time=19925.873184, global_step=58537, preemption_count=0, score=19925.873184, test/accuracy=0.152300, test/loss=5.230590, test/num_examples=10000, total_duration=20628.083653, train/accuracy=0.217654, train/loss=4.364477, validation/accuracy=0.205360, validation/loss=4.498104, validation/num_examples=50000
I0129 07:10:06.417652 139655617963776 logging_writer.py:48] [58600] global_step=58600, grad_norm=1.298864722251892, loss=2.2604007720947266
I0129 07:10:40.297856 139655609571072 logging_writer.py:48] [58700] global_step=58700, grad_norm=1.1845871210098267, loss=2.2839527130126953
I0129 07:11:14.264169 139655617963776 logging_writer.py:48] [58800] global_step=58800, grad_norm=1.3427462577819824, loss=2.194823980331421
I0129 07:11:48.159937 139655609571072 logging_writer.py:48] [58900] global_step=58900, grad_norm=1.1938265562057495, loss=2.045039653778076
I0129 07:12:22.067883 139655617963776 logging_writer.py:48] [59000] global_step=59000, grad_norm=1.3005813360214233, loss=2.1936113834381104
I0129 07:12:55.982501 139655609571072 logging_writer.py:48] [59100] global_step=59100, grad_norm=1.4654005765914917, loss=2.13482666015625
I0129 07:13:29.897738 139655617963776 logging_writer.py:48] [59200] global_step=59200, grad_norm=1.251520037651062, loss=2.2836496829986572
I0129 07:14:03.800360 139655609571072 logging_writer.py:48] [59300] global_step=59300, grad_norm=1.137571096420288, loss=2.0932352542877197
I0129 07:14:37.708247 139655617963776 logging_writer.py:48] [59400] global_step=59400, grad_norm=1.3960095643997192, loss=2.125511646270752
I0129 07:15:11.640722 139655609571072 logging_writer.py:48] [59500] global_step=59500, grad_norm=1.3226901292800903, loss=2.2171335220336914
I0129 07:15:45.549663 139655617963776 logging_writer.py:48] [59600] global_step=59600, grad_norm=1.302700161933899, loss=2.2163662910461426
I0129 07:16:19.480476 139655609571072 logging_writer.py:48] [59700] global_step=59700, grad_norm=1.2489148378372192, loss=2.149467706680298
I0129 07:16:53.372672 139655617963776 logging_writer.py:48] [59800] global_step=59800, grad_norm=1.197109580039978, loss=2.134326457977295
I0129 07:17:27.303598 139655609571072 logging_writer.py:48] [59900] global_step=59900, grad_norm=1.1893866062164307, loss=2.1851534843444824
I0129 07:18:01.420369 139655617963776 logging_writer.py:48] [60000] global_step=60000, grad_norm=1.3888694047927856, loss=2.1764166355133057
I0129 07:18:14.788476 139822745589568 spec.py:321] Evaluating on the training split.
I0129 07:18:20.975299 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 07:18:29.689592 139822745589568 spec.py:349] Evaluating on the test split.
I0129 07:18:32.277667 139822745589568 submission_runner.py:408] Time since start: 21155.63s, 	Step: 60041, 	{'train/accuracy': 0.1741868555545807, 'train/loss': 5.3501811027526855, 'validation/accuracy': 0.16638000309467316, 'validation/loss': 5.571073055267334, 'validation/num_examples': 50000, 'test/accuracy': 0.13580000400543213, 'test/loss': 6.057143211364746, 'test/num_examples': 10000, 'score': 20435.8396422863, 'total_duration': 21155.63431572914, 'accumulated_submission_time': 20435.8396422863, 'accumulated_eval_time': 716.4034960269928, 'accumulated_logging_time': 1.3220326900482178}
I0129 07:18:32.306997 139655617963776 logging_writer.py:48] [60041] accumulated_eval_time=716.403496, accumulated_logging_time=1.322033, accumulated_submission_time=20435.839642, global_step=60041, preemption_count=0, score=20435.839642, test/accuracy=0.135800, test/loss=6.057143, test/num_examples=10000, total_duration=21155.634316, train/accuracy=0.174187, train/loss=5.350181, validation/accuracy=0.166380, validation/loss=5.571073, validation/num_examples=50000
I0129 07:18:52.625456 139656658151168 logging_writer.py:48] [60100] global_step=60100, grad_norm=1.4257341623306274, loss=2.269594192504883
I0129 07:19:26.494883 139655617963776 logging_writer.py:48] [60200] global_step=60200, grad_norm=1.4808013439178467, loss=2.1636438369750977
I0129 07:20:00.384942 139656658151168 logging_writer.py:48] [60300] global_step=60300, grad_norm=1.3772027492523193, loss=2.3041372299194336
I0129 07:20:34.274192 139655617963776 logging_writer.py:48] [60400] global_step=60400, grad_norm=1.3993955850601196, loss=2.0751662254333496
I0129 07:21:08.208256 139656658151168 logging_writer.py:48] [60500] global_step=60500, grad_norm=1.3291898965835571, loss=2.2042157649993896
I0129 07:21:42.107343 139655617963776 logging_writer.py:48] [60600] global_step=60600, grad_norm=1.2586032152175903, loss=2.1553518772125244
I0129 07:22:16.027204 139656658151168 logging_writer.py:48] [60700] global_step=60700, grad_norm=1.4450080394744873, loss=2.1356637477874756
I0129 07:22:49.901702 139655617963776 logging_writer.py:48] [60800] global_step=60800, grad_norm=1.288535475730896, loss=2.1982250213623047
I0129 07:23:23.804974 139656658151168 logging_writer.py:48] [60900] global_step=60900, grad_norm=1.3533297777175903, loss=2.2344884872436523
I0129 07:23:57.705258 139655617963776 logging_writer.py:48] [61000] global_step=61000, grad_norm=1.3765887022018433, loss=2.1861233711242676
I0129 07:24:31.731863 139656658151168 logging_writer.py:48] [61100] global_step=61100, grad_norm=1.2683136463165283, loss=2.1771137714385986
I0129 07:25:05.645758 139655617963776 logging_writer.py:48] [61200] global_step=61200, grad_norm=1.3870793581008911, loss=2.328340530395508
I0129 07:25:39.564209 139656658151168 logging_writer.py:48] [61300] global_step=61300, grad_norm=1.3469829559326172, loss=2.201647996902466
I0129 07:26:13.485001 139655617963776 logging_writer.py:48] [61400] global_step=61400, grad_norm=1.3359906673431396, loss=2.2635908126831055
I0129 07:26:47.379470 139656658151168 logging_writer.py:48] [61500] global_step=61500, grad_norm=1.2129231691360474, loss=2.0932016372680664
I0129 07:27:02.468677 139822745589568 spec.py:321] Evaluating on the training split.
I0129 07:27:08.785723 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 07:27:17.546794 139822745589568 spec.py:349] Evaluating on the test split.
I0129 07:27:20.147119 139822745589568 submission_runner.py:408] Time since start: 21683.50s, 	Step: 61546, 	{'train/accuracy': 0.20653299987316132, 'train/loss': 4.479652404785156, 'validation/accuracy': 0.18648000061511993, 'validation/loss': 4.748819351196289, 'validation/num_examples': 50000, 'test/accuracy': 0.14710000157356262, 'test/loss': 5.390626907348633, 'test/num_examples': 10000, 'score': 20945.94157910347, 'total_duration': 21683.503759384155, 'accumulated_submission_time': 20945.94157910347, 'accumulated_eval_time': 734.0819170475006, 'accumulated_logging_time': 1.3605809211730957}
I0129 07:27:20.178816 139656297445120 logging_writer.py:48] [61546] accumulated_eval_time=734.081917, accumulated_logging_time=1.360581, accumulated_submission_time=20945.941579, global_step=61546, preemption_count=0, score=20945.941579, test/accuracy=0.147100, test/loss=5.390627, test/num_examples=10000, total_duration=21683.503759, train/accuracy=0.206533, train/loss=4.479652, validation/accuracy=0.186480, validation/loss=4.748819, validation/num_examples=50000
I0129 07:27:38.829604 139656649758464 logging_writer.py:48] [61600] global_step=61600, grad_norm=1.3659465312957764, loss=2.0493521690368652
I0129 07:28:12.703284 139656297445120 logging_writer.py:48] [61700] global_step=61700, grad_norm=1.463460087776184, loss=2.1860368251800537
I0129 07:28:46.559722 139656649758464 logging_writer.py:48] [61800] global_step=61800, grad_norm=1.3036267757415771, loss=2.107330083847046
I0129 07:29:20.465380 139656297445120 logging_writer.py:48] [61900] global_step=61900, grad_norm=1.3003044128417969, loss=2.2803354263305664
I0129 07:29:54.363293 139656649758464 logging_writer.py:48] [62000] global_step=62000, grad_norm=1.3014745712280273, loss=2.093985080718994
I0129 07:30:28.307030 139656297445120 logging_writer.py:48] [62100] global_step=62100, grad_norm=1.2933621406555176, loss=2.181403160095215
I0129 07:31:02.269199 139656649758464 logging_writer.py:48] [62200] global_step=62200, grad_norm=1.2733298540115356, loss=2.161924362182617
I0129 07:31:36.117377 139656297445120 logging_writer.py:48] [62300] global_step=62300, grad_norm=1.3281137943267822, loss=2.089189052581787
I0129 07:32:10.009671 139656649758464 logging_writer.py:48] [62400] global_step=62400, grad_norm=1.2427890300750732, loss=2.157647132873535
I0129 07:32:43.917288 139656297445120 logging_writer.py:48] [62500] global_step=62500, grad_norm=1.2689319849014282, loss=2.277761459350586
I0129 07:33:17.778790 139656649758464 logging_writer.py:48] [62600] global_step=62600, grad_norm=1.4092785120010376, loss=2.108220100402832
I0129 07:33:51.668016 139656297445120 logging_writer.py:48] [62700] global_step=62700, grad_norm=1.2703664302825928, loss=2.2835092544555664
I0129 07:34:25.543385 139656649758464 logging_writer.py:48] [62800] global_step=62800, grad_norm=1.4014101028442383, loss=2.2075657844543457
I0129 07:34:59.465177 139656297445120 logging_writer.py:48] [62900] global_step=62900, grad_norm=1.4034355878829956, loss=2.177635431289673
I0129 07:35:33.349158 139656649758464 logging_writer.py:48] [63000] global_step=63000, grad_norm=1.2953956127166748, loss=2.1562678813934326
I0129 07:35:50.449518 139822745589568 spec.py:321] Evaluating on the training split.
I0129 07:35:56.649400 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 07:36:05.427694 139822745589568 spec.py:349] Evaluating on the test split.
I0129 07:36:08.079449 139822745589568 submission_runner.py:408] Time since start: 22211.44s, 	Step: 63052, 	{'train/accuracy': 0.3102877736091614, 'train/loss': 3.5084850788116455, 'validation/accuracy': 0.2889399826526642, 'validation/loss': 3.657221794128418, 'validation/num_examples': 50000, 'test/accuracy': 0.21540001034736633, 'test/loss': 4.478518962860107, 'test/num_examples': 10000, 'score': 21456.14753627777, 'total_duration': 22211.436054468155, 'accumulated_submission_time': 21456.14753627777, 'accumulated_eval_time': 751.711775302887, 'accumulated_logging_time': 1.405496597290039}
I0129 07:36:08.103792 139655617963776 logging_writer.py:48] [63052] accumulated_eval_time=751.711775, accumulated_logging_time=1.405497, accumulated_submission_time=21456.147536, global_step=63052, preemption_count=0, score=21456.147536, test/accuracy=0.215400, test/loss=4.478519, test/num_examples=10000, total_duration=22211.436054, train/accuracy=0.310288, train/loss=3.508485, validation/accuracy=0.288940, validation/loss=3.657222, validation/num_examples=50000
I0129 07:36:24.708663 139655626356480 logging_writer.py:48] [63100] global_step=63100, grad_norm=1.2829351425170898, loss=2.2249059677124023
I0129 07:36:58.601236 139655617963776 logging_writer.py:48] [63200] global_step=63200, grad_norm=1.4525835514068604, loss=2.188108444213867
I0129 07:37:32.595049 139655626356480 logging_writer.py:48] [63300] global_step=63300, grad_norm=1.3471568822860718, loss=2.35014271736145
I0129 07:38:06.511580 139655617963776 logging_writer.py:48] [63400] global_step=63400, grad_norm=1.3131871223449707, loss=2.2534708976745605
I0129 07:38:40.428055 139655626356480 logging_writer.py:48] [63500] global_step=63500, grad_norm=1.2340037822723389, loss=2.2655725479125977
I0129 07:39:14.370116 139655617963776 logging_writer.py:48] [63600] global_step=63600, grad_norm=1.2960853576660156, loss=2.146639347076416
I0129 07:39:48.274810 139655626356480 logging_writer.py:48] [63700] global_step=63700, grad_norm=1.4147272109985352, loss=2.235783576965332
I0129 07:40:22.186809 139655617963776 logging_writer.py:48] [63800] global_step=63800, grad_norm=1.282693862915039, loss=2.105386972427368
I0129 07:40:56.094763 139655626356480 logging_writer.py:48] [63900] global_step=63900, grad_norm=1.2087045907974243, loss=2.114034652709961
I0129 07:41:30.007740 139655617963776 logging_writer.py:48] [64000] global_step=64000, grad_norm=1.3458473682403564, loss=2.1398658752441406
I0129 07:42:03.915698 139655626356480 logging_writer.py:48] [64100] global_step=64100, grad_norm=1.2475613355636597, loss=2.144435167312622
I0129 07:42:37.837833 139655617963776 logging_writer.py:48] [64200] global_step=64200, grad_norm=1.265026569366455, loss=2.1789920330047607
I0129 07:43:11.751195 139655626356480 logging_writer.py:48] [64300] global_step=64300, grad_norm=1.3587560653686523, loss=2.200287342071533
I0129 07:43:45.731557 139655617963776 logging_writer.py:48] [64400] global_step=64400, grad_norm=1.389501929283142, loss=2.173560619354248
I0129 07:44:19.633152 139655626356480 logging_writer.py:48] [64500] global_step=64500, grad_norm=1.3580642938613892, loss=2.152695417404175
I0129 07:44:38.095068 139822745589568 spec.py:321] Evaluating on the training split.
I0129 07:44:44.258670 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 07:44:52.731163 139822745589568 spec.py:349] Evaluating on the test split.
I0129 07:44:55.360865 139822745589568 submission_runner.py:408] Time since start: 22738.72s, 	Step: 64556, 	{'train/accuracy': 0.30291372537612915, 'train/loss': 3.726241111755371, 'validation/accuracy': 0.28321999311447144, 'validation/loss': 3.931478977203369, 'validation/num_examples': 50000, 'test/accuracy': 0.21070000529289246, 'test/loss': 4.761738300323486, 'test/num_examples': 10000, 'score': 21966.07893872261, 'total_duration': 22738.717492341995, 'accumulated_submission_time': 21966.07893872261, 'accumulated_eval_time': 768.9775350093842, 'accumulated_logging_time': 1.4379546642303467}
I0129 07:44:55.390572 139656666543872 logging_writer.py:48] [64556] accumulated_eval_time=768.977535, accumulated_logging_time=1.437955, accumulated_submission_time=21966.078939, global_step=64556, preemption_count=0, score=21966.078939, test/accuracy=0.210700, test/loss=4.761738, test/num_examples=10000, total_duration=22738.717492, train/accuracy=0.302914, train/loss=3.726241, validation/accuracy=0.283220, validation/loss=3.931479, validation/num_examples=50000
I0129 07:45:10.621025 139656834316032 logging_writer.py:48] [64600] global_step=64600, grad_norm=1.2428457736968994, loss=2.129511594772339
I0129 07:45:44.497218 139656666543872 logging_writer.py:48] [64700] global_step=64700, grad_norm=1.308168888092041, loss=2.1433870792388916
I0129 07:46:18.408813 139656834316032 logging_writer.py:48] [64800] global_step=64800, grad_norm=1.330668568611145, loss=2.1719558238983154
I0129 07:46:52.285527 139656666543872 logging_writer.py:48] [64900] global_step=64900, grad_norm=1.2432211637496948, loss=2.073692798614502
I0129 07:47:26.193147 139656834316032 logging_writer.py:48] [65000] global_step=65000, grad_norm=1.2365590333938599, loss=2.1324920654296875
I0129 07:48:00.052242 139656666543872 logging_writer.py:48] [65100] global_step=65100, grad_norm=1.3192362785339355, loss=2.177236318588257
I0129 07:48:33.959056 139656834316032 logging_writer.py:48] [65200] global_step=65200, grad_norm=1.338303804397583, loss=2.1057753562927246
I0129 07:49:07.854211 139656666543872 logging_writer.py:48] [65300] global_step=65300, grad_norm=1.3129363059997559, loss=2.1383886337280273
I0129 07:49:41.764211 139656834316032 logging_writer.py:48] [65400] global_step=65400, grad_norm=1.328838586807251, loss=2.156682014465332
I0129 07:50:15.752631 139656666543872 logging_writer.py:48] [65500] global_step=65500, grad_norm=1.303684949874878, loss=2.1402151584625244
I0129 07:50:49.673592 139656834316032 logging_writer.py:48] [65600] global_step=65600, grad_norm=1.5434836149215698, loss=2.167018413543701
I0129 07:51:23.567874 139656666543872 logging_writer.py:48] [65700] global_step=65700, grad_norm=1.3357608318328857, loss=2.217869997024536
I0129 07:51:57.488338 139656834316032 logging_writer.py:48] [65800] global_step=65800, grad_norm=1.4145303964614868, loss=1.9873614311218262
I0129 07:52:31.380585 139656666543872 logging_writer.py:48] [65900] global_step=65900, grad_norm=1.2467516660690308, loss=2.271303653717041
I0129 07:53:05.306497 139656834316032 logging_writer.py:48] [66000] global_step=66000, grad_norm=1.4924372434616089, loss=2.2657344341278076
I0129 07:53:25.466368 139822745589568 spec.py:321] Evaluating on the training split.
I0129 07:53:31.675603 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 07:53:40.398699 139822745589568 spec.py:349] Evaluating on the test split.
I0129 07:53:42.956744 139822745589568 submission_runner.py:408] Time since start: 23266.31s, 	Step: 66061, 	{'train/accuracy': 0.34384962916374207, 'train/loss': 3.1373069286346436, 'validation/accuracy': 0.3238599896430969, 'validation/loss': 3.2995736598968506, 'validation/num_examples': 50000, 'test/accuracy': 0.24240000545978546, 'test/loss': 4.018667221069336, 'test/num_examples': 10000, 'score': 22476.09085536003, 'total_duration': 23266.313386917114, 'accumulated_submission_time': 22476.09085536003, 'accumulated_eval_time': 786.4678730964661, 'accumulated_logging_time': 1.4794397354125977}
I0129 07:53:42.987768 139655617963776 logging_writer.py:48] [66061] accumulated_eval_time=786.467873, accumulated_logging_time=1.479440, accumulated_submission_time=22476.090855, global_step=66061, preemption_count=0, score=22476.090855, test/accuracy=0.242400, test/loss=4.018667, test/num_examples=10000, total_duration=23266.313387, train/accuracy=0.343850, train/loss=3.137307, validation/accuracy=0.323860, validation/loss=3.299574, validation/num_examples=50000
I0129 07:53:56.522795 139655626356480 logging_writer.py:48] [66100] global_step=66100, grad_norm=1.274897813796997, loss=2.1708929538726807
I0129 07:54:30.378730 139655617963776 logging_writer.py:48] [66200] global_step=66200, grad_norm=1.2960245609283447, loss=2.1960370540618896
I0129 07:55:04.233455 139655626356480 logging_writer.py:48] [66300] global_step=66300, grad_norm=1.2661138772964478, loss=2.1762492656707764
I0129 07:55:38.117150 139655617963776 logging_writer.py:48] [66400] global_step=66400, grad_norm=1.2453718185424805, loss=2.2095835208892822
I0129 07:56:12.033032 139655626356480 logging_writer.py:48] [66500] global_step=66500, grad_norm=1.3440881967544556, loss=2.157456874847412
I0129 07:56:46.117962 139655617963776 logging_writer.py:48] [66600] global_step=66600, grad_norm=1.3566231727600098, loss=2.081200122833252
I0129 07:57:20.021622 139655626356480 logging_writer.py:48] [66700] global_step=66700, grad_norm=1.4275197982788086, loss=2.1528289318084717
I0129 07:57:53.906002 139655617963776 logging_writer.py:48] [66800] global_step=66800, grad_norm=1.4380489587783813, loss=2.1129672527313232
I0129 07:58:27.805038 139655626356480 logging_writer.py:48] [66900] global_step=66900, grad_norm=1.3128340244293213, loss=2.175945281982422
I0129 07:59:01.694511 139655617963776 logging_writer.py:48] [67000] global_step=67000, grad_norm=1.2853442430496216, loss=2.169687032699585
I0129 07:59:35.577922 139655626356480 logging_writer.py:48] [67100] global_step=67100, grad_norm=1.3675330877304077, loss=2.318619966506958
I0129 08:00:09.499512 139655617963776 logging_writer.py:48] [67200] global_step=67200, grad_norm=1.3003458976745605, loss=2.046935558319092
I0129 08:00:43.368850 139655626356480 logging_writer.py:48] [67300] global_step=67300, grad_norm=1.2810291051864624, loss=2.0999367237091064
I0129 08:01:17.239581 139655617963776 logging_writer.py:48] [67400] global_step=67400, grad_norm=1.3916361331939697, loss=2.296410322189331
I0129 08:01:51.148714 139655626356480 logging_writer.py:48] [67500] global_step=67500, grad_norm=1.4685838222503662, loss=2.0325396060943604
I0129 08:02:12.964867 139822745589568 spec.py:321] Evaluating on the training split.
I0129 08:02:19.197739 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 08:02:27.832066 139822745589568 spec.py:349] Evaluating on the test split.
I0129 08:02:30.455813 139822745589568 submission_runner.py:408] Time since start: 23793.81s, 	Step: 67566, 	{'train/accuracy': 0.32421875, 'train/loss': 3.3596620559692383, 'validation/accuracy': 0.31985998153686523, 'validation/loss': 3.3958446979522705, 'validation/num_examples': 50000, 'test/accuracy': 0.2331000119447708, 'test/loss': 4.2524847984313965, 'test/num_examples': 10000, 'score': 22986.0070104599, 'total_duration': 23793.81243133545, 'accumulated_submission_time': 22986.0070104599, 'accumulated_eval_time': 803.9587597846985, 'accumulated_logging_time': 1.5198431015014648}
I0129 08:02:30.499886 139655626356480 logging_writer.py:48] [67566] accumulated_eval_time=803.958760, accumulated_logging_time=1.519843, accumulated_submission_time=22986.007010, global_step=67566, preemption_count=0, score=22986.007010, test/accuracy=0.233100, test/loss=4.252485, test/num_examples=10000, total_duration=23793.812431, train/accuracy=0.324219, train/loss=3.359662, validation/accuracy=0.319860, validation/loss=3.395845, validation/num_examples=50000
I0129 08:02:42.314260 139656666543872 logging_writer.py:48] [67600] global_step=67600, grad_norm=1.4777806997299194, loss=2.250331401824951
I0129 08:03:16.307724 139655626356480 logging_writer.py:48] [67700] global_step=67700, grad_norm=1.527246117591858, loss=2.2349131107330322
I0129 08:03:50.162030 139656666543872 logging_writer.py:48] [67800] global_step=67800, grad_norm=1.3362902402877808, loss=2.150448799133301
I0129 08:04:24.044058 139655626356480 logging_writer.py:48] [67900] global_step=67900, grad_norm=1.2517311573028564, loss=1.9877159595489502
I0129 08:04:57.920604 139656666543872 logging_writer.py:48] [68000] global_step=68000, grad_norm=1.35444974899292, loss=2.1907730102539062
I0129 08:05:31.818705 139655626356480 logging_writer.py:48] [68100] global_step=68100, grad_norm=1.2714447975158691, loss=2.210442304611206
I0129 08:06:05.721250 139656666543872 logging_writer.py:48] [68200] global_step=68200, grad_norm=1.2896220684051514, loss=2.1845240592956543
I0129 08:06:39.639196 139655626356480 logging_writer.py:48] [68300] global_step=68300, grad_norm=1.3519080877304077, loss=2.1994872093200684
I0129 08:07:13.558823 139656666543872 logging_writer.py:48] [68400] global_step=68400, grad_norm=1.3929356336593628, loss=2.1680521965026855
I0129 08:07:47.488162 139655626356480 logging_writer.py:48] [68500] global_step=68500, grad_norm=1.366827130317688, loss=2.1316709518432617
I0129 08:08:21.417446 139656666543872 logging_writer.py:48] [68600] global_step=68600, grad_norm=1.3806101083755493, loss=2.0287582874298096
I0129 08:08:55.329765 139655626356480 logging_writer.py:48] [68700] global_step=68700, grad_norm=1.2950483560562134, loss=2.061904191970825
I0129 08:09:29.272034 139656666543872 logging_writer.py:48] [68800] global_step=68800, grad_norm=1.3243963718414307, loss=2.1862730979919434
I0129 08:10:03.176364 139655626356480 logging_writer.py:48] [68900] global_step=68900, grad_norm=1.2819907665252686, loss=2.0735716819763184
I0129 08:10:37.107589 139656666543872 logging_writer.py:48] [69000] global_step=69000, grad_norm=1.3267899751663208, loss=2.1641225814819336
I0129 08:11:00.668555 139822745589568 spec.py:321] Evaluating on the training split.
I0129 08:11:06.921748 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 08:11:15.455498 139822745589568 spec.py:349] Evaluating on the test split.
I0129 08:11:18.058889 139822745589568 submission_runner.py:408] Time since start: 24321.42s, 	Step: 69071, 	{'train/accuracy': 0.3446069657802582, 'train/loss': 3.2224009037017822, 'validation/accuracy': 0.32811999320983887, 'validation/loss': 3.3014426231384277, 'validation/num_examples': 50000, 'test/accuracy': 0.2605000138282776, 'test/loss': 3.976634979248047, 'test/num_examples': 10000, 'score': 23496.11438536644, 'total_duration': 24321.4155292511, 'accumulated_submission_time': 23496.11438536644, 'accumulated_eval_time': 821.3490543365479, 'accumulated_logging_time': 1.5737159252166748}
I0129 08:11:18.091048 139655617963776 logging_writer.py:48] [69071] accumulated_eval_time=821.349054, accumulated_logging_time=1.573716, accumulated_submission_time=23496.114385, global_step=69071, preemption_count=0, score=23496.114385, test/accuracy=0.260500, test/loss=3.976635, test/num_examples=10000, total_duration=24321.415529, train/accuracy=0.344607, train/loss=3.222401, validation/accuracy=0.328120, validation/loss=3.301443, validation/num_examples=50000
I0129 08:11:28.259355 139655626356480 logging_writer.py:48] [69100] global_step=69100, grad_norm=1.3641791343688965, loss=2.144782304763794
I0129 08:12:02.134023 139655617963776 logging_writer.py:48] [69200] global_step=69200, grad_norm=1.4710588455200195, loss=2.1920478343963623
I0129 08:12:36.051935 139655626356480 logging_writer.py:48] [69300] global_step=69300, grad_norm=1.3672475814819336, loss=2.1356663703918457
I0129 08:13:09.955557 139655617963776 logging_writer.py:48] [69400] global_step=69400, grad_norm=1.329479694366455, loss=2.0874619483947754
I0129 08:13:43.887658 139655626356480 logging_writer.py:48] [69500] global_step=69500, grad_norm=1.4871615171432495, loss=2.2466044425964355
I0129 08:14:17.790492 139655617963776 logging_writer.py:48] [69600] global_step=69600, grad_norm=1.2947453260421753, loss=2.1165456771850586
I0129 08:14:51.692956 139655626356480 logging_writer.py:48] [69700] global_step=69700, grad_norm=1.4194682836532593, loss=2.1159303188323975
I0129 08:15:25.628084 139655617963776 logging_writer.py:48] [69800] global_step=69800, grad_norm=1.494023084640503, loss=2.074469566345215
I0129 08:15:59.598551 139655626356480 logging_writer.py:48] [69900] global_step=69900, grad_norm=1.3185079097747803, loss=2.15861177444458
I0129 08:16:33.520226 139655617963776 logging_writer.py:48] [70000] global_step=70000, grad_norm=1.4779495000839233, loss=2.171614408493042
I0129 08:17:07.394838 139655626356480 logging_writer.py:48] [70100] global_step=70100, grad_norm=1.4843066930770874, loss=2.2606711387634277
I0129 08:17:41.322826 139655617963776 logging_writer.py:48] [70200] global_step=70200, grad_norm=1.4414138793945312, loss=2.17596697807312
I0129 08:18:15.208205 139655626356480 logging_writer.py:48] [70300] global_step=70300, grad_norm=1.307367205619812, loss=2.1191940307617188
I0129 08:18:49.118685 139655617963776 logging_writer.py:48] [70400] global_step=70400, grad_norm=1.3371927738189697, loss=2.154231548309326
I0129 08:19:23.020597 139655626356480 logging_writer.py:48] [70500] global_step=70500, grad_norm=1.4418507814407349, loss=2.086827039718628
I0129 08:19:48.251136 139822745589568 spec.py:321] Evaluating on the training split.
I0129 08:19:54.488980 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 08:20:03.226287 139822745589568 spec.py:349] Evaluating on the test split.
I0129 08:20:05.814055 139822745589568 submission_runner.py:408] Time since start: 24849.17s, 	Step: 70576, 	{'train/accuracy': 0.3404615819454193, 'train/loss': 3.1158764362335205, 'validation/accuracy': 0.3131199777126312, 'validation/loss': 3.278167486190796, 'validation/num_examples': 50000, 'test/accuracy': 0.23570001125335693, 'test/loss': 3.938102960586548, 'test/num_examples': 10000, 'score': 24006.213018655777, 'total_duration': 24849.170696020126, 'accumulated_submission_time': 24006.213018655777, 'accumulated_eval_time': 838.9119355678558, 'accumulated_logging_time': 1.6166942119598389}
I0129 08:20:05.850252 139656834316032 logging_writer.py:48] [70576] accumulated_eval_time=838.911936, accumulated_logging_time=1.616694, accumulated_submission_time=24006.213019, global_step=70576, preemption_count=0, score=24006.213019, test/accuracy=0.235700, test/loss=3.938103, test/num_examples=10000, total_duration=24849.170696, train/accuracy=0.340462, train/loss=3.115876, validation/accuracy=0.313120, validation/loss=3.278167, validation/num_examples=50000
I0129 08:20:14.319070 139658730145536 logging_writer.py:48] [70600] global_step=70600, grad_norm=1.4514174461364746, loss=2.155089855194092
I0129 08:20:48.169736 139656834316032 logging_writer.py:48] [70700] global_step=70700, grad_norm=1.563631534576416, loss=2.278412103652954
I0129 08:21:22.068468 139658730145536 logging_writer.py:48] [70800] global_step=70800, grad_norm=1.403580665588379, loss=2.250925302505493
I0129 08:21:55.969426 139656834316032 logging_writer.py:48] [70900] global_step=70900, grad_norm=1.415326714515686, loss=2.0491199493408203
I0129 08:22:30.080203 139658730145536 logging_writer.py:48] [71000] global_step=71000, grad_norm=1.3878798484802246, loss=2.1873795986175537
I0129 08:23:03.998989 139656834316032 logging_writer.py:48] [71100] global_step=71100, grad_norm=1.3101081848144531, loss=2.0938878059387207
I0129 08:23:37.878672 139658730145536 logging_writer.py:48] [71200] global_step=71200, grad_norm=1.42606782913208, loss=2.056884288787842
I0129 08:24:11.790335 139656834316032 logging_writer.py:48] [71300] global_step=71300, grad_norm=1.3654279708862305, loss=2.1449739933013916
I0129 08:24:45.691085 139658730145536 logging_writer.py:48] [71400] global_step=71400, grad_norm=1.3816938400268555, loss=2.1024365425109863
I0129 08:25:19.602775 139656834316032 logging_writer.py:48] [71500] global_step=71500, grad_norm=1.3802469968795776, loss=2.212066650390625
I0129 08:25:53.522368 139658730145536 logging_writer.py:48] [71600] global_step=71600, grad_norm=1.4425400495529175, loss=2.032501220703125
I0129 08:26:27.413172 139656834316032 logging_writer.py:48] [71700] global_step=71700, grad_norm=1.3399051427841187, loss=2.2208540439605713
I0129 08:27:01.298443 139658730145536 logging_writer.py:48] [71800] global_step=71800, grad_norm=1.3272632360458374, loss=2.088982582092285
I0129 08:27:35.229481 139656834316032 logging_writer.py:48] [71900] global_step=71900, grad_norm=1.4082616567611694, loss=1.9939335584640503
I0129 08:28:09.136165 139658730145536 logging_writer.py:48] [72000] global_step=72000, grad_norm=1.457857608795166, loss=2.1557681560516357
I0129 08:28:36.066049 139822745589568 spec.py:321] Evaluating on the training split.
I0129 08:28:42.308084 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 08:28:50.939447 139822745589568 spec.py:349] Evaluating on the test split.
I0129 08:28:53.482491 139822745589568 submission_runner.py:408] Time since start: 25376.84s, 	Step: 72081, 	{'train/accuracy': 0.46934789419174194, 'train/loss': 2.3059208393096924, 'validation/accuracy': 0.4316200017929077, 'validation/loss': 2.5475010871887207, 'validation/num_examples': 50000, 'test/accuracy': 0.31870001554489136, 'test/loss': 3.359588146209717, 'test/num_examples': 10000, 'score': 24516.367866277695, 'total_duration': 25376.83913421631, 'accumulated_submission_time': 24516.367866277695, 'accumulated_eval_time': 856.3283641338348, 'accumulated_logging_time': 1.6619768142700195}
I0129 08:28:53.520477 139655626356480 logging_writer.py:48] [72081] accumulated_eval_time=856.328364, accumulated_logging_time=1.661977, accumulated_submission_time=24516.367866, global_step=72081, preemption_count=0, score=24516.367866, test/accuracy=0.318700, test/loss=3.359588, test/num_examples=10000, total_duration=25376.839134, train/accuracy=0.469348, train/loss=2.305921, validation/accuracy=0.431620, validation/loss=2.547501, validation/num_examples=50000
I0129 08:29:00.443389 139656297445120 logging_writer.py:48] [72100] global_step=72100, grad_norm=1.3760297298431396, loss=2.07667875289917
I0129 08:29:34.325472 139655626356480 logging_writer.py:48] [72200] global_step=72200, grad_norm=1.3290748596191406, loss=2.06785249710083
I0129 08:30:08.207082 139656297445120 logging_writer.py:48] [72300] global_step=72300, grad_norm=1.4521830081939697, loss=2.2060275077819824
I0129 08:30:42.076050 139655626356480 logging_writer.py:48] [72400] global_step=72400, grad_norm=1.6896812915802002, loss=2.2334682941436768
I0129 08:31:16.006217 139656297445120 logging_writer.py:48] [72500] global_step=72500, grad_norm=1.437673807144165, loss=2.1911325454711914
I0129 08:31:49.932593 139655626356480 logging_writer.py:48] [72600] global_step=72600, grad_norm=1.2832249402999878, loss=2.0505764484405518
I0129 08:32:23.846077 139656297445120 logging_writer.py:48] [72700] global_step=72700, grad_norm=1.4453437328338623, loss=2.1999576091766357
I0129 08:32:57.764952 139655626356480 logging_writer.py:48] [72800] global_step=72800, grad_norm=1.2907277345657349, loss=2.0811727046966553
I0129 08:33:31.666507 139656297445120 logging_writer.py:48] [72900] global_step=72900, grad_norm=1.4481301307678223, loss=2.1219329833984375
I0129 08:34:05.559306 139655626356480 logging_writer.py:48] [73000] global_step=73000, grad_norm=1.333521842956543, loss=2.0859200954437256
I0129 08:34:39.455599 139656297445120 logging_writer.py:48] [73100] global_step=73100, grad_norm=1.6153990030288696, loss=2.193480968475342
I0129 08:35:13.373160 139655626356480 logging_writer.py:48] [73200] global_step=73200, grad_norm=1.3811691999435425, loss=2.155893325805664
I0129 08:35:47.335189 139656297445120 logging_writer.py:48] [73300] global_step=73300, grad_norm=1.3799107074737549, loss=2.1855785846710205
I0129 08:36:21.257943 139655626356480 logging_writer.py:48] [73400] global_step=73400, grad_norm=1.385636329650879, loss=2.0150418281555176
I0129 08:36:55.156733 139656297445120 logging_writer.py:48] [73500] global_step=73500, grad_norm=1.7408918142318726, loss=2.276430368423462
I0129 08:37:23.780533 139822745589568 spec.py:321] Evaluating on the training split.
I0129 08:37:30.020420 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 08:37:38.644293 139822745589568 spec.py:349] Evaluating on the test split.
I0129 08:37:41.138602 139822745589568 submission_runner.py:408] Time since start: 25904.50s, 	Step: 73586, 	{'train/accuracy': 0.5002591013908386, 'train/loss': 2.164903163909912, 'validation/accuracy': 0.45933997631073, 'validation/loss': 2.4115843772888184, 'validation/num_examples': 50000, 'test/accuracy': 0.3611000180244446, 'test/loss': 3.1056861877441406, 'test/num_examples': 10000, 'score': 25026.566119670868, 'total_duration': 25904.495133399963, 'accumulated_submission_time': 25026.566119670868, 'accumulated_eval_time': 873.6862845420837, 'accumulated_logging_time': 1.7103638648986816}
I0129 08:37:41.169412 139656834316032 logging_writer.py:48] [73586] accumulated_eval_time=873.686285, accumulated_logging_time=1.710364, accumulated_submission_time=25026.566120, global_step=73586, preemption_count=0, score=25026.566120, test/accuracy=0.361100, test/loss=3.105686, test/num_examples=10000, total_duration=25904.495133, train/accuracy=0.500259, train/loss=2.164903, validation/accuracy=0.459340, validation/loss=2.411584, validation/num_examples=50000
I0129 08:37:46.260944 139658730145536 logging_writer.py:48] [73600] global_step=73600, grad_norm=1.3071624040603638, loss=1.944068431854248
I0129 08:38:20.166255 139656834316032 logging_writer.py:48] [73700] global_step=73700, grad_norm=1.4604071378707886, loss=2.1315996646881104
I0129 08:38:54.046292 139658730145536 logging_writer.py:48] [73800] global_step=73800, grad_norm=1.3312231302261353, loss=2.073676347732544
I0129 08:39:27.910792 139656834316032 logging_writer.py:48] [73900] global_step=73900, grad_norm=1.474068284034729, loss=2.144331455230713
I0129 08:40:01.844167 139658730145536 logging_writer.py:48] [74000] global_step=74000, grad_norm=1.4045138359069824, loss=2.078822612762451
I0129 08:40:35.722350 139656834316032 logging_writer.py:48] [74100] global_step=74100, grad_norm=1.4053038358688354, loss=1.9955098628997803
I0129 08:41:09.677285 139658730145536 logging_writer.py:48] [74200] global_step=74200, grad_norm=1.4610081911087036, loss=2.0548019409179688
I0129 08:41:43.522902 139656834316032 logging_writer.py:48] [74300] global_step=74300, grad_norm=1.50050687789917, loss=2.18727970123291
I0129 08:42:17.482973 139658730145536 logging_writer.py:48] [74400] global_step=74400, grad_norm=1.497009515762329, loss=2.0241475105285645
I0129 08:42:51.364829 139656834316032 logging_writer.py:48] [74500] global_step=74500, grad_norm=1.4140079021453857, loss=2.125373363494873
I0129 08:43:25.275583 139658730145536 logging_writer.py:48] [74600] global_step=74600, grad_norm=1.4529091119766235, loss=2.167773723602295
I0129 08:43:59.157403 139656834316032 logging_writer.py:48] [74700] global_step=74700, grad_norm=1.4539296627044678, loss=2.1160216331481934
I0129 08:44:33.040417 139658730145536 logging_writer.py:48] [74800] global_step=74800, grad_norm=1.3954938650131226, loss=2.2524569034576416
I0129 08:45:06.968769 139656834316032 logging_writer.py:48] [74900] global_step=74900, grad_norm=1.5073375701904297, loss=2.0796990394592285
I0129 08:45:40.892243 139658730145536 logging_writer.py:48] [75000] global_step=75000, grad_norm=1.5606689453125, loss=2.019138813018799
I0129 08:46:11.188537 139822745589568 spec.py:321] Evaluating on the training split.
I0129 08:46:17.374641 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 08:46:26.151590 139822745589568 spec.py:349] Evaluating on the test split.
I0129 08:46:28.733167 139822745589568 submission_runner.py:408] Time since start: 26432.09s, 	Step: 75091, 	{'train/accuracy': 0.3360172212123871, 'train/loss': 3.280001163482666, 'validation/accuracy': 0.3144199848175049, 'validation/loss': 3.4575531482696533, 'validation/num_examples': 50000, 'test/accuracy': 0.23020000755786896, 'test/loss': 4.258174419403076, 'test/num_examples': 10000, 'score': 25536.524069309235, 'total_duration': 26432.089807510376, 'accumulated_submission_time': 25536.524069309235, 'accumulated_eval_time': 891.2308826446533, 'accumulated_logging_time': 1.750760555267334}
I0129 08:46:28.764291 139655617963776 logging_writer.py:48] [75091] accumulated_eval_time=891.230883, accumulated_logging_time=1.750761, accumulated_submission_time=25536.524069, global_step=75091, preemption_count=0, score=25536.524069, test/accuracy=0.230200, test/loss=4.258174, test/num_examples=10000, total_duration=26432.089808, train/accuracy=0.336017, train/loss=3.280001, validation/accuracy=0.314420, validation/loss=3.457553, validation/num_examples=50000
I0129 08:46:32.144739 139655626356480 logging_writer.py:48] [75100] global_step=75100, grad_norm=1.3569575548171997, loss=2.1133639812469482
I0129 08:47:05.987120 139655617963776 logging_writer.py:48] [75200] global_step=75200, grad_norm=1.4241365194320679, loss=2.091585159301758
I0129 08:47:39.881235 139655626356480 logging_writer.py:48] [75300] global_step=75300, grad_norm=1.4477254152297974, loss=2.105353355407715
I0129 08:48:13.781588 139655617963776 logging_writer.py:48] [75400] global_step=75400, grad_norm=1.4365543127059937, loss=2.0397233963012695
I0129 08:48:47.800199 139655626356480 logging_writer.py:48] [75500] global_step=75500, grad_norm=1.513401746749878, loss=2.157552719116211
I0129 08:49:21.706333 139655617963776 logging_writer.py:48] [75600] global_step=75600, grad_norm=1.469138503074646, loss=2.215536594390869
I0129 08:49:55.639202 139655626356480 logging_writer.py:48] [75700] global_step=75700, grad_norm=1.4355452060699463, loss=2.147698402404785
I0129 08:50:29.552202 139655617963776 logging_writer.py:48] [75800] global_step=75800, grad_norm=1.6415637731552124, loss=2.1254773139953613
I0129 08:51:03.438828 139655626356480 logging_writer.py:48] [75900] global_step=75900, grad_norm=1.4302196502685547, loss=2.0944013595581055
I0129 08:51:37.347774 139655617963776 logging_writer.py:48] [76000] global_step=76000, grad_norm=1.2879713773727417, loss=2.099970579147339
I0129 08:52:11.210255 139655626356480 logging_writer.py:48] [76100] global_step=76100, grad_norm=1.4174991846084595, loss=2.0831289291381836
I0129 08:52:45.117245 139655617963776 logging_writer.py:48] [76200] global_step=76200, grad_norm=1.437624454498291, loss=2.042966604232788
I0129 08:53:19.043668 139655626356480 logging_writer.py:48] [76300] global_step=76300, grad_norm=1.3534036874771118, loss=2.021221160888672
I0129 08:53:52.970016 139655617963776 logging_writer.py:48] [76400] global_step=76400, grad_norm=1.503296136856079, loss=2.158489465713501
I0129 08:54:26.899873 139655626356480 logging_writer.py:48] [76500] global_step=76500, grad_norm=1.3996237516403198, loss=2.031019926071167
I0129 08:54:58.768608 139822745589568 spec.py:321] Evaluating on the training split.
I0129 08:55:05.100058 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 08:55:13.738389 139822745589568 spec.py:349] Evaluating on the test split.
I0129 08:55:16.311555 139822745589568 submission_runner.py:408] Time since start: 26959.67s, 	Step: 76595, 	{'train/accuracy': 0.36898118257522583, 'train/loss': 3.100395917892456, 'validation/accuracy': 0.3448599874973297, 'validation/loss': 3.2606098651885986, 'validation/num_examples': 50000, 'test/accuracy': 0.2645000219345093, 'test/loss': 4.023133277893066, 'test/num_examples': 10000, 'score': 26046.467567443848, 'total_duration': 26959.668189525604, 'accumulated_submission_time': 26046.467567443848, 'accumulated_eval_time': 908.7737927436829, 'accumulated_logging_time': 1.7912750244140625}
I0129 08:55:16.341911 139655617963776 logging_writer.py:48] [76595] accumulated_eval_time=908.773793, accumulated_logging_time=1.791275, accumulated_submission_time=26046.467567, global_step=76595, preemption_count=0, score=26046.467567, test/accuracy=0.264500, test/loss=4.023133, test/num_examples=10000, total_duration=26959.668190, train/accuracy=0.368981, train/loss=3.100396, validation/accuracy=0.344860, validation/loss=3.260610, validation/num_examples=50000
I0129 08:55:18.379609 139656658151168 logging_writer.py:48] [76600] global_step=76600, grad_norm=1.6763159036636353, loss=2.1839704513549805
I0129 08:55:52.222240 139655617963776 logging_writer.py:48] [76700] global_step=76700, grad_norm=1.4548671245574951, loss=2.162116765975952
I0129 08:56:26.104414 139656658151168 logging_writer.py:48] [76800] global_step=76800, grad_norm=1.5079644918441772, loss=2.068864107131958
I0129 08:57:00.030427 139655617963776 logging_writer.py:48] [76900] global_step=76900, grad_norm=1.44656240940094, loss=2.1255712509155273
I0129 08:57:33.930832 139656658151168 logging_writer.py:48] [77000] global_step=77000, grad_norm=1.2958835363388062, loss=2.078472137451172
I0129 08:58:07.839968 139655617963776 logging_writer.py:48] [77100] global_step=77100, grad_norm=1.3945269584655762, loss=2.1170427799224854
I0129 08:58:41.716096 139656658151168 logging_writer.py:48] [77200] global_step=77200, grad_norm=1.5207895040512085, loss=2.24198842048645
I0129 08:59:15.605298 139655617963776 logging_writer.py:48] [77300] global_step=77300, grad_norm=1.5963155031204224, loss=2.1276938915252686
I0129 08:59:49.479198 139656658151168 logging_writer.py:48] [77400] global_step=77400, grad_norm=1.4408495426177979, loss=2.0737414360046387
I0129 09:00:23.376165 139655617963776 logging_writer.py:48] [77500] global_step=77500, grad_norm=1.367945671081543, loss=2.0559592247009277
I0129 09:00:57.281899 139656658151168 logging_writer.py:48] [77600] global_step=77600, grad_norm=1.3349735736846924, loss=2.0683040618896484
I0129 09:01:31.329768 139655617963776 logging_writer.py:48] [77700] global_step=77700, grad_norm=1.3683801889419556, loss=2.196946859359741
I0129 09:02:05.256192 139656658151168 logging_writer.py:48] [77800] global_step=77800, grad_norm=1.4418467283248901, loss=2.167489767074585
I0129 09:02:39.181711 139655617963776 logging_writer.py:48] [77900] global_step=77900, grad_norm=1.5934911966323853, loss=2.1682686805725098
I0129 09:03:13.094547 139656658151168 logging_writer.py:48] [78000] global_step=78000, grad_norm=1.4093257188796997, loss=2.1243863105773926
I0129 09:03:46.477374 139822745589568 spec.py:321] Evaluating on the training split.
I0129 09:03:52.616099 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 09:04:01.121840 139822745589568 spec.py:349] Evaluating on the test split.
I0129 09:04:03.830997 139822745589568 submission_runner.py:408] Time since start: 27487.19s, 	Step: 78100, 	{'train/accuracy': 0.2097417116165161, 'train/loss': 4.448638916015625, 'validation/accuracy': 0.19473999738693237, 'validation/loss': 4.624502658843994, 'validation/num_examples': 50000, 'test/accuracy': 0.1477000117301941, 'test/loss': 5.23762321472168, 'test/num_examples': 10000, 'score': 26556.542511701584, 'total_duration': 27487.187576293945, 'accumulated_submission_time': 26556.542511701584, 'accumulated_eval_time': 926.1273169517517, 'accumulated_logging_time': 1.8315112590789795}
I0129 09:04:03.860710 139655626356480 logging_writer.py:48] [78100] accumulated_eval_time=926.127317, accumulated_logging_time=1.831511, accumulated_submission_time=26556.542512, global_step=78100, preemption_count=0, score=26556.542512, test/accuracy=0.147700, test/loss=5.237623, test/num_examples=10000, total_duration=27487.187576, train/accuracy=0.209742, train/loss=4.448639, validation/accuracy=0.194740, validation/loss=4.624503, validation/num_examples=50000
I0129 09:04:04.214972 139656297445120 logging_writer.py:48] [78100] global_step=78100, grad_norm=1.6219040155410767, loss=2.121530771255493
I0129 09:04:38.110006 139655626356480 logging_writer.py:48] [78200] global_step=78200, grad_norm=1.3656400442123413, loss=2.0010592937469482
I0129 09:05:11.966719 139656297445120 logging_writer.py:48] [78300] global_step=78300, grad_norm=1.3702315092086792, loss=2.065523624420166
I0129 09:05:45.851886 139655626356480 logging_writer.py:48] [78400] global_step=78400, grad_norm=1.4534735679626465, loss=2.1408309936523438
I0129 09:06:19.762282 139656297445120 logging_writer.py:48] [78500] global_step=78500, grad_norm=1.4367705583572388, loss=2.110828161239624
I0129 09:06:53.678978 139655626356480 logging_writer.py:48] [78600] global_step=78600, grad_norm=1.5252066850662231, loss=2.064927101135254
I0129 09:07:27.609370 139656297445120 logging_writer.py:48] [78700] global_step=78700, grad_norm=1.2685620784759521, loss=1.8647319078445435
I0129 09:08:01.617213 139655626356480 logging_writer.py:48] [78800] global_step=78800, grad_norm=1.5381487607955933, loss=2.107079267501831
I0129 09:08:35.565725 139656297445120 logging_writer.py:48] [78900] global_step=78900, grad_norm=1.4452486038208008, loss=2.108645439147949
I0129 09:09:09.494091 139655626356480 logging_writer.py:48] [79000] global_step=79000, grad_norm=1.49844491481781, loss=2.1614434719085693
I0129 09:09:43.411674 139656297445120 logging_writer.py:48] [79100] global_step=79100, grad_norm=1.4708333015441895, loss=2.1091322898864746
I0129 09:10:17.352113 139655626356480 logging_writer.py:48] [79200] global_step=79200, grad_norm=1.5013089179992676, loss=2.255406379699707
I0129 09:10:51.246617 139656297445120 logging_writer.py:48] [79300] global_step=79300, grad_norm=1.546454906463623, loss=2.150622844696045
I0129 09:11:25.181800 139655626356480 logging_writer.py:48] [79400] global_step=79400, grad_norm=1.3699182271957397, loss=1.9834108352661133
I0129 09:11:59.103220 139656297445120 logging_writer.py:48] [79500] global_step=79500, grad_norm=1.331269383430481, loss=2.0896074771881104
I0129 09:12:33.037747 139655626356480 logging_writer.py:48] [79600] global_step=79600, grad_norm=1.3194990158081055, loss=2.000615358352661
I0129 09:12:33.859710 139822745589568 spec.py:321] Evaluating on the training split.
I0129 09:12:40.021843 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 09:12:48.629830 139822745589568 spec.py:349] Evaluating on the test split.
I0129 09:12:51.237876 139822745589568 submission_runner.py:408] Time since start: 28014.59s, 	Step: 79604, 	{'train/accuracy': 0.22132094204425812, 'train/loss': 4.309572219848633, 'validation/accuracy': 0.20763999223709106, 'validation/loss': 4.423527240753174, 'validation/num_examples': 50000, 'test/accuracy': 0.14880000054836273, 'test/loss': 5.156861782073975, 'test/num_examples': 10000, 'score': 27066.480364322662, 'total_duration': 28014.59451031685, 'accumulated_submission_time': 27066.480364322662, 'accumulated_eval_time': 943.505437374115, 'accumulated_logging_time': 1.8714027404785156}
I0129 09:12:51.266631 139655617963776 logging_writer.py:48] [79604] accumulated_eval_time=943.505437, accumulated_logging_time=1.871403, accumulated_submission_time=27066.480364, global_step=79604, preemption_count=0, score=27066.480364, test/accuracy=0.148800, test/loss=5.156862, test/num_examples=10000, total_duration=28014.594510, train/accuracy=0.221321, train/loss=4.309572, validation/accuracy=0.207640, validation/loss=4.423527, validation/num_examples=50000
I0129 09:13:24.100400 139655626356480 logging_writer.py:48] [79700] global_step=79700, grad_norm=1.4353718757629395, loss=1.9544587135314941
I0129 09:13:57.970462 139655617963776 logging_writer.py:48] [79800] global_step=79800, grad_norm=1.665117859840393, loss=2.0572612285614014
I0129 09:14:31.923453 139655626356480 logging_writer.py:48] [79900] global_step=79900, grad_norm=1.4817981719970703, loss=2.067633628845215
I0129 09:15:05.845569 139655617963776 logging_writer.py:48] [80000] global_step=80000, grad_norm=1.4237136840820312, loss=2.0535781383514404
I0129 09:15:39.751871 139655626356480 logging_writer.py:48] [80100] global_step=80100, grad_norm=1.3460650444030762, loss=2.2093727588653564
I0129 09:16:13.668599 139655617963776 logging_writer.py:48] [80200] global_step=80200, grad_norm=1.5485105514526367, loss=2.06034517288208
I0129 09:16:47.587914 139655626356480 logging_writer.py:48] [80300] global_step=80300, grad_norm=1.3732876777648926, loss=2.112663745880127
I0129 09:17:21.478340 139655617963776 logging_writer.py:48] [80400] global_step=80400, grad_norm=1.4750306606292725, loss=2.0242342948913574
I0129 09:17:55.396699 139655626356480 logging_writer.py:48] [80500] global_step=80500, grad_norm=1.402125597000122, loss=2.066216468811035
I0129 09:18:29.307091 139655617963776 logging_writer.py:48] [80600] global_step=80600, grad_norm=1.523923397064209, loss=2.103736400604248
I0129 09:19:03.217614 139655626356480 logging_writer.py:48] [80700] global_step=80700, grad_norm=1.4321666955947876, loss=1.9919629096984863
I0129 09:19:37.143794 139655617963776 logging_writer.py:48] [80800] global_step=80800, grad_norm=1.4746174812316895, loss=1.9927148818969727
I0129 09:20:11.071985 139655626356480 logging_writer.py:48] [80900] global_step=80900, grad_norm=1.5199490785598755, loss=2.1099798679351807
I0129 09:20:44.968745 139655617963776 logging_writer.py:48] [81000] global_step=81000, grad_norm=1.4631880521774292, loss=2.053903102874756
I0129 09:21:18.966415 139655626356480 logging_writer.py:48] [81100] global_step=81100, grad_norm=1.500059962272644, loss=1.990429401397705
I0129 09:21:21.482434 139822745589568 spec.py:321] Evaluating on the training split.
I0129 09:21:27.672801 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 09:21:36.566538 139822745589568 spec.py:349] Evaluating on the test split.
I0129 09:21:39.058071 139822745589568 submission_runner.py:408] Time since start: 28542.41s, 	Step: 81109, 	{'train/accuracy': 0.33380499482154846, 'train/loss': 3.2794435024261475, 'validation/accuracy': 0.3070800006389618, 'validation/loss': 3.5193474292755127, 'validation/num_examples': 50000, 'test/accuracy': 0.23000000417232513, 'test/loss': 4.20773458480835, 'test/num_examples': 10000, 'score': 27576.634883880615, 'total_duration': 28542.41470336914, 'accumulated_submission_time': 27576.634883880615, 'accumulated_eval_time': 961.0810222625732, 'accumulated_logging_time': 1.9099252223968506}
I0129 09:21:39.092648 139656649758464 logging_writer.py:48] [81109] accumulated_eval_time=961.081022, accumulated_logging_time=1.909925, accumulated_submission_time=27576.634884, global_step=81109, preemption_count=0, score=27576.634884, test/accuracy=0.230000, test/loss=4.207735, test/num_examples=10000, total_duration=28542.414703, train/accuracy=0.333805, train/loss=3.279444, validation/accuracy=0.307080, validation/loss=3.519347, validation/num_examples=50000
I0129 09:22:10.240525 139656658151168 logging_writer.py:48] [81200] global_step=81200, grad_norm=1.5306254625320435, loss=1.9166909456253052
I0129 09:22:44.115493 139656649758464 logging_writer.py:48] [81300] global_step=81300, grad_norm=1.4855036735534668, loss=2.143062114715576
I0129 09:23:18.002290 139656658151168 logging_writer.py:48] [81400] global_step=81400, grad_norm=1.449298620223999, loss=2.2193822860717773
I0129 09:23:51.925960 139656649758464 logging_writer.py:48] [81500] global_step=81500, grad_norm=1.4508206844329834, loss=2.1839094161987305
I0129 09:24:25.848546 139656658151168 logging_writer.py:48] [81600] global_step=81600, grad_norm=1.7964987754821777, loss=2.0934643745422363
I0129 09:24:59.784051 139656649758464 logging_writer.py:48] [81700] global_step=81700, grad_norm=1.5985277891159058, loss=2.0186424255371094
I0129 09:25:33.699073 139656658151168 logging_writer.py:48] [81800] global_step=81800, grad_norm=1.5510163307189941, loss=2.1536788940429688
I0129 09:26:07.627622 139656649758464 logging_writer.py:48] [81900] global_step=81900, grad_norm=1.6340985298156738, loss=2.151951789855957
I0129 09:26:41.530261 139656658151168 logging_writer.py:48] [82000] global_step=82000, grad_norm=1.4252454042434692, loss=2.033491849899292
I0129 09:27:15.461232 139656649758464 logging_writer.py:48] [82100] global_step=82100, grad_norm=1.4398574829101562, loss=2.0120530128479004
I0129 09:27:49.429037 139656658151168 logging_writer.py:48] [82200] global_step=82200, grad_norm=1.4829319715499878, loss=2.0690343379974365
I0129 09:28:23.334350 139656649758464 logging_writer.py:48] [82300] global_step=82300, grad_norm=1.717939853668213, loss=2.048403739929199
I0129 09:28:57.217905 139656658151168 logging_writer.py:48] [82400] global_step=82400, grad_norm=1.4178968667984009, loss=2.0549893379211426
I0129 09:29:31.121401 139656649758464 logging_writer.py:48] [82500] global_step=82500, grad_norm=1.5619064569473267, loss=2.1570885181427
I0129 09:30:04.996896 139656658151168 logging_writer.py:48] [82600] global_step=82600, grad_norm=1.6650599241256714, loss=2.0305609703063965
I0129 09:30:09.219947 139822745589568 spec.py:321] Evaluating on the training split.
I0129 09:30:15.355868 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 09:30:24.202471 139822745589568 spec.py:349] Evaluating on the test split.
I0129 09:30:26.794573 139822745589568 submission_runner.py:408] Time since start: 29070.15s, 	Step: 82614, 	{'train/accuracy': 0.395228773355484, 'train/loss': 2.817749261856079, 'validation/accuracy': 0.367499977350235, 'validation/loss': 3.013368606567383, 'validation/num_examples': 50000, 'test/accuracy': 0.28200000524520874, 'test/loss': 3.757661819458008, 'test/num_examples': 10000, 'score': 28086.697027683258, 'total_duration': 29070.151161670685, 'accumulated_submission_time': 28086.697027683258, 'accumulated_eval_time': 978.6555554866791, 'accumulated_logging_time': 1.955564022064209}
I0129 09:30:26.828576 139655617963776 logging_writer.py:48] [82614] accumulated_eval_time=978.655555, accumulated_logging_time=1.955564, accumulated_submission_time=28086.697028, global_step=82614, preemption_count=0, score=28086.697028, test/accuracy=0.282000, test/loss=3.757662, test/num_examples=10000, total_duration=29070.151162, train/accuracy=0.395229, train/loss=2.817749, validation/accuracy=0.367500, validation/loss=3.013369, validation/num_examples=50000
I0129 09:30:56.284096 139655626356480 logging_writer.py:48] [82700] global_step=82700, grad_norm=1.4813313484191895, loss=2.09122896194458
I0129 09:31:30.131300 139655617963776 logging_writer.py:48] [82800] global_step=82800, grad_norm=1.6483895778656006, loss=1.943071722984314
I0129 09:32:04.062934 139655626356480 logging_writer.py:48] [82900] global_step=82900, grad_norm=1.7076654434204102, loss=2.17025089263916
I0129 09:32:37.971542 139655617963776 logging_writer.py:48] [83000] global_step=83000, grad_norm=1.5391120910644531, loss=2.050579071044922
I0129 09:33:11.896273 139655626356480 logging_writer.py:48] [83100] global_step=83100, grad_norm=1.4491350650787354, loss=2.0902652740478516
I0129 09:33:45.816635 139655617963776 logging_writer.py:48] [83200] global_step=83200, grad_norm=1.3750200271606445, loss=1.9608863592147827
I0129 09:34:19.822305 139655626356480 logging_writer.py:48] [83300] global_step=83300, grad_norm=1.6758825778961182, loss=2.154374361038208
I0129 09:34:53.759238 139655617963776 logging_writer.py:48] [83400] global_step=83400, grad_norm=1.5505919456481934, loss=2.0445520877838135
I0129 09:35:27.679777 139655626356480 logging_writer.py:48] [83500] global_step=83500, grad_norm=1.4771867990493774, loss=2.1082355976104736
I0129 09:36:01.591084 139655617963776 logging_writer.py:48] [83600] global_step=83600, grad_norm=1.558138132095337, loss=2.098069906234741
I0129 09:36:35.513546 139655626356480 logging_writer.py:48] [83700] global_step=83700, grad_norm=1.6552073955535889, loss=2.2272744178771973
I0129 09:37:09.411757 139655617963776 logging_writer.py:48] [83800] global_step=83800, grad_norm=1.396250605583191, loss=2.143385410308838
I0129 09:37:43.330735 139655626356480 logging_writer.py:48] [83900] global_step=83900, grad_norm=1.3036649227142334, loss=2.097508668899536
I0129 09:38:17.243689 139655617963776 logging_writer.py:48] [84000] global_step=84000, grad_norm=1.4511996507644653, loss=2.1468963623046875
I0129 09:38:51.165473 139655626356480 logging_writer.py:48] [84100] global_step=84100, grad_norm=1.436592698097229, loss=2.141887664794922
I0129 09:38:57.064284 139822745589568 spec.py:321] Evaluating on the training split.
I0129 09:39:03.368386 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 09:39:12.026953 139822745589568 spec.py:349] Evaluating on the test split.
I0129 09:39:14.493863 139822745589568 submission_runner.py:408] Time since start: 29597.85s, 	Step: 84119, 	{'train/accuracy': 0.3988759517669678, 'train/loss': 2.8061952590942383, 'validation/accuracy': 0.3729199767112732, 'validation/loss': 3.0000438690185547, 'validation/num_examples': 50000, 'test/accuracy': 0.2729000151157379, 'test/loss': 3.788983106613159, 'test/num_examples': 10000, 'score': 28596.871851682663, 'total_duration': 29597.85049009323, 'accumulated_submission_time': 28596.871851682663, 'accumulated_eval_time': 996.0850801467896, 'accumulated_logging_time': 1.9987788200378418}
I0129 09:39:14.528589 139656666543872 logging_writer.py:48] [84119] accumulated_eval_time=996.085080, accumulated_logging_time=1.998779, accumulated_submission_time=28596.871852, global_step=84119, preemption_count=0, score=28596.871852, test/accuracy=0.272900, test/loss=3.788983, test/num_examples=10000, total_duration=29597.850490, train/accuracy=0.398876, train/loss=2.806195, validation/accuracy=0.372920, validation/loss=3.000044, validation/num_examples=50000
I0129 09:39:42.279551 139656834316032 logging_writer.py:48] [84200] global_step=84200, grad_norm=1.3853144645690918, loss=2.1189236640930176
I0129 09:40:16.139778 139656666543872 logging_writer.py:48] [84300] global_step=84300, grad_norm=1.5414323806762695, loss=2.0665159225463867
I0129 09:40:50.079664 139656834316032 logging_writer.py:48] [84400] global_step=84400, grad_norm=1.610349416732788, loss=2.1708245277404785
I0129 09:41:24.007679 139656666543872 logging_writer.py:48] [84500] global_step=84500, grad_norm=1.5443780422210693, loss=2.055171012878418
I0129 09:41:57.910688 139656834316032 logging_writer.py:48] [84600] global_step=84600, grad_norm=1.4722871780395508, loss=2.080778121948242
I0129 09:42:31.831696 139656666543872 logging_writer.py:48] [84700] global_step=84700, grad_norm=1.6243243217468262, loss=2.1461708545684814
I0129 09:43:05.752534 139656834316032 logging_writer.py:48] [84800] global_step=84800, grad_norm=1.5960067510604858, loss=2.029848337173462
I0129 09:43:39.653743 139656666543872 logging_writer.py:48] [84900] global_step=84900, grad_norm=1.60860013961792, loss=2.089606285095215
I0129 09:44:13.551459 139656834316032 logging_writer.py:48] [85000] global_step=85000, grad_norm=1.5844639539718628, loss=2.0818960666656494
I0129 09:44:47.453386 139656666543872 logging_writer.py:48] [85100] global_step=85100, grad_norm=1.608329176902771, loss=2.1409075260162354
I0129 09:45:21.392876 139656834316032 logging_writer.py:48] [85200] global_step=85200, grad_norm=1.5162057876586914, loss=2.065892219543457
I0129 09:45:55.318297 139656666543872 logging_writer.py:48] [85300] global_step=85300, grad_norm=1.4806803464889526, loss=2.0632200241088867
I0129 09:46:29.206897 139656834316032 logging_writer.py:48] [85400] global_step=85400, grad_norm=1.6175742149353027, loss=1.9566245079040527
I0129 09:47:03.211390 139656666543872 logging_writer.py:48] [85500] global_step=85500, grad_norm=1.489885687828064, loss=1.9504642486572266
I0129 09:47:37.131612 139656834316032 logging_writer.py:48] [85600] global_step=85600, grad_norm=1.4613069295883179, loss=1.9402135610580444
I0129 09:47:44.742979 139822745589568 spec.py:321] Evaluating on the training split.
I0129 09:47:50.998879 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 09:47:59.599677 139822745589568 spec.py:349] Evaluating on the test split.
I0129 09:48:02.208203 139822745589568 submission_runner.py:408] Time since start: 30125.56s, 	Step: 85624, 	{'train/accuracy': 0.49443957209587097, 'train/loss': 2.2480671405792236, 'validation/accuracy': 0.4607999920845032, 'validation/loss': 2.4581475257873535, 'validation/num_examples': 50000, 'test/accuracy': 0.35190001130104065, 'test/loss': 3.282273292541504, 'test/num_examples': 10000, 'score': 29107.02249646187, 'total_duration': 30125.56484889984, 'accumulated_submission_time': 29107.02249646187, 'accumulated_eval_time': 1013.5502715110779, 'accumulated_logging_time': 2.0445313453674316}
I0129 09:48:02.242959 139656649758464 logging_writer.py:48] [85624] accumulated_eval_time=1013.550272, accumulated_logging_time=2.044531, accumulated_submission_time=29107.022496, global_step=85624, preemption_count=0, score=29107.022496, test/accuracy=0.351900, test/loss=3.282273, test/num_examples=10000, total_duration=30125.564849, train/accuracy=0.494440, train/loss=2.248067, validation/accuracy=0.460800, validation/loss=2.458148, validation/num_examples=50000
I0129 09:48:28.277415 139656658151168 logging_writer.py:48] [85700] global_step=85700, grad_norm=1.490150809288025, loss=2.0956907272338867
I0129 09:49:02.108814 139656649758464 logging_writer.py:48] [85800] global_step=85800, grad_norm=1.5698658227920532, loss=2.008075714111328
I0129 09:49:35.984309 139656658151168 logging_writer.py:48] [85900] global_step=85900, grad_norm=1.5589343309402466, loss=2.0507278442382812
I0129 09:50:09.870950 139656649758464 logging_writer.py:48] [86000] global_step=86000, grad_norm=1.5859298706054688, loss=2.038278579711914
I0129 09:50:43.787881 139656658151168 logging_writer.py:48] [86100] global_step=86100, grad_norm=1.5391799211502075, loss=2.1409945487976074
I0129 09:51:17.687102 139656649758464 logging_writer.py:48] [86200] global_step=86200, grad_norm=1.493475079536438, loss=2.103450059890747
I0129 09:51:51.614575 139656658151168 logging_writer.py:48] [86300] global_step=86300, grad_norm=1.5547044277191162, loss=1.9382243156433105
I0129 09:52:25.491554 139656649758464 logging_writer.py:48] [86400] global_step=86400, grad_norm=1.4348832368850708, loss=2.0590858459472656
I0129 09:52:59.422564 139656658151168 logging_writer.py:48] [86500] global_step=86500, grad_norm=1.5762873888015747, loss=2.03999662399292
I0129 09:53:33.522541 139656649758464 logging_writer.py:48] [86600] global_step=86600, grad_norm=1.6960117816925049, loss=2.0632598400115967
I0129 09:54:07.451946 139656658151168 logging_writer.py:48] [86700] global_step=86700, grad_norm=1.4272748231887817, loss=2.121976137161255
I0129 09:54:41.333121 139656649758464 logging_writer.py:48] [86800] global_step=86800, grad_norm=1.4698383808135986, loss=2.040804147720337
I0129 09:55:15.229954 139656658151168 logging_writer.py:48] [86900] global_step=86900, grad_norm=1.5349010229110718, loss=2.091459274291992
I0129 09:55:49.105370 139656649758464 logging_writer.py:48] [87000] global_step=87000, grad_norm=1.3817001581192017, loss=2.0248279571533203
I0129 09:56:22.992402 139656658151168 logging_writer.py:48] [87100] global_step=87100, grad_norm=1.6540056467056274, loss=1.9840874671936035
I0129 09:56:32.298405 139822745589568 spec.py:321] Evaluating on the training split.
I0129 09:56:38.490118 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 09:56:47.190638 139822745589568 spec.py:349] Evaluating on the test split.
I0129 09:56:49.814860 139822745589568 submission_runner.py:408] Time since start: 30653.17s, 	Step: 87129, 	{'train/accuracy': 0.1268136203289032, 'train/loss': 5.291484355926514, 'validation/accuracy': 0.12189999967813492, 'validation/loss': 5.354971885681152, 'validation/num_examples': 50000, 'test/accuracy': 0.08920000493526459, 'test/loss': 5.882399559020996, 'test/num_examples': 10000, 'score': 29617.017424106598, 'total_duration': 30653.171503305435, 'accumulated_submission_time': 29617.017424106598, 'accumulated_eval_time': 1031.0666897296906, 'accumulated_logging_time': 2.0885136127471924}
I0129 09:56:49.847550 139655617963776 logging_writer.py:48] [87129] accumulated_eval_time=1031.066690, accumulated_logging_time=2.088514, accumulated_submission_time=29617.017424, global_step=87129, preemption_count=0, score=29617.017424, test/accuracy=0.089200, test/loss=5.882400, test/num_examples=10000, total_duration=30653.171503, train/accuracy=0.126814, train/loss=5.291484, validation/accuracy=0.121900, validation/loss=5.354972, validation/num_examples=50000
I0129 09:57:14.235866 139656297445120 logging_writer.py:48] [87200] global_step=87200, grad_norm=1.453844666481018, loss=2.031766176223755
I0129 09:57:48.156921 139655617963776 logging_writer.py:48] [87300] global_step=87300, grad_norm=1.5108150243759155, loss=2.029675006866455
I0129 09:58:22.043774 139656297445120 logging_writer.py:48] [87400] global_step=87400, grad_norm=1.4461297988891602, loss=2.106322765350342
I0129 09:58:55.985308 139655617963776 logging_writer.py:48] [87500] global_step=87500, grad_norm=1.4714932441711426, loss=2.01568865776062
I0129 09:59:29.920893 139656297445120 logging_writer.py:48] [87600] global_step=87600, grad_norm=1.4392577409744263, loss=2.036503791809082
I0129 10:00:03.826657 139655617963776 logging_writer.py:48] [87700] global_step=87700, grad_norm=1.5951130390167236, loss=2.1352720260620117
I0129 10:00:37.884879 139656297445120 logging_writer.py:48] [87800] global_step=87800, grad_norm=1.5597691535949707, loss=1.9729653596878052
I0129 10:01:11.796584 139655617963776 logging_writer.py:48] [87900] global_step=87900, grad_norm=1.5148977041244507, loss=1.9545716047286987
I0129 10:01:45.727068 139656297445120 logging_writer.py:48] [88000] global_step=88000, grad_norm=1.5534234046936035, loss=1.9607908725738525
I0129 10:02:19.633064 139655617963776 logging_writer.py:48] [88100] global_step=88100, grad_norm=1.625159740447998, loss=1.9633065462112427
I0129 10:02:53.574703 139656297445120 logging_writer.py:48] [88200] global_step=88200, grad_norm=1.6142442226409912, loss=2.1016459465026855
I0129 10:03:27.481825 139655617963776 logging_writer.py:48] [88300] global_step=88300, grad_norm=1.9335180521011353, loss=2.0445685386657715
I0129 10:04:01.404486 139656297445120 logging_writer.py:48] [88400] global_step=88400, grad_norm=1.5522841215133667, loss=2.064657211303711
I0129 10:04:35.324182 139655617963776 logging_writer.py:48] [88500] global_step=88500, grad_norm=1.6547507047653198, loss=2.147449016571045
I0129 10:05:09.246913 139656297445120 logging_writer.py:48] [88600] global_step=88600, grad_norm=1.5456231832504272, loss=2.028069496154785
I0129 10:05:19.879222 139822745589568 spec.py:321] Evaluating on the training split.
I0129 10:05:26.135250 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 10:05:34.603854 139822745589568 spec.py:349] Evaluating on the test split.
I0129 10:05:37.190667 139822745589568 submission_runner.py:408] Time since start: 31180.55s, 	Step: 88633, 	{'train/accuracy': 0.4404296875, 'train/loss': 2.522449254989624, 'validation/accuracy': 0.41787999868392944, 'validation/loss': 2.665285348892212, 'validation/num_examples': 50000, 'test/accuracy': 0.31200000643730164, 'test/loss': 3.441679000854492, 'test/num_examples': 10000, 'score': 30126.987594604492, 'total_duration': 31180.547289133072, 'accumulated_submission_time': 30126.987594604492, 'accumulated_eval_time': 1048.3781082630157, 'accumulated_logging_time': 2.131108283996582}
I0129 10:05:37.234919 139656649758464 logging_writer.py:48] [88633] accumulated_eval_time=1048.378108, accumulated_logging_time=2.131108, accumulated_submission_time=30126.987595, global_step=88633, preemption_count=0, score=30126.987595, test/accuracy=0.312000, test/loss=3.441679, test/num_examples=10000, total_duration=31180.547289, train/accuracy=0.440430, train/loss=2.522449, validation/accuracy=0.417880, validation/loss=2.665285, validation/num_examples=50000
I0129 10:06:00.260701 139656658151168 logging_writer.py:48] [88700] global_step=88700, grad_norm=1.5337797403335571, loss=2.056042194366455
I0129 10:06:34.202836 139656649758464 logging_writer.py:48] [88800] global_step=88800, grad_norm=1.4746527671813965, loss=1.9044225215911865
I0129 10:07:08.118097 139656658151168 logging_writer.py:48] [88900] global_step=88900, grad_norm=1.4835954904556274, loss=2.0326712131500244
I0129 10:07:41.962950 139656649758464 logging_writer.py:48] [89000] global_step=89000, grad_norm=1.539035677909851, loss=1.9970041513442993
I0129 10:08:15.865879 139656658151168 logging_writer.py:48] [89100] global_step=89100, grad_norm=1.490700364112854, loss=1.911769151687622
I0129 10:08:49.774053 139656649758464 logging_writer.py:48] [89200] global_step=89200, grad_norm=1.5836213827133179, loss=2.0092906951904297
I0129 10:09:23.632844 139656658151168 logging_writer.py:48] [89300] global_step=89300, grad_norm=1.5992002487182617, loss=1.9834245443344116
I0129 10:09:57.511986 139656649758464 logging_writer.py:48] [89400] global_step=89400, grad_norm=1.5981074571609497, loss=1.9298735857009888
I0129 10:10:31.433374 139656658151168 logging_writer.py:48] [89500] global_step=89500, grad_norm=1.6301461458206177, loss=1.9393844604492188
I0129 10:11:05.355405 139656649758464 logging_writer.py:48] [89600] global_step=89600, grad_norm=1.6241470575332642, loss=2.0657341480255127
I0129 10:11:39.285004 139656658151168 logging_writer.py:48] [89700] global_step=89700, grad_norm=1.5630772113800049, loss=1.9050488471984863
I0129 10:12:13.168570 139656649758464 logging_writer.py:48] [89800] global_step=89800, grad_norm=1.5134443044662476, loss=1.9964654445648193
I0129 10:12:47.072448 139656658151168 logging_writer.py:48] [89900] global_step=89900, grad_norm=1.618712067604065, loss=2.177436113357544
I0129 10:13:21.045827 139656649758464 logging_writer.py:48] [90000] global_step=90000, grad_norm=1.5303401947021484, loss=1.945694088935852
I0129 10:13:54.967661 139656658151168 logging_writer.py:48] [90100] global_step=90100, grad_norm=1.6730024814605713, loss=2.1340255737304688
I0129 10:14:07.314386 139822745589568 spec.py:321] Evaluating on the training split.
I0129 10:14:13.497307 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 10:14:22.149816 139822745589568 spec.py:349] Evaluating on the test split.
I0129 10:14:24.764275 139822745589568 submission_runner.py:408] Time since start: 31708.12s, 	Step: 90138, 	{'train/accuracy': 0.5174585580825806, 'train/loss': 2.07250714302063, 'validation/accuracy': 0.47110000252723694, 'validation/loss': 2.372112512588501, 'validation/num_examples': 50000, 'test/accuracy': 0.3660000264644623, 'test/loss': 3.1758997440338135, 'test/num_examples': 10000, 'score': 30637.007561683655, 'total_duration': 31708.120919704437, 'accumulated_submission_time': 30637.007561683655, 'accumulated_eval_time': 1065.8279626369476, 'accumulated_logging_time': 2.1842033863067627}
I0129 10:14:24.799529 139655617963776 logging_writer.py:48] [90138] accumulated_eval_time=1065.827963, accumulated_logging_time=2.184203, accumulated_submission_time=30637.007562, global_step=90138, preemption_count=0, score=30637.007562, test/accuracy=0.366000, test/loss=3.175900, test/num_examples=10000, total_duration=31708.120920, train/accuracy=0.517459, train/loss=2.072507, validation/accuracy=0.471100, validation/loss=2.372113, validation/num_examples=50000
I0129 10:14:46.132182 139656297445120 logging_writer.py:48] [90200] global_step=90200, grad_norm=1.665069818496704, loss=2.038639545440674
I0129 10:15:20.015969 139655617963776 logging_writer.py:48] [90300] global_step=90300, grad_norm=1.6370865106582642, loss=2.0943899154663086
I0129 10:15:53.897388 139656297445120 logging_writer.py:48] [90400] global_step=90400, grad_norm=1.8765336275100708, loss=1.9810351133346558
I0129 10:16:27.777358 139655617963776 logging_writer.py:48] [90500] global_step=90500, grad_norm=1.520859956741333, loss=1.986786127090454
I0129 10:17:01.718122 139656297445120 logging_writer.py:48] [90600] global_step=90600, grad_norm=1.6252416372299194, loss=1.9979418516159058
I0129 10:17:35.633454 139655617963776 logging_writer.py:48] [90700] global_step=90700, grad_norm=1.5441956520080566, loss=2.1271579265594482
I0129 10:18:09.539167 139656297445120 logging_writer.py:48] [90800] global_step=90800, grad_norm=1.6005533933639526, loss=2.0545718669891357
I0129 10:18:43.462805 139655617963776 logging_writer.py:48] [90900] global_step=90900, grad_norm=1.585676908493042, loss=1.9435770511627197
I0129 10:19:17.386632 139656297445120 logging_writer.py:48] [91000] global_step=91000, grad_norm=1.672624945640564, loss=2.1591265201568604
I0129 10:19:51.359777 139655617963776 logging_writer.py:48] [91100] global_step=91100, grad_norm=1.5059635639190674, loss=1.9935039281845093
I0129 10:20:25.267985 139656297445120 logging_writer.py:48] [91200] global_step=91200, grad_norm=1.582735538482666, loss=2.0034172534942627
I0129 10:20:59.154960 139655617963776 logging_writer.py:48] [91300] global_step=91300, grad_norm=1.5708379745483398, loss=1.94723641872406
I0129 10:21:33.090149 139656297445120 logging_writer.py:48] [91400] global_step=91400, grad_norm=1.5375139713287354, loss=1.960620641708374
I0129 10:22:06.990208 139655617963776 logging_writer.py:48] [91500] global_step=91500, grad_norm=1.4235780239105225, loss=1.8779962062835693
I0129 10:22:40.919229 139656297445120 logging_writer.py:48] [91600] global_step=91600, grad_norm=1.7334136962890625, loss=2.040472984313965
I0129 10:22:54.967323 139822745589568 spec.py:321] Evaluating on the training split.
I0129 10:23:01.956812 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 10:23:10.511598 139822745589568 spec.py:349] Evaluating on the test split.
I0129 10:23:13.111578 139822745589568 submission_runner.py:408] Time since start: 32236.47s, 	Step: 91643, 	{'train/accuracy': 0.4838368892669678, 'train/loss': 2.2562386989593506, 'validation/accuracy': 0.44661998748779297, 'validation/loss': 2.4910802841186523, 'validation/num_examples': 50000, 'test/accuracy': 0.3335000276565552, 'test/loss': 3.2831525802612305, 'test/num_examples': 10000, 'score': 31147.1129257679, 'total_duration': 32236.46764421463, 'accumulated_submission_time': 31147.1129257679, 'accumulated_eval_time': 1083.9716200828552, 'accumulated_logging_time': 2.2286715507507324}
I0129 10:23:13.145349 139655617963776 logging_writer.py:48] [91643] accumulated_eval_time=1083.971620, accumulated_logging_time=2.228672, accumulated_submission_time=31147.112926, global_step=91643, preemption_count=0, score=31147.112926, test/accuracy=0.333500, test/loss=3.283153, test/num_examples=10000, total_duration=32236.467644, train/accuracy=0.483837, train/loss=2.256239, validation/accuracy=0.446620, validation/loss=2.491080, validation/num_examples=50000
I0129 10:23:32.778429 139655626356480 logging_writer.py:48] [91700] global_step=91700, grad_norm=1.6014671325683594, loss=1.9691877365112305
I0129 10:24:06.633059 139655617963776 logging_writer.py:48] [91800] global_step=91800, grad_norm=1.5586190223693848, loss=2.078886032104492
I0129 10:24:40.540576 139655626356480 logging_writer.py:48] [91900] global_step=91900, grad_norm=1.7697685956954956, loss=2.0727615356445312
I0129 10:25:14.453915 139655617963776 logging_writer.py:48] [92000] global_step=92000, grad_norm=1.5404906272888184, loss=2.0368447303771973
I0129 10:25:48.370643 139655626356480 logging_writer.py:48] [92100] global_step=92100, grad_norm=1.6401804685592651, loss=2.231928825378418
I0129 10:26:22.356068 139655617963776 logging_writer.py:48] [92200] global_step=92200, grad_norm=1.5440009832382202, loss=2.008455514907837
I0129 10:26:56.263735 139655626356480 logging_writer.py:48] [92300] global_step=92300, grad_norm=1.6386702060699463, loss=1.9028629064559937
I0129 10:27:30.165740 139655617963776 logging_writer.py:48] [92400] global_step=92400, grad_norm=1.7856141328811646, loss=1.949916958808899
I0129 10:28:04.078090 139655626356480 logging_writer.py:48] [92500] global_step=92500, grad_norm=1.439890742301941, loss=2.0257112979888916
I0129 10:28:37.930866 139655617963776 logging_writer.py:48] [92600] global_step=92600, grad_norm=1.5489531755447388, loss=1.9912463426589966
I0129 10:29:11.805557 139655626356480 logging_writer.py:48] [92700] global_step=92700, grad_norm=1.5854823589324951, loss=1.911308765411377
I0129 10:29:45.689628 139655617963776 logging_writer.py:48] [92800] global_step=92800, grad_norm=1.5406575202941895, loss=2.052405595779419
I0129 10:30:19.575549 139655626356480 logging_writer.py:48] [92900] global_step=92900, grad_norm=1.612357497215271, loss=1.9903992414474487
I0129 10:30:53.475639 139655617963776 logging_writer.py:48] [93000] global_step=93000, grad_norm=1.699479103088379, loss=1.9317963123321533
I0129 10:31:27.417252 139655626356480 logging_writer.py:48] [93100] global_step=93100, grad_norm=1.6228113174438477, loss=2.0223445892333984
I0129 10:31:43.151225 139822745589568 spec.py:321] Evaluating on the training split.
I0129 10:31:49.359182 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 10:31:58.000919 139822745589568 spec.py:349] Evaluating on the test split.
I0129 10:32:00.561027 139822745589568 submission_runner.py:408] Time since start: 32763.92s, 	Step: 93148, 	{'train/accuracy': 0.5083904266357422, 'train/loss': 2.1743996143341064, 'validation/accuracy': 0.47637999057769775, 'validation/loss': 2.3738784790039062, 'validation/num_examples': 50000, 'test/accuracy': 0.3620000183582306, 'test/loss': 3.1680712699890137, 'test/num_examples': 10000, 'score': 31657.057655096054, 'total_duration': 32763.91767191887, 'accumulated_submission_time': 31657.057655096054, 'accumulated_eval_time': 1101.381390094757, 'accumulated_logging_time': 2.27234148979187}
I0129 10:32:00.595263 139655617963776 logging_writer.py:48] [93148] accumulated_eval_time=1101.381390, accumulated_logging_time=2.272341, accumulated_submission_time=31657.057655, global_step=93148, preemption_count=0, score=31657.057655, test/accuracy=0.362000, test/loss=3.168071, test/num_examples=10000, total_duration=32763.917672, train/accuracy=0.508390, train/loss=2.174400, validation/accuracy=0.476380, validation/loss=2.373878, validation/num_examples=50000
I0129 10:32:18.584323 139655626356480 logging_writer.py:48] [93200] global_step=93200, grad_norm=1.5882819890975952, loss=1.9437923431396484
I0129 10:32:52.640342 139655617963776 logging_writer.py:48] [93300] global_step=93300, grad_norm=1.5846773386001587, loss=2.013624429702759
I0129 10:33:26.565410 139655626356480 logging_writer.py:48] [93400] global_step=93400, grad_norm=1.686805009841919, loss=1.8824118375778198
I0129 10:34:00.477705 139655617963776 logging_writer.py:48] [93500] global_step=93500, grad_norm=1.7431052923202515, loss=2.0114073753356934
I0129 10:34:34.401003 139655626356480 logging_writer.py:48] [93600] global_step=93600, grad_norm=1.5155941247940063, loss=1.9499013423919678
I0129 10:35:08.318512 139655617963776 logging_writer.py:48] [93700] global_step=93700, grad_norm=1.7605615854263306, loss=1.9598575830459595
I0129 10:35:42.216590 139655626356480 logging_writer.py:48] [93800] global_step=93800, grad_norm=1.6505999565124512, loss=2.098018169403076
I0129 10:36:16.126254 139655617963776 logging_writer.py:48] [93900] global_step=93900, grad_norm=1.7342597246170044, loss=1.9629040956497192
I0129 10:36:50.553102 139655626356480 logging_writer.py:48] [94000] global_step=94000, grad_norm=1.5181375741958618, loss=2.0132765769958496
I0129 10:37:24.469737 139655617963776 logging_writer.py:48] [94100] global_step=94100, grad_norm=1.6207656860351562, loss=1.9898070096969604
I0129 10:37:58.399080 139655626356480 logging_writer.py:48] [94200] global_step=94200, grad_norm=1.5317484140396118, loss=2.0478482246398926
I0129 10:38:32.304196 139655617963776 logging_writer.py:48] [94300] global_step=94300, grad_norm=1.5462956428527832, loss=2.013895273208618
I0129 10:39:06.364115 139655626356480 logging_writer.py:48] [94400] global_step=94400, grad_norm=1.7094552516937256, loss=1.9206790924072266
I0129 10:39:40.287963 139655617963776 logging_writer.py:48] [94500] global_step=94500, grad_norm=1.6490222215652466, loss=2.063467502593994
I0129 10:40:14.219289 139655626356480 logging_writer.py:48] [94600] global_step=94600, grad_norm=1.710650086402893, loss=2.0566329956054688
I0129 10:40:30.644226 139822745589568 spec.py:321] Evaluating on the training split.
I0129 10:40:36.775641 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 10:40:45.319583 139822745589568 spec.py:349] Evaluating on the test split.
I0129 10:40:47.914765 139822745589568 submission_runner.py:408] Time since start: 33291.27s, 	Step: 94650, 	{'train/accuracy': 0.545918345451355, 'train/loss': 1.944828987121582, 'validation/accuracy': 0.5107399821281433, 'validation/loss': 2.1304807662963867, 'validation/num_examples': 50000, 'test/accuracy': 0.39640000462532043, 'test/loss': 2.8958516120910645, 'test/num_examples': 10000, 'score': 32167.045583724976, 'total_duration': 33291.27138733864, 'accumulated_submission_time': 32167.045583724976, 'accumulated_eval_time': 1118.651871919632, 'accumulated_logging_time': 2.31717848777771}
I0129 10:40:47.953786 139656834316032 logging_writer.py:48] [94650] accumulated_eval_time=1118.651872, accumulated_logging_time=2.317178, accumulated_submission_time=32167.045584, global_step=94650, preemption_count=0, score=32167.045584, test/accuracy=0.396400, test/loss=2.895852, test/num_examples=10000, total_duration=33291.271387, train/accuracy=0.545918, train/loss=1.944829, validation/accuracy=0.510740, validation/loss=2.130481, validation/num_examples=50000
I0129 10:41:05.230347 139658730145536 logging_writer.py:48] [94700] global_step=94700, grad_norm=1.6717191934585571, loss=1.9701550006866455
I0129 10:41:39.097868 139656834316032 logging_writer.py:48] [94800] global_step=94800, grad_norm=1.4771620035171509, loss=1.897412896156311
I0129 10:42:12.969105 139658730145536 logging_writer.py:48] [94900] global_step=94900, grad_norm=1.8428736925125122, loss=2.077418327331543
I0129 10:42:46.904562 139656834316032 logging_writer.py:48] [95000] global_step=95000, grad_norm=1.4970102310180664, loss=1.9639092683792114
I0129 10:43:20.824827 139658730145536 logging_writer.py:48] [95100] global_step=95100, grad_norm=1.6306856870651245, loss=1.9334584474563599
I0129 10:43:54.757689 139656834316032 logging_writer.py:48] [95200] global_step=95200, grad_norm=1.6817044019699097, loss=1.898158311843872
I0129 10:44:28.691024 139658730145536 logging_writer.py:48] [95300] global_step=95300, grad_norm=1.5885212421417236, loss=1.9059594869613647
I0129 10:45:02.599024 139656834316032 logging_writer.py:48] [95400] global_step=95400, grad_norm=1.628980278968811, loss=1.9915237426757812
I0129 10:45:36.711216 139658730145536 logging_writer.py:48] [95500] global_step=95500, grad_norm=1.7692246437072754, loss=2.1544370651245117
I0129 10:46:10.606993 139656834316032 logging_writer.py:48] [95600] global_step=95600, grad_norm=1.6681849956512451, loss=2.004277467727661
I0129 10:46:44.535574 139658730145536 logging_writer.py:48] [95700] global_step=95700, grad_norm=1.6533056497573853, loss=2.007798671722412
I0129 10:47:18.452605 139656834316032 logging_writer.py:48] [95800] global_step=95800, grad_norm=1.6451432704925537, loss=1.962575912475586
I0129 10:47:52.379981 139658730145536 logging_writer.py:48] [95900] global_step=95900, grad_norm=1.5734765529632568, loss=2.0902974605560303
I0129 10:48:26.259249 139656834316032 logging_writer.py:48] [96000] global_step=96000, grad_norm=1.7247763872146606, loss=2.082305431365967
I0129 10:49:00.178103 139658730145536 logging_writer.py:48] [96100] global_step=96100, grad_norm=1.5595344305038452, loss=1.8545836210250854
I0129 10:49:17.961919 139822745589568 spec.py:321] Evaluating on the training split.
I0129 10:49:24.136135 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 10:49:32.876199 139822745589568 spec.py:349] Evaluating on the test split.
I0129 10:49:35.454856 139822745589568 submission_runner.py:408] Time since start: 33818.81s, 	Step: 96154, 	{'train/accuracy': 0.3833506107330322, 'train/loss': 2.867633581161499, 'validation/accuracy': 0.35995998978614807, 'validation/loss': 3.0436670780181885, 'validation/num_examples': 50000, 'test/accuracy': 0.2696000039577484, 'test/loss': 3.7725107669830322, 'test/num_examples': 10000, 'score': 32676.99285697937, 'total_duration': 33818.811498880386, 'accumulated_submission_time': 32676.99285697937, 'accumulated_eval_time': 1136.1447837352753, 'accumulated_logging_time': 2.3657150268554688}
I0129 10:49:35.490333 139655609571072 logging_writer.py:48] [96154] accumulated_eval_time=1136.144784, accumulated_logging_time=2.365715, accumulated_submission_time=32676.992857, global_step=96154, preemption_count=0, score=32676.992857, test/accuracy=0.269600, test/loss=3.772511, test/num_examples=10000, total_duration=33818.811499, train/accuracy=0.383351, train/loss=2.867634, validation/accuracy=0.359960, validation/loss=3.043667, validation/num_examples=50000
I0129 10:49:51.409641 139655617963776 logging_writer.py:48] [96200] global_step=96200, grad_norm=1.7533131837844849, loss=2.056861400604248
I0129 10:50:25.255040 139655609571072 logging_writer.py:48] [96300] global_step=96300, grad_norm=2.1477246284484863, loss=2.023082733154297
I0129 10:50:59.126533 139655617963776 logging_writer.py:48] [96400] global_step=96400, grad_norm=1.4878100156784058, loss=1.9008065462112427
I0129 10:51:33.042017 139655609571072 logging_writer.py:48] [96500] global_step=96500, grad_norm=1.7580063343048096, loss=1.9636932611465454
I0129 10:52:07.068474 139655617963776 logging_writer.py:48] [96600] global_step=96600, grad_norm=1.691032886505127, loss=1.9972631931304932
I0129 10:52:40.959543 139655609571072 logging_writer.py:48] [96700] global_step=96700, grad_norm=1.640761375427246, loss=1.9572865962982178
I0129 10:53:14.890391 139655617963776 logging_writer.py:48] [96800] global_step=96800, grad_norm=1.6047847270965576, loss=1.8754692077636719
I0129 10:53:48.789849 139655609571072 logging_writer.py:48] [96900] global_step=96900, grad_norm=1.7668989896774292, loss=2.065685272216797
I0129 10:54:22.706635 139655617963776 logging_writer.py:48] [97000] global_step=97000, grad_norm=1.565801739692688, loss=1.9549901485443115
I0129 10:54:56.618373 139655609571072 logging_writer.py:48] [97100] global_step=97100, grad_norm=1.6333562135696411, loss=1.9700219631195068
I0129 10:55:30.528467 139655617963776 logging_writer.py:48] [97200] global_step=97200, grad_norm=1.9178385734558105, loss=2.083648920059204
I0129 10:56:04.459435 139655609571072 logging_writer.py:48] [97300] global_step=97300, grad_norm=1.7741577625274658, loss=1.931596040725708
I0129 10:56:38.394703 139655617963776 logging_writer.py:48] [97400] global_step=97400, grad_norm=1.7722573280334473, loss=1.970255970954895
I0129 10:57:12.272424 139655609571072 logging_writer.py:48] [97500] global_step=97500, grad_norm=1.8420947790145874, loss=1.938694715499878
I0129 10:57:46.167542 139655617963776 logging_writer.py:48] [97600] global_step=97600, grad_norm=1.8319014310836792, loss=1.938693642616272
I0129 10:58:05.651131 139822745589568 spec.py:321] Evaluating on the training split.
I0129 10:58:11.831635 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 10:58:20.401221 139822745589568 spec.py:349] Evaluating on the test split.
I0129 10:58:22.974430 139822745589568 submission_runner.py:408] Time since start: 34346.33s, 	Step: 97659, 	{'train/accuracy': 0.5059789419174194, 'train/loss': 2.157892942428589, 'validation/accuracy': 0.47516000270843506, 'validation/loss': 2.326932668685913, 'validation/num_examples': 50000, 'test/accuracy': 0.35910001397132874, 'test/loss': 3.0999302864074707, 'test/num_examples': 10000, 'score': 33187.0912899971, 'total_duration': 34346.33107614517, 'accumulated_submission_time': 33187.0912899971, 'accumulated_eval_time': 1153.468049287796, 'accumulated_logging_time': 2.4127352237701416}
I0129 10:58:23.011794 139656666543872 logging_writer.py:48] [97659] accumulated_eval_time=1153.468049, accumulated_logging_time=2.412735, accumulated_submission_time=33187.091290, global_step=97659, preemption_count=0, score=33187.091290, test/accuracy=0.359100, test/loss=3.099930, test/num_examples=10000, total_duration=34346.331076, train/accuracy=0.505979, train/loss=2.157893, validation/accuracy=0.475160, validation/loss=2.326933, validation/num_examples=50000
I0129 10:58:37.332610 139656834316032 logging_writer.py:48] [97700] global_step=97700, grad_norm=1.9064141511917114, loss=1.9346816539764404
I0129 10:59:11.155297 139656666543872 logging_writer.py:48] [97800] global_step=97800, grad_norm=1.6815234422683716, loss=2.0868067741394043
I0129 10:59:45.059647 139656834316032 logging_writer.py:48] [97900] global_step=97900, grad_norm=1.643088459968567, loss=1.9022830724716187
I0129 11:00:18.931527 139656666543872 logging_writer.py:48] [98000] global_step=98000, grad_norm=1.8760324716567993, loss=2.015120267868042
I0129 11:00:52.825233 139656834316032 logging_writer.py:48] [98100] global_step=98100, grad_norm=1.77360200881958, loss=1.9310721158981323
I0129 11:01:26.723030 139656666543872 logging_writer.py:48] [98200] global_step=98200, grad_norm=1.7482984066009521, loss=1.8823695182800293
I0129 11:02:00.596743 139656834316032 logging_writer.py:48] [98300] global_step=98300, grad_norm=1.7071611881256104, loss=1.9453706741333008
I0129 11:02:34.467747 139656666543872 logging_writer.py:48] [98400] global_step=98400, grad_norm=1.6825010776519775, loss=2.01651668548584
I0129 11:03:08.344316 139656834316032 logging_writer.py:48] [98500] global_step=98500, grad_norm=1.6496176719665527, loss=1.9316266775131226
I0129 11:03:42.252696 139656666543872 logging_writer.py:48] [98600] global_step=98600, grad_norm=1.788785457611084, loss=2.011199951171875
I0129 11:04:16.167675 139656834316032 logging_writer.py:48] [98700] global_step=98700, grad_norm=1.7321105003356934, loss=2.0575063228607178
I0129 11:04:50.094032 139656666543872 logging_writer.py:48] [98800] global_step=98800, grad_norm=1.822801113128662, loss=1.9580076932907104
I0129 11:05:24.083711 139656834316032 logging_writer.py:48] [98900] global_step=98900, grad_norm=1.7219277620315552, loss=1.9386087656021118
I0129 11:05:57.962394 139656666543872 logging_writer.py:48] [99000] global_step=99000, grad_norm=1.6042147874832153, loss=1.9625569581985474
I0129 11:06:31.902654 139656834316032 logging_writer.py:48] [99100] global_step=99100, grad_norm=1.602107286453247, loss=1.833523154258728
I0129 11:06:53.056638 139822745589568 spec.py:321] Evaluating on the training split.
I0129 11:06:59.230690 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 11:07:08.011270 139822745589568 spec.py:349] Evaluating on the test split.
I0129 11:07:10.576946 139822745589568 submission_runner.py:408] Time since start: 34873.93s, 	Step: 99164, 	{'train/accuracy': 0.5677016973495483, 'train/loss': 1.840865135192871, 'validation/accuracy': 0.5112400054931641, 'validation/loss': 2.122875928878784, 'validation/num_examples': 50000, 'test/accuracy': 0.3920000195503235, 'test/loss': 2.8797783851623535, 'test/num_examples': 10000, 'score': 33697.07434248924, 'total_duration': 34873.93353009224, 'accumulated_submission_time': 33697.07434248924, 'accumulated_eval_time': 1170.9882607460022, 'accumulated_logging_time': 2.4600107669830322}
I0129 11:07:10.615963 139656297445120 logging_writer.py:48] [99164] accumulated_eval_time=1170.988261, accumulated_logging_time=2.460011, accumulated_submission_time=33697.074342, global_step=99164, preemption_count=0, score=33697.074342, test/accuracy=0.392000, test/loss=2.879778, test/num_examples=10000, total_duration=34873.933530, train/accuracy=0.567702, train/loss=1.840865, validation/accuracy=0.511240, validation/loss=2.122876, validation/num_examples=50000
I0129 11:07:23.141774 139656649758464 logging_writer.py:48] [99200] global_step=99200, grad_norm=1.663781762123108, loss=2.035679578781128
I0129 11:07:56.973922 139656297445120 logging_writer.py:48] [99300] global_step=99300, grad_norm=1.9083340167999268, loss=2.03570294380188
I0129 11:08:30.827590 139656649758464 logging_writer.py:48] [99400] global_step=99400, grad_norm=1.7887444496154785, loss=1.8904352188110352
I0129 11:09:04.710870 139656297445120 logging_writer.py:48] [99500] global_step=99500, grad_norm=1.62699294090271, loss=1.950965166091919
I0129 11:09:38.581315 139656649758464 logging_writer.py:48] [99600] global_step=99600, grad_norm=1.7245776653289795, loss=1.9559025764465332
I0129 11:10:12.512034 139656297445120 logging_writer.py:48] [99700] global_step=99700, grad_norm=1.5975626707077026, loss=1.8850607872009277
I0129 11:10:46.383924 139656649758464 logging_writer.py:48] [99800] global_step=99800, grad_norm=1.7423315048217773, loss=1.8087420463562012
I0129 11:11:20.483431 139656297445120 logging_writer.py:48] [99900] global_step=99900, grad_norm=1.6315162181854248, loss=2.104951858520508
I0129 11:11:54.353329 139656649758464 logging_writer.py:48] [100000] global_step=100000, grad_norm=1.7990198135375977, loss=1.918345332145691
I0129 11:12:28.242301 139656297445120 logging_writer.py:48] [100100] global_step=100100, grad_norm=1.7800796031951904, loss=2.0113112926483154
I0129 11:13:02.159926 139656649758464 logging_writer.py:48] [100200] global_step=100200, grad_norm=1.553663969039917, loss=1.7821146249771118
I0129 11:13:36.055256 139656297445120 logging_writer.py:48] [100300] global_step=100300, grad_norm=1.7351733446121216, loss=2.0667057037353516
I0129 11:14:09.993740 139656649758464 logging_writer.py:48] [100400] global_step=100400, grad_norm=1.845268726348877, loss=1.9169867038726807
I0129 11:14:43.930373 139656297445120 logging_writer.py:48] [100500] global_step=100500, grad_norm=1.7866346836090088, loss=1.9813579320907593
I0129 11:15:17.847226 139656649758464 logging_writer.py:48] [100600] global_step=100600, grad_norm=1.8359113931655884, loss=1.8861944675445557
I0129 11:15:40.707440 139822745589568 spec.py:321] Evaluating on the training split.
I0129 11:15:46.916609 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 11:15:55.418199 139822745589568 spec.py:349] Evaluating on the test split.
I0129 11:15:58.069478 139822745589568 submission_runner.py:408] Time since start: 35401.43s, 	Step: 100669, 	{'train/accuracy': 0.4465082883834839, 'train/loss': 2.48357892036438, 'validation/accuracy': 0.4145599901676178, 'validation/loss': 2.690641403198242, 'validation/num_examples': 50000, 'test/accuracy': 0.30480000376701355, 'test/loss': 3.538769245147705, 'test/num_examples': 10000, 'score': 34207.1025724411, 'total_duration': 35401.42611813545, 'accumulated_submission_time': 34207.1025724411, 'accumulated_eval_time': 1188.3502779006958, 'accumulated_logging_time': 2.5082507133483887}
I0129 11:15:58.109276 139655626356480 logging_writer.py:48] [100669] accumulated_eval_time=1188.350278, accumulated_logging_time=2.508251, accumulated_submission_time=34207.102572, global_step=100669, preemption_count=0, score=34207.102572, test/accuracy=0.304800, test/loss=3.538769, test/num_examples=10000, total_duration=35401.426118, train/accuracy=0.446508, train/loss=2.483579, validation/accuracy=0.414560, validation/loss=2.690641, validation/num_examples=50000
I0129 11:16:08.970870 139656297445120 logging_writer.py:48] [100700] global_step=100700, grad_norm=1.8444039821624756, loss=2.0789976119995117
I0129 11:16:42.799912 139655626356480 logging_writer.py:48] [100800] global_step=100800, grad_norm=1.824759602546692, loss=1.7984408140182495
I0129 11:17:16.672987 139656297445120 logging_writer.py:48] [100900] global_step=100900, grad_norm=1.8483916521072388, loss=2.043511390686035
I0129 11:17:50.698810 139655626356480 logging_writer.py:48] [101000] global_step=101000, grad_norm=1.8083336353302002, loss=2.184896469116211
I0129 11:18:24.583385 139656297445120 logging_writer.py:48] [101100] global_step=101100, grad_norm=1.7277908325195312, loss=1.8448408842086792
I0129 11:18:58.422881 139655626356480 logging_writer.py:48] [101200] global_step=101200, grad_norm=1.879302978515625, loss=1.96181058883667
I0129 11:19:32.301187 139656297445120 logging_writer.py:48] [101300] global_step=101300, grad_norm=1.7444391250610352, loss=1.8989248275756836
I0129 11:20:06.187440 139655626356480 logging_writer.py:48] [101400] global_step=101400, grad_norm=1.8547290563583374, loss=1.9922597408294678
I0129 11:20:40.083609 139656297445120 logging_writer.py:48] [101500] global_step=101500, grad_norm=1.6124510765075684, loss=1.9150055646896362
I0129 11:21:13.978754 139655626356480 logging_writer.py:48] [101600] global_step=101600, grad_norm=1.8580071926116943, loss=1.920055627822876
I0129 11:21:47.912086 139656297445120 logging_writer.py:48] [101700] global_step=101700, grad_norm=1.9880669116973877, loss=1.845557451248169
I0129 11:22:21.810040 139655626356480 logging_writer.py:48] [101800] global_step=101800, grad_norm=1.704877257347107, loss=1.9129667282104492
I0129 11:22:55.732562 139656297445120 logging_writer.py:48] [101900] global_step=101900, grad_norm=1.8009394407272339, loss=2.11161732673645
I0129 11:23:29.627525 139655626356480 logging_writer.py:48] [102000] global_step=102000, grad_norm=1.826796531677246, loss=1.9600858688354492
I0129 11:24:03.563501 139656297445120 logging_writer.py:48] [102100] global_step=102100, grad_norm=1.7987864017486572, loss=1.962161898612976
I0129 11:24:28.194439 139822745589568 spec.py:321] Evaluating on the training split.
I0129 11:24:34.392662 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 11:24:43.040608 139822745589568 spec.py:349] Evaluating on the test split.
I0129 11:24:45.630015 139822745589568 submission_runner.py:408] Time since start: 35928.99s, 	Step: 102174, 	{'train/accuracy': 0.5240353941917419, 'train/loss': 2.0571160316467285, 'validation/accuracy': 0.4918999969959259, 'validation/loss': 2.25252103805542, 'validation/num_examples': 50000, 'test/accuracy': 0.3846000134944916, 'test/loss': 2.9995460510253906, 'test/num_examples': 10000, 'score': 34717.1273932457, 'total_duration': 35928.986558914185, 'accumulated_submission_time': 34717.1273932457, 'accumulated_eval_time': 1205.7857220172882, 'accumulated_logging_time': 2.5568745136260986}
I0129 11:24:45.668225 139655617963776 logging_writer.py:48] [102174] accumulated_eval_time=1205.785722, accumulated_logging_time=2.556875, accumulated_submission_time=34717.127393, global_step=102174, preemption_count=0, score=34717.127393, test/accuracy=0.384600, test/loss=2.999546, test/num_examples=10000, total_duration=35928.986559, train/accuracy=0.524035, train/loss=2.057116, validation/accuracy=0.491900, validation/loss=2.252521, validation/num_examples=50000
I0129 11:24:54.816406 139655626356480 logging_writer.py:48] [102200] global_step=102200, grad_norm=1.902805209159851, loss=1.876771092414856
I0129 11:25:28.660076 139655617963776 logging_writer.py:48] [102300] global_step=102300, grad_norm=1.821415901184082, loss=1.7714661359786987
I0129 11:26:02.552636 139655626356480 logging_writer.py:48] [102400] global_step=102400, grad_norm=1.8418662548065186, loss=1.9279656410217285
I0129 11:26:36.454275 139655617963776 logging_writer.py:48] [102500] global_step=102500, grad_norm=1.7008494138717651, loss=1.9557394981384277
I0129 11:27:10.384157 139655626356480 logging_writer.py:48] [102600] global_step=102600, grad_norm=1.6009052991867065, loss=1.9697695970535278
I0129 11:27:44.299696 139655617963776 logging_writer.py:48] [102700] global_step=102700, grad_norm=1.6798018217086792, loss=1.801748514175415
I0129 11:28:18.221443 139655626356480 logging_writer.py:48] [102800] global_step=102800, grad_norm=1.7249070405960083, loss=1.8554396629333496
I0129 11:28:52.104924 139655617963776 logging_writer.py:48] [102900] global_step=102900, grad_norm=1.8894468545913696, loss=1.9963018894195557
I0129 11:29:25.999014 139655626356480 logging_writer.py:48] [103000] global_step=103000, grad_norm=1.7601076364517212, loss=2.051408290863037
I0129 11:29:59.925469 139655617963776 logging_writer.py:48] [103100] global_step=103100, grad_norm=1.7485899925231934, loss=1.8800387382507324
I0129 11:30:33.855941 139655626356480 logging_writer.py:48] [103200] global_step=103200, grad_norm=1.9404665231704712, loss=1.963623046875
I0129 11:31:07.922749 139655617963776 logging_writer.py:48] [103300] global_step=103300, grad_norm=1.8892956972122192, loss=2.00118350982666
I0129 11:31:41.841276 139655626356480 logging_writer.py:48] [103400] global_step=103400, grad_norm=1.7140799760818481, loss=1.900103211402893
I0129 11:32:15.701220 139655617963776 logging_writer.py:48] [103500] global_step=103500, grad_norm=1.7637889385223389, loss=1.9823247194290161
I0129 11:32:49.618346 139655626356480 logging_writer.py:48] [103600] global_step=103600, grad_norm=1.8852598667144775, loss=2.090778350830078
I0129 11:33:15.856486 139822745589568 spec.py:321] Evaluating on the training split.
I0129 11:33:22.040142 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 11:33:30.789814 139822745589568 spec.py:349] Evaluating on the test split.
I0129 11:33:33.412052 139822745589568 submission_runner.py:408] Time since start: 36456.77s, 	Step: 103679, 	{'train/accuracy': 0.5104631781578064, 'train/loss': 2.159630060195923, 'validation/accuracy': 0.471919983625412, 'validation/loss': 2.3893167972564697, 'validation/num_examples': 50000, 'test/accuracy': 0.3644000291824341, 'test/loss': 3.1780288219451904, 'test/num_examples': 10000, 'score': 35227.254885435104, 'total_duration': 36456.76866769791, 'accumulated_submission_time': 35227.254885435104, 'accumulated_eval_time': 1223.3412280082703, 'accumulated_logging_time': 2.6039724349975586}
I0129 11:33:33.453492 139655617963776 logging_writer.py:48] [103679] accumulated_eval_time=1223.341228, accumulated_logging_time=2.603972, accumulated_submission_time=35227.254885, global_step=103679, preemption_count=0, score=35227.254885, test/accuracy=0.364400, test/loss=3.178029, test/num_examples=10000, total_duration=36456.768668, train/accuracy=0.510463, train/loss=2.159630, validation/accuracy=0.471920, validation/loss=2.389317, validation/num_examples=50000
I0129 11:33:40.898967 139656666543872 logging_writer.py:48] [103700] global_step=103700, grad_norm=1.6905814409255981, loss=1.8724384307861328
I0129 11:34:14.733191 139655617963776 logging_writer.py:48] [103800] global_step=103800, grad_norm=1.8692830801010132, loss=1.8911094665527344
I0129 11:34:48.567129 139656666543872 logging_writer.py:48] [103900] global_step=103900, grad_norm=1.8320059776306152, loss=1.8620880842208862
I0129 11:35:22.450674 139655617963776 logging_writer.py:48] [104000] global_step=104000, grad_norm=1.913375973701477, loss=1.9979512691497803
I0129 11:35:56.375941 139656666543872 logging_writer.py:48] [104100] global_step=104100, grad_norm=1.780913233757019, loss=1.9859038591384888
I0129 11:36:30.299054 139655617963776 logging_writer.py:48] [104200] global_step=104200, grad_norm=1.7078700065612793, loss=1.8719968795776367
I0129 11:37:04.221128 139656666543872 logging_writer.py:48] [104300] global_step=104300, grad_norm=1.7688225507736206, loss=1.8667470216751099
I0129 11:37:38.238236 139655617963776 logging_writer.py:48] [104400] global_step=104400, grad_norm=1.6894629001617432, loss=1.8818851709365845
I0129 11:38:12.132875 139656666543872 logging_writer.py:48] [104500] global_step=104500, grad_norm=1.9991190433502197, loss=1.9679384231567383
I0129 11:38:46.054233 139655617963776 logging_writer.py:48] [104600] global_step=104600, grad_norm=1.7315149307250977, loss=1.9415487051010132
I0129 11:39:19.978990 139656666543872 logging_writer.py:48] [104700] global_step=104700, grad_norm=2.045936107635498, loss=1.9852731227874756
I0129 11:39:53.908797 139655617963776 logging_writer.py:48] [104800] global_step=104800, grad_norm=1.7464226484298706, loss=1.9732067584991455
I0129 11:40:27.860209 139656666543872 logging_writer.py:48] [104900] global_step=104900, grad_norm=1.7583681344985962, loss=1.8388373851776123
I0129 11:41:01.759507 139655617963776 logging_writer.py:48] [105000] global_step=105000, grad_norm=1.7203552722930908, loss=1.8589171171188354
I0129 11:41:35.700285 139656666543872 logging_writer.py:48] [105100] global_step=105100, grad_norm=1.751546859741211, loss=1.8818044662475586
I0129 11:42:03.596673 139822745589568 spec.py:321] Evaluating on the training split.
I0129 11:42:09.800210 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 11:42:18.330761 139822745589568 spec.py:349] Evaluating on the test split.
I0129 11:42:20.921555 139822745589568 submission_runner.py:408] Time since start: 36984.28s, 	Step: 105184, 	{'train/accuracy': 0.5213249325752258, 'train/loss': 2.0907678604125977, 'validation/accuracy': 0.48767998814582825, 'validation/loss': 2.284471035003662, 'validation/num_examples': 50000, 'test/accuracy': 0.388700008392334, 'test/loss': 2.9926905632019043, 'test/num_examples': 10000, 'score': 35737.33686733246, 'total_duration': 36984.278197050095, 'accumulated_submission_time': 35737.33686733246, 'accumulated_eval_time': 1240.666074514389, 'accumulated_logging_time': 2.6544456481933594}
I0129 11:42:20.960223 139655626356480 logging_writer.py:48] [105184] accumulated_eval_time=1240.666075, accumulated_logging_time=2.654446, accumulated_submission_time=35737.336867, global_step=105184, preemption_count=0, score=35737.336867, test/accuracy=0.388700, test/loss=2.992691, test/num_examples=10000, total_duration=36984.278197, train/accuracy=0.521325, train/loss=2.090768, validation/accuracy=0.487680, validation/loss=2.284471, validation/num_examples=50000
I0129 11:42:26.726697 139656297445120 logging_writer.py:48] [105200] global_step=105200, grad_norm=1.6650481224060059, loss=1.7762715816497803
I0129 11:43:00.579160 139655626356480 logging_writer.py:48] [105300] global_step=105300, grad_norm=1.7042665481567383, loss=1.9321033954620361
I0129 11:43:34.481289 139656297445120 logging_writer.py:48] [105400] global_step=105400, grad_norm=1.823837399482727, loss=2.0050487518310547
I0129 11:44:08.420132 139655626356480 logging_writer.py:48] [105500] global_step=105500, grad_norm=1.906073808670044, loss=1.9366093873977661
I0129 11:44:42.329893 139656297445120 logging_writer.py:48] [105600] global_step=105600, grad_norm=1.8997690677642822, loss=1.8856735229492188
I0129 11:45:16.212261 139655626356480 logging_writer.py:48] [105700] global_step=105700, grad_norm=1.9318557977676392, loss=2.0873231887817383
I0129 11:45:50.135312 139656297445120 logging_writer.py:48] [105800] global_step=105800, grad_norm=1.8442230224609375, loss=1.9436874389648438
I0129 11:46:24.035649 139655626356480 logging_writer.py:48] [105900] global_step=105900, grad_norm=1.8164198398590088, loss=1.854427456855774
I0129 11:46:57.959084 139656297445120 logging_writer.py:48] [106000] global_step=106000, grad_norm=1.8434072732925415, loss=1.8972748517990112
I0129 11:47:31.847426 139655626356480 logging_writer.py:48] [106100] global_step=106100, grad_norm=1.8315309286117554, loss=1.9138190746307373
I0129 11:48:05.736161 139656297445120 logging_writer.py:48] [106200] global_step=106200, grad_norm=1.7692581415176392, loss=2.001047372817993
I0129 11:48:39.615893 139655626356480 logging_writer.py:48] [106300] global_step=106300, grad_norm=1.891806960105896, loss=1.9344217777252197
I0129 11:49:13.499142 139656297445120 logging_writer.py:48] [106400] global_step=106400, grad_norm=1.9102171659469604, loss=1.9913876056671143
I0129 11:49:47.436537 139655626356480 logging_writer.py:48] [106500] global_step=106500, grad_norm=1.9650055170059204, loss=1.9267421960830688
I0129 11:50:21.405317 139656297445120 logging_writer.py:48] [106600] global_step=106600, grad_norm=1.7893108129501343, loss=1.9927942752838135
I0129 11:50:51.026716 139822745589568 spec.py:321] Evaluating on the training split.
I0129 11:50:57.200593 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 11:51:05.769783 139822745589568 spec.py:349] Evaluating on the test split.
I0129 11:51:08.427469 139822745589568 submission_runner.py:408] Time since start: 37511.78s, 	Step: 106689, 	{'train/accuracy': 0.5049425959587097, 'train/loss': 2.1846439838409424, 'validation/accuracy': 0.47189998626708984, 'validation/loss': 2.3848423957824707, 'validation/num_examples': 50000, 'test/accuracy': 0.36070001125335693, 'test/loss': 3.1824216842651367, 'test/num_examples': 10000, 'score': 36247.34074640274, 'total_duration': 37511.78411793709, 'accumulated_submission_time': 36247.34074640274, 'accumulated_eval_time': 1258.0668041706085, 'accumulated_logging_time': 2.7028579711914062}
I0129 11:51:08.457469 139656666543872 logging_writer.py:48] [106689] accumulated_eval_time=1258.066804, accumulated_logging_time=2.702858, accumulated_submission_time=36247.340746, global_step=106689, preemption_count=0, score=36247.340746, test/accuracy=0.360700, test/loss=3.182422, test/num_examples=10000, total_duration=37511.784118, train/accuracy=0.504943, train/loss=2.184644, validation/accuracy=0.471900, validation/loss=2.384842, validation/num_examples=50000
I0129 11:51:12.518550 139656834316032 logging_writer.py:48] [106700] global_step=106700, grad_norm=2.034217596054077, loss=1.854087471961975
I0129 11:51:46.358639 139656666543872 logging_writer.py:48] [106800] global_step=106800, grad_norm=2.018669605255127, loss=1.9138004779815674
I0129 11:52:20.241889 139656834316032 logging_writer.py:48] [106900] global_step=106900, grad_norm=2.0213701725006104, loss=1.867226243019104
I0129 11:52:54.135735 139656666543872 logging_writer.py:48] [107000] global_step=107000, grad_norm=1.7505359649658203, loss=1.9858447313308716
I0129 11:53:28.047266 139656834316032 logging_writer.py:48] [107100] global_step=107100, grad_norm=1.8328059911727905, loss=1.900076150894165
I0129 11:54:01.947896 139656666543872 logging_writer.py:48] [107200] global_step=107200, grad_norm=1.854711651802063, loss=1.8318899869918823
I0129 11:54:35.826824 139656834316032 logging_writer.py:48] [107300] global_step=107300, grad_norm=1.8155148029327393, loss=1.9560027122497559
I0129 11:55:09.752756 139656666543872 logging_writer.py:48] [107400] global_step=107400, grad_norm=1.810091257095337, loss=1.862627387046814
I0129 11:55:43.654902 139656834316032 logging_writer.py:48] [107500] global_step=107500, grad_norm=1.725197196006775, loss=1.846716046333313
I0129 11:56:17.589152 139656666543872 logging_writer.py:48] [107600] global_step=107600, grad_norm=1.9596662521362305, loss=2.0960798263549805
I0129 11:56:51.544254 139656834316032 logging_writer.py:48] [107700] global_step=107700, grad_norm=1.8979294300079346, loss=1.7992634773254395
I0129 11:57:25.439471 139656666543872 logging_writer.py:48] [107800] global_step=107800, grad_norm=1.794255018234253, loss=1.869042158126831
I0129 11:57:59.361150 139656834316032 logging_writer.py:48] [107900] global_step=107900, grad_norm=1.9575430154800415, loss=1.8942536115646362
I0129 11:58:33.251131 139656666543872 logging_writer.py:48] [108000] global_step=108000, grad_norm=1.7438163757324219, loss=2.0125114917755127
I0129 11:59:07.153491 139656834316032 logging_writer.py:48] [108100] global_step=108100, grad_norm=2.0459344387054443, loss=1.845467448234558
I0129 11:59:38.515622 139822745589568 spec.py:321] Evaluating on the training split.
I0129 11:59:44.695533 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 11:59:53.161916 139822745589568 spec.py:349] Evaluating on the test split.
I0129 11:59:55.732729 139822745589568 submission_runner.py:408] Time since start: 38039.09s, 	Step: 108194, 	{'train/accuracy': 0.48692601919174194, 'train/loss': 2.2414798736572266, 'validation/accuracy': 0.43511998653411865, 'validation/loss': 2.5659117698669434, 'validation/num_examples': 50000, 'test/accuracy': 0.31310001015663147, 'test/loss': 3.4565787315368652, 'test/num_examples': 10000, 'score': 36757.33936190605, 'total_duration': 38039.08936858177, 'accumulated_submission_time': 36757.33936190605, 'accumulated_eval_time': 1275.2838730812073, 'accumulated_logging_time': 2.7409493923187256}
I0129 11:59:55.770472 139656297445120 logging_writer.py:48] [108194] accumulated_eval_time=1275.283873, accumulated_logging_time=2.740949, accumulated_submission_time=36757.339362, global_step=108194, preemption_count=0, score=36757.339362, test/accuracy=0.313100, test/loss=3.456579, test/num_examples=10000, total_duration=38039.089369, train/accuracy=0.486926, train/loss=2.241480, validation/accuracy=0.435120, validation/loss=2.565912, validation/num_examples=50000
I0129 11:59:58.152016 139656649758464 logging_writer.py:48] [108200] global_step=108200, grad_norm=1.9261215925216675, loss=1.8981801271438599
I0129 12:00:32.008552 139656297445120 logging_writer.py:48] [108300] global_step=108300, grad_norm=1.9088501930236816, loss=1.8924814462661743
I0129 12:01:05.870350 139656649758464 logging_writer.py:48] [108400] global_step=108400, grad_norm=2.2598090171813965, loss=2.0107052326202393
I0129 12:01:39.736220 139656297445120 logging_writer.py:48] [108500] global_step=108500, grad_norm=1.9353481531143188, loss=2.0102996826171875
I0129 12:02:13.635949 139656649758464 logging_writer.py:48] [108600] global_step=108600, grad_norm=1.9675475358963013, loss=1.8735915422439575
I0129 12:02:47.522848 139656297445120 logging_writer.py:48] [108700] global_step=108700, grad_norm=1.8291957378387451, loss=1.8992069959640503
I0129 12:03:21.589622 139656649758464 logging_writer.py:48] [108800] global_step=108800, grad_norm=1.9516810178756714, loss=1.8548072576522827
I0129 12:03:55.502305 139656297445120 logging_writer.py:48] [108900] global_step=108900, grad_norm=2.3237318992614746, loss=1.991888165473938
I0129 12:04:29.394654 139656649758464 logging_writer.py:48] [109000] global_step=109000, grad_norm=1.7639384269714355, loss=1.8512376546859741
I0129 12:05:03.298471 139656297445120 logging_writer.py:48] [109100] global_step=109100, grad_norm=1.8260830640792847, loss=1.8768497705459595
I0129 12:05:37.206644 139656649758464 logging_writer.py:48] [109200] global_step=109200, grad_norm=2.0560271739959717, loss=1.7581729888916016
I0129 12:06:11.070904 139656297445120 logging_writer.py:48] [109300] global_step=109300, grad_norm=1.9490766525268555, loss=1.8261281251907349
I0129 12:06:44.981911 139656649758464 logging_writer.py:48] [109400] global_step=109400, grad_norm=2.108025074005127, loss=1.8653963804244995
I0129 12:07:18.893322 139656297445120 logging_writer.py:48] [109500] global_step=109500, grad_norm=1.881077527999878, loss=1.9925514459609985
I0129 12:07:52.803545 139656649758464 logging_writer.py:48] [109600] global_step=109600, grad_norm=1.9317286014556885, loss=1.9373626708984375
I0129 12:08:25.830080 139822745589568 spec.py:321] Evaluating on the training split.
I0129 12:08:31.990209 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 12:08:40.632835 139822745589568 spec.py:349] Evaluating on the test split.
I0129 12:08:43.185019 139822745589568 submission_runner.py:408] Time since start: 38566.54s, 	Step: 109699, 	{'train/accuracy': 0.5515983700752258, 'train/loss': 1.900874376296997, 'validation/accuracy': 0.5029199719429016, 'validation/loss': 2.1730642318725586, 'validation/num_examples': 50000, 'test/accuracy': 0.39980003237724304, 'test/loss': 2.946348190307617, 'test/num_examples': 10000, 'score': 37267.33650159836, 'total_duration': 38566.54166126251, 'accumulated_submission_time': 37267.33650159836, 'accumulated_eval_time': 1292.6387770175934, 'accumulated_logging_time': 2.7895431518554688}
I0129 12:08:43.223414 139656666543872 logging_writer.py:48] [109699] accumulated_eval_time=1292.638777, accumulated_logging_time=2.789543, accumulated_submission_time=37267.336502, global_step=109699, preemption_count=0, score=37267.336502, test/accuracy=0.399800, test/loss=2.946348, test/num_examples=10000, total_duration=38566.541661, train/accuracy=0.551598, train/loss=1.900874, validation/accuracy=0.502920, validation/loss=2.173064, validation/num_examples=50000
I0129 12:08:43.918063 139656834316032 logging_writer.py:48] [109700] global_step=109700, grad_norm=1.8642971515655518, loss=1.897461175918579
I0129 12:09:17.753946 139656666543872 logging_writer.py:48] [109800] global_step=109800, grad_norm=2.033182382583618, loss=1.854851245880127
I0129 12:09:51.744996 139656834316032 logging_writer.py:48] [109900] global_step=109900, grad_norm=1.8401899337768555, loss=1.9067943096160889
I0129 12:10:25.654435 139656666543872 logging_writer.py:48] [110000] global_step=110000, grad_norm=2.077430009841919, loss=1.9327603578567505
I0129 12:10:59.559876 139656834316032 logging_writer.py:48] [110100] global_step=110100, grad_norm=1.769075632095337, loss=1.8967349529266357
I0129 12:11:33.481350 139656666543872 logging_writer.py:48] [110200] global_step=110200, grad_norm=2.086435556411743, loss=1.92927086353302
I0129 12:12:07.405999 139656834316032 logging_writer.py:48] [110300] global_step=110300, grad_norm=1.9026808738708496, loss=1.9734739065170288
I0129 12:12:41.307383 139656666543872 logging_writer.py:48] [110400] global_step=110400, grad_norm=1.9192359447479248, loss=1.98807954788208
I0129 12:13:15.236033 139656834316032 logging_writer.py:48] [110500] global_step=110500, grad_norm=1.9286112785339355, loss=1.879298448562622
I0129 12:13:49.137401 139656666543872 logging_writer.py:48] [110600] global_step=110600, grad_norm=1.9091284275054932, loss=1.832046627998352
I0129 12:14:23.069745 139656834316032 logging_writer.py:48] [110700] global_step=110700, grad_norm=2.0164754390716553, loss=1.9239310026168823
I0129 12:14:56.981923 139656666543872 logging_writer.py:48] [110800] global_step=110800, grad_norm=1.8699193000793457, loss=1.8623278141021729
I0129 12:15:30.883419 139656834316032 logging_writer.py:48] [110900] global_step=110900, grad_norm=2.0817158222198486, loss=1.9116129875183105
I0129 12:16:04.805778 139656666543872 logging_writer.py:48] [111000] global_step=111000, grad_norm=2.0458266735076904, loss=2.0021908283233643
I0129 12:16:38.836259 139656834316032 logging_writer.py:48] [111100] global_step=111100, grad_norm=1.877946376800537, loss=1.8508720397949219
I0129 12:17:12.770758 139656666543872 logging_writer.py:48] [111200] global_step=111200, grad_norm=2.054905652999878, loss=1.8326677083969116
I0129 12:17:13.259046 139822745589568 spec.py:321] Evaluating on the training split.
I0129 12:17:19.492339 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 12:17:28.190638 139822745589568 spec.py:349] Evaluating on the test split.
I0129 12:17:30.804891 139822745589568 submission_runner.py:408] Time since start: 39094.16s, 	Step: 111203, 	{'train/accuracy': 0.5053411722183228, 'train/loss': 2.157813310623169, 'validation/accuracy': 0.46757999062538147, 'validation/loss': 2.3963406085968018, 'validation/num_examples': 50000, 'test/accuracy': 0.35910001397132874, 'test/loss': 3.1881215572357178, 'test/num_examples': 10000, 'score': 37777.31170344353, 'total_duration': 39094.161532878876, 'accumulated_submission_time': 37777.31170344353, 'accumulated_eval_time': 1310.1845960617065, 'accumulated_logging_time': 2.837157964706421}
I0129 12:17:30.845279 139655626356480 logging_writer.py:48] [111203] accumulated_eval_time=1310.184596, accumulated_logging_time=2.837158, accumulated_submission_time=37777.311703, global_step=111203, preemption_count=0, score=37777.311703, test/accuracy=0.359100, test/loss=3.188122, test/num_examples=10000, total_duration=39094.161533, train/accuracy=0.505341, train/loss=2.157813, validation/accuracy=0.467580, validation/loss=2.396341, validation/num_examples=50000
I0129 12:18:04.059545 139656297445120 logging_writer.py:48] [111300] global_step=111300, grad_norm=1.899125099182129, loss=2.0268404483795166
I0129 12:18:37.949502 139655626356480 logging_writer.py:48] [111400] global_step=111400, grad_norm=1.8705500364303589, loss=1.846668004989624
I0129 12:19:11.849584 139656297445120 logging_writer.py:48] [111500] global_step=111500, grad_norm=1.9620290994644165, loss=1.8537890911102295
I0129 12:19:45.760692 139655626356480 logging_writer.py:48] [111600] global_step=111600, grad_norm=1.9379990100860596, loss=1.9213764667510986
I0129 12:20:19.700062 139656297445120 logging_writer.py:48] [111700] global_step=111700, grad_norm=1.8896056413650513, loss=1.904934048652649
I0129 12:20:53.595968 139655626356480 logging_writer.py:48] [111800] global_step=111800, grad_norm=1.8676221370697021, loss=1.8930683135986328
I0129 12:21:27.508333 139656297445120 logging_writer.py:48] [111900] global_step=111900, grad_norm=1.9519376754760742, loss=1.7484021186828613
I0129 12:22:01.431221 139655626356480 logging_writer.py:48] [112000] global_step=112000, grad_norm=1.846824288368225, loss=1.8278369903564453
I0129 12:22:35.338279 139656297445120 logging_writer.py:48] [112100] global_step=112100, grad_norm=1.9056533575057983, loss=1.8241534233093262
I0129 12:23:09.313868 139655626356480 logging_writer.py:48] [112200] global_step=112200, grad_norm=1.7823846340179443, loss=1.768156886100769
I0129 12:23:43.219295 139656297445120 logging_writer.py:48] [112300] global_step=112300, grad_norm=2.1106514930725098, loss=1.8284921646118164
I0129 12:24:17.114022 139655626356480 logging_writer.py:48] [112400] global_step=112400, grad_norm=1.9190486669540405, loss=1.8937468528747559
I0129 12:24:51.011654 139656297445120 logging_writer.py:48] [112500] global_step=112500, grad_norm=1.8937815427780151, loss=1.8777234554290771
I0129 12:25:24.929066 139655626356480 logging_writer.py:48] [112600] global_step=112600, grad_norm=1.8703974485397339, loss=1.896939754486084
I0129 12:25:58.840432 139656297445120 logging_writer.py:48] [112700] global_step=112700, grad_norm=1.9914060831069946, loss=1.8384919166564941
I0129 12:26:01.016929 139822745589568 spec.py:321] Evaluating on the training split.
I0129 12:26:07.318684 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 12:26:15.972011 139822745589568 spec.py:349] Evaluating on the test split.
I0129 12:26:18.572242 139822745589568 submission_runner.py:408] Time since start: 39621.93s, 	Step: 112708, 	{'train/accuracy': 0.48280054330825806, 'train/loss': 2.318225383758545, 'validation/accuracy': 0.4461199939250946, 'validation/loss': 2.5451242923736572, 'validation/num_examples': 50000, 'test/accuracy': 0.3433000147342682, 'test/loss': 3.334402084350586, 'test/num_examples': 10000, 'score': 38287.42334794998, 'total_duration': 39621.92883038521, 'accumulated_submission_time': 38287.42334794998, 'accumulated_eval_time': 1327.739814043045, 'accumulated_logging_time': 2.886847972869873}
I0129 12:26:18.607896 139655626356480 logging_writer.py:48] [112708] accumulated_eval_time=1327.739814, accumulated_logging_time=2.886848, accumulated_submission_time=38287.423348, global_step=112708, preemption_count=0, score=38287.423348, test/accuracy=0.343300, test/loss=3.334402, test/num_examples=10000, total_duration=39621.928830, train/accuracy=0.482801, train/loss=2.318225, validation/accuracy=0.446120, validation/loss=2.545124, validation/num_examples=50000
I0129 12:26:50.101377 139656297445120 logging_writer.py:48] [112800] global_step=112800, grad_norm=1.9167417287826538, loss=1.8312126398086548
I0129 12:27:23.972304 139655626356480 logging_writer.py:48] [112900] global_step=112900, grad_norm=1.992279291152954, loss=1.8006548881530762
I0129 12:27:57.835045 139656297445120 logging_writer.py:48] [113000] global_step=113000, grad_norm=1.996476650238037, loss=1.8254742622375488
I0129 12:28:31.746639 139655626356480 logging_writer.py:48] [113100] global_step=113100, grad_norm=2.081089735031128, loss=1.86394464969635
I0129 12:29:05.719037 139656297445120 logging_writer.py:48] [113200] global_step=113200, grad_norm=2.059863328933716, loss=1.7325462102890015
I0129 12:29:39.636264 139655626356480 logging_writer.py:48] [113300] global_step=113300, grad_norm=1.9554836750030518, loss=1.7545086145401
I0129 12:30:13.512345 139656297445120 logging_writer.py:48] [113400] global_step=113400, grad_norm=1.9890639781951904, loss=1.8690682649612427
I0129 12:30:47.385570 139655626356480 logging_writer.py:48] [113500] global_step=113500, grad_norm=2.0017025470733643, loss=1.9318468570709229
I0129 12:31:21.258945 139656297445120 logging_writer.py:48] [113600] global_step=113600, grad_norm=2.1312692165374756, loss=1.835409164428711
I0129 12:31:55.147796 139655626356480 logging_writer.py:48] [113700] global_step=113700, grad_norm=2.044745445251465, loss=1.7348358631134033
I0129 12:32:29.009179 139656297445120 logging_writer.py:48] [113800] global_step=113800, grad_norm=2.0143864154815674, loss=1.8646219968795776
I0129 12:33:02.905659 139655626356480 logging_writer.py:48] [113900] global_step=113900, grad_norm=2.1497509479522705, loss=1.8632861375808716
I0129 12:33:36.831084 139656297445120 logging_writer.py:48] [114000] global_step=114000, grad_norm=2.1199634075164795, loss=1.9661451578140259
I0129 12:34:10.714455 139655626356480 logging_writer.py:48] [114100] global_step=114100, grad_norm=2.118809700012207, loss=1.8424749374389648
I0129 12:34:44.620715 139656297445120 logging_writer.py:48] [114200] global_step=114200, grad_norm=2.068232297897339, loss=1.7740840911865234
I0129 12:34:48.837724 139822745589568 spec.py:321] Evaluating on the training split.
I0129 12:34:55.081011 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 12:35:03.951174 139822745589568 spec.py:349] Evaluating on the test split.
I0129 12:35:06.473046 139822745589568 submission_runner.py:408] Time since start: 40149.83s, 	Step: 114214, 	{'train/accuracy': 0.5449019074440002, 'train/loss': 1.9557873010635376, 'validation/accuracy': 0.5061599612236023, 'validation/loss': 2.1710429191589355, 'validation/num_examples': 50000, 'test/accuracy': 0.38360002636909485, 'test/loss': 3.029585599899292, 'test/num_examples': 10000, 'score': 38797.589587688446, 'total_duration': 40149.82967543602, 'accumulated_submission_time': 38797.589587688446, 'accumulated_eval_time': 1345.3750817775726, 'accumulated_logging_time': 2.9340431690216064}
I0129 12:35:06.509648 139656649758464 logging_writer.py:48] [114214] accumulated_eval_time=1345.375082, accumulated_logging_time=2.934043, accumulated_submission_time=38797.589588, global_step=114214, preemption_count=0, score=38797.589588, test/accuracy=0.383600, test/loss=3.029586, test/num_examples=10000, total_duration=40149.829675, train/accuracy=0.544902, train/loss=1.955787, validation/accuracy=0.506160, validation/loss=2.171043, validation/num_examples=50000
I0129 12:35:35.995420 139656658151168 logging_writer.py:48] [114300] global_step=114300, grad_norm=1.876481294631958, loss=1.8158130645751953
I0129 12:36:09.950723 139656649758464 logging_writer.py:48] [114400] global_step=114400, grad_norm=1.9982510805130005, loss=1.815675973892212
I0129 12:36:43.857565 139656658151168 logging_writer.py:48] [114500] global_step=114500, grad_norm=2.0552594661712646, loss=1.930098533630371
I0129 12:37:17.782580 139656649758464 logging_writer.py:48] [114600] global_step=114600, grad_norm=2.010309934616089, loss=1.9868829250335693
I0129 12:37:51.668277 139656658151168 logging_writer.py:48] [114700] global_step=114700, grad_norm=1.8852207660675049, loss=1.933506965637207
I0129 12:38:25.605006 139656649758464 logging_writer.py:48] [114800] global_step=114800, grad_norm=1.93752121925354, loss=1.8870116472244263
I0129 12:38:59.505625 139656658151168 logging_writer.py:48] [114900] global_step=114900, grad_norm=2.003146171569824, loss=1.8918670415878296
I0129 12:39:33.424782 139656649758464 logging_writer.py:48] [115000] global_step=115000, grad_norm=1.9858375787734985, loss=1.9261553287506104
I0129 12:40:07.313605 139656658151168 logging_writer.py:48] [115100] global_step=115100, grad_norm=1.9359327554702759, loss=1.72994065284729
I0129 12:40:41.237129 139656649758464 logging_writer.py:48] [115200] global_step=115200, grad_norm=2.000760793685913, loss=1.783941388130188
I0129 12:41:15.130697 139656658151168 logging_writer.py:48] [115300] global_step=115300, grad_norm=1.9934802055358887, loss=1.815051794052124
I0129 12:41:49.060708 139656649758464 logging_writer.py:48] [115400] global_step=115400, grad_norm=2.0309252738952637, loss=1.8079792261123657
I0129 12:42:23.086406 139656658151168 logging_writer.py:48] [115500] global_step=115500, grad_norm=2.0491647720336914, loss=1.7889211177825928
I0129 12:42:57.014306 139656649758464 logging_writer.py:48] [115600] global_step=115600, grad_norm=2.1128525733947754, loss=1.9111833572387695
I0129 12:43:30.935885 139656658151168 logging_writer.py:48] [115700] global_step=115700, grad_norm=2.3356423377990723, loss=1.8126362562179565
I0129 12:43:36.521767 139822745589568 spec.py:321] Evaluating on the training split.
I0129 12:43:42.675682 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 12:43:51.126268 139822745589568 spec.py:349] Evaluating on the test split.
I0129 12:43:53.685021 139822745589568 submission_runner.py:408] Time since start: 40677.04s, 	Step: 115718, 	{'train/accuracy': 0.5570591688156128, 'train/loss': 1.8826926946640015, 'validation/accuracy': 0.527999997138977, 'validation/loss': 2.093355178833008, 'validation/num_examples': 50000, 'test/accuracy': 0.4082000255584717, 'test/loss': 2.8958003520965576, 'test/num_examples': 10000, 'score': 39307.54098248482, 'total_duration': 40677.041610240936, 'accumulated_submission_time': 39307.54098248482, 'accumulated_eval_time': 1362.5382542610168, 'accumulated_logging_time': 2.980867624282837}
I0129 12:43:53.722251 139655626356480 logging_writer.py:48] [115718] accumulated_eval_time=1362.538254, accumulated_logging_time=2.980868, accumulated_submission_time=39307.540982, global_step=115718, preemption_count=0, score=39307.540982, test/accuracy=0.408200, test/loss=2.895800, test/num_examples=10000, total_duration=40677.041610, train/accuracy=0.557059, train/loss=1.882693, validation/accuracy=0.528000, validation/loss=2.093355, validation/num_examples=50000
I0129 12:44:21.825571 139656297445120 logging_writer.py:48] [115800] global_step=115800, grad_norm=2.111093759536743, loss=1.8629168272018433
I0129 12:44:55.684118 139655626356480 logging_writer.py:48] [115900] global_step=115900, grad_norm=2.0966129302978516, loss=1.7695366144180298
I0129 12:45:29.566904 139656297445120 logging_writer.py:48] [116000] global_step=116000, grad_norm=2.259509801864624, loss=1.7888612747192383
I0129 12:46:03.477525 139655626356480 logging_writer.py:48] [116100] global_step=116100, grad_norm=1.9372912645339966, loss=1.8371778726577759
I0129 12:46:37.400539 139656297445120 logging_writer.py:48] [116200] global_step=116200, grad_norm=2.1905272006988525, loss=1.7776778936386108
I0129 12:47:11.313105 139655626356480 logging_writer.py:48] [116300] global_step=116300, grad_norm=2.0636982917785645, loss=1.8229219913482666
I0129 12:47:45.244966 139656297445120 logging_writer.py:48] [116400] global_step=116400, grad_norm=2.1280293464660645, loss=1.7922024726867676
I0129 12:48:19.166999 139655626356480 logging_writer.py:48] [116500] global_step=116500, grad_norm=2.056492567062378, loss=1.7261128425598145
I0129 12:48:53.136555 139656297445120 logging_writer.py:48] [116600] global_step=116600, grad_norm=2.328397512435913, loss=1.8842644691467285
I0129 12:49:27.045305 139655626356480 logging_writer.py:48] [116700] global_step=116700, grad_norm=2.0047428607940674, loss=1.8136178255081177
I0129 12:50:00.951560 139656297445120 logging_writer.py:48] [116800] global_step=116800, grad_norm=2.260059356689453, loss=1.8568782806396484
I0129 12:50:34.821792 139655626356480 logging_writer.py:48] [116900] global_step=116900, grad_norm=2.1410491466522217, loss=1.872287631034851
I0129 12:51:08.754174 139656297445120 logging_writer.py:48] [117000] global_step=117000, grad_norm=2.0603866577148438, loss=1.7658677101135254
I0129 12:51:42.647031 139655626356480 logging_writer.py:48] [117100] global_step=117100, grad_norm=2.0276317596435547, loss=1.7395167350769043
I0129 12:52:16.571339 139656297445120 logging_writer.py:48] [117200] global_step=117200, grad_norm=2.3746917247772217, loss=1.7955714464187622
I0129 12:52:23.828457 139822745589568 spec.py:321] Evaluating on the training split.
I0129 12:52:29.986397 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 12:52:38.455195 139822745589568 spec.py:349] Evaluating on the test split.
I0129 12:52:41.026086 139822745589568 submission_runner.py:408] Time since start: 41204.38s, 	Step: 117223, 	{'train/accuracy': 0.6294443607330322, 'train/loss': 1.476343035697937, 'validation/accuracy': 0.5562199950218201, 'validation/loss': 1.8933178186416626, 'validation/num_examples': 50000, 'test/accuracy': 0.4335000216960907, 'test/loss': 2.682591438293457, 'test/num_examples': 10000, 'score': 39817.586441755295, 'total_duration': 41204.382727622986, 'accumulated_submission_time': 39817.586441755295, 'accumulated_eval_time': 1379.7358441352844, 'accumulated_logging_time': 3.027287483215332}
I0129 12:52:41.064929 139655626356480 logging_writer.py:48] [117223] accumulated_eval_time=1379.735844, accumulated_logging_time=3.027287, accumulated_submission_time=39817.586442, global_step=117223, preemption_count=0, score=39817.586442, test/accuracy=0.433500, test/loss=2.682591, test/num_examples=10000, total_duration=41204.382728, train/accuracy=0.629444, train/loss=1.476343, validation/accuracy=0.556220, validation/loss=1.893318, validation/num_examples=50000
I0129 12:53:07.493852 139656649758464 logging_writer.py:48] [117300] global_step=117300, grad_norm=2.1868879795074463, loss=1.6758651733398438
I0129 12:53:41.351106 139655626356480 logging_writer.py:48] [117400] global_step=117400, grad_norm=2.0091800689697266, loss=1.8048436641693115
I0129 12:54:15.259002 139656649758464 logging_writer.py:48] [117500] global_step=117500, grad_norm=2.156970262527466, loss=1.8168634176254272
I0129 12:54:49.143206 139655626356480 logging_writer.py:48] [117600] global_step=117600, grad_norm=2.129239320755005, loss=1.8650405406951904
I0129 12:55:23.144043 139656649758464 logging_writer.py:48] [117700] global_step=117700, grad_norm=2.162250280380249, loss=1.856182336807251
I0129 12:55:57.049456 139655626356480 logging_writer.py:48] [117800] global_step=117800, grad_norm=2.161369562149048, loss=1.8054397106170654
I0129 12:56:30.961409 139656649758464 logging_writer.py:48] [117900] global_step=117900, grad_norm=2.1239936351776123, loss=1.795642614364624
I0129 12:57:04.905530 139655626356480 logging_writer.py:48] [118000] global_step=118000, grad_norm=2.0720901489257812, loss=1.7209680080413818
I0129 12:57:38.834856 139656649758464 logging_writer.py:48] [118100] global_step=118100, grad_norm=2.3004376888275146, loss=1.74627685546875
I0129 12:58:12.713219 139655626356480 logging_writer.py:48] [118200] global_step=118200, grad_norm=2.0996289253234863, loss=1.8109822273254395
I0129 12:58:46.654077 139656649758464 logging_writer.py:48] [118300] global_step=118300, grad_norm=2.245630979537964, loss=1.911081314086914
I0129 12:59:20.569777 139655626356480 logging_writer.py:48] [118400] global_step=118400, grad_norm=2.0030713081359863, loss=1.7976455688476562
I0129 12:59:54.470589 139656649758464 logging_writer.py:48] [118500] global_step=118500, grad_norm=1.9743907451629639, loss=1.7173113822937012
I0129 13:00:28.386042 139655626356480 logging_writer.py:48] [118600] global_step=118600, grad_norm=2.1731858253479004, loss=1.891788125038147
I0129 13:01:02.278511 139656649758464 logging_writer.py:48] [118700] global_step=118700, grad_norm=2.230733633041382, loss=1.7979793548583984
I0129 13:01:11.254042 139822745589568 spec.py:321] Evaluating on the training split.
I0129 13:01:17.392442 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 13:01:25.780967 139822745589568 spec.py:349] Evaluating on the test split.
I0129 13:01:28.418550 139822745589568 submission_runner.py:408] Time since start: 41731.78s, 	Step: 118728, 	{'train/accuracy': 0.45798787474632263, 'train/loss': 2.500563621520996, 'validation/accuracy': 0.42545998096466064, 'validation/loss': 2.721994638442993, 'validation/num_examples': 50000, 'test/accuracy': 0.3305000066757202, 'test/loss': 3.4887149333953857, 'test/num_examples': 10000, 'score': 40327.71361851692, 'total_duration': 41731.77513575554, 'accumulated_submission_time': 40327.71361851692, 'accumulated_eval_time': 1396.9002561569214, 'accumulated_logging_time': 3.074965238571167}
I0129 13:01:28.458591 139656297445120 logging_writer.py:48] [118728] accumulated_eval_time=1396.900256, accumulated_logging_time=3.074965, accumulated_submission_time=40327.713619, global_step=118728, preemption_count=0, score=40327.713619, test/accuracy=0.330500, test/loss=3.488715, test/num_examples=10000, total_duration=41731.775136, train/accuracy=0.457988, train/loss=2.500564, validation/accuracy=0.425460, validation/loss=2.721995, validation/num_examples=50000
I0129 13:01:53.299052 139656666543872 logging_writer.py:48] [118800] global_step=118800, grad_norm=2.267575740814209, loss=1.8960323333740234
I0129 13:02:27.119465 139656297445120 logging_writer.py:48] [118900] global_step=118900, grad_norm=2.134521007537842, loss=1.867253065109253
I0129 13:03:01.024040 139656666543872 logging_writer.py:48] [119000] global_step=119000, grad_norm=2.186579942703247, loss=1.7559431791305542
I0129 13:03:34.943070 139656297445120 logging_writer.py:48] [119100] global_step=119100, grad_norm=2.2822155952453613, loss=1.835495114326477
I0129 13:04:08.845397 139656666543872 logging_writer.py:48] [119200] global_step=119200, grad_norm=2.2657995223999023, loss=1.7806172370910645
I0129 13:04:42.752031 139656297445120 logging_writer.py:48] [119300] global_step=119300, grad_norm=2.1381845474243164, loss=1.8940503597259521
I0129 13:05:16.686934 139656666543872 logging_writer.py:48] [119400] global_step=119400, grad_norm=2.14778470993042, loss=1.8865991830825806
I0129 13:05:50.572081 139656297445120 logging_writer.py:48] [119500] global_step=119500, grad_norm=2.0976855754852295, loss=1.8253995180130005
I0129 13:06:24.456531 139656666543872 logging_writer.py:48] [119600] global_step=119600, grad_norm=2.200083017349243, loss=1.6940356492996216
I0129 13:06:58.367693 139656297445120 logging_writer.py:48] [119700] global_step=119700, grad_norm=2.0274884700775146, loss=1.7114818096160889
I0129 13:07:32.266104 139656666543872 logging_writer.py:48] [119800] global_step=119800, grad_norm=2.2783844470977783, loss=1.9056367874145508
I0129 13:08:06.174023 139656297445120 logging_writer.py:48] [119900] global_step=119900, grad_norm=2.226397752761841, loss=1.8368347883224487
I0129 13:08:40.163484 139656666543872 logging_writer.py:48] [120000] global_step=120000, grad_norm=2.1144256591796875, loss=1.759926438331604
I0129 13:09:14.044536 139656297445120 logging_writer.py:48] [120100] global_step=120100, grad_norm=1.9406781196594238, loss=1.7287933826446533
I0129 13:09:47.945119 139656666543872 logging_writer.py:48] [120200] global_step=120200, grad_norm=2.156017303466797, loss=1.7616252899169922
I0129 13:09:58.601703 139822745589568 spec.py:321] Evaluating on the training split.
I0129 13:10:04.903064 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 13:10:13.336413 139822745589568 spec.py:349] Evaluating on the test split.
I0129 13:10:15.915441 139822745589568 submission_runner.py:408] Time since start: 42259.27s, 	Step: 120233, 	{'train/accuracy': 0.6004663705825806, 'train/loss': 1.6669267416000366, 'validation/accuracy': 0.5487599968910217, 'validation/loss': 1.962204933166504, 'validation/num_examples': 50000, 'test/accuracy': 0.4288000166416168, 'test/loss': 2.7302029132843018, 'test/num_examples': 10000, 'score': 40837.7944047451, 'total_duration': 42259.27208185196, 'accumulated_submission_time': 40837.7944047451, 'accumulated_eval_time': 1414.213954925537, 'accumulated_logging_time': 3.124530076980591}
I0129 13:10:15.957350 139655617963776 logging_writer.py:48] [120233] accumulated_eval_time=1414.213955, accumulated_logging_time=3.124530, accumulated_submission_time=40837.794405, global_step=120233, preemption_count=0, score=40837.794405, test/accuracy=0.428800, test/loss=2.730203, test/num_examples=10000, total_duration=42259.272082, train/accuracy=0.600466, train/loss=1.666927, validation/accuracy=0.548760, validation/loss=1.962205, validation/num_examples=50000
I0129 13:10:38.992482 139655626356480 logging_writer.py:48] [120300] global_step=120300, grad_norm=2.101325273513794, loss=1.7719542980194092
I0129 13:11:12.888290 139655617963776 logging_writer.py:48] [120400] global_step=120400, grad_norm=2.1757984161376953, loss=1.9056276082992554
I0129 13:11:46.772197 139655626356480 logging_writer.py:48] [120500] global_step=120500, grad_norm=2.16343355178833, loss=1.9192726612091064
I0129 13:12:20.662682 139655617963776 logging_writer.py:48] [120600] global_step=120600, grad_norm=2.280078887939453, loss=1.9009968042373657
I0129 13:12:54.591856 139655626356480 logging_writer.py:48] [120700] global_step=120700, grad_norm=2.2489280700683594, loss=1.7781181335449219
I0129 13:13:28.500495 139655617963776 logging_writer.py:48] [120800] global_step=120800, grad_norm=2.2527694702148438, loss=1.7637606859207153
I0129 13:14:02.393381 139655626356480 logging_writer.py:48] [120900] global_step=120900, grad_norm=2.3297598361968994, loss=1.7710334062576294
I0129 13:14:36.353366 139655617963776 logging_writer.py:48] [121000] global_step=121000, grad_norm=2.2189042568206787, loss=1.7064449787139893
I0129 13:15:10.248964 139655626356480 logging_writer.py:48] [121100] global_step=121100, grad_norm=2.2955827713012695, loss=1.7069276571273804
I0129 13:15:44.169581 139655617963776 logging_writer.py:48] [121200] global_step=121200, grad_norm=2.1772658824920654, loss=1.7250193357467651
I0129 13:16:18.077073 139655626356480 logging_writer.py:48] [121300] global_step=121300, grad_norm=2.036102533340454, loss=1.6912280321121216
I0129 13:16:51.999882 139655617963776 logging_writer.py:48] [121400] global_step=121400, grad_norm=2.148155689239502, loss=1.8858840465545654
I0129 13:17:25.909312 139655626356480 logging_writer.py:48] [121500] global_step=121500, grad_norm=2.0800468921661377, loss=1.7363719940185547
I0129 13:17:59.817944 139655617963776 logging_writer.py:48] [121600] global_step=121600, grad_norm=2.0930988788604736, loss=1.7454253435134888
I0129 13:18:33.733485 139655626356480 logging_writer.py:48] [121700] global_step=121700, grad_norm=2.0810585021972656, loss=1.839707374572754
I0129 13:18:46.089446 139822745589568 spec.py:321] Evaluating on the training split.
I0129 13:18:52.437627 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 13:19:00.831025 139822745589568 spec.py:349] Evaluating on the test split.
I0129 13:19:03.493004 139822745589568 submission_runner.py:408] Time since start: 42786.85s, 	Step: 121738, 	{'train/accuracy': 0.5869738459587097, 'train/loss': 1.7295794486999512, 'validation/accuracy': 0.5414199829101562, 'validation/loss': 1.9991382360458374, 'validation/num_examples': 50000, 'test/accuracy': 0.42270001769065857, 'test/loss': 2.8023416996002197, 'test/num_examples': 10000, 'score': 41347.86622738838, 'total_duration': 42786.84964418411, 'accumulated_submission_time': 41347.86622738838, 'accumulated_eval_time': 1431.6174721717834, 'accumulated_logging_time': 3.175598382949829}
I0129 13:19:03.535354 139655617963776 logging_writer.py:48] [121738] accumulated_eval_time=1431.617472, accumulated_logging_time=3.175598, accumulated_submission_time=41347.866227, global_step=121738, preemption_count=0, score=41347.866227, test/accuracy=0.422700, test/loss=2.802342, test/num_examples=10000, total_duration=42786.849644, train/accuracy=0.586974, train/loss=1.729579, validation/accuracy=0.541420, validation/loss=1.999138, validation/num_examples=50000
I0129 13:19:24.893382 139656666543872 logging_writer.py:48] [121800] global_step=121800, grad_norm=2.2379939556121826, loss=1.9037564992904663
I0129 13:19:58.762337 139655617963776 logging_writer.py:48] [121900] global_step=121900, grad_norm=2.2083516120910645, loss=1.8775814771652222
I0129 13:20:32.635398 139656666543872 logging_writer.py:48] [122000] global_step=122000, grad_norm=2.1787357330322266, loss=1.7418087720870972
I0129 13:21:06.476617 139655617963776 logging_writer.py:48] [122100] global_step=122100, grad_norm=2.0615813732147217, loss=1.6834348440170288
I0129 13:21:40.448527 139656666543872 logging_writer.py:48] [122200] global_step=122200, grad_norm=2.1607425212860107, loss=1.7013020515441895
I0129 13:22:14.324231 139655617963776 logging_writer.py:48] [122300] global_step=122300, grad_norm=2.0892140865325928, loss=1.8014836311340332
I0129 13:22:48.252500 139656666543872 logging_writer.py:48] [122400] global_step=122400, grad_norm=2.2582359313964844, loss=1.7285399436950684
I0129 13:23:22.132311 139655617963776 logging_writer.py:48] [122500] global_step=122500, grad_norm=2.281313896179199, loss=1.7915980815887451
I0129 13:23:56.061303 139656666543872 logging_writer.py:48] [122600] global_step=122600, grad_norm=2.267303943634033, loss=1.785103440284729
I0129 13:24:29.958155 139655617963776 logging_writer.py:48] [122700] global_step=122700, grad_norm=2.330944776535034, loss=1.8235008716583252
I0129 13:25:03.849091 139656666543872 logging_writer.py:48] [122800] global_step=122800, grad_norm=2.5166304111480713, loss=1.7197496891021729
I0129 13:25:37.732056 139655617963776 logging_writer.py:48] [122900] global_step=122900, grad_norm=2.331104278564453, loss=1.7982378005981445
I0129 13:26:11.643423 139656666543872 logging_writer.py:48] [123000] global_step=123000, grad_norm=2.1780173778533936, loss=1.836649775505066
I0129 13:26:45.545037 139655617963776 logging_writer.py:48] [123100] global_step=123100, grad_norm=2.2904269695281982, loss=1.8222894668579102
I0129 13:27:19.429608 139656666543872 logging_writer.py:48] [123200] global_step=123200, grad_norm=2.354672908782959, loss=1.7739506959915161
I0129 13:27:33.818624 139822745589568 spec.py:321] Evaluating on the training split.
I0129 13:27:40.263839 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 13:27:48.751131 139822745589568 spec.py:349] Evaluating on the test split.
I0129 13:27:51.356922 139822745589568 submission_runner.py:408] Time since start: 43314.71s, 	Step: 123244, 	{'train/accuracy': 0.5721460580825806, 'train/loss': 1.803208351135254, 'validation/accuracy': 0.5319399833679199, 'validation/loss': 2.029165267944336, 'validation/num_examples': 50000, 'test/accuracy': 0.3977000117301941, 'test/loss': 2.858058214187622, 'test/num_examples': 10000, 'score': 41858.08864212036, 'total_duration': 43314.71329832077, 'accumulated_submission_time': 41858.08864212036, 'accumulated_eval_time': 1449.155464887619, 'accumulated_logging_time': 3.2273247241973877}
I0129 13:27:51.397816 139655626356480 logging_writer.py:48] [123244] accumulated_eval_time=1449.155465, accumulated_logging_time=3.227325, accumulated_submission_time=41858.088642, global_step=123244, preemption_count=0, score=41858.088642, test/accuracy=0.397700, test/loss=2.858058, test/num_examples=10000, total_duration=43314.713298, train/accuracy=0.572146, train/loss=1.803208, validation/accuracy=0.531940, validation/loss=2.029165, validation/num_examples=50000
I0129 13:28:10.713545 139656297445120 logging_writer.py:48] [123300] global_step=123300, grad_norm=2.1755611896514893, loss=1.6140886545181274
I0129 13:28:44.585527 139655626356480 logging_writer.py:48] [123400] global_step=123400, grad_norm=2.181605100631714, loss=1.7318828105926514
I0129 13:29:18.454414 139656297445120 logging_writer.py:48] [123500] global_step=123500, grad_norm=2.22309947013855, loss=1.7305874824523926
I0129 13:29:52.341320 139655626356480 logging_writer.py:48] [123600] global_step=123600, grad_norm=2.2080931663513184, loss=1.8135976791381836
I0129 13:30:26.234782 139656297445120 logging_writer.py:48] [123700] global_step=123700, grad_norm=2.408839464187622, loss=1.8260234594345093
I0129 13:31:00.148643 139655626356480 logging_writer.py:48] [123800] global_step=123800, grad_norm=2.4123876094818115, loss=1.8093304634094238
I0129 13:31:34.062535 139656297445120 logging_writer.py:48] [123900] global_step=123900, grad_norm=2.341709613800049, loss=1.808416485786438
I0129 13:32:07.981958 139655626356480 logging_writer.py:48] [124000] global_step=124000, grad_norm=2.2867212295532227, loss=1.731440544128418
I0129 13:32:41.903498 139656297445120 logging_writer.py:48] [124100] global_step=124100, grad_norm=2.132416009902954, loss=1.7478282451629639
I0129 13:33:15.796207 139655626356480 logging_writer.py:48] [124200] global_step=124200, grad_norm=2.109957218170166, loss=1.690396785736084
I0129 13:33:49.718767 139656297445120 logging_writer.py:48] [124300] global_step=124300, grad_norm=2.2197623252868652, loss=1.7387787103652954
I0129 13:34:23.703996 139655626356480 logging_writer.py:48] [124400] global_step=124400, grad_norm=2.5402257442474365, loss=1.7328600883483887
I0129 13:34:57.586534 139656297445120 logging_writer.py:48] [124500] global_step=124500, grad_norm=2.3728394508361816, loss=1.702423334121704
I0129 13:35:31.464689 139655626356480 logging_writer.py:48] [124600] global_step=124600, grad_norm=2.3785452842712402, loss=1.7015221118927002
I0129 13:36:05.326938 139656297445120 logging_writer.py:48] [124700] global_step=124700, grad_norm=2.2757935523986816, loss=1.722916841506958
I0129 13:36:21.412090 139822745589568 spec.py:321] Evaluating on the training split.
I0129 13:36:27.568316 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 13:36:36.024695 139822745589568 spec.py:349] Evaluating on the test split.
I0129 13:36:38.704236 139822745589568 submission_runner.py:408] Time since start: 43842.06s, 	Step: 124749, 	{'train/accuracy': 0.6416613459587097, 'train/loss': 1.4549739360809326, 'validation/accuracy': 0.5970799922943115, 'validation/loss': 1.6888469457626343, 'validation/num_examples': 50000, 'test/accuracy': 0.46960002183914185, 'test/loss': 2.4458189010620117, 'test/num_examples': 10000, 'score': 42368.04260277748, 'total_duration': 43842.06088280678, 'accumulated_submission_time': 42368.04260277748, 'accumulated_eval_time': 1466.4475784301758, 'accumulated_logging_time': 3.278285026550293}
I0129 13:36:38.743175 139655626356480 logging_writer.py:48] [124749] accumulated_eval_time=1466.447578, accumulated_logging_time=3.278285, accumulated_submission_time=42368.042603, global_step=124749, preemption_count=0, score=42368.042603, test/accuracy=0.469600, test/loss=2.445819, test/num_examples=10000, total_duration=43842.060883, train/accuracy=0.641661, train/loss=1.454974, validation/accuracy=0.597080, validation/loss=1.688847, validation/num_examples=50000
I0129 13:36:56.333483 139656666543872 logging_writer.py:48] [124800] global_step=124800, grad_norm=2.2698397636413574, loss=1.6564580202102661
I0129 13:37:30.197618 139655626356480 logging_writer.py:48] [124900] global_step=124900, grad_norm=2.2556025981903076, loss=1.7012207508087158
I0129 13:38:04.094128 139656666543872 logging_writer.py:48] [125000] global_step=125000, grad_norm=2.442155599594116, loss=1.6677422523498535
I0129 13:38:37.953394 139655626356480 logging_writer.py:48] [125100] global_step=125100, grad_norm=2.119858741760254, loss=1.681444525718689
I0129 13:39:11.863515 139656666543872 logging_writer.py:48] [125200] global_step=125200, grad_norm=2.1820452213287354, loss=1.7921046018600464
I0129 13:39:45.776224 139655626356480 logging_writer.py:48] [125300] global_step=125300, grad_norm=2.449350595474243, loss=1.6895253658294678
I0129 13:40:19.667533 139656666543872 logging_writer.py:48] [125400] global_step=125400, grad_norm=2.4772098064422607, loss=1.805616855621338
I0129 13:40:53.647935 139655626356480 logging_writer.py:48] [125500] global_step=125500, grad_norm=2.4025754928588867, loss=1.8481076955795288
I0129 13:41:27.577258 139656666543872 logging_writer.py:48] [125600] global_step=125600, grad_norm=2.355579376220703, loss=1.7906593084335327
I0129 13:42:01.455702 139655626356480 logging_writer.py:48] [125700] global_step=125700, grad_norm=2.206928253173828, loss=1.6123610734939575
I0129 13:42:35.374672 139656666543872 logging_writer.py:48] [125800] global_step=125800, grad_norm=2.4799675941467285, loss=1.8520644903182983
I0129 13:43:09.251795 139655626356480 logging_writer.py:48] [125900] global_step=125900, grad_norm=2.394777297973633, loss=1.7586746215820312
I0129 13:43:43.172249 139656666543872 logging_writer.py:48] [126000] global_step=126000, grad_norm=2.52986216545105, loss=1.7695635557174683
I0129 13:44:17.064595 139655626356480 logging_writer.py:48] [126100] global_step=126100, grad_norm=2.3789501190185547, loss=1.7633206844329834
I0129 13:44:50.963764 139656666543872 logging_writer.py:48] [126200] global_step=126200, grad_norm=2.304981231689453, loss=1.7499250173568726
I0129 13:45:08.746997 139822745589568 spec.py:321] Evaluating on the training split.
I0129 13:45:14.973155 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 13:45:23.460267 139822745589568 spec.py:349] Evaluating on the test split.
I0129 13:45:26.034199 139822745589568 submission_runner.py:408] Time since start: 44369.39s, 	Step: 126254, 	{'train/accuracy': 0.6292649507522583, 'train/loss': 1.4908218383789062, 'validation/accuracy': 0.5594800114631653, 'validation/loss': 1.9337760210037231, 'validation/num_examples': 50000, 'test/accuracy': 0.4337000250816345, 'test/loss': 2.7682411670684814, 'test/num_examples': 10000, 'score': 42877.98620200157, 'total_duration': 44369.39072751999, 'accumulated_submission_time': 42877.98620200157, 'accumulated_eval_time': 1483.7346332073212, 'accumulated_logging_time': 3.327343463897705}
I0129 13:45:26.073485 139656649758464 logging_writer.py:48] [126254] accumulated_eval_time=1483.734633, accumulated_logging_time=3.327343, accumulated_submission_time=42877.986202, global_step=126254, preemption_count=0, score=42877.986202, test/accuracy=0.433700, test/loss=2.768241, test/num_examples=10000, total_duration=44369.390728, train/accuracy=0.629265, train/loss=1.490822, validation/accuracy=0.559480, validation/loss=1.933776, validation/num_examples=50000
I0129 13:45:42.007731 139656658151168 logging_writer.py:48] [126300] global_step=126300, grad_norm=2.3891825675964355, loss=1.8532638549804688
I0129 13:46:15.891433 139656649758464 logging_writer.py:48] [126400] global_step=126400, grad_norm=2.492591381072998, loss=1.7445740699768066
I0129 13:46:49.765363 139656658151168 logging_writer.py:48] [126500] global_step=126500, grad_norm=2.2212002277374268, loss=1.541182518005371
I0129 13:47:23.722354 139656649758464 logging_writer.py:48] [126600] global_step=126600, grad_norm=2.2833282947540283, loss=1.7710177898406982
I0129 13:47:57.627030 139656658151168 logging_writer.py:48] [126700] global_step=126700, grad_norm=2.5202791690826416, loss=1.7452794313430786
I0129 13:48:31.477551 139656649758464 logging_writer.py:48] [126800] global_step=126800, grad_norm=2.43986177444458, loss=1.7488676309585571
I0129 13:49:05.333555 139656658151168 logging_writer.py:48] [126900] global_step=126900, grad_norm=2.4451305866241455, loss=1.8668286800384521
I0129 13:49:39.234446 139656649758464 logging_writer.py:48] [127000] global_step=127000, grad_norm=2.507122278213501, loss=1.7143754959106445
I0129 13:50:13.127925 139656658151168 logging_writer.py:48] [127100] global_step=127100, grad_norm=2.396150827407837, loss=1.8009767532348633
I0129 13:50:47.068290 139656649758464 logging_writer.py:48] [127200] global_step=127200, grad_norm=2.3903088569641113, loss=1.6933866739273071
I0129 13:51:20.949535 139656658151168 logging_writer.py:48] [127300] global_step=127300, grad_norm=2.2222652435302734, loss=1.6387792825698853
I0129 13:51:54.874553 139656649758464 logging_writer.py:48] [127400] global_step=127400, grad_norm=2.409440279006958, loss=1.6848547458648682
I0129 13:52:28.790408 139656658151168 logging_writer.py:48] [127500] global_step=127500, grad_norm=2.304436445236206, loss=1.5676212310791016
I0129 13:53:02.685124 139656649758464 logging_writer.py:48] [127600] global_step=127600, grad_norm=2.2816946506500244, loss=1.7172493934631348
I0129 13:53:36.682698 139656658151168 logging_writer.py:48] [127700] global_step=127700, grad_norm=2.547079086303711, loss=1.8085997104644775
I0129 13:53:56.134216 139822745589568 spec.py:321] Evaluating on the training split.
I0129 13:54:02.403312 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 13:54:11.184713 139822745589568 spec.py:349] Evaluating on the test split.
I0129 13:54:13.748886 139822745589568 submission_runner.py:408] Time since start: 44897.11s, 	Step: 127759, 	{'train/accuracy': 0.6521045565605164, 'train/loss': 1.3957295417785645, 'validation/accuracy': 0.5881999731063843, 'validation/loss': 1.7238211631774902, 'validation/num_examples': 50000, 'test/accuracy': 0.4629000127315521, 'test/loss': 2.50555157661438, 'test/num_examples': 10000, 'score': 43387.98578906059, 'total_duration': 44897.10552716255, 'accumulated_submission_time': 43387.98578906059, 'accumulated_eval_time': 1501.349282026291, 'accumulated_logging_time': 3.3759353160858154}
I0129 13:54:13.791583 139656297445120 logging_writer.py:48] [127759] accumulated_eval_time=1501.349282, accumulated_logging_time=3.375935, accumulated_submission_time=43387.985789, global_step=127759, preemption_count=0, score=43387.985789, test/accuracy=0.462900, test/loss=2.505552, test/num_examples=10000, total_duration=44897.105527, train/accuracy=0.652105, train/loss=1.395730, validation/accuracy=0.588200, validation/loss=1.723821, validation/num_examples=50000
I0129 13:54:28.043573 139656666543872 logging_writer.py:48] [127800] global_step=127800, grad_norm=2.401376962661743, loss=1.8439232110977173
I0129 13:55:01.875191 139656297445120 logging_writer.py:48] [127900] global_step=127900, grad_norm=2.6213736534118652, loss=1.6542327404022217
I0129 13:55:35.706327 139656666543872 logging_writer.py:48] [128000] global_step=128000, grad_norm=2.4858765602111816, loss=1.8240200281143188
I0129 13:56:09.562474 139656297445120 logging_writer.py:48] [128100] global_step=128100, grad_norm=2.288494110107422, loss=1.7445573806762695
I0129 13:56:43.417253 139656666543872 logging_writer.py:48] [128200] global_step=128200, grad_norm=2.3599483966827393, loss=1.6526589393615723
I0129 13:57:17.288404 139656297445120 logging_writer.py:48] [128300] global_step=128300, grad_norm=2.5900306701660156, loss=1.7903289794921875
I0129 13:57:51.156366 139656666543872 logging_writer.py:48] [128400] global_step=128400, grad_norm=2.456167459487915, loss=1.5709354877471924
I0129 13:58:25.007914 139656297445120 logging_writer.py:48] [128500] global_step=128500, grad_norm=2.7459888458251953, loss=1.7303061485290527
I0129 13:58:58.896231 139656666543872 logging_writer.py:48] [128600] global_step=128600, grad_norm=2.65183424949646, loss=1.7145087718963623
I0129 13:59:32.788904 139656297445120 logging_writer.py:48] [128700] global_step=128700, grad_norm=2.307547092437744, loss=1.6382099390029907
I0129 14:00:06.725186 139656666543872 logging_writer.py:48] [128800] global_step=128800, grad_norm=2.3471462726593018, loss=1.713515043258667
I0129 14:00:40.595287 139656297445120 logging_writer.py:48] [128900] global_step=128900, grad_norm=2.581493616104126, loss=1.6343624591827393
I0129 14:01:14.492267 139656666543872 logging_writer.py:48] [129000] global_step=129000, grad_norm=2.409475088119507, loss=1.6845676898956299
I0129 14:01:48.373593 139656297445120 logging_writer.py:48] [129100] global_step=129100, grad_norm=2.3307371139526367, loss=1.7885727882385254
I0129 14:02:22.291593 139656666543872 logging_writer.py:48] [129200] global_step=129200, grad_norm=2.441688299179077, loss=1.6914920806884766
I0129 14:02:43.779263 139822745589568 spec.py:321] Evaluating on the training split.
I0129 14:02:49.934658 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 14:02:58.693856 139822745589568 spec.py:349] Evaluating on the test split.
I0129 14:03:01.291941 139822745589568 submission_runner.py:408] Time since start: 45424.65s, 	Step: 129265, 	{'train/accuracy': 0.6463648080825806, 'train/loss': 1.4183142185211182, 'validation/accuracy': 0.5917400121688843, 'validation/loss': 1.7195110321044922, 'validation/num_examples': 50000, 'test/accuracy': 0.47220003604888916, 'test/loss': 2.4591338634490967, 'test/num_examples': 10000, 'score': 43897.913105010986, 'total_duration': 45424.64857959747, 'accumulated_submission_time': 43897.913105010986, 'accumulated_eval_time': 1518.861918926239, 'accumulated_logging_time': 3.4282279014587402}
I0129 14:03:01.334090 139655626356480 logging_writer.py:48] [129265] accumulated_eval_time=1518.861919, accumulated_logging_time=3.428228, accumulated_submission_time=43897.913105, global_step=129265, preemption_count=0, score=43897.913105, test/accuracy=0.472200, test/loss=2.459134, test/num_examples=10000, total_duration=45424.648580, train/accuracy=0.646365, train/loss=1.418314, validation/accuracy=0.591740, validation/loss=1.719511, validation/num_examples=50000
I0129 14:03:13.525660 139656649758464 logging_writer.py:48] [129300] global_step=129300, grad_norm=2.436081647872925, loss=1.722411036491394
I0129 14:03:47.339748 139655626356480 logging_writer.py:48] [129400] global_step=129400, grad_norm=2.4889111518859863, loss=1.8337628841400146
I0129 14:04:21.162314 139656649758464 logging_writer.py:48] [129500] global_step=129500, grad_norm=2.303659677505493, loss=1.621694803237915
I0129 14:04:55.013475 139655626356480 logging_writer.py:48] [129600] global_step=129600, grad_norm=2.523383855819702, loss=1.8690719604492188
I0129 14:05:28.893083 139656649758464 logging_writer.py:48] [129700] global_step=129700, grad_norm=2.6950490474700928, loss=1.588292121887207
I0129 14:06:02.813917 139655626356480 logging_writer.py:48] [129800] global_step=129800, grad_norm=2.2471914291381836, loss=1.740538239479065
I0129 14:06:36.803747 139656649758464 logging_writer.py:48] [129900] global_step=129900, grad_norm=2.414801836013794, loss=1.5860321521759033
I0129 14:07:10.717046 139655626356480 logging_writer.py:48] [130000] global_step=130000, grad_norm=2.6505539417266846, loss=1.7159725427627563
I0129 14:07:44.595690 139656649758464 logging_writer.py:48] [130100] global_step=130100, grad_norm=2.6096622943878174, loss=1.75291109085083
I0129 14:08:18.504616 139655626356480 logging_writer.py:48] [130200] global_step=130200, grad_norm=2.613966941833496, loss=1.5990383625030518
I0129 14:08:52.414819 139656649758464 logging_writer.py:48] [130300] global_step=130300, grad_norm=2.5112266540527344, loss=1.6054811477661133
I0129 14:09:26.308238 139655626356480 logging_writer.py:48] [130400] global_step=130400, grad_norm=2.5503528118133545, loss=1.6842228174209595
I0129 14:10:00.234821 139656649758464 logging_writer.py:48] [130500] global_step=130500, grad_norm=2.638709545135498, loss=1.6373463869094849
I0129 14:10:34.130356 139655626356480 logging_writer.py:48] [130600] global_step=130600, grad_norm=2.427562713623047, loss=1.6417014598846436
I0129 14:11:08.046563 139656649758464 logging_writer.py:48] [130700] global_step=130700, grad_norm=2.49934458732605, loss=1.6763546466827393
I0129 14:11:31.584299 139822745589568 spec.py:321] Evaluating on the training split.
I0129 14:11:37.894649 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 14:11:46.293452 139822745589568 spec.py:349] Evaluating on the test split.
I0129 14:11:48.851737 139822745589568 submission_runner.py:408] Time since start: 45952.21s, 	Step: 130771, 	{'train/accuracy': 0.6597377061843872, 'train/loss': 1.3550430536270142, 'validation/accuracy': 0.6078000068664551, 'validation/loss': 1.6432515382766724, 'validation/num_examples': 50000, 'test/accuracy': 0.48350003361701965, 'test/loss': 2.3731515407562256, 'test/num_examples': 10000, 'score': 44408.10325551033, 'total_duration': 45952.20838069916, 'accumulated_submission_time': 44408.10325551033, 'accumulated_eval_time': 1536.1293251514435, 'accumulated_logging_time': 3.4792776107788086}
I0129 14:11:48.894381 139655609571072 logging_writer.py:48] [130771] accumulated_eval_time=1536.129325, accumulated_logging_time=3.479278, accumulated_submission_time=44408.103256, global_step=130771, preemption_count=0, score=44408.103256, test/accuracy=0.483500, test/loss=2.373152, test/num_examples=10000, total_duration=45952.208381, train/accuracy=0.659738, train/loss=1.355043, validation/accuracy=0.607800, validation/loss=1.643252, validation/num_examples=50000
I0129 14:11:59.063741 139655617963776 logging_writer.py:48] [130800] global_step=130800, grad_norm=2.3247790336608887, loss=1.7084077596664429
I0129 14:12:32.918901 139655609571072 logging_writer.py:48] [130900] global_step=130900, grad_norm=2.5091922283172607, loss=1.710974931716919
I0129 14:13:06.841823 139655617963776 logging_writer.py:48] [131000] global_step=131000, grad_norm=2.69242262840271, loss=1.6241865158081055
I0129 14:13:40.737172 139655609571072 logging_writer.py:48] [131100] global_step=131100, grad_norm=2.7029800415039062, loss=1.6097962856292725
I0129 14:14:14.614544 139655617963776 logging_writer.py:48] [131200] global_step=131200, grad_norm=2.5916872024536133, loss=1.6188551187515259
I0129 14:14:48.501386 139655609571072 logging_writer.py:48] [131300] global_step=131300, grad_norm=2.593439817428589, loss=1.6940085887908936
I0129 14:15:22.341987 139655617963776 logging_writer.py:48] [131400] global_step=131400, grad_norm=2.552372932434082, loss=1.6233179569244385
I0129 14:15:56.219395 139655609571072 logging_writer.py:48] [131500] global_step=131500, grad_norm=2.611137866973877, loss=1.6384432315826416
I0129 14:16:30.108851 139655617963776 logging_writer.py:48] [131600] global_step=131600, grad_norm=2.5052852630615234, loss=1.6094688177108765
I0129 14:17:04.011796 139655609571072 logging_writer.py:48] [131700] global_step=131700, grad_norm=3.08061146736145, loss=1.818200707435608
I0129 14:17:37.846168 139655617963776 logging_writer.py:48] [131800] global_step=131800, grad_norm=2.5887038707733154, loss=1.6286323070526123
I0129 14:18:11.740742 139655609571072 logging_writer.py:48] [131900] global_step=131900, grad_norm=2.601875066757202, loss=1.643311619758606
I0129 14:18:45.636490 139655617963776 logging_writer.py:48] [132000] global_step=132000, grad_norm=2.474141836166382, loss=1.65077543258667
I0129 14:19:19.537556 139655609571072 logging_writer.py:48] [132100] global_step=132100, grad_norm=2.55045485496521, loss=1.6713403463363647
I0129 14:19:53.502774 139655617963776 logging_writer.py:48] [132200] global_step=132200, grad_norm=2.7909581661224365, loss=1.5642259120941162
I0129 14:20:19.074167 139822745589568 spec.py:321] Evaluating on the training split.
I0129 14:20:25.247081 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 14:20:33.684424 139822745589568 spec.py:349] Evaluating on the test split.
I0129 14:20:36.232810 139822745589568 submission_runner.py:408] Time since start: 46479.59s, 	Step: 132277, 	{'train/accuracy': 0.5997488498687744, 'train/loss': 1.664632797241211, 'validation/accuracy': 0.5585799813270569, 'validation/loss': 1.9178231954574585, 'validation/num_examples': 50000, 'test/accuracy': 0.4361000061035156, 'test/loss': 2.7067646980285645, 'test/num_examples': 10000, 'score': 44918.220878601074, 'total_duration': 46479.58945250511, 'accumulated_submission_time': 44918.220878601074, 'accumulated_eval_time': 1553.2879321575165, 'accumulated_logging_time': 3.5328681468963623}
I0129 14:20:36.276867 139655609571072 logging_writer.py:48] [132277] accumulated_eval_time=1553.287932, accumulated_logging_time=3.532868, accumulated_submission_time=44918.220879, global_step=132277, preemption_count=0, score=44918.220879, test/accuracy=0.436100, test/loss=2.706765, test/num_examples=10000, total_duration=46479.589453, train/accuracy=0.599749, train/loss=1.664633, validation/accuracy=0.558580, validation/loss=1.917823, validation/num_examples=50000
I0129 14:20:44.385424 139655626356480 logging_writer.py:48] [132300] global_step=132300, grad_norm=2.557328701019287, loss=1.5525614023208618
I0129 14:21:18.205662 139655609571072 logging_writer.py:48] [132400] global_step=132400, grad_norm=2.657212257385254, loss=1.5859625339508057
I0129 14:21:52.039672 139655626356480 logging_writer.py:48] [132500] global_step=132500, grad_norm=2.8158493041992188, loss=1.727342128753662
I0129 14:22:25.925567 139655609571072 logging_writer.py:48] [132600] global_step=132600, grad_norm=2.6391899585723877, loss=1.730422854423523
I0129 14:22:59.833075 139655626356480 logging_writer.py:48] [132700] global_step=132700, grad_norm=2.971369981765747, loss=1.6823102235794067
I0129 14:23:33.728115 139655609571072 logging_writer.py:48] [132800] global_step=132800, grad_norm=2.7541844844818115, loss=1.679816722869873
I0129 14:24:07.612596 139655626356480 logging_writer.py:48] [132900] global_step=132900, grad_norm=2.6582353115081787, loss=1.6477024555206299
I0129 14:24:41.501432 139655609571072 logging_writer.py:48] [133000] global_step=133000, grad_norm=2.8136868476867676, loss=1.8094797134399414
I0129 14:25:15.400413 139655626356480 logging_writer.py:48] [133100] global_step=133100, grad_norm=2.5878145694732666, loss=1.6380467414855957
I0129 14:25:49.263563 139655609571072 logging_writer.py:48] [133200] global_step=133200, grad_norm=2.8845696449279785, loss=1.7098777294158936
I0129 14:26:23.237959 139655626356480 logging_writer.py:48] [133300] global_step=133300, grad_norm=2.6629490852355957, loss=1.6723459959030151
I0129 14:26:57.079850 139655609571072 logging_writer.py:48] [133400] global_step=133400, grad_norm=2.7207794189453125, loss=1.5455796718597412
I0129 14:27:30.937328 139655626356480 logging_writer.py:48] [133500] global_step=133500, grad_norm=2.596359968185425, loss=1.5973864793777466
I0129 14:28:04.832346 139655609571072 logging_writer.py:48] [133600] global_step=133600, grad_norm=2.47153377532959, loss=1.6348882913589478
I0129 14:28:38.707594 139655626356480 logging_writer.py:48] [133700] global_step=133700, grad_norm=2.526843309402466, loss=1.6886816024780273
I0129 14:29:06.333238 139822745589568 spec.py:321] Evaluating on the training split.
I0129 14:29:12.527370 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 14:29:21.335703 139822745589568 spec.py:349] Evaluating on the test split.
I0129 14:29:23.776955 139822745589568 submission_runner.py:408] Time since start: 47007.13s, 	Step: 133783, 	{'train/accuracy': 0.623445451259613, 'train/loss': 1.5336153507232666, 'validation/accuracy': 0.577239990234375, 'validation/loss': 1.8043313026428223, 'validation/num_examples': 50000, 'test/accuracy': 0.4651000201702118, 'test/loss': 2.5390968322753906, 'test/num_examples': 10000, 'score': 45428.21768307686, 'total_duration': 47007.13358902931, 'accumulated_submission_time': 45428.21768307686, 'accumulated_eval_time': 1570.7316064834595, 'accumulated_logging_time': 3.585947036743164}
I0129 14:29:23.817846 139655626356480 logging_writer.py:48] [133783] accumulated_eval_time=1570.731606, accumulated_logging_time=3.585947, accumulated_submission_time=45428.217683, global_step=133783, preemption_count=0, score=45428.217683, test/accuracy=0.465100, test/loss=2.539097, test/num_examples=10000, total_duration=47007.133589, train/accuracy=0.623445, train/loss=1.533615, validation/accuracy=0.577240, validation/loss=1.804331, validation/num_examples=50000
I0129 14:29:29.912026 139656297445120 logging_writer.py:48] [133800] global_step=133800, grad_norm=2.55773663520813, loss=1.5022790431976318
I0129 14:30:03.771773 139655626356480 logging_writer.py:48] [133900] global_step=133900, grad_norm=2.8098158836364746, loss=1.8089460134506226
I0129 14:30:37.650023 139656297445120 logging_writer.py:48] [134000] global_step=134000, grad_norm=2.7363314628601074, loss=1.6201372146606445
I0129 14:31:11.515603 139655626356480 logging_writer.py:48] [134100] global_step=134100, grad_norm=2.944741725921631, loss=1.7652164697647095
I0129 14:31:45.366183 139656297445120 logging_writer.py:48] [134200] global_step=134200, grad_norm=2.5162084102630615, loss=1.5642341375350952
I0129 14:32:19.225004 139655626356480 logging_writer.py:48] [134300] global_step=134300, grad_norm=2.743023633956909, loss=1.626428246498108
I0129 14:32:53.164539 139656297445120 logging_writer.py:48] [134400] global_step=134400, grad_norm=2.731088399887085, loss=1.6055371761322021
I0129 14:33:27.044365 139655626356480 logging_writer.py:48] [134500] global_step=134500, grad_norm=2.7670629024505615, loss=1.6597249507904053
I0129 14:34:00.941223 139656297445120 logging_writer.py:48] [134600] global_step=134600, grad_norm=2.553269147872925, loss=1.6152217388153076
I0129 14:34:34.849184 139655626356480 logging_writer.py:48] [134700] global_step=134700, grad_norm=2.7105159759521484, loss=1.6309759616851807
I0129 14:35:08.758025 139656297445120 logging_writer.py:48] [134800] global_step=134800, grad_norm=2.6556994915008545, loss=1.5857844352722168
I0129 14:35:42.693028 139655626356480 logging_writer.py:48] [134900] global_step=134900, grad_norm=2.620713472366333, loss=1.5785706043243408
I0129 14:36:16.568365 139656297445120 logging_writer.py:48] [135000] global_step=135000, grad_norm=2.7318668365478516, loss=1.617638111114502
I0129 14:36:50.479140 139655626356480 logging_writer.py:48] [135100] global_step=135100, grad_norm=2.734166383743286, loss=1.5206819772720337
I0129 14:37:24.367761 139656297445120 logging_writer.py:48] [135200] global_step=135200, grad_norm=2.4964921474456787, loss=1.531638741493225
I0129 14:37:54.001548 139822745589568 spec.py:321] Evaluating on the training split.
I0129 14:38:00.190839 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 14:38:08.913997 139822745589568 spec.py:349] Evaluating on the test split.
I0129 14:38:11.378417 139822745589568 submission_runner.py:408] Time since start: 47534.74s, 	Step: 135289, 	{'train/accuracy': 0.6740872263908386, 'train/loss': 1.2957236766815186, 'validation/accuracy': 0.5938000082969666, 'validation/loss': 1.7162760496139526, 'validation/num_examples': 50000, 'test/accuracy': 0.46070003509521484, 'test/loss': 2.485707998275757, 'test/num_examples': 10000, 'score': 45938.33998990059, 'total_duration': 47534.735048532486, 'accumulated_submission_time': 45938.33998990059, 'accumulated_eval_time': 1588.1084327697754, 'accumulated_logging_time': 3.637380599975586}
I0129 14:38:11.419143 139655617963776 logging_writer.py:48] [135289] accumulated_eval_time=1588.108433, accumulated_logging_time=3.637381, accumulated_submission_time=45938.339990, global_step=135289, preemption_count=0, score=45938.339990, test/accuracy=0.460700, test/loss=2.485708, test/num_examples=10000, total_duration=47534.735049, train/accuracy=0.674087, train/loss=1.295724, validation/accuracy=0.593800, validation/loss=1.716276, validation/num_examples=50000
I0129 14:38:15.512411 139656666543872 logging_writer.py:48] [135300] global_step=135300, grad_norm=3.0965559482574463, loss=1.6466877460479736
I0129 14:38:49.348125 139655617963776 logging_writer.py:48] [135400] global_step=135400, grad_norm=2.642354726791382, loss=1.59544038772583
I0129 14:39:23.285733 139656666543872 logging_writer.py:48] [135500] global_step=135500, grad_norm=2.6725878715515137, loss=1.5734710693359375
I0129 14:39:57.182058 139655617963776 logging_writer.py:48] [135600] global_step=135600, grad_norm=2.8831913471221924, loss=1.5352168083190918
I0129 14:40:31.073220 139656666543872 logging_writer.py:48] [135700] global_step=135700, grad_norm=2.6990344524383545, loss=1.5607990026474
I0129 14:41:04.984800 139655617963776 logging_writer.py:48] [135800] global_step=135800, grad_norm=2.740617513656616, loss=1.5949097871780396
I0129 14:41:38.889814 139656666543872 logging_writer.py:48] [135900] global_step=135900, grad_norm=2.8359384536743164, loss=1.636422872543335
I0129 14:42:12.797046 139655617963776 logging_writer.py:48] [136000] global_step=136000, grad_norm=2.5536012649536133, loss=1.509614109992981
I0129 14:42:46.701234 139656666543872 logging_writer.py:48] [136100] global_step=136100, grad_norm=2.672215461730957, loss=1.6504168510437012
I0129 14:43:20.612045 139655617963776 logging_writer.py:48] [136200] global_step=136200, grad_norm=2.8961808681488037, loss=1.59665846824646
I0129 14:43:54.514501 139656666543872 logging_writer.py:48] [136300] global_step=136300, grad_norm=2.676584005355835, loss=1.5476577281951904
I0129 14:44:28.358368 139655617963776 logging_writer.py:48] [136400] global_step=136400, grad_norm=2.636526346206665, loss=1.6353693008422852
I0129 14:45:02.236193 139656666543872 logging_writer.py:48] [136500] global_step=136500, grad_norm=2.7004289627075195, loss=1.5624886751174927
I0129 14:45:36.193803 139655617963776 logging_writer.py:48] [136600] global_step=136600, grad_norm=2.758547306060791, loss=1.6377724409103394
I0129 14:46:10.070374 139656666543872 logging_writer.py:48] [136700] global_step=136700, grad_norm=2.7913978099823, loss=1.5996729135513306
I0129 14:46:41.418983 139822745589568 spec.py:321] Evaluating on the training split.
I0129 14:46:47.727313 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 14:46:56.237665 139822745589568 spec.py:349] Evaluating on the test split.
I0129 14:46:58.838030 139822745589568 submission_runner.py:408] Time since start: 48062.19s, 	Step: 136794, 	{'train/accuracy': 0.6825972199440002, 'train/loss': 1.2317051887512207, 'validation/accuracy': 0.6187599897384644, 'validation/loss': 1.594221830368042, 'validation/num_examples': 50000, 'test/accuracy': 0.497700035572052, 'test/loss': 2.348714590072632, 'test/num_examples': 10000, 'score': 46448.27879500389, 'total_duration': 48062.19457030296, 'accumulated_submission_time': 46448.27879500389, 'accumulated_eval_time': 1605.5273563861847, 'accumulated_logging_time': 3.687337875366211}
I0129 14:46:58.882465 139655626356480 logging_writer.py:48] [136794] accumulated_eval_time=1605.527356, accumulated_logging_time=3.687338, accumulated_submission_time=46448.278795, global_step=136794, preemption_count=0, score=46448.278795, test/accuracy=0.497700, test/loss=2.348715, test/num_examples=10000, total_duration=48062.194570, train/accuracy=0.682597, train/loss=1.231705, validation/accuracy=0.618760, validation/loss=1.594222, validation/num_examples=50000
I0129 14:47:01.251888 139656297445120 logging_writer.py:48] [136800] global_step=136800, grad_norm=2.7251200675964355, loss=1.575494647026062
I0129 14:47:35.104543 139655626356480 logging_writer.py:48] [136900] global_step=136900, grad_norm=3.1664741039276123, loss=1.636030912399292
I0129 14:48:08.940356 139656297445120 logging_writer.py:48] [137000] global_step=137000, grad_norm=2.6056206226348877, loss=1.5840580463409424
I0129 14:48:42.812557 139655626356480 logging_writer.py:48] [137100] global_step=137100, grad_norm=2.797668218612671, loss=1.6155459880828857
I0129 14:49:16.705228 139656297445120 logging_writer.py:48] [137200] global_step=137200, grad_norm=2.8137967586517334, loss=1.5465608835220337
I0129 14:49:50.574088 139655626356480 logging_writer.py:48] [137300] global_step=137300, grad_norm=2.844982147216797, loss=1.6046576499938965
I0129 14:50:24.434258 139656297445120 logging_writer.py:48] [137400] global_step=137400, grad_norm=2.8720877170562744, loss=1.5727649927139282
I0129 14:50:58.341812 139655626356480 logging_writer.py:48] [137500] global_step=137500, grad_norm=2.8724358081817627, loss=1.5576629638671875
I0129 14:51:32.236165 139656297445120 logging_writer.py:48] [137600] global_step=137600, grad_norm=2.870527505874634, loss=1.5730817317962646
I0129 14:52:06.185376 139655626356480 logging_writer.py:48] [137700] global_step=137700, grad_norm=2.8480217456817627, loss=1.7556995153427124
I0129 14:52:40.053186 139656297445120 logging_writer.py:48] [137800] global_step=137800, grad_norm=2.9061527252197266, loss=1.8028819561004639
I0129 14:53:13.975239 139655626356480 logging_writer.py:48] [137900] global_step=137900, grad_norm=2.7914764881134033, loss=1.7249218225479126
I0129 14:53:47.877585 139656297445120 logging_writer.py:48] [138000] global_step=138000, grad_norm=2.9539098739624023, loss=1.6088297367095947
I0129 14:54:21.791104 139655626356480 logging_writer.py:48] [138100] global_step=138100, grad_norm=2.882376194000244, loss=1.5878832340240479
I0129 14:54:55.666610 139656297445120 logging_writer.py:48] [138200] global_step=138200, grad_norm=2.6684763431549072, loss=1.5807840824127197
I0129 14:55:29.030130 139822745589568 spec.py:321] Evaluating on the training split.
I0129 14:55:35.213590 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 14:55:43.733749 139822745589568 spec.py:349] Evaluating on the test split.
I0129 14:55:46.330310 139822745589568 submission_runner.py:408] Time since start: 48589.69s, 	Step: 138300, 	{'train/accuracy': 0.6966079473495483, 'train/loss': 1.193784475326538, 'validation/accuracy': 0.6342799663543701, 'validation/loss': 1.5047619342803955, 'validation/num_examples': 50000, 'test/accuracy': 0.5135000348091125, 'test/loss': 2.21742844581604, 'test/num_examples': 10000, 'score': 46958.36615371704, 'total_duration': 48589.68688797951, 'accumulated_submission_time': 46958.36615371704, 'accumulated_eval_time': 1622.827439069748, 'accumulated_logging_time': 3.740651845932007}
I0129 14:55:46.373427 139655609571072 logging_writer.py:48] [138300] accumulated_eval_time=1622.827439, accumulated_logging_time=3.740652, accumulated_submission_time=46958.366154, global_step=138300, preemption_count=0, score=46958.366154, test/accuracy=0.513500, test/loss=2.217428, test/num_examples=10000, total_duration=48589.686888, train/accuracy=0.696608, train/loss=1.193784, validation/accuracy=0.634280, validation/loss=1.504762, validation/num_examples=50000
I0129 14:55:46.741168 139656658151168 logging_writer.py:48] [138300] global_step=138300, grad_norm=3.033635377883911, loss=1.57996666431427
I0129 14:56:20.575159 139655609571072 logging_writer.py:48] [138400] global_step=138400, grad_norm=2.8480618000030518, loss=1.6901297569274902
I0129 14:56:54.458070 139656658151168 logging_writer.py:48] [138500] global_step=138500, grad_norm=2.920928478240967, loss=1.6618181467056274
I0129 14:57:28.336899 139655609571072 logging_writer.py:48] [138600] global_step=138600, grad_norm=2.556507110595703, loss=1.529123306274414
I0129 14:58:02.216819 139656658151168 logging_writer.py:48] [138700] global_step=138700, grad_norm=2.612935781478882, loss=1.5291759967803955
I0129 14:58:36.281264 139655609571072 logging_writer.py:48] [138800] global_step=138800, grad_norm=2.9857535362243652, loss=1.608322262763977
I0129 14:59:10.153210 139656658151168 logging_writer.py:48] [138900] global_step=138900, grad_norm=2.9926319122314453, loss=1.690129280090332
I0129 14:59:44.074794 139655609571072 logging_writer.py:48] [139000] global_step=139000, grad_norm=2.9267797470092773, loss=1.520103096961975
I0129 15:00:17.941810 139656658151168 logging_writer.py:48] [139100] global_step=139100, grad_norm=2.905407428741455, loss=1.639151930809021
I0129 15:00:51.839200 139655609571072 logging_writer.py:48] [139200] global_step=139200, grad_norm=2.777047634124756, loss=1.5271871089935303
I0129 15:01:25.725784 139656658151168 logging_writer.py:48] [139300] global_step=139300, grad_norm=3.0728282928466797, loss=1.6606048345565796
I0129 15:01:59.610429 139655609571072 logging_writer.py:48] [139400] global_step=139400, grad_norm=2.964143753051758, loss=1.6996114253997803
I0129 15:02:33.495348 139656658151168 logging_writer.py:48] [139500] global_step=139500, grad_norm=2.8403079509735107, loss=1.5805871486663818
I0129 15:03:07.363937 139655609571072 logging_writer.py:48] [139600] global_step=139600, grad_norm=2.7732856273651123, loss=1.5729899406433105
I0129 15:03:41.227333 139656658151168 logging_writer.py:48] [139700] global_step=139700, grad_norm=3.032247304916382, loss=1.569580316543579
I0129 15:04:15.086318 139655609571072 logging_writer.py:48] [139800] global_step=139800, grad_norm=2.8918466567993164, loss=1.4795067310333252
I0129 15:04:16.593559 139822745589568 spec.py:321] Evaluating on the training split.
I0129 15:04:22.830914 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 15:04:31.491544 139822745589568 spec.py:349] Evaluating on the test split.
I0129 15:04:34.064509 139822745589568 submission_runner.py:408] Time since start: 49117.42s, 	Step: 139806, 	{'train/accuracy': 0.6668726205825806, 'train/loss': 1.3343795537948608, 'validation/accuracy': 0.6074599623680115, 'validation/loss': 1.647443413734436, 'validation/num_examples': 50000, 'test/accuracy': 0.46820002794265747, 'test/loss': 2.447305917739868, 'test/num_examples': 10000, 'score': 47468.524106025696, 'total_duration': 49117.42114567757, 'accumulated_submission_time': 47468.524106025696, 'accumulated_eval_time': 1640.2983448505402, 'accumulated_logging_time': 3.7930803298950195}
I0129 15:04:34.104254 139656297445120 logging_writer.py:48] [139806] accumulated_eval_time=1640.298345, accumulated_logging_time=3.793080, accumulated_submission_time=47468.524106, global_step=139806, preemption_count=0, score=47468.524106, test/accuracy=0.468200, test/loss=2.447306, test/num_examples=10000, total_duration=49117.421146, train/accuracy=0.666873, train/loss=1.334380, validation/accuracy=0.607460, validation/loss=1.647443, validation/num_examples=50000
I0129 15:05:06.404297 139656649758464 logging_writer.py:48] [139900] global_step=139900, grad_norm=2.98541259765625, loss=1.5435378551483154
I0129 15:05:40.263980 139656297445120 logging_writer.py:48] [140000] global_step=140000, grad_norm=3.264500617980957, loss=1.5262517929077148
I0129 15:06:14.140751 139656649758464 logging_writer.py:48] [140100] global_step=140100, grad_norm=2.9730918407440186, loss=1.4958430528640747
I0129 15:06:48.052645 139656297445120 logging_writer.py:48] [140200] global_step=140200, grad_norm=2.852416753768921, loss=1.5079704523086548
I0129 15:07:21.964945 139656649758464 logging_writer.py:48] [140300] global_step=140300, grad_norm=2.99157977104187, loss=1.5368255376815796
I0129 15:07:55.872507 139656297445120 logging_writer.py:48] [140400] global_step=140400, grad_norm=2.8564634323120117, loss=1.506117820739746
I0129 15:08:29.765623 139656649758464 logging_writer.py:48] [140500] global_step=140500, grad_norm=3.1156349182128906, loss=1.6406404972076416
I0129 15:09:03.638985 139656297445120 logging_writer.py:48] [140600] global_step=140600, grad_norm=2.9631903171539307, loss=1.5670037269592285
I0129 15:09:37.529833 139656649758464 logging_writer.py:48] [140700] global_step=140700, grad_norm=3.029829978942871, loss=1.524198293685913
I0129 15:10:11.393436 139656297445120 logging_writer.py:48] [140800] global_step=140800, grad_norm=3.0352494716644287, loss=1.5641452074050903
I0129 15:10:45.219387 139656649758464 logging_writer.py:48] [140900] global_step=140900, grad_norm=2.8007867336273193, loss=1.530596137046814
I0129 15:11:19.146393 139656297445120 logging_writer.py:48] [141000] global_step=141000, grad_norm=3.1089112758636475, loss=1.5165890455245972
I0129 15:11:53.028858 139656649758464 logging_writer.py:48] [141100] global_step=141100, grad_norm=3.136099100112915, loss=1.533268690109253
I0129 15:12:26.877598 139656297445120 logging_writer.py:48] [141200] global_step=141200, grad_norm=2.8250749111175537, loss=1.5245583057403564
I0129 15:13:00.725907 139656649758464 logging_writer.py:48] [141300] global_step=141300, grad_norm=2.9102327823638916, loss=1.6011707782745361
I0129 15:13:04.258049 139822745589568 spec.py:321] Evaluating on the training split.
I0129 15:13:10.464438 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 15:13:18.842068 139822745589568 spec.py:349] Evaluating on the test split.
I0129 15:13:21.407066 139822745589568 submission_runner.py:408] Time since start: 49644.76s, 	Step: 141312, 	{'train/accuracy': 0.7024075388908386, 'train/loss': 1.1639950275421143, 'validation/accuracy': 0.6421200037002563, 'validation/loss': 1.4964680671691895, 'validation/num_examples': 50000, 'test/accuracy': 0.5249000191688538, 'test/loss': 2.221546173095703, 'test/num_examples': 10000, 'score': 47978.61784863472, 'total_duration': 49644.76369333267, 'accumulated_submission_time': 47978.61784863472, 'accumulated_eval_time': 1657.447308063507, 'accumulated_logging_time': 3.8419196605682373}
I0129 15:13:21.451282 139655626356480 logging_writer.py:48] [141312] accumulated_eval_time=1657.447308, accumulated_logging_time=3.841920, accumulated_submission_time=47978.617849, global_step=141312, preemption_count=0, score=47978.617849, test/accuracy=0.524900, test/loss=2.221546, test/num_examples=10000, total_duration=49644.763693, train/accuracy=0.702408, train/loss=1.163995, validation/accuracy=0.642120, validation/loss=1.496468, validation/num_examples=50000
I0129 15:13:51.525574 139656658151168 logging_writer.py:48] [141400] global_step=141400, grad_norm=3.1183457374572754, loss=1.5405223369598389
I0129 15:14:25.332470 139655626356480 logging_writer.py:48] [141500] global_step=141500, grad_norm=3.1660513877868652, loss=1.5474462509155273
I0129 15:14:59.184402 139656658151168 logging_writer.py:48] [141600] global_step=141600, grad_norm=3.042119026184082, loss=1.5692980289459229
I0129 15:15:33.058318 139655626356480 logging_writer.py:48] [141700] global_step=141700, grad_norm=2.992825508117676, loss=1.4737255573272705
I0129 15:16:06.938749 139656658151168 logging_writer.py:48] [141800] global_step=141800, grad_norm=3.1962413787841797, loss=1.5023369789123535
I0129 15:16:40.810514 139655626356480 logging_writer.py:48] [141900] global_step=141900, grad_norm=2.9183132648468018, loss=1.5422320365905762
I0129 15:17:14.696190 139656658151168 logging_writer.py:48] [142000] global_step=142000, grad_norm=2.9644627571105957, loss=1.5042307376861572
I0129 15:17:48.583738 139655626356480 logging_writer.py:48] [142100] global_step=142100, grad_norm=3.262022018432617, loss=1.6738895177841187
I0129 15:18:22.564469 139656658151168 logging_writer.py:48] [142200] global_step=142200, grad_norm=3.3353703022003174, loss=1.6601812839508057
I0129 15:18:56.466668 139655626356480 logging_writer.py:48] [142300] global_step=142300, grad_norm=3.255366325378418, loss=1.5923389196395874
I0129 15:19:30.319989 139656658151168 logging_writer.py:48] [142400] global_step=142400, grad_norm=3.173980474472046, loss=1.5752835273742676
I0129 15:20:04.212674 139655626356480 logging_writer.py:48] [142500] global_step=142500, grad_norm=2.8264479637145996, loss=1.5174683332443237
I0129 15:20:38.118880 139656658151168 logging_writer.py:48] [142600] global_step=142600, grad_norm=3.0482115745544434, loss=1.5358710289001465
I0129 15:21:12.041656 139655626356480 logging_writer.py:48] [142700] global_step=142700, grad_norm=3.153454303741455, loss=1.5386300086975098
I0129 15:21:45.951399 139656658151168 logging_writer.py:48] [142800] global_step=142800, grad_norm=3.108323574066162, loss=1.4864609241485596
I0129 15:21:51.512425 139822745589568 spec.py:321] Evaluating on the training split.
I0129 15:21:57.680675 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 15:22:06.652624 139822745589568 spec.py:349] Evaluating on the test split.
I0129 15:22:09.498520 139822745589568 submission_runner.py:408] Time since start: 50172.86s, 	Step: 142818, 	{'train/accuracy': 0.7105787396430969, 'train/loss': 1.1328246593475342, 'validation/accuracy': 0.649399995803833, 'validation/loss': 1.4419379234313965, 'validation/num_examples': 50000, 'test/accuracy': 0.5152000188827515, 'test/loss': 2.194200038909912, 'test/num_examples': 10000, 'score': 48488.61668562889, 'total_duration': 50172.85512781143, 'accumulated_submission_time': 48488.61668562889, 'accumulated_eval_time': 1675.4333300590515, 'accumulated_logging_time': 3.895407199859619}
I0129 15:22:09.532418 139656649758464 logging_writer.py:48] [142818] accumulated_eval_time=1675.433330, accumulated_logging_time=3.895407, accumulated_submission_time=48488.616686, global_step=142818, preemption_count=0, score=48488.616686, test/accuracy=0.515200, test/loss=2.194200, test/num_examples=10000, total_duration=50172.855128, train/accuracy=0.710579, train/loss=1.132825, validation/accuracy=0.649400, validation/loss=1.441938, validation/num_examples=50000
I0129 15:22:37.616633 139658730145536 logging_writer.py:48] [142900] global_step=142900, grad_norm=3.0454845428466797, loss=1.4372776746749878
I0129 15:23:11.440472 139656649758464 logging_writer.py:48] [143000] global_step=143000, grad_norm=3.132967710494995, loss=1.6356481313705444
I0129 15:23:45.297039 139658730145536 logging_writer.py:48] [143100] global_step=143100, grad_norm=3.063392162322998, loss=1.5068345069885254
I0129 15:24:19.191994 139656649758464 logging_writer.py:48] [143200] global_step=143200, grad_norm=3.143099784851074, loss=1.5783987045288086
I0129 15:24:53.127341 139658730145536 logging_writer.py:48] [143300] global_step=143300, grad_norm=3.2272605895996094, loss=1.5476791858673096
I0129 15:25:27.036922 139656649758464 logging_writer.py:48] [143400] global_step=143400, grad_norm=3.0209977626800537, loss=1.3825408220291138
I0129 15:26:00.887836 139658730145536 logging_writer.py:48] [143500] global_step=143500, grad_norm=3.0724048614501953, loss=1.4422187805175781
I0129 15:26:34.744860 139656649758464 logging_writer.py:48] [143600] global_step=143600, grad_norm=3.046306610107422, loss=1.5684140920639038
I0129 15:27:08.635385 139658730145536 logging_writer.py:48] [143700] global_step=143700, grad_norm=3.1681811809539795, loss=1.5798946619033813
I0129 15:27:42.486639 139656649758464 logging_writer.py:48] [143800] global_step=143800, grad_norm=3.087216854095459, loss=1.4881535768508911
I0129 15:28:16.312453 139658730145536 logging_writer.py:48] [143900] global_step=143900, grad_norm=3.0577590465545654, loss=1.4714826345443726
I0129 15:28:50.153598 139656649758464 logging_writer.py:48] [144000] global_step=144000, grad_norm=3.260688543319702, loss=1.4848321676254272
I0129 15:29:24.024668 139658730145536 logging_writer.py:48] [144100] global_step=144100, grad_norm=3.2124361991882324, loss=1.4684033393859863
I0129 15:29:58.431844 139656649758464 logging_writer.py:48] [144200] global_step=144200, grad_norm=3.16062331199646, loss=1.4712045192718506
I0129 15:30:32.327377 139658730145536 logging_writer.py:48] [144300] global_step=144300, grad_norm=3.240032196044922, loss=1.4608774185180664
I0129 15:30:39.576995 139822745589568 spec.py:321] Evaluating on the training split.
I0129 15:30:45.833685 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 15:30:54.308130 139822745589568 spec.py:349] Evaluating on the test split.
I0129 15:30:57.317511 139822745589568 submission_runner.py:408] Time since start: 50700.67s, 	Step: 144323, 	{'train/accuracy': 0.749422013759613, 'train/loss': 0.9607897996902466, 'validation/accuracy': 0.6571199893951416, 'validation/loss': 1.4050936698913574, 'validation/num_examples': 50000, 'test/accuracy': 0.532800018787384, 'test/loss': 2.105572462081909, 'test/num_examples': 10000, 'score': 48998.60055851936, 'total_duration': 50700.674156188965, 'accumulated_submission_time': 48998.60055851936, 'accumulated_eval_time': 1693.17382478714, 'accumulated_logging_time': 3.9376351833343506}
I0129 15:30:57.359376 139655626356480 logging_writer.py:48] [144323] accumulated_eval_time=1693.173825, accumulated_logging_time=3.937635, accumulated_submission_time=48998.600559, global_step=144323, preemption_count=0, score=48998.600559, test/accuracy=0.532800, test/loss=2.105572, test/num_examples=10000, total_duration=50700.674156, train/accuracy=0.749422, train/loss=0.960790, validation/accuracy=0.657120, validation/loss=1.405094, validation/num_examples=50000
I0129 15:31:23.759280 139656297445120 logging_writer.py:48] [144400] global_step=144400, grad_norm=3.15169620513916, loss=1.3908343315124512
I0129 15:31:57.596371 139655626356480 logging_writer.py:48] [144500] global_step=144500, grad_norm=3.190410614013672, loss=1.5203064680099487
I0129 15:32:31.465922 139656297445120 logging_writer.py:48] [144600] global_step=144600, grad_norm=3.1882004737854004, loss=1.4767931699752808
I0129 15:33:05.374416 139655626356480 logging_writer.py:48] [144700] global_step=144700, grad_norm=3.2442171573638916, loss=1.6668905019760132
I0129 15:33:39.250471 139656297445120 logging_writer.py:48] [144800] global_step=144800, grad_norm=3.5642738342285156, loss=1.5935511589050293
I0129 15:34:13.154479 139655626356480 logging_writer.py:48] [144900] global_step=144900, grad_norm=3.360281467437744, loss=1.4249979257583618
I0129 15:34:47.047093 139656297445120 logging_writer.py:48] [145000] global_step=145000, grad_norm=3.099595546722412, loss=1.4436100721359253
I0129 15:35:20.933725 139655626356480 logging_writer.py:48] [145100] global_step=145100, grad_norm=3.3035855293273926, loss=1.4492628574371338
I0129 15:35:54.802342 139656297445120 logging_writer.py:48] [145200] global_step=145200, grad_norm=3.286883592605591, loss=1.4921245574951172
I0129 15:36:28.699248 139655626356480 logging_writer.py:48] [145300] global_step=145300, grad_norm=3.121614456176758, loss=1.4782216548919678
I0129 15:37:02.590721 139656297445120 logging_writer.py:48] [145400] global_step=145400, grad_norm=3.505911350250244, loss=1.4426780939102173
I0129 15:37:36.534543 139655626356480 logging_writer.py:48] [145500] global_step=145500, grad_norm=3.4254395961761475, loss=1.5441679954528809
I0129 15:38:10.413536 139656297445120 logging_writer.py:48] [145600] global_step=145600, grad_norm=3.196413040161133, loss=1.4553136825561523
I0129 15:38:44.323964 139655626356480 logging_writer.py:48] [145700] global_step=145700, grad_norm=3.2268495559692383, loss=1.5038400888442993
I0129 15:39:18.235299 139656297445120 logging_writer.py:48] [145800] global_step=145800, grad_norm=2.9721333980560303, loss=1.4163507223129272
I0129 15:39:27.533771 139822745589568 spec.py:321] Evaluating on the training split.
I0129 15:39:33.687559 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 15:39:42.068785 139822745589568 spec.py:349] Evaluating on the test split.
I0129 15:39:44.699975 139822745589568 submission_runner.py:408] Time since start: 51228.06s, 	Step: 145829, 	{'train/accuracy': 0.7521324753761292, 'train/loss': 0.9460116028785706, 'validation/accuracy': 0.6695399880409241, 'validation/loss': 1.3460698127746582, 'validation/num_examples': 50000, 'test/accuracy': 0.5408000349998474, 'test/loss': 2.0579564571380615, 'test/num_examples': 10000, 'score': 49508.71371245384, 'total_duration': 51228.05661511421, 'accumulated_submission_time': 49508.71371245384, 'accumulated_eval_time': 1710.3399860858917, 'accumulated_logging_time': 3.9888112545013428}
I0129 15:39:44.748381 139656666543872 logging_writer.py:48] [145829] accumulated_eval_time=1710.339986, accumulated_logging_time=3.988811, accumulated_submission_time=49508.713712, global_step=145829, preemption_count=0, score=49508.713712, test/accuracy=0.540800, test/loss=2.057956, test/num_examples=10000, total_duration=51228.056615, train/accuracy=0.752132, train/loss=0.946012, validation/accuracy=0.669540, validation/loss=1.346070, validation/num_examples=50000
I0129 15:40:09.153657 139656834316032 logging_writer.py:48] [145900] global_step=145900, grad_norm=3.532559394836426, loss=1.6543989181518555
I0129 15:40:42.982929 139656666543872 logging_writer.py:48] [146000] global_step=146000, grad_norm=3.0983855724334717, loss=1.4218472242355347
I0129 15:41:16.842505 139656834316032 logging_writer.py:48] [146100] global_step=146100, grad_norm=3.2247655391693115, loss=1.5722758769989014
I0129 15:41:50.728096 139656666543872 logging_writer.py:48] [146200] global_step=146200, grad_norm=3.1187868118286133, loss=1.5095831155776978
I0129 15:42:24.620980 139656834316032 logging_writer.py:48] [146300] global_step=146300, grad_norm=3.4877078533172607, loss=1.4524002075195312
I0129 15:42:58.469408 139656666543872 logging_writer.py:48] [146400] global_step=146400, grad_norm=3.32230806350708, loss=1.5797163248062134
I0129 15:43:32.347895 139656834316032 logging_writer.py:48] [146500] global_step=146500, grad_norm=3.4072439670562744, loss=1.4592896699905396
I0129 15:44:06.335143 139656666543872 logging_writer.py:48] [146600] global_step=146600, grad_norm=3.424410104751587, loss=1.5164891481399536
I0129 15:44:40.239484 139656834316032 logging_writer.py:48] [146700] global_step=146700, grad_norm=3.4059362411499023, loss=1.5027028322219849
I0129 15:45:14.101801 139656666543872 logging_writer.py:48] [146800] global_step=146800, grad_norm=3.144198179244995, loss=1.4574649333953857
I0129 15:45:48.007116 139656834316032 logging_writer.py:48] [146900] global_step=146900, grad_norm=3.3066720962524414, loss=1.427329421043396
I0129 15:46:21.896053 139656666543872 logging_writer.py:48] [147000] global_step=147000, grad_norm=3.4377291202545166, loss=1.4951099157333374
I0129 15:46:55.767565 139656834316032 logging_writer.py:48] [147100] global_step=147100, grad_norm=3.362988233566284, loss=1.5175375938415527
I0129 15:47:29.646572 139656666543872 logging_writer.py:48] [147200] global_step=147200, grad_norm=3.3208224773406982, loss=1.422644019126892
I0129 15:48:03.553878 139656834316032 logging_writer.py:48] [147300] global_step=147300, grad_norm=3.3183913230895996, loss=1.4979424476623535
I0129 15:48:14.900863 139822745589568 spec.py:321] Evaluating on the training split.
I0129 15:48:21.147943 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 15:48:29.581234 139822745589568 spec.py:349] Evaluating on the test split.
I0129 15:48:32.037810 139822745589568 submission_runner.py:408] Time since start: 51755.39s, 	Step: 147335, 	{'train/accuracy': 0.7348732352256775, 'train/loss': 1.0262718200683594, 'validation/accuracy': 0.6617599725723267, 'validation/loss': 1.3817018270492554, 'validation/num_examples': 50000, 'test/accuracy': 0.5372000336647034, 'test/loss': 2.097731828689575, 'test/num_examples': 10000, 'score': 50018.80451273918, 'total_duration': 51755.39443898201, 'accumulated_submission_time': 50018.80451273918, 'accumulated_eval_time': 1727.4768795967102, 'accumulated_logging_time': 4.0461931228637695}
I0129 15:48:32.090266 139656649758464 logging_writer.py:48] [147335] accumulated_eval_time=1727.476880, accumulated_logging_time=4.046193, accumulated_submission_time=50018.804513, global_step=147335, preemption_count=0, score=50018.804513, test/accuracy=0.537200, test/loss=2.097732, test/num_examples=10000, total_duration=51755.394439, train/accuracy=0.734873, train/loss=1.026272, validation/accuracy=0.661760, validation/loss=1.381702, validation/num_examples=50000
I0129 15:48:54.457320 139656658151168 logging_writer.py:48] [147400] global_step=147400, grad_norm=3.1491408348083496, loss=1.4332022666931152
I0129 15:49:28.326964 139656649758464 logging_writer.py:48] [147500] global_step=147500, grad_norm=3.6061158180236816, loss=1.355340838432312
I0129 15:50:02.223974 139656658151168 logging_writer.py:48] [147600] global_step=147600, grad_norm=3.534470796585083, loss=1.5630383491516113
I0129 15:50:36.174994 139656649758464 logging_writer.py:48] [147700] global_step=147700, grad_norm=3.403369188308716, loss=1.4892222881317139
I0129 15:51:10.072937 139656658151168 logging_writer.py:48] [147800] global_step=147800, grad_norm=3.183746814727783, loss=1.3633389472961426
I0129 15:51:43.966592 139656649758464 logging_writer.py:48] [147900] global_step=147900, grad_norm=3.37821626663208, loss=1.512785792350769
I0129 15:52:17.868742 139656658151168 logging_writer.py:48] [148000] global_step=148000, grad_norm=3.5955538749694824, loss=1.46864914894104
I0129 15:52:51.785183 139656649758464 logging_writer.py:48] [148100] global_step=148100, grad_norm=3.5435807704925537, loss=1.4892581701278687
I0129 15:53:25.704096 139656658151168 logging_writer.py:48] [148200] global_step=148200, grad_norm=3.465752363204956, loss=1.3750025033950806
I0129 15:53:59.619532 139656649758464 logging_writer.py:48] [148300] global_step=148300, grad_norm=3.3887012004852295, loss=1.4183727502822876
I0129 15:54:33.540453 139656658151168 logging_writer.py:48] [148400] global_step=148400, grad_norm=3.4243476390838623, loss=1.4440892934799194
I0129 15:55:07.440315 139656649758464 logging_writer.py:48] [148500] global_step=148500, grad_norm=3.5073468685150146, loss=1.4900236129760742
I0129 15:55:41.336193 139656658151168 logging_writer.py:48] [148600] global_step=148600, grad_norm=3.136749505996704, loss=1.430311679840088
I0129 15:56:15.201209 139656649758464 logging_writer.py:48] [148700] global_step=148700, grad_norm=3.5336756706237793, loss=1.4973318576812744
I0129 15:56:49.136753 139656658151168 logging_writer.py:48] [148800] global_step=148800, grad_norm=3.2822282314300537, loss=1.3607021570205688
I0129 15:57:02.141966 139822745589568 spec.py:321] Evaluating on the training split.
I0129 15:57:08.441958 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 15:57:16.835271 139822745589568 spec.py:349] Evaluating on the test split.
I0129 15:57:19.397908 139822745589568 submission_runner.py:408] Time since start: 52282.75s, 	Step: 148840, 	{'train/accuracy': 0.7347337007522583, 'train/loss': 1.017677903175354, 'validation/accuracy': 0.6627399921417236, 'validation/loss': 1.392814040184021, 'validation/num_examples': 50000, 'test/accuracy': 0.5409000515937805, 'test/loss': 2.0931143760681152, 'test/num_examples': 10000, 'score': 50528.794149160385, 'total_duration': 52282.75450348854, 'accumulated_submission_time': 50528.794149160385, 'accumulated_eval_time': 1744.73273396492, 'accumulated_logging_time': 4.108054876327515}
I0129 15:57:19.441243 139655617963776 logging_writer.py:48] [148840] accumulated_eval_time=1744.732734, accumulated_logging_time=4.108055, accumulated_submission_time=50528.794149, global_step=148840, preemption_count=0, score=50528.794149, test/accuracy=0.540900, test/loss=2.093114, test/num_examples=10000, total_duration=52282.754503, train/accuracy=0.734734, train/loss=1.017678, validation/accuracy=0.662740, validation/loss=1.392814, validation/num_examples=50000
I0129 15:57:40.086603 139655626356480 logging_writer.py:48] [148900] global_step=148900, grad_norm=3.586803674697876, loss=1.4647386074066162
I0129 15:58:13.935898 139655617963776 logging_writer.py:48] [149000] global_step=149000, grad_norm=3.460587978363037, loss=1.4338247776031494
I0129 15:58:47.809087 139655626356480 logging_writer.py:48] [149100] global_step=149100, grad_norm=3.8568027019500732, loss=1.4638776779174805
I0129 15:59:21.658944 139655617963776 logging_writer.py:48] [149200] global_step=149200, grad_norm=3.386491298675537, loss=1.461868166923523
I0129 15:59:55.508469 139655626356480 logging_writer.py:48] [149300] global_step=149300, grad_norm=3.540165662765503, loss=1.4720813035964966
I0129 16:00:29.378749 139655617963776 logging_writer.py:48] [149400] global_step=149400, grad_norm=3.2958292961120605, loss=1.3315980434417725
I0129 16:01:03.209196 139655626356480 logging_writer.py:48] [149500] global_step=149500, grad_norm=3.454238176345825, loss=1.4838850498199463
I0129 16:01:37.108446 139655617963776 logging_writer.py:48] [149600] global_step=149600, grad_norm=3.7034122943878174, loss=1.386551022529602
I0129 16:02:10.993086 139655626356480 logging_writer.py:48] [149700] global_step=149700, grad_norm=3.653430938720703, loss=1.4481569528579712
I0129 16:02:44.853192 139655617963776 logging_writer.py:48] [149800] global_step=149800, grad_norm=3.5816915035247803, loss=1.4105536937713623
I0129 16:03:18.724168 139655626356480 logging_writer.py:48] [149900] global_step=149900, grad_norm=3.078308343887329, loss=1.3636494874954224
I0129 16:03:52.679899 139655617963776 logging_writer.py:48] [150000] global_step=150000, grad_norm=3.3544204235076904, loss=1.3851593732833862
I0129 16:04:26.561116 139655626356480 logging_writer.py:48] [150100] global_step=150100, grad_norm=3.4199910163879395, loss=1.336537480354309
I0129 16:05:00.435856 139655617963776 logging_writer.py:48] [150200] global_step=150200, grad_norm=3.507441520690918, loss=1.4292848110198975
I0129 16:05:34.346098 139655626356480 logging_writer.py:48] [150300] global_step=150300, grad_norm=3.612234354019165, loss=1.4521656036376953
I0129 16:05:49.406160 139822745589568 spec.py:321] Evaluating on the training split.
I0129 16:05:55.560308 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 16:06:04.204582 139822745589568 spec.py:349] Evaluating on the test split.
I0129 16:06:06.812698 139822745589568 submission_runner.py:408] Time since start: 52810.17s, 	Step: 150346, 	{'train/accuracy': 0.7524314522743225, 'train/loss': 0.9449632167816162, 'validation/accuracy': 0.682159960269928, 'validation/loss': 1.306384563446045, 'validation/num_examples': 50000, 'test/accuracy': 0.556600034236908, 'test/loss': 2.0261521339416504, 'test/num_examples': 10000, 'score': 51038.696164131165, 'total_duration': 52810.16929721832, 'accumulated_submission_time': 51038.696164131165, 'accumulated_eval_time': 1762.1391913890839, 'accumulated_logging_time': 4.162874937057495}
I0129 16:06:06.862724 139655617963776 logging_writer.py:48] [150346] accumulated_eval_time=1762.139191, accumulated_logging_time=4.162875, accumulated_submission_time=51038.696164, global_step=150346, preemption_count=0, score=51038.696164, test/accuracy=0.556600, test/loss=2.026152, test/num_examples=10000, total_duration=52810.169297, train/accuracy=0.752431, train/loss=0.944963, validation/accuracy=0.682160, validation/loss=1.306385, validation/num_examples=50000
I0129 16:06:25.491616 139656649758464 logging_writer.py:48] [150400] global_step=150400, grad_norm=3.3088366985321045, loss=1.4081683158874512
I0129 16:06:59.338449 139655617963776 logging_writer.py:48] [150500] global_step=150500, grad_norm=3.4011425971984863, loss=1.4297597408294678
I0129 16:07:33.170278 139656649758464 logging_writer.py:48] [150600] global_step=150600, grad_norm=3.329989433288574, loss=1.3772046566009521
I0129 16:08:07.083973 139655617963776 logging_writer.py:48] [150700] global_step=150700, grad_norm=3.5450446605682373, loss=1.333364486694336
I0129 16:08:40.987290 139656649758464 logging_writer.py:48] [150800] global_step=150800, grad_norm=3.497297763824463, loss=1.3466477394104004
I0129 16:09:14.886795 139655617963776 logging_writer.py:48] [150900] global_step=150900, grad_norm=3.8815977573394775, loss=1.374233603477478
I0129 16:09:48.789717 139656649758464 logging_writer.py:48] [151000] global_step=151000, grad_norm=3.6614577770233154, loss=1.3714066743850708
I0129 16:10:22.755994 139655617963776 logging_writer.py:48] [151100] global_step=151100, grad_norm=3.602800130844116, loss=1.4123308658599854
I0129 16:10:56.637110 139656649758464 logging_writer.py:48] [151200] global_step=151200, grad_norm=3.648186683654785, loss=1.3865890502929688
I0129 16:11:30.531588 139655617963776 logging_writer.py:48] [151300] global_step=151300, grad_norm=3.8219151496887207, loss=1.436215877532959
I0129 16:12:04.427039 139656649758464 logging_writer.py:48] [151400] global_step=151400, grad_norm=3.8759334087371826, loss=1.3676278591156006
I0129 16:12:38.336182 139655617963776 logging_writer.py:48] [151500] global_step=151500, grad_norm=3.5912013053894043, loss=1.4081429243087769
I0129 16:13:12.267620 139656649758464 logging_writer.py:48] [151600] global_step=151600, grad_norm=3.5291013717651367, loss=1.3480074405670166
I0129 16:13:46.178450 139655617963776 logging_writer.py:48] [151700] global_step=151700, grad_norm=3.6701138019561768, loss=1.5053507089614868
I0129 16:14:20.036419 139656649758464 logging_writer.py:48] [151800] global_step=151800, grad_norm=3.5499260425567627, loss=1.4662301540374756
I0129 16:14:37.110162 139822745589568 spec.py:321] Evaluating on the training split.
I0129 16:14:43.298693 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 16:14:51.881512 139822745589568 spec.py:349] Evaluating on the test split.
I0129 16:14:54.485707 139822745589568 submission_runner.py:408] Time since start: 53337.84s, 	Step: 151852, 	{'train/accuracy': 0.7565967440605164, 'train/loss': 0.9240195155143738, 'validation/accuracy': 0.6843799948692322, 'validation/loss': 1.2921584844589233, 'validation/num_examples': 50000, 'test/accuracy': 0.5592000484466553, 'test/loss': 2.0278122425079346, 'test/num_examples': 10000, 'score': 51548.87925410271, 'total_duration': 53337.84234023094, 'accumulated_submission_time': 51548.87925410271, 'accumulated_eval_time': 1779.5146894454956, 'accumulated_logging_time': 4.226062297821045}
I0129 16:14:54.530462 139656666543872 logging_writer.py:48] [151852] accumulated_eval_time=1779.514689, accumulated_logging_time=4.226062, accumulated_submission_time=51548.879254, global_step=151852, preemption_count=0, score=51548.879254, test/accuracy=0.559200, test/loss=2.027812, test/num_examples=10000, total_duration=53337.842340, train/accuracy=0.756597, train/loss=0.924020, validation/accuracy=0.684380, validation/loss=1.292158, validation/num_examples=50000
I0129 16:15:11.114589 139658730145536 logging_writer.py:48] [151900] global_step=151900, grad_norm=3.555927038192749, loss=1.3554229736328125
I0129 16:15:44.949859 139656666543872 logging_writer.py:48] [152000] global_step=152000, grad_norm=3.5198631286621094, loss=1.283050298690796
I0129 16:16:18.823129 139658730145536 logging_writer.py:48] [152100] global_step=152100, grad_norm=3.764747381210327, loss=1.4511390924453735
I0129 16:16:52.812353 139656666543872 logging_writer.py:48] [152200] global_step=152200, grad_norm=3.4617695808410645, loss=1.4523857831954956
I0129 16:17:26.722697 139658730145536 logging_writer.py:48] [152300] global_step=152300, grad_norm=3.59529447555542, loss=1.3338769674301147
I0129 16:18:00.627191 139656666543872 logging_writer.py:48] [152400] global_step=152400, grad_norm=4.009929180145264, loss=1.4299180507659912
I0129 16:18:34.536437 139658730145536 logging_writer.py:48] [152500] global_step=152500, grad_norm=3.195646286010742, loss=1.2719966173171997
I0129 16:19:08.442558 139656666543872 logging_writer.py:48] [152600] global_step=152600, grad_norm=3.63712739944458, loss=1.3407962322235107
I0129 16:19:42.332096 139658730145536 logging_writer.py:48] [152700] global_step=152700, grad_norm=3.6192989349365234, loss=1.370449423789978
I0129 16:20:16.221992 139656666543872 logging_writer.py:48] [152800] global_step=152800, grad_norm=3.7687699794769287, loss=1.2300796508789062
I0129 16:20:50.125889 139658730145536 logging_writer.py:48] [152900] global_step=152900, grad_norm=3.5791380405426025, loss=1.3069401979446411
I0129 16:21:24.042943 139656666543872 logging_writer.py:48] [153000] global_step=153000, grad_norm=4.042111873626709, loss=1.4209266901016235
I0129 16:21:57.945470 139658730145536 logging_writer.py:48] [153100] global_step=153100, grad_norm=3.75425124168396, loss=1.31476628780365
I0129 16:22:31.844773 139656666543872 logging_writer.py:48] [153200] global_step=153200, grad_norm=3.477013349533081, loss=1.1916576623916626
I0129 16:23:05.788869 139658730145536 logging_writer.py:48] [153300] global_step=153300, grad_norm=3.6675403118133545, loss=1.3193703889846802
I0129 16:23:24.584545 139822745589568 spec.py:321] Evaluating on the training split.
I0129 16:23:30.765977 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 16:23:39.410544 139822745589568 spec.py:349] Evaluating on the test split.
I0129 16:23:41.968494 139822745589568 submission_runner.py:408] Time since start: 53865.33s, 	Step: 153357, 	{'train/accuracy': 0.7704081535339355, 'train/loss': 0.8769794702529907, 'validation/accuracy': 0.6804400086402893, 'validation/loss': 1.3156960010528564, 'validation/num_examples': 50000, 'test/accuracy': 0.5514000058174133, 'test/loss': 2.0436854362487793, 'test/num_examples': 10000, 'score': 52058.87175607681, 'total_duration': 53865.325132369995, 'accumulated_submission_time': 52058.87175607681, 'accumulated_eval_time': 1796.8985974788666, 'accumulated_logging_time': 4.281770467758179}
I0129 16:23:42.014427 139655626356480 logging_writer.py:48] [153357] accumulated_eval_time=1796.898597, accumulated_logging_time=4.281770, accumulated_submission_time=52058.871756, global_step=153357, preemption_count=0, score=52058.871756, test/accuracy=0.551400, test/loss=2.043685, test/num_examples=10000, total_duration=53865.325132, train/accuracy=0.770408, train/loss=0.876979, validation/accuracy=0.680440, validation/loss=1.315696, validation/num_examples=50000
I0129 16:23:56.895326 139656297445120 logging_writer.py:48] [153400] global_step=153400, grad_norm=3.866257905960083, loss=1.3637446165084839
I0129 16:24:30.759444 139655626356480 logging_writer.py:48] [153500] global_step=153500, grad_norm=3.7100813388824463, loss=1.434070348739624
I0129 16:25:04.637085 139656297445120 logging_writer.py:48] [153600] global_step=153600, grad_norm=4.058478355407715, loss=1.368025541305542
I0129 16:25:38.500722 139655626356480 logging_writer.py:48] [153700] global_step=153700, grad_norm=3.9177818298339844, loss=1.5243699550628662
I0129 16:26:12.413644 139656297445120 logging_writer.py:48] [153800] global_step=153800, grad_norm=3.6889212131500244, loss=1.3149018287658691
I0129 16:26:46.326071 139655626356480 logging_writer.py:48] [153900] global_step=153900, grad_norm=3.6261026859283447, loss=1.3142931461334229
I0129 16:27:20.230959 139656297445120 logging_writer.py:48] [154000] global_step=154000, grad_norm=4.147096633911133, loss=1.3946186304092407
I0129 16:27:54.133277 139655626356480 logging_writer.py:48] [154100] global_step=154100, grad_norm=3.784334182739258, loss=1.2921282052993774
I0129 16:28:28.046684 139656297445120 logging_writer.py:48] [154200] global_step=154200, grad_norm=3.979513168334961, loss=1.3659485578536987
I0129 16:29:01.957891 139655626356480 logging_writer.py:48] [154300] global_step=154300, grad_norm=3.6569275856018066, loss=1.3267450332641602
I0129 16:29:35.929904 139656297445120 logging_writer.py:48] [154400] global_step=154400, grad_norm=4.092653274536133, loss=1.3481711149215698
I0129 16:30:09.780938 139655626356480 logging_writer.py:48] [154500] global_step=154500, grad_norm=3.74102520942688, loss=1.3794713020324707
I0129 16:30:43.647351 139656297445120 logging_writer.py:48] [154600] global_step=154600, grad_norm=3.6422529220581055, loss=1.4008761644363403
I0129 16:31:17.557953 139655626356480 logging_writer.py:48] [154700] global_step=154700, grad_norm=3.668553352355957, loss=1.280096411705017
I0129 16:31:51.422160 139656297445120 logging_writer.py:48] [154800] global_step=154800, grad_norm=3.517815589904785, loss=1.3471434116363525
I0129 16:32:12.231115 139822745589568 spec.py:321] Evaluating on the training split.
I0129 16:32:18.388549 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 16:32:27.073825 139822745589568 spec.py:349] Evaluating on the test split.
I0129 16:32:29.673649 139822745589568 submission_runner.py:408] Time since start: 54393.03s, 	Step: 154863, 	{'train/accuracy': 0.7912148833274841, 'train/loss': 0.7727125287055969, 'validation/accuracy': 0.6976999640464783, 'validation/loss': 1.230203628540039, 'validation/num_examples': 50000, 'test/accuracy': 0.5733000040054321, 'test/loss': 1.9361377954483032, 'test/num_examples': 10000, 'score': 52569.02870512009, 'total_duration': 54393.03028893471, 'accumulated_submission_time': 52569.02870512009, 'accumulated_eval_time': 1814.3410923480988, 'accumulated_logging_time': 4.336676836013794}
I0129 16:32:29.724152 139655617963776 logging_writer.py:48] [154863] accumulated_eval_time=1814.341092, accumulated_logging_time=4.336677, accumulated_submission_time=52569.028705, global_step=154863, preemption_count=0, score=52569.028705, test/accuracy=0.573300, test/loss=1.936138, test/num_examples=10000, total_duration=54393.030289, train/accuracy=0.791215, train/loss=0.772713, validation/accuracy=0.697700, validation/loss=1.230204, validation/num_examples=50000
I0129 16:32:42.618622 139656658151168 logging_writer.py:48] [154900] global_step=154900, grad_norm=3.683913469314575, loss=1.3068783283233643
I0129 16:33:16.390710 139655617963776 logging_writer.py:48] [155000] global_step=155000, grad_norm=3.8441176414489746, loss=1.41921067237854
I0129 16:33:50.222126 139656658151168 logging_writer.py:48] [155100] global_step=155100, grad_norm=3.887995719909668, loss=1.327655553817749
I0129 16:34:24.116009 139655617963776 logging_writer.py:48] [155200] global_step=155200, grad_norm=4.031177520751953, loss=1.2799272537231445
I0129 16:34:57.986878 139656658151168 logging_writer.py:48] [155300] global_step=155300, grad_norm=4.497232913970947, loss=1.4178931713104248
I0129 16:35:31.885501 139655617963776 logging_writer.py:48] [155400] global_step=155400, grad_norm=4.199435234069824, loss=1.3982534408569336
I0129 16:36:05.874615 139656658151168 logging_writer.py:48] [155500] global_step=155500, grad_norm=4.2353410720825195, loss=1.2727354764938354
I0129 16:36:39.760550 139655617963776 logging_writer.py:48] [155600] global_step=155600, grad_norm=4.1117072105407715, loss=1.3466567993164062
I0129 16:37:13.620974 139656658151168 logging_writer.py:48] [155700] global_step=155700, grad_norm=3.864697217941284, loss=1.268661379814148
I0129 16:37:47.520028 139655617963776 logging_writer.py:48] [155800] global_step=155800, grad_norm=3.7874791622161865, loss=1.221897840499878
I0129 16:38:21.412599 139656658151168 logging_writer.py:48] [155900] global_step=155900, grad_norm=3.877180576324463, loss=1.3724356889724731
I0129 16:38:55.278517 139655617963776 logging_writer.py:48] [156000] global_step=156000, grad_norm=3.723115921020508, loss=1.3331935405731201
I0129 16:39:29.160769 139656658151168 logging_writer.py:48] [156100] global_step=156100, grad_norm=4.159875392913818, loss=1.2630113363265991
I0129 16:40:03.066593 139655617963776 logging_writer.py:48] [156200] global_step=156200, grad_norm=4.057272434234619, loss=1.3576843738555908
I0129 16:40:36.947868 139656658151168 logging_writer.py:48] [156300] global_step=156300, grad_norm=4.141096591949463, loss=1.3969181776046753
I0129 16:40:59.826435 139822745589568 spec.py:321] Evaluating on the training split.
I0129 16:41:06.159262 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 16:41:14.690899 139822745589568 spec.py:349] Evaluating on the test split.
I0129 16:41:17.188441 139822745589568 submission_runner.py:408] Time since start: 54920.55s, 	Step: 156369, 	{'train/accuracy': 0.7887436151504517, 'train/loss': 0.7775185704231262, 'validation/accuracy': 0.6951199769973755, 'validation/loss': 1.2335890531539917, 'validation/num_examples': 50000, 'test/accuracy': 0.5689000487327576, 'test/loss': 1.9560490846633911, 'test/num_examples': 10000, 'score': 53079.0686750412, 'total_duration': 54920.545063734055, 'accumulated_submission_time': 53079.0686750412, 'accumulated_eval_time': 1831.7030427455902, 'accumulated_logging_time': 4.396013021469116}
I0129 16:41:17.233679 139656649758464 logging_writer.py:48] [156369] accumulated_eval_time=1831.703043, accumulated_logging_time=4.396013, accumulated_submission_time=53079.068675, global_step=156369, preemption_count=0, score=53079.068675, test/accuracy=0.568900, test/loss=1.956049, test/num_examples=10000, total_duration=54920.545064, train/accuracy=0.788744, train/loss=0.777519, validation/accuracy=0.695120, validation/loss=1.233589, validation/num_examples=50000
I0129 16:41:28.074280 139658730145536 logging_writer.py:48] [156400] global_step=156400, grad_norm=4.000057220458984, loss=1.2640657424926758
I0129 16:42:01.910283 139656649758464 logging_writer.py:48] [156500] global_step=156500, grad_norm=3.8086674213409424, loss=1.202988862991333
I0129 16:42:35.903746 139658730145536 logging_writer.py:48] [156600] global_step=156600, grad_norm=3.9008352756500244, loss=1.2443654537200928
I0129 16:43:09.783535 139656649758464 logging_writer.py:48] [156700] global_step=156700, grad_norm=3.8264429569244385, loss=1.3447426557540894
I0129 16:43:43.689214 139658730145536 logging_writer.py:48] [156800] global_step=156800, grad_norm=3.9335553646087646, loss=1.2111235857009888
I0129 16:44:17.551681 139656649758464 logging_writer.py:48] [156900] global_step=156900, grad_norm=3.6270499229431152, loss=1.2447701692581177
I0129 16:44:51.451744 139658730145536 logging_writer.py:48] [157000] global_step=157000, grad_norm=3.7400591373443604, loss=1.1650694608688354
I0129 16:45:25.324279 139656649758464 logging_writer.py:48] [157100] global_step=157100, grad_norm=3.88014817237854, loss=1.3139768838882446
I0129 16:45:59.213291 139658730145536 logging_writer.py:48] [157200] global_step=157200, grad_norm=3.8347177505493164, loss=1.3155561685562134
I0129 16:46:33.100639 139656649758464 logging_writer.py:48] [157300] global_step=157300, grad_norm=3.823011875152588, loss=1.1492427587509155
I0129 16:47:06.981882 139658730145536 logging_writer.py:48] [157400] global_step=157400, grad_norm=4.2450361251831055, loss=1.3218351602554321
I0129 16:47:40.849175 139656649758464 logging_writer.py:48] [157500] global_step=157500, grad_norm=4.129356384277344, loss=1.2094374895095825
I0129 16:48:14.738658 139658730145536 logging_writer.py:48] [157600] global_step=157600, grad_norm=4.172128200531006, loss=1.2416491508483887
I0129 16:48:48.623645 139656649758464 logging_writer.py:48] [157700] global_step=157700, grad_norm=3.859527587890625, loss=1.2018717527389526
I0129 16:49:22.574490 139658730145536 logging_writer.py:48] [157800] global_step=157800, grad_norm=3.9514756202697754, loss=1.232457160949707
I0129 16:49:47.479670 139822745589568 spec.py:321] Evaluating on the training split.
I0129 16:49:53.663849 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 16:50:02.291942 139822745589568 spec.py:349] Evaluating on the test split.
I0129 16:50:04.856565 139822745589568 submission_runner.py:408] Time since start: 55448.21s, 	Step: 157875, 	{'train/accuracy': 0.7881656289100647, 'train/loss': 0.7825496196746826, 'validation/accuracy': 0.6997399926185608, 'validation/loss': 1.2260318994522095, 'validation/num_examples': 50000, 'test/accuracy': 0.5729000568389893, 'test/loss': 1.9439477920532227, 'test/num_examples': 10000, 'score': 53589.254455804825, 'total_duration': 55448.21320796013, 'accumulated_submission_time': 53589.254455804825, 'accumulated_eval_time': 1849.0799005031586, 'accumulated_logging_time': 4.450916528701782}
I0129 16:50:04.901252 139655626356480 logging_writer.py:48] [157875] accumulated_eval_time=1849.079901, accumulated_logging_time=4.450917, accumulated_submission_time=53589.254456, global_step=157875, preemption_count=0, score=53589.254456, test/accuracy=0.572900, test/loss=1.943948, test/num_examples=10000, total_duration=55448.213208, train/accuracy=0.788166, train/loss=0.782550, validation/accuracy=0.699740, validation/loss=1.226032, validation/num_examples=50000
I0129 16:50:13.728075 139656297445120 logging_writer.py:48] [157900] global_step=157900, grad_norm=4.060420036315918, loss=1.309256911277771
I0129 16:50:47.560785 139655626356480 logging_writer.py:48] [158000] global_step=158000, grad_norm=4.129710674285889, loss=1.2610571384429932
I0129 16:51:21.396676 139656297445120 logging_writer.py:48] [158100] global_step=158100, grad_norm=4.079006195068359, loss=1.335415005683899
I0129 16:51:55.241174 139655626356480 logging_writer.py:48] [158200] global_step=158200, grad_norm=3.991748571395874, loss=1.2514375448226929
I0129 16:52:29.130580 139656297445120 logging_writer.py:48] [158300] global_step=158300, grad_norm=4.4081645011901855, loss=1.380955457687378
I0129 16:53:03.020603 139655626356480 logging_writer.py:48] [158400] global_step=158400, grad_norm=4.379570007324219, loss=1.3831303119659424
I0129 16:53:36.858490 139656297445120 logging_writer.py:48] [158500] global_step=158500, grad_norm=4.0336127281188965, loss=1.2638555765151978
I0129 16:54:10.706899 139655626356480 logging_writer.py:48] [158600] global_step=158600, grad_norm=4.065955638885498, loss=1.1958963871002197
I0129 16:54:44.548788 139656297445120 logging_writer.py:48] [158700] global_step=158700, grad_norm=4.101681232452393, loss=1.3184425830841064
I0129 16:55:18.419914 139655626356480 logging_writer.py:48] [158800] global_step=158800, grad_norm=3.8639352321624756, loss=1.3068485260009766
I0129 16:55:52.383922 139656297445120 logging_writer.py:48] [158900] global_step=158900, grad_norm=4.206316947937012, loss=1.2339476346969604
I0129 16:56:26.284356 139655626356480 logging_writer.py:48] [159000] global_step=159000, grad_norm=4.54353666305542, loss=1.323737621307373
I0129 16:57:00.158986 139656297445120 logging_writer.py:48] [159100] global_step=159100, grad_norm=4.087698936462402, loss=1.3124810457229614
I0129 16:57:34.054292 139655626356480 logging_writer.py:48] [159200] global_step=159200, grad_norm=4.130446434020996, loss=1.2604342699050903
I0129 16:58:07.957433 139656297445120 logging_writer.py:48] [159300] global_step=159300, grad_norm=4.390053749084473, loss=1.2738362550735474
I0129 16:58:35.198431 139822745589568 spec.py:321] Evaluating on the training split.
I0129 16:58:41.414907 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 16:58:49.814887 139822745589568 spec.py:349] Evaluating on the test split.
I0129 16:58:52.422056 139822745589568 submission_runner.py:408] Time since start: 55975.78s, 	Step: 159382, 	{'train/accuracy': 0.7869299650192261, 'train/loss': 0.7920331358909607, 'validation/accuracy': 0.7021799683570862, 'validation/loss': 1.2132388353347778, 'validation/num_examples': 50000, 'test/accuracy': 0.5769000053405762, 'test/loss': 1.903723120689392, 'test/num_examples': 10000, 'score': 54099.491090774536, 'total_duration': 55975.7786552906, 'accumulated_submission_time': 54099.491090774536, 'accumulated_eval_time': 1866.303447008133, 'accumulated_logging_time': 4.505049228668213}
I0129 16:58:52.471307 139656649758464 logging_writer.py:48] [159382] accumulated_eval_time=1866.303447, accumulated_logging_time=4.505049, accumulated_submission_time=54099.491091, global_step=159382, preemption_count=0, score=54099.491091, test/accuracy=0.576900, test/loss=1.903723, test/num_examples=10000, total_duration=55975.778655, train/accuracy=0.786930, train/loss=0.792033, validation/accuracy=0.702180, validation/loss=1.213239, validation/num_examples=50000
I0129 16:58:58.921190 139656834316032 logging_writer.py:48] [159400] global_step=159400, grad_norm=4.120102882385254, loss=1.4022974967956543
I0129 16:59:32.750936 139656649758464 logging_writer.py:48] [159500] global_step=159500, grad_norm=4.170653343200684, loss=1.2645305395126343
I0129 17:00:06.613383 139656834316032 logging_writer.py:48] [159600] global_step=159600, grad_norm=4.173917770385742, loss=1.192686676979065
I0129 17:00:40.483164 139656649758464 logging_writer.py:48] [159700] global_step=159700, grad_norm=4.210713863372803, loss=1.2018115520477295
I0129 17:01:14.421504 139656834316032 logging_writer.py:48] [159800] global_step=159800, grad_norm=4.264460563659668, loss=1.230090618133545
I0129 17:01:48.330341 139656649758464 logging_writer.py:48] [159900] global_step=159900, grad_norm=4.2469706535339355, loss=1.2834032773971558
I0129 17:02:22.291352 139656834316032 logging_writer.py:48] [160000] global_step=160000, grad_norm=4.185673713684082, loss=1.1572439670562744
I0129 17:02:56.179940 139656649758464 logging_writer.py:48] [160100] global_step=160100, grad_norm=3.9640400409698486, loss=1.2689597606658936
I0129 17:03:30.077301 139656834316032 logging_writer.py:48] [160200] global_step=160200, grad_norm=4.180370330810547, loss=1.3286044597625732
I0129 17:04:03.956484 139656649758464 logging_writer.py:48] [160300] global_step=160300, grad_norm=4.45399808883667, loss=1.3235551118850708
I0129 17:04:37.891369 139656834316032 logging_writer.py:48] [160400] global_step=160400, grad_norm=4.455595970153809, loss=1.2662383317947388
I0129 17:05:11.793264 139656649758464 logging_writer.py:48] [160500] global_step=160500, grad_norm=4.385241508483887, loss=1.2648558616638184
I0129 17:05:45.692913 139656834316032 logging_writer.py:48] [160600] global_step=160600, grad_norm=4.37529993057251, loss=1.334562063217163
I0129 17:06:19.579330 139656649758464 logging_writer.py:48] [160700] global_step=160700, grad_norm=4.441361904144287, loss=1.2999194860458374
I0129 17:06:53.459866 139656834316032 logging_writer.py:48] [160800] global_step=160800, grad_norm=4.321473121643066, loss=1.3103768825531006
I0129 17:07:22.758859 139822745589568 spec.py:321] Evaluating on the training split.
I0129 17:07:29.002524 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 17:07:37.689192 139822745589568 spec.py:349] Evaluating on the test split.
I0129 17:07:40.238852 139822745589568 submission_runner.py:408] Time since start: 56503.60s, 	Step: 160888, 	{'train/accuracy': 0.8048867583274841, 'train/loss': 0.7252900004386902, 'validation/accuracy': 0.7111799716949463, 'validation/loss': 1.1790040731430054, 'validation/num_examples': 50000, 'test/accuracy': 0.5863000154495239, 'test/loss': 1.8755098581314087, 'test/num_examples': 10000, 'score': 54609.7164978981, 'total_duration': 56503.595430374146, 'accumulated_submission_time': 54609.7164978981, 'accumulated_eval_time': 1883.7833399772644, 'accumulated_logging_time': 4.564141511917114}
I0129 17:07:40.288179 139655626356480 logging_writer.py:48] [160888] accumulated_eval_time=1883.783340, accumulated_logging_time=4.564142, accumulated_submission_time=54609.716498, global_step=160888, preemption_count=0, score=54609.716498, test/accuracy=0.586300, test/loss=1.875510, test/num_examples=10000, total_duration=56503.595430, train/accuracy=0.804887, train/loss=0.725290, validation/accuracy=0.711180, validation/loss=1.179004, validation/num_examples=50000
I0129 17:07:44.696516 139656297445120 logging_writer.py:48] [160900] global_step=160900, grad_norm=4.010364055633545, loss=1.2728761434555054
I0129 17:08:18.555239 139655626356480 logging_writer.py:48] [161000] global_step=161000, grad_norm=4.370872974395752, loss=1.2033360004425049
I0129 17:08:52.507004 139656297445120 logging_writer.py:48] [161100] global_step=161100, grad_norm=4.109890460968018, loss=1.2165104150772095
I0129 17:09:26.389355 139655626356480 logging_writer.py:48] [161200] global_step=161200, grad_norm=4.254268169403076, loss=1.2134500741958618
I0129 17:10:00.305840 139656297445120 logging_writer.py:48] [161300] global_step=161300, grad_norm=4.249337196350098, loss=1.3229711055755615
I0129 17:10:34.195086 139655626356480 logging_writer.py:48] [161400] global_step=161400, grad_norm=4.204646110534668, loss=1.1626484394073486
I0129 17:11:08.098622 139656297445120 logging_writer.py:48] [161500] global_step=161500, grad_norm=4.790111064910889, loss=1.3049726486206055
I0129 17:11:42.024297 139655626356480 logging_writer.py:48] [161600] global_step=161600, grad_norm=4.336973667144775, loss=1.2110944986343384
I0129 17:12:15.950562 139656297445120 logging_writer.py:48] [161700] global_step=161700, grad_norm=4.235386848449707, loss=1.0994573831558228
I0129 17:12:49.868049 139655626356480 logging_writer.py:48] [161800] global_step=161800, grad_norm=4.0037922859191895, loss=1.148184061050415
I0129 17:13:23.790544 139656297445120 logging_writer.py:48] [161900] global_step=161900, grad_norm=4.069725036621094, loss=1.1459388732910156
I0129 17:13:57.683782 139655626356480 logging_writer.py:48] [162000] global_step=162000, grad_norm=4.4255757331848145, loss=1.2779685258865356
I0129 17:14:31.604695 139656297445120 logging_writer.py:48] [162100] global_step=162100, grad_norm=4.353476047515869, loss=1.2055543661117554
I0129 17:15:05.582772 139655626356480 logging_writer.py:48] [162200] global_step=162200, grad_norm=4.526634216308594, loss=1.2508865594863892
I0129 17:15:39.444419 139656297445120 logging_writer.py:48] [162300] global_step=162300, grad_norm=4.409247398376465, loss=1.3130128383636475
I0129 17:16:10.418245 139822745589568 spec.py:321] Evaluating on the training split.
I0129 17:16:16.639529 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 17:16:25.110418 139822745589568 spec.py:349] Evaluating on the test split.
I0129 17:16:27.591098 139822745589568 submission_runner.py:408] Time since start: 57030.95s, 	Step: 162393, 	{'train/accuracy': 0.8147919178009033, 'train/loss': 0.689312756061554, 'validation/accuracy': 0.7153199911117554, 'validation/loss': 1.1588680744171143, 'validation/num_examples': 50000, 'test/accuracy': 0.5861999988555908, 'test/loss': 1.867242455482483, 'test/num_examples': 10000, 'score': 55119.78022289276, 'total_duration': 57030.947724580765, 'accumulated_submission_time': 55119.78022289276, 'accumulated_eval_time': 1900.9561693668365, 'accumulated_logging_time': 4.627662658691406}
I0129 17:16:27.642509 139656666543872 logging_writer.py:48] [162393] accumulated_eval_time=1900.956169, accumulated_logging_time=4.627663, accumulated_submission_time=55119.780223, global_step=162393, preemption_count=0, score=55119.780223, test/accuracy=0.586200, test/loss=1.867242, test/num_examples=10000, total_duration=57030.947725, train/accuracy=0.814792, train/loss=0.689313, validation/accuracy=0.715320, validation/loss=1.158868, validation/num_examples=50000
I0129 17:16:30.369580 139656834316032 logging_writer.py:48] [162400] global_step=162400, grad_norm=4.350412845611572, loss=1.1597446203231812
I0129 17:17:04.209442 139656666543872 logging_writer.py:48] [162500] global_step=162500, grad_norm=4.501352787017822, loss=1.2675782442092896
I0129 17:17:38.097299 139656834316032 logging_writer.py:48] [162600] global_step=162600, grad_norm=4.33756160736084, loss=1.1525323390960693
I0129 17:18:12.005373 139656666543872 logging_writer.py:48] [162700] global_step=162700, grad_norm=4.583900451660156, loss=1.209944486618042
I0129 17:18:45.929757 139656834316032 logging_writer.py:48] [162800] global_step=162800, grad_norm=4.74729061126709, loss=1.240333080291748
I0129 17:19:19.839560 139656666543872 logging_writer.py:48] [162900] global_step=162900, grad_norm=4.503149032592773, loss=1.1404623985290527
I0129 17:19:53.733735 139656834316032 logging_writer.py:48] [163000] global_step=163000, grad_norm=4.321873664855957, loss=1.1614234447479248
I0129 17:20:27.649405 139656666543872 logging_writer.py:48] [163100] global_step=163100, grad_norm=4.6853766441345215, loss=1.228578805923462
I0129 17:21:01.539192 139656834316032 logging_writer.py:48] [163200] global_step=163200, grad_norm=4.444393634796143, loss=1.2168272733688354
I0129 17:21:35.532054 139656666543872 logging_writer.py:48] [163300] global_step=163300, grad_norm=4.490057468414307, loss=1.2651960849761963
I0129 17:22:09.430172 139656834316032 logging_writer.py:48] [163400] global_step=163400, grad_norm=4.292387962341309, loss=1.1856223344802856
I0129 17:22:43.303651 139656666543872 logging_writer.py:48] [163500] global_step=163500, grad_norm=4.541728496551514, loss=1.1692804098129272
I0129 17:23:17.211226 139656834316032 logging_writer.py:48] [163600] global_step=163600, grad_norm=4.522937774658203, loss=1.170353651046753
I0129 17:23:51.119340 139656666543872 logging_writer.py:48] [163700] global_step=163700, grad_norm=4.355105400085449, loss=1.171558141708374
I0129 17:24:25.035803 139656834316032 logging_writer.py:48] [163800] global_step=163800, grad_norm=4.164149284362793, loss=1.079338550567627
I0129 17:24:57.730944 139822745589568 spec.py:321] Evaluating on the training split.
I0129 17:25:03.975001 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 17:25:12.439442 139822745589568 spec.py:349] Evaluating on the test split.
I0129 17:25:15.000042 139822745589568 submission_runner.py:408] Time since start: 57558.36s, 	Step: 163898, 	{'train/accuracy': 0.8363161683082581, 'train/loss': 0.6028015613555908, 'validation/accuracy': 0.7212600111961365, 'validation/loss': 1.1305629014968872, 'validation/num_examples': 50000, 'test/accuracy': 0.5986000299453735, 'test/loss': 1.7955706119537354, 'test/num_examples': 10000, 'score': 55629.80427956581, 'total_duration': 57558.35658311844, 'accumulated_submission_time': 55629.80427956581, 'accumulated_eval_time': 1918.2251298427582, 'accumulated_logging_time': 4.691629648208618}
I0129 17:25:15.048446 139656297445120 logging_writer.py:48] [163898] accumulated_eval_time=1918.225130, accumulated_logging_time=4.691630, accumulated_submission_time=55629.804280, global_step=163898, preemption_count=0, score=55629.804280, test/accuracy=0.598600, test/loss=1.795571, test/num_examples=10000, total_duration=57558.356583, train/accuracy=0.836316, train/loss=0.602802, validation/accuracy=0.721260, validation/loss=1.130563, validation/num_examples=50000
I0129 17:25:16.072461 139656649758464 logging_writer.py:48] [163900] global_step=163900, grad_norm=4.270225524902344, loss=1.1739861965179443
I0129 17:25:49.959975 139656297445120 logging_writer.py:48] [164000] global_step=164000, grad_norm=4.580589294433594, loss=1.201256275177002
I0129 17:26:23.855579 139656649758464 logging_writer.py:48] [164100] global_step=164100, grad_norm=4.541589260101318, loss=1.1312133073806763
I0129 17:26:57.762495 139656297445120 logging_writer.py:48] [164200] global_step=164200, grad_norm=4.625701427459717, loss=1.1241440773010254
I0129 17:27:31.675418 139656649758464 logging_writer.py:48] [164300] global_step=164300, grad_norm=4.7112717628479, loss=1.2196745872497559
I0129 17:28:05.660507 139656297445120 logging_writer.py:48] [164400] global_step=164400, grad_norm=4.717089653015137, loss=1.288797378540039
I0129 17:28:39.571660 139656649758464 logging_writer.py:48] [164500] global_step=164500, grad_norm=4.758688926696777, loss=1.1490927934646606
I0129 17:29:13.484402 139656297445120 logging_writer.py:48] [164600] global_step=164600, grad_norm=4.401178359985352, loss=1.2031376361846924
I0129 17:29:47.415636 139656649758464 logging_writer.py:48] [164700] global_step=164700, grad_norm=4.674456596374512, loss=1.1864542961120605
I0129 17:30:21.343508 139656297445120 logging_writer.py:48] [164800] global_step=164800, grad_norm=4.621377468109131, loss=1.1981273889541626
I0129 17:30:55.251831 139656649758464 logging_writer.py:48] [164900] global_step=164900, grad_norm=4.926095008850098, loss=1.2174040079116821
I0129 17:31:29.165984 139656297445120 logging_writer.py:48] [165000] global_step=165000, grad_norm=4.7018938064575195, loss=1.1383028030395508
I0129 17:32:03.084190 139656649758464 logging_writer.py:48] [165100] global_step=165100, grad_norm=4.277561187744141, loss=1.1249488592147827
I0129 17:32:36.991991 139656297445120 logging_writer.py:48] [165200] global_step=165200, grad_norm=4.851039886474609, loss=1.1619422435760498
I0129 17:33:10.903560 139656649758464 logging_writer.py:48] [165300] global_step=165300, grad_norm=4.638232707977295, loss=1.1410051584243774
I0129 17:33:44.810786 139656297445120 logging_writer.py:48] [165400] global_step=165400, grad_norm=4.958128452301025, loss=1.1308884620666504
I0129 17:33:45.302341 139822745589568 spec.py:321] Evaluating on the training split.
I0129 17:33:51.513623 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 17:33:59.889617 139822745589568 spec.py:349] Evaluating on the test split.
I0129 17:34:02.456856 139822745589568 submission_runner.py:408] Time since start: 58085.81s, 	Step: 165403, 	{'train/accuracy': 0.8367944955825806, 'train/loss': 0.594135046005249, 'validation/accuracy': 0.7243399620056152, 'validation/loss': 1.119240164756775, 'validation/num_examples': 50000, 'test/accuracy': 0.6026000380516052, 'test/loss': 1.800337553024292, 'test/num_examples': 10000, 'score': 56139.99762535095, 'total_duration': 58085.81350302696, 'accumulated_submission_time': 56139.99762535095, 'accumulated_eval_time': 1935.3796126842499, 'accumulated_logging_time': 4.7491774559021}
I0129 17:34:02.504086 139655617963776 logging_writer.py:48] [165403] accumulated_eval_time=1935.379613, accumulated_logging_time=4.749177, accumulated_submission_time=56139.997625, global_step=165403, preemption_count=0, score=56139.997625, test/accuracy=0.602600, test/loss=1.800338, test/num_examples=10000, total_duration=58085.813503, train/accuracy=0.836794, train/loss=0.594135, validation/accuracy=0.724340, validation/loss=1.119240, validation/num_examples=50000
I0129 17:34:35.806426 139655626356480 logging_writer.py:48] [165500] global_step=165500, grad_norm=4.4934492111206055, loss=1.1523680686950684
I0129 17:35:09.698719 139655617963776 logging_writer.py:48] [165600] global_step=165600, grad_norm=4.471798419952393, loss=1.1554582118988037
I0129 17:35:43.610812 139655626356480 logging_writer.py:48] [165700] global_step=165700, grad_norm=4.658456325531006, loss=1.1406198740005493
I0129 17:36:17.546379 139655617963776 logging_writer.py:48] [165800] global_step=165800, grad_norm=4.669437885284424, loss=1.1713262796401978
I0129 17:36:51.477098 139655626356480 logging_writer.py:48] [165900] global_step=165900, grad_norm=4.263960838317871, loss=1.1339937448501587
I0129 17:37:25.378718 139655617963776 logging_writer.py:48] [166000] global_step=166000, grad_norm=4.518223762512207, loss=1.1549711227416992
I0129 17:37:59.291952 139655626356480 logging_writer.py:48] [166100] global_step=166100, grad_norm=4.249393939971924, loss=1.0899890661239624
I0129 17:38:33.193657 139655617963776 logging_writer.py:48] [166200] global_step=166200, grad_norm=4.410006999969482, loss=1.0968526601791382
I0129 17:39:07.102120 139655626356480 logging_writer.py:48] [166300] global_step=166300, grad_norm=4.730375289916992, loss=1.1741811037063599
I0129 17:39:40.997653 139655617963776 logging_writer.py:48] [166400] global_step=166400, grad_norm=4.66839075088501, loss=1.2083641290664673
I0129 17:40:14.925791 139655626356480 logging_writer.py:48] [166500] global_step=166500, grad_norm=4.362589359283447, loss=1.041811466217041
I0129 17:40:48.856772 139655617963776 logging_writer.py:48] [166600] global_step=166600, grad_norm=4.603213310241699, loss=1.0719298124313354
I0129 17:41:22.853667 139655626356480 logging_writer.py:48] [166700] global_step=166700, grad_norm=4.684197425842285, loss=1.233544111251831
I0129 17:41:56.760905 139655617963776 logging_writer.py:48] [166800] global_step=166800, grad_norm=4.722502708435059, loss=1.0872262716293335
I0129 17:42:30.678598 139655626356480 logging_writer.py:48] [166900] global_step=166900, grad_norm=4.8099541664123535, loss=1.1350826025009155
I0129 17:42:32.535344 139822745589568 spec.py:321] Evaluating on the training split.
I0129 17:42:38.808091 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 17:42:47.226404 139822745589568 spec.py:349] Evaluating on the test split.
I0129 17:42:49.947055 139822745589568 submission_runner.py:408] Time since start: 58613.30s, 	Step: 166907, 	{'train/accuracy': 0.8360371589660645, 'train/loss': 0.5923864245414734, 'validation/accuracy': 0.7259199619293213, 'validation/loss': 1.1185106039047241, 'validation/num_examples': 50000, 'test/accuracy': 0.6011000275611877, 'test/loss': 1.818732500076294, 'test/num_examples': 10000, 'score': 56649.969014406204, 'total_duration': 58613.30370092392, 'accumulated_submission_time': 56649.969014406204, 'accumulated_eval_time': 1952.7912888526917, 'accumulated_logging_time': 4.805515766143799}
I0129 17:42:49.999031 139656649758464 logging_writer.py:48] [166907] accumulated_eval_time=1952.791289, accumulated_logging_time=4.805516, accumulated_submission_time=56649.969014, global_step=166907, preemption_count=0, score=56649.969014, test/accuracy=0.601100, test/loss=1.818733, test/num_examples=10000, total_duration=58613.303701, train/accuracy=0.836037, train/loss=0.592386, validation/accuracy=0.725920, validation/loss=1.118511, validation/num_examples=50000
I0129 17:43:21.822796 139656666543872 logging_writer.py:48] [167000] global_step=167000, grad_norm=4.499627113342285, loss=1.1344614028930664
I0129 17:43:55.647037 139656649758464 logging_writer.py:48] [167100] global_step=167100, grad_norm=5.099942207336426, loss=1.2120375633239746
I0129 17:44:29.530004 139656666543872 logging_writer.py:48] [167200] global_step=167200, grad_norm=4.514586925506592, loss=1.0656646490097046
I0129 17:45:03.442120 139656649758464 logging_writer.py:48] [167300] global_step=167300, grad_norm=4.478695869445801, loss=1.1162440776824951
I0129 17:45:37.377838 139656666543872 logging_writer.py:48] [167400] global_step=167400, grad_norm=4.607968807220459, loss=1.104373574256897
I0129 17:46:11.282514 139656649758464 logging_writer.py:48] [167500] global_step=167500, grad_norm=4.731710433959961, loss=1.0973670482635498
I0129 17:46:45.217853 139656666543872 logging_writer.py:48] [167600] global_step=167600, grad_norm=4.662734508514404, loss=1.117305040359497
I0129 17:47:19.124799 139656649758464 logging_writer.py:48] [167700] global_step=167700, grad_norm=5.264724254608154, loss=1.1247962713241577
I0129 17:47:53.110015 139656666543872 logging_writer.py:48] [167800] global_step=167800, grad_norm=4.99341344833374, loss=1.160814642906189
I0129 17:48:27.004824 139656649758464 logging_writer.py:48] [167900] global_step=167900, grad_norm=5.140703201293945, loss=1.1087933778762817
I0129 17:49:00.911999 139656666543872 logging_writer.py:48] [168000] global_step=168000, grad_norm=4.720300674438477, loss=1.0393612384796143
I0129 17:49:34.800898 139656649758464 logging_writer.py:48] [168100] global_step=168100, grad_norm=5.3652544021606445, loss=1.133612871170044
I0129 17:50:08.712379 139656666543872 logging_writer.py:48] [168200] global_step=168200, grad_norm=4.846968650817871, loss=1.183122992515564
I0129 17:50:42.597023 139656649758464 logging_writer.py:48] [168300] global_step=168300, grad_norm=4.645383358001709, loss=1.057733416557312
I0129 17:51:16.530808 139656666543872 logging_writer.py:48] [168400] global_step=168400, grad_norm=4.41732931137085, loss=1.0332263708114624
I0129 17:51:20.075976 139822745589568 spec.py:321] Evaluating on the training split.
I0129 17:51:26.910454 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 17:51:35.478560 139822745589568 spec.py:349] Evaluating on the test split.
I0129 17:51:38.103616 139822745589568 submission_runner.py:408] Time since start: 59141.46s, 	Step: 168412, 	{'train/accuracy': 0.8469985723495483, 'train/loss': 0.5509080290794373, 'validation/accuracy': 0.7348999977111816, 'validation/loss': 1.0821901559829712, 'validation/num_examples': 50000, 'test/accuracy': 0.6110000014305115, 'test/loss': 1.7652047872543335, 'test/num_examples': 10000, 'score': 57159.98609948158, 'total_duration': 59141.460246801376, 'accumulated_submission_time': 57159.98609948158, 'accumulated_eval_time': 1970.818876504898, 'accumulated_logging_time': 4.8666205406188965}
I0129 17:51:38.151290 139655609571072 logging_writer.py:48] [168412] accumulated_eval_time=1970.818877, accumulated_logging_time=4.866621, accumulated_submission_time=57159.986099, global_step=168412, preemption_count=0, score=57159.986099, test/accuracy=0.611000, test/loss=1.765205, test/num_examples=10000, total_duration=59141.460247, train/accuracy=0.846999, train/loss=0.550908, validation/accuracy=0.734900, validation/loss=1.082190, validation/num_examples=50000
I0129 17:52:08.301458 139655617963776 logging_writer.py:48] [168500] global_step=168500, grad_norm=4.732217788696289, loss=1.1336907148361206
I0129 17:52:42.144486 139655609571072 logging_writer.py:48] [168600] global_step=168600, grad_norm=5.183652877807617, loss=1.13990318775177
I0129 17:53:16.002231 139655617963776 logging_writer.py:48] [168700] global_step=168700, grad_norm=5.230890274047852, loss=1.1519815921783447
I0129 17:53:49.876162 139655609571072 logging_writer.py:48] [168800] global_step=168800, grad_norm=5.559665679931641, loss=1.084625482559204
I0129 17:54:23.842603 139655617963776 logging_writer.py:48] [168900] global_step=168900, grad_norm=4.889401435852051, loss=1.1228644847869873
I0129 17:54:57.735491 139655609571072 logging_writer.py:48] [169000] global_step=169000, grad_norm=4.882485389709473, loss=1.1634114980697632
I0129 17:55:31.628655 139655617963776 logging_writer.py:48] [169100] global_step=169100, grad_norm=4.782940864562988, loss=1.0911554098129272
I0129 17:56:05.510893 139655609571072 logging_writer.py:48] [169200] global_step=169200, grad_norm=4.819615364074707, loss=1.0305575132369995
I0129 17:56:39.429277 139655617963776 logging_writer.py:48] [169300] global_step=169300, grad_norm=5.2549920082092285, loss=1.2196857929229736
I0129 17:57:13.322964 139655609571072 logging_writer.py:48] [169400] global_step=169400, grad_norm=4.605937957763672, loss=1.042447805404663
I0129 17:57:47.233272 139655617963776 logging_writer.py:48] [169500] global_step=169500, grad_norm=4.799185276031494, loss=1.0338960886001587
I0129 17:58:21.111809 139655609571072 logging_writer.py:48] [169600] global_step=169600, grad_norm=4.664828300476074, loss=1.0795669555664062
I0129 17:58:55.032934 139655617963776 logging_writer.py:48] [169700] global_step=169700, grad_norm=5.107034683227539, loss=1.0690892934799194
I0129 17:59:28.932789 139655609571072 logging_writer.py:48] [169800] global_step=169800, grad_norm=4.579163074493408, loss=1.1124207973480225
I0129 18:00:02.855934 139655617963776 logging_writer.py:48] [169900] global_step=169900, grad_norm=4.928529739379883, loss=1.011561393737793
I0129 18:00:08.437413 139822745589568 spec.py:321] Evaluating on the training split.
I0129 18:00:14.688433 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 18:00:23.084980 139822745589568 spec.py:349] Evaluating on the test split.
I0129 18:00:25.998280 139822745589568 submission_runner.py:408] Time since start: 59669.35s, 	Step: 169918, 	{'train/accuracy': 0.8470583558082581, 'train/loss': 0.5498757362365723, 'validation/accuracy': 0.7370799779891968, 'validation/loss': 1.0780162811279297, 'validation/num_examples': 50000, 'test/accuracy': 0.610200047492981, 'test/loss': 1.7671650648117065, 'test/num_examples': 10000, 'score': 57670.21110057831, 'total_duration': 59669.354877471924, 'accumulated_submission_time': 57670.21110057831, 'accumulated_eval_time': 1988.379658460617, 'accumulated_logging_time': 4.923551321029663}
I0129 18:00:26.049501 139656666543872 logging_writer.py:48] [169918] accumulated_eval_time=1988.379658, accumulated_logging_time=4.923551, accumulated_submission_time=57670.211101, global_step=169918, preemption_count=0, score=57670.211101, test/accuracy=0.610200, test/loss=1.767165, test/num_examples=10000, total_duration=59669.354877, train/accuracy=0.847058, train/loss=0.549876, validation/accuracy=0.737080, validation/loss=1.078016, validation/num_examples=50000
I0129 18:00:54.149215 139656834316032 logging_writer.py:48] [170000] global_step=170000, grad_norm=5.221745014190674, loss=1.2125935554504395
I0129 18:01:28.022871 139656666543872 logging_writer.py:48] [170100] global_step=170100, grad_norm=4.72089147567749, loss=0.9843700528144836
I0129 18:02:01.902308 139656834316032 logging_writer.py:48] [170200] global_step=170200, grad_norm=4.880946636199951, loss=1.0437873601913452
I0129 18:02:35.813964 139656666543872 logging_writer.py:48] [170300] global_step=170300, grad_norm=4.874937534332275, loss=1.0339767932891846
I0129 18:03:09.719987 139656834316032 logging_writer.py:48] [170400] global_step=170400, grad_norm=5.296621322631836, loss=1.1040960550308228
I0129 18:03:43.629432 139656666543872 logging_writer.py:48] [170500] global_step=170500, grad_norm=5.077588081359863, loss=1.05223548412323
I0129 18:04:17.513549 139656834316032 logging_writer.py:48] [170600] global_step=170600, grad_norm=4.963990688323975, loss=1.1213500499725342
I0129 18:04:51.387212 139656666543872 logging_writer.py:48] [170700] global_step=170700, grad_norm=5.1693243980407715, loss=1.1140658855438232
I0129 18:05:25.303292 139656834316032 logging_writer.py:48] [170800] global_step=170800, grad_norm=4.8832197189331055, loss=1.0136017799377441
I0129 18:05:59.218403 139656666543872 logging_writer.py:48] [170900] global_step=170900, grad_norm=5.767894744873047, loss=1.1904069185256958
I0129 18:06:33.132051 139656834316032 logging_writer.py:48] [171000] global_step=171000, grad_norm=5.000683784484863, loss=1.0589962005615234
I0129 18:07:07.090087 139656666543872 logging_writer.py:48] [171100] global_step=171100, grad_norm=4.950989723205566, loss=1.0008422136306763
I0129 18:07:40.983003 139656834316032 logging_writer.py:48] [171200] global_step=171200, grad_norm=4.934201717376709, loss=1.0463868379592896
I0129 18:08:14.894388 139656666543872 logging_writer.py:48] [171300] global_step=171300, grad_norm=4.834103107452393, loss=1.0572043657302856
I0129 18:08:48.796309 139656834316032 logging_writer.py:48] [171400] global_step=171400, grad_norm=4.984067916870117, loss=1.1007508039474487
I0129 18:08:56.062556 139822745589568 spec.py:321] Evaluating on the training split.
I0129 18:09:02.317035 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 18:09:11.000583 139822745589568 spec.py:349] Evaluating on the test split.
I0129 18:09:13.576526 139822745589568 submission_runner.py:408] Time since start: 60196.93s, 	Step: 171423, 	{'train/accuracy': 0.8548508882522583, 'train/loss': 0.5203155279159546, 'validation/accuracy': 0.738599956035614, 'validation/loss': 1.0618312358856201, 'validation/num_examples': 50000, 'test/accuracy': 0.6117000579833984, 'test/loss': 1.7470028400421143, 'test/num_examples': 10000, 'score': 58180.162395238876, 'total_duration': 60196.933161735535, 'accumulated_submission_time': 58180.162395238876, 'accumulated_eval_time': 2005.8935883045197, 'accumulated_logging_time': 4.9838104248046875}
I0129 18:09:13.624676 139656649758464 logging_writer.py:48] [171423] accumulated_eval_time=2005.893588, accumulated_logging_time=4.983810, accumulated_submission_time=58180.162395, global_step=171423, preemption_count=0, score=58180.162395, test/accuracy=0.611700, test/loss=1.747003, test/num_examples=10000, total_duration=60196.933162, train/accuracy=0.854851, train/loss=0.520316, validation/accuracy=0.738600, validation/loss=1.061831, validation/num_examples=50000
I0129 18:09:40.039284 139656658151168 logging_writer.py:48] [171500] global_step=171500, grad_norm=4.8903937339782715, loss=0.9555214643478394
I0129 18:10:13.878994 139656649758464 logging_writer.py:48] [171600] global_step=171600, grad_norm=4.492191314697266, loss=1.017357587814331
I0129 18:10:47.793118 139656658151168 logging_writer.py:48] [171700] global_step=171700, grad_norm=5.178202152252197, loss=1.1390043497085571
I0129 18:11:21.686757 139656649758464 logging_writer.py:48] [171800] global_step=171800, grad_norm=4.743505001068115, loss=0.9848438501358032
I0129 18:11:55.587719 139656658151168 logging_writer.py:48] [171900] global_step=171900, grad_norm=4.573785305023193, loss=0.9559910297393799
I0129 18:12:29.481259 139656649758464 logging_writer.py:48] [172000] global_step=172000, grad_norm=4.887054443359375, loss=1.0044968128204346
I0129 18:13:03.398258 139656658151168 logging_writer.py:48] [172100] global_step=172100, grad_norm=5.373790264129639, loss=1.1401550769805908
I0129 18:13:37.378465 139656649758464 logging_writer.py:48] [172200] global_step=172200, grad_norm=4.578885078430176, loss=1.0236896276474
I0129 18:14:11.239132 139656658151168 logging_writer.py:48] [172300] global_step=172300, grad_norm=5.136777400970459, loss=1.1446701288223267
I0129 18:14:45.121486 139656649758464 logging_writer.py:48] [172400] global_step=172400, grad_norm=5.375227928161621, loss=1.0687896013259888
I0129 18:15:19.041058 139656658151168 logging_writer.py:48] [172500] global_step=172500, grad_norm=4.937671184539795, loss=1.0339692831039429
I0129 18:15:52.913720 139656649758464 logging_writer.py:48] [172600] global_step=172600, grad_norm=5.078080177307129, loss=1.0266495943069458
I0129 18:16:26.831714 139656658151168 logging_writer.py:48] [172700] global_step=172700, grad_norm=5.284786701202393, loss=1.0632307529449463
I0129 18:17:00.726308 139656649758464 logging_writer.py:48] [172800] global_step=172800, grad_norm=5.2065911293029785, loss=1.0654652118682861
I0129 18:17:34.660184 139656658151168 logging_writer.py:48] [172900] global_step=172900, grad_norm=4.967528343200684, loss=1.0144057273864746
I0129 18:17:43.618312 139822745589568 spec.py:321] Evaluating on the training split.
I0129 18:17:49.856392 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 18:17:58.343410 139822745589568 spec.py:349] Evaluating on the test split.
I0129 18:18:00.946246 139822745589568 submission_runner.py:408] Time since start: 60724.30s, 	Step: 172928, 	{'train/accuracy': 0.870515763759613, 'train/loss': 0.4607931077480316, 'validation/accuracy': 0.7422800064086914, 'validation/loss': 1.0552629232406616, 'validation/num_examples': 50000, 'test/accuracy': 0.6200000047683716, 'test/loss': 1.7335784435272217, 'test/num_examples': 10000, 'score': 58690.095504283905, 'total_duration': 60724.30287194252, 'accumulated_submission_time': 58690.095504283905, 'accumulated_eval_time': 2023.2214815616608, 'accumulated_logging_time': 5.041860818862915}
I0129 18:18:00.997075 139655626356480 logging_writer.py:48] [172928] accumulated_eval_time=2023.221482, accumulated_logging_time=5.041861, accumulated_submission_time=58690.095504, global_step=172928, preemption_count=0, score=58690.095504, test/accuracy=0.620000, test/loss=1.733578, test/num_examples=10000, total_duration=60724.302872, train/accuracy=0.870516, train/loss=0.460793, validation/accuracy=0.742280, validation/loss=1.055263, validation/num_examples=50000
I0129 18:18:25.716526 139656297445120 logging_writer.py:48] [173000] global_step=173000, grad_norm=4.683286190032959, loss=0.9645446538925171
I0129 18:18:59.563962 139655626356480 logging_writer.py:48] [173100] global_step=173100, grad_norm=4.894842147827148, loss=1.0117112398147583
I0129 18:19:33.474029 139656297445120 logging_writer.py:48] [173200] global_step=173200, grad_norm=5.116408824920654, loss=1.02714204788208
I0129 18:20:07.464567 139655626356480 logging_writer.py:48] [173300] global_step=173300, grad_norm=4.829976558685303, loss=0.9664578437805176
I0129 18:20:41.352345 139656297445120 logging_writer.py:48] [173400] global_step=173400, grad_norm=4.817601680755615, loss=0.9348831176757812
I0129 18:21:15.226687 139655626356480 logging_writer.py:48] [173500] global_step=173500, grad_norm=4.95126485824585, loss=0.9828304648399353
I0129 18:21:49.094931 139656297445120 logging_writer.py:48] [173600] global_step=173600, grad_norm=5.056088447570801, loss=1.0001646280288696
I0129 18:22:22.955672 139655626356480 logging_writer.py:48] [173700] global_step=173700, grad_norm=5.263161659240723, loss=1.0996313095092773
I0129 18:22:56.825540 139656297445120 logging_writer.py:48] [173800] global_step=173800, grad_norm=4.9187798500061035, loss=0.9819270968437195
I0129 18:23:30.703532 139655626356480 logging_writer.py:48] [173900] global_step=173900, grad_norm=4.5748419761657715, loss=0.9260841012001038
I0129 18:24:04.619203 139656297445120 logging_writer.py:48] [174000] global_step=174000, grad_norm=4.8175811767578125, loss=0.947955846786499
I0129 18:24:38.509164 139655626356480 logging_writer.py:48] [174100] global_step=174100, grad_norm=5.549599647521973, loss=1.0795766115188599
I0129 18:25:12.437403 139656297445120 logging_writer.py:48] [174200] global_step=174200, grad_norm=5.573494911193848, loss=1.1285392045974731
I0129 18:25:46.339910 139655626356480 logging_writer.py:48] [174300] global_step=174300, grad_norm=5.13444709777832, loss=1.0632576942443848
I0129 18:26:20.342692 139656297445120 logging_writer.py:48] [174400] global_step=174400, grad_norm=5.280763626098633, loss=1.0126670598983765
I0129 18:26:30.992034 139822745589568 spec.py:321] Evaluating on the training split.
I0129 18:26:37.184660 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 18:26:45.729195 139822745589568 spec.py:349] Evaluating on the test split.
I0129 18:26:48.274210 139822745589568 submission_runner.py:408] Time since start: 61251.63s, 	Step: 174433, 	{'train/accuracy': 0.8722695708274841, 'train/loss': 0.4543417692184448, 'validation/accuracy': 0.7435599565505981, 'validation/loss': 1.0551124811172485, 'validation/num_examples': 50000, 'test/accuracy': 0.6190000176429749, 'test/loss': 1.7485793828964233, 'test/num_examples': 10000, 'score': 59200.02728843689, 'total_duration': 61251.63083457947, 'accumulated_submission_time': 59200.02728843689, 'accumulated_eval_time': 2040.50359749794, 'accumulated_logging_time': 5.102504730224609}
I0129 18:26:48.326518 139655609571072 logging_writer.py:48] [174433] accumulated_eval_time=2040.503597, accumulated_logging_time=5.102505, accumulated_submission_time=59200.027288, global_step=174433, preemption_count=0, score=59200.027288, test/accuracy=0.619000, test/loss=1.748579, test/num_examples=10000, total_duration=61251.630835, train/accuracy=0.872270, train/loss=0.454342, validation/accuracy=0.743560, validation/loss=1.055112, validation/num_examples=50000
I0129 18:27:11.386785 139656658151168 logging_writer.py:48] [174500] global_step=174500, grad_norm=5.256827354431152, loss=1.0687898397445679
I0129 18:27:45.284859 139655609571072 logging_writer.py:48] [174600] global_step=174600, grad_norm=5.284718036651611, loss=0.9613166451454163
I0129 18:28:19.176438 139656658151168 logging_writer.py:48] [174700] global_step=174700, grad_norm=5.110813617706299, loss=1.017757773399353
I0129 18:28:53.080831 139655609571072 logging_writer.py:48] [174800] global_step=174800, grad_norm=5.149327278137207, loss=0.981467068195343
I0129 18:29:26.978515 139656658151168 logging_writer.py:48] [174900] global_step=174900, grad_norm=4.9665961265563965, loss=0.9933841228485107
I0129 18:30:00.891842 139655609571072 logging_writer.py:48] [175000] global_step=175000, grad_norm=5.382347106933594, loss=1.037431240081787
I0129 18:30:34.837793 139656658151168 logging_writer.py:48] [175100] global_step=175100, grad_norm=5.444180965423584, loss=1.1060019731521606
I0129 18:31:08.753030 139655609571072 logging_writer.py:48] [175200] global_step=175200, grad_norm=5.270918369293213, loss=1.0942336320877075
I0129 18:31:42.660920 139656658151168 logging_writer.py:48] [175300] global_step=175300, grad_norm=5.0344014167785645, loss=0.9752405285835266
I0129 18:32:16.557965 139655609571072 logging_writer.py:48] [175400] global_step=175400, grad_norm=4.994569778442383, loss=0.9969562292098999
I0129 18:32:50.486105 139656658151168 logging_writer.py:48] [175500] global_step=175500, grad_norm=5.0686540603637695, loss=1.0271713733673096
I0129 18:33:24.466145 139655609571072 logging_writer.py:48] [175600] global_step=175600, grad_norm=5.139769077301025, loss=0.9960826635360718
I0129 18:33:58.368254 139656658151168 logging_writer.py:48] [175700] global_step=175700, grad_norm=4.826606273651123, loss=0.9817371368408203
I0129 18:34:32.294233 139655609571072 logging_writer.py:48] [175800] global_step=175800, grad_norm=5.168692588806152, loss=1.0856190919876099
I0129 18:35:06.216889 139656658151168 logging_writer.py:48] [175900] global_step=175900, grad_norm=5.162532806396484, loss=0.9928704500198364
I0129 18:35:18.584228 139822745589568 spec.py:321] Evaluating on the training split.
I0129 18:35:24.862477 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 18:35:33.490885 139822745589568 spec.py:349] Evaluating on the test split.
I0129 18:35:36.054799 139822745589568 submission_runner.py:408] Time since start: 61779.41s, 	Step: 175938, 	{'train/accuracy': 0.8727478981018066, 'train/loss': 0.4520764946937561, 'validation/accuracy': 0.7457000017166138, 'validation/loss': 1.0432484149932861, 'validation/num_examples': 50000, 'test/accuracy': 0.6223000288009644, 'test/loss': 1.732593059539795, 'test/num_examples': 10000, 'score': 59710.2247235775, 'total_duration': 61779.41142606735, 'accumulated_submission_time': 59710.2247235775, 'accumulated_eval_time': 2057.9741168022156, 'accumulated_logging_time': 5.1639111042022705}
I0129 18:35:36.105955 139656649758464 logging_writer.py:48] [175938] accumulated_eval_time=2057.974117, accumulated_logging_time=5.163911, accumulated_submission_time=59710.224724, global_step=175938, preemption_count=0, score=59710.224724, test/accuracy=0.622300, test/loss=1.732593, test/num_examples=10000, total_duration=61779.411426, train/accuracy=0.872748, train/loss=0.452076, validation/accuracy=0.745700, validation/loss=1.043248, validation/num_examples=50000
I0129 18:35:57.433969 139656834316032 logging_writer.py:48] [176000] global_step=176000, grad_norm=5.360081195831299, loss=1.1019636392593384
I0129 18:36:31.329908 139656649758464 logging_writer.py:48] [176100] global_step=176100, grad_norm=5.049284934997559, loss=0.90150386095047
I0129 18:37:05.221753 139656834316032 logging_writer.py:48] [176200] global_step=176200, grad_norm=4.955264091491699, loss=0.8758835196495056
I0129 18:37:39.129734 139656649758464 logging_writer.py:48] [176300] global_step=176300, grad_norm=5.2087483406066895, loss=1.0856863260269165
I0129 18:38:13.039897 139656834316032 logging_writer.py:48] [176400] global_step=176400, grad_norm=5.068012237548828, loss=0.9746707081794739
I0129 18:38:46.957358 139656649758464 logging_writer.py:48] [176500] global_step=176500, grad_norm=5.5833635330200195, loss=0.9547438025474548
I0129 18:39:20.866131 139656834316032 logging_writer.py:48] [176600] global_step=176600, grad_norm=5.226419448852539, loss=0.9987974762916565
I0129 18:39:54.847633 139656649758464 logging_writer.py:48] [176700] global_step=176700, grad_norm=5.494893550872803, loss=0.9859390258789062
I0129 18:40:28.770685 139656834316032 logging_writer.py:48] [176800] global_step=176800, grad_norm=5.291903495788574, loss=1.0462884902954102
I0129 18:41:02.671283 139656649758464 logging_writer.py:48] [176900] global_step=176900, grad_norm=4.745943069458008, loss=0.8886100053787231
I0129 18:41:36.578295 139656834316032 logging_writer.py:48] [177000] global_step=177000, grad_norm=5.124232292175293, loss=0.9543747305870056
I0129 18:42:10.476521 139656649758464 logging_writer.py:48] [177100] global_step=177100, grad_norm=4.9069671630859375, loss=0.9401813745498657
I0129 18:42:44.380801 139656834316032 logging_writer.py:48] [177200] global_step=177200, grad_norm=5.175009250640869, loss=1.02082359790802
I0129 18:43:18.284845 139656649758464 logging_writer.py:48] [177300] global_step=177300, grad_norm=5.62472677230835, loss=1.0168734788894653
I0129 18:43:52.215384 139656834316032 logging_writer.py:48] [177400] global_step=177400, grad_norm=5.440313816070557, loss=0.8893327713012695
I0129 18:44:06.269639 139822745589568 spec.py:321] Evaluating on the training split.
I0129 18:44:12.469206 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 18:44:20.963224 139822745589568 spec.py:349] Evaluating on the test split.
I0129 18:44:23.566387 139822745589568 submission_runner.py:408] Time since start: 62306.92s, 	Step: 177443, 	{'train/accuracy': 0.875996470451355, 'train/loss': 0.442630410194397, 'validation/accuracy': 0.7466399669647217, 'validation/loss': 1.0333547592163086, 'validation/num_examples': 50000, 'test/accuracy': 0.6240000128746033, 'test/loss': 1.7179020643234253, 'test/num_examples': 10000, 'score': 60220.32627439499, 'total_duration': 62306.92303276062, 'accumulated_submission_time': 60220.32627439499, 'accumulated_eval_time': 2075.270829439163, 'accumulated_logging_time': 5.224120616912842}
I0129 18:44:23.616129 139655626356480 logging_writer.py:48] [177443] accumulated_eval_time=2075.270829, accumulated_logging_time=5.224121, accumulated_submission_time=60220.326274, global_step=177443, preemption_count=0, score=60220.326274, test/accuracy=0.624000, test/loss=1.717902, test/num_examples=10000, total_duration=62306.923033, train/accuracy=0.875996, train/loss=0.442630, validation/accuracy=0.746640, validation/loss=1.033355, validation/num_examples=50000
I0129 18:44:43.270747 139656297445120 logging_writer.py:48] [177500] global_step=177500, grad_norm=5.3363823890686035, loss=1.0038243532180786
I0129 18:45:17.149132 139655626356480 logging_writer.py:48] [177600] global_step=177600, grad_norm=5.183362007141113, loss=1.0138047933578491
I0129 18:45:51.058109 139656297445120 logging_writer.py:48] [177700] global_step=177700, grad_norm=5.182783603668213, loss=0.9798352718353271
I0129 18:46:25.007359 139655626356480 logging_writer.py:48] [177800] global_step=177800, grad_norm=5.082762241363525, loss=0.988510012626648
I0129 18:46:58.921370 139656297445120 logging_writer.py:48] [177900] global_step=177900, grad_norm=5.106245040893555, loss=0.8822094202041626
I0129 18:47:32.823890 139655626356480 logging_writer.py:48] [178000] global_step=178000, grad_norm=4.766924858093262, loss=0.912481427192688
I0129 18:48:06.761611 139656297445120 logging_writer.py:48] [178100] global_step=178100, grad_norm=4.964595794677734, loss=0.9128665924072266
I0129 18:48:40.649884 139655626356480 logging_writer.py:48] [178200] global_step=178200, grad_norm=5.506463527679443, loss=0.884445071220398
I0129 18:49:14.575227 139656297445120 logging_writer.py:48] [178300] global_step=178300, grad_norm=5.366647720336914, loss=1.0220396518707275
I0129 18:49:48.452990 139655626356480 logging_writer.py:48] [178400] global_step=178400, grad_norm=5.116106033325195, loss=0.9294608235359192
I0129 18:50:22.358845 139656297445120 logging_writer.py:48] [178500] global_step=178500, grad_norm=5.221081733703613, loss=0.9436647891998291
I0129 18:50:56.253102 139655626356480 logging_writer.py:48] [178600] global_step=178600, grad_norm=5.699088096618652, loss=0.98609459400177
I0129 18:51:30.170221 139656297445120 logging_writer.py:48] [178700] global_step=178700, grad_norm=5.02031946182251, loss=0.8842016458511353
I0129 18:52:04.064515 139655626356480 logging_writer.py:48] [178800] global_step=178800, grad_norm=5.13236665725708, loss=0.9536550045013428
I0129 18:52:38.177199 139656297445120 logging_writer.py:48] [178900] global_step=178900, grad_norm=5.265737056732178, loss=0.9400379657745361
I0129 18:52:53.584169 139822745589568 spec.py:321] Evaluating on the training split.
I0129 18:52:59.814228 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 18:53:08.431214 139822745589568 spec.py:349] Evaluating on the test split.
I0129 18:53:11.075524 139822745589568 submission_runner.py:408] Time since start: 62834.43s, 	Step: 178947, 	{'train/accuracy': 0.8785873651504517, 'train/loss': 0.42702656984329224, 'validation/accuracy': 0.7484599947929382, 'validation/loss': 1.0259389877319336, 'validation/num_examples': 50000, 'test/accuracy': 0.6215000152587891, 'test/loss': 1.717150092124939, 'test/num_examples': 10000, 'score': 60730.232352018356, 'total_duration': 62834.43216466904, 'accumulated_submission_time': 60730.232352018356, 'accumulated_eval_time': 2092.762161254883, 'accumulated_logging_time': 5.284364223480225}
I0129 18:53:11.126796 139655617963776 logging_writer.py:48] [178947] accumulated_eval_time=2092.762161, accumulated_logging_time=5.284364, accumulated_submission_time=60730.232352, global_step=178947, preemption_count=0, score=60730.232352, test/accuracy=0.621500, test/loss=1.717150, test/num_examples=10000, total_duration=62834.432165, train/accuracy=0.878587, train/loss=0.427027, validation/accuracy=0.748460, validation/loss=1.025939, validation/num_examples=50000
I0129 18:53:29.377779 139655626356480 logging_writer.py:48] [179000] global_step=179000, grad_norm=5.026580810546875, loss=0.8703441619873047
I0129 18:54:03.235025 139655617963776 logging_writer.py:48] [179100] global_step=179100, grad_norm=4.7147135734558105, loss=0.8332569003105164
I0129 18:54:37.134707 139655626356480 logging_writer.py:48] [179200] global_step=179200, grad_norm=5.176135063171387, loss=0.9861962199211121
I0129 18:55:11.002061 139655617963776 logging_writer.py:48] [179300] global_step=179300, grad_norm=5.149054050445557, loss=0.8915420770645142
I0129 18:55:44.903463 139655626356480 logging_writer.py:48] [179400] global_step=179400, grad_norm=5.309926986694336, loss=0.9500294923782349
I0129 18:56:18.790667 139655617963776 logging_writer.py:48] [179500] global_step=179500, grad_norm=5.22758674621582, loss=1.0029497146606445
I0129 18:56:52.689403 139655626356480 logging_writer.py:48] [179600] global_step=179600, grad_norm=5.116490364074707, loss=0.9609788656234741
I0129 18:57:26.594546 139655617963776 logging_writer.py:48] [179700] global_step=179700, grad_norm=5.024116516113281, loss=0.9261358976364136
I0129 18:58:00.468764 139655626356480 logging_writer.py:48] [179800] global_step=179800, grad_norm=5.086927890777588, loss=0.9689661264419556
I0129 18:58:34.399232 139655617963776 logging_writer.py:48] [179900] global_step=179900, grad_norm=5.490120887756348, loss=1.00088632106781
I0129 18:59:08.410402 139655626356480 logging_writer.py:48] [180000] global_step=180000, grad_norm=5.56356954574585, loss=1.0229465961456299
I0129 18:59:42.288908 139655617963776 logging_writer.py:48] [180100] global_step=180100, grad_norm=5.203721046447754, loss=0.9801722168922424
I0129 19:00:16.182468 139655626356480 logging_writer.py:48] [180200] global_step=180200, grad_norm=5.228023052215576, loss=0.9129862785339355
I0129 19:00:50.063459 139655617963776 logging_writer.py:48] [180300] global_step=180300, grad_norm=5.292746543884277, loss=0.9692285656929016
I0129 19:01:23.924289 139655626356480 logging_writer.py:48] [180400] global_step=180400, grad_norm=5.6508402824401855, loss=0.97036212682724
I0129 19:01:41.340165 139822745589568 spec.py:321] Evaluating on the training split.
I0129 19:01:47.639510 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 19:01:56.067885 139822745589568 spec.py:349] Evaluating on the test split.
I0129 19:01:58.625543 139822745589568 submission_runner.py:408] Time since start: 63361.98s, 	Step: 180453, 	{'train/accuracy': 0.8818159699440002, 'train/loss': 0.412717342376709, 'validation/accuracy': 0.7495399713516235, 'validation/loss': 1.0254219770431519, 'validation/num_examples': 50000, 'test/accuracy': 0.6262000203132629, 'test/loss': 1.7159655094146729, 'test/num_examples': 10000, 'score': 61240.38385462761, 'total_duration': 63361.98218679428, 'accumulated_submission_time': 61240.38385462761, 'accumulated_eval_time': 2110.0475058555603, 'accumulated_logging_time': 5.346153259277344}
I0129 19:01:58.678712 139656297445120 logging_writer.py:48] [180453] accumulated_eval_time=2110.047506, accumulated_logging_time=5.346153, accumulated_submission_time=61240.383855, global_step=180453, preemption_count=0, score=61240.383855, test/accuracy=0.626200, test/loss=1.715966, test/num_examples=10000, total_duration=63361.982187, train/accuracy=0.881816, train/loss=0.412717, validation/accuracy=0.749540, validation/loss=1.025422, validation/num_examples=50000
I0129 19:02:14.909121 139656658151168 logging_writer.py:48] [180500] global_step=180500, grad_norm=5.337649822235107, loss=0.984464168548584
I0129 19:02:48.765123 139656297445120 logging_writer.py:48] [180600] global_step=180600, grad_norm=5.250221252441406, loss=0.9312210083007812
I0129 19:03:22.587355 139656658151168 logging_writer.py:48] [180700] global_step=180700, grad_norm=5.038247585296631, loss=0.893713653087616
I0129 19:03:56.510271 139656297445120 logging_writer.py:48] [180800] global_step=180800, grad_norm=4.99783992767334, loss=0.9592421054840088
I0129 19:04:30.435056 139656658151168 logging_writer.py:48] [180900] global_step=180900, grad_norm=5.779016971588135, loss=0.9084446430206299
I0129 19:05:04.332209 139656297445120 logging_writer.py:48] [181000] global_step=181000, grad_norm=5.102965831756592, loss=0.9719902873039246
I0129 19:05:38.433197 139656658151168 logging_writer.py:48] [181100] global_step=181100, grad_norm=5.469627380371094, loss=0.9411245584487915
I0129 19:06:12.298653 139656297445120 logging_writer.py:48] [181200] global_step=181200, grad_norm=4.868287086486816, loss=0.9128180146217346
I0129 19:06:46.182739 139656658151168 logging_writer.py:48] [181300] global_step=181300, grad_norm=5.22026252746582, loss=0.92906653881073
I0129 19:07:20.093503 139656297445120 logging_writer.py:48] [181400] global_step=181400, grad_norm=5.154284477233887, loss=0.9664856195449829
I0129 19:07:53.957504 139656658151168 logging_writer.py:48] [181500] global_step=181500, grad_norm=5.568314552307129, loss=0.9607617855072021
I0129 19:08:27.834138 139656297445120 logging_writer.py:48] [181600] global_step=181600, grad_norm=5.339947700500488, loss=0.8904949426651001
I0129 19:09:01.735851 139656658151168 logging_writer.py:48] [181700] global_step=181700, grad_norm=4.816287517547607, loss=0.8330686092376709
I0129 19:09:35.619385 139656297445120 logging_writer.py:48] [181800] global_step=181800, grad_norm=5.2614288330078125, loss=0.9339608550071716
I0129 19:10:09.507343 139656658151168 logging_writer.py:48] [181900] global_step=181900, grad_norm=5.281818389892578, loss=0.9492558240890503
I0129 19:10:28.952826 139822745589568 spec.py:321] Evaluating on the training split.
I0129 19:10:35.266167 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 19:10:43.857142 139822745589568 spec.py:349] Evaluating on the test split.
I0129 19:10:46.422008 139822745589568 submission_runner.py:408] Time since start: 63889.78s, 	Step: 181959, 	{'train/accuracy': 0.8836495280265808, 'train/loss': 0.4111799895763397, 'validation/accuracy': 0.7505399584770203, 'validation/loss': 1.0216362476348877, 'validation/num_examples': 50000, 'test/accuracy': 0.6243000030517578, 'test/loss': 1.715208888053894, 'test/num_examples': 10000, 'score': 61750.597148656845, 'total_duration': 63889.77865052223, 'accumulated_submission_time': 61750.597148656845, 'accumulated_eval_time': 2127.516664505005, 'accumulated_logging_time': 5.409894943237305}
I0129 19:10:46.472341 139655617963776 logging_writer.py:48] [181959] accumulated_eval_time=2127.516665, accumulated_logging_time=5.409895, accumulated_submission_time=61750.597149, global_step=181959, preemption_count=0, score=61750.597149, test/accuracy=0.624300, test/loss=1.715209, test/num_examples=10000, total_duration=63889.778651, train/accuracy=0.883650, train/loss=0.411180, validation/accuracy=0.750540, validation/loss=1.021636, validation/num_examples=50000
I0129 19:11:00.725670 139655626356480 logging_writer.py:48] [182000] global_step=182000, grad_norm=5.290525913238525, loss=0.9840519428253174
I0129 19:11:34.514278 139655617963776 logging_writer.py:48] [182100] global_step=182100, grad_norm=5.393802642822266, loss=0.9290346503257751
I0129 19:12:08.416759 139655626356480 logging_writer.py:48] [182200] global_step=182200, grad_norm=5.199991703033447, loss=0.898970365524292
I0129 19:12:42.315506 139655617963776 logging_writer.py:48] [182300] global_step=182300, grad_norm=5.369144916534424, loss=1.0193991661071777
I0129 19:13:16.226549 139655626356480 logging_writer.py:48] [182400] global_step=182400, grad_norm=5.552198886871338, loss=0.9089053273200989
I0129 19:13:50.083587 139655617963776 logging_writer.py:48] [182500] global_step=182500, grad_norm=5.273820400238037, loss=0.9425263404846191
I0129 19:14:23.969571 139655626356480 logging_writer.py:48] [182600] global_step=182600, grad_norm=5.033254146575928, loss=0.90000319480896
I0129 19:14:57.856109 139655617963776 logging_writer.py:48] [182700] global_step=182700, grad_norm=5.403137683868408, loss=0.9776203036308289
I0129 19:15:31.738753 139655626356480 logging_writer.py:48] [182800] global_step=182800, grad_norm=5.498867988586426, loss=0.9802208542823792
I0129 19:16:05.612851 139655617963776 logging_writer.py:48] [182900] global_step=182900, grad_norm=5.116648197174072, loss=0.841221272945404
I0129 19:16:39.491101 139655626356480 logging_writer.py:48] [183000] global_step=183000, grad_norm=5.384098052978516, loss=0.9388536810874939
I0129 19:17:13.374487 139655617963776 logging_writer.py:48] [183100] global_step=183100, grad_norm=5.174067974090576, loss=0.9250475168228149
I0129 19:17:47.283911 139655626356480 logging_writer.py:48] [183200] global_step=183200, grad_norm=5.032074928283691, loss=0.9075758457183838
I0129 19:18:21.192023 139655617963776 logging_writer.py:48] [183300] global_step=183300, grad_norm=5.382914066314697, loss=1.0209875106811523
I0129 19:18:55.174237 139655626356480 logging_writer.py:48] [183400] global_step=183400, grad_norm=5.109158992767334, loss=0.8045098185539246
I0129 19:19:16.682658 139822745589568 spec.py:321] Evaluating on the training split.
I0129 19:19:22.994135 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 19:19:31.400618 139822745589568 spec.py:349] Evaluating on the test split.
I0129 19:19:34.002098 139822745589568 submission_runner.py:408] Time since start: 64417.36s, 	Step: 183465, 	{'train/accuracy': 0.884785532951355, 'train/loss': 0.4123013913631439, 'validation/accuracy': 0.7511999607086182, 'validation/loss': 1.020043134689331, 'validation/num_examples': 50000, 'test/accuracy': 0.6269000172615051, 'test/loss': 1.7149865627288818, 'test/num_examples': 10000, 'score': 62260.746554374695, 'total_duration': 64417.35872173309, 'accumulated_submission_time': 62260.746554374695, 'accumulated_eval_time': 2144.8360488414764, 'accumulated_logging_time': 5.469226121902466}
I0129 19:19:34.053525 139655609571072 logging_writer.py:48] [183465] accumulated_eval_time=2144.836049, accumulated_logging_time=5.469226, accumulated_submission_time=62260.746554, global_step=183465, preemption_count=0, score=62260.746554, test/accuracy=0.626900, test/loss=1.714987, test/num_examples=10000, total_duration=64417.358722, train/accuracy=0.884786, train/loss=0.412301, validation/accuracy=0.751200, validation/loss=1.020043, validation/num_examples=50000
I0129 19:19:46.221266 139655617963776 logging_writer.py:48] [183500] global_step=183500, grad_norm=4.9131646156311035, loss=0.8607667684555054
I0129 19:20:20.084379 139655609571072 logging_writer.py:48] [183600] global_step=183600, grad_norm=5.356258869171143, loss=0.9151375889778137
I0129 19:20:53.993767 139655617963776 logging_writer.py:48] [183700] global_step=183700, grad_norm=5.090161323547363, loss=0.8951177000999451
I0129 19:21:27.898035 139655609571072 logging_writer.py:48] [183800] global_step=183800, grad_norm=5.026398181915283, loss=1.0318691730499268
I0129 19:22:01.800780 139655617963776 logging_writer.py:48] [183900] global_step=183900, grad_norm=5.170690536499023, loss=0.9669058918952942
I0129 19:22:35.689219 139655609571072 logging_writer.py:48] [184000] global_step=184000, grad_norm=5.267488479614258, loss=0.9234561324119568
I0129 19:23:09.555875 139655617963776 logging_writer.py:48] [184100] global_step=184100, grad_norm=5.234147548675537, loss=0.9364139437675476
I0129 19:23:43.407143 139655609571072 logging_writer.py:48] [184200] global_step=184200, grad_norm=5.535393238067627, loss=0.966452956199646
I0129 19:24:17.297708 139655617963776 logging_writer.py:48] [184300] global_step=184300, grad_norm=5.318606853485107, loss=0.8858069777488708
I0129 19:24:51.283877 139655609571072 logging_writer.py:48] [184400] global_step=184400, grad_norm=5.36970329284668, loss=0.9521770477294922
I0129 19:25:25.185030 139655617963776 logging_writer.py:48] [184500] global_step=184500, grad_norm=5.493232250213623, loss=0.9584753513336182
I0129 19:25:59.059350 139655609571072 logging_writer.py:48] [184600] global_step=184600, grad_norm=5.164389610290527, loss=0.9422409534454346
I0129 19:26:32.972436 139655617963776 logging_writer.py:48] [184700] global_step=184700, grad_norm=5.103647708892822, loss=0.96519935131073
I0129 19:27:06.880253 139655609571072 logging_writer.py:48] [184800] global_step=184800, grad_norm=5.038095474243164, loss=0.9030391573905945
I0129 19:27:40.768111 139655617963776 logging_writer.py:48] [184900] global_step=184900, grad_norm=5.164923191070557, loss=0.9053258299827576
I0129 19:28:04.326931 139822745589568 spec.py:321] Evaluating on the training split.
I0129 19:28:10.558568 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 19:28:19.074841 139822745589568 spec.py:349] Evaluating on the test split.
I0129 19:28:21.632399 139822745589568 submission_runner.py:408] Time since start: 64944.99s, 	Step: 184971, 	{'train/accuracy': 0.8868184089660645, 'train/loss': 0.4034609794616699, 'validation/accuracy': 0.7515400052070618, 'validation/loss': 1.0192183256149292, 'validation/num_examples': 50000, 'test/accuracy': 0.6272000074386597, 'test/loss': 1.7114449739456177, 'test/num_examples': 10000, 'score': 62770.95883798599, 'total_duration': 64944.98903656006, 'accumulated_submission_time': 62770.95883798599, 'accumulated_eval_time': 2162.1414761543274, 'accumulated_logging_time': 5.530078649520874}
I0129 19:28:21.682312 139655609571072 logging_writer.py:48] [184971] accumulated_eval_time=2162.141476, accumulated_logging_time=5.530079, accumulated_submission_time=62770.958838, global_step=184971, preemption_count=0, score=62770.958838, test/accuracy=0.627200, test/loss=1.711445, test/num_examples=10000, total_duration=64944.989037, train/accuracy=0.886818, train/loss=0.403461, validation/accuracy=0.751540, validation/loss=1.019218, validation/num_examples=50000
I0129 19:28:31.859726 139655617963776 logging_writer.py:48] [185000] global_step=185000, grad_norm=5.134820938110352, loss=0.8901612758636475
I0129 19:29:05.709027 139655609571072 logging_writer.py:48] [185100] global_step=185100, grad_norm=5.384821891784668, loss=0.9423261880874634
I0129 19:29:39.590479 139655617963776 logging_writer.py:48] [185200] global_step=185200, grad_norm=5.012448787689209, loss=0.9463468790054321
I0129 19:30:13.465217 139655609571072 logging_writer.py:48] [185300] global_step=185300, grad_norm=4.943231105804443, loss=0.9199063181877136
I0129 19:30:47.365193 139655617963776 logging_writer.py:48] [185400] global_step=185400, grad_norm=5.624654293060303, loss=0.9092653393745422
I0129 19:31:21.294602 139655609571072 logging_writer.py:48] [185500] global_step=185500, grad_norm=5.19800329208374, loss=0.9912445545196533
I0129 19:31:55.186277 139655617963776 logging_writer.py:48] [185600] global_step=185600, grad_norm=5.472724437713623, loss=1.1226985454559326
I0129 19:32:19.055769 139655609571072 logging_writer.py:48] [185672] global_step=185672, preemption_count=0, score=63008.265499
I0129 19:32:19.522073 139822745589568 checkpoints.py:490] Saving checkpoint at step: 185672
I0129 19:32:20.624549 139822745589568 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_1/imagenet_resnet_jax/trial_4/checkpoint_185672
I0129 19:32:20.646106 139822745589568 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_1/imagenet_resnet_jax/trial_4/checkpoint_185672.
I0129 19:32:21.427466 139822745589568 submission_runner.py:583] Tuning trial 4/5
I0129 19:32:21.427703 139822745589568 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.004958460849689891, one_minus_beta1=0.13625575743, beta2=0.6291854735396584, weight_decay=0.1147386261512052, warmup_factor=0.02)
I0129 19:32:21.436239 139822745589568 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0013352996902540326, 'train/loss': 6.91108512878418, 'validation/accuracy': 0.001019999966956675, 'validation/loss': 6.91091251373291, 'validation/num_examples': 50000, 'test/accuracy': 0.0012000000569969416, 'test/loss': 6.910790920257568, 'test/num_examples': 10000, 'score': 33.559484243392944, 'total_duration': 50.918386936187744, 'accumulated_submission_time': 33.559484243392944, 'accumulated_eval_time': 17.358678817749023, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1492, {'train/accuracy': 0.18839684128761292, 'train/loss': 4.257904052734375, 'validation/accuracy': 0.1703599989414215, 'validation/loss': 4.3842315673828125, 'validation/num_examples': 50000, 'test/accuracy': 0.1371000111103058, 'test/loss': 4.80158805847168, 'test/num_examples': 10000, 'score': 543.7184791564941, 'total_duration': 578.5034921169281, 'accumulated_submission_time': 543.7184791564941, 'accumulated_eval_time': 34.713916540145874, 'accumulated_logging_time': 0.019278764724731445, 'global_step': 1492, 'preemption_count': 0}), (2985, {'train/accuracy': 0.35174185037612915, 'train/loss': 3.024991035461426, 'validation/accuracy': 0.32603999972343445, 'validation/loss': 3.193049192428589, 'validation/num_examples': 50000, 'test/accuracy': 0.24540001153945923, 'test/loss': 3.8982651233673096, 'test/num_examples': 10000, 'score': 1053.8195695877075, 'total_duration': 1105.9118909835815, 'accumulated_submission_time': 1053.8195695877075, 'accumulated_eval_time': 51.93747568130493, 'accumulated_logging_time': 0.04973602294921875, 'global_step': 2985, 'preemption_count': 0}), (4479, {'train/accuracy': 0.409877210855484, 'train/loss': 2.6678028106689453, 'validation/accuracy': 0.3876599967479706, 'validation/loss': 2.80322003364563, 'validation/num_examples': 50000, 'test/accuracy': 0.2946000099182129, 'test/loss': 3.5155205726623535, 'test/num_examples': 10000, 'score': 1563.8707466125488, 'total_duration': 1633.739863872528, 'accumulated_submission_time': 1563.8707466125488, 'accumulated_eval_time': 69.63504076004028, 'accumulated_logging_time': 0.07636284828186035, 'global_step': 4479, 'preemption_count': 0}), (5974, {'train/accuracy': 0.4144810140132904, 'train/loss': 2.693990468978882, 'validation/accuracy': 0.35273998975753784, 'validation/loss': 3.0620031356811523, 'validation/num_examples': 50000, 'test/accuracy': 0.27010002732276917, 'test/loss': 3.7617311477661133, 'test/num_examples': 10000, 'score': 2073.9059176445007, 'total_duration': 2161.2912888526917, 'accumulated_submission_time': 2073.9059176445007, 'accumulated_eval_time': 87.07250952720642, 'accumulated_logging_time': 0.10388541221618652, 'global_step': 5974, 'preemption_count': 0}), (7470, {'train/accuracy': 0.3179607689380646, 'train/loss': 3.236488103866577, 'validation/accuracy': 0.29165998101234436, 'validation/loss': 3.4889159202575684, 'validation/num_examples': 50000, 'test/accuracy': 0.2379000186920166, 'test/loss': 4.055367946624756, 'test/num_examples': 10000, 'score': 2584.0340342521667, 'total_duration': 2689.035103559494, 'accumulated_submission_time': 2584.0340342521667, 'accumulated_eval_time': 104.60787105560303, 'accumulated_logging_time': 0.1320948600769043, 'global_step': 7470, 'preemption_count': 0}), (8967, {'train/accuracy': 0.3252750337123871, 'train/loss': 3.3958663940429688, 'validation/accuracy': 0.30667999386787415, 'validation/loss': 3.4880292415618896, 'validation/num_examples': 50000, 'test/accuracy': 0.2201000154018402, 'test/loss': 4.388706684112549, 'test/num_examples': 10000, 'score': 3094.1442699432373, 'total_duration': 3216.537830352783, 'accumulated_submission_time': 3094.1442699432373, 'accumulated_eval_time': 121.91842865943909, 'accumulated_logging_time': 0.16176819801330566, 'global_step': 8967, 'preemption_count': 0}), (10464, {'train/accuracy': 0.22395168244838715, 'train/loss': 4.2264933586120605, 'validation/accuracy': 0.21407999098300934, 'validation/loss': 4.341734409332275, 'validation/num_examples': 50000, 'test/accuracy': 0.15730001032352448, 'test/loss': 5.09550666809082, 'test/num_examples': 10000, 'score': 3604.070210456848, 'total_duration': 3743.9415435791016, 'accumulated_submission_time': 3604.070210456848, 'accumulated_eval_time': 139.31764602661133, 'accumulated_logging_time': 0.18913674354553223, 'global_step': 10464, 'preemption_count': 0}), (11962, {'train/accuracy': 0.3482939898967743, 'train/loss': 3.1312360763549805, 'validation/accuracy': 0.32655999064445496, 'validation/loss': 3.3243465423583984, 'validation/num_examples': 50000, 'test/accuracy': 0.24170000851154327, 'test/loss': 4.087606906890869, 'test/num_examples': 10000, 'score': 4114.044556617737, 'total_duration': 4272.033766746521, 'accumulated_submission_time': 4114.044556617737, 'accumulated_eval_time': 157.3548982143402, 'accumulated_logging_time': 0.2187190055847168, 'global_step': 11962, 'preemption_count': 0}), (13460, {'train/accuracy': 0.2033442258834839, 'train/loss': 4.297685623168945, 'validation/accuracy': 0.19129998981952667, 'validation/loss': 4.41984748840332, 'validation/num_examples': 50000, 'test/accuracy': 0.14250001311302185, 'test/loss': 5.075582027435303, 'test/num_examples': 10000, 'score': 4624.091902256012, 'total_duration': 4799.691284656525, 'accumulated_submission_time': 4624.091902256012, 'accumulated_eval_time': 174.88282465934753, 'accumulated_logging_time': 0.24899768829345703, 'global_step': 13460, 'preemption_count': 0}), (14959, {'train/accuracy': 0.2525111436843872, 'train/loss': 3.8521933555603027, 'validation/accuracy': 0.2320999950170517, 'validation/loss': 4.034037113189697, 'validation/num_examples': 50000, 'test/accuracy': 0.1761000156402588, 'test/loss': 4.722900867462158, 'test/num_examples': 10000, 'score': 5134.104543209076, 'total_duration': 5327.868116140366, 'accumulated_submission_time': 5134.104543209076, 'accumulated_eval_time': 192.96386647224426, 'accumulated_logging_time': 0.28130149841308594, 'global_step': 14959, 'preemption_count': 0}), (16459, {'train/accuracy': 0.1354631632566452, 'train/loss': 5.622068405151367, 'validation/accuracy': 0.11997999995946884, 'validation/loss': 5.786570072174072, 'validation/num_examples': 50000, 'test/accuracy': 0.08390000462532043, 'test/loss': 6.400252819061279, 'test/num_examples': 10000, 'score': 5644.231992006302, 'total_duration': 5855.515547513962, 'accumulated_submission_time': 5644.231992006302, 'accumulated_eval_time': 210.40357780456543, 'accumulated_logging_time': 0.3103921413421631, 'global_step': 16459, 'preemption_count': 0}), (17959, {'train/accuracy': 0.3703164756298065, 'train/loss': 2.9259555339813232, 'validation/accuracy': 0.34158000349998474, 'validation/loss': 3.1281611919403076, 'validation/num_examples': 50000, 'test/accuracy': 0.254800021648407, 'test/loss': 3.834587574005127, 'test/num_examples': 10000, 'score': 6154.353933811188, 'total_duration': 6383.206515073776, 'accumulated_submission_time': 6154.353933811188, 'accumulated_eval_time': 227.88853096961975, 'accumulated_logging_time': 0.34072422981262207, 'global_step': 17959, 'preemption_count': 0}), (19460, {'train/accuracy': 0.24842554330825806, 'train/loss': 4.295435428619385, 'validation/accuracy': 0.22613999247550964, 'validation/loss': 4.466485023498535, 'validation/num_examples': 50000, 'test/accuracy': 0.1698000133037567, 'test/loss': 5.243460178375244, 'test/num_examples': 10000, 'score': 6664.5083796978, 'total_duration': 6911.110089302063, 'accumulated_submission_time': 6664.5083796978, 'accumulated_eval_time': 245.55494594573975, 'accumulated_logging_time': 0.3720059394836426, 'global_step': 19460, 'preemption_count': 0}), (20961, {'train/accuracy': 0.2622568607330322, 'train/loss': 3.897775888442993, 'validation/accuracy': 0.25540000200271606, 'validation/loss': 3.955528974533081, 'validation/num_examples': 50000, 'test/accuracy': 0.18300001323223114, 'test/loss': 4.831855773925781, 'test/num_examples': 10000, 'score': 7174.447612285614, 'total_duration': 7438.4239411354065, 'accumulated_submission_time': 7174.447612285614, 'accumulated_eval_time': 262.85289573669434, 'accumulated_logging_time': 0.3986082077026367, 'global_step': 20961, 'preemption_count': 0}), (22462, {'train/accuracy': 0.12033641338348389, 'train/loss': 5.798086643218994, 'validation/accuracy': 0.10941999405622482, 'validation/loss': 5.968713760375977, 'validation/num_examples': 50000, 'test/accuracy': 0.0861000046133995, 'test/loss': 6.424266338348389, 'test/num_examples': 10000, 'score': 7684.4579641819, 'total_duration': 7965.822619438171, 'accumulated_submission_time': 7684.4579641819, 'accumulated_eval_time': 280.15942215919495, 'accumulated_logging_time': 0.43025684356689453, 'global_step': 22462, 'preemption_count': 0}), (23963, {'train/accuracy': 0.08017776906490326, 'train/loss': 6.423879146575928, 'validation/accuracy': 0.07471999526023865, 'validation/loss': 6.4704060554504395, 'validation/num_examples': 50000, 'test/accuracy': 0.055500004440546036, 'test/loss': 6.987226963043213, 'test/num_examples': 10000, 'score': 8194.56183886528, 'total_duration': 8493.26726603508, 'accumulated_submission_time': 8194.56183886528, 'accumulated_eval_time': 297.41808342933655, 'accumulated_logging_time': 0.4613358974456787, 'global_step': 23963, 'preemption_count': 0}), (25464, {'train/accuracy': 0.06762196868658066, 'train/loss': 8.211507797241211, 'validation/accuracy': 0.05689999833703041, 'validation/loss': 8.37997817993164, 'validation/num_examples': 50000, 'test/accuracy': 0.040800001472234726, 'test/loss': 9.22226333618164, 'test/num_examples': 10000, 'score': 8704.487569570541, 'total_duration': 9021.073375463486, 'accumulated_submission_time': 8704.487569570541, 'accumulated_eval_time': 315.2127788066864, 'accumulated_logging_time': 0.494720458984375, 'global_step': 25464, 'preemption_count': 0}), (26965, {'train/accuracy': 0.18293605744838715, 'train/loss': 4.7571492195129395, 'validation/accuracy': 0.17357999086380005, 'validation/loss': 4.842849254608154, 'validation/num_examples': 50000, 'test/accuracy': 0.11590000241994858, 'test/loss': 5.787625312805176, 'test/num_examples': 10000, 'score': 9214.40810918808, 'total_duration': 9548.710106372833, 'accumulated_submission_time': 9214.40810918808, 'accumulated_eval_time': 332.8436703681946, 'accumulated_logging_time': 0.5292325019836426, 'global_step': 26965, 'preemption_count': 0}), (28467, {'train/accuracy': 0.1595384180545807, 'train/loss': 5.077648639678955, 'validation/accuracy': 0.14961999654769897, 'validation/loss': 5.2059807777404785, 'validation/num_examples': 50000, 'test/accuracy': 0.11750000715255737, 'test/loss': 5.716974258422852, 'test/num_examples': 10000, 'score': 9724.49069738388, 'total_duration': 10076.344948291779, 'accumulated_submission_time': 9724.49069738388, 'accumulated_eval_time': 350.31236839294434, 'accumulated_logging_time': 0.5601718425750732, 'global_step': 28467, 'preemption_count': 0}), (29969, {'train/accuracy': 0.2542450428009033, 'train/loss': 3.912951707839966, 'validation/accuracy': 0.23863999545574188, 'validation/loss': 4.062727928161621, 'validation/num_examples': 50000, 'test/accuracy': 0.18040001392364502, 'test/loss': 4.745062351226807, 'test/num_examples': 10000, 'score': 10234.423271417618, 'total_duration': 10603.58622789383, 'accumulated_submission_time': 10234.423271417618, 'accumulated_eval_time': 367.536589384079, 'accumulated_logging_time': 0.5917963981628418, 'global_step': 29969, 'preemption_count': 0}), (31471, {'train/accuracy': 0.34287306666374207, 'train/loss': 3.108630418777466, 'validation/accuracy': 0.33239999413490295, 'validation/loss': 3.178079843521118, 'validation/num_examples': 50000, 'test/accuracy': 0.24860000610351562, 'test/loss': 3.8949763774871826, 'test/num_examples': 10000, 'score': 10744.5173869133, 'total_duration': 11131.000636100769, 'accumulated_submission_time': 10744.5173869133, 'accumulated_eval_time': 384.77102971076965, 'accumulated_logging_time': 0.6250383853912354, 'global_step': 31471, 'preemption_count': 0}), (32974, {'train/accuracy': 0.27313855290412903, 'train/loss': 4.065606594085693, 'validation/accuracy': 0.26225998997688293, 'validation/loss': 4.184144973754883, 'validation/num_examples': 50000, 'test/accuracy': 0.19510000944137573, 'test/loss': 5.094240665435791, 'test/num_examples': 10000, 'score': 11254.584214448929, 'total_duration': 11658.715369939804, 'accumulated_submission_time': 11254.584214448929, 'accumulated_eval_time': 402.33535385131836, 'accumulated_logging_time': 0.6580126285552979, 'global_step': 32974, 'preemption_count': 0}), (34477, {'train/accuracy': 0.1878587305545807, 'train/loss': 4.851592540740967, 'validation/accuracy': 0.17073999345302582, 'validation/loss': 5.058218955993652, 'validation/num_examples': 50000, 'test/accuracy': 0.11810000240802765, 'test/loss': 5.938706398010254, 'test/num_examples': 10000, 'score': 11764.796533107758, 'total_duration': 12186.59768295288, 'accumulated_submission_time': 11764.796533107758, 'accumulated_eval_time': 419.9206705093384, 'accumulated_logging_time': 0.6911814212799072, 'global_step': 34477, 'preemption_count': 0}), (35980, {'train/accuracy': 0.24667170643806458, 'train/loss': 4.081879615783691, 'validation/accuracy': 0.231019988656044, 'validation/loss': 4.22337007522583, 'validation/num_examples': 50000, 'test/accuracy': 0.1688000112771988, 'test/loss': 4.942447185516357, 'test/num_examples': 10000, 'score': 12274.719363689423, 'total_duration': 12714.083587408066, 'accumulated_submission_time': 12274.719363689423, 'accumulated_eval_time': 437.3967123031616, 'accumulated_logging_time': 0.727304220199585, 'global_step': 35980, 'preemption_count': 0}), (37484, {'train/accuracy': 0.2384207546710968, 'train/loss': 4.461237907409668, 'validation/accuracy': 0.22425998747348785, 'validation/loss': 4.545414447784424, 'validation/num_examples': 50000, 'test/accuracy': 0.15650001168251038, 'test/loss': 5.578945159912109, 'test/num_examples': 10000, 'score': 12784.916873216629, 'total_duration': 13241.621287107468, 'accumulated_submission_time': 12784.916873216629, 'accumulated_eval_time': 454.6478660106659, 'accumulated_logging_time': 0.764479398727417, 'global_step': 37484, 'preemption_count': 0}), (38987, {'train/accuracy': 0.2852160334587097, 'train/loss': 3.7270145416259766, 'validation/accuracy': 0.2743600010871887, 'validation/loss': 3.8535454273223877, 'validation/num_examples': 50000, 'test/accuracy': 0.19460001587867737, 'test/loss': 4.6907501220703125, 'test/num_examples': 10000, 'score': 13295.052711725235, 'total_duration': 13769.374715805054, 'accumulated_submission_time': 13295.052711725235, 'accumulated_eval_time': 472.18113017082214, 'accumulated_logging_time': 0.797905683517456, 'global_step': 38987, 'preemption_count': 0}), (40490, {'train/accuracy': 0.3294602930545807, 'train/loss': 3.2785985469818115, 'validation/accuracy': 0.30687999725341797, 'validation/loss': 3.4649877548217773, 'validation/num_examples': 50000, 'test/accuracy': 0.24260000884532928, 'test/loss': 4.134172439575195, 'test/num_examples': 10000, 'score': 13804.977420091629, 'total_duration': 14296.749955415726, 'accumulated_submission_time': 13804.977420091629, 'accumulated_eval_time': 489.544287443161, 'accumulated_logging_time': 0.8351831436157227, 'global_step': 40490, 'preemption_count': 0}), (41994, {'train/accuracy': 0.10829878598451614, 'train/loss': 6.09940767288208, 'validation/accuracy': 0.10523999482393265, 'validation/loss': 6.120722770690918, 'validation/num_examples': 50000, 'test/accuracy': 0.07490000128746033, 'test/loss': 6.715982437133789, 'test/num_examples': 10000, 'score': 14315.08835530281, 'total_duration': 14824.222242355347, 'accumulated_submission_time': 14315.08835530281, 'accumulated_eval_time': 506.81652092933655, 'accumulated_logging_time': 0.8721778392791748, 'global_step': 41994, 'preemption_count': 0}), (43497, {'train/accuracy': 0.13149712979793549, 'train/loss': 4.971019744873047, 'validation/accuracy': 0.11965999752283096, 'validation/loss': 5.1093292236328125, 'validation/num_examples': 50000, 'test/accuracy': 0.08090000599622726, 'test/loss': 5.751832008361816, 'test/num_examples': 10000, 'score': 14825.161313533783, 'total_duration': 15351.805294513702, 'accumulated_submission_time': 14825.161313533783, 'accumulated_eval_time': 524.241007566452, 'accumulated_logging_time': 0.9064865112304688, 'global_step': 43497, 'preemption_count': 0}), (44999, {'train/accuracy': 0.17918924987316132, 'train/loss': 5.077031135559082, 'validation/accuracy': 0.16359999775886536, 'validation/loss': 5.281920433044434, 'validation/num_examples': 50000, 'test/accuracy': 0.12550000846385956, 'test/loss': 5.8240647315979, 'test/num_examples': 10000, 'score': 15335.105654001236, 'total_duration': 15879.07846212387, 'accumulated_submission_time': 15335.105654001236, 'accumulated_eval_time': 541.479898929596, 'accumulated_logging_time': 0.9436588287353516, 'global_step': 44999, 'preemption_count': 0}), (46503, {'train/accuracy': 0.3250558078289032, 'train/loss': 3.3820459842681885, 'validation/accuracy': 0.3132599890232086, 'validation/loss': 3.5227527618408203, 'validation/num_examples': 50000, 'test/accuracy': 0.23610001802444458, 'test/loss': 4.2942023277282715, 'test/num_examples': 10000, 'score': 15845.155004501343, 'total_duration': 16406.597977876663, 'accumulated_submission_time': 15845.155004501343, 'accumulated_eval_time': 558.8642518520355, 'accumulated_logging_time': 0.9791169166564941, 'global_step': 46503, 'preemption_count': 0}), (48007, {'train/accuracy': 0.3681640625, 'train/loss': 3.0340774059295654, 'validation/accuracy': 0.34505999088287354, 'validation/loss': 3.1679186820983887, 'validation/num_examples': 50000, 'test/accuracy': 0.26680001616477966, 'test/loss': 3.835937261581421, 'test/num_examples': 10000, 'score': 16355.190060853958, 'total_duration': 16934.3196310997, 'accumulated_submission_time': 16355.190060853958, 'accumulated_eval_time': 576.4580047130585, 'accumulated_logging_time': 1.020794153213501, 'global_step': 48007, 'preemption_count': 0}), (49511, {'train/accuracy': 0.2969347834587097, 'train/loss': 3.6690821647644043, 'validation/accuracy': 0.28205999732017517, 'validation/loss': 3.842806100845337, 'validation/num_examples': 50000, 'test/accuracy': 0.20390000939369202, 'test/loss': 4.691739082336426, 'test/num_examples': 10000, 'score': 16865.38143491745, 'total_duration': 17461.97078728676, 'accumulated_submission_time': 16865.38143491745, 'accumulated_eval_time': 593.8302228450775, 'accumulated_logging_time': 1.0560753345489502, 'global_step': 49511, 'preemption_count': 0}), (51014, {'train/accuracy': 0.15383848547935486, 'train/loss': 4.8142242431640625, 'validation/accuracy': 0.14691999554634094, 'validation/loss': 4.868548393249512, 'validation/num_examples': 50000, 'test/accuracy': 0.10580000281333923, 'test/loss': 5.460964679718018, 'test/num_examples': 10000, 'score': 17375.29843711853, 'total_duration': 17989.424177646637, 'accumulated_submission_time': 17375.29843711853, 'accumulated_eval_time': 611.2783124446869, 'accumulated_logging_time': 1.094315767288208, 'global_step': 51014, 'preemption_count': 0}), (52519, {'train/accuracy': 0.45874521136283875, 'train/loss': 2.4128456115722656, 'validation/accuracy': 0.39997997879981995, 'validation/loss': 2.8159847259521484, 'validation/num_examples': 50000, 'test/accuracy': 0.30150002241134644, 'test/loss': 3.607529878616333, 'test/num_examples': 10000, 'score': 17885.518147945404, 'total_duration': 18517.496727705002, 'accumulated_submission_time': 17885.518147945404, 'accumulated_eval_time': 629.0414938926697, 'accumulated_logging_time': 1.1306431293487549, 'global_step': 52519, 'preemption_count': 0}), (54023, {'train/accuracy': 0.23064811527729034, 'train/loss': 4.139097213745117, 'validation/accuracy': 0.21379999816417694, 'validation/loss': 4.321166038513184, 'validation/num_examples': 50000, 'test/accuracy': 0.16940000653266907, 'test/loss': 4.901164531707764, 'test/num_examples': 10000, 'score': 18395.49843478203, 'total_duration': 19044.81125664711, 'accumulated_submission_time': 18395.49843478203, 'accumulated_eval_time': 646.2846746444702, 'accumulated_logging_time': 1.1675300598144531, 'global_step': 54023, 'preemption_count': 0}), (55527, {'train/accuracy': 0.2505779564380646, 'train/loss': 3.9523448944091797, 'validation/accuracy': 0.23389999568462372, 'validation/loss': 4.098372936248779, 'validation/num_examples': 50000, 'test/accuracy': 0.172200009226799, 'test/loss': 4.754857063293457, 'test/num_examples': 10000, 'score': 18905.47820210457, 'total_duration': 19573.068316936493, 'accumulated_submission_time': 18905.47820210457, 'accumulated_eval_time': 664.4693939685822, 'accumulated_logging_time': 1.2078561782836914, 'global_step': 55527, 'preemption_count': 0}), (57032, {'train/accuracy': 0.12844786047935486, 'train/loss': 5.659578323364258, 'validation/accuracy': 0.12205999344587326, 'validation/loss': 5.704127788543701, 'validation/num_examples': 50000, 'test/accuracy': 0.0820000022649765, 'test/loss': 6.516331672668457, 'test/num_examples': 10000, 'score': 19415.634298086166, 'total_duration': 20100.56913280487, 'accumulated_submission_time': 19415.634298086166, 'accumulated_eval_time': 681.7302577495575, 'accumulated_logging_time': 1.2400367259979248, 'global_step': 57032, 'preemption_count': 0}), (58537, {'train/accuracy': 0.21765385568141937, 'train/loss': 4.364476680755615, 'validation/accuracy': 0.20535999536514282, 'validation/loss': 4.498104095458984, 'validation/num_examples': 50000, 'test/accuracy': 0.15230000019073486, 'test/loss': 5.230589866638184, 'test/num_examples': 10000, 'score': 19925.873183965683, 'total_duration': 20628.083652734756, 'accumulated_submission_time': 19925.873183965683, 'accumulated_eval_time': 698.9143342971802, 'accumulated_logging_time': 1.2803306579589844, 'global_step': 58537, 'preemption_count': 0}), (60041, {'train/accuracy': 0.1741868555545807, 'train/loss': 5.3501811027526855, 'validation/accuracy': 0.16638000309467316, 'validation/loss': 5.571073055267334, 'validation/num_examples': 50000, 'test/accuracy': 0.13580000400543213, 'test/loss': 6.057143211364746, 'test/num_examples': 10000, 'score': 20435.8396422863, 'total_duration': 21155.63431572914, 'accumulated_submission_time': 20435.8396422863, 'accumulated_eval_time': 716.4034960269928, 'accumulated_logging_time': 1.3220326900482178, 'global_step': 60041, 'preemption_count': 0}), (61546, {'train/accuracy': 0.20653299987316132, 'train/loss': 4.479652404785156, 'validation/accuracy': 0.18648000061511993, 'validation/loss': 4.748819351196289, 'validation/num_examples': 50000, 'test/accuracy': 0.14710000157356262, 'test/loss': 5.390626907348633, 'test/num_examples': 10000, 'score': 20945.94157910347, 'total_duration': 21683.503759384155, 'accumulated_submission_time': 20945.94157910347, 'accumulated_eval_time': 734.0819170475006, 'accumulated_logging_time': 1.3605809211730957, 'global_step': 61546, 'preemption_count': 0}), (63052, {'train/accuracy': 0.3102877736091614, 'train/loss': 3.5084850788116455, 'validation/accuracy': 0.2889399826526642, 'validation/loss': 3.657221794128418, 'validation/num_examples': 50000, 'test/accuracy': 0.21540001034736633, 'test/loss': 4.478518962860107, 'test/num_examples': 10000, 'score': 21456.14753627777, 'total_duration': 22211.436054468155, 'accumulated_submission_time': 21456.14753627777, 'accumulated_eval_time': 751.711775302887, 'accumulated_logging_time': 1.405496597290039, 'global_step': 63052, 'preemption_count': 0}), (64556, {'train/accuracy': 0.30291372537612915, 'train/loss': 3.726241111755371, 'validation/accuracy': 0.28321999311447144, 'validation/loss': 3.931478977203369, 'validation/num_examples': 50000, 'test/accuracy': 0.21070000529289246, 'test/loss': 4.761738300323486, 'test/num_examples': 10000, 'score': 21966.07893872261, 'total_duration': 22738.717492341995, 'accumulated_submission_time': 21966.07893872261, 'accumulated_eval_time': 768.9775350093842, 'accumulated_logging_time': 1.4379546642303467, 'global_step': 64556, 'preemption_count': 0}), (66061, {'train/accuracy': 0.34384962916374207, 'train/loss': 3.1373069286346436, 'validation/accuracy': 0.3238599896430969, 'validation/loss': 3.2995736598968506, 'validation/num_examples': 50000, 'test/accuracy': 0.24240000545978546, 'test/loss': 4.018667221069336, 'test/num_examples': 10000, 'score': 22476.09085536003, 'total_duration': 23266.313386917114, 'accumulated_submission_time': 22476.09085536003, 'accumulated_eval_time': 786.4678730964661, 'accumulated_logging_time': 1.4794397354125977, 'global_step': 66061, 'preemption_count': 0}), (67566, {'train/accuracy': 0.32421875, 'train/loss': 3.3596620559692383, 'validation/accuracy': 0.31985998153686523, 'validation/loss': 3.3958446979522705, 'validation/num_examples': 50000, 'test/accuracy': 0.2331000119447708, 'test/loss': 4.2524847984313965, 'test/num_examples': 10000, 'score': 22986.0070104599, 'total_duration': 23793.81243133545, 'accumulated_submission_time': 22986.0070104599, 'accumulated_eval_time': 803.9587597846985, 'accumulated_logging_time': 1.5198431015014648, 'global_step': 67566, 'preemption_count': 0}), (69071, {'train/accuracy': 0.3446069657802582, 'train/loss': 3.2224009037017822, 'validation/accuracy': 0.32811999320983887, 'validation/loss': 3.3014426231384277, 'validation/num_examples': 50000, 'test/accuracy': 0.2605000138282776, 'test/loss': 3.976634979248047, 'test/num_examples': 10000, 'score': 23496.11438536644, 'total_duration': 24321.4155292511, 'accumulated_submission_time': 23496.11438536644, 'accumulated_eval_time': 821.3490543365479, 'accumulated_logging_time': 1.5737159252166748, 'global_step': 69071, 'preemption_count': 0}), (70576, {'train/accuracy': 0.3404615819454193, 'train/loss': 3.1158764362335205, 'validation/accuracy': 0.3131199777126312, 'validation/loss': 3.278167486190796, 'validation/num_examples': 50000, 'test/accuracy': 0.23570001125335693, 'test/loss': 3.938102960586548, 'test/num_examples': 10000, 'score': 24006.213018655777, 'total_duration': 24849.170696020126, 'accumulated_submission_time': 24006.213018655777, 'accumulated_eval_time': 838.9119355678558, 'accumulated_logging_time': 1.6166942119598389, 'global_step': 70576, 'preemption_count': 0}), (72081, {'train/accuracy': 0.46934789419174194, 'train/loss': 2.3059208393096924, 'validation/accuracy': 0.4316200017929077, 'validation/loss': 2.5475010871887207, 'validation/num_examples': 50000, 'test/accuracy': 0.31870001554489136, 'test/loss': 3.359588146209717, 'test/num_examples': 10000, 'score': 24516.367866277695, 'total_duration': 25376.83913421631, 'accumulated_submission_time': 24516.367866277695, 'accumulated_eval_time': 856.3283641338348, 'accumulated_logging_time': 1.6619768142700195, 'global_step': 72081, 'preemption_count': 0}), (73586, {'train/accuracy': 0.5002591013908386, 'train/loss': 2.164903163909912, 'validation/accuracy': 0.45933997631073, 'validation/loss': 2.4115843772888184, 'validation/num_examples': 50000, 'test/accuracy': 0.3611000180244446, 'test/loss': 3.1056861877441406, 'test/num_examples': 10000, 'score': 25026.566119670868, 'total_duration': 25904.495133399963, 'accumulated_submission_time': 25026.566119670868, 'accumulated_eval_time': 873.6862845420837, 'accumulated_logging_time': 1.7103638648986816, 'global_step': 73586, 'preemption_count': 0}), (75091, {'train/accuracy': 0.3360172212123871, 'train/loss': 3.280001163482666, 'validation/accuracy': 0.3144199848175049, 'validation/loss': 3.4575531482696533, 'validation/num_examples': 50000, 'test/accuracy': 0.23020000755786896, 'test/loss': 4.258174419403076, 'test/num_examples': 10000, 'score': 25536.524069309235, 'total_duration': 26432.089807510376, 'accumulated_submission_time': 25536.524069309235, 'accumulated_eval_time': 891.2308826446533, 'accumulated_logging_time': 1.750760555267334, 'global_step': 75091, 'preemption_count': 0}), (76595, {'train/accuracy': 0.36898118257522583, 'train/loss': 3.100395917892456, 'validation/accuracy': 0.3448599874973297, 'validation/loss': 3.2606098651885986, 'validation/num_examples': 50000, 'test/accuracy': 0.2645000219345093, 'test/loss': 4.023133277893066, 'test/num_examples': 10000, 'score': 26046.467567443848, 'total_duration': 26959.668189525604, 'accumulated_submission_time': 26046.467567443848, 'accumulated_eval_time': 908.7737927436829, 'accumulated_logging_time': 1.7912750244140625, 'global_step': 76595, 'preemption_count': 0}), (78100, {'train/accuracy': 0.2097417116165161, 'train/loss': 4.448638916015625, 'validation/accuracy': 0.19473999738693237, 'validation/loss': 4.624502658843994, 'validation/num_examples': 50000, 'test/accuracy': 0.1477000117301941, 'test/loss': 5.23762321472168, 'test/num_examples': 10000, 'score': 26556.542511701584, 'total_duration': 27487.187576293945, 'accumulated_submission_time': 26556.542511701584, 'accumulated_eval_time': 926.1273169517517, 'accumulated_logging_time': 1.8315112590789795, 'global_step': 78100, 'preemption_count': 0}), (79604, {'train/accuracy': 0.22132094204425812, 'train/loss': 4.309572219848633, 'validation/accuracy': 0.20763999223709106, 'validation/loss': 4.423527240753174, 'validation/num_examples': 50000, 'test/accuracy': 0.14880000054836273, 'test/loss': 5.156861782073975, 'test/num_examples': 10000, 'score': 27066.480364322662, 'total_duration': 28014.59451031685, 'accumulated_submission_time': 27066.480364322662, 'accumulated_eval_time': 943.505437374115, 'accumulated_logging_time': 1.8714027404785156, 'global_step': 79604, 'preemption_count': 0}), (81109, {'train/accuracy': 0.33380499482154846, 'train/loss': 3.2794435024261475, 'validation/accuracy': 0.3070800006389618, 'validation/loss': 3.5193474292755127, 'validation/num_examples': 50000, 'test/accuracy': 0.23000000417232513, 'test/loss': 4.20773458480835, 'test/num_examples': 10000, 'score': 27576.634883880615, 'total_duration': 28542.41470336914, 'accumulated_submission_time': 27576.634883880615, 'accumulated_eval_time': 961.0810222625732, 'accumulated_logging_time': 1.9099252223968506, 'global_step': 81109, 'preemption_count': 0}), (82614, {'train/accuracy': 0.395228773355484, 'train/loss': 2.817749261856079, 'validation/accuracy': 0.367499977350235, 'validation/loss': 3.013368606567383, 'validation/num_examples': 50000, 'test/accuracy': 0.28200000524520874, 'test/loss': 3.757661819458008, 'test/num_examples': 10000, 'score': 28086.697027683258, 'total_duration': 29070.151161670685, 'accumulated_submission_time': 28086.697027683258, 'accumulated_eval_time': 978.6555554866791, 'accumulated_logging_time': 1.955564022064209, 'global_step': 82614, 'preemption_count': 0}), (84119, {'train/accuracy': 0.3988759517669678, 'train/loss': 2.8061952590942383, 'validation/accuracy': 0.3729199767112732, 'validation/loss': 3.0000438690185547, 'validation/num_examples': 50000, 'test/accuracy': 0.2729000151157379, 'test/loss': 3.788983106613159, 'test/num_examples': 10000, 'score': 28596.871851682663, 'total_duration': 29597.85049009323, 'accumulated_submission_time': 28596.871851682663, 'accumulated_eval_time': 996.0850801467896, 'accumulated_logging_time': 1.9987788200378418, 'global_step': 84119, 'preemption_count': 0}), (85624, {'train/accuracy': 0.49443957209587097, 'train/loss': 2.2480671405792236, 'validation/accuracy': 0.4607999920845032, 'validation/loss': 2.4581475257873535, 'validation/num_examples': 50000, 'test/accuracy': 0.35190001130104065, 'test/loss': 3.282273292541504, 'test/num_examples': 10000, 'score': 29107.02249646187, 'total_duration': 30125.56484889984, 'accumulated_submission_time': 29107.02249646187, 'accumulated_eval_time': 1013.5502715110779, 'accumulated_logging_time': 2.0445313453674316, 'global_step': 85624, 'preemption_count': 0}), (87129, {'train/accuracy': 0.1268136203289032, 'train/loss': 5.291484355926514, 'validation/accuracy': 0.12189999967813492, 'validation/loss': 5.354971885681152, 'validation/num_examples': 50000, 'test/accuracy': 0.08920000493526459, 'test/loss': 5.882399559020996, 'test/num_examples': 10000, 'score': 29617.017424106598, 'total_duration': 30653.171503305435, 'accumulated_submission_time': 29617.017424106598, 'accumulated_eval_time': 1031.0666897296906, 'accumulated_logging_time': 2.0885136127471924, 'global_step': 87129, 'preemption_count': 0}), (88633, {'train/accuracy': 0.4404296875, 'train/loss': 2.522449254989624, 'validation/accuracy': 0.41787999868392944, 'validation/loss': 2.665285348892212, 'validation/num_examples': 50000, 'test/accuracy': 0.31200000643730164, 'test/loss': 3.441679000854492, 'test/num_examples': 10000, 'score': 30126.987594604492, 'total_duration': 31180.547289133072, 'accumulated_submission_time': 30126.987594604492, 'accumulated_eval_time': 1048.3781082630157, 'accumulated_logging_time': 2.131108283996582, 'global_step': 88633, 'preemption_count': 0}), (90138, {'train/accuracy': 0.5174585580825806, 'train/loss': 2.07250714302063, 'validation/accuracy': 0.47110000252723694, 'validation/loss': 2.372112512588501, 'validation/num_examples': 50000, 'test/accuracy': 0.3660000264644623, 'test/loss': 3.1758997440338135, 'test/num_examples': 10000, 'score': 30637.007561683655, 'total_duration': 31708.120919704437, 'accumulated_submission_time': 30637.007561683655, 'accumulated_eval_time': 1065.8279626369476, 'accumulated_logging_time': 2.1842033863067627, 'global_step': 90138, 'preemption_count': 0}), (91643, {'train/accuracy': 0.4838368892669678, 'train/loss': 2.2562386989593506, 'validation/accuracy': 0.44661998748779297, 'validation/loss': 2.4910802841186523, 'validation/num_examples': 50000, 'test/accuracy': 0.3335000276565552, 'test/loss': 3.2831525802612305, 'test/num_examples': 10000, 'score': 31147.1129257679, 'total_duration': 32236.46764421463, 'accumulated_submission_time': 31147.1129257679, 'accumulated_eval_time': 1083.9716200828552, 'accumulated_logging_time': 2.2286715507507324, 'global_step': 91643, 'preemption_count': 0}), (93148, {'train/accuracy': 0.5083904266357422, 'train/loss': 2.1743996143341064, 'validation/accuracy': 0.47637999057769775, 'validation/loss': 2.3738784790039062, 'validation/num_examples': 50000, 'test/accuracy': 0.3620000183582306, 'test/loss': 3.1680712699890137, 'test/num_examples': 10000, 'score': 31657.057655096054, 'total_duration': 32763.91767191887, 'accumulated_submission_time': 31657.057655096054, 'accumulated_eval_time': 1101.381390094757, 'accumulated_logging_time': 2.27234148979187, 'global_step': 93148, 'preemption_count': 0}), (94650, {'train/accuracy': 0.545918345451355, 'train/loss': 1.944828987121582, 'validation/accuracy': 0.5107399821281433, 'validation/loss': 2.1304807662963867, 'validation/num_examples': 50000, 'test/accuracy': 0.39640000462532043, 'test/loss': 2.8958516120910645, 'test/num_examples': 10000, 'score': 32167.045583724976, 'total_duration': 33291.27138733864, 'accumulated_submission_time': 32167.045583724976, 'accumulated_eval_time': 1118.651871919632, 'accumulated_logging_time': 2.31717848777771, 'global_step': 94650, 'preemption_count': 0}), (96154, {'train/accuracy': 0.3833506107330322, 'train/loss': 2.867633581161499, 'validation/accuracy': 0.35995998978614807, 'validation/loss': 3.0436670780181885, 'validation/num_examples': 50000, 'test/accuracy': 0.2696000039577484, 'test/loss': 3.7725107669830322, 'test/num_examples': 10000, 'score': 32676.99285697937, 'total_duration': 33818.811498880386, 'accumulated_submission_time': 32676.99285697937, 'accumulated_eval_time': 1136.1447837352753, 'accumulated_logging_time': 2.3657150268554688, 'global_step': 96154, 'preemption_count': 0}), (97659, {'train/accuracy': 0.5059789419174194, 'train/loss': 2.157892942428589, 'validation/accuracy': 0.47516000270843506, 'validation/loss': 2.326932668685913, 'validation/num_examples': 50000, 'test/accuracy': 0.35910001397132874, 'test/loss': 3.0999302864074707, 'test/num_examples': 10000, 'score': 33187.0912899971, 'total_duration': 34346.33107614517, 'accumulated_submission_time': 33187.0912899971, 'accumulated_eval_time': 1153.468049287796, 'accumulated_logging_time': 2.4127352237701416, 'global_step': 97659, 'preemption_count': 0}), (99164, {'train/accuracy': 0.5677016973495483, 'train/loss': 1.840865135192871, 'validation/accuracy': 0.5112400054931641, 'validation/loss': 2.122875928878784, 'validation/num_examples': 50000, 'test/accuracy': 0.3920000195503235, 'test/loss': 2.8797783851623535, 'test/num_examples': 10000, 'score': 33697.07434248924, 'total_duration': 34873.93353009224, 'accumulated_submission_time': 33697.07434248924, 'accumulated_eval_time': 1170.9882607460022, 'accumulated_logging_time': 2.4600107669830322, 'global_step': 99164, 'preemption_count': 0}), (100669, {'train/accuracy': 0.4465082883834839, 'train/loss': 2.48357892036438, 'validation/accuracy': 0.4145599901676178, 'validation/loss': 2.690641403198242, 'validation/num_examples': 50000, 'test/accuracy': 0.30480000376701355, 'test/loss': 3.538769245147705, 'test/num_examples': 10000, 'score': 34207.1025724411, 'total_duration': 35401.42611813545, 'accumulated_submission_time': 34207.1025724411, 'accumulated_eval_time': 1188.3502779006958, 'accumulated_logging_time': 2.5082507133483887, 'global_step': 100669, 'preemption_count': 0}), (102174, {'train/accuracy': 0.5240353941917419, 'train/loss': 2.0571160316467285, 'validation/accuracy': 0.4918999969959259, 'validation/loss': 2.25252103805542, 'validation/num_examples': 50000, 'test/accuracy': 0.3846000134944916, 'test/loss': 2.9995460510253906, 'test/num_examples': 10000, 'score': 34717.1273932457, 'total_duration': 35928.986558914185, 'accumulated_submission_time': 34717.1273932457, 'accumulated_eval_time': 1205.7857220172882, 'accumulated_logging_time': 2.5568745136260986, 'global_step': 102174, 'preemption_count': 0}), (103679, {'train/accuracy': 0.5104631781578064, 'train/loss': 2.159630060195923, 'validation/accuracy': 0.471919983625412, 'validation/loss': 2.3893167972564697, 'validation/num_examples': 50000, 'test/accuracy': 0.3644000291824341, 'test/loss': 3.1780288219451904, 'test/num_examples': 10000, 'score': 35227.254885435104, 'total_duration': 36456.76866769791, 'accumulated_submission_time': 35227.254885435104, 'accumulated_eval_time': 1223.3412280082703, 'accumulated_logging_time': 2.6039724349975586, 'global_step': 103679, 'preemption_count': 0}), (105184, {'train/accuracy': 0.5213249325752258, 'train/loss': 2.0907678604125977, 'validation/accuracy': 0.48767998814582825, 'validation/loss': 2.284471035003662, 'validation/num_examples': 50000, 'test/accuracy': 0.388700008392334, 'test/loss': 2.9926905632019043, 'test/num_examples': 10000, 'score': 35737.33686733246, 'total_duration': 36984.278197050095, 'accumulated_submission_time': 35737.33686733246, 'accumulated_eval_time': 1240.666074514389, 'accumulated_logging_time': 2.6544456481933594, 'global_step': 105184, 'preemption_count': 0}), (106689, {'train/accuracy': 0.5049425959587097, 'train/loss': 2.1846439838409424, 'validation/accuracy': 0.47189998626708984, 'validation/loss': 2.3848423957824707, 'validation/num_examples': 50000, 'test/accuracy': 0.36070001125335693, 'test/loss': 3.1824216842651367, 'test/num_examples': 10000, 'score': 36247.34074640274, 'total_duration': 37511.78411793709, 'accumulated_submission_time': 36247.34074640274, 'accumulated_eval_time': 1258.0668041706085, 'accumulated_logging_time': 2.7028579711914062, 'global_step': 106689, 'preemption_count': 0}), (108194, {'train/accuracy': 0.48692601919174194, 'train/loss': 2.2414798736572266, 'validation/accuracy': 0.43511998653411865, 'validation/loss': 2.5659117698669434, 'validation/num_examples': 50000, 'test/accuracy': 0.31310001015663147, 'test/loss': 3.4565787315368652, 'test/num_examples': 10000, 'score': 36757.33936190605, 'total_duration': 38039.08936858177, 'accumulated_submission_time': 36757.33936190605, 'accumulated_eval_time': 1275.2838730812073, 'accumulated_logging_time': 2.7409493923187256, 'global_step': 108194, 'preemption_count': 0}), (109699, {'train/accuracy': 0.5515983700752258, 'train/loss': 1.900874376296997, 'validation/accuracy': 0.5029199719429016, 'validation/loss': 2.1730642318725586, 'validation/num_examples': 50000, 'test/accuracy': 0.39980003237724304, 'test/loss': 2.946348190307617, 'test/num_examples': 10000, 'score': 37267.33650159836, 'total_duration': 38566.54166126251, 'accumulated_submission_time': 37267.33650159836, 'accumulated_eval_time': 1292.6387770175934, 'accumulated_logging_time': 2.7895431518554688, 'global_step': 109699, 'preemption_count': 0}), (111203, {'train/accuracy': 0.5053411722183228, 'train/loss': 2.157813310623169, 'validation/accuracy': 0.46757999062538147, 'validation/loss': 2.3963406085968018, 'validation/num_examples': 50000, 'test/accuracy': 0.35910001397132874, 'test/loss': 3.1881215572357178, 'test/num_examples': 10000, 'score': 37777.31170344353, 'total_duration': 39094.161532878876, 'accumulated_submission_time': 37777.31170344353, 'accumulated_eval_time': 1310.1845960617065, 'accumulated_logging_time': 2.837157964706421, 'global_step': 111203, 'preemption_count': 0}), (112708, {'train/accuracy': 0.48280054330825806, 'train/loss': 2.318225383758545, 'validation/accuracy': 0.4461199939250946, 'validation/loss': 2.5451242923736572, 'validation/num_examples': 50000, 'test/accuracy': 0.3433000147342682, 'test/loss': 3.334402084350586, 'test/num_examples': 10000, 'score': 38287.42334794998, 'total_duration': 39621.92883038521, 'accumulated_submission_time': 38287.42334794998, 'accumulated_eval_time': 1327.739814043045, 'accumulated_logging_time': 2.886847972869873, 'global_step': 112708, 'preemption_count': 0}), (114214, {'train/accuracy': 0.5449019074440002, 'train/loss': 1.9557873010635376, 'validation/accuracy': 0.5061599612236023, 'validation/loss': 2.1710429191589355, 'validation/num_examples': 50000, 'test/accuracy': 0.38360002636909485, 'test/loss': 3.029585599899292, 'test/num_examples': 10000, 'score': 38797.589587688446, 'total_duration': 40149.82967543602, 'accumulated_submission_time': 38797.589587688446, 'accumulated_eval_time': 1345.3750817775726, 'accumulated_logging_time': 2.9340431690216064, 'global_step': 114214, 'preemption_count': 0}), (115718, {'train/accuracy': 0.5570591688156128, 'train/loss': 1.8826926946640015, 'validation/accuracy': 0.527999997138977, 'validation/loss': 2.093355178833008, 'validation/num_examples': 50000, 'test/accuracy': 0.4082000255584717, 'test/loss': 2.8958003520965576, 'test/num_examples': 10000, 'score': 39307.54098248482, 'total_duration': 40677.041610240936, 'accumulated_submission_time': 39307.54098248482, 'accumulated_eval_time': 1362.5382542610168, 'accumulated_logging_time': 2.980867624282837, 'global_step': 115718, 'preemption_count': 0}), (117223, {'train/accuracy': 0.6294443607330322, 'train/loss': 1.476343035697937, 'validation/accuracy': 0.5562199950218201, 'validation/loss': 1.8933178186416626, 'validation/num_examples': 50000, 'test/accuracy': 0.4335000216960907, 'test/loss': 2.682591438293457, 'test/num_examples': 10000, 'score': 39817.586441755295, 'total_duration': 41204.382727622986, 'accumulated_submission_time': 39817.586441755295, 'accumulated_eval_time': 1379.7358441352844, 'accumulated_logging_time': 3.027287483215332, 'global_step': 117223, 'preemption_count': 0}), (118728, {'train/accuracy': 0.45798787474632263, 'train/loss': 2.500563621520996, 'validation/accuracy': 0.42545998096466064, 'validation/loss': 2.721994638442993, 'validation/num_examples': 50000, 'test/accuracy': 0.3305000066757202, 'test/loss': 3.4887149333953857, 'test/num_examples': 10000, 'score': 40327.71361851692, 'total_duration': 41731.77513575554, 'accumulated_submission_time': 40327.71361851692, 'accumulated_eval_time': 1396.9002561569214, 'accumulated_logging_time': 3.074965238571167, 'global_step': 118728, 'preemption_count': 0}), (120233, {'train/accuracy': 0.6004663705825806, 'train/loss': 1.6669267416000366, 'validation/accuracy': 0.5487599968910217, 'validation/loss': 1.962204933166504, 'validation/num_examples': 50000, 'test/accuracy': 0.4288000166416168, 'test/loss': 2.7302029132843018, 'test/num_examples': 10000, 'score': 40837.7944047451, 'total_duration': 42259.27208185196, 'accumulated_submission_time': 40837.7944047451, 'accumulated_eval_time': 1414.213954925537, 'accumulated_logging_time': 3.124530076980591, 'global_step': 120233, 'preemption_count': 0}), (121738, {'train/accuracy': 0.5869738459587097, 'train/loss': 1.7295794486999512, 'validation/accuracy': 0.5414199829101562, 'validation/loss': 1.9991382360458374, 'validation/num_examples': 50000, 'test/accuracy': 0.42270001769065857, 'test/loss': 2.8023416996002197, 'test/num_examples': 10000, 'score': 41347.86622738838, 'total_duration': 42786.84964418411, 'accumulated_submission_time': 41347.86622738838, 'accumulated_eval_time': 1431.6174721717834, 'accumulated_logging_time': 3.175598382949829, 'global_step': 121738, 'preemption_count': 0}), (123244, {'train/accuracy': 0.5721460580825806, 'train/loss': 1.803208351135254, 'validation/accuracy': 0.5319399833679199, 'validation/loss': 2.029165267944336, 'validation/num_examples': 50000, 'test/accuracy': 0.3977000117301941, 'test/loss': 2.858058214187622, 'test/num_examples': 10000, 'score': 41858.08864212036, 'total_duration': 43314.71329832077, 'accumulated_submission_time': 41858.08864212036, 'accumulated_eval_time': 1449.155464887619, 'accumulated_logging_time': 3.2273247241973877, 'global_step': 123244, 'preemption_count': 0}), (124749, {'train/accuracy': 0.6416613459587097, 'train/loss': 1.4549739360809326, 'validation/accuracy': 0.5970799922943115, 'validation/loss': 1.6888469457626343, 'validation/num_examples': 50000, 'test/accuracy': 0.46960002183914185, 'test/loss': 2.4458189010620117, 'test/num_examples': 10000, 'score': 42368.04260277748, 'total_duration': 43842.06088280678, 'accumulated_submission_time': 42368.04260277748, 'accumulated_eval_time': 1466.4475784301758, 'accumulated_logging_time': 3.278285026550293, 'global_step': 124749, 'preemption_count': 0}), (126254, {'train/accuracy': 0.6292649507522583, 'train/loss': 1.4908218383789062, 'validation/accuracy': 0.5594800114631653, 'validation/loss': 1.9337760210037231, 'validation/num_examples': 50000, 'test/accuracy': 0.4337000250816345, 'test/loss': 2.7682411670684814, 'test/num_examples': 10000, 'score': 42877.98620200157, 'total_duration': 44369.39072751999, 'accumulated_submission_time': 42877.98620200157, 'accumulated_eval_time': 1483.7346332073212, 'accumulated_logging_time': 3.327343463897705, 'global_step': 126254, 'preemption_count': 0}), (127759, {'train/accuracy': 0.6521045565605164, 'train/loss': 1.3957295417785645, 'validation/accuracy': 0.5881999731063843, 'validation/loss': 1.7238211631774902, 'validation/num_examples': 50000, 'test/accuracy': 0.4629000127315521, 'test/loss': 2.50555157661438, 'test/num_examples': 10000, 'score': 43387.98578906059, 'total_duration': 44897.10552716255, 'accumulated_submission_time': 43387.98578906059, 'accumulated_eval_time': 1501.349282026291, 'accumulated_logging_time': 3.3759353160858154, 'global_step': 127759, 'preemption_count': 0}), (129265, {'train/accuracy': 0.6463648080825806, 'train/loss': 1.4183142185211182, 'validation/accuracy': 0.5917400121688843, 'validation/loss': 1.7195110321044922, 'validation/num_examples': 50000, 'test/accuracy': 0.47220003604888916, 'test/loss': 2.4591338634490967, 'test/num_examples': 10000, 'score': 43897.913105010986, 'total_duration': 45424.64857959747, 'accumulated_submission_time': 43897.913105010986, 'accumulated_eval_time': 1518.861918926239, 'accumulated_logging_time': 3.4282279014587402, 'global_step': 129265, 'preemption_count': 0}), (130771, {'train/accuracy': 0.6597377061843872, 'train/loss': 1.3550430536270142, 'validation/accuracy': 0.6078000068664551, 'validation/loss': 1.6432515382766724, 'validation/num_examples': 50000, 'test/accuracy': 0.48350003361701965, 'test/loss': 2.3731515407562256, 'test/num_examples': 10000, 'score': 44408.10325551033, 'total_duration': 45952.20838069916, 'accumulated_submission_time': 44408.10325551033, 'accumulated_eval_time': 1536.1293251514435, 'accumulated_logging_time': 3.4792776107788086, 'global_step': 130771, 'preemption_count': 0}), (132277, {'train/accuracy': 0.5997488498687744, 'train/loss': 1.664632797241211, 'validation/accuracy': 0.5585799813270569, 'validation/loss': 1.9178231954574585, 'validation/num_examples': 50000, 'test/accuracy': 0.4361000061035156, 'test/loss': 2.7067646980285645, 'test/num_examples': 10000, 'score': 44918.220878601074, 'total_duration': 46479.58945250511, 'accumulated_submission_time': 44918.220878601074, 'accumulated_eval_time': 1553.2879321575165, 'accumulated_logging_time': 3.5328681468963623, 'global_step': 132277, 'preemption_count': 0}), (133783, {'train/accuracy': 0.623445451259613, 'train/loss': 1.5336153507232666, 'validation/accuracy': 0.577239990234375, 'validation/loss': 1.8043313026428223, 'validation/num_examples': 50000, 'test/accuracy': 0.4651000201702118, 'test/loss': 2.5390968322753906, 'test/num_examples': 10000, 'score': 45428.21768307686, 'total_duration': 47007.13358902931, 'accumulated_submission_time': 45428.21768307686, 'accumulated_eval_time': 1570.7316064834595, 'accumulated_logging_time': 3.585947036743164, 'global_step': 133783, 'preemption_count': 0}), (135289, {'train/accuracy': 0.6740872263908386, 'train/loss': 1.2957236766815186, 'validation/accuracy': 0.5938000082969666, 'validation/loss': 1.7162760496139526, 'validation/num_examples': 50000, 'test/accuracy': 0.46070003509521484, 'test/loss': 2.485707998275757, 'test/num_examples': 10000, 'score': 45938.33998990059, 'total_duration': 47534.735048532486, 'accumulated_submission_time': 45938.33998990059, 'accumulated_eval_time': 1588.1084327697754, 'accumulated_logging_time': 3.637380599975586, 'global_step': 135289, 'preemption_count': 0}), (136794, {'train/accuracy': 0.6825972199440002, 'train/loss': 1.2317051887512207, 'validation/accuracy': 0.6187599897384644, 'validation/loss': 1.594221830368042, 'validation/num_examples': 50000, 'test/accuracy': 0.497700035572052, 'test/loss': 2.348714590072632, 'test/num_examples': 10000, 'score': 46448.27879500389, 'total_duration': 48062.19457030296, 'accumulated_submission_time': 46448.27879500389, 'accumulated_eval_time': 1605.5273563861847, 'accumulated_logging_time': 3.687337875366211, 'global_step': 136794, 'preemption_count': 0}), (138300, {'train/accuracy': 0.6966079473495483, 'train/loss': 1.193784475326538, 'validation/accuracy': 0.6342799663543701, 'validation/loss': 1.5047619342803955, 'validation/num_examples': 50000, 'test/accuracy': 0.5135000348091125, 'test/loss': 2.21742844581604, 'test/num_examples': 10000, 'score': 46958.36615371704, 'total_duration': 48589.68688797951, 'accumulated_submission_time': 46958.36615371704, 'accumulated_eval_time': 1622.827439069748, 'accumulated_logging_time': 3.740651845932007, 'global_step': 138300, 'preemption_count': 0}), (139806, {'train/accuracy': 0.6668726205825806, 'train/loss': 1.3343795537948608, 'validation/accuracy': 0.6074599623680115, 'validation/loss': 1.647443413734436, 'validation/num_examples': 50000, 'test/accuracy': 0.46820002794265747, 'test/loss': 2.447305917739868, 'test/num_examples': 10000, 'score': 47468.524106025696, 'total_duration': 49117.42114567757, 'accumulated_submission_time': 47468.524106025696, 'accumulated_eval_time': 1640.2983448505402, 'accumulated_logging_time': 3.7930803298950195, 'global_step': 139806, 'preemption_count': 0}), (141312, {'train/accuracy': 0.7024075388908386, 'train/loss': 1.1639950275421143, 'validation/accuracy': 0.6421200037002563, 'validation/loss': 1.4964680671691895, 'validation/num_examples': 50000, 'test/accuracy': 0.5249000191688538, 'test/loss': 2.221546173095703, 'test/num_examples': 10000, 'score': 47978.61784863472, 'total_duration': 49644.76369333267, 'accumulated_submission_time': 47978.61784863472, 'accumulated_eval_time': 1657.447308063507, 'accumulated_logging_time': 3.8419196605682373, 'global_step': 141312, 'preemption_count': 0}), (142818, {'train/accuracy': 0.7105787396430969, 'train/loss': 1.1328246593475342, 'validation/accuracy': 0.649399995803833, 'validation/loss': 1.4419379234313965, 'validation/num_examples': 50000, 'test/accuracy': 0.5152000188827515, 'test/loss': 2.194200038909912, 'test/num_examples': 10000, 'score': 48488.61668562889, 'total_duration': 50172.85512781143, 'accumulated_submission_time': 48488.61668562889, 'accumulated_eval_time': 1675.4333300590515, 'accumulated_logging_time': 3.895407199859619, 'global_step': 142818, 'preemption_count': 0}), (144323, {'train/accuracy': 0.749422013759613, 'train/loss': 0.9607897996902466, 'validation/accuracy': 0.6571199893951416, 'validation/loss': 1.4050936698913574, 'validation/num_examples': 50000, 'test/accuracy': 0.532800018787384, 'test/loss': 2.105572462081909, 'test/num_examples': 10000, 'score': 48998.60055851936, 'total_duration': 50700.674156188965, 'accumulated_submission_time': 48998.60055851936, 'accumulated_eval_time': 1693.17382478714, 'accumulated_logging_time': 3.9376351833343506, 'global_step': 144323, 'preemption_count': 0}), (145829, {'train/accuracy': 0.7521324753761292, 'train/loss': 0.9460116028785706, 'validation/accuracy': 0.6695399880409241, 'validation/loss': 1.3460698127746582, 'validation/num_examples': 50000, 'test/accuracy': 0.5408000349998474, 'test/loss': 2.0579564571380615, 'test/num_examples': 10000, 'score': 49508.71371245384, 'total_duration': 51228.05661511421, 'accumulated_submission_time': 49508.71371245384, 'accumulated_eval_time': 1710.3399860858917, 'accumulated_logging_time': 3.9888112545013428, 'global_step': 145829, 'preemption_count': 0}), (147335, {'train/accuracy': 0.7348732352256775, 'train/loss': 1.0262718200683594, 'validation/accuracy': 0.6617599725723267, 'validation/loss': 1.3817018270492554, 'validation/num_examples': 50000, 'test/accuracy': 0.5372000336647034, 'test/loss': 2.097731828689575, 'test/num_examples': 10000, 'score': 50018.80451273918, 'total_duration': 51755.39443898201, 'accumulated_submission_time': 50018.80451273918, 'accumulated_eval_time': 1727.4768795967102, 'accumulated_logging_time': 4.0461931228637695, 'global_step': 147335, 'preemption_count': 0}), (148840, {'train/accuracy': 0.7347337007522583, 'train/loss': 1.017677903175354, 'validation/accuracy': 0.6627399921417236, 'validation/loss': 1.392814040184021, 'validation/num_examples': 50000, 'test/accuracy': 0.5409000515937805, 'test/loss': 2.0931143760681152, 'test/num_examples': 10000, 'score': 50528.794149160385, 'total_duration': 52282.75450348854, 'accumulated_submission_time': 50528.794149160385, 'accumulated_eval_time': 1744.73273396492, 'accumulated_logging_time': 4.108054876327515, 'global_step': 148840, 'preemption_count': 0}), (150346, {'train/accuracy': 0.7524314522743225, 'train/loss': 0.9449632167816162, 'validation/accuracy': 0.682159960269928, 'validation/loss': 1.306384563446045, 'validation/num_examples': 50000, 'test/accuracy': 0.556600034236908, 'test/loss': 2.0261521339416504, 'test/num_examples': 10000, 'score': 51038.696164131165, 'total_duration': 52810.16929721832, 'accumulated_submission_time': 51038.696164131165, 'accumulated_eval_time': 1762.1391913890839, 'accumulated_logging_time': 4.162874937057495, 'global_step': 150346, 'preemption_count': 0}), (151852, {'train/accuracy': 0.7565967440605164, 'train/loss': 0.9240195155143738, 'validation/accuracy': 0.6843799948692322, 'validation/loss': 1.2921584844589233, 'validation/num_examples': 50000, 'test/accuracy': 0.5592000484466553, 'test/loss': 2.0278122425079346, 'test/num_examples': 10000, 'score': 51548.87925410271, 'total_duration': 53337.84234023094, 'accumulated_submission_time': 51548.87925410271, 'accumulated_eval_time': 1779.5146894454956, 'accumulated_logging_time': 4.226062297821045, 'global_step': 151852, 'preemption_count': 0}), (153357, {'train/accuracy': 0.7704081535339355, 'train/loss': 0.8769794702529907, 'validation/accuracy': 0.6804400086402893, 'validation/loss': 1.3156960010528564, 'validation/num_examples': 50000, 'test/accuracy': 0.5514000058174133, 'test/loss': 2.0436854362487793, 'test/num_examples': 10000, 'score': 52058.87175607681, 'total_duration': 53865.325132369995, 'accumulated_submission_time': 52058.87175607681, 'accumulated_eval_time': 1796.8985974788666, 'accumulated_logging_time': 4.281770467758179, 'global_step': 153357, 'preemption_count': 0}), (154863, {'train/accuracy': 0.7912148833274841, 'train/loss': 0.7727125287055969, 'validation/accuracy': 0.6976999640464783, 'validation/loss': 1.230203628540039, 'validation/num_examples': 50000, 'test/accuracy': 0.5733000040054321, 'test/loss': 1.9361377954483032, 'test/num_examples': 10000, 'score': 52569.02870512009, 'total_duration': 54393.03028893471, 'accumulated_submission_time': 52569.02870512009, 'accumulated_eval_time': 1814.3410923480988, 'accumulated_logging_time': 4.336676836013794, 'global_step': 154863, 'preemption_count': 0}), (156369, {'train/accuracy': 0.7887436151504517, 'train/loss': 0.7775185704231262, 'validation/accuracy': 0.6951199769973755, 'validation/loss': 1.2335890531539917, 'validation/num_examples': 50000, 'test/accuracy': 0.5689000487327576, 'test/loss': 1.9560490846633911, 'test/num_examples': 10000, 'score': 53079.0686750412, 'total_duration': 54920.545063734055, 'accumulated_submission_time': 53079.0686750412, 'accumulated_eval_time': 1831.7030427455902, 'accumulated_logging_time': 4.396013021469116, 'global_step': 156369, 'preemption_count': 0}), (157875, {'train/accuracy': 0.7881656289100647, 'train/loss': 0.7825496196746826, 'validation/accuracy': 0.6997399926185608, 'validation/loss': 1.2260318994522095, 'validation/num_examples': 50000, 'test/accuracy': 0.5729000568389893, 'test/loss': 1.9439477920532227, 'test/num_examples': 10000, 'score': 53589.254455804825, 'total_duration': 55448.21320796013, 'accumulated_submission_time': 53589.254455804825, 'accumulated_eval_time': 1849.0799005031586, 'accumulated_logging_time': 4.450916528701782, 'global_step': 157875, 'preemption_count': 0}), (159382, {'train/accuracy': 0.7869299650192261, 'train/loss': 0.7920331358909607, 'validation/accuracy': 0.7021799683570862, 'validation/loss': 1.2132388353347778, 'validation/num_examples': 50000, 'test/accuracy': 0.5769000053405762, 'test/loss': 1.903723120689392, 'test/num_examples': 10000, 'score': 54099.491090774536, 'total_duration': 55975.7786552906, 'accumulated_submission_time': 54099.491090774536, 'accumulated_eval_time': 1866.303447008133, 'accumulated_logging_time': 4.505049228668213, 'global_step': 159382, 'preemption_count': 0}), (160888, {'train/accuracy': 0.8048867583274841, 'train/loss': 0.7252900004386902, 'validation/accuracy': 0.7111799716949463, 'validation/loss': 1.1790040731430054, 'validation/num_examples': 50000, 'test/accuracy': 0.5863000154495239, 'test/loss': 1.8755098581314087, 'test/num_examples': 10000, 'score': 54609.7164978981, 'total_duration': 56503.595430374146, 'accumulated_submission_time': 54609.7164978981, 'accumulated_eval_time': 1883.7833399772644, 'accumulated_logging_time': 4.564141511917114, 'global_step': 160888, 'preemption_count': 0}), (162393, {'train/accuracy': 0.8147919178009033, 'train/loss': 0.689312756061554, 'validation/accuracy': 0.7153199911117554, 'validation/loss': 1.1588680744171143, 'validation/num_examples': 50000, 'test/accuracy': 0.5861999988555908, 'test/loss': 1.867242455482483, 'test/num_examples': 10000, 'score': 55119.78022289276, 'total_duration': 57030.947724580765, 'accumulated_submission_time': 55119.78022289276, 'accumulated_eval_time': 1900.9561693668365, 'accumulated_logging_time': 4.627662658691406, 'global_step': 162393, 'preemption_count': 0}), (163898, {'train/accuracy': 0.8363161683082581, 'train/loss': 0.6028015613555908, 'validation/accuracy': 0.7212600111961365, 'validation/loss': 1.1305629014968872, 'validation/num_examples': 50000, 'test/accuracy': 0.5986000299453735, 'test/loss': 1.7955706119537354, 'test/num_examples': 10000, 'score': 55629.80427956581, 'total_duration': 57558.35658311844, 'accumulated_submission_time': 55629.80427956581, 'accumulated_eval_time': 1918.2251298427582, 'accumulated_logging_time': 4.691629648208618, 'global_step': 163898, 'preemption_count': 0}), (165403, {'train/accuracy': 0.8367944955825806, 'train/loss': 0.594135046005249, 'validation/accuracy': 0.7243399620056152, 'validation/loss': 1.119240164756775, 'validation/num_examples': 50000, 'test/accuracy': 0.6026000380516052, 'test/loss': 1.800337553024292, 'test/num_examples': 10000, 'score': 56139.99762535095, 'total_duration': 58085.81350302696, 'accumulated_submission_time': 56139.99762535095, 'accumulated_eval_time': 1935.3796126842499, 'accumulated_logging_time': 4.7491774559021, 'global_step': 165403, 'preemption_count': 0}), (166907, {'train/accuracy': 0.8360371589660645, 'train/loss': 0.5923864245414734, 'validation/accuracy': 0.7259199619293213, 'validation/loss': 1.1185106039047241, 'validation/num_examples': 50000, 'test/accuracy': 0.6011000275611877, 'test/loss': 1.818732500076294, 'test/num_examples': 10000, 'score': 56649.969014406204, 'total_duration': 58613.30370092392, 'accumulated_submission_time': 56649.969014406204, 'accumulated_eval_time': 1952.7912888526917, 'accumulated_logging_time': 4.805515766143799, 'global_step': 166907, 'preemption_count': 0}), (168412, {'train/accuracy': 0.8469985723495483, 'train/loss': 0.5509080290794373, 'validation/accuracy': 0.7348999977111816, 'validation/loss': 1.0821901559829712, 'validation/num_examples': 50000, 'test/accuracy': 0.6110000014305115, 'test/loss': 1.7652047872543335, 'test/num_examples': 10000, 'score': 57159.98609948158, 'total_duration': 59141.460246801376, 'accumulated_submission_time': 57159.98609948158, 'accumulated_eval_time': 1970.818876504898, 'accumulated_logging_time': 4.8666205406188965, 'global_step': 168412, 'preemption_count': 0}), (169918, {'train/accuracy': 0.8470583558082581, 'train/loss': 0.5498757362365723, 'validation/accuracy': 0.7370799779891968, 'validation/loss': 1.0780162811279297, 'validation/num_examples': 50000, 'test/accuracy': 0.610200047492981, 'test/loss': 1.7671650648117065, 'test/num_examples': 10000, 'score': 57670.21110057831, 'total_duration': 59669.354877471924, 'accumulated_submission_time': 57670.21110057831, 'accumulated_eval_time': 1988.379658460617, 'accumulated_logging_time': 4.923551321029663, 'global_step': 169918, 'preemption_count': 0}), (171423, {'train/accuracy': 0.8548508882522583, 'train/loss': 0.5203155279159546, 'validation/accuracy': 0.738599956035614, 'validation/loss': 1.0618312358856201, 'validation/num_examples': 50000, 'test/accuracy': 0.6117000579833984, 'test/loss': 1.7470028400421143, 'test/num_examples': 10000, 'score': 58180.162395238876, 'total_duration': 60196.933161735535, 'accumulated_submission_time': 58180.162395238876, 'accumulated_eval_time': 2005.8935883045197, 'accumulated_logging_time': 4.9838104248046875, 'global_step': 171423, 'preemption_count': 0}), (172928, {'train/accuracy': 0.870515763759613, 'train/loss': 0.4607931077480316, 'validation/accuracy': 0.7422800064086914, 'validation/loss': 1.0552629232406616, 'validation/num_examples': 50000, 'test/accuracy': 0.6200000047683716, 'test/loss': 1.7335784435272217, 'test/num_examples': 10000, 'score': 58690.095504283905, 'total_duration': 60724.30287194252, 'accumulated_submission_time': 58690.095504283905, 'accumulated_eval_time': 2023.2214815616608, 'accumulated_logging_time': 5.041860818862915, 'global_step': 172928, 'preemption_count': 0}), (174433, {'train/accuracy': 0.8722695708274841, 'train/loss': 0.4543417692184448, 'validation/accuracy': 0.7435599565505981, 'validation/loss': 1.0551124811172485, 'validation/num_examples': 50000, 'test/accuracy': 0.6190000176429749, 'test/loss': 1.7485793828964233, 'test/num_examples': 10000, 'score': 59200.02728843689, 'total_duration': 61251.63083457947, 'accumulated_submission_time': 59200.02728843689, 'accumulated_eval_time': 2040.50359749794, 'accumulated_logging_time': 5.102504730224609, 'global_step': 174433, 'preemption_count': 0}), (175938, {'train/accuracy': 0.8727478981018066, 'train/loss': 0.4520764946937561, 'validation/accuracy': 0.7457000017166138, 'validation/loss': 1.0432484149932861, 'validation/num_examples': 50000, 'test/accuracy': 0.6223000288009644, 'test/loss': 1.732593059539795, 'test/num_examples': 10000, 'score': 59710.2247235775, 'total_duration': 61779.41142606735, 'accumulated_submission_time': 59710.2247235775, 'accumulated_eval_time': 2057.9741168022156, 'accumulated_logging_time': 5.1639111042022705, 'global_step': 175938, 'preemption_count': 0}), (177443, {'train/accuracy': 0.875996470451355, 'train/loss': 0.442630410194397, 'validation/accuracy': 0.7466399669647217, 'validation/loss': 1.0333547592163086, 'validation/num_examples': 50000, 'test/accuracy': 0.6240000128746033, 'test/loss': 1.7179020643234253, 'test/num_examples': 10000, 'score': 60220.32627439499, 'total_duration': 62306.92303276062, 'accumulated_submission_time': 60220.32627439499, 'accumulated_eval_time': 2075.270829439163, 'accumulated_logging_time': 5.224120616912842, 'global_step': 177443, 'preemption_count': 0}), (178947, {'train/accuracy': 0.8785873651504517, 'train/loss': 0.42702656984329224, 'validation/accuracy': 0.7484599947929382, 'validation/loss': 1.0259389877319336, 'validation/num_examples': 50000, 'test/accuracy': 0.6215000152587891, 'test/loss': 1.717150092124939, 'test/num_examples': 10000, 'score': 60730.232352018356, 'total_duration': 62834.43216466904, 'accumulated_submission_time': 60730.232352018356, 'accumulated_eval_time': 2092.762161254883, 'accumulated_logging_time': 5.284364223480225, 'global_step': 178947, 'preemption_count': 0}), (180453, {'train/accuracy': 0.8818159699440002, 'train/loss': 0.412717342376709, 'validation/accuracy': 0.7495399713516235, 'validation/loss': 1.0254219770431519, 'validation/num_examples': 50000, 'test/accuracy': 0.6262000203132629, 'test/loss': 1.7159655094146729, 'test/num_examples': 10000, 'score': 61240.38385462761, 'total_duration': 63361.98218679428, 'accumulated_submission_time': 61240.38385462761, 'accumulated_eval_time': 2110.0475058555603, 'accumulated_logging_time': 5.346153259277344, 'global_step': 180453, 'preemption_count': 0}), (181959, {'train/accuracy': 0.8836495280265808, 'train/loss': 0.4111799895763397, 'validation/accuracy': 0.7505399584770203, 'validation/loss': 1.0216362476348877, 'validation/num_examples': 50000, 'test/accuracy': 0.6243000030517578, 'test/loss': 1.715208888053894, 'test/num_examples': 10000, 'score': 61750.597148656845, 'total_duration': 63889.77865052223, 'accumulated_submission_time': 61750.597148656845, 'accumulated_eval_time': 2127.516664505005, 'accumulated_logging_time': 5.409894943237305, 'global_step': 181959, 'preemption_count': 0}), (183465, {'train/accuracy': 0.884785532951355, 'train/loss': 0.4123013913631439, 'validation/accuracy': 0.7511999607086182, 'validation/loss': 1.020043134689331, 'validation/num_examples': 50000, 'test/accuracy': 0.6269000172615051, 'test/loss': 1.7149865627288818, 'test/num_examples': 10000, 'score': 62260.746554374695, 'total_duration': 64417.35872173309, 'accumulated_submission_time': 62260.746554374695, 'accumulated_eval_time': 2144.8360488414764, 'accumulated_logging_time': 5.469226121902466, 'global_step': 183465, 'preemption_count': 0}), (184971, {'train/accuracy': 0.8868184089660645, 'train/loss': 0.4034609794616699, 'validation/accuracy': 0.7515400052070618, 'validation/loss': 1.0192183256149292, 'validation/num_examples': 50000, 'test/accuracy': 0.6272000074386597, 'test/loss': 1.7114449739456177, 'test/num_examples': 10000, 'score': 62770.95883798599, 'total_duration': 64944.98903656006, 'accumulated_submission_time': 62770.95883798599, 'accumulated_eval_time': 2162.1414761543274, 'accumulated_logging_time': 5.530078649520874, 'global_step': 184971, 'preemption_count': 0})], 'global_step': 185672}
I0129 19:32:21.436581 139822745589568 submission_runner.py:586] Timing: 63008.26549935341
I0129 19:32:21.436647 139822745589568 submission_runner.py:588] Total number of evals: 124
I0129 19:32:21.436689 139822745589568 submission_runner.py:589] ====================
I0129 19:32:21.436733 139822745589568 submission_runner.py:542] Using RNG seed 916031063
I0129 19:32:21.437948 139822745589568 submission_runner.py:551] --- Tuning run 5/5 ---
I0129 19:32:21.438052 139822745589568 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_1/imagenet_resnet_jax/trial_5.
I0129 19:32:21.439513 139822745589568 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_1/imagenet_resnet_jax/trial_5/hparams.json.
I0129 19:32:21.440289 139822745589568 submission_runner.py:206] Initializing dataset.
I0129 19:32:21.448786 139822745589568 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0129 19:32:21.458891 139822745589568 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0129 19:32:21.646497 139822745589568 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0129 19:32:21.882111 139822745589568 submission_runner.py:213] Initializing model.
I0129 19:32:27.434710 139822745589568 submission_runner.py:255] Initializing optimizer.
I0129 19:32:27.836052 139822745589568 submission_runner.py:262] Initializing metrics bundle.
I0129 19:32:27.836207 139822745589568 submission_runner.py:280] Initializing checkpoint and logger.
I0129 19:32:27.851697 139822745589568 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_1/imagenet_resnet_jax/trial_5 with prefix checkpoint_
I0129 19:32:27.851820 139822745589568 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_1/imagenet_resnet_jax/trial_5/meta_data_0.json.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0129 19:32:40.514416 139822745589568 logger_utils.py:220] Unable to record git information. Continuing without it.
I0129 19:32:53.004256 139822745589568 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_1/imagenet_resnet_jax/trial_5/flags_0.json.
I0129 19:32:53.009779 139822745589568 submission_runner.py:314] Starting training loop.
I0129 19:33:28.210118 139656666543872 logging_writer.py:48] [0] global_step=0, grad_norm=0.6531614065170288, loss=6.929165840148926
I0129 19:33:28.223593 139822745589568 spec.py:321] Evaluating on the training split.
I0129 19:33:34.438276 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 19:33:42.989053 139822745589568 spec.py:349] Evaluating on the test split.
I0129 19:33:45.627452 139822745589568 submission_runner.py:408] Time since start: 52.62s, 	Step: 1, 	{'train/accuracy': 0.0011160713620483875, 'train/loss': 6.910408973693848, 'validation/accuracy': 0.001019999966956675, 'validation/loss': 6.91091251373291, 'validation/num_examples': 50000, 'test/accuracy': 0.0012000000569969416, 'test/loss': 6.910790920257568, 'test/num_examples': 10000, 'score': 35.2137176990509, 'total_duration': 52.61761689186096, 'accumulated_submission_time': 35.2137176990509, 'accumulated_eval_time': 17.4037983417511, 'accumulated_logging_time': 0}
I0129 19:33:45.638462 139655592802048 logging_writer.py:48] [1] accumulated_eval_time=17.403798, accumulated_logging_time=0, accumulated_submission_time=35.213718, global_step=1, preemption_count=0, score=35.213718, test/accuracy=0.001200, test/loss=6.910791, test/num_examples=10000, total_duration=52.617617, train/accuracy=0.001116, train/loss=6.910409, validation/accuracy=0.001020, validation/loss=6.910913, validation/num_examples=50000
I0129 19:34:19.777821 139656297445120 logging_writer.py:48] [100] global_step=100, grad_norm=0.663040280342102, loss=6.8140034675598145
I0129 19:34:54.167049 139655592802048 logging_writer.py:48] [200] global_step=200, grad_norm=0.8227038979530334, loss=6.589010238647461
I0129 19:35:28.391644 139656297445120 logging_writer.py:48] [300] global_step=300, grad_norm=0.9289721250534058, loss=6.247857570648193
I0129 19:36:02.629689 139655592802048 logging_writer.py:48] [400] global_step=400, grad_norm=2.1543655395507812, loss=6.0050435066223145
I0129 19:36:36.885569 139656297445120 logging_writer.py:48] [500] global_step=500, grad_norm=2.6169073581695557, loss=5.83037805557251
I0129 19:37:11.113499 139655592802048 logging_writer.py:48] [600] global_step=600, grad_norm=2.4998018741607666, loss=5.599595069885254
I0129 19:37:45.340228 139656297445120 logging_writer.py:48] [700] global_step=700, grad_norm=2.980853319168091, loss=5.444866180419922
I0129 19:38:19.589148 139655592802048 logging_writer.py:48] [800] global_step=800, grad_norm=3.150921583175659, loss=5.260720729827881
I0129 19:38:53.819048 139656297445120 logging_writer.py:48] [900] global_step=900, grad_norm=4.428671836853027, loss=5.154052734375
I0129 19:39:28.074218 139655592802048 logging_writer.py:48] [1000] global_step=1000, grad_norm=5.686033725738525, loss=5.021340847015381
I0129 19:40:02.296746 139656297445120 logging_writer.py:48] [1100] global_step=1100, grad_norm=4.927740097045898, loss=5.036457538604736
I0129 19:40:36.525526 139655592802048 logging_writer.py:48] [1200] global_step=1200, grad_norm=4.266154766082764, loss=4.740296840667725
I0129 19:41:10.747389 139656297445120 logging_writer.py:48] [1300] global_step=1300, grad_norm=7.378279685974121, loss=4.7035369873046875
I0129 19:41:45.091742 139655592802048 logging_writer.py:48] [1400] global_step=1400, grad_norm=3.498098611831665, loss=4.737800598144531
I0129 19:42:15.723270 139822745589568 spec.py:321] Evaluating on the training split.
I0129 19:42:22.081797 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 19:42:30.579817 139822745589568 spec.py:349] Evaluating on the test split.
I0129 19:42:33.179055 139822745589568 submission_runner.py:408] Time since start: 580.17s, 	Step: 1491, 	{'train/accuracy': 0.16388313472270966, 'train/loss': 4.366621017456055, 'validation/accuracy': 0.14970000088214874, 'validation/loss': 4.4936041831970215, 'validation/num_examples': 50000, 'test/accuracy': 0.11600000411272049, 'test/loss': 4.982067584991455, 'test/num_examples': 10000, 'score': 545.2359344959259, 'total_duration': 580.1692199707031, 'accumulated_submission_time': 545.2359344959259, 'accumulated_eval_time': 34.85956573486328, 'accumulated_logging_time': 0.019899845123291016}
I0129 19:42:33.196590 139655592802048 logging_writer.py:48] [1491] accumulated_eval_time=34.859566, accumulated_logging_time=0.019900, accumulated_submission_time=545.235934, global_step=1491, preemption_count=0, score=545.235934, test/accuracy=0.116000, test/loss=4.982068, test/num_examples=10000, total_duration=580.169220, train/accuracy=0.163883, train/loss=4.366621, validation/accuracy=0.149700, validation/loss=4.493604, validation/num_examples=50000
I0129 19:42:36.620132 139656297445120 logging_writer.py:48] [1500] global_step=1500, grad_norm=8.455964088439941, loss=4.490656852722168
I0129 19:43:10.813503 139655592802048 logging_writer.py:48] [1600] global_step=1600, grad_norm=5.818283557891846, loss=4.199949741363525
I0129 19:43:45.055857 139656297445120 logging_writer.py:48] [1700] global_step=1700, grad_norm=6.348964214324951, loss=4.261138916015625
I0129 19:44:19.333329 139655592802048 logging_writer.py:48] [1800] global_step=1800, grad_norm=7.798219203948975, loss=4.2230224609375
I0129 19:44:53.583089 139656297445120 logging_writer.py:48] [1900] global_step=1900, grad_norm=9.411898612976074, loss=3.9792754650115967
I0129 19:45:27.843709 139655592802048 logging_writer.py:48] [2000] global_step=2000, grad_norm=5.529086589813232, loss=4.043638229370117
I0129 19:46:02.083020 139656297445120 logging_writer.py:48] [2100] global_step=2100, grad_norm=3.7657535076141357, loss=3.979494333267212
I0129 19:46:36.337082 139655592802048 logging_writer.py:48] [2200] global_step=2200, grad_norm=11.020764350891113, loss=3.882485866546631
I0129 19:47:10.546138 139656297445120 logging_writer.py:48] [2300] global_step=2300, grad_norm=4.304226875305176, loss=3.751120090484619
I0129 19:47:44.767418 139655592802048 logging_writer.py:48] [2400] global_step=2400, grad_norm=3.6088368892669678, loss=3.809051752090454
I0129 19:48:19.064767 139656297445120 logging_writer.py:48] [2500] global_step=2500, grad_norm=5.025324821472168, loss=3.569359302520752
I0129 19:48:53.273545 139655592802048 logging_writer.py:48] [2600] global_step=2600, grad_norm=3.036635637283325, loss=3.595855712890625
I0129 19:49:27.495517 139656297445120 logging_writer.py:48] [2700] global_step=2700, grad_norm=4.636557102203369, loss=3.566530227661133
I0129 19:50:01.707944 139655592802048 logging_writer.py:48] [2800] global_step=2800, grad_norm=3.4298319816589355, loss=3.4382073879241943
I0129 19:50:35.915441 139656297445120 logging_writer.py:48] [2900] global_step=2900, grad_norm=2.908033847808838, loss=3.4983572959899902
I0129 19:51:03.413091 139822745589568 spec.py:321] Evaluating on the training split.
I0129 19:51:09.716962 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 19:51:18.229495 139822745589568 spec.py:349] Evaluating on the test split.
I0129 19:51:20.852199 139822745589568 submission_runner.py:408] Time since start: 1107.84s, 	Step: 2982, 	{'train/accuracy': 0.33591756224632263, 'train/loss': 3.0918776988983154, 'validation/accuracy': 0.3116599917411804, 'validation/loss': 3.2620902061462402, 'validation/num_examples': 50000, 'test/accuracy': 0.22990001738071442, 'test/loss': 3.9335262775421143, 'test/num_examples': 10000, 'score': 1055.3937442302704, 'total_duration': 1107.8423628807068, 'accumulated_submission_time': 1055.3937442302704, 'accumulated_eval_time': 52.29863715171814, 'accumulated_logging_time': 0.04640698432922363}
I0129 19:51:20.870253 139655576016640 logging_writer.py:48] [2982] accumulated_eval_time=52.298637, accumulated_logging_time=0.046407, accumulated_submission_time=1055.393744, global_step=2982, preemption_count=0, score=1055.393744, test/accuracy=0.229900, test/loss=3.933526, test/num_examples=10000, total_duration=1107.842363, train/accuracy=0.335918, train/loss=3.091878, validation/accuracy=0.311660, validation/loss=3.262090, validation/num_examples=50000
I0129 19:51:27.353092 139655584409344 logging_writer.py:48] [3000] global_step=3000, grad_norm=3.623570442199707, loss=3.342705011367798
I0129 19:52:01.510441 139655576016640 logging_writer.py:48] [3100] global_step=3100, grad_norm=2.609907627105713, loss=3.2798941135406494
I0129 19:52:35.701825 139655584409344 logging_writer.py:48] [3200] global_step=3200, grad_norm=4.582949638366699, loss=3.4084901809692383
I0129 19:53:09.898235 139655576016640 logging_writer.py:48] [3300] global_step=3300, grad_norm=4.720758438110352, loss=3.199920892715454
I0129 19:53:44.103000 139655584409344 logging_writer.py:48] [3400] global_step=3400, grad_norm=3.744729518890381, loss=3.154346466064453
I0129 19:54:18.318736 139655576016640 logging_writer.py:48] [3500] global_step=3500, grad_norm=3.0292046070098877, loss=3.134864330291748
I0129 19:54:52.612935 139655584409344 logging_writer.py:48] [3600] global_step=3600, grad_norm=4.179172992706299, loss=3.2390646934509277
I0129 19:55:26.814674 139655576016640 logging_writer.py:48] [3700] global_step=3700, grad_norm=3.0236034393310547, loss=3.0259528160095215
I0129 19:56:01.031307 139655584409344 logging_writer.py:48] [3800] global_step=3800, grad_norm=2.8445801734924316, loss=3.0771026611328125
I0129 19:56:35.205825 139655576016640 logging_writer.py:48] [3900] global_step=3900, grad_norm=3.3739190101623535, loss=3.075991630554199
I0129 19:57:09.398969 139655584409344 logging_writer.py:48] [4000] global_step=4000, grad_norm=2.6403205394744873, loss=3.1833198070526123
I0129 19:57:43.588271 139655576016640 logging_writer.py:48] [4100] global_step=4100, grad_norm=3.1399142742156982, loss=2.9088311195373535
I0129 19:58:17.796961 139655584409344 logging_writer.py:48] [4200] global_step=4200, grad_norm=3.7165937423706055, loss=2.9216346740722656
I0129 19:58:51.996693 139655576016640 logging_writer.py:48] [4300] global_step=4300, grad_norm=4.0859198570251465, loss=2.8567347526550293
I0129 19:59:26.196876 139655584409344 logging_writer.py:48] [4400] global_step=4400, grad_norm=3.4942069053649902, loss=2.704817771911621
I0129 19:59:50.954043 139822745589568 spec.py:321] Evaluating on the training split.
I0129 19:59:57.172492 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 20:00:06.006343 139822745589568 spec.py:349] Evaluating on the test split.
I0129 20:00:08.647636 139822745589568 submission_runner.py:408] Time since start: 1635.64s, 	Step: 4474, 	{'train/accuracy': 0.5045041441917419, 'train/loss': 2.1400482654571533, 'validation/accuracy': 0.42861998081207275, 'validation/loss': 2.554784059524536, 'validation/num_examples': 50000, 'test/accuracy': 0.3278000056743622, 'test/loss': 3.2873244285583496, 'test/num_examples': 10000, 'score': 1565.4191055297852, 'total_duration': 1635.6378026008606, 'accumulated_submission_time': 1565.4191055297852, 'accumulated_eval_time': 69.99219536781311, 'accumulated_logging_time': 0.07351899147033691}
I0129 20:00:08.662966 139656649758464 logging_writer.py:48] [4474] accumulated_eval_time=69.992195, accumulated_logging_time=0.073519, accumulated_submission_time=1565.419106, global_step=4474, preemption_count=0, score=1565.419106, test/accuracy=0.327800, test/loss=3.287324, test/num_examples=10000, total_duration=1635.637803, train/accuracy=0.504504, train/loss=2.140048, validation/accuracy=0.428620, validation/loss=2.554784, validation/num_examples=50000
I0129 20:00:17.881101 139656658151168 logging_writer.py:48] [4500] global_step=4500, grad_norm=3.007810115814209, loss=2.8760828971862793
I0129 20:00:51.974246 139656649758464 logging_writer.py:48] [4600] global_step=4600, grad_norm=3.0840649604797363, loss=2.75673508644104
I0129 20:01:26.181930 139656658151168 logging_writer.py:48] [4700] global_step=4700, grad_norm=3.461113214492798, loss=2.6423499584198
I0129 20:02:00.334201 139656649758464 logging_writer.py:48] [4800] global_step=4800, grad_norm=2.249314546585083, loss=3.0212597846984863
I0129 20:02:34.496049 139656658151168 logging_writer.py:48] [4900] global_step=4900, grad_norm=3.3029825687408447, loss=2.5702602863311768
I0129 20:03:08.663244 139656649758464 logging_writer.py:48] [5000] global_step=5000, grad_norm=3.2302021980285645, loss=2.6348865032196045
I0129 20:03:42.829576 139656658151168 logging_writer.py:48] [5100] global_step=5100, grad_norm=2.5621225833892822, loss=2.544522285461426
I0129 20:04:16.998354 139656649758464 logging_writer.py:48] [5200] global_step=5200, grad_norm=1.9211032390594482, loss=2.524170398712158
I0129 20:04:51.158887 139656658151168 logging_writer.py:48] [5300] global_step=5300, grad_norm=1.8374550342559814, loss=2.527914047241211
I0129 20:05:25.334601 139656649758464 logging_writer.py:48] [5400] global_step=5400, grad_norm=2.6043293476104736, loss=2.5194005966186523
I0129 20:05:59.476454 139656658151168 logging_writer.py:48] [5500] global_step=5500, grad_norm=2.719815254211426, loss=2.449582576751709
I0129 20:06:33.618746 139656649758464 logging_writer.py:48] [5600] global_step=5600, grad_norm=2.4545693397521973, loss=2.6520373821258545
I0129 20:07:07.759304 139656658151168 logging_writer.py:48] [5700] global_step=5700, grad_norm=2.9064443111419678, loss=2.387444496154785
I0129 20:07:41.901105 139656649758464 logging_writer.py:48] [5800] global_step=5800, grad_norm=1.7383170127868652, loss=2.4923577308654785
I0129 20:08:16.132203 139656658151168 logging_writer.py:48] [5900] global_step=5900, grad_norm=2.6515097618103027, loss=2.509531021118164
I0129 20:08:38.811059 139822745589568 spec.py:321] Evaluating on the training split.
I0129 20:08:45.043299 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 20:08:53.760459 139822745589568 spec.py:349] Evaluating on the test split.
I0129 20:08:56.383756 139822745589568 submission_runner.py:408] Time since start: 2163.37s, 	Step: 5968, 	{'train/accuracy': 0.5469945669174194, 'train/loss': 1.9179505109786987, 'validation/accuracy': 0.499019980430603, 'validation/loss': 2.203223943710327, 'validation/num_examples': 50000, 'test/accuracy': 0.3862000107765198, 'test/loss': 2.930450439453125, 'test/num_examples': 10000, 'score': 2075.5087237358093, 'total_duration': 2163.3739235401154, 'accumulated_submission_time': 2075.5087237358093, 'accumulated_eval_time': 87.56485724449158, 'accumulated_logging_time': 0.09717965126037598}
I0129 20:08:56.402015 139655584409344 logging_writer.py:48] [5968] accumulated_eval_time=87.564857, accumulated_logging_time=0.097180, accumulated_submission_time=2075.508724, global_step=5968, preemption_count=0, score=2075.508724, test/accuracy=0.386200, test/loss=2.930450, test/num_examples=10000, total_duration=2163.373924, train/accuracy=0.546995, train/loss=1.917951, validation/accuracy=0.499020, validation/loss=2.203224, validation/num_examples=50000
I0129 20:09:07.670387 139655592802048 logging_writer.py:48] [6000] global_step=6000, grad_norm=2.8497161865234375, loss=2.4775760173797607
I0129 20:09:41.776228 139655584409344 logging_writer.py:48] [6100] global_step=6100, grad_norm=1.637333869934082, loss=2.4649064540863037
I0129 20:10:15.930402 139655592802048 logging_writer.py:48] [6200] global_step=6200, grad_norm=2.6538712978363037, loss=2.443358898162842
I0129 20:10:50.059842 139655584409344 logging_writer.py:48] [6300] global_step=6300, grad_norm=2.1348416805267334, loss=2.4179019927978516
I0129 20:11:24.218502 139655592802048 logging_writer.py:48] [6400] global_step=6400, grad_norm=1.991904377937317, loss=2.504340171813965
I0129 20:11:58.370063 139655584409344 logging_writer.py:48] [6500] global_step=6500, grad_norm=2.4093830585479736, loss=2.3700122833251953
I0129 20:12:32.531309 139655592802048 logging_writer.py:48] [6600] global_step=6600, grad_norm=2.5140540599823, loss=2.3962862491607666
I0129 20:13:06.653546 139655584409344 logging_writer.py:48] [6700] global_step=6700, grad_norm=1.607911467552185, loss=2.3549015522003174
I0129 20:13:40.810883 139655592802048 logging_writer.py:48] [6800] global_step=6800, grad_norm=2.1299564838409424, loss=2.2435739040374756
I0129 20:14:14.949355 139655584409344 logging_writer.py:48] [6900] global_step=6900, grad_norm=2.3891665935516357, loss=2.3300819396972656
I0129 20:14:49.220927 139655592802048 logging_writer.py:48] [7000] global_step=7000, grad_norm=2.104832649230957, loss=2.298412322998047
I0129 20:15:23.400380 139655584409344 logging_writer.py:48] [7100] global_step=7100, grad_norm=2.4233179092407227, loss=2.380096197128296
I0129 20:15:57.578281 139655592802048 logging_writer.py:48] [7200] global_step=7200, grad_norm=1.9879441261291504, loss=2.3554272651672363
I0129 20:16:31.731218 139655584409344 logging_writer.py:48] [7300] global_step=7300, grad_norm=1.5916416645050049, loss=2.3756229877471924
I0129 20:17:05.887829 139655592802048 logging_writer.py:48] [7400] global_step=7400, grad_norm=1.939137578010559, loss=2.2005465030670166
I0129 20:17:26.521540 139822745589568 spec.py:321] Evaluating on the training split.
I0129 20:17:32.780961 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 20:17:41.522294 139822745589568 spec.py:349] Evaluating on the test split.
I0129 20:17:44.169319 139822745589568 submission_runner.py:408] Time since start: 2691.16s, 	Step: 7462, 	{'train/accuracy': 0.5733617544174194, 'train/loss': 1.7808259725570679, 'validation/accuracy': 0.5251399874687195, 'validation/loss': 2.0447330474853516, 'validation/num_examples': 50000, 'test/accuracy': 0.40890002250671387, 'test/loss': 2.7833967208862305, 'test/num_examples': 10000, 'score': 2585.5674998760223, 'total_duration': 2691.1594779491425, 'accumulated_submission_time': 2585.5674998760223, 'accumulated_eval_time': 105.21260619163513, 'accumulated_logging_time': 0.1253807544708252}
I0129 20:17:44.189301 139655584409344 logging_writer.py:48] [7462] accumulated_eval_time=105.212606, accumulated_logging_time=0.125381, accumulated_submission_time=2585.567500, global_step=7462, preemption_count=0, score=2585.567500, test/accuracy=0.408900, test/loss=2.783397, test/num_examples=10000, total_duration=2691.159478, train/accuracy=0.573362, train/loss=1.780826, validation/accuracy=0.525140, validation/loss=2.044733, validation/num_examples=50000
I0129 20:17:57.511965 139655592802048 logging_writer.py:48] [7500] global_step=7500, grad_norm=2.0146090984344482, loss=2.2979702949523926
I0129 20:18:31.607285 139655584409344 logging_writer.py:48] [7600] global_step=7600, grad_norm=1.9998722076416016, loss=2.321582555770874
I0129 20:19:05.755934 139655592802048 logging_writer.py:48] [7700] global_step=7700, grad_norm=2.3349578380584717, loss=2.363867998123169
I0129 20:19:39.897942 139655584409344 logging_writer.py:48] [7800] global_step=7800, grad_norm=1.51539945602417, loss=2.1277170181274414
I0129 20:20:14.044172 139655592802048 logging_writer.py:48] [7900] global_step=7900, grad_norm=2.005581855773926, loss=2.2993762493133545
I0129 20:20:48.192670 139655584409344 logging_writer.py:48] [8000] global_step=8000, grad_norm=2.367307186126709, loss=2.375840425491333
I0129 20:21:22.422556 139655592802048 logging_writer.py:48] [8100] global_step=8100, grad_norm=2.0292491912841797, loss=2.2071127891540527
I0129 20:21:56.577949 139655584409344 logging_writer.py:48] [8200] global_step=8200, grad_norm=2.0202863216400146, loss=2.274071216583252
I0129 20:22:30.742082 139655592802048 logging_writer.py:48] [8300] global_step=8300, grad_norm=1.8997282981872559, loss=2.1550064086914062
I0129 20:23:04.871951 139655584409344 logging_writer.py:48] [8400] global_step=8400, grad_norm=1.7045754194259644, loss=2.1076245307922363
I0129 20:23:39.002328 139655592802048 logging_writer.py:48] [8500] global_step=8500, grad_norm=2.0667481422424316, loss=2.2483813762664795
I0129 20:24:13.156587 139655584409344 logging_writer.py:48] [8600] global_step=8600, grad_norm=1.7936155796051025, loss=2.198131799697876
I0129 20:24:47.310618 139655592802048 logging_writer.py:48] [8700] global_step=8700, grad_norm=1.5539461374282837, loss=2.0955348014831543
I0129 20:25:21.437036 139655584409344 logging_writer.py:48] [8800] global_step=8800, grad_norm=1.9295732975006104, loss=2.2227180004119873
I0129 20:25:55.570335 139655592802048 logging_writer.py:48] [8900] global_step=8900, grad_norm=1.5584334135055542, loss=2.1339502334594727
I0129 20:26:14.497106 139822745589568 spec.py:321] Evaluating on the training split.
I0129 20:26:20.739257 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 20:26:29.095733 139822745589568 spec.py:349] Evaluating on the test split.
I0129 20:26:31.794762 139822745589568 submission_runner.py:408] Time since start: 3218.78s, 	Step: 8957, 	{'train/accuracy': 0.6165497303009033, 'train/loss': 1.5784637928009033, 'validation/accuracy': 0.5652799606323242, 'validation/loss': 1.8457132577896118, 'validation/num_examples': 50000, 'test/accuracy': 0.4390000104904175, 'test/loss': 2.5789635181427, 'test/num_examples': 10000, 'score': 3095.8154296875, 'total_duration': 3218.784925699234, 'accumulated_submission_time': 3095.8154296875, 'accumulated_eval_time': 122.51022338867188, 'accumulated_logging_time': 0.15499329566955566}
I0129 20:26:31.814839 139658730145536 logging_writer.py:48] [8957] accumulated_eval_time=122.510223, accumulated_logging_time=0.154993, accumulated_submission_time=3095.815430, global_step=8957, preemption_count=0, score=3095.815430, test/accuracy=0.439000, test/loss=2.578964, test/num_examples=10000, total_duration=3218.784926, train/accuracy=0.616550, train/loss=1.578464, validation/accuracy=0.565280, validation/loss=1.845713, validation/num_examples=50000
I0129 20:26:46.832669 139658738538240 logging_writer.py:48] [9000] global_step=9000, grad_norm=1.3094823360443115, loss=2.179161310195923
I0129 20:27:20.966703 139658730145536 logging_writer.py:48] [9100] global_step=9100, grad_norm=1.691879391670227, loss=2.1598739624023438
I0129 20:27:55.171639 139658738538240 logging_writer.py:48] [9200] global_step=9200, grad_norm=1.7227325439453125, loss=2.1181912422180176
I0129 20:28:29.313691 139658730145536 logging_writer.py:48] [9300] global_step=9300, grad_norm=1.5656077861785889, loss=2.0357251167297363
I0129 20:29:03.478160 139658738538240 logging_writer.py:48] [9400] global_step=9400, grad_norm=1.469129204750061, loss=2.0519092082977295
I0129 20:29:37.663339 139658730145536 logging_writer.py:48] [9500] global_step=9500, grad_norm=1.5120971202850342, loss=2.1388068199157715
I0129 20:30:11.837359 139658738538240 logging_writer.py:48] [9600] global_step=9600, grad_norm=1.8089814186096191, loss=2.110491991043091
I0129 20:30:45.985311 139658730145536 logging_writer.py:48] [9700] global_step=9700, grad_norm=1.569728136062622, loss=2.0857107639312744
I0129 20:31:20.148589 139658738538240 logging_writer.py:48] [9800] global_step=9800, grad_norm=1.5953452587127686, loss=2.120490550994873
I0129 20:31:54.320687 139658730145536 logging_writer.py:48] [9900] global_step=9900, grad_norm=1.7043026685714722, loss=2.088223934173584
I0129 20:32:28.477959 139658738538240 logging_writer.py:48] [10000] global_step=10000, grad_norm=1.7488713264465332, loss=2.130824089050293
I0129 20:33:02.603560 139658730145536 logging_writer.py:48] [10100] global_step=10100, grad_norm=2.087082862854004, loss=2.131995677947998
I0129 20:33:36.735373 139658738538240 logging_writer.py:48] [10200] global_step=10200, grad_norm=1.7803322076797485, loss=2.1123173236846924
I0129 20:34:10.862296 139658730145536 logging_writer.py:48] [10300] global_step=10300, grad_norm=1.5385676622390747, loss=1.9962003231048584
I0129 20:34:45.083165 139658738538240 logging_writer.py:48] [10400] global_step=10400, grad_norm=1.4112449884414673, loss=2.090853691101074
I0129 20:35:01.952930 139822745589568 spec.py:321] Evaluating on the training split.
I0129 20:35:08.221762 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 20:35:16.716083 139822745589568 spec.py:349] Evaluating on the test split.
I0129 20:35:19.325607 139822745589568 submission_runner.py:408] Time since start: 3746.32s, 	Step: 10451, 	{'train/accuracy': 0.6073620915412903, 'train/loss': 1.6080734729766846, 'validation/accuracy': 0.5593799948692322, 'validation/loss': 1.851853370666504, 'validation/num_examples': 50000, 'test/accuracy': 0.44040003418922424, 'test/loss': 2.6110658645629883, 'test/num_examples': 10000, 'score': 3605.894075870514, 'total_duration': 3746.3157720565796, 'accumulated_submission_time': 3605.894075870514, 'accumulated_eval_time': 139.8828718662262, 'accumulated_logging_time': 0.18477296829223633}
I0129 20:35:19.345271 139655592802048 logging_writer.py:48] [10451] accumulated_eval_time=139.882872, accumulated_logging_time=0.184773, accumulated_submission_time=3605.894076, global_step=10451, preemption_count=0, score=3605.894076, test/accuracy=0.440400, test/loss=2.611066, test/num_examples=10000, total_duration=3746.315772, train/accuracy=0.607362, train/loss=1.608073, validation/accuracy=0.559380, validation/loss=1.851853, validation/num_examples=50000
I0129 20:35:36.433014 139656297445120 logging_writer.py:48] [10500] global_step=10500, grad_norm=2.2842674255371094, loss=2.006235122680664
I0129 20:36:10.524649 139655592802048 logging_writer.py:48] [10600] global_step=10600, grad_norm=1.9908627271652222, loss=1.9291768074035645
I0129 20:36:44.654759 139656297445120 logging_writer.py:48] [10700] global_step=10700, grad_norm=1.5185779333114624, loss=1.9642904996871948
I0129 20:37:18.797150 139655592802048 logging_writer.py:48] [10800] global_step=10800, grad_norm=1.6245977878570557, loss=2.0234031677246094
I0129 20:37:52.970582 139656297445120 logging_writer.py:48] [10900] global_step=10900, grad_norm=1.332924723625183, loss=1.907057762145996
I0129 20:38:27.139404 139655592802048 logging_writer.py:48] [11000] global_step=11000, grad_norm=2.0400567054748535, loss=1.9553192853927612
I0129 20:39:01.298369 139656297445120 logging_writer.py:48] [11100] global_step=11100, grad_norm=1.941211462020874, loss=2.0896687507629395
I0129 20:39:35.465623 139655592802048 logging_writer.py:48] [11200] global_step=11200, grad_norm=1.2793827056884766, loss=2.0523321628570557
I0129 20:40:09.608493 139656297445120 logging_writer.py:48] [11300] global_step=11300, grad_norm=1.5873425006866455, loss=2.0169222354888916
I0129 20:40:43.754816 139655592802048 logging_writer.py:48] [11400] global_step=11400, grad_norm=1.9546797275543213, loss=1.9986729621887207
I0129 20:41:17.991295 139656297445120 logging_writer.py:48] [11500] global_step=11500, grad_norm=2.138925313949585, loss=1.9538052082061768
I0129 20:41:52.151474 139655592802048 logging_writer.py:48] [11600] global_step=11600, grad_norm=1.579132080078125, loss=1.9989218711853027
I0129 20:42:26.310143 139656297445120 logging_writer.py:48] [11700] global_step=11700, grad_norm=1.9250065088272095, loss=1.8807449340820312
I0129 20:43:00.451656 139655592802048 logging_writer.py:48] [11800] global_step=11800, grad_norm=1.7818905115127563, loss=2.002699136734009
I0129 20:43:34.603547 139656297445120 logging_writer.py:48] [11900] global_step=11900, grad_norm=1.3509832620620728, loss=1.9491698741912842
I0129 20:43:49.439467 139822745589568 spec.py:321] Evaluating on the training split.
I0129 20:43:55.671822 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 20:44:04.275697 139822745589568 spec.py:349] Evaluating on the test split.
I0129 20:44:06.901333 139822745589568 submission_runner.py:408] Time since start: 4273.89s, 	Step: 11945, 	{'train/accuracy': 0.6295639276504517, 'train/loss': 1.503599762916565, 'validation/accuracy': 0.5816599726676941, 'validation/loss': 1.7506680488586426, 'validation/num_examples': 50000, 'test/accuracy': 0.4595000147819519, 'test/loss': 2.491586923599243, 'test/num_examples': 10000, 'score': 4115.927650213242, 'total_duration': 4273.891491889954, 'accumulated_submission_time': 4115.927650213242, 'accumulated_eval_time': 157.3446958065033, 'accumulated_logging_time': 0.21372079849243164}
I0129 20:44:06.921196 139655576016640 logging_writer.py:48] [11945] accumulated_eval_time=157.344696, accumulated_logging_time=0.213721, accumulated_submission_time=4115.927650, global_step=11945, preemption_count=0, score=4115.927650, test/accuracy=0.459500, test/loss=2.491587, test/num_examples=10000, total_duration=4273.891492, train/accuracy=0.629564, train/loss=1.503600, validation/accuracy=0.581660, validation/loss=1.750668, validation/num_examples=50000
I0129 20:44:26.027157 139655584409344 logging_writer.py:48] [12000] global_step=12000, grad_norm=1.8839813470840454, loss=1.9919825792312622
I0129 20:45:00.113922 139655576016640 logging_writer.py:48] [12100] global_step=12100, grad_norm=1.7077970504760742, loss=2.0073304176330566
I0129 20:45:34.257365 139655584409344 logging_writer.py:48] [12200] global_step=12200, grad_norm=1.6003150939941406, loss=2.023582696914673
I0129 20:46:08.404810 139655576016640 logging_writer.py:48] [12300] global_step=12300, grad_norm=1.7448564767837524, loss=1.901346206665039
I0129 20:46:42.546225 139655584409344 logging_writer.py:48] [12400] global_step=12400, grad_norm=1.2768083810806274, loss=2.007824182510376
I0129 20:47:16.660533 139655576016640 logging_writer.py:48] [12500] global_step=12500, grad_norm=1.551520586013794, loss=1.9625916481018066
I0129 20:47:50.889958 139655584409344 logging_writer.py:48] [12600] global_step=12600, grad_norm=1.3033502101898193, loss=2.0569636821746826
I0129 20:48:25.022364 139655576016640 logging_writer.py:48] [12700] global_step=12700, grad_norm=1.4962435960769653, loss=1.9246577024459839
I0129 20:48:59.176990 139655584409344 logging_writer.py:48] [12800] global_step=12800, grad_norm=1.6594007015228271, loss=1.9744343757629395
I0129 20:49:33.325015 139655576016640 logging_writer.py:48] [12900] global_step=12900, grad_norm=1.6567866802215576, loss=1.9864450693130493
I0129 20:50:07.488454 139655584409344 logging_writer.py:48] [13000] global_step=13000, grad_norm=1.7060059309005737, loss=1.943320870399475
I0129 20:50:41.628046 139655576016640 logging_writer.py:48] [13100] global_step=13100, grad_norm=1.8516207933425903, loss=1.9973528385162354
I0129 20:51:15.780656 139655584409344 logging_writer.py:48] [13200] global_step=13200, grad_norm=1.430984377861023, loss=1.9635772705078125
I0129 20:51:49.935763 139655576016640 logging_writer.py:48] [13300] global_step=13300, grad_norm=1.2104268074035645, loss=1.9391720294952393
I0129 20:52:24.086012 139655584409344 logging_writer.py:48] [13400] global_step=13400, grad_norm=1.4555009603500366, loss=2.082134246826172
I0129 20:52:37.206930 139822745589568 spec.py:321] Evaluating on the training split.
I0129 20:52:43.423781 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 20:52:51.987161 139822745589568 spec.py:349] Evaluating on the test split.
I0129 20:52:54.620842 139822745589568 submission_runner.py:408] Time since start: 4801.61s, 	Step: 13440, 	{'train/accuracy': 0.6180644035339355, 'train/loss': 1.5705331563949585, 'validation/accuracy': 0.5727199912071228, 'validation/loss': 1.823119044303894, 'validation/num_examples': 50000, 'test/accuracy': 0.44770002365112305, 'test/loss': 2.5825207233428955, 'test/num_examples': 10000, 'score': 4626.153445720673, 'total_duration': 4801.611012220383, 'accumulated_submission_time': 4626.153445720673, 'accumulated_eval_time': 174.75857639312744, 'accumulated_logging_time': 0.24312424659729004}
I0129 20:52:54.639764 139656658151168 logging_writer.py:48] [13440] accumulated_eval_time=174.758576, accumulated_logging_time=0.243124, accumulated_submission_time=4626.153446, global_step=13440, preemption_count=0, score=4626.153446, test/accuracy=0.447700, test/loss=2.582521, test/num_examples=10000, total_duration=4801.611012, train/accuracy=0.618064, train/loss=1.570533, validation/accuracy=0.572720, validation/loss=1.823119, validation/num_examples=50000
I0129 20:53:15.442410 139656666543872 logging_writer.py:48] [13500] global_step=13500, grad_norm=1.6466327905654907, loss=2.0570921897888184
I0129 20:53:49.538903 139656658151168 logging_writer.py:48] [13600] global_step=13600, grad_norm=1.5022737979888916, loss=1.9354348182678223
I0129 20:54:23.706911 139656666543872 logging_writer.py:48] [13700] global_step=13700, grad_norm=1.5348751544952393, loss=1.8552311658859253
I0129 20:54:57.834781 139656658151168 logging_writer.py:48] [13800] global_step=13800, grad_norm=1.4889538288116455, loss=2.0077786445617676
I0129 20:55:31.913654 139656666543872 logging_writer.py:48] [13900] global_step=13900, grad_norm=1.4172554016113281, loss=1.8618481159210205
I0129 20:56:06.020328 139656658151168 logging_writer.py:48] [14000] global_step=14000, grad_norm=1.524664282798767, loss=2.078930377960205
I0129 20:56:40.114953 139656666543872 logging_writer.py:48] [14100] global_step=14100, grad_norm=1.7846673727035522, loss=1.859283208847046
I0129 20:57:14.231633 139656658151168 logging_writer.py:48] [14200] global_step=14200, grad_norm=1.5961036682128906, loss=1.8481942415237427
I0129 20:57:48.355714 139656666543872 logging_writer.py:48] [14300] global_step=14300, grad_norm=1.5872511863708496, loss=1.9530072212219238
I0129 20:58:22.481000 139656658151168 logging_writer.py:48] [14400] global_step=14400, grad_norm=1.7773339748382568, loss=1.9129836559295654
I0129 20:58:56.644774 139656666543872 logging_writer.py:48] [14500] global_step=14500, grad_norm=1.4943495988845825, loss=1.9352697134017944
I0129 20:59:30.786860 139656658151168 logging_writer.py:48] [14600] global_step=14600, grad_norm=1.7347310781478882, loss=1.8642810583114624
I0129 21:00:04.923462 139656666543872 logging_writer.py:48] [14700] global_step=14700, grad_norm=1.3036292791366577, loss=1.9437499046325684
I0129 21:00:39.046949 139656658151168 logging_writer.py:48] [14800] global_step=14800, grad_norm=1.8867062330245972, loss=1.8659719228744507
I0129 21:01:13.233809 139656666543872 logging_writer.py:48] [14900] global_step=14900, grad_norm=1.5237095355987549, loss=1.7873588800430298
I0129 21:01:24.637473 139822745589568 spec.py:321] Evaluating on the training split.
I0129 21:01:30.932438 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 21:01:39.515208 139822745589568 spec.py:349] Evaluating on the test split.
I0129 21:01:42.150413 139822745589568 submission_runner.py:408] Time since start: 5329.14s, 	Step: 14935, 	{'train/accuracy': 0.68363356590271, 'train/loss': 1.2555805444717407, 'validation/accuracy': 0.6050199866294861, 'validation/loss': 1.6677595376968384, 'validation/num_examples': 50000, 'test/accuracy': 0.4702000319957733, 'test/loss': 2.433048725128174, 'test/num_examples': 10000, 'score': 5136.090697288513, 'total_duration': 5329.14058303833, 'accumulated_submission_time': 5136.090697288513, 'accumulated_eval_time': 192.2714822292328, 'accumulated_logging_time': 0.2714576721191406}
I0129 21:01:42.170265 139655592802048 logging_writer.py:48] [14935] accumulated_eval_time=192.271482, accumulated_logging_time=0.271458, accumulated_submission_time=5136.090697, global_step=14935, preemption_count=0, score=5136.090697, test/accuracy=0.470200, test/loss=2.433049, test/num_examples=10000, total_duration=5329.140583, train/accuracy=0.683634, train/loss=1.255581, validation/accuracy=0.605020, validation/loss=1.667760, validation/num_examples=50000
I0129 21:02:04.652386 139656297445120 logging_writer.py:48] [15000] global_step=15000, grad_norm=1.9346810579299927, loss=1.8046988248825073
I0129 21:02:38.753688 139655592802048 logging_writer.py:48] [15100] global_step=15100, grad_norm=1.3849912881851196, loss=2.0174765586853027
I0129 21:03:12.836394 139656297445120 logging_writer.py:48] [15200] global_step=15200, grad_norm=1.8550626039505005, loss=1.9385794401168823
I0129 21:03:46.951159 139655592802048 logging_writer.py:48] [15300] global_step=15300, grad_norm=1.578231930732727, loss=1.845207929611206
I0129 21:04:21.087474 139656297445120 logging_writer.py:48] [15400] global_step=15400, grad_norm=1.5094631910324097, loss=1.9467538595199585
I0129 21:04:55.201948 139655592802048 logging_writer.py:48] [15500] global_step=15500, grad_norm=1.8484822511672974, loss=1.8841831684112549
I0129 21:05:29.329212 139656297445120 logging_writer.py:48] [15600] global_step=15600, grad_norm=1.6349153518676758, loss=2.0386481285095215
I0129 21:06:03.466627 139655592802048 logging_writer.py:48] [15700] global_step=15700, grad_norm=1.402597427368164, loss=2.022190570831299
I0129 21:06:37.603279 139656297445120 logging_writer.py:48] [15800] global_step=15800, grad_norm=1.7916089296340942, loss=1.8300971984863281
I0129 21:07:11.730047 139655592802048 logging_writer.py:48] [15900] global_step=15900, grad_norm=1.4788199663162231, loss=1.899160623550415
I0129 21:07:45.944393 139656297445120 logging_writer.py:48] [16000] global_step=16000, grad_norm=1.6869529485702515, loss=1.9684648513793945
I0129 21:08:20.073167 139655592802048 logging_writer.py:48] [16100] global_step=16100, grad_norm=1.887794017791748, loss=1.7775449752807617
I0129 21:08:54.224206 139656297445120 logging_writer.py:48] [16200] global_step=16200, grad_norm=1.5212218761444092, loss=1.8938463926315308
I0129 21:09:28.361306 139655592802048 logging_writer.py:48] [16300] global_step=16300, grad_norm=1.4894611835479736, loss=1.731632947921753
I0129 21:10:02.506939 139656297445120 logging_writer.py:48] [16400] global_step=16400, grad_norm=1.4014593362808228, loss=1.813277006149292
I0129 21:10:12.221799 139822745589568 spec.py:321] Evaluating on the training split.
I0129 21:10:18.473591 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 21:10:26.897313 139822745589568 spec.py:349] Evaluating on the test split.
I0129 21:10:29.635512 139822745589568 submission_runner.py:408] Time since start: 5856.63s, 	Step: 16430, 	{'train/accuracy': 0.6679487824440002, 'train/loss': 1.3212976455688477, 'validation/accuracy': 0.6027599573135376, 'validation/loss': 1.6629142761230469, 'validation/num_examples': 50000, 'test/accuracy': 0.47850000858306885, 'test/loss': 2.401660203933716, 'test/num_examples': 10000, 'score': 5646.080711364746, 'total_duration': 5856.62567782402, 'accumulated_submission_time': 5646.080711364746, 'accumulated_eval_time': 209.68515920639038, 'accumulated_logging_time': 0.30024099349975586}
I0129 21:10:29.655508 139655584409344 logging_writer.py:48] [16430] accumulated_eval_time=209.685159, accumulated_logging_time=0.300241, accumulated_submission_time=5646.080711, global_step=16430, preemption_count=0, score=5646.080711, test/accuracy=0.478500, test/loss=2.401660, test/num_examples=10000, total_duration=5856.625678, train/accuracy=0.667949, train/loss=1.321298, validation/accuracy=0.602760, validation/loss=1.662914, validation/num_examples=50000
I0129 21:10:53.857572 139656666543872 logging_writer.py:48] [16500] global_step=16500, grad_norm=1.694225549697876, loss=1.8460807800292969
I0129 21:11:27.945932 139655584409344 logging_writer.py:48] [16600] global_step=16600, grad_norm=1.787238359451294, loss=1.8246361017227173
I0129 21:12:02.034432 139656666543872 logging_writer.py:48] [16700] global_step=16700, grad_norm=1.646081805229187, loss=1.9140782356262207
I0129 21:12:36.152167 139655584409344 logging_writer.py:48] [16800] global_step=16800, grad_norm=1.7945352792739868, loss=1.8261972665786743
I0129 21:13:10.274389 139656666543872 logging_writer.py:48] [16900] global_step=16900, grad_norm=1.6442278623580933, loss=1.8254542350769043
I0129 21:13:44.383357 139655584409344 logging_writer.py:48] [17000] global_step=17000, grad_norm=1.6808297634124756, loss=1.868171215057373
I0129 21:14:18.595495 139656666543872 logging_writer.py:48] [17100] global_step=17100, grad_norm=1.5380032062530518, loss=1.8346660137176514
I0129 21:14:52.690677 139655584409344 logging_writer.py:48] [17200] global_step=17200, grad_norm=1.4592639207839966, loss=1.8248069286346436
I0129 21:15:26.818856 139656666543872 logging_writer.py:48] [17300] global_step=17300, grad_norm=1.4303398132324219, loss=1.7545198202133179
I0129 21:16:00.927537 139655584409344 logging_writer.py:48] [17400] global_step=17400, grad_norm=1.620552897453308, loss=1.8132131099700928
I0129 21:16:35.036753 139656666543872 logging_writer.py:48] [17500] global_step=17500, grad_norm=1.4014328718185425, loss=1.857391595840454
I0129 21:17:09.162353 139655584409344 logging_writer.py:48] [17600] global_step=17600, grad_norm=1.6148765087127686, loss=1.8138748407363892
I0129 21:17:43.287447 139656666543872 logging_writer.py:48] [17700] global_step=17700, grad_norm=1.5353140830993652, loss=1.9394136667251587
I0129 21:18:17.384730 139655584409344 logging_writer.py:48] [17800] global_step=17800, grad_norm=1.4454399347305298, loss=1.916870355606079
I0129 21:18:51.496445 139656666543872 logging_writer.py:48] [17900] global_step=17900, grad_norm=1.5582767724990845, loss=1.8795803785324097
I0129 21:18:59.835853 139822745589568 spec.py:321] Evaluating on the training split.
I0129 21:19:06.208178 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 21:19:14.682362 139822745589568 spec.py:349] Evaluating on the test split.
I0129 21:19:17.364736 139822745589568 submission_runner.py:408] Time since start: 6384.35s, 	Step: 17926, 	{'train/accuracy': 0.6713767647743225, 'train/loss': 1.295975685119629, 'validation/accuracy': 0.6087599992752075, 'validation/loss': 1.628176212310791, 'validation/num_examples': 50000, 'test/accuracy': 0.4845000207424164, 'test/loss': 2.360335111618042, 'test/num_examples': 10000, 'score': 6156.199065208435, 'total_duration': 6384.354900598526, 'accumulated_submission_time': 6156.199065208435, 'accumulated_eval_time': 227.21400547027588, 'accumulated_logging_time': 0.3311781883239746}
I0129 21:19:17.385967 139655592802048 logging_writer.py:48] [17926] accumulated_eval_time=227.214005, accumulated_logging_time=0.331178, accumulated_submission_time=6156.199065, global_step=17926, preemption_count=0, score=6156.199065, test/accuracy=0.484500, test/loss=2.360335, test/num_examples=10000, total_duration=6384.354901, train/accuracy=0.671377, train/loss=1.295976, validation/accuracy=0.608760, validation/loss=1.628176, validation/num_examples=50000
I0129 21:19:42.956108 139656297445120 logging_writer.py:48] [18000] global_step=18000, grad_norm=1.5921686887741089, loss=1.9505499601364136
I0129 21:20:17.022373 139655592802048 logging_writer.py:48] [18100] global_step=18100, grad_norm=1.5067026615142822, loss=1.8173999786376953
I0129 21:20:51.120182 139656297445120 logging_writer.py:48] [18200] global_step=18200, grad_norm=1.4302386045455933, loss=1.763014316558838
I0129 21:21:25.303192 139655592802048 logging_writer.py:48] [18300] global_step=18300, grad_norm=1.6214317083358765, loss=1.8221478462219238
I0129 21:21:59.452594 139656297445120 logging_writer.py:48] [18400] global_step=18400, grad_norm=1.596808671951294, loss=1.9523422718048096
I0129 21:22:33.583122 139655592802048 logging_writer.py:48] [18500] global_step=18500, grad_norm=1.4648911952972412, loss=1.7601838111877441
I0129 21:23:07.705750 139656297445120 logging_writer.py:48] [18600] global_step=18600, grad_norm=1.6444835662841797, loss=1.867838740348816
I0129 21:23:41.845438 139655592802048 logging_writer.py:48] [18700] global_step=18700, grad_norm=1.4051907062530518, loss=1.7690966129302979
I0129 21:24:15.989815 139656297445120 logging_writer.py:48] [18800] global_step=18800, grad_norm=1.8255747556686401, loss=1.9145985841751099
I0129 21:24:50.136125 139655592802048 logging_writer.py:48] [18900] global_step=18900, grad_norm=1.4762307405471802, loss=1.834902048110962
I0129 21:25:24.273089 139656297445120 logging_writer.py:48] [19000] global_step=19000, grad_norm=1.8261442184448242, loss=1.9652795791625977
I0129 21:25:58.407256 139655592802048 logging_writer.py:48] [19100] global_step=19100, grad_norm=1.7141923904418945, loss=1.8445252180099487
I0129 21:26:32.542320 139656297445120 logging_writer.py:48] [19200] global_step=19200, grad_norm=1.7826828956604004, loss=1.9622125625610352
I0129 21:27:06.691461 139655592802048 logging_writer.py:48] [19300] global_step=19300, grad_norm=1.7155909538269043, loss=1.8349361419677734
I0129 21:27:40.887607 139656297445120 logging_writer.py:48] [19400] global_step=19400, grad_norm=1.6362788677215576, loss=1.9918737411499023
I0129 21:27:47.537349 139822745589568 spec.py:321] Evaluating on the training split.
I0129 21:27:53.782019 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 21:28:02.371927 139822745589568 spec.py:349] Evaluating on the test split.
I0129 21:28:04.891322 139822745589568 submission_runner.py:408] Time since start: 6911.88s, 	Step: 19421, 	{'train/accuracy': 0.6640226244926453, 'train/loss': 1.3366535902023315, 'validation/accuracy': 0.6086399555206299, 'validation/loss': 1.63140869140625, 'validation/num_examples': 50000, 'test/accuracy': 0.47950002551078796, 'test/loss': 2.3939144611358643, 'test/num_examples': 10000, 'score': 6666.292007684708, 'total_duration': 6911.881479263306, 'accumulated_submission_time': 6666.292007684708, 'accumulated_eval_time': 244.56793308258057, 'accumulated_logging_time': 0.36154890060424805}
I0129 21:28:04.914228 139656666543872 logging_writer.py:48] [19421] accumulated_eval_time=244.567933, accumulated_logging_time=0.361549, accumulated_submission_time=6666.292008, global_step=19421, preemption_count=0, score=6666.292008, test/accuracy=0.479500, test/loss=2.393914, test/num_examples=10000, total_duration=6911.881479, train/accuracy=0.664023, train/loss=1.336654, validation/accuracy=0.608640, validation/loss=1.631409, validation/num_examples=50000
I0129 21:28:32.214562 139656834316032 logging_writer.py:48] [19500] global_step=19500, grad_norm=1.6701549291610718, loss=1.8546627759933472
I0129 21:29:06.276904 139656666543872 logging_writer.py:48] [19600] global_step=19600, grad_norm=1.7475084066390991, loss=1.865680456161499
I0129 21:29:40.353646 139656834316032 logging_writer.py:48] [19700] global_step=19700, grad_norm=1.8336387872695923, loss=1.8661167621612549
I0129 21:30:14.463219 139656666543872 logging_writer.py:48] [19800] global_step=19800, grad_norm=1.6285957098007202, loss=1.862964391708374
I0129 21:30:48.563985 139656834316032 logging_writer.py:48] [19900] global_step=19900, grad_norm=1.536766767501831, loss=1.907314658164978
I0129 21:31:22.682388 139656666543872 logging_writer.py:48] [20000] global_step=20000, grad_norm=1.5496413707733154, loss=1.7855172157287598
I0129 21:31:56.763064 139656834316032 logging_writer.py:48] [20100] global_step=20100, grad_norm=1.387894630432129, loss=1.7271828651428223
I0129 21:32:30.867596 139656666543872 logging_writer.py:48] [20200] global_step=20200, grad_norm=1.4407066106796265, loss=1.787287950515747
I0129 21:33:04.989117 139656834316032 logging_writer.py:48] [20300] global_step=20300, grad_norm=1.7694820165634155, loss=1.9264330863952637
I0129 21:33:39.079432 139656666543872 logging_writer.py:48] [20400] global_step=20400, grad_norm=1.6821484565734863, loss=1.8005870580673218
I0129 21:34:13.287355 139656834316032 logging_writer.py:48] [20500] global_step=20500, grad_norm=1.904317021369934, loss=1.753780484199524
I0129 21:34:47.432097 139656666543872 logging_writer.py:48] [20600] global_step=20600, grad_norm=1.7275185585021973, loss=1.7555955648422241
I0129 21:35:21.568578 139656834316032 logging_writer.py:48] [20700] global_step=20700, grad_norm=1.452388048171997, loss=1.7490932941436768
I0129 21:35:55.704520 139656666543872 logging_writer.py:48] [20800] global_step=20800, grad_norm=1.8212698698043823, loss=1.739713191986084
I0129 21:36:29.828549 139656834316032 logging_writer.py:48] [20900] global_step=20900, grad_norm=1.6513657569885254, loss=1.9732153415679932
I0129 21:36:35.100342 139822745589568 spec.py:321] Evaluating on the training split.
I0129 21:36:42.006860 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 21:36:50.761524 139822745589568 spec.py:349] Evaluating on the test split.
I0129 21:36:53.280659 139822745589568 submission_runner.py:408] Time since start: 7440.27s, 	Step: 20917, 	{'train/accuracy': 0.6470623016357422, 'train/loss': 1.4147604703903198, 'validation/accuracy': 0.5916999578475952, 'validation/loss': 1.7275773286819458, 'validation/num_examples': 50000, 'test/accuracy': 0.4636000096797943, 'test/loss': 2.501309394836426, 'test/num_examples': 10000, 'score': 7176.418109893799, 'total_duration': 7440.27081990242, 'accumulated_submission_time': 7176.418109893799, 'accumulated_eval_time': 262.74822521209717, 'accumulated_logging_time': 0.39341068267822266}
I0129 21:36:53.302070 139655584409344 logging_writer.py:48] [20917] accumulated_eval_time=262.748225, accumulated_logging_time=0.393411, accumulated_submission_time=7176.418110, global_step=20917, preemption_count=0, score=7176.418110, test/accuracy=0.463600, test/loss=2.501309, test/num_examples=10000, total_duration=7440.270820, train/accuracy=0.647062, train/loss=1.414760, validation/accuracy=0.591700, validation/loss=1.727577, validation/num_examples=50000
I0129 21:37:21.955234 139655592802048 logging_writer.py:48] [21000] global_step=21000, grad_norm=1.670156717300415, loss=1.8351659774780273
I0129 21:37:56.067219 139655584409344 logging_writer.py:48] [21100] global_step=21100, grad_norm=1.7248698472976685, loss=1.9241915941238403
I0129 21:38:30.204583 139655592802048 logging_writer.py:48] [21200] global_step=21200, grad_norm=1.6136857271194458, loss=1.8606467247009277
I0129 21:39:04.327137 139655584409344 logging_writer.py:48] [21300] global_step=21300, grad_norm=1.6697955131530762, loss=1.846335530281067
I0129 21:39:38.456351 139655592802048 logging_writer.py:48] [21400] global_step=21400, grad_norm=1.5123636722564697, loss=1.7829804420471191
I0129 21:40:12.586854 139655584409344 logging_writer.py:48] [21500] global_step=21500, grad_norm=1.58612060546875, loss=1.8290975093841553
I0129 21:40:46.826267 139655592802048 logging_writer.py:48] [21600] global_step=21600, grad_norm=1.9241515398025513, loss=1.9196460247039795
I0129 21:41:20.969012 139655584409344 logging_writer.py:48] [21700] global_step=21700, grad_norm=1.5039316415786743, loss=1.6924375295639038
I0129 21:41:55.114876 139655592802048 logging_writer.py:48] [21800] global_step=21800, grad_norm=1.9257376194000244, loss=1.8939262628555298
I0129 21:42:29.209352 139655584409344 logging_writer.py:48] [21900] global_step=21900, grad_norm=1.5209307670593262, loss=1.8805543184280396
I0129 21:43:03.338010 139655592802048 logging_writer.py:48] [22000] global_step=22000, grad_norm=1.5321722030639648, loss=1.8180830478668213
I0129 21:43:37.461712 139655584409344 logging_writer.py:48] [22100] global_step=22100, grad_norm=1.608697772026062, loss=1.7831621170043945
I0129 21:44:11.561499 139655592802048 logging_writer.py:48] [22200] global_step=22200, grad_norm=1.6622700691223145, loss=1.7145402431488037
I0129 21:44:45.666037 139655584409344 logging_writer.py:48] [22300] global_step=22300, grad_norm=1.7375679016113281, loss=1.7798633575439453
I0129 21:45:19.747908 139655592802048 logging_writer.py:48] [22400] global_step=22400, grad_norm=1.8604278564453125, loss=1.7631618976593018
I0129 21:45:23.312769 139822745589568 spec.py:321] Evaluating on the training split.
I0129 21:45:29.562630 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 21:45:38.236744 139822745589568 spec.py:349] Evaluating on the test split.
I0129 21:45:40.859161 139822745589568 submission_runner.py:408] Time since start: 7967.85s, 	Step: 22412, 	{'train/accuracy': 0.6626673936843872, 'train/loss': 1.3498613834381104, 'validation/accuracy': 0.6109600067138672, 'validation/loss': 1.6298362016677856, 'validation/num_examples': 50000, 'test/accuracy': 0.48340001702308655, 'test/loss': 2.372260332107544, 'test/num_examples': 10000, 'score': 7686.367953538895, 'total_duration': 7967.849324464798, 'accumulated_submission_time': 7686.367953538895, 'accumulated_eval_time': 280.29457664489746, 'accumulated_logging_time': 0.42447733879089355}
I0129 21:45:40.880382 139656834316032 logging_writer.py:48] [22412] accumulated_eval_time=280.294577, accumulated_logging_time=0.424477, accumulated_submission_time=7686.367954, global_step=22412, preemption_count=0, score=7686.367954, test/accuracy=0.483400, test/loss=2.372260, test/num_examples=10000, total_duration=7967.849324, train/accuracy=0.662667, train/loss=1.349861, validation/accuracy=0.610960, validation/loss=1.629836, validation/num_examples=50000
I0129 21:46:11.186541 139658730145536 logging_writer.py:48] [22500] global_step=22500, grad_norm=1.725416660308838, loss=1.8058470487594604
I0129 21:46:45.289567 139656834316032 logging_writer.py:48] [22600] global_step=22600, grad_norm=1.561360478401184, loss=1.728554368019104
I0129 21:47:19.485960 139658730145536 logging_writer.py:48] [22700] global_step=22700, grad_norm=1.6488286256790161, loss=1.691166639328003
I0129 21:47:53.605960 139656834316032 logging_writer.py:48] [22800] global_step=22800, grad_norm=1.5917541980743408, loss=1.7935714721679688
I0129 21:48:27.756873 139658730145536 logging_writer.py:48] [22900] global_step=22900, grad_norm=1.653708577156067, loss=1.6880489587783813
I0129 21:49:01.883046 139656834316032 logging_writer.py:48] [23000] global_step=23000, grad_norm=1.90037202835083, loss=1.8154441118240356
I0129 21:49:36.008231 139658730145536 logging_writer.py:48] [23100] global_step=23100, grad_norm=1.7909126281738281, loss=1.81605064868927
I0129 21:50:10.134443 139656834316032 logging_writer.py:48] [23200] global_step=23200, grad_norm=1.6718331575393677, loss=1.7979695796966553
I0129 21:50:44.276849 139658730145536 logging_writer.py:48] [23300] global_step=23300, grad_norm=1.6472223997116089, loss=1.77532160282135
I0129 21:51:18.404952 139656834316032 logging_writer.py:48] [23400] global_step=23400, grad_norm=1.631181001663208, loss=1.8462674617767334
I0129 21:51:52.543214 139658730145536 logging_writer.py:48] [23500] global_step=23500, grad_norm=1.93495774269104, loss=1.9120715856552124
I0129 21:52:26.633651 139656834316032 logging_writer.py:48] [23600] global_step=23600, grad_norm=1.608969807624817, loss=1.7915914058685303
I0129 21:53:00.753186 139658730145536 logging_writer.py:48] [23700] global_step=23700, grad_norm=1.7002685070037842, loss=1.776641845703125
I0129 21:53:34.851288 139656834316032 logging_writer.py:48] [23800] global_step=23800, grad_norm=1.5689448118209839, loss=1.7525578737258911
I0129 21:54:09.082589 139658730145536 logging_writer.py:48] [23900] global_step=23900, grad_norm=1.95858633518219, loss=1.7412993907928467
I0129 21:54:10.941850 139822745589568 spec.py:321] Evaluating on the training split.
I0129 21:54:17.163495 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 21:54:25.575476 139822745589568 spec.py:349] Evaluating on the test split.
I0129 21:54:28.199375 139822745589568 submission_runner.py:408] Time since start: 8495.19s, 	Step: 23907, 	{'train/accuracy': 0.7183513641357422, 'train/loss': 1.0917080640792847, 'validation/accuracy': 0.6167199611663818, 'validation/loss': 1.610385775566101, 'validation/num_examples': 50000, 'test/accuracy': 0.4861000180244446, 'test/loss': 2.371723175048828, 'test/num_examples': 10000, 'score': 8196.371249198914, 'total_duration': 8495.189532518387, 'accumulated_submission_time': 8196.371249198914, 'accumulated_eval_time': 297.5520570278168, 'accumulated_logging_time': 0.4546489715576172}
I0129 21:54:28.220707 139655592802048 logging_writer.py:48] [23907] accumulated_eval_time=297.552057, accumulated_logging_time=0.454649, accumulated_submission_time=8196.371249, global_step=23907, preemption_count=0, score=8196.371249, test/accuracy=0.486100, test/loss=2.371723, test/num_examples=10000, total_duration=8495.189533, train/accuracy=0.718351, train/loss=1.091708, validation/accuracy=0.616720, validation/loss=1.610386, validation/num_examples=50000
I0129 21:55:00.287955 139656297445120 logging_writer.py:48] [24000] global_step=24000, grad_norm=1.7765289545059204, loss=1.8685293197631836
I0129 21:55:34.411554 139655592802048 logging_writer.py:48] [24100] global_step=24100, grad_norm=1.9791679382324219, loss=1.8855878114700317
I0129 21:56:08.537157 139656297445120 logging_writer.py:48] [24200] global_step=24200, grad_norm=1.5441340208053589, loss=1.756664514541626
I0129 21:56:42.634779 139655592802048 logging_writer.py:48] [24300] global_step=24300, grad_norm=1.6367318630218506, loss=1.7545219659805298
I0129 21:57:16.757665 139656297445120 logging_writer.py:48] [24400] global_step=24400, grad_norm=1.850702166557312, loss=1.8081023693084717
I0129 21:57:50.881983 139655592802048 logging_writer.py:48] [24500] global_step=24500, grad_norm=1.6737695932388306, loss=1.7451865673065186
I0129 21:58:24.988654 139656297445120 logging_writer.py:48] [24600] global_step=24600, grad_norm=1.9335291385650635, loss=1.7982642650604248
I0129 21:58:59.113538 139655592802048 logging_writer.py:48] [24700] global_step=24700, grad_norm=1.6480016708374023, loss=1.7273755073547363
I0129 21:59:33.244114 139656297445120 logging_writer.py:48] [24800] global_step=24800, grad_norm=1.5846201181411743, loss=1.7163392305374146
I0129 22:00:07.366533 139655592802048 logging_writer.py:48] [24900] global_step=24900, grad_norm=1.6143124103546143, loss=1.877902626991272
I0129 22:00:41.572320 139656297445120 logging_writer.py:48] [25000] global_step=25000, grad_norm=1.775582194328308, loss=1.742870807647705
I0129 22:01:15.700960 139655592802048 logging_writer.py:48] [25100] global_step=25100, grad_norm=1.6876801252365112, loss=1.7589361667633057
I0129 22:01:49.792940 139656297445120 logging_writer.py:48] [25200] global_step=25200, grad_norm=1.7152752876281738, loss=1.6878353357315063
I0129 22:02:23.908561 139655592802048 logging_writer.py:48] [25300] global_step=25300, grad_norm=1.6510802507400513, loss=1.7358946800231934
I0129 22:02:58.016396 139656297445120 logging_writer.py:48] [25400] global_step=25400, grad_norm=1.9519517421722412, loss=1.8908973932266235
I0129 22:02:58.503429 139822745589568 spec.py:321] Evaluating on the training split.
I0129 22:03:04.911297 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 22:03:13.554666 139822745589568 spec.py:349] Evaluating on the test split.
I0129 22:03:16.169877 139822745589568 submission_runner.py:408] Time since start: 9023.16s, 	Step: 25403, 	{'train/accuracy': 0.6804647445678711, 'train/loss': 1.2763067483901978, 'validation/accuracy': 0.6107999682426453, 'validation/loss': 1.6338390111923218, 'validation/num_examples': 50000, 'test/accuracy': 0.47310003638267517, 'test/loss': 2.415712833404541, 'test/num_examples': 10000, 'score': 8706.593649148941, 'total_duration': 9023.160031318665, 'accumulated_submission_time': 8706.593649148941, 'accumulated_eval_time': 315.21845388412476, 'accumulated_logging_time': 0.4857900142669678}
I0129 22:03:16.191228 139656666543872 logging_writer.py:48] [25403] accumulated_eval_time=315.218454, accumulated_logging_time=0.485790, accumulated_submission_time=8706.593649, global_step=25403, preemption_count=0, score=8706.593649, test/accuracy=0.473100, test/loss=2.415713, test/num_examples=10000, total_duration=9023.160031, train/accuracy=0.680465, train/loss=1.276307, validation/accuracy=0.610800, validation/loss=1.633839, validation/num_examples=50000
I0129 22:03:49.602295 139656834316032 logging_writer.py:48] [25500] global_step=25500, grad_norm=1.9495429992675781, loss=1.7083656787872314
I0129 22:04:23.695616 139656666543872 logging_writer.py:48] [25600] global_step=25600, grad_norm=1.7530614137649536, loss=1.8765467405319214
I0129 22:04:57.809302 139656834316032 logging_writer.py:48] [25700] global_step=25700, grad_norm=1.628597617149353, loss=1.805224895477295
I0129 22:05:31.946110 139656666543872 logging_writer.py:48] [25800] global_step=25800, grad_norm=1.6488885879516602, loss=1.740851640701294
I0129 22:06:06.065113 139656834316032 logging_writer.py:48] [25900] global_step=25900, grad_norm=1.6444852352142334, loss=1.9184266328811646
I0129 22:06:40.151596 139656666543872 logging_writer.py:48] [26000] global_step=26000, grad_norm=1.8202643394470215, loss=1.6872303485870361
I0129 22:07:14.373940 139656834316032 logging_writer.py:48] [26100] global_step=26100, grad_norm=1.5887609720230103, loss=1.7431756258010864
I0129 22:07:48.482953 139656666543872 logging_writer.py:48] [26200] global_step=26200, grad_norm=1.7880336046218872, loss=1.823591947555542
I0129 22:08:22.614602 139656834316032 logging_writer.py:48] [26300] global_step=26300, grad_norm=1.7793534994125366, loss=1.7362607717514038
I0129 22:08:56.743023 139656666543872 logging_writer.py:48] [26400] global_step=26400, grad_norm=1.797046422958374, loss=1.8025094270706177
I0129 22:09:30.863917 139656834316032 logging_writer.py:48] [26500] global_step=26500, grad_norm=1.6680856943130493, loss=1.7193535566329956
I0129 22:10:04.995495 139656666543872 logging_writer.py:48] [26600] global_step=26600, grad_norm=1.6990543603897095, loss=1.6625189781188965
I0129 22:10:39.089763 139656834316032 logging_writer.py:48] [26700] global_step=26700, grad_norm=1.7659165859222412, loss=1.6385271549224854
I0129 22:11:13.190497 139656666543872 logging_writer.py:48] [26800] global_step=26800, grad_norm=1.8167694807052612, loss=1.8118481636047363
I0129 22:11:46.401529 139822745589568 spec.py:321] Evaluating on the training split.
I0129 22:11:52.641165 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 22:12:01.200883 139822745589568 spec.py:349] Evaluating on the test split.
I0129 22:12:03.946932 139822745589568 submission_runner.py:408] Time since start: 9550.94s, 	Step: 26899, 	{'train/accuracy': 0.6908880472183228, 'train/loss': 1.2166250944137573, 'validation/accuracy': 0.6270999908447266, 'validation/loss': 1.541603446006775, 'validation/num_examples': 50000, 'test/accuracy': 0.4951000213623047, 'test/loss': 2.3069241046905518, 'test/num_examples': 10000, 'score': 9216.743519544601, 'total_duration': 9550.937099695206, 'accumulated_submission_time': 9216.743519544601, 'accumulated_eval_time': 332.76383209228516, 'accumulated_logging_time': 0.5164616107940674}
I0129 22:12:03.969093 139655576016640 logging_writer.py:48] [26899] accumulated_eval_time=332.763832, accumulated_logging_time=0.516462, accumulated_submission_time=9216.743520, global_step=26899, preemption_count=0, score=9216.743520, test/accuracy=0.495100, test/loss=2.306924, test/num_examples=10000, total_duration=9550.937100, train/accuracy=0.690888, train/loss=1.216625, validation/accuracy=0.627100, validation/loss=1.541603, validation/num_examples=50000
I0129 22:12:04.655652 139655584409344 logging_writer.py:48] [26900] global_step=26900, grad_norm=1.724843144416809, loss=1.7425168752670288
I0129 22:12:38.715558 139655576016640 logging_writer.py:48] [27000] global_step=27000, grad_norm=1.8012964725494385, loss=1.9380286931991577
I0129 22:13:12.800248 139655584409344 logging_writer.py:48] [27100] global_step=27100, grad_norm=1.7101103067398071, loss=1.8092637062072754
I0129 22:13:46.945513 139655576016640 logging_writer.py:48] [27200] global_step=27200, grad_norm=1.6603796482086182, loss=1.83292555809021
I0129 22:14:21.053716 139655584409344 logging_writer.py:48] [27300] global_step=27300, grad_norm=1.6481903791427612, loss=1.6826931238174438
I0129 22:14:55.152072 139655576016640 logging_writer.py:48] [27400] global_step=27400, grad_norm=1.683725118637085, loss=1.650954246520996
I0129 22:15:29.243357 139655584409344 logging_writer.py:48] [27500] global_step=27500, grad_norm=1.5263534784317017, loss=1.6669472455978394
I0129 22:16:03.348767 139655576016640 logging_writer.py:48] [27600] global_step=27600, grad_norm=1.7525454759597778, loss=1.7337000370025635
I0129 22:16:37.443547 139655584409344 logging_writer.py:48] [27700] global_step=27700, grad_norm=1.6168417930603027, loss=1.6912683248519897
I0129 22:17:11.542257 139655576016640 logging_writer.py:48] [27800] global_step=27800, grad_norm=1.7781957387924194, loss=1.7938627004623413
I0129 22:17:45.644615 139655584409344 logging_writer.py:48] [27900] global_step=27900, grad_norm=1.9265968799591064, loss=1.829712152481079
I0129 22:18:19.746609 139655576016640 logging_writer.py:48] [28000] global_step=28000, grad_norm=1.8271559476852417, loss=1.6625617742538452
I0129 22:18:53.876260 139655584409344 logging_writer.py:48] [28100] global_step=28100, grad_norm=1.7730778455734253, loss=1.7345095872879028
I0129 22:19:27.964954 139655576016640 logging_writer.py:48] [28200] global_step=28200, grad_norm=1.526523232460022, loss=1.698650598526001
I0129 22:20:02.065094 139655584409344 logging_writer.py:48] [28300] global_step=28300, grad_norm=1.944748044013977, loss=1.643746256828308
I0129 22:20:34.021493 139822745589568 spec.py:321] Evaluating on the training split.
I0129 22:20:40.278901 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 22:20:48.725620 139822745589568 spec.py:349] Evaluating on the test split.
I0129 22:20:51.377684 139822745589568 submission_runner.py:408] Time since start: 10078.37s, 	Step: 28395, 	{'train/accuracy': 0.6801259517669678, 'train/loss': 1.2542836666107178, 'validation/accuracy': 0.6233199834823608, 'validation/loss': 1.5583561658859253, 'validation/num_examples': 50000, 'test/accuracy': 0.4909000098705292, 'test/loss': 2.328392505645752, 'test/num_examples': 10000, 'score': 9726.73595571518, 'total_duration': 10078.36783695221, 'accumulated_submission_time': 9726.73595571518, 'accumulated_eval_time': 350.1199746131897, 'accumulated_logging_time': 0.5478644371032715}
I0129 22:20:51.401916 139655584409344 logging_writer.py:48] [28395] accumulated_eval_time=350.119975, accumulated_logging_time=0.547864, accumulated_submission_time=9726.735956, global_step=28395, preemption_count=0, score=9726.735956, test/accuracy=0.490900, test/loss=2.328393, test/num_examples=10000, total_duration=10078.367837, train/accuracy=0.680126, train/loss=1.254284, validation/accuracy=0.623320, validation/loss=1.558356, validation/num_examples=50000
I0129 22:20:53.453797 139656658151168 logging_writer.py:48] [28400] global_step=28400, grad_norm=1.7565914392471313, loss=1.7519904375076294
I0129 22:21:27.515637 139655584409344 logging_writer.py:48] [28500] global_step=28500, grad_norm=1.920164704322815, loss=1.6965610980987549
I0129 22:22:01.575243 139656658151168 logging_writer.py:48] [28600] global_step=28600, grad_norm=1.6874192953109741, loss=1.6970418691635132
I0129 22:22:35.651074 139655584409344 logging_writer.py:48] [28700] global_step=28700, grad_norm=1.63886559009552, loss=1.7434245347976685
I0129 22:23:09.745530 139656658151168 logging_writer.py:48] [28800] global_step=28800, grad_norm=1.4769542217254639, loss=1.6331692934036255
I0129 22:23:43.867440 139655584409344 logging_writer.py:48] [28900] global_step=28900, grad_norm=1.659390926361084, loss=1.8295177221298218
I0129 22:24:17.958218 139656658151168 logging_writer.py:48] [29000] global_step=29000, grad_norm=1.939400315284729, loss=1.8048211336135864
I0129 22:24:52.082839 139655584409344 logging_writer.py:48] [29100] global_step=29100, grad_norm=1.7423995733261108, loss=1.5640968084335327
I0129 22:25:26.200958 139656658151168 logging_writer.py:48] [29200] global_step=29200, grad_norm=1.6415040493011475, loss=1.6874161958694458
I0129 22:26:00.337214 139655584409344 logging_writer.py:48] [29300] global_step=29300, grad_norm=1.6542894840240479, loss=1.7768290042877197
I0129 22:26:34.438615 139656658151168 logging_writer.py:48] [29400] global_step=29400, grad_norm=1.6924312114715576, loss=1.7307522296905518
I0129 22:27:08.646770 139655584409344 logging_writer.py:48] [29500] global_step=29500, grad_norm=1.659389615058899, loss=1.814963459968567
I0129 22:27:42.764498 139656658151168 logging_writer.py:48] [29600] global_step=29600, grad_norm=1.60395085811615, loss=1.7226693630218506
I0129 22:28:16.880255 139655584409344 logging_writer.py:48] [29700] global_step=29700, grad_norm=1.7026333808898926, loss=1.6002215147018433
I0129 22:28:51.009851 139656658151168 logging_writer.py:48] [29800] global_step=29800, grad_norm=1.715345859527588, loss=1.669095754623413
I0129 22:29:21.506745 139822745589568 spec.py:321] Evaluating on the training split.
I0129 22:29:27.761121 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 22:29:36.332431 139822745589568 spec.py:349] Evaluating on the test split.
I0129 22:29:38.965175 139822745589568 submission_runner.py:408] Time since start: 10605.96s, 	Step: 29891, 	{'train/accuracy': 0.6824178695678711, 'train/loss': 1.2555991411209106, 'validation/accuracy': 0.625499963760376, 'validation/loss': 1.545114517211914, 'validation/num_examples': 50000, 'test/accuracy': 0.49800002574920654, 'test/loss': 2.307891368865967, 'test/num_examples': 10000, 'score': 10236.778829574585, 'total_duration': 10605.955340862274, 'accumulated_submission_time': 10236.778829574585, 'accumulated_eval_time': 367.57837295532227, 'accumulated_logging_time': 0.5819680690765381}
I0129 22:29:38.989681 139655584409344 logging_writer.py:48] [29891] accumulated_eval_time=367.578373, accumulated_logging_time=0.581968, accumulated_submission_time=10236.778830, global_step=29891, preemption_count=0, score=10236.778830, test/accuracy=0.498000, test/loss=2.307891, test/num_examples=10000, total_duration=10605.955341, train/accuracy=0.682418, train/loss=1.255599, validation/accuracy=0.625500, validation/loss=1.545115, validation/num_examples=50000
I0129 22:29:42.412785 139655592802048 logging_writer.py:48] [29900] global_step=29900, grad_norm=1.6917095184326172, loss=1.7751129865646362
I0129 22:30:16.456325 139655584409344 logging_writer.py:48] [30000] global_step=30000, grad_norm=1.738887906074524, loss=1.7065143585205078
I0129 22:30:50.527786 139655592802048 logging_writer.py:48] [30100] global_step=30100, grad_norm=1.5963358879089355, loss=1.7150851488113403
I0129 22:31:24.643523 139655584409344 logging_writer.py:48] [30200] global_step=30200, grad_norm=1.72345769405365, loss=1.7403655052185059
I0129 22:31:58.787140 139655592802048 logging_writer.py:48] [30300] global_step=30300, grad_norm=1.7330520153045654, loss=1.607486367225647
I0129 22:32:32.927134 139655584409344 logging_writer.py:48] [30400] global_step=30400, grad_norm=1.5227773189544678, loss=1.7268484830856323
I0129 22:33:07.046144 139655592802048 logging_writer.py:48] [30500] global_step=30500, grad_norm=1.8046122789382935, loss=1.7551347017288208
I0129 22:33:41.252936 139655584409344 logging_writer.py:48] [30600] global_step=30600, grad_norm=2.3069372177124023, loss=1.7977014780044556
I0129 22:34:15.396652 139655592802048 logging_writer.py:48] [30700] global_step=30700, grad_norm=1.6729862689971924, loss=1.7960222959518433
I0129 22:34:49.523710 139655584409344 logging_writer.py:48] [30800] global_step=30800, grad_norm=1.892866611480713, loss=1.6880121231079102
I0129 22:35:23.673083 139655592802048 logging_writer.py:48] [30900] global_step=30900, grad_norm=1.785298228263855, loss=1.606825351715088
I0129 22:35:57.807139 139655584409344 logging_writer.py:48] [31000] global_step=31000, grad_norm=1.6662766933441162, loss=1.6765693426132202
I0129 22:36:31.904201 139655592802048 logging_writer.py:48] [31100] global_step=31100, grad_norm=1.8385951519012451, loss=1.6857664585113525
I0129 22:37:06.032557 139655584409344 logging_writer.py:48] [31200] global_step=31200, grad_norm=1.8403210639953613, loss=1.6688449382781982
I0129 22:37:40.157956 139655592802048 logging_writer.py:48] [31300] global_step=31300, grad_norm=1.8421971797943115, loss=1.7585076093673706
I0129 22:38:09.292723 139822745589568 spec.py:321] Evaluating on the training split.
I0129 22:38:15.522551 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 22:38:24.150642 139822745589568 spec.py:349] Evaluating on the test split.
I0129 22:38:26.751507 139822745589568 submission_runner.py:408] Time since start: 11133.74s, 	Step: 31387, 	{'train/accuracy': 0.6882373690605164, 'train/loss': 1.2267718315124512, 'validation/accuracy': 0.6324999928474426, 'validation/loss': 1.5249176025390625, 'validation/num_examples': 50000, 'test/accuracy': 0.5041000247001648, 'test/loss': 2.2895352840423584, 'test/num_examples': 10000, 'score': 10747.021076440811, 'total_duration': 11133.741675138474, 'accumulated_submission_time': 10747.021076440811, 'accumulated_eval_time': 385.03712701797485, 'accumulated_logging_time': 0.6160974502563477}
I0129 22:38:26.776774 139656658151168 logging_writer.py:48] [31387] accumulated_eval_time=385.037127, accumulated_logging_time=0.616097, accumulated_submission_time=10747.021076, global_step=31387, preemption_count=0, score=10747.021076, test/accuracy=0.504100, test/loss=2.289535, test/num_examples=10000, total_duration=11133.741675, train/accuracy=0.688237, train/loss=1.226772, validation/accuracy=0.632500, validation/loss=1.524918, validation/num_examples=50000
I0129 22:38:31.547991 139656666543872 logging_writer.py:48] [31400] global_step=31400, grad_norm=1.7246079444885254, loss=1.8039252758026123
I0129 22:39:05.622864 139656658151168 logging_writer.py:48] [31500] global_step=31500, grad_norm=1.959265112876892, loss=1.8417216539382935
I0129 22:39:39.722290 139656666543872 logging_writer.py:48] [31600] global_step=31600, grad_norm=1.7059251070022583, loss=1.7733653783798218
I0129 22:40:13.892961 139656658151168 logging_writer.py:48] [31700] global_step=31700, grad_norm=1.793554425239563, loss=1.7073819637298584
I0129 22:40:48.017519 139656666543872 logging_writer.py:48] [31800] global_step=31800, grad_norm=1.7596651315689087, loss=1.753469467163086
I0129 22:41:22.144275 139656658151168 logging_writer.py:48] [31900] global_step=31900, grad_norm=1.7666549682617188, loss=1.7119098901748657
I0129 22:41:56.235465 139656666543872 logging_writer.py:48] [32000] global_step=32000, grad_norm=1.672589659690857, loss=1.7493656873703003
I0129 22:42:30.317960 139656658151168 logging_writer.py:48] [32100] global_step=32100, grad_norm=1.9151663780212402, loss=1.7087769508361816
I0129 22:43:04.413960 139656666543872 logging_writer.py:48] [32200] global_step=32200, grad_norm=1.7526187896728516, loss=1.7177568674087524
I0129 22:43:38.490863 139656658151168 logging_writer.py:48] [32300] global_step=32300, grad_norm=1.7457212209701538, loss=1.7234302759170532
I0129 22:44:12.605556 139656666543872 logging_writer.py:48] [32400] global_step=32400, grad_norm=1.8827440738677979, loss=1.7487176656723022
I0129 22:44:46.707575 139656658151168 logging_writer.py:48] [32500] global_step=32500, grad_norm=1.9059425592422485, loss=1.7170941829681396
I0129 22:45:20.842318 139656666543872 logging_writer.py:48] [32600] global_step=32600, grad_norm=1.6960034370422363, loss=1.648431420326233
I0129 22:45:54.962211 139656658151168 logging_writer.py:48] [32700] global_step=32700, grad_norm=1.7978618144989014, loss=1.693651556968689
I0129 22:46:29.074159 139656666543872 logging_writer.py:48] [32800] global_step=32800, grad_norm=1.7193982601165771, loss=1.6533244848251343
I0129 22:46:56.933068 139822745589568 spec.py:321] Evaluating on the training split.
I0129 22:47:03.293086 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 22:47:11.976214 139822745589568 spec.py:349] Evaluating on the test split.
I0129 22:47:14.618402 139822745589568 submission_runner.py:408] Time since start: 11661.61s, 	Step: 32883, 	{'train/accuracy': 0.6759008169174194, 'train/loss': 1.282272458076477, 'validation/accuracy': 0.6207599639892578, 'validation/loss': 1.5945985317230225, 'validation/num_examples': 50000, 'test/accuracy': 0.48730000853538513, 'test/loss': 2.34653377532959, 'test/num_examples': 10000, 'score': 11257.115474939346, 'total_duration': 11661.608564853668, 'accumulated_submission_time': 11257.115474939346, 'accumulated_eval_time': 402.7224214076996, 'accumulated_logging_time': 0.6510787010192871}
I0129 22:47:14.645192 139656834316032 logging_writer.py:48] [32883] accumulated_eval_time=402.722421, accumulated_logging_time=0.651079, accumulated_submission_time=11257.115475, global_step=32883, preemption_count=0, score=11257.115475, test/accuracy=0.487300, test/loss=2.346534, test/num_examples=10000, total_duration=11661.608565, train/accuracy=0.675901, train/loss=1.282272, validation/accuracy=0.620760, validation/loss=1.594599, validation/num_examples=50000
I0129 22:47:20.794912 139658730145536 logging_writer.py:48] [32900] global_step=32900, grad_norm=1.6405036449432373, loss=1.7124109268188477
I0129 22:47:54.895783 139656834316032 logging_writer.py:48] [33000] global_step=33000, grad_norm=1.7486293315887451, loss=1.7390100955963135
I0129 22:48:28.990492 139658730145536 logging_writer.py:48] [33100] global_step=33100, grad_norm=1.6724181175231934, loss=1.598934292793274
I0129 22:49:03.107907 139656834316032 logging_writer.py:48] [33200] global_step=33200, grad_norm=1.6153119802474976, loss=1.6517471075057983
I0129 22:49:37.230662 139658730145536 logging_writer.py:48] [33300] global_step=33300, grad_norm=1.8131868839263916, loss=1.733740210533142
I0129 22:50:11.357437 139656834316032 logging_writer.py:48] [33400] global_step=33400, grad_norm=1.7831016778945923, loss=1.6441068649291992
I0129 22:50:45.483205 139658730145536 logging_writer.py:48] [33500] global_step=33500, grad_norm=1.7694218158721924, loss=1.793043851852417
I0129 22:51:19.601419 139656834316032 logging_writer.py:48] [33600] global_step=33600, grad_norm=1.7965261936187744, loss=1.7952167987823486
I0129 22:51:53.686358 139658730145536 logging_writer.py:48] [33700] global_step=33700, grad_norm=1.8489437103271484, loss=1.7342705726623535
I0129 22:52:27.811883 139656834316032 logging_writer.py:48] [33800] global_step=33800, grad_norm=1.738450050354004, loss=1.7496347427368164
I0129 22:53:01.910506 139658730145536 logging_writer.py:48] [33900] global_step=33900, grad_norm=2.0212666988372803, loss=1.8262053728103638
I0129 22:53:36.143125 139656834316032 logging_writer.py:48] [34000] global_step=34000, grad_norm=1.6155619621276855, loss=1.6860134601593018
I0129 22:54:10.252434 139658730145536 logging_writer.py:48] [34100] global_step=34100, grad_norm=1.836719274520874, loss=1.8046841621398926
I0129 22:54:44.352312 139656834316032 logging_writer.py:48] [34200] global_step=34200, grad_norm=2.0732569694519043, loss=1.7019977569580078
I0129 22:55:18.458925 139658730145536 logging_writer.py:48] [34300] global_step=34300, grad_norm=1.8046183586120605, loss=1.7141854763031006
I0129 22:55:44.855235 139822745589568 spec.py:321] Evaluating on the training split.
I0129 22:55:51.138044 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 22:55:59.727355 139822745589568 spec.py:349] Evaluating on the test split.
I0129 22:56:02.360970 139822745589568 submission_runner.py:408] Time since start: 12189.35s, 	Step: 34379, 	{'train/accuracy': 0.7140066623687744, 'train/loss': 1.1095143556594849, 'validation/accuracy': 0.6321399807929993, 'validation/loss': 1.5328267812728882, 'validation/num_examples': 50000, 'test/accuracy': 0.4999000132083893, 'test/loss': 2.2806482315063477, 'test/num_examples': 10000, 'score': 11767.26364517212, 'total_duration': 12189.351138353348, 'accumulated_submission_time': 11767.26364517212, 'accumulated_eval_time': 420.22812843322754, 'accumulated_logging_time': 0.6878213882446289}
I0129 22:56:02.383782 139656297445120 logging_writer.py:48] [34379] accumulated_eval_time=420.228128, accumulated_logging_time=0.687821, accumulated_submission_time=11767.263645, global_step=34379, preemption_count=0, score=11767.263645, test/accuracy=0.499900, test/loss=2.280648, test/num_examples=10000, total_duration=12189.351138, train/accuracy=0.714007, train/loss=1.109514, validation/accuracy=0.632140, validation/loss=1.532827, validation/num_examples=50000
I0129 22:56:09.876573 139656649758464 logging_writer.py:48] [34400] global_step=34400, grad_norm=1.7382254600524902, loss=1.6332991123199463
I0129 22:56:43.946661 139656297445120 logging_writer.py:48] [34500] global_step=34500, grad_norm=1.8084394931793213, loss=1.6990764141082764
I0129 22:57:18.022935 139656649758464 logging_writer.py:48] [34600] global_step=34600, grad_norm=1.8389521837234497, loss=1.6577787399291992
I0129 22:57:52.133927 139656297445120 logging_writer.py:48] [34700] global_step=34700, grad_norm=1.7117573022842407, loss=1.7325334548950195
I0129 22:58:26.196981 139656649758464 logging_writer.py:48] [34800] global_step=34800, grad_norm=1.896227240562439, loss=1.7950526475906372
I0129 22:59:00.298971 139656297445120 logging_writer.py:48] [34900] global_step=34900, grad_norm=1.8346806764602661, loss=1.7332310676574707
I0129 22:59:34.390851 139656649758464 logging_writer.py:48] [35000] global_step=35000, grad_norm=1.757548451423645, loss=1.595804214477539
I0129 23:00:08.539487 139656297445120 logging_writer.py:48] [35100] global_step=35100, grad_norm=1.7092398405075073, loss=1.6505846977233887
I0129 23:00:42.656773 139656649758464 logging_writer.py:48] [35200] global_step=35200, grad_norm=1.6610761880874634, loss=1.6759626865386963
I0129 23:01:16.767215 139656297445120 logging_writer.py:48] [35300] global_step=35300, grad_norm=1.6402966976165771, loss=1.6478954553604126
I0129 23:01:50.862506 139656649758464 logging_writer.py:48] [35400] global_step=35400, grad_norm=1.7799317836761475, loss=1.6632424592971802
I0129 23:02:24.987363 139656297445120 logging_writer.py:48] [35500] global_step=35500, grad_norm=1.7706115245819092, loss=1.666313886642456
I0129 23:02:59.094962 139656649758464 logging_writer.py:48] [35600] global_step=35600, grad_norm=1.747071385383606, loss=1.74663245677948
I0129 23:03:33.215784 139656297445120 logging_writer.py:48] [35700] global_step=35700, grad_norm=1.831122636795044, loss=1.7032132148742676
I0129 23:04:07.329904 139656649758464 logging_writer.py:48] [35800] global_step=35800, grad_norm=1.9455772638320923, loss=1.7428497076034546
I0129 23:04:32.380043 139822745589568 spec.py:321] Evaluating on the training split.
I0129 23:04:38.713009 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 23:04:47.360640 139822745589568 spec.py:349] Evaluating on the test split.
I0129 23:04:49.982965 139822745589568 submission_runner.py:408] Time since start: 12716.97s, 	Step: 35875, 	{'train/accuracy': 0.6957509517669678, 'train/loss': 1.1831389665603638, 'validation/accuracy': 0.6268399953842163, 'validation/loss': 1.5437438488006592, 'validation/num_examples': 50000, 'test/accuracy': 0.4993000328540802, 'test/loss': 2.2765913009643555, 'test/num_examples': 10000, 'score': 12277.20189833641, 'total_duration': 12716.973129034042, 'accumulated_submission_time': 12277.20189833641, 'accumulated_eval_time': 437.83101439476013, 'accumulated_logging_time': 0.7197163105010986}
I0129 23:04:50.006939 139655576016640 logging_writer.py:48] [35875] accumulated_eval_time=437.831014, accumulated_logging_time=0.719716, accumulated_submission_time=12277.201898, global_step=35875, preemption_count=0, score=12277.201898, test/accuracy=0.499300, test/loss=2.276591, test/num_examples=10000, total_duration=12716.973129, train/accuracy=0.695751, train/loss=1.183139, validation/accuracy=0.626840, validation/loss=1.543744, validation/num_examples=50000
I0129 23:04:58.887197 139655584409344 logging_writer.py:48] [35900] global_step=35900, grad_norm=1.961497187614441, loss=1.6803139448165894
I0129 23:05:32.938242 139655576016640 logging_writer.py:48] [36000] global_step=36000, grad_norm=1.8648362159729004, loss=1.7097208499908447
I0129 23:06:07.052648 139655584409344 logging_writer.py:48] [36100] global_step=36100, grad_norm=1.7608777284622192, loss=1.716989278793335
I0129 23:06:41.237561 139655576016640 logging_writer.py:48] [36200] global_step=36200, grad_norm=1.535033941268921, loss=1.5046966075897217
I0129 23:07:15.370410 139655584409344 logging_writer.py:48] [36300] global_step=36300, grad_norm=1.7618600130081177, loss=1.586841344833374
I0129 23:07:49.479969 139655576016640 logging_writer.py:48] [36400] global_step=36400, grad_norm=1.7107758522033691, loss=1.6656317710876465
I0129 23:08:23.602818 139655584409344 logging_writer.py:48] [36500] global_step=36500, grad_norm=1.8678359985351562, loss=1.7816169261932373
I0129 23:08:57.700268 139655576016640 logging_writer.py:48] [36600] global_step=36600, grad_norm=1.8604481220245361, loss=1.8330059051513672
I0129 23:09:31.813510 139655584409344 logging_writer.py:48] [36700] global_step=36700, grad_norm=1.7815531492233276, loss=1.570255160331726
I0129 23:10:05.906298 139655576016640 logging_writer.py:48] [36800] global_step=36800, grad_norm=1.7258052825927734, loss=1.4864535331726074
I0129 23:10:40.004076 139655584409344 logging_writer.py:48] [36900] global_step=36900, grad_norm=1.7853379249572754, loss=1.6213839054107666
I0129 23:11:14.090633 139655576016640 logging_writer.py:48] [37000] global_step=37000, grad_norm=1.7954752445220947, loss=1.694201111793518
I0129 23:11:48.193264 139655584409344 logging_writer.py:48] [37100] global_step=37100, grad_norm=2.031621217727661, loss=1.706398367881775
I0129 23:12:22.304337 139655576016640 logging_writer.py:48] [37200] global_step=37200, grad_norm=1.6704689264297485, loss=1.5900942087173462
I0129 23:12:56.369509 139655584409344 logging_writer.py:48] [37300] global_step=37300, grad_norm=1.8681919574737549, loss=1.7633278369903564
I0129 23:13:20.089525 139822745589568 spec.py:321] Evaluating on the training split.
I0129 23:13:26.301491 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 23:13:34.735447 139822745589568 spec.py:349] Evaluating on the test split.
I0129 23:13:37.369555 139822745589568 submission_runner.py:408] Time since start: 13244.36s, 	Step: 37371, 	{'train/accuracy': 0.7004544138908386, 'train/loss': 1.1742907762527466, 'validation/accuracy': 0.6349799633026123, 'validation/loss': 1.5201865434646606, 'validation/num_examples': 50000, 'test/accuracy': 0.5092000365257263, 'test/loss': 2.2315361499786377, 'test/num_examples': 10000, 'score': 12787.222305297852, 'total_duration': 13244.35971903801, 'accumulated_submission_time': 12787.222305297852, 'accumulated_eval_time': 455.1110055446625, 'accumulated_logging_time': 0.7531900405883789}
I0129 23:13:37.395593 139656666543872 logging_writer.py:48] [37371] accumulated_eval_time=455.111006, accumulated_logging_time=0.753190, accumulated_submission_time=12787.222305, global_step=37371, preemption_count=0, score=12787.222305, test/accuracy=0.509200, test/loss=2.231536, test/num_examples=10000, total_duration=13244.359719, train/accuracy=0.700454, train/loss=1.174291, validation/accuracy=0.634980, validation/loss=1.520187, validation/num_examples=50000
I0129 23:13:47.615031 139656834316032 logging_writer.py:48] [37400] global_step=37400, grad_norm=2.021186113357544, loss=1.6216402053833008
I0129 23:14:21.667217 139656666543872 logging_writer.py:48] [37500] global_step=37500, grad_norm=1.8424792289733887, loss=1.7696906328201294
I0129 23:14:55.736192 139656834316032 logging_writer.py:48] [37600] global_step=37600, grad_norm=1.632798433303833, loss=1.6400370597839355
I0129 23:15:29.816996 139656666543872 logging_writer.py:48] [37700] global_step=37700, grad_norm=1.8623521327972412, loss=1.771653413772583
I0129 23:16:03.922329 139656834316032 logging_writer.py:48] [37800] global_step=37800, grad_norm=1.9096636772155762, loss=1.6309605836868286
I0129 23:16:38.018332 139656666543872 logging_writer.py:48] [37900] global_step=37900, grad_norm=1.7118562459945679, loss=1.7167332172393799
I0129 23:17:12.134827 139656834316032 logging_writer.py:48] [38000] global_step=38000, grad_norm=2.0268547534942627, loss=1.6633596420288086
I0129 23:17:46.247776 139656666543872 logging_writer.py:48] [38100] global_step=38100, grad_norm=2.01507306098938, loss=1.6557695865631104
I0129 23:18:20.354820 139656834316032 logging_writer.py:48] [38200] global_step=38200, grad_norm=1.7405989170074463, loss=1.6854088306427002
I0129 23:18:54.444819 139656666543872 logging_writer.py:48] [38300] global_step=38300, grad_norm=1.7794321775436401, loss=1.6044825315475464
I0129 23:19:28.531206 139656834316032 logging_writer.py:48] [38400] global_step=38400, grad_norm=1.6350321769714355, loss=1.5725853443145752
I0129 23:20:02.683795 139656666543872 logging_writer.py:48] [38500] global_step=38500, grad_norm=1.9136404991149902, loss=1.6367254257202148
I0129 23:20:36.749786 139656834316032 logging_writer.py:48] [38600] global_step=38600, grad_norm=1.8291079998016357, loss=1.6474262475967407
I0129 23:21:10.814161 139656666543872 logging_writer.py:48] [38700] global_step=38700, grad_norm=1.768465280532837, loss=1.6616942882537842
I0129 23:21:44.895091 139656834316032 logging_writer.py:48] [38800] global_step=38800, grad_norm=1.64484441280365, loss=1.7122586965560913
I0129 23:22:07.540791 139822745589568 spec.py:321] Evaluating on the training split.
I0129 23:22:13.810129 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 23:22:22.306074 139822745589568 spec.py:349] Evaluating on the test split.
I0129 23:22:24.917445 139822745589568 submission_runner.py:408] Time since start: 13771.91s, 	Step: 38868, 	{'train/accuracy': 0.6949737071990967, 'train/loss': 1.1847723722457886, 'validation/accuracy': 0.6327599883079529, 'validation/loss': 1.518254041671753, 'validation/num_examples': 50000, 'test/accuracy': 0.5082000494003296, 'test/loss': 2.2616703510284424, 'test/num_examples': 10000, 'score': 13297.305636644363, 'total_duration': 13771.90760755539, 'accumulated_submission_time': 13297.305636644363, 'accumulated_eval_time': 472.48762464523315, 'accumulated_logging_time': 0.7889235019683838}
I0129 23:22:24.944279 139655592802048 logging_writer.py:48] [38868] accumulated_eval_time=472.487625, accumulated_logging_time=0.788924, accumulated_submission_time=13297.305637, global_step=38868, preemption_count=0, score=13297.305637, test/accuracy=0.508200, test/loss=2.261670, test/num_examples=10000, total_duration=13771.907608, train/accuracy=0.694974, train/loss=1.184772, validation/accuracy=0.632760, validation/loss=1.518254, validation/num_examples=50000
I0129 23:22:36.177826 139656297445120 logging_writer.py:48] [38900] global_step=38900, grad_norm=1.9500207901000977, loss=1.6199960708618164
I0129 23:23:10.230585 139655592802048 logging_writer.py:48] [39000] global_step=39000, grad_norm=1.7943451404571533, loss=1.671696662902832
I0129 23:23:44.308153 139656297445120 logging_writer.py:48] [39100] global_step=39100, grad_norm=1.7015233039855957, loss=1.5785130262374878
I0129 23:24:18.419016 139655592802048 logging_writer.py:48] [39200] global_step=39200, grad_norm=1.998785376548767, loss=1.6783511638641357
I0129 23:24:52.534855 139656297445120 logging_writer.py:48] [39300] global_step=39300, grad_norm=1.691923975944519, loss=1.7312352657318115
I0129 23:25:26.624130 139655592802048 logging_writer.py:48] [39400] global_step=39400, grad_norm=1.6490789651870728, loss=1.7414969205856323
I0129 23:26:00.736412 139656297445120 logging_writer.py:48] [39500] global_step=39500, grad_norm=1.7477935552597046, loss=1.5720722675323486
I0129 23:26:34.939803 139655592802048 logging_writer.py:48] [39600] global_step=39600, grad_norm=1.9897288084030151, loss=1.649022102355957
I0129 23:27:09.054145 139656297445120 logging_writer.py:48] [39700] global_step=39700, grad_norm=1.5645442008972168, loss=1.5959450006484985
I0129 23:27:43.159511 139655592802048 logging_writer.py:48] [39800] global_step=39800, grad_norm=1.645593523979187, loss=1.6608128547668457
I0129 23:28:17.271641 139656297445120 logging_writer.py:48] [39900] global_step=39900, grad_norm=1.8976452350616455, loss=1.6733518838882446
I0129 23:28:51.374939 139655592802048 logging_writer.py:48] [40000] global_step=40000, grad_norm=1.7541124820709229, loss=1.6242613792419434
I0129 23:29:25.487289 139656297445120 logging_writer.py:48] [40100] global_step=40100, grad_norm=1.867864966392517, loss=1.587590217590332
I0129 23:29:59.556356 139655592802048 logging_writer.py:48] [40200] global_step=40200, grad_norm=2.0685346126556396, loss=1.6567938327789307
I0129 23:30:33.652076 139656297445120 logging_writer.py:48] [40300] global_step=40300, grad_norm=1.9283833503723145, loss=1.6774805784225464
I0129 23:30:54.929238 139822745589568 spec.py:321] Evaluating on the training split.
I0129 23:31:01.286064 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 23:31:10.055287 139822745589568 spec.py:349] Evaluating on the test split.
I0129 23:31:12.715121 139822745589568 submission_runner.py:408] Time since start: 14299.71s, 	Step: 40364, 	{'train/accuracy': 0.6914859414100647, 'train/loss': 1.2236030101776123, 'validation/accuracy': 0.6318399906158447, 'validation/loss': 1.5228060483932495, 'validation/num_examples': 50000, 'test/accuracy': 0.49720001220703125, 'test/loss': 2.3025379180908203, 'test/num_examples': 10000, 'score': 13807.231940984726, 'total_duration': 14299.705271959305, 'accumulated_submission_time': 13807.231940984726, 'accumulated_eval_time': 490.2734615802765, 'accumulated_logging_time': 0.8251686096191406}
I0129 23:31:12.745806 139655576016640 logging_writer.py:48] [40364] accumulated_eval_time=490.273462, accumulated_logging_time=0.825169, accumulated_submission_time=13807.231941, global_step=40364, preemption_count=0, score=13807.231941, test/accuracy=0.497200, test/loss=2.302538, test/num_examples=10000, total_duration=14299.705272, train/accuracy=0.691486, train/loss=1.223603, validation/accuracy=0.631840, validation/loss=1.522806, validation/num_examples=50000
I0129 23:31:25.349539 139655584409344 logging_writer.py:48] [40400] global_step=40400, grad_norm=1.825215458869934, loss=1.6208086013793945
I0129 23:31:59.376232 139655576016640 logging_writer.py:48] [40500] global_step=40500, grad_norm=1.770035982131958, loss=1.6897871494293213
I0129 23:32:33.452989 139655584409344 logging_writer.py:48] [40600] global_step=40600, grad_norm=2.0121498107910156, loss=1.8012323379516602
I0129 23:33:07.612832 139655576016640 logging_writer.py:48] [40700] global_step=40700, grad_norm=1.8303579092025757, loss=1.6524698734283447
I0129 23:33:41.710493 139655584409344 logging_writer.py:48] [40800] global_step=40800, grad_norm=1.8690942525863647, loss=1.5520867109298706
I0129 23:34:15.822443 139655576016640 logging_writer.py:48] [40900] global_step=40900, grad_norm=1.996952772140503, loss=1.647148609161377
I0129 23:34:49.919707 139655584409344 logging_writer.py:48] [41000] global_step=41000, grad_norm=1.8438149690628052, loss=1.516271710395813
I0129 23:35:23.989711 139655576016640 logging_writer.py:48] [41100] global_step=41100, grad_norm=1.6820335388183594, loss=1.6201891899108887
I0129 23:35:58.095678 139655584409344 logging_writer.py:48] [41200] global_step=41200, grad_norm=1.8736929893493652, loss=1.6534441709518433
I0129 23:36:32.175770 139655576016640 logging_writer.py:48] [41300] global_step=41300, grad_norm=1.640354037284851, loss=1.7001585960388184
I0129 23:37:06.258925 139655584409344 logging_writer.py:48] [41400] global_step=41400, grad_norm=1.8796546459197998, loss=1.6139757633209229
I0129 23:37:40.319108 139655576016640 logging_writer.py:48] [41500] global_step=41500, grad_norm=1.826047420501709, loss=1.810342788696289
I0129 23:38:14.415863 139655584409344 logging_writer.py:48] [41600] global_step=41600, grad_norm=1.996914267539978, loss=1.7892398834228516
I0129 23:38:48.512482 139655576016640 logging_writer.py:48] [41700] global_step=41700, grad_norm=1.8499382734298706, loss=1.7588379383087158
I0129 23:39:22.665069 139655584409344 logging_writer.py:48] [41800] global_step=41800, grad_norm=1.6994085311889648, loss=1.6458429098129272
I0129 23:39:42.924620 139822745589568 spec.py:321] Evaluating on the training split.
I0129 23:39:49.129901 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 23:39:57.706197 139822745589568 spec.py:349] Evaluating on the test split.
I0129 23:40:00.325037 139822745589568 submission_runner.py:408] Time since start: 14827.32s, 	Step: 41861, 	{'train/accuracy': 0.6909677982330322, 'train/loss': 1.2019314765930176, 'validation/accuracy': 0.6317600011825562, 'validation/loss': 1.5205481052398682, 'validation/num_examples': 50000, 'test/accuracy': 0.5012000203132629, 'test/loss': 2.2280113697052, 'test/num_examples': 10000, 'score': 14317.350157022476, 'total_duration': 14827.315202951431, 'accumulated_submission_time': 14317.350157022476, 'accumulated_eval_time': 507.6738419532776, 'accumulated_logging_time': 0.8653068542480469}
I0129 23:40:00.354068 139655576016640 logging_writer.py:48] [41861] accumulated_eval_time=507.673842, accumulated_logging_time=0.865307, accumulated_submission_time=14317.350157, global_step=41861, preemption_count=0, score=14317.350157, test/accuracy=0.501200, test/loss=2.228011, test/num_examples=10000, total_duration=14827.315203, train/accuracy=0.690968, train/loss=1.201931, validation/accuracy=0.631760, validation/loss=1.520548, validation/num_examples=50000
I0129 23:40:13.971006 139656658151168 logging_writer.py:48] [41900] global_step=41900, grad_norm=1.8579702377319336, loss=1.7493442296981812
I0129 23:40:47.973944 139655576016640 logging_writer.py:48] [42000] global_step=42000, grad_norm=1.7846120595932007, loss=1.6046243906021118
I0129 23:41:22.048865 139656658151168 logging_writer.py:48] [42100] global_step=42100, grad_norm=1.8803631067276, loss=1.5492308139801025
I0129 23:41:56.137424 139655576016640 logging_writer.py:48] [42200] global_step=42200, grad_norm=2.054569959640503, loss=1.712929129600525
I0129 23:42:30.229244 139656658151168 logging_writer.py:48] [42300] global_step=42300, grad_norm=1.9837230443954468, loss=1.6238282918930054
I0129 23:43:04.338058 139655576016640 logging_writer.py:48] [42400] global_step=42400, grad_norm=1.8114548921585083, loss=1.7059686183929443
I0129 23:43:38.448104 139656658151168 logging_writer.py:48] [42500] global_step=42500, grad_norm=1.9268807172775269, loss=1.6566412448883057
I0129 23:44:12.537299 139655576016640 logging_writer.py:48] [42600] global_step=42600, grad_norm=1.866378903388977, loss=1.5797497034072876
I0129 23:44:46.625983 139656658151168 logging_writer.py:48] [42700] global_step=42700, grad_norm=1.8610599040985107, loss=1.6015650033950806
I0129 23:45:20.720185 139655576016640 logging_writer.py:48] [42800] global_step=42800, grad_norm=1.8292597532272339, loss=1.6406311988830566
I0129 23:45:54.808866 139656658151168 logging_writer.py:48] [42900] global_step=42900, grad_norm=1.8935527801513672, loss=1.7496299743652344
I0129 23:46:29.018445 139655576016640 logging_writer.py:48] [43000] global_step=43000, grad_norm=1.9615927934646606, loss=1.6417970657348633
I0129 23:47:03.085091 139656658151168 logging_writer.py:48] [43100] global_step=43100, grad_norm=1.8369940519332886, loss=1.7810547351837158
I0129 23:47:37.189552 139655576016640 logging_writer.py:48] [43200] global_step=43200, grad_norm=1.7666130065917969, loss=1.5797178745269775
I0129 23:48:11.263553 139656658151168 logging_writer.py:48] [43300] global_step=43300, grad_norm=1.952819585800171, loss=1.6125518083572388
I0129 23:48:30.487054 139822745589568 spec.py:321] Evaluating on the training split.
I0129 23:48:36.818029 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 23:48:45.439674 139822745589568 spec.py:349] Evaluating on the test split.
I0129 23:48:47.958360 139822745589568 submission_runner.py:408] Time since start: 15354.95s, 	Step: 43358, 	{'train/accuracy': 0.726980984210968, 'train/loss': 1.0523135662078857, 'validation/accuracy': 0.6317399740219116, 'validation/loss': 1.5127232074737549, 'validation/num_examples': 50000, 'test/accuracy': 0.5042999982833862, 'test/loss': 2.259047508239746, 'test/num_examples': 10000, 'score': 14827.423000097275, 'total_duration': 15354.948499202728, 'accumulated_submission_time': 14827.423000097275, 'accumulated_eval_time': 525.1450872421265, 'accumulated_logging_time': 0.903350830078125}
I0129 23:48:47.988025 139656649758464 logging_writer.py:48] [43358] accumulated_eval_time=525.145087, accumulated_logging_time=0.903351, accumulated_submission_time=14827.423000, global_step=43358, preemption_count=0, score=14827.423000, test/accuracy=0.504300, test/loss=2.259048, test/num_examples=10000, total_duration=15354.948499, train/accuracy=0.726981, train/loss=1.052314, validation/accuracy=0.631740, validation/loss=1.512723, validation/num_examples=50000
I0129 23:49:02.619364 139656666543872 logging_writer.py:48] [43400] global_step=43400, grad_norm=1.9705876111984253, loss=1.6888242959976196
I0129 23:49:36.678637 139656649758464 logging_writer.py:48] [43500] global_step=43500, grad_norm=1.7219089269638062, loss=1.5430889129638672
I0129 23:50:10.775708 139656666543872 logging_writer.py:48] [43600] global_step=43600, grad_norm=1.9206496477127075, loss=1.7425553798675537
I0129 23:50:44.861114 139656649758464 logging_writer.py:48] [43700] global_step=43700, grad_norm=1.6041094064712524, loss=1.5492210388183594
I0129 23:51:18.974519 139656666543872 logging_writer.py:48] [43800] global_step=43800, grad_norm=1.7383828163146973, loss=1.6839334964752197
I0129 23:51:53.083538 139656649758464 logging_writer.py:48] [43900] global_step=43900, grad_norm=1.7852892875671387, loss=1.567818522453308
I0129 23:52:27.184159 139656666543872 logging_writer.py:48] [44000] global_step=44000, grad_norm=1.8271985054016113, loss=1.7007343769073486
I0129 23:53:01.365705 139656649758464 logging_writer.py:48] [44100] global_step=44100, grad_norm=1.9994357824325562, loss=1.6414215564727783
I0129 23:53:35.463899 139656666543872 logging_writer.py:48] [44200] global_step=44200, grad_norm=1.9916372299194336, loss=1.7276924848556519
I0129 23:54:09.557175 139656649758464 logging_writer.py:48] [44300] global_step=44300, grad_norm=1.7819206714630127, loss=1.4735193252563477
I0129 23:54:43.670988 139656666543872 logging_writer.py:48] [44400] global_step=44400, grad_norm=1.7151998281478882, loss=1.5999866724014282
I0129 23:55:17.767137 139656649758464 logging_writer.py:48] [44500] global_step=44500, grad_norm=2.01138973236084, loss=1.6856669187545776
I0129 23:55:51.880461 139656666543872 logging_writer.py:48] [44600] global_step=44600, grad_norm=1.7267169952392578, loss=1.5949320793151855
I0129 23:56:25.972166 139656649758464 logging_writer.py:48] [44700] global_step=44700, grad_norm=1.8428877592086792, loss=1.6582138538360596
I0129 23:57:00.060831 139656666543872 logging_writer.py:48] [44800] global_step=44800, grad_norm=1.889560580253601, loss=1.618504524230957
I0129 23:57:18.280573 139822745589568 spec.py:321] Evaluating on the training split.
I0129 23:57:24.579803 139822745589568 spec.py:333] Evaluating on the validation split.
I0129 23:57:32.998266 139822745589568 spec.py:349] Evaluating on the test split.
I0129 23:57:35.616315 139822745589568 submission_runner.py:408] Time since start: 15882.61s, 	Step: 44855, 	{'train/accuracy': 0.7177534699440002, 'train/loss': 1.0975770950317383, 'validation/accuracy': 0.6431800127029419, 'validation/loss': 1.4693011045455933, 'validation/num_examples': 50000, 'test/accuracy': 0.5175000429153442, 'test/loss': 2.1700892448425293, 'test/num_examples': 10000, 'score': 15337.656279802322, 'total_duration': 15882.606467962265, 'accumulated_submission_time': 15337.656279802322, 'accumulated_eval_time': 542.4807982444763, 'accumulated_logging_time': 0.9419848918914795}
I0129 23:57:35.641435 139655576016640 logging_writer.py:48] [44855] accumulated_eval_time=542.480798, accumulated_logging_time=0.941985, accumulated_submission_time=15337.656280, global_step=44855, preemption_count=0, score=15337.656280, test/accuracy=0.517500, test/loss=2.170089, test/num_examples=10000, total_duration=15882.606468, train/accuracy=0.717753, train/loss=1.097577, validation/accuracy=0.643180, validation/loss=1.469301, validation/num_examples=50000
I0129 23:57:51.344404 139655584409344 logging_writer.py:48] [44900] global_step=44900, grad_norm=1.8317070007324219, loss=1.5903048515319824
I0129 23:58:25.410984 139655576016640 logging_writer.py:48] [45000] global_step=45000, grad_norm=1.8945990800857544, loss=1.5849957466125488
I0129 23:58:59.508875 139655584409344 logging_writer.py:48] [45100] global_step=45100, grad_norm=2.016657829284668, loss=1.6196050643920898
I0129 23:59:33.688137 139655576016640 logging_writer.py:48] [45200] global_step=45200, grad_norm=1.7800277471542358, loss=1.5854625701904297
I0130 00:00:07.806930 139655584409344 logging_writer.py:48] [45300] global_step=45300, grad_norm=1.8373897075653076, loss=1.7206157445907593
I0130 00:00:41.913956 139655576016640 logging_writer.py:48] [45400] global_step=45400, grad_norm=1.9008969068527222, loss=1.5766340494155884
I0130 00:01:16.033215 139655584409344 logging_writer.py:48] [45500] global_step=45500, grad_norm=1.743078589439392, loss=1.7101120948791504
I0130 00:01:50.141653 139655576016640 logging_writer.py:48] [45600] global_step=45600, grad_norm=1.759853482246399, loss=1.6738120317459106
I0130 00:02:24.254731 139655584409344 logging_writer.py:48] [45700] global_step=45700, grad_norm=1.93276846408844, loss=1.6433314085006714
I0130 00:02:58.389560 139655576016640 logging_writer.py:48] [45800] global_step=45800, grad_norm=2.066965103149414, loss=1.710550308227539
I0130 00:03:32.477413 139655584409344 logging_writer.py:48] [45900] global_step=45900, grad_norm=1.7632523775100708, loss=1.7059059143066406
I0130 00:04:06.584375 139655576016640 logging_writer.py:48] [46000] global_step=46000, grad_norm=1.8021844625473022, loss=1.6019409894943237
I0130 00:04:40.671084 139655584409344 logging_writer.py:48] [46100] global_step=46100, grad_norm=1.8953874111175537, loss=1.616504430770874
I0130 00:05:14.780062 139655576016640 logging_writer.py:48] [46200] global_step=46200, grad_norm=1.9078961610794067, loss=1.6730351448059082
I0130 00:05:48.896254 139655584409344 logging_writer.py:48] [46300] global_step=46300, grad_norm=1.8260215520858765, loss=1.5810457468032837
I0130 00:06:05.840922 139822745589568 spec.py:321] Evaluating on the training split.
I0130 00:06:12.045575 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 00:06:20.456214 139822745589568 spec.py:349] Evaluating on the test split.
I0130 00:06:22.993858 139822745589568 submission_runner.py:408] Time since start: 16409.98s, 	Step: 46351, 	{'train/accuracy': 0.7119937539100647, 'train/loss': 1.1093826293945312, 'validation/accuracy': 0.6420199871063232, 'validation/loss': 1.4766788482666016, 'validation/num_examples': 50000, 'test/accuracy': 0.5148000121116638, 'test/loss': 2.2351808547973633, 'test/num_examples': 10000, 'score': 15847.795320510864, 'total_duration': 16409.9840195179, 'accumulated_submission_time': 15847.795320510864, 'accumulated_eval_time': 559.6336979866028, 'accumulated_logging_time': 0.9760580062866211}
I0130 00:06:23.021291 139655584409344 logging_writer.py:48] [46351] accumulated_eval_time=559.633698, accumulated_logging_time=0.976058, accumulated_submission_time=15847.795321, global_step=46351, preemption_count=0, score=15847.795321, test/accuracy=0.514800, test/loss=2.235181, test/num_examples=10000, total_duration=16409.984020, train/accuracy=0.711994, train/loss=1.109383, validation/accuracy=0.642020, validation/loss=1.476679, validation/num_examples=50000
I0130 00:06:40.057276 139656666543872 logging_writer.py:48] [46400] global_step=46400, grad_norm=1.7798779010772705, loss=1.5489767789840698
I0130 00:07:14.120133 139655584409344 logging_writer.py:48] [46500] global_step=46500, grad_norm=1.7358521223068237, loss=1.5510022640228271
I0130 00:07:48.188169 139656666543872 logging_writer.py:48] [46600] global_step=46600, grad_norm=1.9191746711730957, loss=1.6152794361114502
I0130 00:08:22.267465 139655584409344 logging_writer.py:48] [46700] global_step=46700, grad_norm=1.8311649560928345, loss=1.6692105531692505
I0130 00:08:56.349050 139656666543872 logging_writer.py:48] [46800] global_step=46800, grad_norm=1.904461145401001, loss=1.572896122932434
I0130 00:09:30.458723 139655584409344 logging_writer.py:48] [46900] global_step=46900, grad_norm=1.756900668144226, loss=1.6029307842254639
I0130 00:10:04.581683 139656666543872 logging_writer.py:48] [47000] global_step=47000, grad_norm=2.4244184494018555, loss=1.6755149364471436
I0130 00:10:38.700576 139655584409344 logging_writer.py:48] [47100] global_step=47100, grad_norm=1.7038962841033936, loss=1.487661361694336
I0130 00:11:12.800434 139656666543872 logging_writer.py:48] [47200] global_step=47200, grad_norm=1.8898435831069946, loss=1.6156789064407349
I0130 00:11:46.906935 139655584409344 logging_writer.py:48] [47300] global_step=47300, grad_norm=1.8426852226257324, loss=1.6056602001190186
I0130 00:12:20.989218 139656666543872 logging_writer.py:48] [47400] global_step=47400, grad_norm=1.9392975568771362, loss=1.6148333549499512
I0130 00:12:55.175584 139655584409344 logging_writer.py:48] [47500] global_step=47500, grad_norm=1.834025263786316, loss=1.6484620571136475
I0130 00:13:29.278475 139656666543872 logging_writer.py:48] [47600] global_step=47600, grad_norm=1.887834906578064, loss=1.683424472808838
I0130 00:14:03.383727 139655584409344 logging_writer.py:48] [47700] global_step=47700, grad_norm=1.751939296722412, loss=1.6522177457809448
I0130 00:14:37.467045 139656666543872 logging_writer.py:48] [47800] global_step=47800, grad_norm=2.120408296585083, loss=1.6550601720809937
I0130 00:14:53.302214 139822745589568 spec.py:321] Evaluating on the training split.
I0130 00:14:59.577812 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 00:15:08.016864 139822745589568 spec.py:349] Evaluating on the test split.
I0130 00:15:10.637202 139822745589568 submission_runner.py:408] Time since start: 16937.63s, 	Step: 47848, 	{'train/accuracy': 0.7031847834587097, 'train/loss': 1.1564139127731323, 'validation/accuracy': 0.6360999941825867, 'validation/loss': 1.489286184310913, 'validation/num_examples': 50000, 'test/accuracy': 0.5045000314712524, 'test/loss': 2.241513967514038, 'test/num_examples': 10000, 'score': 16358.012321472168, 'total_duration': 16937.62736606598, 'accumulated_submission_time': 16358.012321472168, 'accumulated_eval_time': 576.9686605930328, 'accumulated_logging_time': 1.016944408416748}
I0130 00:15:10.663372 139655584409344 logging_writer.py:48] [47848] accumulated_eval_time=576.968661, accumulated_logging_time=1.016944, accumulated_submission_time=16358.012321, global_step=47848, preemption_count=0, score=16358.012321, test/accuracy=0.504500, test/loss=2.241514, test/num_examples=10000, total_duration=16937.627366, train/accuracy=0.703185, train/loss=1.156414, validation/accuracy=0.636100, validation/loss=1.489286, validation/num_examples=50000
I0130 00:15:28.727354 139656297445120 logging_writer.py:48] [47900] global_step=47900, grad_norm=1.7496215105056763, loss=1.6007952690124512
I0130 00:16:02.761991 139655584409344 logging_writer.py:48] [48000] global_step=48000, grad_norm=1.8531458377838135, loss=1.5994484424591064
I0130 00:16:36.827194 139656297445120 logging_writer.py:48] [48100] global_step=48100, grad_norm=1.78450608253479, loss=1.5488536357879639
I0130 00:17:10.887002 139655584409344 logging_writer.py:48] [48200] global_step=48200, grad_norm=1.6227171421051025, loss=1.5656108856201172
I0130 00:17:45.001919 139656297445120 logging_writer.py:48] [48300] global_step=48300, grad_norm=1.852932095527649, loss=1.6717333793640137
I0130 00:18:19.088034 139655584409344 logging_writer.py:48] [48400] global_step=48400, grad_norm=1.7111800909042358, loss=1.5799378156661987
I0130 00:18:53.189769 139656297445120 logging_writer.py:48] [48500] global_step=48500, grad_norm=2.158867597579956, loss=1.6575490236282349
I0130 00:19:27.359005 139655584409344 logging_writer.py:48] [48600] global_step=48600, grad_norm=1.8668200969696045, loss=1.678306221961975
I0130 00:20:01.435928 139656297445120 logging_writer.py:48] [48700] global_step=48700, grad_norm=1.648848533630371, loss=1.529923915863037
I0130 00:20:35.521075 139655584409344 logging_writer.py:48] [48800] global_step=48800, grad_norm=1.92372465133667, loss=1.660526156425476
I0130 00:21:10.178428 139656297445120 logging_writer.py:48] [48900] global_step=48900, grad_norm=1.836002230644226, loss=1.6020617485046387
I0130 00:21:44.263822 139655584409344 logging_writer.py:48] [49000] global_step=49000, grad_norm=1.9292219877243042, loss=1.5976662635803223
I0130 00:22:18.371743 139656297445120 logging_writer.py:48] [49100] global_step=49100, grad_norm=2.0165843963623047, loss=1.6516153812408447
I0130 00:22:52.461185 139655584409344 logging_writer.py:48] [49200] global_step=49200, grad_norm=1.723725438117981, loss=1.6209368705749512
I0130 00:23:26.512052 139656297445120 logging_writer.py:48] [49300] global_step=49300, grad_norm=1.8336342573165894, loss=1.502663254737854
I0130 00:23:40.971762 139822745589568 spec.py:321] Evaluating on the training split.
I0130 00:23:47.259926 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 00:23:56.011590 139822745589568 spec.py:349] Evaluating on the test split.
I0130 00:23:58.610484 139822745589568 submission_runner.py:408] Time since start: 17465.60s, 	Step: 49344, 	{'train/accuracy': 0.7031847834587097, 'train/loss': 1.1441307067871094, 'validation/accuracy': 0.6427199840545654, 'validation/loss': 1.4674408435821533, 'validation/num_examples': 50000, 'test/accuracy': 0.5211000442504883, 'test/loss': 2.169034481048584, 'test/num_examples': 10000, 'score': 16868.260396003723, 'total_duration': 17465.60064959526, 'accumulated_submission_time': 16868.260396003723, 'accumulated_eval_time': 594.6073455810547, 'accumulated_logging_time': 1.0527050495147705}
I0130 00:23:58.639890 139655584409344 logging_writer.py:48] [49344] accumulated_eval_time=594.607346, accumulated_logging_time=1.052705, accumulated_submission_time=16868.260396, global_step=49344, preemption_count=0, score=16868.260396, test/accuracy=0.521100, test/loss=2.169034, test/num_examples=10000, total_duration=17465.600650, train/accuracy=0.703185, train/loss=1.144131, validation/accuracy=0.642720, validation/loss=1.467441, validation/num_examples=50000
I0130 00:24:18.015475 139656666543872 logging_writer.py:48] [49400] global_step=49400, grad_norm=2.053438663482666, loss=1.6869407892227173
I0130 00:24:52.049353 139655584409344 logging_writer.py:48] [49500] global_step=49500, grad_norm=1.7179888486862183, loss=1.5903306007385254
I0130 00:25:26.115167 139656666543872 logging_writer.py:48] [49600] global_step=49600, grad_norm=1.6994682550430298, loss=1.6296589374542236
I0130 00:26:00.281998 139655584409344 logging_writer.py:48] [49700] global_step=49700, grad_norm=1.8574718236923218, loss=1.7135040760040283
I0130 00:26:34.356609 139656666543872 logging_writer.py:48] [49800] global_step=49800, grad_norm=1.6982256174087524, loss=1.4868640899658203
I0130 00:27:08.434757 139655584409344 logging_writer.py:48] [49900] global_step=49900, grad_norm=1.9606003761291504, loss=1.703139305114746
I0130 00:27:42.514081 139656666543872 logging_writer.py:48] [50000] global_step=50000, grad_norm=2.001675844192505, loss=1.5067704916000366
I0130 00:28:16.602288 139655584409344 logging_writer.py:48] [50100] global_step=50100, grad_norm=1.9501971006393433, loss=1.6957054138183594
I0130 00:28:50.665056 139656666543872 logging_writer.py:48] [50200] global_step=50200, grad_norm=1.7823541164398193, loss=1.594240427017212
I0130 00:29:24.756541 139655584409344 logging_writer.py:48] [50300] global_step=50300, grad_norm=2.124776601791382, loss=1.6073191165924072
I0130 00:29:58.846810 139656666543872 logging_writer.py:48] [50400] global_step=50400, grad_norm=1.7633018493652344, loss=1.3942700624465942
I0130 00:30:32.951818 139655584409344 logging_writer.py:48] [50500] global_step=50500, grad_norm=1.8213155269622803, loss=1.6810832023620605
I0130 00:31:07.034820 139656666543872 logging_writer.py:48] [50600] global_step=50600, grad_norm=1.8789966106414795, loss=1.5795255899429321
I0130 00:31:41.141232 139655584409344 logging_writer.py:48] [50700] global_step=50700, grad_norm=1.6949166059494019, loss=1.574022650718689
I0130 00:32:15.235587 139656666543872 logging_writer.py:48] [50800] global_step=50800, grad_norm=1.780787467956543, loss=1.5855449438095093
I0130 00:32:28.659024 139822745589568 spec.py:321] Evaluating on the training split.
I0130 00:32:35.122216 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 00:32:43.565407 139822745589568 spec.py:349] Evaluating on the test split.
I0130 00:32:46.193007 139822745589568 submission_runner.py:408] Time since start: 17993.18s, 	Step: 50841, 	{'train/accuracy': 0.7056361436843872, 'train/loss': 1.1398847103118896, 'validation/accuracy': 0.645039975643158, 'validation/loss': 1.4592891931533813, 'validation/num_examples': 50000, 'test/accuracy': 0.5149000287055969, 'test/loss': 2.193108081817627, 'test/num_examples': 10000, 'score': 17378.219309806824, 'total_duration': 17993.183161497116, 'accumulated_submission_time': 17378.219309806824, 'accumulated_eval_time': 612.1412818431854, 'accumulated_logging_time': 1.091554880142212}
I0130 00:32:46.224753 139655584409344 logging_writer.py:48] [50841] accumulated_eval_time=612.141282, accumulated_logging_time=1.091555, accumulated_submission_time=17378.219310, global_step=50841, preemption_count=0, score=17378.219310, test/accuracy=0.514900, test/loss=2.193108, test/num_examples=10000, total_duration=17993.183161, train/accuracy=0.705636, train/loss=1.139885, validation/accuracy=0.645040, validation/loss=1.459289, validation/num_examples=50000
I0130 00:33:06.640288 139656297445120 logging_writer.py:48] [50900] global_step=50900, grad_norm=1.818081259727478, loss=1.664726972579956
I0130 00:33:40.678920 139655584409344 logging_writer.py:48] [51000] global_step=51000, grad_norm=1.8003443479537964, loss=1.596088171005249
I0130 00:34:14.750563 139656297445120 logging_writer.py:48] [51100] global_step=51100, grad_norm=1.712141513824463, loss=1.5190068483352661
I0130 00:34:48.855884 139655584409344 logging_writer.py:48] [51200] global_step=51200, grad_norm=1.8766833543777466, loss=1.6119933128356934
I0130 00:35:22.963347 139656297445120 logging_writer.py:48] [51300] global_step=51300, grad_norm=1.8181439638137817, loss=1.5955872535705566
I0130 00:35:57.081035 139655584409344 logging_writer.py:48] [51400] global_step=51400, grad_norm=1.77272629737854, loss=1.5409672260284424
I0130 00:36:31.191975 139656297445120 logging_writer.py:48] [51500] global_step=51500, grad_norm=1.9545773267745972, loss=1.6841553449630737
I0130 00:37:05.278438 139655584409344 logging_writer.py:48] [51600] global_step=51600, grad_norm=1.7837077379226685, loss=1.6226441860198975
I0130 00:37:39.368829 139656297445120 logging_writer.py:48] [51700] global_step=51700, grad_norm=1.8077595233917236, loss=1.6697616577148438
I0130 00:38:13.438831 139655584409344 logging_writer.py:48] [51800] global_step=51800, grad_norm=1.8316268920898438, loss=1.6377114057540894
I0130 00:38:47.535065 139656297445120 logging_writer.py:48] [51900] global_step=51900, grad_norm=2.347585916519165, loss=1.5862928628921509
I0130 00:39:21.730698 139655584409344 logging_writer.py:48] [52000] global_step=52000, grad_norm=1.9315125942230225, loss=1.4974056482315063
I0130 00:39:55.835794 139656297445120 logging_writer.py:48] [52100] global_step=52100, grad_norm=2.1648569107055664, loss=1.6230103969573975
I0130 00:40:29.944752 139655584409344 logging_writer.py:48] [52200] global_step=52200, grad_norm=1.7932782173156738, loss=1.6752667427062988
I0130 00:41:04.044684 139656297445120 logging_writer.py:48] [52300] global_step=52300, grad_norm=1.9375643730163574, loss=1.5797295570373535
I0130 00:41:16.484105 139822745589568 spec.py:321] Evaluating on the training split.
I0130 00:41:22.754569 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 00:41:31.202361 139822745589568 spec.py:349] Evaluating on the test split.
I0130 00:41:33.949755 139822745589568 submission_runner.py:408] Time since start: 18520.94s, 	Step: 52338, 	{'train/accuracy': 0.7645886540412903, 'train/loss': 0.9032778739929199, 'validation/accuracy': 0.6543799638748169, 'validation/loss': 1.4254825115203857, 'validation/num_examples': 50000, 'test/accuracy': 0.520300030708313, 'test/loss': 2.1776282787323, 'test/num_examples': 10000, 'score': 17888.419243574142, 'total_duration': 18520.939923524857, 'accumulated_submission_time': 17888.419243574142, 'accumulated_eval_time': 629.606897354126, 'accumulated_logging_time': 1.1325068473815918}
I0130 00:41:33.977960 139655576016640 logging_writer.py:48] [52338] accumulated_eval_time=629.606897, accumulated_logging_time=1.132507, accumulated_submission_time=17888.419244, global_step=52338, preemption_count=0, score=17888.419244, test/accuracy=0.520300, test/loss=2.177628, test/num_examples=10000, total_duration=18520.939924, train/accuracy=0.764589, train/loss=0.903278, validation/accuracy=0.654380, validation/loss=1.425483, validation/num_examples=50000
I0130 00:41:55.422245 139655592802048 logging_writer.py:48] [52400] global_step=52400, grad_norm=1.947163701057434, loss=1.607566475868225
I0130 00:42:29.486324 139655576016640 logging_writer.py:48] [52500] global_step=52500, grad_norm=1.7643983364105225, loss=1.5134912729263306
I0130 00:43:03.556595 139655592802048 logging_writer.py:48] [52600] global_step=52600, grad_norm=1.9598133563995361, loss=1.7428975105285645
I0130 00:43:37.629640 139655576016640 logging_writer.py:48] [52700] global_step=52700, grad_norm=1.816559076309204, loss=1.4705727100372314
I0130 00:44:11.731848 139655592802048 logging_writer.py:48] [52800] global_step=52800, grad_norm=1.9433883428573608, loss=1.6154838800430298
I0130 00:44:45.820922 139655576016640 logging_writer.py:48] [52900] global_step=52900, grad_norm=1.693150281906128, loss=1.5613152980804443
I0130 00:45:19.918609 139655592802048 logging_writer.py:48] [53000] global_step=53000, grad_norm=1.7628998756408691, loss=1.5396004915237427
I0130 00:45:54.009762 139655576016640 logging_writer.py:48] [53100] global_step=53100, grad_norm=1.7380709648132324, loss=1.6554168462753296
I0130 00:46:28.183030 139655592802048 logging_writer.py:48] [53200] global_step=53200, grad_norm=1.752062439918518, loss=1.54742431640625
I0130 00:47:02.244725 139655576016640 logging_writer.py:48] [53300] global_step=53300, grad_norm=1.8186274766921997, loss=1.7039861679077148
I0130 00:47:36.292278 139655592802048 logging_writer.py:48] [53400] global_step=53400, grad_norm=2.020134925842285, loss=1.6679233312606812
I0130 00:48:10.387105 139655576016640 logging_writer.py:48] [53500] global_step=53500, grad_norm=1.8962109088897705, loss=1.5186436176300049
I0130 00:48:44.484739 139655592802048 logging_writer.py:48] [53600] global_step=53600, grad_norm=1.9030624628067017, loss=1.6228046417236328
I0130 00:49:18.577218 139655576016640 logging_writer.py:48] [53700] global_step=53700, grad_norm=1.872133731842041, loss=1.5845433473587036
I0130 00:49:52.702146 139655592802048 logging_writer.py:48] [53800] global_step=53800, grad_norm=1.8940550088882446, loss=1.5547112226486206
I0130 00:50:04.102538 139822745589568 spec.py:321] Evaluating on the training split.
I0130 00:50:10.343077 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 00:50:18.721084 139822745589568 spec.py:349] Evaluating on the test split.
I0130 00:50:21.359209 139822745589568 submission_runner.py:408] Time since start: 19048.35s, 	Step: 53835, 	{'train/accuracy': 0.7227758169174194, 'train/loss': 1.0640006065368652, 'validation/accuracy': 0.6343599557876587, 'validation/loss': 1.4988964796066284, 'validation/num_examples': 50000, 'test/accuracy': 0.5029000043869019, 'test/loss': 2.2745490074157715, 'test/num_examples': 10000, 'score': 18398.483984470367, 'total_duration': 19048.349376678467, 'accumulated_submission_time': 18398.483984470367, 'accumulated_eval_time': 646.8635385036469, 'accumulated_logging_time': 1.1695225238800049}
I0130 00:50:21.388543 139655592802048 logging_writer.py:48] [53835] accumulated_eval_time=646.863539, accumulated_logging_time=1.169523, accumulated_submission_time=18398.483984, global_step=53835, preemption_count=0, score=18398.483984, test/accuracy=0.502900, test/loss=2.274549, test/num_examples=10000, total_duration=19048.349377, train/accuracy=0.722776, train/loss=1.064001, validation/accuracy=0.634360, validation/loss=1.498896, validation/num_examples=50000
I0130 00:50:43.889151 139656649758464 logging_writer.py:48] [53900] global_step=53900, grad_norm=1.97567617893219, loss=1.6874709129333496
I0130 00:51:17.967123 139655592802048 logging_writer.py:48] [54000] global_step=54000, grad_norm=1.733000636100769, loss=1.5809818506240845
I0130 00:51:52.055471 139656649758464 logging_writer.py:48] [54100] global_step=54100, grad_norm=2.1412830352783203, loss=1.6836225986480713
I0130 00:52:26.183099 139655592802048 logging_writer.py:48] [54200] global_step=54200, grad_norm=1.8577488660812378, loss=1.67171049118042
I0130 00:53:00.386094 139656649758464 logging_writer.py:48] [54300] global_step=54300, grad_norm=1.8483649492263794, loss=1.5853474140167236
I0130 00:53:34.495279 139655592802048 logging_writer.py:48] [54400] global_step=54400, grad_norm=1.8707243204116821, loss=1.5800187587738037
I0130 00:54:08.591967 139656649758464 logging_writer.py:48] [54500] global_step=54500, grad_norm=1.972886323928833, loss=1.6730655431747437
I0130 00:54:42.697065 139655592802048 logging_writer.py:48] [54600] global_step=54600, grad_norm=1.954907774925232, loss=1.619175672531128
I0130 00:55:16.774513 139656649758464 logging_writer.py:48] [54700] global_step=54700, grad_norm=2.0106682777404785, loss=1.6433289051055908
I0130 00:55:50.884109 139655592802048 logging_writer.py:48] [54800] global_step=54800, grad_norm=2.2053298950195312, loss=1.6126201152801514
I0130 00:56:24.968120 139656649758464 logging_writer.py:48] [54900] global_step=54900, grad_norm=1.8821947574615479, loss=1.5678184032440186
I0130 00:56:59.049672 139655592802048 logging_writer.py:48] [55000] global_step=55000, grad_norm=1.8165338039398193, loss=1.6232856512069702
I0130 00:57:33.120894 139656649758464 logging_writer.py:48] [55100] global_step=55100, grad_norm=1.8359423875808716, loss=1.594225287437439
I0130 00:58:07.203065 139655592802048 logging_writer.py:48] [55200] global_step=55200, grad_norm=1.90090811252594, loss=1.6134989261627197
I0130 00:58:41.281292 139656649758464 logging_writer.py:48] [55300] global_step=55300, grad_norm=2.127793073654175, loss=1.617601990699768
I0130 00:58:51.664016 139822745589568 spec.py:321] Evaluating on the training split.
I0130 00:58:57.896130 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 00:59:06.537409 139822745589568 spec.py:349] Evaluating on the test split.
I0130 00:59:09.252347 139822745589568 submission_runner.py:408] Time since start: 19576.24s, 	Step: 55332, 	{'train/accuracy': 0.7080675959587097, 'train/loss': 1.1316912174224854, 'validation/accuracy': 0.6315799951553345, 'validation/loss': 1.5241401195526123, 'validation/num_examples': 50000, 'test/accuracy': 0.5034000277519226, 'test/loss': 2.26247239112854, 'test/num_examples': 10000, 'score': 18908.700251817703, 'total_duration': 19576.242521762848, 'accumulated_submission_time': 18908.700251817703, 'accumulated_eval_time': 664.4518418312073, 'accumulated_logging_time': 1.2079482078552246}
I0130 00:59:09.277946 139656666543872 logging_writer.py:48] [55332] accumulated_eval_time=664.451842, accumulated_logging_time=1.207948, accumulated_submission_time=18908.700252, global_step=55332, preemption_count=0, score=18908.700252, test/accuracy=0.503400, test/loss=2.262472, test/num_examples=10000, total_duration=19576.242522, train/accuracy=0.708068, train/loss=1.131691, validation/accuracy=0.631580, validation/loss=1.524140, validation/num_examples=50000
I0130 00:59:32.860264 139656834316032 logging_writer.py:48] [55400] global_step=55400, grad_norm=1.8534098863601685, loss=1.5458623170852661
I0130 01:00:06.927839 139656666543872 logging_writer.py:48] [55500] global_step=55500, grad_norm=1.9969960451126099, loss=1.6863703727722168
I0130 01:00:41.008439 139656834316032 logging_writer.py:48] [55600] global_step=55600, grad_norm=1.9371346235275269, loss=1.60491943359375
I0130 01:01:15.120138 139656666543872 logging_writer.py:48] [55700] global_step=55700, grad_norm=2.014279842376709, loss=1.6922894716262817
I0130 01:01:49.241494 139656834316032 logging_writer.py:48] [55800] global_step=55800, grad_norm=1.9128878116607666, loss=1.5956939458847046
I0130 01:02:23.358078 139656666543872 logging_writer.py:48] [55900] global_step=55900, grad_norm=1.900894284248352, loss=1.7130204439163208
I0130 01:02:57.442755 139656834316032 logging_writer.py:48] [56000] global_step=56000, grad_norm=1.9359632730484009, loss=1.6897166967391968
I0130 01:03:31.557471 139656666543872 logging_writer.py:48] [56100] global_step=56100, grad_norm=1.904576301574707, loss=1.5329357385635376
I0130 01:04:05.662603 139656834316032 logging_writer.py:48] [56200] global_step=56200, grad_norm=2.0700082778930664, loss=1.6093618869781494
I0130 01:04:39.748800 139656666543872 logging_writer.py:48] [56300] global_step=56300, grad_norm=1.8823013305664062, loss=1.5925158262252808
I0130 01:05:13.846943 139656834316032 logging_writer.py:48] [56400] global_step=56400, grad_norm=1.781856656074524, loss=1.5698233842849731
I0130 01:05:48.023607 139656666543872 logging_writer.py:48] [56500] global_step=56500, grad_norm=1.8027830123901367, loss=1.5305490493774414
I0130 01:06:22.087341 139656834316032 logging_writer.py:48] [56600] global_step=56600, grad_norm=1.8163901567459106, loss=1.5093655586242676
I0130 01:06:56.152120 139656666543872 logging_writer.py:48] [56700] global_step=56700, grad_norm=1.9781560897827148, loss=1.7333518266677856
I0130 01:07:30.237978 139656834316032 logging_writer.py:48] [56800] global_step=56800, grad_norm=1.938551902770996, loss=1.5595849752426147
I0130 01:07:39.575988 139822745589568 spec.py:321] Evaluating on the training split.
I0130 01:07:45.950397 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 01:07:54.604609 139822745589568 spec.py:349] Evaluating on the test split.
I0130 01:07:57.198848 139822745589568 submission_runner.py:408] Time since start: 20104.19s, 	Step: 56829, 	{'train/accuracy': 0.7178930044174194, 'train/loss': 1.0814168453216553, 'validation/accuracy': 0.6469199657440186, 'validation/loss': 1.4597644805908203, 'validation/num_examples': 50000, 'test/accuracy': 0.5152000188827515, 'test/loss': 2.216600179672241, 'test/num_examples': 10000, 'score': 19418.939255475998, 'total_duration': 20104.18901371956, 'accumulated_submission_time': 19418.939255475998, 'accumulated_eval_time': 682.0746810436249, 'accumulated_logging_time': 1.2414581775665283}
I0130 01:07:57.231557 139655592802048 logging_writer.py:48] [56829] accumulated_eval_time=682.074681, accumulated_logging_time=1.241458, accumulated_submission_time=19418.939255, global_step=56829, preemption_count=0, score=19418.939255, test/accuracy=0.515200, test/loss=2.216600, test/num_examples=10000, total_duration=20104.189014, train/accuracy=0.717893, train/loss=1.081417, validation/accuracy=0.646920, validation/loss=1.459764, validation/num_examples=50000
I0130 01:08:21.755078 139656297445120 logging_writer.py:48] [56900] global_step=56900, grad_norm=1.8583420515060425, loss=1.5670065879821777
I0130 01:08:55.821271 139655592802048 logging_writer.py:48] [57000] global_step=57000, grad_norm=1.8390265703201294, loss=1.6045341491699219
I0130 01:09:29.885940 139656297445120 logging_writer.py:48] [57100] global_step=57100, grad_norm=1.7437934875488281, loss=1.4023144245147705
I0130 01:10:03.995076 139655592802048 logging_writer.py:48] [57200] global_step=57200, grad_norm=1.7491188049316406, loss=1.5420645475387573
I0130 01:10:38.105240 139656297445120 logging_writer.py:48] [57300] global_step=57300, grad_norm=1.848109245300293, loss=1.629384160041809
I0130 01:11:12.198729 139655592802048 logging_writer.py:48] [57400] global_step=57400, grad_norm=1.9118849039077759, loss=1.4651381969451904
I0130 01:11:46.297774 139656297445120 logging_writer.py:48] [57500] global_step=57500, grad_norm=1.8278802633285522, loss=1.5264413356781006
I0130 01:12:20.446373 139655592802048 logging_writer.py:48] [57600] global_step=57600, grad_norm=1.9312350749969482, loss=1.5975470542907715
I0130 01:12:54.545382 139656297445120 logging_writer.py:48] [57700] global_step=57700, grad_norm=1.8814167976379395, loss=1.5110373497009277
I0130 01:13:28.649131 139655592802048 logging_writer.py:48] [57800] global_step=57800, grad_norm=1.833437442779541, loss=1.6024898290634155
I0130 01:14:02.725348 139656297445120 logging_writer.py:48] [57900] global_step=57900, grad_norm=1.9115078449249268, loss=1.5767797231674194
I0130 01:14:36.831317 139655592802048 logging_writer.py:48] [58000] global_step=58000, grad_norm=1.7429200410842896, loss=1.543632984161377
I0130 01:15:10.902742 139656297445120 logging_writer.py:48] [58100] global_step=58100, grad_norm=1.8172245025634766, loss=1.6546987295150757
I0130 01:15:44.995865 139655592802048 logging_writer.py:48] [58200] global_step=58200, grad_norm=2.169320821762085, loss=1.5492520332336426
I0130 01:16:19.066469 139656297445120 logging_writer.py:48] [58300] global_step=58300, grad_norm=1.7956422567367554, loss=1.4970908164978027
I0130 01:16:27.406603 139822745589568 spec.py:321] Evaluating on the training split.
I0130 01:16:34.308175 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 01:16:42.958226 139822745589568 spec.py:349] Evaluating on the test split.
I0130 01:16:45.593832 139822745589568 submission_runner.py:408] Time since start: 20632.58s, 	Step: 58326, 	{'train/accuracy': 0.7149434089660645, 'train/loss': 1.1157106161117554, 'validation/accuracy': 0.6482999920845032, 'validation/loss': 1.441026210784912, 'validation/num_examples': 50000, 'test/accuracy': 0.5131000280380249, 'test/loss': 2.1649599075317383, 'test/num_examples': 10000, 'score': 19929.052931785583, 'total_duration': 20632.583993673325, 'accumulated_submission_time': 19929.052931785583, 'accumulated_eval_time': 700.2618687152863, 'accumulated_logging_time': 1.2832703590393066}
I0130 01:16:45.625361 139656666543872 logging_writer.py:48] [58326] accumulated_eval_time=700.261869, accumulated_logging_time=1.283270, accumulated_submission_time=19929.052932, global_step=58326, preemption_count=0, score=19929.052932, test/accuracy=0.513100, test/loss=2.164960, test/num_examples=10000, total_duration=20632.583994, train/accuracy=0.714943, train/loss=1.115711, validation/accuracy=0.648300, validation/loss=1.441026, validation/num_examples=50000
I0130 01:17:11.145028 139656834316032 logging_writer.py:48] [58400] global_step=58400, grad_norm=1.8770499229431152, loss=1.6532236337661743
I0130 01:17:45.174906 139656666543872 logging_writer.py:48] [58500] global_step=58500, grad_norm=1.8461850881576538, loss=1.4858020544052124
I0130 01:18:19.197719 139656834316032 logging_writer.py:48] [58600] global_step=58600, grad_norm=1.8817540407180786, loss=1.6130894422531128
I0130 01:18:53.282034 139656666543872 logging_writer.py:48] [58700] global_step=58700, grad_norm=2.0998239517211914, loss=1.6673803329467773
I0130 01:19:27.459074 139656834316032 logging_writer.py:48] [58800] global_step=58800, grad_norm=1.9010837078094482, loss=1.5425143241882324
I0130 01:20:01.561200 139656666543872 logging_writer.py:48] [58900] global_step=58900, grad_norm=1.797441005706787, loss=1.446656346321106
I0130 01:20:35.651842 139656834316032 logging_writer.py:48] [59000] global_step=59000, grad_norm=2.1756811141967773, loss=1.5543909072875977
I0130 01:21:09.760539 139656666543872 logging_writer.py:48] [59100] global_step=59100, grad_norm=1.8922107219696045, loss=1.4263882637023926
I0130 01:21:43.880491 139656834316032 logging_writer.py:48] [59200] global_step=59200, grad_norm=1.984498381614685, loss=1.6545741558074951
I0130 01:22:17.971334 139656666543872 logging_writer.py:48] [59300] global_step=59300, grad_norm=1.9062775373458862, loss=1.5108829736709595
I0130 01:22:52.100703 139656834316032 logging_writer.py:48] [59400] global_step=59400, grad_norm=1.9497112035751343, loss=1.4759975671768188
I0130 01:23:26.194424 139656666543872 logging_writer.py:48] [59500] global_step=59500, grad_norm=1.913534164428711, loss=1.6267112493515015
I0130 01:24:00.286130 139656834316032 logging_writer.py:48] [59600] global_step=59600, grad_norm=2.02007794380188, loss=1.549611210823059
I0130 01:24:34.364903 139656666543872 logging_writer.py:48] [59700] global_step=59700, grad_norm=2.1884026527404785, loss=1.5191699266433716
I0130 01:25:08.487671 139656834316032 logging_writer.py:48] [59800] global_step=59800, grad_norm=1.9348706007003784, loss=1.5121389627456665
I0130 01:25:15.775313 139822745589568 spec.py:321] Evaluating on the training split.
I0130 01:25:22.041219 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 01:25:30.631507 139822745589568 spec.py:349] Evaluating on the test split.
I0130 01:25:33.244408 139822745589568 submission_runner.py:408] Time since start: 21160.23s, 	Step: 59823, 	{'train/accuracy': 0.7216398119926453, 'train/loss': 1.0750752687454224, 'validation/accuracy': 0.6587199568748474, 'validation/loss': 1.4014229774475098, 'validation/num_examples': 50000, 'test/accuracy': 0.5279000401496887, 'test/loss': 2.151419162750244, 'test/num_examples': 10000, 'score': 20439.141530275345, 'total_duration': 21160.234573602676, 'accumulated_submission_time': 20439.141530275345, 'accumulated_eval_time': 717.730926990509, 'accumulated_logging_time': 1.3240761756896973}
I0130 01:25:33.272564 139655592802048 logging_writer.py:48] [59823] accumulated_eval_time=717.730927, accumulated_logging_time=1.324076, accumulated_submission_time=20439.141530, global_step=59823, preemption_count=0, score=20439.141530, test/accuracy=0.527900, test/loss=2.151419, test/num_examples=10000, total_duration=21160.234574, train/accuracy=0.721640, train/loss=1.075075, validation/accuracy=0.658720, validation/loss=1.401423, validation/num_examples=50000
I0130 01:25:59.879426 139656297445120 logging_writer.py:48] [59900] global_step=59900, grad_norm=1.8077183961868286, loss=1.5434348583221436
I0130 01:26:33.954426 139655592802048 logging_writer.py:48] [60000] global_step=60000, grad_norm=1.7671154737472534, loss=1.5099873542785645
I0130 01:27:08.049155 139656297445120 logging_writer.py:48] [60100] global_step=60100, grad_norm=2.082655191421509, loss=1.6096723079681396
I0130 01:27:42.139570 139655592802048 logging_writer.py:48] [60200] global_step=60200, grad_norm=1.9386337995529175, loss=1.4717004299163818
I0130 01:28:16.244513 139656297445120 logging_writer.py:48] [60300] global_step=60300, grad_norm=2.1671903133392334, loss=1.6619287729263306
I0130 01:28:50.357495 139655592802048 logging_writer.py:48] [60400] global_step=60400, grad_norm=1.8539490699768066, loss=1.4240535497665405
I0130 01:29:24.442085 139656297445120 logging_writer.py:48] [60500] global_step=60500, grad_norm=1.9466099739074707, loss=1.5894558429718018
I0130 01:29:58.546568 139655592802048 logging_writer.py:48] [60600] global_step=60600, grad_norm=1.7900221347808838, loss=1.5179433822631836
I0130 01:30:32.646094 139656297445120 logging_writer.py:48] [60700] global_step=60700, grad_norm=1.765133261680603, loss=1.4872997999191284
I0130 01:31:06.721112 139655592802048 logging_writer.py:48] [60800] global_step=60800, grad_norm=2.3061444759368896, loss=1.560563325881958
I0130 01:31:40.807253 139656297445120 logging_writer.py:48] [60900] global_step=60900, grad_norm=1.8753401041030884, loss=1.552628993988037
I0130 01:32:14.978031 139655592802048 logging_writer.py:48] [61000] global_step=61000, grad_norm=1.9673452377319336, loss=1.6019772291183472
I0130 01:32:49.047573 139656297445120 logging_writer.py:48] [61100] global_step=61100, grad_norm=2.0031819343566895, loss=1.5962555408477783
I0130 01:33:23.099392 139655592802048 logging_writer.py:48] [61200] global_step=61200, grad_norm=2.012723207473755, loss=1.7061693668365479
I0130 01:33:57.167777 139656297445120 logging_writer.py:48] [61300] global_step=61300, grad_norm=1.7187680006027222, loss=1.5630052089691162
I0130 01:34:03.444097 139822745589568 spec.py:321] Evaluating on the training split.
I0130 01:34:09.700013 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 01:34:18.083936 139822745589568 spec.py:349] Evaluating on the test split.
I0130 01:34:20.699826 139822745589568 submission_runner.py:408] Time since start: 21687.69s, 	Step: 61320, 	{'train/accuracy': 0.7239716053009033, 'train/loss': 1.0552133321762085, 'validation/accuracy': 0.6634399890899658, 'validation/loss': 1.3708513975143433, 'validation/num_examples': 50000, 'test/accuracy': 0.5362000465393066, 'test/loss': 2.0705716609954834, 'test/num_examples': 10000, 'score': 20949.247843265533, 'total_duration': 21687.68998861313, 'accumulated_submission_time': 20949.247843265533, 'accumulated_eval_time': 734.9866235256195, 'accumulated_logging_time': 1.3634462356567383}
I0130 01:34:20.728747 139655584409344 logging_writer.py:48] [61320] accumulated_eval_time=734.986624, accumulated_logging_time=1.363446, accumulated_submission_time=20949.247843, global_step=61320, preemption_count=0, score=20949.247843, test/accuracy=0.536200, test/loss=2.070572, test/num_examples=10000, total_duration=21687.689989, train/accuracy=0.723972, train/loss=1.055213, validation/accuracy=0.663440, validation/loss=1.370851, validation/num_examples=50000
I0130 01:34:48.297216 139655592802048 logging_writer.py:48] [61400] global_step=61400, grad_norm=1.8486908674240112, loss=1.5721046924591064
I0130 01:35:22.320327 139655584409344 logging_writer.py:48] [61500] global_step=61500, grad_norm=1.9946403503417969, loss=1.5429719686508179
I0130 01:35:56.355415 139655592802048 logging_writer.py:48] [61600] global_step=61600, grad_norm=1.9783544540405273, loss=1.475708246231079
I0130 01:36:30.412289 139655584409344 logging_writer.py:48] [61700] global_step=61700, grad_norm=2.0520238876342773, loss=1.574364185333252
I0130 01:37:04.482938 139655592802048 logging_writer.py:48] [61800] global_step=61800, grad_norm=1.7899420261383057, loss=1.4455403089523315
I0130 01:37:38.596214 139655584409344 logging_writer.py:48] [61900] global_step=61900, grad_norm=1.9379303455352783, loss=1.6188786029815674
I0130 01:38:12.680051 139655592802048 logging_writer.py:48] [62000] global_step=62000, grad_norm=2.086272716522217, loss=1.4717921018600464
I0130 01:38:46.768727 139655584409344 logging_writer.py:48] [62100] global_step=62100, grad_norm=2.2562081813812256, loss=1.558626413345337
I0130 01:39:20.930781 139655592802048 logging_writer.py:48] [62200] global_step=62200, grad_norm=2.1158688068389893, loss=1.5199916362762451
I0130 01:39:55.043288 139655584409344 logging_writer.py:48] [62300] global_step=62300, grad_norm=2.0423760414123535, loss=1.5174992084503174
I0130 01:40:29.160354 139655592802048 logging_writer.py:48] [62400] global_step=62400, grad_norm=1.9656766653060913, loss=1.6132134199142456
I0130 01:41:03.252230 139655584409344 logging_writer.py:48] [62500] global_step=62500, grad_norm=1.9566497802734375, loss=1.6357300281524658
I0130 01:41:37.360956 139655592802048 logging_writer.py:48] [62600] global_step=62600, grad_norm=1.9745043516159058, loss=1.4760148525238037
I0130 01:42:11.444201 139655584409344 logging_writer.py:48] [62700] global_step=62700, grad_norm=2.0584969520568848, loss=1.6999411582946777
I0130 01:42:45.552378 139655592802048 logging_writer.py:48] [62800] global_step=62800, grad_norm=2.1243114471435547, loss=1.53926420211792
I0130 01:42:50.820804 139822745589568 spec.py:321] Evaluating on the training split.
I0130 01:42:57.084846 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 01:43:05.566127 139822745589568 spec.py:349] Evaluating on the test split.
I0130 01:43:08.164836 139822745589568 submission_runner.py:408] Time since start: 22215.15s, 	Step: 62817, 	{'train/accuracy': 0.7576530575752258, 'train/loss': 0.9276350736618042, 'validation/accuracy': 0.6600199937820435, 'validation/loss': 1.3925979137420654, 'validation/num_examples': 50000, 'test/accuracy': 0.5330000519752502, 'test/loss': 2.138416290283203, 'test/num_examples': 10000, 'score': 21459.278631210327, 'total_duration': 22215.154264211655, 'accumulated_submission_time': 21459.278631210327, 'accumulated_eval_time': 752.3298766613007, 'accumulated_logging_time': 1.4020438194274902}
I0130 01:43:08.214325 139656658151168 logging_writer.py:48] [62817] accumulated_eval_time=752.329877, accumulated_logging_time=1.402044, accumulated_submission_time=21459.278631, global_step=62817, preemption_count=0, score=21459.278631, test/accuracy=0.533000, test/loss=2.138416, test/num_examples=10000, total_duration=22215.154264, train/accuracy=0.757653, train/loss=0.927635, validation/accuracy=0.660020, validation/loss=1.392598, validation/num_examples=50000
I0130 01:43:36.838424 139656666543872 logging_writer.py:48] [62900] global_step=62900, grad_norm=2.0937867164611816, loss=1.5175610780715942
I0130 01:44:10.919572 139656658151168 logging_writer.py:48] [63000] global_step=63000, grad_norm=1.8704733848571777, loss=1.4734801054000854
I0130 01:44:45.000318 139656666543872 logging_writer.py:48] [63100] global_step=63100, grad_norm=2.1656455993652344, loss=1.5666022300720215
I0130 01:45:19.083486 139656658151168 logging_writer.py:48] [63200] global_step=63200, grad_norm=2.007550001144409, loss=1.526367425918579
I0130 01:45:53.247274 139656666543872 logging_writer.py:48] [63300] global_step=63300, grad_norm=1.9338477849960327, loss=1.7038094997406006
I0130 01:46:27.343183 139656658151168 logging_writer.py:48] [63400] global_step=63400, grad_norm=2.316185474395752, loss=1.6101067066192627
I0130 01:47:01.436688 139656666543872 logging_writer.py:48] [63500] global_step=63500, grad_norm=1.8923696279525757, loss=1.6015657186508179
I0130 01:47:35.527038 139656658151168 logging_writer.py:48] [63600] global_step=63600, grad_norm=1.8848776817321777, loss=1.566845178604126
I0130 01:48:09.628542 139656666543872 logging_writer.py:48] [63700] global_step=63700, grad_norm=2.2298388481140137, loss=1.55765700340271
I0130 01:48:43.735567 139656658151168 logging_writer.py:48] [63800] global_step=63800, grad_norm=2.075212240219116, loss=1.5346360206604004
I0130 01:49:17.828848 139656666543872 logging_writer.py:48] [63900] global_step=63900, grad_norm=2.0569326877593994, loss=1.492539644241333
I0130 01:49:51.921487 139656658151168 logging_writer.py:48] [64000] global_step=64000, grad_norm=1.948724627494812, loss=1.522242784500122
I0130 01:50:26.025225 139656666543872 logging_writer.py:48] [64100] global_step=64100, grad_norm=1.9507267475128174, loss=1.502396821975708
I0130 01:51:00.105634 139656658151168 logging_writer.py:48] [64200] global_step=64200, grad_norm=1.9746872186660767, loss=1.5742287635803223
I0130 01:51:34.220706 139656666543872 logging_writer.py:48] [64300] global_step=64300, grad_norm=2.0768473148345947, loss=1.5540425777435303
I0130 01:51:38.455420 139822745589568 spec.py:321] Evaluating on the training split.
I0130 01:51:44.670216 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 01:51:53.223042 139822745589568 spec.py:349] Evaluating on the test split.
I0130 01:51:55.728734 139822745589568 submission_runner.py:408] Time since start: 22742.72s, 	Step: 64314, 	{'train/accuracy': 0.7278180718421936, 'train/loss': 1.0450934171676636, 'validation/accuracy': 0.6518799662590027, 'validation/loss': 1.4381275177001953, 'validation/num_examples': 50000, 'test/accuracy': 0.5248000025749207, 'test/loss': 2.1725211143493652, 'test/num_examples': 10000, 'score': 21969.453302383423, 'total_duration': 22742.71888899803, 'accumulated_submission_time': 21969.453302383423, 'accumulated_eval_time': 769.6031460762024, 'accumulated_logging_time': 1.4653689861297607}
I0130 01:51:55.763623 139655584409344 logging_writer.py:48] [64314] accumulated_eval_time=769.603146, accumulated_logging_time=1.465369, accumulated_submission_time=21969.453302, global_step=64314, preemption_count=0, score=21969.453302, test/accuracy=0.524800, test/loss=2.172521, test/num_examples=10000, total_duration=22742.718889, train/accuracy=0.727818, train/loss=1.045093, validation/accuracy=0.651880, validation/loss=1.438128, validation/num_examples=50000
I0130 01:52:25.477830 139655592802048 logging_writer.py:48] [64400] global_step=64400, grad_norm=1.906015157699585, loss=1.5280537605285645
I0130 01:52:59.538822 139655584409344 logging_writer.py:48] [64500] global_step=64500, grad_norm=1.8572078943252563, loss=1.5122830867767334
I0130 01:53:33.614330 139655592802048 logging_writer.py:48] [64600] global_step=64600, grad_norm=2.1166508197784424, loss=1.4350515604019165
I0130 01:54:07.703734 139655584409344 logging_writer.py:48] [64700] global_step=64700, grad_norm=1.999621868133545, loss=1.5192217826843262
I0130 01:54:41.784345 139655592802048 logging_writer.py:48] [64800] global_step=64800, grad_norm=2.0652661323547363, loss=1.5691090822219849
I0130 01:55:15.879875 139655584409344 logging_writer.py:48] [64900] global_step=64900, grad_norm=1.9644312858581543, loss=1.4821529388427734
I0130 01:55:49.955112 139655592802048 logging_writer.py:48] [65000] global_step=65000, grad_norm=1.911456823348999, loss=1.4926520586013794
I0130 01:56:24.041008 139655584409344 logging_writer.py:48] [65100] global_step=65100, grad_norm=1.9191964864730835, loss=1.454167366027832
I0130 01:56:58.086340 139655592802048 logging_writer.py:48] [65200] global_step=65200, grad_norm=1.8824955224990845, loss=1.4421470165252686
I0130 01:57:32.164443 139655584409344 logging_writer.py:48] [65300] global_step=65300, grad_norm=1.9650099277496338, loss=1.4904953241348267
I0130 01:58:06.223617 139655592802048 logging_writer.py:48] [65400] global_step=65400, grad_norm=1.9011332988739014, loss=1.5347851514816284
I0130 01:58:40.269990 139655584409344 logging_writer.py:48] [65500] global_step=65500, grad_norm=1.8673018217086792, loss=1.4690272808074951
I0130 01:59:14.441062 139655592802048 logging_writer.py:48] [65600] global_step=65600, grad_norm=1.981835126876831, loss=1.5359214544296265
I0130 01:59:48.536782 139655584409344 logging_writer.py:48] [65700] global_step=65700, grad_norm=2.1614792346954346, loss=1.5080909729003906
I0130 02:00:22.655956 139655592802048 logging_writer.py:48] [65800] global_step=65800, grad_norm=2.0582375526428223, loss=1.41201651096344
I0130 02:00:25.867426 139822745589568 spec.py:321] Evaluating on the training split.
I0130 02:00:32.118040 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 02:00:40.796354 139822745589568 spec.py:349] Evaluating on the test split.
I0130 02:00:43.402682 139822745589568 submission_runner.py:408] Time since start: 23270.39s, 	Step: 65811, 	{'train/accuracy': 0.7356903553009033, 'train/loss': 1.0099648237228394, 'validation/accuracy': 0.6637600064277649, 'validation/loss': 1.3708646297454834, 'validation/num_examples': 50000, 'test/accuracy': 0.5311000347137451, 'test/loss': 2.125349998474121, 'test/num_examples': 10000, 'score': 22479.49555540085, 'total_duration': 23270.3928463459, 'accumulated_submission_time': 22479.49555540085, 'accumulated_eval_time': 787.1383633613586, 'accumulated_logging_time': 1.5099318027496338}
I0130 02:00:43.436409 139656658151168 logging_writer.py:48] [65811] accumulated_eval_time=787.138363, accumulated_logging_time=1.509932, accumulated_submission_time=22479.495555, global_step=65811, preemption_count=0, score=22479.495555, test/accuracy=0.531100, test/loss=2.125350, test/num_examples=10000, total_duration=23270.392846, train/accuracy=0.735690, train/loss=1.009965, validation/accuracy=0.663760, validation/loss=1.370865, validation/num_examples=50000
I0130 02:01:14.095038 139656666543872 logging_writer.py:48] [65900] global_step=65900, grad_norm=2.0835959911346436, loss=1.7118817567825317
I0130 02:01:48.175448 139656658151168 logging_writer.py:48] [66000] global_step=66000, grad_norm=2.2057900428771973, loss=1.6068075895309448
I0130 02:02:22.291235 139656666543872 logging_writer.py:48] [66100] global_step=66100, grad_norm=2.0379889011383057, loss=1.560070276260376
I0130 02:02:56.393374 139656658151168 logging_writer.py:48] [66200] global_step=66200, grad_norm=1.914886474609375, loss=1.5740582942962646
I0130 02:03:30.496859 139656666543872 logging_writer.py:48] [66300] global_step=66300, grad_norm=1.8242918252944946, loss=1.530505895614624
I0130 02:04:04.557343 139656658151168 logging_writer.py:48] [66400] global_step=66400, grad_norm=1.9351567029953003, loss=1.5613000392913818
I0130 02:04:38.659227 139656666543872 logging_writer.py:48] [66500] global_step=66500, grad_norm=1.9411123991012573, loss=1.5090937614440918
I0130 02:05:12.749265 139656658151168 logging_writer.py:48] [66600] global_step=66600, grad_norm=2.0204458236694336, loss=1.4618134498596191
I0130 02:05:46.907705 139656666543872 logging_writer.py:48] [66700] global_step=66700, grad_norm=2.0614943504333496, loss=1.5196653604507446
I0130 02:06:21.035415 139656658151168 logging_writer.py:48] [66800] global_step=66800, grad_norm=1.9051088094711304, loss=1.4030500650405884
I0130 02:06:55.138034 139656666543872 logging_writer.py:48] [66900] global_step=66900, grad_norm=1.8628612756729126, loss=1.535121202468872
I0130 02:07:29.221572 139656658151168 logging_writer.py:48] [67000] global_step=67000, grad_norm=1.9119763374328613, loss=1.566741943359375
I0130 02:08:03.331858 139656666543872 logging_writer.py:48] [67100] global_step=67100, grad_norm=1.966285228729248, loss=1.6565487384796143
I0130 02:08:37.435763 139656658151168 logging_writer.py:48] [67200] global_step=67200, grad_norm=1.8961087465286255, loss=1.4693759679794312
I0130 02:09:11.541332 139656666543872 logging_writer.py:48] [67300] global_step=67300, grad_norm=1.9812275171279907, loss=1.4736558198928833
I0130 02:09:13.734431 139822745589568 spec.py:321] Evaluating on the training split.
I0130 02:09:19.934079 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 02:09:28.441796 139822745589568 spec.py:349] Evaluating on the test split.
I0130 02:09:31.075448 139822745589568 submission_runner.py:408] Time since start: 23798.07s, 	Step: 67308, 	{'train/accuracy': 0.7371651530265808, 'train/loss': 0.9949511885643005, 'validation/accuracy': 0.664359986782074, 'validation/loss': 1.357032299041748, 'validation/num_examples': 50000, 'test/accuracy': 0.5324000120162964, 'test/loss': 2.098825216293335, 'test/num_examples': 10000, 'score': 22989.733140707016, 'total_duration': 23798.065600156784, 'accumulated_submission_time': 22989.733140707016, 'accumulated_eval_time': 804.4793326854706, 'accumulated_logging_time': 1.5526759624481201}
I0130 02:09:31.105850 139655584409344 logging_writer.py:48] [67308] accumulated_eval_time=804.479333, accumulated_logging_time=1.552676, accumulated_submission_time=22989.733141, global_step=67308, preemption_count=0, score=22989.733141, test/accuracy=0.532400, test/loss=2.098825, test/num_examples=10000, total_duration=23798.065600, train/accuracy=0.737165, train/loss=0.994951, validation/accuracy=0.664360, validation/loss=1.357032, validation/num_examples=50000
I0130 02:10:02.778595 139655592802048 logging_writer.py:48] [67400] global_step=67400, grad_norm=2.094353675842285, loss=1.598107933998108
I0130 02:10:36.838562 139655584409344 logging_writer.py:48] [67500] global_step=67500, grad_norm=1.8527863025665283, loss=1.4622966051101685
I0130 02:11:10.909059 139655592802048 logging_writer.py:48] [67600] global_step=67600, grad_norm=1.9981213808059692, loss=1.5647600889205933
I0130 02:11:45.000360 139655584409344 logging_writer.py:48] [67700] global_step=67700, grad_norm=1.9497746229171753, loss=1.5323002338409424
I0130 02:12:19.122225 139655592802048 logging_writer.py:48] [67800] global_step=67800, grad_norm=2.1279680728912354, loss=1.5169087648391724
I0130 02:12:53.209234 139655584409344 logging_writer.py:48] [67900] global_step=67900, grad_norm=2.012503147125244, loss=1.4173961877822876
I0130 02:13:27.289974 139655592802048 logging_writer.py:48] [68000] global_step=68000, grad_norm=2.0448877811431885, loss=1.5179499387741089
I0130 02:14:01.373629 139655584409344 logging_writer.py:48] [68100] global_step=68100, grad_norm=1.9342074394226074, loss=1.6229281425476074
I0130 02:14:35.439084 139655592802048 logging_writer.py:48] [68200] global_step=68200, grad_norm=2.3426196575164795, loss=1.5538438558578491
I0130 02:15:09.488255 139655584409344 logging_writer.py:48] [68300] global_step=68300, grad_norm=2.3103842735290527, loss=1.5633808374404907
I0130 02:15:43.552873 139655592802048 logging_writer.py:48] [68400] global_step=68400, grad_norm=2.2667059898376465, loss=1.52424955368042
I0130 02:16:17.606288 139655584409344 logging_writer.py:48] [68500] global_step=68500, grad_norm=1.959914207458496, loss=1.4930158853530884
I0130 02:16:51.697581 139655592802048 logging_writer.py:48] [68600] global_step=68600, grad_norm=1.9686154127120972, loss=1.4403029680252075
I0130 02:17:25.785799 139655584409344 logging_writer.py:48] [68700] global_step=68700, grad_norm=2.1267452239990234, loss=1.4306859970092773
I0130 02:17:59.876154 139655592802048 logging_writer.py:48] [68800] global_step=68800, grad_norm=2.409790515899658, loss=1.556074857711792
I0130 02:18:01.392644 139822745589568 spec.py:321] Evaluating on the training split.
I0130 02:18:07.644649 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 02:18:16.253571 139822745589568 spec.py:349] Evaluating on the test split.
I0130 02:18:18.876526 139822745589568 submission_runner.py:408] Time since start: 24325.87s, 	Step: 68806, 	{'train/accuracy': 0.7223772406578064, 'train/loss': 1.0757921934127808, 'validation/accuracy': 0.6567599773406982, 'validation/loss': 1.3976572751998901, 'validation/num_examples': 50000, 'test/accuracy': 0.5281000137329102, 'test/loss': 2.129694700241089, 'test/num_examples': 10000, 'score': 23499.958671808243, 'total_duration': 24325.86668920517, 'accumulated_submission_time': 23499.958671808243, 'accumulated_eval_time': 821.963175535202, 'accumulated_logging_time': 1.5933401584625244}
I0130 02:18:18.906891 139655576016640 logging_writer.py:48] [68806] accumulated_eval_time=821.963176, accumulated_logging_time=1.593340, accumulated_submission_time=23499.958672, global_step=68806, preemption_count=0, score=23499.958672, test/accuracy=0.528100, test/loss=2.129695, test/num_examples=10000, total_duration=24325.866689, train/accuracy=0.722377, train/loss=1.075792, validation/accuracy=0.656760, validation/loss=1.397657, validation/num_examples=50000
I0130 02:18:51.481649 139655584409344 logging_writer.py:48] [68900] global_step=68900, grad_norm=2.177500009536743, loss=1.4760184288024902
I0130 02:19:25.568687 139655576016640 logging_writer.py:48] [69000] global_step=69000, grad_norm=2.128124475479126, loss=1.5512962341308594
I0130 02:19:59.678999 139655584409344 logging_writer.py:48] [69100] global_step=69100, grad_norm=2.2225072383880615, loss=1.47309410572052
I0130 02:20:33.797907 139655576016640 logging_writer.py:48] [69200] global_step=69200, grad_norm=2.2206976413726807, loss=1.5639694929122925
I0130 02:21:07.871684 139655584409344 logging_writer.py:48] [69300] global_step=69300, grad_norm=1.9360841512680054, loss=1.503203272819519
I0130 02:21:41.983587 139655576016640 logging_writer.py:48] [69400] global_step=69400, grad_norm=2.181286334991455, loss=1.5077959299087524
I0130 02:22:16.067278 139655584409344 logging_writer.py:48] [69500] global_step=69500, grad_norm=2.2202632427215576, loss=1.6040571928024292
I0130 02:22:50.156276 139655576016640 logging_writer.py:48] [69600] global_step=69600, grad_norm=2.1650314331054688, loss=1.4977431297302246
I0130 02:23:24.222348 139655584409344 logging_writer.py:48] [69700] global_step=69700, grad_norm=1.9446971416473389, loss=1.4653854370117188
I0130 02:23:58.332615 139655576016640 logging_writer.py:48] [69800] global_step=69800, grad_norm=2.008596658706665, loss=1.3780341148376465
I0130 02:24:32.397598 139655584409344 logging_writer.py:48] [69900] global_step=69900, grad_norm=1.9933816194534302, loss=1.5553693771362305
I0130 02:25:06.487532 139655576016640 logging_writer.py:48] [70000] global_step=70000, grad_norm=1.8564252853393555, loss=1.5347955226898193
I0130 02:25:40.654232 139655584409344 logging_writer.py:48] [70100] global_step=70100, grad_norm=2.0183515548706055, loss=1.5861691236495972
I0130 02:26:14.728178 139655576016640 logging_writer.py:48] [70200] global_step=70200, grad_norm=2.1410601139068604, loss=1.5139098167419434
I0130 02:26:48.794633 139655584409344 logging_writer.py:48] [70300] global_step=70300, grad_norm=2.0443837642669678, loss=1.4770432710647583
I0130 02:26:48.950115 139822745589568 spec.py:321] Evaluating on the training split.
I0130 02:26:55.186987 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 02:27:03.768025 139822745589568 spec.py:349] Evaluating on the test split.
I0130 02:27:06.409698 139822745589568 submission_runner.py:408] Time since start: 24853.40s, 	Step: 70302, 	{'train/accuracy': 0.7234334945678711, 'train/loss': 1.0573880672454834, 'validation/accuracy': 0.656719982624054, 'validation/loss': 1.40852689743042, 'validation/num_examples': 50000, 'test/accuracy': 0.5242000222206116, 'test/loss': 2.144479513168335, 'test/num_examples': 10000, 'score': 24009.941289901733, 'total_duration': 24853.39986562729, 'accumulated_submission_time': 24009.941289901733, 'accumulated_eval_time': 839.4227304458618, 'accumulated_logging_time': 1.6327154636383057}
I0130 02:27:06.439361 139656649758464 logging_writer.py:48] [70302] accumulated_eval_time=839.422730, accumulated_logging_time=1.632715, accumulated_submission_time=24009.941290, global_step=70302, preemption_count=0, score=24009.941290, test/accuracy=0.524200, test/loss=2.144480, test/num_examples=10000, total_duration=24853.399866, train/accuracy=0.723433, train/loss=1.057388, validation/accuracy=0.656720, validation/loss=1.408527, validation/num_examples=50000
I0130 02:27:40.114194 139656834316032 logging_writer.py:48] [70400] global_step=70400, grad_norm=1.9851988554000854, loss=1.496372103691101
I0130 02:28:14.153863 139656649758464 logging_writer.py:48] [70500] global_step=70500, grad_norm=1.9207042455673218, loss=1.449316143989563
I0130 02:28:48.184689 139656834316032 logging_writer.py:48] [70600] global_step=70600, grad_norm=1.7616002559661865, loss=1.4681273698806763
I0130 02:29:22.251013 139656649758464 logging_writer.py:48] [70700] global_step=70700, grad_norm=2.1890177726745605, loss=1.623374342918396
I0130 02:29:56.319397 139656834316032 logging_writer.py:48] [70800] global_step=70800, grad_norm=1.8727149963378906, loss=1.551896333694458
I0130 02:30:30.414556 139656649758464 logging_writer.py:48] [70900] global_step=70900, grad_norm=2.071469306945801, loss=1.4605664014816284
I0130 02:31:04.523776 139656834316032 logging_writer.py:48] [71000] global_step=71000, grad_norm=2.078739881515503, loss=1.5630685091018677
I0130 02:31:38.620979 139656649758464 logging_writer.py:48] [71100] global_step=71100, grad_norm=1.990930199623108, loss=1.411989688873291
I0130 02:32:12.767543 139656834316032 logging_writer.py:48] [71200] global_step=71200, grad_norm=2.103837251663208, loss=1.5013309717178345
I0130 02:32:46.850680 139656649758464 logging_writer.py:48] [71300] global_step=71300, grad_norm=2.0279595851898193, loss=1.5322505235671997
I0130 02:33:20.942446 139656834316032 logging_writer.py:48] [71400] global_step=71400, grad_norm=1.8961420059204102, loss=1.4570274353027344
I0130 02:33:54.999434 139656649758464 logging_writer.py:48] [71500] global_step=71500, grad_norm=2.110924243927002, loss=1.6427435874938965
I0130 02:34:29.068773 139656834316032 logging_writer.py:48] [71600] global_step=71600, grad_norm=2.236713409423828, loss=1.4241149425506592
I0130 02:35:03.119155 139656649758464 logging_writer.py:48] [71700] global_step=71700, grad_norm=2.2559759616851807, loss=1.6090176105499268
I0130 02:35:36.652064 139822745589568 spec.py:321] Evaluating on the training split.
I0130 02:35:42.920621 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 02:35:51.313934 139822745589568 spec.py:349] Evaluating on the test split.
I0130 02:35:53.925080 139822745589568 submission_runner.py:408] Time since start: 25380.92s, 	Step: 71800, 	{'train/accuracy': 0.7745535373687744, 'train/loss': 0.8367967009544373, 'validation/accuracy': 0.6677199602127075, 'validation/loss': 1.3619836568832397, 'validation/num_examples': 50000, 'test/accuracy': 0.5440000295639038, 'test/loss': 2.086559772491455, 'test/num_examples': 10000, 'score': 24520.091975688934, 'total_duration': 25380.915219783783, 'accumulated_submission_time': 24520.091975688934, 'accumulated_eval_time': 856.6956856250763, 'accumulated_logging_time': 1.6729493141174316}
I0130 02:35:53.962390 139655576016640 logging_writer.py:48] [71800] accumulated_eval_time=856.695686, accumulated_logging_time=1.672949, accumulated_submission_time=24520.091976, global_step=71800, preemption_count=0, score=24520.091976, test/accuracy=0.544000, test/loss=2.086560, test/num_examples=10000, total_duration=25380.915220, train/accuracy=0.774554, train/loss=0.836797, validation/accuracy=0.667720, validation/loss=1.361984, validation/num_examples=50000
I0130 02:35:54.310979 139655584409344 logging_writer.py:48] [71800] global_step=71800, grad_norm=1.9429386854171753, loss=1.442307710647583
I0130 02:36:28.368143 139655576016640 logging_writer.py:48] [71900] global_step=71900, grad_norm=2.0850391387939453, loss=1.3929738998413086
I0130 02:37:02.438636 139655584409344 logging_writer.py:48] [72000] global_step=72000, grad_norm=2.1199514865875244, loss=1.4877912998199463
I0130 02:37:36.529810 139655576016640 logging_writer.py:48] [72100] global_step=72100, grad_norm=2.2237486839294434, loss=1.508166790008545
I0130 02:38:10.629688 139655584409344 logging_writer.py:48] [72200] global_step=72200, grad_norm=2.002995491027832, loss=1.4348143339157104
I0130 02:38:44.800743 139655576016640 logging_writer.py:48] [72300] global_step=72300, grad_norm=2.050694465637207, loss=1.5383702516555786
I0130 02:39:18.894082 139655584409344 logging_writer.py:48] [72400] global_step=72400, grad_norm=2.1445558071136475, loss=1.6317692995071411
I0130 02:39:52.968586 139655576016640 logging_writer.py:48] [72500] global_step=72500, grad_norm=2.0911448001861572, loss=1.5360212326049805
I0130 02:40:27.038654 139655584409344 logging_writer.py:48] [72600] global_step=72600, grad_norm=1.9613350629806519, loss=1.4939452409744263
I0130 02:41:01.120908 139655576016640 logging_writer.py:48] [72700] global_step=72700, grad_norm=2.196157932281494, loss=1.5347002744674683
I0130 02:41:35.209431 139655584409344 logging_writer.py:48] [72800] global_step=72800, grad_norm=1.9827066659927368, loss=1.434648871421814
I0130 02:42:09.309427 139655576016640 logging_writer.py:48] [72900] global_step=72900, grad_norm=2.043503522872925, loss=1.4694753885269165
I0130 02:42:43.401487 139655584409344 logging_writer.py:48] [73000] global_step=73000, grad_norm=2.0022830963134766, loss=1.5022178888320923
I0130 02:43:17.484312 139655576016640 logging_writer.py:48] [73100] global_step=73100, grad_norm=1.9529573917388916, loss=1.6446728706359863
I0130 02:43:51.596928 139655584409344 logging_writer.py:48] [73200] global_step=73200, grad_norm=2.669508218765259, loss=1.6174983978271484
I0130 02:44:24.143746 139822745589568 spec.py:321] Evaluating on the training split.
I0130 02:44:30.393458 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 02:44:39.044273 139822745589568 spec.py:349] Evaluating on the test split.
I0130 02:44:41.684898 139822745589568 submission_runner.py:408] Time since start: 25908.68s, 	Step: 73297, 	{'train/accuracy': 0.7525908350944519, 'train/loss': 0.937186062335968, 'validation/accuracy': 0.6672799587249756, 'validation/loss': 1.3505654335021973, 'validation/num_examples': 50000, 'test/accuracy': 0.5418000221252441, 'test/loss': 2.0513622760772705, 'test/num_examples': 10000, 'score': 25030.211496591568, 'total_duration': 25908.675062417984, 'accumulated_submission_time': 25030.211496591568, 'accumulated_eval_time': 874.2368021011353, 'accumulated_logging_time': 1.720144510269165}
I0130 02:44:41.719624 139655584409344 logging_writer.py:48] [73297] accumulated_eval_time=874.236802, accumulated_logging_time=1.720145, accumulated_submission_time=25030.211497, global_step=73297, preemption_count=0, score=25030.211497, test/accuracy=0.541800, test/loss=2.051362, test/num_examples=10000, total_duration=25908.675062, train/accuracy=0.752591, train/loss=0.937186, validation/accuracy=0.667280, validation/loss=1.350565, validation/num_examples=50000
I0130 02:44:43.096617 139655592802048 logging_writer.py:48] [73300] global_step=73300, grad_norm=2.1869611740112305, loss=1.5894618034362793
I0130 02:45:17.190665 139655584409344 logging_writer.py:48] [73400] global_step=73400, grad_norm=1.917671799659729, loss=1.410982370376587
I0130 02:45:51.259813 139655592802048 logging_writer.py:48] [73500] global_step=73500, grad_norm=2.4913253784179688, loss=1.6020411252975464
I0130 02:46:25.346589 139655584409344 logging_writer.py:48] [73600] global_step=73600, grad_norm=2.190539836883545, loss=1.4013320207595825
I0130 02:46:59.452100 139655592802048 logging_writer.py:48] [73700] global_step=73700, grad_norm=2.076145648956299, loss=1.4998136758804321
I0130 02:47:33.545360 139655584409344 logging_writer.py:48] [73800] global_step=73800, grad_norm=1.9038227796554565, loss=1.4398599863052368
I0130 02:48:07.652560 139655592802048 logging_writer.py:48] [73900] global_step=73900, grad_norm=1.9660413265228271, loss=1.548513412475586
I0130 02:48:41.754727 139655584409344 logging_writer.py:48] [74000] global_step=74000, grad_norm=2.1131222248077393, loss=1.418233036994934
I0130 02:49:15.853636 139655592802048 logging_writer.py:48] [74100] global_step=74100, grad_norm=1.821117639541626, loss=1.3973664045333862
I0130 02:49:49.949719 139655584409344 logging_writer.py:48] [74200] global_step=74200, grad_norm=2.063293933868408, loss=1.3470828533172607
I0130 02:50:24.031017 139655592802048 logging_writer.py:48] [74300] global_step=74300, grad_norm=2.11842679977417, loss=1.486862301826477
I0130 02:50:58.121415 139655584409344 logging_writer.py:48] [74400] global_step=74400, grad_norm=2.2849133014678955, loss=1.4395290613174438
I0130 02:51:32.179521 139655592802048 logging_writer.py:48] [74500] global_step=74500, grad_norm=2.0630483627319336, loss=1.4195921421051025
I0130 02:52:06.465096 139655584409344 logging_writer.py:48] [74600] global_step=74600, grad_norm=2.0456361770629883, loss=1.5452847480773926
I0130 02:52:40.539700 139655592802048 logging_writer.py:48] [74700] global_step=74700, grad_norm=2.1053571701049805, loss=1.4468666315078735
I0130 02:53:11.710231 139822745589568 spec.py:321] Evaluating on the training split.
I0130 02:53:18.044733 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 02:53:26.720947 139822745589568 spec.py:349] Evaluating on the test split.
I0130 02:53:29.266615 139822745589568 submission_runner.py:408] Time since start: 26436.26s, 	Step: 74793, 	{'train/accuracy': 0.7455755472183228, 'train/loss': 0.9578787088394165, 'validation/accuracy': 0.6665999889373779, 'validation/loss': 1.366343379020691, 'validation/num_examples': 50000, 'test/accuracy': 0.5467000007629395, 'test/loss': 2.070542812347412, 'test/num_examples': 10000, 'score': 25540.138279676437, 'total_duration': 26436.25676727295, 'accumulated_submission_time': 25540.138279676437, 'accumulated_eval_time': 891.7931699752808, 'accumulated_logging_time': 1.7655150890350342}
I0130 02:53:29.301095 139656297445120 logging_writer.py:48] [74793] accumulated_eval_time=891.793170, accumulated_logging_time=1.765515, accumulated_submission_time=25540.138280, global_step=74793, preemption_count=0, score=25540.138280, test/accuracy=0.546700, test/loss=2.070543, test/num_examples=10000, total_duration=26436.256767, train/accuracy=0.745576, train/loss=0.957879, validation/accuracy=0.666600, validation/loss=1.366343, validation/num_examples=50000
I0130 02:53:32.036595 139656649758464 logging_writer.py:48] [74800] global_step=74800, grad_norm=2.183634042739868, loss=1.605679988861084
I0130 02:54:06.069636 139656297445120 logging_writer.py:48] [74900] global_step=74900, grad_norm=1.9874948263168335, loss=1.4057358503341675
I0130 02:54:40.140634 139656649758464 logging_writer.py:48] [75000] global_step=75000, grad_norm=2.0130112171173096, loss=1.416614294052124
I0130 02:55:14.243764 139656297445120 logging_writer.py:48] [75100] global_step=75100, grad_norm=2.218229293823242, loss=1.4920239448547363
I0130 02:55:48.343312 139656649758464 logging_writer.py:48] [75200] global_step=75200, grad_norm=2.158531665802002, loss=1.4942471981048584
I0130 02:56:22.432632 139656297445120 logging_writer.py:48] [75300] global_step=75300, grad_norm=2.12675404548645, loss=1.5246164798736572
I0130 02:56:56.547543 139656649758464 logging_writer.py:48] [75400] global_step=75400, grad_norm=1.9875341653823853, loss=1.4813002347946167
I0130 02:57:30.659271 139656297445120 logging_writer.py:48] [75500] global_step=75500, grad_norm=2.233274221420288, loss=1.498865008354187
I0130 02:58:04.755307 139656649758464 logging_writer.py:48] [75600] global_step=75600, grad_norm=2.065066337585449, loss=1.645064353942871
I0130 02:58:38.982378 139656297445120 logging_writer.py:48] [75700] global_step=75700, grad_norm=2.0145621299743652, loss=1.5021090507507324
I0130 02:59:13.067816 139656649758464 logging_writer.py:48] [75800] global_step=75800, grad_norm=1.9675873517990112, loss=1.4886877536773682
I0130 02:59:47.141622 139656297445120 logging_writer.py:48] [75900] global_step=75900, grad_norm=2.3254406452178955, loss=1.477346658706665
I0130 03:00:21.222953 139656649758464 logging_writer.py:48] [76000] global_step=76000, grad_norm=2.1087074279785156, loss=1.5399565696716309
I0130 03:00:55.281343 139656297445120 logging_writer.py:48] [76100] global_step=76100, grad_norm=1.955910563468933, loss=1.450028657913208
I0130 03:01:29.363748 139656649758464 logging_writer.py:48] [76200] global_step=76200, grad_norm=1.997635006904602, loss=1.4324040412902832
I0130 03:01:59.473632 139822745589568 spec.py:321] Evaluating on the training split.
I0130 03:02:05.796292 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 03:02:14.175335 139822745589568 spec.py:349] Evaluating on the test split.
I0130 03:02:16.793778 139822745589568 submission_runner.py:408] Time since start: 26963.78s, 	Step: 76290, 	{'train/accuracy': 0.73441481590271, 'train/loss': 1.0014700889587402, 'validation/accuracy': 0.6627599596977234, 'validation/loss': 1.3964588642120361, 'validation/num_examples': 50000, 'test/accuracy': 0.5372000336647034, 'test/loss': 2.145915985107422, 'test/num_examples': 10000, 'score': 26050.24849486351, 'total_duration': 26963.783942461014, 'accumulated_submission_time': 26050.24849486351, 'accumulated_eval_time': 909.11328291893, 'accumulated_logging_time': 1.809826374053955}
I0130 03:02:16.827126 139656658151168 logging_writer.py:48] [76290] accumulated_eval_time=909.113283, accumulated_logging_time=1.809826, accumulated_submission_time=26050.248495, global_step=76290, preemption_count=0, score=26050.248495, test/accuracy=0.537200, test/loss=2.145916, test/num_examples=10000, total_duration=26963.783942, train/accuracy=0.734415, train/loss=1.001470, validation/accuracy=0.662760, validation/loss=1.396459, validation/num_examples=50000
I0130 03:02:20.556828 139656666543872 logging_writer.py:48] [76300] global_step=76300, grad_norm=2.23099684715271, loss=1.427367091178894
I0130 03:02:54.594078 139656658151168 logging_writer.py:48] [76400] global_step=76400, grad_norm=2.00221586227417, loss=1.531246304512024
I0130 03:03:28.649276 139656666543872 logging_writer.py:48] [76500] global_step=76500, grad_norm=2.0413978099823, loss=1.423020601272583
I0130 03:04:02.727613 139656658151168 logging_writer.py:48] [76600] global_step=76600, grad_norm=2.1281118392944336, loss=1.498153805732727
I0130 03:04:36.828348 139656666543872 logging_writer.py:48] [76700] global_step=76700, grad_norm=2.133054494857788, loss=1.5299382209777832
I0130 03:05:11.147080 139656658151168 logging_writer.py:48] [76800] global_step=76800, grad_norm=2.070716142654419, loss=1.3954195976257324
I0130 03:05:45.239530 139656666543872 logging_writer.py:48] [76900] global_step=76900, grad_norm=2.027975559234619, loss=1.4946801662445068
I0130 03:06:19.330947 139656658151168 logging_writer.py:48] [77000] global_step=77000, grad_norm=2.1554512977600098, loss=1.4636561870574951
I0130 03:06:53.416639 139656666543872 logging_writer.py:48] [77100] global_step=77100, grad_norm=2.068303346633911, loss=1.5130603313446045
I0130 03:07:27.510531 139656658151168 logging_writer.py:48] [77200] global_step=77200, grad_norm=2.0064055919647217, loss=1.5597792863845825
I0130 03:08:01.595626 139656666543872 logging_writer.py:48] [77300] global_step=77300, grad_norm=2.3108534812927246, loss=1.492651343345642
I0130 03:08:35.690180 139656658151168 logging_writer.py:48] [77400] global_step=77400, grad_norm=2.110513925552368, loss=1.5314106941223145
I0130 03:09:09.777004 139656666543872 logging_writer.py:48] [77500] global_step=77500, grad_norm=2.19258975982666, loss=1.4810585975646973
I0130 03:09:43.852212 139656658151168 logging_writer.py:48] [77600] global_step=77600, grad_norm=2.119553327560425, loss=1.4475617408752441
I0130 03:10:17.947109 139656666543872 logging_writer.py:48] [77700] global_step=77700, grad_norm=2.2634692192077637, loss=1.5634912252426147
I0130 03:10:47.086772 139822745589568 spec.py:321] Evaluating on the training split.
I0130 03:10:53.332353 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 03:11:01.686467 139822745589568 spec.py:349] Evaluating on the test split.
I0130 03:11:04.364841 139822745589568 submission_runner.py:408] Time since start: 27491.35s, 	Step: 77787, 	{'train/accuracy': 0.7419283986091614, 'train/loss': 0.9948947429656982, 'validation/accuracy': 0.668179988861084, 'validation/loss': 1.3509660959243774, 'validation/num_examples': 50000, 'test/accuracy': 0.5320000052452087, 'test/loss': 2.0675227642059326, 'test/num_examples': 10000, 'score': 26560.447185516357, 'total_duration': 27491.354991436005, 'accumulated_submission_time': 26560.447185516357, 'accumulated_eval_time': 926.3913035392761, 'accumulated_logging_time': 1.8542098999023438}
I0130 03:11:04.394508 139656297445120 logging_writer.py:48] [77787] accumulated_eval_time=926.391304, accumulated_logging_time=1.854210, accumulated_submission_time=26560.447186, global_step=77787, preemption_count=0, score=26560.447186, test/accuracy=0.532000, test/loss=2.067523, test/num_examples=10000, total_duration=27491.354991, train/accuracy=0.741928, train/loss=0.994895, validation/accuracy=0.668180, validation/loss=1.350966, validation/num_examples=50000
I0130 03:11:09.167227 139656649758464 logging_writer.py:48] [77800] global_step=77800, grad_norm=2.071218967437744, loss=1.4778004884719849
I0130 03:11:43.200980 139656297445120 logging_writer.py:48] [77900] global_step=77900, grad_norm=2.236687183380127, loss=1.4804829359054565
I0130 03:12:17.368638 139656649758464 logging_writer.py:48] [78000] global_step=78000, grad_norm=1.9682220220565796, loss=1.500382661819458
I0130 03:12:51.437299 139656297445120 logging_writer.py:48] [78100] global_step=78100, grad_norm=2.124284267425537, loss=1.4964442253112793
I0130 03:13:25.524041 139656649758464 logging_writer.py:48] [78200] global_step=78200, grad_norm=2.2542762756347656, loss=1.4044651985168457
I0130 03:13:59.603167 139656297445120 logging_writer.py:48] [78300] global_step=78300, grad_norm=2.0229814052581787, loss=1.4042342901229858
I0130 03:14:33.686367 139656649758464 logging_writer.py:48] [78400] global_step=78400, grad_norm=2.02142071723938, loss=1.4737205505371094
I0130 03:15:07.760458 139656297445120 logging_writer.py:48] [78500] global_step=78500, grad_norm=2.0327858924865723, loss=1.471128225326538
I0130 03:15:41.842140 139656649758464 logging_writer.py:48] [78600] global_step=78600, grad_norm=2.1917495727539062, loss=1.5026582479476929
I0130 03:16:15.932293 139656297445120 logging_writer.py:48] [78700] global_step=78700, grad_norm=2.0547382831573486, loss=1.3650494813919067
I0130 03:16:50.049418 139656649758464 logging_writer.py:48] [78800] global_step=78800, grad_norm=2.190786838531494, loss=1.4384628534317017
I0130 03:17:24.139516 139656297445120 logging_writer.py:48] [78900] global_step=78900, grad_norm=2.3183846473693848, loss=1.487584114074707
I0130 03:17:58.237126 139656649758464 logging_writer.py:48] [79000] global_step=79000, grad_norm=2.200300693511963, loss=1.5030680894851685
I0130 03:18:32.425662 139656297445120 logging_writer.py:48] [79100] global_step=79100, grad_norm=2.195570230484009, loss=1.4600578546524048
I0130 03:19:06.532802 139656649758464 logging_writer.py:48] [79200] global_step=79200, grad_norm=2.218029499053955, loss=1.597025990486145
I0130 03:19:34.619487 139822745589568 spec.py:321] Evaluating on the training split.
I0130 03:19:40.855711 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 03:19:49.469082 139822745589568 spec.py:349] Evaluating on the test split.
I0130 03:19:52.058507 139822745589568 submission_runner.py:408] Time since start: 28019.05s, 	Step: 79284, 	{'train/accuracy': 0.7411909699440002, 'train/loss': 0.993799090385437, 'validation/accuracy': 0.6715399622917175, 'validation/loss': 1.3349510431289673, 'validation/num_examples': 50000, 'test/accuracy': 0.5475000143051147, 'test/loss': 2.050487756729126, 'test/num_examples': 10000, 'score': 27070.60922384262, 'total_duration': 28019.048672914505, 'accumulated_submission_time': 27070.60922384262, 'accumulated_eval_time': 943.8302927017212, 'accumulated_logging_time': 1.894223690032959}
I0130 03:19:52.090244 139655592802048 logging_writer.py:48] [79284] accumulated_eval_time=943.830293, accumulated_logging_time=1.894224, accumulated_submission_time=27070.609224, global_step=79284, preemption_count=0, score=27070.609224, test/accuracy=0.547500, test/loss=2.050488, test/num_examples=10000, total_duration=28019.048673, train/accuracy=0.741191, train/loss=0.993799, validation/accuracy=0.671540, validation/loss=1.334951, validation/num_examples=50000
I0130 03:19:57.881696 139656658151168 logging_writer.py:48] [79300] global_step=79300, grad_norm=2.2253952026367188, loss=1.522764801979065
I0130 03:20:31.933099 139655592802048 logging_writer.py:48] [79400] global_step=79400, grad_norm=1.9930607080459595, loss=1.4154384136199951
I0130 03:21:05.970440 139656658151168 logging_writer.py:48] [79500] global_step=79500, grad_norm=2.0991902351379395, loss=1.4457507133483887
I0130 03:21:40.055290 139655592802048 logging_writer.py:48] [79600] global_step=79600, grad_norm=2.0845096111297607, loss=1.4436705112457275
I0130 03:22:14.146381 139656658151168 logging_writer.py:48] [79700] global_step=79700, grad_norm=1.9833141565322876, loss=1.3543095588684082
I0130 03:22:48.201336 139655592802048 logging_writer.py:48] [79800] global_step=79800, grad_norm=2.1836349964141846, loss=1.4225493669509888
I0130 03:23:22.295483 139656658151168 logging_writer.py:48] [79900] global_step=79900, grad_norm=2.0499284267425537, loss=1.375474452972412
I0130 03:23:56.358160 139655592802048 logging_writer.py:48] [80000] global_step=80000, grad_norm=1.9296494722366333, loss=1.4506862163543701
I0130 03:24:30.445288 139656658151168 logging_writer.py:48] [80100] global_step=80100, grad_norm=2.287970781326294, loss=1.5719091892242432
I0130 03:25:04.640690 139655592802048 logging_writer.py:48] [80200] global_step=80200, grad_norm=2.240530490875244, loss=1.4609652757644653
I0130 03:25:38.724149 139656658151168 logging_writer.py:48] [80300] global_step=80300, grad_norm=2.2924373149871826, loss=1.5398343801498413
I0130 03:26:12.817796 139655592802048 logging_writer.py:48] [80400] global_step=80400, grad_norm=2.1892592906951904, loss=1.4344462156295776
I0130 03:26:46.889780 139656658151168 logging_writer.py:48] [80500] global_step=80500, grad_norm=2.1412370204925537, loss=1.4969797134399414
I0130 03:27:20.990407 139655592802048 logging_writer.py:48] [80600] global_step=80600, grad_norm=2.1594059467315674, loss=1.4811408519744873
I0130 03:27:55.055871 139656658151168 logging_writer.py:48] [80700] global_step=80700, grad_norm=2.2823710441589355, loss=1.3585643768310547
I0130 03:28:22.114314 139822745589568 spec.py:321] Evaluating on the training split.
I0130 03:28:28.318249 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 03:28:36.873480 139822745589568 spec.py:349] Evaluating on the test split.
I0130 03:28:39.469612 139822745589568 submission_runner.py:408] Time since start: 28546.46s, 	Step: 80781, 	{'train/accuracy': 0.7743940949440002, 'train/loss': 0.8552932739257812, 'validation/accuracy': 0.6669399738311768, 'validation/loss': 1.3668314218521118, 'validation/num_examples': 50000, 'test/accuracy': 0.5409000515937805, 'test/loss': 2.0658974647521973, 'test/num_examples': 10000, 'score': 27580.572791814804, 'total_duration': 28546.45977640152, 'accumulated_submission_time': 27580.572791814804, 'accumulated_eval_time': 961.185555934906, 'accumulated_logging_time': 1.9354100227355957}
I0130 03:28:39.503170 139656297445120 logging_writer.py:48] [80781] accumulated_eval_time=961.185556, accumulated_logging_time=1.935410, accumulated_submission_time=27580.572792, global_step=80781, preemption_count=0, score=27580.572792, test/accuracy=0.540900, test/loss=2.065897, test/num_examples=10000, total_duration=28546.459776, train/accuracy=0.774394, train/loss=0.855293, validation/accuracy=0.666940, validation/loss=1.366831, validation/num_examples=50000
I0130 03:28:46.320845 139656649758464 logging_writer.py:48] [80800] global_step=80800, grad_norm=2.0223705768585205, loss=1.367780327796936
I0130 03:29:20.355110 139656297445120 logging_writer.py:48] [80900] global_step=80900, grad_norm=2.1183955669403076, loss=1.458431601524353
I0130 03:29:54.411911 139656649758464 logging_writer.py:48] [81000] global_step=81000, grad_norm=2.298442840576172, loss=1.4766461849212646
I0130 03:30:28.511297 139656297445120 logging_writer.py:48] [81100] global_step=81100, grad_norm=2.1155478954315186, loss=1.3653075695037842
I0130 03:31:02.623635 139656649758464 logging_writer.py:48] [81200] global_step=81200, grad_norm=2.11285138130188, loss=1.3187057971954346
I0130 03:31:36.801176 139656297445120 logging_writer.py:48] [81300] global_step=81300, grad_norm=2.3043458461761475, loss=1.4955110549926758
I0130 03:32:10.900195 139656649758464 logging_writer.py:48] [81400] global_step=81400, grad_norm=1.9930459260940552, loss=1.4963881969451904
I0130 03:32:45.012674 139656297445120 logging_writer.py:48] [81500] global_step=81500, grad_norm=2.2543249130249023, loss=1.56098210811615
I0130 03:33:19.102033 139656649758464 logging_writer.py:48] [81600] global_step=81600, grad_norm=2.254009485244751, loss=1.461242914199829
I0130 03:33:53.215218 139656297445120 logging_writer.py:48] [81700] global_step=81700, grad_norm=2.40083384513855, loss=1.4473316669464111
I0130 03:34:27.294888 139656649758464 logging_writer.py:48] [81800] global_step=81800, grad_norm=2.2539026737213135, loss=1.5232148170471191
I0130 03:35:01.385056 139656297445120 logging_writer.py:48] [81900] global_step=81900, grad_norm=2.3975751399993896, loss=1.4823776483535767
I0130 03:35:35.498775 139656649758464 logging_writer.py:48] [82000] global_step=82000, grad_norm=2.1799960136413574, loss=1.401645541191101
I0130 03:36:09.570929 139656297445120 logging_writer.py:48] [82100] global_step=82100, grad_norm=1.9776248931884766, loss=1.3754611015319824
I0130 03:36:43.622173 139656649758464 logging_writer.py:48] [82200] global_step=82200, grad_norm=2.1802473068237305, loss=1.4729877710342407
I0130 03:37:09.672319 139822745589568 spec.py:321] Evaluating on the training split.
I0130 03:37:15.992735 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 03:37:24.404678 139822745589568 spec.py:349] Evaluating on the test split.
I0130 03:37:27.026240 139822745589568 submission_runner.py:408] Time since start: 29074.02s, 	Step: 82278, 	{'train/accuracy': 0.7655652165412903, 'train/loss': 0.8683584332466125, 'validation/accuracy': 0.6740999817848206, 'validation/loss': 1.3325233459472656, 'validation/num_examples': 50000, 'test/accuracy': 0.5436000227928162, 'test/loss': 2.066192865371704, 'test/num_examples': 10000, 'score': 28090.680698633194, 'total_duration': 29074.016382932663, 'accumulated_submission_time': 28090.680698633194, 'accumulated_eval_time': 978.5394492149353, 'accumulated_logging_time': 1.9794235229492188}
I0130 03:37:27.059215 139655576016640 logging_writer.py:48] [82278] accumulated_eval_time=978.539449, accumulated_logging_time=1.979424, accumulated_submission_time=28090.680699, global_step=82278, preemption_count=0, score=28090.680699, test/accuracy=0.543600, test/loss=2.066193, test/num_examples=10000, total_duration=29074.016383, train/accuracy=0.765565, train/loss=0.868358, validation/accuracy=0.674100, validation/loss=1.332523, validation/num_examples=50000
I0130 03:37:34.888326 139655584409344 logging_writer.py:48] [82300] global_step=82300, grad_norm=2.231290340423584, loss=1.3984835147857666
I0130 03:38:09.017727 139655576016640 logging_writer.py:48] [82400] global_step=82400, grad_norm=2.3240184783935547, loss=1.4679008722305298
I0130 03:38:43.073296 139655584409344 logging_writer.py:48] [82500] global_step=82500, grad_norm=2.065934896469116, loss=1.5123693943023682
I0130 03:39:17.144979 139655576016640 logging_writer.py:48] [82600] global_step=82600, grad_norm=2.071014404296875, loss=1.4605212211608887
I0130 03:39:51.230417 139655584409344 logging_writer.py:48] [82700] global_step=82700, grad_norm=2.4285686016082764, loss=1.5018905401229858
I0130 03:40:25.300734 139655576016640 logging_writer.py:48] [82800] global_step=82800, grad_norm=2.333402156829834, loss=1.4193741083145142
I0130 03:40:59.339989 139655584409344 logging_writer.py:48] [82900] global_step=82900, grad_norm=2.317492723464966, loss=1.536731481552124
I0130 03:41:33.401294 139655576016640 logging_writer.py:48] [83000] global_step=83000, grad_norm=2.236724615097046, loss=1.4626580476760864
I0130 03:42:07.486782 139655584409344 logging_writer.py:48] [83100] global_step=83100, grad_norm=2.201258659362793, loss=1.4656741619110107
I0130 03:42:41.534806 139655576016640 logging_writer.py:48] [83200] global_step=83200, grad_norm=2.0117955207824707, loss=1.3073230981826782
I0130 03:43:15.619891 139655584409344 logging_writer.py:48] [83300] global_step=83300, grad_norm=2.531399965286255, loss=1.5385856628417969
I0130 03:43:49.719070 139655576016640 logging_writer.py:48] [83400] global_step=83400, grad_norm=2.1511871814727783, loss=1.405747413635254
I0130 03:44:23.816728 139655584409344 logging_writer.py:48] [83500] global_step=83500, grad_norm=2.335052251815796, loss=1.4854371547698975
I0130 03:44:57.988033 139655576016640 logging_writer.py:48] [83600] global_step=83600, grad_norm=2.318822145462036, loss=1.4380457401275635
I0130 03:45:32.091706 139655584409344 logging_writer.py:48] [83700] global_step=83700, grad_norm=2.1981728076934814, loss=1.5883691310882568
I0130 03:45:57.133363 139822745589568 spec.py:321] Evaluating on the training split.
I0130 03:46:03.479192 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 03:46:12.163892 139822745589568 spec.py:349] Evaluating on the test split.
I0130 03:46:14.746442 139822745589568 submission_runner.py:408] Time since start: 29601.74s, 	Step: 83775, 	{'train/accuracy': 0.7556401491165161, 'train/loss': 0.9285488128662109, 'validation/accuracy': 0.6721199750900269, 'validation/loss': 1.338887095451355, 'validation/num_examples': 50000, 'test/accuracy': 0.5362000465393066, 'test/loss': 2.076328754425049, 'test/num_examples': 10000, 'score': 28600.69172692299, 'total_duration': 29601.736602783203, 'accumulated_submission_time': 28600.69172692299, 'accumulated_eval_time': 996.1524906158447, 'accumulated_logging_time': 2.0239923000335693}
I0130 03:46:14.782243 139656834316032 logging_writer.py:48] [83775] accumulated_eval_time=996.152491, accumulated_logging_time=2.023992, accumulated_submission_time=28600.691727, global_step=83775, preemption_count=0, score=28600.691727, test/accuracy=0.536200, test/loss=2.076329, test/num_examples=10000, total_duration=29601.736603, train/accuracy=0.755640, train/loss=0.928549, validation/accuracy=0.672120, validation/loss=1.338887, validation/num_examples=50000
I0130 03:46:23.647737 139658730145536 logging_writer.py:48] [83800] global_step=83800, grad_norm=2.2577834129333496, loss=1.573938250541687
I0130 03:46:57.702270 139656834316032 logging_writer.py:48] [83900] global_step=83900, grad_norm=2.2351796627044678, loss=1.4637877941131592
I0130 03:47:31.778893 139658730145536 logging_writer.py:48] [84000] global_step=84000, grad_norm=2.4616472721099854, loss=1.538333773612976
I0130 03:48:05.868768 139656834316032 logging_writer.py:48] [84100] global_step=84100, grad_norm=2.1788878440856934, loss=1.539582371711731
I0130 03:48:39.971676 139658730145536 logging_writer.py:48] [84200] global_step=84200, grad_norm=2.159029006958008, loss=1.467828392982483
I0130 03:49:14.066366 139656834316032 logging_writer.py:48] [84300] global_step=84300, grad_norm=2.379359006881714, loss=1.4596025943756104
I0130 03:49:48.152068 139658730145536 logging_writer.py:48] [84400] global_step=84400, grad_norm=2.20060658454895, loss=1.4941933155059814
I0130 03:50:22.248068 139656834316032 logging_writer.py:48] [84500] global_step=84500, grad_norm=2.1974523067474365, loss=1.4662933349609375
I0130 03:50:56.334290 139658730145536 logging_writer.py:48] [84600] global_step=84600, grad_norm=2.5283432006835938, loss=1.455924153327942
I0130 03:51:30.510923 139656834316032 logging_writer.py:48] [84700] global_step=84700, grad_norm=2.247204303741455, loss=1.5028128623962402
I0130 03:52:04.605693 139658730145536 logging_writer.py:48] [84800] global_step=84800, grad_norm=2.217989206314087, loss=1.4061663150787354
I0130 03:52:38.686134 139656834316032 logging_writer.py:48] [84900] global_step=84900, grad_norm=2.1675384044647217, loss=1.4714696407318115
I0130 03:53:12.752440 139658730145536 logging_writer.py:48] [85000] global_step=85000, grad_norm=2.2208385467529297, loss=1.4518461227416992
I0130 03:53:46.821983 139656834316032 logging_writer.py:48] [85100] global_step=85100, grad_norm=2.1844301223754883, loss=1.531973958015442
I0130 03:54:20.859576 139658730145536 logging_writer.py:48] [85200] global_step=85200, grad_norm=2.0703139305114746, loss=1.3998923301696777
I0130 03:54:44.846185 139822745589568 spec.py:321] Evaluating on the training split.
I0130 03:54:51.047289 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 03:54:59.667479 139822745589568 spec.py:349] Evaluating on the test split.
I0130 03:55:02.322572 139822745589568 submission_runner.py:408] Time since start: 30129.31s, 	Step: 85272, 	{'train/accuracy': 0.7602040767669678, 'train/loss': 0.8968546390533447, 'validation/accuracy': 0.6789399981498718, 'validation/loss': 1.307782769203186, 'validation/num_examples': 50000, 'test/accuracy': 0.5489000082015991, 'test/loss': 2.041975259780884, 'test/num_examples': 10000, 'score': 29110.69561982155, 'total_duration': 30129.31273341179, 'accumulated_submission_time': 29110.69561982155, 'accumulated_eval_time': 1013.6288385391235, 'accumulated_logging_time': 2.0686020851135254}
I0130 03:55:02.355466 139655584409344 logging_writer.py:48] [85272] accumulated_eval_time=1013.628839, accumulated_logging_time=2.068602, accumulated_submission_time=29110.695620, global_step=85272, preemption_count=0, score=29110.695620, test/accuracy=0.548900, test/loss=2.041975, test/num_examples=10000, total_duration=30129.312733, train/accuracy=0.760204, train/loss=0.896855, validation/accuracy=0.678940, validation/loss=1.307783, validation/num_examples=50000
I0130 03:55:12.221387 139655592802048 logging_writer.py:48] [85300] global_step=85300, grad_norm=2.246467113494873, loss=1.4420371055603027
I0130 03:55:46.213707 139655584409344 logging_writer.py:48] [85400] global_step=85400, grad_norm=2.199658155441284, loss=1.4013895988464355
I0130 03:56:20.241147 139655592802048 logging_writer.py:48] [85500] global_step=85500, grad_norm=2.0474371910095215, loss=1.4088681936264038
I0130 03:56:54.295796 139655584409344 logging_writer.py:48] [85600] global_step=85600, grad_norm=2.2498385906219482, loss=1.3331565856933594
I0130 03:57:28.369023 139655592802048 logging_writer.py:48] [85700] global_step=85700, grad_norm=2.319711923599243, loss=1.422252893447876
I0130 03:58:02.513797 139655584409344 logging_writer.py:48] [85800] global_step=85800, grad_norm=2.2098002433776855, loss=1.4835604429244995
I0130 03:58:36.582775 139655592802048 logging_writer.py:48] [85900] global_step=85900, grad_norm=2.4230422973632812, loss=1.4444875717163086
I0130 03:59:10.657925 139655584409344 logging_writer.py:48] [86000] global_step=86000, grad_norm=2.355119466781616, loss=1.4097925424575806
I0130 03:59:44.736841 139655592802048 logging_writer.py:48] [86100] global_step=86100, grad_norm=2.0656518936157227, loss=1.4995946884155273
I0130 04:00:18.807852 139655584409344 logging_writer.py:48] [86200] global_step=86200, grad_norm=2.1742279529571533, loss=1.4453611373901367
I0130 04:00:52.905004 139655592802048 logging_writer.py:48] [86300] global_step=86300, grad_norm=2.1905617713928223, loss=1.3143001794815063
I0130 04:01:27.008107 139655584409344 logging_writer.py:48] [86400] global_step=86400, grad_norm=2.345878839492798, loss=1.4696828126907349
I0130 04:02:01.094385 139655592802048 logging_writer.py:48] [86500] global_step=86500, grad_norm=2.06478214263916, loss=1.381131649017334
I0130 04:02:35.199527 139655584409344 logging_writer.py:48] [86600] global_step=86600, grad_norm=2.1605851650238037, loss=1.4122166633605957
I0130 04:03:09.302838 139655592802048 logging_writer.py:48] [86700] global_step=86700, grad_norm=2.3435158729553223, loss=1.5280293226242065
I0130 04:03:32.616022 139822745589568 spec.py:321] Evaluating on the training split.
I0130 04:03:38.867279 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 04:03:47.494960 139822745589568 spec.py:349] Evaluating on the test split.
I0130 04:03:50.126992 139822745589568 submission_runner.py:408] Time since start: 30657.12s, 	Step: 86770, 	{'train/accuracy': 0.7505978941917419, 'train/loss': 0.9393353462219238, 'validation/accuracy': 0.6771399974822998, 'validation/loss': 1.3065553903579712, 'validation/num_examples': 50000, 'test/accuracy': 0.5494000315666199, 'test/loss': 2.0108368396759033, 'test/num_examples': 10000, 'score': 29620.89577460289, 'total_duration': 30657.11715722084, 'accumulated_submission_time': 29620.89577460289, 'accumulated_eval_time': 1031.1397771835327, 'accumulated_logging_time': 2.111499547958374}
I0130 04:03:50.165884 139655576016640 logging_writer.py:48] [86770] accumulated_eval_time=1031.139777, accumulated_logging_time=2.111500, accumulated_submission_time=29620.895775, global_step=86770, preemption_count=0, score=29620.895775, test/accuracy=0.549400, test/loss=2.010837, test/num_examples=10000, total_duration=30657.117157, train/accuracy=0.750598, train/loss=0.939335, validation/accuracy=0.677140, validation/loss=1.306555, validation/num_examples=50000
I0130 04:04:00.728104 139655584409344 logging_writer.py:48] [86800] global_step=86800, grad_norm=2.2688846588134766, loss=1.339301586151123
I0130 04:04:34.765211 139655576016640 logging_writer.py:48] [86900] global_step=86900, grad_norm=2.042027711868286, loss=1.4576729536056519
I0130 04:05:08.963173 139655584409344 logging_writer.py:48] [87000] global_step=87000, grad_norm=2.215256929397583, loss=1.4214282035827637
I0130 04:05:43.038633 139655576016640 logging_writer.py:48] [87100] global_step=87100, grad_norm=2.2172281742095947, loss=1.4117604494094849
I0130 04:06:17.150652 139655584409344 logging_writer.py:48] [87200] global_step=87200, grad_norm=2.274136781692505, loss=1.3880029916763306
I0130 04:06:51.203884 139655576016640 logging_writer.py:48] [87300] global_step=87300, grad_norm=2.09309720993042, loss=1.3296934366226196
I0130 04:07:25.278909 139655584409344 logging_writer.py:48] [87400] global_step=87400, grad_norm=2.237424373626709, loss=1.503722071647644
I0130 04:07:59.355650 139655576016640 logging_writer.py:48] [87500] global_step=87500, grad_norm=2.205103874206543, loss=1.3649128675460815
I0130 04:08:33.429800 139655584409344 logging_writer.py:48] [87600] global_step=87600, grad_norm=2.2697689533233643, loss=1.4627487659454346
I0130 04:09:07.501254 139655576016640 logging_writer.py:48] [87700] global_step=87700, grad_norm=2.194683790206909, loss=1.4036738872528076
I0130 04:09:41.560904 139655584409344 logging_writer.py:48] [87800] global_step=87800, grad_norm=2.2034223079681396, loss=1.3554329872131348
I0130 04:10:15.638236 139655576016640 logging_writer.py:48] [87900] global_step=87900, grad_norm=2.3182754516601562, loss=1.3409712314605713
I0130 04:10:49.693879 139655584409344 logging_writer.py:48] [88000] global_step=88000, grad_norm=2.174699544906616, loss=1.3311976194381714
I0130 04:11:23.840880 139655576016640 logging_writer.py:48] [88100] global_step=88100, grad_norm=2.1218485832214355, loss=1.3240797519683838
I0130 04:11:57.951719 139655584409344 logging_writer.py:48] [88200] global_step=88200, grad_norm=2.3250186443328857, loss=1.38675856590271
I0130 04:12:20.263249 139822745589568 spec.py:321] Evaluating on the training split.
I0130 04:12:26.586694 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 04:12:35.061197 139822745589568 spec.py:349] Evaluating on the test split.
I0130 04:12:37.731093 139822745589568 submission_runner.py:408] Time since start: 31184.72s, 	Step: 88267, 	{'train/accuracy': 0.7596260905265808, 'train/loss': 0.9097063541412354, 'validation/accuracy': 0.6830799579620361, 'validation/loss': 1.2865016460418701, 'validation/num_examples': 50000, 'test/accuracy': 0.5533000230789185, 'test/loss': 2.0117292404174805, 'test/num_examples': 10000, 'score': 30130.933844089508, 'total_duration': 31184.721259593964, 'accumulated_submission_time': 30130.933844089508, 'accumulated_eval_time': 1048.6075825691223, 'accumulated_logging_time': 2.1597437858581543}
I0130 04:12:37.768940 139656649758464 logging_writer.py:48] [88267] accumulated_eval_time=1048.607583, accumulated_logging_time=2.159744, accumulated_submission_time=30130.933844, global_step=88267, preemption_count=0, score=30130.933844, test/accuracy=0.553300, test/loss=2.011729, test/num_examples=10000, total_duration=31184.721260, train/accuracy=0.759626, train/loss=0.909706, validation/accuracy=0.683080, validation/loss=1.286502, validation/num_examples=50000
I0130 04:12:49.357775 139656666543872 logging_writer.py:48] [88300] global_step=88300, grad_norm=2.0714762210845947, loss=1.3706591129302979
I0130 04:13:23.411506 139656649758464 logging_writer.py:48] [88400] global_step=88400, grad_norm=2.222086191177368, loss=1.4485667943954468
I0130 04:13:57.450170 139656666543872 logging_writer.py:48] [88500] global_step=88500, grad_norm=2.1701180934906006, loss=1.425889253616333
I0130 04:14:31.521410 139656649758464 logging_writer.py:48] [88600] global_step=88600, grad_norm=2.203355073928833, loss=1.4323406219482422
I0130 04:15:05.601958 139656666543872 logging_writer.py:48] [88700] global_step=88700, grad_norm=2.2271361351013184, loss=1.4074277877807617
I0130 04:15:39.709364 139656649758464 logging_writer.py:48] [88800] global_step=88800, grad_norm=2.08109712600708, loss=1.27458655834198
I0130 04:16:13.820411 139656666543872 logging_writer.py:48] [88900] global_step=88900, grad_norm=2.4096314907073975, loss=1.4609768390655518
I0130 04:16:47.923475 139656649758464 logging_writer.py:48] [89000] global_step=89000, grad_norm=2.2420663833618164, loss=1.378749132156372
I0130 04:17:22.012392 139656666543872 logging_writer.py:48] [89100] global_step=89100, grad_norm=2.2223823070526123, loss=1.3311920166015625
I0130 04:17:56.184622 139656649758464 logging_writer.py:48] [89200] global_step=89200, grad_norm=2.2710907459259033, loss=1.404219388961792
I0130 04:18:30.262335 139656666543872 logging_writer.py:48] [89300] global_step=89300, grad_norm=2.1604161262512207, loss=1.3852252960205078
I0130 04:19:04.352305 139656649758464 logging_writer.py:48] [89400] global_step=89400, grad_norm=2.080472230911255, loss=1.296642541885376
I0130 04:19:38.449016 139656666543872 logging_writer.py:48] [89500] global_step=89500, grad_norm=2.4418375492095947, loss=1.305113434791565
I0130 04:20:12.552737 139656649758464 logging_writer.py:48] [89600] global_step=89600, grad_norm=2.2683145999908447, loss=1.3553861379623413
I0130 04:20:46.644458 139656666543872 logging_writer.py:48] [89700] global_step=89700, grad_norm=2.2524454593658447, loss=1.2985846996307373
I0130 04:21:07.948194 139822745589568 spec.py:321] Evaluating on the training split.
I0130 04:21:14.403841 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 04:21:22.682726 139822745589568 spec.py:349] Evaluating on the test split.
I0130 04:21:25.222163 139822745589568 submission_runner.py:408] Time since start: 31712.21s, 	Step: 89764, 	{'train/accuracy': 0.7589086294174194, 'train/loss': 0.9112529158592224, 'validation/accuracy': 0.682379961013794, 'validation/loss': 1.2872778177261353, 'validation/num_examples': 50000, 'test/accuracy': 0.5552000403404236, 'test/loss': 2.016308069229126, 'test/num_examples': 10000, 'score': 30641.0526971817, 'total_duration': 31712.21232533455, 'accumulated_submission_time': 30641.0526971817, 'accumulated_eval_time': 1065.8815150260925, 'accumulated_logging_time': 2.206571578979492}
I0130 04:21:25.256603 139655592802048 logging_writer.py:48] [89764] accumulated_eval_time=1065.881515, accumulated_logging_time=2.206572, accumulated_submission_time=30641.052697, global_step=89764, preemption_count=0, score=30641.052697, test/accuracy=0.555200, test/loss=2.016308, test/num_examples=10000, total_duration=31712.212325, train/accuracy=0.758909, train/loss=0.911253, validation/accuracy=0.682380, validation/loss=1.287278, validation/num_examples=50000
I0130 04:21:37.869852 139656297445120 logging_writer.py:48] [89800] global_step=89800, grad_norm=2.499924421310425, loss=1.3872287273406982
I0130 04:22:11.910736 139655592802048 logging_writer.py:48] [89900] global_step=89900, grad_norm=2.4602603912353516, loss=1.491554617881775
I0130 04:22:45.985751 139656297445120 logging_writer.py:48] [90000] global_step=90000, grad_norm=2.1302969455718994, loss=1.370526909828186
I0130 04:23:20.067271 139655592802048 logging_writer.py:48] [90100] global_step=90100, grad_norm=2.1732177734375, loss=1.4641015529632568
I0130 04:23:54.148559 139656297445120 logging_writer.py:48] [90200] global_step=90200, grad_norm=2.1222214698791504, loss=1.3721147775650024
I0130 04:24:28.427850 139655592802048 logging_writer.py:48] [90300] global_step=90300, grad_norm=2.2690932750701904, loss=1.4254788160324097
I0130 04:25:02.533046 139656297445120 logging_writer.py:48] [90400] global_step=90400, grad_norm=2.417287588119507, loss=1.4048781394958496
I0130 04:25:36.631410 139655592802048 logging_writer.py:48] [90500] global_step=90500, grad_norm=2.150991439819336, loss=1.3486920595169067
I0130 04:26:10.713479 139656297445120 logging_writer.py:48] [90600] global_step=90600, grad_norm=2.086582899093628, loss=1.3934983015060425
I0130 04:26:44.793612 139655592802048 logging_writer.py:48] [90700] global_step=90700, grad_norm=2.315695285797119, loss=1.5273956060409546
I0130 04:27:18.849365 139656297445120 logging_writer.py:48] [90800] global_step=90800, grad_norm=2.381174087524414, loss=1.4449130296707153
I0130 04:27:52.943284 139655592802048 logging_writer.py:48] [90900] global_step=90900, grad_norm=2.473031997680664, loss=1.345334768295288
I0130 04:28:27.026330 139656297445120 logging_writer.py:48] [91000] global_step=91000, grad_norm=2.372603178024292, loss=1.4689109325408936
I0130 04:29:01.119827 139655592802048 logging_writer.py:48] [91100] global_step=91100, grad_norm=2.298332691192627, loss=1.440345048904419
I0130 04:29:35.196665 139656297445120 logging_writer.py:48] [91200] global_step=91200, grad_norm=2.586101531982422, loss=1.3547309637069702
I0130 04:29:55.466460 139822745589568 spec.py:321] Evaluating on the training split.
I0130 04:30:01.825351 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 04:30:10.190324 139822745589568 spec.py:349] Evaluating on the test split.
I0130 04:30:12.775378 139822745589568 submission_runner.py:408] Time since start: 32239.77s, 	Step: 91261, 	{'train/accuracy': 0.7894411683082581, 'train/loss': 0.784350574016571, 'validation/accuracy': 0.6829000115394592, 'validation/loss': 1.2923929691314697, 'validation/num_examples': 50000, 'test/accuracy': 0.5560000538825989, 'test/loss': 2.0203568935394287, 'test/num_examples': 10000, 'score': 31151.199570178986, 'total_duration': 32239.76553440094, 'accumulated_submission_time': 31151.199570178986, 'accumulated_eval_time': 1083.1903893947601, 'accumulated_logging_time': 2.25357723236084}
I0130 04:30:12.813874 139656658151168 logging_writer.py:48] [91261] accumulated_eval_time=1083.190389, accumulated_logging_time=2.253577, accumulated_submission_time=31151.199570, global_step=91261, preemption_count=0, score=31151.199570, test/accuracy=0.556000, test/loss=2.020357, test/num_examples=10000, total_duration=32239.765534, train/accuracy=0.789441, train/loss=0.784351, validation/accuracy=0.682900, validation/loss=1.292393, validation/num_examples=50000
I0130 04:30:26.459441 139656666543872 logging_writer.py:48] [91300] global_step=91300, grad_norm=2.261854887008667, loss=1.3538620471954346
I0130 04:31:00.608607 139656658151168 logging_writer.py:48] [91400] global_step=91400, grad_norm=2.285235643386841, loss=1.3589396476745605
I0130 04:31:34.683046 139656666543872 logging_writer.py:48] [91500] global_step=91500, grad_norm=2.227355480194092, loss=1.328848958015442
I0130 04:32:08.778815 139656658151168 logging_writer.py:48] [91600] global_step=91600, grad_norm=2.4374685287475586, loss=1.3862078189849854
I0130 04:32:42.846559 139656666543872 logging_writer.py:48] [91700] global_step=91700, grad_norm=2.30499529838562, loss=1.3626948595046997
I0130 04:33:16.921094 139656658151168 logging_writer.py:48] [91800] global_step=91800, grad_norm=2.3039188385009766, loss=1.4699817895889282
I0130 04:33:51.028098 139656666543872 logging_writer.py:48] [91900] global_step=91900, grad_norm=2.268217086791992, loss=1.3817906379699707
I0130 04:34:25.111938 139656658151168 logging_writer.py:48] [92000] global_step=92000, grad_norm=2.293031692504883, loss=1.4348808526992798
I0130 04:34:59.215657 139656666543872 logging_writer.py:48] [92100] global_step=92100, grad_norm=2.4974238872528076, loss=1.5391786098480225
I0130 04:35:33.304774 139656658151168 logging_writer.py:48] [92200] global_step=92200, grad_norm=2.1497957706451416, loss=1.3733564615249634
I0130 04:36:07.410702 139656666543872 logging_writer.py:48] [92300] global_step=92300, grad_norm=2.186053514480591, loss=1.2865813970565796
I0130 04:36:41.507724 139656658151168 logging_writer.py:48] [92400] global_step=92400, grad_norm=2.376661539077759, loss=1.3982475996017456
I0130 04:37:15.612574 139656666543872 logging_writer.py:48] [92500] global_step=92500, grad_norm=2.114786386489868, loss=1.339706301689148
I0130 04:37:49.896379 139656658151168 logging_writer.py:48] [92600] global_step=92600, grad_norm=2.158024549484253, loss=1.350477695465088
I0130 04:38:23.976289 139656666543872 logging_writer.py:48] [92700] global_step=92700, grad_norm=2.7026102542877197, loss=1.3562811613082886
I0130 04:38:42.864065 139822745589568 spec.py:321] Evaluating on the training split.
I0130 04:38:49.100300 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 04:38:57.523189 139822745589568 spec.py:349] Evaluating on the test split.
I0130 04:39:00.145264 139822745589568 submission_runner.py:408] Time since start: 32767.14s, 	Step: 92757, 	{'train/accuracy': 0.7609016299247742, 'train/loss': 0.8901998996734619, 'validation/accuracy': 0.6728399991989136, 'validation/loss': 1.3282952308654785, 'validation/num_examples': 50000, 'test/accuracy': 0.5391000509262085, 'test/loss': 2.09073805809021, 'test/num_examples': 10000, 'score': 31661.18835258484, 'total_duration': 32767.13542819023, 'accumulated_submission_time': 31661.18835258484, 'accumulated_eval_time': 1100.471552848816, 'accumulated_logging_time': 2.3035874366760254}
I0130 04:39:00.180156 139655584409344 logging_writer.py:48] [92757] accumulated_eval_time=1100.471553, accumulated_logging_time=2.303587, accumulated_submission_time=31661.188353, global_step=92757, preemption_count=0, score=31661.188353, test/accuracy=0.539100, test/loss=2.090738, test/num_examples=10000, total_duration=32767.135428, train/accuracy=0.760902, train/loss=0.890200, validation/accuracy=0.672840, validation/loss=1.328295, validation/num_examples=50000
I0130 04:39:15.189332 139655592802048 logging_writer.py:48] [92800] global_step=92800, grad_norm=2.2793195247650146, loss=1.4109985828399658
I0130 04:39:49.229336 139655584409344 logging_writer.py:48] [92900] global_step=92900, grad_norm=2.3608670234680176, loss=1.4439098834991455
I0130 04:40:23.293730 139655592802048 logging_writer.py:48] [93000] global_step=93000, grad_norm=2.4381792545318604, loss=1.3690824508666992
I0130 04:40:57.392199 139655584409344 logging_writer.py:48] [93100] global_step=93100, grad_norm=2.2563297748565674, loss=1.409660816192627
I0130 04:41:31.488867 139655592802048 logging_writer.py:48] [93200] global_step=93200, grad_norm=2.361912965774536, loss=1.3556416034698486
I0130 04:42:05.572192 139655584409344 logging_writer.py:48] [93300] global_step=93300, grad_norm=2.275790214538574, loss=1.4502207040786743
I0130 04:42:39.679030 139655592802048 logging_writer.py:48] [93400] global_step=93400, grad_norm=2.4754526615142822, loss=1.2786048650741577
I0130 04:43:13.762841 139655584409344 logging_writer.py:48] [93500] global_step=93500, grad_norm=2.3004560470581055, loss=1.4039504528045654
I0130 04:43:47.817690 139655592802048 logging_writer.py:48] [93600] global_step=93600, grad_norm=2.2338409423828125, loss=1.3488285541534424
I0130 04:44:21.976732 139655584409344 logging_writer.py:48] [93700] global_step=93700, grad_norm=2.446714401245117, loss=1.454564094543457
I0130 04:44:56.044221 139655592802048 logging_writer.py:48] [93800] global_step=93800, grad_norm=2.2699642181396484, loss=1.4786956310272217
I0130 04:45:30.131419 139655584409344 logging_writer.py:48] [93900] global_step=93900, grad_norm=2.2883870601654053, loss=1.3544944524765015
I0130 04:46:04.234706 139655592802048 logging_writer.py:48] [94000] global_step=94000, grad_norm=2.8092265129089355, loss=1.394517183303833
I0130 04:46:38.326417 139655584409344 logging_writer.py:48] [94100] global_step=94100, grad_norm=2.2412586212158203, loss=1.3591375350952148
I0130 04:47:12.412803 139655592802048 logging_writer.py:48] [94200] global_step=94200, grad_norm=2.427241325378418, loss=1.497811198234558
I0130 04:47:30.288291 139822745589568 spec.py:321] Evaluating on the training split.
I0130 04:47:36.518016 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 04:47:44.935304 139822745589568 spec.py:349] Evaluating on the test split.
I0130 04:47:47.540712 139822745589568 submission_runner.py:408] Time since start: 33294.53s, 	Step: 94254, 	{'train/accuracy': 0.7751315236091614, 'train/loss': 0.8435813784599304, 'validation/accuracy': 0.6869999766349792, 'validation/loss': 1.2639058828353882, 'validation/num_examples': 50000, 'test/accuracy': 0.5612000226974487, 'test/loss': 1.9653502702713013, 'test/num_examples': 10000, 'score': 32171.235827207565, 'total_duration': 33294.53087544441, 'accumulated_submission_time': 32171.235827207565, 'accumulated_eval_time': 1117.723935842514, 'accumulated_logging_time': 2.349766731262207}
I0130 04:47:47.579530 139656666543872 logging_writer.py:48] [94254] accumulated_eval_time=1117.723936, accumulated_logging_time=2.349767, accumulated_submission_time=32171.235827, global_step=94254, preemption_count=0, score=32171.235827, test/accuracy=0.561200, test/loss=1.965350, test/num_examples=10000, total_duration=33294.530875, train/accuracy=0.775132, train/loss=0.843581, validation/accuracy=0.687000, validation/loss=1.263906, validation/num_examples=50000
I0130 04:48:03.606106 139656834316032 logging_writer.py:48] [94300] global_step=94300, grad_norm=2.643864870071411, loss=1.4327677488327026
I0130 04:48:37.647583 139656666543872 logging_writer.py:48] [94400] global_step=94400, grad_norm=2.3340518474578857, loss=1.2777528762817383
I0130 04:49:11.713383 139656834316032 logging_writer.py:48] [94500] global_step=94500, grad_norm=2.5004591941833496, loss=1.4809467792510986
I0130 04:49:45.815022 139656666543872 logging_writer.py:48] [94600] global_step=94600, grad_norm=2.288810968399048, loss=1.3341877460479736
I0130 04:50:19.923784 139656834316032 logging_writer.py:48] [94700] global_step=94700, grad_norm=2.2671701908111572, loss=1.2965866327285767
I0130 04:50:54.092823 139656666543872 logging_writer.py:48] [94800] global_step=94800, grad_norm=2.2275187969207764, loss=1.3666787147521973
I0130 04:51:28.189799 139656834316032 logging_writer.py:48] [94900] global_step=94900, grad_norm=2.5402607917785645, loss=1.4028280973434448
I0130 04:52:02.269430 139656666543872 logging_writer.py:48] [95000] global_step=95000, grad_norm=2.1339516639709473, loss=1.3368549346923828
I0130 04:52:36.341423 139656834316032 logging_writer.py:48] [95100] global_step=95100, grad_norm=2.229585647583008, loss=1.3524794578552246
I0130 04:53:10.429898 139656666543872 logging_writer.py:48] [95200] global_step=95200, grad_norm=2.4203059673309326, loss=1.3031671047210693
I0130 04:53:44.517782 139656834316032 logging_writer.py:48] [95300] global_step=95300, grad_norm=2.298396348953247, loss=1.326669692993164
I0130 04:54:18.591534 139656666543872 logging_writer.py:48] [95400] global_step=95400, grad_norm=2.349271297454834, loss=1.3971073627471924
I0130 04:54:52.641818 139656834316032 logging_writer.py:48] [95500] global_step=95500, grad_norm=2.455448865890503, loss=1.4852409362792969
I0130 04:55:26.681174 139656666543872 logging_writer.py:48] [95600] global_step=95600, grad_norm=2.404815196990967, loss=1.3434522151947021
I0130 04:56:00.735478 139656834316032 logging_writer.py:48] [95700] global_step=95700, grad_norm=2.4186856746673584, loss=1.4163215160369873
I0130 04:56:17.590341 139822745589568 spec.py:321] Evaluating on the training split.
I0130 04:56:23.907849 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 04:56:32.543304 139822745589568 spec.py:349] Evaluating on the test split.
I0130 04:56:35.063264 139822745589568 submission_runner.py:408] Time since start: 33822.05s, 	Step: 95751, 	{'train/accuracy': 0.7679567933082581, 'train/loss': 0.8627640008926392, 'validation/accuracy': 0.681659996509552, 'validation/loss': 1.2954697608947754, 'validation/num_examples': 50000, 'test/accuracy': 0.555400013923645, 'test/loss': 2.016524314880371, 'test/num_examples': 10000, 'score': 32681.184484004974, 'total_duration': 33822.053425073624, 'accumulated_submission_time': 32681.184484004974, 'accumulated_eval_time': 1135.1968188285828, 'accumulated_logging_time': 2.4002671241760254}
I0130 04:56:35.102984 139656658151168 logging_writer.py:48] [95751] accumulated_eval_time=1135.196819, accumulated_logging_time=2.400267, accumulated_submission_time=32681.184484, global_step=95751, preemption_count=0, score=32681.184484, test/accuracy=0.555400, test/loss=2.016524, test/num_examples=10000, total_duration=33822.053425, train/accuracy=0.767957, train/loss=0.862764, validation/accuracy=0.681660, validation/loss=1.295470, validation/num_examples=50000
I0130 04:56:52.125862 139658730145536 logging_writer.py:48] [95800] global_step=95800, grad_norm=2.4506688117980957, loss=1.3816735744476318
I0130 04:57:26.179602 139656658151168 logging_writer.py:48] [95900] global_step=95900, grad_norm=2.409942388534546, loss=1.4867827892303467
I0130 04:58:00.340780 139658730145536 logging_writer.py:48] [96000] global_step=96000, grad_norm=2.6037604808807373, loss=1.389793872833252
I0130 04:58:34.436414 139656658151168 logging_writer.py:48] [96100] global_step=96100, grad_norm=2.6608593463897705, loss=1.292806625366211
I0130 04:59:08.531966 139658730145536 logging_writer.py:48] [96200] global_step=96200, grad_norm=2.366333246231079, loss=1.3862709999084473
I0130 04:59:42.637363 139656658151168 logging_writer.py:48] [96300] global_step=96300, grad_norm=2.468526601791382, loss=1.3821110725402832
I0130 05:00:16.720022 139658730145536 logging_writer.py:48] [96400] global_step=96400, grad_norm=2.157827138900757, loss=1.2806379795074463
I0130 05:00:50.812568 139656658151168 logging_writer.py:48] [96500] global_step=96500, grad_norm=2.5089499950408936, loss=1.3549894094467163
I0130 05:01:24.845787 139658730145536 logging_writer.py:48] [96600] global_step=96600, grad_norm=2.4366674423217773, loss=1.3774334192276
I0130 05:01:58.873059 139656658151168 logging_writer.py:48] [96700] global_step=96700, grad_norm=2.1505281925201416, loss=1.3245242834091187
I0130 05:02:32.944394 139658730145536 logging_writer.py:48] [96800] global_step=96800, grad_norm=2.398550033569336, loss=1.3608183860778809
I0130 05:03:07.018460 139656658151168 logging_writer.py:48] [96900] global_step=96900, grad_norm=2.3411896228790283, loss=1.3719083070755005
I0130 05:03:41.138633 139658730145536 logging_writer.py:48] [97000] global_step=97000, grad_norm=2.3192038536071777, loss=1.3132600784301758
I0130 05:04:15.302037 139656658151168 logging_writer.py:48] [97100] global_step=97100, grad_norm=2.552905321121216, loss=1.396209716796875
I0130 05:04:49.383941 139658730145536 logging_writer.py:48] [97200] global_step=97200, grad_norm=2.5564024448394775, loss=1.4467687606811523
I0130 05:05:05.189916 139822745589568 spec.py:321] Evaluating on the training split.
I0130 05:05:12.080696 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 05:05:20.781928 139822745589568 spec.py:349] Evaluating on the test split.
I0130 05:05:23.338804 139822745589568 submission_runner.py:408] Time since start: 34350.33s, 	Step: 97248, 	{'train/accuracy': 0.7716238498687744, 'train/loss': 0.8549726605415344, 'validation/accuracy': 0.691540002822876, 'validation/loss': 1.2567367553710938, 'validation/num_examples': 50000, 'test/accuracy': 0.5631999969482422, 'test/loss': 1.985821008682251, 'test/num_examples': 10000, 'score': 33191.21137213707, 'total_duration': 34350.32893657684, 'accumulated_submission_time': 33191.21137213707, 'accumulated_eval_time': 1153.3456366062164, 'accumulated_logging_time': 2.44881272315979}
I0130 05:05:23.374878 139656297445120 logging_writer.py:48] [97248] accumulated_eval_time=1153.345637, accumulated_logging_time=2.448813, accumulated_submission_time=33191.211372, global_step=97248, preemption_count=0, score=33191.211372, test/accuracy=0.563200, test/loss=1.985821, test/num_examples=10000, total_duration=34350.328937, train/accuracy=0.771624, train/loss=0.854973, validation/accuracy=0.691540, validation/loss=1.256737, validation/num_examples=50000
I0130 05:05:41.416557 139656649758464 logging_writer.py:48] [97300] global_step=97300, grad_norm=2.329995632171631, loss=1.3290637731552124
I0130 05:06:15.469211 139656297445120 logging_writer.py:48] [97400] global_step=97400, grad_norm=2.3679733276367188, loss=1.3195416927337646
I0130 05:06:49.545714 139656649758464 logging_writer.py:48] [97500] global_step=97500, grad_norm=2.356595516204834, loss=1.2989975214004517
I0130 05:07:23.629536 139656297445120 logging_writer.py:48] [97600] global_step=97600, grad_norm=2.215958833694458, loss=1.2698640823364258
I0130 05:07:57.734507 139656649758464 logging_writer.py:48] [97700] global_step=97700, grad_norm=2.4829814434051514, loss=1.33788001537323
I0130 05:08:31.811851 139656297445120 logging_writer.py:48] [97800] global_step=97800, grad_norm=2.531622886657715, loss=1.4311249256134033
I0130 05:09:05.881955 139656649758464 logging_writer.py:48] [97900] global_step=97900, grad_norm=2.204740524291992, loss=1.3413598537445068
I0130 05:09:39.960483 139656297445120 logging_writer.py:48] [98000] global_step=98000, grad_norm=2.6643307209014893, loss=1.4594829082489014
I0130 05:10:14.050935 139656649758464 logging_writer.py:48] [98100] global_step=98100, grad_norm=2.312880039215088, loss=1.3784171342849731
I0130 05:10:48.213392 139656297445120 logging_writer.py:48] [98200] global_step=98200, grad_norm=2.315368175506592, loss=1.2932708263397217
I0130 05:11:22.307752 139656649758464 logging_writer.py:48] [98300] global_step=98300, grad_norm=2.541139602661133, loss=1.3235753774642944
I0130 05:11:56.408572 139656297445120 logging_writer.py:48] [98400] global_step=98400, grad_norm=2.8020122051239014, loss=1.4838498830795288
I0130 05:12:30.503688 139656649758464 logging_writer.py:48] [98500] global_step=98500, grad_norm=2.4025142192840576, loss=1.3285865783691406
I0130 05:13:04.577341 139656297445120 logging_writer.py:48] [98600] global_step=98600, grad_norm=2.4007441997528076, loss=1.3697060346603394
I0130 05:13:38.690949 139656649758464 logging_writer.py:48] [98700] global_step=98700, grad_norm=2.506094217300415, loss=1.4392887353897095
I0130 05:13:53.502666 139822745589568 spec.py:321] Evaluating on the training split.
I0130 05:13:59.792701 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 05:14:08.232761 139822745589568 spec.py:349] Evaluating on the test split.
I0130 05:14:11.516888 139822745589568 submission_runner.py:408] Time since start: 34878.51s, 	Step: 98745, 	{'train/accuracy': 0.7628945708274841, 'train/loss': 0.8821855187416077, 'validation/accuracy': 0.6861000061035156, 'validation/loss': 1.2782268524169922, 'validation/num_examples': 50000, 'test/accuracy': 0.5597000122070312, 'test/loss': 1.9715975522994995, 'test/num_examples': 10000, 'score': 33701.27909350395, 'total_duration': 34878.50706458092, 'accumulated_submission_time': 33701.27909350395, 'accumulated_eval_time': 1171.3598392009735, 'accumulated_logging_time': 2.493917226791382}
I0130 05:14:11.546347 139656658151168 logging_writer.py:48] [98745] accumulated_eval_time=1171.359839, accumulated_logging_time=2.493917, accumulated_submission_time=33701.279094, global_step=98745, preemption_count=0, score=33701.279094, test/accuracy=0.559700, test/loss=1.971598, test/num_examples=10000, total_duration=34878.507065, train/accuracy=0.762895, train/loss=0.882186, validation/accuracy=0.686100, validation/loss=1.278227, validation/num_examples=50000
I0130 05:14:31.156282 139656834316032 logging_writer.py:48] [98800] global_step=98800, grad_norm=2.393141746520996, loss=1.3261505365371704
I0130 05:15:05.193221 139656658151168 logging_writer.py:48] [98900] global_step=98900, grad_norm=2.447571277618408, loss=1.3204123973846436
I0130 05:15:39.269931 139656834316032 logging_writer.py:48] [99000] global_step=99000, grad_norm=2.279268980026245, loss=1.4073331356048584
I0130 05:16:13.319654 139656658151168 logging_writer.py:48] [99100] global_step=99100, grad_norm=2.261481285095215, loss=1.2719422578811646
I0130 05:16:47.390472 139656834316032 logging_writer.py:48] [99200] global_step=99200, grad_norm=2.484827756881714, loss=1.4195724725723267
I0130 05:17:21.561220 139656658151168 logging_writer.py:48] [99300] global_step=99300, grad_norm=2.331120729446411, loss=1.3428573608398438
I0130 05:17:55.650452 139656834316032 logging_writer.py:48] [99400] global_step=99400, grad_norm=2.2478597164154053, loss=1.330549955368042
I0130 05:18:29.749380 139656658151168 logging_writer.py:48] [99500] global_step=99500, grad_norm=2.4565343856811523, loss=1.3697212934494019
I0130 05:19:03.839981 139656834316032 logging_writer.py:48] [99600] global_step=99600, grad_norm=2.8299996852874756, loss=1.349365472793579
I0130 05:19:37.919567 139656658151168 logging_writer.py:48] [99700] global_step=99700, grad_norm=2.4206995964050293, loss=1.3166825771331787
I0130 05:20:12.012558 139656834316032 logging_writer.py:48] [99800] global_step=99800, grad_norm=2.482245683670044, loss=1.2106837034225464
I0130 05:20:46.095588 139656658151168 logging_writer.py:48] [99900] global_step=99900, grad_norm=2.6148321628570557, loss=1.43594229221344
I0130 05:21:20.169125 139656834316032 logging_writer.py:48] [100000] global_step=100000, grad_norm=2.558453321456909, loss=1.3171272277832031
I0130 05:21:54.233997 139656658151168 logging_writer.py:48] [100100] global_step=100100, grad_norm=2.5790836811065674, loss=1.400309443473816
I0130 05:22:28.329180 139656834316032 logging_writer.py:48] [100200] global_step=100200, grad_norm=2.1159515380859375, loss=1.207024097442627
I0130 05:22:41.775721 139822745589568 spec.py:321] Evaluating on the training split.
I0130 05:22:48.048781 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 05:22:56.447406 139822745589568 spec.py:349] Evaluating on the test split.
I0130 05:22:59.066707 139822745589568 submission_runner.py:408] Time since start: 35406.06s, 	Step: 100241, 	{'train/accuracy': 0.8028938174247742, 'train/loss': 0.7278432250022888, 'validation/accuracy': 0.6812599897384644, 'validation/loss': 1.3054099082946777, 'validation/num_examples': 50000, 'test/accuracy': 0.5509000420570374, 'test/loss': 2.0477240085601807, 'test/num_examples': 10000, 'score': 34210.90730881691, 'total_duration': 35406.05687189102, 'accumulated_submission_time': 34210.90730881691, 'accumulated_eval_time': 1188.65078830719, 'accumulated_logging_time': 3.0731360912323}
I0130 05:22:59.103976 139655592802048 logging_writer.py:48] [100241] accumulated_eval_time=1188.650788, accumulated_logging_time=3.073136, accumulated_submission_time=34210.907309, global_step=100241, preemption_count=0, score=34210.907309, test/accuracy=0.550900, test/loss=2.047724, test/num_examples=10000, total_duration=35406.056872, train/accuracy=0.802894, train/loss=0.727843, validation/accuracy=0.681260, validation/loss=1.305410, validation/num_examples=50000
I0130 05:23:19.500953 139656297445120 logging_writer.py:48] [100300] global_step=100300, grad_norm=2.435275077819824, loss=1.46588134765625
I0130 05:23:53.523105 139655592802048 logging_writer.py:48] [100400] global_step=100400, grad_norm=2.2977848052978516, loss=1.300217866897583
I0130 05:24:27.644445 139656297445120 logging_writer.py:48] [100500] global_step=100500, grad_norm=2.3321309089660645, loss=1.3737714290618896
I0130 05:25:01.673761 139655592802048 logging_writer.py:48] [100600] global_step=100600, grad_norm=2.4411213397979736, loss=1.2322070598602295
I0130 05:25:35.742599 139656297445120 logging_writer.py:48] [100700] global_step=100700, grad_norm=2.320026397705078, loss=1.3823968172073364
I0130 05:26:09.816251 139655592802048 logging_writer.py:48] [100800] global_step=100800, grad_norm=2.6751151084899902, loss=1.2501353025436401
I0130 05:26:43.882947 139656297445120 logging_writer.py:48] [100900] global_step=100900, grad_norm=2.8477399349212646, loss=1.4509860277175903
I0130 05:27:17.975425 139655592802048 logging_writer.py:48] [101000] global_step=101000, grad_norm=2.650296926498413, loss=1.5070371627807617
I0130 05:27:52.016743 139656297445120 logging_writer.py:48] [101100] global_step=101100, grad_norm=2.4259517192840576, loss=1.255952000617981
I0130 05:28:26.080723 139655592802048 logging_writer.py:48] [101200] global_step=101200, grad_norm=2.5533628463745117, loss=1.3506999015808105
I0130 05:29:00.164021 139656297445120 logging_writer.py:48] [101300] global_step=101300, grad_norm=2.5478270053863525, loss=1.3089444637298584
I0130 05:29:34.234602 139655592802048 logging_writer.py:48] [101400] global_step=101400, grad_norm=2.741257667541504, loss=1.3264375925064087
I0130 05:30:08.314657 139656297445120 logging_writer.py:48] [101500] global_step=101500, grad_norm=2.3718760013580322, loss=1.2745552062988281
I0130 05:30:42.499675 139655592802048 logging_writer.py:48] [101600] global_step=101600, grad_norm=2.413080930709839, loss=1.316412091255188
I0130 05:31:16.596293 139656297445120 logging_writer.py:48] [101700] global_step=101700, grad_norm=2.4940638542175293, loss=1.2860870361328125
I0130 05:31:29.353644 139822745589568 spec.py:321] Evaluating on the training split.
I0130 05:31:35.858509 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 05:31:44.226922 139822745589568 spec.py:349] Evaluating on the test split.
I0130 05:31:46.878905 139822745589568 submission_runner.py:408] Time since start: 35933.87s, 	Step: 101739, 	{'train/accuracy': 0.788504421710968, 'train/loss': 0.7852997183799744, 'validation/accuracy': 0.6868799924850464, 'validation/loss': 1.2690773010253906, 'validation/num_examples': 50000, 'test/accuracy': 0.5597000122070312, 'test/loss': 1.9839948415756226, 'test/num_examples': 10000, 'score': 34721.09689593315, 'total_duration': 35933.86906027794, 'accumulated_submission_time': 34721.09689593315, 'accumulated_eval_time': 1206.1760022640228, 'accumulated_logging_time': 3.119248867034912}
I0130 05:31:46.913624 139655576016640 logging_writer.py:48] [101739] accumulated_eval_time=1206.176002, accumulated_logging_time=3.119249, accumulated_submission_time=34721.096896, global_step=101739, preemption_count=0, score=34721.096896, test/accuracy=0.559700, test/loss=1.983995, test/num_examples=10000, total_duration=35933.869060, train/accuracy=0.788504, train/loss=0.785300, validation/accuracy=0.686880, validation/loss=1.269077, validation/num_examples=50000
I0130 05:32:08.038553 139655584409344 logging_writer.py:48] [101800] global_step=101800, grad_norm=2.4059572219848633, loss=1.296222448348999
I0130 05:32:42.087831 139655576016640 logging_writer.py:48] [101900] global_step=101900, grad_norm=2.915135145187378, loss=1.4559564590454102
I0130 05:33:16.158641 139655584409344 logging_writer.py:48] [102000] global_step=102000, grad_norm=2.590542793273926, loss=1.3606644868850708
I0130 05:33:50.232863 139655576016640 logging_writer.py:48] [102100] global_step=102100, grad_norm=2.456512212753296, loss=1.333609700202942
I0130 05:34:24.279094 139655584409344 logging_writer.py:48] [102200] global_step=102200, grad_norm=2.3700973987579346, loss=1.2864843606948853
I0130 05:34:58.374855 139655576016640 logging_writer.py:48] [102300] global_step=102300, grad_norm=2.2676775455474854, loss=1.240548849105835
I0130 05:35:32.468063 139655584409344 logging_writer.py:48] [102400] global_step=102400, grad_norm=2.267094135284424, loss=1.3236815929412842
I0130 05:36:06.544794 139655576016640 logging_writer.py:48] [102500] global_step=102500, grad_norm=2.3753662109375, loss=1.3537747859954834
I0130 05:36:40.647495 139655584409344 logging_writer.py:48] [102600] global_step=102600, grad_norm=2.4821372032165527, loss=1.3883105516433716
I0130 05:37:14.801691 139655576016640 logging_writer.py:48] [102700] global_step=102700, grad_norm=2.249086618423462, loss=1.2119767665863037
I0130 05:37:48.879601 139655584409344 logging_writer.py:48] [102800] global_step=102800, grad_norm=2.5633158683776855, loss=1.3037077188491821
I0130 05:38:22.933081 139655576016640 logging_writer.py:48] [102900] global_step=102900, grad_norm=2.7271742820739746, loss=1.3722575902938843
I0130 05:38:57.001835 139655584409344 logging_writer.py:48] [103000] global_step=103000, grad_norm=2.735664129257202, loss=1.4207804203033447
I0130 05:39:31.081245 139655576016640 logging_writer.py:48] [103100] global_step=103100, grad_norm=2.6477549076080322, loss=1.322429895401001
I0130 05:40:05.181552 139655584409344 logging_writer.py:48] [103200] global_step=103200, grad_norm=2.447711944580078, loss=1.3713449239730835
I0130 05:40:16.931969 139822745589568 spec.py:321] Evaluating on the training split.
I0130 05:40:23.179207 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 05:40:31.640695 139822745589568 spec.py:349] Evaluating on the test split.
I0130 05:40:34.311233 139822745589568 submission_runner.py:408] Time since start: 36461.30s, 	Step: 103236, 	{'train/accuracy': 0.7882851958274841, 'train/loss': 0.7907478213310242, 'validation/accuracy': 0.6976199746131897, 'validation/loss': 1.247995376586914, 'validation/num_examples': 50000, 'test/accuracy': 0.5722000002861023, 'test/loss': 1.93617844581604, 'test/num_examples': 10000, 'score': 35231.05541777611, 'total_duration': 36461.30139732361, 'accumulated_submission_time': 35231.05541777611, 'accumulated_eval_time': 1223.5552270412445, 'accumulated_logging_time': 3.1634182929992676}
I0130 05:40:34.350306 139656666543872 logging_writer.py:48] [103236] accumulated_eval_time=1223.555227, accumulated_logging_time=3.163418, accumulated_submission_time=35231.055418, global_step=103236, preemption_count=0, score=35231.055418, test/accuracy=0.572200, test/loss=1.936178, test/num_examples=10000, total_duration=36461.301397, train/accuracy=0.788285, train/loss=0.790748, validation/accuracy=0.697620, validation/loss=1.247995, validation/num_examples=50000
I0130 05:40:56.495605 139656834316032 logging_writer.py:48] [103300] global_step=103300, grad_norm=2.606170177459717, loss=1.3576552867889404
I0130 05:41:30.559534 139656666543872 logging_writer.py:48] [103400] global_step=103400, grad_norm=2.4382832050323486, loss=1.2954193353652954
I0130 05:42:04.636287 139656834316032 logging_writer.py:48] [103500] global_step=103500, grad_norm=2.54306697845459, loss=1.3115830421447754
I0130 05:42:38.722839 139656666543872 logging_writer.py:48] [103600] global_step=103600, grad_norm=2.5957019329071045, loss=1.443752408027649
I0130 05:43:12.804066 139656834316032 logging_writer.py:48] [103700] global_step=103700, grad_norm=2.3954498767852783, loss=1.2839957475662231
I0130 05:43:46.920962 139656666543872 logging_writer.py:48] [103800] global_step=103800, grad_norm=2.5851786136627197, loss=1.2423800230026245
I0130 05:44:21.108976 139656834316032 logging_writer.py:48] [103900] global_step=103900, grad_norm=2.5963053703308105, loss=1.2735620737075806
I0130 05:44:55.186440 139656666543872 logging_writer.py:48] [104000] global_step=104000, grad_norm=2.5152955055236816, loss=1.3575682640075684
I0130 05:45:29.278228 139656834316032 logging_writer.py:48] [104100] global_step=104100, grad_norm=2.5663392543792725, loss=1.3954224586486816
I0130 05:46:03.375622 139656666543872 logging_writer.py:48] [104200] global_step=104200, grad_norm=2.449441432952881, loss=1.2035013437271118
I0130 05:46:37.460952 139656834316032 logging_writer.py:48] [104300] global_step=104300, grad_norm=2.6389782428741455, loss=1.2840158939361572
I0130 05:47:11.540834 139656666543872 logging_writer.py:48] [104400] global_step=104400, grad_norm=2.5070834159851074, loss=1.2513631582260132
I0130 05:47:45.606749 139656834316032 logging_writer.py:48] [104500] global_step=104500, grad_norm=2.6985082626342773, loss=1.363399624824524
I0130 05:48:19.650715 139656666543872 logging_writer.py:48] [104600] global_step=104600, grad_norm=2.4354805946350098, loss=1.3272416591644287
I0130 05:48:53.709204 139656834316032 logging_writer.py:48] [104700] global_step=104700, grad_norm=2.609544515609741, loss=1.3422342538833618
I0130 05:49:04.412602 139822745589568 spec.py:321] Evaluating on the training split.
I0130 05:49:10.642359 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 05:49:19.005110 139822745589568 spec.py:349] Evaluating on the test split.
I0130 05:49:21.623122 139822745589568 submission_runner.py:408] Time since start: 36988.61s, 	Step: 104733, 	{'train/accuracy': 0.7801936864852905, 'train/loss': 0.8114662766456604, 'validation/accuracy': 0.6913599967956543, 'validation/loss': 1.254683017730713, 'validation/num_examples': 50000, 'test/accuracy': 0.5595000386238098, 'test/loss': 1.9859092235565186, 'test/num_examples': 10000, 'score': 35741.056241989136, 'total_duration': 36988.61328577995, 'accumulated_submission_time': 35741.056241989136, 'accumulated_eval_time': 1240.7657074928284, 'accumulated_logging_time': 3.2133994102478027}
I0130 05:49:21.659316 139655592802048 logging_writer.py:48] [104733] accumulated_eval_time=1240.765707, accumulated_logging_time=3.213399, accumulated_submission_time=35741.056242, global_step=104733, preemption_count=0, score=35741.056242, test/accuracy=0.559500, test/loss=1.985909, test/num_examples=10000, total_duration=36988.613286, train/accuracy=0.780194, train/loss=0.811466, validation/accuracy=0.691360, validation/loss=1.254683, validation/num_examples=50000
I0130 05:49:44.798975 139656297445120 logging_writer.py:48] [104800] global_step=104800, grad_norm=2.8149523735046387, loss=1.419951319694519
I0130 05:50:18.791076 139655592802048 logging_writer.py:48] [104900] global_step=104900, grad_norm=2.606816530227661, loss=1.2594130039215088
I0130 05:50:52.886339 139656297445120 logging_writer.py:48] [105000] global_step=105000, grad_norm=2.522456169128418, loss=1.2919777631759644
I0130 05:51:26.942800 139655592802048 logging_writer.py:48] [105100] global_step=105100, grad_norm=2.569288730621338, loss=1.281020998954773
I0130 05:52:01.013875 139656297445120 logging_writer.py:48] [105200] global_step=105200, grad_norm=2.5888493061065674, loss=1.1921489238739014
I0130 05:52:35.078038 139655592802048 logging_writer.py:48] [105300] global_step=105300, grad_norm=2.3786559104919434, loss=1.2540515661239624
I0130 05:53:09.134093 139656297445120 logging_writer.py:48] [105400] global_step=105400, grad_norm=2.6186163425445557, loss=1.3506232500076294
I0130 05:53:43.205993 139655592802048 logging_writer.py:48] [105500] global_step=105500, grad_norm=2.6265153884887695, loss=1.3268252611160278
I0130 05:54:17.257129 139656297445120 logging_writer.py:48] [105600] global_step=105600, grad_norm=2.348055362701416, loss=1.290465235710144
I0130 05:54:51.321850 139655592802048 logging_writer.py:48] [105700] global_step=105700, grad_norm=2.607280731201172, loss=1.4466623067855835
I0130 05:55:25.391528 139656297445120 logging_writer.py:48] [105800] global_step=105800, grad_norm=2.807985782623291, loss=1.2950810194015503
I0130 05:55:59.463126 139655592802048 logging_writer.py:48] [105900] global_step=105900, grad_norm=2.8096604347229004, loss=1.3217840194702148
I0130 05:56:33.538593 139656297445120 logging_writer.py:48] [106000] global_step=106000, grad_norm=2.605905294418335, loss=1.3081918954849243
I0130 05:57:07.844341 139655592802048 logging_writer.py:48] [106100] global_step=106100, grad_norm=2.3030447959899902, loss=1.296370506286621
I0130 05:57:41.943578 139656297445120 logging_writer.py:48] [106200] global_step=106200, grad_norm=2.544461488723755, loss=1.3660850524902344
I0130 05:57:51.643265 139822745589568 spec.py:321] Evaluating on the training split.
I0130 05:57:57.875593 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 05:58:06.549457 139822745589568 spec.py:349] Evaluating on the test split.
I0130 05:58:09.196566 139822745589568 submission_runner.py:408] Time since start: 37516.19s, 	Step: 106230, 	{'train/accuracy': 0.7833226919174194, 'train/loss': 0.8032361268997192, 'validation/accuracy': 0.6902399659156799, 'validation/loss': 1.247302532196045, 'validation/num_examples': 50000, 'test/accuracy': 0.5678000450134277, 'test/loss': 1.9703844785690308, 'test/num_examples': 10000, 'score': 36250.98030781746, 'total_duration': 37516.186690330505, 'accumulated_submission_time': 36250.98030781746, 'accumulated_eval_time': 1258.3189299106598, 'accumulated_logging_time': 3.259010076522827}
I0130 05:58:09.252286 139656666543872 logging_writer.py:48] [106230] accumulated_eval_time=1258.318930, accumulated_logging_time=3.259010, accumulated_submission_time=36250.980308, global_step=106230, preemption_count=0, score=36250.980308, test/accuracy=0.567800, test/loss=1.970384, test/num_examples=10000, total_duration=37516.186690, train/accuracy=0.783323, train/loss=0.803236, validation/accuracy=0.690240, validation/loss=1.247303, validation/num_examples=50000
I0130 05:58:33.432918 139656834316032 logging_writer.py:48] [106300] global_step=106300, grad_norm=2.612353563308716, loss=1.3650848865509033
I0130 05:59:07.479542 139656666543872 logging_writer.py:48] [106400] global_step=106400, grad_norm=2.7597644329071045, loss=1.3952568769454956
I0130 05:59:41.544038 139656834316032 logging_writer.py:48] [106500] global_step=106500, grad_norm=2.735966205596924, loss=1.3308558464050293
I0130 06:00:15.612535 139656666543872 logging_writer.py:48] [106600] global_step=106600, grad_norm=2.7335705757141113, loss=1.3890196084976196
I0130 06:00:49.694636 139656834316032 logging_writer.py:48] [106700] global_step=106700, grad_norm=2.631485939025879, loss=1.2155566215515137
I0130 06:01:23.764086 139656666543872 logging_writer.py:48] [106800] global_step=106800, grad_norm=2.6857495307922363, loss=1.3392820358276367
I0130 06:01:57.858139 139656834316032 logging_writer.py:48] [106900] global_step=106900, grad_norm=2.4801270961761475, loss=1.213305950164795
I0130 06:02:31.918536 139656666543872 logging_writer.py:48] [107000] global_step=107000, grad_norm=2.827630043029785, loss=1.3750982284545898
I0130 06:03:06.029158 139656834316032 logging_writer.py:48] [107100] global_step=107100, grad_norm=2.6055920124053955, loss=1.2810442447662354
I0130 06:03:40.283032 139656666543872 logging_writer.py:48] [107200] global_step=107200, grad_norm=2.494945526123047, loss=1.2226054668426514
I0130 06:04:14.359204 139656834316032 logging_writer.py:48] [107300] global_step=107300, grad_norm=2.65899658203125, loss=1.411180019378662
I0130 06:04:48.428312 139656666543872 logging_writer.py:48] [107400] global_step=107400, grad_norm=2.5056040287017822, loss=1.2726768255233765
I0130 06:05:22.490277 139656834316032 logging_writer.py:48] [107500] global_step=107500, grad_norm=2.657694101333618, loss=1.2107937335968018
I0130 06:05:56.558058 139656666543872 logging_writer.py:48] [107600] global_step=107600, grad_norm=2.8314404487609863, loss=1.4142651557922363
I0130 06:06:30.598593 139656834316032 logging_writer.py:48] [107700] global_step=107700, grad_norm=2.467179298400879, loss=1.188036561012268
I0130 06:06:39.277045 139822745589568 spec.py:321] Evaluating on the training split.
I0130 06:06:45.502732 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 06:06:54.197337 139822745589568 spec.py:349] Evaluating on the test split.
I0130 06:06:56.825357 139822745589568 submission_runner.py:408] Time since start: 38043.82s, 	Step: 107727, 	{'train/accuracy': 0.7869698405265808, 'train/loss': 0.769432544708252, 'validation/accuracy': 0.6983199715614319, 'validation/loss': 1.2229222059249878, 'validation/num_examples': 50000, 'test/accuracy': 0.5678000450134277, 'test/loss': 1.9578731060028076, 'test/num_examples': 10000, 'score': 36760.9456949234, 'total_duration': 38043.81551671028, 'accumulated_submission_time': 36760.9456949234, 'accumulated_eval_time': 1275.867201089859, 'accumulated_logging_time': 3.323164701461792}
I0130 06:06:56.865245 139656297445120 logging_writer.py:48] [107727] accumulated_eval_time=1275.867201, accumulated_logging_time=3.323165, accumulated_submission_time=36760.945695, global_step=107727, preemption_count=0, score=36760.945695, test/accuracy=0.567800, test/loss=1.957873, test/num_examples=10000, total_duration=38043.815517, train/accuracy=0.786970, train/loss=0.769433, validation/accuracy=0.698320, validation/loss=1.222922, validation/num_examples=50000
I0130 06:07:22.046279 139656649758464 logging_writer.py:48] [107800] global_step=107800, grad_norm=2.3735411167144775, loss=1.2679696083068848
I0130 06:07:56.073452 139656297445120 logging_writer.py:48] [107900] global_step=107900, grad_norm=2.6139774322509766, loss=1.2660503387451172
I0130 06:08:30.132708 139656649758464 logging_writer.py:48] [108000] global_step=108000, grad_norm=2.829923391342163, loss=1.4065678119659424
I0130 06:09:04.188043 139656297445120 logging_writer.py:48] [108100] global_step=108100, grad_norm=2.45406436920166, loss=1.2240550518035889
I0130 06:09:38.245417 139656649758464 logging_writer.py:48] [108200] global_step=108200, grad_norm=2.653735399246216, loss=1.3391555547714233
I0130 06:10:12.275712 139656297445120 logging_writer.py:48] [108300] global_step=108300, grad_norm=2.4800519943237305, loss=1.2461371421813965
I0130 06:10:46.457557 139656649758464 logging_writer.py:48] [108400] global_step=108400, grad_norm=2.8588271141052246, loss=1.3416157960891724
I0130 06:11:20.473999 139656297445120 logging_writer.py:48] [108500] global_step=108500, grad_norm=2.837205410003662, loss=1.3859057426452637
I0130 06:11:54.507335 139656649758464 logging_writer.py:48] [108600] global_step=108600, grad_norm=2.9617557525634766, loss=1.2461986541748047
I0130 06:12:28.596546 139656297445120 logging_writer.py:48] [108700] global_step=108700, grad_norm=2.5996360778808594, loss=1.282624363899231
I0130 06:13:02.664460 139656649758464 logging_writer.py:48] [108800] global_step=108800, grad_norm=2.6379613876342773, loss=1.2690677642822266
I0130 06:13:36.752632 139656297445120 logging_writer.py:48] [108900] global_step=108900, grad_norm=2.6110594272613525, loss=1.3377958536148071
I0130 06:14:10.862593 139656649758464 logging_writer.py:48] [109000] global_step=109000, grad_norm=2.602565288543701, loss=1.3238246440887451
I0130 06:14:44.950835 139656297445120 logging_writer.py:48] [109100] global_step=109100, grad_norm=2.539440870285034, loss=1.2199193239212036
I0130 06:15:19.052032 139656649758464 logging_writer.py:48] [109200] global_step=109200, grad_norm=2.651973247528076, loss=1.2239303588867188
I0130 06:15:27.021409 139822745589568 spec.py:321] Evaluating on the training split.
I0130 06:15:33.271299 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 06:15:41.668149 139822745589568 spec.py:349] Evaluating on the test split.
I0130 06:15:44.332039 139822745589568 submission_runner.py:408] Time since start: 38571.32s, 	Step: 109225, 	{'train/accuracy': 0.8082947731018066, 'train/loss': 0.6996628046035767, 'validation/accuracy': 0.6953799724578857, 'validation/loss': 1.2410041093826294, 'validation/num_examples': 50000, 'test/accuracy': 0.5749000310897827, 'test/loss': 1.9538441896438599, 'test/num_examples': 10000, 'score': 37271.04302740097, 'total_duration': 38571.32220196724, 'accumulated_submission_time': 37271.04302740097, 'accumulated_eval_time': 1293.1777880191803, 'accumulated_logging_time': 3.372274875640869}
I0130 06:15:44.369029 139655592802048 logging_writer.py:48] [109225] accumulated_eval_time=1293.177788, accumulated_logging_time=3.372275, accumulated_submission_time=37271.043027, global_step=109225, preemption_count=0, score=37271.043027, test/accuracy=0.574900, test/loss=1.953844, test/num_examples=10000, total_duration=38571.322202, train/accuracy=0.808295, train/loss=0.699663, validation/accuracy=0.695380, validation/loss=1.241004, validation/num_examples=50000
I0130 06:16:10.232089 139656666543872 logging_writer.py:48] [109300] global_step=109300, grad_norm=2.37201189994812, loss=1.1520206928253174
I0130 06:16:44.301436 139655592802048 logging_writer.py:48] [109400] global_step=109400, grad_norm=2.6496827602386475, loss=1.2653006315231323
I0130 06:17:18.436634 139656666543872 logging_writer.py:48] [109500] global_step=109500, grad_norm=2.6285605430603027, loss=1.3184113502502441
I0130 06:17:52.526839 139655592802048 logging_writer.py:48] [109600] global_step=109600, grad_norm=2.8050296306610107, loss=1.319902777671814
I0130 06:18:26.622550 139656666543872 logging_writer.py:48] [109700] global_step=109700, grad_norm=2.8983237743377686, loss=1.2962809801101685
I0130 06:19:00.682166 139655592802048 logging_writer.py:48] [109800] global_step=109800, grad_norm=2.983290195465088, loss=1.2318763732910156
I0130 06:19:34.766758 139656666543872 logging_writer.py:48] [109900] global_step=109900, grad_norm=2.5843944549560547, loss=1.295133113861084
I0130 06:20:08.821815 139655592802048 logging_writer.py:48] [110000] global_step=110000, grad_norm=2.6067490577697754, loss=1.2663836479187012
I0130 06:20:42.924318 139656666543872 logging_writer.py:48] [110100] global_step=110100, grad_norm=2.636644124984741, loss=1.2824981212615967
I0130 06:21:17.003006 139655592802048 logging_writer.py:48] [110200] global_step=110200, grad_norm=2.5763132572174072, loss=1.2856943607330322
I0130 06:21:51.116818 139656666543872 logging_writer.py:48] [110300] global_step=110300, grad_norm=2.7235052585601807, loss=1.3163689374923706
I0130 06:22:25.198737 139655592802048 logging_writer.py:48] [110400] global_step=110400, grad_norm=2.8014559745788574, loss=1.338133454322815
I0130 06:22:59.281641 139656666543872 logging_writer.py:48] [110500] global_step=110500, grad_norm=2.7223267555236816, loss=1.2836081981658936
I0130 06:23:33.550982 139655592802048 logging_writer.py:48] [110600] global_step=110600, grad_norm=2.6263577938079834, loss=1.2318804264068604
I0130 06:24:07.603716 139656666543872 logging_writer.py:48] [110700] global_step=110700, grad_norm=2.6835944652557373, loss=1.322119951248169
I0130 06:24:14.564338 139822745589568 spec.py:321] Evaluating on the training split.
I0130 06:24:20.834605 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 06:24:29.504524 139822745589568 spec.py:349] Evaluating on the test split.
I0130 06:24:32.175538 139822745589568 submission_runner.py:408] Time since start: 39099.17s, 	Step: 110722, 	{'train/accuracy': 0.813875138759613, 'train/loss': 0.6721604466438293, 'validation/accuracy': 0.7019599676132202, 'validation/loss': 1.222309947013855, 'validation/num_examples': 50000, 'test/accuracy': 0.5769000053405762, 'test/loss': 1.93671452999115, 'test/num_examples': 10000, 'score': 37781.17570281029, 'total_duration': 39099.16570162773, 'accumulated_submission_time': 37781.17570281029, 'accumulated_eval_time': 1310.7889490127563, 'accumulated_logging_time': 3.420320987701416}
I0130 06:24:32.215371 139655576016640 logging_writer.py:48] [110722] accumulated_eval_time=1310.788949, accumulated_logging_time=3.420321, accumulated_submission_time=37781.175703, global_step=110722, preemption_count=0, score=37781.175703, test/accuracy=0.576900, test/loss=1.936715, test/num_examples=10000, total_duration=39099.165702, train/accuracy=0.813875, train/loss=0.672160, validation/accuracy=0.701960, validation/loss=1.222310, validation/num_examples=50000
I0130 06:24:59.121993 139655584409344 logging_writer.py:48] [110800] global_step=110800, grad_norm=2.7963082790374756, loss=1.2169079780578613
I0130 06:25:33.203805 139655576016640 logging_writer.py:48] [110900] global_step=110900, grad_norm=2.599442958831787, loss=1.2764455080032349
I0130 06:26:07.309598 139655584409344 logging_writer.py:48] [111000] global_step=111000, grad_norm=2.974788188934326, loss=1.3859035968780518
I0130 06:26:41.432377 139655576016640 logging_writer.py:48] [111100] global_step=111100, grad_norm=2.5060665607452393, loss=1.2543120384216309
I0130 06:27:15.552712 139655584409344 logging_writer.py:48] [111200] global_step=111200, grad_norm=2.670151472091675, loss=1.2489198446273804
I0130 06:27:49.644582 139655576016640 logging_writer.py:48] [111300] global_step=111300, grad_norm=3.085773229598999, loss=1.3749750852584839
I0130 06:28:23.739342 139655584409344 logging_writer.py:48] [111400] global_step=111400, grad_norm=2.832376718521118, loss=1.2291556596755981
I0130 06:28:57.845375 139655576016640 logging_writer.py:48] [111500] global_step=111500, grad_norm=2.667508840560913, loss=1.2266706228256226
I0130 06:29:31.928866 139655584409344 logging_writer.py:48] [111600] global_step=111600, grad_norm=2.716810941696167, loss=1.2872028350830078
I0130 06:30:06.025157 139655576016640 logging_writer.py:48] [111700] global_step=111700, grad_norm=2.688218593597412, loss=1.277659296989441
I0130 06:30:40.243267 139655584409344 logging_writer.py:48] [111800] global_step=111800, grad_norm=2.529329299926758, loss=1.1923458576202393
I0130 06:31:14.329514 139655576016640 logging_writer.py:48] [111900] global_step=111900, grad_norm=2.7027761936187744, loss=1.1824069023132324
I0130 06:31:48.417870 139655584409344 logging_writer.py:48] [112000] global_step=112000, grad_norm=2.5809848308563232, loss=1.2254712581634521
I0130 06:32:22.501466 139655576016640 logging_writer.py:48] [112100] global_step=112100, grad_norm=2.682304620742798, loss=1.2543259859085083
I0130 06:32:56.577568 139655584409344 logging_writer.py:48] [112200] global_step=112200, grad_norm=2.8952362537384033, loss=1.2134820222854614
I0130 06:33:02.511997 139822745589568 spec.py:321] Evaluating on the training split.
I0130 06:33:08.717290 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 06:33:17.190606 139822745589568 spec.py:349] Evaluating on the test split.
I0130 06:33:19.747225 139822745589568 submission_runner.py:408] Time since start: 39626.74s, 	Step: 112219, 	{'train/accuracy': 0.8077367544174194, 'train/loss': 0.6889608502388, 'validation/accuracy': 0.7064599990844727, 'validation/loss': 1.1906296014785767, 'validation/num_examples': 50000, 'test/accuracy': 0.5773000121116638, 'test/loss': 1.9144089221954346, 'test/num_examples': 10000, 'score': 38291.41183042526, 'total_duration': 39626.737365961075, 'accumulated_submission_time': 38291.41183042526, 'accumulated_eval_time': 1328.0241174697876, 'accumulated_logging_time': 3.4695346355438232}
I0130 06:33:19.787732 139656658151168 logging_writer.py:48] [112219] accumulated_eval_time=1328.024117, accumulated_logging_time=3.469535, accumulated_submission_time=38291.411830, global_step=112219, preemption_count=0, score=38291.411830, test/accuracy=0.577300, test/loss=1.914409, test/num_examples=10000, total_duration=39626.737366, train/accuracy=0.807737, train/loss=0.688961, validation/accuracy=0.706460, validation/loss=1.190630, validation/num_examples=50000
I0130 06:33:47.692741 139656666543872 logging_writer.py:48] [112300] global_step=112300, grad_norm=2.724000930786133, loss=1.2539916038513184
I0130 06:34:21.707069 139656658151168 logging_writer.py:48] [112400] global_step=112400, grad_norm=2.838438034057617, loss=1.273181438446045
I0130 06:34:55.765685 139656666543872 logging_writer.py:48] [112500] global_step=112500, grad_norm=2.9644594192504883, loss=1.2415438890457153
I0130 06:35:29.846417 139656658151168 logging_writer.py:48] [112600] global_step=112600, grad_norm=2.66351580619812, loss=1.2636884450912476
I0130 06:36:03.899311 139656666543872 logging_writer.py:48] [112700] global_step=112700, grad_norm=2.860119581222534, loss=1.2530765533447266
I0130 06:36:37.982332 139656658151168 logging_writer.py:48] [112800] global_step=112800, grad_norm=2.593698501586914, loss=1.2358837127685547
I0130 06:37:12.133591 139656666543872 logging_writer.py:48] [112900] global_step=112900, grad_norm=2.7143311500549316, loss=1.2511862516403198
I0130 06:37:46.207177 139656658151168 logging_writer.py:48] [113000] global_step=113000, grad_norm=2.578251600265503, loss=1.2083215713500977
I0130 06:38:20.307074 139656666543872 logging_writer.py:48] [113100] global_step=113100, grad_norm=2.8470444679260254, loss=1.2171978950500488
I0130 06:38:54.421446 139656658151168 logging_writer.py:48] [113200] global_step=113200, grad_norm=2.9262912273406982, loss=1.1470983028411865
I0130 06:39:28.530859 139656666543872 logging_writer.py:48] [113300] global_step=113300, grad_norm=2.7056310176849365, loss=1.1699758768081665
I0130 06:40:02.631961 139656658151168 logging_writer.py:48] [113400] global_step=113400, grad_norm=2.760667085647583, loss=1.2105741500854492
I0130 06:40:36.719542 139656666543872 logging_writer.py:48] [113500] global_step=113500, grad_norm=2.840327262878418, loss=1.3316936492919922
I0130 06:41:10.770282 139656658151168 logging_writer.py:48] [113600] global_step=113600, grad_norm=2.776426076889038, loss=1.183072805404663
I0130 06:41:44.830806 139656666543872 logging_writer.py:48] [113700] global_step=113700, grad_norm=2.7854092121124268, loss=1.1564736366271973
I0130 06:41:49.748450 139822745589568 spec.py:321] Evaluating on the training split.
I0130 06:41:55.978262 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 06:42:04.489053 139822745589568 spec.py:349] Evaluating on the test split.
I0130 06:42:07.130049 139822745589568 submission_runner.py:408] Time since start: 40154.12s, 	Step: 113716, 	{'train/accuracy': 0.7943239808082581, 'train/loss': 0.7536799311637878, 'validation/accuracy': 0.6958999633789062, 'validation/loss': 1.2339434623718262, 'validation/num_examples': 50000, 'test/accuracy': 0.5742000341415405, 'test/loss': 1.97724187374115, 'test/num_examples': 10000, 'score': 38801.31273698807, 'total_duration': 40154.12018656731, 'accumulated_submission_time': 38801.31273698807, 'accumulated_eval_time': 1345.4056491851807, 'accumulated_logging_time': 3.519113063812256}
I0130 06:42:07.183577 139655592802048 logging_writer.py:48] [113716] accumulated_eval_time=1345.405649, accumulated_logging_time=3.519113, accumulated_submission_time=38801.312737, global_step=113716, preemption_count=0, score=38801.312737, test/accuracy=0.574200, test/loss=1.977242, test/num_examples=10000, total_duration=40154.120187, train/accuracy=0.794324, train/loss=0.753680, validation/accuracy=0.695900, validation/loss=1.233943, validation/num_examples=50000
I0130 06:42:36.109314 139656297445120 logging_writer.py:48] [113800] global_step=113800, grad_norm=2.7870800495147705, loss=1.2261559963226318
I0130 06:43:10.182557 139655592802048 logging_writer.py:48] [113900] global_step=113900, grad_norm=2.759809732437134, loss=1.2309513092041016
I0130 06:43:44.576455 139656297445120 logging_writer.py:48] [114000] global_step=114000, grad_norm=2.797218084335327, loss=1.2698484659194946
I0130 06:44:18.687044 139655592802048 logging_writer.py:48] [114100] global_step=114100, grad_norm=2.9357240200042725, loss=1.2346997261047363
I0130 06:44:52.787696 139656297445120 logging_writer.py:48] [114200] global_step=114200, grad_norm=2.6194262504577637, loss=1.1927862167358398
I0130 06:45:26.879621 139655592802048 logging_writer.py:48] [114300] global_step=114300, grad_norm=2.7457921504974365, loss=1.2191696166992188
I0130 06:46:00.973749 139656297445120 logging_writer.py:48] [114400] global_step=114400, grad_norm=3.096858263015747, loss=1.2407385110855103
I0130 06:46:35.056946 139655592802048 logging_writer.py:48] [114500] global_step=114500, grad_norm=3.08644962310791, loss=1.3053081035614014
I0130 06:47:09.131964 139656297445120 logging_writer.py:48] [114600] global_step=114600, grad_norm=2.803110361099243, loss=1.3220844268798828
I0130 06:47:43.215034 139655592802048 logging_writer.py:48] [114700] global_step=114700, grad_norm=2.838712453842163, loss=1.298367977142334
I0130 06:48:17.261417 139656297445120 logging_writer.py:48] [114800] global_step=114800, grad_norm=2.7727768421173096, loss=1.2604087591171265
I0130 06:48:51.331861 139655592802048 logging_writer.py:48] [114900] global_step=114900, grad_norm=2.826313018798828, loss=1.3068352937698364
I0130 06:49:25.388599 139656297445120 logging_writer.py:48] [115000] global_step=115000, grad_norm=2.637298583984375, loss=1.3351645469665527
I0130 06:49:59.468488 139655592802048 logging_writer.py:48] [115100] global_step=115100, grad_norm=2.8123300075531006, loss=1.1260102987289429
I0130 06:50:33.614555 139656297445120 logging_writer.py:48] [115200] global_step=115200, grad_norm=2.7260236740112305, loss=1.2121728658676147
I0130 06:50:37.168025 139822745589568 spec.py:321] Evaluating on the training split.
I0130 06:50:43.404073 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 06:50:52.046380 139822745589568 spec.py:349] Evaluating on the test split.
I0130 06:50:54.615361 139822745589568 submission_runner.py:408] Time since start: 40681.61s, 	Step: 115212, 	{'train/accuracy': 0.7994260191917419, 'train/loss': 0.7419718503952026, 'validation/accuracy': 0.6971399784088135, 'validation/loss': 1.2301865816116333, 'validation/num_examples': 50000, 'test/accuracy': 0.5767000317573547, 'test/loss': 1.9383008480072021, 'test/num_examples': 10000, 'score': 39311.23474597931, 'total_duration': 40681.605519771576, 'accumulated_submission_time': 39311.23474597931, 'accumulated_eval_time': 1362.8529393672943, 'accumulated_logging_time': 3.5819525718688965}
I0130 06:50:54.654578 139656809137920 logging_writer.py:48] [115212] accumulated_eval_time=1362.852939, accumulated_logging_time=3.581953, accumulated_submission_time=39311.234746, global_step=115212, preemption_count=0, score=39311.234746, test/accuracy=0.576700, test/loss=1.938301, test/num_examples=10000, total_duration=40681.605520, train/accuracy=0.799426, train/loss=0.741972, validation/accuracy=0.697140, validation/loss=1.230187, validation/num_examples=50000
I0130 06:51:24.912890 139656817530624 logging_writer.py:48] [115300] global_step=115300, grad_norm=2.870368480682373, loss=1.196215033531189
I0130 06:51:58.947562 139656809137920 logging_writer.py:48] [115400] global_step=115400, grad_norm=2.730546712875366, loss=1.2005465030670166
I0130 06:52:33.012596 139656817530624 logging_writer.py:48] [115500] global_step=115500, grad_norm=2.555806875228882, loss=1.1924153566360474
I0130 06:53:07.105258 139656809137920 logging_writer.py:48] [115600] global_step=115600, grad_norm=3.077404499053955, loss=1.2633490562438965
I0130 06:53:41.212464 139656817530624 logging_writer.py:48] [115700] global_step=115700, grad_norm=2.757229804992676, loss=1.1475489139556885
I0130 06:54:15.281682 139656809137920 logging_writer.py:48] [115800] global_step=115800, grad_norm=2.6466197967529297, loss=1.2114075422286987
I0130 06:54:49.375329 139656817530624 logging_writer.py:48] [115900] global_step=115900, grad_norm=2.7626729011535645, loss=1.1862398386001587
I0130 06:55:23.454447 139656809137920 logging_writer.py:48] [116000] global_step=116000, grad_norm=2.8212335109710693, loss=1.1978362798690796
I0130 06:55:57.533540 139656817530624 logging_writer.py:48] [116100] global_step=116100, grad_norm=2.999980926513672, loss=1.2812325954437256
I0130 06:56:31.609493 139656809137920 logging_writer.py:48] [116200] global_step=116200, grad_norm=2.916111469268799, loss=1.1674765348434448
I0130 06:57:05.876651 139656817530624 logging_writer.py:48] [116300] global_step=116300, grad_norm=2.975964307785034, loss=1.2525471448898315
I0130 06:57:39.952184 139656809137920 logging_writer.py:48] [116400] global_step=116400, grad_norm=2.6444175243377686, loss=1.1761012077331543
I0130 06:58:13.999843 139656817530624 logging_writer.py:48] [116500] global_step=116500, grad_norm=2.782228469848633, loss=1.1670348644256592
I0130 06:58:48.062940 139656809137920 logging_writer.py:48] [116600] global_step=116600, grad_norm=2.9469821453094482, loss=1.2459824085235596
I0130 06:59:22.151524 139656817530624 logging_writer.py:48] [116700] global_step=116700, grad_norm=2.7807700634002686, loss=1.1571582555770874
I0130 06:59:24.683934 139822745589568 spec.py:321] Evaluating on the training split.
I0130 06:59:30.927871 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 06:59:39.709617 139822745589568 spec.py:349] Evaluating on the test split.
I0130 06:59:42.234413 139822745589568 submission_runner.py:408] Time since start: 41209.22s, 	Step: 116709, 	{'train/accuracy': 0.8053252100944519, 'train/loss': 0.7091084718704224, 'validation/accuracy': 0.7084199786186218, 'validation/loss': 1.1827819347381592, 'validation/num_examples': 50000, 'test/accuracy': 0.5850000381469727, 'test/loss': 1.901528239250183, 'test/num_examples': 10000, 'score': 39821.202984809875, 'total_duration': 41209.22456550598, 'accumulated_submission_time': 39821.202984809875, 'accumulated_eval_time': 1380.4033725261688, 'accumulated_logging_time': 3.630246162414551}
I0130 06:59:42.276380 139656792352512 logging_writer.py:48] [116709] accumulated_eval_time=1380.403373, accumulated_logging_time=3.630246, accumulated_submission_time=39821.202985, global_step=116709, preemption_count=0, score=39821.202985, test/accuracy=0.585000, test/loss=1.901528, test/num_examples=10000, total_duration=41209.224566, train/accuracy=0.805325, train/loss=0.709108, validation/accuracy=0.708420, validation/loss=1.182782, validation/num_examples=50000
I0130 07:00:13.580680 139656800745216 logging_writer.py:48] [116800] global_step=116800, grad_norm=2.8658783435821533, loss=1.2464203834533691
I0130 07:00:47.648531 139656792352512 logging_writer.py:48] [116900] global_step=116900, grad_norm=3.0134074687957764, loss=1.2502050399780273
I0130 07:01:21.693724 139656800745216 logging_writer.py:48] [117000] global_step=117000, grad_norm=2.635456085205078, loss=1.150652289390564
I0130 07:01:55.742565 139656792352512 logging_writer.py:48] [117100] global_step=117100, grad_norm=3.3824405670166016, loss=1.170289397239685
I0130 07:02:29.822205 139656800745216 logging_writer.py:48] [117200] global_step=117200, grad_norm=2.8268351554870605, loss=1.1481389999389648
I0130 07:03:03.911615 139656792352512 logging_writer.py:48] [117300] global_step=117300, grad_norm=2.80047869682312, loss=1.1507128477096558
I0130 07:03:38.169974 139656800745216 logging_writer.py:48] [117400] global_step=117400, grad_norm=2.666952610015869, loss=1.2270351648330688
I0130 07:04:12.243417 139656792352512 logging_writer.py:48] [117500] global_step=117500, grad_norm=3.014138698577881, loss=1.2451434135437012
I0130 07:04:46.320365 139656800745216 logging_writer.py:48] [117600] global_step=117600, grad_norm=3.077141284942627, loss=1.2579395771026611
I0130 07:05:20.397104 139656792352512 logging_writer.py:48] [117700] global_step=117700, grad_norm=2.990184783935547, loss=1.2145464420318604
I0130 07:05:54.481863 139656800745216 logging_writer.py:48] [117800] global_step=117800, grad_norm=2.796304225921631, loss=1.217009425163269
I0130 07:06:28.567529 139656792352512 logging_writer.py:48] [117900] global_step=117900, grad_norm=2.7290053367614746, loss=1.2290188074111938
I0130 07:07:02.632528 139656800745216 logging_writer.py:48] [118000] global_step=118000, grad_norm=2.706962823867798, loss=1.1561477184295654
I0130 07:07:36.715330 139656792352512 logging_writer.py:48] [118100] global_step=118100, grad_norm=2.9026870727539062, loss=1.1697404384613037
I0130 07:08:10.800016 139656800745216 logging_writer.py:48] [118200] global_step=118200, grad_norm=2.8589320182800293, loss=1.2300825119018555
I0130 07:08:12.316029 139822745589568 spec.py:321] Evaluating on the training split.
I0130 07:08:18.563175 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 07:08:27.203751 139822745589568 spec.py:349] Evaluating on the test split.
I0130 07:08:29.884748 139822745589568 submission_runner.py:408] Time since start: 41736.87s, 	Step: 118206, 	{'train/accuracy': 0.8056241869926453, 'train/loss': 0.7094724178314209, 'validation/accuracy': 0.7069999575614929, 'validation/loss': 1.1886725425720215, 'validation/num_examples': 50000, 'test/accuracy': 0.5771000385284424, 'test/loss': 1.9114309549331665, 'test/num_examples': 10000, 'score': 40331.18197154999, 'total_duration': 41736.874911785126, 'accumulated_submission_time': 40331.18197154999, 'accumulated_eval_time': 1397.9720528125763, 'accumulated_logging_time': 3.681635856628418}
I0130 07:08:29.926148 139656783959808 logging_writer.py:48] [118206] accumulated_eval_time=1397.972053, accumulated_logging_time=3.681636, accumulated_submission_time=40331.181972, global_step=118206, preemption_count=0, score=40331.181972, test/accuracy=0.577100, test/loss=1.911431, test/num_examples=10000, total_duration=41736.874912, train/accuracy=0.805624, train/loss=0.709472, validation/accuracy=0.707000, validation/loss=1.188673, validation/num_examples=50000
I0130 07:09:02.291267 139656792352512 logging_writer.py:48] [118300] global_step=118300, grad_norm=2.606733560562134, loss=1.266377329826355
I0130 07:09:36.335703 139656783959808 logging_writer.py:48] [118400] global_step=118400, grad_norm=2.9419050216674805, loss=1.1697708368301392
I0130 07:10:10.631029 139656792352512 logging_writer.py:48] [118500] global_step=118500, grad_norm=2.7539596557617188, loss=1.1629399061203003
I0130 07:10:44.698774 139656783959808 logging_writer.py:48] [118600] global_step=118600, grad_norm=3.3462696075439453, loss=1.2718350887298584
I0130 07:11:18.754607 139656792352512 logging_writer.py:48] [118700] global_step=118700, grad_norm=2.9232733249664307, loss=1.1777387857437134
I0130 07:11:52.812294 139656783959808 logging_writer.py:48] [118800] global_step=118800, grad_norm=2.9604766368865967, loss=1.2319244146347046
I0130 07:12:26.897657 139656792352512 logging_writer.py:48] [118900] global_step=118900, grad_norm=3.0926101207733154, loss=1.2797349691390991
I0130 07:13:00.949569 139656783959808 logging_writer.py:48] [119000] global_step=119000, grad_norm=2.6429710388183594, loss=1.1492173671722412
I0130 07:13:35.008765 139656792352512 logging_writer.py:48] [119100] global_step=119100, grad_norm=2.702688694000244, loss=1.218093752861023
I0130 07:14:09.068837 139656783959808 logging_writer.py:48] [119200] global_step=119200, grad_norm=2.863971471786499, loss=1.1980758905410767
I0130 07:14:43.115863 139656792352512 logging_writer.py:48] [119300] global_step=119300, grad_norm=3.21895432472229, loss=1.2304977178573608
I0130 07:15:17.189654 139656783959808 logging_writer.py:48] [119400] global_step=119400, grad_norm=3.0901098251342773, loss=1.321791410446167
I0130 07:15:51.278411 139656792352512 logging_writer.py:48] [119500] global_step=119500, grad_norm=2.8267147541046143, loss=1.174111008644104
I0130 07:16:25.380743 139656783959808 logging_writer.py:48] [119600] global_step=119600, grad_norm=2.8933064937591553, loss=1.094784140586853
I0130 07:16:59.573619 139656792352512 logging_writer.py:48] [119700] global_step=119700, grad_norm=2.835930109024048, loss=1.1125662326812744
I0130 07:17:00.059790 139822745589568 spec.py:321] Evaluating on the training split.
I0130 07:17:06.345064 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 07:17:14.859132 139822745589568 spec.py:349] Evaluating on the test split.
I0130 07:17:17.344877 139822745589568 submission_runner.py:408] Time since start: 42264.34s, 	Step: 119703, 	{'train/accuracy': 0.8384486436843872, 'train/loss': 0.5811982154846191, 'validation/accuracy': 0.7049599885940552, 'validation/loss': 1.1874204874038696, 'validation/num_examples': 50000, 'test/accuracy': 0.5821000337600708, 'test/loss': 1.9020187854766846, 'test/num_examples': 10000, 'score': 40841.25449848175, 'total_duration': 42264.33503699303, 'accumulated_submission_time': 40841.25449848175, 'accumulated_eval_time': 1415.2570950984955, 'accumulated_logging_time': 3.732133626937866}
I0130 07:17:17.386879 139656825923328 logging_writer.py:48] [119703] accumulated_eval_time=1415.257095, accumulated_logging_time=3.732134, accumulated_submission_time=40841.254498, global_step=119703, preemption_count=0, score=40841.254498, test/accuracy=0.582100, test/loss=1.902019, test/num_examples=10000, total_duration=42264.335037, train/accuracy=0.838449, train/loss=0.581198, validation/accuracy=0.704960, validation/loss=1.187420, validation/num_examples=50000
I0130 07:17:50.756125 139656834316032 logging_writer.py:48] [119800] global_step=119800, grad_norm=3.1731791496276855, loss=1.2810224294662476
I0130 07:18:24.804144 139656825923328 logging_writer.py:48] [119900] global_step=119900, grad_norm=2.9948341846466064, loss=1.18850839138031
I0130 07:18:58.874983 139656834316032 logging_writer.py:48] [120000] global_step=120000, grad_norm=3.15785813331604, loss=1.2332909107208252
I0130 07:19:32.961112 139656825923328 logging_writer.py:48] [120100] global_step=120100, grad_norm=2.701923370361328, loss=1.1805380582809448
I0130 07:20:07.038443 139656834316032 logging_writer.py:48] [120200] global_step=120200, grad_norm=2.931344509124756, loss=1.150705099105835
I0130 07:20:41.141376 139656825923328 logging_writer.py:48] [120300] global_step=120300, grad_norm=2.7841124534606934, loss=1.1321767568588257
I0130 07:21:15.230463 139656834316032 logging_writer.py:48] [120400] global_step=120400, grad_norm=3.0118260383605957, loss=1.2728359699249268
I0130 07:21:49.331265 139656825923328 logging_writer.py:48] [120500] global_step=120500, grad_norm=3.182792901992798, loss=1.2699004411697388
I0130 07:22:23.419237 139656834316032 logging_writer.py:48] [120600] global_step=120600, grad_norm=3.1815452575683594, loss=1.2615549564361572
I0130 07:22:57.495610 139656825923328 logging_writer.py:48] [120700] global_step=120700, grad_norm=3.1953959465026855, loss=1.1475924253463745
I0130 07:23:31.666000 139656834316032 logging_writer.py:48] [120800] global_step=120800, grad_norm=3.044935703277588, loss=1.1772406101226807
I0130 07:24:05.732155 139656825923328 logging_writer.py:48] [120900] global_step=120900, grad_norm=2.9142048358917236, loss=1.1291983127593994
I0130 07:24:39.781794 139656834316032 logging_writer.py:48] [121000] global_step=121000, grad_norm=3.2191832065582275, loss=1.121577501296997
I0130 07:25:13.817316 139656825923328 logging_writer.py:48] [121100] global_step=121100, grad_norm=2.664825916290283, loss=1.1469693183898926
I0130 07:25:47.848844 139656834316032 logging_writer.py:48] [121200] global_step=121200, grad_norm=2.910031318664551, loss=1.1526398658752441
I0130 07:25:47.856892 139822745589568 spec.py:321] Evaluating on the training split.
I0130 07:25:54.042274 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 07:26:02.768190 139822745589568 spec.py:349] Evaluating on the test split.
I0130 07:26:05.251502 139822745589568 submission_runner.py:408] Time since start: 42792.24s, 	Step: 121201, 	{'train/accuracy': 0.8234614133834839, 'train/loss': 0.6339573860168457, 'validation/accuracy': 0.7074999809265137, 'validation/loss': 1.1968789100646973, 'validation/num_examples': 50000, 'test/accuracy': 0.5819000005722046, 'test/loss': 1.924055814743042, 'test/num_examples': 10000, 'score': 41351.663058280945, 'total_duration': 42792.24165916443, 'accumulated_submission_time': 41351.663058280945, 'accumulated_eval_time': 1432.6516358852386, 'accumulated_logging_time': 3.7847323417663574}
I0130 07:26:05.296845 139656792352512 logging_writer.py:48] [121201] accumulated_eval_time=1432.651636, accumulated_logging_time=3.784732, accumulated_submission_time=41351.663058, global_step=121201, preemption_count=0, score=41351.663058, test/accuracy=0.581900, test/loss=1.924056, test/num_examples=10000, total_duration=42792.241659, train/accuracy=0.823461, train/loss=0.633957, validation/accuracy=0.707500, validation/loss=1.196879, validation/num_examples=50000
I0130 07:26:39.306604 139656800745216 logging_writer.py:48] [121300] global_step=121300, grad_norm=3.025408983230591, loss=1.1271904706954956
I0130 07:27:13.336011 139656792352512 logging_writer.py:48] [121400] global_step=121400, grad_norm=3.1020305156707764, loss=1.321312665939331
I0130 07:27:47.410304 139656800745216 logging_writer.py:48] [121500] global_step=121500, grad_norm=2.769331693649292, loss=1.1817669868469238
I0130 07:28:21.489917 139656792352512 logging_writer.py:48] [121600] global_step=121600, grad_norm=2.911621570587158, loss=1.195801854133606
I0130 07:28:55.566236 139656800745216 logging_writer.py:48] [121700] global_step=121700, grad_norm=2.9873135089874268, loss=1.2104185819625854
I0130 07:29:29.651708 139656792352512 logging_writer.py:48] [121800] global_step=121800, grad_norm=2.8386428356170654, loss=1.2714474201202393
I0130 07:30:03.819340 139656800745216 logging_writer.py:48] [121900] global_step=121900, grad_norm=2.920032262802124, loss=1.1606523990631104
I0130 07:30:37.889899 139656792352512 logging_writer.py:48] [122000] global_step=122000, grad_norm=2.8328020572662354, loss=1.143370270729065
I0130 07:31:11.986121 139656800745216 logging_writer.py:48] [122100] global_step=122100, grad_norm=3.170025110244751, loss=1.1123833656311035
I0130 07:31:46.067502 139656792352512 logging_writer.py:48] [122200] global_step=122200, grad_norm=3.0130975246429443, loss=1.1335548162460327
I0130 07:32:20.147208 139656800745216 logging_writer.py:48] [122300] global_step=122300, grad_norm=3.2498438358306885, loss=1.2153799533843994
I0130 07:32:54.243141 139656792352512 logging_writer.py:48] [122400] global_step=122400, grad_norm=3.0563671588897705, loss=1.14936101436615
I0130 07:33:28.323141 139656800745216 logging_writer.py:48] [122500] global_step=122500, grad_norm=3.0632376670837402, loss=1.1825675964355469
I0130 07:34:02.386364 139656792352512 logging_writer.py:48] [122600] global_step=122600, grad_norm=3.0483860969543457, loss=1.198672890663147
I0130 07:34:35.273271 139822745589568 spec.py:321] Evaluating on the training split.
I0130 07:34:41.652121 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 07:34:50.060855 139822745589568 spec.py:349] Evaluating on the test split.
I0130 07:34:52.739602 139822745589568 submission_runner.py:408] Time since start: 43319.73s, 	Step: 122698, 	{'train/accuracy': 0.8205317258834839, 'train/loss': 0.6411925554275513, 'validation/accuracy': 0.707040011882782, 'validation/loss': 1.196536898612976, 'validation/num_examples': 50000, 'test/accuracy': 0.5800000429153442, 'test/loss': 1.9360084533691406, 'test/num_examples': 10000, 'score': 41861.57820510864, 'total_duration': 43319.72976899147, 'accumulated_submission_time': 41861.57820510864, 'accumulated_eval_time': 1450.1179354190826, 'accumulated_logging_time': 3.8395774364471436}
I0130 07:34:52.778712 139656783959808 logging_writer.py:48] [122698] accumulated_eval_time=1450.117935, accumulated_logging_time=3.839577, accumulated_submission_time=41861.578205, global_step=122698, preemption_count=0, score=41861.578205, test/accuracy=0.580000, test/loss=1.936008, test/num_examples=10000, total_duration=43319.729769, train/accuracy=0.820532, train/loss=0.641193, validation/accuracy=0.707040, validation/loss=1.196537, validation/num_examples=50000
I0130 07:34:53.809692 139656792352512 logging_writer.py:48] [122700] global_step=122700, grad_norm=2.868753433227539, loss=1.2170119285583496
I0130 07:35:27.887607 139656783959808 logging_writer.py:48] [122800] global_step=122800, grad_norm=3.3265163898468018, loss=1.1725038290023804
I0130 07:36:01.962274 139656792352512 logging_writer.py:48] [122900] global_step=122900, grad_norm=3.0318806171417236, loss=1.2150778770446777
I0130 07:36:36.135525 139656783959808 logging_writer.py:48] [123000] global_step=123000, grad_norm=3.2499866485595703, loss=1.179797887802124
I0130 07:37:10.240971 139656792352512 logging_writer.py:48] [123100] global_step=123100, grad_norm=3.0797746181488037, loss=1.2266801595687866
I0130 07:37:44.334622 139656783959808 logging_writer.py:48] [123200] global_step=123200, grad_norm=3.238159418106079, loss=1.1413254737854004
I0130 07:38:18.420743 139656792352512 logging_writer.py:48] [123300] global_step=123300, grad_norm=3.0653579235076904, loss=1.0691497325897217
I0130 07:38:52.481879 139656783959808 logging_writer.py:48] [123400] global_step=123400, grad_norm=3.1100375652313232, loss=1.1810393333435059
I0130 07:39:26.543539 139656792352512 logging_writer.py:48] [123500] global_step=123500, grad_norm=2.8793580532073975, loss=1.1609678268432617
I0130 07:40:00.632910 139656783959808 logging_writer.py:48] [123600] global_step=123600, grad_norm=3.2261528968811035, loss=1.2317414283752441
I0130 07:40:34.704398 139656792352512 logging_writer.py:48] [123700] global_step=123700, grad_norm=3.1020240783691406, loss=1.203873634338379
I0130 07:41:08.801703 139656783959808 logging_writer.py:48] [123800] global_step=123800, grad_norm=3.1640000343322754, loss=1.1825833320617676
I0130 07:41:42.867736 139656792352512 logging_writer.py:48] [123900] global_step=123900, grad_norm=3.2470157146453857, loss=1.1617224216461182
I0130 07:42:16.946929 139656783959808 logging_writer.py:48] [124000] global_step=124000, grad_norm=3.1863296031951904, loss=1.1562740802764893
I0130 07:42:51.045298 139656792352512 logging_writer.py:48] [124100] global_step=124100, grad_norm=2.91851806640625, loss=1.1873375177383423
I0130 07:43:22.742197 139822745589568 spec.py:321] Evaluating on the training split.
I0130 07:43:28.968492 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 07:43:37.364256 139822745589568 spec.py:349] Evaluating on the test split.
I0130 07:43:39.979612 139822745589568 submission_runner.py:408] Time since start: 43846.97s, 	Step: 124194, 	{'train/accuracy': 0.8153499364852905, 'train/loss': 0.6613895297050476, 'validation/accuracy': 0.7084000110626221, 'validation/loss': 1.1882474422454834, 'validation/num_examples': 50000, 'test/accuracy': 0.5819000005722046, 'test/loss': 1.9214253425598145, 'test/num_examples': 10000, 'score': 42371.47960424423, 'total_duration': 43846.969765901566, 'accumulated_submission_time': 42371.47960424423, 'accumulated_eval_time': 1467.3553059101105, 'accumulated_logging_time': 3.890242338180542}
I0130 07:43:40.023603 139656783959808 logging_writer.py:48] [124194] accumulated_eval_time=1467.355306, accumulated_logging_time=3.890242, accumulated_submission_time=42371.479604, global_step=124194, preemption_count=0, score=42371.479604, test/accuracy=0.581900, test/loss=1.921425, test/num_examples=10000, total_duration=43846.969766, train/accuracy=0.815350, train/loss=0.661390, validation/accuracy=0.708400, validation/loss=1.188247, validation/num_examples=50000
I0130 07:43:42.405552 139656825923328 logging_writer.py:48] [124200] global_step=124200, grad_norm=2.904679298400879, loss=1.108869194984436
I0130 07:44:16.448535 139656783959808 logging_writer.py:48] [124300] global_step=124300, grad_norm=2.910038709640503, loss=1.1023266315460205
I0130 07:44:50.470685 139656825923328 logging_writer.py:48] [124400] global_step=124400, grad_norm=3.3914177417755127, loss=1.2115830183029175
I0130 07:45:24.510833 139656783959808 logging_writer.py:48] [124500] global_step=124500, grad_norm=2.9429478645324707, loss=1.1162948608398438
I0130 07:45:58.587693 139656825923328 logging_writer.py:48] [124600] global_step=124600, grad_norm=3.1545238494873047, loss=1.103025197982788
I0130 07:46:32.659711 139656783959808 logging_writer.py:48] [124700] global_step=124700, grad_norm=2.972925901412964, loss=1.1318247318267822
I0130 07:47:06.744112 139656825923328 logging_writer.py:48] [124800] global_step=124800, grad_norm=3.12396502494812, loss=1.0174496173858643
I0130 07:47:40.825375 139656783959808 logging_writer.py:48] [124900] global_step=124900, grad_norm=3.161363124847412, loss=1.06658935546875
I0130 07:48:14.908172 139656825923328 logging_writer.py:48] [125000] global_step=125000, grad_norm=2.8200066089630127, loss=1.0167604684829712
I0130 07:48:48.943757 139656783959808 logging_writer.py:48] [125100] global_step=125100, grad_norm=3.024021625518799, loss=1.135396957397461
I0130 07:49:22.993680 139656825923328 logging_writer.py:48] [125200] global_step=125200, grad_norm=2.9966211318969727, loss=1.1408203840255737
I0130 07:49:57.200228 139656783959808 logging_writer.py:48] [125300] global_step=125300, grad_norm=3.252814531326294, loss=1.0702437162399292
I0130 07:50:31.297056 139656825923328 logging_writer.py:48] [125400] global_step=125400, grad_norm=3.1112771034240723, loss=1.1841768026351929
I0130 07:51:05.340569 139656783959808 logging_writer.py:48] [125500] global_step=125500, grad_norm=3.255899429321289, loss=1.2620960474014282
I0130 07:51:39.425974 139656825923328 logging_writer.py:48] [125600] global_step=125600, grad_norm=3.278684377670288, loss=1.200782299041748
I0130 07:52:10.236173 139822745589568 spec.py:321] Evaluating on the training split.
I0130 07:52:16.579517 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 07:52:25.001068 139822745589568 spec.py:349] Evaluating on the test split.
I0130 07:52:27.626134 139822745589568 submission_runner.py:408] Time since start: 44374.62s, 	Step: 125692, 	{'train/accuracy': 0.8228236436843872, 'train/loss': 0.6361862421035767, 'validation/accuracy': 0.7134000062942505, 'validation/loss': 1.174013614654541, 'validation/num_examples': 50000, 'test/accuracy': 0.5859000086784363, 'test/loss': 1.9083994626998901, 'test/num_examples': 10000, 'score': 42881.63268017769, 'total_duration': 44374.61629462242, 'accumulated_submission_time': 42881.63268017769, 'accumulated_eval_time': 1484.7452561855316, 'accumulated_logging_time': 3.9434943199157715}
I0130 07:52:27.666770 139656800745216 logging_writer.py:48] [125692] accumulated_eval_time=1484.745256, accumulated_logging_time=3.943494, accumulated_submission_time=42881.632680, global_step=125692, preemption_count=0, score=42881.632680, test/accuracy=0.585900, test/loss=1.908399, test/num_examples=10000, total_duration=44374.616295, train/accuracy=0.822824, train/loss=0.636186, validation/accuracy=0.713400, validation/loss=1.174014, validation/num_examples=50000
I0130 07:52:30.741992 139656809137920 logging_writer.py:48] [125700] global_step=125700, grad_norm=3.127883195877075, loss=1.0399051904678345
I0130 07:53:04.790720 139656800745216 logging_writer.py:48] [125800] global_step=125800, grad_norm=3.147454023361206, loss=1.216655969619751
I0130 07:53:38.854688 139656809137920 logging_writer.py:48] [125900] global_step=125900, grad_norm=3.1442320346832275, loss=1.1466888189315796
I0130 07:54:12.918111 139656800745216 logging_writer.py:48] [126000] global_step=126000, grad_norm=3.012995481491089, loss=1.1525375843048096
I0130 07:54:46.991173 139656809137920 logging_writer.py:48] [126100] global_step=126100, grad_norm=3.246995449066162, loss=1.1128352880477905
I0130 07:55:21.047771 139656800745216 logging_writer.py:48] [126200] global_step=126200, grad_norm=3.1747307777404785, loss=1.1846601963043213
I0130 07:55:55.130113 139656809137920 logging_writer.py:48] [126300] global_step=126300, grad_norm=3.0796735286712646, loss=1.1822118759155273
I0130 07:56:29.245613 139656800745216 logging_writer.py:48] [126400] global_step=126400, grad_norm=3.433934211730957, loss=1.1189665794372559
I0130 07:57:03.304679 139656809137920 logging_writer.py:48] [126500] global_step=126500, grad_norm=2.729975461959839, loss=0.999153733253479
I0130 07:57:37.363914 139656800745216 logging_writer.py:48] [126600] global_step=126600, grad_norm=3.2906594276428223, loss=1.1796560287475586
I0130 07:58:11.437815 139656809137920 logging_writer.py:48] [126700] global_step=126700, grad_norm=3.1133675575256348, loss=1.115565299987793
I0130 07:58:45.533210 139656800745216 logging_writer.py:48] [126800] global_step=126800, grad_norm=3.5633223056793213, loss=1.129931926727295
I0130 07:59:19.597595 139656809137920 logging_writer.py:48] [126900] global_step=126900, grad_norm=3.1793768405914307, loss=1.2314411401748657
I0130 07:59:53.681758 139656800745216 logging_writer.py:48] [127000] global_step=127000, grad_norm=3.4210715293884277, loss=1.1390966176986694
I0130 08:00:27.736661 139656809137920 logging_writer.py:48] [127100] global_step=127100, grad_norm=3.388953685760498, loss=1.1975926160812378
I0130 08:00:57.852026 139822745589568 spec.py:321] Evaluating on the training split.
I0130 08:01:04.175144 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 08:01:12.609507 139822745589568 spec.py:349] Evaluating on the test split.
I0130 08:01:15.218841 139822745589568 submission_runner.py:408] Time since start: 44902.21s, 	Step: 127190, 	{'train/accuracy': 0.8191167116165161, 'train/loss': 0.6429269909858704, 'validation/accuracy': 0.7112399935722351, 'validation/loss': 1.1914821863174438, 'validation/num_examples': 50000, 'test/accuracy': 0.5815000534057617, 'test/loss': 1.9391067028045654, 'test/num_examples': 10000, 'score': 43391.758655786514, 'total_duration': 44902.20900511742, 'accumulated_submission_time': 43391.758655786514, 'accumulated_eval_time': 1502.1120376586914, 'accumulated_logging_time': 3.9931235313415527}
I0130 08:01:15.261497 139656783959808 logging_writer.py:48] [127190] accumulated_eval_time=1502.112038, accumulated_logging_time=3.993124, accumulated_submission_time=43391.758656, global_step=127190, preemption_count=0, score=43391.758656, test/accuracy=0.581500, test/loss=1.939107, test/num_examples=10000, total_duration=44902.209005, train/accuracy=0.819117, train/loss=0.642927, validation/accuracy=0.711240, validation/loss=1.191482, validation/num_examples=50000
I0130 08:01:19.014344 139656792352512 logging_writer.py:48] [127200] global_step=127200, grad_norm=3.304075002670288, loss=1.1039516925811768
I0130 08:01:53.066137 139656783959808 logging_writer.py:48] [127300] global_step=127300, grad_norm=3.0157179832458496, loss=1.1219487190246582
I0130 08:02:27.123292 139656792352512 logging_writer.py:48] [127400] global_step=127400, grad_norm=2.9601733684539795, loss=1.0637290477752686
I0130 08:03:01.211826 139656783959808 logging_writer.py:48] [127500] global_step=127500, grad_norm=2.8824853897094727, loss=0.984428882598877
I0130 08:03:35.409356 139656792352512 logging_writer.py:48] [127600] global_step=127600, grad_norm=3.044522762298584, loss=1.1549897193908691
I0130 08:04:09.495721 139656783959808 logging_writer.py:48] [127700] global_step=127700, grad_norm=3.443571090698242, loss=1.1944149732589722
I0130 08:04:43.570304 139656792352512 logging_writer.py:48] [127800] global_step=127800, grad_norm=3.536951780319214, loss=1.2052826881408691
I0130 08:05:17.665124 139656783959808 logging_writer.py:48] [127900] global_step=127900, grad_norm=3.2515482902526855, loss=1.1118626594543457
I0130 08:05:51.736238 139656792352512 logging_writer.py:48] [128000] global_step=128000, grad_norm=3.3146684169769287, loss=1.23486328125
I0130 08:06:25.850614 139656783959808 logging_writer.py:48] [128100] global_step=128100, grad_norm=3.1562581062316895, loss=1.1199445724487305
I0130 08:06:59.912403 139656792352512 logging_writer.py:48] [128200] global_step=128200, grad_norm=3.096433639526367, loss=1.053694486618042
I0130 08:07:34.013763 139656783959808 logging_writer.py:48] [128300] global_step=128300, grad_norm=3.442190170288086, loss=1.0981786251068115
I0130 08:08:08.121610 139656792352512 logging_writer.py:48] [128400] global_step=128400, grad_norm=3.2287709712982178, loss=1.072935938835144
I0130 08:08:42.218698 139656783959808 logging_writer.py:48] [128500] global_step=128500, grad_norm=3.144209623336792, loss=1.0957295894622803
I0130 08:09:16.302466 139656792352512 logging_writer.py:48] [128600] global_step=128600, grad_norm=3.0334506034851074, loss=1.0784980058670044
I0130 08:09:45.531087 139822745589568 spec.py:321] Evaluating on the training split.
I0130 08:09:51.782144 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 08:10:00.386924 139822745589568 spec.py:349] Evaluating on the test split.
I0130 08:10:03.141393 139822745589568 submission_runner.py:408] Time since start: 45430.13s, 	Step: 128687, 	{'train/accuracy': 0.8657525181770325, 'train/loss': 0.4807564318180084, 'validation/accuracy': 0.7155599594116211, 'validation/loss': 1.1618179082870483, 'validation/num_examples': 50000, 'test/accuracy': 0.5940000414848328, 'test/loss': 1.8702696561813354, 'test/num_examples': 10000, 'score': 43901.96710586548, 'total_duration': 45430.13155961037, 'accumulated_submission_time': 43901.96710586548, 'accumulated_eval_time': 1519.7223196029663, 'accumulated_logging_time': 4.045359134674072}
I0130 08:10:03.181602 139656792352512 logging_writer.py:48] [128687] accumulated_eval_time=1519.722320, accumulated_logging_time=4.045359, accumulated_submission_time=43901.967106, global_step=128687, preemption_count=0, score=43901.967106, test/accuracy=0.594000, test/loss=1.870270, test/num_examples=10000, total_duration=45430.131560, train/accuracy=0.865753, train/loss=0.480756, validation/accuracy=0.715560, validation/loss=1.161818, validation/num_examples=50000
I0130 08:10:07.965676 139656809137920 logging_writer.py:48] [128700] global_step=128700, grad_norm=2.996694803237915, loss=1.057778239250183
I0130 08:10:42.009131 139656792352512 logging_writer.py:48] [128800] global_step=128800, grad_norm=3.1195852756500244, loss=1.1251420974731445
I0130 08:11:16.078746 139656809137920 logging_writer.py:48] [128900] global_step=128900, grad_norm=3.1523306369781494, loss=1.0555468797683716
I0130 08:11:50.176728 139656792352512 logging_writer.py:48] [129000] global_step=129000, grad_norm=3.040952205657959, loss=1.0217784643173218
I0130 08:12:24.230481 139656809137920 logging_writer.py:48] [129100] global_step=129100, grad_norm=3.199458360671997, loss=1.1535409688949585
I0130 08:12:58.294566 139656792352512 logging_writer.py:48] [129200] global_step=129200, grad_norm=3.12741756439209, loss=1.1529648303985596
I0130 08:13:32.346670 139656809137920 logging_writer.py:48] [129300] global_step=129300, grad_norm=3.5192959308624268, loss=1.0654735565185547
I0130 08:14:06.402940 139656792352512 logging_writer.py:48] [129400] global_step=129400, grad_norm=3.455134868621826, loss=1.1921330690383911
I0130 08:14:40.455729 139656809137920 logging_writer.py:48] [129500] global_step=129500, grad_norm=3.1506874561309814, loss=1.0349721908569336
I0130 08:15:14.540236 139656792352512 logging_writer.py:48] [129600] global_step=129600, grad_norm=3.284278631210327, loss=1.179311752319336
I0130 08:15:48.625313 139656809137920 logging_writer.py:48] [129700] global_step=129700, grad_norm=3.1080379486083984, loss=1.0200504064559937
I0130 08:16:22.945278 139656792352512 logging_writer.py:48] [129800] global_step=129800, grad_norm=3.322990655899048, loss=1.1527214050292969
I0130 08:16:57.041873 139656809137920 logging_writer.py:48] [129900] global_step=129900, grad_norm=3.1443450450897217, loss=1.0593245029449463
I0130 08:17:31.142496 139656792352512 logging_writer.py:48] [130000] global_step=130000, grad_norm=3.6210665702819824, loss=1.1293798685073853
I0130 08:18:05.237783 139656809137920 logging_writer.py:48] [130100] global_step=130100, grad_norm=3.2618894577026367, loss=1.1400233507156372
I0130 08:18:33.335210 139822745589568 spec.py:321] Evaluating on the training split.
I0130 08:18:39.590859 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 08:18:47.963607 139822745589568 spec.py:349] Evaluating on the test split.
I0130 08:18:50.556843 139822745589568 submission_runner.py:408] Time since start: 45957.55s, 	Step: 130184, 	{'train/accuracy': 0.8509646058082581, 'train/loss': 0.52117919921875, 'validation/accuracy': 0.7196599841117859, 'validation/loss': 1.150366187095642, 'validation/num_examples': 50000, 'test/accuracy': 0.5944000482559204, 'test/loss': 1.8714016675949097, 'test/num_examples': 10000, 'score': 44412.060628175735, 'total_duration': 45957.546800136566, 'accumulated_submission_time': 44412.060628175735, 'accumulated_eval_time': 1536.9437124729156, 'accumulated_logging_time': 4.095425844192505}
I0130 08:18:50.603151 139656825923328 logging_writer.py:48] [130184] accumulated_eval_time=1536.943712, accumulated_logging_time=4.095426, accumulated_submission_time=44412.060628, global_step=130184, preemption_count=0, score=44412.060628, test/accuracy=0.594400, test/loss=1.871402, test/num_examples=10000, total_duration=45957.546800, train/accuracy=0.850965, train/loss=0.521179, validation/accuracy=0.719660, validation/loss=1.150366, validation/num_examples=50000
I0130 08:18:56.402510 139656834316032 logging_writer.py:48] [130200] global_step=130200, grad_norm=3.1945230960845947, loss=1.056400179862976
I0130 08:19:30.444891 139656825923328 logging_writer.py:48] [130300] global_step=130300, grad_norm=3.3396098613739014, loss=1.0529155731201172
I0130 08:20:04.513717 139656834316032 logging_writer.py:48] [130400] global_step=130400, grad_norm=3.3186018466949463, loss=1.1076281070709229
I0130 08:20:38.585174 139656825923328 logging_writer.py:48] [130500] global_step=130500, grad_norm=3.3176445960998535, loss=1.092587947845459
I0130 08:21:12.680861 139656834316032 logging_writer.py:48] [130600] global_step=130600, grad_norm=2.9537832736968994, loss=1.025802731513977
I0130 08:21:46.761920 139656825923328 logging_writer.py:48] [130700] global_step=130700, grad_norm=3.5179319381713867, loss=1.1635956764221191
I0130 08:22:20.843571 139656834316032 logging_writer.py:48] [130800] global_step=130800, grad_norm=3.257394313812256, loss=1.165647029876709
I0130 08:22:54.942592 139656825923328 logging_writer.py:48] [130900] global_step=130900, grad_norm=3.2493104934692383, loss=1.0821850299835205
I0130 08:23:29.161045 139656834316032 logging_writer.py:48] [131000] global_step=131000, grad_norm=3.1036689281463623, loss=1.0263065099716187
I0130 08:24:03.243189 139656825923328 logging_writer.py:48] [131100] global_step=131100, grad_norm=3.628305435180664, loss=1.0456979274749756
I0130 08:24:37.304735 139656834316032 logging_writer.py:48] [131200] global_step=131200, grad_norm=3.7426669597625732, loss=1.0631085634231567
I0130 08:25:11.380591 139656825923328 logging_writer.py:48] [131300] global_step=131300, grad_norm=3.229387044906616, loss=1.0839693546295166
I0130 08:25:45.458575 139656834316032 logging_writer.py:48] [131400] global_step=131400, grad_norm=3.7319791316986084, loss=1.0789200067520142
I0130 08:26:19.524255 139656825923328 logging_writer.py:48] [131500] global_step=131500, grad_norm=3.209726572036743, loss=1.005419373512268
I0130 08:26:53.591789 139656834316032 logging_writer.py:48] [131600] global_step=131600, grad_norm=3.0756497383117676, loss=1.010014295578003
I0130 08:27:20.672477 139822745589568 spec.py:321] Evaluating on the training split.
I0130 08:27:26.888406 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 08:27:35.439731 139822745589568 spec.py:349] Evaluating on the test split.
I0130 08:27:38.050435 139822745589568 submission_runner.py:408] Time since start: 46485.04s, 	Step: 131681, 	{'train/accuracy': 0.8581393361091614, 'train/loss': 0.5003353953361511, 'validation/accuracy': 0.721340000629425, 'validation/loss': 1.128917932510376, 'validation/num_examples': 50000, 'test/accuracy': 0.5985000133514404, 'test/loss': 1.852310061454773, 'test/num_examples': 10000, 'score': 44922.07017183304, 'total_duration': 46485.04060125351, 'accumulated_submission_time': 44922.07017183304, 'accumulated_eval_time': 1554.3216423988342, 'accumulated_logging_time': 4.150796890258789}
I0130 08:27:38.097503 139656666543872 logging_writer.py:48] [131681] accumulated_eval_time=1554.321642, accumulated_logging_time=4.150797, accumulated_submission_time=44922.070172, global_step=131681, preemption_count=0, score=44922.070172, test/accuracy=0.598500, test/loss=1.852310, test/num_examples=10000, total_duration=46485.040601, train/accuracy=0.858139, train/loss=0.500335, validation/accuracy=0.721340, validation/loss=1.128918, validation/num_examples=50000
I0130 08:27:44.920078 139656783959808 logging_writer.py:48] [131700] global_step=131700, grad_norm=3.4463412761688232, loss=1.1750441789627075
I0130 08:28:19.008350 139656666543872 logging_writer.py:48] [131800] global_step=131800, grad_norm=3.6053292751312256, loss=1.1030478477478027
I0130 08:28:53.088887 139656783959808 logging_writer.py:48] [131900] global_step=131900, grad_norm=3.571589231491089, loss=1.040761947631836
I0130 08:29:27.176644 139656666543872 logging_writer.py:48] [132000] global_step=132000, grad_norm=3.3244996070861816, loss=1.086052417755127
I0130 08:30:01.350053 139656783959808 logging_writer.py:48] [132100] global_step=132100, grad_norm=3.3920204639434814, loss=1.0406049489974976
I0130 08:30:35.443101 139656666543872 logging_writer.py:48] [132200] global_step=132200, grad_norm=3.2564821243286133, loss=0.976306676864624
I0130 08:31:09.525327 139656783959808 logging_writer.py:48] [132300] global_step=132300, grad_norm=3.0612313747406006, loss=0.9381054639816284
I0130 08:31:43.604036 139656666543872 logging_writer.py:48] [132400] global_step=132400, grad_norm=3.2488887310028076, loss=1.0030977725982666
I0130 08:32:17.673271 139656783959808 logging_writer.py:48] [132500] global_step=132500, grad_norm=3.6775906085968018, loss=1.0988807678222656
I0130 08:32:51.773969 139656666543872 logging_writer.py:48] [132600] global_step=132600, grad_norm=3.1845884323120117, loss=1.1279655694961548
I0130 08:33:25.873314 139656783959808 logging_writer.py:48] [132700] global_step=132700, grad_norm=3.5450873374938965, loss=1.0832898616790771
I0130 08:33:59.970519 139656666543872 logging_writer.py:48] [132800] global_step=132800, grad_norm=3.500870704650879, loss=1.139188289642334
I0130 08:34:34.065420 139656783959808 logging_writer.py:48] [132900] global_step=132900, grad_norm=3.4560763835906982, loss=1.0104846954345703
I0130 08:35:08.132832 139656666543872 logging_writer.py:48] [133000] global_step=133000, grad_norm=3.6140732765197754, loss=1.203529715538025
I0130 08:35:42.220373 139656783959808 logging_writer.py:48] [133100] global_step=133100, grad_norm=3.1455700397491455, loss=1.0222960710525513
I0130 08:36:08.260536 139822745589568 spec.py:321] Evaluating on the training split.
I0130 08:36:14.592500 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 08:36:22.984217 139822745589568 spec.py:349] Evaluating on the test split.
I0130 08:36:25.617802 139822745589568 submission_runner.py:408] Time since start: 47012.61s, 	Step: 133178, 	{'train/accuracy': 0.8520009517669678, 'train/loss': 0.5243096351623535, 'validation/accuracy': 0.7253999710083008, 'validation/loss': 1.120864987373352, 'validation/num_examples': 50000, 'test/accuracy': 0.5966000556945801, 'test/loss': 1.8334498405456543, 'test/num_examples': 10000, 'score': 45432.17251110077, 'total_duration': 47012.60780596733, 'accumulated_submission_time': 45432.17251110077, 'accumulated_eval_time': 1571.6787357330322, 'accumulated_logging_time': 4.2078258991241455}
I0130 08:36:25.657823 139656817530624 logging_writer.py:48] [133178] accumulated_eval_time=1571.678736, accumulated_logging_time=4.207826, accumulated_submission_time=45432.172511, global_step=133178, preemption_count=0, score=45432.172511, test/accuracy=0.596600, test/loss=1.833450, test/num_examples=10000, total_duration=47012.607806, train/accuracy=0.852001, train/loss=0.524310, validation/accuracy=0.725400, validation/loss=1.120865, validation/num_examples=50000
I0130 08:36:33.688091 139656825923328 logging_writer.py:48] [133200] global_step=133200, grad_norm=3.5649380683898926, loss=1.0706710815429688
I0130 08:37:07.733264 139656817530624 logging_writer.py:48] [133300] global_step=133300, grad_norm=3.7078263759613037, loss=1.0650582313537598
I0130 08:37:41.790960 139656825923328 logging_writer.py:48] [133400] global_step=133400, grad_norm=3.525519609451294, loss=0.9916732907295227
I0130 08:38:15.881751 139656817530624 logging_writer.py:48] [133500] global_step=133500, grad_norm=3.514019727706909, loss=1.039278507232666
I0130 08:38:49.985084 139656825923328 logging_writer.py:48] [133600] global_step=133600, grad_norm=3.195120096206665, loss=0.9988598823547363
I0130 08:39:24.075288 139656817530624 logging_writer.py:48] [133700] global_step=133700, grad_norm=3.215622901916504, loss=1.1097228527069092
I0130 08:39:58.164417 139656825923328 logging_writer.py:48] [133800] global_step=133800, grad_norm=3.097454309463501, loss=0.9547407627105713
I0130 08:40:32.257682 139656817530624 logging_writer.py:48] [133900] global_step=133900, grad_norm=3.48769474029541, loss=1.144819736480713
I0130 08:41:06.371162 139656825923328 logging_writer.py:48] [134000] global_step=134000, grad_norm=3.3141472339630127, loss=1.0378204584121704
I0130 08:41:40.460605 139656817530624 logging_writer.py:48] [134100] global_step=134100, grad_norm=3.6996428966522217, loss=1.0842995643615723
I0130 08:42:14.530402 139656825923328 logging_writer.py:48] [134200] global_step=134200, grad_norm=3.3775646686553955, loss=1.0067203044891357
I0130 08:42:48.610461 139656817530624 logging_writer.py:48] [134300] global_step=134300, grad_norm=3.2306113243103027, loss=1.0316059589385986
I0130 08:43:22.739729 139656825923328 logging_writer.py:48] [134400] global_step=134400, grad_norm=3.1704046726226807, loss=1.0107240676879883
I0130 08:43:56.834581 139656817530624 logging_writer.py:48] [134500] global_step=134500, grad_norm=3.158416986465454, loss=1.0702602863311768
I0130 08:44:30.929217 139656825923328 logging_writer.py:48] [134600] global_step=134600, grad_norm=3.2428224086761475, loss=1.049956202507019
I0130 08:44:55.624615 139822745589568 spec.py:321] Evaluating on the training split.
I0130 08:45:02.662389 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 08:45:11.144031 139822745589568 spec.py:349] Evaluating on the test split.
I0130 08:45:13.735387 139822745589568 submission_runner.py:408] Time since start: 47540.73s, 	Step: 134674, 	{'train/accuracy': 0.8475764989852905, 'train/loss': 0.5381889343261719, 'validation/accuracy': 0.7217999696731567, 'validation/loss': 1.1407110691070557, 'validation/num_examples': 50000, 'test/accuracy': 0.6020000576972961, 'test/loss': 1.8749887943267822, 'test/num_examples': 10000, 'score': 45942.078404426575, 'total_duration': 47540.725546360016, 'accumulated_submission_time': 45942.078404426575, 'accumulated_eval_time': 1589.7894802093506, 'accumulated_logging_time': 4.2574567794799805}
I0130 08:45:13.778455 139656792352512 logging_writer.py:48] [134674] accumulated_eval_time=1589.789480, accumulated_logging_time=4.257457, accumulated_submission_time=45942.078404, global_step=134674, preemption_count=0, score=45942.078404, test/accuracy=0.602000, test/loss=1.874989, test/num_examples=10000, total_duration=47540.725546, train/accuracy=0.847576, train/loss=0.538189, validation/accuracy=0.721800, validation/loss=1.140711, validation/num_examples=50000
I0130 08:45:22.987373 139656800745216 logging_writer.py:48] [134700] global_step=134700, grad_norm=3.385747194290161, loss=1.0896178483963013
I0130 08:45:57.050053 139656792352512 logging_writer.py:48] [134800] global_step=134800, grad_norm=3.470613718032837, loss=1.035668969154358
I0130 08:46:31.134987 139656800745216 logging_writer.py:48] [134900] global_step=134900, grad_norm=3.653855800628662, loss=1.039475679397583
I0130 08:47:05.234865 139656792352512 logging_writer.py:48] [135000] global_step=135000, grad_norm=3.934964179992676, loss=1.0294376611709595
I0130 08:47:39.328276 139656800745216 logging_writer.py:48] [135100] global_step=135100, grad_norm=3.129732131958008, loss=0.9741091728210449
I0130 08:48:13.395661 139656792352512 logging_writer.py:48] [135200] global_step=135200, grad_norm=3.357234239578247, loss=0.9843363761901855
I0130 08:48:47.451191 139656800745216 logging_writer.py:48] [135300] global_step=135300, grad_norm=3.719337224960327, loss=1.0955393314361572
I0130 08:49:21.523926 139656792352512 logging_writer.py:48] [135400] global_step=135400, grad_norm=3.3012099266052246, loss=1.0310611724853516
I0130 08:49:55.669207 139656800745216 logging_writer.py:48] [135500] global_step=135500, grad_norm=3.271702766418457, loss=0.9271326661109924
I0130 08:50:29.742420 139656792352512 logging_writer.py:48] [135600] global_step=135600, grad_norm=3.3651282787323, loss=0.9686129093170166
I0130 08:51:03.842364 139656800745216 logging_writer.py:48] [135700] global_step=135700, grad_norm=3.3411028385162354, loss=1.0255751609802246
I0130 08:51:37.912264 139656792352512 logging_writer.py:48] [135800] global_step=135800, grad_norm=3.590238094329834, loss=1.0270732641220093
I0130 08:52:11.958930 139656800745216 logging_writer.py:48] [135900] global_step=135900, grad_norm=3.465024471282959, loss=1.0606729984283447
I0130 08:52:46.052507 139656792352512 logging_writer.py:48] [136000] global_step=136000, grad_norm=3.2876296043395996, loss=0.9288463592529297
I0130 08:53:20.122291 139656800745216 logging_writer.py:48] [136100] global_step=136100, grad_norm=3.309211492538452, loss=1.041408896446228
I0130 08:53:43.802467 139822745589568 spec.py:321] Evaluating on the training split.
I0130 08:53:50.083902 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 08:53:58.452041 139822745589568 spec.py:349] Evaluating on the test split.
I0130 08:54:01.062448 139822745589568 submission_runner.py:408] Time since start: 48068.05s, 	Step: 136171, 	{'train/accuracy': 0.8484932780265808, 'train/loss': 0.5277658104896545, 'validation/accuracy': 0.7238399982452393, 'validation/loss': 1.1313154697418213, 'validation/num_examples': 50000, 'test/accuracy': 0.5981000065803528, 'test/loss': 1.8592684268951416, 'test/num_examples': 10000, 'score': 46452.042345047, 'total_duration': 48068.0526099205, 'accumulated_submission_time': 46452.042345047, 'accumulated_eval_time': 1607.0494379997253, 'accumulated_logging_time': 4.309852600097656}
I0130 08:54:01.104303 139656825923328 logging_writer.py:48] [136171] accumulated_eval_time=1607.049438, accumulated_logging_time=4.309853, accumulated_submission_time=46452.042345, global_step=136171, preemption_count=0, score=46452.042345, test/accuracy=0.598100, test/loss=1.859268, test/num_examples=10000, total_duration=48068.052610, train/accuracy=0.848493, train/loss=0.527766, validation/accuracy=0.723840, validation/loss=1.131315, validation/num_examples=50000
I0130 08:54:11.321342 139656834316032 logging_writer.py:48] [136200] global_step=136200, grad_norm=3.543802261352539, loss=0.9919723272323608
I0130 08:54:45.365514 139656825923328 logging_writer.py:48] [136300] global_step=136300, grad_norm=3.2144293785095215, loss=0.9420347809791565
I0130 08:55:19.438107 139656834316032 logging_writer.py:48] [136400] global_step=136400, grad_norm=3.3740854263305664, loss=0.9868459701538086
I0130 08:55:53.517435 139656825923328 logging_writer.py:48] [136500] global_step=136500, grad_norm=3.631056785583496, loss=1.0380414724349976
I0130 08:56:27.684166 139656834316032 logging_writer.py:48] [136600] global_step=136600, grad_norm=3.6166305541992188, loss=1.1570698022842407
I0130 08:57:01.768735 139656825923328 logging_writer.py:48] [136700] global_step=136700, grad_norm=3.5346009731292725, loss=1.0113846063613892
I0130 08:57:35.845015 139656834316032 logging_writer.py:48] [136800] global_step=136800, grad_norm=3.3743176460266113, loss=0.999901294708252
I0130 08:58:09.909507 139656825923328 logging_writer.py:48] [136900] global_step=136900, grad_norm=3.488668441772461, loss=1.0193595886230469
I0130 08:58:43.990601 139656834316032 logging_writer.py:48] [137000] global_step=137000, grad_norm=3.395312547683716, loss=0.9693131446838379
I0130 08:59:18.074877 139656825923328 logging_writer.py:48] [137100] global_step=137100, grad_norm=3.758946180343628, loss=1.0468471050262451
I0130 08:59:52.150809 139656834316032 logging_writer.py:48] [137200] global_step=137200, grad_norm=3.2827768325805664, loss=0.9475857019424438
I0130 09:00:26.236161 139656825923328 logging_writer.py:48] [137300] global_step=137300, grad_norm=3.551273822784424, loss=1.0189526081085205
I0130 09:01:00.323265 139656834316032 logging_writer.py:48] [137400] global_step=137400, grad_norm=3.399923086166382, loss=0.993111789226532
I0130 09:01:34.412179 139656825923328 logging_writer.py:48] [137500] global_step=137500, grad_norm=3.737545967102051, loss=0.9663852453231812
I0130 09:02:08.492046 139656834316032 logging_writer.py:48] [137600] global_step=137600, grad_norm=4.006673336029053, loss=0.9950354099273682
I0130 09:02:31.114445 139822745589568 spec.py:321] Evaluating on the training split.
I0130 09:02:37.345352 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 09:02:45.675918 139822745589568 spec.py:349] Evaluating on the test split.
I0130 09:02:48.368584 139822745589568 submission_runner.py:408] Time since start: 48595.36s, 	Step: 137668, 	{'train/accuracy': 0.865652859210968, 'train/loss': 0.4712446928024292, 'validation/accuracy': 0.7237399816513062, 'validation/loss': 1.1422827243804932, 'validation/num_examples': 50000, 'test/accuracy': 0.5975000262260437, 'test/loss': 1.8960363864898682, 'test/num_examples': 10000, 'score': 46961.99141907692, 'total_duration': 48595.35859775543, 'accumulated_submission_time': 46961.99141907692, 'accumulated_eval_time': 1624.3033895492554, 'accumulated_logging_time': 4.361900568008423}
I0130 09:02:48.413005 139656783959808 logging_writer.py:48] [137668] accumulated_eval_time=1624.303390, accumulated_logging_time=4.361901, accumulated_submission_time=46961.991419, global_step=137668, preemption_count=0, score=46961.991419, test/accuracy=0.597500, test/loss=1.896036, test/num_examples=10000, total_duration=48595.358598, train/accuracy=0.865653, train/loss=0.471245, validation/accuracy=0.723740, validation/loss=1.142283, validation/num_examples=50000
I0130 09:02:59.751405 139656792352512 logging_writer.py:48] [137700] global_step=137700, grad_norm=3.9400346279144287, loss=1.1378560066223145
I0130 09:03:33.807835 139656783959808 logging_writer.py:48] [137800] global_step=137800, grad_norm=3.7724878787994385, loss=1.1664074659347534
I0130 09:04:07.885454 139656792352512 logging_writer.py:48] [137900] global_step=137900, grad_norm=3.87263560295105, loss=1.082760214805603
I0130 09:04:41.955766 139656783959808 logging_writer.py:48] [138000] global_step=138000, grad_norm=3.9522993564605713, loss=0.9812872409820557
I0130 09:05:16.028062 139656792352512 logging_writer.py:48] [138100] global_step=138100, grad_norm=3.812089443206787, loss=0.9701158404350281
I0130 09:05:50.087392 139656783959808 logging_writer.py:48] [138200] global_step=138200, grad_norm=3.724760055541992, loss=1.0180957317352295
I0130 09:06:24.169779 139656792352512 logging_writer.py:48] [138300] global_step=138300, grad_norm=3.3440957069396973, loss=1.0306286811828613
I0130 09:06:58.248223 139656783959808 logging_writer.py:48] [138400] global_step=138400, grad_norm=3.780780553817749, loss=1.1100382804870605
I0130 09:07:32.317763 139656792352512 logging_writer.py:48] [138500] global_step=138500, grad_norm=3.672212839126587, loss=1.0855722427368164
I0130 09:08:06.419440 139656783959808 logging_writer.py:48] [138600] global_step=138600, grad_norm=3.4768309593200684, loss=1.008976936340332
I0130 09:08:40.500246 139656792352512 logging_writer.py:48] [138700] global_step=138700, grad_norm=3.8173859119415283, loss=1.0101443529129028
I0130 09:09:14.582588 139656783959808 logging_writer.py:48] [138800] global_step=138800, grad_norm=4.214208126068115, loss=1.0457733869552612
I0130 09:09:48.723653 139656792352512 logging_writer.py:48] [138900] global_step=138900, grad_norm=3.813420534133911, loss=1.0527933835983276
I0130 09:10:22.804397 139656783959808 logging_writer.py:48] [139000] global_step=139000, grad_norm=3.6228036880493164, loss=0.945042073726654
I0130 09:10:56.877749 139656792352512 logging_writer.py:48] [139100] global_step=139100, grad_norm=3.785846710205078, loss=1.0444873571395874
I0130 09:11:18.498917 139822745589568 spec.py:321] Evaluating on the training split.
I0130 09:11:24.753050 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 09:11:33.312223 139822745589568 spec.py:349] Evaluating on the test split.
I0130 09:11:35.941462 139822745589568 submission_runner.py:408] Time since start: 49122.93s, 	Step: 139165, 	{'train/accuracy': 0.8843072056770325, 'train/loss': 0.402018278837204, 'validation/accuracy': 0.7286999821662903, 'validation/loss': 1.1118732690811157, 'validation/num_examples': 50000, 'test/accuracy': 0.612500011920929, 'test/loss': 1.8532575368881226, 'test/num_examples': 10000, 'score': 47472.01818680763, 'total_duration': 49122.931619644165, 'accumulated_submission_time': 47472.01818680763, 'accumulated_eval_time': 1641.7458896636963, 'accumulated_logging_time': 4.415813446044922}
I0130 09:11:35.989112 139656825923328 logging_writer.py:48] [139165] accumulated_eval_time=1641.745890, accumulated_logging_time=4.415813, accumulated_submission_time=47472.018187, global_step=139165, preemption_count=0, score=47472.018187, test/accuracy=0.612500, test/loss=1.853258, test/num_examples=10000, total_duration=49122.931620, train/accuracy=0.884307, train/loss=0.402018, validation/accuracy=0.728700, validation/loss=1.111873, validation/num_examples=50000
I0130 09:11:48.244038 139656834316032 logging_writer.py:48] [139200] global_step=139200, grad_norm=3.5059409141540527, loss=0.9499508142471313
I0130 09:12:22.300746 139656825923328 logging_writer.py:48] [139300] global_step=139300, grad_norm=3.6743218898773193, loss=1.0682741403579712
I0130 09:12:56.325879 139656834316032 logging_writer.py:48] [139400] global_step=139400, grad_norm=3.8126516342163086, loss=1.0737881660461426
I0130 09:13:30.356735 139656825923328 logging_writer.py:48] [139500] global_step=139500, grad_norm=3.6106786727905273, loss=0.9533289074897766
I0130 09:14:04.446487 139656834316032 logging_writer.py:48] [139600] global_step=139600, grad_norm=3.4033305644989014, loss=0.9613422155380249
I0130 09:14:38.521129 139656825923328 logging_writer.py:48] [139700] global_step=139700, grad_norm=3.7245712280273438, loss=1.0154560804367065
I0130 09:15:12.558430 139656834316032 logging_writer.py:48] [139800] global_step=139800, grad_norm=3.454770565032959, loss=0.9566981196403503
I0130 09:15:46.625348 139656825923328 logging_writer.py:48] [139900] global_step=139900, grad_norm=3.947965145111084, loss=0.9683065414428711
I0130 09:16:20.779516 139656834316032 logging_writer.py:48] [140000] global_step=140000, grad_norm=3.6782894134521484, loss=0.9675491452217102
I0130 09:16:54.865407 139656825923328 logging_writer.py:48] [140100] global_step=140100, grad_norm=3.803079843521118, loss=0.9424161314964294
I0130 09:17:28.910052 139656834316032 logging_writer.py:48] [140200] global_step=140200, grad_norm=3.927882194519043, loss=0.9845629930496216
I0130 09:18:02.979264 139656825923328 logging_writer.py:48] [140300] global_step=140300, grad_norm=3.571638345718384, loss=0.9744535088539124
I0130 09:18:37.081860 139656834316032 logging_writer.py:48] [140400] global_step=140400, grad_norm=3.4868245124816895, loss=0.9404674172401428
I0130 09:19:11.192065 139656825923328 logging_writer.py:48] [140500] global_step=140500, grad_norm=3.727599620819092, loss=1.0074412822723389
I0130 09:19:45.281942 139656834316032 logging_writer.py:48] [140600] global_step=140600, grad_norm=4.073323726654053, loss=1.0068539381027222
I0130 09:20:06.212397 139822745589568 spec.py:321] Evaluating on the training split.
I0130 09:20:12.477631 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 09:20:20.927366 139822745589568 spec.py:349] Evaluating on the test split.
I0130 09:20:23.548811 139822745589568 submission_runner.py:408] Time since start: 49650.54s, 	Step: 140663, 	{'train/accuracy': 0.87890625, 'train/loss': 0.42630431056022644, 'validation/accuracy': 0.7313599586486816, 'validation/loss': 1.112805724143982, 'validation/num_examples': 50000, 'test/accuracy': 0.6103000044822693, 'test/loss': 1.8223932981491089, 'test/num_examples': 10000, 'score': 47982.18217277527, 'total_duration': 49650.53896570206, 'accumulated_submission_time': 47982.18217277527, 'accumulated_eval_time': 1659.0822570323944, 'accumulated_logging_time': 4.472891807556152}
I0130 09:20:23.591866 139656792352512 logging_writer.py:48] [140663] accumulated_eval_time=1659.082257, accumulated_logging_time=4.472892, accumulated_submission_time=47982.182173, global_step=140663, preemption_count=0, score=47982.182173, test/accuracy=0.610300, test/loss=1.822393, test/num_examples=10000, total_duration=49650.538966, train/accuracy=0.878906, train/loss=0.426304, validation/accuracy=0.731360, validation/loss=1.112806, validation/num_examples=50000
I0130 09:20:36.534302 139656800745216 logging_writer.py:48] [140700] global_step=140700, grad_norm=3.5844247341156006, loss=0.9758620858192444
I0130 09:21:10.596328 139656792352512 logging_writer.py:48] [140800] global_step=140800, grad_norm=3.951345443725586, loss=0.9843794107437134
I0130 09:21:44.684083 139656800745216 logging_writer.py:48] [140900] global_step=140900, grad_norm=3.6551740169525146, loss=0.9796310067176819
I0130 09:22:18.769292 139656792352512 logging_writer.py:48] [141000] global_step=141000, grad_norm=3.3903627395629883, loss=0.9741724133491516
I0130 09:22:52.946651 139656800745216 logging_writer.py:48] [141100] global_step=141100, grad_norm=3.5314528942108154, loss=0.9951795935630798
I0130 09:23:27.045238 139656792352512 logging_writer.py:48] [141200] global_step=141200, grad_norm=3.639101982116699, loss=0.9775016903877258
I0130 09:24:01.121866 139656800745216 logging_writer.py:48] [141300] global_step=141300, grad_norm=3.6464684009552, loss=1.0120047330856323
I0130 09:24:35.178639 139656792352512 logging_writer.py:48] [141400] global_step=141400, grad_norm=3.5075228214263916, loss=0.9071497917175293
I0130 09:25:09.238964 139656800745216 logging_writer.py:48] [141500] global_step=141500, grad_norm=3.824054718017578, loss=1.0110126733779907
I0130 09:25:43.310654 139656792352512 logging_writer.py:48] [141600] global_step=141600, grad_norm=3.606297254562378, loss=0.9967426061630249
I0130 09:26:17.372639 139656800745216 logging_writer.py:48] [141700] global_step=141700, grad_norm=3.7448291778564453, loss=0.916354775428772
I0130 09:26:51.472645 139656792352512 logging_writer.py:48] [141800] global_step=141800, grad_norm=3.6653101444244385, loss=0.9651366472244263
I0130 09:27:25.551545 139656800745216 logging_writer.py:48] [141900] global_step=141900, grad_norm=3.737077474594116, loss=0.9623560309410095
I0130 09:27:59.635058 139656792352512 logging_writer.py:48] [142000] global_step=142000, grad_norm=3.3504364490509033, loss=0.9319667816162109
I0130 09:28:33.712647 139656800745216 logging_writer.py:48] [142100] global_step=142100, grad_norm=4.118896961212158, loss=1.0320665836334229
I0130 09:28:53.631368 139822745589568 spec.py:321] Evaluating on the training split.
I0130 09:28:59.838864 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 09:29:08.525338 139822745589568 spec.py:349] Evaluating on the test split.
I0130 09:29:11.497812 139822745589568 submission_runner.py:408] Time since start: 50178.49s, 	Step: 142160, 	{'train/accuracy': 0.8776904940605164, 'train/loss': 0.42073261737823486, 'validation/accuracy': 0.7316799759864807, 'validation/loss': 1.1086206436157227, 'validation/num_examples': 50000, 'test/accuracy': 0.6021000146865845, 'test/loss': 1.8438477516174316, 'test/num_examples': 10000, 'score': 48492.161371946335, 'total_duration': 50178.48799037933, 'accumulated_submission_time': 48492.161371946335, 'accumulated_eval_time': 1676.9486873149872, 'accumulated_logging_time': 4.525469541549683}
I0130 09:29:11.533849 139656817530624 logging_writer.py:48] [142160] accumulated_eval_time=1676.948687, accumulated_logging_time=4.525470, accumulated_submission_time=48492.161372, global_step=142160, preemption_count=0, score=48492.161372, test/accuracy=0.602100, test/loss=1.843848, test/num_examples=10000, total_duration=50178.487990, train/accuracy=0.877690, train/loss=0.420733, validation/accuracy=0.731680, validation/loss=1.108621, validation/num_examples=50000
I0130 09:29:25.570886 139656825923328 logging_writer.py:48] [142200] global_step=142200, grad_norm=4.313091278076172, loss=1.0404056310653687
I0130 09:29:59.640623 139656817530624 logging_writer.py:48] [142300] global_step=142300, grad_norm=3.8401427268981934, loss=0.991566002368927
I0130 09:30:33.699880 139656825923328 logging_writer.py:48] [142400] global_step=142400, grad_norm=3.6922922134399414, loss=1.0014079809188843
I0130 09:31:07.782380 139656817530624 logging_writer.py:48] [142500] global_step=142500, grad_norm=4.1392107009887695, loss=0.9940577745437622
I0130 09:31:41.870804 139656825923328 logging_writer.py:48] [142600] global_step=142600, grad_norm=3.6382129192352295, loss=0.9672566652297974
I0130 09:32:15.974450 139656817530624 logging_writer.py:48] [142700] global_step=142700, grad_norm=3.5600268840789795, loss=1.0044605731964111
I0130 09:32:50.042336 139656825923328 logging_writer.py:48] [142800] global_step=142800, grad_norm=3.53657865524292, loss=0.9007712006568909
I0130 09:33:24.115782 139656817530624 logging_writer.py:48] [142900] global_step=142900, grad_norm=3.811756134033203, loss=0.8868049383163452
I0130 09:33:58.208704 139656825923328 logging_writer.py:48] [143000] global_step=143000, grad_norm=3.6632630825042725, loss=1.0617351531982422
I0130 09:34:32.320816 139656817530624 logging_writer.py:48] [143100] global_step=143100, grad_norm=4.084512710571289, loss=0.9499989151954651
I0130 09:35:06.413438 139656825923328 logging_writer.py:48] [143200] global_step=143200, grad_norm=3.5414347648620605, loss=0.9833539128303528
I0130 09:35:40.498469 139656817530624 logging_writer.py:48] [143300] global_step=143300, grad_norm=3.6374549865722656, loss=0.9653195142745972
I0130 09:36:14.653299 139656825923328 logging_writer.py:48] [143400] global_step=143400, grad_norm=3.7716243267059326, loss=0.870887279510498
I0130 09:36:48.737544 139656817530624 logging_writer.py:48] [143500] global_step=143500, grad_norm=3.6065053939819336, loss=0.8767382502555847
I0130 09:37:22.812027 139656825923328 logging_writer.py:48] [143600] global_step=143600, grad_norm=3.6599628925323486, loss=0.9985690712928772
I0130 09:37:41.716105 139822745589568 spec.py:321] Evaluating on the training split.
I0130 09:37:48.004958 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 09:37:56.622332 139822745589568 spec.py:349] Evaluating on the test split.
I0130 09:37:59.224284 139822745589568 submission_runner.py:408] Time since start: 50706.21s, 	Step: 143657, 	{'train/accuracy': 0.8729472160339355, 'train/loss': 0.44094493985176086, 'validation/accuracy': 0.7316799759864807, 'validation/loss': 1.1113321781158447, 'validation/num_examples': 50000, 'test/accuracy': 0.6050000190734863, 'test/loss': 1.8393492698669434, 'test/num_examples': 10000, 'score': 49002.2856194973, 'total_duration': 50706.214452028275, 'accumulated_submission_time': 49002.2856194973, 'accumulated_eval_time': 1694.4568555355072, 'accumulated_logging_time': 4.5699193477630615}
I0130 09:37:59.269467 139656792352512 logging_writer.py:48] [143657] accumulated_eval_time=1694.456856, accumulated_logging_time=4.569919, accumulated_submission_time=49002.285619, global_step=143657, preemption_count=0, score=49002.285619, test/accuracy=0.605000, test/loss=1.839349, test/num_examples=10000, total_duration=50706.214452, train/accuracy=0.872947, train/loss=0.440945, validation/accuracy=0.731680, validation/loss=1.111332, validation/num_examples=50000
I0130 09:38:14.263661 139656800745216 logging_writer.py:48] [143700] global_step=143700, grad_norm=3.736767530441284, loss=0.9654778242111206
I0130 09:38:48.253149 139656792352512 logging_writer.py:48] [143800] global_step=143800, grad_norm=3.6731274127960205, loss=0.9088097214698792
I0130 09:39:22.297504 139656800745216 logging_writer.py:48] [143900] global_step=143900, grad_norm=3.747042655944824, loss=0.9708384871482849
I0130 09:39:56.371143 139656792352512 logging_writer.py:48] [144000] global_step=144000, grad_norm=4.301308631896973, loss=0.9656710028648376
I0130 09:40:30.469565 139656800745216 logging_writer.py:48] [144100] global_step=144100, grad_norm=3.6607768535614014, loss=0.8855077028274536
I0130 09:41:04.569823 139656792352512 logging_writer.py:48] [144200] global_step=144200, grad_norm=3.80888032913208, loss=0.9569387435913086
I0130 09:41:38.685995 139656800745216 logging_writer.py:48] [144300] global_step=144300, grad_norm=4.23774528503418, loss=0.8603301048278809
I0130 09:42:12.758237 139656792352512 logging_writer.py:48] [144400] global_step=144400, grad_norm=3.6236557960510254, loss=0.8305343389511108
I0130 09:42:46.915523 139656800745216 logging_writer.py:48] [144500] global_step=144500, grad_norm=4.012857913970947, loss=0.9323842525482178
I0130 09:43:21.000641 139656792352512 logging_writer.py:48] [144600] global_step=144600, grad_norm=3.8167033195495605, loss=0.9336013793945312
I0130 09:43:55.079495 139656800745216 logging_writer.py:48] [144700] global_step=144700, grad_norm=3.8812220096588135, loss=1.0544395446777344
I0130 09:44:29.180042 139656792352512 logging_writer.py:48] [144800] global_step=144800, grad_norm=4.092550754547119, loss=0.9891341924667358
I0130 09:45:03.286597 139656800745216 logging_writer.py:48] [144900] global_step=144900, grad_norm=3.684002161026001, loss=0.8765442371368408
I0130 09:45:37.373288 139656792352512 logging_writer.py:48] [145000] global_step=145000, grad_norm=3.4619333744049072, loss=0.8535518050193787
I0130 09:46:11.480755 139656800745216 logging_writer.py:48] [145100] global_step=145100, grad_norm=3.6523802280426025, loss=0.872442364692688
I0130 09:46:29.351429 139822745589568 spec.py:321] Evaluating on the training split.
I0130 09:46:35.708326 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 09:46:44.150413 139822745589568 spec.py:349] Evaluating on the test split.
I0130 09:46:46.803558 139822745589568 submission_runner.py:408] Time since start: 51233.79s, 	Step: 145154, 	{'train/accuracy': 0.8752989172935486, 'train/loss': 0.4280344545841217, 'validation/accuracy': 0.7317799925804138, 'validation/loss': 1.1089249849319458, 'validation/num_examples': 50000, 'test/accuracy': 0.6053000092506409, 'test/loss': 1.8584744930267334, 'test/num_examples': 10000, 'score': 49512.30458474159, 'total_duration': 51233.79352784157, 'accumulated_submission_time': 49512.30458474159, 'accumulated_eval_time': 1711.908765077591, 'accumulated_logging_time': 4.625460624694824}
I0130 09:46:46.846418 139656800745216 logging_writer.py:48] [145154] accumulated_eval_time=1711.908765, accumulated_logging_time=4.625461, accumulated_submission_time=49512.304585, global_step=145154, preemption_count=0, score=49512.304585, test/accuracy=0.605300, test/loss=1.858474, test/num_examples=10000, total_duration=51233.793528, train/accuracy=0.875299, train/loss=0.428034, validation/accuracy=0.731780, validation/loss=1.108925, validation/num_examples=50000
I0130 09:47:02.848715 139656817530624 logging_writer.py:48] [145200] global_step=145200, grad_norm=4.294852256774902, loss=0.9242867231369019
I0130 09:47:36.918060 139656800745216 logging_writer.py:48] [145300] global_step=145300, grad_norm=3.90274715423584, loss=0.9052612781524658
I0130 09:48:10.964779 139656817530624 logging_writer.py:48] [145400] global_step=145400, grad_norm=3.5396921634674072, loss=0.8570324182510376
I0130 09:48:45.015731 139656800745216 logging_writer.py:48] [145500] global_step=145500, grad_norm=3.9003546237945557, loss=0.9419243335723877
I0130 09:49:19.154827 139656817530624 logging_writer.py:48] [145600] global_step=145600, grad_norm=3.6072797775268555, loss=0.8826073408126831
I0130 09:49:53.212972 139656800745216 logging_writer.py:48] [145700] global_step=145700, grad_norm=3.4130122661590576, loss=0.8902029991149902
I0130 09:50:27.281620 139656817530624 logging_writer.py:48] [145800] global_step=145800, grad_norm=3.721727132797241, loss=0.9044843912124634
I0130 09:51:01.355401 139656800745216 logging_writer.py:48] [145900] global_step=145900, grad_norm=4.009309768676758, loss=1.0166435241699219
I0130 09:51:35.405466 139656817530624 logging_writer.py:48] [146000] global_step=146000, grad_norm=4.222457408905029, loss=0.9333900809288025
I0130 09:52:09.488714 139656800745216 logging_writer.py:48] [146100] global_step=146100, grad_norm=3.8438754081726074, loss=0.9910869598388672
I0130 09:52:43.542026 139656817530624 logging_writer.py:48] [146200] global_step=146200, grad_norm=4.413665294647217, loss=0.9518136382102966
I0130 09:53:17.591878 139656800745216 logging_writer.py:48] [146300] global_step=146300, grad_norm=3.9433975219726562, loss=0.9042947292327881
I0130 09:53:51.663455 139656817530624 logging_writer.py:48] [146400] global_step=146400, grad_norm=3.6768009662628174, loss=0.924307107925415
I0130 09:54:25.705231 139656800745216 logging_writer.py:48] [146500] global_step=146500, grad_norm=3.712322950363159, loss=0.9204071164131165
I0130 09:54:59.779886 139656817530624 logging_writer.py:48] [146600] global_step=146600, grad_norm=4.132530689239502, loss=0.9304823875427246
I0130 09:55:16.955874 139822745589568 spec.py:321] Evaluating on the training split.
I0130 09:55:23.170843 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 09:55:31.558894 139822745589568 spec.py:349] Evaluating on the test split.
I0130 09:55:34.174474 139822745589568 submission_runner.py:408] Time since start: 51761.16s, 	Step: 146652, 	{'train/accuracy': 0.8880739808082581, 'train/loss': 0.3907930254936218, 'validation/accuracy': 0.734279990196228, 'validation/loss': 1.1064696311950684, 'validation/num_examples': 50000, 'test/accuracy': 0.6035000085830688, 'test/loss': 1.850600242614746, 'test/num_examples': 10000, 'score': 50022.35403752327, 'total_duration': 51761.16456365585, 'accumulated_submission_time': 50022.35403752327, 'accumulated_eval_time': 1729.127257347107, 'accumulated_logging_time': 4.677754163742065}
I0130 09:55:34.219424 139656792352512 logging_writer.py:48] [146652] accumulated_eval_time=1729.127257, accumulated_logging_time=4.677754, accumulated_submission_time=50022.354038, global_step=146652, preemption_count=0, score=50022.354038, test/accuracy=0.603500, test/loss=1.850600, test/num_examples=10000, total_duration=51761.164564, train/accuracy=0.888074, train/loss=0.390793, validation/accuracy=0.734280, validation/loss=1.106470, validation/num_examples=50000
I0130 09:55:50.994128 139656809137920 logging_writer.py:48] [146700] global_step=146700, grad_norm=3.8008310794830322, loss=0.9714544415473938
I0130 09:56:25.014274 139656792352512 logging_writer.py:48] [146800] global_step=146800, grad_norm=4.00822639465332, loss=0.911989152431488
I0130 09:56:59.064076 139656809137920 logging_writer.py:48] [146900] global_step=146900, grad_norm=3.9931178092956543, loss=0.8434929847717285
I0130 09:57:33.109122 139656792352512 logging_writer.py:48] [147000] global_step=147000, grad_norm=3.8677186965942383, loss=0.9021565914154053
I0130 09:58:07.176667 139656809137920 logging_writer.py:48] [147100] global_step=147100, grad_norm=3.9263319969177246, loss=0.9436545372009277
I0130 09:58:41.225732 139656792352512 logging_writer.py:48] [147200] global_step=147200, grad_norm=3.786388874053955, loss=0.8632749319076538
I0130 09:59:15.303201 139656809137920 logging_writer.py:48] [147300] global_step=147300, grad_norm=3.7036173343658447, loss=0.9364754557609558
I0130 09:59:49.392716 139656792352512 logging_writer.py:48] [147400] global_step=147400, grad_norm=3.6707117557525635, loss=0.8730566501617432
I0130 10:00:23.481638 139656809137920 logging_writer.py:48] [147500] global_step=147500, grad_norm=3.7787885665893555, loss=0.7815631628036499
I0130 10:00:57.563296 139656792352512 logging_writer.py:48] [147600] global_step=147600, grad_norm=3.930401563644409, loss=0.9170507788658142
I0130 10:01:31.647088 139656809137920 logging_writer.py:48] [147700] global_step=147700, grad_norm=4.042978763580322, loss=0.9134180545806885
I0130 10:02:05.737416 139656792352512 logging_writer.py:48] [147800] global_step=147800, grad_norm=3.632166624069214, loss=0.8720412254333496
I0130 10:02:39.896275 139656809137920 logging_writer.py:48] [147900] global_step=147900, grad_norm=4.3260650634765625, loss=0.9440746903419495
I0130 10:03:13.993662 139656792352512 logging_writer.py:48] [148000] global_step=148000, grad_norm=4.0495429039001465, loss=0.9196230173110962
I0130 10:03:48.111497 139656809137920 logging_writer.py:48] [148100] global_step=148100, grad_norm=3.9589271545410156, loss=0.9064470529556274
I0130 10:04:04.272463 139822745589568 spec.py:321] Evaluating on the training split.
I0130 10:04:10.513158 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 10:04:19.107566 139822745589568 spec.py:349] Evaluating on the test split.
I0130 10:04:21.748415 139822745589568 submission_runner.py:408] Time since start: 52288.74s, 	Step: 148149, 	{'train/accuracy': 0.91019606590271, 'train/loss': 0.3179280161857605, 'validation/accuracy': 0.7368199825286865, 'validation/loss': 1.0996601581573486, 'validation/num_examples': 50000, 'test/accuracy': 0.6094000339508057, 'test/loss': 1.84394109249115, 'test/num_examples': 10000, 'score': 50532.34589600563, 'total_duration': 52288.738582372665, 'accumulated_submission_time': 50532.34589600563, 'accumulated_eval_time': 1746.6031787395477, 'accumulated_logging_time': 4.732522487640381}
I0130 10:04:21.797231 139656792352512 logging_writer.py:48] [148149] accumulated_eval_time=1746.603179, accumulated_logging_time=4.732522, accumulated_submission_time=50532.345896, global_step=148149, preemption_count=0, score=50532.345896, test/accuracy=0.609400, test/loss=1.843941, test/num_examples=10000, total_duration=52288.738582, train/accuracy=0.910196, train/loss=0.317928, validation/accuracy=0.736820, validation/loss=1.099660, validation/num_examples=50000
I0130 10:04:39.490972 139656800745216 logging_writer.py:48] [148200] global_step=148200, grad_norm=3.7565484046936035, loss=0.8299258947372437
I0130 10:05:13.546987 139656792352512 logging_writer.py:48] [148300] global_step=148300, grad_norm=3.982936143875122, loss=0.8350083827972412
I0130 10:05:47.628633 139656800745216 logging_writer.py:48] [148400] global_step=148400, grad_norm=4.297008991241455, loss=0.8732539415359497
I0130 10:06:21.720010 139656792352512 logging_writer.py:48] [148500] global_step=148500, grad_norm=3.7733888626098633, loss=0.8894721865653992
I0130 10:06:55.794814 139656800745216 logging_writer.py:48] [148600] global_step=148600, grad_norm=3.870591402053833, loss=0.9085817933082581
I0130 10:07:29.879102 139656792352512 logging_writer.py:48] [148700] global_step=148700, grad_norm=3.957965135574341, loss=0.9023408889770508
I0130 10:08:04.524351 139656800745216 logging_writer.py:48] [148800] global_step=148800, grad_norm=4.207268238067627, loss=0.8374435901641846
I0130 10:08:38.611996 139656792352512 logging_writer.py:48] [148900] global_step=148900, grad_norm=3.924121618270874, loss=0.8767375349998474
I0130 10:09:12.761148 139656800745216 logging_writer.py:48] [149000] global_step=149000, grad_norm=3.8293066024780273, loss=0.8782568573951721
I0130 10:09:46.832620 139656792352512 logging_writer.py:48] [149100] global_step=149100, grad_norm=4.040949821472168, loss=0.9353717565536499
I0130 10:10:20.944328 139656800745216 logging_writer.py:48] [149200] global_step=149200, grad_norm=4.148752689361572, loss=0.9058200120925903
I0130 10:10:55.057516 139656792352512 logging_writer.py:48] [149300] global_step=149300, grad_norm=4.227078914642334, loss=0.8339179754257202
I0130 10:11:29.149795 139656800745216 logging_writer.py:48] [149400] global_step=149400, grad_norm=3.818864107131958, loss=0.7949962615966797
I0130 10:12:03.232385 139656792352512 logging_writer.py:48] [149500] global_step=149500, grad_norm=4.189526081085205, loss=0.954650342464447
I0130 10:12:37.313777 139656800745216 logging_writer.py:48] [149600] global_step=149600, grad_norm=3.7988085746765137, loss=0.8197811841964722
I0130 10:12:51.777029 139822745589568 spec.py:321] Evaluating on the training split.
I0130 10:12:58.024127 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 10:13:06.651027 139822745589568 spec.py:349] Evaluating on the test split.
I0130 10:13:09.273790 139822745589568 submission_runner.py:408] Time since start: 52816.26s, 	Step: 149644, 	{'train/accuracy': 0.9052136540412903, 'train/loss': 0.3252921402454376, 'validation/accuracy': 0.7401399612426758, 'validation/loss': 1.093822717666626, 'validation/num_examples': 50000, 'test/accuracy': 0.6175000071525574, 'test/loss': 1.8288391828536987, 'test/num_examples': 10000, 'score': 51042.26584935188, 'total_duration': 52816.26387476921, 'accumulated_submission_time': 51042.26584935188, 'accumulated_eval_time': 1764.0998284816742, 'accumulated_logging_time': 4.791525602340698}
I0130 10:13:09.343489 139656809137920 logging_writer.py:48] [149644] accumulated_eval_time=1764.099828, accumulated_logging_time=4.791526, accumulated_submission_time=51042.265849, global_step=149644, preemption_count=0, score=51042.265849, test/accuracy=0.617500, test/loss=1.828839, test/num_examples=10000, total_duration=52816.263875, train/accuracy=0.905214, train/loss=0.325292, validation/accuracy=0.740140, validation/loss=1.093823, validation/num_examples=50000
I0130 10:13:28.784005 139656834316032 logging_writer.py:48] [149700] global_step=149700, grad_norm=3.9618284702301025, loss=0.8987439274787903
I0130 10:14:02.833754 139656809137920 logging_writer.py:48] [149800] global_step=149800, grad_norm=4.297709941864014, loss=0.8737659454345703
I0130 10:14:36.886250 139656834316032 logging_writer.py:48] [149900] global_step=149900, grad_norm=4.228979110717773, loss=0.8533267974853516
I0130 10:15:10.962342 139656809137920 logging_writer.py:48] [150000] global_step=150000, grad_norm=3.8990705013275146, loss=0.8460147380828857
I0130 10:15:45.073164 139656834316032 logging_writer.py:48] [150100] global_step=150100, grad_norm=3.84186053276062, loss=0.8519582748413086
I0130 10:16:19.139655 139656809137920 logging_writer.py:48] [150200] global_step=150200, grad_norm=3.7310948371887207, loss=0.8816611766815186
I0130 10:16:53.201320 139656834316032 logging_writer.py:48] [150300] global_step=150300, grad_norm=3.8092408180236816, loss=0.8709595203399658
I0130 10:17:27.280400 139656809137920 logging_writer.py:48] [150400] global_step=150400, grad_norm=4.01101541519165, loss=0.8932278156280518
I0130 10:18:01.327990 139656834316032 logging_writer.py:48] [150500] global_step=150500, grad_norm=4.150500297546387, loss=0.8585500717163086
I0130 10:18:35.384961 139656809137920 logging_writer.py:48] [150600] global_step=150600, grad_norm=4.025258541107178, loss=0.8905357122421265
I0130 10:19:09.460044 139656834316032 logging_writer.py:48] [150700] global_step=150700, grad_norm=3.977112054824829, loss=0.7898411750793457
I0130 10:19:43.556787 139656809137920 logging_writer.py:48] [150800] global_step=150800, grad_norm=3.6964991092681885, loss=0.8179252743721008
I0130 10:20:17.651135 139656834316032 logging_writer.py:48] [150900] global_step=150900, grad_norm=4.049156665802002, loss=0.8211807012557983
I0130 10:20:51.750278 139656809137920 logging_writer.py:48] [151000] global_step=151000, grad_norm=3.946687936782837, loss=0.8589214086532593
I0130 10:21:25.844671 139656834316032 logging_writer.py:48] [151100] global_step=151100, grad_norm=4.066250324249268, loss=0.8520110249519348
I0130 10:21:39.295803 139822745589568 spec.py:321] Evaluating on the training split.
I0130 10:21:45.559358 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 10:21:54.188453 139822745589568 spec.py:349] Evaluating on the test split.
I0130 10:21:56.773002 139822745589568 submission_runner.py:408] Time since start: 53343.76s, 	Step: 151141, 	{'train/accuracy': 0.906668484210968, 'train/loss': 0.3213539719581604, 'validation/accuracy': 0.7433599829673767, 'validation/loss': 1.087909460067749, 'validation/num_examples': 50000, 'test/accuracy': 0.6135000586509705, 'test/loss': 1.828898549079895, 'test/num_examples': 10000, 'score': 51552.15370512009, 'total_duration': 53343.76315808296, 'accumulated_submission_time': 51552.15370512009, 'accumulated_eval_time': 1781.5769836902618, 'accumulated_logging_time': 4.874719858169556}
I0130 10:21:56.815982 139656783959808 logging_writer.py:48] [151141] accumulated_eval_time=1781.576984, accumulated_logging_time=4.874720, accumulated_submission_time=51552.153705, global_step=151141, preemption_count=0, score=51552.153705, test/accuracy=0.613500, test/loss=1.828899, test/num_examples=10000, total_duration=53343.763158, train/accuracy=0.906668, train/loss=0.321354, validation/accuracy=0.743360, validation/loss=1.087909, validation/num_examples=50000
I0130 10:22:17.252174 139656792352512 logging_writer.py:48] [151200] global_step=151200, grad_norm=4.174312114715576, loss=0.8614349365234375
I0130 10:22:51.520213 139656783959808 logging_writer.py:48] [151300] global_step=151300, grad_norm=4.218735218048096, loss=0.8916983008384705
I0130 10:23:25.624432 139656792352512 logging_writer.py:48] [151400] global_step=151400, grad_norm=4.135875701904297, loss=0.8058596849441528
I0130 10:23:59.706018 139656783959808 logging_writer.py:48] [151500] global_step=151500, grad_norm=4.124331474304199, loss=0.8670067191123962
I0130 10:24:33.816938 139656792352512 logging_writer.py:48] [151600] global_step=151600, grad_norm=4.131489276885986, loss=0.8313040733337402
I0130 10:25:07.919930 139656783959808 logging_writer.py:48] [151700] global_step=151700, grad_norm=4.173811435699463, loss=0.9357190728187561
I0130 10:25:42.005023 139656792352512 logging_writer.py:48] [151800] global_step=151800, grad_norm=4.4066996574401855, loss=0.9179016947746277
I0130 10:26:16.105625 139656783959808 logging_writer.py:48] [151900] global_step=151900, grad_norm=4.163142681121826, loss=0.8750373721122742
I0130 10:26:50.202865 139656792352512 logging_writer.py:48] [152000] global_step=152000, grad_norm=4.0196051597595215, loss=0.7774946093559265
I0130 10:27:24.256525 139656783959808 logging_writer.py:48] [152100] global_step=152100, grad_norm=3.837087392807007, loss=0.8561543226242065
I0130 10:27:58.331863 139656792352512 logging_writer.py:48] [152200] global_step=152200, grad_norm=4.014763832092285, loss=0.9086649417877197
I0130 10:28:32.379785 139656783959808 logging_writer.py:48] [152300] global_step=152300, grad_norm=3.6849794387817383, loss=0.7809893488883972
I0130 10:29:06.673301 139656792352512 logging_writer.py:48] [152400] global_step=152400, grad_norm=4.217095851898193, loss=0.8528945446014404
I0130 10:29:40.743583 139656783959808 logging_writer.py:48] [152500] global_step=152500, grad_norm=3.6373519897460938, loss=0.7593230605125427
I0130 10:30:14.839806 139656792352512 logging_writer.py:48] [152600] global_step=152600, grad_norm=3.8358027935028076, loss=0.8006141185760498
I0130 10:30:26.912955 139822745589568 spec.py:321] Evaluating on the training split.
I0130 10:30:33.256888 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 10:30:41.896359 139822745589568 spec.py:349] Evaluating on the test split.
I0130 10:30:44.526862 139822745589568 submission_runner.py:408] Time since start: 53871.52s, 	Step: 152637, 	{'train/accuracy': 0.9010881781578064, 'train/loss': 0.34705764055252075, 'validation/accuracy': 0.7376999855041504, 'validation/loss': 1.1096341609954834, 'validation/num_examples': 50000, 'test/accuracy': 0.6148000359535217, 'test/loss': 1.844693660736084, 'test/num_examples': 10000, 'score': 52062.18849515915, 'total_duration': 53871.51702570915, 'accumulated_submission_time': 52062.18849515915, 'accumulated_eval_time': 1799.1908648014069, 'accumulated_logging_time': 4.9273035526275635}
I0130 10:30:44.574337 139656792352512 logging_writer.py:48] [152637] accumulated_eval_time=1799.190865, accumulated_logging_time=4.927304, accumulated_submission_time=52062.188495, global_step=152637, preemption_count=0, score=52062.188495, test/accuracy=0.614800, test/loss=1.844694, test/num_examples=10000, total_duration=53871.517026, train/accuracy=0.901088, train/loss=0.347058, validation/accuracy=0.737700, validation/loss=1.109634, validation/num_examples=50000
I0130 10:31:06.392646 139656809137920 logging_writer.py:48] [152700] global_step=152700, grad_norm=4.300735950469971, loss=0.7939159870147705
I0130 10:31:40.418392 139656792352512 logging_writer.py:48] [152800] global_step=152800, grad_norm=3.6916208267211914, loss=0.7563885450363159
I0130 10:32:14.464608 139656809137920 logging_writer.py:48] [152900] global_step=152900, grad_norm=4.208616733551025, loss=0.7699414491653442
I0130 10:32:48.531223 139656792352512 logging_writer.py:48] [153000] global_step=153000, grad_norm=4.102057456970215, loss=0.8501176238059998
I0130 10:33:22.592482 139656809137920 logging_writer.py:48] [153100] global_step=153100, grad_norm=4.0396857261657715, loss=0.7756580114364624
I0130 10:33:56.689279 139656792352512 logging_writer.py:48] [153200] global_step=153200, grad_norm=4.207517147064209, loss=0.7220955491065979
I0130 10:34:30.785688 139656809137920 logging_writer.py:48] [153300] global_step=153300, grad_norm=3.777268648147583, loss=0.7899767160415649
I0130 10:35:04.870229 139656792352512 logging_writer.py:48] [153400] global_step=153400, grad_norm=4.005800247192383, loss=0.8729551434516907
I0130 10:35:39.055143 139656809137920 logging_writer.py:48] [153500] global_step=153500, grad_norm=4.124528884887695, loss=0.8481552600860596
I0130 10:36:13.142208 139656792352512 logging_writer.py:48] [153600] global_step=153600, grad_norm=5.200185775756836, loss=0.8831912279129028
I0130 10:36:47.253419 139656809137920 logging_writer.py:48] [153700] global_step=153700, grad_norm=4.202195167541504, loss=0.9850245714187622
I0130 10:37:21.335272 139656792352512 logging_writer.py:48] [153800] global_step=153800, grad_norm=3.8770058155059814, loss=0.7880427241325378
I0130 10:37:55.444977 139656809137920 logging_writer.py:48] [153900] global_step=153900, grad_norm=4.115392208099365, loss=0.7870299816131592
I0130 10:38:29.545099 139656792352512 logging_writer.py:48] [154000] global_step=154000, grad_norm=3.983647584915161, loss=0.8474845886230469
I0130 10:39:03.656157 139656809137920 logging_writer.py:48] [154100] global_step=154100, grad_norm=4.355051517486572, loss=0.8556325435638428
I0130 10:39:14.718609 139822745589568 spec.py:321] Evaluating on the training split.
I0130 10:39:20.982603 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 10:39:29.310288 139822745589568 spec.py:349] Evaluating on the test split.
I0130 10:39:31.934080 139822745589568 submission_runner.py:408] Time since start: 54398.92s, 	Step: 154134, 	{'train/accuracy': 0.9094188213348389, 'train/loss': 0.31328409910202026, 'validation/accuracy': 0.7424799799919128, 'validation/loss': 1.0863618850708008, 'validation/num_examples': 50000, 'test/accuracy': 0.6132000088691711, 'test/loss': 1.8464604616165161, 'test/num_examples': 10000, 'score': 52572.27054524422, 'total_duration': 54398.92409610748, 'accumulated_submission_time': 52572.27054524422, 'accumulated_eval_time': 1816.406150817871, 'accumulated_logging_time': 4.986354112625122}
I0130 10:39:31.978244 139656800745216 logging_writer.py:48] [154134] accumulated_eval_time=1816.406151, accumulated_logging_time=4.986354, accumulated_submission_time=52572.270545, global_step=154134, preemption_count=0, score=52572.270545, test/accuracy=0.613200, test/loss=1.846460, test/num_examples=10000, total_duration=54398.924096, train/accuracy=0.909419, train/loss=0.313284, validation/accuracy=0.742480, validation/loss=1.086362, validation/num_examples=50000
I0130 10:39:54.794823 139656817530624 logging_writer.py:48] [154200] global_step=154200, grad_norm=4.5883378982543945, loss=0.8928117752075195
I0130 10:40:28.836993 139656800745216 logging_writer.py:48] [154300] global_step=154300, grad_norm=4.488528251647949, loss=0.8609849214553833
I0130 10:41:02.867852 139656817530624 logging_writer.py:48] [154400] global_step=154400, grad_norm=4.442908763885498, loss=0.8584988713264465
I0130 10:41:36.955873 139656800745216 logging_writer.py:48] [154500] global_step=154500, grad_norm=3.976569890975952, loss=0.7697568535804749
I0130 10:42:11.011747 139656817530624 logging_writer.py:48] [154600] global_step=154600, grad_norm=4.70034646987915, loss=0.9248173236846924
I0130 10:42:45.124605 139656800745216 logging_writer.py:48] [154700] global_step=154700, grad_norm=4.083646774291992, loss=0.8130526542663574
I0130 10:43:19.176982 139656817530624 logging_writer.py:48] [154800] global_step=154800, grad_norm=4.003490447998047, loss=0.8473792672157288
I0130 10:43:53.263645 139656800745216 logging_writer.py:48] [154900] global_step=154900, grad_norm=4.157644748687744, loss=0.8170523643493652
I0130 10:44:27.339607 139656817530624 logging_writer.py:48] [155000] global_step=155000, grad_norm=4.438790798187256, loss=0.8601863980293274
I0130 10:45:01.419765 139656800745216 logging_writer.py:48] [155100] global_step=155100, grad_norm=3.971702814102173, loss=0.8285118937492371
I0130 10:45:35.511320 139656817530624 logging_writer.py:48] [155200] global_step=155200, grad_norm=4.353614807128906, loss=0.8091222643852234
I0130 10:46:09.584587 139656800745216 logging_writer.py:48] [155300] global_step=155300, grad_norm=4.114625453948975, loss=0.8730448484420776
I0130 10:46:43.698049 139656817530624 logging_writer.py:48] [155400] global_step=155400, grad_norm=3.9808621406555176, loss=0.8631216883659363
I0130 10:47:17.784179 139656800745216 logging_writer.py:48] [155500] global_step=155500, grad_norm=4.278590679168701, loss=0.755616307258606
I0130 10:47:51.860776 139656817530624 logging_writer.py:48] [155600] global_step=155600, grad_norm=3.9897308349609375, loss=0.8446511030197144
I0130 10:48:02.237248 139822745589568 spec.py:321] Evaluating on the training split.
I0130 10:48:08.440004 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 10:48:16.889264 139822745589568 spec.py:349] Evaluating on the test split.
I0130 10:48:19.591372 139822745589568 submission_runner.py:408] Time since start: 54926.58s, 	Step: 155632, 	{'train/accuracy': 0.9090999364852905, 'train/loss': 0.3144540786743164, 'validation/accuracy': 0.7421799898147583, 'validation/loss': 1.0850337743759155, 'validation/num_examples': 50000, 'test/accuracy': 0.6140000224113464, 'test/loss': 1.8501039743423462, 'test/num_examples': 10000, 'score': 53082.467106580734, 'total_duration': 54926.58153581619, 'accumulated_submission_time': 53082.467106580734, 'accumulated_eval_time': 1833.7602362632751, 'accumulated_logging_time': 5.041287660598755}
I0130 10:48:19.636466 139656783959808 logging_writer.py:48] [155632] accumulated_eval_time=1833.760236, accumulated_logging_time=5.041288, accumulated_submission_time=53082.467107, global_step=155632, preemption_count=0, score=53082.467107, test/accuracy=0.614000, test/loss=1.850104, test/num_examples=10000, total_duration=54926.581536, train/accuracy=0.909100, train/loss=0.314454, validation/accuracy=0.742180, validation/loss=1.085034, validation/num_examples=50000
I0130 10:48:43.135937 139656792352512 logging_writer.py:48] [155700] global_step=155700, grad_norm=4.084745407104492, loss=0.7819830179214478
I0130 10:49:17.232604 139656783959808 logging_writer.py:48] [155800] global_step=155800, grad_norm=3.883737087249756, loss=0.7072399854660034
I0130 10:49:51.321407 139656792352512 logging_writer.py:48] [155900] global_step=155900, grad_norm=3.9925167560577393, loss=0.7930005192756653
I0130 10:50:25.408159 139656783959808 logging_writer.py:48] [156000] global_step=156000, grad_norm=4.082913875579834, loss=0.8346766829490662
I0130 10:50:59.505468 139656792352512 logging_writer.py:48] [156100] global_step=156100, grad_norm=4.133966445922852, loss=0.783048152923584
I0130 10:51:33.591845 139656783959808 logging_writer.py:48] [156200] global_step=156200, grad_norm=4.207729816436768, loss=0.9023077487945557
I0130 10:52:07.682880 139656792352512 logging_writer.py:48] [156300] global_step=156300, grad_norm=4.3793134689331055, loss=0.8108915090560913
I0130 10:52:41.752193 139656783959808 logging_writer.py:48] [156400] global_step=156400, grad_norm=3.902359962463379, loss=0.7661530375480652
I0130 10:53:15.846587 139656792352512 logging_writer.py:48] [156500] global_step=156500, grad_norm=4.0652055740356445, loss=0.7506852746009827
I0130 10:53:49.938095 139656783959808 logging_writer.py:48] [156600] global_step=156600, grad_norm=4.013375759124756, loss=0.7595734000205994
I0130 10:54:24.015024 139656792352512 logging_writer.py:48] [156700] global_step=156700, grad_norm=4.325716972351074, loss=0.8274075984954834
I0130 10:54:58.081139 139656783959808 logging_writer.py:48] [156800] global_step=156800, grad_norm=4.020602703094482, loss=0.7638925909996033
I0130 10:55:32.253329 139656792352512 logging_writer.py:48] [156900] global_step=156900, grad_norm=3.8579728603363037, loss=0.7648171186447144
I0130 10:56:06.346794 139656783959808 logging_writer.py:48] [157000] global_step=157000, grad_norm=4.163144588470459, loss=0.7191709280014038
I0130 10:56:40.434458 139656792352512 logging_writer.py:48] [157100] global_step=157100, grad_norm=3.938117265701294, loss=0.8055858016014099
I0130 10:56:49.794236 139822745589568 spec.py:321] Evaluating on the training split.
I0130 10:56:56.090752 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 10:57:04.568433 139822745589568 spec.py:349] Evaluating on the test split.
I0130 10:57:07.161050 139822745589568 submission_runner.py:408] Time since start: 55454.15s, 	Step: 157129, 	{'train/accuracy': 0.9382573366165161, 'train/loss': 0.2249719649553299, 'validation/accuracy': 0.7435799837112427, 'validation/loss': 1.074958086013794, 'validation/num_examples': 50000, 'test/accuracy': 0.6165000200271606, 'test/loss': 1.826570749282837, 'test/num_examples': 10000, 'score': 53592.5631840229, 'total_duration': 55454.151193380356, 'accumulated_submission_time': 53592.5631840229, 'accumulated_eval_time': 1851.1270196437836, 'accumulated_logging_time': 5.0969154834747314}
I0130 10:57:07.214063 139656783959808 logging_writer.py:48] [157129] accumulated_eval_time=1851.127020, accumulated_logging_time=5.096915, accumulated_submission_time=53592.563184, global_step=157129, preemption_count=0, score=53592.563184, test/accuracy=0.616500, test/loss=1.826571, test/num_examples=10000, total_duration=55454.151193, train/accuracy=0.938257, train/loss=0.224972, validation/accuracy=0.743580, validation/loss=1.074958, validation/num_examples=50000
I0130 10:57:31.697146 139656800745216 logging_writer.py:48] [157200] global_step=157200, grad_norm=4.501138687133789, loss=0.7974661588668823
I0130 10:58:05.723835 139656783959808 logging_writer.py:48] [157300] global_step=157300, grad_norm=3.883392810821533, loss=0.7169065475463867
I0130 10:58:39.780749 139656800745216 logging_writer.py:48] [157400] global_step=157400, grad_norm=4.070217132568359, loss=0.8064950704574585
I0130 10:59:13.883574 139656783959808 logging_writer.py:48] [157500] global_step=157500, grad_norm=4.734456539154053, loss=0.749735414981842
I0130 10:59:47.984108 139656800745216 logging_writer.py:48] [157600] global_step=157600, grad_norm=4.284003257751465, loss=0.7505752444267273
I0130 11:00:22.080752 139656783959808 logging_writer.py:48] [157700] global_step=157700, grad_norm=4.504856109619141, loss=0.7845406532287598
I0130 11:00:56.152479 139656800745216 logging_writer.py:48] [157800] global_step=157800, grad_norm=3.7581145763397217, loss=0.7255818843841553
I0130 11:01:30.242589 139656783959808 logging_writer.py:48] [157900] global_step=157900, grad_norm=4.39863395690918, loss=0.7676156759262085
I0130 11:02:04.332675 139656800745216 logging_writer.py:48] [158000] global_step=158000, grad_norm=4.243074893951416, loss=0.7848570346832275
I0130 11:02:38.494426 139656783959808 logging_writer.py:48] [158100] global_step=158100, grad_norm=4.104292392730713, loss=0.8209022879600525
I0130 11:03:12.569949 139656800745216 logging_writer.py:48] [158200] global_step=158200, grad_norm=4.390493869781494, loss=0.810003399848938
I0130 11:03:46.660990 139656783959808 logging_writer.py:48] [158300] global_step=158300, grad_norm=4.490007400512695, loss=0.900174617767334
I0130 11:04:20.741656 139656800745216 logging_writer.py:48] [158400] global_step=158400, grad_norm=4.147064208984375, loss=0.8437603712081909
I0130 11:04:54.834799 139656783959808 logging_writer.py:48] [158500] global_step=158500, grad_norm=4.139344692230225, loss=0.7195011377334595
I0130 11:05:28.926743 139656800745216 logging_writer.py:48] [158600] global_step=158600, grad_norm=4.292942047119141, loss=0.7051019072532654
I0130 11:05:37.247333 139822745589568 spec.py:321] Evaluating on the training split.
I0130 11:05:43.436338 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 11:05:51.916699 139822745589568 spec.py:349] Evaluating on the test split.
I0130 11:05:54.438523 139822745589568 submission_runner.py:408] Time since start: 55981.43s, 	Step: 158626, 	{'train/accuracy': 0.9301857352256775, 'train/loss': 0.24374496936798096, 'validation/accuracy': 0.7449399828910828, 'validation/loss': 1.078855276107788, 'validation/num_examples': 50000, 'test/accuracy': 0.6187000274658203, 'test/loss': 1.83165442943573, 'test/num_examples': 10000, 'score': 54102.53592252731, 'total_duration': 55981.42848825455, 'accumulated_submission_time': 54102.53592252731, 'accumulated_eval_time': 1868.3179905414581, 'accumulated_logging_time': 5.159096002578735}
I0130 11:05:54.485860 139656809137920 logging_writer.py:48] [158626] accumulated_eval_time=1868.317991, accumulated_logging_time=5.159096, accumulated_submission_time=54102.535923, global_step=158626, preemption_count=0, score=54102.535923, test/accuracy=0.618700, test/loss=1.831654, test/num_examples=10000, total_duration=55981.428488, train/accuracy=0.930186, train/loss=0.243745, validation/accuracy=0.744940, validation/loss=1.078855, validation/num_examples=50000
I0130 11:06:20.003809 139656825923328 logging_writer.py:48] [158700] global_step=158700, grad_norm=4.346642017364502, loss=0.798105776309967
I0130 11:06:54.045217 139656809137920 logging_writer.py:48] [158800] global_step=158800, grad_norm=4.129042148590088, loss=0.7972657084465027
I0130 11:07:28.110986 139656825923328 logging_writer.py:48] [158900] global_step=158900, grad_norm=4.606744766235352, loss=0.7758579254150391
I0130 11:08:02.197005 139656809137920 logging_writer.py:48] [159000] global_step=159000, grad_norm=4.219711780548096, loss=0.7730088233947754
I0130 11:08:36.241228 139656825923328 logging_writer.py:48] [159100] global_step=159100, grad_norm=4.274378299713135, loss=0.8440825939178467
I0130 11:09:10.389019 139656809137920 logging_writer.py:48] [159200] global_step=159200, grad_norm=4.090580463409424, loss=0.834599494934082
I0130 11:09:44.485678 139656825923328 logging_writer.py:48] [159300] global_step=159300, grad_norm=4.22994327545166, loss=0.7859799861907959
I0130 11:10:18.582565 139656809137920 logging_writer.py:48] [159400] global_step=159400, grad_norm=4.511876583099365, loss=0.8607978820800781
I0130 11:10:52.676310 139656825923328 logging_writer.py:48] [159500] global_step=159500, grad_norm=4.326028347015381, loss=0.751708447933197
I0130 11:11:26.767958 139656809137920 logging_writer.py:48] [159600] global_step=159600, grad_norm=4.259842395782471, loss=0.7042103409767151
I0130 11:12:00.862341 139656825923328 logging_writer.py:48] [159700] global_step=159700, grad_norm=4.641332149505615, loss=0.7840890884399414
I0130 11:12:34.941107 139656809137920 logging_writer.py:48] [159800] global_step=159800, grad_norm=4.165336608886719, loss=0.7599389553070068
I0130 11:13:09.023915 139656825923328 logging_writer.py:48] [159900] global_step=159900, grad_norm=4.5227742195129395, loss=0.7956832051277161
I0130 11:13:43.100516 139656809137920 logging_writer.py:48] [160000] global_step=160000, grad_norm=4.091527462005615, loss=0.6920424699783325
I0130 11:14:17.156908 139656825923328 logging_writer.py:48] [160100] global_step=160100, grad_norm=4.445396900177002, loss=0.7924667596817017
I0130 11:14:24.456482 139822745589568 spec.py:321] Evaluating on the training split.
I0130 11:14:30.691132 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 11:14:39.219232 139822745589568 spec.py:349] Evaluating on the test split.
I0130 11:14:41.825362 139822745589568 submission_runner.py:408] Time since start: 56508.82s, 	Step: 160123, 	{'train/accuracy': 0.9317004084587097, 'train/loss': 0.23847176134586334, 'validation/accuracy': 0.7465999722480774, 'validation/loss': 1.0716962814331055, 'validation/num_examples': 50000, 'test/accuracy': 0.622700035572052, 'test/loss': 1.8242336511611938, 'test/num_examples': 10000, 'score': 54612.44617629051, 'total_duration': 56508.81551861763, 'accumulated_submission_time': 54612.44617629051, 'accumulated_eval_time': 1885.6868290901184, 'accumulated_logging_time': 5.215553045272827}
I0130 11:14:41.875370 139656800745216 logging_writer.py:48] [160123] accumulated_eval_time=1885.686829, accumulated_logging_time=5.215553, accumulated_submission_time=54612.446176, global_step=160123, preemption_count=0, score=54612.446176, test/accuracy=0.622700, test/loss=1.824234, test/num_examples=10000, total_duration=56508.815519, train/accuracy=0.931700, train/loss=0.238472, validation/accuracy=0.746600, validation/loss=1.071696, validation/num_examples=50000
I0130 11:15:08.424773 139656817530624 logging_writer.py:48] [160200] global_step=160200, grad_norm=4.131412506103516, loss=0.8311024308204651
I0130 11:15:42.633893 139656800745216 logging_writer.py:48] [160300] global_step=160300, grad_norm=4.481148719787598, loss=0.7553852796554565
I0130 11:16:16.646864 139656817530624 logging_writer.py:48] [160400] global_step=160400, grad_norm=4.562719821929932, loss=0.7517079710960388
I0130 11:16:50.681709 139656800745216 logging_writer.py:48] [160500] global_step=160500, grad_norm=4.603900909423828, loss=0.8016769886016846
I0130 11:17:24.784847 139656817530624 logging_writer.py:48] [160600] global_step=160600, grad_norm=4.215214729309082, loss=0.823391854763031
I0130 11:17:58.880742 139656800745216 logging_writer.py:48] [160700] global_step=160700, grad_norm=4.558986186981201, loss=0.8186665773391724
I0130 11:18:32.981920 139656817530624 logging_writer.py:48] [160800] global_step=160800, grad_norm=4.81463623046875, loss=0.7888733148574829
I0130 11:19:07.075551 139656800745216 logging_writer.py:48] [160900] global_step=160900, grad_norm=4.370392322540283, loss=0.8458353281021118
I0130 11:19:41.163090 139656817530624 logging_writer.py:48] [161000] global_step=161000, grad_norm=4.531854629516602, loss=0.6960543394088745
I0130 11:20:15.254121 139656800745216 logging_writer.py:48] [161100] global_step=161100, grad_norm=4.516921043395996, loss=0.7999482750892639
I0130 11:20:49.341765 139656817530624 logging_writer.py:48] [161200] global_step=161200, grad_norm=4.1559295654296875, loss=0.6986838579177856
I0130 11:21:23.443446 139656800745216 logging_writer.py:48] [161300] global_step=161300, grad_norm=4.568875789642334, loss=0.81759113073349
I0130 11:21:57.784849 139656817530624 logging_writer.py:48] [161400] global_step=161400, grad_norm=4.7607903480529785, loss=0.7184261083602905
I0130 11:22:31.876216 139656800745216 logging_writer.py:48] [161500] global_step=161500, grad_norm=4.720093727111816, loss=0.8050117492675781
I0130 11:23:05.954834 139656817530624 logging_writer.py:48] [161600] global_step=161600, grad_norm=4.415657997131348, loss=0.7793228626251221
I0130 11:23:11.894551 139822745589568 spec.py:321] Evaluating on the training split.
I0130 11:23:18.088985 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 11:23:26.477428 139822745589568 spec.py:349] Evaluating on the test split.
I0130 11:23:29.094859 139822745589568 submission_runner.py:408] Time since start: 57036.09s, 	Step: 161619, 	{'train/accuracy': 0.9328961968421936, 'train/loss': 0.23286132514476776, 'validation/accuracy': 0.7475000023841858, 'validation/loss': 1.0679634809494019, 'validation/num_examples': 50000, 'test/accuracy': 0.6202000379562378, 'test/loss': 1.8374985456466675, 'test/num_examples': 10000, 'score': 55122.4037668705, 'total_duration': 57036.08502626419, 'accumulated_submission_time': 55122.4037668705, 'accumulated_eval_time': 1902.8871002197266, 'accumulated_logging_time': 5.275780916213989}
I0130 11:23:29.140587 139656825923328 logging_writer.py:48] [161619] accumulated_eval_time=1902.887100, accumulated_logging_time=5.275781, accumulated_submission_time=55122.403767, global_step=161619, preemption_count=0, score=55122.403767, test/accuracy=0.620200, test/loss=1.837499, test/num_examples=10000, total_duration=57036.085026, train/accuracy=0.932896, train/loss=0.232861, validation/accuracy=0.747500, validation/loss=1.067963, validation/num_examples=50000
I0130 11:23:57.042519 139656834316032 logging_writer.py:48] [161700] global_step=161700, grad_norm=4.188318252563477, loss=0.6643797159194946
I0130 11:24:31.065847 139656825923328 logging_writer.py:48] [161800] global_step=161800, grad_norm=4.287649154663086, loss=0.6727070212364197
I0130 11:25:05.165305 139656834316032 logging_writer.py:48] [161900] global_step=161900, grad_norm=4.227466106414795, loss=0.6917945742607117
I0130 11:25:39.253753 139656825923328 logging_writer.py:48] [162000] global_step=162000, grad_norm=4.6237382888793945, loss=0.7590243816375732
I0130 11:26:13.371093 139656834316032 logging_writer.py:48] [162100] global_step=162100, grad_norm=4.538111209869385, loss=0.7127777338027954
I0130 11:26:47.472521 139656825923328 logging_writer.py:48] [162200] global_step=162200, grad_norm=4.414987087249756, loss=0.7517496347427368
I0130 11:27:21.597594 139656834316032 logging_writer.py:48] [162300] global_step=162300, grad_norm=4.483796119689941, loss=0.8224445581436157
I0130 11:27:55.718845 139656825923328 logging_writer.py:48] [162400] global_step=162400, grad_norm=4.42726469039917, loss=0.6892610788345337
I0130 11:28:29.816323 139656834316032 logging_writer.py:48] [162500] global_step=162500, grad_norm=4.226025581359863, loss=0.716559886932373
I0130 11:29:04.020683 139656825923328 logging_writer.py:48] [162600] global_step=162600, grad_norm=4.4869232177734375, loss=0.6877585649490356
I0130 11:29:38.139891 139656834316032 logging_writer.py:48] [162700] global_step=162700, grad_norm=4.621868133544922, loss=0.7288800477981567
I0130 11:30:12.249531 139656825923328 logging_writer.py:48] [162800] global_step=162800, grad_norm=5.068893909454346, loss=0.783711850643158
I0130 11:30:46.336134 139656834316032 logging_writer.py:48] [162900] global_step=162900, grad_norm=3.9926857948303223, loss=0.6619685888290405
I0130 11:31:20.417448 139656825923328 logging_writer.py:48] [163000] global_step=163000, grad_norm=4.0530500411987305, loss=0.7669275999069214
I0130 11:31:54.531537 139656834316032 logging_writer.py:48] [163100] global_step=163100, grad_norm=4.619467735290527, loss=0.743488073348999
I0130 11:31:59.115217 139822745589568 spec.py:321] Evaluating on the training split.
I0130 11:32:05.400313 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 11:32:14.055950 139822745589568 spec.py:349] Evaluating on the test split.
I0130 11:32:16.651659 139822745589568 submission_runner.py:408] Time since start: 57563.64s, 	Step: 163115, 	{'train/accuracy': 0.9344307780265808, 'train/loss': 0.22658495604991913, 'validation/accuracy': 0.7470999956130981, 'validation/loss': 1.070637583732605, 'validation/num_examples': 50000, 'test/accuracy': 0.6237000226974487, 'test/loss': 1.8353160619735718, 'test/num_examples': 10000, 'score': 55632.31921625137, 'total_duration': 57563.64182114601, 'accumulated_submission_time': 55632.31921625137, 'accumulated_eval_time': 1920.4235010147095, 'accumulated_logging_time': 5.330445289611816}
I0130 11:32:16.702877 139656800745216 logging_writer.py:48] [163115] accumulated_eval_time=1920.423501, accumulated_logging_time=5.330445, accumulated_submission_time=55632.319216, global_step=163115, preemption_count=0, score=55632.319216, test/accuracy=0.623700, test/loss=1.835316, test/num_examples=10000, total_duration=57563.641821, train/accuracy=0.934431, train/loss=0.226585, validation/accuracy=0.747100, validation/loss=1.070638, validation/num_examples=50000
I0130 11:32:45.965350 139656809137920 logging_writer.py:48] [163200] global_step=163200, grad_norm=4.224283695220947, loss=0.788855791091919
I0130 11:33:20.004266 139656800745216 logging_writer.py:48] [163300] global_step=163300, grad_norm=4.450226306915283, loss=0.7752534747123718
I0130 11:33:54.028401 139656809137920 logging_writer.py:48] [163400] global_step=163400, grad_norm=4.544495582580566, loss=0.7179470658302307
I0130 11:34:28.078829 139656800745216 logging_writer.py:48] [163500] global_step=163500, grad_norm=4.427989482879639, loss=0.7223226428031921
I0130 11:35:02.142772 139656809137920 logging_writer.py:48] [163600] global_step=163600, grad_norm=4.112585544586182, loss=0.6528473496437073
I0130 11:35:36.267859 139656800745216 logging_writer.py:48] [163700] global_step=163700, grad_norm=4.781734943389893, loss=0.7653564810752869
I0130 11:36:10.343620 139656809137920 logging_writer.py:48] [163800] global_step=163800, grad_norm=4.323073387145996, loss=0.6826877593994141
I0130 11:36:44.414077 139656800745216 logging_writer.py:48] [163900] global_step=163900, grad_norm=4.1819915771484375, loss=0.7037717700004578
I0130 11:37:18.481017 139656809137920 logging_writer.py:48] [164000] global_step=164000, grad_norm=4.304224491119385, loss=0.7529171109199524
I0130 11:37:52.558799 139656800745216 logging_writer.py:48] [164100] global_step=164100, grad_norm=4.610605716705322, loss=0.721197247505188
I0130 11:38:26.638889 139656809137920 logging_writer.py:48] [164200] global_step=164200, grad_norm=4.12238883972168, loss=0.6595712304115295
I0130 11:39:00.733963 139656800745216 logging_writer.py:48] [164300] global_step=164300, grad_norm=4.402926445007324, loss=0.732608437538147
I0130 11:39:34.824991 139656809137920 logging_writer.py:48] [164400] global_step=164400, grad_norm=4.971691131591797, loss=0.8029213547706604
I0130 11:40:08.916940 139656800745216 logging_writer.py:48] [164500] global_step=164500, grad_norm=4.315881252288818, loss=0.7131040692329407
I0130 11:40:42.994504 139656809137920 logging_writer.py:48] [164600] global_step=164600, grad_norm=4.527536392211914, loss=0.7053787112236023
I0130 11:40:46.882461 139822745589568 spec.py:321] Evaluating on the training split.
I0130 11:40:53.087291 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 11:41:01.487640 139822745589568 spec.py:349] Evaluating on the test split.
I0130 11:41:04.132286 139822745589568 submission_runner.py:408] Time since start: 58091.12s, 	Step: 164613, 	{'train/accuracy': 0.9340322017669678, 'train/loss': 0.22421029210090637, 'validation/accuracy': 0.7490800023078918, 'validation/loss': 1.070961356163025, 'validation/num_examples': 50000, 'test/accuracy': 0.622700035572052, 'test/loss': 1.8354753255844116, 'test/num_examples': 10000, 'score': 56142.438957214355, 'total_duration': 58091.122450351715, 'accumulated_submission_time': 56142.438957214355, 'accumulated_eval_time': 1937.6732861995697, 'accumulated_logging_time': 5.390793085098267}
I0130 11:41:04.178573 139656825923328 logging_writer.py:48] [164613] accumulated_eval_time=1937.673286, accumulated_logging_time=5.390793, accumulated_submission_time=56142.438957, global_step=164613, preemption_count=0, score=56142.438957, test/accuracy=0.622700, test/loss=1.835475, test/num_examples=10000, total_duration=58091.122450, train/accuracy=0.934032, train/loss=0.224210, validation/accuracy=0.749080, validation/loss=1.070961, validation/num_examples=50000
I0130 11:41:34.156255 139656834316032 logging_writer.py:48] [164700] global_step=164700, grad_norm=4.4027910232543945, loss=0.7137826681137085
I0130 11:42:08.425220 139656825923328 logging_writer.py:48] [164800] global_step=164800, grad_norm=4.529500961303711, loss=0.7320512533187866
I0130 11:42:42.476373 139656834316032 logging_writer.py:48] [164900] global_step=164900, grad_norm=4.442700386047363, loss=0.7139983177185059
I0130 11:43:16.515482 139656825923328 logging_writer.py:48] [165000] global_step=165000, grad_norm=4.2923431396484375, loss=0.734365701675415
I0130 11:43:50.584124 139656834316032 logging_writer.py:48] [165100] global_step=165100, grad_norm=4.2709503173828125, loss=0.7216724157333374
I0130 11:44:24.657800 139656825923328 logging_writer.py:48] [165200] global_step=165200, grad_norm=5.084896087646484, loss=0.7089912295341492
I0130 11:44:58.769500 139656834316032 logging_writer.py:48] [165300] global_step=165300, grad_norm=4.393470287322998, loss=0.6835662722587585
I0130 11:45:32.856448 139656825923328 logging_writer.py:48] [165400] global_step=165400, grad_norm=4.36808443069458, loss=0.7461546659469604
I0130 11:46:06.975064 139656834316032 logging_writer.py:48] [165500] global_step=165500, grad_norm=4.346600532531738, loss=0.7331326007843018
I0130 11:46:41.060939 139656825923328 logging_writer.py:48] [165600] global_step=165600, grad_norm=4.474294662475586, loss=0.7319283485412598
I0130 11:47:15.166531 139656834316032 logging_writer.py:48] [165700] global_step=165700, grad_norm=4.435932636260986, loss=0.6934875249862671
I0130 11:47:49.239052 139656825923328 logging_writer.py:48] [165800] global_step=165800, grad_norm=4.573455810546875, loss=0.7198681831359863
I0130 11:48:23.347013 139656834316032 logging_writer.py:48] [165900] global_step=165900, grad_norm=4.173074722290039, loss=0.6926230788230896
I0130 11:48:57.631849 139656825923328 logging_writer.py:48] [166000] global_step=166000, grad_norm=4.717558860778809, loss=0.6760255098342896
I0130 11:49:31.745619 139656834316032 logging_writer.py:48] [166100] global_step=166100, grad_norm=4.386366367340088, loss=0.6525479555130005
I0130 11:49:34.283376 139822745589568 spec.py:321] Evaluating on the training split.
I0130 11:49:40.589880 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 11:49:49.208808 139822745589568 spec.py:349] Evaluating on the test split.
I0130 11:49:51.806048 139822745589568 submission_runner.py:408] Time since start: 58618.80s, 	Step: 166109, 	{'train/accuracy': 0.9427216053009033, 'train/loss': 0.2048528641462326, 'validation/accuracy': 0.7487599849700928, 'validation/loss': 1.0646597146987915, 'validation/num_examples': 50000, 'test/accuracy': 0.624500036239624, 'test/loss': 1.8405344486236572, 'test/num_examples': 10000, 'score': 56652.48220562935, 'total_duration': 58618.796217918396, 'accumulated_submission_time': 56652.48220562935, 'accumulated_eval_time': 1955.1959567070007, 'accumulated_logging_time': 5.446483373641968}
I0130 11:49:51.855839 139656783959808 logging_writer.py:48] [166109] accumulated_eval_time=1955.195957, accumulated_logging_time=5.446483, accumulated_submission_time=56652.482206, global_step=166109, preemption_count=0, score=56652.482206, test/accuracy=0.624500, test/loss=1.840534, test/num_examples=10000, total_duration=58618.796218, train/accuracy=0.942722, train/loss=0.204853, validation/accuracy=0.748760, validation/loss=1.064660, validation/num_examples=50000
I0130 11:50:23.196359 139656800745216 logging_writer.py:48] [166200] global_step=166200, grad_norm=4.1760029792785645, loss=0.7164491415023804
I0130 11:50:57.256605 139656783959808 logging_writer.py:48] [166300] global_step=166300, grad_norm=4.4600653648376465, loss=0.7433294653892517
I0130 11:51:31.360706 139656800745216 logging_writer.py:48] [166400] global_step=166400, grad_norm=4.764693737030029, loss=0.7505843639373779
I0130 11:52:05.456273 139656783959808 logging_writer.py:48] [166500] global_step=166500, grad_norm=4.78434944152832, loss=0.6845826506614685
I0130 11:52:39.532519 139656800745216 logging_writer.py:48] [166600] global_step=166600, grad_norm=4.289239406585693, loss=0.6626175045967102
I0130 11:53:13.628577 139656783959808 logging_writer.py:48] [166700] global_step=166700, grad_norm=4.401238918304443, loss=0.7685129046440125
I0130 11:53:47.715127 139656800745216 logging_writer.py:48] [166800] global_step=166800, grad_norm=4.483936309814453, loss=0.6762546896934509
I0130 11:54:21.761552 139656783959808 logging_writer.py:48] [166900] global_step=166900, grad_norm=4.530752658843994, loss=0.6609232425689697
I0130 11:54:55.850167 139656800745216 logging_writer.py:48] [167000] global_step=167000, grad_norm=4.455664157867432, loss=0.697152316570282
I0130 11:55:30.036851 139656783959808 logging_writer.py:48] [167100] global_step=167100, grad_norm=4.420135021209717, loss=0.7660048007965088
I0130 11:56:04.106410 139656800745216 logging_writer.py:48] [167200] global_step=167200, grad_norm=3.981616497039795, loss=0.6717235445976257
I0130 11:56:38.202080 139656783959808 logging_writer.py:48] [167300] global_step=167300, grad_norm=4.48063325881958, loss=0.7181810736656189
I0130 11:57:12.275178 139656800745216 logging_writer.py:48] [167400] global_step=167400, grad_norm=4.5105133056640625, loss=0.6949434280395508
I0130 11:57:46.391169 139656783959808 logging_writer.py:48] [167500] global_step=167500, grad_norm=4.406149387359619, loss=0.678919792175293
I0130 11:58:20.501780 139656800745216 logging_writer.py:48] [167600] global_step=167600, grad_norm=4.316919803619385, loss=0.7009848356246948
I0130 11:58:22.014289 139822745589568 spec.py:321] Evaluating on the training split.
I0130 11:58:28.253195 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 11:58:36.649548 139822745589568 spec.py:349] Evaluating on the test split.
I0130 11:58:39.312057 139822745589568 submission_runner.py:408] Time since start: 59146.30s, 	Step: 167606, 	{'train/accuracy': 0.9520288109779358, 'train/loss': 0.17483839392662048, 'validation/accuracy': 0.750499963760376, 'validation/loss': 1.0643622875213623, 'validation/num_examples': 50000, 'test/accuracy': 0.6233000159263611, 'test/loss': 1.8415911197662354, 'test/num_examples': 10000, 'score': 57162.58238697052, 'total_duration': 59146.30214428902, 'accumulated_submission_time': 57162.58238697052, 'accumulated_eval_time': 1972.4936077594757, 'accumulated_logging_time': 5.505061149597168}
I0130 11:58:39.357997 139656825923328 logging_writer.py:48] [167606] accumulated_eval_time=1972.493608, accumulated_logging_time=5.505061, accumulated_submission_time=57162.582387, global_step=167606, preemption_count=0, score=57162.582387, test/accuracy=0.623300, test/loss=1.841591, test/num_examples=10000, total_duration=59146.302144, train/accuracy=0.952029, train/loss=0.174838, validation/accuracy=0.750500, validation/loss=1.064362, validation/num_examples=50000
I0130 11:59:11.741168 139656834316032 logging_writer.py:48] [167700] global_step=167700, grad_norm=4.48857307434082, loss=0.6927943229675293
I0130 11:59:45.790083 139656825923328 logging_writer.py:48] [167800] global_step=167800, grad_norm=4.600216388702393, loss=0.6715543866157532
I0130 12:00:19.856328 139656834316032 logging_writer.py:48] [167900] global_step=167900, grad_norm=4.654402256011963, loss=0.6673592329025269
I0130 12:00:53.944037 139656825923328 logging_writer.py:48] [168000] global_step=168000, grad_norm=4.0469489097595215, loss=0.6159772276878357
I0130 12:01:28.003765 139656834316032 logging_writer.py:48] [168100] global_step=168100, grad_norm=4.741125583648682, loss=0.6736223697662354
I0130 12:02:02.176108 139656825923328 logging_writer.py:48] [168200] global_step=168200, grad_norm=4.656208038330078, loss=0.7398781776428223
I0130 12:02:36.271755 139656834316032 logging_writer.py:48] [168300] global_step=168300, grad_norm=4.463768005371094, loss=0.6818788647651672
I0130 12:03:10.334756 139656825923328 logging_writer.py:48] [168400] global_step=168400, grad_norm=4.491214275360107, loss=0.6603078246116638
I0130 12:03:44.380555 139656834316032 logging_writer.py:48] [168500] global_step=168500, grad_norm=4.4459638595581055, loss=0.7173048257827759
I0130 12:04:18.460128 139656825923328 logging_writer.py:48] [168600] global_step=168600, grad_norm=4.657412052154541, loss=0.7239059209823608
I0130 12:04:52.530247 139656834316032 logging_writer.py:48] [168700] global_step=168700, grad_norm=4.610492706298828, loss=0.6803926825523376
I0130 12:05:26.601984 139656825923328 logging_writer.py:48] [168800] global_step=168800, grad_norm=4.682063102722168, loss=0.6863188743591309
I0130 12:06:00.710462 139656834316032 logging_writer.py:48] [168900] global_step=168900, grad_norm=4.769768238067627, loss=0.7046893835067749
I0130 12:06:34.811901 139656825923328 logging_writer.py:48] [169000] global_step=169000, grad_norm=4.8256659507751465, loss=0.7450137138366699
I0130 12:07:08.917863 139656834316032 logging_writer.py:48] [169100] global_step=169100, grad_norm=4.49721097946167, loss=0.7118937969207764
I0130 12:07:09.412047 139822745589568 spec.py:321] Evaluating on the training split.
I0130 12:07:15.611455 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 12:07:24.103647 139822745589568 spec.py:349] Evaluating on the test split.
I0130 12:07:26.727021 139822745589568 submission_runner.py:408] Time since start: 59673.72s, 	Step: 169103, 	{'train/accuracy': 0.9522680044174194, 'train/loss': 0.17501652240753174, 'validation/accuracy': 0.750819981098175, 'validation/loss': 1.0584666728973389, 'validation/num_examples': 50000, 'test/accuracy': 0.6274000406265259, 'test/loss': 1.835326075553894, 'test/num_examples': 10000, 'score': 57672.57480549812, 'total_duration': 59673.71718621254, 'accumulated_submission_time': 57672.57480549812, 'accumulated_eval_time': 1989.808545589447, 'accumulated_logging_time': 5.561115026473999}
I0130 12:07:26.777106 139656792352512 logging_writer.py:48] [169103] accumulated_eval_time=1989.808546, accumulated_logging_time=5.561115, accumulated_submission_time=57672.574805, global_step=169103, preemption_count=0, score=57672.574805, test/accuracy=0.627400, test/loss=1.835326, test/num_examples=10000, total_duration=59673.717186, train/accuracy=0.952268, train/loss=0.175017, validation/accuracy=0.750820, validation/loss=1.058467, validation/num_examples=50000
I0130 12:08:00.158404 139656800745216 logging_writer.py:48] [169200] global_step=169200, grad_norm=4.7812275886535645, loss=0.6354982256889343
I0130 12:08:34.285549 139656792352512 logging_writer.py:48] [169300] global_step=169300, grad_norm=4.428710460662842, loss=0.766858696937561
I0130 12:09:08.376390 139656800745216 logging_writer.py:48] [169400] global_step=169400, grad_norm=4.609787940979004, loss=0.6363956928253174
I0130 12:09:42.462844 139656792352512 logging_writer.py:48] [169500] global_step=169500, grad_norm=4.601023197174072, loss=0.666393518447876
I0130 12:10:16.543477 139656800745216 logging_writer.py:48] [169600] global_step=169600, grad_norm=4.251596927642822, loss=0.6703556180000305
I0130 12:10:50.611401 139656792352512 logging_writer.py:48] [169700] global_step=169700, grad_norm=4.048304557800293, loss=0.6933723092079163
I0130 12:11:24.669845 139656800745216 logging_writer.py:48] [169800] global_step=169800, grad_norm=4.281798362731934, loss=0.7286752462387085
I0130 12:11:58.793002 139656792352512 logging_writer.py:48] [169900] global_step=169900, grad_norm=4.434032917022705, loss=0.6790452599525452
I0130 12:12:32.875784 139656800745216 logging_writer.py:48] [170000] global_step=170000, grad_norm=5.03092622756958, loss=0.7642735242843628
I0130 12:13:06.931481 139656792352512 logging_writer.py:48] [170100] global_step=170100, grad_norm=4.211790561676025, loss=0.6596966981887817
I0130 12:13:40.985465 139656800745216 logging_writer.py:48] [170200] global_step=170200, grad_norm=4.689966678619385, loss=0.6745378375053406
I0130 12:14:15.072021 139656792352512 logging_writer.py:48] [170300] global_step=170300, grad_norm=4.4805378913879395, loss=0.6624005436897278
I0130 12:14:49.142390 139656800745216 logging_writer.py:48] [170400] global_step=170400, grad_norm=4.409077167510986, loss=0.6534768342971802
I0130 12:15:23.308962 139656792352512 logging_writer.py:48] [170500] global_step=170500, grad_norm=4.589545249938965, loss=0.6716249585151672
I0130 12:15:56.827746 139822745589568 spec.py:321] Evaluating on the training split.
I0130 12:16:03.164425 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 12:16:11.688901 139822745589568 spec.py:349] Evaluating on the test split.
I0130 12:16:14.308335 139822745589568 submission_runner.py:408] Time since start: 60201.30s, 	Step: 170600, 	{'train/accuracy': 0.9530253410339355, 'train/loss': 0.16859164834022522, 'validation/accuracy': 0.7524200081825256, 'validation/loss': 1.058369517326355, 'validation/num_examples': 50000, 'test/accuracy': 0.6294000148773193, 'test/loss': 1.825202226638794, 'test/num_examples': 10000, 'score': 58182.56483054161, 'total_duration': 60201.29833936691, 'accumulated_submission_time': 58182.56483054161, 'accumulated_eval_time': 2007.2889399528503, 'accumulated_logging_time': 5.620239496231079}
I0130 12:16:14.360258 139656817530624 logging_writer.py:48] [170600] accumulated_eval_time=2007.288940, accumulated_logging_time=5.620239, accumulated_submission_time=58182.564831, global_step=170600, preemption_count=0, score=58182.564831, test/accuracy=0.629400, test/loss=1.825202, test/num_examples=10000, total_duration=60201.298339, train/accuracy=0.953025, train/loss=0.168592, validation/accuracy=0.752420, validation/loss=1.058370, validation/num_examples=50000
I0130 12:16:14.717194 139656825923328 logging_writer.py:48] [170600] global_step=170600, grad_norm=5.162607192993164, loss=0.6956082582473755
I0130 12:16:48.743856 139656817530624 logging_writer.py:48] [170700] global_step=170700, grad_norm=4.787017345428467, loss=0.7192877531051636
I0130 12:17:22.788559 139656825923328 logging_writer.py:48] [170800] global_step=170800, grad_norm=4.437494277954102, loss=0.7105376720428467
I0130 12:17:56.856845 139656817530624 logging_writer.py:48] [170900] global_step=170900, grad_norm=4.5620036125183105, loss=0.7521142363548279
I0130 12:18:30.940362 139656825923328 logging_writer.py:48] [171000] global_step=171000, grad_norm=4.3670430183410645, loss=0.7282801866531372
I0130 12:19:04.987847 139656817530624 logging_writer.py:48] [171100] global_step=171100, grad_norm=4.577800750732422, loss=0.6516659259796143
I0130 12:19:39.077157 139656825923328 logging_writer.py:48] [171200] global_step=171200, grad_norm=4.483098030090332, loss=0.6728103756904602
I0130 12:20:13.155186 139656817530624 logging_writer.py:48] [171300] global_step=171300, grad_norm=4.751002788543701, loss=0.6776527762413025
I0130 12:20:47.256662 139656825923328 logging_writer.py:48] [171400] global_step=171400, grad_norm=4.3577494621276855, loss=0.7050687074661255
I0130 12:21:21.367027 139656817530624 logging_writer.py:48] [171500] global_step=171500, grad_norm=4.430912017822266, loss=0.6017290353775024
I0130 12:21:55.554102 139656825923328 logging_writer.py:48] [171600] global_step=171600, grad_norm=4.324424743652344, loss=0.6131255030632019
I0130 12:22:29.654085 139656817530624 logging_writer.py:48] [171700] global_step=171700, grad_norm=4.671634197235107, loss=0.7496792674064636
I0130 12:23:03.745497 139656825923328 logging_writer.py:48] [171800] global_step=171800, grad_norm=4.724898815155029, loss=0.6377874612808228
I0130 12:23:37.861773 139656817530624 logging_writer.py:48] [171900] global_step=171900, grad_norm=4.5246710777282715, loss=0.5954843163490295
I0130 12:24:11.982051 139656825923328 logging_writer.py:48] [172000] global_step=172000, grad_norm=4.549208164215088, loss=0.6353880167007446
I0130 12:24:44.534534 139822745589568 spec.py:321] Evaluating on the training split.
I0130 12:24:50.762033 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 12:24:59.345973 139822745589568 spec.py:349] Evaluating on the test split.
I0130 12:25:01.980689 139822745589568 submission_runner.py:408] Time since start: 60728.97s, 	Step: 172097, 	{'train/accuracy': 0.9518095850944519, 'train/loss': 0.17187651991844177, 'validation/accuracy': 0.7515000104904175, 'validation/loss': 1.0592249631881714, 'validation/num_examples': 50000, 'test/accuracy': 0.628600001335144, 'test/loss': 1.8379465341567993, 'test/num_examples': 10000, 'score': 58692.67744731903, 'total_duration': 60728.97080159187, 'accumulated_submission_time': 58692.67744731903, 'accumulated_eval_time': 2024.7350087165833, 'accumulated_logging_time': 5.683067798614502}
I0130 12:25:02.061625 139656792352512 logging_writer.py:48] [172097] accumulated_eval_time=2024.735009, accumulated_logging_time=5.683068, accumulated_submission_time=58692.677447, global_step=172097, preemption_count=0, score=58692.677447, test/accuracy=0.628600, test/loss=1.837947, test/num_examples=10000, total_duration=60728.970802, train/accuracy=0.951810, train/loss=0.171877, validation/accuracy=0.751500, validation/loss=1.059225, validation/num_examples=50000
I0130 12:25:03.439853 139656800745216 logging_writer.py:48] [172100] global_step=172100, grad_norm=4.353446006774902, loss=0.7105482220649719
I0130 12:25:37.498989 139656792352512 logging_writer.py:48] [172200] global_step=172200, grad_norm=4.979401588439941, loss=0.6757999658584595
I0130 12:26:11.560864 139656800745216 logging_writer.py:48] [172300] global_step=172300, grad_norm=4.790284633636475, loss=0.7078486084938049
I0130 12:26:45.621538 139656792352512 logging_writer.py:48] [172400] global_step=172400, grad_norm=4.675256729125977, loss=0.6526461243629456
I0130 12:27:19.720696 139656800745216 logging_writer.py:48] [172500] global_step=172500, grad_norm=4.905550479888916, loss=0.6597793102264404
I0130 12:27:53.817444 139656792352512 logging_writer.py:48] [172600] global_step=172600, grad_norm=4.39443826675415, loss=0.6619558334350586
I0130 12:28:27.975532 139656800745216 logging_writer.py:48] [172700] global_step=172700, grad_norm=4.353357315063477, loss=0.6824031472206116
I0130 12:29:02.044862 139656792352512 logging_writer.py:48] [172800] global_step=172800, grad_norm=4.487922191619873, loss=0.6649284958839417
I0130 12:29:36.129513 139656800745216 logging_writer.py:48] [172900] global_step=172900, grad_norm=4.534764289855957, loss=0.6479067802429199
I0130 12:30:10.242105 139656792352512 logging_writer.py:48] [173000] global_step=173000, grad_norm=4.674501895904541, loss=0.617621898651123
I0130 12:30:44.372864 139656800745216 logging_writer.py:48] [173100] global_step=173100, grad_norm=4.326290607452393, loss=0.6676362156867981
I0130 12:31:18.464079 139656792352512 logging_writer.py:48] [173200] global_step=173200, grad_norm=4.687078475952148, loss=0.6406286358833313
I0130 12:31:52.513707 139656800745216 logging_writer.py:48] [173300] global_step=173300, grad_norm=4.48878812789917, loss=0.6372550129890442
I0130 12:32:26.600671 139656792352512 logging_writer.py:48] [173400] global_step=173400, grad_norm=4.564116954803467, loss=0.6256132125854492
I0130 12:33:00.649830 139656800745216 logging_writer.py:48] [173500] global_step=173500, grad_norm=4.278651237487793, loss=0.5934536457061768
I0130 12:33:32.132880 139822745589568 spec.py:321] Evaluating on the training split.
I0130 12:33:39.002648 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 12:33:47.463740 139822745589568 spec.py:349] Evaluating on the test split.
I0130 12:33:50.073395 139822745589568 submission_runner.py:408] Time since start: 61257.06s, 	Step: 173594, 	{'train/accuracy': 0.9528858065605164, 'train/loss': 0.16821564733982086, 'validation/accuracy': 0.754040002822876, 'validation/loss': 1.060206651687622, 'validation/num_examples': 50000, 'test/accuracy': 0.6281000375747681, 'test/loss': 1.8330553770065308, 'test/num_examples': 10000, 'score': 59202.68797492981, 'total_duration': 61257.06341433525, 'accumulated_submission_time': 59202.68797492981, 'accumulated_eval_time': 2042.6753525733948, 'accumulated_logging_time': 5.773506164550781}
I0130 12:33:50.121237 139656817530624 logging_writer.py:48] [173594] accumulated_eval_time=2042.675353, accumulated_logging_time=5.773506, accumulated_submission_time=59202.687975, global_step=173594, preemption_count=0, score=59202.687975, test/accuracy=0.628100, test/loss=1.833055, test/num_examples=10000, total_duration=61257.063414, train/accuracy=0.952886, train/loss=0.168216, validation/accuracy=0.754040, validation/loss=1.060207, validation/num_examples=50000
I0130 12:33:52.537978 139656825923328 logging_writer.py:48] [173600] global_step=173600, grad_norm=4.509027481079102, loss=0.6511105298995972
I0130 12:34:26.596788 139656817530624 logging_writer.py:48] [173700] global_step=173700, grad_norm=4.637670516967773, loss=0.697654664516449
I0130 12:35:00.776627 139656825923328 logging_writer.py:48] [173800] global_step=173800, grad_norm=4.220429420471191, loss=0.6327948570251465
I0130 12:35:34.867922 139656817530624 logging_writer.py:48] [173900] global_step=173900, grad_norm=4.322484493255615, loss=0.5812582969665527
I0130 12:36:08.991710 139656825923328 logging_writer.py:48] [174000] global_step=174000, grad_norm=4.2139739990234375, loss=0.5692927241325378
I0130 12:36:43.107168 139656817530624 logging_writer.py:48] [174100] global_step=174100, grad_norm=5.6711039543151855, loss=0.6770925521850586
I0130 12:37:17.204229 139656825923328 logging_writer.py:48] [174200] global_step=174200, grad_norm=4.757208347320557, loss=0.775749921798706
I0130 12:37:51.307267 139656817530624 logging_writer.py:48] [174300] global_step=174300, grad_norm=4.793710708618164, loss=0.6504364609718323
I0130 12:38:25.404333 139656825923328 logging_writer.py:48] [174400] global_step=174400, grad_norm=4.57211446762085, loss=0.6283526420593262
I0130 12:38:59.511444 139656817530624 logging_writer.py:48] [174500] global_step=174500, grad_norm=4.62603759765625, loss=0.702012300491333
I0130 12:39:33.627887 139656825923328 logging_writer.py:48] [174600] global_step=174600, grad_norm=4.381332874298096, loss=0.5963461399078369
I0130 12:40:07.711940 139656817530624 logging_writer.py:48] [174700] global_step=174700, grad_norm=5.011917591094971, loss=0.6671531796455383
I0130 12:40:41.810545 139656825923328 logging_writer.py:48] [174800] global_step=174800, grad_norm=5.1413116455078125, loss=0.6607247591018677
I0130 12:41:15.908834 139656817530624 logging_writer.py:48] [174900] global_step=174900, grad_norm=4.626657009124756, loss=0.6525945067405701
I0130 12:41:50.117913 139656825923328 logging_writer.py:48] [175000] global_step=175000, grad_norm=4.715073585510254, loss=0.6458353996276855
I0130 12:42:20.292154 139822745589568 spec.py:321] Evaluating on the training split.
I0130 12:42:26.617878 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 12:42:35.212161 139822745589568 spec.py:349] Evaluating on the test split.
I0130 12:42:37.873050 139822745589568 submission_runner.py:408] Time since start: 61784.86s, 	Step: 175090, 	{'train/accuracy': 0.9554169178009033, 'train/loss': 0.16442981362342834, 'validation/accuracy': 0.754040002822876, 'validation/loss': 1.0577527284622192, 'validation/num_examples': 50000, 'test/accuracy': 0.6284000277519226, 'test/loss': 1.8298102617263794, 'test/num_examples': 10000, 'score': 59712.79919052124, 'total_duration': 61784.8630464077, 'accumulated_submission_time': 59712.79919052124, 'accumulated_eval_time': 2060.2560591697693, 'accumulated_logging_time': 5.830533027648926}
I0130 12:42:37.924667 139656783959808 logging_writer.py:48] [175090] accumulated_eval_time=2060.256059, accumulated_logging_time=5.830533, accumulated_submission_time=59712.799191, global_step=175090, preemption_count=0, score=59712.799191, test/accuracy=0.628400, test/loss=1.829810, test/num_examples=10000, total_duration=61784.863046, train/accuracy=0.955417, train/loss=0.164430, validation/accuracy=0.754040, validation/loss=1.057753, validation/num_examples=50000
I0130 12:42:41.668082 139656792352512 logging_writer.py:48] [175100] global_step=175100, grad_norm=4.677342414855957, loss=0.6589639186859131
I0130 12:43:15.718743 139656783959808 logging_writer.py:48] [175200] global_step=175200, grad_norm=4.5843305587768555, loss=0.7476409673690796
I0130 12:43:49.799578 139656792352512 logging_writer.py:48] [175300] global_step=175300, grad_norm=4.877585411071777, loss=0.6299923658370972
I0130 12:44:23.867743 139656783959808 logging_writer.py:48] [175400] global_step=175400, grad_norm=4.811732769012451, loss=0.6772343516349792
I0130 12:44:57.944017 139656792352512 logging_writer.py:48] [175500] global_step=175500, grad_norm=4.330934524536133, loss=0.684823751449585
I0130 12:45:32.019767 139656783959808 logging_writer.py:48] [175600] global_step=175600, grad_norm=4.460148334503174, loss=0.6323734521865845
I0130 12:46:06.075811 139656792352512 logging_writer.py:48] [175700] global_step=175700, grad_norm=5.036344528198242, loss=0.6719000935554504
I0130 12:46:40.156041 139656783959808 logging_writer.py:48] [175800] global_step=175800, grad_norm=4.70002555847168, loss=0.6550140380859375
I0130 12:47:14.243241 139656792352512 logging_writer.py:48] [175900] global_step=175900, grad_norm=4.273006916046143, loss=0.6195793151855469
I0130 12:47:48.345222 139656783959808 logging_writer.py:48] [176000] global_step=176000, grad_norm=4.6366095542907715, loss=0.7717828750610352
I0130 12:48:22.530131 139656792352512 logging_writer.py:48] [176100] global_step=176100, grad_norm=4.620856761932373, loss=0.6002309322357178
I0130 12:48:56.622499 139656783959808 logging_writer.py:48] [176200] global_step=176200, grad_norm=4.604811668395996, loss=0.5936533212661743
I0130 12:49:30.733546 139656792352512 logging_writer.py:48] [176300] global_step=176300, grad_norm=4.7042317390441895, loss=0.7305028438568115
I0130 12:50:04.834922 139656783959808 logging_writer.py:48] [176400] global_step=176400, grad_norm=4.743996620178223, loss=0.6369677782058716
I0130 12:50:38.922280 139656792352512 logging_writer.py:48] [176500] global_step=176500, grad_norm=4.632827281951904, loss=0.6166836023330688
I0130 12:51:08.056698 139822745589568 spec.py:321] Evaluating on the training split.
I0130 12:51:14.295739 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 12:51:22.749194 139822745589568 spec.py:349] Evaluating on the test split.
I0130 12:51:25.372577 139822745589568 submission_runner.py:408] Time since start: 62312.36s, 	Step: 176587, 	{'train/accuracy': 0.9605588316917419, 'train/loss': 0.1476965844631195, 'validation/accuracy': 0.7540599703788757, 'validation/loss': 1.0551767349243164, 'validation/num_examples': 50000, 'test/accuracy': 0.626800000667572, 'test/loss': 1.8359673023223877, 'test/num_examples': 10000, 'score': 60222.87142467499, 'total_duration': 62312.36273407936, 'accumulated_submission_time': 60222.87142467499, 'accumulated_eval_time': 2077.571899175644, 'accumulated_logging_time': 5.891931772232056}
I0130 12:51:25.425513 139656783959808 logging_writer.py:48] [176587] accumulated_eval_time=2077.571899, accumulated_logging_time=5.891932, accumulated_submission_time=60222.871425, global_step=176587, preemption_count=0, score=60222.871425, test/accuracy=0.626800, test/loss=1.835967, test/num_examples=10000, total_duration=62312.362734, train/accuracy=0.960559, train/loss=0.147697, validation/accuracy=0.754060, validation/loss=1.055177, validation/num_examples=50000
I0130 12:51:30.218507 139656817530624 logging_writer.py:48] [176600] global_step=176600, grad_norm=4.718891620635986, loss=0.6242533922195435
I0130 12:52:04.274062 139656783959808 logging_writer.py:48] [176700] global_step=176700, grad_norm=4.499896049499512, loss=0.5918673872947693
I0130 12:52:38.341013 139656817530624 logging_writer.py:48] [176800] global_step=176800, grad_norm=4.8668212890625, loss=0.6962367296218872
I0130 12:53:12.452448 139656783959808 logging_writer.py:48] [176900] global_step=176900, grad_norm=4.781064510345459, loss=0.5893926620483398
I0130 12:53:46.546342 139656817530624 logging_writer.py:48] [177000] global_step=177000, grad_norm=4.700495719909668, loss=0.6036352515220642
I0130 12:54:20.634814 139656783959808 logging_writer.py:48] [177100] global_step=177100, grad_norm=4.376038551330566, loss=0.5842998623847961
I0130 12:54:54.819819 139656817530624 logging_writer.py:48] [177200] global_step=177200, grad_norm=4.468282222747803, loss=0.6895367503166199
I0130 12:55:28.912195 139656783959808 logging_writer.py:48] [177300] global_step=177300, grad_norm=4.510768413543701, loss=0.6511356830596924
I0130 12:56:02.994461 139656817530624 logging_writer.py:48] [177400] global_step=177400, grad_norm=4.554007053375244, loss=0.5577087998390198
I0130 12:56:37.091175 139656783959808 logging_writer.py:48] [177500] global_step=177500, grad_norm=4.705788612365723, loss=0.638749361038208
I0130 12:57:11.178723 139656817530624 logging_writer.py:48] [177600] global_step=177600, grad_norm=4.78612756729126, loss=0.6373236775398254
I0130 12:57:45.293767 139656783959808 logging_writer.py:48] [177700] global_step=177700, grad_norm=4.567109107971191, loss=0.5868473052978516
I0130 12:58:19.389408 139656817530624 logging_writer.py:48] [177800] global_step=177800, grad_norm=4.260703086853027, loss=0.6129661202430725
I0130 12:58:53.464687 139656783959808 logging_writer.py:48] [177900] global_step=177900, grad_norm=4.129812717437744, loss=0.5757313370704651
I0130 12:59:27.550207 139656817530624 logging_writer.py:48] [178000] global_step=178000, grad_norm=4.13826847076416, loss=0.5941547751426697
I0130 12:59:55.640856 139822745589568 spec.py:321] Evaluating on the training split.
I0130 13:00:02.033432 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 13:00:10.424212 139822745589568 spec.py:349] Evaluating on the test split.
I0130 13:00:13.036762 139822745589568 submission_runner.py:408] Time since start: 62840.03s, 	Step: 178084, 	{'train/accuracy': 0.9595822691917419, 'train/loss': 0.1489991694688797, 'validation/accuracy': 0.7542799711227417, 'validation/loss': 1.0563178062438965, 'validation/num_examples': 50000, 'test/accuracy': 0.6317000389099121, 'test/loss': 1.831668496131897, 'test/num_examples': 10000, 'score': 60733.02505970001, 'total_duration': 62840.026733636856, 'accumulated_submission_time': 60733.02505970001, 'accumulated_eval_time': 2094.967592716217, 'accumulated_logging_time': 5.955878973007202}
I0130 13:00:13.085403 139656792352512 logging_writer.py:48] [178084] accumulated_eval_time=2094.967593, accumulated_logging_time=5.955879, accumulated_submission_time=60733.025060, global_step=178084, preemption_count=0, score=60733.025060, test/accuracy=0.631700, test/loss=1.831668, test/num_examples=10000, total_duration=62840.026734, train/accuracy=0.959582, train/loss=0.148999, validation/accuracy=0.754280, validation/loss=1.056318, validation/num_examples=50000
I0130 13:00:18.880638 139656800745216 logging_writer.py:48] [178100] global_step=178100, grad_norm=3.942621946334839, loss=0.5438281893730164
I0130 13:00:52.917694 139656792352512 logging_writer.py:48] [178200] global_step=178200, grad_norm=4.2957611083984375, loss=0.5735282301902771
I0130 13:01:26.931922 139656800745216 logging_writer.py:48] [178300] global_step=178300, grad_norm=4.539766788482666, loss=0.6622930765151978
I0130 13:02:01.217593 139656792352512 logging_writer.py:48] [178400] global_step=178400, grad_norm=4.361280918121338, loss=0.5857015252113342
I0130 13:02:35.277311 139656800745216 logging_writer.py:48] [178500] global_step=178500, grad_norm=4.3856892585754395, loss=0.6009776592254639
I0130 13:03:09.348460 139656792352512 logging_writer.py:48] [178600] global_step=178600, grad_norm=4.49556303024292, loss=0.6380555033683777
I0130 13:03:43.392385 139656800745216 logging_writer.py:48] [178700] global_step=178700, grad_norm=4.164351940155029, loss=0.5713927149772644
I0130 13:04:17.462605 139656792352512 logging_writer.py:48] [178800] global_step=178800, grad_norm=4.316634178161621, loss=0.6483210325241089
I0130 13:04:51.530576 139656800745216 logging_writer.py:48] [178900] global_step=178900, grad_norm=5.072632312774658, loss=0.6187252402305603
I0130 13:05:25.598679 139656792352512 logging_writer.py:48] [179000] global_step=179000, grad_norm=4.015865325927734, loss=0.6039758324623108
I0130 13:05:59.638301 139656800745216 logging_writer.py:48] [179100] global_step=179100, grad_norm=3.9494049549102783, loss=0.5220668315887451
I0130 13:06:33.689538 139656792352512 logging_writer.py:48] [179200] global_step=179200, grad_norm=4.54835844039917, loss=0.6532800793647766
I0130 13:07:07.759312 139656800745216 logging_writer.py:48] [179300] global_step=179300, grad_norm=4.60803747177124, loss=0.5632092952728271
I0130 13:07:41.845041 139656792352512 logging_writer.py:48] [179400] global_step=179400, grad_norm=4.944097995758057, loss=0.6511068940162659
I0130 13:08:16.037614 139656800745216 logging_writer.py:48] [179500] global_step=179500, grad_norm=5.0885725021362305, loss=0.6617246270179749
I0130 13:08:43.090820 139822745589568 spec.py:321] Evaluating on the training split.
I0130 13:08:49.263844 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 13:08:57.868368 139822745589568 spec.py:349] Evaluating on the test split.
I0130 13:09:00.525560 139822745589568 submission_runner.py:408] Time since start: 63367.52s, 	Step: 179581, 	{'train/accuracy': 0.9616549611091614, 'train/loss': 0.14457814395427704, 'validation/accuracy': 0.7544199824333191, 'validation/loss': 1.0557457208633423, 'validation/num_examples': 50000, 'test/accuracy': 0.6299000382423401, 'test/loss': 1.8318003416061401, 'test/num_examples': 10000, 'score': 61242.96651220322, 'total_duration': 63367.515714883804, 'accumulated_submission_time': 61242.96651220322, 'accumulated_eval_time': 2112.402285337448, 'accumulated_logging_time': 6.015355348587036}
I0130 13:09:00.572628 139656825923328 logging_writer.py:48] [179581] accumulated_eval_time=2112.402285, accumulated_logging_time=6.015355, accumulated_submission_time=61242.966512, global_step=179581, preemption_count=0, score=61242.966512, test/accuracy=0.629900, test/loss=1.831800, test/num_examples=10000, total_duration=63367.515715, train/accuracy=0.961655, train/loss=0.144578, validation/accuracy=0.754420, validation/loss=1.055746, validation/num_examples=50000
I0130 13:09:07.417605 139656834316032 logging_writer.py:48] [179600] global_step=179600, grad_norm=4.999276638031006, loss=0.6145814657211304
I0130 13:09:41.490906 139656825923328 logging_writer.py:48] [179700] global_step=179700, grad_norm=3.976417064666748, loss=0.6058696508407593
I0130 13:10:15.533861 139656834316032 logging_writer.py:48] [179800] global_step=179800, grad_norm=4.403420448303223, loss=0.6549058556556702
I0130 13:10:49.616740 139656825923328 logging_writer.py:48] [179900] global_step=179900, grad_norm=4.917288780212402, loss=0.6295171976089478
I0130 13:11:23.729233 139656834316032 logging_writer.py:48] [180000] global_step=180000, grad_norm=4.946835517883301, loss=0.6537348628044128
I0130 13:11:57.805398 139656825923328 logging_writer.py:48] [180100] global_step=180100, grad_norm=4.791168212890625, loss=0.6923214793205261
I0130 13:12:31.880112 139656834316032 logging_writer.py:48] [180200] global_step=180200, grad_norm=4.732038497924805, loss=0.6271779537200928
I0130 13:13:05.951073 139656825923328 logging_writer.py:48] [180300] global_step=180300, grad_norm=4.331787109375, loss=0.5867236256599426
I0130 13:13:40.026478 139656834316032 logging_writer.py:48] [180400] global_step=180400, grad_norm=4.854413986206055, loss=0.6093398332595825
I0130 13:14:14.129400 139656825923328 logging_writer.py:48] [180500] global_step=180500, grad_norm=4.613527297973633, loss=0.6685804724693298
I0130 13:14:48.322041 139656834316032 logging_writer.py:48] [180600] global_step=180600, grad_norm=4.921931743621826, loss=0.6028728485107422
I0130 13:15:22.412606 139656825923328 logging_writer.py:48] [180700] global_step=180700, grad_norm=4.387505531311035, loss=0.5979058742523193
I0130 13:15:56.501701 139656834316032 logging_writer.py:48] [180800] global_step=180800, grad_norm=4.671468257904053, loss=0.6324405670166016
I0130 13:16:30.580083 139656825923328 logging_writer.py:48] [180900] global_step=180900, grad_norm=4.568639755249023, loss=0.5872632265090942
I0130 13:17:04.652810 139656834316032 logging_writer.py:48] [181000] global_step=181000, grad_norm=5.55996036529541, loss=0.655271053314209
I0130 13:17:30.682545 139822745589568 spec.py:321] Evaluating on the training split.
I0130 13:17:36.938405 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 13:17:45.277376 139822745589568 spec.py:349] Evaluating on the test split.
I0130 13:17:47.794451 139822745589568 submission_runner.py:408] Time since start: 63894.78s, 	Step: 181078, 	{'train/accuracy': 0.9592434167861938, 'train/loss': 0.1504308581352234, 'validation/accuracy': 0.7547599673271179, 'validation/loss': 1.0523484945297241, 'validation/num_examples': 50000, 'test/accuracy': 0.629800021648407, 'test/loss': 1.8307338953018188, 'test/num_examples': 10000, 'score': 61753.014424562454, 'total_duration': 63894.78460788727, 'accumulated_submission_time': 61753.014424562454, 'accumulated_eval_time': 2129.514147043228, 'accumulated_logging_time': 6.073501348495483}
I0130 13:17:47.849249 139656809137920 logging_writer.py:48] [181078] accumulated_eval_time=2129.514147, accumulated_logging_time=6.073501, accumulated_submission_time=61753.014425, global_step=181078, preemption_count=0, score=61753.014425, test/accuracy=0.629800, test/loss=1.830734, test/num_examples=10000, total_duration=63894.784608, train/accuracy=0.959243, train/loss=0.150431, validation/accuracy=0.754760, validation/loss=1.052348, validation/num_examples=50000
I0130 13:17:55.707345 139656817530624 logging_writer.py:48] [181100] global_step=181100, grad_norm=4.457429885864258, loss=0.615096926689148
I0130 13:18:29.718510 139656809137920 logging_writer.py:48] [181200] global_step=181200, grad_norm=4.810136795043945, loss=0.6250635981559753
I0130 13:19:03.765353 139656817530624 logging_writer.py:48] [181300] global_step=181300, grad_norm=4.534045219421387, loss=0.6299400925636292
I0130 13:19:37.810061 139656809137920 logging_writer.py:48] [181400] global_step=181400, grad_norm=4.48614501953125, loss=0.6672804951667786
I0130 13:20:11.900522 139656817530624 logging_writer.py:48] [181500] global_step=181500, grad_norm=4.7319655418396, loss=0.6735057830810547
I0130 13:20:45.977119 139656809137920 logging_writer.py:48] [181600] global_step=181600, grad_norm=4.2647480964660645, loss=0.5994017124176025
I0130 13:21:20.081484 139656817530624 logging_writer.py:48] [181700] global_step=181700, grad_norm=4.2102155685424805, loss=0.5750423073768616
I0130 13:21:54.273767 139656809137920 logging_writer.py:48] [181800] global_step=181800, grad_norm=4.739623546600342, loss=0.6874393820762634
I0130 13:22:28.347809 139656817530624 logging_writer.py:48] [181900] global_step=181900, grad_norm=4.358006477355957, loss=0.6346710324287415
I0130 13:23:02.412543 139656809137920 logging_writer.py:48] [182000] global_step=182000, grad_norm=4.521533012390137, loss=0.6596457958221436
I0130 13:23:36.523725 139656817530624 logging_writer.py:48] [182100] global_step=182100, grad_norm=4.65545654296875, loss=0.6293655633926392
I0130 13:24:10.604710 139656809137920 logging_writer.py:48] [182200] global_step=182200, grad_norm=4.177913188934326, loss=0.6253227591514587
I0130 13:24:44.730647 139656817530624 logging_writer.py:48] [182300] global_step=182300, grad_norm=5.059300899505615, loss=0.6600874662399292
I0130 13:25:18.811165 139656809137920 logging_writer.py:48] [182400] global_step=182400, grad_norm=4.509013652801514, loss=0.6040501594543457
I0130 13:25:52.917656 139656817530624 logging_writer.py:48] [182500] global_step=182500, grad_norm=4.605483055114746, loss=0.5868476033210754
I0130 13:26:17.954722 139822745589568 spec.py:321] Evaluating on the training split.
I0130 13:26:24.228078 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 13:26:32.718946 139822745589568 spec.py:349] Evaluating on the test split.
I0130 13:26:35.212370 139822745589568 submission_runner.py:408] Time since start: 64422.20s, 	Step: 182575, 	{'train/accuracy': 0.9605388641357422, 'train/loss': 0.14766532182693481, 'validation/accuracy': 0.7547999620437622, 'validation/loss': 1.0509791374206543, 'validation/num_examples': 50000, 'test/accuracy': 0.6289000511169434, 'test/loss': 1.8282490968704224, 'test/num_examples': 10000, 'score': 62263.06065821648, 'total_duration': 64422.20252633095, 'accumulated_submission_time': 62263.06065821648, 'accumulated_eval_time': 2146.771764278412, 'accumulated_logging_time': 6.1375648975372314}
I0130 13:26:35.266963 139656800745216 logging_writer.py:48] [182575] accumulated_eval_time=2146.771764, accumulated_logging_time=6.137565, accumulated_submission_time=62263.060658, global_step=182575, preemption_count=0, score=62263.060658, test/accuracy=0.628900, test/loss=1.828249, test/num_examples=10000, total_duration=64422.202526, train/accuracy=0.960539, train/loss=0.147665, validation/accuracy=0.754800, validation/loss=1.050979, validation/num_examples=50000
I0130 13:26:44.146343 139656825923328 logging_writer.py:48] [182600] global_step=182600, grad_norm=4.236796855926514, loss=0.5693089962005615
I0130 13:27:18.171373 139656800745216 logging_writer.py:48] [182700] global_step=182700, grad_norm=4.439651012420654, loss=0.645243227481842
I0130 13:27:52.196671 139656825923328 logging_writer.py:48] [182800] global_step=182800, grad_norm=4.519423961639404, loss=0.6121290922164917
I0130 13:28:26.360900 139656800745216 logging_writer.py:48] [182900] global_step=182900, grad_norm=4.243171691894531, loss=0.5626640319824219
I0130 13:29:00.403104 139656825923328 logging_writer.py:48] [183000] global_step=183000, grad_norm=4.676695346832275, loss=0.6462454795837402
I0130 13:29:34.490964 139656800745216 logging_writer.py:48] [183100] global_step=183100, grad_norm=4.4875640869140625, loss=0.5877344012260437
I0130 13:30:08.548594 139656825923328 logging_writer.py:48] [183200] global_step=183200, grad_norm=4.442550182342529, loss=0.5960671901702881
I0130 13:30:42.612675 139656800745216 logging_writer.py:48] [183300] global_step=183300, grad_norm=4.816526412963867, loss=0.6763915419578552
I0130 13:31:16.685426 139656825923328 logging_writer.py:48] [183400] global_step=183400, grad_norm=4.5241827964782715, loss=0.5704213380813599
I0130 13:31:50.750836 139656800745216 logging_writer.py:48] [183500] global_step=183500, grad_norm=4.171738147735596, loss=0.5802268981933594
I0130 13:32:24.837239 139656825923328 logging_writer.py:48] [183600] global_step=183600, grad_norm=4.410167217254639, loss=0.5928523540496826
I0130 13:32:58.925259 139656800745216 logging_writer.py:48] [183700] global_step=183700, grad_norm=4.432012557983398, loss=0.5980599522590637
I0130 13:33:32.991953 139656825923328 logging_writer.py:48] [183800] global_step=183800, grad_norm=4.399200916290283, loss=0.7135884761810303
I0130 13:34:07.067672 139656800745216 logging_writer.py:48] [183900] global_step=183900, grad_norm=4.2033843994140625, loss=0.6334443688392639
I0130 13:34:41.199776 139656825923328 logging_writer.py:48] [184000] global_step=184000, grad_norm=4.414162635803223, loss=0.6123607754707336
I0130 13:35:05.543322 139822745589568 spec.py:321] Evaluating on the training split.
I0130 13:35:11.738055 139822745589568 spec.py:333] Evaluating on the validation split.
I0130 13:35:20.205585 139822745589568 spec.py:349] Evaluating on the test split.
I0130 13:35:22.854059 139822745589568 submission_runner.py:408] Time since start: 64949.84s, 	Step: 184073, 	{'train/accuracy': 0.9612962007522583, 'train/loss': 0.14372682571411133, 'validation/accuracy': 0.7549600005149841, 'validation/loss': 1.0522350072860718, 'validation/num_examples': 50000, 'test/accuracy': 0.6287000179290771, 'test/loss': 1.829309105873108, 'test/num_examples': 10000, 'score': 62773.27559399605, 'total_duration': 64949.84421133995, 'accumulated_submission_time': 62773.27559399605, 'accumulated_eval_time': 2164.0824568271637, 'accumulated_logging_time': 6.202368974685669}
I0130 13:35:22.908823 139656809137920 logging_writer.py:48] [184073] accumulated_eval_time=2164.082457, accumulated_logging_time=6.202369, accumulated_submission_time=62773.275594, global_step=184073, preemption_count=0, score=62773.275594, test/accuracy=0.628700, test/loss=1.829309, test/num_examples=10000, total_duration=64949.844211, train/accuracy=0.961296, train/loss=0.143727, validation/accuracy=0.754960, validation/loss=1.052235, validation/num_examples=50000
I0130 13:35:32.443186 139656817530624 logging_writer.py:48] [184100] global_step=184100, grad_norm=4.351657390594482, loss=0.5869482159614563
I0130 13:36:06.470329 139656809137920 logging_writer.py:48] [184200] global_step=184200, grad_norm=4.22119665145874, loss=0.6215924024581909
I0130 13:36:40.508914 139656817530624 logging_writer.py:48] [184300] global_step=184300, grad_norm=4.243238925933838, loss=0.5865504145622253
I0130 13:37:14.592088 139656809137920 logging_writer.py:48] [184400] global_step=184400, grad_norm=4.505526065826416, loss=0.6685689091682434
I0130 13:37:48.685583 139656817530624 logging_writer.py:48] [184500] global_step=184500, grad_norm=4.882504940032959, loss=0.6798336505889893
I0130 13:38:22.792645 139656809137920 logging_writer.py:48] [184600] global_step=184600, grad_norm=4.335899829864502, loss=0.631866455078125
I0130 13:38:56.861943 139656817530624 logging_writer.py:48] [184700] global_step=184700, grad_norm=4.175480842590332, loss=0.6218714118003845
I0130 13:39:17.826923 139656809137920 logging_writer.py:48] [184763] global_step=184763, preemption_count=0, score=63008.128185
I0130 13:39:18.280605 139822745589568 checkpoints.py:490] Saving checkpoint at step: 184763
I0130 13:39:19.357238 139822745589568 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_1/imagenet_resnet_jax/trial_5/checkpoint_184763
I0130 13:39:19.382666 139822745589568 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_1/imagenet_resnet_jax/trial_5/checkpoint_184763.
I0130 13:39:20.184946 139822745589568 submission_runner.py:583] Tuning trial 5/5
I0130 13:39:20.185168 139822745589568 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.1, label_smoothing=0.0, learning_rate=0.0017486387539278373, one_minus_beta1=0.06733926164, beta2=0.9955159689799007, weight_decay=0.08121616522670176, warmup_factor=0.02)
I0130 13:39:20.194442 139822745589568 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0011160713620483875, 'train/loss': 6.910408973693848, 'validation/accuracy': 0.001019999966956675, 'validation/loss': 6.91091251373291, 'validation/num_examples': 50000, 'test/accuracy': 0.0012000000569969416, 'test/loss': 6.910790920257568, 'test/num_examples': 10000, 'score': 35.2137176990509, 'total_duration': 52.61761689186096, 'accumulated_submission_time': 35.2137176990509, 'accumulated_eval_time': 17.4037983417511, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1491, {'train/accuracy': 0.16388313472270966, 'train/loss': 4.366621017456055, 'validation/accuracy': 0.14970000088214874, 'validation/loss': 4.4936041831970215, 'validation/num_examples': 50000, 'test/accuracy': 0.11600000411272049, 'test/loss': 4.982067584991455, 'test/num_examples': 10000, 'score': 545.2359344959259, 'total_duration': 580.1692199707031, 'accumulated_submission_time': 545.2359344959259, 'accumulated_eval_time': 34.85956573486328, 'accumulated_logging_time': 0.019899845123291016, 'global_step': 1491, 'preemption_count': 0}), (2982, {'train/accuracy': 0.33591756224632263, 'train/loss': 3.0918776988983154, 'validation/accuracy': 0.3116599917411804, 'validation/loss': 3.2620902061462402, 'validation/num_examples': 50000, 'test/accuracy': 0.22990001738071442, 'test/loss': 3.9335262775421143, 'test/num_examples': 10000, 'score': 1055.3937442302704, 'total_duration': 1107.8423628807068, 'accumulated_submission_time': 1055.3937442302704, 'accumulated_eval_time': 52.29863715171814, 'accumulated_logging_time': 0.04640698432922363, 'global_step': 2982, 'preemption_count': 0}), (4474, {'train/accuracy': 0.5045041441917419, 'train/loss': 2.1400482654571533, 'validation/accuracy': 0.42861998081207275, 'validation/loss': 2.554784059524536, 'validation/num_examples': 50000, 'test/accuracy': 0.3278000056743622, 'test/loss': 3.2873244285583496, 'test/num_examples': 10000, 'score': 1565.4191055297852, 'total_duration': 1635.6378026008606, 'accumulated_submission_time': 1565.4191055297852, 'accumulated_eval_time': 69.99219536781311, 'accumulated_logging_time': 0.07351899147033691, 'global_step': 4474, 'preemption_count': 0}), (5968, {'train/accuracy': 0.5469945669174194, 'train/loss': 1.9179505109786987, 'validation/accuracy': 0.499019980430603, 'validation/loss': 2.203223943710327, 'validation/num_examples': 50000, 'test/accuracy': 0.3862000107765198, 'test/loss': 2.930450439453125, 'test/num_examples': 10000, 'score': 2075.5087237358093, 'total_duration': 2163.3739235401154, 'accumulated_submission_time': 2075.5087237358093, 'accumulated_eval_time': 87.56485724449158, 'accumulated_logging_time': 0.09717965126037598, 'global_step': 5968, 'preemption_count': 0}), (7462, {'train/accuracy': 0.5733617544174194, 'train/loss': 1.7808259725570679, 'validation/accuracy': 0.5251399874687195, 'validation/loss': 2.0447330474853516, 'validation/num_examples': 50000, 'test/accuracy': 0.40890002250671387, 'test/loss': 2.7833967208862305, 'test/num_examples': 10000, 'score': 2585.5674998760223, 'total_duration': 2691.1594779491425, 'accumulated_submission_time': 2585.5674998760223, 'accumulated_eval_time': 105.21260619163513, 'accumulated_logging_time': 0.1253807544708252, 'global_step': 7462, 'preemption_count': 0}), (8957, {'train/accuracy': 0.6165497303009033, 'train/loss': 1.5784637928009033, 'validation/accuracy': 0.5652799606323242, 'validation/loss': 1.8457132577896118, 'validation/num_examples': 50000, 'test/accuracy': 0.4390000104904175, 'test/loss': 2.5789635181427, 'test/num_examples': 10000, 'score': 3095.8154296875, 'total_duration': 3218.784925699234, 'accumulated_submission_time': 3095.8154296875, 'accumulated_eval_time': 122.51022338867188, 'accumulated_logging_time': 0.15499329566955566, 'global_step': 8957, 'preemption_count': 0}), (10451, {'train/accuracy': 0.6073620915412903, 'train/loss': 1.6080734729766846, 'validation/accuracy': 0.5593799948692322, 'validation/loss': 1.851853370666504, 'validation/num_examples': 50000, 'test/accuracy': 0.44040003418922424, 'test/loss': 2.6110658645629883, 'test/num_examples': 10000, 'score': 3605.894075870514, 'total_duration': 3746.3157720565796, 'accumulated_submission_time': 3605.894075870514, 'accumulated_eval_time': 139.8828718662262, 'accumulated_logging_time': 0.18477296829223633, 'global_step': 10451, 'preemption_count': 0}), (11945, {'train/accuracy': 0.6295639276504517, 'train/loss': 1.503599762916565, 'validation/accuracy': 0.5816599726676941, 'validation/loss': 1.7506680488586426, 'validation/num_examples': 50000, 'test/accuracy': 0.4595000147819519, 'test/loss': 2.491586923599243, 'test/num_examples': 10000, 'score': 4115.927650213242, 'total_duration': 4273.891491889954, 'accumulated_submission_time': 4115.927650213242, 'accumulated_eval_time': 157.3446958065033, 'accumulated_logging_time': 0.21372079849243164, 'global_step': 11945, 'preemption_count': 0}), (13440, {'train/accuracy': 0.6180644035339355, 'train/loss': 1.5705331563949585, 'validation/accuracy': 0.5727199912071228, 'validation/loss': 1.823119044303894, 'validation/num_examples': 50000, 'test/accuracy': 0.44770002365112305, 'test/loss': 2.5825207233428955, 'test/num_examples': 10000, 'score': 4626.153445720673, 'total_duration': 4801.611012220383, 'accumulated_submission_time': 4626.153445720673, 'accumulated_eval_time': 174.75857639312744, 'accumulated_logging_time': 0.24312424659729004, 'global_step': 13440, 'preemption_count': 0}), (14935, {'train/accuracy': 0.68363356590271, 'train/loss': 1.2555805444717407, 'validation/accuracy': 0.6050199866294861, 'validation/loss': 1.6677595376968384, 'validation/num_examples': 50000, 'test/accuracy': 0.4702000319957733, 'test/loss': 2.433048725128174, 'test/num_examples': 10000, 'score': 5136.090697288513, 'total_duration': 5329.14058303833, 'accumulated_submission_time': 5136.090697288513, 'accumulated_eval_time': 192.2714822292328, 'accumulated_logging_time': 0.2714576721191406, 'global_step': 14935, 'preemption_count': 0}), (16430, {'train/accuracy': 0.6679487824440002, 'train/loss': 1.3212976455688477, 'validation/accuracy': 0.6027599573135376, 'validation/loss': 1.6629142761230469, 'validation/num_examples': 50000, 'test/accuracy': 0.47850000858306885, 'test/loss': 2.401660203933716, 'test/num_examples': 10000, 'score': 5646.080711364746, 'total_duration': 5856.62567782402, 'accumulated_submission_time': 5646.080711364746, 'accumulated_eval_time': 209.68515920639038, 'accumulated_logging_time': 0.30024099349975586, 'global_step': 16430, 'preemption_count': 0}), (17926, {'train/accuracy': 0.6713767647743225, 'train/loss': 1.295975685119629, 'validation/accuracy': 0.6087599992752075, 'validation/loss': 1.628176212310791, 'validation/num_examples': 50000, 'test/accuracy': 0.4845000207424164, 'test/loss': 2.360335111618042, 'test/num_examples': 10000, 'score': 6156.199065208435, 'total_duration': 6384.354900598526, 'accumulated_submission_time': 6156.199065208435, 'accumulated_eval_time': 227.21400547027588, 'accumulated_logging_time': 0.3311781883239746, 'global_step': 17926, 'preemption_count': 0}), (19421, {'train/accuracy': 0.6640226244926453, 'train/loss': 1.3366535902023315, 'validation/accuracy': 0.6086399555206299, 'validation/loss': 1.63140869140625, 'validation/num_examples': 50000, 'test/accuracy': 0.47950002551078796, 'test/loss': 2.3939144611358643, 'test/num_examples': 10000, 'score': 6666.292007684708, 'total_duration': 6911.881479263306, 'accumulated_submission_time': 6666.292007684708, 'accumulated_eval_time': 244.56793308258057, 'accumulated_logging_time': 0.36154890060424805, 'global_step': 19421, 'preemption_count': 0}), (20917, {'train/accuracy': 0.6470623016357422, 'train/loss': 1.4147604703903198, 'validation/accuracy': 0.5916999578475952, 'validation/loss': 1.7275773286819458, 'validation/num_examples': 50000, 'test/accuracy': 0.4636000096797943, 'test/loss': 2.501309394836426, 'test/num_examples': 10000, 'score': 7176.418109893799, 'total_duration': 7440.27081990242, 'accumulated_submission_time': 7176.418109893799, 'accumulated_eval_time': 262.74822521209717, 'accumulated_logging_time': 0.39341068267822266, 'global_step': 20917, 'preemption_count': 0}), (22412, {'train/accuracy': 0.6626673936843872, 'train/loss': 1.3498613834381104, 'validation/accuracy': 0.6109600067138672, 'validation/loss': 1.6298362016677856, 'validation/num_examples': 50000, 'test/accuracy': 0.48340001702308655, 'test/loss': 2.372260332107544, 'test/num_examples': 10000, 'score': 7686.367953538895, 'total_duration': 7967.849324464798, 'accumulated_submission_time': 7686.367953538895, 'accumulated_eval_time': 280.29457664489746, 'accumulated_logging_time': 0.42447733879089355, 'global_step': 22412, 'preemption_count': 0}), (23907, {'train/accuracy': 0.7183513641357422, 'train/loss': 1.0917080640792847, 'validation/accuracy': 0.6167199611663818, 'validation/loss': 1.610385775566101, 'validation/num_examples': 50000, 'test/accuracy': 0.4861000180244446, 'test/loss': 2.371723175048828, 'test/num_examples': 10000, 'score': 8196.371249198914, 'total_duration': 8495.189532518387, 'accumulated_submission_time': 8196.371249198914, 'accumulated_eval_time': 297.5520570278168, 'accumulated_logging_time': 0.4546489715576172, 'global_step': 23907, 'preemption_count': 0}), (25403, {'train/accuracy': 0.6804647445678711, 'train/loss': 1.2763067483901978, 'validation/accuracy': 0.6107999682426453, 'validation/loss': 1.6338390111923218, 'validation/num_examples': 50000, 'test/accuracy': 0.47310003638267517, 'test/loss': 2.415712833404541, 'test/num_examples': 10000, 'score': 8706.593649148941, 'total_duration': 9023.160031318665, 'accumulated_submission_time': 8706.593649148941, 'accumulated_eval_time': 315.21845388412476, 'accumulated_logging_time': 0.4857900142669678, 'global_step': 25403, 'preemption_count': 0}), (26899, {'train/accuracy': 0.6908880472183228, 'train/loss': 1.2166250944137573, 'validation/accuracy': 0.6270999908447266, 'validation/loss': 1.541603446006775, 'validation/num_examples': 50000, 'test/accuracy': 0.4951000213623047, 'test/loss': 2.3069241046905518, 'test/num_examples': 10000, 'score': 9216.743519544601, 'total_duration': 9550.937099695206, 'accumulated_submission_time': 9216.743519544601, 'accumulated_eval_time': 332.76383209228516, 'accumulated_logging_time': 0.5164616107940674, 'global_step': 26899, 'preemption_count': 0}), (28395, {'train/accuracy': 0.6801259517669678, 'train/loss': 1.2542836666107178, 'validation/accuracy': 0.6233199834823608, 'validation/loss': 1.5583561658859253, 'validation/num_examples': 50000, 'test/accuracy': 0.4909000098705292, 'test/loss': 2.328392505645752, 'test/num_examples': 10000, 'score': 9726.73595571518, 'total_duration': 10078.36783695221, 'accumulated_submission_time': 9726.73595571518, 'accumulated_eval_time': 350.1199746131897, 'accumulated_logging_time': 0.5478644371032715, 'global_step': 28395, 'preemption_count': 0}), (29891, {'train/accuracy': 0.6824178695678711, 'train/loss': 1.2555991411209106, 'validation/accuracy': 0.625499963760376, 'validation/loss': 1.545114517211914, 'validation/num_examples': 50000, 'test/accuracy': 0.49800002574920654, 'test/loss': 2.307891368865967, 'test/num_examples': 10000, 'score': 10236.778829574585, 'total_duration': 10605.955340862274, 'accumulated_submission_time': 10236.778829574585, 'accumulated_eval_time': 367.57837295532227, 'accumulated_logging_time': 0.5819680690765381, 'global_step': 29891, 'preemption_count': 0}), (31387, {'train/accuracy': 0.6882373690605164, 'train/loss': 1.2267718315124512, 'validation/accuracy': 0.6324999928474426, 'validation/loss': 1.5249176025390625, 'validation/num_examples': 50000, 'test/accuracy': 0.5041000247001648, 'test/loss': 2.2895352840423584, 'test/num_examples': 10000, 'score': 10747.021076440811, 'total_duration': 11133.741675138474, 'accumulated_submission_time': 10747.021076440811, 'accumulated_eval_time': 385.03712701797485, 'accumulated_logging_time': 0.6160974502563477, 'global_step': 31387, 'preemption_count': 0}), (32883, {'train/accuracy': 0.6759008169174194, 'train/loss': 1.282272458076477, 'validation/accuracy': 0.6207599639892578, 'validation/loss': 1.5945985317230225, 'validation/num_examples': 50000, 'test/accuracy': 0.48730000853538513, 'test/loss': 2.34653377532959, 'test/num_examples': 10000, 'score': 11257.115474939346, 'total_duration': 11661.608564853668, 'accumulated_submission_time': 11257.115474939346, 'accumulated_eval_time': 402.7224214076996, 'accumulated_logging_time': 0.6510787010192871, 'global_step': 32883, 'preemption_count': 0}), (34379, {'train/accuracy': 0.7140066623687744, 'train/loss': 1.1095143556594849, 'validation/accuracy': 0.6321399807929993, 'validation/loss': 1.5328267812728882, 'validation/num_examples': 50000, 'test/accuracy': 0.4999000132083893, 'test/loss': 2.2806482315063477, 'test/num_examples': 10000, 'score': 11767.26364517212, 'total_duration': 12189.351138353348, 'accumulated_submission_time': 11767.26364517212, 'accumulated_eval_time': 420.22812843322754, 'accumulated_logging_time': 0.6878213882446289, 'global_step': 34379, 'preemption_count': 0}), (35875, {'train/accuracy': 0.6957509517669678, 'train/loss': 1.1831389665603638, 'validation/accuracy': 0.6268399953842163, 'validation/loss': 1.5437438488006592, 'validation/num_examples': 50000, 'test/accuracy': 0.4993000328540802, 'test/loss': 2.2765913009643555, 'test/num_examples': 10000, 'score': 12277.20189833641, 'total_duration': 12716.973129034042, 'accumulated_submission_time': 12277.20189833641, 'accumulated_eval_time': 437.83101439476013, 'accumulated_logging_time': 0.7197163105010986, 'global_step': 35875, 'preemption_count': 0}), (37371, {'train/accuracy': 0.7004544138908386, 'train/loss': 1.1742907762527466, 'validation/accuracy': 0.6349799633026123, 'validation/loss': 1.5201865434646606, 'validation/num_examples': 50000, 'test/accuracy': 0.5092000365257263, 'test/loss': 2.2315361499786377, 'test/num_examples': 10000, 'score': 12787.222305297852, 'total_duration': 13244.35971903801, 'accumulated_submission_time': 12787.222305297852, 'accumulated_eval_time': 455.1110055446625, 'accumulated_logging_time': 0.7531900405883789, 'global_step': 37371, 'preemption_count': 0}), (38868, {'train/accuracy': 0.6949737071990967, 'train/loss': 1.1847723722457886, 'validation/accuracy': 0.6327599883079529, 'validation/loss': 1.518254041671753, 'validation/num_examples': 50000, 'test/accuracy': 0.5082000494003296, 'test/loss': 2.2616703510284424, 'test/num_examples': 10000, 'score': 13297.305636644363, 'total_duration': 13771.90760755539, 'accumulated_submission_time': 13297.305636644363, 'accumulated_eval_time': 472.48762464523315, 'accumulated_logging_time': 0.7889235019683838, 'global_step': 38868, 'preemption_count': 0}), (40364, {'train/accuracy': 0.6914859414100647, 'train/loss': 1.2236030101776123, 'validation/accuracy': 0.6318399906158447, 'validation/loss': 1.5228060483932495, 'validation/num_examples': 50000, 'test/accuracy': 0.49720001220703125, 'test/loss': 2.3025379180908203, 'test/num_examples': 10000, 'score': 13807.231940984726, 'total_duration': 14299.705271959305, 'accumulated_submission_time': 13807.231940984726, 'accumulated_eval_time': 490.2734615802765, 'accumulated_logging_time': 0.8251686096191406, 'global_step': 40364, 'preemption_count': 0}), (41861, {'train/accuracy': 0.6909677982330322, 'train/loss': 1.2019314765930176, 'validation/accuracy': 0.6317600011825562, 'validation/loss': 1.5205481052398682, 'validation/num_examples': 50000, 'test/accuracy': 0.5012000203132629, 'test/loss': 2.2280113697052, 'test/num_examples': 10000, 'score': 14317.350157022476, 'total_duration': 14827.315202951431, 'accumulated_submission_time': 14317.350157022476, 'accumulated_eval_time': 507.6738419532776, 'accumulated_logging_time': 0.8653068542480469, 'global_step': 41861, 'preemption_count': 0}), (43358, {'train/accuracy': 0.726980984210968, 'train/loss': 1.0523135662078857, 'validation/accuracy': 0.6317399740219116, 'validation/loss': 1.5127232074737549, 'validation/num_examples': 50000, 'test/accuracy': 0.5042999982833862, 'test/loss': 2.259047508239746, 'test/num_examples': 10000, 'score': 14827.423000097275, 'total_duration': 15354.948499202728, 'accumulated_submission_time': 14827.423000097275, 'accumulated_eval_time': 525.1450872421265, 'accumulated_logging_time': 0.903350830078125, 'global_step': 43358, 'preemption_count': 0}), (44855, {'train/accuracy': 0.7177534699440002, 'train/loss': 1.0975770950317383, 'validation/accuracy': 0.6431800127029419, 'validation/loss': 1.4693011045455933, 'validation/num_examples': 50000, 'test/accuracy': 0.5175000429153442, 'test/loss': 2.1700892448425293, 'test/num_examples': 10000, 'score': 15337.656279802322, 'total_duration': 15882.606467962265, 'accumulated_submission_time': 15337.656279802322, 'accumulated_eval_time': 542.4807982444763, 'accumulated_logging_time': 0.9419848918914795, 'global_step': 44855, 'preemption_count': 0}), (46351, {'train/accuracy': 0.7119937539100647, 'train/loss': 1.1093826293945312, 'validation/accuracy': 0.6420199871063232, 'validation/loss': 1.4766788482666016, 'validation/num_examples': 50000, 'test/accuracy': 0.5148000121116638, 'test/loss': 2.2351808547973633, 'test/num_examples': 10000, 'score': 15847.795320510864, 'total_duration': 16409.9840195179, 'accumulated_submission_time': 15847.795320510864, 'accumulated_eval_time': 559.6336979866028, 'accumulated_logging_time': 0.9760580062866211, 'global_step': 46351, 'preemption_count': 0}), (47848, {'train/accuracy': 0.7031847834587097, 'train/loss': 1.1564139127731323, 'validation/accuracy': 0.6360999941825867, 'validation/loss': 1.489286184310913, 'validation/num_examples': 50000, 'test/accuracy': 0.5045000314712524, 'test/loss': 2.241513967514038, 'test/num_examples': 10000, 'score': 16358.012321472168, 'total_duration': 16937.62736606598, 'accumulated_submission_time': 16358.012321472168, 'accumulated_eval_time': 576.9686605930328, 'accumulated_logging_time': 1.016944408416748, 'global_step': 47848, 'preemption_count': 0}), (49344, {'train/accuracy': 0.7031847834587097, 'train/loss': 1.1441307067871094, 'validation/accuracy': 0.6427199840545654, 'validation/loss': 1.4674408435821533, 'validation/num_examples': 50000, 'test/accuracy': 0.5211000442504883, 'test/loss': 2.169034481048584, 'test/num_examples': 10000, 'score': 16868.260396003723, 'total_duration': 17465.60064959526, 'accumulated_submission_time': 16868.260396003723, 'accumulated_eval_time': 594.6073455810547, 'accumulated_logging_time': 1.0527050495147705, 'global_step': 49344, 'preemption_count': 0}), (50841, {'train/accuracy': 0.7056361436843872, 'train/loss': 1.1398847103118896, 'validation/accuracy': 0.645039975643158, 'validation/loss': 1.4592891931533813, 'validation/num_examples': 50000, 'test/accuracy': 0.5149000287055969, 'test/loss': 2.193108081817627, 'test/num_examples': 10000, 'score': 17378.219309806824, 'total_duration': 17993.183161497116, 'accumulated_submission_time': 17378.219309806824, 'accumulated_eval_time': 612.1412818431854, 'accumulated_logging_time': 1.091554880142212, 'global_step': 50841, 'preemption_count': 0}), (52338, {'train/accuracy': 0.7645886540412903, 'train/loss': 0.9032778739929199, 'validation/accuracy': 0.6543799638748169, 'validation/loss': 1.4254825115203857, 'validation/num_examples': 50000, 'test/accuracy': 0.520300030708313, 'test/loss': 2.1776282787323, 'test/num_examples': 10000, 'score': 17888.419243574142, 'total_duration': 18520.939923524857, 'accumulated_submission_time': 17888.419243574142, 'accumulated_eval_time': 629.606897354126, 'accumulated_logging_time': 1.1325068473815918, 'global_step': 52338, 'preemption_count': 0}), (53835, {'train/accuracy': 0.7227758169174194, 'train/loss': 1.0640006065368652, 'validation/accuracy': 0.6343599557876587, 'validation/loss': 1.4988964796066284, 'validation/num_examples': 50000, 'test/accuracy': 0.5029000043869019, 'test/loss': 2.2745490074157715, 'test/num_examples': 10000, 'score': 18398.483984470367, 'total_duration': 19048.349376678467, 'accumulated_submission_time': 18398.483984470367, 'accumulated_eval_time': 646.8635385036469, 'accumulated_logging_time': 1.1695225238800049, 'global_step': 53835, 'preemption_count': 0}), (55332, {'train/accuracy': 0.7080675959587097, 'train/loss': 1.1316912174224854, 'validation/accuracy': 0.6315799951553345, 'validation/loss': 1.5241401195526123, 'validation/num_examples': 50000, 'test/accuracy': 0.5034000277519226, 'test/loss': 2.26247239112854, 'test/num_examples': 10000, 'score': 18908.700251817703, 'total_duration': 19576.242521762848, 'accumulated_submission_time': 18908.700251817703, 'accumulated_eval_time': 664.4518418312073, 'accumulated_logging_time': 1.2079482078552246, 'global_step': 55332, 'preemption_count': 0}), (56829, {'train/accuracy': 0.7178930044174194, 'train/loss': 1.0814168453216553, 'validation/accuracy': 0.6469199657440186, 'validation/loss': 1.4597644805908203, 'validation/num_examples': 50000, 'test/accuracy': 0.5152000188827515, 'test/loss': 2.216600179672241, 'test/num_examples': 10000, 'score': 19418.939255475998, 'total_duration': 20104.18901371956, 'accumulated_submission_time': 19418.939255475998, 'accumulated_eval_time': 682.0746810436249, 'accumulated_logging_time': 1.2414581775665283, 'global_step': 56829, 'preemption_count': 0}), (58326, {'train/accuracy': 0.7149434089660645, 'train/loss': 1.1157106161117554, 'validation/accuracy': 0.6482999920845032, 'validation/loss': 1.441026210784912, 'validation/num_examples': 50000, 'test/accuracy': 0.5131000280380249, 'test/loss': 2.1649599075317383, 'test/num_examples': 10000, 'score': 19929.052931785583, 'total_duration': 20632.583993673325, 'accumulated_submission_time': 19929.052931785583, 'accumulated_eval_time': 700.2618687152863, 'accumulated_logging_time': 1.2832703590393066, 'global_step': 58326, 'preemption_count': 0}), (59823, {'train/accuracy': 0.7216398119926453, 'train/loss': 1.0750752687454224, 'validation/accuracy': 0.6587199568748474, 'validation/loss': 1.4014229774475098, 'validation/num_examples': 50000, 'test/accuracy': 0.5279000401496887, 'test/loss': 2.151419162750244, 'test/num_examples': 10000, 'score': 20439.141530275345, 'total_duration': 21160.234573602676, 'accumulated_submission_time': 20439.141530275345, 'accumulated_eval_time': 717.730926990509, 'accumulated_logging_time': 1.3240761756896973, 'global_step': 59823, 'preemption_count': 0}), (61320, {'train/accuracy': 0.7239716053009033, 'train/loss': 1.0552133321762085, 'validation/accuracy': 0.6634399890899658, 'validation/loss': 1.3708513975143433, 'validation/num_examples': 50000, 'test/accuracy': 0.5362000465393066, 'test/loss': 2.0705716609954834, 'test/num_examples': 10000, 'score': 20949.247843265533, 'total_duration': 21687.68998861313, 'accumulated_submission_time': 20949.247843265533, 'accumulated_eval_time': 734.9866235256195, 'accumulated_logging_time': 1.3634462356567383, 'global_step': 61320, 'preemption_count': 0}), (62817, {'train/accuracy': 0.7576530575752258, 'train/loss': 0.9276350736618042, 'validation/accuracy': 0.6600199937820435, 'validation/loss': 1.3925979137420654, 'validation/num_examples': 50000, 'test/accuracy': 0.5330000519752502, 'test/loss': 2.138416290283203, 'test/num_examples': 10000, 'score': 21459.278631210327, 'total_duration': 22215.154264211655, 'accumulated_submission_time': 21459.278631210327, 'accumulated_eval_time': 752.3298766613007, 'accumulated_logging_time': 1.4020438194274902, 'global_step': 62817, 'preemption_count': 0}), (64314, {'train/accuracy': 0.7278180718421936, 'train/loss': 1.0450934171676636, 'validation/accuracy': 0.6518799662590027, 'validation/loss': 1.4381275177001953, 'validation/num_examples': 50000, 'test/accuracy': 0.5248000025749207, 'test/loss': 2.1725211143493652, 'test/num_examples': 10000, 'score': 21969.453302383423, 'total_duration': 22742.71888899803, 'accumulated_submission_time': 21969.453302383423, 'accumulated_eval_time': 769.6031460762024, 'accumulated_logging_time': 1.4653689861297607, 'global_step': 64314, 'preemption_count': 0}), (65811, {'train/accuracy': 0.7356903553009033, 'train/loss': 1.0099648237228394, 'validation/accuracy': 0.6637600064277649, 'validation/loss': 1.3708646297454834, 'validation/num_examples': 50000, 'test/accuracy': 0.5311000347137451, 'test/loss': 2.125349998474121, 'test/num_examples': 10000, 'score': 22479.49555540085, 'total_duration': 23270.3928463459, 'accumulated_submission_time': 22479.49555540085, 'accumulated_eval_time': 787.1383633613586, 'accumulated_logging_time': 1.5099318027496338, 'global_step': 65811, 'preemption_count': 0}), (67308, {'train/accuracy': 0.7371651530265808, 'train/loss': 0.9949511885643005, 'validation/accuracy': 0.664359986782074, 'validation/loss': 1.357032299041748, 'validation/num_examples': 50000, 'test/accuracy': 0.5324000120162964, 'test/loss': 2.098825216293335, 'test/num_examples': 10000, 'score': 22989.733140707016, 'total_duration': 23798.065600156784, 'accumulated_submission_time': 22989.733140707016, 'accumulated_eval_time': 804.4793326854706, 'accumulated_logging_time': 1.5526759624481201, 'global_step': 67308, 'preemption_count': 0}), (68806, {'train/accuracy': 0.7223772406578064, 'train/loss': 1.0757921934127808, 'validation/accuracy': 0.6567599773406982, 'validation/loss': 1.3976572751998901, 'validation/num_examples': 50000, 'test/accuracy': 0.5281000137329102, 'test/loss': 2.129694700241089, 'test/num_examples': 10000, 'score': 23499.958671808243, 'total_duration': 24325.86668920517, 'accumulated_submission_time': 23499.958671808243, 'accumulated_eval_time': 821.963175535202, 'accumulated_logging_time': 1.5933401584625244, 'global_step': 68806, 'preemption_count': 0}), (70302, {'train/accuracy': 0.7234334945678711, 'train/loss': 1.0573880672454834, 'validation/accuracy': 0.656719982624054, 'validation/loss': 1.40852689743042, 'validation/num_examples': 50000, 'test/accuracy': 0.5242000222206116, 'test/loss': 2.144479513168335, 'test/num_examples': 10000, 'score': 24009.941289901733, 'total_duration': 24853.39986562729, 'accumulated_submission_time': 24009.941289901733, 'accumulated_eval_time': 839.4227304458618, 'accumulated_logging_time': 1.6327154636383057, 'global_step': 70302, 'preemption_count': 0}), (71800, {'train/accuracy': 0.7745535373687744, 'train/loss': 0.8367967009544373, 'validation/accuracy': 0.6677199602127075, 'validation/loss': 1.3619836568832397, 'validation/num_examples': 50000, 'test/accuracy': 0.5440000295639038, 'test/loss': 2.086559772491455, 'test/num_examples': 10000, 'score': 24520.091975688934, 'total_duration': 25380.915219783783, 'accumulated_submission_time': 24520.091975688934, 'accumulated_eval_time': 856.6956856250763, 'accumulated_logging_time': 1.6729493141174316, 'global_step': 71800, 'preemption_count': 0}), (73297, {'train/accuracy': 0.7525908350944519, 'train/loss': 0.937186062335968, 'validation/accuracy': 0.6672799587249756, 'validation/loss': 1.3505654335021973, 'validation/num_examples': 50000, 'test/accuracy': 0.5418000221252441, 'test/loss': 2.0513622760772705, 'test/num_examples': 10000, 'score': 25030.211496591568, 'total_duration': 25908.675062417984, 'accumulated_submission_time': 25030.211496591568, 'accumulated_eval_time': 874.2368021011353, 'accumulated_logging_time': 1.720144510269165, 'global_step': 73297, 'preemption_count': 0}), (74793, {'train/accuracy': 0.7455755472183228, 'train/loss': 0.9578787088394165, 'validation/accuracy': 0.6665999889373779, 'validation/loss': 1.366343379020691, 'validation/num_examples': 50000, 'test/accuracy': 0.5467000007629395, 'test/loss': 2.070542812347412, 'test/num_examples': 10000, 'score': 25540.138279676437, 'total_duration': 26436.25676727295, 'accumulated_submission_time': 25540.138279676437, 'accumulated_eval_time': 891.7931699752808, 'accumulated_logging_time': 1.7655150890350342, 'global_step': 74793, 'preemption_count': 0}), (76290, {'train/accuracy': 0.73441481590271, 'train/loss': 1.0014700889587402, 'validation/accuracy': 0.6627599596977234, 'validation/loss': 1.3964588642120361, 'validation/num_examples': 50000, 'test/accuracy': 0.5372000336647034, 'test/loss': 2.145915985107422, 'test/num_examples': 10000, 'score': 26050.24849486351, 'total_duration': 26963.783942461014, 'accumulated_submission_time': 26050.24849486351, 'accumulated_eval_time': 909.11328291893, 'accumulated_logging_time': 1.809826374053955, 'global_step': 76290, 'preemption_count': 0}), (77787, {'train/accuracy': 0.7419283986091614, 'train/loss': 0.9948947429656982, 'validation/accuracy': 0.668179988861084, 'validation/loss': 1.3509660959243774, 'validation/num_examples': 50000, 'test/accuracy': 0.5320000052452087, 'test/loss': 2.0675227642059326, 'test/num_examples': 10000, 'score': 26560.447185516357, 'total_duration': 27491.354991436005, 'accumulated_submission_time': 26560.447185516357, 'accumulated_eval_time': 926.3913035392761, 'accumulated_logging_time': 1.8542098999023438, 'global_step': 77787, 'preemption_count': 0}), (79284, {'train/accuracy': 0.7411909699440002, 'train/loss': 0.993799090385437, 'validation/accuracy': 0.6715399622917175, 'validation/loss': 1.3349510431289673, 'validation/num_examples': 50000, 'test/accuracy': 0.5475000143051147, 'test/loss': 2.050487756729126, 'test/num_examples': 10000, 'score': 27070.60922384262, 'total_duration': 28019.048672914505, 'accumulated_submission_time': 27070.60922384262, 'accumulated_eval_time': 943.8302927017212, 'accumulated_logging_time': 1.894223690032959, 'global_step': 79284, 'preemption_count': 0}), (80781, {'train/accuracy': 0.7743940949440002, 'train/loss': 0.8552932739257812, 'validation/accuracy': 0.6669399738311768, 'validation/loss': 1.3668314218521118, 'validation/num_examples': 50000, 'test/accuracy': 0.5409000515937805, 'test/loss': 2.0658974647521973, 'test/num_examples': 10000, 'score': 27580.572791814804, 'total_duration': 28546.45977640152, 'accumulated_submission_time': 27580.572791814804, 'accumulated_eval_time': 961.185555934906, 'accumulated_logging_time': 1.9354100227355957, 'global_step': 80781, 'preemption_count': 0}), (82278, {'train/accuracy': 0.7655652165412903, 'train/loss': 0.8683584332466125, 'validation/accuracy': 0.6740999817848206, 'validation/loss': 1.3325233459472656, 'validation/num_examples': 50000, 'test/accuracy': 0.5436000227928162, 'test/loss': 2.066192865371704, 'test/num_examples': 10000, 'score': 28090.680698633194, 'total_duration': 29074.016382932663, 'accumulated_submission_time': 28090.680698633194, 'accumulated_eval_time': 978.5394492149353, 'accumulated_logging_time': 1.9794235229492188, 'global_step': 82278, 'preemption_count': 0}), (83775, {'train/accuracy': 0.7556401491165161, 'train/loss': 0.9285488128662109, 'validation/accuracy': 0.6721199750900269, 'validation/loss': 1.338887095451355, 'validation/num_examples': 50000, 'test/accuracy': 0.5362000465393066, 'test/loss': 2.076328754425049, 'test/num_examples': 10000, 'score': 28600.69172692299, 'total_duration': 29601.736602783203, 'accumulated_submission_time': 28600.69172692299, 'accumulated_eval_time': 996.1524906158447, 'accumulated_logging_time': 2.0239923000335693, 'global_step': 83775, 'preemption_count': 0}), (85272, {'train/accuracy': 0.7602040767669678, 'train/loss': 0.8968546390533447, 'validation/accuracy': 0.6789399981498718, 'validation/loss': 1.307782769203186, 'validation/num_examples': 50000, 'test/accuracy': 0.5489000082015991, 'test/loss': 2.041975259780884, 'test/num_examples': 10000, 'score': 29110.69561982155, 'total_duration': 30129.31273341179, 'accumulated_submission_time': 29110.69561982155, 'accumulated_eval_time': 1013.6288385391235, 'accumulated_logging_time': 2.0686020851135254, 'global_step': 85272, 'preemption_count': 0}), (86770, {'train/accuracy': 0.7505978941917419, 'train/loss': 0.9393353462219238, 'validation/accuracy': 0.6771399974822998, 'validation/loss': 1.3065553903579712, 'validation/num_examples': 50000, 'test/accuracy': 0.5494000315666199, 'test/loss': 2.0108368396759033, 'test/num_examples': 10000, 'score': 29620.89577460289, 'total_duration': 30657.11715722084, 'accumulated_submission_time': 29620.89577460289, 'accumulated_eval_time': 1031.1397771835327, 'accumulated_logging_time': 2.111499547958374, 'global_step': 86770, 'preemption_count': 0}), (88267, {'train/accuracy': 0.7596260905265808, 'train/loss': 0.9097063541412354, 'validation/accuracy': 0.6830799579620361, 'validation/loss': 1.2865016460418701, 'validation/num_examples': 50000, 'test/accuracy': 0.5533000230789185, 'test/loss': 2.0117292404174805, 'test/num_examples': 10000, 'score': 30130.933844089508, 'total_duration': 31184.721259593964, 'accumulated_submission_time': 30130.933844089508, 'accumulated_eval_time': 1048.6075825691223, 'accumulated_logging_time': 2.1597437858581543, 'global_step': 88267, 'preemption_count': 0}), (89764, {'train/accuracy': 0.7589086294174194, 'train/loss': 0.9112529158592224, 'validation/accuracy': 0.682379961013794, 'validation/loss': 1.2872778177261353, 'validation/num_examples': 50000, 'test/accuracy': 0.5552000403404236, 'test/loss': 2.016308069229126, 'test/num_examples': 10000, 'score': 30641.0526971817, 'total_duration': 31712.21232533455, 'accumulated_submission_time': 30641.0526971817, 'accumulated_eval_time': 1065.8815150260925, 'accumulated_logging_time': 2.206571578979492, 'global_step': 89764, 'preemption_count': 0}), (91261, {'train/accuracy': 0.7894411683082581, 'train/loss': 0.784350574016571, 'validation/accuracy': 0.6829000115394592, 'validation/loss': 1.2923929691314697, 'validation/num_examples': 50000, 'test/accuracy': 0.5560000538825989, 'test/loss': 2.0203568935394287, 'test/num_examples': 10000, 'score': 31151.199570178986, 'total_duration': 32239.76553440094, 'accumulated_submission_time': 31151.199570178986, 'accumulated_eval_time': 1083.1903893947601, 'accumulated_logging_time': 2.25357723236084, 'global_step': 91261, 'preemption_count': 0}), (92757, {'train/accuracy': 0.7609016299247742, 'train/loss': 0.8901998996734619, 'validation/accuracy': 0.6728399991989136, 'validation/loss': 1.3282952308654785, 'validation/num_examples': 50000, 'test/accuracy': 0.5391000509262085, 'test/loss': 2.09073805809021, 'test/num_examples': 10000, 'score': 31661.18835258484, 'total_duration': 32767.13542819023, 'accumulated_submission_time': 31661.18835258484, 'accumulated_eval_time': 1100.471552848816, 'accumulated_logging_time': 2.3035874366760254, 'global_step': 92757, 'preemption_count': 0}), (94254, {'train/accuracy': 0.7751315236091614, 'train/loss': 0.8435813784599304, 'validation/accuracy': 0.6869999766349792, 'validation/loss': 1.2639058828353882, 'validation/num_examples': 50000, 'test/accuracy': 0.5612000226974487, 'test/loss': 1.9653502702713013, 'test/num_examples': 10000, 'score': 32171.235827207565, 'total_duration': 33294.53087544441, 'accumulated_submission_time': 32171.235827207565, 'accumulated_eval_time': 1117.723935842514, 'accumulated_logging_time': 2.349766731262207, 'global_step': 94254, 'preemption_count': 0}), (95751, {'train/accuracy': 0.7679567933082581, 'train/loss': 0.8627640008926392, 'validation/accuracy': 0.681659996509552, 'validation/loss': 1.2954697608947754, 'validation/num_examples': 50000, 'test/accuracy': 0.555400013923645, 'test/loss': 2.016524314880371, 'test/num_examples': 10000, 'score': 32681.184484004974, 'total_duration': 33822.053425073624, 'accumulated_submission_time': 32681.184484004974, 'accumulated_eval_time': 1135.1968188285828, 'accumulated_logging_time': 2.4002671241760254, 'global_step': 95751, 'preemption_count': 0}), (97248, {'train/accuracy': 0.7716238498687744, 'train/loss': 0.8549726605415344, 'validation/accuracy': 0.691540002822876, 'validation/loss': 1.2567367553710938, 'validation/num_examples': 50000, 'test/accuracy': 0.5631999969482422, 'test/loss': 1.985821008682251, 'test/num_examples': 10000, 'score': 33191.21137213707, 'total_duration': 34350.32893657684, 'accumulated_submission_time': 33191.21137213707, 'accumulated_eval_time': 1153.3456366062164, 'accumulated_logging_time': 2.44881272315979, 'global_step': 97248, 'preemption_count': 0}), (98745, {'train/accuracy': 0.7628945708274841, 'train/loss': 0.8821855187416077, 'validation/accuracy': 0.6861000061035156, 'validation/loss': 1.2782268524169922, 'validation/num_examples': 50000, 'test/accuracy': 0.5597000122070312, 'test/loss': 1.9715975522994995, 'test/num_examples': 10000, 'score': 33701.27909350395, 'total_duration': 34878.50706458092, 'accumulated_submission_time': 33701.27909350395, 'accumulated_eval_time': 1171.3598392009735, 'accumulated_logging_time': 2.493917226791382, 'global_step': 98745, 'preemption_count': 0}), (100241, {'train/accuracy': 0.8028938174247742, 'train/loss': 0.7278432250022888, 'validation/accuracy': 0.6812599897384644, 'validation/loss': 1.3054099082946777, 'validation/num_examples': 50000, 'test/accuracy': 0.5509000420570374, 'test/loss': 2.0477240085601807, 'test/num_examples': 10000, 'score': 34210.90730881691, 'total_duration': 35406.05687189102, 'accumulated_submission_time': 34210.90730881691, 'accumulated_eval_time': 1188.65078830719, 'accumulated_logging_time': 3.0731360912323, 'global_step': 100241, 'preemption_count': 0}), (101739, {'train/accuracy': 0.788504421710968, 'train/loss': 0.7852997183799744, 'validation/accuracy': 0.6868799924850464, 'validation/loss': 1.2690773010253906, 'validation/num_examples': 50000, 'test/accuracy': 0.5597000122070312, 'test/loss': 1.9839948415756226, 'test/num_examples': 10000, 'score': 34721.09689593315, 'total_duration': 35933.86906027794, 'accumulated_submission_time': 34721.09689593315, 'accumulated_eval_time': 1206.1760022640228, 'accumulated_logging_time': 3.119248867034912, 'global_step': 101739, 'preemption_count': 0}), (103236, {'train/accuracy': 0.7882851958274841, 'train/loss': 0.7907478213310242, 'validation/accuracy': 0.6976199746131897, 'validation/loss': 1.247995376586914, 'validation/num_examples': 50000, 'test/accuracy': 0.5722000002861023, 'test/loss': 1.93617844581604, 'test/num_examples': 10000, 'score': 35231.05541777611, 'total_duration': 36461.30139732361, 'accumulated_submission_time': 35231.05541777611, 'accumulated_eval_time': 1223.5552270412445, 'accumulated_logging_time': 3.1634182929992676, 'global_step': 103236, 'preemption_count': 0}), (104733, {'train/accuracy': 0.7801936864852905, 'train/loss': 0.8114662766456604, 'validation/accuracy': 0.6913599967956543, 'validation/loss': 1.254683017730713, 'validation/num_examples': 50000, 'test/accuracy': 0.5595000386238098, 'test/loss': 1.9859092235565186, 'test/num_examples': 10000, 'score': 35741.056241989136, 'total_duration': 36988.61328577995, 'accumulated_submission_time': 35741.056241989136, 'accumulated_eval_time': 1240.7657074928284, 'accumulated_logging_time': 3.2133994102478027, 'global_step': 104733, 'preemption_count': 0}), (106230, {'train/accuracy': 0.7833226919174194, 'train/loss': 0.8032361268997192, 'validation/accuracy': 0.6902399659156799, 'validation/loss': 1.247302532196045, 'validation/num_examples': 50000, 'test/accuracy': 0.5678000450134277, 'test/loss': 1.9703844785690308, 'test/num_examples': 10000, 'score': 36250.98030781746, 'total_duration': 37516.186690330505, 'accumulated_submission_time': 36250.98030781746, 'accumulated_eval_time': 1258.3189299106598, 'accumulated_logging_time': 3.259010076522827, 'global_step': 106230, 'preemption_count': 0}), (107727, {'train/accuracy': 0.7869698405265808, 'train/loss': 0.769432544708252, 'validation/accuracy': 0.6983199715614319, 'validation/loss': 1.2229222059249878, 'validation/num_examples': 50000, 'test/accuracy': 0.5678000450134277, 'test/loss': 1.9578731060028076, 'test/num_examples': 10000, 'score': 36760.9456949234, 'total_duration': 38043.81551671028, 'accumulated_submission_time': 36760.9456949234, 'accumulated_eval_time': 1275.867201089859, 'accumulated_logging_time': 3.323164701461792, 'global_step': 107727, 'preemption_count': 0}), (109225, {'train/accuracy': 0.8082947731018066, 'train/loss': 0.6996628046035767, 'validation/accuracy': 0.6953799724578857, 'validation/loss': 1.2410041093826294, 'validation/num_examples': 50000, 'test/accuracy': 0.5749000310897827, 'test/loss': 1.9538441896438599, 'test/num_examples': 10000, 'score': 37271.04302740097, 'total_duration': 38571.32220196724, 'accumulated_submission_time': 37271.04302740097, 'accumulated_eval_time': 1293.1777880191803, 'accumulated_logging_time': 3.372274875640869, 'global_step': 109225, 'preemption_count': 0}), (110722, {'train/accuracy': 0.813875138759613, 'train/loss': 0.6721604466438293, 'validation/accuracy': 0.7019599676132202, 'validation/loss': 1.222309947013855, 'validation/num_examples': 50000, 'test/accuracy': 0.5769000053405762, 'test/loss': 1.93671452999115, 'test/num_examples': 10000, 'score': 37781.17570281029, 'total_duration': 39099.16570162773, 'accumulated_submission_time': 37781.17570281029, 'accumulated_eval_time': 1310.7889490127563, 'accumulated_logging_time': 3.420320987701416, 'global_step': 110722, 'preemption_count': 0}), (112219, {'train/accuracy': 0.8077367544174194, 'train/loss': 0.6889608502388, 'validation/accuracy': 0.7064599990844727, 'validation/loss': 1.1906296014785767, 'validation/num_examples': 50000, 'test/accuracy': 0.5773000121116638, 'test/loss': 1.9144089221954346, 'test/num_examples': 10000, 'score': 38291.41183042526, 'total_duration': 39626.737365961075, 'accumulated_submission_time': 38291.41183042526, 'accumulated_eval_time': 1328.0241174697876, 'accumulated_logging_time': 3.4695346355438232, 'global_step': 112219, 'preemption_count': 0}), (113716, {'train/accuracy': 0.7943239808082581, 'train/loss': 0.7536799311637878, 'validation/accuracy': 0.6958999633789062, 'validation/loss': 1.2339434623718262, 'validation/num_examples': 50000, 'test/accuracy': 0.5742000341415405, 'test/loss': 1.97724187374115, 'test/num_examples': 10000, 'score': 38801.31273698807, 'total_duration': 40154.12018656731, 'accumulated_submission_time': 38801.31273698807, 'accumulated_eval_time': 1345.4056491851807, 'accumulated_logging_time': 3.519113063812256, 'global_step': 113716, 'preemption_count': 0}), (115212, {'train/accuracy': 0.7994260191917419, 'train/loss': 0.7419718503952026, 'validation/accuracy': 0.6971399784088135, 'validation/loss': 1.2301865816116333, 'validation/num_examples': 50000, 'test/accuracy': 0.5767000317573547, 'test/loss': 1.9383008480072021, 'test/num_examples': 10000, 'score': 39311.23474597931, 'total_duration': 40681.605519771576, 'accumulated_submission_time': 39311.23474597931, 'accumulated_eval_time': 1362.8529393672943, 'accumulated_logging_time': 3.5819525718688965, 'global_step': 115212, 'preemption_count': 0}), (116709, {'train/accuracy': 0.8053252100944519, 'train/loss': 0.7091084718704224, 'validation/accuracy': 0.7084199786186218, 'validation/loss': 1.1827819347381592, 'validation/num_examples': 50000, 'test/accuracy': 0.5850000381469727, 'test/loss': 1.901528239250183, 'test/num_examples': 10000, 'score': 39821.202984809875, 'total_duration': 41209.22456550598, 'accumulated_submission_time': 39821.202984809875, 'accumulated_eval_time': 1380.4033725261688, 'accumulated_logging_time': 3.630246162414551, 'global_step': 116709, 'preemption_count': 0}), (118206, {'train/accuracy': 0.8056241869926453, 'train/loss': 0.7094724178314209, 'validation/accuracy': 0.7069999575614929, 'validation/loss': 1.1886725425720215, 'validation/num_examples': 50000, 'test/accuracy': 0.5771000385284424, 'test/loss': 1.9114309549331665, 'test/num_examples': 10000, 'score': 40331.18197154999, 'total_duration': 41736.874911785126, 'accumulated_submission_time': 40331.18197154999, 'accumulated_eval_time': 1397.9720528125763, 'accumulated_logging_time': 3.681635856628418, 'global_step': 118206, 'preemption_count': 0}), (119703, {'train/accuracy': 0.8384486436843872, 'train/loss': 0.5811982154846191, 'validation/accuracy': 0.7049599885940552, 'validation/loss': 1.1874204874038696, 'validation/num_examples': 50000, 'test/accuracy': 0.5821000337600708, 'test/loss': 1.9020187854766846, 'test/num_examples': 10000, 'score': 40841.25449848175, 'total_duration': 42264.33503699303, 'accumulated_submission_time': 40841.25449848175, 'accumulated_eval_time': 1415.2570950984955, 'accumulated_logging_time': 3.732133626937866, 'global_step': 119703, 'preemption_count': 0}), (121201, {'train/accuracy': 0.8234614133834839, 'train/loss': 0.6339573860168457, 'validation/accuracy': 0.7074999809265137, 'validation/loss': 1.1968789100646973, 'validation/num_examples': 50000, 'test/accuracy': 0.5819000005722046, 'test/loss': 1.924055814743042, 'test/num_examples': 10000, 'score': 41351.663058280945, 'total_duration': 42792.24165916443, 'accumulated_submission_time': 41351.663058280945, 'accumulated_eval_time': 1432.6516358852386, 'accumulated_logging_time': 3.7847323417663574, 'global_step': 121201, 'preemption_count': 0}), (122698, {'train/accuracy': 0.8205317258834839, 'train/loss': 0.6411925554275513, 'validation/accuracy': 0.707040011882782, 'validation/loss': 1.196536898612976, 'validation/num_examples': 50000, 'test/accuracy': 0.5800000429153442, 'test/loss': 1.9360084533691406, 'test/num_examples': 10000, 'score': 41861.57820510864, 'total_duration': 43319.72976899147, 'accumulated_submission_time': 41861.57820510864, 'accumulated_eval_time': 1450.1179354190826, 'accumulated_logging_time': 3.8395774364471436, 'global_step': 122698, 'preemption_count': 0}), (124194, {'train/accuracy': 0.8153499364852905, 'train/loss': 0.6613895297050476, 'validation/accuracy': 0.7084000110626221, 'validation/loss': 1.1882474422454834, 'validation/num_examples': 50000, 'test/accuracy': 0.5819000005722046, 'test/loss': 1.9214253425598145, 'test/num_examples': 10000, 'score': 42371.47960424423, 'total_duration': 43846.969765901566, 'accumulated_submission_time': 42371.47960424423, 'accumulated_eval_time': 1467.3553059101105, 'accumulated_logging_time': 3.890242338180542, 'global_step': 124194, 'preemption_count': 0}), (125692, {'train/accuracy': 0.8228236436843872, 'train/loss': 0.6361862421035767, 'validation/accuracy': 0.7134000062942505, 'validation/loss': 1.174013614654541, 'validation/num_examples': 50000, 'test/accuracy': 0.5859000086784363, 'test/loss': 1.9083994626998901, 'test/num_examples': 10000, 'score': 42881.63268017769, 'total_duration': 44374.61629462242, 'accumulated_submission_time': 42881.63268017769, 'accumulated_eval_time': 1484.7452561855316, 'accumulated_logging_time': 3.9434943199157715, 'global_step': 125692, 'preemption_count': 0}), (127190, {'train/accuracy': 0.8191167116165161, 'train/loss': 0.6429269909858704, 'validation/accuracy': 0.7112399935722351, 'validation/loss': 1.1914821863174438, 'validation/num_examples': 50000, 'test/accuracy': 0.5815000534057617, 'test/loss': 1.9391067028045654, 'test/num_examples': 10000, 'score': 43391.758655786514, 'total_duration': 44902.20900511742, 'accumulated_submission_time': 43391.758655786514, 'accumulated_eval_time': 1502.1120376586914, 'accumulated_logging_time': 3.9931235313415527, 'global_step': 127190, 'preemption_count': 0}), (128687, {'train/accuracy': 0.8657525181770325, 'train/loss': 0.4807564318180084, 'validation/accuracy': 0.7155599594116211, 'validation/loss': 1.1618179082870483, 'validation/num_examples': 50000, 'test/accuracy': 0.5940000414848328, 'test/loss': 1.8702696561813354, 'test/num_examples': 10000, 'score': 43901.96710586548, 'total_duration': 45430.13155961037, 'accumulated_submission_time': 43901.96710586548, 'accumulated_eval_time': 1519.7223196029663, 'accumulated_logging_time': 4.045359134674072, 'global_step': 128687, 'preemption_count': 0}), (130184, {'train/accuracy': 0.8509646058082581, 'train/loss': 0.52117919921875, 'validation/accuracy': 0.7196599841117859, 'validation/loss': 1.150366187095642, 'validation/num_examples': 50000, 'test/accuracy': 0.5944000482559204, 'test/loss': 1.8714016675949097, 'test/num_examples': 10000, 'score': 44412.060628175735, 'total_duration': 45957.546800136566, 'accumulated_submission_time': 44412.060628175735, 'accumulated_eval_time': 1536.9437124729156, 'accumulated_logging_time': 4.095425844192505, 'global_step': 130184, 'preemption_count': 0}), (131681, {'train/accuracy': 0.8581393361091614, 'train/loss': 0.5003353953361511, 'validation/accuracy': 0.721340000629425, 'validation/loss': 1.128917932510376, 'validation/num_examples': 50000, 'test/accuracy': 0.5985000133514404, 'test/loss': 1.852310061454773, 'test/num_examples': 10000, 'score': 44922.07017183304, 'total_duration': 46485.04060125351, 'accumulated_submission_time': 44922.07017183304, 'accumulated_eval_time': 1554.3216423988342, 'accumulated_logging_time': 4.150796890258789, 'global_step': 131681, 'preemption_count': 0}), (133178, {'train/accuracy': 0.8520009517669678, 'train/loss': 0.5243096351623535, 'validation/accuracy': 0.7253999710083008, 'validation/loss': 1.120864987373352, 'validation/num_examples': 50000, 'test/accuracy': 0.5966000556945801, 'test/loss': 1.8334498405456543, 'test/num_examples': 10000, 'score': 45432.17251110077, 'total_duration': 47012.60780596733, 'accumulated_submission_time': 45432.17251110077, 'accumulated_eval_time': 1571.6787357330322, 'accumulated_logging_time': 4.2078258991241455, 'global_step': 133178, 'preemption_count': 0}), (134674, {'train/accuracy': 0.8475764989852905, 'train/loss': 0.5381889343261719, 'validation/accuracy': 0.7217999696731567, 'validation/loss': 1.1407110691070557, 'validation/num_examples': 50000, 'test/accuracy': 0.6020000576972961, 'test/loss': 1.8749887943267822, 'test/num_examples': 10000, 'score': 45942.078404426575, 'total_duration': 47540.725546360016, 'accumulated_submission_time': 45942.078404426575, 'accumulated_eval_time': 1589.7894802093506, 'accumulated_logging_time': 4.2574567794799805, 'global_step': 134674, 'preemption_count': 0}), (136171, {'train/accuracy': 0.8484932780265808, 'train/loss': 0.5277658104896545, 'validation/accuracy': 0.7238399982452393, 'validation/loss': 1.1313154697418213, 'validation/num_examples': 50000, 'test/accuracy': 0.5981000065803528, 'test/loss': 1.8592684268951416, 'test/num_examples': 10000, 'score': 46452.042345047, 'total_duration': 48068.0526099205, 'accumulated_submission_time': 46452.042345047, 'accumulated_eval_time': 1607.0494379997253, 'accumulated_logging_time': 4.309852600097656, 'global_step': 136171, 'preemption_count': 0}), (137668, {'train/accuracy': 0.865652859210968, 'train/loss': 0.4712446928024292, 'validation/accuracy': 0.7237399816513062, 'validation/loss': 1.1422827243804932, 'validation/num_examples': 50000, 'test/accuracy': 0.5975000262260437, 'test/loss': 1.8960363864898682, 'test/num_examples': 10000, 'score': 46961.99141907692, 'total_duration': 48595.35859775543, 'accumulated_submission_time': 46961.99141907692, 'accumulated_eval_time': 1624.3033895492554, 'accumulated_logging_time': 4.361900568008423, 'global_step': 137668, 'preemption_count': 0}), (139165, {'train/accuracy': 0.8843072056770325, 'train/loss': 0.402018278837204, 'validation/accuracy': 0.7286999821662903, 'validation/loss': 1.1118732690811157, 'validation/num_examples': 50000, 'test/accuracy': 0.612500011920929, 'test/loss': 1.8532575368881226, 'test/num_examples': 10000, 'score': 47472.01818680763, 'total_duration': 49122.931619644165, 'accumulated_submission_time': 47472.01818680763, 'accumulated_eval_time': 1641.7458896636963, 'accumulated_logging_time': 4.415813446044922, 'global_step': 139165, 'preemption_count': 0}), (140663, {'train/accuracy': 0.87890625, 'train/loss': 0.42630431056022644, 'validation/accuracy': 0.7313599586486816, 'validation/loss': 1.112805724143982, 'validation/num_examples': 50000, 'test/accuracy': 0.6103000044822693, 'test/loss': 1.8223932981491089, 'test/num_examples': 10000, 'score': 47982.18217277527, 'total_duration': 49650.53896570206, 'accumulated_submission_time': 47982.18217277527, 'accumulated_eval_time': 1659.0822570323944, 'accumulated_logging_time': 4.472891807556152, 'global_step': 140663, 'preemption_count': 0}), (142160, {'train/accuracy': 0.8776904940605164, 'train/loss': 0.42073261737823486, 'validation/accuracy': 0.7316799759864807, 'validation/loss': 1.1086206436157227, 'validation/num_examples': 50000, 'test/accuracy': 0.6021000146865845, 'test/loss': 1.8438477516174316, 'test/num_examples': 10000, 'score': 48492.161371946335, 'total_duration': 50178.48799037933, 'accumulated_submission_time': 48492.161371946335, 'accumulated_eval_time': 1676.9486873149872, 'accumulated_logging_time': 4.525469541549683, 'global_step': 142160, 'preemption_count': 0}), (143657, {'train/accuracy': 0.8729472160339355, 'train/loss': 0.44094493985176086, 'validation/accuracy': 0.7316799759864807, 'validation/loss': 1.1113321781158447, 'validation/num_examples': 50000, 'test/accuracy': 0.6050000190734863, 'test/loss': 1.8393492698669434, 'test/num_examples': 10000, 'score': 49002.2856194973, 'total_duration': 50706.214452028275, 'accumulated_submission_time': 49002.2856194973, 'accumulated_eval_time': 1694.4568555355072, 'accumulated_logging_time': 4.5699193477630615, 'global_step': 143657, 'preemption_count': 0}), (145154, {'train/accuracy': 0.8752989172935486, 'train/loss': 0.4280344545841217, 'validation/accuracy': 0.7317799925804138, 'validation/loss': 1.1089249849319458, 'validation/num_examples': 50000, 'test/accuracy': 0.6053000092506409, 'test/loss': 1.8584744930267334, 'test/num_examples': 10000, 'score': 49512.30458474159, 'total_duration': 51233.79352784157, 'accumulated_submission_time': 49512.30458474159, 'accumulated_eval_time': 1711.908765077591, 'accumulated_logging_time': 4.625460624694824, 'global_step': 145154, 'preemption_count': 0}), (146652, {'train/accuracy': 0.8880739808082581, 'train/loss': 0.3907930254936218, 'validation/accuracy': 0.734279990196228, 'validation/loss': 1.1064696311950684, 'validation/num_examples': 50000, 'test/accuracy': 0.6035000085830688, 'test/loss': 1.850600242614746, 'test/num_examples': 10000, 'score': 50022.35403752327, 'total_duration': 51761.16456365585, 'accumulated_submission_time': 50022.35403752327, 'accumulated_eval_time': 1729.127257347107, 'accumulated_logging_time': 4.677754163742065, 'global_step': 146652, 'preemption_count': 0}), (148149, {'train/accuracy': 0.91019606590271, 'train/loss': 0.3179280161857605, 'validation/accuracy': 0.7368199825286865, 'validation/loss': 1.0996601581573486, 'validation/num_examples': 50000, 'test/accuracy': 0.6094000339508057, 'test/loss': 1.84394109249115, 'test/num_examples': 10000, 'score': 50532.34589600563, 'total_duration': 52288.738582372665, 'accumulated_submission_time': 50532.34589600563, 'accumulated_eval_time': 1746.6031787395477, 'accumulated_logging_time': 4.732522487640381, 'global_step': 148149, 'preemption_count': 0}), (149644, {'train/accuracy': 0.9052136540412903, 'train/loss': 0.3252921402454376, 'validation/accuracy': 0.7401399612426758, 'validation/loss': 1.093822717666626, 'validation/num_examples': 50000, 'test/accuracy': 0.6175000071525574, 'test/loss': 1.8288391828536987, 'test/num_examples': 10000, 'score': 51042.26584935188, 'total_duration': 52816.26387476921, 'accumulated_submission_time': 51042.26584935188, 'accumulated_eval_time': 1764.0998284816742, 'accumulated_logging_time': 4.791525602340698, 'global_step': 149644, 'preemption_count': 0}), (151141, {'train/accuracy': 0.906668484210968, 'train/loss': 0.3213539719581604, 'validation/accuracy': 0.7433599829673767, 'validation/loss': 1.087909460067749, 'validation/num_examples': 50000, 'test/accuracy': 0.6135000586509705, 'test/loss': 1.828898549079895, 'test/num_examples': 10000, 'score': 51552.15370512009, 'total_duration': 53343.76315808296, 'accumulated_submission_time': 51552.15370512009, 'accumulated_eval_time': 1781.5769836902618, 'accumulated_logging_time': 4.874719858169556, 'global_step': 151141, 'preemption_count': 0}), (152637, {'train/accuracy': 0.9010881781578064, 'train/loss': 0.34705764055252075, 'validation/accuracy': 0.7376999855041504, 'validation/loss': 1.1096341609954834, 'validation/num_examples': 50000, 'test/accuracy': 0.6148000359535217, 'test/loss': 1.844693660736084, 'test/num_examples': 10000, 'score': 52062.18849515915, 'total_duration': 53871.51702570915, 'accumulated_submission_time': 52062.18849515915, 'accumulated_eval_time': 1799.1908648014069, 'accumulated_logging_time': 4.9273035526275635, 'global_step': 152637, 'preemption_count': 0}), (154134, {'train/accuracy': 0.9094188213348389, 'train/loss': 0.31328409910202026, 'validation/accuracy': 0.7424799799919128, 'validation/loss': 1.0863618850708008, 'validation/num_examples': 50000, 'test/accuracy': 0.6132000088691711, 'test/loss': 1.8464604616165161, 'test/num_examples': 10000, 'score': 52572.27054524422, 'total_duration': 54398.92409610748, 'accumulated_submission_time': 52572.27054524422, 'accumulated_eval_time': 1816.406150817871, 'accumulated_logging_time': 4.986354112625122, 'global_step': 154134, 'preemption_count': 0}), (155632, {'train/accuracy': 0.9090999364852905, 'train/loss': 0.3144540786743164, 'validation/accuracy': 0.7421799898147583, 'validation/loss': 1.0850337743759155, 'validation/num_examples': 50000, 'test/accuracy': 0.6140000224113464, 'test/loss': 1.8501039743423462, 'test/num_examples': 10000, 'score': 53082.467106580734, 'total_duration': 54926.58153581619, 'accumulated_submission_time': 53082.467106580734, 'accumulated_eval_time': 1833.7602362632751, 'accumulated_logging_time': 5.041287660598755, 'global_step': 155632, 'preemption_count': 0}), (157129, {'train/accuracy': 0.9382573366165161, 'train/loss': 0.2249719649553299, 'validation/accuracy': 0.7435799837112427, 'validation/loss': 1.074958086013794, 'validation/num_examples': 50000, 'test/accuracy': 0.6165000200271606, 'test/loss': 1.826570749282837, 'test/num_examples': 10000, 'score': 53592.5631840229, 'total_duration': 55454.151193380356, 'accumulated_submission_time': 53592.5631840229, 'accumulated_eval_time': 1851.1270196437836, 'accumulated_logging_time': 5.0969154834747314, 'global_step': 157129, 'preemption_count': 0}), (158626, {'train/accuracy': 0.9301857352256775, 'train/loss': 0.24374496936798096, 'validation/accuracy': 0.7449399828910828, 'validation/loss': 1.078855276107788, 'validation/num_examples': 50000, 'test/accuracy': 0.6187000274658203, 'test/loss': 1.83165442943573, 'test/num_examples': 10000, 'score': 54102.53592252731, 'total_duration': 55981.42848825455, 'accumulated_submission_time': 54102.53592252731, 'accumulated_eval_time': 1868.3179905414581, 'accumulated_logging_time': 5.159096002578735, 'global_step': 158626, 'preemption_count': 0}), (160123, {'train/accuracy': 0.9317004084587097, 'train/loss': 0.23847176134586334, 'validation/accuracy': 0.7465999722480774, 'validation/loss': 1.0716962814331055, 'validation/num_examples': 50000, 'test/accuracy': 0.622700035572052, 'test/loss': 1.8242336511611938, 'test/num_examples': 10000, 'score': 54612.44617629051, 'total_duration': 56508.81551861763, 'accumulated_submission_time': 54612.44617629051, 'accumulated_eval_time': 1885.6868290901184, 'accumulated_logging_time': 5.215553045272827, 'global_step': 160123, 'preemption_count': 0}), (161619, {'train/accuracy': 0.9328961968421936, 'train/loss': 0.23286132514476776, 'validation/accuracy': 0.7475000023841858, 'validation/loss': 1.0679634809494019, 'validation/num_examples': 50000, 'test/accuracy': 0.6202000379562378, 'test/loss': 1.8374985456466675, 'test/num_examples': 10000, 'score': 55122.4037668705, 'total_duration': 57036.08502626419, 'accumulated_submission_time': 55122.4037668705, 'accumulated_eval_time': 1902.8871002197266, 'accumulated_logging_time': 5.275780916213989, 'global_step': 161619, 'preemption_count': 0}), (163115, {'train/accuracy': 0.9344307780265808, 'train/loss': 0.22658495604991913, 'validation/accuracy': 0.7470999956130981, 'validation/loss': 1.070637583732605, 'validation/num_examples': 50000, 'test/accuracy': 0.6237000226974487, 'test/loss': 1.8353160619735718, 'test/num_examples': 10000, 'score': 55632.31921625137, 'total_duration': 57563.64182114601, 'accumulated_submission_time': 55632.31921625137, 'accumulated_eval_time': 1920.4235010147095, 'accumulated_logging_time': 5.330445289611816, 'global_step': 163115, 'preemption_count': 0}), (164613, {'train/accuracy': 0.9340322017669678, 'train/loss': 0.22421029210090637, 'validation/accuracy': 0.7490800023078918, 'validation/loss': 1.070961356163025, 'validation/num_examples': 50000, 'test/accuracy': 0.622700035572052, 'test/loss': 1.8354753255844116, 'test/num_examples': 10000, 'score': 56142.438957214355, 'total_duration': 58091.122450351715, 'accumulated_submission_time': 56142.438957214355, 'accumulated_eval_time': 1937.6732861995697, 'accumulated_logging_time': 5.390793085098267, 'global_step': 164613, 'preemption_count': 0}), (166109, {'train/accuracy': 0.9427216053009033, 'train/loss': 0.2048528641462326, 'validation/accuracy': 0.7487599849700928, 'validation/loss': 1.0646597146987915, 'validation/num_examples': 50000, 'test/accuracy': 0.624500036239624, 'test/loss': 1.8405344486236572, 'test/num_examples': 10000, 'score': 56652.48220562935, 'total_duration': 58618.796217918396, 'accumulated_submission_time': 56652.48220562935, 'accumulated_eval_time': 1955.1959567070007, 'accumulated_logging_time': 5.446483373641968, 'global_step': 166109, 'preemption_count': 0}), (167606, {'train/accuracy': 0.9520288109779358, 'train/loss': 0.17483839392662048, 'validation/accuracy': 0.750499963760376, 'validation/loss': 1.0643622875213623, 'validation/num_examples': 50000, 'test/accuracy': 0.6233000159263611, 'test/loss': 1.8415911197662354, 'test/num_examples': 10000, 'score': 57162.58238697052, 'total_duration': 59146.30214428902, 'accumulated_submission_time': 57162.58238697052, 'accumulated_eval_time': 1972.4936077594757, 'accumulated_logging_time': 5.505061149597168, 'global_step': 167606, 'preemption_count': 0}), (169103, {'train/accuracy': 0.9522680044174194, 'train/loss': 0.17501652240753174, 'validation/accuracy': 0.750819981098175, 'validation/loss': 1.0584666728973389, 'validation/num_examples': 50000, 'test/accuracy': 0.6274000406265259, 'test/loss': 1.835326075553894, 'test/num_examples': 10000, 'score': 57672.57480549812, 'total_duration': 59673.71718621254, 'accumulated_submission_time': 57672.57480549812, 'accumulated_eval_time': 1989.808545589447, 'accumulated_logging_time': 5.561115026473999, 'global_step': 169103, 'preemption_count': 0}), (170600, {'train/accuracy': 0.9530253410339355, 'train/loss': 0.16859164834022522, 'validation/accuracy': 0.7524200081825256, 'validation/loss': 1.058369517326355, 'validation/num_examples': 50000, 'test/accuracy': 0.6294000148773193, 'test/loss': 1.825202226638794, 'test/num_examples': 10000, 'score': 58182.56483054161, 'total_duration': 60201.29833936691, 'accumulated_submission_time': 58182.56483054161, 'accumulated_eval_time': 2007.2889399528503, 'accumulated_logging_time': 5.620239496231079, 'global_step': 170600, 'preemption_count': 0}), (172097, {'train/accuracy': 0.9518095850944519, 'train/loss': 0.17187651991844177, 'validation/accuracy': 0.7515000104904175, 'validation/loss': 1.0592249631881714, 'validation/num_examples': 50000, 'test/accuracy': 0.628600001335144, 'test/loss': 1.8379465341567993, 'test/num_examples': 10000, 'score': 58692.67744731903, 'total_duration': 60728.97080159187, 'accumulated_submission_time': 58692.67744731903, 'accumulated_eval_time': 2024.7350087165833, 'accumulated_logging_time': 5.683067798614502, 'global_step': 172097, 'preemption_count': 0}), (173594, {'train/accuracy': 0.9528858065605164, 'train/loss': 0.16821564733982086, 'validation/accuracy': 0.754040002822876, 'validation/loss': 1.060206651687622, 'validation/num_examples': 50000, 'test/accuracy': 0.6281000375747681, 'test/loss': 1.8330553770065308, 'test/num_examples': 10000, 'score': 59202.68797492981, 'total_duration': 61257.06341433525, 'accumulated_submission_time': 59202.68797492981, 'accumulated_eval_time': 2042.6753525733948, 'accumulated_logging_time': 5.773506164550781, 'global_step': 173594, 'preemption_count': 0}), (175090, {'train/accuracy': 0.9554169178009033, 'train/loss': 0.16442981362342834, 'validation/accuracy': 0.754040002822876, 'validation/loss': 1.0577527284622192, 'validation/num_examples': 50000, 'test/accuracy': 0.6284000277519226, 'test/loss': 1.8298102617263794, 'test/num_examples': 10000, 'score': 59712.79919052124, 'total_duration': 61784.8630464077, 'accumulated_submission_time': 59712.79919052124, 'accumulated_eval_time': 2060.2560591697693, 'accumulated_logging_time': 5.830533027648926, 'global_step': 175090, 'preemption_count': 0}), (176587, {'train/accuracy': 0.9605588316917419, 'train/loss': 0.1476965844631195, 'validation/accuracy': 0.7540599703788757, 'validation/loss': 1.0551767349243164, 'validation/num_examples': 50000, 'test/accuracy': 0.626800000667572, 'test/loss': 1.8359673023223877, 'test/num_examples': 10000, 'score': 60222.87142467499, 'total_duration': 62312.36273407936, 'accumulated_submission_time': 60222.87142467499, 'accumulated_eval_time': 2077.571899175644, 'accumulated_logging_time': 5.891931772232056, 'global_step': 176587, 'preemption_count': 0}), (178084, {'train/accuracy': 0.9595822691917419, 'train/loss': 0.1489991694688797, 'validation/accuracy': 0.7542799711227417, 'validation/loss': 1.0563178062438965, 'validation/num_examples': 50000, 'test/accuracy': 0.6317000389099121, 'test/loss': 1.831668496131897, 'test/num_examples': 10000, 'score': 60733.02505970001, 'total_duration': 62840.026733636856, 'accumulated_submission_time': 60733.02505970001, 'accumulated_eval_time': 2094.967592716217, 'accumulated_logging_time': 5.955878973007202, 'global_step': 178084, 'preemption_count': 0}), (179581, {'train/accuracy': 0.9616549611091614, 'train/loss': 0.14457814395427704, 'validation/accuracy': 0.7544199824333191, 'validation/loss': 1.0557457208633423, 'validation/num_examples': 50000, 'test/accuracy': 0.6299000382423401, 'test/loss': 1.8318003416061401, 'test/num_examples': 10000, 'score': 61242.96651220322, 'total_duration': 63367.515714883804, 'accumulated_submission_time': 61242.96651220322, 'accumulated_eval_time': 2112.402285337448, 'accumulated_logging_time': 6.015355348587036, 'global_step': 179581, 'preemption_count': 0}), (181078, {'train/accuracy': 0.9592434167861938, 'train/loss': 0.1504308581352234, 'validation/accuracy': 0.7547599673271179, 'validation/loss': 1.0523484945297241, 'validation/num_examples': 50000, 'test/accuracy': 0.629800021648407, 'test/loss': 1.8307338953018188, 'test/num_examples': 10000, 'score': 61753.014424562454, 'total_duration': 63894.78460788727, 'accumulated_submission_time': 61753.014424562454, 'accumulated_eval_time': 2129.514147043228, 'accumulated_logging_time': 6.073501348495483, 'global_step': 181078, 'preemption_count': 0}), (182575, {'train/accuracy': 0.9605388641357422, 'train/loss': 0.14766532182693481, 'validation/accuracy': 0.7547999620437622, 'validation/loss': 1.0509791374206543, 'validation/num_examples': 50000, 'test/accuracy': 0.6289000511169434, 'test/loss': 1.8282490968704224, 'test/num_examples': 10000, 'score': 62263.06065821648, 'total_duration': 64422.20252633095, 'accumulated_submission_time': 62263.06065821648, 'accumulated_eval_time': 2146.771764278412, 'accumulated_logging_time': 6.1375648975372314, 'global_step': 182575, 'preemption_count': 0}), (184073, {'train/accuracy': 0.9612962007522583, 'train/loss': 0.14372682571411133, 'validation/accuracy': 0.7549600005149841, 'validation/loss': 1.0522350072860718, 'validation/num_examples': 50000, 'test/accuracy': 0.6287000179290771, 'test/loss': 1.829309105873108, 'test/num_examples': 10000, 'score': 62773.27559399605, 'total_duration': 64949.84421133995, 'accumulated_submission_time': 62773.27559399605, 'accumulated_eval_time': 2164.0824568271637, 'accumulated_logging_time': 6.202368974685669, 'global_step': 184073, 'preemption_count': 0})], 'global_step': 184763}
I0130 13:39:20.194807 139822745589568 submission_runner.py:586] Timing: 63008.12818527222
I0130 13:39:20.194877 139822745589568 submission_runner.py:588] Total number of evals: 124
I0130 13:39:20.194919 139822745589568 submission_runner.py:589] ====================
I0130 13:39:20.196422 139822745589568 submission_runner.py:673] Final imagenet_resnet score: 63008.04301953316
