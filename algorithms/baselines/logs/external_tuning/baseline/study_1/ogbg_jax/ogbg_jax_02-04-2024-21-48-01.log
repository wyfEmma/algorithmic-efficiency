python3 submission_runner.py --framework=jax --workload=ogbg --submission_path=prize_qualification_baselines/external_tuning/jax_nadamw_full_budget.py --tuning_search_space=prize_qualification_baselines/external_tuning/tuning_search_space.json --data_dir=/data/ogbg --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=prize_qualification/study_1 --overwrite=true --save_checkpoints=false --num_tuning_trials=5 --rng_seed=356686224 --max_global_steps=80000 2>&1 | tee -a /logs/ogbg_jax_02-04-2024-21-48-01.log
I0204 21:48:22.464605 139919816816448 logger_utils.py:76] Creating experiment directory at /experiment_runs/prize_qualification/study_1/ogbg_jax.
I0204 21:48:23.555112 139919816816448 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I0204 21:48:23.556858 139919816816448 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0204 21:48:23.557072 139919816816448 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0204 21:48:23.558507 139919816816448 submission_runner.py:542] Using RNG seed 356686224
I0204 21:48:24.686233 139919816816448 submission_runner.py:551] --- Tuning run 1/5 ---
I0204 21:48:24.686503 139919816816448 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_1/ogbg_jax/trial_1.
I0204 21:48:24.687085 139919816816448 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_1/ogbg_jax/trial_1/hparams.json.
I0204 21:48:24.875348 139919816816448 submission_runner.py:206] Initializing dataset.
I0204 21:48:24.990500 139919816816448 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0204 21:48:24.996449 139919816816448 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow_datasets/core/reader.py:101: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.counter(...)` instead.
W0204 21:48:25.259948 139919816816448 deprecation.py:364] From /usr/local/lib/python3.8/dist-packages/tensorflow_datasets/core/reader.py:101: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.counter(...)` instead.
I0204 21:48:25.327677 139919816816448 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0204 21:48:25.408447 139919816816448 submission_runner.py:213] Initializing model.
I0204 21:48:30.463408 139919816816448 submission_runner.py:255] Initializing optimizer.
I0204 21:48:31.116051 139919816816448 submission_runner.py:262] Initializing metrics bundle.
I0204 21:48:31.116301 139919816816448 submission_runner.py:280] Initializing checkpoint and logger.
I0204 21:48:31.117496 139919816816448 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_1/ogbg_jax/trial_1 with prefix checkpoint_
I0204 21:48:31.117666 139919816816448 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_1/ogbg_jax/trial_1/meta_data_0.json.
I0204 21:48:31.117886 139919816816448 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0204 21:48:31.117950 139919816816448 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0204 21:48:31.472723 139919816816448 logger_utils.py:220] Unable to record git information. Continuing without it.
I0204 21:48:31.794455 139919816816448 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_1/ogbg_jax/trial_1/flags_0.json.
I0204 21:48:31.805998 139919816816448 submission_runner.py:314] Starting training loop.
I0204 21:48:50.988489 139755618563840 logging_writer.py:48] [0] global_step=0, grad_norm=2.529244899749756, loss=0.7269789576530457
I0204 21:48:51.008487 139919816816448 spec.py:321] Evaluating on the training split.
I0204 21:48:51.015828 139919816816448 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0204 21:48:51.020372 139919816816448 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0204 21:48:51.100517 139919816816448 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0204 21:50:47.112047 139919816816448 spec.py:333] Evaluating on the validation split.
I0204 21:50:47.116150 139919816816448 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0204 21:50:47.120683 139919816816448 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0204 21:50:47.193456 139919816816448 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split validation, from /data/ogbg/ogbg_molpcba/0.1.3
I0204 21:52:21.288130 139919816816448 spec.py:349] Evaluating on the test split.
I0204 21:52:21.291700 139919816816448 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0204 21:52:21.295735 139919816816448 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0204 21:52:21.365044 139919816816448 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split test, from /data/ogbg/ogbg_molpcba/0.1.3
I0204 21:53:58.565102 139919816816448 submission_runner.py:408] Time since start: 326.76s, 	Step: 1, 	{'train/accuracy': 0.532486081123352, 'train/loss': 0.7277461886405945, 'train/mean_average_precision': 0.022446237910565617, 'validation/accuracy': 0.5230699777603149, 'validation/loss': 0.7331878542900085, 'validation/mean_average_precision': 0.025538990812299604, 'validation/num_examples': 43793, 'test/accuracy': 0.5214918851852417, 'test/loss': 0.7347381114959717, 'test/mean_average_precision': 0.02681250091619177, 'test/num_examples': 43793, 'score': 19.20242476463318, 'total_duration': 326.7589704990387, 'accumulated_submission_time': 19.20242476463318, 'accumulated_eval_time': 307.5564877986908, 'accumulated_logging_time': 0}
I0204 21:53:58.584433 139751071487744 logging_writer.py:48] [1] accumulated_eval_time=307.556488, accumulated_logging_time=0, accumulated_submission_time=19.202425, global_step=1, preemption_count=0, score=19.202425, test/accuracy=0.521492, test/loss=0.734738, test/mean_average_precision=0.026813, test/num_examples=43793, total_duration=326.758970, train/accuracy=0.532486, train/loss=0.727746, train/mean_average_precision=0.022446, validation/accuracy=0.523070, validation/loss=0.733188, validation/mean_average_precision=0.025539, validation/num_examples=43793
I0204 21:54:30.525780 139752438810368 logging_writer.py:48] [100] global_step=100, grad_norm=0.6413539052009583, loss=0.4432244300842285
I0204 21:55:02.406823 139751071487744 logging_writer.py:48] [200] global_step=200, grad_norm=0.36645594239234924, loss=0.33015167713165283
I0204 21:55:34.299558 139752438810368 logging_writer.py:48] [300] global_step=300, grad_norm=0.26892924308776855, loss=0.23649920523166656
I0204 21:56:06.111811 139751071487744 logging_writer.py:48] [400] global_step=400, grad_norm=0.17710056900978088, loss=0.16276665031909943
I0204 21:56:37.997040 139752438810368 logging_writer.py:48] [500] global_step=500, grad_norm=0.11002148687839508, loss=0.11727607995271683
I0204 21:57:09.871646 139751071487744 logging_writer.py:48] [600] global_step=600, grad_norm=0.07019314914941788, loss=0.08634267747402191
I0204 21:57:41.742401 139752438810368 logging_writer.py:48] [700] global_step=700, grad_norm=0.045416999608278275, loss=0.0738825649023056
I0204 21:57:58.671859 139919816816448 spec.py:321] Evaluating on the training split.
I0204 21:59:50.416216 139919816816448 spec.py:333] Evaluating on the validation split.
I0204 21:59:53.887151 139919816816448 spec.py:349] Evaluating on the test split.
I0204 21:59:57.248846 139919816816448 submission_runner.py:408] Time since start: 685.44s, 	Step: 753, 	{'train/accuracy': 0.986735999584198, 'train/loss': 0.06938512623310089, 'train/mean_average_precision': 0.032705299679279695, 'validation/accuracy': 0.9841179251670837, 'validation/loss': 0.07758407294750214, 'validation/mean_average_precision': 0.033466476626962026, 'validation/num_examples': 43793, 'test/accuracy': 0.983142077922821, 'test/loss': 0.08040352910757065, 'test/mean_average_precision': 0.034597994751347755, 'test/num_examples': 43793, 'score': 259.26010942459106, 'total_duration': 685.4427721500397, 'accumulated_submission_time': 259.26010942459106, 'accumulated_eval_time': 426.1334116458893, 'accumulated_logging_time': 0.030203819274902344}
I0204 21:59:57.271406 139752296675072 logging_writer.py:48] [753] accumulated_eval_time=426.133412, accumulated_logging_time=0.030204, accumulated_submission_time=259.260109, global_step=753, preemption_count=0, score=259.260109, test/accuracy=0.983142, test/loss=0.080404, test/mean_average_precision=0.034598, test/num_examples=43793, total_duration=685.442772, train/accuracy=0.986736, train/loss=0.069385, train/mean_average_precision=0.032705, validation/accuracy=0.984118, validation/loss=0.077584, validation/mean_average_precision=0.033466, validation/num_examples=43793
I0204 22:00:12.965283 139752455595776 logging_writer.py:48] [800] global_step=800, grad_norm=0.03596245497465134, loss=0.06578028202056885
I0204 22:00:45.377839 139752296675072 logging_writer.py:48] [900] global_step=900, grad_norm=0.15797464549541473, loss=0.0622996911406517
I0204 22:01:17.734055 139752455595776 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.14582650363445282, loss=0.05729956179857254
I0204 22:01:49.489969 139752296675072 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.22245639562606812, loss=0.05442604795098305
I0204 22:02:21.827198 139752455595776 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.07493934035301208, loss=0.05002373829483986
I0204 22:02:53.399229 139752296675072 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.06657913327217102, loss=0.0552733950316906
I0204 22:03:25.223015 139752455595776 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.12239163368940353, loss=0.04796149581670761
I0204 22:03:56.449529 139752296675072 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.18562611937522888, loss=0.05624759942293167
I0204 22:03:57.389312 139919816816448 spec.py:321] Evaluating on the training split.
I0204 22:05:52.860318 139919816816448 spec.py:333] Evaluating on the validation split.
I0204 22:05:55.863436 139919816816448 spec.py:349] Evaluating on the test split.
I0204 22:05:58.843986 139919816816448 submission_runner.py:408] Time since start: 1047.04s, 	Step: 1504, 	{'train/accuracy': 0.9868547320365906, 'train/loss': 0.051052525639534, 'train/mean_average_precision': 0.07063277791010728, 'validation/accuracy': 0.984196662902832, 'validation/loss': 0.06075628101825714, 'validation/mean_average_precision': 0.06901157142689766, 'validation/num_examples': 43793, 'test/accuracy': 0.9832296967506409, 'test/loss': 0.06415972858667374, 'test/mean_average_precision': 0.07061243126817357, 'test/num_examples': 43793, 'score': 499.3460896015167, 'total_duration': 1047.0379321575165, 'accumulated_submission_time': 499.3460896015167, 'accumulated_eval_time': 547.5880465507507, 'accumulated_logging_time': 0.06566476821899414}
I0204 22:05:58.859836 139752438810368 logging_writer.py:48] [1504] accumulated_eval_time=547.588047, accumulated_logging_time=0.065665, accumulated_submission_time=499.346090, global_step=1504, preemption_count=0, score=499.346090, test/accuracy=0.983230, test/loss=0.064160, test/mean_average_precision=0.070612, test/num_examples=43793, total_duration=1047.037932, train/accuracy=0.986855, train/loss=0.051053, train/mean_average_precision=0.070633, validation/accuracy=0.984197, validation/loss=0.060756, validation/mean_average_precision=0.069012, validation/num_examples=43793
I0204 22:06:29.610954 139752447203072 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.07773273438215256, loss=0.04911070317029953
I0204 22:07:01.120210 139752438810368 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.21938301622867584, loss=0.054803527891635895
I0204 22:07:33.147069 139752447203072 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.05747897922992706, loss=0.045861463993787766
I0204 22:08:04.910691 139752438810368 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.22622846066951752, loss=0.0468873530626297
I0204 22:08:36.998213 139752447203072 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.1376989185810089, loss=0.04836151376366615
I0204 22:09:08.897086 139752438810368 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.07813228666782379, loss=0.048902302980422974
I0204 22:09:40.742134 139752447203072 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.08744592219591141, loss=0.05018153786659241
I0204 22:09:58.851731 139919816816448 spec.py:321] Evaluating on the training split.
I0204 22:11:56.749919 139919816816448 spec.py:333] Evaluating on the validation split.
I0204 22:11:59.819666 139919816816448 spec.py:349] Evaluating on the test split.
I0204 22:12:02.972347 139919816816448 submission_runner.py:408] Time since start: 1411.17s, 	Step: 2258, 	{'train/accuracy': 0.9876076579093933, 'train/loss': 0.04494336619973183, 'train/mean_average_precision': 0.12705054324859275, 'validation/accuracy': 0.9849172234535217, 'validation/loss': 0.053940750658512115, 'validation/mean_average_precision': 0.12254631551189603, 'validation/num_examples': 43793, 'test/accuracy': 0.9839473962783813, 'test/loss': 0.0569424144923687, 'test/mean_average_precision': 0.12682285102300242, 'test/num_examples': 43793, 'score': 739.3077754974365, 'total_duration': 1411.1662957668304, 'accumulated_submission_time': 739.3077754974365, 'accumulated_eval_time': 671.7086169719696, 'accumulated_logging_time': 0.09263157844543457}
I0204 22:12:02.988130 139752296675072 logging_writer.py:48] [2258] accumulated_eval_time=671.708617, accumulated_logging_time=0.092632, accumulated_submission_time=739.307775, global_step=2258, preemption_count=0, score=739.307775, test/accuracy=0.983947, test/loss=0.056942, test/mean_average_precision=0.126823, test/num_examples=43793, total_duration=1411.166296, train/accuracy=0.987608, train/loss=0.044943, train/mean_average_precision=0.127051, validation/accuracy=0.984917, validation/loss=0.053941, validation/mean_average_precision=0.122546, validation/num_examples=43793
I0204 22:12:17.765853 139752455595776 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.132694274187088, loss=0.05107641965150833
I0204 22:12:51.793658 139752296675072 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.07191411405801773, loss=0.04317650943994522
I0204 22:13:23.509417 139752455595776 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.2505562901496887, loss=0.04600374400615692
I0204 22:13:55.539344 139752296675072 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.12305068224668503, loss=0.04226239398121834
I0204 22:14:27.618004 139752455595776 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.09397793561220169, loss=0.04280037432909012
I0204 22:15:00.414936 139752296675072 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.10121738165616989, loss=0.04466331750154495
I0204 22:15:32.300159 139752455595776 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.11531147360801697, loss=0.04954981431365013
I0204 22:16:02.999748 139919816816448 spec.py:321] Evaluating on the training split.
I0204 22:18:05.203368 139919816816448 spec.py:333] Evaluating on the validation split.
I0204 22:18:08.266755 139919816816448 spec.py:349] Evaluating on the test split.
I0204 22:18:11.351574 139919816816448 submission_runner.py:408] Time since start: 1779.55s, 	Step: 2998, 	{'train/accuracy': 0.9880425930023193, 'train/loss': 0.04255932196974754, 'train/mean_average_precision': 0.15393298377261813, 'validation/accuracy': 0.9850654006004333, 'validation/loss': 0.051880139857530594, 'validation/mean_average_precision': 0.14723815533134316, 'validation/num_examples': 43793, 'test/accuracy': 0.9841668605804443, 'test/loss': 0.054550494998693466, 'test/mean_average_precision': 0.1515404133887407, 'test/num_examples': 43793, 'score': 979.2893161773682, 'total_duration': 1779.5455212593079, 'accumulated_submission_time': 979.2893161773682, 'accumulated_eval_time': 800.0604002475739, 'accumulated_logging_time': 0.11982035636901855}
I0204 22:18:11.367357 139752438810368 logging_writer.py:48] [2998] accumulated_eval_time=800.060400, accumulated_logging_time=0.119820, accumulated_submission_time=979.289316, global_step=2998, preemption_count=0, score=979.289316, test/accuracy=0.984167, test/loss=0.054550, test/mean_average_precision=0.151540, test/num_examples=43793, total_duration=1779.545521, train/accuracy=0.988043, train/loss=0.042559, train/mean_average_precision=0.153933, validation/accuracy=0.985065, validation/loss=0.051880, validation/mean_average_precision=0.147238, validation/num_examples=43793
I0204 22:18:12.351629 139752447203072 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.07032322883605957, loss=0.04484592005610466
I0204 22:18:45.199804 139752438810368 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.11637143045663834, loss=0.04280544072389603
I0204 22:19:17.216894 139752447203072 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.11851442605257034, loss=0.04402191564440727
I0204 22:19:49.599572 139752438810368 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.07831112295389175, loss=0.04278057441115379
I0204 22:20:22.519335 139752447203072 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.0658252090215683, loss=0.03988484665751457
I0204 22:20:54.855909 139752438810368 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.1043727919459343, loss=0.04229273647069931
I0204 22:21:27.015644 139752447203072 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.11941822618246078, loss=0.041578300297260284
I0204 22:21:58.725429 139752438810368 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.05422345921397209, loss=0.04300292581319809
I0204 22:22:11.501486 139919816816448 spec.py:321] Evaluating on the training split.
I0204 22:24:09.813215 139919816816448 spec.py:333] Evaluating on the validation split.
I0204 22:24:12.801287 139919816816448 spec.py:349] Evaluating on the test split.
I0204 22:24:15.768761 139919816816448 submission_runner.py:408] Time since start: 2143.96s, 	Step: 3740, 	{'train/accuracy': 0.9883670210838318, 'train/loss': 0.04071572422981262, 'train/mean_average_precision': 0.18442546201450333, 'validation/accuracy': 0.985426664352417, 'validation/loss': 0.0505857840180397, 'validation/mean_average_precision': 0.16857170420016457, 'validation/num_examples': 43793, 'test/accuracy': 0.9844903349876404, 'test/loss': 0.053492479026317596, 'test/mean_average_precision': 0.16623517202618904, 'test/num_examples': 43793, 'score': 1219.3918023109436, 'total_duration': 2143.96270942688, 'accumulated_submission_time': 1219.3918023109436, 'accumulated_eval_time': 924.3276314735413, 'accumulated_logging_time': 0.14711236953735352}
I0204 22:24:15.784297 139752296675072 logging_writer.py:48] [3740] accumulated_eval_time=924.327631, accumulated_logging_time=0.147112, accumulated_submission_time=1219.391802, global_step=3740, preemption_count=0, score=1219.391802, test/accuracy=0.984490, test/loss=0.053492, test/mean_average_precision=0.166235, test/num_examples=43793, total_duration=2143.962709, train/accuracy=0.988367, train/loss=0.040716, train/mean_average_precision=0.184425, validation/accuracy=0.985427, validation/loss=0.050586, validation/mean_average_precision=0.168572, validation/num_examples=43793
I0204 22:24:35.201452 139752455595776 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.05004219710826874, loss=0.03958771750330925
I0204 22:25:07.298514 139752296675072 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.14556270837783813, loss=0.04205498471856117
I0204 22:25:39.683399 139752455595776 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.07405192404985428, loss=0.04518773406744003
I0204 22:26:12.328093 139752296675072 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.07570779323577881, loss=0.04218684136867523
I0204 22:26:45.176115 139752455595776 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.0476732961833477, loss=0.0388311967253685
I0204 22:27:17.852550 139752296675072 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.10156114399433136, loss=0.04308842495083809
I0204 22:27:49.873226 139752455595776 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.04398227483034134, loss=0.0406048446893692
I0204 22:28:15.955082 139919816816448 spec.py:321] Evaluating on the training split.
I0204 22:30:20.259615 139919816816448 spec.py:333] Evaluating on the validation split.
I0204 22:30:23.637224 139919816816448 spec.py:349] Evaluating on the test split.
I0204 22:30:26.928879 139919816816448 submission_runner.py:408] Time since start: 2515.12s, 	Step: 4482, 	{'train/accuracy': 0.9883249402046204, 'train/loss': 0.040089331567287445, 'train/mean_average_precision': 0.2131278146848692, 'validation/accuracy': 0.9855647087097168, 'validation/loss': 0.04924480989575386, 'validation/mean_average_precision': 0.18086458362319696, 'validation/num_examples': 43793, 'test/accuracy': 0.9846583604812622, 'test/loss': 0.0519588366150856, 'test/mean_average_precision': 0.181732471899558, 'test/num_examples': 43793, 'score': 1459.5320043563843, 'total_duration': 2515.1227877140045, 'accumulated_submission_time': 1459.5320043563843, 'accumulated_eval_time': 1055.3013689517975, 'accumulated_logging_time': 0.17404675483703613}
I0204 22:30:26.948519 139758996317952 logging_writer.py:48] [4482] accumulated_eval_time=1055.301369, accumulated_logging_time=0.174047, accumulated_submission_time=1459.532004, global_step=4482, preemption_count=0, score=1459.532004, test/accuracy=0.984658, test/loss=0.051959, test/mean_average_precision=0.181732, test/num_examples=43793, total_duration=2515.122788, train/accuracy=0.988325, train/loss=0.040089, train/mean_average_precision=0.213128, validation/accuracy=0.985565, validation/loss=0.049245, validation/mean_average_precision=0.180865, validation/num_examples=43793
I0204 22:30:33.199504 139858177660672 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.05279168859124184, loss=0.04292334243655205
I0204 22:31:06.293908 139758996317952 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.07935662567615509, loss=0.045454006642103195
I0204 22:31:39.453258 139858177660672 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.06072201952338219, loss=0.038605812937021255
I0204 22:32:12.888419 139758996317952 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.06910568475723267, loss=0.041930221021175385
I0204 22:32:46.071477 139858177660672 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.06130122020840645, loss=0.03956608846783638
I0204 22:33:19.011108 139758996317952 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.06665624678134918, loss=0.042551808059215546
I0204 22:33:51.688845 139858177660672 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.042627085000276566, loss=0.03912673518061638
I0204 22:34:24.505094 139758996317952 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.0409410186111927, loss=0.042777013033628464
I0204 22:34:27.061853 139919816816448 spec.py:321] Evaluating on the training split.
I0204 22:36:26.938547 139919816816448 spec.py:333] Evaluating on the validation split.
I0204 22:36:29.950990 139919816816448 spec.py:349] Evaluating on the test split.
I0204 22:36:33.053698 139919816816448 submission_runner.py:408] Time since start: 2881.25s, 	Step: 5209, 	{'train/accuracy': 0.9886398315429688, 'train/loss': 0.03881854936480522, 'train/mean_average_precision': 0.22067476302031644, 'validation/accuracy': 0.9856414198875427, 'validation/loss': 0.04870504140853882, 'validation/mean_average_precision': 0.1843567487809262, 'validation/num_examples': 43793, 'test/accuracy': 0.9847750663757324, 'test/loss': 0.05137285590171814, 'test/mean_average_precision': 0.18736745437329538, 'test/num_examples': 43793, 'score': 1699.6076345443726, 'total_duration': 2881.247640132904, 'accumulated_submission_time': 1699.6076345443726, 'accumulated_eval_time': 1181.2931609153748, 'accumulated_logging_time': 0.20586609840393066}
I0204 22:36:33.069252 139752480773888 logging_writer.py:48] [5209] accumulated_eval_time=1181.293161, accumulated_logging_time=0.205866, accumulated_submission_time=1699.607635, global_step=5209, preemption_count=0, score=1699.607635, test/accuracy=0.984775, test/loss=0.051373, test/mean_average_precision=0.187367, test/num_examples=43793, total_duration=2881.247640, train/accuracy=0.988640, train/loss=0.038819, train/mean_average_precision=0.220675, validation/accuracy=0.985641, validation/loss=0.048705, validation/mean_average_precision=0.184357, validation/num_examples=43793
I0204 22:37:02.713137 139858186053376 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.045720119029283524, loss=0.04000554233789444
I0204 22:37:34.286835 139752480773888 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.06355057656764984, loss=0.04738892987370491
I0204 22:38:05.755836 139858186053376 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.03522462770342827, loss=0.04261009395122528
I0204 22:38:36.884101 139752480773888 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.05705488845705986, loss=0.04269193485379219
I0204 22:39:08.051805 139858186053376 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.054789792746305466, loss=0.03622361272573471
I0204 22:39:39.405610 139752480773888 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.11291302740573883, loss=0.04131167009472847
I0204 22:40:10.463114 139858186053376 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.032009534537792206, loss=0.038927655667066574
I0204 22:40:33.170695 139919816816448 spec.py:321] Evaluating on the training split.
I0204 22:42:32.646607 139919816816448 spec.py:333] Evaluating on the validation split.
I0204 22:42:35.657968 139919816816448 spec.py:349] Evaluating on the test split.
I0204 22:42:38.628461 139919816816448 submission_runner.py:408] Time since start: 3246.82s, 	Step: 5974, 	{'train/accuracy': 0.9889200329780579, 'train/loss': 0.03803868219256401, 'train/mean_average_precision': 0.24607398812691722, 'validation/accuracy': 0.9857940673828125, 'validation/loss': 0.048042286187410355, 'validation/mean_average_precision': 0.19944095486837685, 'validation/num_examples': 43793, 'test/accuracy': 0.984929621219635, 'test/loss': 0.050627075135707855, 'test/mean_average_precision': 0.20343181880466227, 'test/num_examples': 43793, 'score': 1939.6774501800537, 'total_duration': 3246.82239818573, 'accumulated_submission_time': 1939.6774501800537, 'accumulated_eval_time': 1306.7508709430695, 'accumulated_logging_time': 0.23312973976135254}
I0204 22:42:38.644581 139758987925248 logging_writer.py:48] [5974] accumulated_eval_time=1306.750871, accumulated_logging_time=0.233130, accumulated_submission_time=1939.677450, global_step=5974, preemption_count=0, score=1939.677450, test/accuracy=0.984930, test/loss=0.050627, test/mean_average_precision=0.203432, test/num_examples=43793, total_duration=3246.822398, train/accuracy=0.988920, train/loss=0.038039, train/mean_average_precision=0.246074, validation/accuracy=0.985794, validation/loss=0.048042, validation/mean_average_precision=0.199441, validation/num_examples=43793
I0204 22:42:46.960203 139758996317952 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.05902140960097313, loss=0.03737233579158783
I0204 22:43:17.932943 139758987925248 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.050062764436006546, loss=0.036348454654216766
I0204 22:43:48.643051 139758996317952 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.033746521919965744, loss=0.03938481956720352
I0204 22:44:20.222518 139758987925248 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.028767187148332596, loss=0.03930247575044632
I0204 22:44:52.017683 139758996317952 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.04015635326504707, loss=0.04061874747276306
I0204 22:45:23.716858 139758987925248 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.041659414768218994, loss=0.0401344932615757
I0204 22:45:54.671692 139758996317952 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.030784528702497482, loss=0.03800789266824722
I0204 22:46:25.794542 139758987925248 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.03377675637602806, loss=0.041875798255205154
I0204 22:46:38.745162 139919816816448 spec.py:321] Evaluating on the training split.
I0204 22:48:38.337836 139919816816448 spec.py:333] Evaluating on the validation split.
I0204 22:48:41.360945 139919816816448 spec.py:349] Evaluating on the test split.
I0204 22:48:44.368515 139919816816448 submission_runner.py:408] Time since start: 3612.56s, 	Step: 6743, 	{'train/accuracy': 0.9888412952423096, 'train/loss': 0.03810810670256615, 'train/mean_average_precision': 0.24230055322371255, 'validation/accuracy': 0.9858776926994324, 'validation/loss': 0.04803735390305519, 'validation/mean_average_precision': 0.204004536859809, 'validation/num_examples': 43793, 'test/accuracy': 0.984944760799408, 'test/loss': 0.05091791972517967, 'test/mean_average_precision': 0.2028065330187146, 'test/num_examples': 43793, 'score': 2179.744330406189, 'total_duration': 3612.562463760376, 'accumulated_submission_time': 2179.744330406189, 'accumulated_eval_time': 1432.3741779327393, 'accumulated_logging_time': 0.2616612911224365}
I0204 22:48:44.384695 139752480773888 logging_writer.py:48] [6743] accumulated_eval_time=1432.374178, accumulated_logging_time=0.261661, accumulated_submission_time=2179.744330, global_step=6743, preemption_count=0, score=2179.744330, test/accuracy=0.984945, test/loss=0.050918, test/mean_average_precision=0.202807, test/num_examples=43793, total_duration=3612.562464, train/accuracy=0.988841, train/loss=0.038108, train/mean_average_precision=0.242301, validation/accuracy=0.985878, validation/loss=0.048037, validation/mean_average_precision=0.204005, validation/num_examples=43793
I0204 22:49:02.695672 139858186053376 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.02252996154129505, loss=0.0384615994989872
I0204 22:49:34.060065 139752480773888 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.03518984839320183, loss=0.03958657383918762
I0204 22:50:05.342084 139858186053376 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.032192885875701904, loss=0.039798106998205185
I0204 22:50:36.687896 139752480773888 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.030501794070005417, loss=0.04418535903096199
I0204 22:51:07.774202 139858186053376 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.029285293072462082, loss=0.03851121664047241
I0204 22:51:38.771007 139752480773888 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.03899337351322174, loss=0.03942085802555084
I0204 22:52:09.610799 139858186053376 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.05372146517038345, loss=0.042977601289749146
I0204 22:52:40.291933 139752480773888 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.031862977892160416, loss=0.041869498789310455
I0204 22:52:44.596598 139919816816448 spec.py:321] Evaluating on the training split.
I0204 22:54:43.741658 139919816816448 spec.py:333] Evaluating on the validation split.
I0204 22:54:47.170006 139919816816448 spec.py:349] Evaluating on the test split.
I0204 22:54:50.473971 139919816816448 submission_runner.py:408] Time since start: 3978.67s, 	Step: 7515, 	{'train/accuracy': 0.988990843296051, 'train/loss': 0.03744160756468773, 'train/mean_average_precision': 0.25432717575873776, 'validation/accuracy': 0.9861622452735901, 'validation/loss': 0.046982571482658386, 'validation/mean_average_precision': 0.215558121122398, 'validation/num_examples': 43793, 'test/accuracy': 0.9852067828178406, 'test/loss': 0.04969971254467964, 'test/mean_average_precision': 0.217869939416924, 'test/num_examples': 43793, 'score': 2419.9255475997925, 'total_duration': 3978.6678981781006, 'accumulated_submission_time': 2419.9255475997925, 'accumulated_eval_time': 1558.2514843940735, 'accumulated_logging_time': 0.2884867191314697}
I0204 22:54:50.492167 139758987925248 logging_writer.py:48] [7515] accumulated_eval_time=1558.251484, accumulated_logging_time=0.288487, accumulated_submission_time=2419.925548, global_step=7515, preemption_count=0, score=2419.925548, test/accuracy=0.985207, test/loss=0.049700, test/mean_average_precision=0.217870, test/num_examples=43793, total_duration=3978.667898, train/accuracy=0.988991, train/loss=0.037442, train/mean_average_precision=0.254327, validation/accuracy=0.986162, validation/loss=0.046983, validation/mean_average_precision=0.215558, validation/num_examples=43793
I0204 22:55:19.007229 139758996317952 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.039960674941539764, loss=0.041095070540905
I0204 22:55:50.918169 139758987925248 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.029978379607200623, loss=0.037924304604530334
I0204 22:56:22.751900 139758996317952 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.05045916140079498, loss=0.040044791996479034
I0204 22:56:54.456643 139758987925248 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.035064373165369034, loss=0.03828565403819084
I0204 22:57:26.194809 139758996317952 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.027464071288704872, loss=0.04163217172026634
I0204 22:57:57.619580 139758987925248 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.0242166668176651, loss=0.039952926337718964
I0204 22:58:29.572708 139758996317952 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.03501683101058006, loss=0.036356862634420395
I0204 22:58:50.622663 139919816816448 spec.py:321] Evaluating on the training split.
I0204 23:00:52.360224 139919816816448 spec.py:333] Evaluating on the validation split.
I0204 23:00:55.345551 139919816816448 spec.py:349] Evaluating on the test split.
I0204 23:00:58.302501 139919816816448 submission_runner.py:408] Time since start: 4346.50s, 	Step: 8267, 	{'train/accuracy': 0.9891239404678345, 'train/loss': 0.03680913522839546, 'train/mean_average_precision': 0.2656056616732034, 'validation/accuracy': 0.9862223267555237, 'validation/loss': 0.0464504174888134, 'validation/mean_average_precision': 0.21724025294531527, 'validation/num_examples': 43793, 'test/accuracy': 0.9852614998817444, 'test/loss': 0.04922201484441757, 'test/mean_average_precision': 0.21476680695329606, 'test/num_examples': 43793, 'score': 2660.0192317962646, 'total_duration': 4346.496450185776, 'accumulated_submission_time': 2660.0192317962646, 'accumulated_eval_time': 1685.931292772293, 'accumulated_logging_time': 0.3178873062133789}
I0204 23:00:58.319520 139858177660672 logging_writer.py:48] [8267] accumulated_eval_time=1685.931293, accumulated_logging_time=0.317887, accumulated_submission_time=2660.019232, global_step=8267, preemption_count=0, score=2660.019232, test/accuracy=0.985261, test/loss=0.049222, test/mean_average_precision=0.214767, test/num_examples=43793, total_duration=4346.496450, train/accuracy=0.989124, train/loss=0.036809, train/mean_average_precision=0.265606, validation/accuracy=0.986222, validation/loss=0.046450, validation/mean_average_precision=0.217240, validation/num_examples=43793
I0204 23:01:09.093358 139858186053376 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.02081473357975483, loss=0.039086367934942245
I0204 23:01:40.862587 139858177660672 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.02260412834584713, loss=0.0398918092250824
I0204 23:02:12.169991 139858186053376 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.023051468655467033, loss=0.03768078610301018
I0204 23:02:44.153309 139858177660672 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.02810012176632881, loss=0.037528980523347855
I0204 23:03:15.905360 139858186053376 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.025194989517331123, loss=0.037767328321933746
I0204 23:03:47.596931 139858177660672 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.024913540109992027, loss=0.041052643209695816
I0204 23:04:19.431276 139858186053376 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.02549169398844242, loss=0.040431443601846695
I0204 23:04:50.855067 139858177660672 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.021475300192832947, loss=0.041938964277505875
I0204 23:04:58.456031 139919816816448 spec.py:321] Evaluating on the training split.
I0204 23:07:01.252891 139919816816448 spec.py:333] Evaluating on the validation split.
I0204 23:07:04.363432 139919816816448 spec.py:349] Evaluating on the test split.
I0204 23:07:07.319926 139919816816448 submission_runner.py:408] Time since start: 4715.51s, 	Step: 9025, 	{'train/accuracy': 0.9893156290054321, 'train/loss': 0.036241594702005386, 'train/mean_average_precision': 0.27204220425241077, 'validation/accuracy': 0.9861720204353333, 'validation/loss': 0.04633241519331932, 'validation/mean_average_precision': 0.22725926422197537, 'validation/num_examples': 43793, 'test/accuracy': 0.9852830171585083, 'test/loss': 0.04907473921775818, 'test/mean_average_precision': 0.23377829931046903, 'test/num_examples': 43793, 'score': 2900.125126838684, 'total_duration': 4715.513872861862, 'accumulated_submission_time': 2900.125126838684, 'accumulated_eval_time': 1814.7951426506042, 'accumulated_logging_time': 0.34543848037719727}
I0204 23:07:07.337431 139758987925248 logging_writer.py:48] [9025] accumulated_eval_time=1814.795143, accumulated_logging_time=0.345438, accumulated_submission_time=2900.125127, global_step=9025, preemption_count=0, score=2900.125127, test/accuracy=0.985283, test/loss=0.049075, test/mean_average_precision=0.233778, test/num_examples=43793, total_duration=4715.513873, train/accuracy=0.989316, train/loss=0.036242, train/mean_average_precision=0.272042, validation/accuracy=0.986172, validation/loss=0.046332, validation/mean_average_precision=0.227259, validation/num_examples=43793
I0204 23:07:31.245076 139758996317952 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.022482044994831085, loss=0.03911855071783066
I0204 23:08:03.125652 139758987925248 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.020574597641825676, loss=0.039248328655958176
I0204 23:08:34.841525 139758996317952 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.05030755326151848, loss=0.040269240736961365
I0204 23:09:06.806886 139758987925248 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.024572063237428665, loss=0.03944725915789604
I0204 23:09:38.032195 139758996317952 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.02482205256819725, loss=0.03729677200317383
I0204 23:10:09.752432 139758987925248 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.0332854688167572, loss=0.03294578194618225
I0204 23:10:41.777493 139758996317952 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.033859118819236755, loss=0.03889128938317299
I0204 23:11:07.534831 139919816816448 spec.py:321] Evaluating on the training split.
I0204 23:13:10.618848 139919816816448 spec.py:333] Evaluating on the validation split.
I0204 23:13:13.578522 139919816816448 spec.py:349] Evaluating on the test split.
I0204 23:13:16.491082 139919816816448 submission_runner.py:408] Time since start: 5084.69s, 	Step: 9783, 	{'train/accuracy': 0.9895707368850708, 'train/loss': 0.0354536771774292, 'train/mean_average_precision': 0.2919737580040339, 'validation/accuracy': 0.9863116145133972, 'validation/loss': 0.04579000174999237, 'validation/mean_average_precision': 0.23331101539934118, 'validation/num_examples': 43793, 'test/accuracy': 0.985456109046936, 'test/loss': 0.04847123473882675, 'test/mean_average_precision': 0.22703790808286564, 'test/num_examples': 43793, 'score': 3140.291603088379, 'total_duration': 5084.685031175613, 'accumulated_submission_time': 3140.291603088379, 'accumulated_eval_time': 1943.7513513565063, 'accumulated_logging_time': 0.3736414909362793}
I0204 23:13:16.507929 139858177660672 logging_writer.py:48] [9783] accumulated_eval_time=1943.751351, accumulated_logging_time=0.373641, accumulated_submission_time=3140.291603, global_step=9783, preemption_count=0, score=3140.291603, test/accuracy=0.985456, test/loss=0.048471, test/mean_average_precision=0.227038, test/num_examples=43793, total_duration=5084.685031, train/accuracy=0.989571, train/loss=0.035454, train/mean_average_precision=0.291974, validation/accuracy=0.986312, validation/loss=0.045790, validation/mean_average_precision=0.233311, validation/num_examples=43793
I0204 23:13:22.213276 139858186053376 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.03930895775556564, loss=0.04085785895586014
I0204 23:13:53.524710 139858177660672 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.04270285367965698, loss=0.04107270389795303
I0204 23:14:24.814388 139858186053376 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.030441582202911377, loss=0.03515412658452988
I0204 23:14:56.194701 139858177660672 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.03354216739535332, loss=0.038010966032743454
I0204 23:15:27.722070 139858186053376 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.03307712450623512, loss=0.03787434101104736
I0204 23:15:59.269676 139858177660672 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.0401143915951252, loss=0.041261885315179825
I0204 23:16:30.686170 139858186053376 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.04304569214582443, loss=0.03905492648482323
I0204 23:17:02.123531 139858177660672 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.049610499292612076, loss=0.04264045134186745
I0204 23:17:16.544939 139919816816448 spec.py:321] Evaluating on the training split.
I0204 23:19:13.363941 139919816816448 spec.py:333] Evaluating on the validation split.
I0204 23:19:16.410357 139919816816448 spec.py:349] Evaluating on the test split.
I0204 23:19:19.360221 139919816816448 submission_runner.py:408] Time since start: 5447.55s, 	Step: 10547, 	{'train/accuracy': 0.9897831678390503, 'train/loss': 0.034679215401411057, 'train/mean_average_precision': 0.30023675460463783, 'validation/accuracy': 0.9863839149475098, 'validation/loss': 0.04539710655808449, 'validation/mean_average_precision': 0.23970684941295484, 'validation/num_examples': 43793, 'test/accuracy': 0.9855917096138, 'test/loss': 0.04791358485817909, 'test/mean_average_precision': 0.23332974898083178, 'test/num_examples': 43793, 'score': 3380.29745554924, 'total_duration': 5447.554109573364, 'accumulated_submission_time': 3380.29745554924, 'accumulated_eval_time': 2066.5665271282196, 'accumulated_logging_time': 0.4013993740081787}
I0204 23:19:19.377412 139758987925248 logging_writer.py:48] [10547] accumulated_eval_time=2066.566527, accumulated_logging_time=0.401399, accumulated_submission_time=3380.297456, global_step=10547, preemption_count=0, score=3380.297456, test/accuracy=0.985592, test/loss=0.047914, test/mean_average_precision=0.233330, test/num_examples=43793, total_duration=5447.554110, train/accuracy=0.989783, train/loss=0.034679, train/mean_average_precision=0.300237, validation/accuracy=0.986384, validation/loss=0.045397, validation/mean_average_precision=0.239707, validation/num_examples=43793
I0204 23:19:36.731971 139758996317952 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.019308898597955704, loss=0.035304244607686996
I0204 23:20:07.922128 139758987925248 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.04073197394609451, loss=0.041333574801683426
I0204 23:20:39.438353 139758996317952 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.030875038355588913, loss=0.035420261323451996
I0204 23:21:10.919572 139758987925248 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.03233214095234871, loss=0.03995337337255478
I0204 23:21:42.405514 139758996317952 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.03033822402358055, loss=0.04133819043636322
I0204 23:22:13.656997 139758987925248 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.03339168056845665, loss=0.03735959529876709
I0204 23:22:44.648036 139758996317952 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.024089761078357697, loss=0.03740542754530907
I0204 23:23:15.680085 139758987925248 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.030799567699432373, loss=0.03717143461108208
I0204 23:23:19.363527 139919816816448 spec.py:321] Evaluating on the training split.
I0204 23:25:22.873522 139919816816448 spec.py:333] Evaluating on the validation split.
I0204 23:25:25.907772 139919816816448 spec.py:349] Evaluating on the test split.
I0204 23:25:28.893367 139919816816448 submission_runner.py:408] Time since start: 5817.09s, 	Step: 11313, 	{'train/accuracy': 0.9898716807365417, 'train/loss': 0.03416074067354202, 'train/mean_average_precision': 0.32691572470335784, 'validation/accuracy': 0.9863802194595337, 'validation/loss': 0.045297037810087204, 'validation/mean_average_precision': 0.24094424710127144, 'validation/num_examples': 43793, 'test/accuracy': 0.9855576157569885, 'test/loss': 0.048182737082242966, 'test/mean_average_precision': 0.2386666619664352, 'test/num_examples': 43793, 'score': 3620.2503366470337, 'total_duration': 5817.087311029434, 'accumulated_submission_time': 3620.2503366470337, 'accumulated_eval_time': 2196.096314430237, 'accumulated_logging_time': 0.4317901134490967}
I0204 23:25:28.910468 139752480773888 logging_writer.py:48] [11313] accumulated_eval_time=2196.096314, accumulated_logging_time=0.431790, accumulated_submission_time=3620.250337, global_step=11313, preemption_count=0, score=3620.250337, test/accuracy=0.985558, test/loss=0.048183, test/mean_average_precision=0.238667, test/num_examples=43793, total_duration=5817.087311, train/accuracy=0.989872, train/loss=0.034161, train/mean_average_precision=0.326916, validation/accuracy=0.986380, validation/loss=0.045297, validation/mean_average_precision=0.240944, validation/num_examples=43793
I0204 23:25:58.586423 139858186053376 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.02455422841012478, loss=0.034808795899152756
I0204 23:26:30.229627 139752480773888 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.025374477729201317, loss=0.03754706680774689
I0204 23:27:01.679148 139858186053376 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.027101971209049225, loss=0.03723490238189697
I0204 23:27:32.915183 139752480773888 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.060947198420763016, loss=0.03741176426410675
I0204 23:28:04.739040 139858186053376 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.039455123245716095, loss=0.03485943377017975
I0204 23:28:36.724414 139752480773888 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.029674159362912178, loss=0.03900901973247528
I0204 23:29:08.440538 139858186053376 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.030489709228277206, loss=0.03945522382855415
I0204 23:29:29.164320 139919816816448 spec.py:321] Evaluating on the training split.
I0204 23:31:32.516820 139919816816448 spec.py:333] Evaluating on the validation split.
I0204 23:31:35.525956 139919816816448 spec.py:349] Evaluating on the test split.
I0204 23:31:38.462982 139919816816448 submission_runner.py:408] Time since start: 6186.66s, 	Step: 12067, 	{'train/accuracy': 0.9901003241539001, 'train/loss': 0.033180154860019684, 'train/mean_average_precision': 0.3477610696164991, 'validation/accuracy': 0.986553966999054, 'validation/loss': 0.04491068050265312, 'validation/mean_average_precision': 0.2418638094390883, 'validation/num_examples': 43793, 'test/accuracy': 0.9856814742088318, 'test/loss': 0.04764633998274803, 'test/mean_average_precision': 0.24118049465505706, 'test/num_examples': 43793, 'score': 3860.4717514514923, 'total_duration': 6186.656934499741, 'accumulated_submission_time': 3860.4717514514923, 'accumulated_eval_time': 2325.3949341773987, 'accumulated_logging_time': 0.46141505241394043}
I0204 23:31:38.480445 139752296675072 logging_writer.py:48] [12067] accumulated_eval_time=2325.394934, accumulated_logging_time=0.461415, accumulated_submission_time=3860.471751, global_step=12067, preemption_count=0, score=3860.471751, test/accuracy=0.985681, test/loss=0.047646, test/mean_average_precision=0.241180, test/num_examples=43793, total_duration=6186.656934, train/accuracy=0.990100, train/loss=0.033180, train/mean_average_precision=0.347761, validation/accuracy=0.986554, validation/loss=0.044911, validation/mean_average_precision=0.241864, validation/num_examples=43793
I0204 23:31:50.253622 139758996317952 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.03286120295524597, loss=0.038676511496305466
I0204 23:32:22.849288 139752296675072 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.05671842396259308, loss=0.03699906915426254
I0204 23:32:54.248203 139758996317952 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.05121176317334175, loss=0.034429267048835754
I0204 23:33:26.114944 139752296675072 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.045121364295482635, loss=0.033312711864709854
I0204 23:33:57.963654 139758996317952 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.03326268866658211, loss=0.0355837345123291
I0204 23:34:29.653723 139752296675072 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.051872093230485916, loss=0.040356773883104324
I0204 23:35:01.306143 139758996317952 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.043566539883613586, loss=0.0367204025387764
I0204 23:35:33.111720 139752296675072 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.044012416154146194, loss=0.037849269807338715
I0204 23:35:38.463519 139919816816448 spec.py:321] Evaluating on the training split.
I0204 23:37:41.771238 139919816816448 spec.py:333] Evaluating on the validation split.
I0204 23:37:44.780960 139919816816448 spec.py:349] Evaluating on the test split.
I0204 23:37:47.699583 139919816816448 submission_runner.py:408] Time since start: 6555.89s, 	Step: 12818, 	{'train/accuracy': 0.9901209473609924, 'train/loss': 0.03266278654336929, 'train/mean_average_precision': 0.3559880776898775, 'validation/accuracy': 0.9866014719009399, 'validation/loss': 0.04502946510910988, 'validation/mean_average_precision': 0.2545477236240031, 'validation/num_examples': 43793, 'test/accuracy': 0.985745906829834, 'test/loss': 0.04778912663459778, 'test/mean_average_precision': 0.2443888640165271, 'test/num_examples': 43793, 'score': 4100.424854040146, 'total_duration': 6555.8935306072235, 'accumulated_submission_time': 4100.424854040146, 'accumulated_eval_time': 2454.6309485435486, 'accumulated_logging_time': 0.4894673824310303}
I0204 23:37:47.717321 139758987925248 logging_writer.py:48] [12818] accumulated_eval_time=2454.630949, accumulated_logging_time=0.489467, accumulated_submission_time=4100.424854, global_step=12818, preemption_count=0, score=4100.424854, test/accuracy=0.985746, test/loss=0.047789, test/mean_average_precision=0.244389, test/num_examples=43793, total_duration=6555.893531, train/accuracy=0.990121, train/loss=0.032663, train/mean_average_precision=0.355988, validation/accuracy=0.986601, validation/loss=0.045029, validation/mean_average_precision=0.254548, validation/num_examples=43793
I0204 23:38:14.640806 139858186053376 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.028719890862703323, loss=0.033619511872529984
I0204 23:38:46.834538 139758987925248 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.04341989755630493, loss=0.03432855010032654
I0204 23:39:19.000885 139858186053376 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.052617255598306656, loss=0.04054475203156471
I0204 23:39:50.610026 139758987925248 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.05087239667773247, loss=0.034808628261089325
I0204 23:40:22.412137 139858186053376 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.03500017896294594, loss=0.034706179052591324
I0204 23:40:53.882342 139758987925248 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.08149760961532593, loss=0.04138774424791336
I0204 23:41:25.923815 139858186053376 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.044211454689502716, loss=0.03167077526450157
I0204 23:41:47.742043 139919816816448 spec.py:321] Evaluating on the training split.
I0204 23:43:52.039582 139919816816448 spec.py:333] Evaluating on the validation split.
I0204 23:43:55.014220 139919816816448 spec.py:349] Evaluating on the test split.
I0204 23:43:58.024769 139919816816448 submission_runner.py:408] Time since start: 6926.22s, 	Step: 13570, 	{'train/accuracy': 0.990494430065155, 'train/loss': 0.03165928274393082, 'train/mean_average_precision': 0.38618576851056285, 'validation/accuracy': 0.9867119193077087, 'validation/loss': 0.04467901587486267, 'validation/mean_average_precision': 0.25529698262849987, 'validation/num_examples': 43793, 'test/accuracy': 0.9858596324920654, 'test/loss': 0.04726462811231613, 'test/mean_average_precision': 0.25196132531218934, 'test/num_examples': 43793, 'score': 4340.415853261948, 'total_duration': 6926.2187123298645, 'accumulated_submission_time': 4340.415853261948, 'accumulated_eval_time': 2584.913625717163, 'accumulated_logging_time': 0.519852876663208}
I0204 23:43:58.043327 139752296675072 logging_writer.py:48] [13570] accumulated_eval_time=2584.913626, accumulated_logging_time=0.519853, accumulated_submission_time=4340.415853, global_step=13570, preemption_count=0, score=4340.415853, test/accuracy=0.985860, test/loss=0.047265, test/mean_average_precision=0.251961, test/num_examples=43793, total_duration=6926.218712, train/accuracy=0.990494, train/loss=0.031659, train/mean_average_precision=0.386186, validation/accuracy=0.986712, validation/loss=0.044679, validation/mean_average_precision=0.255297, validation/num_examples=43793
I0204 23:44:08.030872 139758996317952 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.032666757702827454, loss=0.0347861722111702
I0204 23:44:39.752309 139752296675072 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.046340759843587875, loss=0.03855004534125328
I0204 23:45:11.728538 139758996317952 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.03454731032252312, loss=0.038178615272045135
I0204 23:45:43.528859 139752296675072 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.04921811819076538, loss=0.035928260535001755
I0204 23:46:15.140109 139758996317952 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.050231125205755234, loss=0.0355137400329113
I0204 23:46:46.810248 139752296675072 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.036499544978141785, loss=0.03519754484295845
I0204 23:47:18.505747 139758996317952 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.06308149546384811, loss=0.033420905470848083
I0204 23:47:50.271424 139752296675072 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.044942084699869156, loss=0.03453860431909561
I0204 23:47:58.082400 139919816816448 spec.py:321] Evaluating on the training split.
I0204 23:50:01.981086 139919816816448 spec.py:333] Evaluating on the validation split.
I0204 23:50:04.971159 139919816816448 spec.py:349] Evaluating on the test split.
I0204 23:50:07.937203 139919816816448 submission_runner.py:408] Time since start: 7296.13s, 	Step: 14326, 	{'train/accuracy': 0.9905635118484497, 'train/loss': 0.03131844848394394, 'train/mean_average_precision': 0.37796976515685243, 'validation/accuracy': 0.9866960644721985, 'validation/loss': 0.044397562742233276, 'validation/mean_average_precision': 0.2605003860303854, 'validation/num_examples': 43793, 'test/accuracy': 0.985908031463623, 'test/loss': 0.04722025245428085, 'test/mean_average_precision': 0.25244056870739706, 'test/num_examples': 43793, 'score': 4580.423997163773, 'total_duration': 7296.1311457157135, 'accumulated_submission_time': 4580.423997163773, 'accumulated_eval_time': 2714.7683775424957, 'accumulated_logging_time': 0.5493123531341553}
I0204 23:50:07.955608 139752480773888 logging_writer.py:48] [14326] accumulated_eval_time=2714.768378, accumulated_logging_time=0.549312, accumulated_submission_time=4580.423997, global_step=14326, preemption_count=0, score=4580.423997, test/accuracy=0.985908, test/loss=0.047220, test/mean_average_precision=0.252441, test/num_examples=43793, total_duration=7296.131146, train/accuracy=0.990564, train/loss=0.031318, train/mean_average_precision=0.377970, validation/accuracy=0.986696, validation/loss=0.044398, validation/mean_average_precision=0.260500, validation/num_examples=43793
I0204 23:50:31.665694 139758987925248 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.04643712565302849, loss=0.037336669862270355
I0204 23:51:03.155015 139752480773888 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.07393810153007507, loss=0.03297155722975731
I0204 23:51:35.055286 139758987925248 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.05247356742620468, loss=0.03534727916121483
I0204 23:52:06.466617 139752480773888 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.04169977456331253, loss=0.033889152109622955
I0204 23:52:38.001625 139758987925248 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.059531137347221375, loss=0.037483759224414825
I0204 23:53:09.530205 139752480773888 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.04497390240430832, loss=0.03888281062245369
I0204 23:53:40.758072 139758987925248 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.046161435544490814, loss=0.03728639706969261
I0204 23:54:08.158421 139919816816448 spec.py:321] Evaluating on the training split.
I0204 23:56:10.540373 139919816816448 spec.py:333] Evaluating on the validation split.
I0204 23:56:13.548774 139919816816448 spec.py:349] Evaluating on the test split.
I0204 23:56:16.529350 139919816816448 submission_runner.py:408] Time since start: 7664.72s, 	Step: 15088, 	{'train/accuracy': 0.9904943704605103, 'train/loss': 0.03144986182451248, 'train/mean_average_precision': 0.39217741118649246, 'validation/accuracy': 0.9867439866065979, 'validation/loss': 0.044363733381032944, 'validation/mean_average_precision': 0.2609996468968531, 'validation/num_examples': 43793, 'test/accuracy': 0.9859038591384888, 'test/loss': 0.047209352254867554, 'test/mean_average_precision': 0.24937429703177308, 'test/num_examples': 43793, 'score': 4820.594982147217, 'total_duration': 7664.723286628723, 'accumulated_submission_time': 4820.594982147217, 'accumulated_eval_time': 2843.13925409317, 'accumulated_logging_time': 0.5789437294006348}
I0204 23:56:16.547487 139752296675072 logging_writer.py:48] [15088] accumulated_eval_time=2843.139254, accumulated_logging_time=0.578944, accumulated_submission_time=4820.594982, global_step=15088, preemption_count=0, score=4820.594982, test/accuracy=0.985904, test/loss=0.047209, test/mean_average_precision=0.249374, test/num_examples=43793, total_duration=7664.723287, train/accuracy=0.990494, train/loss=0.031450, train/mean_average_precision=0.392177, validation/accuracy=0.986744, validation/loss=0.044364, validation/mean_average_precision=0.261000, validation/num_examples=43793
I0204 23:56:20.741062 139758996317952 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.04376329854130745, loss=0.03586358577013016
I0204 23:56:52.563325 139752296675072 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.05594048276543617, loss=0.03521580621600151
I0204 23:57:23.934339 139758996317952 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.045801445841789246, loss=0.036887627094984055
I0204 23:57:55.520027 139752296675072 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.05926455184817314, loss=0.038570232689380646
I0204 23:58:27.199810 139758996317952 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.04604901000857353, loss=0.03556060418486595
I0204 23:58:58.590270 139752296675072 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.06039494276046753, loss=0.03851413354277611
I0204 23:59:30.383150 139758996317952 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.045917611569166183, loss=0.03451569750905037
I0205 00:00:01.734484 139752296675072 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.04677535593509674, loss=0.03359418734908104
I0205 00:00:16.639794 139919816816448 spec.py:321] Evaluating on the training split.
I0205 00:02:18.178369 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 00:02:21.172476 139919816816448 spec.py:349] Evaluating on the test split.
I0205 00:02:24.144916 139919816816448 submission_runner.py:408] Time since start: 8032.34s, 	Step: 15848, 	{'train/accuracy': 0.990463137626648, 'train/loss': 0.031662777066230774, 'train/mean_average_precision': 0.3854086649106583, 'validation/accuracy': 0.9866436719894409, 'validation/loss': 0.044806282967329025, 'validation/mean_average_precision': 0.2567880837321566, 'validation/num_examples': 43793, 'test/accuracy': 0.9858655333518982, 'test/loss': 0.047489818185567856, 'test/mean_average_precision': 0.24913231873415576, 'test/num_examples': 43793, 'score': 5060.655467748642, 'total_duration': 8032.338857412338, 'accumulated_submission_time': 5060.655467748642, 'accumulated_eval_time': 2970.6443254947662, 'accumulated_logging_time': 0.6084580421447754}
I0205 00:02:24.164013 139752480773888 logging_writer.py:48] [15848] accumulated_eval_time=2970.644325, accumulated_logging_time=0.608458, accumulated_submission_time=5060.655468, global_step=15848, preemption_count=0, score=5060.655468, test/accuracy=0.985866, test/loss=0.047490, test/mean_average_precision=0.249132, test/num_examples=43793, total_duration=8032.338857, train/accuracy=0.990463, train/loss=0.031663, train/mean_average_precision=0.385409, validation/accuracy=0.986644, validation/loss=0.044806, validation/mean_average_precision=0.256788, validation/num_examples=43793
I0205 00:02:40.890374 139858186053376 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.054409902542829514, loss=0.03552989661693573
I0205 00:03:12.299670 139752480773888 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.052090901881456375, loss=0.03614203259348869
I0205 00:03:43.563592 139858186053376 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.03935905173420906, loss=0.0349704846739769
I0205 00:04:14.800482 139752480773888 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.04755858704447746, loss=0.03478797525167465
I0205 00:04:46.430728 139858186053376 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.057446882128715515, loss=0.03218141198158264
I0205 00:05:17.789994 139752480773888 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.0477060042321682, loss=0.034893158823251724
I0205 00:05:49.442306 139858186053376 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.08199770003557205, loss=0.03341994434595108
I0205 00:06:21.346695 139752480773888 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.05678944289684296, loss=0.03537542745471001
I0205 00:06:24.177992 139919816816448 spec.py:321] Evaluating on the training split.
I0205 00:08:25.907167 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 00:08:28.907023 139919816816448 spec.py:349] Evaluating on the test split.
I0205 00:08:31.928727 139919816816448 submission_runner.py:408] Time since start: 8400.12s, 	Step: 16610, 	{'train/accuracy': 0.9904116988182068, 'train/loss': 0.031754765659570694, 'train/mean_average_precision': 0.37209461998537924, 'validation/accuracy': 0.9867720007896423, 'validation/loss': 0.0442337729036808, 'validation/mean_average_precision': 0.2636909103653799, 'validation/num_examples': 43793, 'test/accuracy': 0.9859662055969238, 'test/loss': 0.04691949486732483, 'test/mean_average_precision': 0.2570484000552654, 'test/num_examples': 43793, 'score': 5300.637059926987, 'total_duration': 8400.122673511505, 'accumulated_submission_time': 5300.637059926987, 'accumulated_eval_time': 3098.395025253296, 'accumulated_logging_time': 0.6386611461639404}
I0205 00:08:31.947467 139752296675072 logging_writer.py:48] [16610] accumulated_eval_time=3098.395025, accumulated_logging_time=0.638661, accumulated_submission_time=5300.637060, global_step=16610, preemption_count=0, score=5300.637060, test/accuracy=0.985966, test/loss=0.046919, test/mean_average_precision=0.257048, test/num_examples=43793, total_duration=8400.122674, train/accuracy=0.990412, train/loss=0.031755, train/mean_average_precision=0.372095, validation/accuracy=0.986772, validation/loss=0.044234, validation/mean_average_precision=0.263691, validation/num_examples=43793
I0205 00:09:01.110320 139758987925248 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.0617201142013073, loss=0.036975909024477005
I0205 00:09:32.942394 139752296675072 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.07641420513391495, loss=0.03642575442790985
I0205 00:10:04.867835 139758987925248 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.09382370114326477, loss=0.03727424144744873
I0205 00:10:36.765146 139752296675072 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.05231012403964996, loss=0.0371115542948246
I0205 00:11:08.736305 139758987925248 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.08285806328058243, loss=0.035781361162662506
I0205 00:11:41.079488 139752296675072 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.06300140172243118, loss=0.036468327045440674
I0205 00:12:13.191505 139758987925248 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.07349485903978348, loss=0.032228171825408936
I0205 00:12:32.063817 139919816816448 spec.py:321] Evaluating on the training split.
I0205 00:14:35.359057 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 00:14:38.471115 139919816816448 spec.py:349] Evaluating on the test split.
I0205 00:14:41.569327 139919816816448 submission_runner.py:408] Time since start: 8769.76s, 	Step: 17359, 	{'train/accuracy': 0.9904099106788635, 'train/loss': 0.03174610808491707, 'train/mean_average_precision': 0.37712096185811106, 'validation/accuracy': 0.9866794347763062, 'validation/loss': 0.04448701813817024, 'validation/mean_average_precision': 0.25674223867466034, 'validation/num_examples': 43793, 'test/accuracy': 0.9858945608139038, 'test/loss': 0.047203127294778824, 'test/mean_average_precision': 0.2509193630611767, 'test/num_examples': 43793, 'score': 5540.721517562866, 'total_duration': 8769.763256072998, 'accumulated_submission_time': 5540.721517562866, 'accumulated_eval_time': 3227.900470495224, 'accumulated_logging_time': 0.668442964553833}
I0205 00:14:41.587535 139752480773888 logging_writer.py:48] [17359] accumulated_eval_time=3227.900470, accumulated_logging_time=0.668443, accumulated_submission_time=5540.721518, global_step=17359, preemption_count=0, score=5540.721518, test/accuracy=0.985895, test/loss=0.047203, test/mean_average_precision=0.250919, test/num_examples=43793, total_duration=8769.763256, train/accuracy=0.990410, train/loss=0.031746, train/mean_average_precision=0.377121, validation/accuracy=0.986679, validation/loss=0.044487, validation/mean_average_precision=0.256742, validation/num_examples=43793
I0205 00:14:55.273907 139858186053376 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.0811636745929718, loss=0.03631031513214111
I0205 00:15:27.203275 139752480773888 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.08435262739658356, loss=0.03838728740811348
I0205 00:15:58.812087 139858186053376 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.05655306205153465, loss=0.03460254147648811
I0205 00:16:30.544731 139752480773888 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.05296621099114418, loss=0.034661345183849335
I0205 00:17:02.079309 139858186053376 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.07938159257173538, loss=0.03728409856557846
I0205 00:17:34.125174 139752480773888 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.09173659235239029, loss=0.037194281816482544
I0205 00:18:06.138674 139858186053376 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.097132109105587, loss=0.03730454295873642
I0205 00:18:37.555836 139752480773888 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.08473099768161774, loss=0.03632993996143341
I0205 00:18:41.708222 139919816816448 spec.py:321] Evaluating on the training split.
I0205 00:20:46.365139 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 00:20:49.713772 139919816816448 spec.py:349] Evaluating on the test split.
I0205 00:20:52.927381 139919816816448 submission_runner.py:408] Time since start: 9141.12s, 	Step: 18114, 	{'train/accuracy': 0.9906277656555176, 'train/loss': 0.0312679186463356, 'train/mean_average_precision': 0.38930200731543024, 'validation/accuracy': 0.9866157174110413, 'validation/loss': 0.04464775696396828, 'validation/mean_average_precision': 0.26031631678613026, 'validation/num_examples': 43793, 'test/accuracy': 0.9858099222183228, 'test/loss': 0.04745831713080406, 'test/mean_average_precision': 0.2474147599552491, 'test/num_examples': 43793, 'score': 5780.809211492538, 'total_duration': 9141.121300935745, 'accumulated_submission_time': 5780.809211492538, 'accumulated_eval_time': 3359.1195571422577, 'accumulated_logging_time': 0.6974725723266602}
I0205 00:20:52.948426 139758987925248 logging_writer.py:48] [18114] accumulated_eval_time=3359.119557, accumulated_logging_time=0.697473, accumulated_submission_time=5780.809211, global_step=18114, preemption_count=0, score=5780.809211, test/accuracy=0.985810, test/loss=0.047458, test/mean_average_precision=0.247415, test/num_examples=43793, total_duration=9141.121301, train/accuracy=0.990628, train/loss=0.031268, train/mean_average_precision=0.389302, validation/accuracy=0.986616, validation/loss=0.044648, validation/mean_average_precision=0.260316, validation/num_examples=43793
I0205 00:21:20.830593 139758996317952 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.061599940061569214, loss=0.0350174754858017
I0205 00:21:52.764676 139758987925248 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.11014663428068161, loss=0.035693880170583725
I0205 00:22:25.111999 139758996317952 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.0843166634440422, loss=0.036400265991687775
I0205 00:22:57.075373 139758987925248 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.05544234439730644, loss=0.03309161216020584
I0205 00:23:29.285259 139758996317952 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.05431133881211281, loss=0.03576217591762543
I0205 00:24:01.620774 139758987925248 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.0671699047088623, loss=0.0309382863342762
I0205 00:24:33.881064 139758996317952 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.05157894268631935, loss=0.03295848146080971
I0205 00:24:53.110031 139919816816448 spec.py:321] Evaluating on the training split.
I0205 00:26:57.037950 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 00:27:00.043410 139919816816448 spec.py:349] Evaluating on the test split.
I0205 00:27:03.114359 139919816816448 submission_runner.py:408] Time since start: 9511.31s, 	Step: 18861, 	{'train/accuracy': 0.990624189376831, 'train/loss': 0.030974138528108597, 'train/mean_average_precision': 0.39346113920306736, 'validation/accuracy': 0.9867175817489624, 'validation/loss': 0.04483654722571373, 'validation/mean_average_precision': 0.2644804500767574, 'validation/num_examples': 43793, 'test/accuracy': 0.9858949780464172, 'test/loss': 0.04775327071547508, 'test/mean_average_precision': 0.24872658993641045, 'test/num_examples': 43793, 'score': 6020.937862634659, 'total_duration': 9511.308304786682, 'accumulated_submission_time': 6020.937862634659, 'accumulated_eval_time': 3489.12383890152, 'accumulated_logging_time': 0.7305166721343994}
I0205 00:27:03.132977 139752296675072 logging_writer.py:48] [18861] accumulated_eval_time=3489.123839, accumulated_logging_time=0.730517, accumulated_submission_time=6020.937863, global_step=18861, preemption_count=0, score=6020.937863, test/accuracy=0.985895, test/loss=0.047753, test/mean_average_precision=0.248727, test/num_examples=43793, total_duration=9511.308305, train/accuracy=0.990624, train/loss=0.030974, train/mean_average_precision=0.393461, validation/accuracy=0.986718, validation/loss=0.044837, validation/mean_average_precision=0.264480, validation/num_examples=43793
I0205 00:27:16.418442 139752480773888 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.06268821656703949, loss=0.03484129160642624
I0205 00:27:48.134008 139752296675072 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.07419955730438232, loss=0.03731667250394821
I0205 00:28:20.089920 139752480773888 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.06812354922294617, loss=0.03427135571837425
I0205 00:28:51.785243 139752296675072 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.0841948390007019, loss=0.031815532594919205
I0205 00:29:23.533851 139752480773888 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.07028955966234207, loss=0.0344894714653492
I0205 00:29:55.358085 139752296675072 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.05871855095028877, loss=0.033195484429597855
I0205 00:30:26.880444 139752480773888 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.06045733764767647, loss=0.031983762979507446
I0205 00:30:58.089059 139752296675072 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.1344577819108963, loss=0.03655058145523071
I0205 00:31:03.155816 139919816816448 spec.py:321] Evaluating on the training split.
I0205 00:33:06.481445 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 00:33:09.543098 139919816816448 spec.py:349] Evaluating on the test split.
I0205 00:33:12.536936 139919816816448 submission_runner.py:408] Time since start: 9880.73s, 	Step: 19617, 	{'train/accuracy': 0.9908477663993835, 'train/loss': 0.030303241685032845, 'train/mean_average_precision': 0.4078293290951283, 'validation/accuracy': 0.9866339564323425, 'validation/loss': 0.044566091150045395, 'validation/mean_average_precision': 0.2658152452066989, 'validation/num_examples': 43793, 'test/accuracy': 0.9857138991355896, 'test/loss': 0.04753638803958893, 'test/mean_average_precision': 0.24737294701466878, 'test/num_examples': 43793, 'score': 6260.9293966293335, 'total_duration': 9880.730882644653, 'accumulated_submission_time': 6260.9293966293335, 'accumulated_eval_time': 3618.5049121379852, 'accumulated_logging_time': 0.7599573135375977}
I0205 00:33:12.555753 139758996317952 logging_writer.py:48] [19617] accumulated_eval_time=3618.504912, accumulated_logging_time=0.759957, accumulated_submission_time=6260.929397, global_step=19617, preemption_count=0, score=6260.929397, test/accuracy=0.985714, test/loss=0.047536, test/mean_average_precision=0.247373, test/num_examples=43793, total_duration=9880.730883, train/accuracy=0.990848, train/loss=0.030303, train/mean_average_precision=0.407829, validation/accuracy=0.986634, validation/loss=0.044566, validation/mean_average_precision=0.265815, validation/num_examples=43793
I0205 00:33:38.943082 139858186053376 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.07229150086641312, loss=0.03479411080479622
I0205 00:34:10.526959 139758996317952 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.07900839298963547, loss=0.03739162161946297
I0205 00:34:42.288805 139858186053376 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.06618289649486542, loss=0.032461266964673996
I0205 00:35:13.797745 139758996317952 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.08453524857759476, loss=0.03521987050771713
I0205 00:35:45.399623 139858186053376 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.07101946324110031, loss=0.03247920423746109
I0205 00:36:16.950306 139758996317952 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.05111264809966087, loss=0.03243432939052582
I0205 00:36:48.401125 139858186053376 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.07296528667211533, loss=0.03284597024321556
I0205 00:37:12.661524 139919816816448 spec.py:321] Evaluating on the training split.
I0205 00:39:13.394833 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 00:39:16.696275 139919816816448 spec.py:349] Evaluating on the test split.
I0205 00:39:20.028113 139919816816448 submission_runner.py:408] Time since start: 10248.22s, 	Step: 20378, 	{'train/accuracy': 0.9909874200820923, 'train/loss': 0.02956818975508213, 'train/mean_average_precision': 0.43345511416517735, 'validation/accuracy': 0.986757755279541, 'validation/loss': 0.04421599954366684, 'validation/mean_average_precision': 0.26658950567330963, 'validation/num_examples': 43793, 'test/accuracy': 0.985908031463623, 'test/loss': 0.04710817337036133, 'test/mean_average_precision': 0.2536225909294436, 'test/num_examples': 43793, 'score': 6501.003589630127, 'total_duration': 10248.222026586533, 'accumulated_submission_time': 6501.003589630127, 'accumulated_eval_time': 3745.8714208602905, 'accumulated_logging_time': 0.7897212505340576}
I0205 00:39:20.051828 139752296675072 logging_writer.py:48] [20378] accumulated_eval_time=3745.871421, accumulated_logging_time=0.789721, accumulated_submission_time=6501.003590, global_step=20378, preemption_count=0, score=6501.003590, test/accuracy=0.985908, test/loss=0.047108, test/mean_average_precision=0.253623, test/num_examples=43793, total_duration=10248.222027, train/accuracy=0.990987, train/loss=0.029568, train/mean_average_precision=0.433455, validation/accuracy=0.986758, validation/loss=0.044216, validation/mean_average_precision=0.266590, validation/num_examples=43793
I0205 00:39:27.612099 139752480773888 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.06352526694536209, loss=0.0329250693321228
I0205 00:39:59.629431 139752296675072 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.08524452894926071, loss=0.03604263439774513
I0205 00:40:31.960184 139752480773888 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.09461439400911331, loss=0.029927244409918785
I0205 00:41:04.167506 139752296675072 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.1051187738776207, loss=0.0357745997607708
I0205 00:41:35.874350 139752480773888 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.08484981209039688, loss=0.03474384918808937
I0205 00:42:07.485607 139752296675072 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.06605669111013412, loss=0.033036861568689346
I0205 00:42:39.030636 139752480773888 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.059876639395952225, loss=0.03550256788730621
I0205 00:43:10.727215 139752296675072 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.0726219043135643, loss=0.032562024891376495
I0205 00:43:20.332293 139919816816448 spec.py:321] Evaluating on the training split.
I0205 00:45:22.833494 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 00:45:26.002095 139919816816448 spec.py:349] Evaluating on the test split.
I0205 00:45:29.123048 139919816816448 submission_runner.py:408] Time since start: 10617.32s, 	Step: 21131, 	{'train/accuracy': 0.9910106658935547, 'train/loss': 0.029451150447130203, 'train/mean_average_precision': 0.42636138282913516, 'validation/accuracy': 0.9867987632751465, 'validation/loss': 0.044612444937229156, 'validation/mean_average_precision': 0.26530508454363316, 'validation/num_examples': 43793, 'test/accuracy': 0.9859185814857483, 'test/loss': 0.04750733822584152, 'test/mean_average_precision': 0.2502607926465779, 'test/num_examples': 43793, 'score': 6741.249910831451, 'total_duration': 10617.316989421844, 'accumulated_submission_time': 6741.249910831451, 'accumulated_eval_time': 3874.662131547928, 'accumulated_logging_time': 0.8251934051513672}
I0205 00:45:29.142115 139758987925248 logging_writer.py:48] [21131] accumulated_eval_time=3874.662132, accumulated_logging_time=0.825193, accumulated_submission_time=6741.249911, global_step=21131, preemption_count=0, score=6741.249911, test/accuracy=0.985919, test/loss=0.047507, test/mean_average_precision=0.250261, test/num_examples=43793, total_duration=10617.316989, train/accuracy=0.991011, train/loss=0.029451, train/mean_average_precision=0.426361, validation/accuracy=0.986799, validation/loss=0.044612, validation/mean_average_precision=0.265305, validation/num_examples=43793
I0205 00:45:51.585196 139858186053376 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.06590268760919571, loss=0.03290686383843422
I0205 00:46:23.012577 139758987925248 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.10633070021867752, loss=0.0341862216591835
I0205 00:46:54.768109 139858186053376 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.05723162740468979, loss=0.03491407632827759
I0205 00:47:26.573733 139758987925248 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.13959407806396484, loss=0.03253231197595596
I0205 00:47:58.186896 139858186053376 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.060612697154283524, loss=0.036379292607307434
I0205 00:48:29.744556 139758987925248 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.08834483474493027, loss=0.03331293910741806
I0205 00:49:02.047815 139858186053376 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.06116832420229912, loss=0.0338144488632679
I0205 00:49:29.256722 139919816816448 spec.py:321] Evaluating on the training split.
I0205 00:51:33.510030 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 00:51:36.511376 139919816816448 spec.py:349] Evaluating on the test split.
I0205 00:51:39.495941 139919816816448 submission_runner.py:408] Time since start: 10987.69s, 	Step: 21888, 	{'train/accuracy': 0.9911861419677734, 'train/loss': 0.028838839381933212, 'train/mean_average_precision': 0.44508326605191784, 'validation/accuracy': 0.9867784976959229, 'validation/loss': 0.04432143643498421, 'validation/mean_average_precision': 0.27322503824153394, 'validation/num_examples': 43793, 'test/accuracy': 0.985975444316864, 'test/loss': 0.04727840796113014, 'test/mean_average_precision': 0.25190238978307966, 'test/num_examples': 43793, 'score': 6981.331892490387, 'total_duration': 10987.689888238907, 'accumulated_submission_time': 6981.331892490387, 'accumulated_eval_time': 4004.901304244995, 'accumulated_logging_time': 0.8566954135894775}
I0205 00:51:39.515172 139752296675072 logging_writer.py:48] [21888] accumulated_eval_time=4004.901304, accumulated_logging_time=0.856695, accumulated_submission_time=6981.331892, global_step=21888, preemption_count=0, score=6981.331892, test/accuracy=0.985975, test/loss=0.047278, test/mean_average_precision=0.251902, test/num_examples=43793, total_duration=10987.689888, train/accuracy=0.991186, train/loss=0.028839, train/mean_average_precision=0.445083, validation/accuracy=0.986778, validation/loss=0.044321, validation/mean_average_precision=0.273225, validation/num_examples=43793
I0205 00:51:43.644656 139752480773888 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.06429324299097061, loss=0.034287285059690475
I0205 00:52:15.372384 139752296675072 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.10354757308959961, loss=0.03314557671546936
I0205 00:52:46.650038 139752480773888 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.06962993741035461, loss=0.03151541203260422
I0205 00:53:18.225543 139752296675072 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.08362928032875061, loss=0.036549389362335205
I0205 00:53:49.748936 139752480773888 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.07424795627593994, loss=0.03441222757101059
I0205 00:54:21.220272 139752296675072 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.14770536124706268, loss=0.03549456223845482
I0205 00:54:52.550987 139752480773888 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.08081960678100586, loss=0.03154413402080536
I0205 00:55:24.004576 139752296675072 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.07519369572401047, loss=0.037122104316949844
I0205 00:55:39.652549 139919816816448 spec.py:321] Evaluating on the training split.
I0205 00:57:41.174445 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 00:57:44.208338 139919816816448 spec.py:349] Evaluating on the test split.
I0205 00:57:47.196138 139919816816448 submission_runner.py:408] Time since start: 11355.39s, 	Step: 22651, 	{'train/accuracy': 0.9912999868392944, 'train/loss': 0.02892247587442398, 'train/mean_average_precision': 0.45293090255351975, 'validation/accuracy': 0.9866639971733093, 'validation/loss': 0.044177647680044174, 'validation/mean_average_precision': 0.2652814454767527, 'validation/num_examples': 43793, 'test/accuracy': 0.9859510064125061, 'test/loss': 0.046832095831632614, 'test/mean_average_precision': 0.2558588523560185, 'test/num_examples': 43793, 'score': 7221.436220884323, 'total_duration': 11355.389991521835, 'accumulated_submission_time': 7221.436220884323, 'accumulated_eval_time': 4132.4447610378265, 'accumulated_logging_time': 0.8884739875793457}
I0205 00:57:47.217452 139758987925248 logging_writer.py:48] [22651] accumulated_eval_time=4132.444761, accumulated_logging_time=0.888474, accumulated_submission_time=7221.436221, global_step=22651, preemption_count=0, score=7221.436221, test/accuracy=0.985951, test/loss=0.046832, test/mean_average_precision=0.255859, test/num_examples=43793, total_duration=11355.389992, train/accuracy=0.991300, train/loss=0.028922, train/mean_average_precision=0.452931, validation/accuracy=0.986664, validation/loss=0.044178, validation/mean_average_precision=0.265281, validation/num_examples=43793
I0205 00:58:03.242340 139758996317952 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.07054352760314941, loss=0.033506136387586594
I0205 00:58:34.751388 139758987925248 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.10212159901857376, loss=0.03758954256772995
I0205 00:59:06.594280 139758996317952 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.07428749650716782, loss=0.03258156403899193
I0205 00:59:38.059293 139758987925248 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.09218063950538635, loss=0.03584831953048706
I0205 01:00:09.605885 139758996317952 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.08092288672924042, loss=0.034707702696323395
I0205 01:00:41.022199 139758987925248 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.07450905442237854, loss=0.03127782419323921
I0205 01:01:12.665861 139758996317952 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.1124587282538414, loss=0.03395785391330719
I0205 01:01:44.774036 139758987925248 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.08744201064109802, loss=0.036561865359544754
I0205 01:01:47.326748 139919816816448 spec.py:321] Evaluating on the training split.
I0205 01:03:49.648073 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 01:03:52.703519 139919816816448 spec.py:349] Evaluating on the test split.
I0205 01:03:55.706464 139919816816448 submission_runner.py:408] Time since start: 11723.90s, 	Step: 23409, 	{'train/accuracy': 0.9911875128746033, 'train/loss': 0.028938593342900276, 'train/mean_average_precision': 0.44682227201069025, 'validation/accuracy': 0.98687344789505, 'validation/loss': 0.04422194883227348, 'validation/mean_average_precision': 0.26590192837812177, 'validation/num_examples': 43793, 'test/accuracy': 0.9860158562660217, 'test/loss': 0.047047268599271774, 'test/mean_average_precision': 0.2603437330110667, 'test/num_examples': 43793, 'score': 7461.514452457428, 'total_duration': 11723.900267839432, 'accumulated_submission_time': 7461.514452457428, 'accumulated_eval_time': 4260.824286222458, 'accumulated_logging_time': 0.9203827381134033}
I0205 01:03:55.728891 139752296675072 logging_writer.py:48] [23409] accumulated_eval_time=4260.824286, accumulated_logging_time=0.920383, accumulated_submission_time=7461.514452, global_step=23409, preemption_count=0, score=7461.514452, test/accuracy=0.986016, test/loss=0.047047, test/mean_average_precision=0.260344, test/num_examples=43793, total_duration=11723.900268, train/accuracy=0.991188, train/loss=0.028939, train/mean_average_precision=0.446822, validation/accuracy=0.986873, validation/loss=0.044222, validation/mean_average_precision=0.265902, validation/num_examples=43793
I0205 01:04:25.611737 139858186053376 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.0696830153465271, loss=0.0327252633869648
I0205 01:04:56.998996 139752296675072 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.07553261518478394, loss=0.033153120428323746
I0205 01:05:28.678838 139858186053376 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.0899580717086792, loss=0.03550996258854866
I0205 01:06:00.273867 139752296675072 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.10498764365911484, loss=0.033133890479803085
I0205 01:06:31.835345 139858186053376 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.13027627766132355, loss=0.03436320275068283
I0205 01:07:03.857523 139752296675072 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.10869001597166061, loss=0.03533238545060158
I0205 01:07:35.281318 139858186053376 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.08052381873130798, loss=0.031996484845876694
I0205 01:07:55.721292 139919816816448 spec.py:321] Evaluating on the training split.
I0205 01:09:58.656488 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 01:10:01.843736 139919816816448 spec.py:349] Evaluating on the test split.
I0205 01:10:04.884714 139919816816448 submission_runner.py:408] Time since start: 12093.08s, 	Step: 24166, 	{'train/accuracy': 0.9908936619758606, 'train/loss': 0.029993336647748947, 'train/mean_average_precision': 0.4147140270965586, 'validation/accuracy': 0.9866806268692017, 'validation/loss': 0.044428519904613495, 'validation/mean_average_precision': 0.26665260090581644, 'validation/num_examples': 43793, 'test/accuracy': 0.9858659505844116, 'test/loss': 0.04725208878517151, 'test/mean_average_precision': 0.25419347882160603, 'test/num_examples': 43793, 'score': 7701.180008888245, 'total_duration': 12093.078660488129, 'accumulated_submission_time': 7701.180008888245, 'accumulated_eval_time': 4389.98766207695, 'accumulated_logging_time': 1.248992681503296}
I0205 01:10:04.904796 139758987925248 logging_writer.py:48] [24166] accumulated_eval_time=4389.987662, accumulated_logging_time=1.248993, accumulated_submission_time=7701.180009, global_step=24166, preemption_count=0, score=7701.180009, test/accuracy=0.985866, test/loss=0.047252, test/mean_average_precision=0.254193, test/num_examples=43793, total_duration=12093.078660, train/accuracy=0.990894, train/loss=0.029993, train/mean_average_precision=0.414714, validation/accuracy=0.986681, validation/loss=0.044429, validation/mean_average_precision=0.266653, validation/num_examples=43793
I0205 01:10:16.265513 139758996317952 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.07984104752540588, loss=0.036482490599155426
I0205 01:10:48.478962 139758987925248 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.09060758352279663, loss=0.034694988280534744
I0205 01:11:20.821131 139758996317952 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.09365717321634293, loss=0.031713299453258514
I0205 01:11:52.450022 139758987925248 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.06662720441818237, loss=0.0337192565202713
I0205 01:12:24.467738 139758996317952 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.0665493831038475, loss=0.03232811018824577
I0205 01:12:56.524756 139758987925248 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.07931466400623322, loss=0.03632088750600815
I0205 01:13:28.522485 139758996317952 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.07788434624671936, loss=0.03156103938817978
I0205 01:14:00.261903 139758987925248 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.06953364610671997, loss=0.030931824818253517
I0205 01:14:04.947092 139919816816448 spec.py:321] Evaluating on the training split.
I0205 01:16:05.516539 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 01:16:08.576912 139919816816448 spec.py:349] Evaluating on the test split.
I0205 01:16:11.605458 139919816816448 submission_runner.py:408] Time since start: 12459.80s, 	Step: 24915, 	{'train/accuracy': 0.9908816814422607, 'train/loss': 0.029930898919701576, 'train/mean_average_precision': 0.41416300800734485, 'validation/accuracy': 0.9867399334907532, 'validation/loss': 0.04429195448756218, 'validation/mean_average_precision': 0.2720565043987341, 'validation/num_examples': 43793, 'test/accuracy': 0.9859838485717773, 'test/loss': 0.04694953188300133, 'test/mean_average_precision': 0.2616324466083824, 'test/num_examples': 43793, 'score': 7941.191206932068, 'total_duration': 12459.79927778244, 'accumulated_submission_time': 7941.191206932068, 'accumulated_eval_time': 4516.645851135254, 'accumulated_logging_time': 1.2801299095153809}
I0205 01:16:11.625909 139752296675072 logging_writer.py:48] [24915] accumulated_eval_time=4516.645851, accumulated_logging_time=1.280130, accumulated_submission_time=7941.191207, global_step=24915, preemption_count=0, score=7941.191207, test/accuracy=0.985984, test/loss=0.046950, test/mean_average_precision=0.261632, test/num_examples=43793, total_duration=12459.799278, train/accuracy=0.990882, train/loss=0.029931, train/mean_average_precision=0.414163, validation/accuracy=0.986740, validation/loss=0.044292, validation/mean_average_precision=0.272057, validation/num_examples=43793
I0205 01:16:39.313177 139858186053376 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.059299286454916, loss=0.030785271897912025
I0205 01:17:11.021089 139752296675072 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.10702230781316757, loss=0.03440599888563156
I0205 01:17:42.474529 139858186053376 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.06317540258169174, loss=0.031773462891578674
I0205 01:18:14.178116 139752296675072 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.07283324748277664, loss=0.034401994198560715
I0205 01:18:45.681065 139858186053376 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.07816197723150253, loss=0.03117809072136879
I0205 01:19:17.788075 139752296675072 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.07057051360607147, loss=0.03400597348809242
I0205 01:19:49.886906 139858186053376 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.06680458039045334, loss=0.03313898667693138
I0205 01:20:11.904146 139919816816448 spec.py:321] Evaluating on the training split.
I0205 01:22:15.144563 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 01:22:18.528638 139919816816448 spec.py:349] Evaluating on the test split.
I0205 01:22:21.826276 139919816816448 submission_runner.py:408] Time since start: 12830.02s, 	Step: 25671, 	{'train/accuracy': 0.9910153746604919, 'train/loss': 0.02948160283267498, 'train/mean_average_precision': 0.42614096978648197, 'validation/accuracy': 0.9866116046905518, 'validation/loss': 0.044560737907886505, 'validation/mean_average_precision': 0.26972062907659367, 'validation/num_examples': 43793, 'test/accuracy': 0.985874354839325, 'test/loss': 0.047286227345466614, 'test/mean_average_precision': 0.25563352939221756, 'test/num_examples': 43793, 'score': 8181.43877363205, 'total_duration': 12830.020049333572, 'accumulated_submission_time': 8181.43877363205, 'accumulated_eval_time': 4646.567767858505, 'accumulated_logging_time': 1.3114888668060303}
I0205 01:22:21.848492 139758987925248 logging_writer.py:48] [25671] accumulated_eval_time=4646.567768, accumulated_logging_time=1.311489, accumulated_submission_time=8181.438774, global_step=25671, preemption_count=0, score=8181.438774, test/accuracy=0.985874, test/loss=0.047286, test/mean_average_precision=0.255634, test/num_examples=43793, total_duration=12830.020049, train/accuracy=0.991015, train/loss=0.029482, train/mean_average_precision=0.426141, validation/accuracy=0.986612, validation/loss=0.044561, validation/mean_average_precision=0.269721, validation/num_examples=43793
I0205 01:22:31.491782 139758996317952 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.0657712072134018, loss=0.03295652195811272
I0205 01:23:03.645479 139758987925248 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.09945280104875565, loss=0.03443324938416481
I0205 01:23:35.369791 139758996317952 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.07204887270927429, loss=0.0344824381172657
I0205 01:24:07.553465 139758987925248 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.06579513102769852, loss=0.03184458985924721
I0205 01:24:39.360246 139758996317952 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.08619080483913422, loss=0.031364575028419495
I0205 01:25:11.258481 139758987925248 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.08357128500938416, loss=0.034048132598400116
I0205 01:25:43.499942 139758996317952 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.08548225462436676, loss=0.034667499363422394
I0205 01:26:15.322041 139758987925248 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.09454429894685745, loss=0.03302964195609093
I0205 01:26:22.103501 139919816816448 spec.py:321] Evaluating on the training split.
I0205 01:28:25.997182 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 01:28:29.104923 139919816816448 spec.py:349] Evaluating on the test split.
I0205 01:28:32.153952 139919816816448 submission_runner.py:408] Time since start: 13200.35s, 	Step: 26422, 	{'train/accuracy': 0.9910532832145691, 'train/loss': 0.029158132150769234, 'train/mean_average_precision': 0.4336587863156425, 'validation/accuracy': 0.9867833256721497, 'validation/loss': 0.04481257125735283, 'validation/mean_average_precision': 0.26791099113968614, 'validation/num_examples': 43793, 'test/accuracy': 0.9859535694122314, 'test/loss': 0.04782743752002716, 'test/mean_average_precision': 0.25248273817826317, 'test/num_examples': 43793, 'score': 8421.660665512085, 'total_duration': 13200.347779750824, 'accumulated_submission_time': 8421.660665512085, 'accumulated_eval_time': 4776.618052721024, 'accumulated_logging_time': 1.3456156253814697}
I0205 01:28:32.173850 139752296675072 logging_writer.py:48] [26422] accumulated_eval_time=4776.618053, accumulated_logging_time=1.345616, accumulated_submission_time=8421.660666, global_step=26422, preemption_count=0, score=8421.660666, test/accuracy=0.985954, test/loss=0.047827, test/mean_average_precision=0.252483, test/num_examples=43793, total_duration=13200.347780, train/accuracy=0.991053, train/loss=0.029158, train/mean_average_precision=0.433659, validation/accuracy=0.986783, validation/loss=0.044813, validation/mean_average_precision=0.267911, validation/num_examples=43793
I0205 01:28:57.320550 139858186053376 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.07270736247301102, loss=0.032668422907590866
I0205 01:29:29.035908 139752296675072 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.08683022111654282, loss=0.03411174938082695
I0205 01:30:01.241763 139858186053376 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.08021789789199829, loss=0.031715765595436096
I0205 01:30:32.935634 139752296675072 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.08999345451593399, loss=0.035199619829654694
I0205 01:31:04.708333 139858186053376 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.07990778982639313, loss=0.030156925320625305
I0205 01:31:36.769926 139752296675072 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.08825553208589554, loss=0.03609166294336319
I0205 01:32:08.279692 139858186053376 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.0938148945569992, loss=0.03323637694120407
I0205 01:32:32.155775 139919816816448 spec.py:321] Evaluating on the training split.
I0205 01:34:32.343580 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 01:34:35.450744 139919816816448 spec.py:349] Evaluating on the test split.
I0205 01:34:38.490522 139919816816448 submission_runner.py:408] Time since start: 13566.68s, 	Step: 27176, 	{'train/accuracy': 0.9911747574806213, 'train/loss': 0.028925133869051933, 'train/mean_average_precision': 0.44522544274458764, 'validation/accuracy': 0.9868060946464539, 'validation/loss': 0.044306039810180664, 'validation/mean_average_precision': 0.27197369995391546, 'validation/num_examples': 43793, 'test/accuracy': 0.9859000444412231, 'test/loss': 0.04738543927669525, 'test/mean_average_precision': 0.2587810042911728, 'test/num_examples': 43793, 'score': 8661.61139369011, 'total_duration': 13566.684311151505, 'accumulated_submission_time': 8661.61139369011, 'accumulated_eval_time': 4902.952599287033, 'accumulated_logging_time': 1.3763277530670166}
I0205 01:34:38.510993 139752480773888 logging_writer.py:48] [27176] accumulated_eval_time=4902.952599, accumulated_logging_time=1.376328, accumulated_submission_time=8661.611394, global_step=27176, preemption_count=0, score=8661.611394, test/accuracy=0.985900, test/loss=0.047385, test/mean_average_precision=0.258781, test/num_examples=43793, total_duration=13566.684311, train/accuracy=0.991175, train/loss=0.028925, train/mean_average_precision=0.445225, validation/accuracy=0.986806, validation/loss=0.044306, validation/mean_average_precision=0.271974, validation/num_examples=43793
I0205 01:34:46.576701 139758987925248 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.0671892985701561, loss=0.03101329691708088
I0205 01:35:18.519096 139752480773888 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.08083262294530869, loss=0.034637387841939926
I0205 01:35:49.884519 139758987925248 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.07208888232707977, loss=0.03513738512992859
I0205 01:36:21.386653 139752480773888 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.09575877338647842, loss=0.03195247799158096
I0205 01:36:52.999666 139758987925248 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.095488041639328, loss=0.03259570896625519
I0205 01:37:25.641547 139752480773888 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.08872037380933762, loss=0.035365160554647446
I0205 01:37:58.486375 139758987925248 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.0748017355799675, loss=0.03454966843128204
I0205 01:38:31.046782 139752480773888 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.0794849693775177, loss=0.031244315207004547
I0205 01:38:38.676689 139919816816448 spec.py:321] Evaluating on the training split.
I0205 01:40:44.060072 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 01:40:47.172532 139919816816448 spec.py:349] Evaluating on the test split.
I0205 01:40:50.178634 139919816816448 submission_runner.py:408] Time since start: 13938.37s, 	Step: 27925, 	{'train/accuracy': 0.9912655353546143, 'train/loss': 0.028265230357646942, 'train/mean_average_precision': 0.4763750571741873, 'validation/accuracy': 0.9868454337120056, 'validation/loss': 0.0445428192615509, 'validation/mean_average_precision': 0.27583540975590737, 'validation/num_examples': 43793, 'test/accuracy': 0.9859707951545715, 'test/loss': 0.047591667622327805, 'test/mean_average_precision': 0.2610465782854482, 'test/num_examples': 43793, 'score': 8901.743763685226, 'total_duration': 13938.37245965004, 'accumulated_submission_time': 8901.743763685226, 'accumulated_eval_time': 5034.454385757446, 'accumulated_logging_time': 1.4076869487762451}
I0205 01:40:50.200218 139758996317952 logging_writer.py:48] [27925] accumulated_eval_time=5034.454386, accumulated_logging_time=1.407687, accumulated_submission_time=8901.743764, global_step=27925, preemption_count=0, score=8901.743764, test/accuracy=0.985971, test/loss=0.047592, test/mean_average_precision=0.261047, test/num_examples=43793, total_duration=13938.372460, train/accuracy=0.991266, train/loss=0.028265, train/mean_average_precision=0.476375, validation/accuracy=0.986845, validation/loss=0.044543, validation/mean_average_precision=0.275835, validation/num_examples=43793
I0205 01:41:14.874798 139858186053376 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.0784076526761055, loss=0.03303536772727966
I0205 01:41:46.924964 139758996317952 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.07286295294761658, loss=0.03517777845263481
I0205 01:42:18.431446 139858186053376 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.08440771698951721, loss=0.03274593502283096
I0205 01:42:49.798559 139758996317952 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.09212207794189453, loss=0.029674353078007698
I0205 01:43:22.170404 139858186053376 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.08442843705415726, loss=0.03281379118561745
I0205 01:43:54.333960 139758996317952 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.07728450745344162, loss=0.03368173912167549
I0205 01:44:26.534748 139858186053376 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.0898921936750412, loss=0.03786638379096985
I0205 01:44:50.436208 139919816816448 spec.py:321] Evaluating on the training split.
I0205 01:46:51.667657 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 01:46:54.744334 139919816816448 spec.py:349] Evaluating on the test split.
I0205 01:46:57.749986 139919816816448 submission_runner.py:408] Time since start: 14305.94s, 	Step: 28676, 	{'train/accuracy': 0.9915241003036499, 'train/loss': 0.027481885626912117, 'train/mean_average_precision': 0.4806527878827299, 'validation/accuracy': 0.9867256879806519, 'validation/loss': 0.04456234723329544, 'validation/mean_average_precision': 0.27001576131983057, 'validation/num_examples': 43793, 'test/accuracy': 0.9859851598739624, 'test/loss': 0.04734373465180397, 'test/mean_average_precision': 0.2601059381326383, 'test/num_examples': 43793, 'score': 9141.946783781052, 'total_duration': 14305.943809747696, 'accumulated_submission_time': 9141.946783781052, 'accumulated_eval_time': 5161.768011569977, 'accumulated_logging_time': 1.440685749053955}
I0205 01:46:57.770680 139752480773888 logging_writer.py:48] [28676] accumulated_eval_time=5161.768012, accumulated_logging_time=1.440686, accumulated_submission_time=9141.946784, global_step=28676, preemption_count=0, score=9141.946784, test/accuracy=0.985985, test/loss=0.047344, test/mean_average_precision=0.260106, test/num_examples=43793, total_duration=14305.943810, train/accuracy=0.991524, train/loss=0.027482, train/mean_average_precision=0.480653, validation/accuracy=0.986726, validation/loss=0.044562, validation/mean_average_precision=0.270016, validation/num_examples=43793
I0205 01:47:05.805214 139758987925248 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.07084672152996063, loss=0.032077938318252563
I0205 01:47:37.538600 139752480773888 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.07846637070178986, loss=0.031208328902721405
I0205 01:48:09.244050 139758987925248 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.08264614641666412, loss=0.036930669099092484
I0205 01:48:40.943444 139752480773888 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.09465640783309937, loss=0.033893052488565445
I0205 01:49:12.430819 139758987925248 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.07652251422405243, loss=0.0324433371424675
I0205 01:49:44.081386 139752480773888 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.09649522602558136, loss=0.03453467786312103
I0205 01:50:16.213387 139758987925248 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.06834153085947037, loss=0.033793721348047256
I0205 01:50:47.817738 139752480773888 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.08514312654733658, loss=0.03462614119052887
I0205 01:50:57.915843 139919816816448 spec.py:321] Evaluating on the training split.
I0205 01:53:00.041192 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 01:53:03.252409 139919816816448 spec.py:349] Evaluating on the test split.
I0205 01:53:07.279665 139919816816448 submission_runner.py:408] Time since start: 14675.47s, 	Step: 29433, 	{'train/accuracy': 0.9914757609367371, 'train/loss': 0.02768174186348915, 'train/mean_average_precision': 0.4703134444485871, 'validation/accuracy': 0.9867504835128784, 'validation/loss': 0.0449969619512558, 'validation/mean_average_precision': 0.26254527480645284, 'validation/num_examples': 43793, 'test/accuracy': 0.9858773350715637, 'test/loss': 0.047853145748376846, 'test/mean_average_precision': 0.2550559410887236, 'test/num_examples': 43793, 'score': 9382.060484170914, 'total_duration': 14675.473503351212, 'accumulated_submission_time': 9382.060484170914, 'accumulated_eval_time': 5291.131680011749, 'accumulated_logging_time': 1.4723448753356934}
I0205 01:53:07.302245 139752296675072 logging_writer.py:48] [29433] accumulated_eval_time=5291.131680, accumulated_logging_time=1.472345, accumulated_submission_time=9382.060484, global_step=29433, preemption_count=0, score=9382.060484, test/accuracy=0.985877, test/loss=0.047853, test/mean_average_precision=0.255056, test/num_examples=43793, total_duration=14675.473503, train/accuracy=0.991476, train/loss=0.027682, train/mean_average_precision=0.470313, validation/accuracy=0.986750, validation/loss=0.044997, validation/mean_average_precision=0.262545, validation/num_examples=43793
I0205 01:53:28.921651 139758996317952 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.08978012204170227, loss=0.03332475200295448
I0205 01:54:00.801969 139752296675072 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.06722959131002426, loss=0.03363916650414467
I0205 01:54:32.595265 139758996317952 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.08038263022899628, loss=0.031918030232191086
I0205 01:55:04.278070 139752296675072 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.08953843265771866, loss=0.03328157961368561
I0205 01:55:35.856916 139758996317952 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.12271050363779068, loss=0.03142454847693443
I0205 01:56:07.570618 139752296675072 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.10968054831027985, loss=0.03527171164751053
I0205 01:56:39.178599 139758996317952 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.08604972064495087, loss=0.0336633063852787
I0205 01:57:07.368064 139919816816448 spec.py:321] Evaluating on the training split.
I0205 01:59:05.154520 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 01:59:08.184515 139919816816448 spec.py:349] Evaluating on the test split.
I0205 01:59:11.181885 139919816816448 submission_runner.py:408] Time since start: 15039.38s, 	Step: 30189, 	{'train/accuracy': 0.9917797446250916, 'train/loss': 0.026923639699816704, 'train/mean_average_precision': 0.4778185300817284, 'validation/accuracy': 0.986819863319397, 'validation/loss': 0.04437655583024025, 'validation/mean_average_precision': 0.27501181817828557, 'validation/num_examples': 43793, 'test/accuracy': 0.9859526753425598, 'test/loss': 0.04693050682544708, 'test/mean_average_precision': 0.26771399555357955, 'test/num_examples': 43793, 'score': 9622.095024824142, 'total_duration': 15039.375715255737, 'accumulated_submission_time': 9622.095024824142, 'accumulated_eval_time': 5414.945363521576, 'accumulated_logging_time': 1.5057015419006348}
I0205 01:59:11.202750 139752480773888 logging_writer.py:48] [30189] accumulated_eval_time=5414.945364, accumulated_logging_time=1.505702, accumulated_submission_time=9622.095025, global_step=30189, preemption_count=0, score=9622.095025, test/accuracy=0.985953, test/loss=0.046931, test/mean_average_precision=0.267714, test/num_examples=43793, total_duration=15039.375715, train/accuracy=0.991780, train/loss=0.026924, train/mean_average_precision=0.477819, validation/accuracy=0.986820, validation/loss=0.044377, validation/mean_average_precision=0.275012, validation/num_examples=43793
I0205 01:59:14.951140 139858186053376 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.13255766034126282, loss=0.029962411150336266
I0205 01:59:46.625172 139752480773888 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.08240500837564468, loss=0.03405153751373291
I0205 02:00:18.173902 139858186053376 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.08512388169765472, loss=0.03126097843050957
I0205 02:00:49.896796 139752480773888 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.09501905739307404, loss=0.030409757047891617
I0205 02:01:21.480981 139858186053376 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.09402355551719666, loss=0.035086408257484436
I0205 02:01:53.284550 139752480773888 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.08850257843732834, loss=0.030764319002628326
I0205 02:02:25.189660 139858186053376 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.08555837720632553, loss=0.03605696186423302
I0205 02:02:56.808089 139752480773888 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.07127073407173157, loss=0.0326339416205883
I0205 02:03:11.273912 139919816816448 spec.py:321] Evaluating on the training split.
I0205 02:05:11.663272 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 02:05:14.732768 139919816816448 spec.py:349] Evaluating on the test split.
I0205 02:05:17.761102 139919816816448 submission_runner.py:408] Time since start: 15405.96s, 	Step: 30947, 	{'train/accuracy': 0.9916726350784302, 'train/loss': 0.027349548414349556, 'train/mean_average_precision': 0.48438762137735747, 'validation/accuracy': 0.986823558807373, 'validation/loss': 0.04459654912352562, 'validation/mean_average_precision': 0.27586436909845735, 'validation/num_examples': 43793, 'test/accuracy': 0.9860154390335083, 'test/loss': 0.04735671728849411, 'test/mean_average_precision': 0.2568862944563916, 'test/num_examples': 43793, 'score': 9862.135322093964, 'total_duration': 15405.955041885376, 'accumulated_submission_time': 9862.135322093964, 'accumulated_eval_time': 5541.432502031326, 'accumulated_logging_time': 1.5375621318817139}
I0205 02:05:17.786282 139758987925248 logging_writer.py:48] [30947] accumulated_eval_time=5541.432502, accumulated_logging_time=1.537562, accumulated_submission_time=9862.135322, global_step=30947, preemption_count=0, score=9862.135322, test/accuracy=0.986015, test/loss=0.047357, test/mean_average_precision=0.256886, test/num_examples=43793, total_duration=15405.955042, train/accuracy=0.991673, train/loss=0.027350, train/mean_average_precision=0.484388, validation/accuracy=0.986824, validation/loss=0.044597, validation/mean_average_precision=0.275864, validation/num_examples=43793
I0205 02:05:35.221221 139758996317952 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.08826369792222977, loss=0.031217342242598534
I0205 02:06:06.871145 139758987925248 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.06851744651794434, loss=0.03342762216925621
I0205 02:06:38.603409 139758996317952 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.08202797919511795, loss=0.03220760077238083
I0205 02:07:10.555844 139758987925248 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.08412928134202957, loss=0.03237563744187355
I0205 02:07:42.634545 139758996317952 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.08117474615573883, loss=0.02970815636217594
I0205 02:08:14.437840 139758987925248 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.07564730942249298, loss=0.029015174135565758
I0205 02:08:46.270590 139758996317952 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.09817161411046982, loss=0.03372680023312569
I0205 02:09:17.912608 139758987925248 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.0788927972316742, loss=0.03297928720712662
I0205 02:09:17.918072 139919816816448 spec.py:321] Evaluating on the training split.
I0205 02:11:24.389766 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 02:11:27.498981 139919816816448 spec.py:349] Evaluating on the test split.
I0205 02:11:30.620320 139919816816448 submission_runner.py:408] Time since start: 15778.81s, 	Step: 31701, 	{'train/accuracy': 0.9913953542709351, 'train/loss': 0.028089625760912895, 'train/mean_average_precision': 0.4647237618074311, 'validation/accuracy': 0.9867054224014282, 'validation/loss': 0.044793397188186646, 'validation/mean_average_precision': 0.2632075575298583, 'validation/num_examples': 43793, 'test/accuracy': 0.9858874082565308, 'test/loss': 0.04773210734128952, 'test/mean_average_precision': 0.25401344071206705, 'test/num_examples': 43793, 'score': 10102.235368013382, 'total_duration': 15778.814150571823, 'accumulated_submission_time': 10102.235368013382, 'accumulated_eval_time': 5674.134570837021, 'accumulated_logging_time': 1.574239730834961}
I0205 02:11:30.641647 139752480773888 logging_writer.py:48] [31701] accumulated_eval_time=5674.134571, accumulated_logging_time=1.574240, accumulated_submission_time=10102.235368, global_step=31701, preemption_count=0, score=10102.235368, test/accuracy=0.985887, test/loss=0.047732, test/mean_average_precision=0.254013, test/num_examples=43793, total_duration=15778.814151, train/accuracy=0.991395, train/loss=0.028090, train/mean_average_precision=0.464724, validation/accuracy=0.986705, validation/loss=0.044793, validation/mean_average_precision=0.263208, validation/num_examples=43793
I0205 02:12:03.262278 139858186053376 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.06813229620456696, loss=0.031940072774887085
I0205 02:12:35.230624 139752480773888 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.08626114577054977, loss=0.03130869194865227
I0205 02:13:06.806097 139858186053376 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.0845145508646965, loss=0.034251902252435684
I0205 02:13:38.168390 139752480773888 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.08037031441926956, loss=0.030767669901251793
I0205 02:14:09.681958 139858186053376 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.08146310597658157, loss=0.03374140337109566
I0205 02:14:41.396158 139752480773888 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.08481837809085846, loss=0.031589072197675705
I0205 02:15:14.243229 139858186053376 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.074410080909729, loss=0.033848002552986145
I0205 02:15:30.894481 139919816816448 spec.py:321] Evaluating on the training split.
I0205 02:17:33.490491 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 02:17:36.508949 139919816816448 spec.py:349] Evaluating on the test split.
I0205 02:17:39.499272 139919816816448 submission_runner.py:408] Time since start: 16147.69s, 	Step: 32453, 	{'train/accuracy': 0.9914029240608215, 'train/loss': 0.028008729219436646, 'train/mean_average_precision': 0.4645824797357492, 'validation/accuracy': 0.9867374897003174, 'validation/loss': 0.04449669271707535, 'validation/mean_average_precision': 0.26626106581850995, 'validation/num_examples': 43793, 'test/accuracy': 0.9859194159507751, 'test/loss': 0.04711798578500748, 'test/mean_average_precision': 0.26194080149817495, 'test/num_examples': 43793, 'score': 10342.45687031746, 'total_duration': 16147.693119049072, 'accumulated_submission_time': 10342.45687031746, 'accumulated_eval_time': 5802.739213705063, 'accumulated_logging_time': 1.6064386367797852}
I0205 02:17:39.520666 139752296675072 logging_writer.py:48] [32453] accumulated_eval_time=5802.739214, accumulated_logging_time=1.606439, accumulated_submission_time=10342.456870, global_step=32453, preemption_count=0, score=10342.456870, test/accuracy=0.985919, test/loss=0.047118, test/mean_average_precision=0.261941, test/num_examples=43793, total_duration=16147.693119, train/accuracy=0.991403, train/loss=0.028009, train/mean_average_precision=0.464582, validation/accuracy=0.986737, validation/loss=0.044497, validation/mean_average_precision=0.266261, validation/num_examples=43793
I0205 02:17:54.821766 139758987925248 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.08458370715379715, loss=0.03208152949810028
I0205 02:18:26.914875 139752296675072 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.11479870975017548, loss=0.03500489890575409
I0205 02:18:58.672162 139758987925248 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.1127406507730484, loss=0.030232219025492668
I0205 02:19:30.430067 139752296675072 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.10416372120380402, loss=0.03244407847523689
I0205 02:20:02.351609 139758987925248 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.11675432324409485, loss=0.031192438676953316
I0205 02:20:33.868689 139752296675072 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.0773521214723587, loss=0.030816735699772835
I0205 02:21:05.327199 139758987925248 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.09984563291072845, loss=0.033501677215099335
I0205 02:21:36.969030 139752296675072 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.08051952719688416, loss=0.032489996403455734
I0205 02:21:39.507339 139919816816448 spec.py:321] Evaluating on the training split.
I0205 02:23:43.016185 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 02:23:46.084604 139919816816448 spec.py:349] Evaluating on the test split.
I0205 02:23:49.112866 139919816816448 submission_runner.py:408] Time since start: 16517.31s, 	Step: 33209, 	{'train/accuracy': 0.9913453459739685, 'train/loss': 0.02826383337378502, 'train/mean_average_precision': 0.4542848687332359, 'validation/accuracy': 0.9869092106819153, 'validation/loss': 0.04439977928996086, 'validation/mean_average_precision': 0.2762699325917176, 'validation/num_examples': 43793, 'test/accuracy': 0.9859253168106079, 'test/loss': 0.04754733666777611, 'test/mean_average_precision': 0.2548237538397603, 'test/num_examples': 43793, 'score': 10582.4124584198, 'total_duration': 16517.30669927597, 'accumulated_submission_time': 10582.4124584198, 'accumulated_eval_time': 5932.344577074051, 'accumulated_logging_time': 1.6389267444610596}
I0205 02:23:49.134244 139752480773888 logging_writer.py:48] [33209] accumulated_eval_time=5932.344577, accumulated_logging_time=1.638927, accumulated_submission_time=10582.412458, global_step=33209, preemption_count=0, score=10582.412458, test/accuracy=0.985925, test/loss=0.047547, test/mean_average_precision=0.254824, test/num_examples=43793, total_duration=16517.306699, train/accuracy=0.991345, train/loss=0.028264, train/mean_average_precision=0.454285, validation/accuracy=0.986909, validation/loss=0.044400, validation/mean_average_precision=0.276270, validation/num_examples=43793
I0205 02:24:18.293109 139758996317952 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.09894814342260361, loss=0.030610637739300728
I0205 02:24:50.038523 139752480773888 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.1149648055434227, loss=0.0333465151488781
I0205 02:25:21.589021 139758996317952 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.09637990593910217, loss=0.03053879924118519
I0205 02:25:52.964084 139752480773888 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.09787765145301819, loss=0.030086098238825798
I0205 02:26:24.364567 139758996317952 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.07734812051057816, loss=0.028532857075333595
I0205 02:26:56.091191 139752480773888 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.10773060470819473, loss=0.030474554747343063
I0205 02:27:28.279061 139758996317952 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.10026305168867111, loss=0.032140519469976425
I0205 02:27:49.317966 139919816816448 spec.py:321] Evaluating on the training split.
I0205 02:29:48.334635 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 02:29:51.487948 139919816816448 spec.py:349] Evaluating on the test split.
I0205 02:29:54.543942 139919816816448 submission_runner.py:408] Time since start: 16882.74s, 	Step: 33967, 	{'train/accuracy': 0.991368293762207, 'train/loss': 0.027924323454499245, 'train/mean_average_precision': 0.4556313674502258, 'validation/accuracy': 0.986847460269928, 'validation/loss': 0.04521167278289795, 'validation/mean_average_precision': 0.2753139830957439, 'validation/num_examples': 43793, 'test/accuracy': 0.9860167503356934, 'test/loss': 0.048343922942876816, 'test/mean_average_precision': 0.2645479106855683, 'test/num_examples': 43793, 'score': 10822.565240621567, 'total_duration': 16882.73788666725, 'accumulated_submission_time': 10822.565240621567, 'accumulated_eval_time': 6057.570507287979, 'accumulated_logging_time': 1.6709973812103271}
I0205 02:29:54.566209 139752296675072 logging_writer.py:48] [33967] accumulated_eval_time=6057.570507, accumulated_logging_time=1.670997, accumulated_submission_time=10822.565241, global_step=33967, preemption_count=0, score=10822.565241, test/accuracy=0.986017, test/loss=0.048344, test/mean_average_precision=0.264548, test/num_examples=43793, total_duration=16882.737887, train/accuracy=0.991368, train/loss=0.027924, train/mean_average_precision=0.455631, validation/accuracy=0.986847, validation/loss=0.045212, validation/mean_average_precision=0.275314, validation/num_examples=43793
I0205 02:30:05.641767 139858186053376 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.09195161610841751, loss=0.03293915092945099
I0205 02:30:37.879270 139752296675072 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.08828719705343246, loss=0.031238004565238953
I0205 02:31:10.119765 139858186053376 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.0748724490404129, loss=0.03166400268673897
I0205 02:31:42.315933 139752296675072 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.08124157786369324, loss=0.029873250052332878
I0205 02:32:14.621964 139858186053376 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.07857702672481537, loss=0.032422952353954315
I0205 02:32:46.704480 139752296675072 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.08038333803415298, loss=0.030284889042377472
I0205 02:33:18.670572 139858186053376 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.06869705021381378, loss=0.02791222184896469
I0205 02:33:50.284968 139752296675072 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.07704276591539383, loss=0.031116144731640816
I0205 02:33:54.669401 139919816816448 spec.py:321] Evaluating on the training split.
I0205 02:36:02.030110 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 02:36:05.424834 139919816816448 spec.py:349] Evaluating on the test split.
I0205 02:36:08.750925 139919816816448 submission_runner.py:408] Time since start: 17256.94s, 	Step: 34715, 	{'train/accuracy': 0.9916467070579529, 'train/loss': 0.027205245569348335, 'train/mean_average_precision': 0.477621432016776, 'validation/accuracy': 0.9869270324707031, 'validation/loss': 0.04441390186548233, 'validation/mean_average_precision': 0.2780114628194121, 'validation/num_examples': 43793, 'test/accuracy': 0.9859063625335693, 'test/loss': 0.0478287898004055, 'test/mean_average_precision': 0.25468529277239105, 'test/num_examples': 43793, 'score': 11062.638055562973, 'total_duration': 17256.9448492527, 'accumulated_submission_time': 11062.638055562973, 'accumulated_eval_time': 6191.6519594192505, 'accumulated_logging_time': 1.7039594650268555}
I0205 02:36:08.775251 139758987925248 logging_writer.py:48] [34715] accumulated_eval_time=6191.651959, accumulated_logging_time=1.703959, accumulated_submission_time=11062.638056, global_step=34715, preemption_count=0, score=11062.638056, test/accuracy=0.985906, test/loss=0.047829, test/mean_average_precision=0.254685, test/num_examples=43793, total_duration=17256.944849, train/accuracy=0.991647, train/loss=0.027205, train/mean_average_precision=0.477621, validation/accuracy=0.986927, validation/loss=0.044414, validation/mean_average_precision=0.278011, validation/num_examples=43793
I0205 02:36:37.060937 139758996317952 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.10935347527265549, loss=0.03199019283056259
I0205 02:37:09.758793 139758987925248 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.09912626445293427, loss=0.03327276185154915
I0205 02:37:41.581093 139758996317952 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.08466588705778122, loss=0.03264318034052849
I0205 02:38:13.448493 139758987925248 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.08938074111938477, loss=0.032068319618701935
I0205 02:38:44.975699 139758996317952 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.10620275884866714, loss=0.033949896693229675
I0205 02:39:17.067289 139758987925248 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.08096086978912354, loss=0.030829910188913345
I0205 02:39:48.721950 139758996317952 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.08143201470375061, loss=0.031717874109745026
I0205 02:40:08.901960 139919816816448 spec.py:321] Evaluating on the training split.
I0205 02:42:07.086902 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 02:42:10.160468 139919816816448 spec.py:349] Evaluating on the test split.
I0205 02:42:13.195580 139919816816448 submission_runner.py:408] Time since start: 17621.39s, 	Step: 35464, 	{'train/accuracy': 0.9917681217193604, 'train/loss': 0.02679045870900154, 'train/mean_average_precision': 0.4988786142774347, 'validation/accuracy': 0.986970067024231, 'validation/loss': 0.0444306842982769, 'validation/mean_average_precision': 0.2751348822187652, 'validation/num_examples': 43793, 'test/accuracy': 0.9860761165618896, 'test/loss': 0.0475175641477108, 'test/mean_average_precision': 0.2631292091684028, 'test/num_examples': 43793, 'score': 11302.731583595276, 'total_duration': 17621.38951563835, 'accumulated_submission_time': 11302.731583595276, 'accumulated_eval_time': 6315.94552397728, 'accumulated_logging_time': 1.7400023937225342}
I0205 02:42:13.218395 139752296675072 logging_writer.py:48] [35464] accumulated_eval_time=6315.945524, accumulated_logging_time=1.740002, accumulated_submission_time=11302.731584, global_step=35464, preemption_count=0, score=11302.731584, test/accuracy=0.986076, test/loss=0.047518, test/mean_average_precision=0.263129, test/num_examples=43793, total_duration=17621.389516, train/accuracy=0.991768, train/loss=0.026790, train/mean_average_precision=0.498879, validation/accuracy=0.986970, validation/loss=0.044431, validation/mean_average_precision=0.275135, validation/num_examples=43793
I0205 02:42:25.148549 139858186053376 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.10897435992956161, loss=0.03462578356266022
I0205 02:42:56.885485 139752296675072 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.08412857353687286, loss=0.03166969493031502
I0205 02:43:28.468279 139858186053376 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.09310472011566162, loss=0.031180083751678467
I0205 02:44:00.674039 139752296675072 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.11861496418714523, loss=0.03423266485333443
I0205 02:44:32.850992 139858186053376 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.1023598313331604, loss=0.03262554481625557
I0205 02:45:04.698078 139752296675072 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.08025583624839783, loss=0.02883642353117466
I0205 02:45:36.451568 139858186053376 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.10629025846719742, loss=0.031023159623146057
I0205 02:46:08.377713 139752296675072 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.07953616976737976, loss=0.03068462759256363
I0205 02:46:13.465967 139919816816448 spec.py:321] Evaluating on the training split.
I0205 02:48:17.610608 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 02:48:20.668009 139919816816448 spec.py:349] Evaluating on the test split.
I0205 02:48:23.706623 139919816816448 submission_runner.py:408] Time since start: 17991.90s, 	Step: 36217, 	{'train/accuracy': 0.9920303225517273, 'train/loss': 0.02567622810602188, 'train/mean_average_precision': 0.5184381943115008, 'validation/accuracy': 0.9868994355201721, 'validation/loss': 0.04505499452352524, 'validation/mean_average_precision': 0.27438343203587545, 'validation/num_examples': 43793, 'test/accuracy': 0.9859569072723389, 'test/loss': 0.04814833775162697, 'test/mean_average_precision': 0.2617889807945268, 'test/num_examples': 43793, 'score': 11542.948122262955, 'total_duration': 17991.900566101074, 'accumulated_submission_time': 11542.948122262955, 'accumulated_eval_time': 6446.186127901077, 'accumulated_logging_time': 1.7738418579101562}
I0205 02:48:23.729499 139752480773888 logging_writer.py:48] [36217] accumulated_eval_time=6446.186128, accumulated_logging_time=1.773842, accumulated_submission_time=11542.948122, global_step=36217, preemption_count=0, score=11542.948122, test/accuracy=0.985957, test/loss=0.048148, test/mean_average_precision=0.261789, test/num_examples=43793, total_duration=17991.900566, train/accuracy=0.992030, train/loss=0.025676, train/mean_average_precision=0.518438, validation/accuracy=0.986899, validation/loss=0.045055, validation/mean_average_precision=0.274383, validation/num_examples=43793
I0205 02:48:50.032433 139758987925248 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.14062806963920593, loss=0.03498977795243263
I0205 02:49:21.205276 139752480773888 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.09007474780082703, loss=0.030053621158003807
I0205 02:49:52.707206 139758987925248 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.10100162774324417, loss=0.032774798572063446
I0205 02:50:24.500979 139752480773888 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.09516552835702896, loss=0.03334492817521095
I0205 02:50:56.030163 139758987925248 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.1029396802186966, loss=0.0346093587577343
I0205 02:51:27.538108 139752480773888 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.09974480420351028, loss=0.03161299228668213
I0205 02:51:59.694378 139758987925248 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.0843958705663681, loss=0.026817068457603455
I0205 02:52:23.791641 139919816816448 spec.py:321] Evaluating on the training split.
I0205 02:54:23.474016 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 02:54:26.516461 139919816816448 spec.py:349] Evaluating on the test split.
I0205 02:54:29.491764 139919816816448 submission_runner.py:408] Time since start: 18357.69s, 	Step: 36976, 	{'train/accuracy': 0.9921303391456604, 'train/loss': 0.025417350232601166, 'train/mean_average_precision': 0.5227613756415113, 'validation/accuracy': 0.9869270324707031, 'validation/loss': 0.044962819665670395, 'validation/mean_average_precision': 0.2739362064526262, 'validation/num_examples': 43793, 'test/accuracy': 0.9859548211097717, 'test/loss': 0.04786033555865288, 'test/mean_average_precision': 0.2623279002542574, 'test/num_examples': 43793, 'score': 11782.979050397873, 'total_duration': 18357.6857047081, 'accumulated_submission_time': 11782.979050397873, 'accumulated_eval_time': 6571.886204242706, 'accumulated_logging_time': 1.8077149391174316}
I0205 02:54:29.514035 139758996317952 logging_writer.py:48] [36976] accumulated_eval_time=6571.886204, accumulated_logging_time=1.807715, accumulated_submission_time=11782.979050, global_step=36976, preemption_count=0, score=11782.979050, test/accuracy=0.985955, test/loss=0.047860, test/mean_average_precision=0.262328, test/num_examples=43793, total_duration=18357.685705, train/accuracy=0.992130, train/loss=0.025417, train/mean_average_precision=0.522761, validation/accuracy=0.986927, validation/loss=0.044963, validation/mean_average_precision=0.273936, validation/num_examples=43793
I0205 02:54:37.491314 139858186053376 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.09118631482124329, loss=0.031236017122864723
I0205 02:55:09.071797 139758996317952 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.1238958090543747, loss=0.034759681671857834
I0205 02:55:40.435161 139858186053376 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.09846653789281845, loss=0.033144570887088776
I0205 02:56:12.553066 139758996317952 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.09227459132671356, loss=0.031306274235248566
I0205 02:56:44.933758 139858186053376 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.09396467357873917, loss=0.030550282448530197
I0205 02:57:17.015264 139758996317952 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.09674198925495148, loss=0.03131510689854622
I0205 02:57:49.393069 139858186053376 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.08272846043109894, loss=0.03274567797780037
I0205 02:58:21.980028 139758996317952 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.14504146575927734, loss=0.03351105749607086
I0205 02:58:29.803658 139919816816448 spec.py:321] Evaluating on the training split.
I0205 03:00:31.215265 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 03:00:35.312733 139919816816448 spec.py:349] Evaluating on the test split.
I0205 03:00:38.295866 139919816816448 submission_runner.py:408] Time since start: 18726.49s, 	Step: 37725, 	{'train/accuracy': 0.992214024066925, 'train/loss': 0.025279011577367783, 'train/mean_average_precision': 0.532232625381326, 'validation/accuracy': 0.9869457483291626, 'validation/loss': 0.0449865348637104, 'validation/mean_average_precision': 0.27531428077447273, 'validation/num_examples': 43793, 'test/accuracy': 0.9860011339187622, 'test/loss': 0.048079103231430054, 'test/mean_average_precision': 0.26356978639549017, 'test/num_examples': 43793, 'score': 12023.23389339447, 'total_duration': 18726.489814043045, 'accumulated_submission_time': 12023.23389339447, 'accumulated_eval_time': 6700.37837600708, 'accumulated_logging_time': 1.8413872718811035}
I0205 03:00:38.318827 139752296675072 logging_writer.py:48] [37725] accumulated_eval_time=6700.378376, accumulated_logging_time=1.841387, accumulated_submission_time=12023.233893, global_step=37725, preemption_count=0, score=12023.233893, test/accuracy=0.986001, test/loss=0.048079, test/mean_average_precision=0.263570, test/num_examples=43793, total_duration=18726.489814, train/accuracy=0.992214, train/loss=0.025279, train/mean_average_precision=0.532233, validation/accuracy=0.986946, validation/loss=0.044987, validation/mean_average_precision=0.275314, validation/num_examples=43793
I0205 03:01:02.325883 139758987925248 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.1219324842095375, loss=0.029534632340073586
I0205 03:01:34.156370 139752296675072 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.08342285454273224, loss=0.03168167173862457
I0205 03:02:05.439081 139758987925248 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.09538265317678452, loss=0.033241938799619675
I0205 03:02:36.846116 139752296675072 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.10184775292873383, loss=0.031164275482296944
I0205 03:03:08.497487 139758987925248 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.09747408330440521, loss=0.03328317776322365
I0205 03:03:39.650595 139752296675072 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.12593598663806915, loss=0.03052709251642227
I0205 03:04:11.054229 139758987925248 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.09222950786352158, loss=0.03236766532063484
I0205 03:04:38.383667 139919816816448 spec.py:321] Evaluating on the training split.
I0205 03:06:37.710943 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 03:06:40.805855 139919816816448 spec.py:349] Evaluating on the test split.
I0205 03:06:43.792753 139919816816448 submission_runner.py:408] Time since start: 19091.99s, 	Step: 38488, 	{'train/accuracy': 0.9921120405197144, 'train/loss': 0.025479435920715332, 'train/mean_average_precision': 0.5159274884360091, 'validation/accuracy': 0.9868957996368408, 'validation/loss': 0.04491075873374939, 'validation/mean_average_precision': 0.27448658700597134, 'validation/num_examples': 43793, 'test/accuracy': 0.9859851598739624, 'test/loss': 0.04786010459065437, 'test/mean_average_precision': 0.25794411793794275, 'test/num_examples': 43793, 'score': 12263.267944574356, 'total_duration': 19091.986697912216, 'accumulated_submission_time': 12263.267944574356, 'accumulated_eval_time': 6825.7874138355255, 'accumulated_logging_time': 1.8750951290130615}
I0205 03:06:43.816790 139752480773888 logging_writer.py:48] [38488] accumulated_eval_time=6825.787414, accumulated_logging_time=1.875095, accumulated_submission_time=12263.267945, global_step=38488, preemption_count=0, score=12263.267945, test/accuracy=0.985985, test/loss=0.047860, test/mean_average_precision=0.257944, test/num_examples=43793, total_duration=19091.986698, train/accuracy=0.992112, train/loss=0.025479, train/mean_average_precision=0.515927, validation/accuracy=0.986896, validation/loss=0.044911, validation/mean_average_precision=0.274487, validation/num_examples=43793
I0205 03:06:48.165020 139858186053376 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.10621147602796555, loss=0.030060280114412308
I0205 03:07:20.009220 139752480773888 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.09058192372322083, loss=0.030045941472053528
I0205 03:07:51.472455 139858186053376 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.10254193842411041, loss=0.03306281939148903
I0205 03:08:23.987251 139752480773888 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.09861263632774353, loss=0.03183383494615555
I0205 03:08:55.712091 139858186053376 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.09927256405353546, loss=0.02937265858054161
I0205 03:09:27.345861 139752480773888 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.1241450235247612, loss=0.032870203256607056
I0205 03:09:59.128166 139858186053376 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.11855419725179672, loss=0.03179950639605522
I0205 03:10:30.695289 139752480773888 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.10824395716190338, loss=0.031918004155159
I0205 03:10:43.955644 139919816816448 spec.py:321] Evaluating on the training split.
I0205 03:12:43.984446 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 03:12:47.033754 139919816816448 spec.py:349] Evaluating on the test split.
I0205 03:12:50.015603 139919816816448 submission_runner.py:408] Time since start: 19458.21s, 	Step: 39243, 	{'train/accuracy': 0.9920024275779724, 'train/loss': 0.026045996695756912, 'train/mean_average_precision': 0.5064120084947242, 'validation/accuracy': 0.9869615435600281, 'validation/loss': 0.04505595192313194, 'validation/mean_average_precision': 0.2774751291319751, 'validation/num_examples': 43793, 'test/accuracy': 0.9859825968742371, 'test/loss': 0.0481785349547863, 'test/mean_average_precision': 0.26056772462046063, 'test/num_examples': 43793, 'score': 12503.376068115234, 'total_duration': 19458.20954990387, 'accumulated_submission_time': 12503.376068115234, 'accumulated_eval_time': 6951.847330093384, 'accumulated_logging_time': 1.91015625}
I0205 03:12:50.038949 139752296675072 logging_writer.py:48] [39243] accumulated_eval_time=6951.847330, accumulated_logging_time=1.910156, accumulated_submission_time=12503.376068, global_step=39243, preemption_count=0, score=12503.376068, test/accuracy=0.985983, test/loss=0.048179, test/mean_average_precision=0.260568, test/num_examples=43793, total_duration=19458.209550, train/accuracy=0.992002, train/loss=0.026046, train/mean_average_precision=0.506412, validation/accuracy=0.986962, validation/loss=0.045056, validation/mean_average_precision=0.277475, validation/num_examples=43793
I0205 03:13:08.215468 139758996317952 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.09352929145097733, loss=0.031070929020643234
I0205 03:13:39.992816 139752296675072 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.10940750688314438, loss=0.0331234373152256
I0205 03:14:11.484386 139758996317952 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.13633714616298676, loss=0.031703803688287735
I0205 03:14:42.733975 139752296675072 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.10170426964759827, loss=0.030035756528377533
I0205 03:15:14.462410 139758996317952 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.08957760035991669, loss=0.030348503962159157
I0205 03:15:46.009286 139752296675072 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.10072458535432816, loss=0.031455300748348236
I0205 03:16:17.756372 139758996317952 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.09557908028364182, loss=0.030001632869243622
I0205 03:16:49.138972 139752296675072 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.09574680030345917, loss=0.02923787757754326
I0205 03:16:50.097724 139919816816448 spec.py:321] Evaluating on the training split.
I0205 03:18:48.459247 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 03:18:51.502767 139919816816448 spec.py:349] Evaluating on the test split.
I0205 03:18:54.491665 139919816816448 submission_runner.py:408] Time since start: 19822.69s, 	Step: 40004, 	{'train/accuracy': 0.9919900298118591, 'train/loss': 0.025957776233553886, 'train/mean_average_precision': 0.5071904458124843, 'validation/accuracy': 0.9868775010108948, 'validation/loss': 0.0453401543200016, 'validation/mean_average_precision': 0.2724499825709285, 'validation/num_examples': 43793, 'test/accuracy': 0.9859552383422852, 'test/loss': 0.048367008566856384, 'test/mean_average_precision': 0.2582643340499317, 'test/num_examples': 43793, 'score': 12743.403692007065, 'total_duration': 19822.685611486435, 'accumulated_submission_time': 12743.403692007065, 'accumulated_eval_time': 7076.241222858429, 'accumulated_logging_time': 1.944312334060669}
I0205 03:18:54.514298 139752480773888 logging_writer.py:48] [40004] accumulated_eval_time=7076.241223, accumulated_logging_time=1.944312, accumulated_submission_time=12743.403692, global_step=40004, preemption_count=0, score=12743.403692, test/accuracy=0.985955, test/loss=0.048367, test/mean_average_precision=0.258264, test/num_examples=43793, total_duration=19822.685611, train/accuracy=0.991990, train/loss=0.025958, train/mean_average_precision=0.507190, validation/accuracy=0.986878, validation/loss=0.045340, validation/mean_average_precision=0.272450, validation/num_examples=43793
I0205 03:19:25.502227 139758987925248 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.13562123477458954, loss=0.03034316562116146
I0205 03:19:57.118975 139752480773888 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.09250286966562271, loss=0.029198141768574715
I0205 03:20:28.726294 139758987925248 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.09699492156505585, loss=0.028923900797963142
I0205 03:21:00.289629 139752480773888 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.10743992030620575, loss=0.029783587902784348
I0205 03:21:31.987303 139758987925248 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.12110747396945953, loss=0.03357361629605293
I0205 03:22:03.542467 139752480773888 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.08873023092746735, loss=0.02684137038886547
I0205 03:22:34.990015 139758987925248 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.0942312702536583, loss=0.028930703178048134
I0205 03:22:54.518787 139919816816448 spec.py:321] Evaluating on the training split.
I0205 03:24:56.366222 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 03:24:59.465494 139919816816448 spec.py:349] Evaluating on the test split.
I0205 03:25:02.623458 139919816816448 submission_runner.py:408] Time since start: 20190.82s, 	Step: 40763, 	{'train/accuracy': 0.9918793439865112, 'train/loss': 0.026220152154564857, 'train/mean_average_precision': 0.4992228026531985, 'validation/accuracy': 0.9869359731674194, 'validation/loss': 0.04502296820282936, 'validation/mean_average_precision': 0.2775892452836066, 'validation/num_examples': 43793, 'test/accuracy': 0.9860268235206604, 'test/loss': 0.04802815243601799, 'test/mean_average_precision': 0.2584114993433583, 'test/num_examples': 43793, 'score': 12983.375659227371, 'total_duration': 20190.81739640236, 'accumulated_submission_time': 12983.375659227371, 'accumulated_eval_time': 7204.345838546753, 'accumulated_logging_time': 1.9792215824127197}
I0205 03:25:02.648551 139758996317952 logging_writer.py:48] [40763] accumulated_eval_time=7204.345839, accumulated_logging_time=1.979222, accumulated_submission_time=12983.375659, global_step=40763, preemption_count=0, score=12983.375659, test/accuracy=0.986027, test/loss=0.048028, test/mean_average_precision=0.258411, test/num_examples=43793, total_duration=20190.817396, train/accuracy=0.991879, train/loss=0.026220, train/mean_average_precision=0.499223, validation/accuracy=0.986936, validation/loss=0.045023, validation/mean_average_precision=0.277589, validation/num_examples=43793
I0205 03:25:14.690374 139858186053376 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.09841112792491913, loss=0.03137396648526192
I0205 03:25:46.327164 139758996317952 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.11071546375751495, loss=0.03073607198894024
I0205 03:26:17.997441 139858186053376 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.10205858945846558, loss=0.02945667877793312
I0205 03:26:49.472518 139758996317952 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.10332146286964417, loss=0.03195956349372864
I0205 03:27:21.203144 139858186053376 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.09106721729040146, loss=0.03201979771256447
I0205 03:27:52.794063 139758996317952 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.15009701251983643, loss=0.02976599894464016
I0205 03:28:24.564169 139858186053376 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.13209135830402374, loss=0.03254291042685509
I0205 03:28:56.417225 139758996317952 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.08877921849489212, loss=0.028139173984527588
I0205 03:29:02.847630 139919816816448 spec.py:321] Evaluating on the training split.
I0205 03:30:59.988707 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 03:31:03.467675 139919816816448 spec.py:349] Evaluating on the test split.
I0205 03:31:06.847499 139919816816448 submission_runner.py:408] Time since start: 20555.04s, 	Step: 41521, 	{'train/accuracy': 0.9919157028198242, 'train/loss': 0.026108678430318832, 'train/mean_average_precision': 0.5037610390684488, 'validation/accuracy': 0.986825168132782, 'validation/loss': 0.0457451306283474, 'validation/mean_average_precision': 0.2686163429857319, 'validation/num_examples': 43793, 'test/accuracy': 0.9859375357627869, 'test/loss': 0.04884037375450134, 'test/mean_average_precision': 0.2549508720399013, 'test/num_examples': 43793, 'score': 13223.54290318489, 'total_duration': 20555.041393518448, 'accumulated_submission_time': 13223.54290318489, 'accumulated_eval_time': 7328.3456082344055, 'accumulated_logging_time': 2.01588773727417}
I0205 03:31:06.875061 139752296675072 logging_writer.py:48] [41521] accumulated_eval_time=7328.345608, accumulated_logging_time=2.015888, accumulated_submission_time=13223.542903, global_step=41521, preemption_count=0, score=13223.542903, test/accuracy=0.985938, test/loss=0.048840, test/mean_average_precision=0.254951, test/num_examples=43793, total_duration=20555.041394, train/accuracy=0.991916, train/loss=0.026109, train/mean_average_precision=0.503761, validation/accuracy=0.986825, validation/loss=0.045745, validation/mean_average_precision=0.268616, validation/num_examples=43793
I0205 03:31:33.195847 139752480773888 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.1130729392170906, loss=0.03000658005475998
I0205 03:32:05.264072 139752296675072 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.11330283433198929, loss=0.03202509507536888
I0205 03:32:36.970290 139752480773888 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.13920851051807404, loss=0.03395378589630127
I0205 03:33:08.427685 139752296675072 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.09623266011476517, loss=0.03032275289297104
I0205 03:33:40.136116 139752480773888 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.0942753478884697, loss=0.027333831414580345
I0205 03:34:12.425003 139752296675072 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.10422470420598984, loss=0.03032151237130165
I0205 03:34:43.982405 139752480773888 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.11782374978065491, loss=0.02791803888976574
I0205 03:35:06.870992 139919816816448 spec.py:321] Evaluating on the training split.
I0205 03:37:06.341333 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 03:37:09.771578 139919816816448 spec.py:349] Evaluating on the test split.
I0205 03:37:13.196718 139919816816448 submission_runner.py:408] Time since start: 20921.39s, 	Step: 42273, 	{'train/accuracy': 0.9921371936798096, 'train/loss': 0.025424858555197716, 'train/mean_average_precision': 0.517208637472588, 'validation/accuracy': 0.986750066280365, 'validation/loss': 0.045256342738866806, 'validation/mean_average_precision': 0.2774649701945903, 'validation/num_examples': 43793, 'test/accuracy': 0.985990583896637, 'test/loss': 0.04822531342506409, 'test/mean_average_precision': 0.2598064917046744, 'test/num_examples': 43793, 'score': 13463.506523132324, 'total_duration': 20921.390644073486, 'accumulated_submission_time': 13463.506523132324, 'accumulated_eval_time': 7454.671268939972, 'accumulated_logging_time': 2.0549404621124268}
I0205 03:37:13.223484 139758996317952 logging_writer.py:48] [42273] accumulated_eval_time=7454.671269, accumulated_logging_time=2.054940, accumulated_submission_time=13463.506523, global_step=42273, preemption_count=0, score=13463.506523, test/accuracy=0.985991, test/loss=0.048225, test/mean_average_precision=0.259806, test/num_examples=43793, total_duration=20921.390644, train/accuracy=0.992137, train/loss=0.025425, train/mean_average_precision=0.517209, validation/accuracy=0.986750, validation/loss=0.045256, validation/mean_average_precision=0.277465, validation/num_examples=43793
I0205 03:37:22.209892 139858186053376 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.12433584779500961, loss=0.029245270416140556
I0205 03:37:54.068355 139758996317952 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.15008194744586945, loss=0.030590511858463287
I0205 03:38:25.326292 139858186053376 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.12038891762495041, loss=0.02796708047389984
I0205 03:38:56.657157 139758996317952 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.11096829921007156, loss=0.03044714778661728
I0205 03:39:28.669614 139858186053376 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.09897162765264511, loss=0.03121918812394142
I0205 03:40:00.209019 139758996317952 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.11249303072690964, loss=0.02850271202623844
I0205 03:40:32.149032 139858186053376 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.09647740423679352, loss=0.029708636924624443
I0205 03:41:03.805219 139758996317952 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.09998196363449097, loss=0.029676392674446106
I0205 03:41:13.253384 139919816816448 spec.py:321] Evaluating on the training split.
I0205 03:43:15.632256 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 03:43:18.709923 139919816816448 spec.py:349] Evaluating on the test split.
I0205 03:43:21.745890 139919816816448 submission_runner.py:408] Time since start: 21289.94s, 	Step: 43031, 	{'train/accuracy': 0.9923945665359497, 'train/loss': 0.024646632373332977, 'train/mean_average_precision': 0.5287445837948694, 'validation/accuracy': 0.9867159724235535, 'validation/loss': 0.04515005648136139, 'validation/mean_average_precision': 0.27577266251505045, 'validation/num_examples': 43793, 'test/accuracy': 0.9858831763267517, 'test/loss': 0.04805005341768265, 'test/mean_average_precision': 0.2615175880406216, 'test/num_examples': 43793, 'score': 13703.503736972809, 'total_duration': 21289.939838171005, 'accumulated_submission_time': 13703.503736972809, 'accumulated_eval_time': 7583.163735151291, 'accumulated_logging_time': 2.093395233154297}
I0205 03:43:21.769313 139752296675072 logging_writer.py:48] [43031] accumulated_eval_time=7583.163735, accumulated_logging_time=2.093395, accumulated_submission_time=13703.503737, global_step=43031, preemption_count=0, score=13703.503737, test/accuracy=0.985883, test/loss=0.048050, test/mean_average_precision=0.261518, test/num_examples=43793, total_duration=21289.939838, train/accuracy=0.992395, train/loss=0.024647, train/mean_average_precision=0.528745, validation/accuracy=0.986716, validation/loss=0.045150, validation/mean_average_precision=0.275773, validation/num_examples=43793
I0205 03:43:45.690828 139752480773888 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.09892062842845917, loss=0.027489295229315758
I0205 03:44:17.344843 139752296675072 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.1251884400844574, loss=0.028253668919205666
I0205 03:44:49.000430 139752480773888 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.16524942219257355, loss=0.031128136441111565
I0205 03:45:20.372561 139752296675072 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.14349059760570526, loss=0.029365383088588715
I0205 03:45:52.009610 139752480773888 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.11571134626865387, loss=0.029750464484095573
I0205 03:46:24.757773 139752296675072 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.09246093034744263, loss=0.026794204488396645
I0205 03:46:57.432696 139752480773888 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.1311735361814499, loss=0.03134358301758766
I0205 03:47:21.858477 139919816816448 spec.py:321] Evaluating on the training split.
I0205 03:49:23.879436 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 03:49:26.936882 139919816816448 spec.py:349] Evaluating on the test split.
I0205 03:49:30.010940 139919816816448 submission_runner.py:408] Time since start: 21658.20s, 	Step: 43777, 	{'train/accuracy': 0.9924582839012146, 'train/loss': 0.0242256261408329, 'train/mean_average_precision': 0.5548626450600167, 'validation/accuracy': 0.9868783354759216, 'validation/loss': 0.045423127710819244, 'validation/mean_average_precision': 0.27506193304726434, 'validation/num_examples': 43793, 'test/accuracy': 0.986004114151001, 'test/loss': 0.04848497733473778, 'test/mean_average_precision': 0.2589780692558592, 'test/num_examples': 43793, 'score': 13943.558982849121, 'total_duration': 21658.204888105392, 'accumulated_submission_time': 13943.558982849121, 'accumulated_eval_time': 7711.316171169281, 'accumulated_logging_time': 2.129408359527588}
I0205 03:49:30.034709 139758987925248 logging_writer.py:48] [43777] accumulated_eval_time=7711.316171, accumulated_logging_time=2.129408, accumulated_submission_time=13943.558983, global_step=43777, preemption_count=0, score=13943.558983, test/accuracy=0.986004, test/loss=0.048485, test/mean_average_precision=0.258978, test/num_examples=43793, total_duration=21658.204888, train/accuracy=0.992458, train/loss=0.024226, train/mean_average_precision=0.554863, validation/accuracy=0.986878, validation/loss=0.045423, validation/mean_average_precision=0.275062, validation/num_examples=43793
I0205 03:49:37.868290 139758996317952 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.12294719368219376, loss=0.031833846122026443
I0205 03:50:09.626195 139758987925248 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.08714084327220917, loss=0.02723296731710434
I0205 03:50:41.362689 139758996317952 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.10102877020835876, loss=0.031199811026453972
I0205 03:51:13.442906 139758987925248 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.1015409603714943, loss=0.028683455660939217
I0205 03:51:45.536662 139758996317952 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.1237594336271286, loss=0.03383206948637962
I0205 03:52:17.048707 139758987925248 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.10373780876398087, loss=0.030171463266015053
I0205 03:52:48.654963 139758996317952 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.10123130679130554, loss=0.02908778190612793
I0205 03:53:20.163774 139758987925248 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.09552008658647537, loss=0.02733618952333927
I0205 03:53:30.128738 139919816816448 spec.py:321] Evaluating on the training split.
I0205 03:55:27.032176 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 03:55:30.134547 139919816816448 spec.py:349] Evaluating on the test split.
I0205 03:55:33.241966 139919816816448 submission_runner.py:408] Time since start: 22021.44s, 	Step: 44533, 	{'train/accuracy': 0.9928352236747742, 'train/loss': 0.023032398894429207, 'train/mean_average_precision': 0.582281300015573, 'validation/accuracy': 0.9868206977844238, 'validation/loss': 0.04551013559103012, 'validation/mean_average_precision': 0.2740454856114717, 'validation/num_examples': 43793, 'test/accuracy': 0.9860752820968628, 'test/loss': 0.048421405255794525, 'test/mean_average_precision': 0.2575476179842053, 'test/num_examples': 43793, 'score': 14183.621313095093, 'total_duration': 22021.435887813568, 'accumulated_submission_time': 14183.621313095093, 'accumulated_eval_time': 7834.429327011108, 'accumulated_logging_time': 2.1637284755706787}
I0205 03:55:33.269331 139752296675072 logging_writer.py:48] [44533] accumulated_eval_time=7834.429327, accumulated_logging_time=2.163728, accumulated_submission_time=14183.621313, global_step=44533, preemption_count=0, score=14183.621313, test/accuracy=0.986075, test/loss=0.048421, test/mean_average_precision=0.257548, test/num_examples=43793, total_duration=22021.435888, train/accuracy=0.992835, train/loss=0.023032, train/mean_average_precision=0.582281, validation/accuracy=0.986821, validation/loss=0.045510, validation/mean_average_precision=0.274045, validation/num_examples=43793
I0205 03:55:55.569757 139752480773888 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.09582912921905518, loss=0.029543161392211914
I0205 03:56:28.136183 139752296675072 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.09886278957128525, loss=0.027020921930670738
I0205 03:57:00.742763 139752480773888 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.10348794609308243, loss=0.02575874514877796
I0205 03:57:33.275074 139752296675072 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.11421209573745728, loss=0.02867899462580681
I0205 03:58:06.155098 139752480773888 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.1145617663860321, loss=0.029633846133947372
I0205 03:58:38.795277 139752296675072 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.11440125107765198, loss=0.029234638437628746
I0205 03:59:11.397699 139752480773888 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.10377418994903564, loss=0.030585968866944313
I0205 03:59:33.261969 139919816816448 spec.py:321] Evaluating on the training split.
I0205 04:01:38.126961 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 04:01:41.212107 139919816816448 spec.py:349] Evaluating on the test split.
I0205 04:01:44.289189 139919816816448 submission_runner.py:408] Time since start: 22392.48s, 	Step: 45269, 	{'train/accuracy': 0.9929222464561462, 'train/loss': 0.022949127480387688, 'train/mean_average_precision': 0.5656440620216915, 'validation/accuracy': 0.9867017269134521, 'validation/loss': 0.04587439075112343, 'validation/mean_average_precision': 0.27031047176916695, 'validation/num_examples': 43793, 'test/accuracy': 0.9858570694923401, 'test/loss': 0.048946723341941833, 'test/mean_average_precision': 0.25800982874321193, 'test/num_examples': 43793, 'score': 14423.575603961945, 'total_duration': 22392.48312997818, 'accumulated_submission_time': 14423.575603961945, 'accumulated_eval_time': 7965.456509590149, 'accumulated_logging_time': 2.2040088176727295}
I0205 04:01:44.314353 139758996317952 logging_writer.py:48] [45269] accumulated_eval_time=7965.456510, accumulated_logging_time=2.204009, accumulated_submission_time=14423.575604, global_step=45269, preemption_count=0, score=14423.575604, test/accuracy=0.985857, test/loss=0.048947, test/mean_average_precision=0.258010, test/num_examples=43793, total_duration=22392.483130, train/accuracy=0.992922, train/loss=0.022949, train/mean_average_precision=0.565644, validation/accuracy=0.986702, validation/loss=0.045874, validation/mean_average_precision=0.270310, validation/num_examples=43793
I0205 04:01:54.574033 139858186053376 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.10775481164455414, loss=0.027252068743109703
I0205 04:02:26.464937 139758996317952 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.12186837196350098, loss=0.028925947844982147
I0205 04:02:58.279329 139858186053376 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.11800821125507355, loss=0.028833353891968727
I0205 04:03:30.605019 139758996317952 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.11742759495973587, loss=0.031361378729343414
I0205 04:04:03.009291 139858186053376 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.10080525279045105, loss=0.027891572564840317
I0205 04:04:34.595495 139758996317952 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.12105035781860352, loss=0.031443145126104355
I0205 04:05:06.359757 139858186053376 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.10899894684553146, loss=0.032478563487529755
I0205 04:05:37.775176 139758996317952 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.10149272531270981, loss=0.028329430148005486
I0205 04:05:44.318862 139919816816448 spec.py:321] Evaluating on the training split.
I0205 04:07:41.325653 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 04:07:44.393173 139919816816448 spec.py:349] Evaluating on the test split.
I0205 04:07:47.457322 139919816816448 submission_runner.py:408] Time since start: 22755.65s, 	Step: 46022, 	{'train/accuracy': 0.9929423928260803, 'train/loss': 0.022848380729556084, 'train/mean_average_precision': 0.5816965987814018, 'validation/accuracy': 0.9868153929710388, 'validation/loss': 0.04570256546139717, 'validation/mean_average_precision': 0.27749283056789126, 'validation/num_examples': 43793, 'test/accuracy': 0.9859346151351929, 'test/loss': 0.04876365885138512, 'test/mean_average_precision': 0.2573021483270661, 'test/num_examples': 43793, 'score': 14663.548845529556, 'total_duration': 22755.651258468628, 'accumulated_submission_time': 14663.548845529556, 'accumulated_eval_time': 8088.594908952713, 'accumulated_logging_time': 2.2400035858154297}
I0205 04:07:47.482136 139752296675072 logging_writer.py:48] [46022] accumulated_eval_time=8088.594909, accumulated_logging_time=2.240004, accumulated_submission_time=14663.548846, global_step=46022, preemption_count=0, score=14663.548846, test/accuracy=0.985935, test/loss=0.048764, test/mean_average_precision=0.257302, test/num_examples=43793, total_duration=22755.651258, train/accuracy=0.992942, train/loss=0.022848, train/mean_average_precision=0.581697, validation/accuracy=0.986815, validation/loss=0.045703, validation/mean_average_precision=0.277493, validation/num_examples=43793
I0205 04:08:12.368810 139758987925248 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.13055400550365448, loss=0.027408435940742493
I0205 04:08:44.239896 139752296675072 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.10709957778453827, loss=0.028898518532514572
I0205 04:09:16.336636 139758987925248 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.1256999969482422, loss=0.030336931347846985
I0205 04:09:48.367669 139752296675072 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.11372965574264526, loss=0.026666468009352684
I0205 04:10:20.450590 139758987925248 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.11098183691501617, loss=0.026618367061018944
I0205 04:10:51.801540 139752296675072 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.14571605622768402, loss=0.03062293864786625
I0205 04:11:23.852666 139758987925248 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.11734478175640106, loss=0.030237577855587006
I0205 04:11:47.718296 139919816816448 spec.py:321] Evaluating on the training split.
I0205 04:13:46.585560 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 04:13:49.675365 139919816816448 spec.py:349] Evaluating on the test split.
I0205 04:13:52.696188 139919816816448 submission_runner.py:408] Time since start: 23120.89s, 	Step: 46775, 	{'train/accuracy': 0.9926509261131287, 'train/loss': 0.02339848317205906, 'train/mean_average_precision': 0.5500882324221318, 'validation/accuracy': 0.9869351387023926, 'validation/loss': 0.04617488384246826, 'validation/mean_average_precision': 0.27597097459884756, 'validation/num_examples': 43793, 'test/accuracy': 0.9860348105430603, 'test/loss': 0.04929579421877861, 'test/mean_average_precision': 0.26160574698396855, 'test/num_examples': 43793, 'score': 14903.751368761063, 'total_duration': 23120.890134334564, 'accumulated_submission_time': 14903.751368761063, 'accumulated_eval_time': 8213.572768211365, 'accumulated_logging_time': 2.277043104171753}
I0205 04:13:52.720815 139739177916160 logging_writer.py:48] [46775] accumulated_eval_time=8213.572768, accumulated_logging_time=2.277043, accumulated_submission_time=14903.751369, global_step=46775, preemption_count=0, score=14903.751369, test/accuracy=0.986035, test/loss=0.049296, test/mean_average_precision=0.261606, test/num_examples=43793, total_duration=23120.890134, train/accuracy=0.992651, train/loss=0.023398, train/mean_average_precision=0.550088, validation/accuracy=0.986935, validation/loss=0.046175, validation/mean_average_precision=0.275971, validation/num_examples=43793
I0205 04:14:01.065803 139752480773888 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.12485985457897186, loss=0.030127214267849922
I0205 04:14:33.160547 139739177916160 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.12237386405467987, loss=0.02696751430630684
I0205 04:15:05.211112 139752480773888 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.12051159143447876, loss=0.028671851381659508
I0205 04:15:37.887451 139739177916160 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.11676651239395142, loss=0.03007744811475277
I0205 04:16:10.089349 139752480773888 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.11343228816986084, loss=0.027019165456295013
I0205 04:16:41.674209 139739177916160 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.12192795425653458, loss=0.02890627831220627
I0205 04:17:13.459243 139752480773888 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.12408196181058884, loss=0.02911008894443512
I0205 04:17:44.911458 139739177916160 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.11052943021059036, loss=0.02808685414493084
I0205 04:17:52.801933 139919816816448 spec.py:321] Evaluating on the training split.
I0205 04:19:50.406997 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 04:19:53.457517 139919816816448 spec.py:349] Evaluating on the test split.
I0205 04:19:56.479580 139919816816448 submission_runner.py:408] Time since start: 23484.67s, 	Step: 47526, 	{'train/accuracy': 0.9926035404205322, 'train/loss': 0.02364971674978733, 'train/mean_average_precision': 0.5587826948908996, 'validation/accuracy': 0.9867784976959229, 'validation/loss': 0.046003635972738266, 'validation/mean_average_precision': 0.2770768478543392, 'validation/num_examples': 43793, 'test/accuracy': 0.985968291759491, 'test/loss': 0.04908977448940277, 'test/mean_average_precision': 0.2601720723494071, 'test/num_examples': 43793, 'score': 15143.801016807556, 'total_duration': 23484.67352104187, 'accumulated_submission_time': 15143.801016807556, 'accumulated_eval_time': 8337.250361442566, 'accumulated_logging_time': 2.31246018409729}
I0205 04:19:56.504579 139752296675072 logging_writer.py:48] [47526] accumulated_eval_time=8337.250361, accumulated_logging_time=2.312460, accumulated_submission_time=15143.801017, global_step=47526, preemption_count=0, score=15143.801017, test/accuracy=0.985968, test/loss=0.049090, test/mean_average_precision=0.260172, test/num_examples=43793, total_duration=23484.673521, train/accuracy=0.992604, train/loss=0.023650, train/mean_average_precision=0.558783, validation/accuracy=0.986778, validation/loss=0.046004, validation/mean_average_precision=0.277077, validation/num_examples=43793
I0205 04:20:20.152896 139758987925248 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.09946178644895554, loss=0.027361681684851646
I0205 04:20:51.692584 139752296675072 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.12348362058401108, loss=0.03013087436556816
I0205 04:21:24.251141 139758987925248 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.11535099148750305, loss=0.027730295434594154
I0205 04:21:56.865428 139752296675072 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.1217295303940773, loss=0.027948999777436256
I0205 04:22:29.791850 139758987925248 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.13206559419631958, loss=0.02825259044766426
I0205 04:23:02.621337 139752296675072 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.10896316170692444, loss=0.024415867403149605
I0205 04:23:34.363971 139758987925248 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.10180655121803284, loss=0.02572917565703392
I0205 04:23:56.620844 139919816816448 spec.py:321] Evaluating on the training split.
I0205 04:25:54.214979 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 04:25:57.344233 139919816816448 spec.py:349] Evaluating on the test split.
I0205 04:26:00.375569 139919816816448 submission_runner.py:408] Time since start: 23848.57s, 	Step: 48271, 	{'train/accuracy': 0.9926998019218445, 'train/loss': 0.02341970056295395, 'train/mean_average_precision': 0.5763855108885503, 'validation/accuracy': 0.9868454337120056, 'validation/loss': 0.04597317427396774, 'validation/mean_average_precision': 0.2784930722589843, 'validation/num_examples': 43793, 'test/accuracy': 0.986004114151001, 'test/loss': 0.04900640994310379, 'test/mean_average_precision': 0.26046014519958743, 'test/num_examples': 43793, 'score': 15383.883342027664, 'total_duration': 23848.569514989853, 'accumulated_submission_time': 15383.883342027664, 'accumulated_eval_time': 8461.005041599274, 'accumulated_logging_time': 2.348806619644165}
I0205 04:26:00.400539 139739177916160 logging_writer.py:48] [48271] accumulated_eval_time=8461.005042, accumulated_logging_time=2.348807, accumulated_submission_time=15383.883342, global_step=48271, preemption_count=0, score=15383.883342, test/accuracy=0.986004, test/loss=0.049006, test/mean_average_precision=0.260460, test/num_examples=43793, total_duration=23848.569515, train/accuracy=0.992700, train/loss=0.023420, train/mean_average_precision=0.576386, validation/accuracy=0.986845, validation/loss=0.045973, validation/mean_average_precision=0.278493, validation/num_examples=43793
I0205 04:26:09.975269 139758996317952 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.12518055737018585, loss=0.02901248075067997
I0205 04:26:41.453128 139739177916160 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.13593940436840057, loss=0.03148377314209938
I0205 04:27:12.824664 139758996317952 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.10620523244142532, loss=0.0257516261190176
I0205 04:27:44.158237 139739177916160 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.11590497940778732, loss=0.027040405198931694
I0205 04:28:15.623289 139758996317952 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.1424541026353836, loss=0.03024519979953766
I0205 04:28:47.121336 139739177916160 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.11349241435527802, loss=0.02668335847556591
I0205 04:29:19.085764 139758996317952 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.12658420205116272, loss=0.025514904409646988
I0205 04:29:50.521176 139739177916160 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.13793011009693146, loss=0.029711071401834488
I0205 04:30:00.620768 139919816816448 spec.py:321] Evaluating on the training split.
I0205 04:32:00.226812 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 04:32:03.430649 139919816816448 spec.py:349] Evaluating on the test split.
I0205 04:32:06.447206 139919816816448 submission_runner.py:408] Time since start: 24214.64s, 	Step: 49033, 	{'train/accuracy': 0.9927592277526855, 'train/loss': 0.023252515122294426, 'train/mean_average_precision': 0.5598052472954665, 'validation/accuracy': 0.9868369102478027, 'validation/loss': 0.04622039198875427, 'validation/mean_average_precision': 0.276565655192867, 'validation/num_examples': 43793, 'test/accuracy': 0.9859733581542969, 'test/loss': 0.04939107596874237, 'test/mean_average_precision': 0.2595510327017606, 'test/num_examples': 43793, 'score': 15624.072097301483, 'total_duration': 24214.641152620316, 'accumulated_submission_time': 15624.072097301483, 'accumulated_eval_time': 8586.831431388855, 'accumulated_logging_time': 2.3845295906066895}
I0205 04:32:06.472170 139752296675072 logging_writer.py:48] [49033] accumulated_eval_time=8586.831431, accumulated_logging_time=2.384530, accumulated_submission_time=15624.072097, global_step=49033, preemption_count=0, score=15624.072097, test/accuracy=0.985973, test/loss=0.049391, test/mean_average_precision=0.259551, test/num_examples=43793, total_duration=24214.641153, train/accuracy=0.992759, train/loss=0.023253, train/mean_average_precision=0.559805, validation/accuracy=0.986837, validation/loss=0.046220, validation/mean_average_precision=0.276566, validation/num_examples=43793
I0205 04:32:28.030098 139752480773888 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.11864756047725677, loss=0.028322698548436165
I0205 04:32:59.509646 139752296675072 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.15416349470615387, loss=0.02682674489915371
I0205 04:33:32.128698 139752480773888 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.11001639068126678, loss=0.02682492882013321
I0205 04:34:03.953344 139752296675072 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.1297362595796585, loss=0.02861696109175682
I0205 04:34:35.604577 139752480773888 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.13466967642307281, loss=0.026625411584973335
I0205 04:35:07.322703 139752296675072 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.12549610435962677, loss=0.027681827545166016
I0205 04:35:38.991547 139752480773888 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.11912130564451218, loss=0.026491697877645493
I0205 04:36:06.542315 139919816816448 spec.py:321] Evaluating on the training split.
I0205 04:38:04.941493 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 04:38:08.068737 139919816816448 spec.py:349] Evaluating on the test split.
I0205 04:38:11.080564 139919816816448 submission_runner.py:408] Time since start: 24579.27s, 	Step: 49787, 	{'train/accuracy': 0.992779016494751, 'train/loss': 0.02304934710264206, 'train/mean_average_precision': 0.557587615426931, 'validation/accuracy': 0.9867760539054871, 'validation/loss': 0.046717461198568344, 'validation/mean_average_precision': 0.2682286383885479, 'validation/num_examples': 43793, 'test/accuracy': 0.9858781695365906, 'test/loss': 0.04981175810098648, 'test/mean_average_precision': 0.25385138040262395, 'test/num_examples': 43793, 'score': 15864.111089468002, 'total_duration': 24579.27449965477, 'accumulated_submission_time': 15864.111089468002, 'accumulated_eval_time': 8711.369632005692, 'accumulated_logging_time': 2.420367956161499}
I0205 04:38:11.105922 139739177916160 logging_writer.py:48] [49787] accumulated_eval_time=8711.369632, accumulated_logging_time=2.420368, accumulated_submission_time=15864.111089, global_step=49787, preemption_count=0, score=15864.111089, test/accuracy=0.985878, test/loss=0.049812, test/mean_average_precision=0.253851, test/num_examples=43793, total_duration=24579.274500, train/accuracy=0.992779, train/loss=0.023049, train/mean_average_precision=0.557588, validation/accuracy=0.986776, validation/loss=0.046717, validation/mean_average_precision=0.268229, validation/num_examples=43793
I0205 04:38:15.625051 139758987925248 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.12456213682889938, loss=0.027984432876110077
I0205 04:38:47.351018 139739177916160 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.1300240010023117, loss=0.027399178594350815
I0205 04:39:19.445910 139758987925248 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.15258027613162994, loss=0.027393177151679993
I0205 04:39:51.252517 139739177916160 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.151153102517128, loss=0.02583666332066059
I0205 04:40:23.081423 139758987925248 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.12086985260248184, loss=0.028385095298290253
I0205 04:40:54.409815 139739177916160 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.12353415787220001, loss=0.0269106924533844
I0205 04:41:26.511915 139758987925248 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.13613487780094147, loss=0.028084803372621536
I0205 04:41:58.444533 139739177916160 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.13195429742336273, loss=0.02779758721590042
I0205 04:42:11.175341 139919816816448 spec.py:321] Evaluating on the training split.
I0205 04:44:06.462268 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 04:44:09.498111 139919816816448 spec.py:349] Evaluating on the test split.
I0205 04:44:12.509121 139919816816448 submission_runner.py:408] Time since start: 24940.70s, 	Step: 50541, 	{'train/accuracy': 0.9930933713912964, 'train/loss': 0.021858109161257744, 'train/mean_average_precision': 0.5951135774073243, 'validation/accuracy': 0.986832857131958, 'validation/loss': 0.04674899950623512, 'validation/mean_average_precision': 0.27699800295007676, 'validation/num_examples': 43793, 'test/accuracy': 0.9860200881958008, 'test/loss': 0.04974102973937988, 'test/mean_average_precision': 0.2600190963165037, 'test/num_examples': 43793, 'score': 16104.149505615234, 'total_duration': 24940.703069210052, 'accumulated_submission_time': 16104.149505615234, 'accumulated_eval_time': 8832.70336985588, 'accumulated_logging_time': 2.456578016281128}
I0205 04:44:12.534533 139752480773888 logging_writer.py:48] [50541] accumulated_eval_time=8832.703370, accumulated_logging_time=2.456578, accumulated_submission_time=16104.149506, global_step=50541, preemption_count=0, score=16104.149506, test/accuracy=0.986020, test/loss=0.049741, test/mean_average_precision=0.260019, test/num_examples=43793, total_duration=24940.703069, train/accuracy=0.993093, train/loss=0.021858, train/mean_average_precision=0.595114, validation/accuracy=0.986833, validation/loss=0.046749, validation/mean_average_precision=0.276998, validation/num_examples=43793
I0205 04:44:31.648190 139758996317952 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.11609251797199249, loss=0.02400059811770916
I0205 04:45:03.084069 139752480773888 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.16638827323913574, loss=0.02771805040538311
I0205 04:45:34.874094 139758996317952 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.15328124165534973, loss=0.02823842130601406
I0205 04:46:07.096439 139752480773888 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.12083349376916885, loss=0.025213731452822685
I0205 04:46:38.729252 139758996317952 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.12434852868318558, loss=0.025762105360627174
I0205 04:47:10.354453 139752480773888 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.13196370005607605, loss=0.026515159755945206
I0205 04:47:42.112796 139758996317952 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.1301596313714981, loss=0.02692393958568573
I0205 04:48:12.517303 139919816816448 spec.py:321] Evaluating on the training split.
I0205 04:50:11.917086 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 04:50:14.960202 139919816816448 spec.py:349] Evaluating on the test split.
I0205 04:50:17.998736 139919816816448 submission_runner.py:408] Time since start: 25306.19s, 	Step: 51297, 	{'train/accuracy': 0.9931976199150085, 'train/loss': 0.021701239049434662, 'train/mean_average_precision': 0.591113654143453, 'validation/accuracy': 0.9867837429046631, 'validation/loss': 0.046719688922166824, 'validation/mean_average_precision': 0.276880985821581, 'validation/num_examples': 43793, 'test/accuracy': 0.9858894944190979, 'test/loss': 0.049776021391153336, 'test/mean_average_precision': 0.26097687724729496, 'test/num_examples': 43793, 'score': 16344.100379228592, 'total_duration': 25306.192680835724, 'accumulated_submission_time': 16344.100379228592, 'accumulated_eval_time': 8958.184759140015, 'accumulated_logging_time': 2.49343204498291}
I0205 04:50:18.024312 139739177916160 logging_writer.py:48] [51297] accumulated_eval_time=8958.184759, accumulated_logging_time=2.493432, accumulated_submission_time=16344.100379, global_step=51297, preemption_count=0, score=16344.100379, test/accuracy=0.985889, test/loss=0.049776, test/mean_average_precision=0.260977, test/num_examples=43793, total_duration=25306.192681, train/accuracy=0.993198, train/loss=0.021701, train/mean_average_precision=0.591114, validation/accuracy=0.986784, validation/loss=0.046720, validation/mean_average_precision=0.276881, validation/num_examples=43793
I0205 04:50:19.336235 139758987925248 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.15341173112392426, loss=0.029856914654374123
I0205 04:50:50.917702 139739177916160 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.14176936447620392, loss=0.02753528207540512
I0205 04:51:22.434589 139758987925248 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.1363801211118698, loss=0.02794828824698925
I0205 04:51:54.284357 139739177916160 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.13449612259864807, loss=0.02485678903758526
I0205 04:52:25.832936 139758987925248 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.16698725521564484, loss=0.027385246008634567
I0205 04:52:57.086868 139739177916160 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.14247940480709076, loss=0.02924508973956108
I0205 04:53:29.438279 139758987925248 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.16290225088596344, loss=0.02852693572640419
I0205 04:54:02.077373 139739177916160 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.14781007170677185, loss=0.028384244069457054
I0205 04:54:18.308591 139919816816448 spec.py:321] Evaluating on the training split.
I0205 04:56:15.823645 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 04:56:18.876925 139919816816448 spec.py:349] Evaluating on the test split.
I0205 04:56:21.861982 139919816816448 submission_runner.py:408] Time since start: 25670.06s, 	Step: 52052, 	{'train/accuracy': 0.9936760067939758, 'train/loss': 0.02026781253516674, 'train/mean_average_precision': 0.6317658547986209, 'validation/accuracy': 0.9868324398994446, 'validation/loss': 0.04677682742476463, 'validation/mean_average_precision': 0.2784929432021214, 'validation/num_examples': 43793, 'test/accuracy': 0.985987663269043, 'test/loss': 0.049991969019174576, 'test/mean_average_precision': 0.2584287179698735, 'test/num_examples': 43793, 'score': 16584.350385904312, 'total_duration': 25670.05592918396, 'accumulated_submission_time': 16584.350385904312, 'accumulated_eval_time': 9081.7381067276, 'accumulated_logging_time': 2.5313422679901123}
I0205 04:56:21.887269 139752296675072 logging_writer.py:48] [52052] accumulated_eval_time=9081.738107, accumulated_logging_time=2.531342, accumulated_submission_time=16584.350386, global_step=52052, preemption_count=0, score=16584.350386, test/accuracy=0.985988, test/loss=0.049992, test/mean_average_precision=0.258429, test/num_examples=43793, total_duration=25670.055929, train/accuracy=0.993676, train/loss=0.020268, train/mean_average_precision=0.631766, validation/accuracy=0.986832, validation/loss=0.046777, validation/mean_average_precision=0.278493, validation/num_examples=43793
I0205 04:56:37.663010 139758996317952 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.15456262230873108, loss=0.029011696577072144
I0205 04:57:10.015973 139752296675072 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.12394320964813232, loss=0.025684338063001633
I0205 04:57:41.505802 139758996317952 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.12435215711593628, loss=0.0245004091411829
I0205 04:58:13.010377 139752296675072 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.133015975356102, loss=0.02587180957198143
I0205 04:58:44.365196 139758996317952 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.12341772019863129, loss=0.02538333646953106
I0205 04:59:16.112809 139752296675072 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.16561239957809448, loss=0.03038611076772213
I0205 04:59:47.330583 139758996317952 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.1592211127281189, loss=0.02802092395722866
I0205 05:00:18.765749 139752296675072 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.14937053620815277, loss=0.02678941749036312
I0205 05:00:21.959129 139919816816448 spec.py:321] Evaluating on the training split.
I0205 05:02:21.697495 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 05:02:24.777929 139919816816448 spec.py:349] Evaluating on the test split.
I0205 05:02:27.795775 139919816816448 submission_runner.py:408] Time since start: 26035.99s, 	Step: 52811, 	{'train/accuracy': 0.9940836429595947, 'train/loss': 0.019118543714284897, 'train/mean_average_precision': 0.6474843811104636, 'validation/accuracy': 0.9867330193519592, 'validation/loss': 0.047576721757650375, 'validation/mean_average_precision': 0.27017016827719925, 'validation/num_examples': 43793, 'test/accuracy': 0.9858625531196594, 'test/loss': 0.05072097107768059, 'test/mean_average_precision': 0.2562704321462084, 'test/num_examples': 43793, 'score': 16824.390946626663, 'total_duration': 26035.989722013474, 'accumulated_submission_time': 16824.390946626663, 'accumulated_eval_time': 9207.574704885483, 'accumulated_logging_time': 2.5672249794006348}
I0205 05:02:27.821590 139752480773888 logging_writer.py:48] [52811] accumulated_eval_time=9207.574705, accumulated_logging_time=2.567225, accumulated_submission_time=16824.390947, global_step=52811, preemption_count=0, score=16824.390947, test/accuracy=0.985863, test/loss=0.050721, test/mean_average_precision=0.256270, test/num_examples=43793, total_duration=26035.989722, train/accuracy=0.994084, train/loss=0.019119, train/mean_average_precision=0.647484, validation/accuracy=0.986733, validation/loss=0.047577, validation/mean_average_precision=0.270170, validation/num_examples=43793
I0205 05:02:56.983499 139758987925248 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.13179340958595276, loss=0.026616288349032402
I0205 05:03:28.852414 139752480773888 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.1353975236415863, loss=0.024731086567044258
I0205 05:04:00.530719 139758987925248 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.1540430337190628, loss=0.025308283045887947
I0205 05:04:32.279780 139752480773888 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.13386932015419006, loss=0.027073662728071213
I0205 05:05:04.299909 139758987925248 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.15326811373233795, loss=0.02597874589264393
I0205 05:05:36.104140 139752480773888 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.14926324784755707, loss=0.022861596196889877
I0205 05:06:08.259019 139758987925248 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.15290296077728271, loss=0.027937188744544983
I0205 05:06:28.088996 139919816816448 spec.py:321] Evaluating on the training split.
I0205 05:08:24.907993 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 05:08:28.310946 139919816816448 spec.py:349] Evaluating on the test split.
I0205 05:08:31.722813 139919816816448 submission_runner.py:408] Time since start: 26399.92s, 	Step: 53562, 	{'train/accuracy': 0.9939260482788086, 'train/loss': 0.019581323489546776, 'train/mean_average_precision': 0.6418360793553928, 'validation/accuracy': 0.9867784976959229, 'validation/loss': 0.04765912890434265, 'validation/mean_average_precision': 0.27103645399454124, 'validation/num_examples': 43793, 'test/accuracy': 0.985888659954071, 'test/loss': 0.050796233117580414, 'test/mean_average_precision': 0.25489262077263214, 'test/num_examples': 43793, 'score': 17064.626448631287, 'total_duration': 26399.916737556458, 'accumulated_submission_time': 17064.626448631287, 'accumulated_eval_time': 9331.2084608078, 'accumulated_logging_time': 2.6037094593048096}
I0205 05:08:31.752270 139752296675072 logging_writer.py:48] [53562] accumulated_eval_time=9331.208461, accumulated_logging_time=2.603709, accumulated_submission_time=17064.626449, global_step=53562, preemption_count=0, score=17064.626449, test/accuracy=0.985889, test/loss=0.050796, test/mean_average_precision=0.254893, test/num_examples=43793, total_duration=26399.916738, train/accuracy=0.993926, train/loss=0.019581, train/mean_average_precision=0.641836, validation/accuracy=0.986778, validation/loss=0.047659, validation/mean_average_precision=0.271036, validation/num_examples=43793
I0205 05:08:44.423835 139758996317952 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.13466237485408783, loss=0.026116544380784035
I0205 05:09:16.855020 139752296675072 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.18735556304454803, loss=0.025881826877593994
I0205 05:09:49.189974 139758996317952 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.14311251044273376, loss=0.026951180770993233
I0205 05:10:21.471679 139752296675072 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.12747669219970703, loss=0.025293797254562378
I0205 05:10:54.988065 139758996317952 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.16674262285232544, loss=0.026026422157883644
I0205 05:11:27.130589 139752296675072 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.1468464434146881, loss=0.024485724046826363
I0205 05:11:59.056452 139758996317952 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.13558754324913025, loss=0.02475232072174549
I0205 05:12:31.037306 139752296675072 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.1480371057987213, loss=0.02548714354634285
I0205 05:12:31.961753 139919816816448 spec.py:321] Evaluating on the training split.
I0205 05:14:27.366014 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 05:14:30.398520 139919816816448 spec.py:349] Evaluating on the test split.
I0205 05:14:33.407811 139919816816448 submission_runner.py:408] Time since start: 26761.60s, 	Step: 54304, 	{'train/accuracy': 0.9939098954200745, 'train/loss': 0.019574008882045746, 'train/mean_average_precision': 0.6298671260120975, 'validation/accuracy': 0.9867439866065979, 'validation/loss': 0.04797590896487236, 'validation/mean_average_precision': 0.2708166189977605, 'validation/num_examples': 43793, 'test/accuracy': 0.9858903884887695, 'test/loss': 0.05098933354020119, 'test/mean_average_precision': 0.25284671020636434, 'test/num_examples': 43793, 'score': 17304.80330300331, 'total_duration': 26761.601624011993, 'accumulated_submission_time': 17304.80330300331, 'accumulated_eval_time': 9452.654334545135, 'accumulated_logging_time': 2.6448752880096436}
I0205 05:14:33.432896 139739177916160 logging_writer.py:48] [54304] accumulated_eval_time=9452.654335, accumulated_logging_time=2.644875, accumulated_submission_time=17304.803303, global_step=54304, preemption_count=0, score=17304.803303, test/accuracy=0.985890, test/loss=0.050989, test/mean_average_precision=0.252847, test/num_examples=43793, total_duration=26761.601624, train/accuracy=0.993910, train/loss=0.019574, train/mean_average_precision=0.629867, validation/accuracy=0.986744, validation/loss=0.047976, validation/mean_average_precision=0.270817, validation/num_examples=43793
I0205 05:15:04.113780 139758987925248 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.13776268064975739, loss=0.025996753945946693
I0205 05:15:35.544099 139739177916160 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.14239966869354248, loss=0.02529611624777317
I0205 05:16:07.125621 139758987925248 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.16117824614048004, loss=0.026911351829767227
I0205 05:16:38.851285 139739177916160 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.154676616191864, loss=0.026944341138005257
I0205 05:17:10.541553 139758987925248 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.1524549126625061, loss=0.026964399963617325
I0205 05:17:42.063422 139739177916160 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.15031877160072327, loss=0.02485877089202404
I0205 05:18:14.123395 139758987925248 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.15686464309692383, loss=0.02600778453052044
I0205 05:18:33.518225 139919816816448 spec.py:321] Evaluating on the training split.
I0205 05:20:34.070117 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 05:20:37.168734 139919816816448 spec.py:349] Evaluating on the test split.
I0205 05:20:40.176060 139919816816448 submission_runner.py:408] Time since start: 27128.37s, 	Step: 55063, 	{'train/accuracy': 0.9937072992324829, 'train/loss': 0.020246917381882668, 'train/mean_average_precision': 0.6290030533487323, 'validation/accuracy': 0.9866514205932617, 'validation/loss': 0.047722216695547104, 'validation/mean_average_precision': 0.2730129936795538, 'validation/num_examples': 43793, 'test/accuracy': 0.9858301281929016, 'test/loss': 0.05093853175640106, 'test/mean_average_precision': 0.2527652640350336, 'test/num_examples': 43793, 'score': 17544.856785297394, 'total_duration': 27128.370005607605, 'accumulated_submission_time': 17544.856785297394, 'accumulated_eval_time': 9579.312143564224, 'accumulated_logging_time': 2.6809160709381104}
I0205 05:20:40.201684 139752480773888 logging_writer.py:48] [55063] accumulated_eval_time=9579.312144, accumulated_logging_time=2.680916, accumulated_submission_time=17544.856785, global_step=55063, preemption_count=0, score=17544.856785, test/accuracy=0.985830, test/loss=0.050939, test/mean_average_precision=0.252765, test/num_examples=43793, total_duration=27128.370006, train/accuracy=0.993707, train/loss=0.020247, train/mean_average_precision=0.629003, validation/accuracy=0.986651, validation/loss=0.047722, validation/mean_average_precision=0.273013, validation/num_examples=43793
I0205 05:20:52.422042 139758996317952 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.15138739347457886, loss=0.025796247646212578
I0205 05:21:24.307314 139752480773888 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.18002772331237793, loss=0.026292741298675537
I0205 05:21:56.498783 139758996317952 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.1475636065006256, loss=0.02452819235622883
I0205 05:22:28.261169 139752480773888 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.1507476568222046, loss=0.023573217913508415
I0205 05:23:00.123103 139758996317952 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.17977474629878998, loss=0.025924623012542725
I0205 05:23:31.753448 139752480773888 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.13710354268550873, loss=0.023485487326979637
I0205 05:24:03.765359 139758996317952 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.16045165061950684, loss=0.025012049823999405
I0205 05:24:35.900827 139752480773888 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.1784500777721405, loss=0.024679169058799744
I0205 05:24:40.330496 139919816816448 spec.py:321] Evaluating on the training split.
I0205 05:26:36.730754 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 05:26:39.817118 139919816816448 spec.py:349] Evaluating on the test split.
I0205 05:26:42.849563 139919816816448 submission_runner.py:408] Time since start: 27491.04s, 	Step: 55815, 	{'train/accuracy': 0.9936484694480896, 'train/loss': 0.020147372037172318, 'train/mean_average_precision': 0.6260407024076896, 'validation/accuracy': 0.9866623878479004, 'validation/loss': 0.04816872626543045, 'validation/mean_average_precision': 0.2720521240646281, 'validation/num_examples': 43793, 'test/accuracy': 0.9858187437057495, 'test/loss': 0.05146540701389313, 'test/mean_average_precision': 0.25340895317912365, 'test/num_examples': 43793, 'score': 17784.952792406082, 'total_duration': 27491.043491363525, 'accumulated_submission_time': 17784.952792406082, 'accumulated_eval_time': 9701.83114695549, 'accumulated_logging_time': 2.7188405990600586}
I0205 05:26:42.876136 139739177916160 logging_writer.py:48] [55815] accumulated_eval_time=9701.831147, accumulated_logging_time=2.718841, accumulated_submission_time=17784.952792, global_step=55815, preemption_count=0, score=17784.952792, test/accuracy=0.985819, test/loss=0.051465, test/mean_average_precision=0.253409, test/num_examples=43793, total_duration=27491.043491, train/accuracy=0.993648, train/loss=0.020147, train/mean_average_precision=0.626041, validation/accuracy=0.986662, validation/loss=0.048169, validation/mean_average_precision=0.272052, validation/num_examples=43793
I0205 05:27:10.305730 139752296675072 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.17474360764026642, loss=0.023888101801276207
I0205 05:27:42.166791 139739177916160 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.1442137062549591, loss=0.024251248687505722
I0205 05:28:14.146287 139752296675072 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.14845840632915497, loss=0.023198969662189484
I0205 05:28:46.329180 139739177916160 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.15939314663410187, loss=0.024730077013373375
I0205 05:29:18.452577 139752296675072 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.1618257313966751, loss=0.024091321974992752
I0205 05:29:50.263655 139739177916160 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.15230406820774078, loss=0.021183636039495468
I0205 05:30:22.006263 139752296675072 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.12736554443836212, loss=0.023668965324759483
I0205 05:30:43.022894 139919816816448 spec.py:321] Evaluating on the training split.
I0205 05:32:43.251934 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 05:32:46.341001 139919816816448 spec.py:349] Evaluating on the test split.
I0205 05:32:49.352135 139919816816448 submission_runner.py:408] Time since start: 27857.55s, 	Step: 56567, 	{'train/accuracy': 0.993515133857727, 'train/loss': 0.02034720405936241, 'train/mean_average_precision': 0.6213367212781136, 'validation/accuracy': 0.9865515232086182, 'validation/loss': 0.04893568530678749, 'validation/mean_average_precision': 0.2671761914545285, 'validation/num_examples': 43793, 'test/accuracy': 0.985710084438324, 'test/loss': 0.052274417132139206, 'test/mean_average_precision': 0.25407019463630753, 'test/num_examples': 43793, 'score': 18025.06869673729, 'total_duration': 27857.546080112457, 'accumulated_submission_time': 18025.06869673729, 'accumulated_eval_time': 9828.160341501236, 'accumulated_logging_time': 2.756070375442505}
I0205 05:32:49.378082 139758987925248 logging_writer.py:48] [56567] accumulated_eval_time=9828.160342, accumulated_logging_time=2.756070, accumulated_submission_time=18025.068697, global_step=56567, preemption_count=0, score=18025.068697, test/accuracy=0.985710, test/loss=0.052274, test/mean_average_precision=0.254070, test/num_examples=43793, total_duration=27857.546080, train/accuracy=0.993515, train/loss=0.020347, train/mean_average_precision=0.621337, validation/accuracy=0.986552, validation/loss=0.048936, validation/mean_average_precision=0.267176, validation/num_examples=43793
I0205 05:33:01.082362 139758996317952 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.15997815132141113, loss=0.022717755287885666
I0205 05:33:33.009363 139758987925248 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.17110027372837067, loss=0.024181323125958443
I0205 05:34:05.065898 139758996317952 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.15129053592681885, loss=0.025817912071943283
I0205 05:34:36.436342 139758987925248 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.1784648299217224, loss=0.024824468418955803
I0205 05:35:07.937138 139758996317952 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.17249661684036255, loss=0.02466052770614624
I0205 05:35:39.926200 139758987925248 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.15417945384979248, loss=0.02446306310594082
I0205 05:36:11.592318 139758996317952 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.19491752982139587, loss=0.025298966094851494
I0205 05:36:43.501790 139758987925248 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.17099395394325256, loss=0.02518211118876934
I0205 05:36:49.484898 139919816816448 spec.py:321] Evaluating on the training split.
I0205 05:38:45.184388 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 05:38:48.519673 139919816816448 spec.py:349] Evaluating on the test split.
I0205 05:38:51.752791 139919816816448 submission_runner.py:408] Time since start: 28219.95s, 	Step: 57320, 	{'train/accuracy': 0.9935488104820251, 'train/loss': 0.020334692671895027, 'train/mean_average_precision': 0.611723780806605, 'validation/accuracy': 0.9865474700927734, 'validation/loss': 0.049405910074710846, 'validation/mean_average_precision': 0.26607081788312853, 'validation/num_examples': 43793, 'test/accuracy': 0.985775351524353, 'test/loss': 0.05271177738904953, 'test/mean_average_precision': 0.2552477887895668, 'test/num_examples': 43793, 'score': 18265.143191337585, 'total_duration': 28219.94671702385, 'accumulated_submission_time': 18265.143191337585, 'accumulated_eval_time': 9950.428166866302, 'accumulated_logging_time': 2.793794870376587}
I0205 05:38:51.781923 139739177916160 logging_writer.py:48] [57320] accumulated_eval_time=9950.428167, accumulated_logging_time=2.793795, accumulated_submission_time=18265.143191, global_step=57320, preemption_count=0, score=18265.143191, test/accuracy=0.985775, test/loss=0.052712, test/mean_average_precision=0.255248, test/num_examples=43793, total_duration=28219.946717, train/accuracy=0.993549, train/loss=0.020335, train/mean_average_precision=0.611724, validation/accuracy=0.986547, validation/loss=0.049406, validation/mean_average_precision=0.266071, validation/num_examples=43793
I0205 05:39:17.517466 139752296675072 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.1800905019044876, loss=0.023438015952706337
I0205 05:39:49.121328 139739177916160 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.16773298382759094, loss=0.023367492482066154
I0205 05:40:20.563827 139752296675072 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.19249360263347626, loss=0.026453183963894844
I0205 05:40:52.296718 139739177916160 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.18258938193321228, loss=0.023229412734508514
I0205 05:41:23.890019 139752296675072 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.1708502471446991, loss=0.024327246472239494
I0205 05:41:55.842679 139739177916160 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.15340988337993622, loss=0.02204803377389908
I0205 05:42:23.895550 139752296675072 logging_writer.py:48] [57989] global_step=57989, preemption_count=0, score=18477.210753
I0205 05:42:23.948088 139919816816448 checkpoints.py:490] Saving checkpoint at step: 57989
I0205 05:42:24.067397 139919816816448 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_1/ogbg_jax/trial_1/checkpoint_57989
I0205 05:42:24.068533 139919816816448 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_1/ogbg_jax/trial_1/checkpoint_57989.
I0205 05:42:24.246328 139919816816448 submission_runner.py:583] Tuning trial 1/5
I0205 05:42:24.246560 139919816816448 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.1, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0205 05:42:24.250453 139919816816448 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.532486081123352, 'train/loss': 0.7277461886405945, 'train/mean_average_precision': 0.022446237910565617, 'validation/accuracy': 0.5230699777603149, 'validation/loss': 0.7331878542900085, 'validation/mean_average_precision': 0.025538990812299604, 'validation/num_examples': 43793, 'test/accuracy': 0.5214918851852417, 'test/loss': 0.7347381114959717, 'test/mean_average_precision': 0.02681250091619177, 'test/num_examples': 43793, 'score': 19.20242476463318, 'total_duration': 326.7589704990387, 'accumulated_submission_time': 19.20242476463318, 'accumulated_eval_time': 307.5564877986908, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (753, {'train/accuracy': 0.986735999584198, 'train/loss': 0.06938512623310089, 'train/mean_average_precision': 0.032705299679279695, 'validation/accuracy': 0.9841179251670837, 'validation/loss': 0.07758407294750214, 'validation/mean_average_precision': 0.033466476626962026, 'validation/num_examples': 43793, 'test/accuracy': 0.983142077922821, 'test/loss': 0.08040352910757065, 'test/mean_average_precision': 0.034597994751347755, 'test/num_examples': 43793, 'score': 259.26010942459106, 'total_duration': 685.4427721500397, 'accumulated_submission_time': 259.26010942459106, 'accumulated_eval_time': 426.1334116458893, 'accumulated_logging_time': 0.030203819274902344, 'global_step': 753, 'preemption_count': 0}), (1504, {'train/accuracy': 0.9868547320365906, 'train/loss': 0.051052525639534, 'train/mean_average_precision': 0.07063277791010728, 'validation/accuracy': 0.984196662902832, 'validation/loss': 0.06075628101825714, 'validation/mean_average_precision': 0.06901157142689766, 'validation/num_examples': 43793, 'test/accuracy': 0.9832296967506409, 'test/loss': 0.06415972858667374, 'test/mean_average_precision': 0.07061243126817357, 'test/num_examples': 43793, 'score': 499.3460896015167, 'total_duration': 1047.0379321575165, 'accumulated_submission_time': 499.3460896015167, 'accumulated_eval_time': 547.5880465507507, 'accumulated_logging_time': 0.06566476821899414, 'global_step': 1504, 'preemption_count': 0}), (2258, {'train/accuracy': 0.9876076579093933, 'train/loss': 0.04494336619973183, 'train/mean_average_precision': 0.12705054324859275, 'validation/accuracy': 0.9849172234535217, 'validation/loss': 0.053940750658512115, 'validation/mean_average_precision': 0.12254631551189603, 'validation/num_examples': 43793, 'test/accuracy': 0.9839473962783813, 'test/loss': 0.0569424144923687, 'test/mean_average_precision': 0.12682285102300242, 'test/num_examples': 43793, 'score': 739.3077754974365, 'total_duration': 1411.1662957668304, 'accumulated_submission_time': 739.3077754974365, 'accumulated_eval_time': 671.7086169719696, 'accumulated_logging_time': 0.09263157844543457, 'global_step': 2258, 'preemption_count': 0}), (2998, {'train/accuracy': 0.9880425930023193, 'train/loss': 0.04255932196974754, 'train/mean_average_precision': 0.15393298377261813, 'validation/accuracy': 0.9850654006004333, 'validation/loss': 0.051880139857530594, 'validation/mean_average_precision': 0.14723815533134316, 'validation/num_examples': 43793, 'test/accuracy': 0.9841668605804443, 'test/loss': 0.054550494998693466, 'test/mean_average_precision': 0.1515404133887407, 'test/num_examples': 43793, 'score': 979.2893161773682, 'total_duration': 1779.5455212593079, 'accumulated_submission_time': 979.2893161773682, 'accumulated_eval_time': 800.0604002475739, 'accumulated_logging_time': 0.11982035636901855, 'global_step': 2998, 'preemption_count': 0}), (3740, {'train/accuracy': 0.9883670210838318, 'train/loss': 0.04071572422981262, 'train/mean_average_precision': 0.18442546201450333, 'validation/accuracy': 0.985426664352417, 'validation/loss': 0.0505857840180397, 'validation/mean_average_precision': 0.16857170420016457, 'validation/num_examples': 43793, 'test/accuracy': 0.9844903349876404, 'test/loss': 0.053492479026317596, 'test/mean_average_precision': 0.16623517202618904, 'test/num_examples': 43793, 'score': 1219.3918023109436, 'total_duration': 2143.96270942688, 'accumulated_submission_time': 1219.3918023109436, 'accumulated_eval_time': 924.3276314735413, 'accumulated_logging_time': 0.14711236953735352, 'global_step': 3740, 'preemption_count': 0}), (4482, {'train/accuracy': 0.9883249402046204, 'train/loss': 0.040089331567287445, 'train/mean_average_precision': 0.2131278146848692, 'validation/accuracy': 0.9855647087097168, 'validation/loss': 0.04924480989575386, 'validation/mean_average_precision': 0.18086458362319696, 'validation/num_examples': 43793, 'test/accuracy': 0.9846583604812622, 'test/loss': 0.0519588366150856, 'test/mean_average_precision': 0.181732471899558, 'test/num_examples': 43793, 'score': 1459.5320043563843, 'total_duration': 2515.1227877140045, 'accumulated_submission_time': 1459.5320043563843, 'accumulated_eval_time': 1055.3013689517975, 'accumulated_logging_time': 0.17404675483703613, 'global_step': 4482, 'preemption_count': 0}), (5209, {'train/accuracy': 0.9886398315429688, 'train/loss': 0.03881854936480522, 'train/mean_average_precision': 0.22067476302031644, 'validation/accuracy': 0.9856414198875427, 'validation/loss': 0.04870504140853882, 'validation/mean_average_precision': 0.1843567487809262, 'validation/num_examples': 43793, 'test/accuracy': 0.9847750663757324, 'test/loss': 0.05137285590171814, 'test/mean_average_precision': 0.18736745437329538, 'test/num_examples': 43793, 'score': 1699.6076345443726, 'total_duration': 2881.247640132904, 'accumulated_submission_time': 1699.6076345443726, 'accumulated_eval_time': 1181.2931609153748, 'accumulated_logging_time': 0.20586609840393066, 'global_step': 5209, 'preemption_count': 0}), (5974, {'train/accuracy': 0.9889200329780579, 'train/loss': 0.03803868219256401, 'train/mean_average_precision': 0.24607398812691722, 'validation/accuracy': 0.9857940673828125, 'validation/loss': 0.048042286187410355, 'validation/mean_average_precision': 0.19944095486837685, 'validation/num_examples': 43793, 'test/accuracy': 0.984929621219635, 'test/loss': 0.050627075135707855, 'test/mean_average_precision': 0.20343181880466227, 'test/num_examples': 43793, 'score': 1939.6774501800537, 'total_duration': 3246.82239818573, 'accumulated_submission_time': 1939.6774501800537, 'accumulated_eval_time': 1306.7508709430695, 'accumulated_logging_time': 0.23312973976135254, 'global_step': 5974, 'preemption_count': 0}), (6743, {'train/accuracy': 0.9888412952423096, 'train/loss': 0.03810810670256615, 'train/mean_average_precision': 0.24230055322371255, 'validation/accuracy': 0.9858776926994324, 'validation/loss': 0.04803735390305519, 'validation/mean_average_precision': 0.204004536859809, 'validation/num_examples': 43793, 'test/accuracy': 0.984944760799408, 'test/loss': 0.05091791972517967, 'test/mean_average_precision': 0.2028065330187146, 'test/num_examples': 43793, 'score': 2179.744330406189, 'total_duration': 3612.562463760376, 'accumulated_submission_time': 2179.744330406189, 'accumulated_eval_time': 1432.3741779327393, 'accumulated_logging_time': 0.2616612911224365, 'global_step': 6743, 'preemption_count': 0}), (7515, {'train/accuracy': 0.988990843296051, 'train/loss': 0.03744160756468773, 'train/mean_average_precision': 0.25432717575873776, 'validation/accuracy': 0.9861622452735901, 'validation/loss': 0.046982571482658386, 'validation/mean_average_precision': 0.215558121122398, 'validation/num_examples': 43793, 'test/accuracy': 0.9852067828178406, 'test/loss': 0.04969971254467964, 'test/mean_average_precision': 0.217869939416924, 'test/num_examples': 43793, 'score': 2419.9255475997925, 'total_duration': 3978.6678981781006, 'accumulated_submission_time': 2419.9255475997925, 'accumulated_eval_time': 1558.2514843940735, 'accumulated_logging_time': 0.2884867191314697, 'global_step': 7515, 'preemption_count': 0}), (8267, {'train/accuracy': 0.9891239404678345, 'train/loss': 0.03680913522839546, 'train/mean_average_precision': 0.2656056616732034, 'validation/accuracy': 0.9862223267555237, 'validation/loss': 0.0464504174888134, 'validation/mean_average_precision': 0.21724025294531527, 'validation/num_examples': 43793, 'test/accuracy': 0.9852614998817444, 'test/loss': 0.04922201484441757, 'test/mean_average_precision': 0.21476680695329606, 'test/num_examples': 43793, 'score': 2660.0192317962646, 'total_duration': 4346.496450185776, 'accumulated_submission_time': 2660.0192317962646, 'accumulated_eval_time': 1685.931292772293, 'accumulated_logging_time': 0.3178873062133789, 'global_step': 8267, 'preemption_count': 0}), (9025, {'train/accuracy': 0.9893156290054321, 'train/loss': 0.036241594702005386, 'train/mean_average_precision': 0.27204220425241077, 'validation/accuracy': 0.9861720204353333, 'validation/loss': 0.04633241519331932, 'validation/mean_average_precision': 0.22725926422197537, 'validation/num_examples': 43793, 'test/accuracy': 0.9852830171585083, 'test/loss': 0.04907473921775818, 'test/mean_average_precision': 0.23377829931046903, 'test/num_examples': 43793, 'score': 2900.125126838684, 'total_duration': 4715.513872861862, 'accumulated_submission_time': 2900.125126838684, 'accumulated_eval_time': 1814.7951426506042, 'accumulated_logging_time': 0.34543848037719727, 'global_step': 9025, 'preemption_count': 0}), (9783, {'train/accuracy': 0.9895707368850708, 'train/loss': 0.0354536771774292, 'train/mean_average_precision': 0.2919737580040339, 'validation/accuracy': 0.9863116145133972, 'validation/loss': 0.04579000174999237, 'validation/mean_average_precision': 0.23331101539934118, 'validation/num_examples': 43793, 'test/accuracy': 0.985456109046936, 'test/loss': 0.04847123473882675, 'test/mean_average_precision': 0.22703790808286564, 'test/num_examples': 43793, 'score': 3140.291603088379, 'total_duration': 5084.685031175613, 'accumulated_submission_time': 3140.291603088379, 'accumulated_eval_time': 1943.7513513565063, 'accumulated_logging_time': 0.3736414909362793, 'global_step': 9783, 'preemption_count': 0}), (10547, {'train/accuracy': 0.9897831678390503, 'train/loss': 0.034679215401411057, 'train/mean_average_precision': 0.30023675460463783, 'validation/accuracy': 0.9863839149475098, 'validation/loss': 0.04539710655808449, 'validation/mean_average_precision': 0.23970684941295484, 'validation/num_examples': 43793, 'test/accuracy': 0.9855917096138, 'test/loss': 0.04791358485817909, 'test/mean_average_precision': 0.23332974898083178, 'test/num_examples': 43793, 'score': 3380.29745554924, 'total_duration': 5447.554109573364, 'accumulated_submission_time': 3380.29745554924, 'accumulated_eval_time': 2066.5665271282196, 'accumulated_logging_time': 0.4013993740081787, 'global_step': 10547, 'preemption_count': 0}), (11313, {'train/accuracy': 0.9898716807365417, 'train/loss': 0.03416074067354202, 'train/mean_average_precision': 0.32691572470335784, 'validation/accuracy': 0.9863802194595337, 'validation/loss': 0.045297037810087204, 'validation/mean_average_precision': 0.24094424710127144, 'validation/num_examples': 43793, 'test/accuracy': 0.9855576157569885, 'test/loss': 0.048182737082242966, 'test/mean_average_precision': 0.2386666619664352, 'test/num_examples': 43793, 'score': 3620.2503366470337, 'total_duration': 5817.087311029434, 'accumulated_submission_time': 3620.2503366470337, 'accumulated_eval_time': 2196.096314430237, 'accumulated_logging_time': 0.4317901134490967, 'global_step': 11313, 'preemption_count': 0}), (12067, {'train/accuracy': 0.9901003241539001, 'train/loss': 0.033180154860019684, 'train/mean_average_precision': 0.3477610696164991, 'validation/accuracy': 0.986553966999054, 'validation/loss': 0.04491068050265312, 'validation/mean_average_precision': 0.2418638094390883, 'validation/num_examples': 43793, 'test/accuracy': 0.9856814742088318, 'test/loss': 0.04764633998274803, 'test/mean_average_precision': 0.24118049465505706, 'test/num_examples': 43793, 'score': 3860.4717514514923, 'total_duration': 6186.656934499741, 'accumulated_submission_time': 3860.4717514514923, 'accumulated_eval_time': 2325.3949341773987, 'accumulated_logging_time': 0.46141505241394043, 'global_step': 12067, 'preemption_count': 0}), (12818, {'train/accuracy': 0.9901209473609924, 'train/loss': 0.03266278654336929, 'train/mean_average_precision': 0.3559880776898775, 'validation/accuracy': 0.9866014719009399, 'validation/loss': 0.04502946510910988, 'validation/mean_average_precision': 0.2545477236240031, 'validation/num_examples': 43793, 'test/accuracy': 0.985745906829834, 'test/loss': 0.04778912663459778, 'test/mean_average_precision': 0.2443888640165271, 'test/num_examples': 43793, 'score': 4100.424854040146, 'total_duration': 6555.8935306072235, 'accumulated_submission_time': 4100.424854040146, 'accumulated_eval_time': 2454.6309485435486, 'accumulated_logging_time': 0.4894673824310303, 'global_step': 12818, 'preemption_count': 0}), (13570, {'train/accuracy': 0.990494430065155, 'train/loss': 0.03165928274393082, 'train/mean_average_precision': 0.38618576851056285, 'validation/accuracy': 0.9867119193077087, 'validation/loss': 0.04467901587486267, 'validation/mean_average_precision': 0.25529698262849987, 'validation/num_examples': 43793, 'test/accuracy': 0.9858596324920654, 'test/loss': 0.04726462811231613, 'test/mean_average_precision': 0.25196132531218934, 'test/num_examples': 43793, 'score': 4340.415853261948, 'total_duration': 6926.2187123298645, 'accumulated_submission_time': 4340.415853261948, 'accumulated_eval_time': 2584.913625717163, 'accumulated_logging_time': 0.519852876663208, 'global_step': 13570, 'preemption_count': 0}), (14326, {'train/accuracy': 0.9905635118484497, 'train/loss': 0.03131844848394394, 'train/mean_average_precision': 0.37796976515685243, 'validation/accuracy': 0.9866960644721985, 'validation/loss': 0.044397562742233276, 'validation/mean_average_precision': 0.2605003860303854, 'validation/num_examples': 43793, 'test/accuracy': 0.985908031463623, 'test/loss': 0.04722025245428085, 'test/mean_average_precision': 0.25244056870739706, 'test/num_examples': 43793, 'score': 4580.423997163773, 'total_duration': 7296.1311457157135, 'accumulated_submission_time': 4580.423997163773, 'accumulated_eval_time': 2714.7683775424957, 'accumulated_logging_time': 0.5493123531341553, 'global_step': 14326, 'preemption_count': 0}), (15088, {'train/accuracy': 0.9904943704605103, 'train/loss': 0.03144986182451248, 'train/mean_average_precision': 0.39217741118649246, 'validation/accuracy': 0.9867439866065979, 'validation/loss': 0.044363733381032944, 'validation/mean_average_precision': 0.2609996468968531, 'validation/num_examples': 43793, 'test/accuracy': 0.9859038591384888, 'test/loss': 0.047209352254867554, 'test/mean_average_precision': 0.24937429703177308, 'test/num_examples': 43793, 'score': 4820.594982147217, 'total_duration': 7664.723286628723, 'accumulated_submission_time': 4820.594982147217, 'accumulated_eval_time': 2843.13925409317, 'accumulated_logging_time': 0.5789437294006348, 'global_step': 15088, 'preemption_count': 0}), (15848, {'train/accuracy': 0.990463137626648, 'train/loss': 0.031662777066230774, 'train/mean_average_precision': 0.3854086649106583, 'validation/accuracy': 0.9866436719894409, 'validation/loss': 0.044806282967329025, 'validation/mean_average_precision': 0.2567880837321566, 'validation/num_examples': 43793, 'test/accuracy': 0.9858655333518982, 'test/loss': 0.047489818185567856, 'test/mean_average_precision': 0.24913231873415576, 'test/num_examples': 43793, 'score': 5060.655467748642, 'total_duration': 8032.338857412338, 'accumulated_submission_time': 5060.655467748642, 'accumulated_eval_time': 2970.6443254947662, 'accumulated_logging_time': 0.6084580421447754, 'global_step': 15848, 'preemption_count': 0}), (16610, {'train/accuracy': 0.9904116988182068, 'train/loss': 0.031754765659570694, 'train/mean_average_precision': 0.37209461998537924, 'validation/accuracy': 0.9867720007896423, 'validation/loss': 0.0442337729036808, 'validation/mean_average_precision': 0.2636909103653799, 'validation/num_examples': 43793, 'test/accuracy': 0.9859662055969238, 'test/loss': 0.04691949486732483, 'test/mean_average_precision': 0.2570484000552654, 'test/num_examples': 43793, 'score': 5300.637059926987, 'total_duration': 8400.122673511505, 'accumulated_submission_time': 5300.637059926987, 'accumulated_eval_time': 3098.395025253296, 'accumulated_logging_time': 0.6386611461639404, 'global_step': 16610, 'preemption_count': 0}), (17359, {'train/accuracy': 0.9904099106788635, 'train/loss': 0.03174610808491707, 'train/mean_average_precision': 0.37712096185811106, 'validation/accuracy': 0.9866794347763062, 'validation/loss': 0.04448701813817024, 'validation/mean_average_precision': 0.25674223867466034, 'validation/num_examples': 43793, 'test/accuracy': 0.9858945608139038, 'test/loss': 0.047203127294778824, 'test/mean_average_precision': 0.2509193630611767, 'test/num_examples': 43793, 'score': 5540.721517562866, 'total_duration': 8769.763256072998, 'accumulated_submission_time': 5540.721517562866, 'accumulated_eval_time': 3227.900470495224, 'accumulated_logging_time': 0.668442964553833, 'global_step': 17359, 'preemption_count': 0}), (18114, {'train/accuracy': 0.9906277656555176, 'train/loss': 0.0312679186463356, 'train/mean_average_precision': 0.38930200731543024, 'validation/accuracy': 0.9866157174110413, 'validation/loss': 0.04464775696396828, 'validation/mean_average_precision': 0.26031631678613026, 'validation/num_examples': 43793, 'test/accuracy': 0.9858099222183228, 'test/loss': 0.04745831713080406, 'test/mean_average_precision': 0.2474147599552491, 'test/num_examples': 43793, 'score': 5780.809211492538, 'total_duration': 9141.121300935745, 'accumulated_submission_time': 5780.809211492538, 'accumulated_eval_time': 3359.1195571422577, 'accumulated_logging_time': 0.6974725723266602, 'global_step': 18114, 'preemption_count': 0}), (18861, {'train/accuracy': 0.990624189376831, 'train/loss': 0.030974138528108597, 'train/mean_average_precision': 0.39346113920306736, 'validation/accuracy': 0.9867175817489624, 'validation/loss': 0.04483654722571373, 'validation/mean_average_precision': 0.2644804500767574, 'validation/num_examples': 43793, 'test/accuracy': 0.9858949780464172, 'test/loss': 0.04775327071547508, 'test/mean_average_precision': 0.24872658993641045, 'test/num_examples': 43793, 'score': 6020.937862634659, 'total_duration': 9511.308304786682, 'accumulated_submission_time': 6020.937862634659, 'accumulated_eval_time': 3489.12383890152, 'accumulated_logging_time': 0.7305166721343994, 'global_step': 18861, 'preemption_count': 0}), (19617, {'train/accuracy': 0.9908477663993835, 'train/loss': 0.030303241685032845, 'train/mean_average_precision': 0.4078293290951283, 'validation/accuracy': 0.9866339564323425, 'validation/loss': 0.044566091150045395, 'validation/mean_average_precision': 0.2658152452066989, 'validation/num_examples': 43793, 'test/accuracy': 0.9857138991355896, 'test/loss': 0.04753638803958893, 'test/mean_average_precision': 0.24737294701466878, 'test/num_examples': 43793, 'score': 6260.9293966293335, 'total_duration': 9880.730882644653, 'accumulated_submission_time': 6260.9293966293335, 'accumulated_eval_time': 3618.5049121379852, 'accumulated_logging_time': 0.7599573135375977, 'global_step': 19617, 'preemption_count': 0}), (20378, {'train/accuracy': 0.9909874200820923, 'train/loss': 0.02956818975508213, 'train/mean_average_precision': 0.43345511416517735, 'validation/accuracy': 0.986757755279541, 'validation/loss': 0.04421599954366684, 'validation/mean_average_precision': 0.26658950567330963, 'validation/num_examples': 43793, 'test/accuracy': 0.985908031463623, 'test/loss': 0.04710817337036133, 'test/mean_average_precision': 0.2536225909294436, 'test/num_examples': 43793, 'score': 6501.003589630127, 'total_duration': 10248.222026586533, 'accumulated_submission_time': 6501.003589630127, 'accumulated_eval_time': 3745.8714208602905, 'accumulated_logging_time': 0.7897212505340576, 'global_step': 20378, 'preemption_count': 0}), (21131, {'train/accuracy': 0.9910106658935547, 'train/loss': 0.029451150447130203, 'train/mean_average_precision': 0.42636138282913516, 'validation/accuracy': 0.9867987632751465, 'validation/loss': 0.044612444937229156, 'validation/mean_average_precision': 0.26530508454363316, 'validation/num_examples': 43793, 'test/accuracy': 0.9859185814857483, 'test/loss': 0.04750733822584152, 'test/mean_average_precision': 0.2502607926465779, 'test/num_examples': 43793, 'score': 6741.249910831451, 'total_duration': 10617.316989421844, 'accumulated_submission_time': 6741.249910831451, 'accumulated_eval_time': 3874.662131547928, 'accumulated_logging_time': 0.8251934051513672, 'global_step': 21131, 'preemption_count': 0}), (21888, {'train/accuracy': 0.9911861419677734, 'train/loss': 0.028838839381933212, 'train/mean_average_precision': 0.44508326605191784, 'validation/accuracy': 0.9867784976959229, 'validation/loss': 0.04432143643498421, 'validation/mean_average_precision': 0.27322503824153394, 'validation/num_examples': 43793, 'test/accuracy': 0.985975444316864, 'test/loss': 0.04727840796113014, 'test/mean_average_precision': 0.25190238978307966, 'test/num_examples': 43793, 'score': 6981.331892490387, 'total_duration': 10987.689888238907, 'accumulated_submission_time': 6981.331892490387, 'accumulated_eval_time': 4004.901304244995, 'accumulated_logging_time': 0.8566954135894775, 'global_step': 21888, 'preemption_count': 0}), (22651, {'train/accuracy': 0.9912999868392944, 'train/loss': 0.02892247587442398, 'train/mean_average_precision': 0.45293090255351975, 'validation/accuracy': 0.9866639971733093, 'validation/loss': 0.044177647680044174, 'validation/mean_average_precision': 0.2652814454767527, 'validation/num_examples': 43793, 'test/accuracy': 0.9859510064125061, 'test/loss': 0.046832095831632614, 'test/mean_average_precision': 0.2558588523560185, 'test/num_examples': 43793, 'score': 7221.436220884323, 'total_duration': 11355.389991521835, 'accumulated_submission_time': 7221.436220884323, 'accumulated_eval_time': 4132.4447610378265, 'accumulated_logging_time': 0.8884739875793457, 'global_step': 22651, 'preemption_count': 0}), (23409, {'train/accuracy': 0.9911875128746033, 'train/loss': 0.028938593342900276, 'train/mean_average_precision': 0.44682227201069025, 'validation/accuracy': 0.98687344789505, 'validation/loss': 0.04422194883227348, 'validation/mean_average_precision': 0.26590192837812177, 'validation/num_examples': 43793, 'test/accuracy': 0.9860158562660217, 'test/loss': 0.047047268599271774, 'test/mean_average_precision': 0.2603437330110667, 'test/num_examples': 43793, 'score': 7461.514452457428, 'total_duration': 11723.900267839432, 'accumulated_submission_time': 7461.514452457428, 'accumulated_eval_time': 4260.824286222458, 'accumulated_logging_time': 0.9203827381134033, 'global_step': 23409, 'preemption_count': 0}), (24166, {'train/accuracy': 0.9908936619758606, 'train/loss': 0.029993336647748947, 'train/mean_average_precision': 0.4147140270965586, 'validation/accuracy': 0.9866806268692017, 'validation/loss': 0.044428519904613495, 'validation/mean_average_precision': 0.26665260090581644, 'validation/num_examples': 43793, 'test/accuracy': 0.9858659505844116, 'test/loss': 0.04725208878517151, 'test/mean_average_precision': 0.25419347882160603, 'test/num_examples': 43793, 'score': 7701.180008888245, 'total_duration': 12093.078660488129, 'accumulated_submission_time': 7701.180008888245, 'accumulated_eval_time': 4389.98766207695, 'accumulated_logging_time': 1.248992681503296, 'global_step': 24166, 'preemption_count': 0}), (24915, {'train/accuracy': 0.9908816814422607, 'train/loss': 0.029930898919701576, 'train/mean_average_precision': 0.41416300800734485, 'validation/accuracy': 0.9867399334907532, 'validation/loss': 0.04429195448756218, 'validation/mean_average_precision': 0.2720565043987341, 'validation/num_examples': 43793, 'test/accuracy': 0.9859838485717773, 'test/loss': 0.04694953188300133, 'test/mean_average_precision': 0.2616324466083824, 'test/num_examples': 43793, 'score': 7941.191206932068, 'total_duration': 12459.79927778244, 'accumulated_submission_time': 7941.191206932068, 'accumulated_eval_time': 4516.645851135254, 'accumulated_logging_time': 1.2801299095153809, 'global_step': 24915, 'preemption_count': 0}), (25671, {'train/accuracy': 0.9910153746604919, 'train/loss': 0.02948160283267498, 'train/mean_average_precision': 0.42614096978648197, 'validation/accuracy': 0.9866116046905518, 'validation/loss': 0.044560737907886505, 'validation/mean_average_precision': 0.26972062907659367, 'validation/num_examples': 43793, 'test/accuracy': 0.985874354839325, 'test/loss': 0.047286227345466614, 'test/mean_average_precision': 0.25563352939221756, 'test/num_examples': 43793, 'score': 8181.43877363205, 'total_duration': 12830.020049333572, 'accumulated_submission_time': 8181.43877363205, 'accumulated_eval_time': 4646.567767858505, 'accumulated_logging_time': 1.3114888668060303, 'global_step': 25671, 'preemption_count': 0}), (26422, {'train/accuracy': 0.9910532832145691, 'train/loss': 0.029158132150769234, 'train/mean_average_precision': 0.4336587863156425, 'validation/accuracy': 0.9867833256721497, 'validation/loss': 0.04481257125735283, 'validation/mean_average_precision': 0.26791099113968614, 'validation/num_examples': 43793, 'test/accuracy': 0.9859535694122314, 'test/loss': 0.04782743752002716, 'test/mean_average_precision': 0.25248273817826317, 'test/num_examples': 43793, 'score': 8421.660665512085, 'total_duration': 13200.347779750824, 'accumulated_submission_time': 8421.660665512085, 'accumulated_eval_time': 4776.618052721024, 'accumulated_logging_time': 1.3456156253814697, 'global_step': 26422, 'preemption_count': 0}), (27176, {'train/accuracy': 0.9911747574806213, 'train/loss': 0.028925133869051933, 'train/mean_average_precision': 0.44522544274458764, 'validation/accuracy': 0.9868060946464539, 'validation/loss': 0.044306039810180664, 'validation/mean_average_precision': 0.27197369995391546, 'validation/num_examples': 43793, 'test/accuracy': 0.9859000444412231, 'test/loss': 0.04738543927669525, 'test/mean_average_precision': 0.2587810042911728, 'test/num_examples': 43793, 'score': 8661.61139369011, 'total_duration': 13566.684311151505, 'accumulated_submission_time': 8661.61139369011, 'accumulated_eval_time': 4902.952599287033, 'accumulated_logging_time': 1.3763277530670166, 'global_step': 27176, 'preemption_count': 0}), (27925, {'train/accuracy': 0.9912655353546143, 'train/loss': 0.028265230357646942, 'train/mean_average_precision': 0.4763750571741873, 'validation/accuracy': 0.9868454337120056, 'validation/loss': 0.0445428192615509, 'validation/mean_average_precision': 0.27583540975590737, 'validation/num_examples': 43793, 'test/accuracy': 0.9859707951545715, 'test/loss': 0.047591667622327805, 'test/mean_average_precision': 0.2610465782854482, 'test/num_examples': 43793, 'score': 8901.743763685226, 'total_duration': 13938.37245965004, 'accumulated_submission_time': 8901.743763685226, 'accumulated_eval_time': 5034.454385757446, 'accumulated_logging_time': 1.4076869487762451, 'global_step': 27925, 'preemption_count': 0}), (28676, {'train/accuracy': 0.9915241003036499, 'train/loss': 0.027481885626912117, 'train/mean_average_precision': 0.4806527878827299, 'validation/accuracy': 0.9867256879806519, 'validation/loss': 0.04456234723329544, 'validation/mean_average_precision': 0.27001576131983057, 'validation/num_examples': 43793, 'test/accuracy': 0.9859851598739624, 'test/loss': 0.04734373465180397, 'test/mean_average_precision': 0.2601059381326383, 'test/num_examples': 43793, 'score': 9141.946783781052, 'total_duration': 14305.943809747696, 'accumulated_submission_time': 9141.946783781052, 'accumulated_eval_time': 5161.768011569977, 'accumulated_logging_time': 1.440685749053955, 'global_step': 28676, 'preemption_count': 0}), (29433, {'train/accuracy': 0.9914757609367371, 'train/loss': 0.02768174186348915, 'train/mean_average_precision': 0.4703134444485871, 'validation/accuracy': 0.9867504835128784, 'validation/loss': 0.0449969619512558, 'validation/mean_average_precision': 0.26254527480645284, 'validation/num_examples': 43793, 'test/accuracy': 0.9858773350715637, 'test/loss': 0.047853145748376846, 'test/mean_average_precision': 0.2550559410887236, 'test/num_examples': 43793, 'score': 9382.060484170914, 'total_duration': 14675.473503351212, 'accumulated_submission_time': 9382.060484170914, 'accumulated_eval_time': 5291.131680011749, 'accumulated_logging_time': 1.4723448753356934, 'global_step': 29433, 'preemption_count': 0}), (30189, {'train/accuracy': 0.9917797446250916, 'train/loss': 0.026923639699816704, 'train/mean_average_precision': 0.4778185300817284, 'validation/accuracy': 0.986819863319397, 'validation/loss': 0.04437655583024025, 'validation/mean_average_precision': 0.27501181817828557, 'validation/num_examples': 43793, 'test/accuracy': 0.9859526753425598, 'test/loss': 0.04693050682544708, 'test/mean_average_precision': 0.26771399555357955, 'test/num_examples': 43793, 'score': 9622.095024824142, 'total_duration': 15039.375715255737, 'accumulated_submission_time': 9622.095024824142, 'accumulated_eval_time': 5414.945363521576, 'accumulated_logging_time': 1.5057015419006348, 'global_step': 30189, 'preemption_count': 0}), (30947, {'train/accuracy': 0.9916726350784302, 'train/loss': 0.027349548414349556, 'train/mean_average_precision': 0.48438762137735747, 'validation/accuracy': 0.986823558807373, 'validation/loss': 0.04459654912352562, 'validation/mean_average_precision': 0.27586436909845735, 'validation/num_examples': 43793, 'test/accuracy': 0.9860154390335083, 'test/loss': 0.04735671728849411, 'test/mean_average_precision': 0.2568862944563916, 'test/num_examples': 43793, 'score': 9862.135322093964, 'total_duration': 15405.955041885376, 'accumulated_submission_time': 9862.135322093964, 'accumulated_eval_time': 5541.432502031326, 'accumulated_logging_time': 1.5375621318817139, 'global_step': 30947, 'preemption_count': 0}), (31701, {'train/accuracy': 0.9913953542709351, 'train/loss': 0.028089625760912895, 'train/mean_average_precision': 0.4647237618074311, 'validation/accuracy': 0.9867054224014282, 'validation/loss': 0.044793397188186646, 'validation/mean_average_precision': 0.2632075575298583, 'validation/num_examples': 43793, 'test/accuracy': 0.9858874082565308, 'test/loss': 0.04773210734128952, 'test/mean_average_precision': 0.25401344071206705, 'test/num_examples': 43793, 'score': 10102.235368013382, 'total_duration': 15778.814150571823, 'accumulated_submission_time': 10102.235368013382, 'accumulated_eval_time': 5674.134570837021, 'accumulated_logging_time': 1.574239730834961, 'global_step': 31701, 'preemption_count': 0}), (32453, {'train/accuracy': 0.9914029240608215, 'train/loss': 0.028008729219436646, 'train/mean_average_precision': 0.4645824797357492, 'validation/accuracy': 0.9867374897003174, 'validation/loss': 0.04449669271707535, 'validation/mean_average_precision': 0.26626106581850995, 'validation/num_examples': 43793, 'test/accuracy': 0.9859194159507751, 'test/loss': 0.04711798578500748, 'test/mean_average_precision': 0.26194080149817495, 'test/num_examples': 43793, 'score': 10342.45687031746, 'total_duration': 16147.693119049072, 'accumulated_submission_time': 10342.45687031746, 'accumulated_eval_time': 5802.739213705063, 'accumulated_logging_time': 1.6064386367797852, 'global_step': 32453, 'preemption_count': 0}), (33209, {'train/accuracy': 0.9913453459739685, 'train/loss': 0.02826383337378502, 'train/mean_average_precision': 0.4542848687332359, 'validation/accuracy': 0.9869092106819153, 'validation/loss': 0.04439977928996086, 'validation/mean_average_precision': 0.2762699325917176, 'validation/num_examples': 43793, 'test/accuracy': 0.9859253168106079, 'test/loss': 0.04754733666777611, 'test/mean_average_precision': 0.2548237538397603, 'test/num_examples': 43793, 'score': 10582.4124584198, 'total_duration': 16517.30669927597, 'accumulated_submission_time': 10582.4124584198, 'accumulated_eval_time': 5932.344577074051, 'accumulated_logging_time': 1.6389267444610596, 'global_step': 33209, 'preemption_count': 0}), (33967, {'train/accuracy': 0.991368293762207, 'train/loss': 0.027924323454499245, 'train/mean_average_precision': 0.4556313674502258, 'validation/accuracy': 0.986847460269928, 'validation/loss': 0.04521167278289795, 'validation/mean_average_precision': 0.2753139830957439, 'validation/num_examples': 43793, 'test/accuracy': 0.9860167503356934, 'test/loss': 0.048343922942876816, 'test/mean_average_precision': 0.2645479106855683, 'test/num_examples': 43793, 'score': 10822.565240621567, 'total_duration': 16882.73788666725, 'accumulated_submission_time': 10822.565240621567, 'accumulated_eval_time': 6057.570507287979, 'accumulated_logging_time': 1.6709973812103271, 'global_step': 33967, 'preemption_count': 0}), (34715, {'train/accuracy': 0.9916467070579529, 'train/loss': 0.027205245569348335, 'train/mean_average_precision': 0.477621432016776, 'validation/accuracy': 0.9869270324707031, 'validation/loss': 0.04441390186548233, 'validation/mean_average_precision': 0.2780114628194121, 'validation/num_examples': 43793, 'test/accuracy': 0.9859063625335693, 'test/loss': 0.0478287898004055, 'test/mean_average_precision': 0.25468529277239105, 'test/num_examples': 43793, 'score': 11062.638055562973, 'total_duration': 17256.9448492527, 'accumulated_submission_time': 11062.638055562973, 'accumulated_eval_time': 6191.6519594192505, 'accumulated_logging_time': 1.7039594650268555, 'global_step': 34715, 'preemption_count': 0}), (35464, {'train/accuracy': 0.9917681217193604, 'train/loss': 0.02679045870900154, 'train/mean_average_precision': 0.4988786142774347, 'validation/accuracy': 0.986970067024231, 'validation/loss': 0.0444306842982769, 'validation/mean_average_precision': 0.2751348822187652, 'validation/num_examples': 43793, 'test/accuracy': 0.9860761165618896, 'test/loss': 0.0475175641477108, 'test/mean_average_precision': 0.2631292091684028, 'test/num_examples': 43793, 'score': 11302.731583595276, 'total_duration': 17621.38951563835, 'accumulated_submission_time': 11302.731583595276, 'accumulated_eval_time': 6315.94552397728, 'accumulated_logging_time': 1.7400023937225342, 'global_step': 35464, 'preemption_count': 0}), (36217, {'train/accuracy': 0.9920303225517273, 'train/loss': 0.02567622810602188, 'train/mean_average_precision': 0.5184381943115008, 'validation/accuracy': 0.9868994355201721, 'validation/loss': 0.04505499452352524, 'validation/mean_average_precision': 0.27438343203587545, 'validation/num_examples': 43793, 'test/accuracy': 0.9859569072723389, 'test/loss': 0.04814833775162697, 'test/mean_average_precision': 0.2617889807945268, 'test/num_examples': 43793, 'score': 11542.948122262955, 'total_duration': 17991.900566101074, 'accumulated_submission_time': 11542.948122262955, 'accumulated_eval_time': 6446.186127901077, 'accumulated_logging_time': 1.7738418579101562, 'global_step': 36217, 'preemption_count': 0}), (36976, {'train/accuracy': 0.9921303391456604, 'train/loss': 0.025417350232601166, 'train/mean_average_precision': 0.5227613756415113, 'validation/accuracy': 0.9869270324707031, 'validation/loss': 0.044962819665670395, 'validation/mean_average_precision': 0.2739362064526262, 'validation/num_examples': 43793, 'test/accuracy': 0.9859548211097717, 'test/loss': 0.04786033555865288, 'test/mean_average_precision': 0.2623279002542574, 'test/num_examples': 43793, 'score': 11782.979050397873, 'total_duration': 18357.6857047081, 'accumulated_submission_time': 11782.979050397873, 'accumulated_eval_time': 6571.886204242706, 'accumulated_logging_time': 1.8077149391174316, 'global_step': 36976, 'preemption_count': 0}), (37725, {'train/accuracy': 0.992214024066925, 'train/loss': 0.025279011577367783, 'train/mean_average_precision': 0.532232625381326, 'validation/accuracy': 0.9869457483291626, 'validation/loss': 0.0449865348637104, 'validation/mean_average_precision': 0.27531428077447273, 'validation/num_examples': 43793, 'test/accuracy': 0.9860011339187622, 'test/loss': 0.048079103231430054, 'test/mean_average_precision': 0.26356978639549017, 'test/num_examples': 43793, 'score': 12023.23389339447, 'total_duration': 18726.489814043045, 'accumulated_submission_time': 12023.23389339447, 'accumulated_eval_time': 6700.37837600708, 'accumulated_logging_time': 1.8413872718811035, 'global_step': 37725, 'preemption_count': 0}), (38488, {'train/accuracy': 0.9921120405197144, 'train/loss': 0.025479435920715332, 'train/mean_average_precision': 0.5159274884360091, 'validation/accuracy': 0.9868957996368408, 'validation/loss': 0.04491075873374939, 'validation/mean_average_precision': 0.27448658700597134, 'validation/num_examples': 43793, 'test/accuracy': 0.9859851598739624, 'test/loss': 0.04786010459065437, 'test/mean_average_precision': 0.25794411793794275, 'test/num_examples': 43793, 'score': 12263.267944574356, 'total_duration': 19091.986697912216, 'accumulated_submission_time': 12263.267944574356, 'accumulated_eval_time': 6825.7874138355255, 'accumulated_logging_time': 1.8750951290130615, 'global_step': 38488, 'preemption_count': 0}), (39243, {'train/accuracy': 0.9920024275779724, 'train/loss': 0.026045996695756912, 'train/mean_average_precision': 0.5064120084947242, 'validation/accuracy': 0.9869615435600281, 'validation/loss': 0.04505595192313194, 'validation/mean_average_precision': 0.2774751291319751, 'validation/num_examples': 43793, 'test/accuracy': 0.9859825968742371, 'test/loss': 0.0481785349547863, 'test/mean_average_precision': 0.26056772462046063, 'test/num_examples': 43793, 'score': 12503.376068115234, 'total_duration': 19458.20954990387, 'accumulated_submission_time': 12503.376068115234, 'accumulated_eval_time': 6951.847330093384, 'accumulated_logging_time': 1.91015625, 'global_step': 39243, 'preemption_count': 0}), (40004, {'train/accuracy': 0.9919900298118591, 'train/loss': 0.025957776233553886, 'train/mean_average_precision': 0.5071904458124843, 'validation/accuracy': 0.9868775010108948, 'validation/loss': 0.0453401543200016, 'validation/mean_average_precision': 0.2724499825709285, 'validation/num_examples': 43793, 'test/accuracy': 0.9859552383422852, 'test/loss': 0.048367008566856384, 'test/mean_average_precision': 0.2582643340499317, 'test/num_examples': 43793, 'score': 12743.403692007065, 'total_duration': 19822.685611486435, 'accumulated_submission_time': 12743.403692007065, 'accumulated_eval_time': 7076.241222858429, 'accumulated_logging_time': 1.944312334060669, 'global_step': 40004, 'preemption_count': 0}), (40763, {'train/accuracy': 0.9918793439865112, 'train/loss': 0.026220152154564857, 'train/mean_average_precision': 0.4992228026531985, 'validation/accuracy': 0.9869359731674194, 'validation/loss': 0.04502296820282936, 'validation/mean_average_precision': 0.2775892452836066, 'validation/num_examples': 43793, 'test/accuracy': 0.9860268235206604, 'test/loss': 0.04802815243601799, 'test/mean_average_precision': 0.2584114993433583, 'test/num_examples': 43793, 'score': 12983.375659227371, 'total_duration': 20190.81739640236, 'accumulated_submission_time': 12983.375659227371, 'accumulated_eval_time': 7204.345838546753, 'accumulated_logging_time': 1.9792215824127197, 'global_step': 40763, 'preemption_count': 0}), (41521, {'train/accuracy': 0.9919157028198242, 'train/loss': 0.026108678430318832, 'train/mean_average_precision': 0.5037610390684488, 'validation/accuracy': 0.986825168132782, 'validation/loss': 0.0457451306283474, 'validation/mean_average_precision': 0.2686163429857319, 'validation/num_examples': 43793, 'test/accuracy': 0.9859375357627869, 'test/loss': 0.04884037375450134, 'test/mean_average_precision': 0.2549508720399013, 'test/num_examples': 43793, 'score': 13223.54290318489, 'total_duration': 20555.041393518448, 'accumulated_submission_time': 13223.54290318489, 'accumulated_eval_time': 7328.3456082344055, 'accumulated_logging_time': 2.01588773727417, 'global_step': 41521, 'preemption_count': 0}), (42273, {'train/accuracy': 0.9921371936798096, 'train/loss': 0.025424858555197716, 'train/mean_average_precision': 0.517208637472588, 'validation/accuracy': 0.986750066280365, 'validation/loss': 0.045256342738866806, 'validation/mean_average_precision': 0.2774649701945903, 'validation/num_examples': 43793, 'test/accuracy': 0.985990583896637, 'test/loss': 0.04822531342506409, 'test/mean_average_precision': 0.2598064917046744, 'test/num_examples': 43793, 'score': 13463.506523132324, 'total_duration': 20921.390644073486, 'accumulated_submission_time': 13463.506523132324, 'accumulated_eval_time': 7454.671268939972, 'accumulated_logging_time': 2.0549404621124268, 'global_step': 42273, 'preemption_count': 0}), (43031, {'train/accuracy': 0.9923945665359497, 'train/loss': 0.024646632373332977, 'train/mean_average_precision': 0.5287445837948694, 'validation/accuracy': 0.9867159724235535, 'validation/loss': 0.04515005648136139, 'validation/mean_average_precision': 0.27577266251505045, 'validation/num_examples': 43793, 'test/accuracy': 0.9858831763267517, 'test/loss': 0.04805005341768265, 'test/mean_average_precision': 0.2615175880406216, 'test/num_examples': 43793, 'score': 13703.503736972809, 'total_duration': 21289.939838171005, 'accumulated_submission_time': 13703.503736972809, 'accumulated_eval_time': 7583.163735151291, 'accumulated_logging_time': 2.093395233154297, 'global_step': 43031, 'preemption_count': 0}), (43777, {'train/accuracy': 0.9924582839012146, 'train/loss': 0.0242256261408329, 'train/mean_average_precision': 0.5548626450600167, 'validation/accuracy': 0.9868783354759216, 'validation/loss': 0.045423127710819244, 'validation/mean_average_precision': 0.27506193304726434, 'validation/num_examples': 43793, 'test/accuracy': 0.986004114151001, 'test/loss': 0.04848497733473778, 'test/mean_average_precision': 0.2589780692558592, 'test/num_examples': 43793, 'score': 13943.558982849121, 'total_duration': 21658.204888105392, 'accumulated_submission_time': 13943.558982849121, 'accumulated_eval_time': 7711.316171169281, 'accumulated_logging_time': 2.129408359527588, 'global_step': 43777, 'preemption_count': 0}), (44533, {'train/accuracy': 0.9928352236747742, 'train/loss': 0.023032398894429207, 'train/mean_average_precision': 0.582281300015573, 'validation/accuracy': 0.9868206977844238, 'validation/loss': 0.04551013559103012, 'validation/mean_average_precision': 0.2740454856114717, 'validation/num_examples': 43793, 'test/accuracy': 0.9860752820968628, 'test/loss': 0.048421405255794525, 'test/mean_average_precision': 0.2575476179842053, 'test/num_examples': 43793, 'score': 14183.621313095093, 'total_duration': 22021.435887813568, 'accumulated_submission_time': 14183.621313095093, 'accumulated_eval_time': 7834.429327011108, 'accumulated_logging_time': 2.1637284755706787, 'global_step': 44533, 'preemption_count': 0}), (45269, {'train/accuracy': 0.9929222464561462, 'train/loss': 0.022949127480387688, 'train/mean_average_precision': 0.5656440620216915, 'validation/accuracy': 0.9867017269134521, 'validation/loss': 0.04587439075112343, 'validation/mean_average_precision': 0.27031047176916695, 'validation/num_examples': 43793, 'test/accuracy': 0.9858570694923401, 'test/loss': 0.048946723341941833, 'test/mean_average_precision': 0.25800982874321193, 'test/num_examples': 43793, 'score': 14423.575603961945, 'total_duration': 22392.48312997818, 'accumulated_submission_time': 14423.575603961945, 'accumulated_eval_time': 7965.456509590149, 'accumulated_logging_time': 2.2040088176727295, 'global_step': 45269, 'preemption_count': 0}), (46022, {'train/accuracy': 0.9929423928260803, 'train/loss': 0.022848380729556084, 'train/mean_average_precision': 0.5816965987814018, 'validation/accuracy': 0.9868153929710388, 'validation/loss': 0.04570256546139717, 'validation/mean_average_precision': 0.27749283056789126, 'validation/num_examples': 43793, 'test/accuracy': 0.9859346151351929, 'test/loss': 0.04876365885138512, 'test/mean_average_precision': 0.2573021483270661, 'test/num_examples': 43793, 'score': 14663.548845529556, 'total_duration': 22755.651258468628, 'accumulated_submission_time': 14663.548845529556, 'accumulated_eval_time': 8088.594908952713, 'accumulated_logging_time': 2.2400035858154297, 'global_step': 46022, 'preemption_count': 0}), (46775, {'train/accuracy': 0.9926509261131287, 'train/loss': 0.02339848317205906, 'train/mean_average_precision': 0.5500882324221318, 'validation/accuracy': 0.9869351387023926, 'validation/loss': 0.04617488384246826, 'validation/mean_average_precision': 0.27597097459884756, 'validation/num_examples': 43793, 'test/accuracy': 0.9860348105430603, 'test/loss': 0.04929579421877861, 'test/mean_average_precision': 0.26160574698396855, 'test/num_examples': 43793, 'score': 14903.751368761063, 'total_duration': 23120.890134334564, 'accumulated_submission_time': 14903.751368761063, 'accumulated_eval_time': 8213.572768211365, 'accumulated_logging_time': 2.277043104171753, 'global_step': 46775, 'preemption_count': 0}), (47526, {'train/accuracy': 0.9926035404205322, 'train/loss': 0.02364971674978733, 'train/mean_average_precision': 0.5587826948908996, 'validation/accuracy': 0.9867784976959229, 'validation/loss': 0.046003635972738266, 'validation/mean_average_precision': 0.2770768478543392, 'validation/num_examples': 43793, 'test/accuracy': 0.985968291759491, 'test/loss': 0.04908977448940277, 'test/mean_average_precision': 0.2601720723494071, 'test/num_examples': 43793, 'score': 15143.801016807556, 'total_duration': 23484.67352104187, 'accumulated_submission_time': 15143.801016807556, 'accumulated_eval_time': 8337.250361442566, 'accumulated_logging_time': 2.31246018409729, 'global_step': 47526, 'preemption_count': 0}), (48271, {'train/accuracy': 0.9926998019218445, 'train/loss': 0.02341970056295395, 'train/mean_average_precision': 0.5763855108885503, 'validation/accuracy': 0.9868454337120056, 'validation/loss': 0.04597317427396774, 'validation/mean_average_precision': 0.2784930722589843, 'validation/num_examples': 43793, 'test/accuracy': 0.986004114151001, 'test/loss': 0.04900640994310379, 'test/mean_average_precision': 0.26046014519958743, 'test/num_examples': 43793, 'score': 15383.883342027664, 'total_duration': 23848.569514989853, 'accumulated_submission_time': 15383.883342027664, 'accumulated_eval_time': 8461.005041599274, 'accumulated_logging_time': 2.348806619644165, 'global_step': 48271, 'preemption_count': 0}), (49033, {'train/accuracy': 0.9927592277526855, 'train/loss': 0.023252515122294426, 'train/mean_average_precision': 0.5598052472954665, 'validation/accuracy': 0.9868369102478027, 'validation/loss': 0.04622039198875427, 'validation/mean_average_precision': 0.276565655192867, 'validation/num_examples': 43793, 'test/accuracy': 0.9859733581542969, 'test/loss': 0.04939107596874237, 'test/mean_average_precision': 0.2595510327017606, 'test/num_examples': 43793, 'score': 15624.072097301483, 'total_duration': 24214.641152620316, 'accumulated_submission_time': 15624.072097301483, 'accumulated_eval_time': 8586.831431388855, 'accumulated_logging_time': 2.3845295906066895, 'global_step': 49033, 'preemption_count': 0}), (49787, {'train/accuracy': 0.992779016494751, 'train/loss': 0.02304934710264206, 'train/mean_average_precision': 0.557587615426931, 'validation/accuracy': 0.9867760539054871, 'validation/loss': 0.046717461198568344, 'validation/mean_average_precision': 0.2682286383885479, 'validation/num_examples': 43793, 'test/accuracy': 0.9858781695365906, 'test/loss': 0.04981175810098648, 'test/mean_average_precision': 0.25385138040262395, 'test/num_examples': 43793, 'score': 15864.111089468002, 'total_duration': 24579.27449965477, 'accumulated_submission_time': 15864.111089468002, 'accumulated_eval_time': 8711.369632005692, 'accumulated_logging_time': 2.420367956161499, 'global_step': 49787, 'preemption_count': 0}), (50541, {'train/accuracy': 0.9930933713912964, 'train/loss': 0.021858109161257744, 'train/mean_average_precision': 0.5951135774073243, 'validation/accuracy': 0.986832857131958, 'validation/loss': 0.04674899950623512, 'validation/mean_average_precision': 0.27699800295007676, 'validation/num_examples': 43793, 'test/accuracy': 0.9860200881958008, 'test/loss': 0.04974102973937988, 'test/mean_average_precision': 0.2600190963165037, 'test/num_examples': 43793, 'score': 16104.149505615234, 'total_duration': 24940.703069210052, 'accumulated_submission_time': 16104.149505615234, 'accumulated_eval_time': 8832.70336985588, 'accumulated_logging_time': 2.456578016281128, 'global_step': 50541, 'preemption_count': 0}), (51297, {'train/accuracy': 0.9931976199150085, 'train/loss': 0.021701239049434662, 'train/mean_average_precision': 0.591113654143453, 'validation/accuracy': 0.9867837429046631, 'validation/loss': 0.046719688922166824, 'validation/mean_average_precision': 0.276880985821581, 'validation/num_examples': 43793, 'test/accuracy': 0.9858894944190979, 'test/loss': 0.049776021391153336, 'test/mean_average_precision': 0.26097687724729496, 'test/num_examples': 43793, 'score': 16344.100379228592, 'total_duration': 25306.192680835724, 'accumulated_submission_time': 16344.100379228592, 'accumulated_eval_time': 8958.184759140015, 'accumulated_logging_time': 2.49343204498291, 'global_step': 51297, 'preemption_count': 0}), (52052, {'train/accuracy': 0.9936760067939758, 'train/loss': 0.02026781253516674, 'train/mean_average_precision': 0.6317658547986209, 'validation/accuracy': 0.9868324398994446, 'validation/loss': 0.04677682742476463, 'validation/mean_average_precision': 0.2784929432021214, 'validation/num_examples': 43793, 'test/accuracy': 0.985987663269043, 'test/loss': 0.049991969019174576, 'test/mean_average_precision': 0.2584287179698735, 'test/num_examples': 43793, 'score': 16584.350385904312, 'total_duration': 25670.05592918396, 'accumulated_submission_time': 16584.350385904312, 'accumulated_eval_time': 9081.7381067276, 'accumulated_logging_time': 2.5313422679901123, 'global_step': 52052, 'preemption_count': 0}), (52811, {'train/accuracy': 0.9940836429595947, 'train/loss': 0.019118543714284897, 'train/mean_average_precision': 0.6474843811104636, 'validation/accuracy': 0.9867330193519592, 'validation/loss': 0.047576721757650375, 'validation/mean_average_precision': 0.27017016827719925, 'validation/num_examples': 43793, 'test/accuracy': 0.9858625531196594, 'test/loss': 0.05072097107768059, 'test/mean_average_precision': 0.2562704321462084, 'test/num_examples': 43793, 'score': 16824.390946626663, 'total_duration': 26035.989722013474, 'accumulated_submission_time': 16824.390946626663, 'accumulated_eval_time': 9207.574704885483, 'accumulated_logging_time': 2.5672249794006348, 'global_step': 52811, 'preemption_count': 0}), (53562, {'train/accuracy': 0.9939260482788086, 'train/loss': 0.019581323489546776, 'train/mean_average_precision': 0.6418360793553928, 'validation/accuracy': 0.9867784976959229, 'validation/loss': 0.04765912890434265, 'validation/mean_average_precision': 0.27103645399454124, 'validation/num_examples': 43793, 'test/accuracy': 0.985888659954071, 'test/loss': 0.050796233117580414, 'test/mean_average_precision': 0.25489262077263214, 'test/num_examples': 43793, 'score': 17064.626448631287, 'total_duration': 26399.916737556458, 'accumulated_submission_time': 17064.626448631287, 'accumulated_eval_time': 9331.2084608078, 'accumulated_logging_time': 2.6037094593048096, 'global_step': 53562, 'preemption_count': 0}), (54304, {'train/accuracy': 0.9939098954200745, 'train/loss': 0.019574008882045746, 'train/mean_average_precision': 0.6298671260120975, 'validation/accuracy': 0.9867439866065979, 'validation/loss': 0.04797590896487236, 'validation/mean_average_precision': 0.2708166189977605, 'validation/num_examples': 43793, 'test/accuracy': 0.9858903884887695, 'test/loss': 0.05098933354020119, 'test/mean_average_precision': 0.25284671020636434, 'test/num_examples': 43793, 'score': 17304.80330300331, 'total_duration': 26761.601624011993, 'accumulated_submission_time': 17304.80330300331, 'accumulated_eval_time': 9452.654334545135, 'accumulated_logging_time': 2.6448752880096436, 'global_step': 54304, 'preemption_count': 0}), (55063, {'train/accuracy': 0.9937072992324829, 'train/loss': 0.020246917381882668, 'train/mean_average_precision': 0.6290030533487323, 'validation/accuracy': 0.9866514205932617, 'validation/loss': 0.047722216695547104, 'validation/mean_average_precision': 0.2730129936795538, 'validation/num_examples': 43793, 'test/accuracy': 0.9858301281929016, 'test/loss': 0.05093853175640106, 'test/mean_average_precision': 0.2527652640350336, 'test/num_examples': 43793, 'score': 17544.856785297394, 'total_duration': 27128.370005607605, 'accumulated_submission_time': 17544.856785297394, 'accumulated_eval_time': 9579.312143564224, 'accumulated_logging_time': 2.6809160709381104, 'global_step': 55063, 'preemption_count': 0}), (55815, {'train/accuracy': 0.9936484694480896, 'train/loss': 0.020147372037172318, 'train/mean_average_precision': 0.6260407024076896, 'validation/accuracy': 0.9866623878479004, 'validation/loss': 0.04816872626543045, 'validation/mean_average_precision': 0.2720521240646281, 'validation/num_examples': 43793, 'test/accuracy': 0.9858187437057495, 'test/loss': 0.05146540701389313, 'test/mean_average_precision': 0.25340895317912365, 'test/num_examples': 43793, 'score': 17784.952792406082, 'total_duration': 27491.043491363525, 'accumulated_submission_time': 17784.952792406082, 'accumulated_eval_time': 9701.83114695549, 'accumulated_logging_time': 2.7188405990600586, 'global_step': 55815, 'preemption_count': 0}), (56567, {'train/accuracy': 0.993515133857727, 'train/loss': 0.02034720405936241, 'train/mean_average_precision': 0.6213367212781136, 'validation/accuracy': 0.9865515232086182, 'validation/loss': 0.04893568530678749, 'validation/mean_average_precision': 0.2671761914545285, 'validation/num_examples': 43793, 'test/accuracy': 0.985710084438324, 'test/loss': 0.052274417132139206, 'test/mean_average_precision': 0.25407019463630753, 'test/num_examples': 43793, 'score': 18025.06869673729, 'total_duration': 27857.546080112457, 'accumulated_submission_time': 18025.06869673729, 'accumulated_eval_time': 9828.160341501236, 'accumulated_logging_time': 2.756070375442505, 'global_step': 56567, 'preemption_count': 0}), (57320, {'train/accuracy': 0.9935488104820251, 'train/loss': 0.020334692671895027, 'train/mean_average_precision': 0.611723780806605, 'validation/accuracy': 0.9865474700927734, 'validation/loss': 0.049405910074710846, 'validation/mean_average_precision': 0.26607081788312853, 'validation/num_examples': 43793, 'test/accuracy': 0.985775351524353, 'test/loss': 0.05271177738904953, 'test/mean_average_precision': 0.2552477887895668, 'test/num_examples': 43793, 'score': 18265.143191337585, 'total_duration': 28219.94671702385, 'accumulated_submission_time': 18265.143191337585, 'accumulated_eval_time': 9950.428166866302, 'accumulated_logging_time': 2.793794870376587, 'global_step': 57320, 'preemption_count': 0})], 'global_step': 57989}
I0205 05:42:24.250647 139919816816448 submission_runner.py:586] Timing: 18477.2107527256
I0205 05:42:24.250710 139919816816448 submission_runner.py:588] Total number of evals: 77
I0205 05:42:24.250759 139919816816448 submission_runner.py:589] ====================
I0205 05:42:24.250813 139919816816448 submission_runner.py:542] Using RNG seed 356686224
I0205 05:42:24.321285 139919816816448 submission_runner.py:551] --- Tuning run 2/5 ---
I0205 05:42:24.321439 139919816816448 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_1/ogbg_jax/trial_2.
I0205 05:42:24.321872 139919816816448 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_1/ogbg_jax/trial_2/hparams.json.
I0205 05:42:24.472754 139919816816448 submission_runner.py:206] Initializing dataset.
I0205 05:42:24.567524 139919816816448 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0205 05:42:24.571646 139919816816448 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0205 05:42:24.984644 139919816816448 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0205 05:42:25.026277 139919816816448 submission_runner.py:213] Initializing model.
I0205 05:42:27.513391 139919816816448 submission_runner.py:255] Initializing optimizer.
I0205 05:42:28.149615 139919816816448 submission_runner.py:262] Initializing metrics bundle.
I0205 05:42:28.149809 139919816816448 submission_runner.py:280] Initializing checkpoint and logger.
I0205 05:42:28.150528 139919816816448 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_1/ogbg_jax/trial_2 with prefix checkpoint_
I0205 05:42:28.150668 139919816816448 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_1/ogbg_jax/trial_2/meta_data_0.json.
I0205 05:42:28.150878 139919816816448 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0205 05:42:28.150945 139919816816448 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0205 05:42:29.477891 139919816816448 logger_utils.py:220] Unable to record git information. Continuing without it.
I0205 05:42:30.784407 139919816816448 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_1/ogbg_jax/trial_2/flags_0.json.
I0205 05:42:30.793639 139919816816448 submission_runner.py:314] Starting training loop.
I0205 05:42:42.993754 139735665018624 logging_writer.py:48] [0] global_step=0, grad_norm=2.5307114124298096, loss=0.7271946668624878
I0205 05:42:43.005540 139919816816448 spec.py:321] Evaluating on the training split.
I0205 05:44:41.759311 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 05:44:44.873275 139919816816448 spec.py:349] Evaluating on the test split.
I0205 05:44:47.953989 139919816816448 submission_runner.py:408] Time since start: 137.16s, 	Step: 1, 	{'train/accuracy': 0.5325733423233032, 'train/loss': 0.7276434302330017, 'train/mean_average_precision': 0.02318310626876276, 'validation/accuracy': 0.5230699777603149, 'validation/loss': 0.7331878542900085, 'validation/mean_average_precision': 0.025502919733897296, 'validation/num_examples': 43793, 'test/accuracy': 0.5214916467666626, 'test/loss': 0.7347381114959717, 'test/mean_average_precision': 0.02683098870779874, 'test/num_examples': 43793, 'score': 12.211849689483643, 'total_duration': 137.16028547286987, 'accumulated_submission_time': 12.211849689483643, 'accumulated_eval_time': 124.94838285446167, 'accumulated_logging_time': 0}
I0205 05:44:47.963412 139735693903616 logging_writer.py:48] [1] accumulated_eval_time=124.948383, accumulated_logging_time=0, accumulated_submission_time=12.211850, global_step=1, preemption_count=0, score=12.211850, test/accuracy=0.521492, test/loss=0.734738, test/mean_average_precision=0.026831, test/num_examples=43793, total_duration=137.160285, train/accuracy=0.532573, train/loss=0.727643, train/mean_average_precision=0.023183, validation/accuracy=0.523070, validation/loss=0.733188, validation/mean_average_precision=0.025503, validation/num_examples=43793
I0205 05:45:19.892107 139739177916160 logging_writer.py:48] [100] global_step=100, grad_norm=0.45758959650993347, loss=0.39823782444000244
I0205 05:45:51.738268 139735693903616 logging_writer.py:48] [200] global_step=200, grad_norm=0.34824225306510925, loss=0.30278274416923523
I0205 05:46:24.185368 139739177916160 logging_writer.py:48] [300] global_step=300, grad_norm=0.2488573044538498, loss=0.2072843313217163
I0205 05:46:56.038284 139735693903616 logging_writer.py:48] [400] global_step=400, grad_norm=0.1552249789237976, loss=0.13984517753124237
I0205 05:47:28.042357 139739177916160 logging_writer.py:48] [500] global_step=500, grad_norm=0.1021483764052391, loss=0.10229653865098953
I0205 05:47:59.911868 139735693903616 logging_writer.py:48] [600] global_step=600, grad_norm=0.07659836113452911, loss=0.07729928940534592
I0205 05:48:32.321411 139739177916160 logging_writer.py:48] [700] global_step=700, grad_norm=0.11265457421541214, loss=0.06860004365444183
I0205 05:48:48.169443 139919816816448 spec.py:321] Evaluating on the training split.
I0205 05:50:40.999243 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 05:50:44.024029 139919816816448 spec.py:349] Evaluating on the test split.
I0205 05:50:47.043773 139919816816448 submission_runner.py:408] Time since start: 496.25s, 	Step: 750, 	{'train/accuracy': 0.9867556691169739, 'train/loss': 0.06597922742366791, 'train/mean_average_precision': 0.0340223151502751, 'validation/accuracy': 0.9841179251670837, 'validation/loss': 0.07448847591876984, 'validation/mean_average_precision': 0.03621529879272365, 'validation/num_examples': 43793, 'test/accuracy': 0.983142077922821, 'test/loss': 0.07739343494176865, 'test/mean_average_precision': 0.03772474150749012, 'test/num_examples': 43793, 'score': 252.38562417030334, 'total_duration': 496.25007700920105, 'accumulated_submission_time': 252.38562417030334, 'accumulated_eval_time': 243.82266402244568, 'accumulated_logging_time': 0.021685361862182617}
I0205 05:50:47.059132 139735302588160 logging_writer.py:48] [750] accumulated_eval_time=243.822664, accumulated_logging_time=0.021685, accumulated_submission_time=252.385624, global_step=750, preemption_count=0, score=252.385624, test/accuracy=0.983142, test/loss=0.077393, test/mean_average_precision=0.037725, test/num_examples=43793, total_duration=496.250077, train/accuracy=0.986756, train/loss=0.065979, train/mean_average_precision=0.034022, validation/accuracy=0.984118, validation/loss=0.074488, validation/mean_average_precision=0.036215, validation/num_examples=43793
I0205 05:51:03.219033 139752296675072 logging_writer.py:48] [800] global_step=800, grad_norm=0.08017900586128235, loss=0.061718691140413284
I0205 05:51:35.389054 139735302588160 logging_writer.py:48] [900] global_step=900, grad_norm=0.034526605159044266, loss=0.06037572771310806
I0205 05:52:07.340273 139752296675072 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.0625947117805481, loss=0.05610598251223564
I0205 05:52:39.292511 139735302588160 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.05308472365140915, loss=0.05371088534593582
I0205 05:53:11.239444 139752296675072 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.02345551736652851, loss=0.04920197278261185
I0205 05:53:43.183940 139735302588160 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.0435941256582737, loss=0.05432315543293953
I0205 05:54:15.577116 139752296675072 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.02801426127552986, loss=0.04838261008262634
I0205 05:54:47.339460 139919816816448 spec.py:321] Evaluating on the training split.
I0205 05:56:52.155924 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 05:56:55.280739 139919816816448 spec.py:349] Evaluating on the test split.
I0205 05:56:58.269363 139919816816448 submission_runner.py:408] Time since start: 867.48s, 	Step: 1498, 	{'train/accuracy': 0.9871229529380798, 'train/loss': 0.04890026897192001, 'train/mean_average_precision': 0.08247394380439249, 'validation/accuracy': 0.9843728542327881, 'validation/loss': 0.05866231024265289, 'validation/mean_average_precision': 0.08369422795852387, 'validation/num_examples': 43793, 'test/accuracy': 0.9833973050117493, 'test/loss': 0.06206993758678436, 'test/mean_average_precision': 0.08140070449533014, 'test/num_examples': 43793, 'score': 492.63449263572693, 'total_duration': 867.4755127429962, 'accumulated_submission_time': 492.63449263572693, 'accumulated_eval_time': 374.75238394737244, 'accumulated_logging_time': 0.04772686958312988}
I0205 05:56:58.284934 139735693903616 logging_writer.py:48] [1498] accumulated_eval_time=374.752384, accumulated_logging_time=0.047727, accumulated_submission_time=492.634493, global_step=1498, preemption_count=0, score=492.634493, test/accuracy=0.983397, test/loss=0.062070, test/mean_average_precision=0.081401, test/num_examples=43793, total_duration=867.475513, train/accuracy=0.987123, train/loss=0.048900, train/mean_average_precision=0.082474, validation/accuracy=0.984373, validation/loss=0.058662, validation/mean_average_precision=0.083694, validation/num_examples=43793
I0205 05:56:59.325352 139752480773888 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.09543994069099426, loss=0.05740788206458092
I0205 05:57:31.566853 139735693903616 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.027494901791214943, loss=0.05001164972782135
I0205 05:58:03.892406 139752480773888 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.07362222671508789, loss=0.05577222630381584
I0205 05:58:35.969485 139735693903616 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.03281586617231369, loss=0.04670200124382973
I0205 05:59:08.402141 139752480773888 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.06251029670238495, loss=0.047889530658721924
I0205 05:59:41.029266 139735693903616 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.06566455215215683, loss=0.04955573379993439
I0205 06:00:12.920531 139752480773888 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.021909337490797043, loss=0.050403203815221786
I0205 06:00:44.769505 139735693903616 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.029698200523853302, loss=0.051342785358428955
I0205 06:00:58.543053 139919816816448 spec.py:321] Evaluating on the training split.
I0205 06:02:54.819676 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 06:02:57.917479 139919816816448 spec.py:349] Evaluating on the test split.
I0205 06:03:00.948497 139919816816448 submission_runner.py:408] Time since start: 1230.15s, 	Step: 2244, 	{'train/accuracy': 0.9873826503753662, 'train/loss': 0.0455460250377655, 'train/mean_average_precision': 0.12638824552826944, 'validation/accuracy': 0.9847467541694641, 'validation/loss': 0.05478588119149208, 'validation/mean_average_precision': 0.12251270423431268, 'validation/num_examples': 43793, 'test/accuracy': 0.9837561845779419, 'test/loss': 0.05803578719496727, 'test/mean_average_precision': 0.11871780262463053, 'test/num_examples': 43793, 'score': 732.8614349365234, 'total_duration': 1230.1546649932861, 'accumulated_submission_time': 732.8614349365234, 'accumulated_eval_time': 497.15764117240906, 'accumulated_logging_time': 0.0744333267211914}
I0205 06:03:00.963963 139735302588160 logging_writer.py:48] [2244] accumulated_eval_time=497.157641, accumulated_logging_time=0.074433, accumulated_submission_time=732.861435, global_step=2244, preemption_count=0, score=732.861435, test/accuracy=0.983756, test/loss=0.058036, test/mean_average_precision=0.118718, test/num_examples=43793, total_duration=1230.154665, train/accuracy=0.987383, train/loss=0.045546, train/mean_average_precision=0.126388, validation/accuracy=0.984747, validation/loss=0.054786, validation/mean_average_precision=0.122513, validation/num_examples=43793
I0205 06:03:19.246958 139739177916160 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.04927980527281761, loss=0.05211552977561951
I0205 06:03:51.202387 139735302588160 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.037477269768714905, loss=0.045552946627140045
I0205 06:04:23.328382 139739177916160 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.05871981382369995, loss=0.04850281402468681
I0205 06:04:55.014420 139735302588160 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.020068183541297913, loss=0.044243548065423965
I0205 06:05:27.066990 139739177916160 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.05027727037668228, loss=0.04523696377873421
I0205 06:05:58.846060 139735302588160 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.05602389574050903, loss=0.04766494780778885
I0205 06:06:31.361902 139739177916160 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.0450839139521122, loss=0.05197886377573013
I0205 06:07:01.033062 139919816816448 spec.py:321] Evaluating on the training split.
I0205 06:08:56.659465 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 06:08:59.766551 139919816816448 spec.py:349] Evaluating on the test split.
I0205 06:09:02.881427 139919816816448 submission_runner.py:408] Time since start: 1592.09s, 	Step: 2992, 	{'train/accuracy': 0.9876368045806885, 'train/loss': 0.04369545355439186, 'train/mean_average_precision': 0.15692297081332873, 'validation/accuracy': 0.9849249720573425, 'validation/loss': 0.053019069135189056, 'validation/mean_average_precision': 0.14394199531059154, 'validation/num_examples': 43793, 'test/accuracy': 0.9839427471160889, 'test/loss': 0.056046262383461, 'test/mean_average_precision': 0.14198751319090408, 'test/num_examples': 43793, 'score': 972.8983051776886, 'total_duration': 1592.0877315998077, 'accumulated_submission_time': 972.8983051776886, 'accumulated_eval_time': 619.0059733390808, 'accumulated_logging_time': 0.10088014602661133}
I0205 06:09:02.898226 139735693903616 logging_writer.py:48] [2992] accumulated_eval_time=619.005973, accumulated_logging_time=0.100880, accumulated_submission_time=972.898305, global_step=2992, preemption_count=0, score=972.898305, test/accuracy=0.983943, test/loss=0.056046, test/mean_average_precision=0.141988, test/num_examples=43793, total_duration=1592.087732, train/accuracy=0.987637, train/loss=0.043695, train/mean_average_precision=0.156923, validation/accuracy=0.984925, validation/loss=0.053019, validation/mean_average_precision=0.143942, validation/num_examples=43793
I0205 06:09:05.814268 139752480773888 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.023335769772529602, loss=0.046816498041152954
I0205 06:09:37.723218 139735693903616 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.023121856153011322, loss=0.044863052666187286
I0205 06:10:09.539541 139752480773888 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.04462888836860657, loss=0.04653849080204964
I0205 06:10:41.308552 139735693903616 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.022815071046352386, loss=0.044486865401268005
I0205 06:11:12.796702 139752480773888 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.024832304567098618, loss=0.04309028387069702
I0205 06:11:45.048922 139735693903616 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.03392985463142395, loss=0.04464689642190933
I0205 06:12:17.080440 139752480773888 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.058376360684633255, loss=0.04453454166650772
I0205 06:12:48.844515 139735693903616 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.015419278293848038, loss=0.044889770448207855
I0205 06:13:03.100788 139919816816448 spec.py:321] Evaluating on the training split.
I0205 06:14:58.338269 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 06:15:01.392880 139919816816448 spec.py:349] Evaluating on the test split.
I0205 06:15:04.382820 139919816816448 submission_runner.py:408] Time since start: 1953.59s, 	Step: 3746, 	{'train/accuracy': 0.9878502488136292, 'train/loss': 0.04240598902106285, 'train/mean_average_precision': 0.17374295638795922, 'validation/accuracy': 0.985138475894928, 'validation/loss': 0.05185147747397423, 'validation/mean_average_precision': 0.15851693156999125, 'validation/num_examples': 43793, 'test/accuracy': 0.9841310381889343, 'test/loss': 0.05469619855284691, 'test/mean_average_precision': 0.15341582083896163, 'test/num_examples': 43793, 'score': 1213.0681955814362, 'total_duration': 1953.5891268253326, 'accumulated_submission_time': 1213.0681955814362, 'accumulated_eval_time': 740.2879574298859, 'accumulated_logging_time': 0.1300337314605713}
I0205 06:15:04.399039 139739177916160 logging_writer.py:48] [3746] accumulated_eval_time=740.287957, accumulated_logging_time=0.130034, accumulated_submission_time=1213.068196, global_step=3746, preemption_count=0, score=1213.068196, test/accuracy=0.984131, test/loss=0.054696, test/mean_average_precision=0.153416, test/num_examples=43793, total_duration=1953.589127, train/accuracy=0.987850, train/loss=0.042406, train/mean_average_precision=0.173743, validation/accuracy=0.985138, validation/loss=0.051851, validation/mean_average_precision=0.158517, validation/num_examples=43793
I0205 06:15:21.885466 139752296675072 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.01849501207470894, loss=0.04219522699713707
I0205 06:15:53.731340 139739177916160 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.06671203672885895, loss=0.046040166169404984
I0205 06:16:26.045943 139752296675072 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.02164449729025364, loss=0.04701068997383118
I0205 06:16:58.283970 139739177916160 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.019929153844714165, loss=0.044600389897823334
I0205 06:17:30.306204 139752296675072 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.020918982103466988, loss=0.0410958006978035
I0205 06:18:02.207302 139739177916160 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.017950983718037605, loss=0.044295474886894226
I0205 06:18:34.767573 139752296675072 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.016774553805589676, loss=0.04274269565939903
I0205 06:19:04.605351 139919816816448 spec.py:321] Evaluating on the training split.
I0205 06:21:05.278638 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 06:21:08.628018 139919816816448 spec.py:349] Evaluating on the test split.
I0205 06:21:11.965939 139919816816448 submission_runner.py:408] Time since start: 2321.17s, 	Step: 4493, 	{'train/accuracy': 0.9877445101737976, 'train/loss': 0.04246379807591438, 'train/mean_average_precision': 0.2003246065057177, 'validation/accuracy': 0.985008955001831, 'validation/loss': 0.05223214626312256, 'validation/mean_average_precision': 0.16444033733320493, 'validation/num_examples': 43793, 'test/accuracy': 0.9840977787971497, 'test/loss': 0.05511019751429558, 'test/mean_average_precision': 0.16555823629103325, 'test/num_examples': 43793, 'score': 1453.243047952652, 'total_duration': 2321.1722240448, 'accumulated_submission_time': 1453.243047952652, 'accumulated_eval_time': 867.6484885215759, 'accumulated_logging_time': 0.15729570388793945}
I0205 06:21:11.983766 139735693903616 logging_writer.py:48] [4493] accumulated_eval_time=867.648489, accumulated_logging_time=0.157296, accumulated_submission_time=1453.243048, global_step=4493, preemption_count=0, score=1453.243048, test/accuracy=0.984098, test/loss=0.055110, test/mean_average_precision=0.165558, test/num_examples=43793, total_duration=2321.172224, train/accuracy=0.987745, train/loss=0.042464, train/mean_average_precision=0.200325, validation/accuracy=0.985009, validation/loss=0.052232, validation/mean_average_precision=0.164440, validation/num_examples=43793
I0205 06:21:14.589343 139752480773888 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.02045346051454544, loss=0.044842857867479324
I0205 06:21:47.170861 139735693903616 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.03228049725294113, loss=0.04784035682678223
I0205 06:22:19.427749 139752480773888 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.021949827671051025, loss=0.04055935889482498
I0205 06:22:51.278167 139735693903616 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.026293324306607246, loss=0.043701764196157455
I0205 06:23:23.153200 139752480773888 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.012517816387116909, loss=0.04140113666653633
I0205 06:23:55.148112 139735693903616 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.022682946175336838, loss=0.044639118015766144
I0205 06:24:27.107089 139752480773888 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.012252074666321278, loss=0.040674757212400436
I0205 06:24:59.309690 139735693903616 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.018896939232945442, loss=0.04412395879626274
I0205 06:25:12.153097 139919816816448 spec.py:321] Evaluating on the training split.
I0205 06:27:07.328567 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 06:27:10.707301 139919816816448 spec.py:349] Evaluating on the test split.
I0205 06:27:14.026928 139919816816448 submission_runner.py:408] Time since start: 2683.23s, 	Step: 5241, 	{'train/accuracy': 0.9884564280509949, 'train/loss': 0.03982599452137947, 'train/mean_average_precision': 0.2073086561436093, 'validation/accuracy': 0.9855472445487976, 'validation/loss': 0.04924844205379486, 'validation/mean_average_precision': 0.18444835777690646, 'validation/num_examples': 43793, 'test/accuracy': 0.9846347570419312, 'test/loss': 0.05191699415445328, 'test/mean_average_precision': 0.18355441353796362, 'test/num_examples': 43793, 'score': 1693.3792762756348, 'total_duration': 2683.23321557045, 'accumulated_submission_time': 1693.3792762756348, 'accumulated_eval_time': 989.522251367569, 'accumulated_logging_time': 0.18688249588012695}
I0205 06:27:14.045233 139735302588160 logging_writer.py:48] [5241] accumulated_eval_time=989.522251, accumulated_logging_time=0.186882, accumulated_submission_time=1693.379276, global_step=5241, preemption_count=0, score=1693.379276, test/accuracy=0.984635, test/loss=0.051917, test/mean_average_precision=0.183554, test/num_examples=43793, total_duration=2683.233216, train/accuracy=0.988456, train/loss=0.039826, train/mean_average_precision=0.207309, validation/accuracy=0.985547, validation/loss=0.049248, validation/mean_average_precision=0.184448, validation/num_examples=43793
I0205 06:27:33.767178 139739177916160 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.020300619304180145, loss=0.042014919221401215
I0205 06:28:07.413403 139735302588160 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.022790497168898582, loss=0.04898235201835632
I0205 06:28:39.799715 139739177916160 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.014338485896587372, loss=0.04436885565519333
I0205 06:29:12.387019 139735302588160 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.020271681249141693, loss=0.044904641807079315
I0205 06:29:44.662459 139739177916160 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.01791570708155632, loss=0.03772546350955963
I0205 06:30:17.387062 139735302588160 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.02918269857764244, loss=0.04310416802763939
I0205 06:30:49.919044 139739177916160 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.013659059070050716, loss=0.04117118567228317
I0205 06:31:14.067486 139919816816448 spec.py:321] Evaluating on the training split.
I0205 06:33:07.722414 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 06:33:10.811222 139919816816448 spec.py:349] Evaluating on the test split.
I0205 06:33:13.835444 139919816816448 submission_runner.py:408] Time since start: 3043.04s, 	Step: 5976, 	{'train/accuracy': 0.9887798428535461, 'train/loss': 0.03900324925780296, 'train/mean_average_precision': 0.24165443336439402, 'validation/accuracy': 0.9856743216514587, 'validation/loss': 0.048873793333768845, 'validation/mean_average_precision': 0.1980541880902195, 'validation/num_examples': 43793, 'test/accuracy': 0.9846912026405334, 'test/loss': 0.05147066339850426, 'test/mean_average_precision': 0.19535868259729494, 'test/num_examples': 43793, 'score': 1933.3688995838165, 'total_duration': 3043.0417470932007, 'accumulated_submission_time': 1933.3688995838165, 'accumulated_eval_time': 1109.290159702301, 'accumulated_logging_time': 0.21666622161865234}
I0205 06:33:13.852540 139735693903616 logging_writer.py:48] [5976] accumulated_eval_time=1109.290160, accumulated_logging_time=0.216666, accumulated_submission_time=1933.368900, global_step=5976, preemption_count=0, score=1933.368900, test/accuracy=0.984691, test/loss=0.051471, test/mean_average_precision=0.195359, test/num_examples=43793, total_duration=3043.041747, train/accuracy=0.988780, train/loss=0.039003, train/mean_average_precision=0.241654, validation/accuracy=0.985674, validation/loss=0.048874, validation/mean_average_precision=0.198054, validation/num_examples=43793
I0205 06:33:21.933543 139752296675072 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.02250787429511547, loss=0.039905060082674026
I0205 06:33:53.792812 139735693903616 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.014033731073141098, loss=0.03819801285862923
I0205 06:34:25.992961 139752296675072 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.013670044019818306, loss=0.04105081781744957
I0205 06:34:58.111457 139735693903616 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.016873443499207497, loss=0.041152216494083405
I0205 06:35:30.462328 139752296675072 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.016814393922686577, loss=0.042944539338350296
I0205 06:36:02.855159 139735693903616 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.014326274394989014, loss=0.041756559163331985
I0205 06:36:34.826129 139752296675072 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.018171709030866623, loss=0.040054578334093094
I0205 06:37:06.883922 139735693903616 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.020429261028766632, loss=0.04418521374464035
I0205 06:37:13.960212 139919816816448 spec.py:321] Evaluating on the training split.
I0205 06:39:09.864596 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 06:39:12.909545 139919816816448 spec.py:349] Evaluating on the test split.
I0205 06:39:15.899376 139919816816448 submission_runner.py:408] Time since start: 3405.11s, 	Step: 6723, 	{'train/accuracy': 0.9888468384742737, 'train/loss': 0.03804084286093712, 'train/mean_average_precision': 0.26055997380786267, 'validation/accuracy': 0.9860299229621887, 'validation/loss': 0.04745831713080406, 'validation/mean_average_precision': 0.21360613667128348, 'validation/num_examples': 43793, 'test/accuracy': 0.9851002097129822, 'test/loss': 0.05011272802948952, 'test/mean_average_precision': 0.210851843830605, 'test/num_examples': 43793, 'score': 2173.444726228714, 'total_duration': 3405.105672597885, 'accumulated_submission_time': 2173.444726228714, 'accumulated_eval_time': 1231.2292671203613, 'accumulated_logging_time': 0.24484539031982422}
I0205 06:39:15.915795 139759021496064 logging_writer.py:48] [6723] accumulated_eval_time=1231.229267, accumulated_logging_time=0.244845, accumulated_submission_time=2173.444726, global_step=6723, preemption_count=0, score=2173.444726, test/accuracy=0.985100, test/loss=0.050113, test/mean_average_precision=0.210852, test/num_examples=43793, total_duration=3405.105673, train/accuracy=0.988847, train/loss=0.038041, train/mean_average_precision=0.260560, validation/accuracy=0.986030, validation/loss=0.047458, validation/mean_average_precision=0.213606, validation/num_examples=43793
I0205 06:39:40.738506 139857154275072 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.0135900704190135, loss=0.04053172096610069
I0205 06:40:13.163265 139759021496064 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.016228793188929558, loss=0.04148031026124954
I0205 06:40:45.184750 139857154275072 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.02109895832836628, loss=0.0419270396232605
I0205 06:41:17.518787 139759021496064 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.013487282209098339, loss=0.0459374263882637
I0205 06:41:49.609244 139857154275072 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.01988193392753601, loss=0.04100922867655754
I0205 06:42:21.552693 139759021496064 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.016583172604441643, loss=0.04141346365213394
I0205 06:42:53.558537 139857154275072 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.01621117815375328, loss=0.043200694024562836
I0205 06:43:16.105028 139919816816448 spec.py:321] Evaluating on the training split.
I0205 06:45:10.795952 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 06:45:13.896101 139919816816448 spec.py:349] Evaluating on the test split.
I0205 06:45:16.918502 139919816816448 submission_runner.py:408] Time since start: 3766.12s, 	Step: 7472, 	{'train/accuracy': 0.9890567660331726, 'train/loss': 0.03719208762049675, 'train/mean_average_precision': 0.2785736894766114, 'validation/accuracy': 0.9860782027244568, 'validation/loss': 0.0469764843583107, 'validation/mean_average_precision': 0.2222152541430098, 'validation/num_examples': 43793, 'test/accuracy': 0.9851503372192383, 'test/loss': 0.049517057836055756, 'test/mean_average_precision': 0.21981127754706126, 'test/num_examples': 43793, 'score': 2413.602122068405, 'total_duration': 3766.124802827835, 'accumulated_submission_time': 2413.602122068405, 'accumulated_eval_time': 1352.0426914691925, 'accumulated_logging_time': 0.2722771167755127}
I0205 06:45:16.936241 139739177916160 logging_writer.py:48] [7472] accumulated_eval_time=1352.042691, accumulated_logging_time=0.272277, accumulated_submission_time=2413.602122, global_step=7472, preemption_count=0, score=2413.602122, test/accuracy=0.985150, test/loss=0.049517, test/mean_average_precision=0.219811, test/num_examples=43793, total_duration=3766.124803, train/accuracy=0.989057, train/loss=0.037192, train/mean_average_precision=0.278574, validation/accuracy=0.986078, validation/loss=0.046976, validation/mean_average_precision=0.222215, validation/num_examples=43793
I0205 06:45:26.488715 139752480773888 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.013511390425264835, loss=0.04286298155784607
I0205 06:45:58.774512 139739177916160 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.023283377289772034, loss=0.042902883142232895
I0205 06:46:31.344753 139752480773888 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.01785670407116413, loss=0.03956026956439018
I0205 06:47:04.098494 139739177916160 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.020438790321350098, loss=0.041695527732372284
I0205 06:47:36.537707 139752480773888 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.02380283921957016, loss=0.040367819368839264
I0205 06:48:09.202785 139739177916160 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.02058526687324047, loss=0.04275527223944664
I0205 06:48:41.258065 139752480773888 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.02042694389820099, loss=0.040925532579422
I0205 06:49:13.431139 139739177916160 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.02035277895629406, loss=0.03759521245956421
I0205 06:49:16.924218 139919816816448 spec.py:321] Evaluating on the training split.
I0205 06:51:16.696095 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 06:51:19.759432 139919816816448 spec.py:349] Evaluating on the test split.
I0205 06:51:22.790683 139919816816448 submission_runner.py:408] Time since start: 4132.00s, 	Step: 8212, 	{'train/accuracy': 0.9891743659973145, 'train/loss': 0.036153119057416916, 'train/mean_average_precision': 0.3071437430999844, 'validation/accuracy': 0.9860546588897705, 'validation/loss': 0.04705780744552612, 'validation/mean_average_precision': 0.2279129954488742, 'validation/num_examples': 43793, 'test/accuracy': 0.9851882457733154, 'test/loss': 0.049859512597322464, 'test/mean_average_precision': 0.2298814156917866, 'test/num_examples': 43793, 'score': 2653.557772874832, 'total_duration': 4131.996986627579, 'accumulated_submission_time': 2653.557772874832, 'accumulated_eval_time': 1477.9091057777405, 'accumulated_logging_time': 0.30215907096862793}
I0205 06:51:22.807573 139752296675072 logging_writer.py:48] [8212] accumulated_eval_time=1477.909106, accumulated_logging_time=0.302159, accumulated_submission_time=2653.557773, global_step=8212, preemption_count=0, score=2653.557773, test/accuracy=0.985188, test/loss=0.049860, test/mean_average_precision=0.229881, test/num_examples=43793, total_duration=4131.996987, train/accuracy=0.989174, train/loss=0.036153, train/mean_average_precision=0.307144, validation/accuracy=0.986055, validation/loss=0.047058, validation/mean_average_precision=0.227913, validation/num_examples=43793
I0205 06:51:51.238992 139857154275072 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.012609687633812428, loss=0.04062105342745781
I0205 06:52:23.778298 139752296675072 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.01416779775172472, loss=0.041110847145318985
I0205 06:52:55.869285 139857154275072 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.01566845364868641, loss=0.0395779125392437
I0205 06:53:28.032191 139752296675072 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.01863790675997734, loss=0.03934312239289284
I0205 06:53:59.812541 139857154275072 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.0157272070646286, loss=0.03963267058134079
I0205 06:54:31.481631 139752296675072 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.01571076549589634, loss=0.04290251061320305
I0205 06:55:03.782444 139857154275072 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.015257523395121098, loss=0.041581761091947556
I0205 06:55:22.992845 139919816816448 spec.py:321] Evaluating on the training split.
I0205 06:57:20.344496 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 06:57:23.366146 139919816816448 spec.py:349] Evaluating on the test split.
I0205 06:57:26.356906 139919816816448 submission_runner.py:408] Time since start: 4495.56s, 	Step: 8961, 	{'train/accuracy': 0.9895905256271362, 'train/loss': 0.03512311726808548, 'train/mean_average_precision': 0.3273462661718691, 'validation/accuracy': 0.9861902594566345, 'validation/loss': 0.04639050364494324, 'validation/mean_average_precision': 0.23257693387459144, 'validation/num_examples': 43793, 'test/accuracy': 0.9853874444961548, 'test/loss': 0.04906167834997177, 'test/mean_average_precision': 0.23298034352224614, 'test/num_examples': 43793, 'score': 2893.710347890854, 'total_duration': 4495.563076972961, 'accumulated_submission_time': 2893.710347890854, 'accumulated_eval_time': 1601.2729868888855, 'accumulated_logging_time': 0.3315138816833496}
I0205 06:57:26.373469 139739177916160 logging_writer.py:48] [8961] accumulated_eval_time=1601.272987, accumulated_logging_time=0.331514, accumulated_submission_time=2893.710348, global_step=8961, preemption_count=0, score=2893.710348, test/accuracy=0.985387, test/loss=0.049062, test/mean_average_precision=0.232980, test/num_examples=43793, total_duration=4495.563077, train/accuracy=0.989591, train/loss=0.035123, train/mean_average_precision=0.327346, validation/accuracy=0.986190, validation/loss=0.046391, validation/mean_average_precision=0.232577, validation/num_examples=43793
I0205 06:57:39.190288 139752480773888 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.01645800843834877, loss=0.04335745796561241
I0205 06:58:10.684378 139739177916160 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.016379768028855324, loss=0.04148028790950775
I0205 06:58:42.192736 139752480773888 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.013542287983000278, loss=0.0403427928686142
I0205 06:59:14.029246 139739177916160 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.01839383877813816, loss=0.04147754982113838
I0205 06:59:45.485516 139752480773888 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.01619487628340721, loss=0.04021161422133446
I0205 07:00:17.334227 139739177916160 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.012470876798033714, loss=0.038774967193603516
I0205 07:00:48.888290 139752480773888 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.01729627512395382, loss=0.03521619364619255
I0205 07:01:21.135703 139739177916160 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.017144734039902687, loss=0.040592487901449203
I0205 07:01:26.537666 139919816816448 spec.py:321] Evaluating on the training split.
I0205 07:03:20.909569 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 07:03:23.959704 139919816816448 spec.py:349] Evaluating on the test split.
I0205 07:03:26.907027 139919816816448 submission_runner.py:408] Time since start: 4856.11s, 	Step: 9718, 	{'train/accuracy': 0.9898381233215332, 'train/loss': 0.03419634699821472, 'train/mean_average_precision': 0.3502433378063998, 'validation/accuracy': 0.9864122867584229, 'validation/loss': 0.0458182692527771, 'validation/mean_average_precision': 0.24198184268605932, 'validation/num_examples': 43793, 'test/accuracy': 0.985545814037323, 'test/loss': 0.04856901615858078, 'test/mean_average_precision': 0.23707760709095407, 'test/num_examples': 43793, 'score': 3133.842006921768, 'total_duration': 4856.113333940506, 'accumulated_submission_time': 3133.842006921768, 'accumulated_eval_time': 1721.642301082611, 'accumulated_logging_time': 0.36038923263549805}
I0205 07:03:26.924263 139752296675072 logging_writer.py:48] [9718] accumulated_eval_time=1721.642301, accumulated_logging_time=0.360389, accumulated_submission_time=3133.842007, global_step=9718, preemption_count=0, score=3133.842007, test/accuracy=0.985546, test/loss=0.048569, test/mean_average_precision=0.237078, test/num_examples=43793, total_duration=4856.113334, train/accuracy=0.989838, train/loss=0.034196, train/mean_average_precision=0.350243, validation/accuracy=0.986412, validation/loss=0.045818, validation/mean_average_precision=0.241982, validation/num_examples=43793
I0205 07:03:53.727614 139759021496064 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.018801143392920494, loss=0.04198088124394417
I0205 07:04:25.749117 139752296675072 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.018857372924685478, loss=0.042757533490657806
I0205 07:04:57.571944 139759021496064 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.016186196357011795, loss=0.036521486937999725
I0205 07:05:29.739883 139752296675072 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.019213110208511353, loss=0.03927380219101906
I0205 07:06:01.659147 139759021496064 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.02495775744318962, loss=0.038829606026411057
I0205 07:06:33.956618 139752296675072 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.019052082672715187, loss=0.042603809386491776
I0205 07:07:06.356239 139759021496064 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.022688759490847588, loss=0.04028860107064247
I0205 07:07:27.077454 139919816816448 spec.py:321] Evaluating on the training split.
I0205 07:09:24.034056 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 07:09:27.410552 139919816816448 spec.py:349] Evaluating on the test split.
I0205 07:09:30.771693 139919816816448 submission_runner.py:408] Time since start: 5219.98s, 	Step: 10465, 	{'train/accuracy': 0.989568829536438, 'train/loss': 0.034070976078510284, 'train/mean_average_precision': 0.3687249605372311, 'validation/accuracy': 0.9862357378005981, 'validation/loss': 0.04665293172001839, 'validation/mean_average_precision': 0.23713135775538807, 'validation/num_examples': 43793, 'test/accuracy': 0.9853895902633667, 'test/loss': 0.04941052943468094, 'test/mean_average_precision': 0.23950832057838056, 'test/num_examples': 43793, 'score': 3373.964447259903, 'total_duration': 5219.9779760837555, 'accumulated_submission_time': 3373.964447259903, 'accumulated_eval_time': 1845.3364737033844, 'accumulated_logging_time': 0.388399600982666}
I0205 07:09:30.791584 139752480773888 logging_writer.py:48] [10465] accumulated_eval_time=1845.336474, accumulated_logging_time=0.388400, accumulated_submission_time=3373.964447, global_step=10465, preemption_count=0, score=3373.964447, test/accuracy=0.985390, test/loss=0.049411, test/mean_average_precision=0.239508, test/num_examples=43793, total_duration=5219.977976, train/accuracy=0.989569, train/loss=0.034071, train/mean_average_precision=0.368725, validation/accuracy=0.986236, validation/loss=0.046653, validation/mean_average_precision=0.237131, validation/num_examples=43793
I0205 07:09:42.408684 139857154275072 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.021133646368980408, loss=0.04280686005949974
I0205 07:10:14.984713 139752480773888 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.01597648486495018, loss=0.03741331398487091
I0205 07:10:47.213287 139857154275072 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.024421146139502525, loss=0.04422277957201004
I0205 07:11:19.536028 139752480773888 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.01630214788019657, loss=0.03732910752296448
I0205 07:11:52.261784 139857154275072 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.016947008669376373, loss=0.04074272885918617
I0205 07:12:24.450623 139752480773888 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.015148723497986794, loss=0.04195118695497513
I0205 07:12:56.401333 139857154275072 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.01647096313536167, loss=0.04003153741359711
I0205 07:13:28.419613 139752480773888 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.013566752895712852, loss=0.039367351680994034
I0205 07:13:30.958332 139919816816448 spec.py:321] Evaluating on the training split.
I0205 07:15:28.945118 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 07:15:31.971733 139919816816448 spec.py:349] Evaluating on the test split.
I0205 07:15:34.954637 139919816816448 submission_runner.py:408] Time since start: 5584.16s, 	Step: 11209, 	{'train/accuracy': 0.9899488091468811, 'train/loss': 0.03359292447566986, 'train/mean_average_precision': 0.3595911836357722, 'validation/accuracy': 0.9864537119865417, 'validation/loss': 0.045604538172483444, 'validation/mean_average_precision': 0.2528149574337986, 'validation/num_examples': 43793, 'test/accuracy': 0.9855285286903381, 'test/loss': 0.04852926731109619, 'test/mean_average_precision': 0.24168335414789271, 'test/num_examples': 43793, 'score': 3614.0954418182373, 'total_duration': 5584.160804271698, 'accumulated_submission_time': 3614.0954418182373, 'accumulated_eval_time': 1969.3325974941254, 'accumulated_logging_time': 0.42067766189575195}
I0205 07:15:34.971573 139739177916160 logging_writer.py:48] [11209] accumulated_eval_time=1969.332597, accumulated_logging_time=0.420678, accumulated_submission_time=3614.095442, global_step=11209, preemption_count=0, score=3614.095442, test/accuracy=0.985529, test/loss=0.048529, test/mean_average_precision=0.241683, test/num_examples=43793, total_duration=5584.160804, train/accuracy=0.989949, train/loss=0.033593, train/mean_average_precision=0.359591, validation/accuracy=0.986454, validation/loss=0.045605, validation/mean_average_precision=0.252815, validation/num_examples=43793
I0205 07:16:04.241592 139759021496064 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.017929499968886375, loss=0.03840797394514084
I0205 07:16:36.419129 139739177916160 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.015212050639092922, loss=0.036437541246414185
I0205 07:17:08.514649 139759021496064 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.018379144370555878, loss=0.03865021467208862
I0205 07:17:40.424702 139739177916160 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.014567685313522816, loss=0.03908964991569519
I0205 07:18:12.790902 139759021496064 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.025370674207806587, loss=0.03931097686290741
I0205 07:18:45.326639 139739177916160 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.013400150462985039, loss=0.03673964738845825
I0205 07:19:17.620573 139759021496064 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.017354827374219894, loss=0.04068273678421974
I0205 07:19:35.243113 139919816816448 spec.py:321] Evaluating on the training split.
I0205 07:21:29.225656 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 07:21:32.321968 139919816816448 spec.py:349] Evaluating on the test split.
I0205 07:21:35.420936 139919816816448 submission_runner.py:408] Time since start: 5944.63s, 	Step: 11956, 	{'train/accuracy': 0.9904040098190308, 'train/loss': 0.03253566101193428, 'train/mean_average_precision': 0.3633715206238281, 'validation/accuracy': 0.9865913391113281, 'validation/loss': 0.04513591527938843, 'validation/mean_average_precision': 0.2521590708519562, 'validation/num_examples': 43793, 'test/accuracy': 0.985745906829834, 'test/loss': 0.047842685133218765, 'test/mean_average_precision': 0.2463089958443751, 'test/num_examples': 43793, 'score': 3854.3355824947357, 'total_duration': 5944.627243518829, 'accumulated_submission_time': 3854.3355824947357, 'accumulated_eval_time': 2089.5103764533997, 'accumulated_logging_time': 0.44842958450317383}
I0205 07:21:35.439740 139735693903616 logging_writer.py:48] [11956] accumulated_eval_time=2089.510376, accumulated_logging_time=0.448430, accumulated_submission_time=3854.335582, global_step=11956, preemption_count=0, score=3854.335582, test/accuracy=0.985746, test/loss=0.047843, test/mean_average_precision=0.246309, test/num_examples=43793, total_duration=5944.627244, train/accuracy=0.990404, train/loss=0.032536, train/mean_average_precision=0.363372, validation/accuracy=0.986591, validation/loss=0.045136, validation/mean_average_precision=0.252159, validation/num_examples=43793
I0205 07:21:50.179712 139752480773888 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.017208199948072433, loss=0.0397714264690876
I0205 07:22:22.295176 139735693903616 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.015199968591332436, loss=0.040277980268001556
I0205 07:22:54.204603 139752480773888 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.022197917103767395, loss=0.03827015683054924
I0205 07:23:26.336637 139735693903616 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.015178835950791836, loss=0.0356128104031086
I0205 07:23:58.306853 139752480773888 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.0178303811699152, loss=0.03598594292998314
I0205 07:24:30.251416 139735693903616 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.015524568036198616, loss=0.03672163933515549
I0205 07:25:02.004353 139752480773888 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.017727626487612724, loss=0.04085979238152504
I0205 07:25:33.770965 139735693903616 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.01803395338356495, loss=0.03758464381098747
I0205 07:25:35.676299 139919816816448 spec.py:321] Evaluating on the training split.
I0205 07:27:29.201458 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 07:27:32.237593 139919816816448 spec.py:349] Evaluating on the test split.
I0205 07:27:35.250518 139919816816448 submission_runner.py:408] Time since start: 6304.46s, 	Step: 12707, 	{'train/accuracy': 0.9905290603637695, 'train/loss': 0.031725697219371796, 'train/mean_average_precision': 0.3934554074221962, 'validation/accuracy': 0.9866530299186707, 'validation/loss': 0.04535263031721115, 'validation/mean_average_precision': 0.25775737687452344, 'validation/num_examples': 43793, 'test/accuracy': 0.9858217239379883, 'test/loss': 0.04809940606355667, 'test/mean_average_precision': 0.2571150736686474, 'test/num_examples': 43793, 'score': 4094.5407037734985, 'total_duration': 6304.456825494766, 'accumulated_submission_time': 4094.5407037734985, 'accumulated_eval_time': 2209.0845663547516, 'accumulated_logging_time': 0.4778716564178467}
I0205 07:27:35.268257 139752296675072 logging_writer.py:48] [12707] accumulated_eval_time=2209.084566, accumulated_logging_time=0.477872, accumulated_submission_time=4094.540704, global_step=12707, preemption_count=0, score=4094.540704, test/accuracy=0.985822, test/loss=0.048099, test/mean_average_precision=0.257115, test/num_examples=43793, total_duration=6304.456825, train/accuracy=0.990529, train/loss=0.031726, train/mean_average_precision=0.393455, validation/accuracy=0.986653, validation/loss=0.045353, validation/mean_average_precision=0.257757, validation/num_examples=43793
I0205 07:28:05.632749 139759021496064 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.017784126102924347, loss=0.038603827357292175
I0205 07:28:37.903626 139752296675072 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.017978360876441002, loss=0.03557872027158737
I0205 07:29:10.100749 139759021496064 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.01688702031970024, loss=0.0365094393491745
I0205 07:29:41.930571 139752296675072 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.023657720535993576, loss=0.04061887785792351
I0205 07:30:14.035432 139759021496064 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.02229769341647625, loss=0.03712715208530426
I0205 07:30:46.042582 139752296675072 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.0158100463449955, loss=0.0365309864282608
I0205 07:31:18.170561 139759021496064 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.02318873256444931, loss=0.040895428508520126
I0205 07:31:35.582671 139919816816448 spec.py:321] Evaluating on the training split.
I0205 07:33:29.894280 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 07:33:33.025979 139919816816448 spec.py:349] Evaluating on the test split.
I0205 07:33:36.037210 139919816816448 submission_runner.py:408] Time since start: 6665.24s, 	Step: 13455, 	{'train/accuracy': 0.9906842708587646, 'train/loss': 0.03138013184070587, 'train/mean_average_precision': 0.39294846410701095, 'validation/accuracy': 0.9865840077400208, 'validation/loss': 0.04529331624507904, 'validation/mean_average_precision': 0.253105285712875, 'validation/num_examples': 43793, 'test/accuracy': 0.9856991171836853, 'test/loss': 0.04808598756790161, 'test/mean_average_precision': 0.24591885708072264, 'test/num_examples': 43793, 'score': 4334.822235822678, 'total_duration': 6665.2435131073, 'accumulated_submission_time': 4334.822235822678, 'accumulated_eval_time': 2329.5390541553497, 'accumulated_logging_time': 0.5078516006469727}
I0205 07:33:36.055682 139739177916160 logging_writer.py:48] [13455] accumulated_eval_time=2329.539054, accumulated_logging_time=0.507852, accumulated_submission_time=4334.822236, global_step=13455, preemption_count=0, score=4334.822236, test/accuracy=0.985699, test/loss=0.048086, test/mean_average_precision=0.245919, test/num_examples=43793, total_duration=6665.243513, train/accuracy=0.990684, train/loss=0.031380, train/mean_average_precision=0.392948, validation/accuracy=0.986584, validation/loss=0.045293, validation/mean_average_precision=0.253105, validation/num_examples=43793
I0205 07:33:50.721004 139752480773888 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.017959361895918846, loss=0.0338139645755291
I0205 07:34:23.298510 139739177916160 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.019081108272075653, loss=0.036501508206129074
I0205 07:34:55.187794 139752480773888 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.02118550054728985, loss=0.03956286236643791
I0205 07:35:27.494986 139739177916160 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.01758679188787937, loss=0.039498016238212585
I0205 07:35:59.799709 139752480773888 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.01915256306529045, loss=0.036650508642196655
I0205 07:36:31.689893 139739177916160 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.018765490502119064, loss=0.03676871582865715
I0205 07:37:03.757074 139752480773888 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.020829565823078156, loss=0.036592837423086166
I0205 07:37:35.904300 139739177916160 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.02120901644229889, loss=0.035200413316488266
I0205 07:37:36.227846 139919816816448 spec.py:321] Evaluating on the training split.
I0205 07:39:30.105932 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 07:39:33.273072 139919816816448 spec.py:349] Evaluating on the test split.
I0205 07:39:36.301327 139919816816448 submission_runner.py:408] Time since start: 7025.51s, 	Step: 14202, 	{'train/accuracy': 0.9907562732696533, 'train/loss': 0.03044726885855198, 'train/mean_average_precision': 0.42871386983088106, 'validation/accuracy': 0.9867236614227295, 'validation/loss': 0.04521000757813454, 'validation/mean_average_precision': 0.2582183376390017, 'validation/num_examples': 43793, 'test/accuracy': 0.9858486652374268, 'test/loss': 0.048013463616371155, 'test/mean_average_precision': 0.2553494129192967, 'test/num_examples': 43793, 'score': 4574.961861371994, 'total_duration': 7025.507632255554, 'accumulated_submission_time': 4574.961861371994, 'accumulated_eval_time': 2449.6124868392944, 'accumulated_logging_time': 0.5385310649871826}
I0205 07:39:36.319593 139735693903616 logging_writer.py:48] [14202] accumulated_eval_time=2449.612487, accumulated_logging_time=0.538531, accumulated_submission_time=4574.961861, global_step=14202, preemption_count=0, score=4574.961861, test/accuracy=0.985849, test/loss=0.048013, test/mean_average_precision=0.255349, test/num_examples=43793, total_duration=7025.507632, train/accuracy=0.990756, train/loss=0.030447, train/mean_average_precision=0.428714, validation/accuracy=0.986724, validation/loss=0.045210, validation/mean_average_precision=0.258218, validation/num_examples=43793
I0205 07:40:08.328237 139752296675072 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.017949827015399933, loss=0.0367114320397377
I0205 07:40:40.367973 139735693903616 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.022536050528287888, loss=0.03789680823683739
I0205 07:41:12.647459 139752296675072 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.019664155319333076, loss=0.03426157310605049
I0205 07:41:44.572223 139735693903616 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.022710487246513367, loss=0.0365159772336483
I0205 07:42:16.913798 139752296675072 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.018364906311035156, loss=0.03595895692706108
I0205 07:42:48.988856 139735693903616 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.03127434477210045, loss=0.03915197029709816
I0205 07:43:21.384630 139752296675072 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.023107176646590233, loss=0.04024801030755043
I0205 07:43:36.510826 139919816816448 spec.py:321] Evaluating on the training split.
I0205 07:45:32.917915 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 07:45:35.968655 139919816816448 spec.py:349] Evaluating on the test split.
I0205 07:45:38.955538 139919816816448 submission_runner.py:408] Time since start: 7388.16s, 	Step: 14948, 	{'train/accuracy': 0.99077308177948, 'train/loss': 0.03025943599641323, 'train/mean_average_precision': 0.4277688119308474, 'validation/accuracy': 0.9866668581962585, 'validation/loss': 0.04541216418147087, 'validation/mean_average_precision': 0.2544344404267181, 'validation/num_examples': 43793, 'test/accuracy': 0.9857577085494995, 'test/loss': 0.04828036203980446, 'test/mean_average_precision': 0.24394367745182258, 'test/num_examples': 43793, 'score': 4815.122054815292, 'total_duration': 7388.161834478378, 'accumulated_submission_time': 4815.122054815292, 'accumulated_eval_time': 2572.057140827179, 'accumulated_logging_time': 0.5675864219665527}
I0205 07:45:38.974250 139739177916160 logging_writer.py:48] [14948] accumulated_eval_time=2572.057141, accumulated_logging_time=0.567586, accumulated_submission_time=4815.122055, global_step=14948, preemption_count=0, score=4815.122055, test/accuracy=0.985758, test/loss=0.048280, test/mean_average_precision=0.243944, test/num_examples=43793, total_duration=7388.161834, train/accuracy=0.990773, train/loss=0.030259, train/mean_average_precision=0.427769, validation/accuracy=0.986667, validation/loss=0.045412, validation/mean_average_precision=0.254434, validation/num_examples=43793
I0205 07:45:55.977418 139752480773888 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.01887303777039051, loss=0.037292975932359695
I0205 07:46:28.006001 139739177916160 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.018095901235938072, loss=0.0366109237074852
I0205 07:46:59.984402 139752480773888 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.0178199615329504, loss=0.03661464527249336
I0205 07:47:32.242087 139739177916160 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.018941054120659828, loss=0.037947945296764374
I0205 07:48:03.934012 139752480773888 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.020427847281098366, loss=0.0377395860850811
I0205 07:48:35.817971 139739177916160 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.02063758485019207, loss=0.03687155246734619
I0205 07:49:08.265836 139752480773888 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.021823987364768982, loss=0.038709502667188644
I0205 07:49:39.022645 139919816816448 spec.py:321] Evaluating on the training split.
I0205 07:51:34.893522 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 07:51:37.915286 139919816816448 spec.py:349] Evaluating on the test split.
I0205 07:51:40.946267 139919816816448 submission_runner.py:408] Time since start: 7750.15s, 	Step: 15697, 	{'train/accuracy': 0.9912256598472595, 'train/loss': 0.028870854526758194, 'train/mean_average_precision': 0.45138882987386597, 'validation/accuracy': 0.9866424798965454, 'validation/loss': 0.04532580450177193, 'validation/mean_average_precision': 0.2573368540704658, 'validation/num_examples': 43793, 'test/accuracy': 0.9857909679412842, 'test/loss': 0.04812214896082878, 'test/mean_average_precision': 0.24653675549932982, 'test/num_examples': 43793, 'score': 5055.139110803604, 'total_duration': 7750.152556419373, 'accumulated_submission_time': 5055.139110803604, 'accumulated_eval_time': 2693.980701684952, 'accumulated_logging_time': 0.5970251560211182}
I0205 07:51:40.964706 139752296675072 logging_writer.py:48] [15697] accumulated_eval_time=2693.980702, accumulated_logging_time=0.597025, accumulated_submission_time=5055.139111, global_step=15697, preemption_count=0, score=5055.139111, test/accuracy=0.985791, test/loss=0.048122, test/mean_average_precision=0.246537, test/num_examples=43793, total_duration=7750.152556, train/accuracy=0.991226, train/loss=0.028871, train/mean_average_precision=0.451389, validation/accuracy=0.986642, validation/loss=0.045326, validation/mean_average_precision=0.257337, validation/num_examples=43793
I0205 07:51:42.273212 139759021496064 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.022134929895401, loss=0.03529694303870201
I0205 07:52:14.244807 139752296675072 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.017327839508652687, loss=0.034465618431568146
I0205 07:52:46.330578 139759021496064 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.02000802755355835, loss=0.03656382858753204
I0205 07:53:18.401913 139752296675072 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.02553575485944748, loss=0.0371258445084095
I0205 07:53:50.413121 139759021496064 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.02065003663301468, loss=0.036147791892290115
I0205 07:54:22.478677 139752296675072 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.021110299974679947, loss=0.036051854491233826
I0205 07:54:54.179866 139759021496064 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.020504968240857124, loss=0.03337755799293518
I0205 07:55:26.236014 139752296675072 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.019707662984728813, loss=0.03568573668599129
I0205 07:55:40.971529 139919816816448 spec.py:321] Evaluating on the training split.
I0205 07:57:34.099573 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 07:57:37.229756 139919816816448 spec.py:349] Evaluating on the test split.
I0205 07:57:40.255045 139919816816448 submission_runner.py:408] Time since start: 8109.46s, 	Step: 16447, 	{'train/accuracy': 0.9916675090789795, 'train/loss': 0.027591295540332794, 'train/mean_average_precision': 0.49192983157015663, 'validation/accuracy': 0.9866794347763062, 'validation/loss': 0.04554938152432442, 'validation/mean_average_precision': 0.2621947058241379, 'validation/num_examples': 43793, 'test/accuracy': 0.9858705401420593, 'test/loss': 0.04825540632009506, 'test/mean_average_precision': 0.2530989906320853, 'test/num_examples': 43793, 'score': 5295.112805843353, 'total_duration': 8109.461350440979, 'accumulated_submission_time': 5295.112805843353, 'accumulated_eval_time': 2813.2641813755035, 'accumulated_logging_time': 0.6277570724487305}
I0205 07:57:40.273801 139735693903616 logging_writer.py:48] [16447] accumulated_eval_time=2813.264181, accumulated_logging_time=0.627757, accumulated_submission_time=5295.112806, global_step=16447, preemption_count=0, score=5295.112806, test/accuracy=0.985871, test/loss=0.048255, test/mean_average_precision=0.253099, test/num_examples=43793, total_duration=8109.461350, train/accuracy=0.991668, train/loss=0.027591, train/mean_average_precision=0.491930, validation/accuracy=0.986679, validation/loss=0.045549, validation/mean_average_precision=0.262195, validation/num_examples=43793
I0205 07:57:57.848333 139739177916160 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.021225763484835625, loss=0.034054238349199295
I0205 07:58:30.242397 139735693903616 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.02049543708562851, loss=0.03573000431060791
I0205 07:59:02.141263 139739177916160 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.021696293726563454, loss=0.037164513021707535
I0205 07:59:34.516265 139735693903616 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.02181568741798401, loss=0.03682809695601463
I0205 08:00:07.281738 139739177916160 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.02113126963376999, loss=0.03657431900501251
I0205 08:00:39.388156 139735693903616 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.019610926508903503, loss=0.03716230392456055
I0205 08:01:11.525027 139739177916160 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.024384239688515663, loss=0.036129482090473175
I0205 08:01:40.507405 139919816816448 spec.py:321] Evaluating on the training split.
I0205 08:03:35.808474 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 08:03:38.869337 139919816816448 spec.py:349] Evaluating on the test split.
I0205 08:03:41.982542 139919816816448 submission_runner.py:408] Time since start: 8471.19s, 	Step: 17191, 	{'train/accuracy': 0.9916927814483643, 'train/loss': 0.0272750835865736, 'train/mean_average_precision': 0.5031697196652744, 'validation/accuracy': 0.986777663230896, 'validation/loss': 0.04524582624435425, 'validation/mean_average_precision': 0.26525978943555706, 'validation/num_examples': 43793, 'test/accuracy': 0.9858924746513367, 'test/loss': 0.04820572957396507, 'test/mean_average_precision': 0.2521990759737141, 'test/num_examples': 43793, 'score': 5535.315126657486, 'total_duration': 8471.188846826553, 'accumulated_submission_time': 5535.315126657486, 'accumulated_eval_time': 2934.739273548126, 'accumulated_logging_time': 0.6577551364898682}
I0205 08:03:42.001084 139752480773888 logging_writer.py:48] [17191] accumulated_eval_time=2934.739274, accumulated_logging_time=0.657755, accumulated_submission_time=5535.315127, global_step=17191, preemption_count=0, score=5535.315127, test/accuracy=0.985892, test/loss=0.048206, test/mean_average_precision=0.252199, test/num_examples=43793, total_duration=8471.188847, train/accuracy=0.991693, train/loss=0.027275, train/mean_average_precision=0.503170, validation/accuracy=0.986778, validation/loss=0.045246, validation/mean_average_precision=0.265260, validation/num_examples=43793
I0205 08:03:45.165395 139759021496064 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.021680932492017746, loss=0.03593656048178673
I0205 08:04:17.115920 139752480773888 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.021483534947037697, loss=0.033582914620637894
I0205 08:04:48.898503 139759021496064 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.02198079600930214, loss=0.0366034097969532
I0205 08:05:20.745320 139752480773888 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.020737668499350548, loss=0.037886932492256165
I0205 08:05:52.551654 139759021496064 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.02224404737353325, loss=0.0350547656416893
I0205 08:06:24.168521 139752480773888 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.01997731439769268, loss=0.03586480766534805
I0205 08:06:55.780341 139759021496064 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.02703046426177025, loss=0.03772478550672531
I0205 08:07:28.159174 139752480773888 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.02625294029712677, loss=0.03797899931669235
I0205 08:07:42.126633 139919816816448 spec.py:321] Evaluating on the training split.
I0205 08:09:37.831927 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 08:09:40.847286 139919816816448 spec.py:349] Evaluating on the test split.
I0205 08:09:43.843775 139919816816448 submission_runner.py:408] Time since start: 8833.05s, 	Step: 17944, 	{'train/accuracy': 0.9915154576301575, 'train/loss': 0.027695178985595703, 'train/mean_average_precision': 0.5012935847392078, 'validation/accuracy': 0.9867122769355774, 'validation/loss': 0.04593149200081825, 'validation/mean_average_precision': 0.2675033684853384, 'validation/num_examples': 43793, 'test/accuracy': 0.9857863187789917, 'test/loss': 0.04909912124276161, 'test/mean_average_precision': 0.24857881791609326, 'test/num_examples': 43793, 'score': 5775.409202814102, 'total_duration': 8833.050078868866, 'accumulated_submission_time': 5775.409202814102, 'accumulated_eval_time': 3056.4563794136047, 'accumulated_logging_time': 0.6870527267456055}
I0205 08:09:43.863532 139735693903616 logging_writer.py:48] [17944] accumulated_eval_time=3056.456379, accumulated_logging_time=0.687053, accumulated_submission_time=5775.409203, global_step=17944, preemption_count=0, score=5775.409203, test/accuracy=0.985786, test/loss=0.049099, test/mean_average_precision=0.248579, test/num_examples=43793, total_duration=8833.050079, train/accuracy=0.991515, train/loss=0.027695, train/mean_average_precision=0.501294, validation/accuracy=0.986712, validation/loss=0.045931, validation/mean_average_precision=0.267503, validation/num_examples=43793
I0205 08:10:02.183375 139739177916160 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.03356974199414253, loss=0.036414384841918945
I0205 08:10:34.123389 139735693903616 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.024429231882095337, loss=0.03643636405467987
I0205 08:11:06.116389 139739177916160 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.023744823411107063, loss=0.03592598810791969
I0205 08:11:38.229317 139735693903616 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.03540019318461418, loss=0.03497456759214401
I0205 08:12:11.136497 139739177916160 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.02491043135523796, loss=0.03610128536820412
I0205 08:12:42.726161 139735693903616 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.02604309841990471, loss=0.034752000123262405
I0205 08:13:15.129069 139739177916160 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.02241981029510498, loss=0.036252520978450775
I0205 08:13:44.030729 139919816816448 spec.py:321] Evaluating on the training split.
I0205 08:15:35.634243 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 08:15:38.695798 139919816816448 spec.py:349] Evaluating on the test split.
I0205 08:15:41.724515 139919816816448 submission_runner.py:408] Time since start: 9190.93s, 	Step: 18691, 	{'train/accuracy': 0.9915038347244263, 'train/loss': 0.027914177626371384, 'train/mean_average_precision': 0.4830483010954101, 'validation/accuracy': 0.9867801070213318, 'validation/loss': 0.045402124524116516, 'validation/mean_average_precision': 0.2609649209664075, 'validation/num_examples': 43793, 'test/accuracy': 0.9859122633934021, 'test/loss': 0.048295315355062485, 'test/mean_average_precision': 0.25655242134910017, 'test/num_examples': 43793, 'score': 6015.5455322265625, 'total_duration': 9190.930800199509, 'accumulated_submission_time': 6015.5455322265625, 'accumulated_eval_time': 3174.150098800659, 'accumulated_logging_time': 0.7177164554595947}
I0205 08:15:41.743596 139752480773888 logging_writer.py:48] [18691] accumulated_eval_time=3174.150099, accumulated_logging_time=0.717716, accumulated_submission_time=6015.545532, global_step=18691, preemption_count=0, score=6015.545532, test/accuracy=0.985912, test/loss=0.048295, test/mean_average_precision=0.256552, test/num_examples=43793, total_duration=9190.930800, train/accuracy=0.991504, train/loss=0.027914, train/mean_average_precision=0.483048, validation/accuracy=0.986780, validation/loss=0.045402, validation/mean_average_precision=0.260965, validation/num_examples=43793
I0205 08:15:44.968725 139759021496064 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.02197451889514923, loss=0.031759459525346756
I0205 08:16:17.094028 139752480773888 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.026578478515148163, loss=0.03403238207101822
I0205 08:16:49.102268 139759021496064 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.02523406222462654, loss=0.035474520176649094
I0205 08:17:21.455741 139752480773888 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.02474811114370823, loss=0.03608154505491257
I0205 08:17:53.492314 139759021496064 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.029775425791740417, loss=0.034614164382219315
I0205 08:18:25.999843 139752480773888 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.025228338316082954, loss=0.03224453330039978
I0205 08:18:58.801338 139759021496064 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.030099505558609962, loss=0.03395788371562958
I0205 08:19:31.063953 139752480773888 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.02130880393087864, loss=0.03331330418586731
I0205 08:19:41.878477 139919816816448 spec.py:321] Evaluating on the training split.
I0205 08:21:36.639843 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 08:21:39.654810 139919816816448 spec.py:349] Evaluating on the test split.
I0205 08:21:42.594897 139919816816448 submission_runner.py:408] Time since start: 9551.80s, 	Step: 19435, 	{'train/accuracy': 0.9913306832313538, 'train/loss': 0.028145255520939827, 'train/mean_average_precision': 0.4829667100200081, 'validation/accuracy': 0.9866834878921509, 'validation/loss': 0.046191148459911346, 'validation/mean_average_precision': 0.260969365659252, 'validation/num_examples': 43793, 'test/accuracy': 0.9858874082565308, 'test/loss': 0.04917261376976967, 'test/mean_average_precision': 0.2569502106290384, 'test/num_examples': 43793, 'score': 6255.649400234222, 'total_duration': 9551.801201581955, 'accumulated_submission_time': 6255.649400234222, 'accumulated_eval_time': 3294.866469860077, 'accumulated_logging_time': 0.7476849555969238}
I0205 08:21:42.613674 139735693903616 logging_writer.py:48] [19435] accumulated_eval_time=3294.866470, accumulated_logging_time=0.747685, accumulated_submission_time=6255.649400, global_step=19435, preemption_count=0, score=6255.649400, test/accuracy=0.985887, test/loss=0.049173, test/mean_average_precision=0.256950, test/num_examples=43793, total_duration=9551.801202, train/accuracy=0.991331, train/loss=0.028145, train/mean_average_precision=0.482967, validation/accuracy=0.986683, validation/loss=0.046191, validation/mean_average_precision=0.260969, validation/num_examples=43793
I0205 08:22:03.798881 139752296675072 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.023814991116523743, loss=0.03281858563423157
I0205 08:22:35.372390 139735693903616 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.02767966128885746, loss=0.03648727387189865
I0205 08:23:07.294617 139752296675072 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.027886517345905304, loss=0.03484031558036804
I0205 08:23:38.798084 139735693903616 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.024619409814476967, loss=0.037113599479198456
I0205 08:24:10.525780 139752296675072 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.024778073653578758, loss=0.032573699951171875
I0205 08:24:42.188412 139735693903616 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.024403288960456848, loss=0.033903785049915314
I0205 08:25:14.517004 139752296675072 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.03270415961742401, loss=0.032845839858055115
I0205 08:25:42.828560 139919816816448 spec.py:321] Evaluating on the training split.
I0205 08:27:33.473152 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 08:27:36.562607 139919816816448 spec.py:349] Evaluating on the test split.
I0205 08:27:39.520930 139919816816448 submission_runner.py:408] Time since start: 9908.73s, 	Step: 20189, 	{'train/accuracy': 0.9917463064193726, 'train/loss': 0.026905406266450882, 'train/mean_average_precision': 0.4939934566066215, 'validation/accuracy': 0.9866786003112793, 'validation/loss': 0.0461668036878109, 'validation/mean_average_precision': 0.258319712197705, 'validation/num_examples': 43793, 'test/accuracy': 0.9858680367469788, 'test/loss': 0.04932626709342003, 'test/mean_average_precision': 0.24966029993713423, 'test/num_examples': 43793, 'score': 6495.83327627182, 'total_duration': 9908.72722864151, 'accumulated_submission_time': 6495.83327627182, 'accumulated_eval_time': 3411.558787584305, 'accumulated_logging_time': 0.7772469520568848}
I0205 08:27:39.540713 139739177916160 logging_writer.py:48] [20189] accumulated_eval_time=3411.558788, accumulated_logging_time=0.777247, accumulated_submission_time=6495.833276, global_step=20189, preemption_count=0, score=6495.833276, test/accuracy=0.985868, test/loss=0.049326, test/mean_average_precision=0.249660, test/num_examples=43793, total_duration=9908.727229, train/accuracy=0.991746, train/loss=0.026905, train/mean_average_precision=0.493993, validation/accuracy=0.986679, validation/loss=0.046167, validation/mean_average_precision=0.258320, validation/num_examples=43793
I0205 08:27:43.492973 139759021496064 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.024185676127672195, loss=0.032929763197898865
I0205 08:28:15.726760 139739177916160 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.022505125030875206, loss=0.03281835466623306
I0205 08:28:47.645242 139759021496064 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.030053775757551193, loss=0.032716378569602966
I0205 08:29:19.620833 139739177916160 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.03066880628466606, loss=0.03549697622656822
I0205 08:29:51.361486 139759021496064 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.02724873460829258, loss=0.030197778716683388
I0205 08:30:23.585675 139739177916160 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.031504906713962555, loss=0.035352617502212524
I0205 08:30:55.710818 139759021496064 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.02719920128583908, loss=0.03424934297800064
I0205 08:31:27.843974 139739177916160 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.030869944021105766, loss=0.03292061388492584
I0205 08:31:39.768058 139919816816448 spec.py:321] Evaluating on the training split.
I0205 08:33:30.711932 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 08:33:34.091156 139919816816448 spec.py:349] Evaluating on the test split.
I0205 08:33:37.357292 139919816816448 submission_runner.py:408] Time since start: 10266.56s, 	Step: 20938, 	{'train/accuracy': 0.9917852282524109, 'train/loss': 0.026819629594683647, 'train/mean_average_precision': 0.505956641813925, 'validation/accuracy': 0.9866956472396851, 'validation/loss': 0.04661537706851959, 'validation/mean_average_precision': 0.257373007364516, 'validation/num_examples': 43793, 'test/accuracy': 0.9857537150382996, 'test/loss': 0.05003098025918007, 'test/mean_average_precision': 0.24761899832550355, 'test/num_examples': 43793, 'score': 6736.029312372208, 'total_duration': 10266.563576698303, 'accumulated_submission_time': 6736.029312372208, 'accumulated_eval_time': 3529.1479530334473, 'accumulated_logging_time': 0.8081989288330078}
I0205 08:33:37.378354 139752296675072 logging_writer.py:48] [20938] accumulated_eval_time=3529.147953, accumulated_logging_time=0.808199, accumulated_submission_time=6736.029312, global_step=20938, preemption_count=0, score=6736.029312, test/accuracy=0.985754, test/loss=0.050031, test/mean_average_precision=0.247619, test/num_examples=43793, total_duration=10266.563577, train/accuracy=0.991785, train/loss=0.026820, train/mean_average_precision=0.505957, validation/accuracy=0.986696, validation/loss=0.046615, validation/mean_average_precision=0.257373, validation/num_examples=43793
I0205 08:33:57.977948 139752480773888 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.028925113379955292, loss=0.03443213179707527
I0205 08:34:30.212442 139752296675072 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.024941302835941315, loss=0.03249940648674965
I0205 08:35:02.286890 139752480773888 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.023362083360552788, loss=0.03319603204727173
I0205 08:35:33.662804 139752296675072 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.029363270848989487, loss=0.03465716913342476
I0205 08:36:05.638428 139752480773888 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.027894675731658936, loss=0.03519977256655693
I0205 08:36:37.465300 139752296675072 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.04110705852508545, loss=0.03197911009192467
I0205 08:37:09.646785 139752480773888 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.03302452340722084, loss=0.03577284514904022
I0205 08:37:37.510073 139919816816448 spec.py:321] Evaluating on the training split.
I0205 08:39:29.171604 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 08:39:32.246252 139919816816448 spec.py:349] Evaluating on the test split.
I0205 08:39:35.292042 139919816816448 submission_runner.py:408] Time since start: 10624.50s, 	Step: 21689, 	{'train/accuracy': 0.9920173287391663, 'train/loss': 0.02600455842912197, 'train/mean_average_precision': 0.523237135827197, 'validation/accuracy': 0.9867163896560669, 'validation/loss': 0.04635230824351311, 'validation/mean_average_precision': 0.2581223071410054, 'validation/num_examples': 43793, 'test/accuracy': 0.9857938885688782, 'test/loss': 0.04930374398827553, 'test/mean_average_precision': 0.2549565183568808, 'test/num_examples': 43793, 'score': 6976.128599882126, 'total_duration': 10624.498348236084, 'accumulated_submission_time': 6976.128599882126, 'accumulated_eval_time': 3646.9298775196075, 'accumulated_logging_time': 0.841012716293335}
I0205 08:39:35.311100 139739177916160 logging_writer.py:48] [21689] accumulated_eval_time=3646.929878, accumulated_logging_time=0.841013, accumulated_submission_time=6976.128600, global_step=21689, preemption_count=0, score=6976.128600, test/accuracy=0.985794, test/loss=0.049304, test/mean_average_precision=0.254957, test/num_examples=43793, total_duration=10624.498348, train/accuracy=0.992017, train/loss=0.026005, train/mean_average_precision=0.523237, validation/accuracy=0.986716, validation/loss=0.046352, validation/mean_average_precision=0.258122, validation/num_examples=43793
I0205 08:39:39.234629 139759021496064 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.02893962152302265, loss=0.032394908368587494
I0205 08:40:11.116130 139739177916160 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.02793039008975029, loss=0.03349459543824196
I0205 08:40:43.038553 139759021496064 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.029349468648433685, loss=0.03320926055312157
I0205 08:41:15.310917 139739177916160 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.03014443814754486, loss=0.03286211937665939
I0205 08:41:47.380656 139759021496064 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.02649303525686264, loss=0.0313192680478096
I0205 08:42:19.470271 139739177916160 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.03217018395662308, loss=0.03449501097202301
I0205 08:42:51.561751 139759021496064 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.03124774433672428, loss=0.03436331823468208
I0205 08:43:24.036617 139739177916160 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.0348215326666832, loss=0.03417748212814331
I0205 08:43:35.314056 139919816816448 spec.py:321] Evaluating on the training split.
I0205 08:45:27.850701 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 08:45:30.895594 139919816816448 spec.py:349] Evaluating on the test split.
I0205 08:45:33.939369 139919816816448 submission_runner.py:408] Time since start: 10983.15s, 	Step: 22436, 	{'train/accuracy': 0.9925153851509094, 'train/loss': 0.02464495599269867, 'train/mean_average_precision': 0.5653510078298822, 'validation/accuracy': 0.9865344762802124, 'validation/loss': 0.04645871743559837, 'validation/mean_average_precision': 0.2524689786088972, 'validation/num_examples': 43793, 'test/accuracy': 0.9857235550880432, 'test/loss': 0.04915507882833481, 'test/mean_average_precision': 0.24429603715936346, 'test/num_examples': 43793, 'score': 7216.098851442337, 'total_duration': 10983.145557641983, 'accumulated_submission_time': 7216.098851442337, 'accumulated_eval_time': 3765.5550322532654, 'accumulated_logging_time': 0.8723323345184326}
I0205 08:45:33.958742 139752296675072 logging_writer.py:48] [22436] accumulated_eval_time=3765.555032, accumulated_logging_time=0.872332, accumulated_submission_time=7216.098851, global_step=22436, preemption_count=0, score=7216.098851, test/accuracy=0.985724, test/loss=0.049155, test/mean_average_precision=0.244296, test/num_examples=43793, total_duration=10983.145558, train/accuracy=0.992515, train/loss=0.024645, train/mean_average_precision=0.565351, validation/accuracy=0.986534, validation/loss=0.046459, validation/mean_average_precision=0.252469, validation/num_examples=43793
I0205 08:45:54.916179 139752480773888 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.03050532378256321, loss=0.030592288821935654
I0205 08:46:26.878723 139752296675072 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.038263626396656036, loss=0.03585213050246239
I0205 08:46:58.944778 139752480773888 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.03603743389248848, loss=0.032974664121866226
I0205 08:47:30.896991 139752296675072 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.03356350213289261, loss=0.036159541457891464
I0205 08:48:02.634624 139752480773888 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.03435501456260681, loss=0.03179898485541344
I0205 08:48:35.033406 139752296675072 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.033335912972688675, loss=0.03413481265306473
I0205 08:49:08.042328 139752480773888 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.029935801401734352, loss=0.03273509815335274
I0205 08:49:34.096427 139919816816448 spec.py:321] Evaluating on the training split.
I0205 08:51:29.164021 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 08:51:32.297924 139919816816448 spec.py:349] Evaluating on the test split.
I0205 08:51:35.271373 139919816816448 submission_runner.py:408] Time since start: 11344.48s, 	Step: 23182, 	{'train/accuracy': 0.9926448464393616, 'train/loss': 0.023930178955197334, 'train/mean_average_precision': 0.5750513515882668, 'validation/accuracy': 0.9865629076957703, 'validation/loss': 0.04723859578371048, 'validation/mean_average_precision': 0.25317094876484586, 'validation/num_examples': 43793, 'test/accuracy': 0.9857202172279358, 'test/loss': 0.05016908049583435, 'test/mean_average_precision': 0.25159870420610275, 'test/num_examples': 43793, 'score': 7456.205724477768, 'total_duration': 11344.477650642395, 'accumulated_submission_time': 7456.205724477768, 'accumulated_eval_time': 3886.729905128479, 'accumulated_logging_time': 0.9026894569396973}
I0205 08:51:35.290664 139739177916160 logging_writer.py:48] [23182] accumulated_eval_time=3886.729905, accumulated_logging_time=0.902689, accumulated_submission_time=7456.205724, global_step=23182, preemption_count=0, score=7456.205724, test/accuracy=0.985720, test/loss=0.050169, test/mean_average_precision=0.251599, test/num_examples=43793, total_duration=11344.477651, train/accuracy=0.992645, train/loss=0.023930, train/mean_average_precision=0.575051, validation/accuracy=0.986563, validation/loss=0.047239, validation/mean_average_precision=0.253171, validation/num_examples=43793
I0205 08:51:41.375869 139759021496064 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.02862110361456871, loss=0.030944814905524254
I0205 08:52:13.144307 139739177916160 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.032558683305978775, loss=0.03275429829955101
I0205 08:52:44.979370 139759021496064 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.031185679137706757, loss=0.03456737473607063
I0205 08:53:18.443093 139739177916160 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.03775468468666077, loss=0.03225823864340782
I0205 08:53:50.054423 139759021496064 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.02765815518796444, loss=0.03266244754195213
I0205 08:54:21.782859 139739177916160 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.0322861485183239, loss=0.033876560628414154
I0205 08:54:53.550605 139759021496064 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.0348706878721714, loss=0.03265707194805145
I0205 08:55:25.237682 139739177916160 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.03404133766889572, loss=0.03373993933200836
I0205 08:55:35.356943 139919816816448 spec.py:321] Evaluating on the training split.
I0205 08:57:28.625056 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 08:57:31.685202 139919816816448 spec.py:349] Evaluating on the test split.
I0205 08:57:34.723375 139919816816448 submission_runner.py:408] Time since start: 11703.93s, 	Step: 23933, 	{'train/accuracy': 0.9929741024971008, 'train/loss': 0.023070918396115303, 'train/mean_average_precision': 0.5870177889735277, 'validation/accuracy': 0.9866250157356262, 'validation/loss': 0.04693299159407616, 'validation/mean_average_precision': 0.26114968624787654, 'validation/num_examples': 43793, 'test/accuracy': 0.9857092499732971, 'test/loss': 0.04996924847364426, 'test/mean_average_precision': 0.2528821126225342, 'test/num_examples': 43793, 'score': 7696.241092205048, 'total_duration': 11703.929669380188, 'accumulated_submission_time': 7696.241092205048, 'accumulated_eval_time': 4006.09627699852, 'accumulated_logging_time': 0.9327542781829834}
I0205 08:57:34.744145 139735693903616 logging_writer.py:48] [23933] accumulated_eval_time=4006.096277, accumulated_logging_time=0.932754, accumulated_submission_time=7696.241092, global_step=23933, preemption_count=0, score=7696.241092, test/accuracy=0.985709, test/loss=0.049969, test/mean_average_precision=0.252882, test/num_examples=43793, total_duration=11703.929669, train/accuracy=0.992974, train/loss=0.023071, train/mean_average_precision=0.587018, validation/accuracy=0.986625, validation/loss=0.046933, validation/mean_average_precision=0.261150, validation/num_examples=43793
I0205 08:57:56.723728 139752296675072 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.036383602768182755, loss=0.03476618230342865
I0205 08:58:28.987507 139735693903616 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.0372830331325531, loss=0.030961280688643456
I0205 08:59:02.218935 139752296675072 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.038690850138664246, loss=0.03456052020192146
I0205 08:59:34.143824 139735693903616 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.03844647854566574, loss=0.0338585302233696
I0205 09:00:06.370636 139752296675072 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.037762511521577835, loss=0.031770117580890656
I0205 09:00:38.868061 139735693903616 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.035767387598752975, loss=0.03333147242665291
I0205 09:01:11.019138 139752296675072 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.03685647249221802, loss=0.03225426375865936
I0205 09:01:35.050056 139919816816448 spec.py:321] Evaluating on the training split.
I0205 09:03:31.495946 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 09:03:34.591693 139919816816448 spec.py:349] Evaluating on the test split.
I0205 09:03:37.605553 139919816816448 submission_runner.py:408] Time since start: 12066.81s, 	Step: 24676, 	{'train/accuracy': 0.9929828643798828, 'train/loss': 0.022991664707660675, 'train/mean_average_precision': 0.5987426324371437, 'validation/accuracy': 0.9867159724235535, 'validation/loss': 0.04694814234972, 'validation/mean_average_precision': 0.26135489494565817, 'validation/num_examples': 43793, 'test/accuracy': 0.9857812523841858, 'test/loss': 0.049936845898628235, 'test/mean_average_precision': 0.2494686159353731, 'test/num_examples': 43793, 'score': 7936.516567230225, 'total_duration': 12066.811733722687, 'accumulated_submission_time': 7936.516567230225, 'accumulated_eval_time': 4128.651603221893, 'accumulated_logging_time': 0.9643106460571289}
I0205 09:03:37.625311 139739177916160 logging_writer.py:48] [24676] accumulated_eval_time=4128.651603, accumulated_logging_time=0.964311, accumulated_submission_time=7936.516567, global_step=24676, preemption_count=0, score=7936.516567, test/accuracy=0.985781, test/loss=0.049937, test/mean_average_precision=0.249469, test/num_examples=43793, total_duration=12066.811734, train/accuracy=0.992983, train/loss=0.022992, train/mean_average_precision=0.598743, validation/accuracy=0.986716, validation/loss=0.046948, validation/mean_average_precision=0.261355, validation/num_examples=43793
I0205 09:03:45.587749 139752480773888 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.04357762634754181, loss=0.0353134348988533
I0205 09:04:17.849361 139739177916160 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.03220810741186142, loss=0.030549950897693634
I0205 09:04:50.602019 139752480773888 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.03108709678053856, loss=0.03035031445324421
I0205 09:05:22.919035 139739177916160 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.030518433079123497, loss=0.030008813366293907
I0205 09:05:55.274236 139752480773888 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.035685792565345764, loss=0.03315931558609009
I0205 09:06:27.333804 139739177916160 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.030565092340111732, loss=0.031125729903578758
I0205 09:06:59.438269 139752480773888 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.03452010080218315, loss=0.03400660678744316
I0205 09:07:31.520518 139739177916160 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.044518858194351196, loss=0.03104475326836109
I0205 09:07:37.891593 139919816816448 spec.py:321] Evaluating on the training split.
I0205 09:09:35.780482 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 09:09:39.125805 139919816816448 spec.py:349] Evaluating on the test split.
I0205 09:09:42.460681 139919816816448 submission_runner.py:408] Time since start: 12431.67s, 	Step: 25421, 	{'train/accuracy': 0.9923743009567261, 'train/loss': 0.024536333978176117, 'train/mean_average_precision': 0.5646499916322825, 'validation/accuracy': 0.9865953922271729, 'validation/loss': 0.04823179170489311, 'validation/mean_average_precision': 0.2583915106640733, 'validation/num_examples': 43793, 'test/accuracy': 0.9857602119445801, 'test/loss': 0.05137448012828827, 'test/mean_average_precision': 0.24806008408106967, 'test/num_examples': 43793, 'score': 8176.752241849899, 'total_duration': 12431.66696190834, 'accumulated_submission_time': 8176.752241849899, 'accumulated_eval_time': 4253.2206172943115, 'accumulated_logging_time': 0.9951791763305664}
I0205 09:09:42.483309 139735693903616 logging_writer.py:48] [25421] accumulated_eval_time=4253.220617, accumulated_logging_time=0.995179, accumulated_submission_time=8176.752242, global_step=25421, preemption_count=0, score=8176.752242, test/accuracy=0.985760, test/loss=0.051374, test/mean_average_precision=0.248060, test/num_examples=43793, total_duration=12431.666962, train/accuracy=0.992374, train/loss=0.024536, train/mean_average_precision=0.564650, validation/accuracy=0.986595, validation/loss=0.048232, validation/mean_average_precision=0.258392, validation/num_examples=43793
I0205 09:10:08.527905 139752296675072 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.05008074268698692, loss=0.03289540857076645
I0205 09:10:41.205439 139735693903616 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.04372169449925423, loss=0.032262664288282394
I0205 09:11:14.147110 139752296675072 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.040873799473047256, loss=0.03258967399597168
I0205 09:11:46.331425 139735693903616 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.04108745604753494, loss=0.03226086124777794
I0205 09:12:18.501630 139752296675072 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.040518853813409805, loss=0.03299027308821678
I0205 09:12:50.746876 139735693903616 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.0341518335044384, loss=0.031029488891363144
I0205 09:13:23.416778 139752296675072 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.03653939813375473, loss=0.03061470203101635
I0205 09:13:42.481482 139919816816448 spec.py:321] Evaluating on the training split.
I0205 09:15:32.262798 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 09:15:35.340912 139919816816448 spec.py:349] Evaluating on the test split.
I0205 09:15:38.374167 139919816816448 submission_runner.py:408] Time since start: 12787.58s, 	Step: 26160, 	{'train/accuracy': 0.9926239848136902, 'train/loss': 0.02409951202571392, 'train/mean_average_precision': 0.5744237415586881, 'validation/accuracy': 0.986622154712677, 'validation/loss': 0.04719825088977814, 'validation/mean_average_precision': 0.25683031179824894, 'validation/num_examples': 43793, 'test/accuracy': 0.9857054352760315, 'test/loss': 0.05031025409698486, 'test/mean_average_precision': 0.2487609686306002, 'test/num_examples': 43793, 'score': 8416.71799492836, 'total_duration': 12787.58046579361, 'accumulated_submission_time': 8416.71799492836, 'accumulated_eval_time': 4369.113248348236, 'accumulated_logging_time': 1.0298182964324951}
I0205 09:15:38.394608 139739177916160 logging_writer.py:48] [26160] accumulated_eval_time=4369.113248, accumulated_logging_time=1.029818, accumulated_submission_time=8416.717995, global_step=26160, preemption_count=0, score=8416.717995, test/accuracy=0.985705, test/loss=0.050310, test/mean_average_precision=0.248761, test/num_examples=43793, total_duration=12787.580466, train/accuracy=0.992624, train/loss=0.024100, train/mean_average_precision=0.574424, validation/accuracy=0.986622, validation/loss=0.047198, validation/mean_average_precision=0.256830, validation/num_examples=43793
I0205 09:15:51.778628 139752480773888 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.03525816649198532, loss=0.03281573951244354
I0205 09:16:24.162391 139739177916160 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.04199032858014107, loss=0.033118005841970444
I0205 09:16:56.302809 139752480773888 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.03617999330163002, loss=0.03144126012921333
I0205 09:17:28.660149 139739177916160 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.038258668035268784, loss=0.03218100592494011
I0205 09:18:01.395899 139752480773888 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.03688440099358559, loss=0.03222314640879631
I0205 09:18:33.811727 139739177916160 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.0350666344165802, loss=0.03112415410578251
I0205 09:19:06.092905 139752480773888 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.040182821452617645, loss=0.032254114747047424
I0205 09:19:38.638383 139739177916160 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.03364379331469536, loss=0.029247533529996872
I0205 09:19:38.643564 139919816816448 spec.py:321] Evaluating on the training split.
I0205 09:21:34.338308 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 09:21:37.737798 139919816816448 spec.py:349] Evaluating on the test split.
I0205 09:21:41.071956 139919816816448 submission_runner.py:408] Time since start: 13150.28s, 	Step: 26901, 	{'train/accuracy': 0.9923655986785889, 'train/loss': 0.024558234959840775, 'train/mean_average_precision': 0.5520001620106612, 'validation/accuracy': 0.9866526126861572, 'validation/loss': 0.047937121242284775, 'validation/mean_average_precision': 0.25535532196750965, 'validation/num_examples': 43793, 'test/accuracy': 0.9857370257377625, 'test/loss': 0.051448315382003784, 'test/mean_average_precision': 0.24396206410674492, 'test/num_examples': 43793, 'score': 8656.93652176857, 'total_duration': 13150.278239965439, 'accumulated_submission_time': 8656.93652176857, 'accumulated_eval_time': 4491.541553258896, 'accumulated_logging_time': 1.0609753131866455}
I0205 09:21:41.094846 139752296675072 logging_writer.py:48] [26901] accumulated_eval_time=4491.541553, accumulated_logging_time=1.060975, accumulated_submission_time=8656.936522, global_step=26901, preemption_count=0, score=8656.936522, test/accuracy=0.985737, test/loss=0.051448, test/mean_average_precision=0.243962, test/num_examples=43793, total_duration=13150.278240, train/accuracy=0.992366, train/loss=0.024558, train/mean_average_precision=0.552000, validation/accuracy=0.986653, validation/loss=0.047937, validation/mean_average_precision=0.255355, validation/num_examples=43793
I0205 09:22:14.714284 139759021496064 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.04586535692214966, loss=0.03448771312832832
I0205 09:22:47.279943 139752296675072 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.044700924307107925, loss=0.03156411275267601
I0205 09:23:20.037357 139759021496064 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.042989373207092285, loss=0.03219832479953766
I0205 09:23:52.717161 139752296675072 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.042861681431531906, loss=0.033021409064531326
I0205 09:24:25.513999 139759021496064 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.035553909838199615, loss=0.032607048749923706
I0205 09:24:58.365180 139752296675072 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.045457907021045685, loss=0.030705567449331284
I0205 09:25:30.951955 139759021496064 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.042715203016996384, loss=0.03151455149054527
I0205 09:25:41.403385 139919816816448 spec.py:321] Evaluating on the training split.
I0205 09:27:37.591536 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 09:27:40.661854 139919816816448 spec.py:349] Evaluating on the test split.
I0205 09:27:43.666613 139919816816448 submission_runner.py:408] Time since start: 13512.87s, 	Step: 27633, 	{'train/accuracy': 0.9923880100250244, 'train/loss': 0.024168267846107483, 'train/mean_average_precision': 0.5654214180134779, 'validation/accuracy': 0.9865978360176086, 'validation/loss': 0.04857823625206947, 'validation/mean_average_precision': 0.25026398056994087, 'validation/num_examples': 43793, 'test/accuracy': 0.98567134141922, 'test/loss': 0.05205978453159332, 'test/mean_average_precision': 0.2451742423721277, 'test/num_examples': 43793, 'score': 8897.207918643951, 'total_duration': 13512.87291264534, 'accumulated_submission_time': 8897.207918643951, 'accumulated_eval_time': 4613.804739713669, 'accumulated_logging_time': 1.095686435699463}
I0205 09:27:43.688177 139735693903616 logging_writer.py:48] [27633] accumulated_eval_time=4613.804740, accumulated_logging_time=1.095686, accumulated_submission_time=8897.207919, global_step=27633, preemption_count=0, score=8897.207919, test/accuracy=0.985671, test/loss=0.052060, test/mean_average_precision=0.245174, test/num_examples=43793, total_duration=13512.872913, train/accuracy=0.992388, train/loss=0.024168, train/mean_average_precision=0.565421, validation/accuracy=0.986598, validation/loss=0.048578, validation/mean_average_precision=0.250264, validation/num_examples=43793
I0205 09:28:05.558459 139752480773888 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.04387133568525314, loss=0.033397749066352844
I0205 09:28:37.708999 139735693903616 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.040821339935064316, loss=0.032605335116386414
I0205 09:29:09.911685 139752480773888 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.03316625580191612, loss=0.02938239648938179
I0205 09:29:42.222902 139735693903616 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.03966735303401947, loss=0.03154723718762398
I0205 09:30:14.706392 139752480773888 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.04013785719871521, loss=0.03373124077916145
I0205 09:30:47.068157 139735693903616 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.04951591417193413, loss=0.03142295032739639
I0205 09:31:19.563950 139752480773888 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.04010509327054024, loss=0.029479917138814926
I0205 09:31:43.700844 139919816816448 spec.py:321] Evaluating on the training split.
I0205 09:33:37.358012 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 09:33:40.397348 139919816816448 spec.py:349] Evaluating on the test split.
I0205 09:33:43.387271 139919816816448 submission_runner.py:408] Time since start: 13872.59s, 	Step: 28375, 	{'train/accuracy': 0.9930915832519531, 'train/loss': 0.022376207634806633, 'train/mean_average_precision': 0.5991473293839304, 'validation/accuracy': 0.9865767359733582, 'validation/loss': 0.048158515244722366, 'validation/mean_average_precision': 0.2556357184836858, 'validation/num_examples': 43793, 'test/accuracy': 0.9856898784637451, 'test/loss': 0.05165575072169304, 'test/mean_average_precision': 0.24302427229867257, 'test/num_examples': 43793, 'score': 9137.189074516296, 'total_duration': 13872.593567609787, 'accumulated_submission_time': 9137.189074516296, 'accumulated_eval_time': 4733.4911098480225, 'accumulated_logging_time': 1.128706693649292}
I0205 09:33:43.408694 139739177916160 logging_writer.py:48] [28375] accumulated_eval_time=4733.491110, accumulated_logging_time=1.128707, accumulated_submission_time=9137.189075, global_step=28375, preemption_count=0, score=9137.189075, test/accuracy=0.985690, test/loss=0.051656, test/mean_average_precision=0.243024, test/num_examples=43793, total_duration=13872.593568, train/accuracy=0.993092, train/loss=0.022376, train/mean_average_precision=0.599147, validation/accuracy=0.986577, validation/loss=0.048159, validation/mean_average_precision=0.255636, validation/num_examples=43793
I0205 09:33:51.861757 139759021496064 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.04430612921714783, loss=0.030921023339033127
I0205 09:34:23.903593 139739177916160 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.043476495891809464, loss=0.03054121509194374
I0205 09:34:55.595139 139759021496064 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.04816262423992157, loss=0.03392685577273369
I0205 09:35:27.678505 139739177916160 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.04099414497613907, loss=0.03064177930355072
I0205 09:35:59.705357 139759021496064 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.03784361481666565, loss=0.029555998742580414
I0205 09:36:31.873983 139739177916160 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.048142220824956894, loss=0.03390731289982796
I0205 09:37:04.186937 139759021496064 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.04371260106563568, loss=0.03143913298845291
I0205 09:37:36.587182 139739177916160 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.041954413056373596, loss=0.031250011175870895
I0205 09:37:43.684926 139919816816448 spec.py:321] Evaluating on the training split.
I0205 09:39:38.480851 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 09:39:41.559576 139919816816448 spec.py:349] Evaluating on the test split.
I0205 09:39:44.553554 139919816816448 submission_runner.py:408] Time since start: 14233.76s, 	Step: 29123, 	{'train/accuracy': 0.9930724501609802, 'train/loss': 0.02218599058687687, 'train/mean_average_precision': 0.617831682258646, 'validation/accuracy': 0.9865877032279968, 'validation/loss': 0.048793043941259384, 'validation/mean_average_precision': 0.25224042822579723, 'validation/num_examples': 43793, 'test/accuracy': 0.9857964515686035, 'test/loss': 0.051948074251413345, 'test/mean_average_precision': 0.24737597931197716, 'test/num_examples': 43793, 'score': 9377.43240237236, 'total_duration': 14233.759857654572, 'accumulated_submission_time': 9377.43240237236, 'accumulated_eval_time': 4854.35968875885, 'accumulated_logging_time': 1.1624870300292969}
I0205 09:39:44.574264 139735693903616 logging_writer.py:48] [29123] accumulated_eval_time=4854.359689, accumulated_logging_time=1.162487, accumulated_submission_time=9377.432402, global_step=29123, preemption_count=0, score=9377.432402, test/accuracy=0.985796, test/loss=0.051948, test/mean_average_precision=0.247376, test/num_examples=43793, total_duration=14233.759858, train/accuracy=0.993072, train/loss=0.022186, train/mean_average_precision=0.617832, validation/accuracy=0.986588, validation/loss=0.048793, validation/mean_average_precision=0.252240, validation/num_examples=43793
I0205 09:40:09.747467 139752296675072 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.04828008636832237, loss=0.03164141997694969
I0205 09:40:42.171499 139735693903616 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.04523830860853195, loss=0.03246303275227547
I0205 09:41:15.153898 139752296675072 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.04397762566804886, loss=0.032239917665719986
I0205 09:41:47.462615 139735693903616 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.04514974728226662, loss=0.031221190467476845
I0205 09:42:19.839635 139752296675072 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.0462813526391983, loss=0.032030124217271805
I0205 09:42:52.339279 139735693903616 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.0447542741894722, loss=0.030021293088793755
I0205 09:43:25.075351 139752296675072 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.04715536907315254, loss=0.030689632520079613
I0205 09:43:44.564924 139919816816448 spec.py:321] Evaluating on the training split.
I0205 09:45:36.152012 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 09:45:39.210080 139919816816448 spec.py:349] Evaluating on the test split.
I0205 09:45:42.253223 139919816816448 submission_runner.py:408] Time since start: 14591.46s, 	Step: 29861, 	{'train/accuracy': 0.993553876876831, 'train/loss': 0.020813263952732086, 'train/mean_average_precision': 0.6401829967227319, 'validation/accuracy': 0.9865182638168335, 'validation/loss': 0.04869452491402626, 'validation/mean_average_precision': 0.2477831701067681, 'validation/num_examples': 43793, 'test/accuracy': 0.9856991171836853, 'test/loss': 0.05191131681203842, 'test/mean_average_precision': 0.24846553581429545, 'test/num_examples': 43793, 'score': 9617.391327857971, 'total_duration': 14591.459522247314, 'accumulated_submission_time': 9617.391327857971, 'accumulated_eval_time': 4972.0479435920715, 'accumulated_logging_time': 1.1940855979919434}
I0205 09:45:42.274812 139739177916160 logging_writer.py:48] [29861] accumulated_eval_time=4972.047944, accumulated_logging_time=1.194086, accumulated_submission_time=9617.391328, global_step=29861, preemption_count=0, score=9617.391328, test/accuracy=0.985699, test/loss=0.051911, test/mean_average_precision=0.248466, test/num_examples=43793, total_duration=14591.459522, train/accuracy=0.993554, train/loss=0.020813, train/mean_average_precision=0.640183, validation/accuracy=0.986518, validation/loss=0.048695, validation/mean_average_precision=0.247783, validation/num_examples=43793
I0205 09:45:55.532878 139752480773888 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.04495807737112045, loss=0.03032028116285801
I0205 09:46:28.614238 139739177916160 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.045772735029459, loss=0.03299090266227722
I0205 09:47:01.873357 139752480773888 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.05098720267415047, loss=0.03192320838570595
I0205 09:47:34.783679 139739177916160 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.04358213022351265, loss=0.028647668659687042
I0205 09:48:07.663105 139752480773888 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.04722382500767708, loss=0.031731538474559784
I0205 09:48:40.409430 139739177916160 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.05309314653277397, loss=0.0302311722189188
I0205 09:49:13.295436 139752480773888 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.04143444821238518, loss=0.02845650725066662
I0205 09:49:42.555165 139919816816448 spec.py:321] Evaluating on the training split.
I0205 09:51:41.701811 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 09:51:45.103328 139919816816448 spec.py:349] Evaluating on the test split.
I0205 09:51:48.381854 139919816816448 submission_runner.py:408] Time since start: 14957.59s, 	Step: 30589, 	{'train/accuracy': 0.9940480589866638, 'train/loss': 0.019391926005482674, 'train/mean_average_precision': 0.6610642591112672, 'validation/accuracy': 0.9865243434906006, 'validation/loss': 0.04920274019241333, 'validation/mean_average_precision': 0.2488174179315952, 'validation/num_examples': 43793, 'test/accuracy': 0.9856089949607849, 'test/loss': 0.05269637703895569, 'test/mean_average_precision': 0.24340012203764505, 'test/num_examples': 43793, 'score': 9857.636467218399, 'total_duration': 14957.588129997253, 'accumulated_submission_time': 9857.636467218399, 'accumulated_eval_time': 5097.874583005905, 'accumulated_logging_time': 1.2269041538238525}
I0205 09:51:48.407495 139735693903616 logging_writer.py:48] [30589] accumulated_eval_time=5097.874583, accumulated_logging_time=1.226904, accumulated_submission_time=9857.636467, global_step=30589, preemption_count=0, score=9857.636467, test/accuracy=0.985609, test/loss=0.052696, test/mean_average_precision=0.243400, test/num_examples=43793, total_duration=14957.588130, train/accuracy=0.994048, train/loss=0.019392, train/mean_average_precision=0.661064, validation/accuracy=0.986524, validation/loss=0.049203, validation/mean_average_precision=0.248817, validation/num_examples=43793
I0205 09:51:52.347308 139752296675072 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.0434361957013607, loss=0.03173309937119484
I0205 09:52:24.521550 139735693903616 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.04506129026412964, loss=0.029506325721740723
I0205 09:52:56.588963 139752296675072 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.055347539484500885, loss=0.03361436352133751
I0205 09:53:29.178150 139735693903616 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.05308239907026291, loss=0.031340092420578
I0205 09:54:00.984458 139752296675072 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.049780137836933136, loss=0.029640376567840576
I0205 09:54:33.247145 139735693903616 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.045250359922647476, loss=0.0307253897190094
I0205 09:55:05.202749 139752296675072 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.043695054948329926, loss=0.030360279604792595
I0205 09:55:37.894314 139735693903616 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.05131979286670685, loss=0.029705017805099487
I0205 09:55:48.679980 139919816816448 spec.py:321] Evaluating on the training split.
I0205 09:57:43.467856 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 09:57:46.579409 139919816816448 spec.py:349] Evaluating on the test split.
I0205 09:57:49.636170 139919816816448 submission_runner.py:408] Time since start: 15318.84s, 	Step: 31334, 	{'train/accuracy': 0.9936291575431824, 'train/loss': 0.020761188119649887, 'train/mean_average_precision': 0.6274333508360139, 'validation/accuracy': 0.9864720106124878, 'validation/loss': 0.04945521429181099, 'validation/mean_average_precision': 0.24921789351712892, 'validation/num_examples': 43793, 'test/accuracy': 0.9855576157569885, 'test/loss': 0.052920859307050705, 'test/mean_average_precision': 0.23617914446786695, 'test/num_examples': 43793, 'score': 10097.875643253326, 'total_duration': 15318.84247136116, 'accumulated_submission_time': 10097.875643253326, 'accumulated_eval_time': 5218.830732822418, 'accumulated_logging_time': 1.2641723155975342}
I0205 09:57:49.658329 139739177916160 logging_writer.py:48] [31334] accumulated_eval_time=5218.830733, accumulated_logging_time=1.264172, accumulated_submission_time=10097.875643, global_step=31334, preemption_count=0, score=10097.875643, test/accuracy=0.985558, test/loss=0.052921, test/mean_average_precision=0.236179, test/num_examples=43793, total_duration=15318.842471, train/accuracy=0.993629, train/loss=0.020761, train/mean_average_precision=0.627433, validation/accuracy=0.986472, validation/loss=0.049455, validation/mean_average_precision=0.249218, validation/num_examples=43793
I0205 09:58:11.606555 139752480773888 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.05072318762540817, loss=0.029299549758434296
I0205 09:58:44.131094 139739177916160 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.04838286340236664, loss=0.028623588383197784
I0205 09:59:16.429617 139752480773888 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.048792533576488495, loss=0.03191826492547989
I0205 09:59:49.006706 139739177916160 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.05564556643366814, loss=0.03111295960843563
I0205 10:00:21.352233 139752480773888 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.046303365379571915, loss=0.030002091079950333
I0205 10:00:53.700319 139739177916160 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.05267152190208435, loss=0.029967086389660835
I0205 10:01:25.923019 139752480773888 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.055413633584976196, loss=0.03139836713671684
I0205 10:01:49.711822 139919816816448 spec.py:321] Evaluating on the training split.
I0205 10:03:43.955663 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 10:03:46.974705 139919816816448 spec.py:349] Evaluating on the test split.
I0205 10:03:49.976243 139919816816448 submission_runner.py:408] Time since start: 15679.18s, 	Step: 32075, 	{'train/accuracy': 0.9931486248970032, 'train/loss': 0.021551787853240967, 'train/mean_average_precision': 0.6206713202358298, 'validation/accuracy': 0.9865584373474121, 'validation/loss': 0.05010689049959183, 'validation/mean_average_precision': 0.24951108155807555, 'validation/num_examples': 43793, 'test/accuracy': 0.9856974482536316, 'test/loss': 0.053574588149785995, 'test/mean_average_precision': 0.24123737326370379, 'test/num_examples': 43793, 'score': 10337.898092508316, 'total_duration': 15679.182544469833, 'accumulated_submission_time': 10337.898092508316, 'accumulated_eval_time': 5339.095104217529, 'accumulated_logging_time': 1.2972569465637207}
I0205 10:03:49.998656 139735693903616 logging_writer.py:48] [32075] accumulated_eval_time=5339.095104, accumulated_logging_time=1.297257, accumulated_submission_time=10337.898093, global_step=32075, preemption_count=0, score=10337.898093, test/accuracy=0.985697, test/loss=0.053575, test/mean_average_precision=0.241237, test/num_examples=43793, total_duration=15679.182544, train/accuracy=0.993149, train/loss=0.021552, train/mean_average_precision=0.620671, validation/accuracy=0.986558, validation/loss=0.050107, validation/mean_average_precision=0.249511, validation/num_examples=43793
I0205 10:03:58.511765 139752296675072 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.05602487176656723, loss=0.029106957837939262
I0205 10:04:30.914201 139735693903616 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.05472094938158989, loss=0.03149991109967232
I0205 10:05:03.580160 139752296675072 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.057173095643520355, loss=0.029994197189807892
I0205 10:05:35.769342 139735693903616 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.04865051060914993, loss=0.030882442370057106
I0205 10:06:08.241036 139752296675072 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.054109711199998856, loss=0.030119040980935097
I0205 10:06:40.980206 139735693903616 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.0686718299984932, loss=0.03146626427769661
I0205 10:07:13.314820 139752296675072 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.05679294839501381, loss=0.028507927432656288
I0205 10:07:45.322219 139735693903616 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.050777360796928406, loss=0.03069877251982689
I0205 10:07:50.109504 139919816816448 spec.py:321] Evaluating on the training split.
I0205 10:09:42.635025 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 10:09:45.734173 139919816816448 spec.py:349] Evaluating on the test split.
I0205 10:09:48.725546 139919816816448 submission_runner.py:408] Time since start: 16037.93s, 	Step: 32816, 	{'train/accuracy': 0.9932396411895752, 'train/loss': 0.021539898589253426, 'train/mean_average_precision': 0.6191252736954957, 'validation/accuracy': 0.9865308403968811, 'validation/loss': 0.04971396178007126, 'validation/mean_average_precision': 0.2507458835114568, 'validation/num_examples': 43793, 'test/accuracy': 0.9856300950050354, 'test/loss': 0.053199104964733124, 'test/mean_average_precision': 0.24082175433602587, 'test/num_examples': 43793, 'score': 10577.976407766342, 'total_duration': 16037.931844472885, 'accumulated_submission_time': 10577.976407766342, 'accumulated_eval_time': 5457.711098432541, 'accumulated_logging_time': 1.3318133354187012}
I0205 10:09:48.747377 139752480773888 logging_writer.py:48] [32816] accumulated_eval_time=5457.711098, accumulated_logging_time=1.331813, accumulated_submission_time=10577.976408, global_step=32816, preemption_count=0, score=10577.976408, test/accuracy=0.985630, test/loss=0.053199, test/mean_average_precision=0.240822, test/num_examples=43793, total_duration=16037.931844, train/accuracy=0.993240, train/loss=0.021540, train/mean_average_precision=0.619125, validation/accuracy=0.986531, validation/loss=0.049714, validation/mean_average_precision=0.250746, validation/num_examples=43793
I0205 10:10:16.144848 139759021496064 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.06025753542780876, loss=0.029050463810563087
I0205 10:10:47.904475 139752480773888 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.05246751010417938, loss=0.028463199734687805
I0205 10:11:19.679574 139759021496064 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.07067795842885971, loss=0.030083447694778442
I0205 10:11:52.163485 139752480773888 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.053094811737537384, loss=0.02986588142812252
I0205 10:12:24.345263 139759021496064 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.06438127905130386, loss=0.0292836744338274
I0205 10:12:56.020954 139752480773888 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.05641389265656471, loss=0.030924314633011818
I0205 10:13:28.188690 139759021496064 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.05939457193017006, loss=0.029287686571478844
I0205 10:13:48.900299 139919816816448 spec.py:321] Evaluating on the training split.
I0205 10:15:39.741287 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 10:15:42.811166 139919816816448 spec.py:349] Evaluating on the test split.
I0205 10:15:45.832071 139919816816448 submission_runner.py:408] Time since start: 16395.04s, 	Step: 33566, 	{'train/accuracy': 0.9932653307914734, 'train/loss': 0.02148575894534588, 'train/mean_average_precision': 0.6170808507068615, 'validation/accuracy': 0.9865471124649048, 'validation/loss': 0.04991072416305542, 'validation/mean_average_precision': 0.24726883668381008, 'validation/num_examples': 43793, 'test/accuracy': 0.9854704141616821, 'test/loss': 0.05360097438097, 'test/mean_average_precision': 0.2351262883357148, 'test/num_examples': 43793, 'score': 10818.098178625107, 'total_duration': 16395.03837776184, 'accumulated_submission_time': 10818.098178625107, 'accumulated_eval_time': 5574.642826318741, 'accumulated_logging_time': 1.364924430847168}
I0205 10:15:45.853814 139735693903616 logging_writer.py:48] [33566] accumulated_eval_time=5574.642826, accumulated_logging_time=1.364924, accumulated_submission_time=10818.098179, global_step=33566, preemption_count=0, score=10818.098179, test/accuracy=0.985470, test/loss=0.053601, test/mean_average_precision=0.235126, test/num_examples=43793, total_duration=16395.038378, train/accuracy=0.993265, train/loss=0.021486, train/mean_average_precision=0.617081, validation/accuracy=0.986547, validation/loss=0.049911, validation/mean_average_precision=0.247269, validation/num_examples=43793
I0205 10:15:57.190635 139752296675072 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.055360548198223114, loss=0.028172163292765617
I0205 10:16:28.936215 139735693903616 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.05817817524075508, loss=0.027568863704800606
I0205 10:17:00.649110 139752296675072 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.05588642507791519, loss=0.028150631114840508
I0205 10:17:32.702800 139735693903616 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.051238711923360825, loss=0.029530709609389305
I0205 10:18:04.643328 139752296675072 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.05052559822797775, loss=0.02945178560912609
I0205 10:18:36.635103 139735693903616 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.06426497548818588, loss=0.029106562957167625
I0205 10:19:08.603701 139752296675072 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.055736009031534195, loss=0.03053855709731579
I0205 10:19:40.365424 139735693903616 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.06300687789916992, loss=0.028284283354878426
I0205 10:19:46.147058 139919816816448 spec.py:321] Evaluating on the training split.
I0205 10:21:37.706037 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 10:21:40.707972 139919816816448 spec.py:349] Evaluating on the test split.
I0205 10:21:43.675711 139919816816448 submission_runner.py:408] Time since start: 16752.88s, 	Step: 34319, 	{'train/accuracy': 0.9936695694923401, 'train/loss': 0.020347896963357925, 'train/mean_average_precision': 0.6380343914037621, 'validation/accuracy': 0.9864545464515686, 'validation/loss': 0.0497109591960907, 'validation/mean_average_precision': 0.25108792310597466, 'validation/num_examples': 43793, 'test/accuracy': 0.9855020046234131, 'test/loss': 0.05300797149538994, 'test/mean_average_precision': 0.2383980651235781, 'test/num_examples': 43793, 'score': 11058.361001968384, 'total_duration': 16752.882014513016, 'accumulated_submission_time': 11058.361001968384, 'accumulated_eval_time': 5692.171427726746, 'accumulated_logging_time': 1.3974757194519043}
I0205 10:21:43.698064 139739177916160 logging_writer.py:48] [34319] accumulated_eval_time=5692.171428, accumulated_logging_time=1.397476, accumulated_submission_time=11058.361002, global_step=34319, preemption_count=0, score=11058.361002, test/accuracy=0.985502, test/loss=0.053008, test/mean_average_precision=0.238398, test/num_examples=43793, total_duration=16752.882015, train/accuracy=0.993670, train/loss=0.020348, train/mean_average_precision=0.638034, validation/accuracy=0.986455, validation/loss=0.049711, validation/mean_average_precision=0.251088, validation/num_examples=43793
I0205 10:22:10.167054 139752480773888 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.06291303783655167, loss=0.029956096783280373
I0205 10:22:42.376996 139739177916160 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.0536922961473465, loss=0.028304429724812508
I0205 10:23:14.465930 139752480773888 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.055925000458955765, loss=0.026706280186772346
I0205 10:23:46.160683 139739177916160 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.05794456973671913, loss=0.02875637821853161
I0205 10:24:18.175101 139752480773888 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.05700242519378662, loss=0.028311712667346
I0205 10:24:49.883717 139739177916160 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.054363373667001724, loss=0.02988666668534279
I0205 10:25:22.550134 139752480773888 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.06054633855819702, loss=0.028951656073331833
I0205 10:25:43.924850 139919816816448 spec.py:321] Evaluating on the training split.
I0205 10:27:38.746207 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 10:27:41.793336 139919816816448 spec.py:349] Evaluating on the test split.
I0205 10:27:44.783641 139919816816448 submission_runner.py:408] Time since start: 17113.99s, 	Step: 35066, 	{'train/accuracy': 0.9935685992240906, 'train/loss': 0.020250258967280388, 'train/mean_average_precision': 0.6473796514189897, 'validation/accuracy': 0.9865594506263733, 'validation/loss': 0.05094848573207855, 'validation/mean_average_precision': 0.25627819733279783, 'validation/num_examples': 43793, 'test/accuracy': 0.9855298399925232, 'test/loss': 0.0547693707048893, 'test/mean_average_precision': 0.23701332230698652, 'test/num_examples': 43793, 'score': 11298.55425786972, 'total_duration': 17113.989943742752, 'accumulated_submission_time': 11298.55425786972, 'accumulated_eval_time': 5813.030182600021, 'accumulated_logging_time': 1.4321606159210205}
I0205 10:27:44.806662 139752296675072 logging_writer.py:48] [35066] accumulated_eval_time=5813.030183, accumulated_logging_time=1.432161, accumulated_submission_time=11298.554258, global_step=35066, preemption_count=0, score=11298.554258, test/accuracy=0.985530, test/loss=0.054769, test/mean_average_precision=0.237013, test/num_examples=43793, total_duration=17113.989944, train/accuracy=0.993569, train/loss=0.020250, train/mean_average_precision=0.647380, validation/accuracy=0.986559, validation/loss=0.050948, validation/mean_average_precision=0.256278, validation/num_examples=43793
I0205 10:27:56.239695 139759021496064 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.08765052258968353, loss=0.030767783522605896
I0205 10:28:28.843460 139752296675072 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.060074660927057266, loss=0.02927544340491295
I0205 10:29:01.014438 139759021496064 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.06296342611312866, loss=0.02863820269703865
I0205 10:29:33.691688 139752296675072 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.05608493462204933, loss=0.028848107904195786
I0205 10:30:06.084500 139759021496064 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.06856771558523178, loss=0.03089105524122715
I0205 10:30:38.385958 139752296675072 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.05800556391477585, loss=0.028359131887555122
I0205 10:31:10.942121 139759021496064 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.060060396790504456, loss=0.02867780812084675
I0205 10:31:43.097421 139752296675072 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.061834920197725296, loss=0.030603894963860512
I0205 10:31:45.063287 139919816816448 spec.py:321] Evaluating on the training split.
I0205 10:33:36.759815 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 10:33:39.788880 139919816816448 spec.py:349] Evaluating on the test split.
I0205 10:33:42.757182 139919816816448 submission_runner.py:408] Time since start: 17471.96s, 	Step: 35807, 	{'train/accuracy': 0.9941927194595337, 'train/loss': 0.018798677250742912, 'train/mean_average_precision': 0.6769564832266253, 'validation/accuracy': 0.9864675402641296, 'validation/loss': 0.050618451088666916, 'validation/mean_average_precision': 0.2556838216760408, 'validation/num_examples': 43793, 'test/accuracy': 0.9854856133460999, 'test/loss': 0.054121118038892746, 'test/mean_average_precision': 0.23563420421806033, 'test/num_examples': 43793, 'score': 11538.779606342316, 'total_duration': 17471.963469982147, 'accumulated_submission_time': 11538.779606342316, 'accumulated_eval_time': 5930.724012136459, 'accumulated_logging_time': 1.4665467739105225}
I0205 10:33:42.780500 139735693903616 logging_writer.py:48] [35807] accumulated_eval_time=5930.724012, accumulated_logging_time=1.466547, accumulated_submission_time=11538.779606, global_step=35807, preemption_count=0, score=11538.779606, test/accuracy=0.985486, test/loss=0.054121, test/mean_average_precision=0.235634, test/num_examples=43793, total_duration=17471.963470, train/accuracy=0.994193, train/loss=0.018799, train/mean_average_precision=0.676956, validation/accuracy=0.986468, validation/loss=0.050618, validation/mean_average_precision=0.255684, validation/num_examples=43793
I0205 10:34:13.296867 139739177916160 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.06283608824014664, loss=0.030241571366786957
I0205 10:34:45.802215 139735693903616 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.06587359309196472, loss=0.028379417955875397
I0205 10:35:18.016762 139739177916160 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.06103569641709328, loss=0.028539346531033516
I0205 10:35:49.678450 139735693903616 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.07497358322143555, loss=0.02956133708357811
I0205 10:36:21.472291 139739177916160 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.07157965749502182, loss=0.031130967661738396
I0205 10:36:53.140080 139735693903616 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.060803286731243134, loss=0.028213970363140106
I0205 10:37:25.124224 139739177916160 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.05859474837779999, loss=0.029442058876156807
I0205 10:37:42.810342 139919816816448 spec.py:321] Evaluating on the training split.
I0205 10:39:32.436328 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 10:39:35.573358 139919816816448 spec.py:349] Evaluating on the test split.
I0205 10:39:40.376614 139919816816448 submission_runner.py:408] Time since start: 17829.58s, 	Step: 36556, 	{'train/accuracy': 0.9948610663414001, 'train/loss': 0.017193259671330452, 'train/mean_average_precision': 0.712666872419726, 'validation/accuracy': 0.9863424897193909, 'validation/loss': 0.05080199986696243, 'validation/mean_average_precision': 0.2482839235230926, 'validation/num_examples': 43793, 'test/accuracy': 0.9853832721710205, 'test/loss': 0.05435670539736748, 'test/mean_average_precision': 0.23943025390025763, 'test/num_examples': 43793, 'score': 11778.403351545334, 'total_duration': 17829.582918167114, 'accumulated_submission_time': 11778.403351545334, 'accumulated_eval_time': 6048.290234088898, 'accumulated_logging_time': 1.875577449798584}
I0205 10:39:40.399648 139752296675072 logging_writer.py:48] [36556] accumulated_eval_time=6048.290234, accumulated_logging_time=1.875577, accumulated_submission_time=11778.403352, global_step=36556, preemption_count=0, score=11778.403352, test/accuracy=0.985383, test/loss=0.054357, test/mean_average_precision=0.239430, test/num_examples=43793, total_duration=17829.582918, train/accuracy=0.994861, train/loss=0.017193, train/mean_average_precision=0.712667, validation/accuracy=0.986342, validation/loss=0.050802, validation/mean_average_precision=0.248284, validation/num_examples=43793
I0205 10:39:55.068186 139759021496064 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.07064573466777802, loss=0.03019411675632
I0205 10:40:27.514879 139752296675072 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.06451471149921417, loss=0.02993117645382881
I0205 10:40:59.920437 139759021496064 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.0623919703066349, loss=0.027610303834080696
I0205 10:41:32.580524 139752296675072 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.05677757412195206, loss=0.024682283401489258
I0205 10:42:04.874742 139759021496064 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.07404022663831711, loss=0.028629813343286514
I0205 10:42:36.937417 139752296675072 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.07575821131467819, loss=0.030533386394381523
I0205 10:43:09.311853 139759021496064 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.06984692066907883, loss=0.029424555599689484
I0205 10:43:40.456844 139919816816448 spec.py:321] Evaluating on the training split.
I0205 10:45:31.585437 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 10:45:34.699161 139919816816448 spec.py:349] Evaluating on the test split.
I0205 10:45:37.713851 139919816816448 submission_runner.py:408] Time since start: 18186.92s, 	Step: 37297, 	{'train/accuracy': 0.9950770735740662, 'train/loss': 0.016590526327490807, 'train/mean_average_precision': 0.7307588502918585, 'validation/accuracy': 0.9863532185554504, 'validation/loss': 0.051204219460487366, 'validation/mean_average_precision': 0.24673543350830335, 'validation/num_examples': 43793, 'test/accuracy': 0.9853453636169434, 'test/loss': 0.054908283054828644, 'test/mean_average_precision': 0.23354721817647842, 'test/num_examples': 43793, 'score': 12018.428232431412, 'total_duration': 18186.92015695572, 'accumulated_submission_time': 12018.428232431412, 'accumulated_eval_time': 6165.5471975803375, 'accumulated_logging_time': 1.9106330871582031}
I0205 10:45:37.736808 139735693903616 logging_writer.py:48] [37297] accumulated_eval_time=6165.547198, accumulated_logging_time=1.910633, accumulated_submission_time=12018.428232, global_step=37297, preemption_count=0, score=12018.428232, test/accuracy=0.985345, test/loss=0.054908, test/mean_average_precision=0.233547, test/num_examples=43793, total_duration=18186.920157, train/accuracy=0.995077, train/loss=0.016591, train/mean_average_precision=0.730759, validation/accuracy=0.986353, validation/loss=0.051204, validation/mean_average_precision=0.246735, validation/num_examples=43793
I0205 10:45:39.162536 139739177916160 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.06511911749839783, loss=0.028977064415812492
I0205 10:46:11.478889 139735693903616 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.057088177651166916, loss=0.02836713008582592
I0205 10:46:43.422762 139739177916160 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.06480012089014053, loss=0.028985703364014626
I0205 10:47:15.602340 139735693903616 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.076629139482975, loss=0.029078451916575432
I0205 10:47:47.517722 139739177916160 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.07984810322523117, loss=0.030736561864614487
I0205 10:48:19.425016 139735693903616 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.07020855695009232, loss=0.02658357471227646
I0205 10:48:51.204643 139739177916160 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.06615880876779556, loss=0.029158256947994232
I0205 10:49:23.284195 139735693903616 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.07332437485456467, loss=0.029892390593886375
I0205 10:49:37.774102 139919816816448 spec.py:321] Evaluating on the training split.
I0205 10:51:31.001888 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 10:51:34.136159 139919816816448 spec.py:349] Evaluating on the test split.
I0205 10:51:37.153595 139919816816448 submission_runner.py:408] Time since start: 18546.36s, 	Step: 38046, 	{'train/accuracy': 0.9940646886825562, 'train/loss': 0.018991518765687943, 'train/mean_average_precision': 0.6748852044013662, 'validation/accuracy': 0.9863676428794861, 'validation/loss': 0.05183979868888855, 'validation/mean_average_precision': 0.2426539176487821, 'validation/num_examples': 43793, 'test/accuracy': 0.9854186177253723, 'test/loss': 0.05569584295153618, 'test/mean_average_precision': 0.234332385061237, 'test/num_examples': 43793, 'score': 12258.432970285416, 'total_duration': 18546.359894514084, 'accumulated_submission_time': 12258.432970285416, 'accumulated_eval_time': 6284.9266357421875, 'accumulated_logging_time': 1.9459753036499023}
I0205 10:51:37.176538 139752296675072 logging_writer.py:48] [38046] accumulated_eval_time=6284.926636, accumulated_logging_time=1.945975, accumulated_submission_time=12258.432970, global_step=38046, preemption_count=0, score=12258.432970, test/accuracy=0.985419, test/loss=0.055696, test/mean_average_precision=0.234332, test/num_examples=43793, total_duration=18546.359895, train/accuracy=0.994065, train/loss=0.018992, train/mean_average_precision=0.674885, validation/accuracy=0.986368, validation/loss=0.051840, validation/mean_average_precision=0.242654, validation/num_examples=43793
I0205 10:51:54.973472 139759021496064 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.06853442639112473, loss=0.02825961261987686
I0205 10:52:27.443834 139752296675072 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.07117092609405518, loss=0.029879635199904442
I0205 10:52:59.362208 139759021496064 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.07352384179830551, loss=0.027854731306433678
I0205 10:53:31.979138 139752296675072 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.06995002180337906, loss=0.029207155108451843
I0205 10:54:04.979695 139759021496064 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.0900963693857193, loss=0.028259096667170525
I0205 10:54:37.256105 139752296675072 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.06488801538944244, loss=0.02666167914867401
I0205 10:55:09.441502 139759021496064 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.07312880456447601, loss=0.02857375144958496
I0205 10:55:37.194760 139919816816448 spec.py:321] Evaluating on the training split.
I0205 10:57:27.096266 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 10:57:30.150295 139919816816448 spec.py:349] Evaluating on the test split.
I0205 10:57:33.113218 139919816816448 submission_runner.py:408] Time since start: 18902.32s, 	Step: 38788, 	{'train/accuracy': 0.9945722818374634, 'train/loss': 0.01792803965508938, 'train/mean_average_precision': 0.6901741289062857, 'validation/accuracy': 0.9861927032470703, 'validation/loss': 0.0516926646232605, 'validation/mean_average_precision': 0.23949731854974, 'validation/num_examples': 43793, 'test/accuracy': 0.9852977395057678, 'test/loss': 0.05522964522242546, 'test/mean_average_precision': 0.23119722214258617, 'test/num_examples': 43793, 'score': 12498.420560121536, 'total_duration': 18902.31952357292, 'accumulated_submission_time': 12498.420560121536, 'accumulated_eval_time': 6400.845049619675, 'accumulated_logging_time': 1.9799385070800781}
I0205 10:57:33.135884 139735693903616 logging_writer.py:48] [38788] accumulated_eval_time=6400.845050, accumulated_logging_time=1.979939, accumulated_submission_time=12498.420560, global_step=38788, preemption_count=0, score=12498.420560, test/accuracy=0.985298, test/loss=0.055230, test/mean_average_precision=0.231197, test/num_examples=43793, total_duration=18902.319524, train/accuracy=0.994572, train/loss=0.017928, train/mean_average_precision=0.690174, validation/accuracy=0.986193, validation/loss=0.051693, validation/mean_average_precision=0.239497, validation/num_examples=43793
I0205 10:57:37.379625 139739177916160 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.06882677972316742, loss=0.02793053165078163
I0205 10:58:09.559616 139735693903616 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.06207742914557457, loss=0.026749150827527046
I0205 10:58:41.976718 139739177916160 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.07427006959915161, loss=0.029139164835214615
I0205 10:59:14.768539 139735693903616 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.07236572355031967, loss=0.0276699960231781
I0205 10:59:46.668745 139739177916160 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.07169745117425919, loss=0.02918180264532566
I0205 11:00:18.786832 139735693903616 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.07289151847362518, loss=0.027893589809536934
I0205 11:00:50.965340 139739177916160 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.0758139044046402, loss=0.02874765545129776
I0205 11:01:23.216330 139735693903616 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.07146359980106354, loss=0.028283020481467247
I0205 11:01:33.247369 139919816816448 spec.py:321] Evaluating on the training split.
I0205 11:03:24.751365 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 11:03:27.871258 139919816816448 spec.py:349] Evaluating on the test split.
I0205 11:03:30.980476 139919816816448 submission_runner.py:408] Time since start: 19260.19s, 	Step: 39532, 	{'train/accuracy': 0.9941584467887878, 'train/loss': 0.018571147695183754, 'train/mean_average_precision': 0.6757015592453715, 'validation/accuracy': 0.9862909317016602, 'validation/loss': 0.05209149420261383, 'validation/mean_average_precision': 0.24297197023763564, 'validation/num_examples': 43793, 'test/accuracy': 0.9853373169898987, 'test/loss': 0.05594261363148689, 'test/mean_average_precision': 0.22739905876539812, 'test/num_examples': 43793, 'score': 12738.50132727623, 'total_duration': 19260.186782360077, 'accumulated_submission_time': 12738.50132727623, 'accumulated_eval_time': 6518.578117609024, 'accumulated_logging_time': 2.013803243637085}
I0205 11:03:31.004446 139752296675072 logging_writer.py:48] [39532] accumulated_eval_time=6518.578118, accumulated_logging_time=2.013803, accumulated_submission_time=12738.501327, global_step=39532, preemption_count=0, score=12738.501327, test/accuracy=0.985337, test/loss=0.055943, test/mean_average_precision=0.227399, test/num_examples=43793, total_duration=19260.186782, train/accuracy=0.994158, train/loss=0.018571, train/mean_average_precision=0.675702, validation/accuracy=0.986291, validation/loss=0.052091, validation/mean_average_precision=0.242972, validation/num_examples=43793
I0205 11:03:53.572768 139759021496064 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.07157894223928452, loss=0.027339739724993706
I0205 11:04:25.954585 139752296675072 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.07147516310214996, loss=0.02830461598932743
I0205 11:04:58.181938 139759021496064 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.07634661346673965, loss=0.02783643640577793
I0205 11:05:30.515440 139752296675072 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.08254826068878174, loss=0.02838369458913803
I0205 11:06:02.629754 139759021496064 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.06671565026044846, loss=0.026382876560091972
I0205 11:06:34.821678 139752296675072 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.07778676599264145, loss=0.028080396354198456
I0205 11:07:07.343868 139759021496064 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.07238153368234634, loss=0.026557108387351036
I0205 11:07:31.238203 139919816816448 spec.py:321] Evaluating on the training split.
I0205 11:09:18.848059 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 11:09:21.998387 139919816816448 spec.py:349] Evaluating on the test split.
I0205 11:09:25.025113 139919816816448 submission_runner.py:408] Time since start: 19614.23s, 	Step: 40275, 	{'train/accuracy': 0.9939488172531128, 'train/loss': 0.018992193043231964, 'train/mean_average_precision': 0.6637098607290097, 'validation/accuracy': 0.9863002896308899, 'validation/loss': 0.05314275622367859, 'validation/mean_average_precision': 0.2391469379130368, 'validation/num_examples': 43793, 'test/accuracy': 0.9853878617286682, 'test/loss': 0.05714089423418045, 'test/mean_average_precision': 0.22669360247005477, 'test/num_examples': 43793, 'score': 12978.7029337883, 'total_duration': 19614.231418848038, 'accumulated_submission_time': 12978.7029337883, 'accumulated_eval_time': 6632.365000963211, 'accumulated_logging_time': 2.0504045486450195}
I0205 11:09:25.048451 139735693903616 logging_writer.py:48] [40275] accumulated_eval_time=6632.365001, accumulated_logging_time=2.050405, accumulated_submission_time=12978.702934, global_step=40275, preemption_count=0, score=12978.702934, test/accuracy=0.985388, test/loss=0.057141, test/mean_average_precision=0.226694, test/num_examples=43793, total_duration=19614.231419, train/accuracy=0.993949, train/loss=0.018992, train/mean_average_precision=0.663710, validation/accuracy=0.986300, validation/loss=0.053143, validation/mean_average_precision=0.239147, validation/num_examples=43793
I0205 11:09:33.448044 139739177916160 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.0834391638636589, loss=0.027142979204654694
I0205 11:10:05.834915 139735693903616 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.07060068845748901, loss=0.026934590190649033
I0205 11:10:37.776383 139739177916160 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.10104716569185257, loss=0.029687175527215004
I0205 11:11:10.472465 139735693903616 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.07526500523090363, loss=0.025452135130763054
I0205 11:11:42.934924 139739177916160 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.06845641881227493, loss=0.026425058022141457
I0205 11:12:15.622740 139735693903616 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.076522096991539, loss=0.027794282883405685
I0205 11:12:47.848485 139739177916160 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.07258651405572891, loss=0.026807136833667755
I0205 11:13:20.204811 139735693903616 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.07541972398757935, loss=0.02641897462308407
I0205 11:13:25.312309 139919816816448 spec.py:321] Evaluating on the training split.
I0205 11:15:20.257028 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 11:15:23.358088 139919816816448 spec.py:349] Evaluating on the test split.
I0205 11:15:26.372503 139919816816448 submission_runner.py:408] Time since start: 19975.58s, 	Step: 41017, 	{'train/accuracy': 0.993874728679657, 'train/loss': 0.01919444277882576, 'train/mean_average_precision': 0.6714206706921495, 'validation/accuracy': 0.9862105846405029, 'validation/loss': 0.053260985761880875, 'validation/mean_average_precision': 0.23855620487968487, 'validation/num_examples': 43793, 'test/accuracy': 0.9853529334068298, 'test/loss': 0.05703926831483841, 'test/mean_average_precision': 0.22718431605538564, 'test/num_examples': 43793, 'score': 13218.936644792557, 'total_duration': 19975.578807592392, 'accumulated_submission_time': 13218.936644792557, 'accumulated_eval_time': 6753.425145626068, 'accumulated_logging_time': 2.084524631500244}
I0205 11:15:26.396658 139752480773888 logging_writer.py:48] [41017] accumulated_eval_time=6753.425146, accumulated_logging_time=2.084525, accumulated_submission_time=13218.936645, global_step=41017, preemption_count=0, score=13218.936645, test/accuracy=0.985353, test/loss=0.057039, test/mean_average_precision=0.227184, test/num_examples=43793, total_duration=19975.578808, train/accuracy=0.993875, train/loss=0.019194, train/mean_average_precision=0.671421, validation/accuracy=0.986211, validation/loss=0.053261, validation/mean_average_precision=0.238556, validation/num_examples=43793
I0205 11:15:53.471241 139759021496064 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.07681495696306229, loss=0.0286853089928627
I0205 11:16:25.769706 139752480773888 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.07619942724704742, loss=0.02777697890996933
I0205 11:16:58.262270 139759021496064 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.07267501950263977, loss=0.027771584689617157
I0205 11:17:30.518611 139752480773888 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.08003830909729004, loss=0.02751273661851883
I0205 11:18:03.098611 139759021496064 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.0741315558552742, loss=0.025373833253979683
I0205 11:18:34.957293 139752480773888 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.0762593075633049, loss=0.02734428457915783
I0205 11:19:06.804301 139759021496064 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.0895400121808052, loss=0.028180796653032303
I0205 11:19:26.414299 139919816816448 spec.py:321] Evaluating on the training split.
I0205 11:21:18.057561 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 11:21:21.165105 139919816816448 spec.py:349] Evaluating on the test split.
I0205 11:21:24.227876 139919816816448 submission_runner.py:408] Time since start: 20333.43s, 	Step: 41761, 	{'train/accuracy': 0.994343101978302, 'train/loss': 0.0179709792137146, 'train/mean_average_precision': 0.6866316394471659, 'validation/accuracy': 0.9861910939216614, 'validation/loss': 0.05349453166127205, 'validation/mean_average_precision': 0.24149266884140586, 'validation/num_examples': 43793, 'test/accuracy': 0.9853084683418274, 'test/loss': 0.05730481818318367, 'test/mean_average_precision': 0.2291360045914827, 'test/num_examples': 43793, 'score': 13458.923202037811, 'total_duration': 20333.43417596817, 'accumulated_submission_time': 13458.923202037811, 'accumulated_eval_time': 6871.238674879074, 'accumulated_logging_time': 2.11984920501709}
I0205 11:21:24.251399 139735693903616 logging_writer.py:48] [41761] accumulated_eval_time=6871.238675, accumulated_logging_time=2.119849, accumulated_submission_time=13458.923202, global_step=41761, preemption_count=0, score=13458.923202, test/accuracy=0.985308, test/loss=0.057305, test/mean_average_precision=0.229136, test/num_examples=43793, total_duration=20333.434176, train/accuracy=0.994343, train/loss=0.017971, train/mean_average_precision=0.686632, validation/accuracy=0.986191, validation/loss=0.053495, validation/mean_average_precision=0.241493, validation/num_examples=43793
I0205 11:21:38.253427 139739177916160 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.08566185086965561, loss=0.028771260753273964
I0205 11:22:12.421700 139735693903616 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.07321953773498535, loss=0.027301784604787827
I0205 11:22:44.541812 139739177916160 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.06405391544103622, loss=0.02526981383562088
I0205 11:23:16.728119 139735693903616 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.07103621959686279, loss=0.026822511106729507
I0205 11:23:49.083312 139739177916160 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.07718189805746078, loss=0.025326089933514595
I0205 11:24:21.464750 139735693903616 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.07023468613624573, loss=0.026036709547042847
I0205 11:24:53.990886 139739177916160 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.08915292471647263, loss=0.02729407511651516
I0205 11:25:24.512075 139919816816448 spec.py:321] Evaluating on the training split.
I0205 11:27:11.916043 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 11:27:14.974403 139919816816448 spec.py:349] Evaluating on the test split.
I0205 11:27:18.071988 139919816816448 submission_runner.py:408] Time since start: 20687.28s, 	Step: 42496, 	{'train/accuracy': 0.9952325224876404, 'train/loss': 0.015472352504730225, 'train/mean_average_precision': 0.7526896612136231, 'validation/accuracy': 0.9862304329872131, 'validation/loss': 0.05380275472998619, 'validation/mean_average_precision': 0.23870606578947065, 'validation/num_examples': 43793, 'test/accuracy': 0.985352098941803, 'test/loss': 0.05765301361680031, 'test/mean_average_precision': 0.2288912827762337, 'test/num_examples': 43793, 'score': 13699.152914047241, 'total_duration': 20687.278291463852, 'accumulated_submission_time': 13699.152914047241, 'accumulated_eval_time': 6984.798540115356, 'accumulated_logging_time': 2.1546361446380615}
I0205 11:27:18.096801 139752296675072 logging_writer.py:48] [42496] accumulated_eval_time=6984.798540, accumulated_logging_time=2.154636, accumulated_submission_time=13699.152914, global_step=42496, preemption_count=0, score=13699.152914, test/accuracy=0.985352, test/loss=0.057653, test/mean_average_precision=0.228891, test/num_examples=43793, total_duration=20687.278291, train/accuracy=0.995233, train/loss=0.015472, train/mean_average_precision=0.752690, validation/accuracy=0.986230, validation/loss=0.053803, validation/mean_average_precision=0.238706, validation/num_examples=43793
I0205 11:27:19.725878 139752480773888 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.0820336565375328, loss=0.025488395243883133
I0205 11:27:52.005539 139752296675072 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.08998210728168488, loss=0.02706790901720524
I0205 11:28:24.289351 139752480773888 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.0895179957151413, loss=0.028001684695482254
I0205 11:28:56.727832 139752296675072 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.07221214473247528, loss=0.025870705023407936
I0205 11:29:29.492768 139752480773888 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.11570709198713303, loss=0.02672005631029606
I0205 11:30:01.708139 139752296675072 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.08124841749668121, loss=0.026944903656840324
I0205 11:30:34.911816 139752480773888 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.07938161492347717, loss=0.025091800838708878
I0205 11:31:07.784609 139752296675072 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.08838702738285065, loss=0.025552714243531227
I0205 11:31:18.300394 139919816816448 spec.py:321] Evaluating on the training split.
I0205 11:33:11.725979 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 11:33:14.811381 139919816816448 spec.py:349] Evaluating on the test split.
I0205 11:33:17.844071 139919816816448 submission_runner.py:408] Time since start: 21047.05s, 	Step: 43233, 	{'train/accuracy': 0.9956064820289612, 'train/loss': 0.014405405148863792, 'train/mean_average_precision': 0.7794564332097744, 'validation/accuracy': 0.9862028360366821, 'validation/loss': 0.0545261949300766, 'validation/mean_average_precision': 0.23263841615259878, 'validation/num_examples': 43793, 'test/accuracy': 0.9852619171142578, 'test/loss': 0.05838746577501297, 'test/mean_average_precision': 0.22563736970975654, 'test/num_examples': 43793, 'score': 13939.323972702026, 'total_duration': 21047.05036020279, 'accumulated_submission_time': 13939.323972702026, 'accumulated_eval_time': 7104.342164516449, 'accumulated_logging_time': 2.190378427505493}
I0205 11:33:17.868324 139735693903616 logging_writer.py:48] [43233] accumulated_eval_time=7104.342165, accumulated_logging_time=2.190378, accumulated_submission_time=13939.323973, global_step=43233, preemption_count=0, score=13939.323973, test/accuracy=0.985262, test/loss=0.058387, test/mean_average_precision=0.225637, test/num_examples=43793, total_duration=21047.050360, train/accuracy=0.995606, train/loss=0.014405, train/mean_average_precision=0.779456, validation/accuracy=0.986203, validation/loss=0.054526, validation/mean_average_precision=0.232638, validation/num_examples=43793
I0205 11:33:39.695299 139739177916160 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.07567977160215378, loss=0.026963209733366966
I0205 11:34:11.815626 139735693903616 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.0755610540509224, loss=0.025771085172891617
I0205 11:34:44.143871 139739177916160 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.07419008016586304, loss=0.025899404659867287
I0205 11:35:16.569903 139735693903616 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.08538325875997543, loss=0.02614777162671089
I0205 11:35:48.983575 139739177916160 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.08300483226776123, loss=0.02718656323850155
I0205 11:36:21.165752 139735693903616 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.082553431391716, loss=0.02810359001159668
I0205 11:36:53.914300 139739177916160 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.08061523735523224, loss=0.02565176971256733
I0205 11:37:17.862288 139919816816448 spec.py:321] Evaluating on the training split.
I0205 11:39:10.939231 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 11:39:14.038615 139919816816448 spec.py:349] Evaluating on the test split.
I0205 11:39:17.094574 139919816816448 submission_runner.py:408] Time since start: 21406.30s, 	Step: 43974, 	{'train/accuracy': 0.9957359433174133, 'train/loss': 0.01439312007278204, 'train/mean_average_precision': 0.762248438689672, 'validation/accuracy': 0.986229658126831, 'validation/loss': 0.05440077558159828, 'validation/mean_average_precision': 0.23389338677186755, 'validation/num_examples': 43793, 'test/accuracy': 0.9852930903434753, 'test/loss': 0.05838407203555107, 'test/mean_average_precision': 0.22733958924486009, 'test/num_examples': 43793, 'score': 14179.282273769379, 'total_duration': 21406.30087685585, 'accumulated_submission_time': 14179.282273769379, 'accumulated_eval_time': 7223.574427843094, 'accumulated_logging_time': 2.2259573936462402}
I0205 11:39:17.119305 139752296675072 logging_writer.py:48] [43974] accumulated_eval_time=7223.574428, accumulated_logging_time=2.225957, accumulated_submission_time=14179.282274, global_step=43974, preemption_count=0, score=14179.282274, test/accuracy=0.985293, test/loss=0.058384, test/mean_average_precision=0.227340, test/num_examples=43793, total_duration=21406.300877, train/accuracy=0.995736, train/loss=0.014393, train/mean_average_precision=0.762248, validation/accuracy=0.986230, validation/loss=0.054401, validation/mean_average_precision=0.233893, validation/num_examples=43793
I0205 11:39:25.832087 139759021496064 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.08389106392860413, loss=0.027282103896141052
I0205 11:39:57.962067 139752296675072 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.07636934518814087, loss=0.026264123618602753
I0205 11:40:30.965132 139759021496064 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.10291586071252823, loss=0.028691761195659637
I0205 11:41:03.936805 139752296675072 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.09825500100851059, loss=0.028065472841262817
I0205 11:41:36.395329 139759021496064 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.08422260731458664, loss=0.02573511376976967
I0205 11:42:08.728808 139752296675072 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.07851272821426392, loss=0.02512544021010399
I0205 11:42:41.076901 139759021496064 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.08176147937774658, loss=0.02640153281390667
I0205 11:43:13.302832 139752296675072 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.08411316573619843, loss=0.02548510581254959
I0205 11:43:17.245815 139919816816448 spec.py:321] Evaluating on the training split.
I0205 11:45:10.299279 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 11:45:13.353661 139919816816448 spec.py:349] Evaluating on the test split.
I0205 11:45:16.325448 139919816816448 submission_runner.py:408] Time since start: 21765.53s, 	Step: 44713, 	{'train/accuracy': 0.9954484105110168, 'train/loss': 0.015085551887750626, 'train/mean_average_precision': 0.7581501313243499, 'validation/accuracy': 0.9861987829208374, 'validation/loss': 0.054727304726839066, 'validation/mean_average_precision': 0.2367285787323215, 'validation/num_examples': 43793, 'test/accuracy': 0.9853048920631409, 'test/loss': 0.05852444842457771, 'test/mean_average_precision': 0.22815882025587925, 'test/num_examples': 43793, 'score': 14419.376125335693, 'total_duration': 21765.531745910645, 'accumulated_submission_time': 14419.376125335693, 'accumulated_eval_time': 7342.6540105342865, 'accumulated_logging_time': 2.2634549140930176}
I0205 11:45:16.349514 139735693903616 logging_writer.py:48] [44713] accumulated_eval_time=7342.654011, accumulated_logging_time=2.263455, accumulated_submission_time=14419.376125, global_step=44713, preemption_count=0, score=14419.376125, test/accuracy=0.985305, test/loss=0.058524, test/mean_average_precision=0.228159, test/num_examples=43793, total_duration=21765.531746, train/accuracy=0.995448, train/loss=0.015086, train/mean_average_precision=0.758150, validation/accuracy=0.986199, validation/loss=0.054727, validation/mean_average_precision=0.236729, validation/num_examples=43793
I0205 11:45:44.861255 139739177916160 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.08384329825639725, loss=0.024216895923018456
I0205 11:46:17.018415 139735693903616 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.08815500140190125, loss=0.02584165334701538
I0205 11:46:49.126050 139739177916160 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.09044463187456131, loss=0.026170041412115097
I0205 11:47:21.018979 139735693903616 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.0780811533331871, loss=0.026289960369467735
I0205 11:47:53.428903 139739177916160 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.08405479043722153, loss=0.026555052027106285
I0205 11:48:25.895607 139735693903616 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.07756631821393967, loss=0.024433569982647896
I0205 11:48:58.285140 139739177916160 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.09188514947891235, loss=0.025429612025618553
I0205 11:49:16.382143 139919816816448 spec.py:321] Evaluating on the training split.
I0205 11:51:05.426041 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 11:51:08.592433 139919816816448 spec.py:349] Evaluating on the test split.
I0205 11:51:11.637559 139919816816448 submission_runner.py:408] Time since start: 22120.84s, 	Step: 45457, 	{'train/accuracy': 0.9950907230377197, 'train/loss': 0.015709124505519867, 'train/mean_average_precision': 0.7374597770421244, 'validation/accuracy': 0.9861001372337341, 'validation/loss': 0.05494698882102966, 'validation/mean_average_precision': 0.2338895422966686, 'validation/num_examples': 43793, 'test/accuracy': 0.9852097034454346, 'test/loss': 0.058845750987529755, 'test/mean_average_precision': 0.22120940067768596, 'test/num_examples': 43793, 'score': 14659.376512050629, 'total_duration': 22120.843861818314, 'accumulated_submission_time': 14659.376512050629, 'accumulated_eval_time': 7457.90939617157, 'accumulated_logging_time': 2.2987446784973145}
I0205 11:51:11.663228 139752480773888 logging_writer.py:48] [45457] accumulated_eval_time=7457.909396, accumulated_logging_time=2.298745, accumulated_submission_time=14659.376512, global_step=45457, preemption_count=0, score=14659.376512, test/accuracy=0.985210, test/loss=0.058846, test/mean_average_precision=0.221209, test/num_examples=43793, total_duration=22120.843862, train/accuracy=0.995091, train/loss=0.015709, train/mean_average_precision=0.737460, validation/accuracy=0.986100, validation/loss=0.054947, validation/mean_average_precision=0.233890, validation/num_examples=43793
I0205 11:51:26.157229 139759021496064 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.07905818521976471, loss=0.02567448280751705
I0205 11:51:58.968936 139752480773888 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.0927388146519661, loss=0.02664123848080635
I0205 11:52:31.446213 139759021496064 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.08547211438417435, loss=0.025617482140660286
I0205 11:53:03.691553 139752480773888 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.09496305882930756, loss=0.02708830125629902
I0205 11:53:35.793669 139759021496064 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.08836322277784348, loss=0.02817150577902794
I0205 11:54:07.898491 139752480773888 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.08486378192901611, loss=0.025985101237893105
I0205 11:54:40.005210 139759021496064 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.09307891875505447, loss=0.024884456768631935
I0205 11:55:11.885279 139919816816448 spec.py:321] Evaluating on the training split.
I0205 11:57:01.173893 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 11:57:04.340082 139919816816448 spec.py:349] Evaluating on the test split.
I0205 11:57:07.340224 139919816816448 submission_runner.py:408] Time since start: 22476.55s, 	Step: 46200, 	{'train/accuracy': 0.9943767786026001, 'train/loss': 0.017387907952070236, 'train/mean_average_precision': 0.7149168519041117, 'validation/accuracy': 0.9862478971481323, 'validation/loss': 0.05581240355968475, 'validation/mean_average_precision': 0.2359758331316309, 'validation/num_examples': 43793, 'test/accuracy': 0.9853684902191162, 'test/loss': 0.05969163030385971, 'test/mean_average_precision': 0.2266012596243111, 'test/num_examples': 43793, 'score': 14899.567381620407, 'total_duration': 22476.54651904106, 'accumulated_submission_time': 14899.567381620407, 'accumulated_eval_time': 7573.364289522171, 'accumulated_logging_time': 2.335345506668091}
I0205 11:57:07.369072 139735693903616 logging_writer.py:48] [46200] accumulated_eval_time=7573.364290, accumulated_logging_time=2.335346, accumulated_submission_time=14899.567382, global_step=46200, preemption_count=0, score=14899.567382, test/accuracy=0.985368, test/loss=0.059692, test/mean_average_precision=0.226601, test/num_examples=43793, total_duration=22476.546519, train/accuracy=0.994377, train/loss=0.017388, train/mean_average_precision=0.714917, validation/accuracy=0.986248, validation/loss=0.055812, validation/mean_average_precision=0.235976, validation/num_examples=43793
I0205 11:57:07.732148 139752296675072 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.07978462427854538, loss=0.025195235386490822
I0205 11:57:39.658403 139735693903616 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.08810389786958694, loss=0.026370979845523834
I0205 11:58:12.074568 139752296675072 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.08057983964681625, loss=0.024688683450222015
I0205 11:58:44.291571 139735693903616 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.08634275197982788, loss=0.02364254556596279
I0205 11:59:16.632979 139752296675072 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.11921095103025436, loss=0.026634708046913147
I0205 11:59:48.774487 139735693903616 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.08676859736442566, loss=0.025708578526973724
I0205 12:00:21.032377 139752296675072 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.0824524387717247, loss=0.025232071056962013
I0205 12:00:53.240281 139735693903616 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.0892261415719986, loss=0.024408390745520592
I0205 12:01:07.590225 139919816816448 spec.py:321] Evaluating on the training split.
I0205 12:02:57.146509 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 12:03:00.238098 139919816816448 spec.py:349] Evaluating on the test split.
I0205 12:03:03.570346 139919816816448 submission_runner.py:408] Time since start: 22832.78s, 	Step: 46945, 	{'train/accuracy': 0.9943929314613342, 'train/loss': 0.017379935830831528, 'train/mean_average_precision': 0.7088835674631668, 'validation/accuracy': 0.9861464500427246, 'validation/loss': 0.05589861050248146, 'validation/mean_average_precision': 0.23579998318025813, 'validation/num_examples': 43793, 'test/accuracy': 0.9852564930915833, 'test/loss': 0.059784576296806335, 'test/mean_average_precision': 0.22329177658512858, 'test/num_examples': 43793, 'score': 15139.756244659424, 'total_duration': 22832.776628017426, 'accumulated_submission_time': 15139.756244659424, 'accumulated_eval_time': 7689.344358444214, 'accumulated_logging_time': 2.3755228519439697}
I0205 12:03:03.599481 139739177916160 logging_writer.py:48] [46945] accumulated_eval_time=7689.344358, accumulated_logging_time=2.375523, accumulated_submission_time=15139.756245, global_step=46945, preemption_count=0, score=15139.756245, test/accuracy=0.985256, test/loss=0.059785, test/mean_average_precision=0.223292, test/num_examples=43793, total_duration=22832.776628, train/accuracy=0.994393, train/loss=0.017380, train/mean_average_precision=0.708884, validation/accuracy=0.986146, validation/loss=0.055899, validation/mean_average_precision=0.235800, validation/num_examples=43793
I0205 12:03:22.361632 139752480773888 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.07719564437866211, loss=0.025251952931284904
I0205 12:03:54.377132 139739177916160 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.10037525743246078, loss=0.026078499853610992
I0205 12:04:26.494534 139752480773888 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.07899322360754013, loss=0.023887619376182556
I0205 12:04:58.500513 139739177916160 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.09382376074790955, loss=0.025533538311719894
I0205 12:05:30.933732 139752480773888 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.08382132649421692, loss=0.02572130598127842
I0205 12:06:02.997720 139739177916160 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.09380502998828888, loss=0.0246160589158535
I0205 12:06:35.337681 139752480773888 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.09124819189310074, loss=0.02463829703629017
I0205 12:07:03.783557 139919816816448 spec.py:321] Evaluating on the training split.
I0205 12:08:56.753213 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 12:08:59.822676 139919816816448 spec.py:349] Evaluating on the test split.
I0205 12:09:02.934170 139919816816448 submission_runner.py:408] Time since start: 23192.14s, 	Step: 47690, 	{'train/accuracy': 0.9948909282684326, 'train/loss': 0.016112660989165306, 'train/mean_average_precision': 0.7247512557869674, 'validation/accuracy': 0.9861387014389038, 'validation/loss': 0.056316569447517395, 'validation/mean_average_precision': 0.23525812020997874, 'validation/num_examples': 43793, 'test/accuracy': 0.9851852655410767, 'test/loss': 0.060305699706077576, 'test/mean_average_precision': 0.2267443674197929, 'test/num_examples': 43793, 'score': 15379.908016204834, 'total_duration': 23192.140475034714, 'accumulated_submission_time': 15379.908016204834, 'accumulated_eval_time': 7808.494928121567, 'accumulated_logging_time': 2.4163498878479004}
I0205 12:09:02.958693 139735693903616 logging_writer.py:48] [47690] accumulated_eval_time=7808.494928, accumulated_logging_time=2.416350, accumulated_submission_time=15379.908016, global_step=47690, preemption_count=0, score=15379.908016, test/accuracy=0.985185, test/loss=0.060306, test/mean_average_precision=0.226744, test/num_examples=43793, total_duration=23192.140475, train/accuracy=0.994891, train/loss=0.016113, train/mean_average_precision=0.724751, validation/accuracy=0.986139, validation/loss=0.056317, validation/mean_average_precision=0.235258, validation/num_examples=43793
I0205 12:09:06.548529 139752296675072 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.11151821911334991, loss=0.025376977398991585
I0205 12:09:38.746753 139735693903616 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.0811368077993393, loss=0.02465834841132164
I0205 12:10:10.574457 139752296675072 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.11425920575857162, loss=0.025872929021716118
I0205 12:10:42.465432 139735693903616 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.08290904760360718, loss=0.02489650622010231
I0205 12:11:14.585454 139752296675072 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.08658842742443085, loss=0.022565603256225586
I0205 12:11:47.078335 139735693903616 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.08477238565683365, loss=0.024367380887269974
I0205 12:12:19.299597 139752296675072 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.09463046491146088, loss=0.024512451142072678
I0205 12:12:51.249820 139735693903616 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.09422759711742401, loss=0.02652941644191742
I0205 12:13:02.951504 139919816816448 spec.py:321] Evaluating on the training split.
I0205 12:14:52.639896 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 12:14:55.774795 139919816816448 spec.py:349] Evaluating on the test split.
I0205 12:14:58.836627 139919816816448 submission_runner.py:408] Time since start: 23548.04s, 	Step: 48437, 	{'train/accuracy': 0.9958310127258301, 'train/loss': 0.013746333308517933, 'train/mean_average_precision': 0.7851234003757641, 'validation/accuracy': 0.9861135482788086, 'validation/loss': 0.056668639183044434, 'validation/mean_average_precision': 0.23437270029300178, 'validation/num_examples': 43793, 'test/accuracy': 0.9852004647254944, 'test/loss': 0.060763321816921234, 'test/mean_average_precision': 0.22290742347595474, 'test/num_examples': 43793, 'score': 15619.869005203247, 'total_duration': 23548.04278063774, 'accumulated_submission_time': 15619.869005203247, 'accumulated_eval_time': 7924.379849433899, 'accumulated_logging_time': 2.4527883529663086}
I0205 12:14:58.863099 139739177916160 logging_writer.py:48] [48437] accumulated_eval_time=7924.379849, accumulated_logging_time=2.452788, accumulated_submission_time=15619.869005, global_step=48437, preemption_count=0, score=15619.869005, test/accuracy=0.985200, test/loss=0.060763, test/mean_average_precision=0.222907, test/num_examples=43793, total_duration=23548.042781, train/accuracy=0.995831, train/loss=0.013746, train/mean_average_precision=0.785123, validation/accuracy=0.986114, validation/loss=0.056669, validation/mean_average_precision=0.234373, validation/num_examples=43793
I0205 12:15:19.687668 139759021496064 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.07813426852226257, loss=0.023097658529877663
I0205 12:15:51.730877 139739177916160 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.08497362583875656, loss=0.024361470714211464
I0205 12:16:24.167547 139759021496064 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.0989764854311943, loss=0.026259925216436386
I0205 12:16:56.409113 139739177916160 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.08384844660758972, loss=0.02395493909716606
I0205 12:17:28.654413 139759021496064 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.08425533771514893, loss=0.02342165634036064
I0205 12:18:00.414539 139739177916160 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.08937080949544907, loss=0.025785107165575027
I0205 12:18:32.603494 139759021496064 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.0849565863609314, loss=0.0257883220911026
I0205 12:18:59.122036 139919816816448 spec.py:321] Evaluating on the training split.
I0205 12:20:48.858247 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 12:20:51.902665 139919816816448 spec.py:349] Evaluating on the test split.
I0205 12:20:54.921369 139919816816448 submission_runner.py:408] Time since start: 23904.13s, 	Step: 49184, 	{'train/accuracy': 0.9950827360153198, 'train/loss': 0.015535079874098301, 'train/mean_average_precision': 0.746528199797595, 'validation/accuracy': 0.9861204624176025, 'validation/loss': 0.056217651814222336, 'validation/mean_average_precision': 0.2370487070055823, 'validation/num_examples': 43793, 'test/accuracy': 0.9851077795028687, 'test/loss': 0.060557421296834946, 'test/mean_average_precision': 0.2294661277003323, 'test/num_examples': 43793, 'score': 15860.097087621689, 'total_duration': 23904.12767291069, 'accumulated_submission_time': 15860.097087621689, 'accumulated_eval_time': 8040.179135560989, 'accumulated_logging_time': 2.490166187286377}
I0205 12:20:54.947629 139735693903616 logging_writer.py:48] [49184] accumulated_eval_time=8040.179136, accumulated_logging_time=2.490166, accumulated_submission_time=15860.097088, global_step=49184, preemption_count=0, score=15860.097088, test/accuracy=0.985108, test/loss=0.060557, test/mean_average_precision=0.229466, test/num_examples=43793, total_duration=23904.127673, train/accuracy=0.995083, train/loss=0.015535, train/mean_average_precision=0.746528, validation/accuracy=0.986120, validation/loss=0.056218, validation/mean_average_precision=0.237049, validation/num_examples=43793
I0205 12:21:00.390638 139752296675072 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.08371137082576752, loss=0.0241027120500803
I0205 12:21:32.305361 139735693903616 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.08227834105491638, loss=0.02426726184785366
I0205 12:22:04.200809 139752296675072 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.09250342100858688, loss=0.025032559409737587
I0205 12:22:36.120942 139735693903616 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.0970180407166481, loss=0.024211183190345764
I0205 12:23:08.200295 139752296675072 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.09421300143003464, loss=0.024943694472312927
I0205 12:23:39.994267 139735693903616 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.09034806489944458, loss=0.02373788133263588
I0205 12:24:12.328979 139752296675072 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.0992659479379654, loss=0.0251584704965353
I0205 12:24:44.241054 139735693903616 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.07349897176027298, loss=0.02359970286488533
I0205 12:24:55.058357 139919816816448 spec.py:321] Evaluating on the training split.
I0205 12:26:42.674518 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 12:26:45.762840 139919816816448 spec.py:349] Evaluating on the test split.
I0205 12:26:48.764475 139919816816448 submission_runner.py:408] Time since start: 24257.97s, 	Step: 49935, 	{'train/accuracy': 0.996401846408844, 'train/loss': 0.012614203616976738, 'train/mean_average_precision': 0.8146022634550463, 'validation/accuracy': 0.9861245155334473, 'validation/loss': 0.05769200995564461, 'validation/mean_average_precision': 0.23164453547220965, 'validation/num_examples': 43793, 'test/accuracy': 0.9851545095443726, 'test/loss': 0.06196204572916031, 'test/mean_average_precision': 0.2204592669232147, 'test/num_examples': 43793, 'score': 16100.177204370499, 'total_duration': 24257.970780849457, 'accumulated_submission_time': 16100.177204370499, 'accumulated_eval_time': 8153.885211467743, 'accumulated_logging_time': 2.5272974967956543}
I0205 12:26:48.789062 139752480773888 logging_writer.py:48] [49935] accumulated_eval_time=8153.885211, accumulated_logging_time=2.527297, accumulated_submission_time=16100.177204, global_step=49935, preemption_count=0, score=16100.177204, test/accuracy=0.985155, test/loss=0.061962, test/mean_average_precision=0.220459, test/num_examples=43793, total_duration=24257.970781, train/accuracy=0.996402, train/loss=0.012614, train/mean_average_precision=0.814602, validation/accuracy=0.986125, validation/loss=0.057692, validation/mean_average_precision=0.231645, validation/num_examples=43793
I0205 12:27:10.201524 139759021496064 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.08348629623651505, loss=0.02400568686425686
I0205 12:27:42.325172 139752480773888 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.08891364187002182, loss=0.023750662803649902
I0205 12:28:14.270736 139759021496064 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.09291043877601624, loss=0.024665744975209236
I0205 12:28:45.960206 139752480773888 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.09907440841197968, loss=0.02469213493168354
I0205 12:29:18.327986 139759021496064 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.09136658161878586, loss=0.024732248857617378
I0205 12:29:50.745231 139752480773888 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.08858305960893631, loss=0.024845749139785767
I0205 12:30:23.576641 139759021496064 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.0789431631565094, loss=0.02196887694299221
I0205 12:30:49.006534 139919816816448 spec.py:321] Evaluating on the training split.
I0205 12:32:39.115012 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 12:32:42.162881 139919816816448 spec.py:349] Evaluating on the test split.
I0205 12:32:45.167851 139919816816448 submission_runner.py:408] Time since start: 24614.37s, 	Step: 50680, 	{'train/accuracy': 0.9969805479049683, 'train/loss': 0.011179205030202866, 'train/mean_average_precision': 0.8314752558336872, 'validation/accuracy': 0.9861451983451843, 'validation/loss': 0.05840529128909111, 'validation/mean_average_precision': 0.23081387671847295, 'validation/num_examples': 43793, 'test/accuracy': 0.9852147698402405, 'test/loss': 0.06269458681344986, 'test/mean_average_precision': 0.2215540398824933, 'test/num_examples': 43793, 'score': 16340.363671779633, 'total_duration': 24614.37414741516, 'accumulated_submission_time': 16340.363671779633, 'accumulated_eval_time': 8270.046475887299, 'accumulated_logging_time': 2.562939405441284}
I0205 12:32:45.197127 139735693903616 logging_writer.py:48] [50680] accumulated_eval_time=8270.046476, accumulated_logging_time=2.562939, accumulated_submission_time=16340.363672, global_step=50680, preemption_count=0, score=16340.363672, test/accuracy=0.985215, test/loss=0.062695, test/mean_average_precision=0.221554, test/num_examples=43793, total_duration=24614.374147, train/accuracy=0.996981, train/loss=0.011179, train/mean_average_precision=0.831475, validation/accuracy=0.986145, validation/loss=0.058405, validation/mean_average_precision=0.230814, validation/num_examples=43793
I0205 12:32:52.203773 139752296675072 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.09668989479541779, loss=0.024407705292105675
I0205 12:33:24.973479 139735693903616 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.09115014225244522, loss=0.025675687938928604
I0205 12:33:57.700306 139752296675072 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.08599928766489029, loss=0.02303372696042061
I0205 12:34:29.651462 139735693903616 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.09306363761425018, loss=0.023577343672513962
I0205 12:35:01.659497 139752296675072 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.08800508826971054, loss=0.02367699332535267
I0205 12:35:33.950810 139735693903616 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.08948986977338791, loss=0.023791316896677017
I0205 12:36:06.096347 139752296675072 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.08894912153482437, loss=0.025829385966062546
I0205 12:36:38.060189 139735693903616 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.0943082869052887, loss=0.023774288594722748
I0205 12:36:45.324998 139919816816448 spec.py:321] Evaluating on the training split.
I0205 12:38:32.785225 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 12:38:35.831558 139919816816448 spec.py:349] Evaluating on the test split.
I0205 12:38:38.810930 139919816816448 submission_runner.py:408] Time since start: 24968.02s, 	Step: 51424, 	{'train/accuracy': 0.9964102506637573, 'train/loss': 0.012139641679823399, 'train/mean_average_precision': 0.8212676428096144, 'validation/accuracy': 0.9860566854476929, 'validation/loss': 0.05832161754369736, 'validation/mean_average_precision': 0.226055681241575, 'validation/num_examples': 43793, 'test/accuracy': 0.9851212501525879, 'test/loss': 0.06258596479892731, 'test/mean_average_precision': 0.21948238816098117, 'test/num_examples': 43793, 'score': 16580.46027135849, 'total_duration': 24968.01723909378, 'accumulated_submission_time': 16580.46027135849, 'accumulated_eval_time': 8383.532366275787, 'accumulated_logging_time': 2.60349440574646}
I0205 12:38:38.835611 139739177916160 logging_writer.py:48] [51424] accumulated_eval_time=8383.532366, accumulated_logging_time=2.603494, accumulated_submission_time=16580.460271, global_step=51424, preemption_count=0, score=16580.460271, test/accuracy=0.985121, test/loss=0.062586, test/mean_average_precision=0.219482, test/num_examples=43793, total_duration=24968.017239, train/accuracy=0.996410, train/loss=0.012140, train/mean_average_precision=0.821268, validation/accuracy=0.986057, validation/loss=0.058322, validation/mean_average_precision=0.226056, validation/num_examples=43793
I0205 12:39:03.430877 139752480773888 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.08832915872335434, loss=0.0253160260617733
I0205 12:39:35.540251 139739177916160 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.0890670120716095, loss=0.02293422259390354
I0205 12:40:07.883744 139752480773888 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.09685607999563217, loss=0.023651933297514915
I0205 12:40:39.803554 139739177916160 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.11044508218765259, loss=0.025423016399145126
I0205 12:41:12.407170 139752480773888 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.0905764251947403, loss=0.02466013841331005
I0205 12:41:45.510147 139739177916160 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.09527386724948883, loss=0.024259956553578377
I0205 12:42:17.148241 139752480773888 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.08797638863325119, loss=0.02557307854294777
I0205 12:42:38.909289 139919816816448 spec.py:321] Evaluating on the training split.
I0205 12:44:26.663721 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 12:44:29.729324 139919816816448 spec.py:349] Evaluating on the test split.
I0205 12:44:32.751104 139919816816448 submission_runner.py:408] Time since start: 25321.96s, 	Step: 52170, 	{'train/accuracy': 0.9960031509399414, 'train/loss': 0.013010595925152302, 'train/mean_average_precision': 0.7998794085137176, 'validation/accuracy': 0.9860563278198242, 'validation/loss': 0.05870188772678375, 'validation/mean_average_precision': 0.22956517435367943, 'validation/num_examples': 43793, 'test/accuracy': 0.98509681224823, 'test/loss': 0.0628611296415329, 'test/mean_average_precision': 0.21980225443098933, 'test/num_examples': 43793, 'score': 16820.501957178116, 'total_duration': 25321.957409858704, 'accumulated_submission_time': 16820.501957178116, 'accumulated_eval_time': 8497.374136447906, 'accumulated_logging_time': 2.6400325298309326}
I0205 12:44:32.776206 139752296675072 logging_writer.py:48] [52170] accumulated_eval_time=8497.374136, accumulated_logging_time=2.640033, accumulated_submission_time=16820.501957, global_step=52170, preemption_count=0, score=16820.501957, test/accuracy=0.985097, test/loss=0.062861, test/mean_average_precision=0.219802, test/num_examples=43793, total_duration=25321.957410, train/accuracy=0.996003, train/loss=0.013011, train/mean_average_precision=0.799879, validation/accuracy=0.986056, validation/loss=0.058702, validation/mean_average_precision=0.229565, validation/num_examples=43793
I0205 12:44:42.597454 139759021496064 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.09418979287147522, loss=0.023554621264338493
I0205 12:45:14.385752 139752296675072 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.09433475881814957, loss=0.02300124615430832
I0205 12:45:46.343802 139759021496064 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.08189070969820023, loss=0.02319232188165188
I0205 12:46:18.295437 139752296675072 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.0872875526547432, loss=0.02296348288655281
I0205 12:46:50.090544 139759021496064 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.12041174620389938, loss=0.026853587478399277
I0205 12:47:21.933426 139752296675072 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.08806220442056656, loss=0.024608010426163673
I0205 12:47:53.923187 139759021496064 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.09294389188289642, loss=0.023972012102603912
I0205 12:48:26.850293 139752296675072 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.09468645602464676, loss=0.023797383531928062
I0205 12:48:33.043491 139919816816448 spec.py:321] Evaluating on the training split.
I0205 12:50:25.338887 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 12:50:28.419674 139919816816448 spec.py:349] Evaluating on the test split.
I0205 12:50:31.397435 139919816816448 submission_runner.py:408] Time since start: 25680.60s, 	Step: 52920, 	{'train/accuracy': 0.9953522682189941, 'train/loss': 0.014262083917856216, 'train/mean_average_precision': 0.7881900013083352, 'validation/accuracy': 0.9861147403717041, 'validation/loss': 0.059349674731492996, 'validation/mean_average_precision': 0.22541223952187306, 'validation/num_examples': 43793, 'test/accuracy': 0.9851848483085632, 'test/loss': 0.06345284730195999, 'test/mean_average_precision': 0.2199008821173299, 'test/num_examples': 43793, 'score': 17060.735783100128, 'total_duration': 25680.60373020172, 'accumulated_submission_time': 17060.735783100128, 'accumulated_eval_time': 8615.72803235054, 'accumulated_logging_time': 2.6779961585998535}
I0205 12:50:31.423852 139735693903616 logging_writer.py:48] [52920] accumulated_eval_time=8615.728032, accumulated_logging_time=2.677996, accumulated_submission_time=17060.735783, global_step=52920, preemption_count=0, score=17060.735783, test/accuracy=0.985185, test/loss=0.063453, test/mean_average_precision=0.219901, test/num_examples=43793, total_duration=25680.603730, train/accuracy=0.995352, train/loss=0.014262, train/mean_average_precision=0.788190, validation/accuracy=0.986115, validation/loss=0.059350, validation/mean_average_precision=0.225412, validation/num_examples=43793
I0205 12:50:57.154961 139752480773888 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.08691298961639404, loss=0.02224266529083252
I0205 12:51:29.715760 139735693903616 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.09627153724431992, loss=0.023082692176103592
I0205 12:52:01.809936 139752480773888 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.09008185565471649, loss=0.024038685485720634
I0205 12:52:33.777635 139735693903616 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.10337499529123306, loss=0.023956051096320152
I0205 12:53:05.746708 139752480773888 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.09261231124401093, loss=0.021411150693893433
I0205 12:53:37.593734 139735693903616 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.08506359159946442, loss=0.02335985377430916
I0205 12:54:09.700016 139752480773888 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.08451998978853226, loss=0.023783784359693527
I0205 12:54:31.577415 139919816816448 spec.py:321] Evaluating on the training split.
I0205 12:56:16.556519 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 12:56:19.648989 139919816816448 spec.py:349] Evaluating on the test split.
I0205 12:56:22.655144 139919816816448 submission_runner.py:408] Time since start: 26031.86s, 	Step: 53669, 	{'train/accuracy': 0.9954304099082947, 'train/loss': 0.014057978056371212, 'train/mean_average_precision': 0.785213941239531, 'validation/accuracy': 0.9861135482788086, 'validation/loss': 0.05896909534931183, 'validation/mean_average_precision': 0.22280262369836915, 'validation/num_examples': 43793, 'test/accuracy': 0.9851098656654358, 'test/loss': 0.06314091384410858, 'test/mean_average_precision': 0.218001164903323, 'test/num_examples': 43793, 'score': 17300.85765480995, 'total_duration': 26031.861436128616, 'accumulated_submission_time': 17300.85765480995, 'accumulated_eval_time': 8726.805708408356, 'accumulated_logging_time': 2.715599775314331}
I0205 12:56:22.680836 139739177916160 logging_writer.py:48] [53669] accumulated_eval_time=8726.805708, accumulated_logging_time=2.715600, accumulated_submission_time=17300.857655, global_step=53669, preemption_count=0, score=17300.857655, test/accuracy=0.985110, test/loss=0.063141, test/mean_average_precision=0.218001, test/num_examples=43793, total_duration=26031.861436, train/accuracy=0.995430, train/loss=0.014058, train/mean_average_precision=0.785214, validation/accuracy=0.986114, validation/loss=0.058969, validation/mean_average_precision=0.222803, validation/num_examples=43793
I0205 12:56:33.365538 139759021496064 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.12221372127532959, loss=0.02331903949379921
I0205 12:57:05.653321 139739177916160 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.10064303874969482, loss=0.02389003522694111
I0205 12:57:38.197340 139759021496064 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.10318844765424728, loss=0.022173935547471046
I0205 12:58:10.765198 139739177916160 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.09562721848487854, loss=0.023495489731431007
I0205 12:58:43.315620 139759021496064 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.08670729398727417, loss=0.022071722894906998
I0205 12:59:15.764063 139739177916160 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.08941800892353058, loss=0.023033468052744865
I0205 12:59:47.588222 139759021496064 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.0886276364326477, loss=0.023512426763772964
I0205 13:00:19.373943 139739177916160 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.08098433911800385, loss=0.023923048749566078
I0205 13:00:22.881490 139919816816448 spec.py:321] Evaluating on the training split.
I0205 13:02:09.891455 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 13:02:12.885775 139919816816448 spec.py:349] Evaluating on the test split.
I0205 13:02:15.837131 139919816816448 submission_runner.py:408] Time since start: 26385.04s, 	Step: 54412, 	{'train/accuracy': 0.995689332485199, 'train/loss': 0.013544830493628979, 'train/mean_average_precision': 0.7969386333438457, 'validation/accuracy': 0.9860498309135437, 'validation/loss': 0.059502821415662766, 'validation/mean_average_precision': 0.22839872557557458, 'validation/num_examples': 43793, 'test/accuracy': 0.9851149320602417, 'test/loss': 0.06383056193590164, 'test/mean_average_precision': 0.21641405845233108, 'test/num_examples': 43793, 'score': 17541.0244076252, 'total_duration': 26385.043430566788, 'accumulated_submission_time': 17541.0244076252, 'accumulated_eval_time': 8839.761295318604, 'accumulated_logging_time': 2.7525038719177246}
I0205 13:02:15.864144 139735693903616 logging_writer.py:48] [54412] accumulated_eval_time=8839.761295, accumulated_logging_time=2.752504, accumulated_submission_time=17541.024408, global_step=54412, preemption_count=0, score=17541.024408, test/accuracy=0.985115, test/loss=0.063831, test/mean_average_precision=0.216414, test/num_examples=43793, total_duration=26385.043431, train/accuracy=0.995689, train/loss=0.013545, train/mean_average_precision=0.796939, validation/accuracy=0.986050, validation/loss=0.059503, validation/mean_average_precision=0.228399, validation/num_examples=43793
I0205 13:02:44.209201 139752296675072 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.0806027427315712, loss=0.02271188050508499
I0205 13:03:16.185976 139735693903616 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.08928406983613968, loss=0.023322630673646927
I0205 13:03:48.085340 139752296675072 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.09769130498170853, loss=0.024795247241854668
I0205 13:04:19.809519 139735693903616 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.08563936501741409, loss=0.023277567699551582
I0205 13:04:51.364354 139752296675072 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.10144159942865372, loss=0.02242239937186241
I0205 13:05:23.019993 139735693903616 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.09303514659404755, loss=0.02324017509818077
I0205 13:05:55.050442 139752296675072 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.10480985790491104, loss=0.022219380363821983
I0205 13:06:16.100764 139919816816448 spec.py:321] Evaluating on the training split.
I0205 13:08:03.837934 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 13:08:07.115888 139919816816448 spec.py:349] Evaluating on the test split.
I0205 13:08:10.321956 139919816816448 submission_runner.py:408] Time since start: 26739.53s, 	Step: 55167, 	{'train/accuracy': 0.9960854053497314, 'train/loss': 0.012709268368780613, 'train/mean_average_precision': 0.8059383088169321, 'validation/accuracy': 0.9860132932662964, 'validation/loss': 0.05940846726298332, 'validation/mean_average_precision': 0.22421516606585493, 'validation/num_examples': 43793, 'test/accuracy': 0.9850315451622009, 'test/loss': 0.06362029165029526, 'test/mean_average_precision': 0.2174672035747131, 'test/num_examples': 43793, 'score': 17781.22932624817, 'total_duration': 26739.52824115753, 'accumulated_submission_time': 17781.22932624817, 'accumulated_eval_time': 8953.982422113419, 'accumulated_logging_time': 2.7906315326690674}
I0205 13:08:10.352071 139739177916160 logging_writer.py:48] [55167] accumulated_eval_time=8953.982422, accumulated_logging_time=2.790632, accumulated_submission_time=17781.229326, global_step=55167, preemption_count=0, score=17781.229326, test/accuracy=0.985032, test/loss=0.063620, test/mean_average_precision=0.217467, test/num_examples=43793, total_duration=26739.528241, train/accuracy=0.996085, train/loss=0.012709, train/mean_average_precision=0.805938, validation/accuracy=0.986013, validation/loss=0.059408, validation/mean_average_precision=0.224215, validation/num_examples=43793
I0205 13:08:21.489058 139759021496064 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.0973418653011322, loss=0.023409364745020866
I0205 13:08:54.254853 139739177916160 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.08732683956623077, loss=0.02155531942844391
I0205 13:09:26.521355 139759021496064 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.08481759577989578, loss=0.022340470924973488
I0205 13:09:59.049599 139739177916160 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.07800323516130447, loss=0.022996118292212486
I0205 13:10:31.620658 139759021496064 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.07629662752151489, loss=0.021077997982501984
I0205 13:11:04.081097 139739177916160 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.0936221107840538, loss=0.023069357499480247
I0205 13:11:36.745514 139759021496064 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.09578637778759003, loss=0.0225679948925972
I0205 13:12:09.288413 139739177916160 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.10009828209877014, loss=0.022649692371487617
I0205 13:12:10.628375 139919816816448 spec.py:321] Evaluating on the training split.
I0205 13:14:01.361926 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 13:14:04.656163 139919816816448 spec.py:349] Evaluating on the test split.
I0205 13:14:07.849605 139919816816448 submission_runner.py:408] Time since start: 27097.06s, 	Step: 55905, 	{'train/accuracy': 0.9968048930168152, 'train/loss': 0.011059361509978771, 'train/mean_average_precision': 0.8559029305644175, 'validation/accuracy': 0.9860246181488037, 'validation/loss': 0.06033738702535629, 'validation/mean_average_precision': 0.2279856110308997, 'validation/num_examples': 43793, 'test/accuracy': 0.9850290417671204, 'test/loss': 0.06488261371850967, 'test/mean_average_precision': 0.21261188809277975, 'test/num_examples': 43793, 'score': 18021.468203783035, 'total_duration': 27097.05589222908, 'accumulated_submission_time': 18021.468203783035, 'accumulated_eval_time': 9071.203595876694, 'accumulated_logging_time': 2.8327012062072754}
I0205 13:14:07.877575 139752296675072 logging_writer.py:48] [55905] accumulated_eval_time=9071.203596, accumulated_logging_time=2.832701, accumulated_submission_time=18021.468204, global_step=55905, preemption_count=0, score=18021.468204, test/accuracy=0.985029, test/loss=0.064883, test/mean_average_precision=0.212612, test/num_examples=43793, total_duration=27097.055892, train/accuracy=0.996805, train/loss=0.011059, train/mean_average_precision=0.855903, validation/accuracy=0.986025, validation/loss=0.060337, validation/mean_average_precision=0.227986, validation/num_examples=43793
I0205 13:14:38.917193 139752480773888 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.0927787572145462, loss=0.02291588857769966
I0205 13:15:11.502831 139752296675072 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.08186527341604233, loss=0.022022753953933716
I0205 13:15:44.064646 139752480773888 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.1062651127576828, loss=0.022901462391018867
I0205 13:16:16.342784 139752296675072 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.08317555487155914, loss=0.021303588524460793
I0205 13:16:48.730786 139752480773888 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.0802275687456131, loss=0.02066829614341259
I0205 13:17:20.670829 139752296675072 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.09144338220357895, loss=0.02307426556944847
I0205 13:17:52.376468 139752480773888 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.0995042696595192, loss=0.021304579451680183
I0205 13:18:08.063590 139919816816448 spec.py:321] Evaluating on the training split.
I0205 13:19:53.022828 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 13:19:56.098142 139919816816448 spec.py:349] Evaluating on the test split.
I0205 13:19:59.062156 139919816816448 submission_runner.py:408] Time since start: 27448.27s, 	Step: 56650, 	{'train/accuracy': 0.9976639151573181, 'train/loss': 0.009552651084959507, 'train/mean_average_precision': 0.8787869832304159, 'validation/accuracy': 0.985992968082428, 'validation/loss': 0.060625217854976654, 'validation/mean_average_precision': 0.22720330974813813, 'validation/num_examples': 43793, 'test/accuracy': 0.9850433468818665, 'test/loss': 0.06487562507390976, 'test/mean_average_precision': 0.21501642913040128, 'test/num_examples': 43793, 'score': 18261.61960697174, 'total_duration': 27448.26845598221, 'accumulated_submission_time': 18261.61960697174, 'accumulated_eval_time': 9182.202111959457, 'accumulated_logging_time': 2.873664617538452}
I0205 13:19:59.088826 139735693903616 logging_writer.py:48] [56650] accumulated_eval_time=9182.202112, accumulated_logging_time=2.873665, accumulated_submission_time=18261.619607, global_step=56650, preemption_count=0, score=18261.619607, test/accuracy=0.985043, test/loss=0.064876, test/mean_average_precision=0.215016, test/num_examples=43793, total_duration=27448.268456, train/accuracy=0.997664, train/loss=0.009553, train/mean_average_precision=0.878787, validation/accuracy=0.985993, validation/loss=0.060625, validation/mean_average_precision=0.227203, validation/num_examples=43793
I0205 13:20:15.647044 139739177916160 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.08090250939130783, loss=0.021171636879444122
I0205 13:20:48.048584 139735693903616 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.08062764257192612, loss=0.022780125960707664
I0205 13:21:20.169854 139739177916160 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.0830763429403305, loss=0.02226937748491764
I0205 13:21:51.994473 139735693903616 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.09859038889408112, loss=0.02251918613910675
I0205 13:22:23.971619 139739177916160 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.08276981860399246, loss=0.02210087701678276
I0205 13:22:55.923014 139735693903616 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.08766919374465942, loss=0.02287379652261734
I0205 13:23:27.930755 139739177916160 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.09160783141851425, loss=0.023367108777165413
I0205 13:23:34.682885 139735693903616 logging_writer.py:48] [57322] global_step=57322, preemption_count=0, score=18477.167250
I0205 13:23:36.640944 139919816816448 checkpoints.py:490] Saving checkpoint at step: 57322
I0205 13:23:36.759459 139919816816448 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_1/ogbg_jax/trial_2/checkpoint_57322
I0205 13:23:36.760521 139919816816448 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_1/ogbg_jax/trial_2/checkpoint_57322.
I0205 13:23:36.940326 139919816816448 submission_runner.py:583] Tuning trial 2/5
I0205 13:23:36.940559 139919816816448 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.2, learning_rate=0.0008445074561975979, one_minus_beta1=0.11042418465, beta2=0.9978504782314613, weight_decay=0.08135402759553023, warmup_factor=0.05)
I0205 13:23:36.949800 139919816816448 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.5325733423233032, 'train/loss': 0.7276434302330017, 'train/mean_average_precision': 0.02318310626876276, 'validation/accuracy': 0.5230699777603149, 'validation/loss': 0.7331878542900085, 'validation/mean_average_precision': 0.025502919733897296, 'validation/num_examples': 43793, 'test/accuracy': 0.5214916467666626, 'test/loss': 0.7347381114959717, 'test/mean_average_precision': 0.02683098870779874, 'test/num_examples': 43793, 'score': 12.211849689483643, 'total_duration': 137.16028547286987, 'accumulated_submission_time': 12.211849689483643, 'accumulated_eval_time': 124.94838285446167, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (750, {'train/accuracy': 0.9867556691169739, 'train/loss': 0.06597922742366791, 'train/mean_average_precision': 0.0340223151502751, 'validation/accuracy': 0.9841179251670837, 'validation/loss': 0.07448847591876984, 'validation/mean_average_precision': 0.03621529879272365, 'validation/num_examples': 43793, 'test/accuracy': 0.983142077922821, 'test/loss': 0.07739343494176865, 'test/mean_average_precision': 0.03772474150749012, 'test/num_examples': 43793, 'score': 252.38562417030334, 'total_duration': 496.25007700920105, 'accumulated_submission_time': 252.38562417030334, 'accumulated_eval_time': 243.82266402244568, 'accumulated_logging_time': 0.021685361862182617, 'global_step': 750, 'preemption_count': 0}), (1498, {'train/accuracy': 0.9871229529380798, 'train/loss': 0.04890026897192001, 'train/mean_average_precision': 0.08247394380439249, 'validation/accuracy': 0.9843728542327881, 'validation/loss': 0.05866231024265289, 'validation/mean_average_precision': 0.08369422795852387, 'validation/num_examples': 43793, 'test/accuracy': 0.9833973050117493, 'test/loss': 0.06206993758678436, 'test/mean_average_precision': 0.08140070449533014, 'test/num_examples': 43793, 'score': 492.63449263572693, 'total_duration': 867.4755127429962, 'accumulated_submission_time': 492.63449263572693, 'accumulated_eval_time': 374.75238394737244, 'accumulated_logging_time': 0.04772686958312988, 'global_step': 1498, 'preemption_count': 0}), (2244, {'train/accuracy': 0.9873826503753662, 'train/loss': 0.0455460250377655, 'train/mean_average_precision': 0.12638824552826944, 'validation/accuracy': 0.9847467541694641, 'validation/loss': 0.05478588119149208, 'validation/mean_average_precision': 0.12251270423431268, 'validation/num_examples': 43793, 'test/accuracy': 0.9837561845779419, 'test/loss': 0.05803578719496727, 'test/mean_average_precision': 0.11871780262463053, 'test/num_examples': 43793, 'score': 732.8614349365234, 'total_duration': 1230.1546649932861, 'accumulated_submission_time': 732.8614349365234, 'accumulated_eval_time': 497.15764117240906, 'accumulated_logging_time': 0.0744333267211914, 'global_step': 2244, 'preemption_count': 0}), (2992, {'train/accuracy': 0.9876368045806885, 'train/loss': 0.04369545355439186, 'train/mean_average_precision': 0.15692297081332873, 'validation/accuracy': 0.9849249720573425, 'validation/loss': 0.053019069135189056, 'validation/mean_average_precision': 0.14394199531059154, 'validation/num_examples': 43793, 'test/accuracy': 0.9839427471160889, 'test/loss': 0.056046262383461, 'test/mean_average_precision': 0.14198751319090408, 'test/num_examples': 43793, 'score': 972.8983051776886, 'total_duration': 1592.0877315998077, 'accumulated_submission_time': 972.8983051776886, 'accumulated_eval_time': 619.0059733390808, 'accumulated_logging_time': 0.10088014602661133, 'global_step': 2992, 'preemption_count': 0}), (3746, {'train/accuracy': 0.9878502488136292, 'train/loss': 0.04240598902106285, 'train/mean_average_precision': 0.17374295638795922, 'validation/accuracy': 0.985138475894928, 'validation/loss': 0.05185147747397423, 'validation/mean_average_precision': 0.15851693156999125, 'validation/num_examples': 43793, 'test/accuracy': 0.9841310381889343, 'test/loss': 0.05469619855284691, 'test/mean_average_precision': 0.15341582083896163, 'test/num_examples': 43793, 'score': 1213.0681955814362, 'total_duration': 1953.5891268253326, 'accumulated_submission_time': 1213.0681955814362, 'accumulated_eval_time': 740.2879574298859, 'accumulated_logging_time': 0.1300337314605713, 'global_step': 3746, 'preemption_count': 0}), (4493, {'train/accuracy': 0.9877445101737976, 'train/loss': 0.04246379807591438, 'train/mean_average_precision': 0.2003246065057177, 'validation/accuracy': 0.985008955001831, 'validation/loss': 0.05223214626312256, 'validation/mean_average_precision': 0.16444033733320493, 'validation/num_examples': 43793, 'test/accuracy': 0.9840977787971497, 'test/loss': 0.05511019751429558, 'test/mean_average_precision': 0.16555823629103325, 'test/num_examples': 43793, 'score': 1453.243047952652, 'total_duration': 2321.1722240448, 'accumulated_submission_time': 1453.243047952652, 'accumulated_eval_time': 867.6484885215759, 'accumulated_logging_time': 0.15729570388793945, 'global_step': 4493, 'preemption_count': 0}), (5241, {'train/accuracy': 0.9884564280509949, 'train/loss': 0.03982599452137947, 'train/mean_average_precision': 0.2073086561436093, 'validation/accuracy': 0.9855472445487976, 'validation/loss': 0.04924844205379486, 'validation/mean_average_precision': 0.18444835777690646, 'validation/num_examples': 43793, 'test/accuracy': 0.9846347570419312, 'test/loss': 0.05191699415445328, 'test/mean_average_precision': 0.18355441353796362, 'test/num_examples': 43793, 'score': 1693.3792762756348, 'total_duration': 2683.23321557045, 'accumulated_submission_time': 1693.3792762756348, 'accumulated_eval_time': 989.522251367569, 'accumulated_logging_time': 0.18688249588012695, 'global_step': 5241, 'preemption_count': 0}), (5976, {'train/accuracy': 0.9887798428535461, 'train/loss': 0.03900324925780296, 'train/mean_average_precision': 0.24165443336439402, 'validation/accuracy': 0.9856743216514587, 'validation/loss': 0.048873793333768845, 'validation/mean_average_precision': 0.1980541880902195, 'validation/num_examples': 43793, 'test/accuracy': 0.9846912026405334, 'test/loss': 0.05147066339850426, 'test/mean_average_precision': 0.19535868259729494, 'test/num_examples': 43793, 'score': 1933.3688995838165, 'total_duration': 3043.0417470932007, 'accumulated_submission_time': 1933.3688995838165, 'accumulated_eval_time': 1109.290159702301, 'accumulated_logging_time': 0.21666622161865234, 'global_step': 5976, 'preemption_count': 0}), (6723, {'train/accuracy': 0.9888468384742737, 'train/loss': 0.03804084286093712, 'train/mean_average_precision': 0.26055997380786267, 'validation/accuracy': 0.9860299229621887, 'validation/loss': 0.04745831713080406, 'validation/mean_average_precision': 0.21360613667128348, 'validation/num_examples': 43793, 'test/accuracy': 0.9851002097129822, 'test/loss': 0.05011272802948952, 'test/mean_average_precision': 0.210851843830605, 'test/num_examples': 43793, 'score': 2173.444726228714, 'total_duration': 3405.105672597885, 'accumulated_submission_time': 2173.444726228714, 'accumulated_eval_time': 1231.2292671203613, 'accumulated_logging_time': 0.24484539031982422, 'global_step': 6723, 'preemption_count': 0}), (7472, {'train/accuracy': 0.9890567660331726, 'train/loss': 0.03719208762049675, 'train/mean_average_precision': 0.2785736894766114, 'validation/accuracy': 0.9860782027244568, 'validation/loss': 0.0469764843583107, 'validation/mean_average_precision': 0.2222152541430098, 'validation/num_examples': 43793, 'test/accuracy': 0.9851503372192383, 'test/loss': 0.049517057836055756, 'test/mean_average_precision': 0.21981127754706126, 'test/num_examples': 43793, 'score': 2413.602122068405, 'total_duration': 3766.124802827835, 'accumulated_submission_time': 2413.602122068405, 'accumulated_eval_time': 1352.0426914691925, 'accumulated_logging_time': 0.2722771167755127, 'global_step': 7472, 'preemption_count': 0}), (8212, {'train/accuracy': 0.9891743659973145, 'train/loss': 0.036153119057416916, 'train/mean_average_precision': 0.3071437430999844, 'validation/accuracy': 0.9860546588897705, 'validation/loss': 0.04705780744552612, 'validation/mean_average_precision': 0.2279129954488742, 'validation/num_examples': 43793, 'test/accuracy': 0.9851882457733154, 'test/loss': 0.049859512597322464, 'test/mean_average_precision': 0.2298814156917866, 'test/num_examples': 43793, 'score': 2653.557772874832, 'total_duration': 4131.996986627579, 'accumulated_submission_time': 2653.557772874832, 'accumulated_eval_time': 1477.9091057777405, 'accumulated_logging_time': 0.30215907096862793, 'global_step': 8212, 'preemption_count': 0}), (8961, {'train/accuracy': 0.9895905256271362, 'train/loss': 0.03512311726808548, 'train/mean_average_precision': 0.3273462661718691, 'validation/accuracy': 0.9861902594566345, 'validation/loss': 0.04639050364494324, 'validation/mean_average_precision': 0.23257693387459144, 'validation/num_examples': 43793, 'test/accuracy': 0.9853874444961548, 'test/loss': 0.04906167834997177, 'test/mean_average_precision': 0.23298034352224614, 'test/num_examples': 43793, 'score': 2893.710347890854, 'total_duration': 4495.563076972961, 'accumulated_submission_time': 2893.710347890854, 'accumulated_eval_time': 1601.2729868888855, 'accumulated_logging_time': 0.3315138816833496, 'global_step': 8961, 'preemption_count': 0}), (9718, {'train/accuracy': 0.9898381233215332, 'train/loss': 0.03419634699821472, 'train/mean_average_precision': 0.3502433378063998, 'validation/accuracy': 0.9864122867584229, 'validation/loss': 0.0458182692527771, 'validation/mean_average_precision': 0.24198184268605932, 'validation/num_examples': 43793, 'test/accuracy': 0.985545814037323, 'test/loss': 0.04856901615858078, 'test/mean_average_precision': 0.23707760709095407, 'test/num_examples': 43793, 'score': 3133.842006921768, 'total_duration': 4856.113333940506, 'accumulated_submission_time': 3133.842006921768, 'accumulated_eval_time': 1721.642301082611, 'accumulated_logging_time': 0.36038923263549805, 'global_step': 9718, 'preemption_count': 0}), (10465, {'train/accuracy': 0.989568829536438, 'train/loss': 0.034070976078510284, 'train/mean_average_precision': 0.3687249605372311, 'validation/accuracy': 0.9862357378005981, 'validation/loss': 0.04665293172001839, 'validation/mean_average_precision': 0.23713135775538807, 'validation/num_examples': 43793, 'test/accuracy': 0.9853895902633667, 'test/loss': 0.04941052943468094, 'test/mean_average_precision': 0.23950832057838056, 'test/num_examples': 43793, 'score': 3373.964447259903, 'total_duration': 5219.9779760837555, 'accumulated_submission_time': 3373.964447259903, 'accumulated_eval_time': 1845.3364737033844, 'accumulated_logging_time': 0.388399600982666, 'global_step': 10465, 'preemption_count': 0}), (11209, {'train/accuracy': 0.9899488091468811, 'train/loss': 0.03359292447566986, 'train/mean_average_precision': 0.3595911836357722, 'validation/accuracy': 0.9864537119865417, 'validation/loss': 0.045604538172483444, 'validation/mean_average_precision': 0.2528149574337986, 'validation/num_examples': 43793, 'test/accuracy': 0.9855285286903381, 'test/loss': 0.04852926731109619, 'test/mean_average_precision': 0.24168335414789271, 'test/num_examples': 43793, 'score': 3614.0954418182373, 'total_duration': 5584.160804271698, 'accumulated_submission_time': 3614.0954418182373, 'accumulated_eval_time': 1969.3325974941254, 'accumulated_logging_time': 0.42067766189575195, 'global_step': 11209, 'preemption_count': 0}), (11956, {'train/accuracy': 0.9904040098190308, 'train/loss': 0.03253566101193428, 'train/mean_average_precision': 0.3633715206238281, 'validation/accuracy': 0.9865913391113281, 'validation/loss': 0.04513591527938843, 'validation/mean_average_precision': 0.2521590708519562, 'validation/num_examples': 43793, 'test/accuracy': 0.985745906829834, 'test/loss': 0.047842685133218765, 'test/mean_average_precision': 0.2463089958443751, 'test/num_examples': 43793, 'score': 3854.3355824947357, 'total_duration': 5944.627243518829, 'accumulated_submission_time': 3854.3355824947357, 'accumulated_eval_time': 2089.5103764533997, 'accumulated_logging_time': 0.44842958450317383, 'global_step': 11956, 'preemption_count': 0}), (12707, {'train/accuracy': 0.9905290603637695, 'train/loss': 0.031725697219371796, 'train/mean_average_precision': 0.3934554074221962, 'validation/accuracy': 0.9866530299186707, 'validation/loss': 0.04535263031721115, 'validation/mean_average_precision': 0.25775737687452344, 'validation/num_examples': 43793, 'test/accuracy': 0.9858217239379883, 'test/loss': 0.04809940606355667, 'test/mean_average_precision': 0.2571150736686474, 'test/num_examples': 43793, 'score': 4094.5407037734985, 'total_duration': 6304.456825494766, 'accumulated_submission_time': 4094.5407037734985, 'accumulated_eval_time': 2209.0845663547516, 'accumulated_logging_time': 0.4778716564178467, 'global_step': 12707, 'preemption_count': 0}), (13455, {'train/accuracy': 0.9906842708587646, 'train/loss': 0.03138013184070587, 'train/mean_average_precision': 0.39294846410701095, 'validation/accuracy': 0.9865840077400208, 'validation/loss': 0.04529331624507904, 'validation/mean_average_precision': 0.253105285712875, 'validation/num_examples': 43793, 'test/accuracy': 0.9856991171836853, 'test/loss': 0.04808598756790161, 'test/mean_average_precision': 0.24591885708072264, 'test/num_examples': 43793, 'score': 4334.822235822678, 'total_duration': 6665.2435131073, 'accumulated_submission_time': 4334.822235822678, 'accumulated_eval_time': 2329.5390541553497, 'accumulated_logging_time': 0.5078516006469727, 'global_step': 13455, 'preemption_count': 0}), (14202, {'train/accuracy': 0.9907562732696533, 'train/loss': 0.03044726885855198, 'train/mean_average_precision': 0.42871386983088106, 'validation/accuracy': 0.9867236614227295, 'validation/loss': 0.04521000757813454, 'validation/mean_average_precision': 0.2582183376390017, 'validation/num_examples': 43793, 'test/accuracy': 0.9858486652374268, 'test/loss': 0.048013463616371155, 'test/mean_average_precision': 0.2553494129192967, 'test/num_examples': 43793, 'score': 4574.961861371994, 'total_duration': 7025.507632255554, 'accumulated_submission_time': 4574.961861371994, 'accumulated_eval_time': 2449.6124868392944, 'accumulated_logging_time': 0.5385310649871826, 'global_step': 14202, 'preemption_count': 0}), (14948, {'train/accuracy': 0.99077308177948, 'train/loss': 0.03025943599641323, 'train/mean_average_precision': 0.4277688119308474, 'validation/accuracy': 0.9866668581962585, 'validation/loss': 0.04541216418147087, 'validation/mean_average_precision': 0.2544344404267181, 'validation/num_examples': 43793, 'test/accuracy': 0.9857577085494995, 'test/loss': 0.04828036203980446, 'test/mean_average_precision': 0.24394367745182258, 'test/num_examples': 43793, 'score': 4815.122054815292, 'total_duration': 7388.161834478378, 'accumulated_submission_time': 4815.122054815292, 'accumulated_eval_time': 2572.057140827179, 'accumulated_logging_time': 0.5675864219665527, 'global_step': 14948, 'preemption_count': 0}), (15697, {'train/accuracy': 0.9912256598472595, 'train/loss': 0.028870854526758194, 'train/mean_average_precision': 0.45138882987386597, 'validation/accuracy': 0.9866424798965454, 'validation/loss': 0.04532580450177193, 'validation/mean_average_precision': 0.2573368540704658, 'validation/num_examples': 43793, 'test/accuracy': 0.9857909679412842, 'test/loss': 0.04812214896082878, 'test/mean_average_precision': 0.24653675549932982, 'test/num_examples': 43793, 'score': 5055.139110803604, 'total_duration': 7750.152556419373, 'accumulated_submission_time': 5055.139110803604, 'accumulated_eval_time': 2693.980701684952, 'accumulated_logging_time': 0.5970251560211182, 'global_step': 15697, 'preemption_count': 0}), (16447, {'train/accuracy': 0.9916675090789795, 'train/loss': 0.027591295540332794, 'train/mean_average_precision': 0.49192983157015663, 'validation/accuracy': 0.9866794347763062, 'validation/loss': 0.04554938152432442, 'validation/mean_average_precision': 0.2621947058241379, 'validation/num_examples': 43793, 'test/accuracy': 0.9858705401420593, 'test/loss': 0.04825540632009506, 'test/mean_average_precision': 0.2530989906320853, 'test/num_examples': 43793, 'score': 5295.112805843353, 'total_duration': 8109.461350440979, 'accumulated_submission_time': 5295.112805843353, 'accumulated_eval_time': 2813.2641813755035, 'accumulated_logging_time': 0.6277570724487305, 'global_step': 16447, 'preemption_count': 0}), (17191, {'train/accuracy': 0.9916927814483643, 'train/loss': 0.0272750835865736, 'train/mean_average_precision': 0.5031697196652744, 'validation/accuracy': 0.986777663230896, 'validation/loss': 0.04524582624435425, 'validation/mean_average_precision': 0.26525978943555706, 'validation/num_examples': 43793, 'test/accuracy': 0.9858924746513367, 'test/loss': 0.04820572957396507, 'test/mean_average_precision': 0.2521990759737141, 'test/num_examples': 43793, 'score': 5535.315126657486, 'total_duration': 8471.188846826553, 'accumulated_submission_time': 5535.315126657486, 'accumulated_eval_time': 2934.739273548126, 'accumulated_logging_time': 0.6577551364898682, 'global_step': 17191, 'preemption_count': 0}), (17944, {'train/accuracy': 0.9915154576301575, 'train/loss': 0.027695178985595703, 'train/mean_average_precision': 0.5012935847392078, 'validation/accuracy': 0.9867122769355774, 'validation/loss': 0.04593149200081825, 'validation/mean_average_precision': 0.2675033684853384, 'validation/num_examples': 43793, 'test/accuracy': 0.9857863187789917, 'test/loss': 0.04909912124276161, 'test/mean_average_precision': 0.24857881791609326, 'test/num_examples': 43793, 'score': 5775.409202814102, 'total_duration': 8833.050078868866, 'accumulated_submission_time': 5775.409202814102, 'accumulated_eval_time': 3056.4563794136047, 'accumulated_logging_time': 0.6870527267456055, 'global_step': 17944, 'preemption_count': 0}), (18691, {'train/accuracy': 0.9915038347244263, 'train/loss': 0.027914177626371384, 'train/mean_average_precision': 0.4830483010954101, 'validation/accuracy': 0.9867801070213318, 'validation/loss': 0.045402124524116516, 'validation/mean_average_precision': 0.2609649209664075, 'validation/num_examples': 43793, 'test/accuracy': 0.9859122633934021, 'test/loss': 0.048295315355062485, 'test/mean_average_precision': 0.25655242134910017, 'test/num_examples': 43793, 'score': 6015.5455322265625, 'total_duration': 9190.930800199509, 'accumulated_submission_time': 6015.5455322265625, 'accumulated_eval_time': 3174.150098800659, 'accumulated_logging_time': 0.7177164554595947, 'global_step': 18691, 'preemption_count': 0}), (19435, {'train/accuracy': 0.9913306832313538, 'train/loss': 0.028145255520939827, 'train/mean_average_precision': 0.4829667100200081, 'validation/accuracy': 0.9866834878921509, 'validation/loss': 0.046191148459911346, 'validation/mean_average_precision': 0.260969365659252, 'validation/num_examples': 43793, 'test/accuracy': 0.9858874082565308, 'test/loss': 0.04917261376976967, 'test/mean_average_precision': 0.2569502106290384, 'test/num_examples': 43793, 'score': 6255.649400234222, 'total_duration': 9551.801201581955, 'accumulated_submission_time': 6255.649400234222, 'accumulated_eval_time': 3294.866469860077, 'accumulated_logging_time': 0.7476849555969238, 'global_step': 19435, 'preemption_count': 0}), (20189, {'train/accuracy': 0.9917463064193726, 'train/loss': 0.026905406266450882, 'train/mean_average_precision': 0.4939934566066215, 'validation/accuracy': 0.9866786003112793, 'validation/loss': 0.0461668036878109, 'validation/mean_average_precision': 0.258319712197705, 'validation/num_examples': 43793, 'test/accuracy': 0.9858680367469788, 'test/loss': 0.04932626709342003, 'test/mean_average_precision': 0.24966029993713423, 'test/num_examples': 43793, 'score': 6495.83327627182, 'total_duration': 9908.72722864151, 'accumulated_submission_time': 6495.83327627182, 'accumulated_eval_time': 3411.558787584305, 'accumulated_logging_time': 0.7772469520568848, 'global_step': 20189, 'preemption_count': 0}), (20938, {'train/accuracy': 0.9917852282524109, 'train/loss': 0.026819629594683647, 'train/mean_average_precision': 0.505956641813925, 'validation/accuracy': 0.9866956472396851, 'validation/loss': 0.04661537706851959, 'validation/mean_average_precision': 0.257373007364516, 'validation/num_examples': 43793, 'test/accuracy': 0.9857537150382996, 'test/loss': 0.05003098025918007, 'test/mean_average_precision': 0.24761899832550355, 'test/num_examples': 43793, 'score': 6736.029312372208, 'total_duration': 10266.563576698303, 'accumulated_submission_time': 6736.029312372208, 'accumulated_eval_time': 3529.1479530334473, 'accumulated_logging_time': 0.8081989288330078, 'global_step': 20938, 'preemption_count': 0}), (21689, {'train/accuracy': 0.9920173287391663, 'train/loss': 0.02600455842912197, 'train/mean_average_precision': 0.523237135827197, 'validation/accuracy': 0.9867163896560669, 'validation/loss': 0.04635230824351311, 'validation/mean_average_precision': 0.2581223071410054, 'validation/num_examples': 43793, 'test/accuracy': 0.9857938885688782, 'test/loss': 0.04930374398827553, 'test/mean_average_precision': 0.2549565183568808, 'test/num_examples': 43793, 'score': 6976.128599882126, 'total_duration': 10624.498348236084, 'accumulated_submission_time': 6976.128599882126, 'accumulated_eval_time': 3646.9298775196075, 'accumulated_logging_time': 0.841012716293335, 'global_step': 21689, 'preemption_count': 0}), (22436, {'train/accuracy': 0.9925153851509094, 'train/loss': 0.02464495599269867, 'train/mean_average_precision': 0.5653510078298822, 'validation/accuracy': 0.9865344762802124, 'validation/loss': 0.04645871743559837, 'validation/mean_average_precision': 0.2524689786088972, 'validation/num_examples': 43793, 'test/accuracy': 0.9857235550880432, 'test/loss': 0.04915507882833481, 'test/mean_average_precision': 0.24429603715936346, 'test/num_examples': 43793, 'score': 7216.098851442337, 'total_duration': 10983.145557641983, 'accumulated_submission_time': 7216.098851442337, 'accumulated_eval_time': 3765.5550322532654, 'accumulated_logging_time': 0.8723323345184326, 'global_step': 22436, 'preemption_count': 0}), (23182, {'train/accuracy': 0.9926448464393616, 'train/loss': 0.023930178955197334, 'train/mean_average_precision': 0.5750513515882668, 'validation/accuracy': 0.9865629076957703, 'validation/loss': 0.04723859578371048, 'validation/mean_average_precision': 0.25317094876484586, 'validation/num_examples': 43793, 'test/accuracy': 0.9857202172279358, 'test/loss': 0.05016908049583435, 'test/mean_average_precision': 0.25159870420610275, 'test/num_examples': 43793, 'score': 7456.205724477768, 'total_duration': 11344.477650642395, 'accumulated_submission_time': 7456.205724477768, 'accumulated_eval_time': 3886.729905128479, 'accumulated_logging_time': 0.9026894569396973, 'global_step': 23182, 'preemption_count': 0}), (23933, {'train/accuracy': 0.9929741024971008, 'train/loss': 0.023070918396115303, 'train/mean_average_precision': 0.5870177889735277, 'validation/accuracy': 0.9866250157356262, 'validation/loss': 0.04693299159407616, 'validation/mean_average_precision': 0.26114968624787654, 'validation/num_examples': 43793, 'test/accuracy': 0.9857092499732971, 'test/loss': 0.04996924847364426, 'test/mean_average_precision': 0.2528821126225342, 'test/num_examples': 43793, 'score': 7696.241092205048, 'total_duration': 11703.929669380188, 'accumulated_submission_time': 7696.241092205048, 'accumulated_eval_time': 4006.09627699852, 'accumulated_logging_time': 0.9327542781829834, 'global_step': 23933, 'preemption_count': 0}), (24676, {'train/accuracy': 0.9929828643798828, 'train/loss': 0.022991664707660675, 'train/mean_average_precision': 0.5987426324371437, 'validation/accuracy': 0.9867159724235535, 'validation/loss': 0.04694814234972, 'validation/mean_average_precision': 0.26135489494565817, 'validation/num_examples': 43793, 'test/accuracy': 0.9857812523841858, 'test/loss': 0.049936845898628235, 'test/mean_average_precision': 0.2494686159353731, 'test/num_examples': 43793, 'score': 7936.516567230225, 'total_duration': 12066.811733722687, 'accumulated_submission_time': 7936.516567230225, 'accumulated_eval_time': 4128.651603221893, 'accumulated_logging_time': 0.9643106460571289, 'global_step': 24676, 'preemption_count': 0}), (25421, {'train/accuracy': 0.9923743009567261, 'train/loss': 0.024536333978176117, 'train/mean_average_precision': 0.5646499916322825, 'validation/accuracy': 0.9865953922271729, 'validation/loss': 0.04823179170489311, 'validation/mean_average_precision': 0.2583915106640733, 'validation/num_examples': 43793, 'test/accuracy': 0.9857602119445801, 'test/loss': 0.05137448012828827, 'test/mean_average_precision': 0.24806008408106967, 'test/num_examples': 43793, 'score': 8176.752241849899, 'total_duration': 12431.66696190834, 'accumulated_submission_time': 8176.752241849899, 'accumulated_eval_time': 4253.2206172943115, 'accumulated_logging_time': 0.9951791763305664, 'global_step': 25421, 'preemption_count': 0}), (26160, {'train/accuracy': 0.9926239848136902, 'train/loss': 0.02409951202571392, 'train/mean_average_precision': 0.5744237415586881, 'validation/accuracy': 0.986622154712677, 'validation/loss': 0.04719825088977814, 'validation/mean_average_precision': 0.25683031179824894, 'validation/num_examples': 43793, 'test/accuracy': 0.9857054352760315, 'test/loss': 0.05031025409698486, 'test/mean_average_precision': 0.2487609686306002, 'test/num_examples': 43793, 'score': 8416.71799492836, 'total_duration': 12787.58046579361, 'accumulated_submission_time': 8416.71799492836, 'accumulated_eval_time': 4369.113248348236, 'accumulated_logging_time': 1.0298182964324951, 'global_step': 26160, 'preemption_count': 0}), (26901, {'train/accuracy': 0.9923655986785889, 'train/loss': 0.024558234959840775, 'train/mean_average_precision': 0.5520001620106612, 'validation/accuracy': 0.9866526126861572, 'validation/loss': 0.047937121242284775, 'validation/mean_average_precision': 0.25535532196750965, 'validation/num_examples': 43793, 'test/accuracy': 0.9857370257377625, 'test/loss': 0.051448315382003784, 'test/mean_average_precision': 0.24396206410674492, 'test/num_examples': 43793, 'score': 8656.93652176857, 'total_duration': 13150.278239965439, 'accumulated_submission_time': 8656.93652176857, 'accumulated_eval_time': 4491.541553258896, 'accumulated_logging_time': 1.0609753131866455, 'global_step': 26901, 'preemption_count': 0}), (27633, {'train/accuracy': 0.9923880100250244, 'train/loss': 0.024168267846107483, 'train/mean_average_precision': 0.5654214180134779, 'validation/accuracy': 0.9865978360176086, 'validation/loss': 0.04857823625206947, 'validation/mean_average_precision': 0.25026398056994087, 'validation/num_examples': 43793, 'test/accuracy': 0.98567134141922, 'test/loss': 0.05205978453159332, 'test/mean_average_precision': 0.2451742423721277, 'test/num_examples': 43793, 'score': 8897.207918643951, 'total_duration': 13512.87291264534, 'accumulated_submission_time': 8897.207918643951, 'accumulated_eval_time': 4613.804739713669, 'accumulated_logging_time': 1.095686435699463, 'global_step': 27633, 'preemption_count': 0}), (28375, {'train/accuracy': 0.9930915832519531, 'train/loss': 0.022376207634806633, 'train/mean_average_precision': 0.5991473293839304, 'validation/accuracy': 0.9865767359733582, 'validation/loss': 0.048158515244722366, 'validation/mean_average_precision': 0.2556357184836858, 'validation/num_examples': 43793, 'test/accuracy': 0.9856898784637451, 'test/loss': 0.05165575072169304, 'test/mean_average_precision': 0.24302427229867257, 'test/num_examples': 43793, 'score': 9137.189074516296, 'total_duration': 13872.593567609787, 'accumulated_submission_time': 9137.189074516296, 'accumulated_eval_time': 4733.4911098480225, 'accumulated_logging_time': 1.128706693649292, 'global_step': 28375, 'preemption_count': 0}), (29123, {'train/accuracy': 0.9930724501609802, 'train/loss': 0.02218599058687687, 'train/mean_average_precision': 0.617831682258646, 'validation/accuracy': 0.9865877032279968, 'validation/loss': 0.048793043941259384, 'validation/mean_average_precision': 0.25224042822579723, 'validation/num_examples': 43793, 'test/accuracy': 0.9857964515686035, 'test/loss': 0.051948074251413345, 'test/mean_average_precision': 0.24737597931197716, 'test/num_examples': 43793, 'score': 9377.43240237236, 'total_duration': 14233.759857654572, 'accumulated_submission_time': 9377.43240237236, 'accumulated_eval_time': 4854.35968875885, 'accumulated_logging_time': 1.1624870300292969, 'global_step': 29123, 'preemption_count': 0}), (29861, {'train/accuracy': 0.993553876876831, 'train/loss': 0.020813263952732086, 'train/mean_average_precision': 0.6401829967227319, 'validation/accuracy': 0.9865182638168335, 'validation/loss': 0.04869452491402626, 'validation/mean_average_precision': 0.2477831701067681, 'validation/num_examples': 43793, 'test/accuracy': 0.9856991171836853, 'test/loss': 0.05191131681203842, 'test/mean_average_precision': 0.24846553581429545, 'test/num_examples': 43793, 'score': 9617.391327857971, 'total_duration': 14591.459522247314, 'accumulated_submission_time': 9617.391327857971, 'accumulated_eval_time': 4972.0479435920715, 'accumulated_logging_time': 1.1940855979919434, 'global_step': 29861, 'preemption_count': 0}), (30589, {'train/accuracy': 0.9940480589866638, 'train/loss': 0.019391926005482674, 'train/mean_average_precision': 0.6610642591112672, 'validation/accuracy': 0.9865243434906006, 'validation/loss': 0.04920274019241333, 'validation/mean_average_precision': 0.2488174179315952, 'validation/num_examples': 43793, 'test/accuracy': 0.9856089949607849, 'test/loss': 0.05269637703895569, 'test/mean_average_precision': 0.24340012203764505, 'test/num_examples': 43793, 'score': 9857.636467218399, 'total_duration': 14957.588129997253, 'accumulated_submission_time': 9857.636467218399, 'accumulated_eval_time': 5097.874583005905, 'accumulated_logging_time': 1.2269041538238525, 'global_step': 30589, 'preemption_count': 0}), (31334, {'train/accuracy': 0.9936291575431824, 'train/loss': 0.020761188119649887, 'train/mean_average_precision': 0.6274333508360139, 'validation/accuracy': 0.9864720106124878, 'validation/loss': 0.04945521429181099, 'validation/mean_average_precision': 0.24921789351712892, 'validation/num_examples': 43793, 'test/accuracy': 0.9855576157569885, 'test/loss': 0.052920859307050705, 'test/mean_average_precision': 0.23617914446786695, 'test/num_examples': 43793, 'score': 10097.875643253326, 'total_duration': 15318.84247136116, 'accumulated_submission_time': 10097.875643253326, 'accumulated_eval_time': 5218.830732822418, 'accumulated_logging_time': 1.2641723155975342, 'global_step': 31334, 'preemption_count': 0}), (32075, {'train/accuracy': 0.9931486248970032, 'train/loss': 0.021551787853240967, 'train/mean_average_precision': 0.6206713202358298, 'validation/accuracy': 0.9865584373474121, 'validation/loss': 0.05010689049959183, 'validation/mean_average_precision': 0.24951108155807555, 'validation/num_examples': 43793, 'test/accuracy': 0.9856974482536316, 'test/loss': 0.053574588149785995, 'test/mean_average_precision': 0.24123737326370379, 'test/num_examples': 43793, 'score': 10337.898092508316, 'total_duration': 15679.182544469833, 'accumulated_submission_time': 10337.898092508316, 'accumulated_eval_time': 5339.095104217529, 'accumulated_logging_time': 1.2972569465637207, 'global_step': 32075, 'preemption_count': 0}), (32816, {'train/accuracy': 0.9932396411895752, 'train/loss': 0.021539898589253426, 'train/mean_average_precision': 0.6191252736954957, 'validation/accuracy': 0.9865308403968811, 'validation/loss': 0.04971396178007126, 'validation/mean_average_precision': 0.2507458835114568, 'validation/num_examples': 43793, 'test/accuracy': 0.9856300950050354, 'test/loss': 0.053199104964733124, 'test/mean_average_precision': 0.24082175433602587, 'test/num_examples': 43793, 'score': 10577.976407766342, 'total_duration': 16037.931844472885, 'accumulated_submission_time': 10577.976407766342, 'accumulated_eval_time': 5457.711098432541, 'accumulated_logging_time': 1.3318133354187012, 'global_step': 32816, 'preemption_count': 0}), (33566, {'train/accuracy': 0.9932653307914734, 'train/loss': 0.02148575894534588, 'train/mean_average_precision': 0.6170808507068615, 'validation/accuracy': 0.9865471124649048, 'validation/loss': 0.04991072416305542, 'validation/mean_average_precision': 0.24726883668381008, 'validation/num_examples': 43793, 'test/accuracy': 0.9854704141616821, 'test/loss': 0.05360097438097, 'test/mean_average_precision': 0.2351262883357148, 'test/num_examples': 43793, 'score': 10818.098178625107, 'total_duration': 16395.03837776184, 'accumulated_submission_time': 10818.098178625107, 'accumulated_eval_time': 5574.642826318741, 'accumulated_logging_time': 1.364924430847168, 'global_step': 33566, 'preemption_count': 0}), (34319, {'train/accuracy': 0.9936695694923401, 'train/loss': 0.020347896963357925, 'train/mean_average_precision': 0.6380343914037621, 'validation/accuracy': 0.9864545464515686, 'validation/loss': 0.0497109591960907, 'validation/mean_average_precision': 0.25108792310597466, 'validation/num_examples': 43793, 'test/accuracy': 0.9855020046234131, 'test/loss': 0.05300797149538994, 'test/mean_average_precision': 0.2383980651235781, 'test/num_examples': 43793, 'score': 11058.361001968384, 'total_duration': 16752.882014513016, 'accumulated_submission_time': 11058.361001968384, 'accumulated_eval_time': 5692.171427726746, 'accumulated_logging_time': 1.3974757194519043, 'global_step': 34319, 'preemption_count': 0}), (35066, {'train/accuracy': 0.9935685992240906, 'train/loss': 0.020250258967280388, 'train/mean_average_precision': 0.6473796514189897, 'validation/accuracy': 0.9865594506263733, 'validation/loss': 0.05094848573207855, 'validation/mean_average_precision': 0.25627819733279783, 'validation/num_examples': 43793, 'test/accuracy': 0.9855298399925232, 'test/loss': 0.0547693707048893, 'test/mean_average_precision': 0.23701332230698652, 'test/num_examples': 43793, 'score': 11298.55425786972, 'total_duration': 17113.989943742752, 'accumulated_submission_time': 11298.55425786972, 'accumulated_eval_time': 5813.030182600021, 'accumulated_logging_time': 1.4321606159210205, 'global_step': 35066, 'preemption_count': 0}), (35807, {'train/accuracy': 0.9941927194595337, 'train/loss': 0.018798677250742912, 'train/mean_average_precision': 0.6769564832266253, 'validation/accuracy': 0.9864675402641296, 'validation/loss': 0.050618451088666916, 'validation/mean_average_precision': 0.2556838216760408, 'validation/num_examples': 43793, 'test/accuracy': 0.9854856133460999, 'test/loss': 0.054121118038892746, 'test/mean_average_precision': 0.23563420421806033, 'test/num_examples': 43793, 'score': 11538.779606342316, 'total_duration': 17471.963469982147, 'accumulated_submission_time': 11538.779606342316, 'accumulated_eval_time': 5930.724012136459, 'accumulated_logging_time': 1.4665467739105225, 'global_step': 35807, 'preemption_count': 0}), (36556, {'train/accuracy': 0.9948610663414001, 'train/loss': 0.017193259671330452, 'train/mean_average_precision': 0.712666872419726, 'validation/accuracy': 0.9863424897193909, 'validation/loss': 0.05080199986696243, 'validation/mean_average_precision': 0.2482839235230926, 'validation/num_examples': 43793, 'test/accuracy': 0.9853832721710205, 'test/loss': 0.05435670539736748, 'test/mean_average_precision': 0.23943025390025763, 'test/num_examples': 43793, 'score': 11778.403351545334, 'total_duration': 17829.582918167114, 'accumulated_submission_time': 11778.403351545334, 'accumulated_eval_time': 6048.290234088898, 'accumulated_logging_time': 1.875577449798584, 'global_step': 36556, 'preemption_count': 0}), (37297, {'train/accuracy': 0.9950770735740662, 'train/loss': 0.016590526327490807, 'train/mean_average_precision': 0.7307588502918585, 'validation/accuracy': 0.9863532185554504, 'validation/loss': 0.051204219460487366, 'validation/mean_average_precision': 0.24673543350830335, 'validation/num_examples': 43793, 'test/accuracy': 0.9853453636169434, 'test/loss': 0.054908283054828644, 'test/mean_average_precision': 0.23354721817647842, 'test/num_examples': 43793, 'score': 12018.428232431412, 'total_duration': 18186.92015695572, 'accumulated_submission_time': 12018.428232431412, 'accumulated_eval_time': 6165.5471975803375, 'accumulated_logging_time': 1.9106330871582031, 'global_step': 37297, 'preemption_count': 0}), (38046, {'train/accuracy': 0.9940646886825562, 'train/loss': 0.018991518765687943, 'train/mean_average_precision': 0.6748852044013662, 'validation/accuracy': 0.9863676428794861, 'validation/loss': 0.05183979868888855, 'validation/mean_average_precision': 0.2426539176487821, 'validation/num_examples': 43793, 'test/accuracy': 0.9854186177253723, 'test/loss': 0.05569584295153618, 'test/mean_average_precision': 0.234332385061237, 'test/num_examples': 43793, 'score': 12258.432970285416, 'total_duration': 18546.359894514084, 'accumulated_submission_time': 12258.432970285416, 'accumulated_eval_time': 6284.9266357421875, 'accumulated_logging_time': 1.9459753036499023, 'global_step': 38046, 'preemption_count': 0}), (38788, {'train/accuracy': 0.9945722818374634, 'train/loss': 0.01792803965508938, 'train/mean_average_precision': 0.6901741289062857, 'validation/accuracy': 0.9861927032470703, 'validation/loss': 0.0516926646232605, 'validation/mean_average_precision': 0.23949731854974, 'validation/num_examples': 43793, 'test/accuracy': 0.9852977395057678, 'test/loss': 0.05522964522242546, 'test/mean_average_precision': 0.23119722214258617, 'test/num_examples': 43793, 'score': 12498.420560121536, 'total_duration': 18902.31952357292, 'accumulated_submission_time': 12498.420560121536, 'accumulated_eval_time': 6400.845049619675, 'accumulated_logging_time': 1.9799385070800781, 'global_step': 38788, 'preemption_count': 0}), (39532, {'train/accuracy': 0.9941584467887878, 'train/loss': 0.018571147695183754, 'train/mean_average_precision': 0.6757015592453715, 'validation/accuracy': 0.9862909317016602, 'validation/loss': 0.05209149420261383, 'validation/mean_average_precision': 0.24297197023763564, 'validation/num_examples': 43793, 'test/accuracy': 0.9853373169898987, 'test/loss': 0.05594261363148689, 'test/mean_average_precision': 0.22739905876539812, 'test/num_examples': 43793, 'score': 12738.50132727623, 'total_duration': 19260.186782360077, 'accumulated_submission_time': 12738.50132727623, 'accumulated_eval_time': 6518.578117609024, 'accumulated_logging_time': 2.013803243637085, 'global_step': 39532, 'preemption_count': 0}), (40275, {'train/accuracy': 0.9939488172531128, 'train/loss': 0.018992193043231964, 'train/mean_average_precision': 0.6637098607290097, 'validation/accuracy': 0.9863002896308899, 'validation/loss': 0.05314275622367859, 'validation/mean_average_precision': 0.2391469379130368, 'validation/num_examples': 43793, 'test/accuracy': 0.9853878617286682, 'test/loss': 0.05714089423418045, 'test/mean_average_precision': 0.22669360247005477, 'test/num_examples': 43793, 'score': 12978.7029337883, 'total_duration': 19614.231418848038, 'accumulated_submission_time': 12978.7029337883, 'accumulated_eval_time': 6632.365000963211, 'accumulated_logging_time': 2.0504045486450195, 'global_step': 40275, 'preemption_count': 0}), (41017, {'train/accuracy': 0.993874728679657, 'train/loss': 0.01919444277882576, 'train/mean_average_precision': 0.6714206706921495, 'validation/accuracy': 0.9862105846405029, 'validation/loss': 0.053260985761880875, 'validation/mean_average_precision': 0.23855620487968487, 'validation/num_examples': 43793, 'test/accuracy': 0.9853529334068298, 'test/loss': 0.05703926831483841, 'test/mean_average_precision': 0.22718431605538564, 'test/num_examples': 43793, 'score': 13218.936644792557, 'total_duration': 19975.578807592392, 'accumulated_submission_time': 13218.936644792557, 'accumulated_eval_time': 6753.425145626068, 'accumulated_logging_time': 2.084524631500244, 'global_step': 41017, 'preemption_count': 0}), (41761, {'train/accuracy': 0.994343101978302, 'train/loss': 0.0179709792137146, 'train/mean_average_precision': 0.6866316394471659, 'validation/accuracy': 0.9861910939216614, 'validation/loss': 0.05349453166127205, 'validation/mean_average_precision': 0.24149266884140586, 'validation/num_examples': 43793, 'test/accuracy': 0.9853084683418274, 'test/loss': 0.05730481818318367, 'test/mean_average_precision': 0.2291360045914827, 'test/num_examples': 43793, 'score': 13458.923202037811, 'total_duration': 20333.43417596817, 'accumulated_submission_time': 13458.923202037811, 'accumulated_eval_time': 6871.238674879074, 'accumulated_logging_time': 2.11984920501709, 'global_step': 41761, 'preemption_count': 0}), (42496, {'train/accuracy': 0.9952325224876404, 'train/loss': 0.015472352504730225, 'train/mean_average_precision': 0.7526896612136231, 'validation/accuracy': 0.9862304329872131, 'validation/loss': 0.05380275472998619, 'validation/mean_average_precision': 0.23870606578947065, 'validation/num_examples': 43793, 'test/accuracy': 0.985352098941803, 'test/loss': 0.05765301361680031, 'test/mean_average_precision': 0.2288912827762337, 'test/num_examples': 43793, 'score': 13699.152914047241, 'total_duration': 20687.278291463852, 'accumulated_submission_time': 13699.152914047241, 'accumulated_eval_time': 6984.798540115356, 'accumulated_logging_time': 2.1546361446380615, 'global_step': 42496, 'preemption_count': 0}), (43233, {'train/accuracy': 0.9956064820289612, 'train/loss': 0.014405405148863792, 'train/mean_average_precision': 0.7794564332097744, 'validation/accuracy': 0.9862028360366821, 'validation/loss': 0.0545261949300766, 'validation/mean_average_precision': 0.23263841615259878, 'validation/num_examples': 43793, 'test/accuracy': 0.9852619171142578, 'test/loss': 0.05838746577501297, 'test/mean_average_precision': 0.22563736970975654, 'test/num_examples': 43793, 'score': 13939.323972702026, 'total_duration': 21047.05036020279, 'accumulated_submission_time': 13939.323972702026, 'accumulated_eval_time': 7104.342164516449, 'accumulated_logging_time': 2.190378427505493, 'global_step': 43233, 'preemption_count': 0}), (43974, {'train/accuracy': 0.9957359433174133, 'train/loss': 0.01439312007278204, 'train/mean_average_precision': 0.762248438689672, 'validation/accuracy': 0.986229658126831, 'validation/loss': 0.05440077558159828, 'validation/mean_average_precision': 0.23389338677186755, 'validation/num_examples': 43793, 'test/accuracy': 0.9852930903434753, 'test/loss': 0.05838407203555107, 'test/mean_average_precision': 0.22733958924486009, 'test/num_examples': 43793, 'score': 14179.282273769379, 'total_duration': 21406.30087685585, 'accumulated_submission_time': 14179.282273769379, 'accumulated_eval_time': 7223.574427843094, 'accumulated_logging_time': 2.2259573936462402, 'global_step': 43974, 'preemption_count': 0}), (44713, {'train/accuracy': 0.9954484105110168, 'train/loss': 0.015085551887750626, 'train/mean_average_precision': 0.7581501313243499, 'validation/accuracy': 0.9861987829208374, 'validation/loss': 0.054727304726839066, 'validation/mean_average_precision': 0.2367285787323215, 'validation/num_examples': 43793, 'test/accuracy': 0.9853048920631409, 'test/loss': 0.05852444842457771, 'test/mean_average_precision': 0.22815882025587925, 'test/num_examples': 43793, 'score': 14419.376125335693, 'total_duration': 21765.531745910645, 'accumulated_submission_time': 14419.376125335693, 'accumulated_eval_time': 7342.6540105342865, 'accumulated_logging_time': 2.2634549140930176, 'global_step': 44713, 'preemption_count': 0}), (45457, {'train/accuracy': 0.9950907230377197, 'train/loss': 0.015709124505519867, 'train/mean_average_precision': 0.7374597770421244, 'validation/accuracy': 0.9861001372337341, 'validation/loss': 0.05494698882102966, 'validation/mean_average_precision': 0.2338895422966686, 'validation/num_examples': 43793, 'test/accuracy': 0.9852097034454346, 'test/loss': 0.058845750987529755, 'test/mean_average_precision': 0.22120940067768596, 'test/num_examples': 43793, 'score': 14659.376512050629, 'total_duration': 22120.843861818314, 'accumulated_submission_time': 14659.376512050629, 'accumulated_eval_time': 7457.90939617157, 'accumulated_logging_time': 2.2987446784973145, 'global_step': 45457, 'preemption_count': 0}), (46200, {'train/accuracy': 0.9943767786026001, 'train/loss': 0.017387907952070236, 'train/mean_average_precision': 0.7149168519041117, 'validation/accuracy': 0.9862478971481323, 'validation/loss': 0.05581240355968475, 'validation/mean_average_precision': 0.2359758331316309, 'validation/num_examples': 43793, 'test/accuracy': 0.9853684902191162, 'test/loss': 0.05969163030385971, 'test/mean_average_precision': 0.2266012596243111, 'test/num_examples': 43793, 'score': 14899.567381620407, 'total_duration': 22476.54651904106, 'accumulated_submission_time': 14899.567381620407, 'accumulated_eval_time': 7573.364289522171, 'accumulated_logging_time': 2.335345506668091, 'global_step': 46200, 'preemption_count': 0}), (46945, {'train/accuracy': 0.9943929314613342, 'train/loss': 0.017379935830831528, 'train/mean_average_precision': 0.7088835674631668, 'validation/accuracy': 0.9861464500427246, 'validation/loss': 0.05589861050248146, 'validation/mean_average_precision': 0.23579998318025813, 'validation/num_examples': 43793, 'test/accuracy': 0.9852564930915833, 'test/loss': 0.059784576296806335, 'test/mean_average_precision': 0.22329177658512858, 'test/num_examples': 43793, 'score': 15139.756244659424, 'total_duration': 22832.776628017426, 'accumulated_submission_time': 15139.756244659424, 'accumulated_eval_time': 7689.344358444214, 'accumulated_logging_time': 2.3755228519439697, 'global_step': 46945, 'preemption_count': 0}), (47690, {'train/accuracy': 0.9948909282684326, 'train/loss': 0.016112660989165306, 'train/mean_average_precision': 0.7247512557869674, 'validation/accuracy': 0.9861387014389038, 'validation/loss': 0.056316569447517395, 'validation/mean_average_precision': 0.23525812020997874, 'validation/num_examples': 43793, 'test/accuracy': 0.9851852655410767, 'test/loss': 0.060305699706077576, 'test/mean_average_precision': 0.2267443674197929, 'test/num_examples': 43793, 'score': 15379.908016204834, 'total_duration': 23192.140475034714, 'accumulated_submission_time': 15379.908016204834, 'accumulated_eval_time': 7808.494928121567, 'accumulated_logging_time': 2.4163498878479004, 'global_step': 47690, 'preemption_count': 0}), (48437, {'train/accuracy': 0.9958310127258301, 'train/loss': 0.013746333308517933, 'train/mean_average_precision': 0.7851234003757641, 'validation/accuracy': 0.9861135482788086, 'validation/loss': 0.056668639183044434, 'validation/mean_average_precision': 0.23437270029300178, 'validation/num_examples': 43793, 'test/accuracy': 0.9852004647254944, 'test/loss': 0.060763321816921234, 'test/mean_average_precision': 0.22290742347595474, 'test/num_examples': 43793, 'score': 15619.869005203247, 'total_duration': 23548.04278063774, 'accumulated_submission_time': 15619.869005203247, 'accumulated_eval_time': 7924.379849433899, 'accumulated_logging_time': 2.4527883529663086, 'global_step': 48437, 'preemption_count': 0}), (49184, {'train/accuracy': 0.9950827360153198, 'train/loss': 0.015535079874098301, 'train/mean_average_precision': 0.746528199797595, 'validation/accuracy': 0.9861204624176025, 'validation/loss': 0.056217651814222336, 'validation/mean_average_precision': 0.2370487070055823, 'validation/num_examples': 43793, 'test/accuracy': 0.9851077795028687, 'test/loss': 0.060557421296834946, 'test/mean_average_precision': 0.2294661277003323, 'test/num_examples': 43793, 'score': 15860.097087621689, 'total_duration': 23904.12767291069, 'accumulated_submission_time': 15860.097087621689, 'accumulated_eval_time': 8040.179135560989, 'accumulated_logging_time': 2.490166187286377, 'global_step': 49184, 'preemption_count': 0}), (49935, {'train/accuracy': 0.996401846408844, 'train/loss': 0.012614203616976738, 'train/mean_average_precision': 0.8146022634550463, 'validation/accuracy': 0.9861245155334473, 'validation/loss': 0.05769200995564461, 'validation/mean_average_precision': 0.23164453547220965, 'validation/num_examples': 43793, 'test/accuracy': 0.9851545095443726, 'test/loss': 0.06196204572916031, 'test/mean_average_precision': 0.2204592669232147, 'test/num_examples': 43793, 'score': 16100.177204370499, 'total_duration': 24257.970780849457, 'accumulated_submission_time': 16100.177204370499, 'accumulated_eval_time': 8153.885211467743, 'accumulated_logging_time': 2.5272974967956543, 'global_step': 49935, 'preemption_count': 0}), (50680, {'train/accuracy': 0.9969805479049683, 'train/loss': 0.011179205030202866, 'train/mean_average_precision': 0.8314752558336872, 'validation/accuracy': 0.9861451983451843, 'validation/loss': 0.05840529128909111, 'validation/mean_average_precision': 0.23081387671847295, 'validation/num_examples': 43793, 'test/accuracy': 0.9852147698402405, 'test/loss': 0.06269458681344986, 'test/mean_average_precision': 0.2215540398824933, 'test/num_examples': 43793, 'score': 16340.363671779633, 'total_duration': 24614.37414741516, 'accumulated_submission_time': 16340.363671779633, 'accumulated_eval_time': 8270.046475887299, 'accumulated_logging_time': 2.562939405441284, 'global_step': 50680, 'preemption_count': 0}), (51424, {'train/accuracy': 0.9964102506637573, 'train/loss': 0.012139641679823399, 'train/mean_average_precision': 0.8212676428096144, 'validation/accuracy': 0.9860566854476929, 'validation/loss': 0.05832161754369736, 'validation/mean_average_precision': 0.226055681241575, 'validation/num_examples': 43793, 'test/accuracy': 0.9851212501525879, 'test/loss': 0.06258596479892731, 'test/mean_average_precision': 0.21948238816098117, 'test/num_examples': 43793, 'score': 16580.46027135849, 'total_duration': 24968.01723909378, 'accumulated_submission_time': 16580.46027135849, 'accumulated_eval_time': 8383.532366275787, 'accumulated_logging_time': 2.60349440574646, 'global_step': 51424, 'preemption_count': 0}), (52170, {'train/accuracy': 0.9960031509399414, 'train/loss': 0.013010595925152302, 'train/mean_average_precision': 0.7998794085137176, 'validation/accuracy': 0.9860563278198242, 'validation/loss': 0.05870188772678375, 'validation/mean_average_precision': 0.22956517435367943, 'validation/num_examples': 43793, 'test/accuracy': 0.98509681224823, 'test/loss': 0.0628611296415329, 'test/mean_average_precision': 0.21980225443098933, 'test/num_examples': 43793, 'score': 16820.501957178116, 'total_duration': 25321.957409858704, 'accumulated_submission_time': 16820.501957178116, 'accumulated_eval_time': 8497.374136447906, 'accumulated_logging_time': 2.6400325298309326, 'global_step': 52170, 'preemption_count': 0}), (52920, {'train/accuracy': 0.9953522682189941, 'train/loss': 0.014262083917856216, 'train/mean_average_precision': 0.7881900013083352, 'validation/accuracy': 0.9861147403717041, 'validation/loss': 0.059349674731492996, 'validation/mean_average_precision': 0.22541223952187306, 'validation/num_examples': 43793, 'test/accuracy': 0.9851848483085632, 'test/loss': 0.06345284730195999, 'test/mean_average_precision': 0.2199008821173299, 'test/num_examples': 43793, 'score': 17060.735783100128, 'total_duration': 25680.60373020172, 'accumulated_submission_time': 17060.735783100128, 'accumulated_eval_time': 8615.72803235054, 'accumulated_logging_time': 2.6779961585998535, 'global_step': 52920, 'preemption_count': 0}), (53669, {'train/accuracy': 0.9954304099082947, 'train/loss': 0.014057978056371212, 'train/mean_average_precision': 0.785213941239531, 'validation/accuracy': 0.9861135482788086, 'validation/loss': 0.05896909534931183, 'validation/mean_average_precision': 0.22280262369836915, 'validation/num_examples': 43793, 'test/accuracy': 0.9851098656654358, 'test/loss': 0.06314091384410858, 'test/mean_average_precision': 0.218001164903323, 'test/num_examples': 43793, 'score': 17300.85765480995, 'total_duration': 26031.861436128616, 'accumulated_submission_time': 17300.85765480995, 'accumulated_eval_time': 8726.805708408356, 'accumulated_logging_time': 2.715599775314331, 'global_step': 53669, 'preemption_count': 0}), (54412, {'train/accuracy': 0.995689332485199, 'train/loss': 0.013544830493628979, 'train/mean_average_precision': 0.7969386333438457, 'validation/accuracy': 0.9860498309135437, 'validation/loss': 0.059502821415662766, 'validation/mean_average_precision': 0.22839872557557458, 'validation/num_examples': 43793, 'test/accuracy': 0.9851149320602417, 'test/loss': 0.06383056193590164, 'test/mean_average_precision': 0.21641405845233108, 'test/num_examples': 43793, 'score': 17541.0244076252, 'total_duration': 26385.043430566788, 'accumulated_submission_time': 17541.0244076252, 'accumulated_eval_time': 8839.761295318604, 'accumulated_logging_time': 2.7525038719177246, 'global_step': 54412, 'preemption_count': 0}), (55167, {'train/accuracy': 0.9960854053497314, 'train/loss': 0.012709268368780613, 'train/mean_average_precision': 0.8059383088169321, 'validation/accuracy': 0.9860132932662964, 'validation/loss': 0.05940846726298332, 'validation/mean_average_precision': 0.22421516606585493, 'validation/num_examples': 43793, 'test/accuracy': 0.9850315451622009, 'test/loss': 0.06362029165029526, 'test/mean_average_precision': 0.2174672035747131, 'test/num_examples': 43793, 'score': 17781.22932624817, 'total_duration': 26739.52824115753, 'accumulated_submission_time': 17781.22932624817, 'accumulated_eval_time': 8953.982422113419, 'accumulated_logging_time': 2.7906315326690674, 'global_step': 55167, 'preemption_count': 0}), (55905, {'train/accuracy': 0.9968048930168152, 'train/loss': 0.011059361509978771, 'train/mean_average_precision': 0.8559029305644175, 'validation/accuracy': 0.9860246181488037, 'validation/loss': 0.06033738702535629, 'validation/mean_average_precision': 0.2279856110308997, 'validation/num_examples': 43793, 'test/accuracy': 0.9850290417671204, 'test/loss': 0.06488261371850967, 'test/mean_average_precision': 0.21261188809277975, 'test/num_examples': 43793, 'score': 18021.468203783035, 'total_duration': 27097.05589222908, 'accumulated_submission_time': 18021.468203783035, 'accumulated_eval_time': 9071.203595876694, 'accumulated_logging_time': 2.8327012062072754, 'global_step': 55905, 'preemption_count': 0}), (56650, {'train/accuracy': 0.9976639151573181, 'train/loss': 0.009552651084959507, 'train/mean_average_precision': 0.8787869832304159, 'validation/accuracy': 0.985992968082428, 'validation/loss': 0.060625217854976654, 'validation/mean_average_precision': 0.22720330974813813, 'validation/num_examples': 43793, 'test/accuracy': 0.9850433468818665, 'test/loss': 0.06487562507390976, 'test/mean_average_precision': 0.21501642913040128, 'test/num_examples': 43793, 'score': 18261.61960697174, 'total_duration': 27448.26845598221, 'accumulated_submission_time': 18261.61960697174, 'accumulated_eval_time': 9182.202111959457, 'accumulated_logging_time': 2.873664617538452, 'global_step': 56650, 'preemption_count': 0})], 'global_step': 57322}
I0205 13:23:36.950019 139919816816448 submission_runner.py:586] Timing: 18477.167249679565
I0205 13:23:36.950076 139919816816448 submission_runner.py:588] Total number of evals: 77
I0205 13:23:36.950118 139919816816448 submission_runner.py:589] ====================
I0205 13:23:36.950164 139919816816448 submission_runner.py:542] Using RNG seed 356686224
I0205 13:23:37.019165 139919816816448 submission_runner.py:551] --- Tuning run 3/5 ---
I0205 13:23:37.019327 139919816816448 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_1/ogbg_jax/trial_3.
I0205 13:23:37.019770 139919816816448 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_1/ogbg_jax/trial_3/hparams.json.
I0205 13:23:37.153775 139919816816448 submission_runner.py:206] Initializing dataset.
I0205 13:23:37.242630 139919816816448 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0205 13:23:37.246981 139919816816448 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0205 13:23:37.390183 139919816816448 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0205 13:23:37.427700 139919816816448 submission_runner.py:213] Initializing model.
I0205 13:23:39.879330 139919816816448 submission_runner.py:255] Initializing optimizer.
I0205 13:23:40.475148 139919816816448 submission_runner.py:262] Initializing metrics bundle.
I0205 13:23:40.475344 139919816816448 submission_runner.py:280] Initializing checkpoint and logger.
I0205 13:23:40.476085 139919816816448 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_1/ogbg_jax/trial_3 with prefix checkpoint_
I0205 13:23:40.476227 139919816816448 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_1/ogbg_jax/trial_3/meta_data_0.json.
I0205 13:23:40.476447 139919816816448 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0205 13:23:40.476529 139919816816448 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0205 13:23:42.159662 139919816816448 logger_utils.py:220] Unable to record git information. Continuing without it.
I0205 13:23:43.828800 139919816816448 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_1/ogbg_jax/trial_3/flags_0.json.
I0205 13:23:43.832775 139919816816448 submission_runner.py:314] Starting training loop.
I0205 13:23:55.314062 139716963481344 logging_writer.py:48] [0] global_step=0, grad_norm=2.527869462966919, loss=0.726763129234314
I0205 13:23:55.324282 139919816816448 spec.py:321] Evaluating on the training split.
I0205 13:25:43.487539 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 13:25:46.571197 139919816816448 spec.py:349] Evaluating on the test split.
I0205 13:25:49.569714 139919816816448 submission_runner.py:408] Time since start: 125.74s, 	Step: 1, 	{'train/accuracy': 0.5324387550354004, 'train/loss': 0.7276949882507324, 'train/mean_average_precision': 0.02166684367684784, 'validation/accuracy': 0.5230699777603149, 'validation/loss': 0.7331878542900085, 'validation/mean_average_precision': 0.02556818984903369, 'validation/num_examples': 43793, 'test/accuracy': 0.5214916467666626, 'test/loss': 0.7347381114959717, 'test/mean_average_precision': 0.026822867578732854, 'test/num_examples': 43793, 'score': 11.491451501846313, 'total_duration': 125.73687386512756, 'accumulated_submission_time': 11.491451501846313, 'accumulated_eval_time': 114.24537301063538, 'accumulated_logging_time': 0}
I0205 13:25:49.579396 139735693903616 logging_writer.py:48] [1] accumulated_eval_time=114.245373, accumulated_logging_time=0, accumulated_submission_time=11.491452, global_step=1, preemption_count=0, score=11.491452, test/accuracy=0.521492, test/loss=0.734738, test/mean_average_precision=0.026823, test/num_examples=43793, total_duration=125.736874, train/accuracy=0.532439, train/loss=0.727695, train/mean_average_precision=0.021667, validation/accuracy=0.523070, validation/loss=0.733188, validation/mean_average_precision=0.025568, validation/num_examples=43793
I0205 13:26:21.931456 139752296675072 logging_writer.py:48] [100] global_step=100, grad_norm=0.6399087905883789, loss=0.44354528188705444
I0205 13:26:54.475191 139735693903616 logging_writer.py:48] [200] global_step=200, grad_norm=0.3658697009086609, loss=0.3311152756214142
I0205 13:27:26.806229 139752296675072 logging_writer.py:48] [300] global_step=300, grad_norm=0.2687597870826721, loss=0.2370903193950653
I0205 13:27:59.182605 139735693903616 logging_writer.py:48] [400] global_step=400, grad_norm=0.1772719770669937, loss=0.1636001318693161
I0205 13:28:31.413371 139752296675072 logging_writer.py:48] [500] global_step=500, grad_norm=0.10999312251806259, loss=0.11831049621105194
I0205 13:29:03.777800 139735693903616 logging_writer.py:48] [600] global_step=600, grad_norm=0.07090292125940323, loss=0.08696679025888443
I0205 13:29:35.921382 139752296675072 logging_writer.py:48] [700] global_step=700, grad_norm=0.0463285930454731, loss=0.07461165636777878
I0205 13:29:49.796348 139919816816448 spec.py:321] Evaluating on the training split.
I0205 13:31:40.469030 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 13:31:43.519170 139919816816448 spec.py:349] Evaluating on the test split.
I0205 13:31:46.563433 139919816816448 submission_runner.py:408] Time since start: 482.73s, 	Step: 745, 	{'train/accuracy': 0.9867120385169983, 'train/loss': 0.0699601098895073, 'train/mean_average_precision': 0.03136492398996348, 'validation/accuracy': 0.9841179251670837, 'validation/loss': 0.07792092114686966, 'validation/mean_average_precision': 0.03357064180639783, 'validation/num_examples': 43793, 'test/accuracy': 0.983142077922821, 'test/loss': 0.08070765435695648, 'test/mean_average_precision': 0.03482637536778557, 'test/num_examples': 43793, 'score': 251.67628693580627, 'total_duration': 482.73058128356934, 'accumulated_submission_time': 251.67628693580627, 'accumulated_eval_time': 231.01239943504333, 'accumulated_logging_time': 0.02175736427307129}
I0205 13:31:46.579716 139716992366336 logging_writer.py:48] [745] accumulated_eval_time=231.012399, accumulated_logging_time=0.021757, accumulated_submission_time=251.676287, global_step=745, preemption_count=0, score=251.676287, test/accuracy=0.983142, test/loss=0.080708, test/mean_average_precision=0.034826, test/num_examples=43793, total_duration=482.730581, train/accuracy=0.986712, train/loss=0.069960, train/mean_average_precision=0.031365, validation/accuracy=0.984118, validation/loss=0.077921, validation/mean_average_precision=0.033571, validation/num_examples=43793
I0205 13:32:04.606503 139735302588160 logging_writer.py:48] [800] global_step=800, grad_norm=0.056027330458164215, loss=0.06633724272251129
I0205 13:32:36.603560 139716992366336 logging_writer.py:48] [900] global_step=900, grad_norm=0.173196479678154, loss=0.06263812631368637
I0205 13:33:09.252669 139735302588160 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.0694814920425415, loss=0.056953806430101395
I0205 13:33:41.644090 139716992366336 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.26991158723831177, loss=0.05419160798192024
I0205 13:34:14.086389 139735302588160 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.0924992635846138, loss=0.04933130741119385
I0205 13:34:47.205166 139716992366336 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.09585200995206833, loss=0.05534914880990982
I0205 13:35:20.222439 139735302588160 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.16517382860183716, loss=0.04697887599468231
I0205 13:35:46.809839 139919816816448 spec.py:321] Evaluating on the training split.
I0205 13:37:37.116110 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 13:37:40.479339 139919816816448 spec.py:349] Evaluating on the test split.
I0205 13:37:43.630237 139919816816448 submission_runner.py:408] Time since start: 839.80s, 	Step: 1483, 	{'train/accuracy': 0.9868988394737244, 'train/loss': 0.05043470859527588, 'train/mean_average_precision': 0.07136894122746727, 'validation/accuracy': 0.9844674468040466, 'validation/loss': 0.05963057279586792, 'validation/mean_average_precision': 0.06384708825885257, 'validation/num_examples': 43793, 'test/accuracy': 0.9834638833999634, 'test/loss': 0.06271059066057205, 'test/mean_average_precision': 0.06910000269425054, 'test/num_examples': 43793, 'score': 491.8714425563812, 'total_duration': 839.7973530292511, 'accumulated_submission_time': 491.8714425563812, 'accumulated_eval_time': 347.83271861076355, 'accumulated_logging_time': 0.050313711166381836}
I0205 13:37:43.647664 139739177916160 logging_writer.py:48] [1483] accumulated_eval_time=347.832719, accumulated_logging_time=0.050314, accumulated_submission_time=491.871443, global_step=1483, preemption_count=0, score=491.871443, test/accuracy=0.983464, test/loss=0.062711, test/mean_average_precision=0.069100, test/num_examples=43793, total_duration=839.797353, train/accuracy=0.986899, train/loss=0.050435, train/mean_average_precision=0.071369, validation/accuracy=0.984467, validation/loss=0.059631, validation/mean_average_precision=0.063847, validation/num_examples=43793
I0205 13:37:49.435562 139752296675072 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.1945255845785141, loss=0.05593445524573326
I0205 13:38:21.883461 139739177916160 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.0735226646065712, loss=0.04800872877240181
I0205 13:38:54.769403 139752296675072 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.31626036763191223, loss=0.05425874516367912
I0205 13:39:27.045422 139739177916160 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.0668514296412468, loss=0.04404959827661514
I0205 13:39:59.378160 139752296675072 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.23380146920681, loss=0.0455150231719017
I0205 13:40:31.355980 139739177916160 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.15649494528770447, loss=0.046766821295022964
I0205 13:41:03.617120 139752296675072 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.10633885115385056, loss=0.04779592528939247
I0205 13:41:35.710720 139739177916160 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.09110577404499054, loss=0.049360305070877075
I0205 13:41:43.863349 139919816816448 spec.py:321] Evaluating on the training split.
I0205 13:43:34.817776 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 13:43:38.003963 139919816816448 spec.py:349] Evaluating on the test split.
I0205 13:43:41.097683 139919816816448 submission_runner.py:408] Time since start: 1197.26s, 	Step: 2227, 	{'train/accuracy': 0.9876678586006165, 'train/loss': 0.04488957300782204, 'train/mean_average_precision': 0.12329825632978675, 'validation/accuracy': 0.9848230481147766, 'validation/loss': 0.05412972345948219, 'validation/mean_average_precision': 0.11911754906630341, 'validation/num_examples': 43793, 'test/accuracy': 0.9839284420013428, 'test/loss': 0.05710911378264427, 'test/mean_average_precision': 0.12057239133619276, 'test/num_examples': 43793, 'score': 732.0550570487976, 'total_duration': 1197.2648441791534, 'accumulated_submission_time': 732.0550570487976, 'accumulated_eval_time': 465.06700444221497, 'accumulated_logging_time': 0.07920026779174805}
I0205 13:43:41.113371 139716992366336 logging_writer.py:48] [2227] accumulated_eval_time=465.067004, accumulated_logging_time=0.079200, accumulated_submission_time=732.055057, global_step=2227, preemption_count=0, score=732.055057, test/accuracy=0.983928, test/loss=0.057109, test/mean_average_precision=0.120572, test/num_examples=43793, total_duration=1197.264844, train/accuracy=0.987668, train/loss=0.044890, train/mean_average_precision=0.123298, validation/accuracy=0.984823, validation/loss=0.054130, validation/mean_average_precision=0.119118, validation/num_examples=43793
I0205 13:44:04.852858 139735302588160 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.1747646927833557, loss=0.04954589158296585
I0205 13:44:37.110341 139716992366336 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.12483620643615723, loss=0.04110145568847656
I0205 13:45:09.349516 139735302588160 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.3073626756668091, loss=0.04440498724579811
I0205 13:45:41.405374 139716992366336 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.15832477807998657, loss=0.040507279336452484
I0205 13:46:13.543435 139735302588160 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.1512794941663742, loss=0.041122015565633774
I0205 13:46:45.535069 139716992366336 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.1264416128396988, loss=0.04260198771953583
I0205 13:47:17.463790 139735302588160 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.15499956905841827, loss=0.04771292954683304
I0205 13:47:41.363849 139919816816448 spec.py:321] Evaluating on the training split.
I0205 13:49:29.876461 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 13:49:33.034382 139919816816448 spec.py:349] Evaluating on the test split.
I0205 13:49:36.099916 139919816816448 submission_runner.py:408] Time since start: 1552.27s, 	Step: 2976, 	{'train/accuracy': 0.9881260991096497, 'train/loss': 0.04235072061419487, 'train/mean_average_precision': 0.15038947347985554, 'validation/accuracy': 0.9851656556129456, 'validation/loss': 0.051759861409664154, 'validation/mean_average_precision': 0.1425854816835852, 'validation/num_examples': 43793, 'test/accuracy': 0.9842203259468079, 'test/loss': 0.054486583918333054, 'test/mean_average_precision': 0.14665337020102068, 'test/num_examples': 43793, 'score': 972.2738552093506, 'total_duration': 1552.2670772075653, 'accumulated_submission_time': 972.2738552093506, 'accumulated_eval_time': 579.8030261993408, 'accumulated_logging_time': 0.10624456405639648}
I0205 13:49:36.115705 139739177916160 logging_writer.py:48] [2976] accumulated_eval_time=579.803026, accumulated_logging_time=0.106245, accumulated_submission_time=972.273855, global_step=2976, preemption_count=0, score=972.273855, test/accuracy=0.984220, test/loss=0.054487, test/mean_average_precision=0.146653, test/num_examples=43793, total_duration=1552.267077, train/accuracy=0.988126, train/loss=0.042351, train/mean_average_precision=0.150389, validation/accuracy=0.985166, validation/loss=0.051760, validation/mean_average_precision=0.142585, validation/num_examples=43793
I0205 13:49:44.418922 139752296675072 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.09294542670249939, loss=0.04342944547533989
I0205 13:50:16.902276 139739177916160 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.10564645379781723, loss=0.0402473509311676
I0205 13:50:49.435082 139752296675072 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.08180879056453705, loss=0.04184020310640335
I0205 13:51:22.035668 139739177916160 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.10171593725681305, loss=0.04120282083749771
I0205 13:51:54.477902 139752296675072 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.06250941753387451, loss=0.03714030236005783
I0205 13:52:26.769897 139739177916160 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.11052367836236954, loss=0.039773035794496536
I0205 13:52:58.863913 139752296675072 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.16559642553329468, loss=0.03955334797501564
I0205 13:53:30.972303 139739177916160 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.06188676133751869, loss=0.04167430102825165
I0205 13:53:36.346048 139919816816448 spec.py:321] Evaluating on the training split.
I0205 13:55:32.049861 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 13:55:35.130377 139919816816448 spec.py:349] Evaluating on the test split.
I0205 13:55:38.167140 139919816816448 submission_runner.py:408] Time since start: 1914.33s, 	Step: 3718, 	{'train/accuracy': 0.9882087111473083, 'train/loss': 0.0411868691444397, 'train/mean_average_precision': 0.17895131295690636, 'validation/accuracy': 0.9852659106254578, 'validation/loss': 0.05065610259771347, 'validation/mean_average_precision': 0.17215355653285436, 'validation/num_examples': 43793, 'test/accuracy': 0.9843053817749023, 'test/loss': 0.05346042662858963, 'test/mean_average_precision': 0.1665679376148152, 'test/num_examples': 43793, 'score': 1212.473397731781, 'total_duration': 1914.3342943191528, 'accumulated_submission_time': 1212.473397731781, 'accumulated_eval_time': 701.6240711212158, 'accumulated_logging_time': 0.13290810585021973}
I0205 13:55:38.183001 139735302588160 logging_writer.py:48] [3718] accumulated_eval_time=701.624071, accumulated_logging_time=0.132908, accumulated_submission_time=1212.473398, global_step=3718, preemption_count=0, score=1212.473398, test/accuracy=0.984305, test/loss=0.053460, test/mean_average_precision=0.166568, test/num_examples=43793, total_duration=1914.334294, train/accuracy=0.988209, train/loss=0.041187, train/mean_average_precision=0.178951, validation/accuracy=0.985266, validation/loss=0.050656, validation/mean_average_precision=0.172154, validation/num_examples=43793
I0205 13:56:05.043564 139735693903616 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.05221480503678322, loss=0.03688548505306244
I0205 13:56:37.568042 139735302588160 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.16767652332782745, loss=0.03986383229494095
I0205 13:57:10.038707 139735693903616 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.059239305555820465, loss=0.04277064651250839
I0205 13:57:42.401008 139735302588160 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.07499895989894867, loss=0.03966459631919861
I0205 13:58:14.404506 139735693903616 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.06009963899850845, loss=0.0358506441116333
I0205 13:58:46.881328 139735302588160 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.08953725546598434, loss=0.039321769028902054
I0205 13:59:19.098356 139735693903616 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.06007593870162964, loss=0.038138698786497116
I0205 13:59:38.387435 139919816816448 spec.py:321] Evaluating on the training split.
I0205 14:01:26.929105 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 14:01:30.145934 139919816816448 spec.py:349] Evaluating on the test split.
I0205 14:01:33.274063 139919816816448 submission_runner.py:408] Time since start: 2269.44s, 	Step: 4461, 	{'train/accuracy': 0.988564133644104, 'train/loss': 0.039611171931028366, 'train/mean_average_precision': 0.20526388658012243, 'validation/accuracy': 0.9856268167495728, 'validation/loss': 0.04916029050946236, 'validation/mean_average_precision': 0.18699333950853866, 'validation/num_examples': 43793, 'test/accuracy': 0.9846844673156738, 'test/loss': 0.051958292722702026, 'test/mean_average_precision': 0.18455530274110227, 'test/num_examples': 43793, 'score': 1452.6450970172882, 'total_duration': 2269.441216945648, 'accumulated_submission_time': 1452.6450970172882, 'accumulated_eval_time': 816.5106444358826, 'accumulated_logging_time': 0.1609640121459961}
I0205 14:01:33.292234 139716992366336 logging_writer.py:48] [4461] accumulated_eval_time=816.510644, accumulated_logging_time=0.160964, accumulated_submission_time=1452.645097, global_step=4461, preemption_count=0, score=1452.645097, test/accuracy=0.984684, test/loss=0.051958, test/mean_average_precision=0.184555, test/num_examples=43793, total_duration=2269.441217, train/accuracy=0.988564, train/loss=0.039611, train/mean_average_precision=0.205264, validation/accuracy=0.985627, validation/loss=0.049160, validation/mean_average_precision=0.186993, validation/num_examples=43793
I0205 14:01:46.491243 139739177916160 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.04333918169140816, loss=0.03937302902340889
I0205 14:02:18.687206 139716992366336 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.07674811035394669, loss=0.0433017835021019
I0205 14:02:51.214361 139739177916160 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.05897932127118111, loss=0.03510335832834244
I0205 14:03:23.518803 139716992366336 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.07284913957118988, loss=0.03930485621094704
I0205 14:03:56.158848 139739177916160 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.050106026232242584, loss=0.03650977090001106
I0205 14:04:28.395821 139716992366336 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.07318804413080215, loss=0.04056984931230545
I0205 14:05:00.374318 139739177916160 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.053152911365032196, loss=0.036437179893255234
I0205 14:05:32.813112 139716992366336 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.03827902302145958, loss=0.04033846780657768
I0205 14:05:33.478301 139919816816448 spec.py:321] Evaluating on the training split.
I0205 14:07:21.362974 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 14:07:24.435088 139919816816448 spec.py:349] Evaluating on the test split.
I0205 14:07:27.431969 139919816816448 submission_runner.py:408] Time since start: 2623.60s, 	Step: 5203, 	{'train/accuracy': 0.9885818362236023, 'train/loss': 0.03906910866498947, 'train/mean_average_precision': 0.21580086321456005, 'validation/accuracy': 0.9856730699539185, 'validation/loss': 0.04913453385233879, 'validation/mean_average_precision': 0.18620651336788618, 'validation/num_examples': 43793, 'test/accuracy': 0.984832763671875, 'test/loss': 0.05178947001695633, 'test/mean_average_precision': 0.18533525819395427, 'test/num_examples': 43793, 'score': 1692.7988908290863, 'total_duration': 2623.5991291999817, 'accumulated_submission_time': 1692.7988908290863, 'accumulated_eval_time': 930.4642643928528, 'accumulated_logging_time': 0.19129252433776855}
I0205 14:07:27.447845 139735693903616 logging_writer.py:48] [5203] accumulated_eval_time=930.464264, accumulated_logging_time=0.191293, accumulated_submission_time=1692.798891, global_step=5203, preemption_count=0, score=1692.798891, test/accuracy=0.984833, test/loss=0.051789, test/mean_average_precision=0.185335, test/num_examples=43793, total_duration=2623.599129, train/accuracy=0.988582, train/loss=0.039069, train/mean_average_precision=0.215801, validation/accuracy=0.985673, validation/loss=0.049135, validation/mean_average_precision=0.186207, validation/num_examples=43793
I0205 14:07:59.191299 139752296675072 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.05605144798755646, loss=0.03717665374279022
I0205 14:08:31.188076 139735693903616 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.05550239235162735, loss=0.045257773250341415
I0205 14:09:03.325665 139752296675072 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.061633914709091187, loss=0.04067488759756088
I0205 14:09:35.730369 139735693903616 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.06093695014715195, loss=0.04024210944771767
I0205 14:10:07.834346 139752296675072 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.08226580917835236, loss=0.0325806550681591
I0205 14:10:40.346879 139735693903616 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.1213853731751442, loss=0.039310529828071594
I0205 14:11:12.916105 139752296675072 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.04504016414284706, loss=0.035966139286756516
I0205 14:11:27.436266 139919816816448 spec.py:321] Evaluating on the training split.
I0205 14:13:09.724031 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 14:13:13.164657 139919816816448 spec.py:349] Evaluating on the test split.
I0205 14:13:16.503596 139919816816448 submission_runner.py:408] Time since start: 2972.67s, 	Step: 5946, 	{'train/accuracy': 0.9889664649963379, 'train/loss': 0.0375383198261261, 'train/mean_average_precision': 0.2455382972249783, 'validation/accuracy': 0.9859256148338318, 'validation/loss': 0.047312334179878235, 'validation/mean_average_precision': 0.21032265999022526, 'validation/num_examples': 43793, 'test/accuracy': 0.9849889874458313, 'test/loss': 0.05008397996425629, 'test/mean_average_precision': 0.19994347087606382, 'test/num_examples': 43793, 'score': 1932.7561764717102, 'total_duration': 2972.670731782913, 'accumulated_submission_time': 1932.7561764717102, 'accumulated_eval_time': 1039.5315363407135, 'accumulated_logging_time': 0.2181088924407959}
I0205 14:13:16.522640 139735302588160 logging_writer.py:48] [5946] accumulated_eval_time=1039.531536, accumulated_logging_time=0.218109, accumulated_submission_time=1932.756176, global_step=5946, preemption_count=0, score=1932.756176, test/accuracy=0.984989, test/loss=0.050084, test/mean_average_precision=0.199943, test/num_examples=43793, total_duration=2972.670732, train/accuracy=0.988966, train/loss=0.037538, train/mean_average_precision=0.245538, validation/accuracy=0.985926, validation/loss=0.047312, validation/mean_average_precision=0.210323, validation/num_examples=43793
I0205 14:13:34.592293 139759038281472 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.06568343937397003, loss=0.03444339334964752
I0205 14:14:07.399670 139735302588160 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.05376099422574043, loss=0.033514354377985
I0205 14:14:40.041133 139759038281472 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.04083387926220894, loss=0.037028536200523376
I0205 14:15:12.568628 139735302588160 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.03934566676616669, loss=0.03738754615187645
I0205 14:15:44.977698 139759038281472 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.052630357444286346, loss=0.03833970054984093
I0205 14:16:17.488552 139735302588160 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.05311726778745651, loss=0.03783213719725609
I0205 14:16:49.969613 139759038281472 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.02998003363609314, loss=0.03454384207725525
I0205 14:17:16.664690 139919816816448 spec.py:321] Evaluating on the training split.
I0205 14:19:02.283784 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 14:19:05.391340 139919816816448 spec.py:349] Evaluating on the test split.
I0205 14:19:08.344532 139919816816448 submission_runner.py:408] Time since start: 3324.51s, 	Step: 6683, 	{'train/accuracy': 0.989159107208252, 'train/loss': 0.03690975904464722, 'train/mean_average_precision': 0.25773455228098463, 'validation/accuracy': 0.9858171939849854, 'validation/loss': 0.04761821776628494, 'validation/mean_average_precision': 0.2067422098356613, 'validation/num_examples': 43793, 'test/accuracy': 0.9849334359169006, 'test/loss': 0.050239451229572296, 'test/mean_average_precision': 0.2088082447016146, 'test/num_examples': 43793, 'score': 2172.862287044525, 'total_duration': 3324.5116848945618, 'accumulated_submission_time': 2172.862287044525, 'accumulated_eval_time': 1151.2113268375397, 'accumulated_logging_time': 0.250399112701416}
I0205 14:19:08.361040 139739177916160 logging_writer.py:48] [6683] accumulated_eval_time=1151.211327, accumulated_logging_time=0.250399, accumulated_submission_time=2172.862287, global_step=6683, preemption_count=0, score=2172.862287, test/accuracy=0.984933, test/loss=0.050239, test/mean_average_precision=0.208808, test/num_examples=43793, total_duration=3324.511685, train/accuracy=0.989159, train/loss=0.036910, train/mean_average_precision=0.257735, validation/accuracy=0.985817, validation/loss=0.047618, validation/mean_average_precision=0.206742, validation/num_examples=43793
I0205 14:19:14.279779 139752296675072 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.05269132927060127, loss=0.03978550434112549
I0205 14:19:46.221424 139739177916160 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.030456069856882095, loss=0.035970788449048996
I0205 14:20:18.703679 139752296675072 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.05590377375483513, loss=0.03657078370451927
I0205 14:20:50.703510 139739177916160 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.04313230887055397, loss=0.037141066044569016
I0205 14:21:22.792449 139752296675072 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.0305479783564806, loss=0.0413435623049736
I0205 14:21:54.677420 139739177916160 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.03406152129173279, loss=0.03505377471446991
I0205 14:22:26.628223 139752296675072 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.040500033646821976, loss=0.035942193120718
I0205 14:22:59.081340 139739177916160 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.046132054179906845, loss=0.039704468101263046
I0205 14:23:08.574749 139919816816448 spec.py:321] Evaluating on the training split.
I0205 14:24:58.611895 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 14:25:01.775979 139919816816448 spec.py:349] Evaluating on the test split.
I0205 14:25:04.778857 139919816816448 submission_runner.py:408] Time since start: 3680.95s, 	Step: 7430, 	{'train/accuracy': 0.9891023635864258, 'train/loss': 0.036981549113988876, 'train/mean_average_precision': 0.2569534259312815, 'validation/accuracy': 0.9860132932662964, 'validation/loss': 0.047048043459653854, 'validation/mean_average_precision': 0.21624403118395114, 'validation/num_examples': 43793, 'test/accuracy': 0.9851793646812439, 'test/loss': 0.04973500967025757, 'test/mean_average_precision': 0.21493407644466025, 'test/num_examples': 43793, 'score': 2413.044668197632, 'total_duration': 3680.9460167884827, 'accumulated_submission_time': 2413.044668197632, 'accumulated_eval_time': 1267.4153888225555, 'accumulated_logging_time': 0.27773523330688477}
I0205 14:25:04.795879 139735693903616 logging_writer.py:48] [7430] accumulated_eval_time=1267.415389, accumulated_logging_time=0.277735, accumulated_submission_time=2413.044668, global_step=7430, preemption_count=0, score=2413.044668, test/accuracy=0.985179, test/loss=0.049735, test/mean_average_precision=0.214934, test/num_examples=43793, total_duration=3680.946017, train/accuracy=0.989102, train/loss=0.036982, train/mean_average_precision=0.256953, validation/accuracy=0.986013, validation/loss=0.047048, validation/mean_average_precision=0.216244, validation/num_examples=43793
I0205 14:25:27.599450 139759038281472 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.02955997921526432, loss=0.03850614279508591
I0205 14:25:59.914434 139735693903616 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.040418460965156555, loss=0.03877989575266838
I0205 14:26:32.455542 139759038281472 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.03638211265206337, loss=0.03542107716202736
I0205 14:27:04.795419 139735693903616 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.07237395644187927, loss=0.0365983210504055
I0205 14:27:37.610525 139759038281472 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.037157729268074036, loss=0.03518485650420189
I0205 14:28:10.664569 139735693903616 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.05703199654817581, loss=0.04003937169909477
I0205 14:28:43.539882 139759038281472 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.035709697753190994, loss=0.0368930846452713
I0205 14:29:04.873889 139919816816448 spec.py:321] Evaluating on the training split.
I0205 14:30:49.213936 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 14:30:52.269116 139919816816448 spec.py:349] Evaluating on the test split.
I0205 14:30:55.339185 139919816816448 submission_runner.py:408] Time since start: 4031.51s, 	Step: 8166, 	{'train/accuracy': 0.9891697764396667, 'train/loss': 0.03671594336628914, 'train/mean_average_precision': 0.25753546882741873, 'validation/accuracy': 0.9860778450965881, 'validation/loss': 0.047110870480537415, 'validation/mean_average_precision': 0.21821118509284348, 'validation/num_examples': 43793, 'test/accuracy': 0.9851751923561096, 'test/loss': 0.05003559589385986, 'test/mean_average_precision': 0.21506364420129348, 'test/num_examples': 43793, 'score': 2653.0890650749207, 'total_duration': 4031.506335258484, 'accumulated_submission_time': 2653.0890650749207, 'accumulated_eval_time': 1377.8806405067444, 'accumulated_logging_time': 0.30559682846069336}
I0205 14:30:55.356899 139752296675072 logging_writer.py:48] [8166] accumulated_eval_time=1377.880641, accumulated_logging_time=0.305597, accumulated_submission_time=2653.089065, global_step=8166, preemption_count=0, score=2653.089065, test/accuracy=0.985175, test/loss=0.050036, test/mean_average_precision=0.215064, test/num_examples=43793, total_duration=4031.506335, train/accuracy=0.989170, train/loss=0.036716, train/mean_average_precision=0.257535, validation/accuracy=0.986078, validation/loss=0.047111, validation/mean_average_precision=0.218211, validation/num_examples=43793
I0205 14:31:06.623599 139759013103360 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.04761072248220444, loss=0.033641498535871506
I0205 14:31:38.886441 139752296675072 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.02840067632496357, loss=0.036006443202495575
I0205 14:32:11.133087 139759013103360 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.029131362214684486, loss=0.037549540400505066
I0205 14:32:43.168705 139752296675072 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.02959434501826763, loss=0.03428647294640541
I0205 14:33:15.276058 139759013103360 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.036767374724149704, loss=0.034853193908929825
I0205 14:33:47.512162 139752296675072 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.025804845616221428, loss=0.03414328023791313
I0205 14:34:19.552013 139759013103360 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.03855157271027565, loss=0.038910090923309326
I0205 14:34:51.255454 139752296675072 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.029206305742263794, loss=0.03734325245022774
I0205 14:34:55.460418 139919816816448 spec.py:321] Evaluating on the training split.
I0205 14:36:39.352352 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 14:36:44.325243 139919816816448 spec.py:349] Evaluating on the test split.
I0205 14:36:47.394157 139919816816448 submission_runner.py:408] Time since start: 4383.56s, 	Step: 8914, 	{'train/accuracy': 0.9893730878829956, 'train/loss': 0.035877127200365067, 'train/mean_average_precision': 0.2799335236621727, 'validation/accuracy': 0.9860900044441223, 'validation/loss': 0.04634569585323334, 'validation/mean_average_precision': 0.2231633377875584, 'validation/num_examples': 43793, 'test/accuracy': 0.9852905869483948, 'test/loss': 0.04900343716144562, 'test/mean_average_precision': 0.22060165025319775, 'test/num_examples': 43793, 'score': 2893.1614439487457, 'total_duration': 4383.561316490173, 'accumulated_submission_time': 2893.1614439487457, 'accumulated_eval_time': 1489.8143291473389, 'accumulated_logging_time': 0.3341653347015381}
I0205 14:36:47.411501 139735693903616 logging_writer.py:48] [8914] accumulated_eval_time=1489.814329, accumulated_logging_time=0.334165, accumulated_submission_time=2893.161444, global_step=8914, preemption_count=0, score=2893.161444, test/accuracy=0.985291, test/loss=0.049003, test/mean_average_precision=0.220602, test/num_examples=43793, total_duration=4383.561316, train/accuracy=0.989373, train/loss=0.035877, train/mean_average_precision=0.279934, validation/accuracy=0.986090, validation/loss=0.046346, validation/mean_average_precision=0.223163, validation/num_examples=43793
I0205 14:37:15.349431 139759038281472 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.02855686843395233, loss=0.039145614951848984
I0205 14:37:47.427085 139735693903616 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.024077747017145157, loss=0.03595959395170212
I0205 14:38:19.646508 139759038281472 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.03183939680457115, loss=0.0364755317568779
I0205 14:38:52.307040 139735693903616 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.044373929500579834, loss=0.03668064624071121
I0205 14:39:24.776990 139759038281472 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.03175562620162964, loss=0.03703673928976059
I0205 14:39:56.964673 139735693903616 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.03094227984547615, loss=0.033889591693878174
I0205 14:40:29.333745 139759038281472 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.03720296174287796, loss=0.029390953481197357
I0205 14:40:47.584155 139919816816448 spec.py:321] Evaluating on the training split.
I0205 14:42:33.716897 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 14:42:36.855911 139919816816448 spec.py:349] Evaluating on the test split.
I0205 14:42:39.948408 139919816816448 submission_runner.py:408] Time since start: 4736.12s, 	Step: 9658, 	{'train/accuracy': 0.9894848465919495, 'train/loss': 0.035306576639413834, 'train/mean_average_precision': 0.29507402958103773, 'validation/accuracy': 0.9863181114196777, 'validation/loss': 0.04575859755277634, 'validation/mean_average_precision': 0.23779549555324025, 'validation/num_examples': 43793, 'test/accuracy': 0.9855382442474365, 'test/loss': 0.04845280200242996, 'test/mean_average_precision': 0.23480798784396492, 'test/num_examples': 43793, 'score': 3133.3014283180237, 'total_duration': 4736.115564584732, 'accumulated_submission_time': 3133.3014283180237, 'accumulated_eval_time': 1602.1785380840302, 'accumulated_logging_time': 0.36263275146484375}
I0205 14:42:39.966839 139739177916160 logging_writer.py:48] [9658] accumulated_eval_time=1602.178538, accumulated_logging_time=0.362633, accumulated_submission_time=3133.301428, global_step=9658, preemption_count=0, score=3133.301428, test/accuracy=0.985538, test/loss=0.048453, test/mean_average_precision=0.234808, test/num_examples=43793, total_duration=4736.115565, train/accuracy=0.989485, train/loss=0.035307, train/mean_average_precision=0.295074, validation/accuracy=0.986318, validation/loss=0.045759, validation/mean_average_precision=0.237795, validation/num_examples=43793
I0205 14:42:54.123376 139759013103360 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.036080360412597656, loss=0.035206347703933716
I0205 14:43:26.579267 139739177916160 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.05291702598333359, loss=0.039051081985235214
I0205 14:43:58.718867 139759013103360 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.053004272282123566, loss=0.03870921581983566
I0205 14:44:30.827749 139739177916160 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.035431403666734695, loss=0.03212282434105873
I0205 14:45:03.007932 139759013103360 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.05377431586384773, loss=0.03479086235165596
I0205 14:45:34.894040 139739177916160 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.040798526257276535, loss=0.034158896654844284
I0205 14:46:06.801829 139759013103360 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.04833153635263443, loss=0.03834553807973862
I0205 14:46:38.493713 139739177916160 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.047192614525556564, loss=0.03494904935359955
I0205 14:46:40.096784 139919816816448 spec.py:321] Evaluating on the training split.
I0205 14:48:20.054241 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 14:48:23.499045 139919816816448 spec.py:349] Evaluating on the test split.
I0205 14:48:29.342234 139919816816448 submission_runner.py:408] Time since start: 5085.51s, 	Step: 10406, 	{'train/accuracy': 0.9897177815437317, 'train/loss': 0.0344165675342083, 'train/mean_average_precision': 0.31707895892708327, 'validation/accuracy': 0.9863526225090027, 'validation/loss': 0.045562926679849625, 'validation/mean_average_precision': 0.23729907976726822, 'validation/num_examples': 43793, 'test/accuracy': 0.9855803847312927, 'test/loss': 0.0481531098484993, 'test/mean_average_precision': 0.23562830229534015, 'test/num_examples': 43793, 'score': 3373.3994760513306, 'total_duration': 5085.509356498718, 'accumulated_submission_time': 3373.3994760513306, 'accumulated_eval_time': 1711.423900127411, 'accumulated_logging_time': 0.3928844928741455}
I0205 14:48:29.361484 139735693903616 logging_writer.py:48] [10406] accumulated_eval_time=1711.423900, accumulated_logging_time=0.392884, accumulated_submission_time=3373.399476, global_step=10406, preemption_count=0, score=3373.399476, test/accuracy=0.985580, test/loss=0.048153, test/mean_average_precision=0.235628, test/num_examples=43793, total_duration=5085.509356, train/accuracy=0.989718, train/loss=0.034417, train/mean_average_precision=0.317079, validation/accuracy=0.986353, validation/loss=0.045563, validation/mean_average_precision=0.237299, validation/num_examples=43793
I0205 14:49:00.535317 139752296675072 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.05043772980570793, loss=0.039366137236356735
I0205 14:49:32.513866 139735693903616 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.024215158075094223, loss=0.03274841979146004
I0205 14:50:04.504699 139752296675072 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.0767119973897934, loss=0.039313558489084244
I0205 14:50:35.936848 139735693903616 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.027461836114525795, loss=0.03144798427820206
I0205 14:51:07.893010 139752296675072 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.04090743139386177, loss=0.036827150732278824
I0205 14:51:39.981749 139735693903616 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.02916502207517624, loss=0.03693070635199547
I0205 14:52:12.123206 139752296675072 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.04432743042707443, loss=0.0344802662730217
I0205 14:52:29.440241 139919816816448 spec.py:321] Evaluating on the training split.
I0205 14:54:09.835419 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 14:54:12.880688 139919816816448 spec.py:349] Evaluating on the test split.
I0205 14:54:15.859635 139919816816448 submission_runner.py:408] Time since start: 5432.03s, 	Step: 11155, 	{'train/accuracy': 0.989963173866272, 'train/loss': 0.03346941992640495, 'train/mean_average_precision': 0.32815418763912285, 'validation/accuracy': 0.9864525198936462, 'validation/loss': 0.04531967267394066, 'validation/mean_average_precision': 0.24260675495018194, 'validation/num_examples': 43793, 'test/accuracy': 0.985577404499054, 'test/loss': 0.04799189418554306, 'test/mean_average_precision': 0.23709998748098676, 'test/num_examples': 43793, 'score': 3613.4454939365387, 'total_duration': 5432.026793479919, 'accumulated_submission_time': 3613.4454939365387, 'accumulated_eval_time': 1817.8432455062866, 'accumulated_logging_time': 0.4235210418701172}
I0205 14:54:15.876743 139759013103360 logging_writer.py:48] [11155] accumulated_eval_time=1817.843246, accumulated_logging_time=0.423521, accumulated_submission_time=3613.445494, global_step=11155, preemption_count=0, score=3613.445494, test/accuracy=0.985577, test/loss=0.047992, test/mean_average_precision=0.237100, test/num_examples=43793, total_duration=5432.026793, train/accuracy=0.989963, train/loss=0.033469, train/mean_average_precision=0.328154, validation/accuracy=0.986453, validation/loss=0.045320, validation/mean_average_precision=0.242607, validation/num_examples=43793
I0205 14:54:31.286935 139759038281472 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.04644821211695671, loss=0.034392815083265305
I0205 14:55:03.622608 139759013103360 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.032275788486003876, loss=0.03316589817404747
I0205 14:55:36.083280 139759038281472 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.02806445211172104, loss=0.030943090096116066
I0205 14:56:08.518086 139759013103360 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.030534017831087112, loss=0.0337374247610569
I0205 14:56:40.754165 139759038281472 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.029933080077171326, loss=0.03397611901164055
I0205 14:57:13.099321 139759013103360 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.03861822560429573, loss=0.03481915965676308
I0205 14:57:45.124669 139759038281472 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.036364562809467316, loss=0.030938180163502693
I0205 14:58:15.958781 139919816816448 spec.py:321] Evaluating on the training split.
I0205 15:00:01.422336 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 15:00:04.580317 139919816816448 spec.py:349] Evaluating on the test split.
I0205 15:00:07.621981 139919816816448 submission_runner.py:408] Time since start: 5783.79s, 	Step: 11898, 	{'train/accuracy': 0.9901131391525269, 'train/loss': 0.03306407853960991, 'train/mean_average_precision': 0.3472507179399345, 'validation/accuracy': 0.9863274693489075, 'validation/loss': 0.04504416137933731, 'validation/mean_average_precision': 0.2529347650672703, 'validation/num_examples': 43793, 'test/accuracy': 0.9856052398681641, 'test/loss': 0.04766784980893135, 'test/mean_average_precision': 0.24585168700097593, 'test/num_examples': 43793, 'score': 3853.49520945549, 'total_duration': 5783.7891409397125, 'accumulated_submission_time': 3853.49520945549, 'accumulated_eval_time': 1929.5064034461975, 'accumulated_logging_time': 0.4515836238861084}
I0205 15:00:07.639765 139739177916160 logging_writer.py:48] [11898] accumulated_eval_time=1929.506403, accumulated_logging_time=0.451584, accumulated_submission_time=3853.495209, global_step=11898, preemption_count=0, score=3853.495209, test/accuracy=0.985605, test/loss=0.047668, test/mean_average_precision=0.245852, test/num_examples=43793, total_duration=5783.789141, train/accuracy=0.990113, train/loss=0.033064, train/mean_average_precision=0.347251, validation/accuracy=0.986327, validation/loss=0.045044, validation/mean_average_precision=0.252935, validation/num_examples=43793
I0205 15:00:09.026134 139752296675072 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.03804837167263031, loss=0.035607099533081055
I0205 15:00:40.907759 139739177916160 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.03464510664343834, loss=0.03544477000832558
I0205 15:01:13.024256 139752296675072 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.044712673872709274, loss=0.036491263657808304
I0205 15:01:45.776035 139739177916160 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.055188409984111786, loss=0.033182431012392044
I0205 15:02:18.089637 139752296675072 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.037283070385456085, loss=0.029962673783302307
I0205 15:02:50.551344 139739177916160 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.03431614115834236, loss=0.02913924679160118
I0205 15:03:22.768672 139752296675072 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.03610134497284889, loss=0.031846992671489716
I0205 15:03:54.658276 139739177916160 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.04922856017947197, loss=0.03676951304078102
I0205 15:04:07.898478 139919816816448 spec.py:321] Evaluating on the training split.
I0205 15:05:53.802147 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 15:05:56.921869 139919816816448 spec.py:349] Evaluating on the test split.
I0205 15:05:59.971986 139919816816448 submission_runner.py:408] Time since start: 6136.14s, 	Step: 12641, 	{'train/accuracy': 0.9902748465538025, 'train/loss': 0.03211957961320877, 'train/mean_average_precision': 0.36782349838734035, 'validation/accuracy': 0.9866047501564026, 'validation/loss': 0.044995300471782684, 'validation/mean_average_precision': 0.251586093170994, 'validation/num_examples': 43793, 'test/accuracy': 0.9857774972915649, 'test/loss': 0.04777630791068077, 'test/mean_average_precision': 0.24437577786005055, 'test/num_examples': 43793, 'score': 4093.3299930095673, 'total_duration': 6136.139136552811, 'accumulated_submission_time': 4093.3299930095673, 'accumulated_eval_time': 2041.57985496521, 'accumulated_logging_time': 0.8727149963378906}
I0205 15:05:59.990780 139735693903616 logging_writer.py:48] [12641] accumulated_eval_time=2041.579855, accumulated_logging_time=0.872715, accumulated_submission_time=4093.329993, global_step=12641, preemption_count=0, score=4093.329993, test/accuracy=0.985777, test/loss=0.047776, test/mean_average_precision=0.244376, test/num_examples=43793, total_duration=6136.139137, train/accuracy=0.990275, train/loss=0.032120, train/mean_average_precision=0.367823, validation/accuracy=0.986605, validation/loss=0.044995, validation/mean_average_precision=0.251586, validation/num_examples=43793
I0205 15:06:19.649986 139759013103360 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.054999303072690964, loss=0.033255111426115036
I0205 15:06:52.203502 139735693903616 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.053641851991415024, loss=0.03455575183033943
I0205 15:07:24.507215 139759013103360 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.036547984927892685, loss=0.031177282333374023
I0205 15:07:56.591700 139735693903616 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.05066809430718422, loss=0.03156338632106781
I0205 15:08:28.417275 139759013103360 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.05447523295879364, loss=0.037308160215616226
I0205 15:09:00.473450 139735693903616 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.0626194104552269, loss=0.031271595507860184
I0205 15:09:32.217855 139759013103360 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.04697991907596588, loss=0.030261795967817307
I0205 15:10:00.182083 139919816816448 spec.py:321] Evaluating on the training split.
I0205 15:11:40.278871 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 15:11:45.326885 139919816816448 spec.py:349] Evaluating on the test split.
I0205 15:11:48.303384 139919816816448 submission_runner.py:408] Time since start: 6484.47s, 	Step: 13388, 	{'train/accuracy': 0.990310549736023, 'train/loss': 0.03186388686299324, 'train/mean_average_precision': 0.36338862405673744, 'validation/accuracy': 0.9865308403968811, 'validation/loss': 0.04500997066497803, 'validation/mean_average_precision': 0.2489073772637887, 'validation/num_examples': 43793, 'test/accuracy': 0.9857479929924011, 'test/loss': 0.047876618802547455, 'test/mean_average_precision': 0.2479855948983023, 'test/num_examples': 43793, 'score': 4333.489721059799, 'total_duration': 6484.4705328941345, 'accumulated_submission_time': 4333.489721059799, 'accumulated_eval_time': 2149.701101541519, 'accumulated_logging_time': 0.9023532867431641}
I0205 15:11:48.321543 139752296675072 logging_writer.py:48] [13388] accumulated_eval_time=2149.701102, accumulated_logging_time=0.902353, accumulated_submission_time=4333.489721, global_step=13388, preemption_count=0, score=4333.489721, test/accuracy=0.985748, test/loss=0.047877, test/mean_average_precision=0.247986, test/num_examples=43793, total_duration=6484.470533, train/accuracy=0.990311, train/loss=0.031864, train/mean_average_precision=0.363389, validation/accuracy=0.986531, validation/loss=0.045010, validation/mean_average_precision=0.248907, validation/num_examples=43793
I0205 15:11:52.561459 139759038281472 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.0648941621184349, loss=0.037711434066295624
I0205 15:12:25.345389 139752296675072 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.055870238691568375, loss=0.02756836637854576
I0205 15:12:58.141138 139759038281472 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.05413153022527695, loss=0.030699199065566063
I0205 15:13:31.108598 139752296675072 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.05996188893914223, loss=0.03416471183300018
I0205 15:14:03.715400 139759038281472 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.05525680258870125, loss=0.035706907510757446
I0205 15:14:36.650217 139752296675072 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.07257039844989777, loss=0.031617045402526855
I0205 15:15:09.668282 139759038281472 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.046766821295022964, loss=0.03173810988664627
I0205 15:15:42.329745 139752296675072 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.05422152206301689, loss=0.03252536430954933
I0205 15:15:48.617922 139919816816448 spec.py:321] Evaluating on the training split.
I0205 15:17:32.134510 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 15:17:35.216125 139919816816448 spec.py:349] Evaluating on the test split.
I0205 15:17:38.173927 139919816816448 submission_runner.py:408] Time since start: 6834.34s, 	Step: 14120, 	{'train/accuracy': 0.9904751777648926, 'train/loss': 0.031606610864400864, 'train/mean_average_precision': 0.36739894883526464, 'validation/accuracy': 0.9864829182624817, 'validation/loss': 0.044791556894779205, 'validation/mean_average_precision': 0.2517094654821753, 'validation/num_examples': 43793, 'test/accuracy': 0.9856890439987183, 'test/loss': 0.04744245111942291, 'test/mean_average_precision': 0.24579146920607292, 'test/num_examples': 43793, 'score': 4573.749706029892, 'total_duration': 6834.34108376503, 'accumulated_submission_time': 4573.749706029892, 'accumulated_eval_time': 2259.2570674419403, 'accumulated_logging_time': 0.931546688079834}
I0205 15:17:38.192946 139739177916160 logging_writer.py:48] [14120] accumulated_eval_time=2259.257067, accumulated_logging_time=0.931547, accumulated_submission_time=4573.749706, global_step=14120, preemption_count=0, score=4573.749706, test/accuracy=0.985689, test/loss=0.047442, test/mean_average_precision=0.245791, test/num_examples=43793, total_duration=6834.341084, train/accuracy=0.990475, train/loss=0.031607, train/mean_average_precision=0.367399, validation/accuracy=0.986483, validation/loss=0.044792, validation/mean_average_precision=0.251709, validation/num_examples=43793
I0205 15:18:04.580989 139759013103360 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.10167818516492844, loss=0.03033141978085041
I0205 15:18:36.560281 139739177916160 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.054538361728191376, loss=0.030468223616480827
I0205 15:19:08.904059 139759013103360 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.05785368010401726, loss=0.033294450491666794
I0205 15:19:40.601091 139739177916160 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.06251432001590729, loss=0.02857254259288311
I0205 15:20:12.391635 139759013103360 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.05077401176095009, loss=0.03128727898001671
I0205 15:20:44.181596 139739177916160 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.07028906047344208, loss=0.031125472858548164
I0205 15:21:16.160850 139759013103360 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.08673398941755295, loss=0.03337450698018074
I0205 15:21:38.277417 139919816816448 spec.py:321] Evaluating on the training split.
I0205 15:23:18.807197 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 15:23:21.825110 139919816816448 spec.py:349] Evaluating on the test split.
I0205 15:23:26.720779 139919816816448 submission_runner.py:408] Time since start: 7182.89s, 	Step: 14870, 	{'train/accuracy': 0.9906328320503235, 'train/loss': 0.03126322478055954, 'train/mean_average_precision': 0.3707889618155518, 'validation/accuracy': 0.9866303205490112, 'validation/loss': 0.044579531997442245, 'validation/mean_average_precision': 0.2557782403995275, 'validation/num_examples': 43793, 'test/accuracy': 0.9857884645462036, 'test/loss': 0.047305814921855927, 'test/mean_average_precision': 0.2542209605860204, 'test/num_examples': 43793, 'score': 4813.8011820316315, 'total_duration': 7182.887937784195, 'accumulated_submission_time': 4813.8011820316315, 'accumulated_eval_time': 2367.7003812789917, 'accumulated_logging_time': 0.9632065296173096}
I0205 15:23:26.739361 139735693903616 logging_writer.py:48] [14870] accumulated_eval_time=2367.700381, accumulated_logging_time=0.963207, accumulated_submission_time=4813.801182, global_step=14870, preemption_count=0, score=4813.801182, test/accuracy=0.985788, test/loss=0.047306, test/mean_average_precision=0.254221, test/num_examples=43793, total_duration=7182.887938, train/accuracy=0.990633, train/loss=0.031263, train/mean_average_precision=0.370789, validation/accuracy=0.986630, validation/loss=0.044580, validation/mean_average_precision=0.255778, validation/num_examples=43793
I0205 15:23:36.939409 139759038281472 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.06517580896615982, loss=0.03517775610089302
I0205 15:24:08.637450 139735693903616 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.04491947963833809, loss=0.03239087387919426
I0205 15:24:40.243859 139759038281472 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.07754058390855789, loss=0.032501306384801865
I0205 15:25:12.258800 139735693903616 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.07044203579425812, loss=0.031693339347839355
I0205 15:25:43.675617 139759038281472 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.06139179319143295, loss=0.03412855789065361
I0205 15:26:15.390403 139735693903616 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.05154752358794212, loss=0.03493903577327728
I0205 15:26:46.975919 139759038281472 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.055679310113191605, loss=0.03224741667509079
I0205 15:27:18.609631 139735693903616 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.10691823065280914, loss=0.03668041527271271
I0205 15:27:26.875708 139919816816448 spec.py:321] Evaluating on the training split.
I0205 15:29:08.052346 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 15:29:11.077342 139919816816448 spec.py:349] Evaluating on the test split.
I0205 15:29:14.012506 139919816816448 submission_runner.py:408] Time since start: 7530.18s, 	Step: 15627, 	{'train/accuracy': 0.9904218912124634, 'train/loss': 0.031682081520557404, 'train/mean_average_precision': 0.3776189810012174, 'validation/accuracy': 0.9864882230758667, 'validation/loss': 0.045027993619441986, 'validation/mean_average_precision': 0.25533864868194683, 'validation/num_examples': 43793, 'test/accuracy': 0.9857210516929626, 'test/loss': 0.04752392694354057, 'test/mean_average_precision': 0.24927427426660573, 'test/num_examples': 43793, 'score': 5053.904675960541, 'total_duration': 7530.179664611816, 'accumulated_submission_time': 5053.904675960541, 'accumulated_eval_time': 2474.837132692337, 'accumulated_logging_time': 0.9940104484558105}
I0205 15:29:14.030810 139752296675072 logging_writer.py:48] [15627] accumulated_eval_time=2474.837133, accumulated_logging_time=0.994010, accumulated_submission_time=5053.904676, global_step=15627, preemption_count=0, score=5053.904676, test/accuracy=0.985721, test/loss=0.047524, test/mean_average_precision=0.249274, test/num_examples=43793, total_duration=7530.179665, train/accuracy=0.990422, train/loss=0.031682, train/mean_average_precision=0.377619, validation/accuracy=0.986488, validation/loss=0.045028, validation/mean_average_precision=0.255339, validation/num_examples=43793
I0205 15:29:38.031473 139759013103360 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.06392847001552582, loss=0.031737226992845535
I0205 15:30:10.008931 139752296675072 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.05672832205891609, loss=0.029525259509682655
I0205 15:30:42.303111 139759013103360 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.08219941705465317, loss=0.03163612261414528
I0205 15:31:14.409937 139752296675072 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.05074368417263031, loss=0.03188364952802658
I0205 15:31:46.489127 139759013103360 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.058631815016269684, loss=0.030773639678955078
I0205 15:32:18.620289 139752296675072 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.05134626105427742, loss=0.031351327896118164
I0205 15:32:50.630601 139759013103360 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.050663549453020096, loss=0.027838362380862236
I0205 15:33:14.129641 139919816816448 spec.py:321] Evaluating on the training split.
I0205 15:34:55.422090 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 15:34:58.449131 139919816816448 spec.py:349] Evaluating on the test split.
I0205 15:35:01.592975 139919816816448 submission_runner.py:408] Time since start: 7877.76s, 	Step: 16374, 	{'train/accuracy': 0.9905163049697876, 'train/loss': 0.03142475336790085, 'train/mean_average_precision': 0.38393337206895434, 'validation/accuracy': 0.9865962266921997, 'validation/loss': 0.044712018221616745, 'validation/mean_average_precision': 0.2527464871775334, 'validation/num_examples': 43793, 'test/accuracy': 0.9857126474380493, 'test/loss': 0.04740390554070473, 'test/mean_average_precision': 0.24272942220406243, 'test/num_examples': 43793, 'score': 5293.972366333008, 'total_duration': 7877.760135889053, 'accumulated_submission_time': 5293.972366333008, 'accumulated_eval_time': 2582.3004174232483, 'accumulated_logging_time': 1.0230059623718262}
I0205 15:35:01.611392 139735693903616 logging_writer.py:48] [16374] accumulated_eval_time=2582.300417, accumulated_logging_time=1.023006, accumulated_submission_time=5293.972366, global_step=16374, preemption_count=0, score=5293.972366, test/accuracy=0.985713, test/loss=0.047404, test/mean_average_precision=0.242729, test/num_examples=43793, total_duration=7877.760136, train/accuracy=0.990516, train/loss=0.031425, train/mean_average_precision=0.383933, validation/accuracy=0.986596, validation/loss=0.044712, validation/mean_average_precision=0.252746, validation/num_examples=43793
I0205 15:35:10.162472 139759038281472 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.051527153700590134, loss=0.0315072275698185
I0205 15:35:41.725060 139735693903616 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.07883921265602112, loss=0.029613444581627846
I0205 15:36:13.347156 139759038281472 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.05337854102253914, loss=0.030957669019699097
I0205 15:36:44.718309 139735693903616 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.06017627194523811, loss=0.031946323812007904
I0205 15:37:16.315471 139759038281472 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.07503222674131393, loss=0.03203539177775383
I0205 15:37:47.894313 139735693903616 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.0732695609331131, loss=0.033040232956409454
I0205 15:38:19.324619 139759038281472 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.07484212517738342, loss=0.03268831595778465
I0205 15:38:51.184159 139735693903616 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.08606316149234772, loss=0.03262729197740555
I0205 15:39:01.703159 139919816816448 spec.py:321] Evaluating on the training split.
I0205 15:40:44.233073 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 15:40:47.250157 139919816816448 spec.py:349] Evaluating on the test split.
I0205 15:40:50.194117 139919816816448 submission_runner.py:408] Time since start: 8226.36s, 	Step: 17134, 	{'train/accuracy': 0.9904187321662903, 'train/loss': 0.0311676487326622, 'train/mean_average_precision': 0.37887971000370035, 'validation/accuracy': 0.986632764339447, 'validation/loss': 0.04480931907892227, 'validation/mean_average_precision': 0.2590131502594755, 'validation/num_examples': 43793, 'test/accuracy': 0.9857867360115051, 'test/loss': 0.04752666503190994, 'test/mean_average_precision': 0.2523974319454819, 'test/num_examples': 43793, 'score': 5534.025603532791, 'total_duration': 8226.361269712448, 'accumulated_submission_time': 5534.025603532791, 'accumulated_eval_time': 2690.791320323944, 'accumulated_logging_time': 1.059596300125122}
I0205 15:40:50.212383 139752296675072 logging_writer.py:48] [17134] accumulated_eval_time=2690.791320, accumulated_logging_time=1.059596, accumulated_submission_time=5534.025604, global_step=17134, preemption_count=0, score=5534.025604, test/accuracy=0.985787, test/loss=0.047527, test/mean_average_precision=0.252397, test/num_examples=43793, total_duration=8226.361270, train/accuracy=0.990419, train/loss=0.031168, train/mean_average_precision=0.378880, validation/accuracy=0.986633, validation/loss=0.044809, validation/mean_average_precision=0.259013, validation/num_examples=43793
I0205 15:41:11.924937 139759013103360 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.07375716418027878, loss=0.03259612247347832
I0205 15:41:43.658046 139752296675072 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.08155859261751175, loss=0.028240248560905457
I0205 15:42:15.453431 139759013103360 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.08669744431972504, loss=0.03259045258164406
I0205 15:42:47.167960 139752296675072 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.06061635911464691, loss=0.03394118696451187
I0205 15:43:18.829936 139759013103360 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.062171682715415955, loss=0.02996794320642948
I0205 15:43:50.514275 139752296675072 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.05822111666202545, loss=0.03033749759197235
I0205 15:44:22.660590 139759013103360 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.09771662205457687, loss=0.034787617623806
I0205 15:44:50.285686 139919816816448 spec.py:321] Evaluating on the training split.
I0205 15:46:36.612153 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 15:46:39.666592 139919816816448 spec.py:349] Evaluating on the test split.
I0205 15:46:42.652813 139919816816448 submission_runner.py:408] Time since start: 8578.82s, 	Step: 17887, 	{'train/accuracy': 0.9908297061920166, 'train/loss': 0.030224693939089775, 'train/mean_average_precision': 0.39761579377261785, 'validation/accuracy': 0.9866778254508972, 'validation/loss': 0.044581979513168335, 'validation/mean_average_precision': 0.2644920682135977, 'validation/num_examples': 43793, 'test/accuracy': 0.9859261512756348, 'test/loss': 0.04703583940863609, 'test/mean_average_precision': 0.2652222081535117, 'test/num_examples': 43793, 'score': 5774.0658304691315, 'total_duration': 8578.81997179985, 'accumulated_submission_time': 5774.0658304691315, 'accumulated_eval_time': 2803.158401966095, 'accumulated_logging_time': 1.0902180671691895}
I0205 15:46:42.674333 139735693903616 logging_writer.py:48] [17887] accumulated_eval_time=2803.158402, accumulated_logging_time=1.090218, accumulated_submission_time=5774.065830, global_step=17887, preemption_count=0, score=5774.065830, test/accuracy=0.985926, test/loss=0.047036, test/mean_average_precision=0.265222, test/num_examples=43793, total_duration=8578.819972, train/accuracy=0.990830, train/loss=0.030225, train/mean_average_precision=0.397616, validation/accuracy=0.986678, validation/loss=0.044582, validation/mean_average_precision=0.264492, validation/num_examples=43793
I0205 15:46:47.192393 139759038281472 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.07643674314022064, loss=0.03256584703922272
I0205 15:47:19.656415 139735693903616 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.10531090199947357, loss=0.03254501521587372
I0205 15:47:51.572052 139759038281472 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.08031320571899414, loss=0.03333903104066849
I0205 15:48:23.846629 139735693903616 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.09026878327131271, loss=0.031535640358924866
I0205 15:48:55.944773 139759038281472 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.14520061016082764, loss=0.03179319202899933
I0205 15:49:28.282665 139735693903616 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.1072307899594307, loss=0.03227357193827629
I0205 15:50:00.064250 139759038281472 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.07301577925682068, loss=0.029604000970721245
I0205 15:50:32.222703 139735693903616 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.07087952643632889, loss=0.03148476779460907
I0205 15:50:42.803680 139919816816448 spec.py:321] Evaluating on the training split.
I0205 15:52:26.092848 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 15:52:29.199716 139919816816448 spec.py:349] Evaluating on the test split.
I0205 15:52:32.250622 139919816816448 submission_runner.py:408] Time since start: 8928.42s, 	Step: 18634, 	{'train/accuracy': 0.9909319281578064, 'train/loss': 0.02977757528424263, 'train/mean_average_precision': 0.41316582178015493, 'validation/accuracy': 0.9866737127304077, 'validation/loss': 0.044742196798324585, 'validation/mean_average_precision': 0.26652513254352184, 'validation/num_examples': 43793, 'test/accuracy': 0.9858751893043518, 'test/loss': 0.04739736020565033, 'test/mean_average_precision': 0.2532736599645985, 'test/num_examples': 43793, 'score': 6014.162916898727, 'total_duration': 8928.417778730392, 'accumulated_submission_time': 6014.162916898727, 'accumulated_eval_time': 2912.6052956581116, 'accumulated_logging_time': 1.1236302852630615}
I0205 15:52:32.270814 139752296675072 logging_writer.py:48] [18634] accumulated_eval_time=2912.605296, accumulated_logging_time=1.123630, accumulated_submission_time=6014.162917, global_step=18634, preemption_count=0, score=6014.162917, test/accuracy=0.985875, test/loss=0.047397, test/mean_average_precision=0.253274, test/num_examples=43793, total_duration=8928.417779, train/accuracy=0.990932, train/loss=0.029778, train/mean_average_precision=0.413166, validation/accuracy=0.986674, validation/loss=0.044742, validation/mean_average_precision=0.266525, validation/num_examples=43793
I0205 15:52:53.741235 139759013103360 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.08820363879203796, loss=0.027121488004922867
I0205 15:53:25.892242 139752296675072 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.06509674340486526, loss=0.0290419589728117
I0205 15:53:58.060928 139759013103360 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.07225748151540756, loss=0.03157568350434303
I0205 15:54:30.215436 139752296675072 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.08056239038705826, loss=0.032244812697172165
I0205 15:55:02.524016 139759013103360 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.08895111083984375, loss=0.031023750081658363
I0205 15:55:34.582785 139752296675072 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.09006451815366745, loss=0.027544740587472916
I0205 15:56:06.750251 139759013103360 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.08471254259347916, loss=0.03030560165643692
I0205 15:56:32.380915 139919816816448 spec.py:321] Evaluating on the training split.
I0205 15:58:17.096767 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 15:58:20.267840 139919816816448 spec.py:349] Evaluating on the test split.
I0205 15:58:25.262536 139919816816448 submission_runner.py:408] Time since start: 9281.43s, 	Step: 19381, 	{'train/accuracy': 0.9910821914672852, 'train/loss': 0.029007235541939735, 'train/mean_average_precision': 0.43306888295180035, 'validation/accuracy': 0.9867277145385742, 'validation/loss': 0.04532693699002266, 'validation/mean_average_precision': 0.26184248613783995, 'validation/num_examples': 43793, 'test/accuracy': 0.9859354496002197, 'test/loss': 0.04790349304676056, 'test/mean_average_precision': 0.25805148466277017, 'test/num_examples': 43793, 'score': 6254.241195201874, 'total_duration': 9281.429696083069, 'accumulated_submission_time': 6254.241195201874, 'accumulated_eval_time': 3025.486868619919, 'accumulated_logging_time': 1.1551201343536377}
I0205 15:58:25.282204 139739177916160 logging_writer.py:48] [19381] accumulated_eval_time=3025.486869, accumulated_logging_time=1.155120, accumulated_submission_time=6254.241195, global_step=19381, preemption_count=0, score=6254.241195, test/accuracy=0.985935, test/loss=0.047903, test/mean_average_precision=0.258051, test/num_examples=43793, total_duration=9281.429696, train/accuracy=0.991082, train/loss=0.029007, train/mean_average_precision=0.433069, validation/accuracy=0.986728, validation/loss=0.045327, validation/mean_average_precision=0.261842, validation/num_examples=43793
I0205 15:58:31.696460 139759038281472 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.07004339247941971, loss=0.028988484293222427
I0205 15:59:04.402096 139739177916160 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.0878535807132721, loss=0.02802545391023159
I0205 15:59:37.203447 139759038281472 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.09024173021316528, loss=0.032552845776081085
I0205 16:00:10.226000 139739177916160 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.13254688680171967, loss=0.03226979821920395
I0205 16:00:42.435294 139759038281472 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.10272831469774246, loss=0.034763090312480927
I0205 16:01:14.972366 139739177916160 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.07502077519893646, loss=0.027908945456147194
I0205 16:01:47.403678 139759038281472 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.08910118788480759, loss=0.029286038130521774
I0205 16:02:19.728299 139739177916160 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.10642895102500916, loss=0.02778238244354725
I0205 16:02:25.556240 139919816816448 spec.py:321] Evaluating on the training split.
I0205 16:04:14.379961 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 16:04:17.804110 139919816816448 spec.py:349] Evaluating on the test split.
I0205 16:04:21.143520 139919816816448 submission_runner.py:408] Time since start: 9637.31s, 	Step: 20119, 	{'train/accuracy': 0.9911299347877502, 'train/loss': 0.02904364839196205, 'train/mean_average_precision': 0.4524596980950766, 'validation/accuracy': 0.9866428971290588, 'validation/loss': 0.044798992574214935, 'validation/mean_average_precision': 0.26663937524288245, 'validation/num_examples': 43793, 'test/accuracy': 0.9859223961830139, 'test/loss': 0.04723508656024933, 'test/mean_average_precision': 0.2566684458171706, 'test/num_examples': 43793, 'score': 6494.479324102402, 'total_duration': 9637.310660123825, 'accumulated_submission_time': 6494.479324102402, 'accumulated_eval_time': 3141.0740916728973, 'accumulated_logging_time': 1.1857497692108154}
I0205 16:04:21.165493 139735693903616 logging_writer.py:48] [20119] accumulated_eval_time=3141.074092, accumulated_logging_time=1.185750, accumulated_submission_time=6494.479324, global_step=20119, preemption_count=0, score=6494.479324, test/accuracy=0.985922, test/loss=0.047235, test/mean_average_precision=0.256668, test/num_examples=43793, total_duration=9637.310660, train/accuracy=0.991130, train/loss=0.029044, train/mean_average_precision=0.452460, validation/accuracy=0.986643, validation/loss=0.044799, validation/mean_average_precision=0.266639, validation/num_examples=43793
I0205 16:04:48.262883 139759013103360 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.07412629574537277, loss=0.02885211817920208
I0205 16:05:20.864666 139735693903616 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.08756326138973236, loss=0.02911224029958248
I0205 16:05:53.226167 139759013103360 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.06784089654684067, loss=0.029441524296998978
I0205 16:06:25.220492 139735693903616 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.08882495760917664, loss=0.03280550241470337
I0205 16:06:57.400859 139759013103360 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.08099152147769928, loss=0.025380786508321762
I0205 16:07:29.557587 139735693903616 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.14069238305091858, loss=0.031916867941617966
I0205 16:08:02.087670 139759013103360 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.07484191656112671, loss=0.030323917046189308
I0205 16:08:21.248545 139919816816448 spec.py:321] Evaluating on the training split.
I0205 16:10:05.798861 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 16:10:08.851906 139919816816448 spec.py:349] Evaluating on the test split.
I0205 16:10:13.762326 139919816816448 submission_runner.py:408] Time since start: 9989.93s, 	Step: 20860, 	{'train/accuracy': 0.9911418557167053, 'train/loss': 0.028990084305405617, 'train/mean_average_precision': 0.43619180909209165, 'validation/accuracy': 0.9865061044692993, 'validation/loss': 0.044800519943237305, 'validation/mean_average_precision': 0.25785411469819974, 'validation/num_examples': 43793, 'test/accuracy': 0.9855993390083313, 'test/loss': 0.047596536576747894, 'test/mean_average_precision': 0.2488904178834033, 'test/num_examples': 43793, 'score': 6734.529579162598, 'total_duration': 9989.929476737976, 'accumulated_submission_time': 6734.529579162598, 'accumulated_eval_time': 3253.587816953659, 'accumulated_logging_time': 1.2191963195800781}
I0205 16:10:13.782950 139739177916160 logging_writer.py:48] [20860] accumulated_eval_time=3253.587817, accumulated_logging_time=1.219196, accumulated_submission_time=6734.529579, global_step=20860, preemption_count=0, score=6734.529579, test/accuracy=0.985599, test/loss=0.047597, test/mean_average_precision=0.248890, test/num_examples=43793, total_duration=9989.929477, train/accuracy=0.991142, train/loss=0.028990, train/mean_average_precision=0.436192, validation/accuracy=0.986506, validation/loss=0.044801, validation/mean_average_precision=0.257854, validation/num_examples=43793
I0205 16:10:27.051279 139752296675072 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.06857329607009888, loss=0.028239736333489418
I0205 16:10:59.315130 139739177916160 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.09761809557676315, loss=0.03183070197701454
I0205 16:11:30.919129 139752296675072 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.08226869255304337, loss=0.027805553749203682
I0205 16:12:02.689572 139739177916160 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.08561141788959503, loss=0.02876053936779499
I0205 16:12:34.305342 139752296675072 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.12812992930412292, loss=0.030846206471323967
I0205 16:13:06.116279 139739177916160 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.09973602741956711, loss=0.031479611992836
I0205 16:13:38.091565 139752296675072 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.12425190210342407, loss=0.028916507959365845
I0205 16:14:09.951921 139739177916160 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.08324619382619858, loss=0.033184681087732315
I0205 16:14:14.071589 139919816816448 spec.py:321] Evaluating on the training split.
I0205 16:15:57.715900 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 16:16:01.217678 139919816816448 spec.py:349] Evaluating on the test split.
I0205 16:16:04.317686 139919816816448 submission_runner.py:408] Time since start: 10340.48s, 	Step: 21614, 	{'train/accuracy': 0.9909935593605042, 'train/loss': 0.029510680586099625, 'train/mean_average_precision': 0.418704369794744, 'validation/accuracy': 0.9864736199378967, 'validation/loss': 0.04465806111693382, 'validation/mean_average_precision': 0.26883918554463876, 'validation/num_examples': 43793, 'test/accuracy': 0.9857787489891052, 'test/loss': 0.04739959537982941, 'test/mean_average_precision': 0.2512402387290738, 'test/num_examples': 43793, 'score': 6974.786199092865, 'total_duration': 10340.484828233719, 'accumulated_submission_time': 6974.786199092865, 'accumulated_eval_time': 3363.8338465690613, 'accumulated_logging_time': 1.250971794128418}
I0205 16:16:04.337199 139735693903616 logging_writer.py:48] [21614] accumulated_eval_time=3363.833847, accumulated_logging_time=1.250972, accumulated_submission_time=6974.786199, global_step=21614, preemption_count=0, score=6974.786199, test/accuracy=0.985779, test/loss=0.047400, test/mean_average_precision=0.251240, test/num_examples=43793, total_duration=10340.484828, train/accuracy=0.990994, train/loss=0.029511, train/mean_average_precision=0.418704, validation/accuracy=0.986474, validation/loss=0.044658, validation/mean_average_precision=0.268839, validation/num_examples=43793
I0205 16:16:32.219511 139759013103360 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.07247233390808105, loss=0.02816399373114109
I0205 16:17:04.184970 139735693903616 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.09243593364953995, loss=0.030512535944581032
I0205 16:17:35.996823 139759013103360 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.08839372545480728, loss=0.03055178001523018
I0205 16:18:07.811974 139735693903616 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.08971962332725525, loss=0.028433002531528473
I0205 16:18:39.787599 139759013103360 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.07293444126844406, loss=0.02591940388083458
I0205 16:19:11.677253 139735693903616 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.09935735166072845, loss=0.03291499614715576
I0205 16:19:43.839864 139759013103360 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.08780330419540405, loss=0.03174932301044464
I0205 16:20:04.612518 139919816816448 spec.py:321] Evaluating on the training split.
I0205 16:21:44.095940 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 16:21:47.153444 139919816816448 spec.py:349] Evaluating on the test split.
I0205 16:21:50.164362 139919816816448 submission_runner.py:408] Time since start: 10686.33s, 	Step: 22366, 	{'train/accuracy': 0.990916907787323, 'train/loss': 0.029702335596084595, 'train/mean_average_precision': 0.40929954176538713, 'validation/accuracy': 0.9867342114448547, 'validation/loss': 0.04458253085613251, 'validation/mean_average_precision': 0.2692697508641262, 'validation/num_examples': 43793, 'test/accuracy': 0.9858646988868713, 'test/loss': 0.047177691012620926, 'test/mean_average_precision': 0.2555786770835429, 'test/num_examples': 43793, 'score': 7215.030090808868, 'total_duration': 10686.331381559372, 'accumulated_submission_time': 7215.030090808868, 'accumulated_eval_time': 3469.385509490967, 'accumulated_logging_time': 1.2810804843902588}
I0205 16:21:50.183774 139739177916160 logging_writer.py:48] [22366] accumulated_eval_time=3469.385509, accumulated_logging_time=1.281080, accumulated_submission_time=7215.030091, global_step=22366, preemption_count=0, score=7215.030091, test/accuracy=0.985865, test/loss=0.047178, test/mean_average_precision=0.255579, test/num_examples=43793, total_duration=10686.331382, train/accuracy=0.990917, train/loss=0.029702, train/mean_average_precision=0.409300, validation/accuracy=0.986734, validation/loss=0.044583, validation/mean_average_precision=0.269270, validation/num_examples=43793
I0205 16:22:01.489657 139759038281472 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.1542617529630661, loss=0.03076668083667755
I0205 16:22:33.502099 139739177916160 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.11167922616004944, loss=0.028967278078198433
I0205 16:23:05.540449 139759038281472 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.10458248853683472, loss=0.033081796020269394
I0205 16:23:37.190562 139739177916160 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.08423283696174622, loss=0.030019138008356094
I0205 16:24:09.143511 139759038281472 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.09428255259990692, loss=0.032937921583652496
I0205 16:24:41.082508 139739177916160 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.10166022926568985, loss=0.028007851913571358
I0205 16:25:13.217147 139759038281472 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.10670855641365051, loss=0.032101333141326904
I0205 16:25:45.713752 139739177916160 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.13074472546577454, loss=0.030073432251811028
I0205 16:25:50.251000 139919816816448 spec.py:321] Evaluating on the training split.
I0205 16:27:34.120334 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 16:27:37.173661 139919816816448 spec.py:349] Evaluating on the test split.
I0205 16:27:40.211404 139919816816448 submission_runner.py:408] Time since start: 11036.38s, 	Step: 23115, 	{'train/accuracy': 0.990923285484314, 'train/loss': 0.029610656201839447, 'train/mean_average_precision': 0.4129811539648565, 'validation/accuracy': 0.9866039156913757, 'validation/loss': 0.044834770262241364, 'validation/mean_average_precision': 0.2584141344443701, 'validation/num_examples': 43793, 'test/accuracy': 0.9858865737915039, 'test/loss': 0.04729144647717476, 'test/mean_average_precision': 0.25695871540311854, 'test/num_examples': 43793, 'score': 7455.064225435257, 'total_duration': 11036.378544092178, 'accumulated_submission_time': 7455.064225435257, 'accumulated_eval_time': 3579.345846414566, 'accumulated_logging_time': 1.3130922317504883}
I0205 16:27:40.231654 139735693903616 logging_writer.py:48] [23115] accumulated_eval_time=3579.345846, accumulated_logging_time=1.313092, accumulated_submission_time=7455.064225, global_step=23115, preemption_count=0, score=7455.064225, test/accuracy=0.985887, test/loss=0.047291, test/mean_average_precision=0.256959, test/num_examples=43793, total_duration=11036.378544, train/accuracy=0.990923, train/loss=0.029611, train/mean_average_precision=0.412981, validation/accuracy=0.986604, validation/loss=0.044835, validation/mean_average_precision=0.258414, validation/num_examples=43793
I0205 16:28:07.737583 139759013103360 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.07613527774810791, loss=0.02760264463722706
I0205 16:28:39.772918 139735693903616 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.08688918501138687, loss=0.029675383120775223
I0205 16:29:11.788958 139759013103360 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.08363404124975204, loss=0.031591445207595825
I0205 16:29:43.493669 139735693903616 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.08762818574905396, loss=0.027964424341917038
I0205 16:30:15.481764 139759013103360 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.08492425829172134, loss=0.027997810393571854
I0205 16:30:47.306281 139735693903616 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.12480567395687103, loss=0.031155461445450783
I0205 16:31:19.112596 139759013103360 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.09390278160572052, loss=0.02857460081577301
I0205 16:31:40.227005 139919816816448 spec.py:321] Evaluating on the training split.
I0205 16:33:21.061989 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 16:33:24.120880 139919816816448 spec.py:349] Evaluating on the test split.
I0205 16:33:27.124866 139919816816448 submission_runner.py:408] Time since start: 11383.29s, 	Step: 23867, 	{'train/accuracy': 0.9910376667976379, 'train/loss': 0.02931050769984722, 'train/mean_average_precision': 0.4443964159175245, 'validation/accuracy': 0.9865726828575134, 'validation/loss': 0.04488106817007065, 'validation/mean_average_precision': 0.26803957807294937, 'validation/num_examples': 43793, 'test/accuracy': 0.9857938885688782, 'test/loss': 0.04747995734214783, 'test/mean_average_precision': 0.26006297683377805, 'test/num_examples': 43793, 'score': 7695.025829792023, 'total_duration': 11383.292026758194, 'accumulated_submission_time': 7695.025829792023, 'accumulated_eval_time': 3686.2436599731445, 'accumulated_logging_time': 1.3458125591278076}
I0205 16:33:27.145008 139739177916160 logging_writer.py:48] [23867] accumulated_eval_time=3686.243660, accumulated_logging_time=1.345813, accumulated_submission_time=7695.025830, global_step=23867, preemption_count=0, score=7695.025830, test/accuracy=0.985794, test/loss=0.047480, test/mean_average_precision=0.260063, test/num_examples=43793, total_duration=11383.292027, train/accuracy=0.991038, train/loss=0.029311, train/mean_average_precision=0.444396, validation/accuracy=0.986573, validation/loss=0.044881, validation/mean_average_precision=0.268040, validation/num_examples=43793
I0205 16:33:37.817045 139759038281472 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.15154442191123962, loss=0.029531417414546013
I0205 16:34:09.534021 139739177916160 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.09064942598342896, loss=0.031187187880277634
I0205 16:34:41.269735 139759038281472 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.08013040572404861, loss=0.026736652478575706
I0205 16:35:13.231619 139739177916160 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.085645891726017, loss=0.03193410485982895
I0205 16:35:45.342503 139759038281472 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.08474238961935043, loss=0.030513828620314598
I0205 16:36:17.322120 139739177916160 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.1099584624171257, loss=0.027479568496346474
I0205 16:36:50.359241 139759038281472 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.09464490413665771, loss=0.02990340255200863
I0205 16:37:22.941433 139739177916160 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.08270980417728424, loss=0.027932647615671158
I0205 16:37:27.423858 139919816816448 spec.py:321] Evaluating on the training split.
I0205 16:39:13.355472 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 16:39:16.791418 139919816816448 spec.py:349] Evaluating on the test split.
I0205 16:39:20.141332 139919816816448 submission_runner.py:408] Time since start: 11736.31s, 	Step: 24615, 	{'train/accuracy': 0.9911251068115234, 'train/loss': 0.028719495981931686, 'train/mean_average_precision': 0.43497808624609424, 'validation/accuracy': 0.9867350459098816, 'validation/loss': 0.04469083994626999, 'validation/mean_average_precision': 0.2686270225783731, 'validation/num_examples': 43793, 'test/accuracy': 0.9858722686767578, 'test/loss': 0.04751843214035034, 'test/mean_average_precision': 0.257685135742762, 'test/num_examples': 43793, 'score': 7935.27251625061, 'total_duration': 11736.308470726013, 'accumulated_submission_time': 7935.27251625061, 'accumulated_eval_time': 3798.9610619544983, 'accumulated_logging_time': 1.377070426940918}
I0205 16:39:20.163853 139735693903616 logging_writer.py:48] [24615] accumulated_eval_time=3798.961062, accumulated_logging_time=1.377070, accumulated_submission_time=7935.272516, global_step=24615, preemption_count=0, score=7935.272516, test/accuracy=0.985872, test/loss=0.047518, test/mean_average_precision=0.257685, test/num_examples=43793, total_duration=11736.308471, train/accuracy=0.991125, train/loss=0.028719, train/mean_average_precision=0.434978, validation/accuracy=0.986735, validation/loss=0.044691, validation/mean_average_precision=0.268627, validation/num_examples=43793
I0205 16:39:48.108252 139752296675072 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.11301209777593613, loss=0.03262626379728317
I0205 16:40:20.691167 139735693903616 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.0777030661702156, loss=0.02697201818227768
I0205 16:40:53.564646 139752296675072 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.07416494190692902, loss=0.025782497599720955
I0205 16:41:26.288090 139735693903616 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.09142167866230011, loss=0.02707573026418686
I0205 16:41:58.788963 139752296675072 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.11401647329330444, loss=0.031345825642347336
I0205 16:42:31.096442 139735693903616 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.08677805960178375, loss=0.028098376467823982
I0205 16:43:03.663245 139752296675072 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.07857673615217209, loss=0.031000973656773567
I0205 16:43:20.278287 139919816816448 spec.py:321] Evaluating on the training split.
I0205 16:45:03.157266 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 16:45:09.032726 139919816816448 spec.py:349] Evaluating on the test split.
I0205 16:45:12.393488 139919816816448 submission_runner.py:408] Time since start: 12088.56s, 	Step: 25352, 	{'train/accuracy': 0.9912492036819458, 'train/loss': 0.02848394587635994, 'train/mean_average_precision': 0.44107130116975035, 'validation/accuracy': 0.9866904020309448, 'validation/loss': 0.04465167596936226, 'validation/mean_average_precision': 0.2698735193364242, 'validation/num_examples': 43793, 'test/accuracy': 0.9858680367469788, 'test/loss': 0.047340765595436096, 'test/mean_average_precision': 0.2599173126520697, 'test/num_examples': 43793, 'score': 8175.3488891124725, 'total_duration': 12088.560628414154, 'accumulated_submission_time': 8175.3488891124725, 'accumulated_eval_time': 3911.0762073993683, 'accumulated_logging_time': 1.4118154048919678}
I0205 16:45:12.416350 139759013103360 logging_writer.py:48] [25352] accumulated_eval_time=3911.076207, accumulated_logging_time=1.411815, accumulated_submission_time=8175.348889, global_step=25352, preemption_count=0, score=8175.348889, test/accuracy=0.985868, test/loss=0.047341, test/mean_average_precision=0.259917, test/num_examples=43793, total_duration=12088.560628, train/accuracy=0.991249, train/loss=0.028484, train/mean_average_precision=0.441071, validation/accuracy=0.986690, validation/loss=0.044652, validation/mean_average_precision=0.269874, validation/num_examples=43793
I0205 16:45:28.245449 139759038281472 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.09746422618627548, loss=0.026663703843951225
I0205 16:46:00.757964 139759013103360 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.08213010430335999, loss=0.03001427836716175
I0205 16:46:33.188527 139759038281472 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.09172339737415314, loss=0.030532831326127052
I0205 16:47:05.452490 139759013103360 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.07528270035982132, loss=0.028889663517475128
I0205 16:47:38.054531 139759038281472 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.1477823704481125, loss=0.030148981139063835
I0205 16:48:10.147473 139759013103360 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.09823194146156311, loss=0.030077069997787476
I0205 16:48:42.183054 139759038281472 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.09295442700386047, loss=0.027866045013070107
I0205 16:49:12.443721 139919816816448 spec.py:321] Evaluating on the training split.
I0205 16:51:00.106310 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 16:51:03.312361 139919816816448 spec.py:349] Evaluating on the test split.
I0205 16:51:06.328324 139919816816448 submission_runner.py:408] Time since start: 12442.50s, 	Step: 26095, 	{'train/accuracy': 0.9914388656616211, 'train/loss': 0.027753343805670738, 'train/mean_average_precision': 0.4717187337805464, 'validation/accuracy': 0.9866493940353394, 'validation/loss': 0.044874098151922226, 'validation/mean_average_precision': 0.26778515215614146, 'validation/num_examples': 43793, 'test/accuracy': 0.9858587980270386, 'test/loss': 0.04757843539118767, 'test/mean_average_precision': 0.2584336771859823, 'test/num_examples': 43793, 'score': 8415.341947555542, 'total_duration': 12442.495477199554, 'accumulated_submission_time': 8415.341947555542, 'accumulated_eval_time': 4024.9607586860657, 'accumulated_logging_time': 1.4460327625274658}
I0205 16:51:06.349333 139735693903616 logging_writer.py:48] [26095] accumulated_eval_time=4024.960759, accumulated_logging_time=1.446033, accumulated_submission_time=8415.341948, global_step=26095, preemption_count=0, score=8415.341948, test/accuracy=0.985859, test/loss=0.047578, test/mean_average_precision=0.258434, test/num_examples=43793, total_duration=12442.495477, train/accuracy=0.991439, train/loss=0.027753, train/mean_average_precision=0.471719, validation/accuracy=0.986649, validation/loss=0.044874, validation/mean_average_precision=0.267785, validation/num_examples=43793
I0205 16:51:08.470719 139739177916160 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.10368480533361435, loss=0.027231842279434204
I0205 16:51:41.768059 139735693903616 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.10307184606790543, loss=0.03039260022342205
I0205 16:52:13.899742 139739177916160 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.10011675208806992, loss=0.03023582324385643
I0205 16:52:45.958757 139735693903616 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.08513245731592178, loss=0.02869250439107418
I0205 16:53:18.278746 139739177916160 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.0905282199382782, loss=0.02902446873486042
I0205 16:53:50.706607 139735693903616 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.08954744040966034, loss=0.03005729615688324
I0205 16:54:23.037684 139739177916160 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.09506377577781677, loss=0.027579158544540405
I0205 16:54:55.305676 139735693903616 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.09802646934986115, loss=0.031136829406023026
I0205 16:55:06.580782 139919816816448 spec.py:321] Evaluating on the training split.
I0205 16:56:47.909536 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 16:56:51.085753 139919816816448 spec.py:349] Evaluating on the test split.
I0205 16:56:56.281176 139919816816448 submission_runner.py:408] Time since start: 12792.45s, 	Step: 26836, 	{'train/accuracy': 0.9916952848434448, 'train/loss': 0.02694741077721119, 'train/mean_average_precision': 0.4850118415830761, 'validation/accuracy': 0.9866157174110413, 'validation/loss': 0.04488666355609894, 'validation/mean_average_precision': 0.2657575785424613, 'validation/num_examples': 43793, 'test/accuracy': 0.9857859015464783, 'test/loss': 0.04761894419789314, 'test/mean_average_precision': 0.2573965177777483, 'test/num_examples': 43793, 'score': 8655.540459394455, 'total_duration': 12792.448338985443, 'accumulated_submission_time': 8655.540459394455, 'accumulated_eval_time': 4134.6611087322235, 'accumulated_logging_time': 1.4795305728912354}
I0205 16:56:56.302914 139759013103360 logging_writer.py:48] [26836] accumulated_eval_time=4134.661109, accumulated_logging_time=1.479531, accumulated_submission_time=8655.540459, global_step=26836, preemption_count=0, score=8655.540459, test/accuracy=0.985786, test/loss=0.047619, test/mean_average_precision=0.257397, test/num_examples=43793, total_duration=12792.448339, train/accuracy=0.991695, train/loss=0.026947, train/mean_average_precision=0.485012, validation/accuracy=0.986616, validation/loss=0.044887, validation/mean_average_precision=0.265758, validation/num_examples=43793
I0205 16:57:17.210868 139759038281472 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.07652280479669571, loss=0.025825578719377518
I0205 16:57:48.935327 139759013103360 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.09878412634134293, loss=0.03258294239640236
I0205 16:58:21.111493 139759038281472 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.14130249619483948, loss=0.02922828495502472
I0205 16:58:53.052447 139759013103360 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.08797520399093628, loss=0.027596285566687584
I0205 16:59:24.985143 139759038281472 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.08887285739183426, loss=0.0313844159245491
I0205 16:59:56.654053 139759013103360 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.08937887102365494, loss=0.03000248223543167
I0205 17:00:28.414430 139759038281472 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.10722391307353973, loss=0.02665729448199272
I0205 17:00:56.312721 139919816816448 spec.py:321] Evaluating on the training split.
I0205 17:02:37.021885 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 17:02:40.136597 139919816816448 spec.py:349] Evaluating on the test split.
I0205 17:02:43.236663 139919816816448 submission_runner.py:408] Time since start: 13139.40s, 	Step: 27589, 	{'train/accuracy': 0.9915966391563416, 'train/loss': 0.02711980976164341, 'train/mean_average_precision': 0.48331825053568495, 'validation/accuracy': 0.9867532849311829, 'validation/loss': 0.04511391371488571, 'validation/mean_average_precision': 0.2680907991811648, 'validation/num_examples': 43793, 'test/accuracy': 0.9858987927436829, 'test/loss': 0.048005200922489166, 'test/mean_average_precision': 0.25320234524196866, 'test/num_examples': 43793, 'score': 8895.518734931946, 'total_duration': 13139.403820991516, 'accumulated_submission_time': 8895.518734931946, 'accumulated_eval_time': 4241.58500623703, 'accumulated_logging_time': 1.5119729042053223}
I0205 17:02:43.257291 139735693903616 logging_writer.py:48] [27589] accumulated_eval_time=4241.585006, accumulated_logging_time=1.511973, accumulated_submission_time=8895.518735, global_step=27589, preemption_count=0, score=8895.518735, test/accuracy=0.985899, test/loss=0.048005, test/mean_average_precision=0.253202, test/num_examples=43793, total_duration=13139.403821, train/accuracy=0.991597, train/loss=0.027120, train/mean_average_precision=0.483318, validation/accuracy=0.986753, validation/loss=0.045114, validation/mean_average_precision=0.268091, validation/num_examples=43793
I0205 17:02:47.411618 139739177916160 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.09310282766819, loss=0.028164096176624298
I0205 17:03:20.067731 139735693903616 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.1225614845752716, loss=0.03141115605831146
I0205 17:03:51.985045 139739177916160 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.09798926115036011, loss=0.030627310276031494
I0205 17:04:24.115292 139735693903616 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.09101001173257828, loss=0.025919275358319283
I0205 17:04:56.143355 139739177916160 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.09721498936414719, loss=0.028830459341406822
I0205 17:05:27.972386 139735693903616 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.0951882004737854, loss=0.03191192448139191
I0205 17:05:59.803735 139739177916160 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.12164104729890823, loss=0.028761427849531174
I0205 17:06:31.746240 139735693903616 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.12841251492500305, loss=0.025030314922332764
I0205 17:06:43.554542 139919816816448 spec.py:321] Evaluating on the training split.
I0205 17:08:29.631903 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 17:08:32.770208 139919816816448 spec.py:349] Evaluating on the test split.
I0205 17:08:35.860211 139919816816448 submission_runner.py:408] Time since start: 13492.03s, 	Step: 28338, 	{'train/accuracy': 0.9913761615753174, 'train/loss': 0.027994006872177124, 'train/mean_average_precision': 0.4537976301349314, 'validation/accuracy': 0.9864557385444641, 'validation/loss': 0.04573741555213928, 'validation/mean_average_precision': 0.26533273497510806, 'validation/num_examples': 43793, 'test/accuracy': 0.9856258630752563, 'test/loss': 0.048382312059402466, 'test/mean_average_precision': 0.2584529614231575, 'test/num_examples': 43793, 'score': 9135.784817695618, 'total_duration': 13492.027368068695, 'accumulated_submission_time': 9135.784817695618, 'accumulated_eval_time': 4353.890632867813, 'accumulated_logging_time': 1.5436317920684814}
I0205 17:08:35.881953 139752296675072 logging_writer.py:48] [28338] accumulated_eval_time=4353.890633, accumulated_logging_time=1.543632, accumulated_submission_time=9135.784818, global_step=28338, preemption_count=0, score=9135.784818, test/accuracy=0.985626, test/loss=0.048382, test/mean_average_precision=0.258453, test/num_examples=43793, total_duration=13492.027368, train/accuracy=0.991376, train/loss=0.027994, train/mean_average_precision=0.453798, validation/accuracy=0.986456, validation/loss=0.045737, validation/mean_average_precision=0.265333, validation/num_examples=43793
I0205 17:08:56.100345 139759038281472 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.11962760984897614, loss=0.029253581538796425
I0205 17:09:27.985891 139752296675072 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.11315146833658218, loss=0.029791073873639107
I0205 17:09:59.755477 139759038281472 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.11420069634914398, loss=0.03390355780720711
I0205 17:10:31.852836 139752296675072 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.09493428468704224, loss=0.027868002653121948
I0205 17:11:03.587639 139759038281472 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.09065868705511093, loss=0.02679477259516716
I0205 17:11:36.713674 139752296675072 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.09576757997274399, loss=0.03212447836995125
I0205 17:12:09.857515 139759038281472 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.1063842698931694, loss=0.02996186725795269
I0205 17:12:36.054125 139919816816448 spec.py:321] Evaluating on the training split.
I0205 17:14:22.662371 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 17:14:25.820102 139919816816448 spec.py:349] Evaluating on the test split.
I0205 17:14:28.864963 139919816816448 submission_runner.py:408] Time since start: 13845.03s, 	Step: 29081, 	{'train/accuracy': 0.9911956191062927, 'train/loss': 0.028365036472678185, 'train/mean_average_precision': 0.4424061318322148, 'validation/accuracy': 0.9867041707038879, 'validation/loss': 0.044825755059719086, 'validation/mean_average_precision': 0.26958300690427595, 'validation/num_examples': 43793, 'test/accuracy': 0.9858810901641846, 'test/loss': 0.04768301546573639, 'test/mean_average_precision': 0.2606048610788429, 'test/num_examples': 43793, 'score': 9375.92338514328, 'total_duration': 13845.032121896744, 'accumulated_submission_time': 9375.92338514328, 'accumulated_eval_time': 4466.701440811157, 'accumulated_logging_time': 1.5765349864959717}
I0205 17:14:28.886385 139739177916160 logging_writer.py:48] [29081] accumulated_eval_time=4466.701441, accumulated_logging_time=1.576535, accumulated_submission_time=9375.923385, global_step=29081, preemption_count=0, score=9375.923385, test/accuracy=0.985881, test/loss=0.047683, test/mean_average_precision=0.260605, test/num_examples=43793, total_duration=13845.032122, train/accuracy=0.991196, train/loss=0.028365, train/mean_average_precision=0.442406, validation/accuracy=0.986704, validation/loss=0.044826, validation/mean_average_precision=0.269583, validation/num_examples=43793
I0205 17:14:35.380781 139759013103360 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.10691515356302261, loss=0.02777530625462532
I0205 17:15:07.111656 139739177916160 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.1113329827785492, loss=0.029538139700889587
I0205 17:15:38.751047 139759013103360 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.10431041568517685, loss=0.03070693276822567
I0205 17:16:10.426689 139739177916160 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.12346167862415314, loss=0.031223390251398087
I0205 17:16:42.260914 139759013103360 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.10837710648775101, loss=0.02995113842189312
I0205 17:17:14.316741 139739177916160 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.08091090619564056, loss=0.027723783627152443
I0205 17:17:45.901018 139759013103360 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.08535698801279068, loss=0.02702592872083187
I0205 17:18:17.498470 139739177916160 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.0995119959115982, loss=0.028552012518048286
I0205 17:18:29.032078 139919816816448 spec.py:321] Evaluating on the training split.
I0205 17:20:09.652375 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 17:20:12.728700 139919816816448 spec.py:349] Evaluating on the test split.
I0205 17:20:17.790527 139919816816448 submission_runner.py:408] Time since start: 14193.96s, 	Step: 29837, 	{'train/accuracy': 0.9912551641464233, 'train/loss': 0.028403988108038902, 'train/mean_average_precision': 0.45873083964496636, 'validation/accuracy': 0.9867545366287231, 'validation/loss': 0.04485427960753441, 'validation/mean_average_precision': 0.26848781210751976, 'validation/num_examples': 43793, 'test/accuracy': 0.9859964847564697, 'test/loss': 0.04744412377476692, 'test/mean_average_precision': 0.26221145260141127, 'test/num_examples': 43793, 'score': 9616.0367436409, 'total_duration': 14193.957685470581, 'accumulated_submission_time': 9616.0367436409, 'accumulated_eval_time': 4575.459840536118, 'accumulated_logging_time': 1.6094539165496826}
I0205 17:20:17.812217 139735693903616 logging_writer.py:48] [29837] accumulated_eval_time=4575.459841, accumulated_logging_time=1.609454, accumulated_submission_time=9616.036744, global_step=29837, preemption_count=0, score=9616.036744, test/accuracy=0.985996, test/loss=0.047444, test/mean_average_precision=0.262211, test/num_examples=43793, total_duration=14193.957685, train/accuracy=0.991255, train/loss=0.028404, train/mean_average_precision=0.458731, validation/accuracy=0.986755, validation/loss=0.044854, validation/mean_average_precision=0.268488, validation/num_examples=43793
I0205 17:20:38.579770 139759038281472 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.11557690054178238, loss=0.02701452746987343
I0205 17:21:10.914061 139735693903616 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.10884593427181244, loss=0.03188464045524597
I0205 17:21:43.117979 139759038281472 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.11978896707296371, loss=0.029637569561600685
I0205 17:22:15.298416 139735693903616 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.11020036041736603, loss=0.024670837447047234
I0205 17:22:47.061862 139759038281472 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.11257540434598923, loss=0.029968084767460823
I0205 17:23:19.250502 139735693903616 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.11534999310970306, loss=0.0267217755317688
I0205 17:23:51.330744 139759038281472 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.1007087454199791, loss=0.025189848616719246
I0205 17:24:17.947635 139919816816448 spec.py:321] Evaluating on the training split.
I0205 17:26:00.154217 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 17:26:03.331893 139919816816448 spec.py:349] Evaluating on the test split.
I0205 17:26:06.264704 139919816816448 submission_runner.py:408] Time since start: 14542.43s, 	Step: 30584, 	{'train/accuracy': 0.9915154576301575, 'train/loss': 0.027463402599096298, 'train/mean_average_precision': 0.46339895940182796, 'validation/accuracy': 0.9865645170211792, 'validation/loss': 0.044793397188186646, 'validation/mean_average_precision': 0.27743048304507184, 'validation/num_examples': 43793, 'test/accuracy': 0.9858090877532959, 'test/loss': 0.04753444343805313, 'test/mean_average_precision': 0.26575707077668376, 'test/num_examples': 43793, 'score': 9856.137685060501, 'total_duration': 14542.431862354279, 'accumulated_submission_time': 9856.137685060501, 'accumulated_eval_time': 4683.776865005493, 'accumulated_logging_time': 1.6447956562042236}
I0205 17:26:06.285827 139752296675072 logging_writer.py:48] [30584] accumulated_eval_time=4683.776865, accumulated_logging_time=1.644796, accumulated_submission_time=9856.137685, global_step=30584, preemption_count=0, score=9856.137685, test/accuracy=0.985809, test/loss=0.047534, test/mean_average_precision=0.265757, test/num_examples=43793, total_duration=14542.431862, train/accuracy=0.991515, train/loss=0.027463, train/mean_average_precision=0.463399, validation/accuracy=0.986565, validation/loss=0.044793, validation/mean_average_precision=0.277430, validation/num_examples=43793
I0205 17:26:11.677451 139759013103360 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.10874417424201965, loss=0.031072676181793213
I0205 17:26:43.286637 139752296675072 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.08932244777679443, loss=0.02602422423660755
I0205 17:27:15.155529 139759013103360 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.09346867352724075, loss=0.030988784506917
I0205 17:27:47.131683 139752296675072 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.09306573122739792, loss=0.02772866003215313
I0205 17:28:19.009445 139759013103360 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.11234340816736221, loss=0.027033206075429916
I0205 17:28:50.817906 139752296675072 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.10940452665090561, loss=0.029999881982803345
I0205 17:29:23.248588 139759013103360 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.11895495653152466, loss=0.028969133272767067
I0205 17:29:54.869617 139752296675072 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.0910954549908638, loss=0.027664825320243835
I0205 17:30:06.387283 139919816816448 spec.py:321] Evaluating on the training split.
I0205 17:31:48.798377 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 17:31:51.821601 139919816816448 spec.py:349] Evaluating on the test split.
I0205 17:31:56.796562 139919816816448 submission_runner.py:408] Time since start: 14892.96s, 	Step: 31337, 	{'train/accuracy': 0.9915711283683777, 'train/loss': 0.027200734242796898, 'train/mean_average_precision': 0.47814745243094403, 'validation/accuracy': 0.9866875410079956, 'validation/loss': 0.04512999579310417, 'validation/mean_average_precision': 0.2771974970735388, 'validation/num_examples': 43793, 'test/accuracy': 0.9858158230781555, 'test/loss': 0.04799988493323326, 'test/mean_average_precision': 0.2566377316769796, 'test/num_examples': 43793, 'score': 10096.208181619644, 'total_duration': 14892.963721752167, 'accumulated_submission_time': 10096.208181619644, 'accumulated_eval_time': 4794.186100482941, 'accumulated_logging_time': 1.6768467426300049}
I0205 17:31:56.818583 139735693903616 logging_writer.py:48] [31337] accumulated_eval_time=4794.186100, accumulated_logging_time=1.676847, accumulated_submission_time=10096.208182, global_step=31337, preemption_count=0, score=10096.208182, test/accuracy=0.985816, test/loss=0.048000, test/mean_average_precision=0.256638, test/num_examples=43793, total_duration=14892.963722, train/accuracy=0.991571, train/loss=0.027201, train/mean_average_precision=0.478147, validation/accuracy=0.986688, validation/loss=0.045130, validation/mean_average_precision=0.277197, validation/num_examples=43793
I0205 17:32:17.036957 139759038281472 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.09897314012050629, loss=0.025677695870399475
I0205 17:32:48.822020 139735693903616 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.10083547234535217, loss=0.024667948484420776
I0205 17:33:20.525481 139759038281472 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.1265871822834015, loss=0.028930004686117172
I0205 17:33:52.109503 139735693903616 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.09189524501562119, loss=0.02830260619521141
I0205 17:34:24.347247 139759038281472 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.1004805639386177, loss=0.028127454221248627
I0205 17:34:56.254945 139735693903616 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.09174906462430954, loss=0.02601146139204502
I0205 17:35:27.559010 139759038281472 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.10511092096567154, loss=0.02999388985335827
I0205 17:35:56.853394 139919816816448 spec.py:321] Evaluating on the training split.
I0205 17:37:39.207544 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 17:37:42.670154 139919816816448 spec.py:349] Evaluating on the test split.
I0205 17:37:46.148105 139919816816448 submission_runner.py:408] Time since start: 15242.32s, 	Step: 32094, 	{'train/accuracy': 0.991542398929596, 'train/loss': 0.02716778591275215, 'train/mean_average_precision': 0.47173045852876155, 'validation/accuracy': 0.9865958094596863, 'validation/loss': 0.0454927459359169, 'validation/mean_average_precision': 0.26300805545576683, 'validation/num_examples': 43793, 'test/accuracy': 0.9858823418617249, 'test/loss': 0.0483216866850853, 'test/mean_average_precision': 0.25246182403224593, 'test/num_examples': 43793, 'score': 10336.211215496063, 'total_duration': 15242.315237522125, 'accumulated_submission_time': 10336.211215496063, 'accumulated_eval_time': 4903.480740070343, 'accumulated_logging_time': 1.7101600170135498}
I0205 17:37:46.173116 139739177916160 logging_writer.py:48] [32094] accumulated_eval_time=4903.480740, accumulated_logging_time=1.710160, accumulated_submission_time=10336.211215, global_step=32094, preemption_count=0, score=10336.211215, test/accuracy=0.985882, test/loss=0.048322, test/mean_average_precision=0.252462, test/num_examples=43793, total_duration=15242.315238, train/accuracy=0.991542, train/loss=0.027168, train/mean_average_precision=0.471730, validation/accuracy=0.986596, validation/loss=0.045493, validation/mean_average_precision=0.263008, validation/num_examples=43793
I0205 17:37:48.512961 139752296675072 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.11684662103652954, loss=0.026089539751410484
I0205 17:38:20.464174 139739177916160 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.09676070511341095, loss=0.028618525713682175
I0205 17:38:52.362185 139752296675072 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.10718178004026413, loss=0.027127521112561226
I0205 17:39:24.407085 139739177916160 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.13295982778072357, loss=0.029949162155389786
I0205 17:39:56.658612 139752296675072 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.08399996906518936, loss=0.02702670358121395
I0205 17:40:29.283607 139739177916160 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.12385790795087814, loss=0.03023296780884266
I0205 17:41:01.798857 139752296675072 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.10152767598628998, loss=0.02563331089913845
I0205 17:41:33.946289 139739177916160 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.11060258001089096, loss=0.027859890833497047
I0205 17:41:46.286273 139919816816448 spec.py:321] Evaluating on the training split.
I0205 17:43:25.706110 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 17:43:29.104823 139919816816448 spec.py:349] Evaluating on the test split.
I0205 17:43:32.459433 139919816816448 submission_runner.py:408] Time since start: 15588.63s, 	Step: 32840, 	{'train/accuracy': 0.9917683601379395, 'train/loss': 0.026312205940485, 'train/mean_average_precision': 0.5015883602624208, 'validation/accuracy': 0.9866209626197815, 'validation/loss': 0.04509390890598297, 'validation/mean_average_precision': 0.2753164946013637, 'validation/num_examples': 43793, 'test/accuracy': 0.9857816696166992, 'test/loss': 0.04810566455125809, 'test/mean_average_precision': 0.26276872265511453, 'test/num_examples': 43793, 'score': 10576.290118932724, 'total_duration': 15588.626574993134, 'accumulated_submission_time': 10576.290118932724, 'accumulated_eval_time': 5009.653836011887, 'accumulated_logging_time': 1.7483479976654053}
I0205 17:43:32.483066 139759013103360 logging_writer.py:48] [32840] accumulated_eval_time=5009.653836, accumulated_logging_time=1.748348, accumulated_submission_time=10576.290119, global_step=32840, preemption_count=0, score=10576.290119, test/accuracy=0.985782, test/loss=0.048106, test/mean_average_precision=0.262769, test/num_examples=43793, total_duration=15588.626575, train/accuracy=0.991768, train/loss=0.026312, train/mean_average_precision=0.501588, validation/accuracy=0.986621, validation/loss=0.045094, validation/mean_average_precision=0.275316, validation/num_examples=43793
I0205 17:43:52.217944 139759038281472 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.09669823199510574, loss=0.026128247380256653
I0205 17:44:24.331848 139759013103360 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.11829457432031631, loss=0.02515043318271637
I0205 17:44:56.786577 139759038281472 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.10231642425060272, loss=0.029310166835784912
I0205 17:45:29.592531 139759013103360 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.09726081043481827, loss=0.028147533535957336
I0205 17:46:02.103401 139759038281472 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.11140795052051544, loss=0.026965517550706863
I0205 17:46:34.479299 139759013103360 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.10742369294166565, loss=0.02886754460632801
I0205 17:47:06.703709 139759038281472 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.1123514324426651, loss=0.026547692716121674
I0205 17:47:32.776146 139919816816448 spec.py:321] Evaluating on the training split.
I0205 17:49:15.401743 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 17:49:18.798651 139919816816448 spec.py:349] Evaluating on the test split.
I0205 17:49:22.121526 139919816816448 submission_runner.py:408] Time since start: 15938.29s, 	Step: 33583, 	{'train/accuracy': 0.9920535683631897, 'train/loss': 0.025541843846440315, 'train/mean_average_precision': 0.5182077647609746, 'validation/accuracy': 0.9865832328796387, 'validation/loss': 0.045318685472011566, 'validation/mean_average_precision': 0.27103989282553687, 'validation/num_examples': 43793, 'test/accuracy': 0.9858322143554688, 'test/loss': 0.04798828065395355, 'test/mean_average_precision': 0.26138324044392885, 'test/num_examples': 43793, 'score': 10816.54667019844, 'total_duration': 15938.288664340973, 'accumulated_submission_time': 10816.54667019844, 'accumulated_eval_time': 5118.999151468277, 'accumulated_logging_time': 1.7833147048950195}
I0205 17:49:22.145251 139735693903616 logging_writer.py:48] [33583] accumulated_eval_time=5118.999151, accumulated_logging_time=1.783315, accumulated_submission_time=10816.546670, global_step=33583, preemption_count=0, score=10816.546670, test/accuracy=0.985832, test/loss=0.047988, test/mean_average_precision=0.261383, test/num_examples=43793, total_duration=15938.288664, train/accuracy=0.992054, train/loss=0.025542, train/mean_average_precision=0.518208, validation/accuracy=0.986583, validation/loss=0.045319, validation/mean_average_precision=0.271040, validation/num_examples=43793
I0205 17:49:28.147649 139739177916160 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.12265260517597198, loss=0.025396276265382767
I0205 17:50:00.956702 139735693903616 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.11831696331501007, loss=0.024660389870405197
I0205 17:50:32.717645 139739177916160 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.09913546591997147, loss=0.02589552477002144
I0205 17:51:05.005984 139735693903616 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.09857582300901413, loss=0.027486540377140045
I0205 17:51:36.728984 139739177916160 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.11508344858884811, loss=0.02860291302204132
I0205 17:52:08.852809 139735693903616 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.10798472166061401, loss=0.02618146315217018
I0205 17:52:40.282954 139739177916160 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.11332140862941742, loss=0.02680337242782116
I0205 17:53:11.955934 139735693903616 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.10909507423639297, loss=0.024402005597949028
I0205 17:53:22.413982 139919816816448 spec.py:321] Evaluating on the training split.
I0205 17:55:06.002139 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 17:55:09.386996 139919816816448 spec.py:349] Evaluating on the test split.
I0205 17:55:12.821329 139919816816448 submission_runner.py:408] Time since start: 16288.99s, 	Step: 34333, 	{'train/accuracy': 0.992278516292572, 'train/loss': 0.024827653542160988, 'train/mean_average_precision': 0.5308865696339622, 'validation/accuracy': 0.9868900775909424, 'validation/loss': 0.04549545422196388, 'validation/mean_average_precision': 0.27921532311605257, 'validation/num_examples': 43793, 'test/accuracy': 0.9859931468963623, 'test/loss': 0.04836021363735199, 'test/mean_average_precision': 0.2592067698810614, 'test/num_examples': 43793, 'score': 11056.78190946579, 'total_duration': 16288.988464832306, 'accumulated_submission_time': 11056.78190946579, 'accumulated_eval_time': 5229.406427145004, 'accumulated_logging_time': 1.818897008895874}
I0205 17:55:12.847413 139759013103360 logging_writer.py:48] [34333] accumulated_eval_time=5229.406427, accumulated_logging_time=1.818897, accumulated_submission_time=11056.781909, global_step=34333, preemption_count=0, score=11056.781909, test/accuracy=0.985993, test/loss=0.048360, test/mean_average_precision=0.259207, test/num_examples=43793, total_duration=16288.988465, train/accuracy=0.992279, train/loss=0.024828, train/mean_average_precision=0.530887, validation/accuracy=0.986890, validation/loss=0.045495, validation/mean_average_precision=0.279215, validation/num_examples=43793
I0205 17:55:34.982434 139759038281472 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.14151395857334137, loss=0.027793634682893753
I0205 17:56:08.207868 139759013103360 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.09873399883508682, loss=0.025376562029123306
I0205 17:56:40.929377 139759038281472 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.13729721307754517, loss=0.023474065586924553
I0205 17:57:13.321542 139759013103360 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.09667642414569855, loss=0.025941090658307076
I0205 17:57:45.312348 139759038281472 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.13122791051864624, loss=0.026940401643514633
I0205 17:58:17.470691 139759013103360 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.12661077082157135, loss=0.02826051600277424
I0205 17:58:49.200930 139759038281472 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.09012425690889359, loss=0.027794601395726204
I0205 17:59:13.073947 139919816816448 spec.py:321] Evaluating on the training split.
I0205 18:00:54.131282 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 18:00:57.138139 139919816816448 spec.py:349] Evaluating on the test split.
I0205 18:01:00.119742 139919816816448 submission_runner.py:408] Time since start: 16636.29s, 	Step: 35076, 	{'train/accuracy': 0.9921972751617432, 'train/loss': 0.025283345952630043, 'train/mean_average_precision': 0.5149612251079351, 'validation/accuracy': 0.9865661859512329, 'validation/loss': 0.045130666345357895, 'validation/mean_average_precision': 0.2722435905529067, 'validation/num_examples': 43793, 'test/accuracy': 0.985697865486145, 'test/loss': 0.04817037656903267, 'test/mean_average_precision': 0.25795848264725724, 'test/num_examples': 43793, 'score': 11296.9742269516, 'total_duration': 16636.286905050278, 'accumulated_submission_time': 11296.9742269516, 'accumulated_eval_time': 5336.452176809311, 'accumulated_logging_time': 1.8567280769348145}
I0205 18:01:00.142134 139735693903616 logging_writer.py:48] [35076] accumulated_eval_time=5336.452177, accumulated_logging_time=1.856728, accumulated_submission_time=11296.974227, global_step=35076, preemption_count=0, score=11296.974227, test/accuracy=0.985698, test/loss=0.048170, test/mean_average_precision=0.257958, test/num_examples=43793, total_duration=16636.286905, train/accuracy=0.992197, train/loss=0.025283, train/mean_average_precision=0.514961, validation/accuracy=0.986566, validation/loss=0.045131, validation/mean_average_precision=0.272244, validation/num_examples=43793
I0205 18:01:08.406675 139739177916160 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.10908851027488708, loss=0.026084838435053825
I0205 18:01:41.018465 139735693903616 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.11939243227243423, loss=0.029096921905875206
I0205 18:02:13.638285 139739177916160 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.09952349215745926, loss=0.026209617033600807
I0205 18:02:45.970214 139735693903616 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.10644936561584473, loss=0.026872029528021812
I0205 18:03:18.315966 139739177916160 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.13436707854270935, loss=0.03093247301876545
I0205 18:03:50.802260 139735693903616 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.11600132286548615, loss=0.026301799342036247
I0205 18:04:23.241293 139739177916160 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.11753988265991211, loss=0.02651977352797985
I0205 18:04:55.384458 139735693903616 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.1113695502281189, loss=0.0276859812438488
I0205 18:05:00.187441 139919816816448 spec.py:321] Evaluating on the training split.
I0205 18:06:44.141871 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 18:06:47.176773 139919816816448 spec.py:349] Evaluating on the test split.
I0205 18:06:50.204402 139919816816448 submission_runner.py:408] Time since start: 16986.37s, 	Step: 35816, 	{'train/accuracy': 0.99190354347229, 'train/loss': 0.026034832000732422, 'train/mean_average_precision': 0.5028728350713861, 'validation/accuracy': 0.9866254329681396, 'validation/loss': 0.04569826275110245, 'validation/mean_average_precision': 0.27601925715865744, 'validation/num_examples': 43793, 'test/accuracy': 0.9858474135398865, 'test/loss': 0.04843038320541382, 'test/mean_average_precision': 0.2547830847013111, 'test/num_examples': 43793, 'score': 11536.981744527817, 'total_duration': 16986.371559381485, 'accumulated_submission_time': 11536.981744527817, 'accumulated_eval_time': 5446.469096899033, 'accumulated_logging_time': 1.8912177085876465}
I0205 18:06:50.227522 139759013103360 logging_writer.py:48] [35816] accumulated_eval_time=5446.469097, accumulated_logging_time=1.891218, accumulated_submission_time=11536.981745, global_step=35816, preemption_count=0, score=11536.981745, test/accuracy=0.985847, test/loss=0.048430, test/mean_average_precision=0.254783, test/num_examples=43793, total_duration=16986.371559, train/accuracy=0.991904, train/loss=0.026035, train/mean_average_precision=0.502873, validation/accuracy=0.986625, validation/loss=0.045698, validation/mean_average_precision=0.276019, validation/num_examples=43793
I0205 18:07:17.232249 139759038281472 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.1361737698316574, loss=0.029145682230591774
I0205 18:07:48.744279 139759013103360 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.10656774789094925, loss=0.024431884288787842
I0205 18:08:20.646110 139759038281472 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.16281670331954956, loss=0.026081042364239693
I0205 18:08:52.187849 139759013103360 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.14618849754333496, loss=0.02656666375696659
I0205 18:09:23.739980 139759038281472 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.14721368253231049, loss=0.03040541522204876
I0205 18:09:55.327081 139759013103360 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.12830831110477448, loss=0.02600819617509842
I0205 18:10:27.224255 139759038281472 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.10604854673147202, loss=0.028562914580106735
I0205 18:10:50.459656 139919816816448 spec.py:321] Evaluating on the training split.
I0205 18:12:36.233046 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 18:12:39.625718 139919816816448 spec.py:349] Evaluating on the test split.
I0205 18:12:42.967744 139919816816448 submission_runner.py:408] Time since start: 17339.13s, 	Step: 36574, 	{'train/accuracy': 0.9917653799057007, 'train/loss': 0.026342226192355156, 'train/mean_average_precision': 0.49410733515559263, 'validation/accuracy': 0.9865604639053345, 'validation/loss': 0.0457744225859642, 'validation/mean_average_precision': 0.26810133070660647, 'validation/num_examples': 43793, 'test/accuracy': 0.9858174920082092, 'test/loss': 0.048509806394577026, 'test/mean_average_precision': 0.25872792358315133, 'test/num_examples': 43793, 'score': 11777.182457208633, 'total_duration': 17339.134885072708, 'accumulated_submission_time': 11777.182457208633, 'accumulated_eval_time': 5558.977123498917, 'accumulated_logging_time': 1.9253017902374268}
I0205 18:12:43.003895 139735693903616 logging_writer.py:48] [36574] accumulated_eval_time=5558.977123, accumulated_logging_time=1.925302, accumulated_submission_time=11777.182457, global_step=36574, preemption_count=0, score=11777.182457, test/accuracy=0.985817, test/loss=0.048510, test/mean_average_precision=0.258728, test/num_examples=43793, total_duration=17339.134885, train/accuracy=0.991765, train/loss=0.026342, train/mean_average_precision=0.494107, validation/accuracy=0.986560, validation/loss=0.045774, validation/mean_average_precision=0.268101, validation/num_examples=43793
I0205 18:12:51.867926 139739177916160 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.14844581484794617, loss=0.029305459931492805
I0205 18:13:23.948206 139735693903616 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.14514975249767303, loss=0.02972797490656376
I0205 18:13:55.623106 139739177916160 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.11478249728679657, loss=0.026719167828559875
I0205 18:14:27.247966 139735693903616 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.11427974700927734, loss=0.024096285924315453
I0205 18:14:58.656785 139739177916160 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.11579166352748871, loss=0.026312224566936493
I0205 18:15:30.312545 139735693903616 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.1088230088353157, loss=0.02835768833756447
I0205 18:16:01.578713 139739177916160 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.17356999218463898, loss=0.028275178745388985
I0205 18:16:33.497941 139735693903616 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.10946420580148697, loss=0.02528868056833744
I0205 18:16:43.283791 139919816816448 spec.py:321] Evaluating on the training split.
I0205 18:18:22.614155 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 18:18:25.643805 139919816816448 spec.py:349] Evaluating on the test split.
I0205 18:18:28.629658 139919816816448 submission_runner.py:408] Time since start: 17684.80s, 	Step: 37332, 	{'train/accuracy': 0.9916236400604248, 'train/loss': 0.026827674359083176, 'train/mean_average_precision': 0.49001116884105334, 'validation/accuracy': 0.98675936460495, 'validation/loss': 0.04567720368504524, 'validation/mean_average_precision': 0.26503827508144473, 'validation/num_examples': 43793, 'test/accuracy': 0.9857711791992188, 'test/loss': 0.04885249584913254, 'test/mean_average_precision': 0.25305268444256357, 'test/num_examples': 43793, 'score': 12017.42915058136, 'total_duration': 17684.79681611061, 'accumulated_submission_time': 12017.42915058136, 'accumulated_eval_time': 5664.32294178009, 'accumulated_logging_time': 1.9741060733795166}
I0205 18:18:28.652652 139752296675072 logging_writer.py:48] [37332] accumulated_eval_time=5664.322942, accumulated_logging_time=1.974106, accumulated_submission_time=12017.429151, global_step=37332, preemption_count=0, score=12017.429151, test/accuracy=0.985771, test/loss=0.048852, test/mean_average_precision=0.253053, test/num_examples=43793, total_duration=17684.796816, train/accuracy=0.991624, train/loss=0.026828, train/mean_average_precision=0.490011, validation/accuracy=0.986759, validation/loss=0.045677, validation/mean_average_precision=0.265038, validation/num_examples=43793
I0205 18:18:50.500125 139759038281472 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.09372962266206741, loss=0.024570556357502937
I0205 18:19:22.241410 139752296675072 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.14830853044986725, loss=0.02736073173582554
I0205 18:19:53.938767 139759038281472 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.11977693438529968, loss=0.02828528918325901
I0205 18:20:25.628202 139752296675072 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.13460546731948853, loss=0.0273821372538805
I0205 18:20:57.055403 139759038281472 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.1274227797985077, loss=0.02416512928903103
I0205 18:21:28.819908 139752296675072 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.13098761439323425, loss=0.027485299855470657
I0205 18:22:00.860394 139759038281472 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.12737151980400085, loss=0.02916029281914234
I0205 18:22:28.892794 139919816816448 spec.py:321] Evaluating on the training split.
I0205 18:24:11.102810 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 18:24:14.210578 139919816816448 spec.py:349] Evaluating on the test split.
I0205 18:24:17.259417 139919816816448 submission_runner.py:408] Time since start: 18033.43s, 	Step: 38089, 	{'train/accuracy': 0.9918012619018555, 'train/loss': 0.026224201545119286, 'train/mean_average_precision': 0.5010326044071443, 'validation/accuracy': 0.9865389466285706, 'validation/loss': 0.045652519911527634, 'validation/mean_average_precision': 0.27044119487028595, 'validation/num_examples': 43793, 'test/accuracy': 0.9857606291770935, 'test/loss': 0.04854629561305046, 'test/mean_average_precision': 0.2595443308006202, 'test/num_examples': 43793, 'score': 12257.636492729187, 'total_duration': 18033.42644929886, 'accumulated_submission_time': 12257.636492729187, 'accumulated_eval_time': 5772.689391851425, 'accumulated_logging_time': 2.009575843811035}
I0205 18:24:17.282069 139739177916160 logging_writer.py:48] [38089] accumulated_eval_time=5772.689392, accumulated_logging_time=2.009576, accumulated_submission_time=12257.636493, global_step=38089, preemption_count=0, score=12257.636493, test/accuracy=0.985761, test/loss=0.048546, test/mean_average_precision=0.259544, test/num_examples=43793, total_duration=18033.426449, train/accuracy=0.991801, train/loss=0.026224, train/mean_average_precision=0.501033, validation/accuracy=0.986539, validation/loss=0.045653, validation/mean_average_precision=0.270441, validation/num_examples=43793
I0205 18:24:21.163445 139759013103360 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.10870887339115143, loss=0.026225944980978966
I0205 18:24:52.670399 139739177916160 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.1275980919599533, loss=0.028635872527956963
I0205 18:25:24.381328 139759013103360 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.12884759902954102, loss=0.025745145976543427
I0205 18:25:56.048376 139739177916160 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.11690457165241241, loss=0.028213797137141228
I0205 18:26:27.577337 139759013103360 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.11804106086492538, loss=0.02462465688586235
I0205 18:26:59.351768 139739177916160 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.12048793584108353, loss=0.02552752196788788
I0205 18:27:31.145195 139759013103360 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.12080381065607071, loss=0.02815505489706993
I0205 18:28:02.973688 139739177916160 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.12746714055538177, loss=0.026387715712189674
I0205 18:28:17.420512 139919816816448 spec.py:321] Evaluating on the training split.
I0205 18:29:57.403917 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 18:30:00.592776 139919816816448 spec.py:349] Evaluating on the test split.
I0205 18:30:03.735480 139919816816448 submission_runner.py:408] Time since start: 18379.90s, 	Step: 38846, 	{'train/accuracy': 0.9921699166297913, 'train/loss': 0.02522186003625393, 'train/mean_average_precision': 0.5169255787785384, 'validation/accuracy': 0.9866672158241272, 'validation/loss': 0.04605378210544586, 'validation/mean_average_precision': 0.2735889551897641, 'validation/num_examples': 43793, 'test/accuracy': 0.9858554005622864, 'test/loss': 0.04902553930878639, 'test/mean_average_precision': 0.2538144049564341, 'test/num_examples': 43793, 'score': 12497.743663549423, 'total_duration': 18379.902636289597, 'accumulated_submission_time': 12497.743663549423, 'accumulated_eval_time': 5879.004307746887, 'accumulated_logging_time': 2.043231964111328}
I0205 18:30:03.758211 139735693903616 logging_writer.py:48] [38846] accumulated_eval_time=5879.004308, accumulated_logging_time=2.043232, accumulated_submission_time=12497.743664, global_step=38846, preemption_count=0, score=12497.743664, test/accuracy=0.985855, test/loss=0.049026, test/mean_average_precision=0.253814, test/num_examples=43793, total_duration=18379.902636, train/accuracy=0.992170, train/loss=0.025222, train/mean_average_precision=0.516926, validation/accuracy=0.986667, validation/loss=0.046054, validation/mean_average_precision=0.273589, validation/num_examples=43793
I0205 18:30:21.942368 139752296675072 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.12228766083717346, loss=0.024395592510700226
I0205 18:30:53.531884 139735693903616 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.1416662037372589, loss=0.02792995423078537
I0205 18:31:25.338856 139752296675072 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.16725023090839386, loss=0.028211072087287903
I0205 18:31:57.094219 139735693903616 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.14560209214687347, loss=0.027261175215244293
I0205 18:32:28.948145 139752296675072 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.10408326238393784, loss=0.026023391634225845
I0205 18:33:00.527608 139735693903616 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.11774919927120209, loss=0.026809198781847954
I0205 18:33:32.361883 139752296675072 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.1711154282093048, loss=0.025803595781326294
I0205 18:34:03.759798 139919816816448 spec.py:321] Evaluating on the training split.
I0205 18:35:44.984544 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 18:35:48.148318 139919816816448 spec.py:349] Evaluating on the test split.
I0205 18:35:51.196561 139919816816448 submission_runner.py:408] Time since start: 18727.36s, 	Step: 39599, 	{'train/accuracy': 0.9920904040336609, 'train/loss': 0.025093624368309975, 'train/mean_average_precision': 0.5144372891395481, 'validation/accuracy': 0.9867464303970337, 'validation/loss': 0.045916326344013214, 'validation/mean_average_precision': 0.2768331889155624, 'validation/num_examples': 43793, 'test/accuracy': 0.9858794212341309, 'test/loss': 0.048941463232040405, 'test/mean_average_precision': 0.25255159872354327, 'test/num_examples': 43793, 'score': 12737.713785409927, 'total_duration': 18727.36371779442, 'accumulated_submission_time': 12737.713785409927, 'accumulated_eval_time': 5986.441025257111, 'accumulated_logging_time': 2.076756715774536}
I0205 18:35:51.223036 139739177916160 logging_writer.py:48] [39599] accumulated_eval_time=5986.441025, accumulated_logging_time=2.076757, accumulated_submission_time=12737.713785, global_step=39599, preemption_count=0, score=12737.713785, test/accuracy=0.985879, test/loss=0.048941, test/mean_average_precision=0.252552, test/num_examples=43793, total_duration=18727.363718, train/accuracy=0.992090, train/loss=0.025094, train/mean_average_precision=0.514437, validation/accuracy=0.986746, validation/loss=0.045916, validation/mean_average_precision=0.276833, validation/num_examples=43793
I0205 18:35:51.899872 139759038281472 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.12719754874706268, loss=0.02519892156124115
I0205 18:36:24.068731 139739177916160 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.12774832546710968, loss=0.02614869736135006
I0205 18:36:55.884267 139759038281472 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.1489281952381134, loss=0.027179686352610588
I0205 18:37:27.662018 139739177916160 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.12523843348026276, loss=0.024587435647845268
I0205 18:37:59.289246 139759038281472 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.12187034636735916, loss=0.022876910865306854
I0205 18:38:31.186551 139739177916160 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.13262712955474854, loss=0.02537376433610916
I0205 18:39:03.197709 139759038281472 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.14333131909370422, loss=0.02477993629872799
I0205 18:39:35.263187 139739177916160 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.13369445502758026, loss=0.023997878655791283
I0205 18:39:51.268054 139919816816448 spec.py:321] Evaluating on the training split.
I0205 18:41:35.755065 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 18:41:38.873072 139919816816448 spec.py:349] Evaluating on the test split.
I0205 18:41:41.965455 139919816816448 submission_runner.py:408] Time since start: 19078.13s, 	Step: 40351, 	{'train/accuracy': 0.9924927949905396, 'train/loss': 0.023928308859467506, 'train/mean_average_precision': 0.5607641462687354, 'validation/accuracy': 0.9866968989372253, 'validation/loss': 0.045777395367622375, 'validation/mean_average_precision': 0.27961877377697586, 'validation/num_examples': 43793, 'test/accuracy': 0.9858419299125671, 'test/loss': 0.04900488629937172, 'test/mean_average_precision': 0.2534988346590776, 'test/num_examples': 43793, 'score': 12977.72698712349, 'total_duration': 19078.132613658905, 'accumulated_submission_time': 12977.72698712349, 'accumulated_eval_time': 6097.138377904892, 'accumulated_logging_time': 2.1142361164093018}
I0205 18:41:41.988791 139735693903616 logging_writer.py:48] [40351] accumulated_eval_time=6097.138378, accumulated_logging_time=2.114236, accumulated_submission_time=12977.726987, global_step=40351, preemption_count=0, score=12977.726987, test/accuracy=0.985842, test/loss=0.049005, test/mean_average_precision=0.253499, test/num_examples=43793, total_duration=19078.132614, train/accuracy=0.992493, train/loss=0.023928, train/mean_average_precision=0.560764, validation/accuracy=0.986697, validation/loss=0.045777, validation/mean_average_precision=0.279619, validation/num_examples=43793
I0205 18:41:57.993859 139759013103360 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.11148502677679062, loss=0.024597616866230965
I0205 18:42:29.910991 139735693903616 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.1559746116399765, loss=0.028940824791789055
I0205 18:43:01.483589 139759013103360 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.1103418618440628, loss=0.021863695234060287
I0205 18:43:33.757299 139735693903616 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.13147114217281342, loss=0.0239188801497221
I0205 18:44:05.991727 139759013103360 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.15776050090789795, loss=0.026743942871689796
I0205 18:44:37.968312 139735693903616 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.12615962326526642, loss=0.0244568083435297
I0205 18:45:09.709109 139759013103360 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.1191563680768013, loss=0.02438889443874359
I0205 18:45:41.429300 139735693903616 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.13675707578659058, loss=0.02604089304804802
I0205 18:45:42.066450 139919816816448 spec.py:321] Evaluating on the training split.
I0205 18:47:25.925194 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 18:47:28.959716 139919816816448 spec.py:349] Evaluating on the test split.
I0205 18:47:31.976878 139919816816448 submission_runner.py:408] Time since start: 19428.14s, 	Step: 41103, 	{'train/accuracy': 0.9927111864089966, 'train/loss': 0.023247594013810158, 'train/mean_average_precision': 0.5671089549068034, 'validation/accuracy': 0.9865982532501221, 'validation/loss': 0.04591871798038483, 'validation/mean_average_precision': 0.2702917025777467, 'validation/num_examples': 43793, 'test/accuracy': 0.9858731031417847, 'test/loss': 0.048900943249464035, 'test/mean_average_precision': 0.25475077138918667, 'test/num_examples': 43793, 'score': 13217.773389101028, 'total_duration': 19428.14403939247, 'accumulated_submission_time': 13217.773389101028, 'accumulated_eval_time': 6207.048756837845, 'accumulated_logging_time': 2.148378849029541}
I0205 18:47:31.999857 139752296675072 logging_writer.py:48] [41103] accumulated_eval_time=6207.048757, accumulated_logging_time=2.148379, accumulated_submission_time=13217.773389, global_step=41103, preemption_count=0, score=13217.773389, test/accuracy=0.985873, test/loss=0.048901, test/mean_average_precision=0.254751, test/num_examples=43793, total_duration=19428.144039, train/accuracy=0.992711, train/loss=0.023248, train/mean_average_precision=0.567109, validation/accuracy=0.986598, validation/loss=0.045919, validation/mean_average_precision=0.270292, validation/num_examples=43793
I0205 18:48:03.213893 139759038281472 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.11657292395830154, loss=0.02737507037818432
I0205 18:48:35.235064 139752296675072 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.15200120210647583, loss=0.025638120248913765
I0205 18:49:07.243765 139759038281472 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.16017885506153107, loss=0.02735205553472042
I0205 18:49:39.410289 139752296675072 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.11387325823307037, loss=0.022500377148389816
I0205 18:50:11.672102 139759038281472 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.11632449179887772, loss=0.02520770952105522
I0205 18:50:43.571535 139752296675072 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.13459891080856323, loss=0.026625556871294975
I0205 18:51:15.604555 139759038281472 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.1661445051431656, loss=0.028847765177488327
I0205 18:51:32.151621 139919816816448 spec.py:321] Evaluating on the training split.
I0205 18:53:11.432395 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 18:53:14.463519 139919816816448 spec.py:349] Evaluating on the test split.
I0205 18:53:17.532816 139919816816448 submission_runner.py:408] Time since start: 19773.70s, 	Step: 41852, 	{'train/accuracy': 0.9927569031715393, 'train/loss': 0.02298024669289589, 'train/mean_average_precision': 0.5699415537277851, 'validation/accuracy': 0.9866871237754822, 'validation/loss': 0.04621214047074318, 'validation/mean_average_precision': 0.27547600810480405, 'validation/num_examples': 43793, 'test/accuracy': 0.9858137369155884, 'test/loss': 0.049364667385816574, 'test/mean_average_precision': 0.25842190406679577, 'test/num_examples': 43793, 'score': 13457.892441272736, 'total_duration': 19773.699976205826, 'accumulated_submission_time': 13457.892441272736, 'accumulated_eval_time': 6312.429904937744, 'accumulated_logging_time': 2.1833369731903076}
I0205 18:53:17.556400 139735693903616 logging_writer.py:48] [41852] accumulated_eval_time=6312.429905, accumulated_logging_time=2.183337, accumulated_submission_time=13457.892441, global_step=41852, preemption_count=0, score=13457.892441, test/accuracy=0.985814, test/loss=0.049365, test/mean_average_precision=0.258422, test/num_examples=43793, total_duration=19773.699976, train/accuracy=0.992757, train/loss=0.022980, train/mean_average_precision=0.569942, validation/accuracy=0.986687, validation/loss=0.046212, validation/mean_average_precision=0.275476, validation/num_examples=43793
I0205 18:53:33.197067 139759013103360 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.12428466230630875, loss=0.025332851335406303
I0205 18:54:05.418950 139735693903616 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.1252610683441162, loss=0.022375663742423058
I0205 18:54:37.496976 139759013103360 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.1562679558992386, loss=0.025510139763355255
I0205 18:55:09.361570 139735693903616 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.13995039463043213, loss=0.022210245952010155
I0205 18:55:41.313466 139759013103360 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.12067364901304245, loss=0.023362664505839348
I0205 18:56:13.130125 139735693903616 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.1770457774400711, loss=0.02583487518131733
I0205 18:56:44.792324 139759013103360 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.15089541673660278, loss=0.023080073297023773
I0205 18:57:16.436025 139735693903616 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.14693734049797058, loss=0.02410265803337097
I0205 18:57:17.750054 139919816816448 spec.py:321] Evaluating on the training split.
I0205 18:58:59.558972 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 18:59:02.737417 139919816816448 spec.py:349] Evaluating on the test split.
I0205 18:59:05.726874 139919816816448 submission_runner.py:408] Time since start: 20121.89s, 	Step: 42605, 	{'train/accuracy': 0.9927787184715271, 'train/loss': 0.022964106872677803, 'train/mean_average_precision': 0.5776440654013806, 'validation/accuracy': 0.9868137836456299, 'validation/loss': 0.046512555330991745, 'validation/mean_average_precision': 0.2742111553186739, 'validation/num_examples': 43793, 'test/accuracy': 0.985913097858429, 'test/loss': 0.04963935166597366, 'test/mean_average_precision': 0.2544896576267384, 'test/num_examples': 43793, 'score': 13698.05339050293, 'total_duration': 20121.89403295517, 'accumulated_submission_time': 13698.05339050293, 'accumulated_eval_time': 6420.406673908234, 'accumulated_logging_time': 2.2188665866851807}
I0205 18:59:05.751199 139752296675072 logging_writer.py:48] [42605] accumulated_eval_time=6420.406674, accumulated_logging_time=2.218867, accumulated_submission_time=13698.053391, global_step=42605, preemption_count=0, score=13698.053391, test/accuracy=0.985913, test/loss=0.049639, test/mean_average_precision=0.254490, test/num_examples=43793, total_duration=20121.894033, train/accuracy=0.992779, train/loss=0.022964, train/mean_average_precision=0.577644, validation/accuracy=0.986814, validation/loss=0.046513, validation/mean_average_precision=0.274211, validation/num_examples=43793
I0205 18:59:36.845798 139759038281472 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.11174598336219788, loss=0.025941116735339165
I0205 19:00:08.517726 139752296675072 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.15540461242198944, loss=0.023150410503149033
I0205 19:00:40.529616 139759038281472 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.1352205127477646, loss=0.025716619566082954
I0205 19:01:11.965232 139752296675072 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.11814133822917938, loss=0.023438172414898872
I0205 19:01:43.783224 139759038281472 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.14220088720321655, loss=0.021658826619386673
I0205 19:02:15.668996 139752296675072 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.1435975879430771, loss=0.02415291778743267
I0205 19:02:47.420469 139759038281472 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.15959399938583374, loss=0.02508043870329857
I0205 19:03:05.937391 139919816816448 spec.py:321] Evaluating on the training split.
I0205 19:04:47.409710 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 19:04:50.482149 139919816816448 spec.py:349] Evaluating on the test split.
I0205 19:04:53.468253 139919816816448 submission_runner.py:408] Time since start: 20469.64s, 	Step: 43359, 	{'train/accuracy': 0.9925683736801147, 'train/loss': 0.02361549809575081, 'train/mean_average_precision': 0.553858979056238, 'validation/accuracy': 0.9865787625312805, 'validation/loss': 0.04614240303635597, 'validation/mean_average_precision': 0.27882798998942643, 'validation/num_examples': 43793, 'test/accuracy': 0.9858166575431824, 'test/loss': 0.049183864146471024, 'test/mean_average_precision': 0.2583898694445723, 'test/num_examples': 43793, 'score': 13938.207823753357, 'total_duration': 20469.63530278206, 'accumulated_submission_time': 13938.207823753357, 'accumulated_eval_time': 6527.937375545502, 'accumulated_logging_time': 2.253929853439331}
I0205 19:04:53.491585 139735693903616 logging_writer.py:48] [43359] accumulated_eval_time=6527.937376, accumulated_logging_time=2.253930, accumulated_submission_time=13938.207824, global_step=43359, preemption_count=0, score=13938.207824, test/accuracy=0.985817, test/loss=0.049184, test/mean_average_precision=0.258390, test/num_examples=43793, total_duration=20469.635303, train/accuracy=0.992568, train/loss=0.023615, train/mean_average_precision=0.553859, validation/accuracy=0.986579, validation/loss=0.046142, validation/mean_average_precision=0.278828, validation/num_examples=43793
I0205 19:05:07.043529 139739177916160 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.1374354511499405, loss=0.023393895477056503
I0205 19:05:38.657733 139735693903616 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.13933372497558594, loss=0.024871917441487312
I0205 19:06:10.628355 139739177916160 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.12832801043987274, loss=0.021756459027528763
I0205 19:06:42.530441 139735693903616 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.14357776939868927, loss=0.024668937548995018
I0205 19:07:14.329635 139739177916160 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.13503031432628632, loss=0.026544194668531418
I0205 19:07:46.142086 139735693903616 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.13932542502880096, loss=0.02180633880198002
I0205 19:08:17.870280 139739177916160 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.15094900131225586, loss=0.026321405544877052
I0205 19:08:49.753148 139735693903616 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.11856575310230255, loss=0.022406119853258133
I0205 19:08:53.512692 139919816816448 spec.py:321] Evaluating on the training split.
I0205 19:10:33.850665 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 19:10:36.852526 139919816816448 spec.py:349] Evaluating on the test split.
I0205 19:10:39.817758 139919816816448 submission_runner.py:408] Time since start: 20815.98s, 	Step: 44113, 	{'train/accuracy': 0.9924434423446655, 'train/loss': 0.024065496399998665, 'train/mean_average_precision': 0.5444905132486455, 'validation/accuracy': 0.9865491390228271, 'validation/loss': 0.04658720642328262, 'validation/mean_average_precision': 0.27540289113259364, 'validation/num_examples': 43793, 'test/accuracy': 0.9856974482536316, 'test/loss': 0.04961707442998886, 'test/mean_average_precision': 0.2565613538093232, 'test/num_examples': 43793, 'score': 14178.196440935135, 'total_duration': 20815.98492026329, 'accumulated_submission_time': 14178.196440935135, 'accumulated_eval_time': 6634.2423985004425, 'accumulated_logging_time': 2.289602518081665}
I0205 19:10:39.843182 139752296675072 logging_writer.py:48] [44113] accumulated_eval_time=6634.242399, accumulated_logging_time=2.289603, accumulated_submission_time=14178.196441, global_step=44113, preemption_count=0, score=14178.196441, test/accuracy=0.985697, test/loss=0.049617, test/mean_average_precision=0.256561, test/num_examples=43793, total_duration=20815.984920, train/accuracy=0.992443, train/loss=0.024065, train/mean_average_precision=0.544491, validation/accuracy=0.986549, validation/loss=0.046587, validation/mean_average_precision=0.275403, validation/num_examples=43793
I0205 19:11:08.345576 139759013103360 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.1730225682258606, loss=0.027642279863357544
I0205 19:11:40.331440 139752296675072 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.14335238933563232, loss=0.02430744469165802
I0205 19:12:11.801580 139759013103360 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.14615212380886078, loss=0.02426513284444809
I0205 19:12:43.379737 139752296675072 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.12366708368062973, loss=0.021921562030911446
I0205 19:13:15.235512 139759013103360 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.13593600690364838, loss=0.02450198493897915
I0205 19:13:46.900333 139752296675072 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.12581992149353027, loss=0.020501647144556046
I0205 19:14:18.582926 139759013103360 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.1477184295654297, loss=0.020212780684232712
I0205 19:14:40.113938 139919816816448 spec.py:321] Evaluating on the training split.
I0205 19:16:22.135249 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 19:16:25.186781 139919816816448 spec.py:349] Evaluating on the test split.
I0205 19:16:28.155606 139919816816448 submission_runner.py:408] Time since start: 21164.32s, 	Step: 44869, 	{'train/accuracy': 0.9923385977745056, 'train/loss': 0.024227526038885117, 'train/mean_average_precision': 0.5417031986400285, 'validation/accuracy': 0.9865332841873169, 'validation/loss': 0.04700421169400215, 'validation/mean_average_precision': 0.2736799668428959, 'validation/num_examples': 43793, 'test/accuracy': 0.9857791662216187, 'test/loss': 0.05012236163020134, 'test/mean_average_precision': 0.2560017213560228, 'test/num_examples': 43793, 'score': 14418.435846090317, 'total_duration': 21164.322757959366, 'accumulated_submission_time': 14418.435846090317, 'accumulated_eval_time': 6742.28401350975, 'accumulated_logging_time': 2.325899839401245}
I0205 19:16:28.179527 139735693903616 logging_writer.py:48] [44869] accumulated_eval_time=6742.284014, accumulated_logging_time=2.325900, accumulated_submission_time=14418.435846, global_step=44869, preemption_count=0, score=14418.435846, test/accuracy=0.985779, test/loss=0.050122, test/mean_average_precision=0.256002, test/num_examples=43793, total_duration=21164.322758, train/accuracy=0.992339, train/loss=0.024228, train/mean_average_precision=0.541703, validation/accuracy=0.986533, validation/loss=0.047004, validation/mean_average_precision=0.273680, validation/num_examples=43793
I0205 19:16:38.466216 139759038281472 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.1344960331916809, loss=0.023473037406802177
I0205 19:17:10.439202 139735693903616 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.16847975552082062, loss=0.02424381673336029
I0205 19:17:42.194928 139759038281472 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.16776926815509796, loss=0.023467209190130234
I0205 19:18:14.445994 139735693903616 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.15650196373462677, loss=0.025465868413448334
I0205 19:18:46.561532 139759038281472 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.12098093330860138, loss=0.020883949473500252
I0205 19:19:18.819401 139735693903616 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.14702416956424713, loss=0.022906722500920296
I0205 19:19:50.712482 139759038281472 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.12470158189535141, loss=0.02282768115401268
I0205 19:20:22.395079 139735693903616 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.14594534039497375, loss=0.02509375847876072
I0205 19:20:28.282819 139919816816448 spec.py:321] Evaluating on the training split.
I0205 19:22:10.140781 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 19:22:13.136614 139919816816448 spec.py:349] Evaluating on the test split.
I0205 19:22:16.102241 139919816816448 submission_runner.py:408] Time since start: 21512.27s, 	Step: 45619, 	{'train/accuracy': 0.9924070835113525, 'train/loss': 0.02397083304822445, 'train/mean_average_precision': 0.5425693027797756, 'validation/accuracy': 0.9866347908973694, 'validation/loss': 0.0474613681435585, 'validation/mean_average_precision': 0.2696670163389971, 'validation/num_examples': 43793, 'test/accuracy': 0.9858579039573669, 'test/loss': 0.050393376499414444, 'test/mean_average_precision': 0.2503948832383014, 'test/num_examples': 43793, 'score': 14658.508082866669, 'total_duration': 21512.269384622574, 'accumulated_submission_time': 14658.508082866669, 'accumulated_eval_time': 6850.10337138176, 'accumulated_logging_time': 2.360506772994995}
I0205 19:22:16.126162 139739177916160 logging_writer.py:48] [45619] accumulated_eval_time=6850.103371, accumulated_logging_time=2.360507, accumulated_submission_time=14658.508083, global_step=45619, preemption_count=0, score=14658.508083, test/accuracy=0.985858, test/loss=0.050393, test/mean_average_precision=0.250395, test/num_examples=43793, total_duration=21512.269385, train/accuracy=0.992407, train/loss=0.023971, train/mean_average_precision=0.542569, validation/accuracy=0.986635, validation/loss=0.047461, validation/mean_average_precision=0.269667, validation/num_examples=43793
I0205 19:22:42.153402 139759013103360 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.12989720702171326, loss=0.02208203263580799
I0205 19:23:13.852698 139739177916160 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.1505531668663025, loss=0.02649388276040554
I0205 19:23:45.381479 139759013103360 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.18084384500980377, loss=0.02770582027733326
I0205 19:24:17.041694 139739177916160 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.13764359056949615, loss=0.023704443126916885
I0205 19:24:48.586678 139759013103360 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.15354251861572266, loss=0.021308423951268196
I0205 19:25:20.122212 139739177916160 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.14029255509376526, loss=0.02248934470117092
I0205 19:25:51.435065 139759013103360 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.18033356964588165, loss=0.02485048957169056
I0205 19:26:16.363335 139919816816448 spec.py:321] Evaluating on the training split.
I0205 19:27:54.015428 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 19:27:57.027220 139919816816448 spec.py:349] Evaluating on the test split.
I0205 19:28:00.042844 139919816816448 submission_runner.py:408] Time since start: 21856.21s, 	Step: 46379, 	{'train/accuracy': 0.9926638007164001, 'train/loss': 0.023263966664671898, 'train/mean_average_precision': 0.5599083695297632, 'validation/accuracy': 0.9865369200706482, 'validation/loss': 0.04737938195466995, 'validation/mean_average_precision': 0.2749799347087452, 'validation/num_examples': 43793, 'test/accuracy': 0.985714316368103, 'test/loss': 0.050589628517627716, 'test/mean_average_precision': 0.25259962869472635, 'test/num_examples': 43793, 'score': 14898.713564157486, 'total_duration': 21856.210003376007, 'accumulated_submission_time': 14898.713564157486, 'accumulated_eval_time': 6953.782834768295, 'accumulated_logging_time': 2.3951354026794434}
I0205 19:28:00.067181 139752296675072 logging_writer.py:48] [46379] accumulated_eval_time=6953.782835, accumulated_logging_time=2.395135, accumulated_submission_time=14898.713564, global_step=46379, preemption_count=0, score=14898.713564, test/accuracy=0.985714, test/loss=0.050590, test/mean_average_precision=0.252600, test/num_examples=43793, total_duration=21856.210003, train/accuracy=0.992664, train/loss=0.023264, train/mean_average_precision=0.559908, validation/accuracy=0.986537, validation/loss=0.047379, validation/mean_average_precision=0.274980, validation/num_examples=43793
I0205 19:28:07.198356 139759038281472 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.15478548407554626, loss=0.02054045908153057
I0205 19:28:38.722371 139752296675072 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.1313275545835495, loss=0.02096603624522686
I0205 19:29:10.346600 139759038281472 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.15812267363071442, loss=0.025771867483854294
I0205 19:29:41.910918 139752296675072 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.11458703130483627, loss=0.022806737571954727
I0205 19:30:13.626584 139759038281472 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.18858711421489716, loss=0.02526555396616459
I0205 19:30:45.620415 139752296675072 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.12309563905000687, loss=0.021436119452118874
I0205 19:31:18.801134 139759038281472 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.14032357931137085, loss=0.02177460491657257
I0205 19:31:50.762356 139752296675072 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.17248184978961945, loss=0.024512887001037598
I0205 19:32:00.204587 139919816816448 spec.py:321] Evaluating on the training split.
I0205 19:33:41.700024 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 19:33:44.758676 139919816816448 spec.py:349] Evaluating on the test split.
I0205 19:33:47.711335 139919816816448 submission_runner.py:408] Time since start: 22203.88s, 	Step: 47131, 	{'train/accuracy': 0.992831289768219, 'train/loss': 0.022676663473248482, 'train/mean_average_precision': 0.5819867362002796, 'validation/accuracy': 0.9864833354949951, 'validation/loss': 0.04743078351020813, 'validation/mean_average_precision': 0.2747054695439795, 'validation/num_examples': 43793, 'test/accuracy': 0.9857437610626221, 'test/loss': 0.05043184757232666, 'test/mean_average_precision': 0.26107503575082996, 'test/num_examples': 43793, 'score': 15138.819725036621, 'total_duration': 22203.87849545479, 'accumulated_submission_time': 15138.819725036621, 'accumulated_eval_time': 7061.289536476135, 'accumulated_logging_time': 2.430541515350342}
I0205 19:33:47.739053 139735693903616 logging_writer.py:48] [47131] accumulated_eval_time=7061.289536, accumulated_logging_time=2.430542, accumulated_submission_time=15138.819725, global_step=47131, preemption_count=0, score=15138.819725, test/accuracy=0.985744, test/loss=0.050432, test/mean_average_precision=0.261075, test/num_examples=43793, total_duration=22203.878495, train/accuracy=0.992831, train/loss=0.022677, train/mean_average_precision=0.581987, validation/accuracy=0.986483, validation/loss=0.047431, validation/mean_average_precision=0.274705, validation/num_examples=43793
I0205 19:34:09.950616 139739177916160 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.13988903164863586, loss=0.021247386932373047
I0205 19:34:41.527732 139735693903616 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.17011477053165436, loss=0.023387117311358452
I0205 19:35:13.299498 139739177916160 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.1671450436115265, loss=0.022277267649769783
I0205 19:35:44.953289 139735693903616 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.1611170768737793, loss=0.021899348124861717
I0205 19:36:16.826977 139739177916160 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.13857409358024597, loss=0.02210003323853016
I0205 19:36:48.660228 139735693903616 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.14648738503456116, loss=0.025420768186450005
I0205 19:37:20.499675 139739177916160 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.13486821949481964, loss=0.022130222991108894
I0205 19:37:47.809475 139919816816448 spec.py:321] Evaluating on the training split.
I0205 19:39:25.895571 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 19:39:28.944867 139919816816448 spec.py:349] Evaluating on the test split.
I0205 19:39:32.049046 139919816816448 submission_runner.py:408] Time since start: 22548.22s, 	Step: 47888, 	{'train/accuracy': 0.9928385615348816, 'train/loss': 0.022271446883678436, 'train/mean_average_precision': 0.5891975629574058, 'validation/accuracy': 0.9865233302116394, 'validation/loss': 0.04813092201948166, 'validation/mean_average_precision': 0.26875716952864187, 'validation/num_examples': 43793, 'test/accuracy': 0.9857400059700012, 'test/loss': 0.05103105306625366, 'test/mean_average_precision': 0.2514952056072648, 'test/num_examples': 43793, 'score': 15378.858348846436, 'total_duration': 22548.216205596924, 'accumulated_submission_time': 15378.858348846436, 'accumulated_eval_time': 7165.529061079025, 'accumulated_logging_time': 2.469257354736328}
I0205 19:39:32.074239 139759013103360 logging_writer.py:48] [47888] accumulated_eval_time=7165.529061, accumulated_logging_time=2.469257, accumulated_submission_time=15378.858349, global_step=47888, preemption_count=0, score=15378.858349, test/accuracy=0.985740, test/loss=0.051031, test/mean_average_precision=0.251495, test/num_examples=43793, total_duration=22548.216206, train/accuracy=0.992839, train/loss=0.022271, train/mean_average_precision=0.589198, validation/accuracy=0.986523, validation/loss=0.048131, validation/mean_average_precision=0.268757, validation/num_examples=43793
I0205 19:39:36.798829 139759038281472 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.17212679982185364, loss=0.023095132783055305
I0205 19:40:09.892616 139759013103360 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.14246000349521637, loss=0.02321154810488224
I0205 19:40:42.186757 139759038281472 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.12074381858110428, loss=0.01814286969602108
I0205 19:41:14.135364 139759013103360 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.13611344993114471, loss=0.02068500407040119
I0205 19:41:46.275675 139759038281472 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.13768288493156433, loss=0.022645393386483192
I0205 19:42:18.083834 139759013103360 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.14217768609523773, loss=0.024549564346671104
I0205 19:42:49.756469 139759038281472 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.1504717469215393, loss=0.020357605069875717
I0205 19:43:21.361048 139759013103360 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.14597362279891968, loss=0.02115156129002571
I0205 19:43:32.268909 139919816816448 spec.py:321] Evaluating on the training split.
I0205 19:45:15.217270 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 19:45:18.261199 139919816816448 spec.py:349] Evaluating on the test split.
I0205 19:45:21.262422 139919816816448 submission_runner.py:408] Time since start: 22897.43s, 	Step: 48635, 	{'train/accuracy': 0.9935416579246521, 'train/loss': 0.020306341350078583, 'train/mean_average_precision': 0.6341293814817233, 'validation/accuracy': 0.9865320920944214, 'validation/loss': 0.04798257723450661, 'validation/mean_average_precision': 0.27303969033562364, 'validation/num_examples': 43793, 'test/accuracy': 0.9856961965560913, 'test/loss': 0.050840746611356735, 'test/mean_average_precision': 0.25393946714323823, 'test/num_examples': 43793, 'score': 15619.018918275833, 'total_duration': 22897.42957997322, 'accumulated_submission_time': 15619.018918275833, 'accumulated_eval_time': 7274.522526025772, 'accumulated_logging_time': 2.5069985389709473}
I0205 19:45:21.287513 139735693903616 logging_writer.py:48] [48635] accumulated_eval_time=7274.522526, accumulated_logging_time=2.506999, accumulated_submission_time=15619.018918, global_step=48635, preemption_count=0, score=15619.018918, test/accuracy=0.985696, test/loss=0.050841, test/mean_average_precision=0.253939, test/num_examples=43793, total_duration=22897.429580, train/accuracy=0.993542, train/loss=0.020306, train/mean_average_precision=0.634129, validation/accuracy=0.986532, validation/loss=0.047983, validation/mean_average_precision=0.273040, validation/num_examples=43793
I0205 19:45:42.549290 139752296675072 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.154243603348732, loss=0.02413269318640232
I0205 19:46:14.907823 139735693903616 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.13534638285636902, loss=0.020567888393998146
I0205 19:46:47.062069 139752296675072 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.13079892098903656, loss=0.019843848422169685
I0205 19:47:19.116837 139735693903616 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.16857706010341644, loss=0.024243658408522606
I0205 19:47:51.128925 139752296675072 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.17013002932071686, loss=0.022829703986644745
I0205 19:48:23.053683 139735693903616 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.18136893212795258, loss=0.021436359733343124
I0205 19:48:54.744474 139752296675072 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.14652037620544434, loss=0.022106345742940903
I0205 19:49:21.410336 139919816816448 spec.py:321] Evaluating on the training split.
I0205 19:51:01.332930 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 19:51:04.458977 139919816816448 spec.py:349] Evaluating on the test split.
I0205 19:51:07.489197 139919816816448 submission_runner.py:408] Time since start: 23243.66s, 	Step: 49384, 	{'train/accuracy': 0.993671178817749, 'train/loss': 0.020141547545790672, 'train/mean_average_precision': 0.6405901932539897, 'validation/accuracy': 0.9863559007644653, 'validation/loss': 0.048238445073366165, 'validation/mean_average_precision': 0.2759097759683734, 'validation/num_examples': 43793, 'test/accuracy': 0.985526442527771, 'test/loss': 0.051412928849458694, 'test/mean_average_precision': 0.25371684299710057, 'test/num_examples': 43793, 'score': 15859.109230279922, 'total_duration': 23243.656244277954, 'accumulated_submission_time': 15859.109230279922, 'accumulated_eval_time': 7380.601233720779, 'accumulated_logging_time': 2.544231653213501}
I0205 19:51:07.513705 139759013103360 logging_writer.py:48] [49384] accumulated_eval_time=7380.601234, accumulated_logging_time=2.544232, accumulated_submission_time=15859.109230, global_step=49384, preemption_count=0, score=15859.109230, test/accuracy=0.985526, test/loss=0.051413, test/mean_average_precision=0.253717, test/num_examples=43793, total_duration=23243.656244, train/accuracy=0.993671, train/loss=0.020142, train/mean_average_precision=0.640590, validation/accuracy=0.986356, validation/loss=0.048238, validation/mean_average_precision=0.275910, validation/num_examples=43793
I0205 19:51:13.086384 139759038281472 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.16600055992603302, loss=0.022365793585777283
I0205 19:51:45.740118 139759013103360 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.17290541529655457, loss=0.021308058872818947
I0205 19:52:18.461426 139759038281472 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.15978556871414185, loss=0.020477773621678352
I0205 19:52:49.887447 139759013103360 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.144583061337471, loss=0.020432008430361748
I0205 19:53:21.556846 139759038281472 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.17958295345306396, loss=0.022543050348758698
I0205 19:53:53.158149 139759013103360 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.14845220744609833, loss=0.02127527818083763
I0205 19:54:24.846963 139759038281472 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.16335934400558472, loss=0.020350340753793716
I0205 19:54:56.531059 139759013103360 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.14918586611747742, loss=0.01998392678797245
I0205 19:55:07.629056 139919816816448 spec.py:321] Evaluating on the training split.
I0205 19:56:45.943802 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 19:56:48.961862 139919816816448 spec.py:349] Evaluating on the test split.
I0205 19:56:51.934885 139919816816448 submission_runner.py:408] Time since start: 23588.10s, 	Step: 50136, 	{'train/accuracy': 0.9936359524726868, 'train/loss': 0.019929487258195877, 'train/mean_average_precision': 0.634786984643436, 'validation/accuracy': 0.986668050289154, 'validation/loss': 0.048447899520397186, 'validation/mean_average_precision': 0.2804146011801025, 'validation/num_examples': 43793, 'test/accuracy': 0.9857547283172607, 'test/loss': 0.05165000259876251, 'test/mean_average_precision': 0.25744070491900967, 'test/num_examples': 43793, 'score': 16099.191838979721, 'total_duration': 23588.102046728134, 'accumulated_submission_time': 16099.191838979721, 'accumulated_eval_time': 7484.907016038895, 'accumulated_logging_time': 2.5809249877929688}
I0205 19:56:51.959618 139735693903616 logging_writer.py:48] [50136] accumulated_eval_time=7484.907016, accumulated_logging_time=2.580925, accumulated_submission_time=16099.191839, global_step=50136, preemption_count=0, score=16099.191839, test/accuracy=0.985755, test/loss=0.051650, test/mean_average_precision=0.257441, test/num_examples=43793, total_duration=23588.102047, train/accuracy=0.993636, train/loss=0.019929, train/mean_average_precision=0.634787, validation/accuracy=0.986668, validation/loss=0.048448, validation/mean_average_precision=0.280415, validation/num_examples=43793
I0205 19:57:12.713924 139752296675072 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.16545648872852325, loss=0.02314528450369835
I0205 19:57:44.239466 139735693903616 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.168766051530838, loss=0.021054573357105255
I0205 19:58:15.809394 139752296675072 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.1847664713859558, loss=0.022636037319898605
I0205 19:58:47.480309 139735693903616 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.17035971581935883, loss=0.021242260932922363
I0205 19:59:19.338029 139752296675072 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.15501278638839722, loss=0.018074434250593185
I0205 19:59:50.960335 139735693903616 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.18610809743404388, loss=0.021442454308271408
I0205 20:00:22.766082 139752296675072 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.18194974958896637, loss=0.024085216224193573
I0205 20:00:52.200024 139919816816448 spec.py:321] Evaluating on the training split.
I0205 20:02:30.795451 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 20:02:34.161024 139919816816448 spec.py:349] Evaluating on the test split.
I0205 20:02:37.412619 139919816816448 submission_runner.py:408] Time since start: 23933.58s, 	Step: 50894, 	{'train/accuracy': 0.9934646487236023, 'train/loss': 0.020531222224235535, 'train/mean_average_precision': 0.6163652200340647, 'validation/accuracy': 0.9863924384117126, 'validation/loss': 0.0486702099442482, 'validation/mean_average_precision': 0.2739500108171812, 'validation/num_examples': 43793, 'test/accuracy': 0.9856056571006775, 'test/loss': 0.051769357174634933, 'test/mean_average_precision': 0.2514201600194406, 'test/num_examples': 43793, 'score': 16339.400242805481, 'total_duration': 23933.579761743546, 'accumulated_submission_time': 16339.400242805481, 'accumulated_eval_time': 7590.119548559189, 'accumulated_logging_time': 2.616666078567505}
I0205 20:02:37.451301 139759013103360 logging_writer.py:48] [50894] accumulated_eval_time=7590.119549, accumulated_logging_time=2.616666, accumulated_submission_time=16339.400243, global_step=50894, preemption_count=0, score=16339.400243, test/accuracy=0.985606, test/loss=0.051769, test/mean_average_precision=0.251420, test/num_examples=43793, total_duration=23933.579762, train/accuracy=0.993465, train/loss=0.020531, train/mean_average_precision=0.616365, validation/accuracy=0.986392, validation/loss=0.048670, validation/mean_average_precision=0.273950, validation/num_examples=43793
I0205 20:02:39.760231 139759038281472 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.1767931878566742, loss=0.020952120423316956
I0205 20:03:12.276579 139759013103360 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.15860261023044586, loss=0.020174922421574593
I0205 20:03:44.579658 139759038281472 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.15619102120399475, loss=0.021300967782735825
I0205 20:04:16.913500 139759013103360 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.16661356389522552, loss=0.021327855065464973
I0205 20:04:48.915183 139759038281472 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.18867245316505432, loss=0.022819358855485916
I0205 20:05:21.190107 139759013103360 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.19064252078533173, loss=0.022057872265577316
I0205 20:05:53.406314 139759038281472 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.1908203512430191, loss=0.021741105243563652
I0205 20:06:25.513520 139759013103360 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.18942993879318237, loss=0.018194837495684624
I0205 20:06:37.481395 139919816816448 spec.py:321] Evaluating on the training split.
I0205 20:08:15.980945 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 20:08:19.093707 139919816816448 spec.py:349] Evaluating on the test split.
I0205 20:08:22.152751 139919816816448 submission_runner.py:408] Time since start: 24278.32s, 	Step: 51638, 	{'train/accuracy': 0.9933746457099915, 'train/loss': 0.02076597511768341, 'train/mean_average_precision': 0.6203458093296251, 'validation/accuracy': 0.9864504933357239, 'validation/loss': 0.04906805604696274, 'validation/mean_average_precision': 0.272940814365671, 'validation/num_examples': 43793, 'test/accuracy': 0.9855926036834717, 'test/loss': 0.05210414156317711, 'test/mean_average_precision': 0.2548566982480128, 'test/num_examples': 43793, 'score': 16579.39709186554, 'total_duration': 24278.319911956787, 'accumulated_submission_time': 16579.39709186554, 'accumulated_eval_time': 7694.790858745575, 'accumulated_logging_time': 2.667487859725952}
I0205 20:08:22.177020 139735693903616 logging_writer.py:48] [51638] accumulated_eval_time=7694.790859, accumulated_logging_time=2.667488, accumulated_submission_time=16579.397092, global_step=51638, preemption_count=0, score=16579.397092, test/accuracy=0.985593, test/loss=0.052104, test/mean_average_precision=0.254857, test/num_examples=43793, total_duration=24278.319912, train/accuracy=0.993375, train/loss=0.020766, train/mean_average_precision=0.620346, validation/accuracy=0.986450, validation/loss=0.049068, validation/mean_average_precision=0.272941, validation/num_examples=43793
I0205 20:08:42.288573 139739177916160 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.22140048444271088, loss=0.02186906896531582
I0205 20:09:14.145518 139735693903616 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.21681132912635803, loss=0.02350684069097042
I0205 20:09:45.844507 139739177916160 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.16941149532794952, loss=0.022207563742995262
I0205 20:10:17.605690 139735693903616 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.19532208144664764, loss=0.02122531644999981
I0205 20:10:49.206399 139739177916160 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.1743064820766449, loss=0.022288061678409576
I0205 20:11:20.991511 139735693903616 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.1567257195711136, loss=0.019891412928700447
I0205 20:11:52.883368 139739177916160 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.16457240283489227, loss=0.019054125994443893
I0205 20:12:22.423789 139919816816448 spec.py:321] Evaluating on the training split.
I0205 20:14:00.860908 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 20:14:03.961810 139919816816448 spec.py:349] Evaluating on the test split.
I0205 20:14:06.934856 139919816816448 submission_runner.py:408] Time since start: 24623.10s, 	Step: 52394, 	{'train/accuracy': 0.9932540059089661, 'train/loss': 0.02079734019935131, 'train/mean_average_precision': 0.6166767403336669, 'validation/accuracy': 0.9865624904632568, 'validation/loss': 0.049784209579229355, 'validation/mean_average_precision': 0.26973126714833323, 'validation/num_examples': 43793, 'test/accuracy': 0.9857804179191589, 'test/loss': 0.05286939814686775, 'test/mean_average_precision': 0.2487877520350712, 'test/num_examples': 43793, 'score': 16819.611674308777, 'total_duration': 24623.10201358795, 'accumulated_submission_time': 16819.611674308777, 'accumulated_eval_time': 7799.301882743835, 'accumulated_logging_time': 2.7027688026428223}
I0205 20:14:06.960040 139752296675072 logging_writer.py:48] [52394] accumulated_eval_time=7799.301883, accumulated_logging_time=2.702769, accumulated_submission_time=16819.611674, global_step=52394, preemption_count=0, score=16819.611674, test/accuracy=0.985780, test/loss=0.052869, test/mean_average_precision=0.248788, test/num_examples=43793, total_duration=24623.102014, train/accuracy=0.993254, train/loss=0.020797, train/mean_average_precision=0.616677, validation/accuracy=0.986562, validation/loss=0.049784, validation/mean_average_precision=0.269731, validation/num_examples=43793
I0205 20:14:09.353021 139759013103360 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.16623455286026, loss=0.019759315997362137
I0205 20:14:41.661218 139752296675072 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.19373655319213867, loss=0.01867208629846573
I0205 20:15:13.601022 139759013103360 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.25717759132385254, loss=0.023058587685227394
I0205 20:15:45.285605 139752296675072 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.22054193913936615, loss=0.02073824033141136
I0205 20:16:16.930111 139759013103360 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.18507161736488342, loss=0.02014101669192314
I0205 20:16:48.530007 139752296675072 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.18173837661743164, loss=0.020807946100831032
I0205 20:17:20.289788 139759013103360 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.19173714518547058, loss=0.020275898277759552
I0205 20:17:51.869579 139752296675072 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.19530880451202393, loss=0.018590468913316727
I0205 20:18:07.134031 139919816816448 spec.py:321] Evaluating on the training split.
I0205 20:19:41.643110 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 20:19:44.728451 139919816816448 spec.py:349] Evaluating on the test split.
I0205 20:19:47.634093 139919816816448 submission_runner.py:408] Time since start: 24963.80s, 	Step: 53149, 	{'train/accuracy': 0.993232250213623, 'train/loss': 0.02114035189151764, 'train/mean_average_precision': 0.600300756020649, 'validation/accuracy': 0.9863465428352356, 'validation/loss': 0.050232309848070145, 'validation/mean_average_precision': 0.26931284449460385, 'validation/num_examples': 43793, 'test/accuracy': 0.9855176210403442, 'test/loss': 0.05326259136199951, 'test/mean_average_precision': 0.2513808514617977, 'test/num_examples': 43793, 'score': 17059.75382900238, 'total_duration': 24963.80125451088, 'accumulated_submission_time': 17059.75382900238, 'accumulated_eval_time': 7899.80190038681, 'accumulated_logging_time': 2.738955497741699}
I0205 20:19:47.659059 139735693903616 logging_writer.py:48] [53149] accumulated_eval_time=7899.801900, accumulated_logging_time=2.738955, accumulated_submission_time=17059.753829, global_step=53149, preemption_count=0, score=17059.753829, test/accuracy=0.985518, test/loss=0.053263, test/mean_average_precision=0.251381, test/num_examples=43793, total_duration=24963.801255, train/accuracy=0.993232, train/loss=0.021140, train/mean_average_precision=0.600301, validation/accuracy=0.986347, validation/loss=0.050232, validation/mean_average_precision=0.269313, validation/num_examples=43793
I0205 20:20:04.093958 139759038281472 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.19296325743198395, loss=0.02104000933468342
I0205 20:20:35.678970 139735693903616 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.2250683754682541, loss=0.019369050860404968
I0205 20:21:07.193045 139759038281472 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.1828574240207672, loss=0.016280241310596466
I0205 20:21:39.178425 139735693903616 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.20320580899715424, loss=0.02230468951165676
I0205 20:22:11.321966 139759038281472 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.17496317625045776, loss=0.02000890113413334
I0205 20:22:43.088338 139735693903616 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.19630266726016998, loss=0.01985578052699566
I0205 20:23:14.895309 139759038281472 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.17488542199134827, loss=0.019771531224250793
I0205 20:23:46.625697 139735693903616 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.16675271093845367, loss=0.019187841564416885
I0205 20:23:47.947416 139919816816448 spec.py:321] Evaluating on the training split.
I0205 20:25:33.698285 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 20:25:37.173861 139919816816448 spec.py:349] Evaluating on the test split.
I0205 20:25:40.552130 139919816816448 submission_runner.py:408] Time since start: 25316.72s, 	Step: 53905, 	{'train/accuracy': 0.9932581186294556, 'train/loss': 0.020752474665641785, 'train/mean_average_precision': 0.6190629736359106, 'validation/accuracy': 0.9863465428352356, 'validation/loss': 0.050454575568437576, 'validation/mean_average_precision': 0.2680434163900203, 'validation/num_examples': 43793, 'test/accuracy': 0.98549485206604, 'test/loss': 0.053566861897706985, 'test/mean_average_precision': 0.2530998305906409, 'test/num_examples': 43793, 'score': 17300.010417699814, 'total_duration': 25316.719260931015, 'accumulated_submission_time': 17300.010417699814, 'accumulated_eval_time': 8012.406537294388, 'accumulated_logging_time': 2.774937868118286}
I0205 20:25:40.580787 139752296675072 logging_writer.py:48] [53905] accumulated_eval_time=8012.406537, accumulated_logging_time=2.774938, accumulated_submission_time=17300.010418, global_step=53905, preemption_count=0, score=17300.010418, test/accuracy=0.985495, test/loss=0.053567, test/mean_average_precision=0.253100, test/num_examples=43793, total_duration=25316.719261, train/accuracy=0.993258, train/loss=0.020752, train/mean_average_precision=0.619063, validation/accuracy=0.986347, validation/loss=0.050455, validation/mean_average_precision=0.268043, validation/num_examples=43793
I0205 20:26:12.301839 139759013103360 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.22540804743766785, loss=0.01988571509718895
I0205 20:26:44.677384 139752296675072 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.19508568942546844, loss=0.018407057970762253
I0205 20:27:17.088238 139759013103360 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.19857215881347656, loss=0.019196510314941406
I0205 20:27:49.166258 139752296675072 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.19342945516109467, loss=0.019013430923223495
I0205 20:28:21.576700 139759013103360 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.17909738421440125, loss=0.020213648676872253
I0205 20:28:53.628476 139752296675072 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.1741938441991806, loss=0.01925111748278141
I0205 20:29:25.808771 139759013103360 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.23078012466430664, loss=0.02066119760274887
I0205 20:29:40.865678 139919816816448 spec.py:321] Evaluating on the training split.
I0205 20:31:18.560974 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 20:31:21.612507 139919816816448 spec.py:349] Evaluating on the test split.
I0205 20:31:24.638927 139919816816448 submission_runner.py:408] Time since start: 25660.81s, 	Step: 54648, 	{'train/accuracy': 0.9935899972915649, 'train/loss': 0.019713984802365303, 'train/mean_average_precision': 0.6550292985915764, 'validation/accuracy': 0.9860900044441223, 'validation/loss': 0.051197599619627, 'validation/mean_average_precision': 0.2668912946641175, 'validation/num_examples': 43793, 'test/accuracy': 0.9853617548942566, 'test/loss': 0.05401235073804855, 'test/mean_average_precision': 0.24674567276445694, 'test/num_examples': 43793, 'score': 17540.260680675507, 'total_duration': 25660.806088209152, 'accumulated_submission_time': 17540.260680675507, 'accumulated_eval_time': 8116.179745674133, 'accumulated_logging_time': 2.8153786659240723}
I0205 20:31:24.664440 139735693903616 logging_writer.py:48] [54648] accumulated_eval_time=8116.179746, accumulated_logging_time=2.815379, accumulated_submission_time=17540.260681, global_step=54648, preemption_count=0, score=17540.260681, test/accuracy=0.985362, test/loss=0.054012, test/mean_average_precision=0.246746, test/num_examples=43793, total_duration=25660.806088, train/accuracy=0.993590, train/loss=0.019714, train/mean_average_precision=0.655029, validation/accuracy=0.986090, validation/loss=0.051198, validation/mean_average_precision=0.266891, validation/num_examples=43793
I0205 20:31:41.764345 139739177916160 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.1906539797782898, loss=0.020983507856726646
I0205 20:32:13.975189 139735693903616 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.2100483775138855, loss=0.020767997950315475
I0205 20:32:46.320499 139739177916160 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.18267713487148285, loss=0.0181284099817276
I0205 20:33:18.251204 139735693903616 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.17700378596782684, loss=0.019304810091853142
I0205 20:33:50.623452 139739177916160 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.18568448722362518, loss=0.01912742480635643
I0205 20:34:22.208915 139735693903616 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.2334272563457489, loss=0.020237188786268234
I0205 20:34:53.895659 139739177916160 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.19557097554206848, loss=0.018502749502658844
I0205 20:35:24.913165 139919816816448 spec.py:321] Evaluating on the training split.
I0205 20:37:03.834437 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 20:37:06.889596 139919816816448 spec.py:349] Evaluating on the test split.
I0205 20:37:09.870494 139919816816448 submission_runner.py:408] Time since start: 26006.04s, 	Step: 55397, 	{'train/accuracy': 0.9940750002861023, 'train/loss': 0.01836450770497322, 'train/mean_average_precision': 0.6688367949780897, 'validation/accuracy': 0.9861955642700195, 'validation/loss': 0.050676897168159485, 'validation/mean_average_precision': 0.272017019263525, 'validation/num_examples': 43793, 'test/accuracy': 0.9854013323783875, 'test/loss': 0.053795311599969864, 'test/mean_average_precision': 0.2553403779232295, 'test/num_examples': 43793, 'score': 17780.475964784622, 'total_duration': 26006.03764986992, 'accumulated_submission_time': 17780.475964784622, 'accumulated_eval_time': 8221.137031793594, 'accumulated_logging_time': 2.852015495300293}
I0205 20:37:09.896658 139752296675072 logging_writer.py:48] [55397] accumulated_eval_time=8221.137032, accumulated_logging_time=2.852015, accumulated_submission_time=17780.475965, global_step=55397, preemption_count=0, score=17780.475965, test/accuracy=0.985401, test/loss=0.053795, test/mean_average_precision=0.255340, test/num_examples=43793, total_duration=26006.037650, train/accuracy=0.994075, train/loss=0.018365, train/mean_average_precision=0.668837, validation/accuracy=0.986196, validation/loss=0.050677, validation/mean_average_precision=0.272017, validation/num_examples=43793
I0205 20:37:11.201313 139759038281472 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.21522335708141327, loss=0.017152834683656693
I0205 20:37:43.269785 139752296675072 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.24402771890163422, loss=0.019445743411779404
I0205 20:38:15.234408 139759038281472 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.2348039150238037, loss=0.018161818385124207
I0205 20:38:46.975163 139752296675072 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.19726069271564484, loss=0.0196034237742424
I0205 20:39:18.937280 139759038281472 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.2459406554698944, loss=0.01912282221019268
I0205 20:39:50.530340 139752296675072 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.21290113031864166, loss=0.01686236448585987
I0205 20:40:22.237246 139759038281472 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.1970006227493286, loss=0.017722230404615402
I0205 20:40:54.301218 139752296675072 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.1903822124004364, loss=0.015598496422171593
I0205 20:41:09.994129 139919816816448 spec.py:321] Evaluating on the training split.
I0205 20:42:52.195312 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 20:42:55.629494 139919816816448 spec.py:349] Evaluating on the test split.
I0205 20:42:59.062302 139919816816448 submission_runner.py:408] Time since start: 26355.23s, 	Step: 56149, 	{'train/accuracy': 0.994035005569458, 'train/loss': 0.018236299976706505, 'train/mean_average_precision': 0.6705837973689833, 'validation/accuracy': 0.986273467540741, 'validation/loss': 0.05187169834971428, 'validation/mean_average_precision': 0.26658966401857015, 'validation/num_examples': 43793, 'test/accuracy': 0.9854514598846436, 'test/loss': 0.05483526363968849, 'test/mean_average_precision': 0.2518330540823449, 'test/num_examples': 43793, 'score': 18020.540464401245, 'total_duration': 26355.229289531708, 'accumulated_submission_time': 18020.540464401245, 'accumulated_eval_time': 8330.204983472824, 'accumulated_logging_time': 2.890192985534668}
I0205 20:42:59.092388 139735693903616 logging_writer.py:48] [56149] accumulated_eval_time=8330.204983, accumulated_logging_time=2.890193, accumulated_submission_time=18020.540464, global_step=56149, preemption_count=0, score=18020.540464, test/accuracy=0.985451, test/loss=0.054835, test/mean_average_precision=0.251833, test/num_examples=43793, total_duration=26355.229290, train/accuracy=0.994035, train/loss=0.018236, train/mean_average_precision=0.670584, validation/accuracy=0.986273, validation/loss=0.051872, validation/mean_average_precision=0.266590, validation/num_examples=43793
I0205 20:43:15.848093 139759013103360 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.20463839173316956, loss=0.01891675777733326
I0205 20:43:47.609405 139735693903616 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.2075774222612381, loss=0.017367001622915268
I0205 20:44:19.012799 139759013103360 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.1972264051437378, loss=0.014858108013868332
I0205 20:44:50.608351 139735693903616 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.1940188705921173, loss=0.01711900904774666
I0205 20:45:22.120089 139759013103360 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.21020586788654327, loss=0.016063189134001732
I0205 20:45:53.613701 139735693903616 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.20447708666324615, loss=0.01745276339352131
I0205 20:46:25.171085 139759013103360 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.20590627193450928, loss=0.019860943779349327
I0205 20:46:56.551135 139735693903616 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.202631875872612, loss=0.018518293276429176
I0205 20:46:59.064318 139919816816448 spec.py:321] Evaluating on the training split.
I0205 20:48:41.242638 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 20:48:44.293673 139919816816448 spec.py:349] Evaluating on the test split.
I0205 20:48:47.272015 139919816816448 submission_runner.py:408] Time since start: 26703.44s, 	Step: 56909, 	{'train/accuracy': 0.9949232339859009, 'train/loss': 0.015860287472605705, 'train/mean_average_precision': 0.7254194245765323, 'validation/accuracy': 0.9862621426582336, 'validation/loss': 0.05209961161017418, 'validation/mean_average_precision': 0.2709062672047048, 'validation/num_examples': 43793, 'test/accuracy': 0.9854662418365479, 'test/loss': 0.0550340972840786, 'test/mean_average_precision': 0.2515098676230641, 'test/num_examples': 43793, 'score': 18260.47955775261, 'total_duration': 26703.43917274475, 'accumulated_submission_time': 18260.47955775261, 'accumulated_eval_time': 8438.41262793541, 'accumulated_logging_time': 2.932584762573242}
I0205 20:48:47.298789 139739177916160 logging_writer.py:48] [56909] accumulated_eval_time=8438.412628, accumulated_logging_time=2.932585, accumulated_submission_time=18260.479558, global_step=56909, preemption_count=0, score=18260.479558, test/accuracy=0.985466, test/loss=0.055034, test/mean_average_precision=0.251510, test/num_examples=43793, total_duration=26703.439173, train/accuracy=0.994923, train/loss=0.015860, train/mean_average_precision=0.725419, validation/accuracy=0.986262, validation/loss=0.052100, validation/mean_average_precision=0.270906, validation/num_examples=43793
I0205 20:49:17.135350 139752296675072 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.21331480145454407, loss=0.017821745947003365
I0205 20:49:48.495123 139739177916160 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.22909952700138092, loss=0.019230879843235016
I0205 20:50:20.083425 139752296675072 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.2495025247335434, loss=0.01836855709552765
I0205 20:50:51.329437 139739177916160 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.23549588024616241, loss=0.018308104947209358
I0205 20:51:22.745370 139752296675072 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.19914427399635315, loss=0.016752032563090324
I0205 20:51:54.278882 139739177916160 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.23606470227241516, loss=0.01688183657824993
I0205 20:52:24.132582 139752296675072 logging_writer.py:48] [57596] global_step=57596, preemption_count=0, score=18477.267732
I0205 20:52:24.186427 139919816816448 checkpoints.py:490] Saving checkpoint at step: 57596
I0205 20:52:24.297891 139919816816448 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_1/ogbg_jax/trial_3/checkpoint_57596
I0205 20:52:24.298976 139919816816448 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_1/ogbg_jax/trial_3/checkpoint_57596.
I0205 20:52:24.489992 139919816816448 submission_runner.py:583] Tuning trial 3/5
I0205 20:52:24.490243 139919816816448 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.001308209823469072, one_minus_beta1=0.02686663061, beta2=0.9981232922116359, weight_decay=0.16375311233774334, warmup_factor=0.1)
I0205 20:52:24.494946 139919816816448 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.5324387550354004, 'train/loss': 0.7276949882507324, 'train/mean_average_precision': 0.02166684367684784, 'validation/accuracy': 0.5230699777603149, 'validation/loss': 0.7331878542900085, 'validation/mean_average_precision': 0.02556818984903369, 'validation/num_examples': 43793, 'test/accuracy': 0.5214916467666626, 'test/loss': 0.7347381114959717, 'test/mean_average_precision': 0.026822867578732854, 'test/num_examples': 43793, 'score': 11.491451501846313, 'total_duration': 125.73687386512756, 'accumulated_submission_time': 11.491451501846313, 'accumulated_eval_time': 114.24537301063538, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (745, {'train/accuracy': 0.9867120385169983, 'train/loss': 0.0699601098895073, 'train/mean_average_precision': 0.03136492398996348, 'validation/accuracy': 0.9841179251670837, 'validation/loss': 0.07792092114686966, 'validation/mean_average_precision': 0.03357064180639783, 'validation/num_examples': 43793, 'test/accuracy': 0.983142077922821, 'test/loss': 0.08070765435695648, 'test/mean_average_precision': 0.03482637536778557, 'test/num_examples': 43793, 'score': 251.67628693580627, 'total_duration': 482.73058128356934, 'accumulated_submission_time': 251.67628693580627, 'accumulated_eval_time': 231.01239943504333, 'accumulated_logging_time': 0.02175736427307129, 'global_step': 745, 'preemption_count': 0}), (1483, {'train/accuracy': 0.9868988394737244, 'train/loss': 0.05043470859527588, 'train/mean_average_precision': 0.07136894122746727, 'validation/accuracy': 0.9844674468040466, 'validation/loss': 0.05963057279586792, 'validation/mean_average_precision': 0.06384708825885257, 'validation/num_examples': 43793, 'test/accuracy': 0.9834638833999634, 'test/loss': 0.06271059066057205, 'test/mean_average_precision': 0.06910000269425054, 'test/num_examples': 43793, 'score': 491.8714425563812, 'total_duration': 839.7973530292511, 'accumulated_submission_time': 491.8714425563812, 'accumulated_eval_time': 347.83271861076355, 'accumulated_logging_time': 0.050313711166381836, 'global_step': 1483, 'preemption_count': 0}), (2227, {'train/accuracy': 0.9876678586006165, 'train/loss': 0.04488957300782204, 'train/mean_average_precision': 0.12329825632978675, 'validation/accuracy': 0.9848230481147766, 'validation/loss': 0.05412972345948219, 'validation/mean_average_precision': 0.11911754906630341, 'validation/num_examples': 43793, 'test/accuracy': 0.9839284420013428, 'test/loss': 0.05710911378264427, 'test/mean_average_precision': 0.12057239133619276, 'test/num_examples': 43793, 'score': 732.0550570487976, 'total_duration': 1197.2648441791534, 'accumulated_submission_time': 732.0550570487976, 'accumulated_eval_time': 465.06700444221497, 'accumulated_logging_time': 0.07920026779174805, 'global_step': 2227, 'preemption_count': 0}), (2976, {'train/accuracy': 0.9881260991096497, 'train/loss': 0.04235072061419487, 'train/mean_average_precision': 0.15038947347985554, 'validation/accuracy': 0.9851656556129456, 'validation/loss': 0.051759861409664154, 'validation/mean_average_precision': 0.1425854816835852, 'validation/num_examples': 43793, 'test/accuracy': 0.9842203259468079, 'test/loss': 0.054486583918333054, 'test/mean_average_precision': 0.14665337020102068, 'test/num_examples': 43793, 'score': 972.2738552093506, 'total_duration': 1552.2670772075653, 'accumulated_submission_time': 972.2738552093506, 'accumulated_eval_time': 579.8030261993408, 'accumulated_logging_time': 0.10624456405639648, 'global_step': 2976, 'preemption_count': 0}), (3718, {'train/accuracy': 0.9882087111473083, 'train/loss': 0.0411868691444397, 'train/mean_average_precision': 0.17895131295690636, 'validation/accuracy': 0.9852659106254578, 'validation/loss': 0.05065610259771347, 'validation/mean_average_precision': 0.17215355653285436, 'validation/num_examples': 43793, 'test/accuracy': 0.9843053817749023, 'test/loss': 0.05346042662858963, 'test/mean_average_precision': 0.1665679376148152, 'test/num_examples': 43793, 'score': 1212.473397731781, 'total_duration': 1914.3342943191528, 'accumulated_submission_time': 1212.473397731781, 'accumulated_eval_time': 701.6240711212158, 'accumulated_logging_time': 0.13290810585021973, 'global_step': 3718, 'preemption_count': 0}), (4461, {'train/accuracy': 0.988564133644104, 'train/loss': 0.039611171931028366, 'train/mean_average_precision': 0.20526388658012243, 'validation/accuracy': 0.9856268167495728, 'validation/loss': 0.04916029050946236, 'validation/mean_average_precision': 0.18699333950853866, 'validation/num_examples': 43793, 'test/accuracy': 0.9846844673156738, 'test/loss': 0.051958292722702026, 'test/mean_average_precision': 0.18455530274110227, 'test/num_examples': 43793, 'score': 1452.6450970172882, 'total_duration': 2269.441216945648, 'accumulated_submission_time': 1452.6450970172882, 'accumulated_eval_time': 816.5106444358826, 'accumulated_logging_time': 0.1609640121459961, 'global_step': 4461, 'preemption_count': 0}), (5203, {'train/accuracy': 0.9885818362236023, 'train/loss': 0.03906910866498947, 'train/mean_average_precision': 0.21580086321456005, 'validation/accuracy': 0.9856730699539185, 'validation/loss': 0.04913453385233879, 'validation/mean_average_precision': 0.18620651336788618, 'validation/num_examples': 43793, 'test/accuracy': 0.984832763671875, 'test/loss': 0.05178947001695633, 'test/mean_average_precision': 0.18533525819395427, 'test/num_examples': 43793, 'score': 1692.7988908290863, 'total_duration': 2623.5991291999817, 'accumulated_submission_time': 1692.7988908290863, 'accumulated_eval_time': 930.4642643928528, 'accumulated_logging_time': 0.19129252433776855, 'global_step': 5203, 'preemption_count': 0}), (5946, {'train/accuracy': 0.9889664649963379, 'train/loss': 0.0375383198261261, 'train/mean_average_precision': 0.2455382972249783, 'validation/accuracy': 0.9859256148338318, 'validation/loss': 0.047312334179878235, 'validation/mean_average_precision': 0.21032265999022526, 'validation/num_examples': 43793, 'test/accuracy': 0.9849889874458313, 'test/loss': 0.05008397996425629, 'test/mean_average_precision': 0.19994347087606382, 'test/num_examples': 43793, 'score': 1932.7561764717102, 'total_duration': 2972.670731782913, 'accumulated_submission_time': 1932.7561764717102, 'accumulated_eval_time': 1039.5315363407135, 'accumulated_logging_time': 0.2181088924407959, 'global_step': 5946, 'preemption_count': 0}), (6683, {'train/accuracy': 0.989159107208252, 'train/loss': 0.03690975904464722, 'train/mean_average_precision': 0.25773455228098463, 'validation/accuracy': 0.9858171939849854, 'validation/loss': 0.04761821776628494, 'validation/mean_average_precision': 0.2067422098356613, 'validation/num_examples': 43793, 'test/accuracy': 0.9849334359169006, 'test/loss': 0.050239451229572296, 'test/mean_average_precision': 0.2088082447016146, 'test/num_examples': 43793, 'score': 2172.862287044525, 'total_duration': 3324.5116848945618, 'accumulated_submission_time': 2172.862287044525, 'accumulated_eval_time': 1151.2113268375397, 'accumulated_logging_time': 0.250399112701416, 'global_step': 6683, 'preemption_count': 0}), (7430, {'train/accuracy': 0.9891023635864258, 'train/loss': 0.036981549113988876, 'train/mean_average_precision': 0.2569534259312815, 'validation/accuracy': 0.9860132932662964, 'validation/loss': 0.047048043459653854, 'validation/mean_average_precision': 0.21624403118395114, 'validation/num_examples': 43793, 'test/accuracy': 0.9851793646812439, 'test/loss': 0.04973500967025757, 'test/mean_average_precision': 0.21493407644466025, 'test/num_examples': 43793, 'score': 2413.044668197632, 'total_duration': 3680.9460167884827, 'accumulated_submission_time': 2413.044668197632, 'accumulated_eval_time': 1267.4153888225555, 'accumulated_logging_time': 0.27773523330688477, 'global_step': 7430, 'preemption_count': 0}), (8166, {'train/accuracy': 0.9891697764396667, 'train/loss': 0.03671594336628914, 'train/mean_average_precision': 0.25753546882741873, 'validation/accuracy': 0.9860778450965881, 'validation/loss': 0.047110870480537415, 'validation/mean_average_precision': 0.21821118509284348, 'validation/num_examples': 43793, 'test/accuracy': 0.9851751923561096, 'test/loss': 0.05003559589385986, 'test/mean_average_precision': 0.21506364420129348, 'test/num_examples': 43793, 'score': 2653.0890650749207, 'total_duration': 4031.506335258484, 'accumulated_submission_time': 2653.0890650749207, 'accumulated_eval_time': 1377.8806405067444, 'accumulated_logging_time': 0.30559682846069336, 'global_step': 8166, 'preemption_count': 0}), (8914, {'train/accuracy': 0.9893730878829956, 'train/loss': 0.035877127200365067, 'train/mean_average_precision': 0.2799335236621727, 'validation/accuracy': 0.9860900044441223, 'validation/loss': 0.04634569585323334, 'validation/mean_average_precision': 0.2231633377875584, 'validation/num_examples': 43793, 'test/accuracy': 0.9852905869483948, 'test/loss': 0.04900343716144562, 'test/mean_average_precision': 0.22060165025319775, 'test/num_examples': 43793, 'score': 2893.1614439487457, 'total_duration': 4383.561316490173, 'accumulated_submission_time': 2893.1614439487457, 'accumulated_eval_time': 1489.8143291473389, 'accumulated_logging_time': 0.3341653347015381, 'global_step': 8914, 'preemption_count': 0}), (9658, {'train/accuracy': 0.9894848465919495, 'train/loss': 0.035306576639413834, 'train/mean_average_precision': 0.29507402958103773, 'validation/accuracy': 0.9863181114196777, 'validation/loss': 0.04575859755277634, 'validation/mean_average_precision': 0.23779549555324025, 'validation/num_examples': 43793, 'test/accuracy': 0.9855382442474365, 'test/loss': 0.04845280200242996, 'test/mean_average_precision': 0.23480798784396492, 'test/num_examples': 43793, 'score': 3133.3014283180237, 'total_duration': 4736.115564584732, 'accumulated_submission_time': 3133.3014283180237, 'accumulated_eval_time': 1602.1785380840302, 'accumulated_logging_time': 0.36263275146484375, 'global_step': 9658, 'preemption_count': 0}), (10406, {'train/accuracy': 0.9897177815437317, 'train/loss': 0.0344165675342083, 'train/mean_average_precision': 0.31707895892708327, 'validation/accuracy': 0.9863526225090027, 'validation/loss': 0.045562926679849625, 'validation/mean_average_precision': 0.23729907976726822, 'validation/num_examples': 43793, 'test/accuracy': 0.9855803847312927, 'test/loss': 0.0481531098484993, 'test/mean_average_precision': 0.23562830229534015, 'test/num_examples': 43793, 'score': 3373.3994760513306, 'total_duration': 5085.509356498718, 'accumulated_submission_time': 3373.3994760513306, 'accumulated_eval_time': 1711.423900127411, 'accumulated_logging_time': 0.3928844928741455, 'global_step': 10406, 'preemption_count': 0}), (11155, {'train/accuracy': 0.989963173866272, 'train/loss': 0.03346941992640495, 'train/mean_average_precision': 0.32815418763912285, 'validation/accuracy': 0.9864525198936462, 'validation/loss': 0.04531967267394066, 'validation/mean_average_precision': 0.24260675495018194, 'validation/num_examples': 43793, 'test/accuracy': 0.985577404499054, 'test/loss': 0.04799189418554306, 'test/mean_average_precision': 0.23709998748098676, 'test/num_examples': 43793, 'score': 3613.4454939365387, 'total_duration': 5432.026793479919, 'accumulated_submission_time': 3613.4454939365387, 'accumulated_eval_time': 1817.8432455062866, 'accumulated_logging_time': 0.4235210418701172, 'global_step': 11155, 'preemption_count': 0}), (11898, {'train/accuracy': 0.9901131391525269, 'train/loss': 0.03306407853960991, 'train/mean_average_precision': 0.3472507179399345, 'validation/accuracy': 0.9863274693489075, 'validation/loss': 0.04504416137933731, 'validation/mean_average_precision': 0.2529347650672703, 'validation/num_examples': 43793, 'test/accuracy': 0.9856052398681641, 'test/loss': 0.04766784980893135, 'test/mean_average_precision': 0.24585168700097593, 'test/num_examples': 43793, 'score': 3853.49520945549, 'total_duration': 5783.7891409397125, 'accumulated_submission_time': 3853.49520945549, 'accumulated_eval_time': 1929.5064034461975, 'accumulated_logging_time': 0.4515836238861084, 'global_step': 11898, 'preemption_count': 0}), (12641, {'train/accuracy': 0.9902748465538025, 'train/loss': 0.03211957961320877, 'train/mean_average_precision': 0.36782349838734035, 'validation/accuracy': 0.9866047501564026, 'validation/loss': 0.044995300471782684, 'validation/mean_average_precision': 0.251586093170994, 'validation/num_examples': 43793, 'test/accuracy': 0.9857774972915649, 'test/loss': 0.04777630791068077, 'test/mean_average_precision': 0.24437577786005055, 'test/num_examples': 43793, 'score': 4093.3299930095673, 'total_duration': 6136.139136552811, 'accumulated_submission_time': 4093.3299930095673, 'accumulated_eval_time': 2041.57985496521, 'accumulated_logging_time': 0.8727149963378906, 'global_step': 12641, 'preemption_count': 0}), (13388, {'train/accuracy': 0.990310549736023, 'train/loss': 0.03186388686299324, 'train/mean_average_precision': 0.36338862405673744, 'validation/accuracy': 0.9865308403968811, 'validation/loss': 0.04500997066497803, 'validation/mean_average_precision': 0.2489073772637887, 'validation/num_examples': 43793, 'test/accuracy': 0.9857479929924011, 'test/loss': 0.047876618802547455, 'test/mean_average_precision': 0.2479855948983023, 'test/num_examples': 43793, 'score': 4333.489721059799, 'total_duration': 6484.4705328941345, 'accumulated_submission_time': 4333.489721059799, 'accumulated_eval_time': 2149.701101541519, 'accumulated_logging_time': 0.9023532867431641, 'global_step': 13388, 'preemption_count': 0}), (14120, {'train/accuracy': 0.9904751777648926, 'train/loss': 0.031606610864400864, 'train/mean_average_precision': 0.36739894883526464, 'validation/accuracy': 0.9864829182624817, 'validation/loss': 0.044791556894779205, 'validation/mean_average_precision': 0.2517094654821753, 'validation/num_examples': 43793, 'test/accuracy': 0.9856890439987183, 'test/loss': 0.04744245111942291, 'test/mean_average_precision': 0.24579146920607292, 'test/num_examples': 43793, 'score': 4573.749706029892, 'total_duration': 6834.34108376503, 'accumulated_submission_time': 4573.749706029892, 'accumulated_eval_time': 2259.2570674419403, 'accumulated_logging_time': 0.931546688079834, 'global_step': 14120, 'preemption_count': 0}), (14870, {'train/accuracy': 0.9906328320503235, 'train/loss': 0.03126322478055954, 'train/mean_average_precision': 0.3707889618155518, 'validation/accuracy': 0.9866303205490112, 'validation/loss': 0.044579531997442245, 'validation/mean_average_precision': 0.2557782403995275, 'validation/num_examples': 43793, 'test/accuracy': 0.9857884645462036, 'test/loss': 0.047305814921855927, 'test/mean_average_precision': 0.2542209605860204, 'test/num_examples': 43793, 'score': 4813.8011820316315, 'total_duration': 7182.887937784195, 'accumulated_submission_time': 4813.8011820316315, 'accumulated_eval_time': 2367.7003812789917, 'accumulated_logging_time': 0.9632065296173096, 'global_step': 14870, 'preemption_count': 0}), (15627, {'train/accuracy': 0.9904218912124634, 'train/loss': 0.031682081520557404, 'train/mean_average_precision': 0.3776189810012174, 'validation/accuracy': 0.9864882230758667, 'validation/loss': 0.045027993619441986, 'validation/mean_average_precision': 0.25533864868194683, 'validation/num_examples': 43793, 'test/accuracy': 0.9857210516929626, 'test/loss': 0.04752392694354057, 'test/mean_average_precision': 0.24927427426660573, 'test/num_examples': 43793, 'score': 5053.904675960541, 'total_duration': 7530.179664611816, 'accumulated_submission_time': 5053.904675960541, 'accumulated_eval_time': 2474.837132692337, 'accumulated_logging_time': 0.9940104484558105, 'global_step': 15627, 'preemption_count': 0}), (16374, {'train/accuracy': 0.9905163049697876, 'train/loss': 0.03142475336790085, 'train/mean_average_precision': 0.38393337206895434, 'validation/accuracy': 0.9865962266921997, 'validation/loss': 0.044712018221616745, 'validation/mean_average_precision': 0.2527464871775334, 'validation/num_examples': 43793, 'test/accuracy': 0.9857126474380493, 'test/loss': 0.04740390554070473, 'test/mean_average_precision': 0.24272942220406243, 'test/num_examples': 43793, 'score': 5293.972366333008, 'total_duration': 7877.760135889053, 'accumulated_submission_time': 5293.972366333008, 'accumulated_eval_time': 2582.3004174232483, 'accumulated_logging_time': 1.0230059623718262, 'global_step': 16374, 'preemption_count': 0}), (17134, {'train/accuracy': 0.9904187321662903, 'train/loss': 0.0311676487326622, 'train/mean_average_precision': 0.37887971000370035, 'validation/accuracy': 0.986632764339447, 'validation/loss': 0.04480931907892227, 'validation/mean_average_precision': 0.2590131502594755, 'validation/num_examples': 43793, 'test/accuracy': 0.9857867360115051, 'test/loss': 0.04752666503190994, 'test/mean_average_precision': 0.2523974319454819, 'test/num_examples': 43793, 'score': 5534.025603532791, 'total_duration': 8226.361269712448, 'accumulated_submission_time': 5534.025603532791, 'accumulated_eval_time': 2690.791320323944, 'accumulated_logging_time': 1.059596300125122, 'global_step': 17134, 'preemption_count': 0}), (17887, {'train/accuracy': 0.9908297061920166, 'train/loss': 0.030224693939089775, 'train/mean_average_precision': 0.39761579377261785, 'validation/accuracy': 0.9866778254508972, 'validation/loss': 0.044581979513168335, 'validation/mean_average_precision': 0.2644920682135977, 'validation/num_examples': 43793, 'test/accuracy': 0.9859261512756348, 'test/loss': 0.04703583940863609, 'test/mean_average_precision': 0.2652222081535117, 'test/num_examples': 43793, 'score': 5774.0658304691315, 'total_duration': 8578.81997179985, 'accumulated_submission_time': 5774.0658304691315, 'accumulated_eval_time': 2803.158401966095, 'accumulated_logging_time': 1.0902180671691895, 'global_step': 17887, 'preemption_count': 0}), (18634, {'train/accuracy': 0.9909319281578064, 'train/loss': 0.02977757528424263, 'train/mean_average_precision': 0.41316582178015493, 'validation/accuracy': 0.9866737127304077, 'validation/loss': 0.044742196798324585, 'validation/mean_average_precision': 0.26652513254352184, 'validation/num_examples': 43793, 'test/accuracy': 0.9858751893043518, 'test/loss': 0.04739736020565033, 'test/mean_average_precision': 0.2532736599645985, 'test/num_examples': 43793, 'score': 6014.162916898727, 'total_duration': 8928.417778730392, 'accumulated_submission_time': 6014.162916898727, 'accumulated_eval_time': 2912.6052956581116, 'accumulated_logging_time': 1.1236302852630615, 'global_step': 18634, 'preemption_count': 0}), (19381, {'train/accuracy': 0.9910821914672852, 'train/loss': 0.029007235541939735, 'train/mean_average_precision': 0.43306888295180035, 'validation/accuracy': 0.9867277145385742, 'validation/loss': 0.04532693699002266, 'validation/mean_average_precision': 0.26184248613783995, 'validation/num_examples': 43793, 'test/accuracy': 0.9859354496002197, 'test/loss': 0.04790349304676056, 'test/mean_average_precision': 0.25805148466277017, 'test/num_examples': 43793, 'score': 6254.241195201874, 'total_duration': 9281.429696083069, 'accumulated_submission_time': 6254.241195201874, 'accumulated_eval_time': 3025.486868619919, 'accumulated_logging_time': 1.1551201343536377, 'global_step': 19381, 'preemption_count': 0}), (20119, {'train/accuracy': 0.9911299347877502, 'train/loss': 0.02904364839196205, 'train/mean_average_precision': 0.4524596980950766, 'validation/accuracy': 0.9866428971290588, 'validation/loss': 0.044798992574214935, 'validation/mean_average_precision': 0.26663937524288245, 'validation/num_examples': 43793, 'test/accuracy': 0.9859223961830139, 'test/loss': 0.04723508656024933, 'test/mean_average_precision': 0.2566684458171706, 'test/num_examples': 43793, 'score': 6494.479324102402, 'total_duration': 9637.310660123825, 'accumulated_submission_time': 6494.479324102402, 'accumulated_eval_time': 3141.0740916728973, 'accumulated_logging_time': 1.1857497692108154, 'global_step': 20119, 'preemption_count': 0}), (20860, {'train/accuracy': 0.9911418557167053, 'train/loss': 0.028990084305405617, 'train/mean_average_precision': 0.43619180909209165, 'validation/accuracy': 0.9865061044692993, 'validation/loss': 0.044800519943237305, 'validation/mean_average_precision': 0.25785411469819974, 'validation/num_examples': 43793, 'test/accuracy': 0.9855993390083313, 'test/loss': 0.047596536576747894, 'test/mean_average_precision': 0.2488904178834033, 'test/num_examples': 43793, 'score': 6734.529579162598, 'total_duration': 9989.929476737976, 'accumulated_submission_time': 6734.529579162598, 'accumulated_eval_time': 3253.587816953659, 'accumulated_logging_time': 1.2191963195800781, 'global_step': 20860, 'preemption_count': 0}), (21614, {'train/accuracy': 0.9909935593605042, 'train/loss': 0.029510680586099625, 'train/mean_average_precision': 0.418704369794744, 'validation/accuracy': 0.9864736199378967, 'validation/loss': 0.04465806111693382, 'validation/mean_average_precision': 0.26883918554463876, 'validation/num_examples': 43793, 'test/accuracy': 0.9857787489891052, 'test/loss': 0.04739959537982941, 'test/mean_average_precision': 0.2512402387290738, 'test/num_examples': 43793, 'score': 6974.786199092865, 'total_duration': 10340.484828233719, 'accumulated_submission_time': 6974.786199092865, 'accumulated_eval_time': 3363.8338465690613, 'accumulated_logging_time': 1.250971794128418, 'global_step': 21614, 'preemption_count': 0}), (22366, {'train/accuracy': 0.990916907787323, 'train/loss': 0.029702335596084595, 'train/mean_average_precision': 0.40929954176538713, 'validation/accuracy': 0.9867342114448547, 'validation/loss': 0.04458253085613251, 'validation/mean_average_precision': 0.2692697508641262, 'validation/num_examples': 43793, 'test/accuracy': 0.9858646988868713, 'test/loss': 0.047177691012620926, 'test/mean_average_precision': 0.2555786770835429, 'test/num_examples': 43793, 'score': 7215.030090808868, 'total_duration': 10686.331381559372, 'accumulated_submission_time': 7215.030090808868, 'accumulated_eval_time': 3469.385509490967, 'accumulated_logging_time': 1.2810804843902588, 'global_step': 22366, 'preemption_count': 0}), (23115, {'train/accuracy': 0.990923285484314, 'train/loss': 0.029610656201839447, 'train/mean_average_precision': 0.4129811539648565, 'validation/accuracy': 0.9866039156913757, 'validation/loss': 0.044834770262241364, 'validation/mean_average_precision': 0.2584141344443701, 'validation/num_examples': 43793, 'test/accuracy': 0.9858865737915039, 'test/loss': 0.04729144647717476, 'test/mean_average_precision': 0.25695871540311854, 'test/num_examples': 43793, 'score': 7455.064225435257, 'total_duration': 11036.378544092178, 'accumulated_submission_time': 7455.064225435257, 'accumulated_eval_time': 3579.345846414566, 'accumulated_logging_time': 1.3130922317504883, 'global_step': 23115, 'preemption_count': 0}), (23867, {'train/accuracy': 0.9910376667976379, 'train/loss': 0.02931050769984722, 'train/mean_average_precision': 0.4443964159175245, 'validation/accuracy': 0.9865726828575134, 'validation/loss': 0.04488106817007065, 'validation/mean_average_precision': 0.26803957807294937, 'validation/num_examples': 43793, 'test/accuracy': 0.9857938885688782, 'test/loss': 0.04747995734214783, 'test/mean_average_precision': 0.26006297683377805, 'test/num_examples': 43793, 'score': 7695.025829792023, 'total_duration': 11383.292026758194, 'accumulated_submission_time': 7695.025829792023, 'accumulated_eval_time': 3686.2436599731445, 'accumulated_logging_time': 1.3458125591278076, 'global_step': 23867, 'preemption_count': 0}), (24615, {'train/accuracy': 0.9911251068115234, 'train/loss': 0.028719495981931686, 'train/mean_average_precision': 0.43497808624609424, 'validation/accuracy': 0.9867350459098816, 'validation/loss': 0.04469083994626999, 'validation/mean_average_precision': 0.2686270225783731, 'validation/num_examples': 43793, 'test/accuracy': 0.9858722686767578, 'test/loss': 0.04751843214035034, 'test/mean_average_precision': 0.257685135742762, 'test/num_examples': 43793, 'score': 7935.27251625061, 'total_duration': 11736.308470726013, 'accumulated_submission_time': 7935.27251625061, 'accumulated_eval_time': 3798.9610619544983, 'accumulated_logging_time': 1.377070426940918, 'global_step': 24615, 'preemption_count': 0}), (25352, {'train/accuracy': 0.9912492036819458, 'train/loss': 0.02848394587635994, 'train/mean_average_precision': 0.44107130116975035, 'validation/accuracy': 0.9866904020309448, 'validation/loss': 0.04465167596936226, 'validation/mean_average_precision': 0.2698735193364242, 'validation/num_examples': 43793, 'test/accuracy': 0.9858680367469788, 'test/loss': 0.047340765595436096, 'test/mean_average_precision': 0.2599173126520697, 'test/num_examples': 43793, 'score': 8175.3488891124725, 'total_duration': 12088.560628414154, 'accumulated_submission_time': 8175.3488891124725, 'accumulated_eval_time': 3911.0762073993683, 'accumulated_logging_time': 1.4118154048919678, 'global_step': 25352, 'preemption_count': 0}), (26095, {'train/accuracy': 0.9914388656616211, 'train/loss': 0.027753343805670738, 'train/mean_average_precision': 0.4717187337805464, 'validation/accuracy': 0.9866493940353394, 'validation/loss': 0.044874098151922226, 'validation/mean_average_precision': 0.26778515215614146, 'validation/num_examples': 43793, 'test/accuracy': 0.9858587980270386, 'test/loss': 0.04757843539118767, 'test/mean_average_precision': 0.2584336771859823, 'test/num_examples': 43793, 'score': 8415.341947555542, 'total_duration': 12442.495477199554, 'accumulated_submission_time': 8415.341947555542, 'accumulated_eval_time': 4024.9607586860657, 'accumulated_logging_time': 1.4460327625274658, 'global_step': 26095, 'preemption_count': 0}), (26836, {'train/accuracy': 0.9916952848434448, 'train/loss': 0.02694741077721119, 'train/mean_average_precision': 0.4850118415830761, 'validation/accuracy': 0.9866157174110413, 'validation/loss': 0.04488666355609894, 'validation/mean_average_precision': 0.2657575785424613, 'validation/num_examples': 43793, 'test/accuracy': 0.9857859015464783, 'test/loss': 0.04761894419789314, 'test/mean_average_precision': 0.2573965177777483, 'test/num_examples': 43793, 'score': 8655.540459394455, 'total_duration': 12792.448338985443, 'accumulated_submission_time': 8655.540459394455, 'accumulated_eval_time': 4134.6611087322235, 'accumulated_logging_time': 1.4795305728912354, 'global_step': 26836, 'preemption_count': 0}), (27589, {'train/accuracy': 0.9915966391563416, 'train/loss': 0.02711980976164341, 'train/mean_average_precision': 0.48331825053568495, 'validation/accuracy': 0.9867532849311829, 'validation/loss': 0.04511391371488571, 'validation/mean_average_precision': 0.2680907991811648, 'validation/num_examples': 43793, 'test/accuracy': 0.9858987927436829, 'test/loss': 0.048005200922489166, 'test/mean_average_precision': 0.25320234524196866, 'test/num_examples': 43793, 'score': 8895.518734931946, 'total_duration': 13139.403820991516, 'accumulated_submission_time': 8895.518734931946, 'accumulated_eval_time': 4241.58500623703, 'accumulated_logging_time': 1.5119729042053223, 'global_step': 27589, 'preemption_count': 0}), (28338, {'train/accuracy': 0.9913761615753174, 'train/loss': 0.027994006872177124, 'train/mean_average_precision': 0.4537976301349314, 'validation/accuracy': 0.9864557385444641, 'validation/loss': 0.04573741555213928, 'validation/mean_average_precision': 0.26533273497510806, 'validation/num_examples': 43793, 'test/accuracy': 0.9856258630752563, 'test/loss': 0.048382312059402466, 'test/mean_average_precision': 0.2584529614231575, 'test/num_examples': 43793, 'score': 9135.784817695618, 'total_duration': 13492.027368068695, 'accumulated_submission_time': 9135.784817695618, 'accumulated_eval_time': 4353.890632867813, 'accumulated_logging_time': 1.5436317920684814, 'global_step': 28338, 'preemption_count': 0}), (29081, {'train/accuracy': 0.9911956191062927, 'train/loss': 0.028365036472678185, 'train/mean_average_precision': 0.4424061318322148, 'validation/accuracy': 0.9867041707038879, 'validation/loss': 0.044825755059719086, 'validation/mean_average_precision': 0.26958300690427595, 'validation/num_examples': 43793, 'test/accuracy': 0.9858810901641846, 'test/loss': 0.04768301546573639, 'test/mean_average_precision': 0.2606048610788429, 'test/num_examples': 43793, 'score': 9375.92338514328, 'total_duration': 13845.032121896744, 'accumulated_submission_time': 9375.92338514328, 'accumulated_eval_time': 4466.701440811157, 'accumulated_logging_time': 1.5765349864959717, 'global_step': 29081, 'preemption_count': 0}), (29837, {'train/accuracy': 0.9912551641464233, 'train/loss': 0.028403988108038902, 'train/mean_average_precision': 0.45873083964496636, 'validation/accuracy': 0.9867545366287231, 'validation/loss': 0.04485427960753441, 'validation/mean_average_precision': 0.26848781210751976, 'validation/num_examples': 43793, 'test/accuracy': 0.9859964847564697, 'test/loss': 0.04744412377476692, 'test/mean_average_precision': 0.26221145260141127, 'test/num_examples': 43793, 'score': 9616.0367436409, 'total_duration': 14193.957685470581, 'accumulated_submission_time': 9616.0367436409, 'accumulated_eval_time': 4575.459840536118, 'accumulated_logging_time': 1.6094539165496826, 'global_step': 29837, 'preemption_count': 0}), (30584, {'train/accuracy': 0.9915154576301575, 'train/loss': 0.027463402599096298, 'train/mean_average_precision': 0.46339895940182796, 'validation/accuracy': 0.9865645170211792, 'validation/loss': 0.044793397188186646, 'validation/mean_average_precision': 0.27743048304507184, 'validation/num_examples': 43793, 'test/accuracy': 0.9858090877532959, 'test/loss': 0.04753444343805313, 'test/mean_average_precision': 0.26575707077668376, 'test/num_examples': 43793, 'score': 9856.137685060501, 'total_duration': 14542.431862354279, 'accumulated_submission_time': 9856.137685060501, 'accumulated_eval_time': 4683.776865005493, 'accumulated_logging_time': 1.6447956562042236, 'global_step': 30584, 'preemption_count': 0}), (31337, {'train/accuracy': 0.9915711283683777, 'train/loss': 0.027200734242796898, 'train/mean_average_precision': 0.47814745243094403, 'validation/accuracy': 0.9866875410079956, 'validation/loss': 0.04512999579310417, 'validation/mean_average_precision': 0.2771974970735388, 'validation/num_examples': 43793, 'test/accuracy': 0.9858158230781555, 'test/loss': 0.04799988493323326, 'test/mean_average_precision': 0.2566377316769796, 'test/num_examples': 43793, 'score': 10096.208181619644, 'total_duration': 14892.963721752167, 'accumulated_submission_time': 10096.208181619644, 'accumulated_eval_time': 4794.186100482941, 'accumulated_logging_time': 1.6768467426300049, 'global_step': 31337, 'preemption_count': 0}), (32094, {'train/accuracy': 0.991542398929596, 'train/loss': 0.02716778591275215, 'train/mean_average_precision': 0.47173045852876155, 'validation/accuracy': 0.9865958094596863, 'validation/loss': 0.0454927459359169, 'validation/mean_average_precision': 0.26300805545576683, 'validation/num_examples': 43793, 'test/accuracy': 0.9858823418617249, 'test/loss': 0.0483216866850853, 'test/mean_average_precision': 0.25246182403224593, 'test/num_examples': 43793, 'score': 10336.211215496063, 'total_duration': 15242.315237522125, 'accumulated_submission_time': 10336.211215496063, 'accumulated_eval_time': 4903.480740070343, 'accumulated_logging_time': 1.7101600170135498, 'global_step': 32094, 'preemption_count': 0}), (32840, {'train/accuracy': 0.9917683601379395, 'train/loss': 0.026312205940485, 'train/mean_average_precision': 0.5015883602624208, 'validation/accuracy': 0.9866209626197815, 'validation/loss': 0.04509390890598297, 'validation/mean_average_precision': 0.2753164946013637, 'validation/num_examples': 43793, 'test/accuracy': 0.9857816696166992, 'test/loss': 0.04810566455125809, 'test/mean_average_precision': 0.26276872265511453, 'test/num_examples': 43793, 'score': 10576.290118932724, 'total_duration': 15588.626574993134, 'accumulated_submission_time': 10576.290118932724, 'accumulated_eval_time': 5009.653836011887, 'accumulated_logging_time': 1.7483479976654053, 'global_step': 32840, 'preemption_count': 0}), (33583, {'train/accuracy': 0.9920535683631897, 'train/loss': 0.025541843846440315, 'train/mean_average_precision': 0.5182077647609746, 'validation/accuracy': 0.9865832328796387, 'validation/loss': 0.045318685472011566, 'validation/mean_average_precision': 0.27103989282553687, 'validation/num_examples': 43793, 'test/accuracy': 0.9858322143554688, 'test/loss': 0.04798828065395355, 'test/mean_average_precision': 0.26138324044392885, 'test/num_examples': 43793, 'score': 10816.54667019844, 'total_duration': 15938.288664340973, 'accumulated_submission_time': 10816.54667019844, 'accumulated_eval_time': 5118.999151468277, 'accumulated_logging_time': 1.7833147048950195, 'global_step': 33583, 'preemption_count': 0}), (34333, {'train/accuracy': 0.992278516292572, 'train/loss': 0.024827653542160988, 'train/mean_average_precision': 0.5308865696339622, 'validation/accuracy': 0.9868900775909424, 'validation/loss': 0.04549545422196388, 'validation/mean_average_precision': 0.27921532311605257, 'validation/num_examples': 43793, 'test/accuracy': 0.9859931468963623, 'test/loss': 0.04836021363735199, 'test/mean_average_precision': 0.2592067698810614, 'test/num_examples': 43793, 'score': 11056.78190946579, 'total_duration': 16288.988464832306, 'accumulated_submission_time': 11056.78190946579, 'accumulated_eval_time': 5229.406427145004, 'accumulated_logging_time': 1.818897008895874, 'global_step': 34333, 'preemption_count': 0}), (35076, {'train/accuracy': 0.9921972751617432, 'train/loss': 0.025283345952630043, 'train/mean_average_precision': 0.5149612251079351, 'validation/accuracy': 0.9865661859512329, 'validation/loss': 0.045130666345357895, 'validation/mean_average_precision': 0.2722435905529067, 'validation/num_examples': 43793, 'test/accuracy': 0.985697865486145, 'test/loss': 0.04817037656903267, 'test/mean_average_precision': 0.25795848264725724, 'test/num_examples': 43793, 'score': 11296.9742269516, 'total_duration': 16636.286905050278, 'accumulated_submission_time': 11296.9742269516, 'accumulated_eval_time': 5336.452176809311, 'accumulated_logging_time': 1.8567280769348145, 'global_step': 35076, 'preemption_count': 0}), (35816, {'train/accuracy': 0.99190354347229, 'train/loss': 0.026034832000732422, 'train/mean_average_precision': 0.5028728350713861, 'validation/accuracy': 0.9866254329681396, 'validation/loss': 0.04569826275110245, 'validation/mean_average_precision': 0.27601925715865744, 'validation/num_examples': 43793, 'test/accuracy': 0.9858474135398865, 'test/loss': 0.04843038320541382, 'test/mean_average_precision': 0.2547830847013111, 'test/num_examples': 43793, 'score': 11536.981744527817, 'total_duration': 16986.371559381485, 'accumulated_submission_time': 11536.981744527817, 'accumulated_eval_time': 5446.469096899033, 'accumulated_logging_time': 1.8912177085876465, 'global_step': 35816, 'preemption_count': 0}), (36574, {'train/accuracy': 0.9917653799057007, 'train/loss': 0.026342226192355156, 'train/mean_average_precision': 0.49410733515559263, 'validation/accuracy': 0.9865604639053345, 'validation/loss': 0.0457744225859642, 'validation/mean_average_precision': 0.26810133070660647, 'validation/num_examples': 43793, 'test/accuracy': 0.9858174920082092, 'test/loss': 0.048509806394577026, 'test/mean_average_precision': 0.25872792358315133, 'test/num_examples': 43793, 'score': 11777.182457208633, 'total_duration': 17339.134885072708, 'accumulated_submission_time': 11777.182457208633, 'accumulated_eval_time': 5558.977123498917, 'accumulated_logging_time': 1.9253017902374268, 'global_step': 36574, 'preemption_count': 0}), (37332, {'train/accuracy': 0.9916236400604248, 'train/loss': 0.026827674359083176, 'train/mean_average_precision': 0.49001116884105334, 'validation/accuracy': 0.98675936460495, 'validation/loss': 0.04567720368504524, 'validation/mean_average_precision': 0.26503827508144473, 'validation/num_examples': 43793, 'test/accuracy': 0.9857711791992188, 'test/loss': 0.04885249584913254, 'test/mean_average_precision': 0.25305268444256357, 'test/num_examples': 43793, 'score': 12017.42915058136, 'total_duration': 17684.79681611061, 'accumulated_submission_time': 12017.42915058136, 'accumulated_eval_time': 5664.32294178009, 'accumulated_logging_time': 1.9741060733795166, 'global_step': 37332, 'preemption_count': 0}), (38089, {'train/accuracy': 0.9918012619018555, 'train/loss': 0.026224201545119286, 'train/mean_average_precision': 0.5010326044071443, 'validation/accuracy': 0.9865389466285706, 'validation/loss': 0.045652519911527634, 'validation/mean_average_precision': 0.27044119487028595, 'validation/num_examples': 43793, 'test/accuracy': 0.9857606291770935, 'test/loss': 0.04854629561305046, 'test/mean_average_precision': 0.2595443308006202, 'test/num_examples': 43793, 'score': 12257.636492729187, 'total_duration': 18033.42644929886, 'accumulated_submission_time': 12257.636492729187, 'accumulated_eval_time': 5772.689391851425, 'accumulated_logging_time': 2.009575843811035, 'global_step': 38089, 'preemption_count': 0}), (38846, {'train/accuracy': 0.9921699166297913, 'train/loss': 0.02522186003625393, 'train/mean_average_precision': 0.5169255787785384, 'validation/accuracy': 0.9866672158241272, 'validation/loss': 0.04605378210544586, 'validation/mean_average_precision': 0.2735889551897641, 'validation/num_examples': 43793, 'test/accuracy': 0.9858554005622864, 'test/loss': 0.04902553930878639, 'test/mean_average_precision': 0.2538144049564341, 'test/num_examples': 43793, 'score': 12497.743663549423, 'total_duration': 18379.902636289597, 'accumulated_submission_time': 12497.743663549423, 'accumulated_eval_time': 5879.004307746887, 'accumulated_logging_time': 2.043231964111328, 'global_step': 38846, 'preemption_count': 0}), (39599, {'train/accuracy': 0.9920904040336609, 'train/loss': 0.025093624368309975, 'train/mean_average_precision': 0.5144372891395481, 'validation/accuracy': 0.9867464303970337, 'validation/loss': 0.045916326344013214, 'validation/mean_average_precision': 0.2768331889155624, 'validation/num_examples': 43793, 'test/accuracy': 0.9858794212341309, 'test/loss': 0.048941463232040405, 'test/mean_average_precision': 0.25255159872354327, 'test/num_examples': 43793, 'score': 12737.713785409927, 'total_duration': 18727.36371779442, 'accumulated_submission_time': 12737.713785409927, 'accumulated_eval_time': 5986.441025257111, 'accumulated_logging_time': 2.076756715774536, 'global_step': 39599, 'preemption_count': 0}), (40351, {'train/accuracy': 0.9924927949905396, 'train/loss': 0.023928308859467506, 'train/mean_average_precision': 0.5607641462687354, 'validation/accuracy': 0.9866968989372253, 'validation/loss': 0.045777395367622375, 'validation/mean_average_precision': 0.27961877377697586, 'validation/num_examples': 43793, 'test/accuracy': 0.9858419299125671, 'test/loss': 0.04900488629937172, 'test/mean_average_precision': 0.2534988346590776, 'test/num_examples': 43793, 'score': 12977.72698712349, 'total_duration': 19078.132613658905, 'accumulated_submission_time': 12977.72698712349, 'accumulated_eval_time': 6097.138377904892, 'accumulated_logging_time': 2.1142361164093018, 'global_step': 40351, 'preemption_count': 0}), (41103, {'train/accuracy': 0.9927111864089966, 'train/loss': 0.023247594013810158, 'train/mean_average_precision': 0.5671089549068034, 'validation/accuracy': 0.9865982532501221, 'validation/loss': 0.04591871798038483, 'validation/mean_average_precision': 0.2702917025777467, 'validation/num_examples': 43793, 'test/accuracy': 0.9858731031417847, 'test/loss': 0.048900943249464035, 'test/mean_average_precision': 0.25475077138918667, 'test/num_examples': 43793, 'score': 13217.773389101028, 'total_duration': 19428.14403939247, 'accumulated_submission_time': 13217.773389101028, 'accumulated_eval_time': 6207.048756837845, 'accumulated_logging_time': 2.148378849029541, 'global_step': 41103, 'preemption_count': 0}), (41852, {'train/accuracy': 0.9927569031715393, 'train/loss': 0.02298024669289589, 'train/mean_average_precision': 0.5699415537277851, 'validation/accuracy': 0.9866871237754822, 'validation/loss': 0.04621214047074318, 'validation/mean_average_precision': 0.27547600810480405, 'validation/num_examples': 43793, 'test/accuracy': 0.9858137369155884, 'test/loss': 0.049364667385816574, 'test/mean_average_precision': 0.25842190406679577, 'test/num_examples': 43793, 'score': 13457.892441272736, 'total_duration': 19773.699976205826, 'accumulated_submission_time': 13457.892441272736, 'accumulated_eval_time': 6312.429904937744, 'accumulated_logging_time': 2.1833369731903076, 'global_step': 41852, 'preemption_count': 0}), (42605, {'train/accuracy': 0.9927787184715271, 'train/loss': 0.022964106872677803, 'train/mean_average_precision': 0.5776440654013806, 'validation/accuracy': 0.9868137836456299, 'validation/loss': 0.046512555330991745, 'validation/mean_average_precision': 0.2742111553186739, 'validation/num_examples': 43793, 'test/accuracy': 0.985913097858429, 'test/loss': 0.04963935166597366, 'test/mean_average_precision': 0.2544896576267384, 'test/num_examples': 43793, 'score': 13698.05339050293, 'total_duration': 20121.89403295517, 'accumulated_submission_time': 13698.05339050293, 'accumulated_eval_time': 6420.406673908234, 'accumulated_logging_time': 2.2188665866851807, 'global_step': 42605, 'preemption_count': 0}), (43359, {'train/accuracy': 0.9925683736801147, 'train/loss': 0.02361549809575081, 'train/mean_average_precision': 0.553858979056238, 'validation/accuracy': 0.9865787625312805, 'validation/loss': 0.04614240303635597, 'validation/mean_average_precision': 0.27882798998942643, 'validation/num_examples': 43793, 'test/accuracy': 0.9858166575431824, 'test/loss': 0.049183864146471024, 'test/mean_average_precision': 0.2583898694445723, 'test/num_examples': 43793, 'score': 13938.207823753357, 'total_duration': 20469.63530278206, 'accumulated_submission_time': 13938.207823753357, 'accumulated_eval_time': 6527.937375545502, 'accumulated_logging_time': 2.253929853439331, 'global_step': 43359, 'preemption_count': 0}), (44113, {'train/accuracy': 0.9924434423446655, 'train/loss': 0.024065496399998665, 'train/mean_average_precision': 0.5444905132486455, 'validation/accuracy': 0.9865491390228271, 'validation/loss': 0.04658720642328262, 'validation/mean_average_precision': 0.27540289113259364, 'validation/num_examples': 43793, 'test/accuracy': 0.9856974482536316, 'test/loss': 0.04961707442998886, 'test/mean_average_precision': 0.2565613538093232, 'test/num_examples': 43793, 'score': 14178.196440935135, 'total_duration': 20815.98492026329, 'accumulated_submission_time': 14178.196440935135, 'accumulated_eval_time': 6634.2423985004425, 'accumulated_logging_time': 2.289602518081665, 'global_step': 44113, 'preemption_count': 0}), (44869, {'train/accuracy': 0.9923385977745056, 'train/loss': 0.024227526038885117, 'train/mean_average_precision': 0.5417031986400285, 'validation/accuracy': 0.9865332841873169, 'validation/loss': 0.04700421169400215, 'validation/mean_average_precision': 0.2736799668428959, 'validation/num_examples': 43793, 'test/accuracy': 0.9857791662216187, 'test/loss': 0.05012236163020134, 'test/mean_average_precision': 0.2560017213560228, 'test/num_examples': 43793, 'score': 14418.435846090317, 'total_duration': 21164.322757959366, 'accumulated_submission_time': 14418.435846090317, 'accumulated_eval_time': 6742.28401350975, 'accumulated_logging_time': 2.325899839401245, 'global_step': 44869, 'preemption_count': 0}), (45619, {'train/accuracy': 0.9924070835113525, 'train/loss': 0.02397083304822445, 'train/mean_average_precision': 0.5425693027797756, 'validation/accuracy': 0.9866347908973694, 'validation/loss': 0.0474613681435585, 'validation/mean_average_precision': 0.2696670163389971, 'validation/num_examples': 43793, 'test/accuracy': 0.9858579039573669, 'test/loss': 0.050393376499414444, 'test/mean_average_precision': 0.2503948832383014, 'test/num_examples': 43793, 'score': 14658.508082866669, 'total_duration': 21512.269384622574, 'accumulated_submission_time': 14658.508082866669, 'accumulated_eval_time': 6850.10337138176, 'accumulated_logging_time': 2.360506772994995, 'global_step': 45619, 'preemption_count': 0}), (46379, {'train/accuracy': 0.9926638007164001, 'train/loss': 0.023263966664671898, 'train/mean_average_precision': 0.5599083695297632, 'validation/accuracy': 0.9865369200706482, 'validation/loss': 0.04737938195466995, 'validation/mean_average_precision': 0.2749799347087452, 'validation/num_examples': 43793, 'test/accuracy': 0.985714316368103, 'test/loss': 0.050589628517627716, 'test/mean_average_precision': 0.25259962869472635, 'test/num_examples': 43793, 'score': 14898.713564157486, 'total_duration': 21856.210003376007, 'accumulated_submission_time': 14898.713564157486, 'accumulated_eval_time': 6953.782834768295, 'accumulated_logging_time': 2.3951354026794434, 'global_step': 46379, 'preemption_count': 0}), (47131, {'train/accuracy': 0.992831289768219, 'train/loss': 0.022676663473248482, 'train/mean_average_precision': 0.5819867362002796, 'validation/accuracy': 0.9864833354949951, 'validation/loss': 0.04743078351020813, 'validation/mean_average_precision': 0.2747054695439795, 'validation/num_examples': 43793, 'test/accuracy': 0.9857437610626221, 'test/loss': 0.05043184757232666, 'test/mean_average_precision': 0.26107503575082996, 'test/num_examples': 43793, 'score': 15138.819725036621, 'total_duration': 22203.87849545479, 'accumulated_submission_time': 15138.819725036621, 'accumulated_eval_time': 7061.289536476135, 'accumulated_logging_time': 2.430541515350342, 'global_step': 47131, 'preemption_count': 0}), (47888, {'train/accuracy': 0.9928385615348816, 'train/loss': 0.022271446883678436, 'train/mean_average_precision': 0.5891975629574058, 'validation/accuracy': 0.9865233302116394, 'validation/loss': 0.04813092201948166, 'validation/mean_average_precision': 0.26875716952864187, 'validation/num_examples': 43793, 'test/accuracy': 0.9857400059700012, 'test/loss': 0.05103105306625366, 'test/mean_average_precision': 0.2514952056072648, 'test/num_examples': 43793, 'score': 15378.858348846436, 'total_duration': 22548.216205596924, 'accumulated_submission_time': 15378.858348846436, 'accumulated_eval_time': 7165.529061079025, 'accumulated_logging_time': 2.469257354736328, 'global_step': 47888, 'preemption_count': 0}), (48635, {'train/accuracy': 0.9935416579246521, 'train/loss': 0.020306341350078583, 'train/mean_average_precision': 0.6341293814817233, 'validation/accuracy': 0.9865320920944214, 'validation/loss': 0.04798257723450661, 'validation/mean_average_precision': 0.27303969033562364, 'validation/num_examples': 43793, 'test/accuracy': 0.9856961965560913, 'test/loss': 0.050840746611356735, 'test/mean_average_precision': 0.25393946714323823, 'test/num_examples': 43793, 'score': 15619.018918275833, 'total_duration': 22897.42957997322, 'accumulated_submission_time': 15619.018918275833, 'accumulated_eval_time': 7274.522526025772, 'accumulated_logging_time': 2.5069985389709473, 'global_step': 48635, 'preemption_count': 0}), (49384, {'train/accuracy': 0.993671178817749, 'train/loss': 0.020141547545790672, 'train/mean_average_precision': 0.6405901932539897, 'validation/accuracy': 0.9863559007644653, 'validation/loss': 0.048238445073366165, 'validation/mean_average_precision': 0.2759097759683734, 'validation/num_examples': 43793, 'test/accuracy': 0.985526442527771, 'test/loss': 0.051412928849458694, 'test/mean_average_precision': 0.25371684299710057, 'test/num_examples': 43793, 'score': 15859.109230279922, 'total_duration': 23243.656244277954, 'accumulated_submission_time': 15859.109230279922, 'accumulated_eval_time': 7380.601233720779, 'accumulated_logging_time': 2.544231653213501, 'global_step': 49384, 'preemption_count': 0}), (50136, {'train/accuracy': 0.9936359524726868, 'train/loss': 0.019929487258195877, 'train/mean_average_precision': 0.634786984643436, 'validation/accuracy': 0.986668050289154, 'validation/loss': 0.048447899520397186, 'validation/mean_average_precision': 0.2804146011801025, 'validation/num_examples': 43793, 'test/accuracy': 0.9857547283172607, 'test/loss': 0.05165000259876251, 'test/mean_average_precision': 0.25744070491900967, 'test/num_examples': 43793, 'score': 16099.191838979721, 'total_duration': 23588.102046728134, 'accumulated_submission_time': 16099.191838979721, 'accumulated_eval_time': 7484.907016038895, 'accumulated_logging_time': 2.5809249877929688, 'global_step': 50136, 'preemption_count': 0}), (50894, {'train/accuracy': 0.9934646487236023, 'train/loss': 0.020531222224235535, 'train/mean_average_precision': 0.6163652200340647, 'validation/accuracy': 0.9863924384117126, 'validation/loss': 0.0486702099442482, 'validation/mean_average_precision': 0.2739500108171812, 'validation/num_examples': 43793, 'test/accuracy': 0.9856056571006775, 'test/loss': 0.051769357174634933, 'test/mean_average_precision': 0.2514201600194406, 'test/num_examples': 43793, 'score': 16339.400242805481, 'total_duration': 23933.579761743546, 'accumulated_submission_time': 16339.400242805481, 'accumulated_eval_time': 7590.119548559189, 'accumulated_logging_time': 2.616666078567505, 'global_step': 50894, 'preemption_count': 0}), (51638, {'train/accuracy': 0.9933746457099915, 'train/loss': 0.02076597511768341, 'train/mean_average_precision': 0.6203458093296251, 'validation/accuracy': 0.9864504933357239, 'validation/loss': 0.04906805604696274, 'validation/mean_average_precision': 0.272940814365671, 'validation/num_examples': 43793, 'test/accuracy': 0.9855926036834717, 'test/loss': 0.05210414156317711, 'test/mean_average_precision': 0.2548566982480128, 'test/num_examples': 43793, 'score': 16579.39709186554, 'total_duration': 24278.319911956787, 'accumulated_submission_time': 16579.39709186554, 'accumulated_eval_time': 7694.790858745575, 'accumulated_logging_time': 2.667487859725952, 'global_step': 51638, 'preemption_count': 0}), (52394, {'train/accuracy': 0.9932540059089661, 'train/loss': 0.02079734019935131, 'train/mean_average_precision': 0.6166767403336669, 'validation/accuracy': 0.9865624904632568, 'validation/loss': 0.049784209579229355, 'validation/mean_average_precision': 0.26973126714833323, 'validation/num_examples': 43793, 'test/accuracy': 0.9857804179191589, 'test/loss': 0.05286939814686775, 'test/mean_average_precision': 0.2487877520350712, 'test/num_examples': 43793, 'score': 16819.611674308777, 'total_duration': 24623.10201358795, 'accumulated_submission_time': 16819.611674308777, 'accumulated_eval_time': 7799.301882743835, 'accumulated_logging_time': 2.7027688026428223, 'global_step': 52394, 'preemption_count': 0}), (53149, {'train/accuracy': 0.993232250213623, 'train/loss': 0.02114035189151764, 'train/mean_average_precision': 0.600300756020649, 'validation/accuracy': 0.9863465428352356, 'validation/loss': 0.050232309848070145, 'validation/mean_average_precision': 0.26931284449460385, 'validation/num_examples': 43793, 'test/accuracy': 0.9855176210403442, 'test/loss': 0.05326259136199951, 'test/mean_average_precision': 0.2513808514617977, 'test/num_examples': 43793, 'score': 17059.75382900238, 'total_duration': 24963.80125451088, 'accumulated_submission_time': 17059.75382900238, 'accumulated_eval_time': 7899.80190038681, 'accumulated_logging_time': 2.738955497741699, 'global_step': 53149, 'preemption_count': 0}), (53905, {'train/accuracy': 0.9932581186294556, 'train/loss': 0.020752474665641785, 'train/mean_average_precision': 0.6190629736359106, 'validation/accuracy': 0.9863465428352356, 'validation/loss': 0.050454575568437576, 'validation/mean_average_precision': 0.2680434163900203, 'validation/num_examples': 43793, 'test/accuracy': 0.98549485206604, 'test/loss': 0.053566861897706985, 'test/mean_average_precision': 0.2530998305906409, 'test/num_examples': 43793, 'score': 17300.010417699814, 'total_duration': 25316.719260931015, 'accumulated_submission_time': 17300.010417699814, 'accumulated_eval_time': 8012.406537294388, 'accumulated_logging_time': 2.774937868118286, 'global_step': 53905, 'preemption_count': 0}), (54648, {'train/accuracy': 0.9935899972915649, 'train/loss': 0.019713984802365303, 'train/mean_average_precision': 0.6550292985915764, 'validation/accuracy': 0.9860900044441223, 'validation/loss': 0.051197599619627, 'validation/mean_average_precision': 0.2668912946641175, 'validation/num_examples': 43793, 'test/accuracy': 0.9853617548942566, 'test/loss': 0.05401235073804855, 'test/mean_average_precision': 0.24674567276445694, 'test/num_examples': 43793, 'score': 17540.260680675507, 'total_duration': 25660.806088209152, 'accumulated_submission_time': 17540.260680675507, 'accumulated_eval_time': 8116.179745674133, 'accumulated_logging_time': 2.8153786659240723, 'global_step': 54648, 'preemption_count': 0}), (55397, {'train/accuracy': 0.9940750002861023, 'train/loss': 0.01836450770497322, 'train/mean_average_precision': 0.6688367949780897, 'validation/accuracy': 0.9861955642700195, 'validation/loss': 0.050676897168159485, 'validation/mean_average_precision': 0.272017019263525, 'validation/num_examples': 43793, 'test/accuracy': 0.9854013323783875, 'test/loss': 0.053795311599969864, 'test/mean_average_precision': 0.2553403779232295, 'test/num_examples': 43793, 'score': 17780.475964784622, 'total_duration': 26006.03764986992, 'accumulated_submission_time': 17780.475964784622, 'accumulated_eval_time': 8221.137031793594, 'accumulated_logging_time': 2.852015495300293, 'global_step': 55397, 'preemption_count': 0}), (56149, {'train/accuracy': 0.994035005569458, 'train/loss': 0.018236299976706505, 'train/mean_average_precision': 0.6705837973689833, 'validation/accuracy': 0.986273467540741, 'validation/loss': 0.05187169834971428, 'validation/mean_average_precision': 0.26658966401857015, 'validation/num_examples': 43793, 'test/accuracy': 0.9854514598846436, 'test/loss': 0.05483526363968849, 'test/mean_average_precision': 0.2518330540823449, 'test/num_examples': 43793, 'score': 18020.540464401245, 'total_duration': 26355.229289531708, 'accumulated_submission_time': 18020.540464401245, 'accumulated_eval_time': 8330.204983472824, 'accumulated_logging_time': 2.890192985534668, 'global_step': 56149, 'preemption_count': 0}), (56909, {'train/accuracy': 0.9949232339859009, 'train/loss': 0.015860287472605705, 'train/mean_average_precision': 0.7254194245765323, 'validation/accuracy': 0.9862621426582336, 'validation/loss': 0.05209961161017418, 'validation/mean_average_precision': 0.2709062672047048, 'validation/num_examples': 43793, 'test/accuracy': 0.9854662418365479, 'test/loss': 0.0550340972840786, 'test/mean_average_precision': 0.2515098676230641, 'test/num_examples': 43793, 'score': 18260.47955775261, 'total_duration': 26703.43917274475, 'accumulated_submission_time': 18260.47955775261, 'accumulated_eval_time': 8438.41262793541, 'accumulated_logging_time': 2.932584762573242, 'global_step': 56909, 'preemption_count': 0})], 'global_step': 57596}
I0205 20:52:24.495188 139919816816448 submission_runner.py:586] Timing: 18477.267731904984
I0205 20:52:24.495255 139919816816448 submission_runner.py:588] Total number of evals: 77
I0205 20:52:24.495306 139919816816448 submission_runner.py:589] ====================
I0205 20:52:24.495360 139919816816448 submission_runner.py:542] Using RNG seed 356686224
I0205 20:52:24.555637 139919816816448 submission_runner.py:551] --- Tuning run 4/5 ---
I0205 20:52:24.555785 139919816816448 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_1/ogbg_jax/trial_4.
I0205 20:52:24.563311 139919816816448 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_1/ogbg_jax/trial_4/hparams.json.
I0205 20:52:24.693705 139919816816448 submission_runner.py:206] Initializing dataset.
I0205 20:52:24.781935 139919816816448 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0205 20:52:24.787171 139919816816448 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0205 20:52:24.916905 139919816816448 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0205 20:52:24.954110 139919816816448 submission_runner.py:213] Initializing model.
I0205 20:52:27.302327 139919816816448 submission_runner.py:255] Initializing optimizer.
I0205 20:52:27.863313 139919816816448 submission_runner.py:262] Initializing metrics bundle.
I0205 20:52:27.863517 139919816816448 submission_runner.py:280] Initializing checkpoint and logger.
I0205 20:52:27.864213 139919816816448 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_1/ogbg_jax/trial_4 with prefix checkpoint_
I0205 20:52:27.864345 139919816816448 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_1/ogbg_jax/trial_4/meta_data_0.json.
I0205 20:52:27.864552 139919816816448 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0205 20:52:27.864614 139919816816448 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0205 20:52:29.818984 139919816816448 logger_utils.py:220] Unable to record git information. Continuing without it.
I0205 20:52:31.753340 139919816816448 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_1/ogbg_jax/trial_4/flags_0.json.
I0205 20:52:31.757614 139919816816448 submission_runner.py:314] Starting training loop.
I0205 20:52:43.671362 139698412009216 logging_writer.py:48] [0] global_step=0, grad_norm=2.5278704166412354, loss=0.726763129234314
I0205 20:52:43.681261 139919816816448 spec.py:321] Evaluating on the training split.
I0205 20:54:23.958856 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 20:54:27.000658 139919816816448 spec.py:349] Evaluating on the test split.
I0205 20:54:29.987014 139919816816448 submission_runner.py:408] Time since start: 118.23s, 	Step: 1, 	{'train/accuracy': 0.5323500037193298, 'train/loss': 0.7277898788452148, 'train/mean_average_precision': 0.022713738959025315, 'validation/accuracy': 0.5230699777603149, 'validation/loss': 0.7331878542900085, 'validation/mean_average_precision': 0.02551145948130271, 'validation/num_examples': 43793, 'test/accuracy': 0.521492063999176, 'test/loss': 0.7347381114959717, 'test/mean_average_precision': 0.026847203975459377, 'test/num_examples': 43793, 'score': 11.923594951629639, 'total_duration': 118.22934317588806, 'accumulated_submission_time': 11.923594951629639, 'accumulated_eval_time': 106.30569434165955, 'accumulated_logging_time': 0}
I0205 20:54:29.997194 139716992366336 logging_writer.py:48] [1] accumulated_eval_time=106.305694, accumulated_logging_time=0, accumulated_submission_time=11.923595, global_step=1, preemption_count=0, score=11.923595, test/accuracy=0.521492, test/loss=0.734738, test/mean_average_precision=0.026847, test/num_examples=43793, total_duration=118.229343, train/accuracy=0.532350, train/loss=0.727790, train/mean_average_precision=0.022714, validation/accuracy=0.523070, validation/loss=0.733188, validation/mean_average_precision=0.025511, validation/num_examples=43793
I0205 20:55:01.598988 139735302588160 logging_writer.py:48] [100] global_step=100, grad_norm=0.10748223960399628, loss=0.115016408264637
I0205 20:55:33.396507 139716992366336 logging_writer.py:48] [200] global_step=200, grad_norm=0.010978516191244125, loss=0.0653800219297409
I0205 20:56:05.180099 139735302588160 logging_writer.py:48] [300] global_step=300, grad_norm=0.01226961612701416, loss=0.05323926731944084
I0205 20:56:36.578010 139716992366336 logging_writer.py:48] [400] global_step=400, grad_norm=0.010108516551554203, loss=0.05428850278258324
I0205 20:57:08.437091 139735302588160 logging_writer.py:48] [500] global_step=500, grad_norm=0.014944052323698997, loss=0.05867155268788338
I0205 20:57:40.771670 139716992366336 logging_writer.py:48] [600] global_step=600, grad_norm=0.02147824876010418, loss=0.052838027477264404
I0205 20:58:12.841773 139735302588160 logging_writer.py:48] [700] global_step=700, grad_norm=0.013751588761806488, loss=0.05485368147492409
I0205 20:58:29.992748 139919816816448 spec.py:321] Evaluating on the training split.
I0205 21:00:12.308889 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 21:00:15.685073 139919816816448 spec.py:349] Evaluating on the test split.
I0205 21:00:19.010019 139919816816448 submission_runner.py:408] Time since start: 467.25s, 	Step: 756, 	{'train/accuracy': 0.9868640899658203, 'train/loss': 0.05360216647386551, 'train/mean_average_precision': 0.041551055168208686, 'validation/accuracy': 0.9841504096984863, 'validation/loss': 0.0639973059296608, 'validation/mean_average_precision': 0.04087225139175344, 'validation/num_examples': 43793, 'test/accuracy': 0.9831761717796326, 'test/loss': 0.06718604266643524, 'test/mean_average_precision': 0.042759928672415015, 'test/num_examples': 43793, 'score': 251.88759779930115, 'total_duration': 467.25233030319214, 'accumulated_submission_time': 251.88759779930115, 'accumulated_eval_time': 215.32289910316467, 'accumulated_logging_time': 0.02102208137512207}
I0205 21:00:19.027278 139697948911360 logging_writer.py:48] [756] accumulated_eval_time=215.322899, accumulated_logging_time=0.021022, accumulated_submission_time=251.887598, global_step=756, preemption_count=0, score=251.887598, test/accuracy=0.983176, test/loss=0.067186, test/mean_average_precision=0.042760, test/num_examples=43793, total_duration=467.252330, train/accuracy=0.986864, train/loss=0.053602, train/mean_average_precision=0.041551, validation/accuracy=0.984150, validation/loss=0.063997, validation/mean_average_precision=0.040872, validation/num_examples=43793
I0205 21:00:33.595873 139735693903616 logging_writer.py:48] [800] global_step=800, grad_norm=0.011231126263737679, loss=0.05441603064537048
I0205 21:01:06.264150 139697948911360 logging_writer.py:48] [900] global_step=900, grad_norm=0.006347251124680042, loss=0.05663815513253212
I0205 21:01:38.704937 139735693903616 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.0146557055413723, loss=0.05349218100309372
I0205 21:02:11.295284 139697948911360 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.02330232411623001, loss=0.051346272230148315
I0205 21:02:43.494389 139735693903616 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.010931453667581081, loss=0.046686165034770966
I0205 21:03:15.931879 139697948911360 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.006330680102109909, loss=0.054627444595098495
I0205 21:03:47.983230 139735693903616 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.0063886819407343864, loss=0.046706873923540115
I0205 21:04:19.265032 139919816816448 spec.py:321] Evaluating on the training split.
I0205 21:06:01.309535 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 21:06:04.316675 139919816816448 spec.py:349] Evaluating on the test split.
I0205 21:06:07.284094 139919816816448 submission_runner.py:408] Time since start: 815.53s, 	Step: 1500, 	{'train/accuracy': 0.9869927763938904, 'train/loss': 0.05052226781845093, 'train/mean_average_precision': 0.06667004980286692, 'validation/accuracy': 0.9842925071716309, 'validation/loss': 0.06098408252000809, 'validation/mean_average_precision': 0.06739874671941877, 'validation/num_examples': 43793, 'test/accuracy': 0.98328697681427, 'test/loss': 0.06431528925895691, 'test/mean_average_precision': 0.06560076879112647, 'test/num_examples': 43793, 'score': 492.0900263786316, 'total_duration': 815.5264096260071, 'accumulated_submission_time': 492.0900263786316, 'accumulated_eval_time': 323.3419051170349, 'accumulated_logging_time': 0.050023555755615234}
I0205 21:06:07.299528 139716992366336 logging_writer.py:48] [1500] accumulated_eval_time=323.341905, accumulated_logging_time=0.050024, accumulated_submission_time=492.090026, global_step=1500, preemption_count=0, score=492.090026, test/accuracy=0.983287, test/loss=0.064315, test/mean_average_precision=0.065601, test/num_examples=43793, total_duration=815.526410, train/accuracy=0.986993, train/loss=0.050522, train/mean_average_precision=0.066670, validation/accuracy=0.984293, validation/loss=0.060984, validation/mean_average_precision=0.067399, validation/num_examples=43793
I0205 21:06:07.629436 139735302588160 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.01148376613855362, loss=0.05689821019768715
I0205 21:06:39.330008 139716992366336 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.005824810825288296, loss=0.05003557354211807
I0205 21:07:11.168138 139735302588160 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.021564394235610962, loss=0.05770360305905342
I0205 21:07:42.958477 139716992366336 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.03016255609691143, loss=0.04750775918364525
I0205 21:08:14.671535 139735302588160 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.012138745747506618, loss=0.047873880714178085
I0205 21:08:46.323356 139716992366336 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.019660817459225655, loss=0.05158007889986038
I0205 21:09:18.484318 139735302588160 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.013670465908944607, loss=0.05025302618741989
I0205 21:09:50.656777 139716992366336 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.013291680254042149, loss=0.05331333726644516
I0205 21:10:07.603204 139919816816448 spec.py:321] Evaluating on the training split.
I0205 21:11:49.591137 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 21:11:52.697427 139919816816448 spec.py:349] Evaluating on the test split.
I0205 21:11:55.695784 139919816816448 submission_runner.py:408] Time since start: 1163.94s, 	Step: 2254, 	{'train/accuracy': 0.9870300889015198, 'train/loss': 0.04844581335783005, 'train/mean_average_precision': 0.08347689953276526, 'validation/accuracy': 0.9843335151672363, 'validation/loss': 0.058096155524253845, 'validation/mean_average_precision': 0.07764661735126936, 'validation/num_examples': 43793, 'test/accuracy': 0.9833593964576721, 'test/loss': 0.061391979455947876, 'test/mean_average_precision': 0.0773755137544642, 'test/num_examples': 43793, 'score': 732.36257147789, 'total_duration': 1163.9381144046783, 'accumulated_submission_time': 732.36257147789, 'accumulated_eval_time': 431.4344410896301, 'accumulated_logging_time': 0.07634162902832031}
I0205 21:11:55.711298 139735693903616 logging_writer.py:48] [2254] accumulated_eval_time=431.434441, accumulated_logging_time=0.076342, accumulated_submission_time=732.362571, global_step=2254, preemption_count=0, score=732.362571, test/accuracy=0.983359, test/loss=0.061392, test/mean_average_precision=0.077376, test/num_examples=43793, total_duration=1163.938114, train/accuracy=0.987030, train/loss=0.048446, train/mean_average_precision=0.083477, validation/accuracy=0.984334, validation/loss=0.058096, validation/mean_average_precision=0.077647, validation/num_examples=43793
I0205 21:12:10.726296 139739177916160 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.01421270240098238, loss=0.05502523109316826
I0205 21:12:42.483691 139735693903616 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.006373994518071413, loss=0.045601747930049896
I0205 21:13:14.390814 139739177916160 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.02687072567641735, loss=0.04948367178440094
I0205 21:13:45.739221 139735693903616 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.008485310710966587, loss=0.04296081140637398
I0205 21:14:17.456607 139739177916160 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.02185082621872425, loss=0.04490905627608299
I0205 21:14:49.398262 139735693903616 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.019387802109122276, loss=0.046778272837400436
I0205 21:15:21.231283 139739177916160 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.019445396959781647, loss=0.051450904458761215
I0205 21:15:52.518093 139735693903616 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.013287820853292942, loss=0.04780646413564682
I0205 21:15:56.004353 139919816816448 spec.py:321] Evaluating on the training split.
I0205 21:17:31.049940 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 21:17:34.158571 139919816816448 spec.py:349] Evaluating on the test split.
I0205 21:17:37.223054 139919816816448 submission_runner.py:408] Time since start: 1505.47s, 	Step: 3012, 	{'train/accuracy': 0.9873122572898865, 'train/loss': 0.046275608241558075, 'train/mean_average_precision': 0.10629623996906212, 'validation/accuracy': 0.9846282005310059, 'validation/loss': 0.056142181158065796, 'validation/mean_average_precision': 0.09584960419535594, 'validation/num_examples': 43793, 'test/accuracy': 0.9836963415145874, 'test/loss': 0.05957218259572983, 'test/mean_average_precision': 0.09474019072285637, 'test/num_examples': 43793, 'score': 972.623060464859, 'total_duration': 1505.4653851985931, 'accumulated_submission_time': 972.623060464859, 'accumulated_eval_time': 532.6530933380127, 'accumulated_logging_time': 0.10355210304260254}
I0205 21:17:37.238796 139697948911360 logging_writer.py:48] [3012] accumulated_eval_time=532.653093, accumulated_logging_time=0.103552, accumulated_submission_time=972.623060, global_step=3012, preemption_count=0, score=972.623060, test/accuracy=0.983696, test/loss=0.059572, test/mean_average_precision=0.094740, test/num_examples=43793, total_duration=1505.465385, train/accuracy=0.987312, train/loss=0.046276, train/mean_average_precision=0.106296, validation/accuracy=0.984628, validation/loss=0.056142, validation/mean_average_precision=0.095850, validation/num_examples=43793
I0205 21:18:05.742131 139735302588160 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.03615168109536171, loss=0.04621534049510956
I0205 21:18:37.967605 139697948911360 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.021108198910951614, loss=0.04528379440307617
I0205 21:19:09.672792 139735302588160 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.01606880873441696, loss=0.04380287230014801
I0205 21:19:41.338494 139697948911360 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.014342737384140491, loss=0.041838858276605606
I0205 21:20:13.161471 139735302588160 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.035262931138277054, loss=0.04376889392733574
I0205 21:20:44.660521 139697948911360 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.03311483934521675, loss=0.043253324925899506
I0205 21:21:16.501676 139735302588160 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.013933018781244755, loss=0.04469903185963631
I0205 21:21:37.410804 139919816816448 spec.py:321] Evaluating on the training split.
I0205 21:23:20.739983 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 21:23:23.889396 139919816816448 spec.py:349] Evaluating on the test split.
I0205 21:23:27.018617 139919816816448 submission_runner.py:408] Time since start: 1855.26s, 	Step: 3767, 	{'train/accuracy': 0.987439751625061, 'train/loss': 0.045254599303007126, 'train/mean_average_precision': 0.12752641015774666, 'validation/accuracy': 0.9846375584602356, 'validation/loss': 0.05470998212695122, 'validation/mean_average_precision': 0.1125843895014612, 'validation/num_examples': 43793, 'test/accuracy': 0.9836066365242004, 'test/loss': 0.05755944922566414, 'test/mean_average_precision': 0.11312817246859927, 'test/num_examples': 43793, 'score': 1212.7634572982788, 'total_duration': 1855.2609441280365, 'accumulated_submission_time': 1212.7634572982788, 'accumulated_eval_time': 642.2608568668365, 'accumulated_logging_time': 0.1302659511566162}
I0205 21:23:27.033941 139716992366336 logging_writer.py:48] [3767] accumulated_eval_time=642.260857, accumulated_logging_time=0.130266, accumulated_submission_time=1212.763457, global_step=3767, preemption_count=0, score=1212.763457, test/accuracy=0.983607, test/loss=0.057559, test/mean_average_precision=0.113128, test/num_examples=43793, total_duration=1855.260944, train/accuracy=0.987440, train/loss=0.045255, train/mean_average_precision=0.127526, validation/accuracy=0.984638, validation/loss=0.054710, validation/mean_average_precision=0.112584, validation/num_examples=43793
I0205 21:23:37.839348 139739177916160 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.02483564056456089, loss=0.03968667984008789
I0205 21:24:09.521482 139716992366336 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.04411446303129196, loss=0.0423426479101181
I0205 21:24:41.249007 139739177916160 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.018314210698008537, loss=0.046030133962631226
I0205 21:25:13.093409 139716992366336 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.03753167390823364, loss=0.04381892830133438
I0205 21:25:44.835329 139739177916160 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.026491498574614525, loss=0.03949413821101189
I0205 21:26:16.615459 139716992366336 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.028455862775444984, loss=0.04409347102046013
I0205 21:26:48.251677 139739177916160 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.017688853666186333, loss=0.0405285619199276
I0205 21:27:19.892810 139716992366336 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.03326352685689926, loss=0.04286381974816322
I0205 21:27:27.062019 139919816816448 spec.py:321] Evaluating on the training split.
I0205 21:29:02.117548 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 21:29:05.237663 139919816816448 spec.py:349] Evaluating on the test split.
I0205 21:29:08.335826 139919816816448 submission_runner.py:408] Time since start: 2196.58s, 	Step: 4524, 	{'train/accuracy': 0.9877470135688782, 'train/loss': 0.0434085913002491, 'train/mean_average_precision': 0.14673611209759083, 'validation/accuracy': 0.9848189949989319, 'validation/loss': 0.052754007279872894, 'validation/mean_average_precision': 0.12317377579610293, 'validation/num_examples': 43793, 'test/accuracy': 0.9839351773262024, 'test/loss': 0.05557785555720329, 'test/mean_average_precision': 0.12852256187186392, 'test/num_examples': 43793, 'score': 1452.7605681419373, 'total_duration': 2196.578159570694, 'accumulated_submission_time': 1452.7605681419373, 'accumulated_eval_time': 743.5346171855927, 'accumulated_logging_time': 0.1563253402709961}
I0205 21:29:08.351307 139697948911360 logging_writer.py:48] [4524] accumulated_eval_time=743.534617, accumulated_logging_time=0.156325, accumulated_submission_time=1452.760568, global_step=4524, preemption_count=0, score=1452.760568, test/accuracy=0.983935, test/loss=0.055578, test/mean_average_precision=0.128523, test/num_examples=43793, total_duration=2196.578160, train/accuracy=0.987747, train/loss=0.043409, train/mean_average_precision=0.146736, validation/accuracy=0.984819, validation/loss=0.052754, validation/mean_average_precision=0.123174, validation/num_examples=43793
I0205 21:29:32.762338 139735302588160 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.03394957259297371, loss=0.04764240235090256
I0205 21:30:04.425570 139697948911360 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.029257608577609062, loss=0.038654278963804245
I0205 21:30:36.955539 139735302588160 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.03397557884454727, loss=0.042939890176057816
I0205 21:31:09.565926 139697948911360 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.05284556746482849, loss=0.040246881544589996
I0205 21:31:41.981897 139735302588160 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.03175799176096916, loss=0.043746039271354675
I0205 21:32:14.373493 139697948911360 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.021604225039482117, loss=0.039813410490751266
I0205 21:32:46.464535 139735302588160 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.04821954295039177, loss=0.04339844733476639
I0205 21:33:08.524161 139919816816448 spec.py:321] Evaluating on the training split.
I0205 21:34:47.650721 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 21:34:50.698829 139919816816448 spec.py:349] Evaluating on the test split.
I0205 21:34:53.650035 139919816816448 submission_runner.py:408] Time since start: 2541.89s, 	Step: 5270, 	{'train/accuracy': 0.9880778789520264, 'train/loss': 0.04199328273534775, 'train/mean_average_precision': 0.16621979128001976, 'validation/accuracy': 0.9851506352424622, 'validation/loss': 0.05195803567767143, 'validation/mean_average_precision': 0.13905560615272636, 'validation/num_examples': 43793, 'test/accuracy': 0.9842110872268677, 'test/loss': 0.05524878203868866, 'test/mean_average_precision': 0.13999589254461053, 'test/num_examples': 43793, 'score': 1692.9004187583923, 'total_duration': 2541.892335653305, 'accumulated_submission_time': 1692.9004187583923, 'accumulated_eval_time': 848.6604132652283, 'accumulated_logging_time': 0.18234682083129883}
I0205 21:34:53.666009 139716992366336 logging_writer.py:48] [5270] accumulated_eval_time=848.660413, accumulated_logging_time=0.182347, accumulated_submission_time=1692.900419, global_step=5270, preemption_count=0, score=1692.900419, test/accuracy=0.984211, test/loss=0.055249, test/mean_average_precision=0.139996, test/num_examples=43793, total_duration=2541.892336, train/accuracy=0.988078, train/loss=0.041993, train/mean_average_precision=0.166220, validation/accuracy=0.985151, validation/loss=0.051958, validation/mean_average_precision=0.139056, validation/num_examples=43793
I0205 21:35:03.477632 139735693903616 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.03460470214486122, loss=0.041246287524700165
I0205 21:35:34.868831 139716992366336 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.06636001169681549, loss=0.04836738854646683
I0205 21:36:06.850320 139735693903616 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.0861210897564888, loss=0.04623101279139519
I0205 21:36:38.185919 139716992366336 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.03219659626483917, loss=0.0438905693590641
I0205 21:37:09.851007 139735693903616 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.03048357181251049, loss=0.03562520071864128
I0205 21:37:41.656810 139716992366336 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.05550220236182213, loss=0.04196762666106224
I0205 21:38:13.123440 139735693903616 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.06680645048618317, loss=0.03901989385485649
I0205 21:38:44.862401 139716992366336 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.0544784776866436, loss=0.03764974698424339
I0205 21:38:53.654153 139919816816448 spec.py:321] Evaluating on the training split.
I0205 21:40:30.205188 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 21:40:33.205418 139919816816448 spec.py:349] Evaluating on the test split.
I0205 21:40:36.214380 139919816816448 submission_runner.py:408] Time since start: 2884.46s, 	Step: 6029, 	{'train/accuracy': 0.9879913330078125, 'train/loss': 0.0421731173992157, 'train/mean_average_precision': 0.1533392112329769, 'validation/accuracy': 0.9850727319717407, 'validation/loss': 0.0521065816283226, 'validation/mean_average_precision': 0.1399027724744271, 'validation/num_examples': 43793, 'test/accuracy': 0.9840817451477051, 'test/loss': 0.055003903806209564, 'test/mean_average_precision': 0.13523668927573052, 'test/num_examples': 43793, 'score': 1932.8552432060242, 'total_duration': 2884.456712245941, 'accumulated_submission_time': 1932.8552432060242, 'accumulated_eval_time': 951.2205955982208, 'accumulated_logging_time': 0.2111051082611084}
I0205 21:40:36.230683 139697948911360 logging_writer.py:48] [6029] accumulated_eval_time=951.220596, accumulated_logging_time=0.211105, accumulated_submission_time=1932.855243, global_step=6029, preemption_count=0, score=1932.855243, test/accuracy=0.984082, test/loss=0.055004, test/mean_average_precision=0.135237, test/num_examples=43793, total_duration=2884.456712, train/accuracy=0.987991, train/loss=0.042173, train/mean_average_precision=0.153339, validation/accuracy=0.985073, validation/loss=0.052107, validation/mean_average_precision=0.139903, validation/num_examples=43793
I0205 21:40:59.436154 139735302588160 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.044944070279598236, loss=0.035070840269327164
I0205 21:41:31.468669 139697948911360 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.021888071671128273, loss=0.040150389075279236
I0205 21:42:03.040971 139735302588160 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.03524165228009224, loss=0.039748262614011765
I0205 21:42:35.178508 139697948911360 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.050332434475421906, loss=0.04230017960071564
I0205 21:43:07.373540 139735302588160 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.10453573614358902, loss=0.04338297247886658
I0205 21:43:38.828717 139697948911360 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.03525581955909729, loss=0.03794266656041145
I0205 21:44:10.376935 139735302588160 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.06865743547677994, loss=0.04408276081085205
I0205 21:44:36.475429 139919816816448 spec.py:321] Evaluating on the training split.
I0205 21:46:14.459635 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 21:46:17.548619 139919816816448 spec.py:349] Evaluating on the test split.
I0205 21:46:20.521350 139919816816448 submission_runner.py:408] Time since start: 3228.76s, 	Step: 6784, 	{'train/accuracy': 0.9880300164222717, 'train/loss': 0.04225614294409752, 'train/mean_average_precision': 0.16079885263891208, 'validation/accuracy': 0.9850122332572937, 'validation/loss': 0.05152441933751106, 'validation/mean_average_precision': 0.14707804685112305, 'validation/num_examples': 43793, 'test/accuracy': 0.9840371012687683, 'test/loss': 0.05428755655884743, 'test/mean_average_precision': 0.14234278442446044, 'test/num_examples': 43793, 'score': 2173.0677292346954, 'total_duration': 3228.763679265976, 'accumulated_submission_time': 2173.0677292346954, 'accumulated_eval_time': 1055.2664711475372, 'accumulated_logging_time': 0.23958039283752441}
I0205 21:46:20.537966 139759004710656 logging_writer.py:48] [6784] accumulated_eval_time=1055.266471, accumulated_logging_time=0.239580, accumulated_submission_time=2173.067729, global_step=6784, preemption_count=0, score=2173.067729, test/accuracy=0.984037, test/loss=0.054288, test/mean_average_precision=0.142343, test/num_examples=43793, total_duration=3228.763679, train/accuracy=0.988030, train/loss=0.042256, train/mean_average_precision=0.160799, validation/accuracy=0.985012, validation/loss=0.051524, validation/mean_average_precision=0.147078, validation/num_examples=43793
I0205 21:46:25.971642 139857145886464 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.04075422137975693, loss=0.03926489129662514
I0205 21:46:57.555863 139759004710656 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.066859170794487, loss=0.04112907126545906
I0205 21:47:29.261795 139857145886464 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.038337789475917816, loss=0.040522970259189606
I0205 21:48:01.302511 139759004710656 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.02694527804851532, loss=0.04519098624587059
I0205 21:48:32.845895 139857145886464 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.062098823487758636, loss=0.04043610021471977
I0205 21:49:04.253192 139759004710656 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.025642115622758865, loss=0.03925371542572975
I0205 21:49:35.625030 139857145886464 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.059465497732162476, loss=0.044803451746702194
I0205 21:50:07.250772 139759004710656 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.03055110201239586, loss=0.0441678948700428
I0205 21:50:20.598086 139919816816448 spec.py:321] Evaluating on the training split.
I0205 21:51:59.315151 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 21:52:02.725146 139919816816448 spec.py:349] Evaluating on the test split.
I0205 21:52:05.797589 139919816816448 submission_runner.py:408] Time since start: 3574.04s, 	Step: 7543, 	{'train/accuracy': 0.9881213903427124, 'train/loss': 0.04118350148200989, 'train/mean_average_precision': 0.16851221433698832, 'validation/accuracy': 0.9851640462875366, 'validation/loss': 0.05082281306385994, 'validation/mean_average_precision': 0.1580675882901569, 'validation/num_examples': 43793, 'test/accuracy': 0.984203040599823, 'test/loss': 0.05370954051613808, 'test/mean_average_precision': 0.15149005069847016, 'test/num_examples': 43793, 'score': 2413.096028804779, 'total_duration': 3574.0399181842804, 'accumulated_submission_time': 2413.096028804779, 'accumulated_eval_time': 1160.4659247398376, 'accumulated_logging_time': 0.26740050315856934}
I0205 21:52:05.816651 139716992366336 logging_writer.py:48] [7543] accumulated_eval_time=1160.465925, accumulated_logging_time=0.267401, accumulated_submission_time=2413.096029, global_step=7543, preemption_count=0, score=2413.096029, test/accuracy=0.984203, test/loss=0.053710, test/mean_average_precision=0.151490, test/num_examples=43793, total_duration=3574.039918, train/accuracy=0.988121, train/loss=0.041184, train/mean_average_precision=0.168512, validation/accuracy=0.985164, validation/loss=0.050823, validation/mean_average_precision=0.158068, validation/num_examples=43793
I0205 21:52:24.561029 139739177916160 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.04207068681716919, loss=0.04182592034339905
I0205 21:52:56.300294 139716992366336 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.06671467423439026, loss=0.03872273117303848
I0205 21:53:27.846296 139739177916160 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.057508159428834915, loss=0.040170423686504364
I0205 21:54:00.706800 139716992366336 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.04798189550638199, loss=0.037464533001184464
I0205 21:54:32.246385 139739177916160 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.06513553112745285, loss=0.04501741752028465
I0205 21:55:03.996966 139716992366336 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.07201509177684784, loss=0.0406331904232502
I0205 21:55:35.587993 139739177916160 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.028018850833177567, loss=0.035769298672676086
I0205 21:56:06.041548 139919816816448 spec.py:321] Evaluating on the training split.
I0205 21:57:41.793411 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 21:57:47.013223 139919816816448 spec.py:349] Evaluating on the test split.
I0205 21:57:49.953891 139919816816448 submission_runner.py:408] Time since start: 3918.20s, 	Step: 8297, 	{'train/accuracy': 0.9882885813713074, 'train/loss': 0.040482938289642334, 'train/mean_average_precision': 0.18335730076382278, 'validation/accuracy': 0.9854526519775391, 'validation/loss': 0.04996418207883835, 'validation/mean_average_precision': 0.16653534896192793, 'validation/num_examples': 43793, 'test/accuracy': 0.9844747185707092, 'test/loss': 0.05280674248933792, 'test/mean_average_precision': 0.16006408266821517, 'test/num_examples': 43793, 'score': 2653.2899854183197, 'total_duration': 3918.1962225437164, 'accumulated_submission_time': 2653.2899854183197, 'accumulated_eval_time': 1264.378232717514, 'accumulated_logging_time': 0.2971460819244385}
I0205 21:57:49.970705 139735693903616 logging_writer.py:48] [8297] accumulated_eval_time=1264.378233, accumulated_logging_time=0.297146, accumulated_submission_time=2653.289985, global_step=8297, preemption_count=0, score=2653.289985, test/accuracy=0.984475, test/loss=0.052807, test/mean_average_precision=0.160064, test/num_examples=43793, total_duration=3918.196223, train/accuracy=0.988289, train/loss=0.040483, train/mean_average_precision=0.183357, validation/accuracy=0.985453, validation/loss=0.049964, validation/mean_average_precision=0.166535, validation/num_examples=43793
I0205 21:57:51.305652 139857145886464 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.03978676721453667, loss=0.040006935596466064
I0205 21:58:22.891822 139735693903616 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.0382419154047966, loss=0.04124292731285095
I0205 21:58:54.488665 139857145886464 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.07470076531171799, loss=0.039110876619815826
I0205 21:59:26.029797 139735693903616 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.07920264452695847, loss=0.04006721079349518
I0205 21:59:57.220010 139857145886464 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.0439501516520977, loss=0.039451535791158676
I0205 22:00:28.824667 139735693903616 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.05166052654385567, loss=0.04347378388047218
I0205 22:01:00.327647 139857145886464 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.04248189553618431, loss=0.04138762876391411
I0205 22:01:32.168936 139735693903616 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.04339389130473137, loss=0.0453033484518528
I0205 22:01:50.227828 139919816816448 spec.py:321] Evaluating on the training split.
I0205 22:03:26.663274 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 22:03:29.754376 139919816816448 spec.py:349] Evaluating on the test split.
I0205 22:03:32.718140 139919816816448 submission_runner.py:408] Time since start: 4260.96s, 	Step: 9059, 	{'train/accuracy': 0.9884369373321533, 'train/loss': 0.040347833186388016, 'train/mean_average_precision': 0.18804448035695187, 'validation/accuracy': 0.9853227734565735, 'validation/loss': 0.04992473125457764, 'validation/mean_average_precision': 0.15702354491908133, 'validation/num_examples': 43793, 'test/accuracy': 0.9844157695770264, 'test/loss': 0.05274470895528793, 'test/mean_average_precision': 0.1588039709738697, 'test/num_examples': 43793, 'score': 2893.515405654907, 'total_duration': 4260.960469484329, 'accumulated_submission_time': 2893.515405654907, 'accumulated_eval_time': 1366.8684968948364, 'accumulated_logging_time': 0.3246288299560547}
I0205 22:03:32.735737 139735302588160 logging_writer.py:48] [9059] accumulated_eval_time=1366.868497, accumulated_logging_time=0.324629, accumulated_submission_time=2893.515406, global_step=9059, preemption_count=0, score=2893.515406, test/accuracy=0.984416, test/loss=0.052745, test/mean_average_precision=0.158804, test/num_examples=43793, total_duration=4260.960469, train/accuracy=0.988437, train/loss=0.040348, train/mean_average_precision=0.188044, validation/accuracy=0.985323, validation/loss=0.049925, validation/mean_average_precision=0.157024, validation/num_examples=43793
I0205 22:03:45.870580 139739177916160 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.06154673546552658, loss=0.040987882763147354
I0205 22:04:17.422511 139735302588160 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.07280167192220688, loss=0.04056276008486748
I0205 22:04:49.246590 139739177916160 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.040158338844776154, loss=0.042301591485738754
I0205 22:05:20.891481 139735302588160 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.06031457334756851, loss=0.04286801069974899
I0205 22:05:52.486881 139739177916160 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.029076242819428444, loss=0.0375581830739975
I0205 22:06:23.782470 139735302588160 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.05944521352648735, loss=0.0339985229074955
I0205 22:06:55.504144 139739177916160 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.028657346963882446, loss=0.041155196726322174
I0205 22:07:27.326684 139735302588160 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.05603833869099617, loss=0.043246638029813766
I0205 22:07:32.735264 139919816816448 spec.py:321] Evaluating on the training split.
I0205 22:09:11.535141 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 22:09:14.566631 139919816816448 spec.py:349] Evaluating on the test split.
I0205 22:09:17.552263 139919816816448 submission_runner.py:408] Time since start: 4605.79s, 	Step: 9818, 	{'train/accuracy': 0.988334596157074, 'train/loss': 0.04056818038225174, 'train/mean_average_precision': 0.17742767170688034, 'validation/accuracy': 0.9853284358978271, 'validation/loss': 0.05031628534197807, 'validation/mean_average_precision': 0.1624657584398002, 'validation/num_examples': 43793, 'test/accuracy': 0.984444797039032, 'test/loss': 0.053260620683431625, 'test/mean_average_precision': 0.15851285429824236, 'test/num_examples': 43793, 'score': 3133.483088493347, 'total_duration': 4605.794593811035, 'accumulated_submission_time': 3133.483088493347, 'accumulated_eval_time': 1471.6854536533356, 'accumulated_logging_time': 0.35410261154174805}
I0205 22:09:17.569295 139759004710656 logging_writer.py:48] [9818] accumulated_eval_time=1471.685454, accumulated_logging_time=0.354103, accumulated_submission_time=3133.483088, global_step=9818, preemption_count=0, score=3133.483088, test/accuracy=0.984445, test/loss=0.053261, test/mean_average_precision=0.158513, test/num_examples=43793, total_duration=4605.794594, train/accuracy=0.988335, train/loss=0.040568, train/mean_average_precision=0.177428, validation/accuracy=0.985328, validation/loss=0.050316, validation/mean_average_precision=0.162466, validation/num_examples=43793
I0205 22:09:43.697021 139857145886464 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.06191647797822952, loss=0.045171402394771576
I0205 22:10:15.258230 139759004710656 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.043965332210063934, loss=0.03704206272959709
I0205 22:10:46.583873 139857145886464 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.0925295501947403, loss=0.04149620234966278
I0205 22:11:18.062678 139759004710656 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.07069133967161179, loss=0.03918072208762169
I0205 22:11:49.985924 139857145886464 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.05538509786128998, loss=0.044752735644578934
I0205 22:12:21.704597 139759004710656 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.06131534278392792, loss=0.03916308656334877
I0205 22:12:53.113221 139857145886464 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.031395845115184784, loss=0.04444325715303421
I0205 22:13:17.758175 139919816816448 spec.py:321] Evaluating on the training split.
I0205 22:14:55.056228 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 22:14:58.098869 139919816816448 spec.py:349] Evaluating on the test split.
I0205 22:15:01.154348 139919816816448 submission_runner.py:408] Time since start: 4949.40s, 	Step: 10579, 	{'train/accuracy': 0.9884284138679504, 'train/loss': 0.03996816277503967, 'train/mean_average_precision': 0.187111713207441, 'validation/accuracy': 0.985492467880249, 'validation/loss': 0.049749307334423065, 'validation/mean_average_precision': 0.16473999486327945, 'validation/num_examples': 43793, 'test/accuracy': 0.9845644235610962, 'test/loss': 0.052593156695365906, 'test/mean_average_precision': 0.16523719161665631, 'test/num_examples': 43793, 'score': 3373.640467405319, 'total_duration': 4949.39643740654, 'accumulated_submission_time': 3373.640467405319, 'accumulated_eval_time': 1575.0813403129578, 'accumulated_logging_time': 0.38223719596862793}
I0205 22:15:01.186138 139735693903616 logging_writer.py:48] [10579] accumulated_eval_time=1575.081340, accumulated_logging_time=0.382237, accumulated_submission_time=3373.640467, global_step=10579, preemption_count=0, score=3373.640467, test/accuracy=0.984564, test/loss=0.052593, test/mean_average_precision=0.165237, test/num_examples=43793, total_duration=4949.396437, train/accuracy=0.988428, train/loss=0.039968, train/mean_average_precision=0.187112, validation/accuracy=0.985492, validation/loss=0.049749, validation/mean_average_precision=0.164740, validation/num_examples=43793
I0205 22:15:08.181582 139739177916160 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.037198085337877274, loss=0.03952769190073013
I0205 22:15:39.829211 139735693903616 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.0454305000603199, loss=0.0449657142162323
I0205 22:16:11.660608 139739177916160 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.05802018567919731, loss=0.03941768407821655
I0205 22:16:43.197309 139735693903616 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.05485261604189873, loss=0.042049895972013474
I0205 22:17:15.089487 139739177916160 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.033909980207681656, loss=0.0435020811855793
I0205 22:17:46.869050 139735693903616 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.07543670386075974, loss=0.04049213230609894
I0205 22:18:18.528463 139739177916160 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.03787475451827049, loss=0.039084918797016144
I0205 22:18:50.125529 139735693903616 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.04957817867398262, loss=0.04119040071964264
I0205 22:19:01.275988 139919816816448 spec.py:321] Evaluating on the training split.
I0205 22:20:36.768904 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 22:20:39.796206 139919816816448 spec.py:349] Evaluating on the test split.
I0205 22:20:42.780640 139919816816448 submission_runner.py:408] Time since start: 5291.02s, 	Step: 11336, 	{'train/accuracy': 0.988300621509552, 'train/loss': 0.04015148803591728, 'train/mean_average_precision': 0.19675313600423047, 'validation/accuracy': 0.9854080080986023, 'validation/loss': 0.04994430020451546, 'validation/mean_average_precision': 0.17165726892233704, 'validation/num_examples': 43793, 'test/accuracy': 0.9844300746917725, 'test/loss': 0.0530269481241703, 'test/mean_average_precision': 0.1644090315504303, 'test/num_examples': 43793, 'score': 3613.6902854442596, 'total_duration': 5291.022964477539, 'accumulated_submission_time': 3613.6902854442596, 'accumulated_eval_time': 1676.585940361023, 'accumulated_logging_time': 0.4333629608154297}
I0205 22:20:42.798866 139759004710656 logging_writer.py:48] [11336] accumulated_eval_time=1676.585940, accumulated_logging_time=0.433363, accumulated_submission_time=3613.690285, global_step=11336, preemption_count=0, score=3613.690285, test/accuracy=0.984430, test/loss=0.053027, test/mean_average_precision=0.164409, test/num_examples=43793, total_duration=5291.022964, train/accuracy=0.988301, train/loss=0.040151, train/mean_average_precision=0.196753, validation/accuracy=0.985408, validation/loss=0.049944, validation/mean_average_precision=0.171657, validation/num_examples=43793
I0205 22:21:03.299907 139857145886464 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.023367078974843025, loss=0.03679749742150307
I0205 22:21:34.820905 139759004710656 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.02936762385070324, loss=0.039626944810152054
I0205 22:22:06.521672 139857145886464 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.03371582552790642, loss=0.03893871605396271
I0205 22:22:38.105494 139759004710656 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.05940669775009155, loss=0.04076424613595009
I0205 22:23:09.468397 139857145886464 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.028220251202583313, loss=0.03664300963282585
I0205 22:23:40.645102 139759004710656 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.04987907409667969, loss=0.04292362183332443
I0205 22:24:12.001722 139857145886464 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.078742116689682, loss=0.04182148352265358
I0205 22:24:42.906890 139919816816448 spec.py:321] Evaluating on the training split.
I0205 22:26:23.457279 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 22:26:26.503382 139919816816448 spec.py:349] Evaluating on the test split.
I0205 22:26:29.490682 139919816816448 submission_runner.py:408] Time since start: 5637.73s, 	Step: 12099, 	{'train/accuracy': 0.9884596467018127, 'train/loss': 0.03974561765789986, 'train/mean_average_precision': 0.19346122641698732, 'validation/accuracy': 0.9854969382286072, 'validation/loss': 0.04996122419834137, 'validation/mean_average_precision': 0.16202028624192666, 'validation/num_examples': 43793, 'test/accuracy': 0.9845282435417175, 'test/loss': 0.053220633417367935, 'test/mean_average_precision': 0.16265730998179825, 'test/num_examples': 43793, 'score': 3853.765954732895, 'total_duration': 5637.733014345169, 'accumulated_submission_time': 3853.765954732895, 'accumulated_eval_time': 1783.1696891784668, 'accumulated_logging_time': 0.4635937213897705}
I0205 22:26:29.508390 139735693903616 logging_writer.py:48] [12099] accumulated_eval_time=1783.169689, accumulated_logging_time=0.463594, accumulated_submission_time=3853.765955, global_step=12099, preemption_count=0, score=3853.765955, test/accuracy=0.984528, test/loss=0.053221, test/mean_average_precision=0.162657, test/num_examples=43793, total_duration=5637.733014, train/accuracy=0.988460, train/loss=0.039746, train/mean_average_precision=0.193461, validation/accuracy=0.985497, validation/loss=0.049961, validation/mean_average_precision=0.162020, validation/num_examples=43793
I0205 22:26:30.173852 139739177916160 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.0672198086977005, loss=0.042585305869579315
I0205 22:27:02.019236 139735693903616 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.09700308740139008, loss=0.041841741651296616
I0205 22:27:33.662378 139739177916160 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.05722266435623169, loss=0.03655582293868065
I0205 22:28:05.084543 139735693903616 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.04436425492167473, loss=0.03740023449063301
I0205 22:28:37.076005 139739177916160 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.059952981770038605, loss=0.03908834233880043
I0205 22:29:08.637816 139735693903616 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.03714330866932869, loss=0.04244212806224823
I0205 22:29:40.279539 139739177916160 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.05256642773747444, loss=0.04031897708773613
I0205 22:30:11.723602 139735693903616 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.14137381315231323, loss=0.04180371016263962
I0205 22:30:29.539682 139919816816448 spec.py:321] Evaluating on the training split.
I0205 22:32:08.521430 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 22:32:11.610616 139919816816448 spec.py:349] Evaluating on the test split.
I0205 22:32:14.686288 139919816816448 submission_runner.py:408] Time since start: 5982.93s, 	Step: 12858, 	{'train/accuracy': 0.9882970452308655, 'train/loss': 0.04016558453440666, 'train/mean_average_precision': 0.20592343113745598, 'validation/accuracy': 0.9854335784912109, 'validation/loss': 0.05081567540764809, 'validation/mean_average_precision': 0.1689011888219761, 'validation/num_examples': 43793, 'test/accuracy': 0.9844393730163574, 'test/loss': 0.05402006208896637, 'test/mean_average_precision': 0.16800944106961194, 'test/num_examples': 43793, 'score': 4093.766298055649, 'total_duration': 5982.928592443466, 'accumulated_submission_time': 4093.766298055649, 'accumulated_eval_time': 1888.3162214756012, 'accumulated_logging_time': 0.492189884185791}
I0205 22:32:14.703968 139735302588160 logging_writer.py:48] [12858] accumulated_eval_time=1888.316221, accumulated_logging_time=0.492190, accumulated_submission_time=4093.766298, global_step=12858, preemption_count=0, score=4093.766298, test/accuracy=0.984439, test/loss=0.054020, test/mean_average_precision=0.168009, test/num_examples=43793, total_duration=5982.928592, train/accuracy=0.988297, train/loss=0.040166, train/mean_average_precision=0.205923, validation/accuracy=0.985434, validation/loss=0.050816, validation/mean_average_precision=0.168901, validation/num_examples=43793
I0205 22:32:28.329139 139759004710656 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.11029747873544693, loss=0.03724459186196327
I0205 22:33:00.213626 139735302588160 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.048689521849155426, loss=0.037885405123233795
I0205 22:33:31.707715 139759004710656 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.07568921893835068, loss=0.04397061467170715
I0205 22:34:03.495020 139735302588160 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.12521684169769287, loss=0.03881309553980827
I0205 22:34:34.779432 139759004710656 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.05193573981523514, loss=0.036687303334474564
I0205 22:35:06.059138 139735302588160 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.05549100041389465, loss=0.04746113717556
I0205 22:35:37.831979 139759004710656 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.0640636533498764, loss=0.03715435788035393
I0205 22:36:09.283486 139735302588160 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.04550876095890999, loss=0.038945723325014114
I0205 22:36:14.976573 139919816816448 spec.py:321] Evaluating on the training split.
I0205 22:37:56.907692 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 22:38:00.270467 139919816816448 spec.py:349] Evaluating on the test split.
I0205 22:38:03.692300 139919816816448 submission_runner.py:408] Time since start: 6331.93s, 	Step: 13619, 	{'train/accuracy': 0.9884986281394958, 'train/loss': 0.03974822536110878, 'train/mean_average_precision': 0.2073664034633914, 'validation/accuracy': 0.9854303598403931, 'validation/loss': 0.04989549517631531, 'validation/mean_average_precision': 0.16710760332893343, 'validation/num_examples': 43793, 'test/accuracy': 0.9844822883605957, 'test/loss': 0.052785005420446396, 'test/mean_average_precision': 0.1698031474244022, 'test/num_examples': 43793, 'score': 4334.008366107941, 'total_duration': 6331.93460059166, 'accumulated_submission_time': 4334.008366107941, 'accumulated_eval_time': 1997.0318686962128, 'accumulated_logging_time': 0.5204670429229736}
I0205 22:38:03.711974 139739177916160 logging_writer.py:48] [13619] accumulated_eval_time=1997.031869, accumulated_logging_time=0.520467, accumulated_submission_time=4334.008366, global_step=13619, preemption_count=0, score=4334.008366, test/accuracy=0.984482, test/loss=0.052785, test/mean_average_precision=0.169803, test/num_examples=43793, total_duration=6331.934601, train/accuracy=0.988499, train/loss=0.039748, train/mean_average_precision=0.207366, validation/accuracy=0.985430, validation/loss=0.049895, validation/mean_average_precision=0.167108, validation/num_examples=43793
I0205 22:38:30.749303 139857145886464 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.037856508046388626, loss=0.04341374337673187
I0205 22:39:03.415187 139739177916160 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.04498046264052391, loss=0.04302922263741493
I0205 22:39:34.686236 139857145886464 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.029042189940810204, loss=0.0402437224984169
I0205 22:40:05.963033 139739177916160 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.03678311035037041, loss=0.03930272161960602
I0205 22:40:37.274646 139857145886464 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.06490808725357056, loss=0.03990248590707779
I0205 22:41:08.466571 139739177916160 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.13917630910873413, loss=0.04054050147533417
I0205 22:41:40.124239 139857145886464 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.03750261664390564, loss=0.03759525343775749
I0205 22:42:03.776396 139919816816448 spec.py:321] Evaluating on the training split.
I0205 22:43:39.862733 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 22:43:43.008860 139919816816448 spec.py:349] Evaluating on the test split.
I0205 22:43:46.094600 139919816816448 submission_runner.py:408] Time since start: 6674.34s, 	Step: 14376, 	{'train/accuracy': 0.9886337518692017, 'train/loss': 0.03938262537121773, 'train/mean_average_precision': 0.19114874640162213, 'validation/accuracy': 0.9855237007141113, 'validation/loss': 0.04945814982056618, 'validation/mean_average_precision': 0.16695185913129676, 'validation/num_examples': 43793, 'test/accuracy': 0.9846579432487488, 'test/loss': 0.05213755741715431, 'test/mean_average_precision': 0.1663883366384637, 'test/num_examples': 43793, 'score': 4574.039252996445, 'total_duration': 6674.336926460266, 'accumulated_submission_time': 4574.039252996445, 'accumulated_eval_time': 2099.3500323295593, 'accumulated_logging_time': 0.5519809722900391}
I0205 22:43:46.113443 139735302588160 logging_writer.py:48] [14376] accumulated_eval_time=2099.350032, accumulated_logging_time=0.551981, accumulated_submission_time=4574.039253, global_step=14376, preemption_count=0, score=4574.039253, test/accuracy=0.984658, test/loss=0.052138, test/mean_average_precision=0.166388, test/num_examples=43793, total_duration=6674.336926, train/accuracy=0.988634, train/loss=0.039383, train/mean_average_precision=0.191149, validation/accuracy=0.985524, validation/loss=0.049458, validation/mean_average_precision=0.166952, validation/num_examples=43793
I0205 22:43:54.075106 139759004710656 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.057279422879219055, loss=0.04356708750128746
I0205 22:44:26.184175 139735302588160 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.07203300297260284, loss=0.03694949299097061
I0205 22:44:58.622117 139759004710656 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.04512476548552513, loss=0.038151856511831284
I0205 22:45:30.047779 139735302588160 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.03819333389401436, loss=0.038401052355766296
I0205 22:46:01.367277 139759004710656 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.08494832366704941, loss=0.04270455986261368
I0205 22:46:32.799803 139735302588160 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.06778031587600708, loss=0.04512384161353111
I0205 22:47:04.243425 139759004710656 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.04518985003232956, loss=0.041637711226940155
I0205 22:47:36.184569 139735302588160 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.0461362861096859, loss=0.039722368121147156
I0205 22:47:46.137653 139919816816448 spec.py:321] Evaluating on the training split.
I0205 22:49:26.682617 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 22:49:29.744549 139919816816448 spec.py:349] Evaluating on the test split.
I0205 22:49:32.735391 139919816816448 submission_runner.py:408] Time since start: 7020.98s, 	Step: 15132, 	{'train/accuracy': 0.9885455369949341, 'train/loss': 0.0397002212703228, 'train/mean_average_precision': 0.2028603164464443, 'validation/accuracy': 0.9855809211730957, 'validation/loss': 0.04987575486302376, 'validation/mean_average_precision': 0.16837117460650874, 'validation/num_examples': 43793, 'test/accuracy': 0.9846330881118774, 'test/loss': 0.05285630002617836, 'test/mean_average_precision': 0.170379749437865, 'test/num_examples': 43793, 'score': 4814.0303320884705, 'total_duration': 7020.977716207504, 'accumulated_submission_time': 4814.0303320884705, 'accumulated_eval_time': 2205.947719812393, 'accumulated_logging_time': 0.58294677734375}
I0205 22:49:32.754126 139716992366336 logging_writer.py:48] [15132] accumulated_eval_time=2205.947720, accumulated_logging_time=0.582947, accumulated_submission_time=4814.030332, global_step=15132, preemption_count=0, score=4814.030332, test/accuracy=0.984633, test/loss=0.052856, test/mean_average_precision=0.170380, test/num_examples=43793, total_duration=7020.977716, train/accuracy=0.988546, train/loss=0.039700, train/mean_average_precision=0.202860, validation/accuracy=0.985581, validation/loss=0.049876, validation/mean_average_precision=0.168371, validation/num_examples=43793
I0205 22:49:54.656934 139735693903616 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.09579507261514664, loss=0.03884994611144066
I0205 22:50:26.025868 139716992366336 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.06881531327962875, loss=0.04404350370168686
I0205 22:50:57.181183 139735693903616 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.07645311206579208, loss=0.04382835328578949
I0205 22:51:29.029381 139716992366336 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.02994886413216591, loss=0.03736452758312225
I0205 22:52:00.899953 139735693903616 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.1042565330862999, loss=0.043457332998514175
I0205 22:52:32.370641 139716992366336 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.03951065242290497, loss=0.03827016428112984
I0205 22:53:04.017524 139735693903616 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.04318702965974808, loss=0.0360720232129097
I0205 22:53:32.812824 139919816816448 spec.py:321] Evaluating on the training split.
I0205 22:55:07.770815 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 22:55:10.830333 139919816816448 spec.py:349] Evaluating on the test split.
I0205 22:55:13.813232 139919816816448 submission_runner.py:408] Time since start: 7362.06s, 	Step: 15892, 	{'train/accuracy': 0.9885801672935486, 'train/loss': 0.039336010813713074, 'train/mean_average_precision': 0.19845201110629346, 'validation/accuracy': 0.9856215119361877, 'validation/loss': 0.04933474585413933, 'validation/mean_average_precision': 0.17170922124132204, 'validation/num_examples': 43793, 'test/accuracy': 0.9847198724746704, 'test/loss': 0.05216008797287941, 'test/mean_average_precision': 0.17798927606950105, 'test/num_examples': 43793, 'score': 5054.056452035904, 'total_duration': 7362.055555820465, 'accumulated_submission_time': 5054.056452035904, 'accumulated_eval_time': 2306.9480743408203, 'accumulated_logging_time': 0.6138248443603516}
I0205 22:55:13.831559 139735302588160 logging_writer.py:48] [15892] accumulated_eval_time=2306.948074, accumulated_logging_time=0.613825, accumulated_submission_time=5054.056452, global_step=15892, preemption_count=0, score=5054.056452, test/accuracy=0.984720, test/loss=0.052160, test/mean_average_precision=0.177989, test/num_examples=43793, total_duration=7362.055556, train/accuracy=0.988580, train/loss=0.039336, train/mean_average_precision=0.198452, validation/accuracy=0.985622, validation/loss=0.049335, validation/mean_average_precision=0.171709, validation/num_examples=43793
I0205 22:55:16.781416 139759004710656 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.0629667267203331, loss=0.03900119662284851
I0205 22:55:48.635918 139735302588160 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.054293811321258545, loss=0.04198288172483444
I0205 22:56:20.326233 139759004710656 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.06471943110227585, loss=0.03901463374495506
I0205 22:56:52.244301 139735302588160 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.03617265820503235, loss=0.038029737770557404
I0205 22:57:24.203345 139759004710656 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.050645340234041214, loss=0.03386402130126953
I0205 22:57:55.996696 139735302588160 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.0537961907684803, loss=0.039105333387851715
I0205 22:58:28.294375 139759004710656 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.07057438790798187, loss=0.03393421694636345
I0205 22:59:00.359408 139735302588160 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.0423867404460907, loss=0.03942951560020447
I0205 22:59:14.052943 139919816816448 spec.py:321] Evaluating on the training split.
I0205 23:00:54.833742 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 23:00:57.884119 139919816816448 spec.py:349] Evaluating on the test split.
I0205 23:01:00.927992 139919816816448 submission_runner.py:408] Time since start: 7709.17s, 	Step: 16644, 	{'train/accuracy': 0.9884446263313293, 'train/loss': 0.03972674533724785, 'train/mean_average_precision': 0.19522588930474225, 'validation/accuracy': 0.9856138229370117, 'validation/loss': 0.04969978705048561, 'validation/mean_average_precision': 0.1718890388904479, 'validation/num_examples': 43793, 'test/accuracy': 0.9847078323364258, 'test/loss': 0.05255919694900513, 'test/mean_average_precision': 0.17442957654614527, 'test/num_examples': 43793, 'score': 5294.24645280838, 'total_duration': 7709.170320272446, 'accumulated_submission_time': 5294.24645280838, 'accumulated_eval_time': 2413.8230736255646, 'accumulated_logging_time': 0.6432387828826904}
I0205 23:01:00.947727 139716992366336 logging_writer.py:48] [16644] accumulated_eval_time=2413.823074, accumulated_logging_time=0.643239, accumulated_submission_time=5294.246453, global_step=16644, preemption_count=0, score=5294.246453, test/accuracy=0.984708, test/loss=0.052559, test/mean_average_precision=0.174430, test/num_examples=43793, total_duration=7709.170320, train/accuracy=0.988445, train/loss=0.039727, train/mean_average_precision=0.195226, validation/accuracy=0.985614, validation/loss=0.049700, validation/mean_average_precision=0.171889, validation/num_examples=43793
I0205 23:01:19.319720 139735693903616 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.0500074177980423, loss=0.04195476323366165
I0205 23:01:51.198884 139716992366336 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.02764682099223137, loss=0.04055235907435417
I0205 23:02:22.951426 139735693903616 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.05589680373668671, loss=0.04083835333585739
I0205 23:02:54.521850 139716992366336 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.0651969239115715, loss=0.042822323739528656
I0205 23:03:26.535841 139735693903616 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.054424598813056946, loss=0.04048289731144905
I0205 23:03:58.539294 139716992366336 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.035243015736341476, loss=0.04157950356602669
I0205 23:04:31.019988 139735693903616 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.05044311657547951, loss=0.03644772991538048
I0205 23:05:00.953122 139919816816448 spec.py:321] Evaluating on the training split.
I0205 23:06:41.076123 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 23:06:44.505302 139919816816448 spec.py:349] Evaluating on the test split.
I0205 23:06:47.863179 139919816816448 submission_runner.py:408] Time since start: 8056.11s, 	Step: 17393, 	{'train/accuracy': 0.9886024594306946, 'train/loss': 0.03929731249809265, 'train/mean_average_precision': 0.21046139357105487, 'validation/accuracy': 0.9857274889945984, 'validation/loss': 0.04928113892674446, 'validation/mean_average_precision': 0.1825998260774477, 'validation/num_examples': 43793, 'test/accuracy': 0.9848057627677917, 'test/loss': 0.05209784954786301, 'test/mean_average_precision': 0.17891039444669973, 'test/num_examples': 43793, 'score': 5534.219587802887, 'total_duration': 8056.105494737625, 'accumulated_submission_time': 5534.219587802887, 'accumulated_eval_time': 2520.733085632324, 'accumulated_logging_time': 0.6736774444580078}
I0205 23:06:47.885365 139735302588160 logging_writer.py:48] [17393] accumulated_eval_time=2520.733086, accumulated_logging_time=0.673677, accumulated_submission_time=5534.219588, global_step=17393, preemption_count=0, score=5534.219588, test/accuracy=0.984806, test/loss=0.052098, test/mean_average_precision=0.178910, test/num_examples=43793, total_duration=8056.105495, train/accuracy=0.988602, train/loss=0.039297, train/mean_average_precision=0.210461, validation/accuracy=0.985727, validation/loss=0.049281, validation/mean_average_precision=0.182600, validation/num_examples=43793
I0205 23:06:50.513430 139739177916160 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.03366026282310486, loss=0.04070557281374931
I0205 23:07:22.653691 139735302588160 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.04932944104075432, loss=0.04381466656923294
I0205 23:07:54.715856 139739177916160 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.027012040838599205, loss=0.039414260536432266
I0205 23:08:26.618868 139735302588160 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.03178812563419342, loss=0.03935465216636658
I0205 23:08:58.562347 139739177916160 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.057398438453674316, loss=0.04273955896496773
I0205 23:09:30.424108 139735302588160 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.0951717272400856, loss=0.04353511705994606
I0205 23:10:02.232823 139739177916160 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.12182442843914032, loss=0.04120643436908722
I0205 23:10:34.489815 139735302588160 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.045854147523641586, loss=0.04224856570363045
I0205 23:10:47.996257 139919816816448 spec.py:321] Evaluating on the training split.
I0205 23:12:26.585270 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 23:12:29.658872 139919816816448 spec.py:349] Evaluating on the test split.
I0205 23:12:34.880988 139919816816448 submission_runner.py:408] Time since start: 8403.12s, 	Step: 18143, 	{'train/accuracy': 0.9886455535888672, 'train/loss': 0.039150483906269073, 'train/mean_average_precision': 0.19768793801215342, 'validation/accuracy': 0.9855988025665283, 'validation/loss': 0.04940321296453476, 'validation/mean_average_precision': 0.17184504168876086, 'validation/num_examples': 43793, 'test/accuracy': 0.9847438931465149, 'test/loss': 0.0521351657807827, 'test/mean_average_precision': 0.17146011683788295, 'test/num_examples': 43793, 'score': 5774.297125339508, 'total_duration': 8403.12332034111, 'accumulated_submission_time': 5774.297125339508, 'accumulated_eval_time': 2627.6177830696106, 'accumulated_logging_time': 0.7077550888061523}
I0205 23:12:34.900073 139716992366336 logging_writer.py:48] [18143] accumulated_eval_time=2627.617783, accumulated_logging_time=0.707755, accumulated_submission_time=5774.297125, global_step=18143, preemption_count=0, score=5774.297125, test/accuracy=0.984744, test/loss=0.052135, test/mean_average_precision=0.171460, test/num_examples=43793, total_duration=8403.123320, train/accuracy=0.988646, train/loss=0.039150, train/mean_average_precision=0.197688, validation/accuracy=0.985599, validation/loss=0.049403, validation/mean_average_precision=0.171845, validation/num_examples=43793
I0205 23:12:53.293108 139735693903616 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.043864525854587555, loss=0.03859648108482361
I0205 23:13:25.070268 139716992366336 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.09173400700092316, loss=0.040374755859375
I0205 23:13:56.864929 139735693903616 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.07761280238628387, loss=0.04442380741238594
I0205 23:14:28.733509 139716992366336 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.07502353936433792, loss=0.03865550458431244
I0205 23:15:00.234531 139735693903616 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.07193182408809662, loss=0.042274944484233856
I0205 23:15:33.268425 139716992366336 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.05572964996099472, loss=0.03549918532371521
I0205 23:16:05.110401 139735693903616 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.06293882429599762, loss=0.03751370310783386
I0205 23:16:34.997152 139919816816448 spec.py:321] Evaluating on the training split.
I0205 23:18:13.622150 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 23:18:16.773137 139919816816448 spec.py:349] Evaluating on the test split.
I0205 23:18:19.766862 139919816816448 submission_runner.py:408] Time since start: 8748.01s, 	Step: 18896, 	{'train/accuracy': 0.988558292388916, 'train/loss': 0.03911030665040016, 'train/mean_average_precision': 0.2097071248810855, 'validation/accuracy': 0.9856860637664795, 'validation/loss': 0.04947445169091225, 'validation/mean_average_precision': 0.17108575622241304, 'validation/num_examples': 43793, 'test/accuracy': 0.9847944378852844, 'test/loss': 0.05227375775575638, 'test/mean_average_precision': 0.1771656161784761, 'test/num_examples': 43793, 'score': 6014.361703634262, 'total_duration': 8748.009192943573, 'accumulated_submission_time': 6014.361703634262, 'accumulated_eval_time': 2732.3874497413635, 'accumulated_logging_time': 0.7390317916870117}
I0205 23:18:19.786425 139735302588160 logging_writer.py:48] [18896] accumulated_eval_time=2732.387450, accumulated_logging_time=0.739032, accumulated_submission_time=6014.361704, global_step=18896, preemption_count=0, score=6014.361704, test/accuracy=0.984794, test/loss=0.052274, test/mean_average_precision=0.177166, test/num_examples=43793, total_duration=8748.009193, train/accuracy=0.988558, train/loss=0.039110, train/mean_average_precision=0.209707, validation/accuracy=0.985686, validation/loss=0.049474, validation/mean_average_precision=0.171086, validation/num_examples=43793
I0205 23:18:21.397311 139759004710656 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.05143580213189125, loss=0.04048274829983711
I0205 23:18:53.397717 139735302588160 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.042795658111572266, loss=0.04146146774291992
I0205 23:19:25.398941 139759004710656 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.06118321418762207, loss=0.04033907130360603
I0205 23:19:57.165474 139735302588160 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.1215701699256897, loss=0.03664416819810867
I0205 23:20:28.881222 139759004710656 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.05659298226237297, loss=0.04002141207456589
I0205 23:21:00.565800 139735302588160 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.03765007480978966, loss=0.037036702036857605
I0205 23:21:32.472930 139759004710656 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.04567871242761612, loss=0.03501889109611511
I0205 23:22:03.984486 139735302588160 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.08526470512151718, loss=0.04332771897315979
I0205 23:22:19.780617 139919816816448 spec.py:321] Evaluating on the training split.
I0205 23:23:59.301511 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 23:24:02.442330 139919816816448 spec.py:349] Evaluating on the test split.
I0205 23:24:05.400737 139919816816448 submission_runner.py:408] Time since start: 9093.64s, 	Step: 19650, 	{'train/accuracy': 0.9885598421096802, 'train/loss': 0.0392596535384655, 'train/mean_average_precision': 0.203455128542813, 'validation/accuracy': 0.9856438636779785, 'validation/loss': 0.04947600141167641, 'validation/mean_average_precision': 0.16671810223241684, 'validation/num_examples': 43793, 'test/accuracy': 0.984729528427124, 'test/loss': 0.05226048454642296, 'test/mean_average_precision': 0.1714548320037481, 'test/num_examples': 43793, 'score': 6254.322240352631, 'total_duration': 9093.64295911789, 'accumulated_submission_time': 6254.322240352631, 'accumulated_eval_time': 2838.0074141025543, 'accumulated_logging_time': 0.7716796398162842}
I0205 23:24:05.421276 139735693903616 logging_writer.py:48] [19650] accumulated_eval_time=2838.007414, accumulated_logging_time=0.771680, accumulated_submission_time=6254.322240, global_step=19650, preemption_count=0, score=6254.322240, test/accuracy=0.984730, test/loss=0.052260, test/mean_average_precision=0.171455, test/num_examples=43793, total_duration=9093.642959, train/accuracy=0.988560, train/loss=0.039260, train/mean_average_precision=0.203455, validation/accuracy=0.985644, validation/loss=0.049476, validation/mean_average_precision=0.166718, validation/num_examples=43793
I0205 23:24:23.018795 139739177916160 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.0654328241944313, loss=0.04084567353129387
I0205 23:24:57.148061 139735693903616 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.05616981163620949, loss=0.04336318373680115
I0205 23:25:29.062886 139739177916160 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.028175972402095795, loss=0.03847559541463852
I0205 23:26:00.528768 139735693903616 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.050495848059654236, loss=0.03915882483124733
I0205 23:26:31.903236 139739177916160 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.07632966339588165, loss=0.03610851615667343
I0205 23:27:03.277493 139735693903616 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.034854479134082794, loss=0.036450598388910294
I0205 23:27:35.291885 139739177916160 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.03137822449207306, loss=0.03615754097700119
I0205 23:28:05.403928 139919816816448 spec.py:321] Evaluating on the training split.
I0205 23:29:44.296164 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 23:29:47.325064 139919816816448 spec.py:349] Evaluating on the test split.
I0205 23:29:50.352214 139919816816448 submission_runner.py:408] Time since start: 9438.59s, 	Step: 20396, 	{'train/accuracy': 0.9887438416481018, 'train/loss': 0.03857656568288803, 'train/mean_average_precision': 0.212028509293876, 'validation/accuracy': 0.9856438636779785, 'validation/loss': 0.04899085685610771, 'validation/mean_average_precision': 0.1850536108788391, 'validation/num_examples': 43793, 'test/accuracy': 0.9847375750541687, 'test/loss': 0.05189521238207817, 'test/mean_average_precision': 0.17919727958794127, 'test/num_examples': 43793, 'score': 6494.273699045181, 'total_duration': 9438.594544649124, 'accumulated_submission_time': 6494.273699045181, 'accumulated_eval_time': 2942.955656528473, 'accumulated_logging_time': 0.8033504486083984}
I0205 23:29:50.371212 139716992366336 logging_writer.py:48] [20396] accumulated_eval_time=2942.955657, accumulated_logging_time=0.803350, accumulated_submission_time=6494.273699, global_step=20396, preemption_count=0, score=6494.273699, test/accuracy=0.984738, test/loss=0.051895, test/mean_average_precision=0.179197, test/num_examples=43793, total_duration=9438.594545, train/accuracy=0.988744, train/loss=0.038577, train/mean_average_precision=0.212029, validation/accuracy=0.985644, validation/loss=0.048991, validation/mean_average_precision=0.185054, validation/num_examples=43793
I0205 23:29:51.978459 139759004710656 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.03050955757498741, loss=0.03703243285417557
I0205 23:30:23.330236 139716992366336 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.049673400819301605, loss=0.04236668348312378
I0205 23:30:54.995209 139759004710656 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.03947513550519943, loss=0.03349550813436508
I0205 23:31:26.513404 139716992366336 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.08347174525260925, loss=0.041059233248233795
I0205 23:31:57.733765 139759004710656 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.06508389115333557, loss=0.039393819868564606
I0205 23:32:28.997381 139716992366336 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.05857795476913452, loss=0.038241975009441376
I0205 23:33:00.354067 139759004710656 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.079793781042099, loss=0.04094506800174713
I0205 23:33:32.004979 139716992366336 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.06901020556688309, loss=0.0376722626388073
I0205 23:33:50.566523 139919816816448 spec.py:321] Evaluating on the training split.
I0205 23:35:28.147441 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 23:35:31.209775 139919816816448 spec.py:349] Evaluating on the test split.
I0205 23:35:34.263991 139919816816448 submission_runner.py:408] Time since start: 9782.51s, 	Step: 21159, 	{'train/accuracy': 0.9888488054275513, 'train/loss': 0.03830825537443161, 'train/mean_average_precision': 0.2137575777576014, 'validation/accuracy': 0.9856317043304443, 'validation/loss': 0.04880497232079506, 'validation/mean_average_precision': 0.17733595286877527, 'validation/num_examples': 43793, 'test/accuracy': 0.9846756458282471, 'test/loss': 0.05158416926860809, 'test/mean_average_precision': 0.17797966573308246, 'test/num_examples': 43793, 'score': 6734.436192750931, 'total_duration': 9782.50632095337, 'accumulated_submission_time': 6734.436192750931, 'accumulated_eval_time': 3046.6530838012695, 'accumulated_logging_time': 0.834456205368042}
I0205 23:35:34.284355 139735693903616 logging_writer.py:48] [21159] accumulated_eval_time=3046.653084, accumulated_logging_time=0.834456, accumulated_submission_time=6734.436193, global_step=21159, preemption_count=0, score=6734.436193, test/accuracy=0.984676, test/loss=0.051584, test/mean_average_precision=0.177980, test/num_examples=43793, total_duration=9782.506321, train/accuracy=0.988849, train/loss=0.038308, train/mean_average_precision=0.213758, validation/accuracy=0.985632, validation/loss=0.048805, validation/mean_average_precision=0.177336, validation/num_examples=43793
I0205 23:35:48.277934 139739177916160 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.03473327308893204, loss=0.03819988667964935
I0205 23:36:20.056051 139735693903616 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.06337778270244598, loss=0.040304817259311676
I0205 23:36:52.016497 139739177916160 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.03409743309020996, loss=0.04262126609683037
I0205 23:37:24.033662 139735693903616 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.19430147111415863, loss=0.03977055847644806
I0205 23:37:55.720087 139739177916160 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.02874845080077648, loss=0.04119246453046799
I0205 23:38:27.359095 139735693903616 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.029747849330306053, loss=0.036119528114795685
I0205 23:38:59.018021 139739177916160 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.028174180537462234, loss=0.040663689374923706
I0205 23:39:30.871226 139735693903616 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.04223357141017914, loss=0.039393868297338486
I0205 23:39:34.339803 139919816816448 spec.py:321] Evaluating on the training split.
I0205 23:41:12.519818 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 23:41:15.576201 139919816816448 spec.py:349] Evaluating on the test split.
I0205 23:41:18.615099 139919816816448 submission_runner.py:408] Time since start: 10126.86s, 	Step: 21912, 	{'train/accuracy': 0.9886640906333923, 'train/loss': 0.03873438388109207, 'train/mean_average_precision': 0.22411031784235116, 'validation/accuracy': 0.9857713580131531, 'validation/loss': 0.04842014238238335, 'validation/mean_average_precision': 0.18335871642119683, 'validation/num_examples': 43793, 'test/accuracy': 0.984855055809021, 'test/loss': 0.051150087267160416, 'test/mean_average_precision': 0.18211586940575714, 'test/num_examples': 43793, 'score': 6974.460539340973, 'total_duration': 10126.857418060303, 'accumulated_submission_time': 6974.460539340973, 'accumulated_eval_time': 3150.928318500519, 'accumulated_logging_time': 0.8655087947845459}
I0205 23:41:18.634604 139735302588160 logging_writer.py:48] [21912] accumulated_eval_time=3150.928319, accumulated_logging_time=0.865509, accumulated_submission_time=6974.460539, global_step=21912, preemption_count=0, score=6974.460539, test/accuracy=0.984855, test/loss=0.051150, test/mean_average_precision=0.182116, test/num_examples=43793, total_duration=10126.857418, train/accuracy=0.988664, train/loss=0.038734, train/mean_average_precision=0.224110, validation/accuracy=0.985771, validation/loss=0.048420, validation/mean_average_precision=0.183359, validation/num_examples=43793
I0205 23:41:47.226922 139759004710656 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.08581121265888214, loss=0.036946021020412445
I0205 23:42:19.180524 139735302588160 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.04657907038927078, loss=0.03479211404919624
I0205 23:42:51.149696 139759004710656 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.07712487131357193, loss=0.042141638696193695
I0205 23:43:23.098584 139735302588160 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.1347077190876007, loss=0.04327847436070442
I0205 23:43:55.051224 139759004710656 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.12281481921672821, loss=0.04164746776223183
I0205 23:44:26.942476 139735302588160 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.045004282146692276, loss=0.03632131963968277
I0205 23:44:58.248657 139759004710656 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.11629588156938553, loss=0.0434841588139534
I0205 23:45:18.921584 139919816816448 spec.py:321] Evaluating on the training split.
I0205 23:46:54.407491 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 23:46:57.451568 139919816816448 spec.py:349] Evaluating on the test split.
I0205 23:47:00.433304 139919816816448 submission_runner.py:408] Time since start: 10468.68s, 	Step: 22666, 	{'train/accuracy': 0.9887723326683044, 'train/loss': 0.0386015810072422, 'train/mean_average_precision': 0.21176870812917725, 'validation/accuracy': 0.9856633543968201, 'validation/loss': 0.0487646721303463, 'validation/mean_average_precision': 0.17440925763893128, 'validation/num_examples': 43793, 'test/accuracy': 0.9847897887229919, 'test/loss': 0.05144037678837776, 'test/mean_average_precision': 0.17965303088513426, 'test/num_examples': 43793, 'score': 7214.716456651688, 'total_duration': 10468.675518989563, 'accumulated_submission_time': 7214.716456651688, 'accumulated_eval_time': 3252.439876317978, 'accumulated_logging_time': 0.89583420753479}
I0205 23:47:00.452678 139716992366336 logging_writer.py:48] [22666] accumulated_eval_time=3252.439876, accumulated_logging_time=0.895834, accumulated_submission_time=7214.716457, global_step=22666, preemption_count=0, score=7214.716457, test/accuracy=0.984790, test/loss=0.051440, test/mean_average_precision=0.179653, test/num_examples=43793, total_duration=10468.675519, train/accuracy=0.988772, train/loss=0.038602, train/mean_average_precision=0.211769, validation/accuracy=0.985663, validation/loss=0.048765, validation/mean_average_precision=0.174409, validation/num_examples=43793
I0205 23:47:11.632151 139735693903616 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.06535385549068451, loss=0.04344829544425011
I0205 23:47:42.781763 139716992366336 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.04402679577469826, loss=0.042917970567941666
I0205 23:48:14.552216 139735693903616 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.04714272543787956, loss=0.03797364607453346
I0205 23:48:46.187208 139716992366336 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.0846468061208725, loss=0.041555292904376984
I0205 23:49:18.007350 139735693903616 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.08201305568218231, loss=0.0392361581325531
I0205 23:49:49.617121 139716992366336 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.02571791410446167, loss=0.034733597189188004
I0205 23:50:21.563059 139735693903616 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.07716928422451019, loss=0.04146469756960869
I0205 23:50:52.793282 139716992366336 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.04355565458536148, loss=0.04151666909456253
I0205 23:51:00.545306 139919816816448 spec.py:321] Evaluating on the training split.
I0205 23:52:39.512602 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 23:52:42.659883 139919816816448 spec.py:349] Evaluating on the test split.
I0205 23:52:45.689521 139919816816448 submission_runner.py:408] Time since start: 10813.93s, 	Step: 23426, 	{'train/accuracy': 0.9883869886398315, 'train/loss': 0.040114033967256546, 'train/mean_average_precision': 0.20837487977254418, 'validation/accuracy': 0.9855829477310181, 'validation/loss': 0.050475310534238815, 'validation/mean_average_precision': 0.18149632972056115, 'validation/num_examples': 43793, 'test/accuracy': 0.9846773147583008, 'test/loss': 0.053886838257312775, 'test/mean_average_precision': 0.18220864852989385, 'test/num_examples': 43793, 'score': 7454.776722192764, 'total_duration': 10813.931846141815, 'accumulated_submission_time': 7454.776722192764, 'accumulated_eval_time': 3357.5840377807617, 'accumulated_logging_time': 0.9260289669036865}
I0205 23:52:45.709317 139735302588160 logging_writer.py:48] [23426] accumulated_eval_time=3357.584038, accumulated_logging_time=0.926029, accumulated_submission_time=7454.776722, global_step=23426, preemption_count=0, score=7454.776722, test/accuracy=0.984677, test/loss=0.053887, test/mean_average_precision=0.182209, test/num_examples=43793, total_duration=10813.931846, train/accuracy=0.988387, train/loss=0.040114, train/mean_average_precision=0.208375, validation/accuracy=0.985583, validation/loss=0.050475, validation/mean_average_precision=0.181496, validation/num_examples=43793
I0205 23:53:09.475855 139759004710656 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.0953410416841507, loss=0.038428839296102524
I0205 23:53:41.022540 139735302588160 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.04963625594973564, loss=0.03950212150812149
I0205 23:54:12.093793 139759004710656 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.0516539067029953, loss=0.03999936208128929
I0205 23:54:43.580730 139735302588160 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.05355391651391983, loss=0.03715229406952858
I0205 23:55:15.963778 139759004710656 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.030515063554048538, loss=0.04069782420992851
I0205 23:55:48.164229 139735302588160 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.06537342071533203, loss=0.04311028867959976
I0205 23:56:20.123123 139759004710656 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.043076638132333755, loss=0.03588699549436569
I0205 23:56:45.728753 139919816816448 spec.py:321] Evaluating on the training split.
I0205 23:58:23.632366 139919816816448 spec.py:333] Evaluating on the validation split.
I0205 23:58:26.735410 139919816816448 spec.py:349] Evaluating on the test split.
I0205 23:58:29.774433 139919816816448 submission_runner.py:408] Time since start: 11158.02s, 	Step: 24182, 	{'train/accuracy': 0.9885193705558777, 'train/loss': 0.039419785141944885, 'train/mean_average_precision': 0.20867398454830233, 'validation/accuracy': 0.985595166683197, 'validation/loss': 0.04940466210246086, 'validation/mean_average_precision': 0.17790979633379897, 'validation/num_examples': 43793, 'test/accuracy': 0.9847131371498108, 'test/loss': 0.05240046605467796, 'test/mean_average_precision': 0.1733469909451849, 'test/num_examples': 43793, 'score': 7694.763477563858, 'total_duration': 11158.016758441925, 'accumulated_submission_time': 7694.763477563858, 'accumulated_eval_time': 3461.6296710968018, 'accumulated_logging_time': 0.9567892551422119}
I0205 23:58:29.794343 139735693903616 logging_writer.py:48] [24182] accumulated_eval_time=3461.629671, accumulated_logging_time=0.956789, accumulated_submission_time=7694.763478, global_step=24182, preemption_count=0, score=7694.763478, test/accuracy=0.984713, test/loss=0.052400, test/mean_average_precision=0.173347, test/num_examples=43793, total_duration=11158.016758, train/accuracy=0.988519, train/loss=0.039420, train/mean_average_precision=0.208674, validation/accuracy=0.985595, validation/loss=0.049405, validation/mean_average_precision=0.177910, validation/num_examples=43793
I0205 23:58:35.992241 139739177916160 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.0588592067360878, loss=0.04203919321298599
I0205 23:59:08.273025 139735693903616 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.04940216988325119, loss=0.038683630526065826
I0205 23:59:39.867898 139739177916160 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.06199817731976509, loss=0.03718692809343338
I0206 00:00:11.642758 139735693903616 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.05932359769940376, loss=0.03927452489733696
I0206 00:00:43.432358 139739177916160 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.039731282740831375, loss=0.03841286152601242
I0206 00:01:15.457002 139735693903616 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.06447333097457886, loss=0.04474346339702606
I0206 00:01:47.922629 139739177916160 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.12162549048662186, loss=0.03564579039812088
I0206 00:02:19.881435 139735693903616 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.041090257465839386, loss=0.03465190902352333
I0206 00:02:30.114140 139919816816448 spec.py:321] Evaluating on the training split.
I0206 00:04:08.585732 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 00:04:11.727234 139919816816448 spec.py:349] Evaluating on the test split.
I0206 00:04:14.841834 139919816816448 submission_runner.py:408] Time since start: 11503.08s, 	Step: 24932, 	{'train/accuracy': 0.9886791110038757, 'train/loss': 0.039127543568611145, 'train/mean_average_precision': 0.20610971730959943, 'validation/accuracy': 0.9856958389282227, 'validation/loss': 0.04969356581568718, 'validation/mean_average_precision': 0.17424173414313215, 'validation/num_examples': 43793, 'test/accuracy': 0.9847312569618225, 'test/loss': 0.05299022048711777, 'test/mean_average_precision': 0.173159771894493, 'test/num_examples': 43793, 'score': 7935.051184654236, 'total_duration': 11503.084161281586, 'accumulated_submission_time': 7935.051184654236, 'accumulated_eval_time': 3566.3573133945465, 'accumulated_logging_time': 0.9878280162811279}
I0206 00:04:14.863244 139735302588160 logging_writer.py:48] [24932] accumulated_eval_time=3566.357313, accumulated_logging_time=0.987828, accumulated_submission_time=7935.051185, global_step=24932, preemption_count=0, score=7935.051185, test/accuracy=0.984731, test/loss=0.052990, test/mean_average_precision=0.173160, test/num_examples=43793, total_duration=11503.084161, train/accuracy=0.988679, train/loss=0.039128, train/mean_average_precision=0.206110, validation/accuracy=0.985696, validation/loss=0.049694, validation/mean_average_precision=0.174242, validation/num_examples=43793
I0206 00:04:36.702679 139759004710656 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.06244870647788048, loss=0.03550512343645096
I0206 00:05:08.228206 139735302588160 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.05778946354985237, loss=0.04046110063791275
I0206 00:05:40.443396 139759004710656 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.03607651963829994, loss=0.03575580194592476
I0206 00:06:12.378362 139735302588160 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.08790097385644913, loss=0.04164539650082588
I0206 00:06:43.637004 139759004710656 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.07339944690465927, loss=0.03515864536166191
I0206 00:07:15.143810 139735302588160 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.04722747579216957, loss=0.039726097136735916
I0206 00:07:46.741025 139759004710656 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.045716542750597, loss=0.0406176894903183
I0206 00:08:15.053013 139919816816448 spec.py:321] Evaluating on the training split.
I0206 00:09:52.375503 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 00:09:55.519190 139919816816448 spec.py:349] Evaluating on the test split.
I0206 00:09:58.587748 139919816816448 submission_runner.py:408] Time since start: 11846.83s, 	Step: 25691, 	{'train/accuracy': 0.988594651222229, 'train/loss': 0.03934819996356964, 'train/mean_average_precision': 0.20413834738695363, 'validation/accuracy': 0.9856219291687012, 'validation/loss': 0.04998236149549484, 'validation/mean_average_precision': 0.17955457727038346, 'validation/num_examples': 43793, 'test/accuracy': 0.9847291111946106, 'test/loss': 0.053024083375930786, 'test/mean_average_precision': 0.1790258355233416, 'test/num_examples': 43793, 'score': 8175.208832502365, 'total_duration': 11846.830078840256, 'accumulated_submission_time': 8175.208832502365, 'accumulated_eval_time': 3669.8920063972473, 'accumulated_logging_time': 1.0200772285461426}
I0206 00:09:58.608237 139716992366336 logging_writer.py:48] [25691] accumulated_eval_time=3669.892006, accumulated_logging_time=1.020077, accumulated_submission_time=8175.208833, global_step=25691, preemption_count=0, score=8175.208833, test/accuracy=0.984729, test/loss=0.053024, test/mean_average_precision=0.179026, test/num_examples=43793, total_duration=11846.830079, train/accuracy=0.988595, train/loss=0.039348, train/mean_average_precision=0.204138, validation/accuracy=0.985622, validation/loss=0.049982, validation/mean_average_precision=0.179555, validation/num_examples=43793
I0206 00:10:01.857151 139735693903616 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.06346593052148819, loss=0.038102295249700546
I0206 00:10:33.628561 139716992366336 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.040727339684963226, loss=0.03926263749599457
I0206 00:11:05.729689 139735693903616 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.04647676646709442, loss=0.041506391018629074
I0206 00:11:37.126899 139716992366336 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.03950496017932892, loss=0.03618564084172249
I0206 00:12:08.784544 139735693903616 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.044107869267463684, loss=0.03628860414028168
I0206 00:12:41.025230 139716992366336 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.04122808948159218, loss=0.03901912644505501
I0206 00:13:12.787872 139735693903616 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.06076608598232269, loss=0.041626863181591034
I0206 00:13:44.511523 139716992366336 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.039277832955121994, loss=0.03687506541609764
I0206 00:13:58.625934 139919816816448 spec.py:321] Evaluating on the training split.
I0206 00:15:37.882286 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 00:15:40.965719 139919816816448 spec.py:349] Evaluating on the test split.
I0206 00:15:44.013630 139919816816448 submission_runner.py:408] Time since start: 12192.26s, 	Step: 26445, 	{'train/accuracy': 0.9887518882751465, 'train/loss': 0.03899611532688141, 'train/mean_average_precision': 0.21371595474408023, 'validation/accuracy': 0.9856349229812622, 'validation/loss': 0.04871748015284538, 'validation/mean_average_precision': 0.17790845851272608, 'validation/num_examples': 43793, 'test/accuracy': 0.9847640991210938, 'test/loss': 0.05142880603671074, 'test/mean_average_precision': 0.17431201489801942, 'test/num_examples': 43793, 'score': 8415.195302963257, 'total_duration': 12192.255960464478, 'accumulated_submission_time': 8415.195302963257, 'accumulated_eval_time': 3775.2796547412872, 'accumulated_logging_time': 1.051387071609497}
I0206 00:15:44.034083 139735302588160 logging_writer.py:48] [26445] accumulated_eval_time=3775.279655, accumulated_logging_time=1.051387, accumulated_submission_time=8415.195303, global_step=26445, preemption_count=0, score=8415.195303, test/accuracy=0.984764, test/loss=0.051429, test/mean_average_precision=0.174312, test/num_examples=43793, total_duration=12192.255960, train/accuracy=0.988752, train/loss=0.038996, train/mean_average_precision=0.213716, validation/accuracy=0.985635, validation/loss=0.048717, validation/mean_average_precision=0.177908, validation/num_examples=43793
I0206 00:16:02.499510 139759004710656 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.055377569049596786, loss=0.03739756718277931
I0206 00:16:34.672755 139735302588160 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.05403432622551918, loss=0.040282346308231354
I0206 00:17:06.832217 139759004710656 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.03042990155518055, loss=0.03875723481178284
I0206 00:17:38.351572 139735302588160 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.06380727887153625, loss=0.04084232449531555
I0206 00:18:10.257986 139759004710656 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.08870641142129898, loss=0.03626751899719238
I0206 00:18:42.171864 139735302588160 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.07829398661851883, loss=0.04259839653968811
I0206 00:19:14.298834 139759004710656 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.04957723617553711, loss=0.03970137983560562
I0206 00:19:44.072575 139919816816448 spec.py:321] Evaluating on the training split.
I0206 00:21:27.719387 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 00:21:30.963692 139919816816448 spec.py:349] Evaluating on the test split.
I0206 00:21:34.048708 139919816816448 submission_runner.py:408] Time since start: 12542.29s, 	Step: 27195, 	{'train/accuracy': 0.9886566400527954, 'train/loss': 0.03898187354207039, 'train/mean_average_precision': 0.21840967196923997, 'validation/accuracy': 0.985464870929718, 'validation/loss': 0.04904794692993164, 'validation/mean_average_precision': 0.18207372190279822, 'validation/num_examples': 43793, 'test/accuracy': 0.9845766425132751, 'test/loss': 0.051882777363061905, 'test/mean_average_precision': 0.1705559020023737, 'test/num_examples': 43793, 'score': 8655.201495409012, 'total_duration': 12542.291038513184, 'accumulated_submission_time': 8655.201495409012, 'accumulated_eval_time': 3885.2557418346405, 'accumulated_logging_time': 1.0838682651519775}
I0206 00:21:34.069215 139716992366336 logging_writer.py:48] [27195] accumulated_eval_time=3885.255742, accumulated_logging_time=1.083868, accumulated_submission_time=8655.201495, global_step=27195, preemption_count=0, score=8655.201495, test/accuracy=0.984577, test/loss=0.051883, test/mean_average_precision=0.170556, test/num_examples=43793, total_duration=12542.291039, train/accuracy=0.988657, train/loss=0.038982, train/mean_average_precision=0.218410, validation/accuracy=0.985465, validation/loss=0.049048, validation/mean_average_precision=0.182074, validation/num_examples=43793
I0206 00:21:36.074705 139735693903616 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.061807483434677124, loss=0.03723921254277229
I0206 00:22:08.167371 139716992366336 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.026448290795087814, loss=0.03973756730556488
I0206 00:22:39.746687 139735693903616 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.038816723972558975, loss=0.0404292531311512
I0206 00:23:11.534695 139716992366336 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.03422786295413971, loss=0.034722521901130676
I0206 00:23:43.069331 139735693903616 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.05495869740843773, loss=0.038095466792583466
I0206 00:24:15.129735 139716992366336 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.06804675608873367, loss=0.04174725338816643
I0206 00:24:47.091624 139735693903616 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.03593805804848671, loss=0.04098355025053024
I0206 00:25:18.859577 139716992366336 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.0470910407602787, loss=0.03484278917312622
I0206 00:25:34.344408 139919816816448 spec.py:321] Evaluating on the training split.
I0206 00:27:13.930700 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 00:27:16.978252 139919816816448 spec.py:349] Evaluating on the test split.
I0206 00:27:19.934071 139919816816448 submission_runner.py:408] Time since start: 12888.18s, 	Step: 27950, 	{'train/accuracy': 0.9887489676475525, 'train/loss': 0.038225021213293076, 'train/mean_average_precision': 0.22385914357152437, 'validation/accuracy': 0.9858261346817017, 'validation/loss': 0.04866309463977814, 'validation/mean_average_precision': 0.1828395677585086, 'validation/num_examples': 43793, 'test/accuracy': 0.9847881197929382, 'test/loss': 0.05182139575481415, 'test/mean_average_precision': 0.17715077544494223, 'test/num_examples': 43793, 'score': 8895.445118188858, 'total_duration': 12888.176303863525, 'accumulated_submission_time': 8895.445118188858, 'accumulated_eval_time': 3990.845261335373, 'accumulated_logging_time': 1.1152944564819336}
I0206 00:27:19.955640 139735302588160 logging_writer.py:48] [27950] accumulated_eval_time=3990.845261, accumulated_logging_time=1.115294, accumulated_submission_time=8895.445118, global_step=27950, preemption_count=0, score=8895.445118, test/accuracy=0.984788, test/loss=0.051821, test/mean_average_precision=0.177151, test/num_examples=43793, total_duration=12888.176304, train/accuracy=0.988749, train/loss=0.038225, train/mean_average_precision=0.223859, validation/accuracy=0.985826, validation/loss=0.048663, validation/mean_average_precision=0.182840, validation/num_examples=43793
I0206 00:27:36.067111 139739177916160 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.052580349147319794, loss=0.03922547027468681
I0206 00:28:07.466979 139735302588160 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.05256933346390724, loss=0.041823502629995346
I0206 00:28:39.033452 139739177916160 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.03892161697149277, loss=0.03788056597113609
I0206 00:29:10.467333 139735302588160 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.04545753076672554, loss=0.032871589064598083
I0206 00:29:42.014969 139739177916160 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.05487450584769249, loss=0.040150418877601624
I0206 00:30:13.273754 139735302588160 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.09592653810977936, loss=0.04090971499681473
I0206 00:30:44.457425 139739177916160 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.06417970359325409, loss=0.04607011750340462
I0206 00:31:16.124760 139735302588160 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.02889290079474449, loss=0.03546645864844322
I0206 00:31:20.248855 139919816816448 spec.py:321] Evaluating on the training split.
I0206 00:32:57.753656 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 00:33:00.781038 139919816816448 spec.py:349] Evaluating on the test split.
I0206 00:33:03.824671 139919816816448 submission_runner.py:408] Time since start: 13232.07s, 	Step: 28714, 	{'train/accuracy': 0.9886225461959839, 'train/loss': 0.038634009659290314, 'train/mean_average_precision': 0.2167209401037269, 'validation/accuracy': 0.9856897592544556, 'validation/loss': 0.048975858837366104, 'validation/mean_average_precision': 0.17476428675753872, 'validation/num_examples': 43793, 'test/accuracy': 0.9847543835639954, 'test/loss': 0.05206332728266716, 'test/mean_average_precision': 0.17598293705416335, 'test/num_examples': 43793, 'score': 9135.70709347725, 'total_duration': 13232.066885709763, 'accumulated_submission_time': 9135.70709347725, 'accumulated_eval_time': 4094.420918226242, 'accumulated_logging_time': 1.1478347778320312}
I0206 00:33:03.845137 139735693903616 logging_writer.py:48] [28714] accumulated_eval_time=4094.420918, accumulated_logging_time=1.147835, accumulated_submission_time=9135.707093, global_step=28714, preemption_count=0, score=9135.707093, test/accuracy=0.984754, test/loss=0.052063, test/mean_average_precision=0.175983, test/num_examples=43793, total_duration=13232.066886, train/accuracy=0.988623, train/loss=0.038634, train/mean_average_precision=0.216721, validation/accuracy=0.985690, validation/loss=0.048976, validation/mean_average_precision=0.174764, validation/num_examples=43793
I0206 00:33:33.311836 139759004710656 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.03167556971311569, loss=0.03640103340148926
I0206 00:34:06.882738 139735693903616 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.09444102644920349, loss=0.04210726544260979
I0206 00:34:38.282290 139759004710656 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.030701790004968643, loss=0.03844870254397392
I0206 00:35:09.959234 139735693903616 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.040433116257190704, loss=0.03722564876079559
I0206 00:35:41.452843 139759004710656 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.08843297511339188, loss=0.04101322963833809
I0206 00:36:12.917157 139735693903616 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.051704734563827515, loss=0.04162485897541046
I0206 00:36:44.331422 139759004710656 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.04207143187522888, loss=0.041696105152368546
I0206 00:37:04.082411 139919816816448 spec.py:321] Evaluating on the training split.
I0206 00:38:42.650681 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 00:38:46.012661 139919816816448 spec.py:349] Evaluating on the test split.
I0206 00:38:49.323379 139919816816448 submission_runner.py:408] Time since start: 13577.57s, 	Step: 29464, 	{'train/accuracy': 0.9890115261077881, 'train/loss': 0.03765709698200226, 'train/mean_average_precision': 0.22815512080052855, 'validation/accuracy': 0.9858322143554688, 'validation/loss': 0.04821404442191124, 'validation/mean_average_precision': 0.18028048993088003, 'validation/num_examples': 43793, 'test/accuracy': 0.9848394989967346, 'test/loss': 0.05123216658830643, 'test/mean_average_precision': 0.17881495194181926, 'test/num_examples': 43793, 'score': 9375.913627147675, 'total_duration': 13577.565557718277, 'accumulated_submission_time': 9375.913627147675, 'accumulated_eval_time': 4199.661685228348, 'accumulated_logging_time': 1.1791231632232666}
I0206 00:38:49.346736 139735302588160 logging_writer.py:48] [29464] accumulated_eval_time=4199.661685, accumulated_logging_time=1.179123, accumulated_submission_time=9375.913627, global_step=29464, preemption_count=0, score=9375.913627, test/accuracy=0.984839, test/loss=0.051232, test/mean_average_precision=0.178815, test/num_examples=43793, total_duration=13577.565558, train/accuracy=0.989012, train/loss=0.037657, train/mean_average_precision=0.228155, validation/accuracy=0.985832, validation/loss=0.048214, validation/mean_average_precision=0.180280, validation/num_examples=43793
I0206 00:39:01.233494 139739177916160 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.08245163410902023, loss=0.040144484490156174
I0206 00:39:33.514532 139735302588160 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.07852090150117874, loss=0.041679974645376205
I0206 00:40:05.784214 139739177916160 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.03579966723918915, loss=0.036483943462371826
I0206 00:40:38.063375 139735302588160 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.05754511058330536, loss=0.039988406002521515
I0206 00:41:10.144274 139739177916160 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.0647275522351265, loss=0.038077134639024734
I0206 00:41:42.083132 139735302588160 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.09133652597665787, loss=0.04326993599534035
I0206 00:42:13.673772 139739177916160 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.05826052278280258, loss=0.03989564627408981
I0206 00:42:45.318575 139735302588160 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.06907851994037628, loss=0.03729450702667236
I0206 00:42:49.365029 139919816816448 spec.py:321] Evaluating on the training split.
I0206 00:44:27.248275 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 00:44:30.328928 139919816816448 spec.py:349] Evaluating on the test split.
I0206 00:44:33.349108 139919816816448 submission_runner.py:408] Time since start: 13921.59s, 	Step: 30214, 	{'train/accuracy': 0.9887194037437439, 'train/loss': 0.038636524230241776, 'train/mean_average_precision': 0.21387103798558202, 'validation/accuracy': 0.9857904314994812, 'validation/loss': 0.04857921600341797, 'validation/mean_average_precision': 0.18531219922152187, 'validation/num_examples': 43793, 'test/accuracy': 0.984809160232544, 'test/loss': 0.05179094895720482, 'test/mean_average_precision': 0.17941489697811877, 'test/num_examples': 43793, 'score': 9615.896932125092, 'total_duration': 13921.591437339783, 'accumulated_submission_time': 9615.896932125092, 'accumulated_eval_time': 4303.645714521408, 'accumulated_logging_time': 1.2145650386810303}
I0206 00:44:33.370080 139716992366336 logging_writer.py:48] [30214] accumulated_eval_time=4303.645715, accumulated_logging_time=1.214565, accumulated_submission_time=9615.896932, global_step=30214, preemption_count=0, score=9615.896932, test/accuracy=0.984809, test/loss=0.051791, test/mean_average_precision=0.179415, test/num_examples=43793, total_duration=13921.591437, train/accuracy=0.988719, train/loss=0.038637, train/mean_average_precision=0.213871, validation/accuracy=0.985790, validation/loss=0.048579, validation/mean_average_precision=0.185312, validation/num_examples=43793
I0206 00:45:01.341720 139759004710656 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.0318608358502388, loss=0.04028156027197838
I0206 00:45:33.188969 139716992366336 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.0915861576795578, loss=0.03717867657542229
I0206 00:46:05.434948 139759004710656 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.15278013050556183, loss=0.03805528208613396
I0206 00:46:37.237022 139716992366336 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.051373645663261414, loss=0.04333794489502907
I0206 00:47:08.761801 139759004710656 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.030980117619037628, loss=0.036046359688043594
I0206 00:47:40.553648 139716992366336 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.043768711388111115, loss=0.04104144871234894
I0206 00:48:12.008075 139759004710656 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.052625734359025955, loss=0.03945467993617058
I0206 00:48:33.443400 139919816816448 spec.py:321] Evaluating on the training split.
I0206 00:50:10.219450 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 00:50:13.279549 139919816816448 spec.py:349] Evaluating on the test split.
I0206 00:50:16.385816 139919816816448 submission_runner.py:408] Time since start: 14264.63s, 	Step: 30969, 	{'train/accuracy': 0.988382875919342, 'train/loss': 0.03963419795036316, 'train/mean_average_precision': 0.21936030365713555, 'validation/accuracy': 0.9854400753974915, 'validation/loss': 0.050116200000047684, 'validation/mean_average_precision': 0.18122469766378826, 'validation/num_examples': 43793, 'test/accuracy': 0.9845911860466003, 'test/loss': 0.053077880293130875, 'test/mean_average_precision': 0.1786106783703574, 'test/num_examples': 43793, 'score': 9855.93932056427, 'total_duration': 14264.628145694733, 'accumulated_submission_time': 9855.93932056427, 'accumulated_eval_time': 4406.588081121445, 'accumulated_logging_time': 1.246260166168213}
I0206 00:50:16.407388 139735302588160 logging_writer.py:48] [30969] accumulated_eval_time=4406.588081, accumulated_logging_time=1.246260, accumulated_submission_time=9855.939321, global_step=30969, preemption_count=0, score=9855.939321, test/accuracy=0.984591, test/loss=0.053078, test/mean_average_precision=0.178611, test/num_examples=43793, total_duration=14264.628146, train/accuracy=0.988383, train/loss=0.039634, train/mean_average_precision=0.219360, validation/accuracy=0.985440, validation/loss=0.050116, validation/mean_average_precision=0.181225, validation/num_examples=43793
I0206 00:50:26.431727 139739177916160 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.03214782103896141, loss=0.03714780882000923
I0206 00:50:58.017624 139735302588160 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.07273069769144058, loss=0.0393381305038929
I0206 00:51:30.022489 139739177916160 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.04946329444646835, loss=0.0406172089278698
I0206 00:52:01.856730 139735302588160 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.044039297848939896, loss=0.036580052226781845
I0206 00:52:34.039375 139739177916160 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.04252917692065239, loss=0.03511861339211464
I0206 00:53:06.197463 139735302588160 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.03082469291985035, loss=0.033746138215065
I0206 00:53:38.182148 139739177916160 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.05330365151166916, loss=0.04229184240102768
I0206 00:54:10.030714 139735302588160 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.09587930887937546, loss=0.04177503287792206
I0206 00:54:16.681937 139919816816448 spec.py:321] Evaluating on the training split.
I0206 00:55:55.545461 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 00:56:01.551160 139919816816448 spec.py:349] Evaluating on the test split.
I0206 00:56:05.256355 139919816816448 submission_runner.py:408] Time since start: 14613.50s, 	Step: 31722, 	{'train/accuracy': 0.9886970520019531, 'train/loss': 0.03857322037220001, 'train/mean_average_precision': 0.21386019552262678, 'validation/accuracy': 0.9858525395393372, 'validation/loss': 0.048105161637067795, 'validation/mean_average_precision': 0.18594841492022302, 'validation/num_examples': 43793, 'test/accuracy': 0.984890878200531, 'test/loss': 0.05095868930220604, 'test/mean_average_precision': 0.18221947399078975, 'test/num_examples': 43793, 'score': 10096.182999134064, 'total_duration': 14613.498676538467, 'accumulated_submission_time': 10096.182999134064, 'accumulated_eval_time': 4515.162441253662, 'accumulated_logging_time': 1.278688669204712}
I0206 00:56:05.278720 139716992366336 logging_writer.py:48] [31722] accumulated_eval_time=4515.162441, accumulated_logging_time=1.278689, accumulated_submission_time=10096.182999, global_step=31722, preemption_count=0, score=10096.182999, test/accuracy=0.984891, test/loss=0.050959, test/mean_average_precision=0.182219, test/num_examples=43793, total_duration=14613.498677, train/accuracy=0.988697, train/loss=0.038573, train/mean_average_precision=0.213860, validation/accuracy=0.985853, validation/loss=0.048105, validation/mean_average_precision=0.185948, validation/num_examples=43793
I0206 00:56:30.359317 139735693903616 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.04077041894197464, loss=0.03994578495621681
I0206 00:57:02.954347 139716992366336 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.04048920422792435, loss=0.037003785371780396
I0206 00:57:35.248743 139735693903616 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.043638985604047775, loss=0.04126046597957611
I0206 00:58:07.975159 139716992366336 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.07839618623256683, loss=0.03684258088469505
I0206 00:58:40.321572 139735693903616 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.058580756187438965, loss=0.039575736969709396
I0206 00:59:12.390198 139716992366336 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.043986726552248, loss=0.036321789026260376
I0206 00:59:44.311244 139735693903616 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.08980411291122437, loss=0.04174182564020157
I0206 01:00:05.383088 139919816816448 spec.py:321] Evaluating on the training split.
I0206 01:01:42.333456 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 01:01:45.420954 139919816816448 spec.py:349] Evaluating on the test split.
I0206 01:01:48.360629 139919816816448 submission_runner.py:408] Time since start: 14956.60s, 	Step: 32467, 	{'train/accuracy': 0.9887663125991821, 'train/loss': 0.038279566913843155, 'train/mean_average_precision': 0.22456073744872496, 'validation/accuracy': 0.9856763482093811, 'validation/loss': 0.048430588096380234, 'validation/mean_average_precision': 0.18508520542216764, 'validation/num_examples': 43793, 'test/accuracy': 0.9847211241722107, 'test/loss': 0.05151313170790672, 'test/mean_average_precision': 0.17428486357819395, 'test/num_examples': 43793, 'score': 10336.253832101822, 'total_duration': 14956.602952480316, 'accumulated_submission_time': 10336.253832101822, 'accumulated_eval_time': 4618.139933824539, 'accumulated_logging_time': 1.3119385242462158}
I0206 01:01:48.382235 139735302588160 logging_writer.py:48] [32467] accumulated_eval_time=4618.139934, accumulated_logging_time=1.311939, accumulated_submission_time=10336.253832, global_step=32467, preemption_count=0, score=10336.253832, test/accuracy=0.984721, test/loss=0.051513, test/mean_average_precision=0.174285, test/num_examples=43793, total_duration=14956.602952, train/accuracy=0.988766, train/loss=0.038280, train/mean_average_precision=0.224561, validation/accuracy=0.985676, validation/loss=0.048431, validation/mean_average_precision=0.185085, validation/num_examples=43793
I0206 01:01:59.348276 139739177916160 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.05437668785452843, loss=0.036723118275403976
I0206 01:02:31.353784 139735302588160 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.04983147233724594, loss=0.04144701734185219
I0206 01:03:03.046981 139739177916160 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.061235006898641586, loss=0.03651157394051552
I0206 01:03:34.775507 139735302588160 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.058693572878837585, loss=0.03851676732301712
I0206 01:04:07.049884 139739177916160 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.030152926221489906, loss=0.036831703037023544
I0206 01:04:38.779154 139735302588160 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.04597275331616402, loss=0.034971367567777634
I0206 01:05:10.826202 139739177916160 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.03814574331045151, loss=0.038642581552267075
I0206 01:05:43.233990 139735302588160 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.08089563995599747, loss=0.03809356316924095
I0206 01:05:48.430620 139919816816448 spec.py:321] Evaluating on the training split.
I0206 01:07:26.116470 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 01:07:29.118490 139919816816448 spec.py:349] Evaluating on the test split.
I0206 01:07:32.101071 139919816816448 submission_runner.py:408] Time since start: 15300.34s, 	Step: 33217, 	{'train/accuracy': 0.989039421081543, 'train/loss': 0.03772735968232155, 'train/mean_average_precision': 0.23207682323916878, 'validation/accuracy': 0.9859669804573059, 'validation/loss': 0.04795393720269203, 'validation/mean_average_precision': 0.19852442841848777, 'validation/num_examples': 43793, 'test/accuracy': 0.9849864840507507, 'test/loss': 0.05098487809300423, 'test/mean_average_precision': 0.18731190501152217, 'test/num_examples': 43793, 'score': 10576.26902961731, 'total_duration': 15300.3434009552, 'accumulated_submission_time': 10576.26902961731, 'accumulated_eval_time': 4721.810349941254, 'accumulated_logging_time': 1.3457691669464111}
I0206 01:07:32.123216 139735693903616 logging_writer.py:48] [33217] accumulated_eval_time=4721.810350, accumulated_logging_time=1.345769, accumulated_submission_time=10576.269030, global_step=33217, preemption_count=0, score=10576.269030, test/accuracy=0.984986, test/loss=0.050985, test/mean_average_precision=0.187312, test/num_examples=43793, total_duration=15300.343401, train/accuracy=0.989039, train/loss=0.037727, train/mean_average_precision=0.232077, validation/accuracy=0.985967, validation/loss=0.047954, validation/mean_average_precision=0.198524, validation/num_examples=43793
I0206 01:07:58.983504 139759004710656 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.05095011368393898, loss=0.037789054214954376
I0206 01:08:30.528975 139735693903616 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.0745411068201065, loss=0.04020779952406883
I0206 01:09:02.495484 139759004710656 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.05095385015010834, loss=0.03554942458868027
I0206 01:09:34.236388 139735693903616 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.04414414241909981, loss=0.03422170504927635
I0206 01:10:05.885915 139759004710656 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.04579909145832062, loss=0.03401724249124527
I0206 01:10:37.731382 139735693903616 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.04130905866622925, loss=0.03635575249791145
I0206 01:11:09.766643 139759004710656 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.07293234020471573, loss=0.04023449122905731
I0206 01:11:32.207043 139919816816448 spec.py:321] Evaluating on the training split.
I0206 01:13:10.379870 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 01:13:13.453020 139919816816448 spec.py:349] Evaluating on the test split.
I0206 01:13:16.460309 139919816816448 submission_runner.py:408] Time since start: 15644.70s, 	Step: 33971, 	{'train/accuracy': 0.9889031052589417, 'train/loss': 0.03798950836062431, 'train/mean_average_precision': 0.22943758440364542, 'validation/accuracy': 0.9858683347702026, 'validation/loss': 0.048191137611866, 'validation/mean_average_precision': 0.1919149423802982, 'validation/num_examples': 43793, 'test/accuracy': 0.9849759340286255, 'test/loss': 0.05101734399795532, 'test/mean_average_precision': 0.18969810832651524, 'test/num_examples': 43793, 'score': 10816.320188999176, 'total_duration': 15644.702628612518, 'accumulated_submission_time': 10816.320188999176, 'accumulated_eval_time': 4826.063565731049, 'accumulated_logging_time': 1.380265235900879}
I0206 01:13:16.482822 139716992366336 logging_writer.py:48] [33971] accumulated_eval_time=4826.063566, accumulated_logging_time=1.380265, accumulated_submission_time=10816.320189, global_step=33971, preemption_count=0, score=10816.320189, test/accuracy=0.984976, test/loss=0.051017, test/mean_average_precision=0.189698, test/num_examples=43793, total_duration=15644.702629, train/accuracy=0.988903, train/loss=0.037990, train/mean_average_precision=0.229438, validation/accuracy=0.985868, validation/loss=0.048191, validation/mean_average_precision=0.191915, validation/num_examples=43793
I0206 01:13:26.020509 139739177916160 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.06305722147226334, loss=0.04152863845229149
I0206 01:13:57.408304 139716992366336 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.09193950146436691, loss=0.03826995939016342
I0206 01:14:29.532743 139739177916160 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.04962274059653282, loss=0.03643730655312538
I0206 01:15:01.149523 139716992366336 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.08814434707164764, loss=0.03436002880334854
I0206 01:15:32.759706 139739177916160 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.12034523487091064, loss=0.03834330663084984
I0206 01:16:04.274206 139716992366336 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.041091952472925186, loss=0.036372508853673935
I0206 01:16:36.215269 139739177916160 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.07955138385295868, loss=0.033456262201070786
I0206 01:17:08.075645 139716992366336 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.06887305527925491, loss=0.03779835253953934
I0206 01:17:16.568458 139919816816448 spec.py:321] Evaluating on the training split.
I0206 01:18:56.711952 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 01:18:59.833140 139919816816448 spec.py:349] Evaluating on the test split.
I0206 01:19:02.987259 139919816816448 submission_runner.py:408] Time since start: 15991.23s, 	Step: 34728, 	{'train/accuracy': 0.9889699220657349, 'train/loss': 0.037672411650419235, 'train/mean_average_precision': 0.22945121345328973, 'validation/accuracy': 0.9858846068382263, 'validation/loss': 0.048056960105895996, 'validation/mean_average_precision': 0.18836389896567085, 'validation/num_examples': 43793, 'test/accuracy': 0.9849582314491272, 'test/loss': 0.05112144351005554, 'test/mean_average_precision': 0.1860170698764786, 'test/num_examples': 43793, 'score': 11056.37366938591, 'total_duration': 15991.229589700699, 'accumulated_submission_time': 11056.37366938591, 'accumulated_eval_time': 4932.482318401337, 'accumulated_logging_time': 1.4145681858062744}
I0206 01:19:03.009206 139735302588160 logging_writer.py:48] [34728] accumulated_eval_time=4932.482318, accumulated_logging_time=1.414568, accumulated_submission_time=11056.373669, global_step=34728, preemption_count=0, score=11056.373669, test/accuracy=0.984958, test/loss=0.051121, test/mean_average_precision=0.186017, test/num_examples=43793, total_duration=15991.229590, train/accuracy=0.988970, train/loss=0.037672, train/mean_average_precision=0.229451, validation/accuracy=0.985885, validation/loss=0.048057, validation/mean_average_precision=0.188364, validation/num_examples=43793
I0206 01:19:25.779557 139735693903616 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.06780557334423065, loss=0.03812030330300331
I0206 01:19:57.288415 139735302588160 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.0709419921040535, loss=0.03963048383593559
I0206 01:20:28.793844 139735693903616 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.04469982534646988, loss=0.03923463448882103
I0206 01:21:00.083798 139735302588160 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.051714587956666946, loss=0.039324820041656494
I0206 01:21:31.652518 139735693903616 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.07241295278072357, loss=0.0398264117538929
I0206 01:22:03.194344 139735302588160 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.06419403105974197, loss=0.03678305074572563
I0206 01:22:34.793150 139735693903616 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.055557165294885635, loss=0.039235565811395645
I0206 01:23:03.017509 139919816816448 spec.py:321] Evaluating on the training split.
I0206 01:24:42.378583 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 01:24:45.514836 139919816816448 spec.py:349] Evaluating on the test split.
I0206 01:24:48.585978 139919816816448 submission_runner.py:408] Time since start: 16336.83s, 	Step: 35491, 	{'train/accuracy': 0.9889968633651733, 'train/loss': 0.037359390407800674, 'train/mean_average_precision': 0.24800388102685672, 'validation/accuracy': 0.9859061241149902, 'validation/loss': 0.04793180152773857, 'validation/mean_average_precision': 0.18544006624674064, 'validation/num_examples': 43793, 'test/accuracy': 0.9849721789360046, 'test/loss': 0.050931718200445175, 'test/mean_average_precision': 0.18398665496825234, 'test/num_examples': 43793, 'score': 11296.35082745552, 'total_duration': 16336.828307151794, 'accumulated_submission_time': 11296.35082745552, 'accumulated_eval_time': 5038.0507435798645, 'accumulated_logging_time': 1.4474399089813232}
I0206 01:24:48.607829 139739177916160 logging_writer.py:48] [35491] accumulated_eval_time=5038.050744, accumulated_logging_time=1.447440, accumulated_submission_time=11296.350827, global_step=35491, preemption_count=0, score=11296.350827, test/accuracy=0.984972, test/loss=0.050932, test/mean_average_precision=0.183987, test/num_examples=43793, total_duration=16336.828307, train/accuracy=0.988997, train/loss=0.037359, train/mean_average_precision=0.248004, validation/accuracy=0.985906, validation/loss=0.047932, validation/mean_average_precision=0.185440, validation/num_examples=43793
I0206 01:24:51.766721 139759004710656 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.1308591067790985, loss=0.04389664903283119
I0206 01:25:23.585029 139739177916160 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.0370921865105629, loss=0.03638922795653343
I0206 01:25:55.341199 139759004710656 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.09065026044845581, loss=0.037535231560468674
I0206 01:26:27.108560 139739177916160 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.04691630229353905, loss=0.04209832102060318
I0206 01:26:59.150738 139759004710656 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.05758729949593544, loss=0.040537651628255844
I0206 01:27:30.889202 139739177916160 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.06773646175861359, loss=0.03621271625161171
I0206 01:28:02.674440 139759004710656 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.10631779581308365, loss=0.036873359233140945
I0206 01:28:34.408531 139739177916160 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.04311838746070862, loss=0.0381065271794796
I0206 01:28:48.854336 139919816816448 spec.py:321] Evaluating on the training split.
I0206 01:30:28.098489 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 01:30:31.461019 139919816816448 spec.py:349] Evaluating on the test split.
I0206 01:30:34.689237 139919816816448 submission_runner.py:408] Time since start: 16682.93s, 	Step: 36246, 	{'train/accuracy': 0.9889400601387024, 'train/loss': 0.03762971982359886, 'train/mean_average_precision': 0.22899459290143964, 'validation/accuracy': 0.9858976006507874, 'validation/loss': 0.04782705381512642, 'validation/mean_average_precision': 0.19022740009314312, 'validation/num_examples': 43793, 'test/accuracy': 0.9849587082862854, 'test/loss': 0.050787489861249924, 'test/mean_average_precision': 0.18202914172458476, 'test/num_examples': 43793, 'score': 11536.564465284348, 'total_duration': 16682.931549072266, 'accumulated_submission_time': 11536.564465284348, 'accumulated_eval_time': 5143.885582208633, 'accumulated_logging_time': 1.4814918041229248}
I0206 01:30:34.714283 139735302588160 logging_writer.py:48] [36246] accumulated_eval_time=5143.885582, accumulated_logging_time=1.481492, accumulated_submission_time=11536.564465, global_step=36246, preemption_count=0, score=11536.564465, test/accuracy=0.984959, test/loss=0.050787, test/mean_average_precision=0.182029, test/num_examples=43793, total_duration=16682.931549, train/accuracy=0.988940, train/loss=0.037630, train/mean_average_precision=0.228995, validation/accuracy=0.985898, validation/loss=0.047827, validation/mean_average_precision=0.190227, validation/num_examples=43793
I0206 01:30:52.453571 139735693903616 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.06876323372125626, loss=0.0410752147436142
I0206 01:31:24.473735 139735302588160 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.04499909281730652, loss=0.036030855029821396
I0206 01:31:57.003344 139735693903616 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.06701254099607468, loss=0.04268084838986397
I0206 01:32:29.689743 139735302588160 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.0953434556722641, loss=0.04116591811180115
I0206 01:33:02.133000 139735693903616 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.12141048908233643, loss=0.04359458014369011
I0206 01:33:34.640315 139735302588160 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.042047373950481415, loss=0.03636942431330681
I0206 01:34:06.742992 139735693903616 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.06344489008188248, loss=0.031472571194171906
I0206 01:34:34.850362 139919816816448 spec.py:321] Evaluating on the training split.
I0206 01:36:11.359392 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 01:36:14.424289 139919816816448 spec.py:349] Evaluating on the test split.
I0206 01:36:17.479639 139919816816448 submission_runner.py:408] Time since start: 17025.72s, 	Step: 36989, 	{'train/accuracy': 0.9891141057014465, 'train/loss': 0.03726128116250038, 'train/mean_average_precision': 0.23637925039261792, 'validation/accuracy': 0.9858058094978333, 'validation/loss': 0.04803333431482315, 'validation/mean_average_precision': 0.185534120927033, 'validation/num_examples': 43793, 'test/accuracy': 0.9848929643630981, 'test/loss': 0.05071304738521576, 'test/mean_average_precision': 0.1814267991041178, 'test/num_examples': 43793, 'score': 11776.666022777557, 'total_duration': 17025.721967220306, 'accumulated_submission_time': 11776.666022777557, 'accumulated_eval_time': 5246.514811038971, 'accumulated_logging_time': 1.5184621810913086}
I0206 01:36:17.502388 139739177916160 logging_writer.py:48] [36989] accumulated_eval_time=5246.514811, accumulated_logging_time=1.518462, accumulated_submission_time=11776.666023, global_step=36989, preemption_count=0, score=11776.666023, test/accuracy=0.984893, test/loss=0.050713, test/mean_average_precision=0.181427, test/num_examples=43793, total_duration=17025.721967, train/accuracy=0.989114, train/loss=0.037261, train/mean_average_precision=0.236379, validation/accuracy=0.985806, validation/loss=0.048033, validation/mean_average_precision=0.185534, validation/num_examples=43793
I0206 01:36:21.373704 139759004710656 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.07778537273406982, loss=0.0381050705909729
I0206 01:36:53.472605 139739177916160 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.0748804435133934, loss=0.04540001228451729
I0206 01:37:25.460796 139759004710656 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.10408186912536621, loss=0.040029726922512054
I0206 01:37:57.331083 139739177916160 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.05342433229088783, loss=0.037254322320222855
I0206 01:38:28.828104 139759004710656 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.04745280742645264, loss=0.03626104071736336
I0206 01:39:00.725510 139739177916160 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.04204355925321579, loss=0.03981709107756615
I0206 01:39:32.451742 139759004710656 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.08718571066856384, loss=0.04099439084529877
I0206 01:40:04.204492 139739177916160 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.06068787723779678, loss=0.04071202874183655
I0206 01:40:17.644050 139919816816448 spec.py:321] Evaluating on the training split.
I0206 01:41:55.223620 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 01:41:58.216645 139919816816448 spec.py:349] Evaluating on the test split.
I0206 01:42:01.204324 139919816816448 submission_runner.py:408] Time since start: 17369.45s, 	Step: 37742, 	{'train/accuracy': 0.9889169335365295, 'train/loss': 0.037845052778720856, 'train/mean_average_precision': 0.23672534421328206, 'validation/accuracy': 0.9857680797576904, 'validation/loss': 0.0481749102473259, 'validation/mean_average_precision': 0.18994630255589512, 'validation/num_examples': 43793, 'test/accuracy': 0.9848238825798035, 'test/loss': 0.05113251134753227, 'test/mean_average_precision': 0.1817323313886335, 'test/num_examples': 43793, 'score': 12016.776044130325, 'total_duration': 17369.446642637253, 'accumulated_submission_time': 12016.776044130325, 'accumulated_eval_time': 5350.075026988983, 'accumulated_logging_time': 1.5522980690002441}
I0206 01:42:01.227624 139716992366336 logging_writer.py:48] [37742] accumulated_eval_time=5350.075027, accumulated_logging_time=1.552298, accumulated_submission_time=12016.776044, global_step=37742, preemption_count=0, score=12016.776044, test/accuracy=0.984824, test/loss=0.051133, test/mean_average_precision=0.181732, test/num_examples=43793, total_duration=17369.446643, train/accuracy=0.988917, train/loss=0.037845, train/mean_average_precision=0.236725, validation/accuracy=0.985768, validation/loss=0.048175, validation/mean_average_precision=0.189946, validation/num_examples=43793
I0206 01:42:20.291490 139735693903616 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.053970642387866974, loss=0.03639518842101097
I0206 01:42:52.413219 139716992366336 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.050309665501117706, loss=0.038279298692941666
I0206 01:43:24.798482 139735693903616 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.042183827608823776, loss=0.04144834727048874
I0206 01:43:56.956221 139716992366336 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.09132160246372223, loss=0.03899480029940605
I0206 01:44:29.030425 139735693903616 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.13839328289031982, loss=0.04138723760843277
I0206 01:45:00.867580 139716992366336 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.04218395799398422, loss=0.03694974258542061
I0206 01:45:32.707187 139735693903616 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.0587405301630497, loss=0.040313683450222015
I0206 01:46:01.528818 139919816816448 spec.py:321] Evaluating on the training split.
I0206 01:47:36.379997 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 01:47:39.458583 139919816816448 spec.py:349] Evaluating on the test split.
I0206 01:47:42.414369 139919816816448 submission_runner.py:408] Time since start: 17710.66s, 	Step: 38492, 	{'train/accuracy': 0.9891027808189392, 'train/loss': 0.03733918070793152, 'train/mean_average_precision': 0.23002880038922274, 'validation/accuracy': 0.9859219193458557, 'validation/loss': 0.04792803153395653, 'validation/mean_average_precision': 0.19331691561223313, 'validation/num_examples': 43793, 'test/accuracy': 0.9850079417228699, 'test/loss': 0.05091828107833862, 'test/mean_average_precision': 0.1852429104902491, 'test/num_examples': 43793, 'score': 12257.044536828995, 'total_duration': 17710.656693458557, 'accumulated_submission_time': 12257.044536828995, 'accumulated_eval_time': 5450.9605281353, 'accumulated_logging_time': 1.5866341590881348}
I0206 01:47:42.436431 139735302588160 logging_writer.py:48] [38492] accumulated_eval_time=5450.960528, accumulated_logging_time=1.586634, accumulated_submission_time=12257.044537, global_step=38492, preemption_count=0, score=12257.044537, test/accuracy=0.985008, test/loss=0.050918, test/mean_average_precision=0.185243, test/num_examples=43793, total_duration=17710.656693, train/accuracy=0.989103, train/loss=0.037339, train/mean_average_precision=0.230029, validation/accuracy=0.985922, validation/loss=0.047928, validation/mean_average_precision=0.193317, validation/num_examples=43793
I0206 01:47:45.301815 139759004710656 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.10040391236543655, loss=0.035037338733673096
I0206 01:48:16.958972 139735302588160 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.06265266984701157, loss=0.036602720618247986
I0206 01:48:48.520996 139759004710656 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.04789090156555176, loss=0.04054361954331398
I0206 01:49:20.564077 139735302588160 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.12255540490150452, loss=0.038171086460351944
I0206 01:49:52.521765 139759004710656 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.09849037975072861, loss=0.0343906469643116
I0206 01:50:24.011530 139735302588160 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.05565476045012474, loss=0.04055473953485489
I0206 01:50:55.561042 139759004710656 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.053922493010759354, loss=0.03918032348155975
I0206 01:51:27.278875 139735302588160 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.056616172194480896, loss=0.040519922971725464
I0206 01:51:42.534553 139919816816448 spec.py:321] Evaluating on the training split.
I0206 01:53:17.259804 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 01:53:20.299192 139919816816448 spec.py:349] Evaluating on the test split.
I0206 01:53:23.282865 139919816816448 submission_runner.py:408] Time since start: 18051.53s, 	Step: 39249, 	{'train/accuracy': 0.9887359738349915, 'train/loss': 0.038305122405290604, 'train/mean_average_precision': 0.23918781732939232, 'validation/accuracy': 0.9858935475349426, 'validation/loss': 0.04855850338935852, 'validation/mean_average_precision': 0.1977071193646147, 'validation/num_examples': 43793, 'test/accuracy': 0.9849451780319214, 'test/loss': 0.05175068974494934, 'test/mean_average_precision': 0.18730468646185872, 'test/num_examples': 43793, 'score': 12497.11010313034, 'total_duration': 18051.52519583702, 'accumulated_submission_time': 12497.11010313034, 'accumulated_eval_time': 5551.708795070648, 'accumulated_logging_time': 1.6208436489105225}
I0206 01:53:23.305717 139716992366336 logging_writer.py:48] [39249] accumulated_eval_time=5551.708795, accumulated_logging_time=1.620844, accumulated_submission_time=12497.110103, global_step=39249, preemption_count=0, score=12497.110103, test/accuracy=0.984945, test/loss=0.051751, test/mean_average_precision=0.187305, test/num_examples=43793, total_duration=18051.525196, train/accuracy=0.988736, train/loss=0.038305, train/mean_average_precision=0.239188, validation/accuracy=0.985894, validation/loss=0.048559, validation/mean_average_precision=0.197707, validation/num_examples=43793
I0206 01:53:39.816446 139739177916160 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.05075159668922424, loss=0.037637364119291306
I0206 01:54:11.355225 139716992366336 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.08260795474052429, loss=0.04118723422288895
I0206 01:54:42.773952 139739177916160 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.05405399948358536, loss=0.04077240824699402
I0206 01:55:14.469232 139716992366336 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.06189015880227089, loss=0.03586409613490105
I0206 01:55:45.871424 139739177916160 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.10148611664772034, loss=0.03970447555184364
I0206 01:56:17.440433 139716992366336 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.07244802266359329, loss=0.04019714146852493
I0206 01:56:48.866765 139739177916160 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.09799709916114807, loss=0.03940460830926895
I0206 01:57:20.492880 139716992366336 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.07708019018173218, loss=0.03547389432787895
I0206 01:57:23.293747 139919816816448 spec.py:321] Evaluating on the training split.
I0206 01:58:57.681245 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 01:59:00.775840 139919816816448 spec.py:349] Evaluating on the test split.
I0206 01:59:03.868432 139919816816448 submission_runner.py:408] Time since start: 18392.11s, 	Step: 40010, 	{'train/accuracy': 0.9889850616455078, 'train/loss': 0.0373271107673645, 'train/mean_average_precision': 0.2340222648566641, 'validation/accuracy': 0.9859641790390015, 'validation/loss': 0.047820981591939926, 'validation/mean_average_precision': 0.19187176273520837, 'validation/num_examples': 43793, 'test/accuracy': 0.985103964805603, 'test/loss': 0.05049894377589226, 'test/mean_average_precision': 0.19605617002786224, 'test/num_examples': 43793, 'score': 12737.06660580635, 'total_duration': 18392.110761642456, 'accumulated_submission_time': 12737.06660580635, 'accumulated_eval_time': 5652.283429861069, 'accumulated_logging_time': 1.6545679569244385}
I0206 01:59:03.891863 139735302588160 logging_writer.py:48] [40010] accumulated_eval_time=5652.283430, accumulated_logging_time=1.654568, accumulated_submission_time=12737.066606, global_step=40010, preemption_count=0, score=12737.066606, test/accuracy=0.985104, test/loss=0.050499, test/mean_average_precision=0.196056, test/num_examples=43793, total_duration=18392.110762, train/accuracy=0.988985, train/loss=0.037327, train/mean_average_precision=0.234022, validation/accuracy=0.985964, validation/loss=0.047821, validation/mean_average_precision=0.191872, validation/num_examples=43793
I0206 01:59:32.676443 139735693903616 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.07857013493776321, loss=0.03886944428086281
I0206 02:00:04.379634 139735302588160 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.04695643112063408, loss=0.03582845255732536
I0206 02:00:35.772060 139735693903616 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.06647507846355438, loss=0.03442264348268509
I0206 02:01:07.421938 139735302588160 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.053717877715826035, loss=0.03772369772195816
I0206 02:01:39.551296 139735693903616 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.10316401720046997, loss=0.04254474863409996
I0206 02:02:12.174711 139735302588160 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.05565561726689339, loss=0.03335919231176376
I0206 02:02:44.128318 139735693903616 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.05530866980552673, loss=0.03449796512722969
I0206 02:03:04.140879 139919816816448 spec.py:321] Evaluating on the training split.
I0206 02:04:41.687544 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 02:04:44.770365 139919816816448 spec.py:349] Evaluating on the test split.
I0206 02:04:47.732863 139919816816448 submission_runner.py:408] Time since start: 18735.98s, 	Step: 40763, 	{'train/accuracy': 0.9889816045761108, 'train/loss': 0.037441350519657135, 'train/mean_average_precision': 0.229548285313721, 'validation/accuracy': 0.9859832525253296, 'validation/loss': 0.047773219645023346, 'validation/mean_average_precision': 0.19864646194954774, 'validation/num_examples': 43793, 'test/accuracy': 0.9850677847862244, 'test/loss': 0.05081846937537193, 'test/mean_average_precision': 0.19083416605251394, 'test/num_examples': 43793, 'score': 12977.28224658966, 'total_duration': 18735.97519636154, 'accumulated_submission_time': 12977.28224658966, 'accumulated_eval_time': 5755.875368833542, 'accumulated_logging_time': 1.6887316703796387}
I0206 02:04:47.756513 139739177916160 logging_writer.py:48] [40763] accumulated_eval_time=5755.875369, accumulated_logging_time=1.688732, accumulated_submission_time=12977.282247, global_step=40763, preemption_count=0, score=12977.282247, test/accuracy=0.985068, test/loss=0.050818, test/mean_average_precision=0.190834, test/num_examples=43793, total_duration=18735.975196, train/accuracy=0.988982, train/loss=0.037441, train/mean_average_precision=0.229548, validation/accuracy=0.985983, validation/loss=0.047773, validation/mean_average_precision=0.198646, validation/num_examples=43793
I0206 02:04:59.957441 139759004710656 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.045309390872716904, loss=0.03973826766014099
I0206 02:05:31.364312 139739177916160 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.08222341537475586, loss=0.03951779752969742
I0206 02:06:02.960858 139759004710656 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.06290370225906372, loss=0.03661230579018593
I0206 02:06:34.439109 139739177916160 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.07077772915363312, loss=0.03844192251563072
I0206 02:07:05.855775 139759004710656 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.06632818281650543, loss=0.03939271718263626
I0206 02:07:36.950247 139739177916160 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.15557652711868286, loss=0.04008444771170616
I0206 02:08:08.429730 139759004710656 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.08573180437088013, loss=0.04014308750629425
I0206 02:08:40.004056 139739177916160 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.04384322836995125, loss=0.0326690748333931
I0206 02:08:47.932128 139919816816448 spec.py:321] Evaluating on the training split.
I0206 02:10:27.133425 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 02:10:30.171590 139919816816448 spec.py:349] Evaluating on the test split.
I0206 02:10:33.190703 139919816816448 submission_runner.py:408] Time since start: 19081.43s, 	Step: 41526, 	{'train/accuracy': 0.988987386226654, 'train/loss': 0.0376761332154274, 'train/mean_average_precision': 0.23214384180276643, 'validation/accuracy': 0.9857603907585144, 'validation/loss': 0.04862070456147194, 'validation/mean_average_precision': 0.18923447743174016, 'validation/num_examples': 43793, 'test/accuracy': 0.9847068190574646, 'test/loss': 0.05169043317437172, 'test/mean_average_precision': 0.18251327295114547, 'test/num_examples': 43793, 'score': 13217.424660682678, 'total_duration': 19081.433025598526, 'accumulated_submission_time': 13217.424660682678, 'accumulated_eval_time': 5861.133890390396, 'accumulated_logging_time': 1.7246840000152588}
I0206 02:10:33.214747 139716992366336 logging_writer.py:48] [41526] accumulated_eval_time=5861.133890, accumulated_logging_time=1.724684, accumulated_submission_time=13217.424661, global_step=41526, preemption_count=0, score=13217.424661, test/accuracy=0.984707, test/loss=0.051690, test/mean_average_precision=0.182513, test/num_examples=43793, total_duration=19081.433026, train/accuracy=0.988987, train/loss=0.037676, train/mean_average_precision=0.232144, validation/accuracy=0.985760, validation/loss=0.048621, validation/mean_average_precision=0.189234, validation/num_examples=43793
I0206 02:10:56.787143 139735693903616 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.084874726831913, loss=0.03781605511903763
I0206 02:11:28.373838 139716992366336 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.042016658931970596, loss=0.03968193754553795
I0206 02:11:59.821443 139735693903616 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.09491495788097382, loss=0.043965429067611694
I0206 02:12:31.472276 139716992366336 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.05054601654410362, loss=0.03804166615009308
I0206 02:13:02.861674 139735693903616 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.05950791761279106, loss=0.03456335887312889
I0206 02:13:34.389419 139716992366336 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.09788623452186584, loss=0.038913317024707794
I0206 02:14:06.097270 139735693903616 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.08870076388120651, loss=0.03441668301820755
I0206 02:14:33.482540 139919816816448 spec.py:321] Evaluating on the training split.
I0206 02:16:15.186146 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 02:16:18.238035 139919816816448 spec.py:349] Evaluating on the test split.
I0206 02:16:21.236105 139919816816448 submission_runner.py:408] Time since start: 19429.48s, 	Step: 42287, 	{'train/accuracy': 0.9890965819358826, 'train/loss': 0.03697286173701286, 'train/mean_average_precision': 0.24316292176873197, 'validation/accuracy': 0.9859690070152283, 'validation/loss': 0.04771135374903679, 'validation/mean_average_precision': 0.1947566100337512, 'validation/num_examples': 43793, 'test/accuracy': 0.9851654767990112, 'test/loss': 0.05071566626429558, 'test/mean_average_precision': 0.1907572109509011, 'test/num_examples': 43793, 'score': 13457.660992622375, 'total_duration': 19429.478434562683, 'accumulated_submission_time': 13457.660992622375, 'accumulated_eval_time': 5968.8874089717865, 'accumulated_logging_time': 1.7596709728240967}
I0206 02:16:21.260433 139735302588160 logging_writer.py:48] [42287] accumulated_eval_time=5968.887409, accumulated_logging_time=1.759671, accumulated_submission_time=13457.660993, global_step=42287, preemption_count=0, score=13457.660993, test/accuracy=0.985165, test/loss=0.050716, test/mean_average_precision=0.190757, test/num_examples=43793, total_duration=19429.478435, train/accuracy=0.989097, train/loss=0.036973, train/mean_average_precision=0.243163, validation/accuracy=0.985969, validation/loss=0.047711, validation/mean_average_precision=0.194757, validation/num_examples=43793
I0206 02:16:25.820815 139739177916160 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.07510165870189667, loss=0.038804829120635986
I0206 02:16:57.719590 139735302588160 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.10762885212898254, loss=0.03769384324550629
I0206 02:17:29.559486 139739177916160 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.07390706986188889, loss=0.03541602939367294
I0206 02:18:01.696775 139735302588160 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.04018928110599518, loss=0.0366390235722065
I0206 02:18:33.372894 139739177916160 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.09999474883079529, loss=0.03792952373623848
I0206 02:19:04.999195 139735302588160 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.046492673456668854, loss=0.0362626276910305
I0206 02:19:36.755482 139739177916160 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.0842752605676651, loss=0.03568708524107933
I0206 02:20:08.359256 139735302588160 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.10130045562982559, loss=0.03681202232837677
I0206 02:20:21.279388 139919816816448 spec.py:321] Evaluating on the training split.
I0206 02:21:58.195325 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 02:22:01.305400 139919816816448 spec.py:349] Evaluating on the test split.
I0206 02:22:04.283466 139919816816448 submission_runner.py:408] Time since start: 19772.53s, 	Step: 43042, 	{'train/accuracy': 0.98912513256073, 'train/loss': 0.03699813038110733, 'train/mean_average_precision': 0.2451984672837403, 'validation/accuracy': 0.9858740568161011, 'validation/loss': 0.04763544350862503, 'validation/mean_average_precision': 0.19165971281291913, 'validation/num_examples': 43793, 'test/accuracy': 0.9849687814712524, 'test/loss': 0.050660889595746994, 'test/mean_average_precision': 0.1921175012712416, 'test/num_examples': 43793, 'score': 13697.648321390152, 'total_duration': 19772.52578687668, 'accumulated_submission_time': 13697.648321390152, 'accumulated_eval_time': 6071.891428232193, 'accumulated_logging_time': 1.7949151992797852}
I0206 02:22:04.307061 139716992366336 logging_writer.py:48] [43042] accumulated_eval_time=6071.891428, accumulated_logging_time=1.794915, accumulated_submission_time=13697.648321, global_step=43042, preemption_count=0, score=13697.648321, test/accuracy=0.984969, test/loss=0.050661, test/mean_average_precision=0.192118, test/num_examples=43793, total_duration=19772.525787, train/accuracy=0.989125, train/loss=0.036998, train/mean_average_precision=0.245198, validation/accuracy=0.985874, validation/loss=0.047635, validation/mean_average_precision=0.191660, validation/num_examples=43793
I0206 02:22:23.064384 139759004710656 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.06884285062551498, loss=0.03439992666244507
I0206 02:22:54.503119 139716992366336 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.07247327268123627, loss=0.034254349768161774
I0206 02:23:26.282252 139759004710656 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.057879380881786346, loss=0.038407038897275925
I0206 02:23:57.645750 139716992366336 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.04335157573223114, loss=0.036972157657146454
I0206 02:24:29.052772 139759004710656 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.06741274893283844, loss=0.0394216850399971
I0206 02:25:00.875354 139716992366336 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.04388444498181343, loss=0.032210562378168106
I0206 02:25:32.479749 139759004710656 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.08110839128494263, loss=0.03923840820789337
I0206 02:26:04.141076 139716992366336 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.0823603942990303, loss=0.03908076509833336
I0206 02:26:04.458034 139919816816448 spec.py:321] Evaluating on the training split.
I0206 02:27:42.951790 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 02:27:45.958425 139919816816448 spec.py:349] Evaluating on the test split.
I0206 02:27:48.896093 139919816816448 submission_runner.py:408] Time since start: 20117.14s, 	Step: 43802, 	{'train/accuracy': 0.9891272783279419, 'train/loss': 0.03686603158712387, 'train/mean_average_precision': 0.24670075616815768, 'validation/accuracy': 0.9859893321990967, 'validation/loss': 0.04760757461190224, 'validation/mean_average_precision': 0.18880544708293598, 'validation/num_examples': 43793, 'test/accuracy': 0.9850454330444336, 'test/loss': 0.050481002777814865, 'test/mean_average_precision': 0.1850773565321149, 'test/num_examples': 43793, 'score': 13937.767841815948, 'total_duration': 20117.138426303864, 'accumulated_submission_time': 13937.767841815948, 'accumulated_eval_time': 6176.329439401627, 'accumulated_logging_time': 1.8292901515960693}
I0206 02:27:48.920664 139735302588160 logging_writer.py:48] [43802] accumulated_eval_time=6176.329439, accumulated_logging_time=1.829290, accumulated_submission_time=13937.767842, global_step=43802, preemption_count=0, score=13937.767842, test/accuracy=0.985045, test/loss=0.050481, test/mean_average_precision=0.185077, test/num_examples=43793, total_duration=20117.138426, train/accuracy=0.989127, train/loss=0.036866, train/mean_average_precision=0.246701, validation/accuracy=0.985989, validation/loss=0.047608, validation/mean_average_precision=0.188805, validation/num_examples=43793
I0206 02:28:20.076916 139739177916160 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.06782879680395126, loss=0.03336181119084358
I0206 02:28:51.643385 139735302588160 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.04710518941283226, loss=0.04174524545669556
I0206 02:29:23.014672 139739177916160 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.05556550994515419, loss=0.037796661257743835
I0206 02:29:54.226235 139735302588160 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.07804078608751297, loss=0.04386214539408684
I0206 02:30:25.767691 139739177916160 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.10300015658140182, loss=0.04199931398034096
I0206 02:30:57.080085 139735302588160 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.08883015066385269, loss=0.038779906928539276
I0206 02:31:28.990651 139739177916160 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.09931690245866776, loss=0.03397377207875252
I0206 02:31:49.177889 139919816816448 spec.py:321] Evaluating on the training split.
I0206 02:33:23.768566 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 02:33:26.792186 139919816816448 spec.py:349] Evaluating on the test split.
I0206 02:33:29.772586 139919816816448 submission_runner.py:408] Time since start: 20458.01s, 	Step: 44565, 	{'train/accuracy': 0.9892011284828186, 'train/loss': 0.036679789423942566, 'train/mean_average_precision': 0.25198689902907984, 'validation/accuracy': 0.9859905242919922, 'validation/loss': 0.0475577712059021, 'validation/mean_average_precision': 0.19386990843271576, 'validation/num_examples': 43793, 'test/accuracy': 0.9850686192512512, 'test/loss': 0.050529588013887405, 'test/mean_average_precision': 0.19110150840640316, 'test/num_examples': 43793, 'score': 14177.993677854538, 'total_duration': 20458.01491880417, 'accumulated_submission_time': 14177.993677854538, 'accumulated_eval_time': 6276.924092531204, 'accumulated_logging_time': 1.8644392490386963}
I0206 02:33:29.796005 139716992366336 logging_writer.py:48] [44565] accumulated_eval_time=6276.924093, accumulated_logging_time=1.864439, accumulated_submission_time=14177.993678, global_step=44565, preemption_count=0, score=14177.993678, test/accuracy=0.985069, test/loss=0.050530, test/mean_average_precision=0.191102, test/num_examples=43793, total_duration=20458.014919, train/accuracy=0.989201, train/loss=0.036680, train/mean_average_precision=0.251987, validation/accuracy=0.985991, validation/loss=0.047558, validation/mean_average_precision=0.193870, validation/num_examples=43793
I0206 02:33:41.141909 139759004710656 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.04820381477475166, loss=0.0377381257712841
I0206 02:34:12.834123 139716992366336 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.07133036851882935, loss=0.03355349227786064
I0206 02:34:44.268593 139759004710656 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.17229513823986053, loss=0.03304154798388481
I0206 02:35:15.422067 139716992366336 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.050137314945459366, loss=0.037303026765584946
I0206 02:35:47.276710 139759004710656 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.05931611731648445, loss=0.037708912044763565
I0206 02:36:18.813549 139716992366336 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.055820100009441376, loss=0.03820692375302315
I0206 02:36:50.448388 139759004710656 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.06388512998819351, loss=0.03676623851060867
I0206 02:37:22.552850 139716992366336 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.06829892843961716, loss=0.03404765576124191
I0206 02:37:29.954892 139919816816448 spec.py:321] Evaluating on the training split.
I0206 02:39:09.192209 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 02:39:12.293462 139919816816448 spec.py:349] Evaluating on the test split.
I0206 02:39:15.395436 139919816816448 submission_runner.py:408] Time since start: 20803.64s, 	Step: 45324, 	{'train/accuracy': 0.9893069863319397, 'train/loss': 0.03671472892165184, 'train/mean_average_precision': 0.2476210653071464, 'validation/accuracy': 0.9859580397605896, 'validation/loss': 0.04814805090427399, 'validation/mean_average_precision': 0.19630345933155982, 'validation/num_examples': 43793, 'test/accuracy': 0.9850033521652222, 'test/loss': 0.051478613168001175, 'test/mean_average_precision': 0.1892441479288406, 'test/num_examples': 43793, 'score': 14418.119437456131, 'total_duration': 20803.637764453888, 'accumulated_submission_time': 14418.119437456131, 'accumulated_eval_time': 6382.364597320557, 'accumulated_logging_time': 1.9001915454864502}
I0206 02:39:15.420204 139735302588160 logging_writer.py:48] [45324] accumulated_eval_time=6382.364597, accumulated_logging_time=1.900192, accumulated_submission_time=14418.119437, global_step=45324, preemption_count=0, score=14418.119437, test/accuracy=0.985003, test/loss=0.051479, test/mean_average_precision=0.189244, test/num_examples=43793, total_duration=20803.637764, train/accuracy=0.989307, train/loss=0.036715, train/mean_average_precision=0.247621, validation/accuracy=0.985958, validation/loss=0.048148, validation/mean_average_precision=0.196303, validation/num_examples=43793
I0206 02:39:39.825321 139735693903616 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.053402625024318695, loss=0.03646315261721611
I0206 02:40:11.873744 139735302588160 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.10177171975374222, loss=0.035246193408966064
I0206 02:40:43.616877 139735693903616 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.1753658801317215, loss=0.042442742735147476
I0206 02:41:15.885851 139735302588160 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.10329856723546982, loss=0.035326939076185226
I0206 02:41:48.147974 139735693903616 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.08061861991882324, loss=0.041095610707998276
I0206 02:42:20.050787 139735302588160 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.08478043973445892, loss=0.04075898602604866
I0206 02:42:52.014715 139735693903616 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.06761688739061356, loss=0.03597384691238403
I0206 02:43:15.476638 139919816816448 spec.py:321] Evaluating on the training split.
I0206 02:44:54.648317 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 02:44:57.769960 139919816816448 spec.py:349] Evaluating on the test split.
I0206 02:45:00.938397 139919816816448 submission_runner.py:408] Time since start: 21149.18s, 	Step: 46075, 	{'train/accuracy': 0.989287257194519, 'train/loss': 0.03662101551890373, 'train/mean_average_precision': 0.25164082840433777, 'validation/accuracy': 0.9860518574714661, 'validation/loss': 0.04700767621397972, 'validation/mean_average_precision': 0.19854097081874142, 'validation/num_examples': 43793, 'test/accuracy': 0.9850778579711914, 'test/loss': 0.05002352595329285, 'test/mean_average_precision': 0.19016802194958837, 'test/num_examples': 43793, 'score': 14658.144562959671, 'total_duration': 21149.18072462082, 'accumulated_submission_time': 14658.144562959671, 'accumulated_eval_time': 6487.826305150986, 'accumulated_logging_time': 1.9359188079833984}
I0206 02:45:00.962329 139716992366336 logging_writer.py:48] [46075] accumulated_eval_time=6487.826305, accumulated_logging_time=1.935919, accumulated_submission_time=14658.144563, global_step=46075, preemption_count=0, score=14658.144563, test/accuracy=0.985078, test/loss=0.050024, test/mean_average_precision=0.190168, test/num_examples=43793, total_duration=21149.180725, train/accuracy=0.989287, train/loss=0.036621, train/mean_average_precision=0.251641, validation/accuracy=0.986052, validation/loss=0.047008, validation/mean_average_precision=0.198541, validation/num_examples=43793
I0206 02:45:09.202059 139759004710656 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.09489418566226959, loss=0.034003857523202896
I0206 02:45:40.617718 139716992366336 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.07498277723789215, loss=0.03631167113780975
I0206 02:46:12.392873 139759004710656 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.07140657305717468, loss=0.04025682806968689
I0206 02:46:44.294910 139716992366336 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.08660296350717545, loss=0.03399877995252609
I0206 02:47:16.295678 139759004710656 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.07746049761772156, loss=0.032429616898298264
I0206 02:47:48.018327 139716992366336 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.1643994003534317, loss=0.03928159177303314
I0206 02:48:19.438153 139759004710656 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.05085036903619766, loss=0.0372021310031414
I0206 02:48:51.693989 139716992366336 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.08730089664459229, loss=0.03765278682112694
I0206 02:49:01.135604 139919816816448 spec.py:321] Evaluating on the training split.
I0206 02:50:38.858577 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 02:50:42.007273 139919816816448 spec.py:349] Evaluating on the test split.
I0206 02:50:45.091661 139919816816448 submission_runner.py:408] Time since start: 21493.33s, 	Step: 46830, 	{'train/accuracy': 0.9892902970314026, 'train/loss': 0.03647993132472038, 'train/mean_average_precision': 0.2555223045112521, 'validation/accuracy': 0.9860911965370178, 'validation/loss': 0.04714075103402138, 'validation/mean_average_precision': 0.20659838503066164, 'validation/num_examples': 43793, 'test/accuracy': 0.9850918054580688, 'test/loss': 0.05010078102350235, 'test/mean_average_precision': 0.20143208691363684, 'test/num_examples': 43793, 'score': 14898.28629231453, 'total_duration': 21493.333992242813, 'accumulated_submission_time': 14898.28629231453, 'accumulated_eval_time': 6591.78231549263, 'accumulated_logging_time': 1.970759391784668}
I0206 02:50:45.115726 139735302588160 logging_writer.py:48] [46830] accumulated_eval_time=6591.782315, accumulated_logging_time=1.970759, accumulated_submission_time=14898.286292, global_step=46830, preemption_count=0, score=14898.286292, test/accuracy=0.985092, test/loss=0.050101, test/mean_average_precision=0.201432, test/num_examples=43793, total_duration=21493.333992, train/accuracy=0.989290, train/loss=0.036480, train/mean_average_precision=0.255522, validation/accuracy=0.986091, validation/loss=0.047141, validation/mean_average_precision=0.206598, validation/num_examples=43793
I0206 02:51:07.674377 139735693903616 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.05879330635070801, loss=0.03418039157986641
I0206 02:51:39.293147 139735302588160 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.090290367603302, loss=0.03730223700404167
I0206 02:52:11.060979 139735693903616 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.04776088148355484, loss=0.039709579199552536
I0206 02:52:42.276382 139735302588160 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.1194830909371376, loss=0.035068657249212265
I0206 02:53:14.301258 139735693903616 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.07544486969709396, loss=0.037841930985450745
I0206 02:53:46.253021 139735302588160 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.1012885570526123, loss=0.0372304692864418
I0206 02:54:17.947000 139735693903616 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.05274108797311783, loss=0.037278030067682266
I0206 02:54:45.381661 139919816816448 spec.py:321] Evaluating on the training split.
I0206 02:56:29.893134 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 02:56:33.250456 139919816816448 spec.py:349] Evaluating on the test split.
I0206 02:56:36.518999 139919816816448 submission_runner.py:408] Time since start: 21844.76s, 	Step: 47588, 	{'train/accuracy': 0.9891781806945801, 'train/loss': 0.03695470094680786, 'train/mean_average_precision': 0.25523705425384435, 'validation/accuracy': 0.9860566854476929, 'validation/loss': 0.047206051647663116, 'validation/mean_average_precision': 0.20168868363425443, 'validation/num_examples': 43793, 'test/accuracy': 0.9851625561714172, 'test/loss': 0.050044842064380646, 'test/mean_average_precision': 0.1926117407578925, 'test/num_examples': 43793, 'score': 15138.5211622715, 'total_duration': 21844.761209726334, 'accumulated_submission_time': 15138.5211622715, 'accumulated_eval_time': 6702.919489622116, 'accumulated_logging_time': 2.0053157806396484}
I0206 02:56:36.545609 139716992366336 logging_writer.py:48] [47588] accumulated_eval_time=6702.919490, accumulated_logging_time=2.005316, accumulated_submission_time=15138.521162, global_step=47588, preemption_count=0, score=15138.521162, test/accuracy=0.985163, test/loss=0.050045, test/mean_average_precision=0.192612, test/num_examples=43793, total_duration=21844.761210, train/accuracy=0.989178, train/loss=0.036955, train/mean_average_precision=0.255237, validation/accuracy=0.986057, validation/loss=0.047206, validation/mean_average_precision=0.201689, validation/num_examples=43793
I0206 02:56:40.907561 139739177916160 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.057408493012189865, loss=0.03740252926945686
I0206 02:57:12.890892 139716992366336 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.05755907669663429, loss=0.040178630501031876
I0206 02:57:44.100790 139739177916160 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.06369435787200928, loss=0.03653956577181816
I0206 02:58:15.438331 139716992366336 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.1023905798792839, loss=0.03926810622215271
I0206 02:58:46.719267 139739177916160 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.07813861221075058, loss=0.03510642424225807
I0206 02:59:18.234989 139716992366336 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.07728878408670425, loss=0.02949489839375019
I0206 02:59:49.764117 139739177916160 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.054597120732069016, loss=0.03413967415690422
I0206 03:00:21.325137 139716992366336 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.04588347673416138, loss=0.034453071653842926
I0206 03:00:36.531592 139919816816448 spec.py:321] Evaluating on the training split.
I0206 03:02:12.099894 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 03:02:15.098542 139919816816448 spec.py:349] Evaluating on the test split.
I0206 03:02:18.110527 139919816816448 submission_runner.py:408] Time since start: 22186.35s, 	Step: 48349, 	{'train/accuracy': 0.9893152117729187, 'train/loss': 0.03633428364992142, 'train/mean_average_precision': 0.24807640584328788, 'validation/accuracy': 0.9860628247261047, 'validation/loss': 0.047021202743053436, 'validation/mean_average_precision': 0.20590093571940493, 'validation/num_examples': 43793, 'test/accuracy': 0.9851372838020325, 'test/loss': 0.04998750612139702, 'test/mean_average_precision': 0.19512421181932446, 'test/num_examples': 43793, 'score': 15378.47325873375, 'total_duration': 22186.35283780098, 'accumulated_submission_time': 15378.47325873375, 'accumulated_eval_time': 6804.498358488083, 'accumulated_logging_time': 2.044494390487671}
I0206 03:02:18.135290 139735302588160 logging_writer.py:48] [48349] accumulated_eval_time=6804.498358, accumulated_logging_time=2.044494, accumulated_submission_time=15378.473259, global_step=48349, preemption_count=0, score=15378.473259, test/accuracy=0.985137, test/loss=0.049988, test/mean_average_precision=0.195124, test/num_examples=43793, total_duration=22186.352838, train/accuracy=0.989315, train/loss=0.036334, train/mean_average_precision=0.248076, validation/accuracy=0.986063, validation/loss=0.047021, validation/mean_average_precision=0.205901, validation/num_examples=43793
I0206 03:02:34.603982 139735693903616 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.06879066675901413, loss=0.04099750891327858
I0206 03:03:06.070375 139735302588160 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.0666034147143364, loss=0.0337417870759964
I0206 03:03:37.215097 139735693903616 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.07902990281581879, loss=0.034419383853673935
I0206 03:04:08.873067 139735302588160 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.10840681940317154, loss=0.04025469347834587
I0206 03:04:40.071719 139735693903616 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.08156467974185944, loss=0.032770704478025436
I0206 03:05:11.885331 139735302588160 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.04937214031815529, loss=0.034173633903265
I0206 03:05:43.191147 139735693903616 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.09210476279258728, loss=0.038875870406627655
I0206 03:06:14.613620 139735302588160 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.05629254877567291, loss=0.039388928562402725
I0206 03:06:18.121996 139919816816448 spec.py:321] Evaluating on the training split.
I0206 03:07:55.950336 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 03:07:59.004462 139919816816448 spec.py:349] Evaluating on the test split.
I0206 03:08:02.057185 139919816816448 submission_runner.py:408] Time since start: 22530.30s, 	Step: 49112, 	{'train/accuracy': 0.9893460273742676, 'train/loss': 0.03618969768285751, 'train/mean_average_precision': 0.24361136020841997, 'validation/accuracy': 0.9861687421798706, 'validation/loss': 0.046943336725234985, 'validation/mean_average_precision': 0.20543517990381355, 'validation/num_examples': 43793, 'test/accuracy': 0.985241711139679, 'test/loss': 0.04996630921959877, 'test/mean_average_precision': 0.19933563518916733, 'test/num_examples': 43793, 'score': 15618.42730998993, 'total_duration': 22530.29949116707, 'accumulated_submission_time': 15618.42730998993, 'accumulated_eval_time': 6908.433473587036, 'accumulated_logging_time': 2.0814638137817383}
I0206 03:08:02.086694 139716992366336 logging_writer.py:48] [49112] accumulated_eval_time=6908.433474, accumulated_logging_time=2.081464, accumulated_submission_time=15618.427310, global_step=49112, preemption_count=0, score=15618.427310, test/accuracy=0.985242, test/loss=0.049966, test/mean_average_precision=0.199336, test/num_examples=43793, total_duration=22530.299491, train/accuracy=0.989346, train/loss=0.036190, train/mean_average_precision=0.243611, validation/accuracy=0.986169, validation/loss=0.046943, validation/mean_average_precision=0.205435, validation/num_examples=43793
I0206 03:08:30.867850 139759004710656 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.07400132715702057, loss=0.036275364458560944
I0206 03:09:03.341342 139716992366336 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.062093980610370636, loss=0.034847717732191086
I0206 03:09:34.604218 139759004710656 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.10965980589389801, loss=0.037968140095472336
I0206 03:10:06.010722 139716992366336 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.09324204921722412, loss=0.03445538505911827
I0206 03:10:37.432074 139759004710656 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.06851206719875336, loss=0.03731076419353485
I0206 03:11:08.940580 139716992366336 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.08054742217063904, loss=0.034277550876140594
I0206 03:11:40.528427 139759004710656 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.0958440750837326, loss=0.039518896490335464
I0206 03:12:02.325783 139919816816448 spec.py:321] Evaluating on the training split.
I0206 03:13:37.260598 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 03:13:40.258407 139919816816448 spec.py:349] Evaluating on the test split.
I0206 03:13:43.269345 139919816816448 submission_runner.py:408] Time since start: 22871.51s, 	Step: 49869, 	{'train/accuracy': 0.9894251823425293, 'train/loss': 0.036164749413728714, 'train/mean_average_precision': 0.2601694948034501, 'validation/accuracy': 0.9860376119613647, 'validation/loss': 0.04690441116690636, 'validation/mean_average_precision': 0.20304835872201743, 'validation/num_examples': 43793, 'test/accuracy': 0.9851536750793457, 'test/loss': 0.049883656203746796, 'test/mean_average_precision': 0.20062726799576788, 'test/num_examples': 43793, 'score': 15858.633188962936, 'total_duration': 22871.511667251587, 'accumulated_submission_time': 15858.633188962936, 'accumulated_eval_time': 7009.377001285553, 'accumulated_logging_time': 2.1225428581237793}
I0206 03:13:43.293799 139735302588160 logging_writer.py:48] [49869] accumulated_eval_time=7009.377001, accumulated_logging_time=2.122543, accumulated_submission_time=15858.633189, global_step=49869, preemption_count=0, score=15858.633189, test/accuracy=0.985154, test/loss=0.049884, test/mean_average_precision=0.200627, test/num_examples=43793, total_duration=22871.511667, train/accuracy=0.989425, train/loss=0.036165, train/mean_average_precision=0.260169, validation/accuracy=0.986038, validation/loss=0.046904, validation/mean_average_precision=0.203048, validation/num_examples=43793
I0206 03:13:53.347327 139735693903616 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.06011783704161644, loss=0.03398813679814339
I0206 03:14:24.798363 139735302588160 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.0982319787144661, loss=0.03388679027557373
I0206 03:14:56.366622 139735693903616 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.1353614628314972, loss=0.035273659974336624
I0206 03:15:27.630569 139735302588160 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.10076890140771866, loss=0.03809582069516182
I0206 03:15:59.407882 139735693903616 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.05746723338961601, loss=0.037310946732759476
I0206 03:16:30.675164 139735302588160 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.09631065279245377, loss=0.03853754699230194
I0206 03:17:02.317611 139735693903616 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.10172374546527863, loss=0.03945624828338623
I0206 03:17:34.015292 139735302588160 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.06718325614929199, loss=0.03150045871734619
I0206 03:17:43.468868 139919816816448 spec.py:321] Evaluating on the training split.
I0206 03:19:21.351863 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 03:19:24.434804 139919816816448 spec.py:349] Evaluating on the test split.
I0206 03:19:27.397291 139919816816448 submission_runner.py:408] Time since start: 23215.64s, 	Step: 50631, 	{'train/accuracy': 0.9894543290138245, 'train/loss': 0.035730570554733276, 'train/mean_average_precision': 0.26220534981854027, 'validation/accuracy': 0.9860794544219971, 'validation/loss': 0.04686795175075531, 'validation/mean_average_precision': 0.20630899890789783, 'validation/num_examples': 43793, 'test/accuracy': 0.9852059483528137, 'test/loss': 0.04976944252848625, 'test/mean_average_precision': 0.1960011431400151, 'test/num_examples': 43793, 'score': 16098.7771692276, 'total_duration': 23215.639623880386, 'accumulated_submission_time': 16098.7771692276, 'accumulated_eval_time': 7113.305378198624, 'accumulated_logging_time': 2.1576476097106934}
I0206 03:19:27.422896 139716992366336 logging_writer.py:48] [50631] accumulated_eval_time=7113.305378, accumulated_logging_time=2.157648, accumulated_submission_time=16098.777169, global_step=50631, preemption_count=0, score=16098.777169, test/accuracy=0.985206, test/loss=0.049769, test/mean_average_precision=0.196001, test/num_examples=43793, total_duration=23215.639624, train/accuracy=0.989454, train/loss=0.035731, train/mean_average_precision=0.262205, validation/accuracy=0.986079, validation/loss=0.046868, validation/mean_average_precision=0.206309, validation/num_examples=43793
I0206 03:19:49.676988 139739177916160 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.09884878247976303, loss=0.03906707465648651
I0206 03:20:21.463414 139716992366336 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.0770171508193016, loss=0.038117751479148865
I0206 03:20:53.077719 139739177916160 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.0864153653383255, loss=0.032979730516672134
I0206 03:21:25.204048 139716992366336 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.10586969554424286, loss=0.034191299229860306
I0206 03:21:56.970369 139739177916160 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.08891066163778305, loss=0.036240123212337494
I0206 03:22:29.480603 139716992366336 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.06385046243667603, loss=0.03594456985592842
I0206 03:23:02.194438 139739177916160 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.08077622950077057, loss=0.04501957818865776
I0206 03:23:27.563997 139919816816448 spec.py:321] Evaluating on the training split.
I0206 03:25:05.337895 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 03:25:08.361036 139919816816448 spec.py:349] Evaluating on the test split.
I0206 03:25:11.407594 139919816816448 submission_runner.py:408] Time since start: 23559.65s, 	Step: 51381, 	{'train/accuracy': 0.9894623160362244, 'train/loss': 0.03548566251993179, 'train/mean_average_precision': 0.26926252193776995, 'validation/accuracy': 0.986139714717865, 'validation/loss': 0.047138918191194534, 'validation/mean_average_precision': 0.20698086382406308, 'validation/num_examples': 43793, 'test/accuracy': 0.9851823449134827, 'test/loss': 0.050275906920433044, 'test/mean_average_precision': 0.20079182484563576, 'test/num_examples': 43793, 'score': 16338.885677576065, 'total_duration': 23559.649918794632, 'accumulated_submission_time': 16338.885677576065, 'accumulated_eval_time': 7217.148921728134, 'accumulated_logging_time': 2.195411443710327}
I0206 03:25:11.434639 139735302588160 logging_writer.py:48] [51381] accumulated_eval_time=7217.148922, accumulated_logging_time=2.195411, accumulated_submission_time=16338.885678, global_step=51381, preemption_count=0, score=16338.885678, test/accuracy=0.985182, test/loss=0.050276, test/mean_average_precision=0.200792, test/num_examples=43793, total_duration=23559.649919, train/accuracy=0.989462, train/loss=0.035486, train/mean_average_precision=0.269263, validation/accuracy=0.986140, validation/loss=0.047139, validation/mean_average_precision=0.206981, validation/num_examples=43793
I0206 03:25:18.118741 139735693903616 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.059581153094768524, loss=0.036025527864694595
I0206 03:25:50.122919 139735302588160 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.05990198999643326, loss=0.037294652312994
I0206 03:26:22.137449 139735693903616 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.04864555597305298, loss=0.03360430896282196
I0206 03:26:54.160925 139735302588160 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.09893582761287689, loss=0.03595069423317909
I0206 03:27:25.697850 139735693903616 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.07627549767494202, loss=0.04048064351081848
I0206 03:27:57.014499 139735302588160 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.0805535763502121, loss=0.03972531855106354
I0206 03:28:28.641852 139735693903616 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.11096078902482986, loss=0.039302799850702286
I0206 03:29:00.104787 139735302588160 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.12112664431333542, loss=0.04137718304991722
I0206 03:29:11.692169 139919816816448 spec.py:321] Evaluating on the training split.
I0206 03:30:51.116443 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 03:30:54.567791 139919816816448 spec.py:349] Evaluating on the test split.
I0206 03:30:57.901651 139919816816448 submission_runner.py:408] Time since start: 23906.14s, 	Step: 52137, 	{'train/accuracy': 0.9895649552345276, 'train/loss': 0.03519775718450546, 'train/mean_average_precision': 0.274967294293767, 'validation/accuracy': 0.9861720204353333, 'validation/loss': 0.04690484330058098, 'validation/mean_average_precision': 0.20737156710161236, 'validation/num_examples': 43793, 'test/accuracy': 0.9852589964866638, 'test/loss': 0.04989057034254074, 'test/mean_average_precision': 0.19962056512129037, 'test/num_examples': 43793, 'score': 16579.111583709717, 'total_duration': 23906.143963336945, 'accumulated_submission_time': 16579.111583709717, 'accumulated_eval_time': 7323.358339309692, 'accumulated_logging_time': 2.2338502407073975}
I0206 03:30:57.931200 139739177916160 logging_writer.py:48] [52137] accumulated_eval_time=7323.358339, accumulated_logging_time=2.233850, accumulated_submission_time=16579.111584, global_step=52137, preemption_count=0, score=16579.111584, test/accuracy=0.985259, test/loss=0.049891, test/mean_average_precision=0.199621, test/num_examples=43793, total_duration=23906.143963, train/accuracy=0.989565, train/loss=0.035198, train/mean_average_precision=0.274967, validation/accuracy=0.986172, validation/loss=0.046905, validation/mean_average_precision=0.207372, validation/num_examples=43793
I0206 03:31:18.048825 139759004710656 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.08026306331157684, loss=0.03317422792315483
I0206 03:31:49.828855 139739177916160 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.07243334501981735, loss=0.033206742256879807
I0206 03:32:21.938021 139759004710656 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.09807388484477997, loss=0.033405471593141556
I0206 03:32:54.360315 139739177916160 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.07485335320234299, loss=0.037429917603731155
I0206 03:33:26.650162 139759004710656 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.12202111631631851, loss=0.0413077138364315
I0206 03:33:58.322794 139739177916160 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.1288989633321762, loss=0.04094633087515831
I0206 03:34:30.321812 139759004710656 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.11824645847082138, loss=0.03825739398598671
I0206 03:34:58.007703 139919816816448 spec.py:321] Evaluating on the training split.
I0206 03:36:36.705626 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 03:36:39.796252 139919816816448 spec.py:349] Evaluating on the test split.
I0206 03:36:42.824459 139919816816448 submission_runner.py:408] Time since start: 24251.07s, 	Step: 52887, 	{'train/accuracy': 0.9894105792045593, 'train/loss': 0.035687174648046494, 'train/mean_average_precision': 0.27322752372913345, 'validation/accuracy': 0.9862263798713684, 'validation/loss': 0.04678715765476227, 'validation/mean_average_precision': 0.2090118597592896, 'validation/num_examples': 43793, 'test/accuracy': 0.9852981567382812, 'test/loss': 0.04992193728685379, 'test/mean_average_precision': 0.19961591993997813, 'test/num_examples': 43793, 'score': 16819.154947042465, 'total_duration': 24251.066781044006, 'accumulated_submission_time': 16819.154947042465, 'accumulated_eval_time': 7428.175041437149, 'accumulated_logging_time': 2.2763447761535645}
I0206 03:36:42.850319 139716992366336 logging_writer.py:48] [52887] accumulated_eval_time=7428.175041, accumulated_logging_time=2.276345, accumulated_submission_time=16819.154947, global_step=52887, preemption_count=0, score=16819.154947, test/accuracy=0.985298, test/loss=0.049922, test/mean_average_precision=0.199616, test/num_examples=43793, total_duration=24251.066781, train/accuracy=0.989411, train/loss=0.035687, train/mean_average_precision=0.273228, validation/accuracy=0.986226, validation/loss=0.046787, validation/mean_average_precision=0.209012, validation/num_examples=43793
I0206 03:36:47.345314 139735693903616 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.06655844300985336, loss=0.03566572815179825
I0206 03:37:19.324129 139716992366336 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.07229091227054596, loss=0.03224707022309303
I0206 03:37:51.604422 139735693903616 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.09009435027837753, loss=0.035345859825611115
I0206 03:38:23.391128 139716992366336 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.11255095154047012, loss=0.03728710860013962
I0206 03:38:55.202721 139735693903616 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.11821115016937256, loss=0.036577749997377396
I0206 03:39:26.764930 139716992366336 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.07097037136554718, loss=0.03218308463692665
I0206 03:39:58.290541 139735693903616 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.10107166320085526, loss=0.037288203835487366
I0206 03:40:30.040536 139716992366336 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.0975157618522644, loss=0.03771774843335152
I0206 03:40:42.993648 139919816816448 spec.py:321] Evaluating on the training split.
I0206 03:42:19.487810 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 03:42:22.548611 139919816816448 spec.py:349] Evaluating on the test split.
I0206 03:42:25.574111 139919816816448 submission_runner.py:408] Time since start: 24593.82s, 	Step: 53642, 	{'train/accuracy': 0.9896687865257263, 'train/loss': 0.035085875540971756, 'train/mean_average_precision': 0.27331444403648536, 'validation/accuracy': 0.9860932230949402, 'validation/loss': 0.046895693987607956, 'validation/mean_average_precision': 0.20682647016133246, 'validation/num_examples': 43793, 'test/accuracy': 0.9852105379104614, 'test/loss': 0.04975775256752968, 'test/mean_average_precision': 0.19736626017748696, 'test/num_examples': 43793, 'score': 17059.26643562317, 'total_duration': 24593.81644487381, 'accumulated_submission_time': 17059.26643562317, 'accumulated_eval_time': 7530.755472898483, 'accumulated_logging_time': 2.313117027282715}
I0206 03:42:25.599966 139735302588160 logging_writer.py:48] [53642] accumulated_eval_time=7530.755473, accumulated_logging_time=2.313117, accumulated_submission_time=17059.266436, global_step=53642, preemption_count=0, score=17059.266436, test/accuracy=0.985211, test/loss=0.049758, test/mean_average_precision=0.197366, test/num_examples=43793, total_duration=24593.816445, train/accuracy=0.989669, train/loss=0.035086, train/mean_average_precision=0.273314, validation/accuracy=0.986093, validation/loss=0.046896, validation/mean_average_precision=0.206826, validation/num_examples=43793
I0206 03:42:44.330008 139759004710656 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.07723692804574966, loss=0.03471943736076355
I0206 03:43:16.479289 139735302588160 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.05380316823720932, loss=0.03585416078567505
I0206 03:43:48.200920 139759004710656 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.11872892826795578, loss=0.033531054854393005
I0206 03:44:20.204292 139735302588160 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.13720378279685974, loss=0.03876983001828194
I0206 03:44:51.760334 139759004710656 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.05405771732330322, loss=0.033403974026441574
I0206 03:45:23.288838 139735302588160 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.15549184381961823, loss=0.03568877652287483
I0206 03:45:55.010613 139759004710656 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.10683789104223251, loss=0.03744064271450043
I0206 03:46:25.731886 139919816816448 spec.py:321] Evaluating on the training split.
I0206 03:48:00.956683 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 03:48:06.513167 139919816816448 spec.py:349] Evaluating on the test split.
I0206 03:48:09.542573 139919816816448 submission_runner.py:408] Time since start: 24937.78s, 	Step: 54398, 	{'train/accuracy': 0.9894760251045227, 'train/loss': 0.03575289249420166, 'train/mean_average_precision': 0.26404784370415507, 'validation/accuracy': 0.9861346483230591, 'validation/loss': 0.04675270989537239, 'validation/mean_average_precision': 0.20830687535056122, 'validation/num_examples': 43793, 'test/accuracy': 0.9852269887924194, 'test/loss': 0.04970965534448624, 'test/mean_average_precision': 0.1957582689787017, 'test/num_examples': 43793, 'score': 17299.366802215576, 'total_duration': 24937.78490638733, 'accumulated_submission_time': 17299.366802215576, 'accumulated_eval_time': 7634.566126823425, 'accumulated_logging_time': 2.349884271621704}
I0206 03:48:09.569532 139716992366336 logging_writer.py:48] [54398] accumulated_eval_time=7634.566127, accumulated_logging_time=2.349884, accumulated_submission_time=17299.366802, global_step=54398, preemption_count=0, score=17299.366802, test/accuracy=0.985227, test/loss=0.049710, test/mean_average_precision=0.195758, test/num_examples=43793, total_duration=24937.784906, train/accuracy=0.989476, train/loss=0.035753, train/mean_average_precision=0.264048, validation/accuracy=0.986135, validation/loss=0.046753, validation/mean_average_precision=0.208307, validation/num_examples=43793
I0206 03:48:10.611319 139735693903616 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.13030597567558289, loss=0.03530562296509743
I0206 03:48:42.196773 139716992366336 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.06531579047441483, loss=0.034758370369672775
I0206 03:49:13.751672 139735693903616 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.10265938937664032, loss=0.03750309720635414
I0206 03:49:45.732675 139716992366336 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.05818993225693703, loss=0.03641166910529137
I0206 03:50:17.489898 139735693903616 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.11049240082502365, loss=0.03854496777057648
I0206 03:50:49.111005 139716992366336 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.07895588874816895, loss=0.03386995568871498
I0206 03:51:21.041734 139735693903616 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.12523344159126282, loss=0.0378379225730896
I0206 03:51:52.446177 139716992366336 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.08052392303943634, loss=0.03341799974441528
I0206 03:52:09.616716 139919816816448 spec.py:321] Evaluating on the training split.
I0206 03:53:42.897708 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 03:53:45.934346 139919816816448 spec.py:349] Evaluating on the test split.
I0206 03:53:48.911544 139919816816448 submission_runner.py:408] Time since start: 25277.15s, 	Step: 55156, 	{'train/accuracy': 0.9895650148391724, 'train/loss': 0.035204917192459106, 'train/mean_average_precision': 0.2680131655883736, 'validation/accuracy': 0.986196756362915, 'validation/loss': 0.04677639529109001, 'validation/mean_average_precision': 0.2084649784585847, 'validation/num_examples': 43793, 'test/accuracy': 0.9852808713912964, 'test/loss': 0.049832772463560104, 'test/mean_average_precision': 0.202013044800903, 'test/num_examples': 43793, 'score': 17539.38094496727, 'total_duration': 25277.153856515884, 'accumulated_submission_time': 17539.38094496727, 'accumulated_eval_time': 7733.860890388489, 'accumulated_logging_time': 2.3890788555145264}
I0206 03:53:48.939393 139735302588160 logging_writer.py:48] [55156] accumulated_eval_time=7733.860890, accumulated_logging_time=2.389079, accumulated_submission_time=17539.380945, global_step=55156, preemption_count=0, score=17539.380945, test/accuracy=0.985281, test/loss=0.049833, test/mean_average_precision=0.202013, test/num_examples=43793, total_duration=25277.153857, train/accuracy=0.989565, train/loss=0.035205, train/mean_average_precision=0.268013, validation/accuracy=0.986197, validation/loss=0.046776, validation/mean_average_precision=0.208465, validation/num_examples=43793
I0206 03:54:03.349980 139739177916160 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.13143247365951538, loss=0.03975909575819969
I0206 03:54:34.419119 139735302588160 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.1050117015838623, loss=0.033301010727882385
I0206 03:55:06.004760 139739177916160 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.08645153045654297, loss=0.03365076705813408
I0206 03:55:37.267871 139735302588160 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.05541183054447174, loss=0.036886926740407944
I0206 03:56:08.427168 139739177916160 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.09046116471290588, loss=0.0321890190243721
I0206 03:56:39.858585 139735302588160 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.0734783411026001, loss=0.035742759704589844
I0206 03:57:11.120542 139739177916160 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.09851773083209991, loss=0.034266021102666855
I0206 03:57:42.389995 139735302588160 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.09785926342010498, loss=0.03367571160197258
I0206 03:57:49.016570 139919816816448 spec.py:321] Evaluating on the training split.
I0206 03:59:25.168691 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 03:59:28.253177 139919816816448 spec.py:349] Evaluating on the test split.
I0206 03:59:31.267054 139919816816448 submission_runner.py:408] Time since start: 25619.51s, 	Step: 55922, 	{'train/accuracy': 0.9894495010375977, 'train/loss': 0.03555284067988396, 'train/mean_average_precision': 0.2755270016128907, 'validation/accuracy': 0.9862328767776489, 'validation/loss': 0.046522095799446106, 'validation/mean_average_precision': 0.21318092778342917, 'validation/num_examples': 43793, 'test/accuracy': 0.9852501749992371, 'test/loss': 0.04951382055878639, 'test/mean_average_precision': 0.20283296752641977, 'test/num_examples': 43793, 'score': 17779.42640852928, 'total_duration': 25619.509373903275, 'accumulated_submission_time': 17779.42640852928, 'accumulated_eval_time': 7836.111313343048, 'accumulated_logging_time': 2.427854061126709}
I0206 03:59:31.294006 139716992366336 logging_writer.py:48] [55922] accumulated_eval_time=7836.111313, accumulated_logging_time=2.427854, accumulated_submission_time=17779.426409, global_step=55922, preemption_count=0, score=17779.426409, test/accuracy=0.985250, test/loss=0.049514, test/mean_average_precision=0.202833, test/num_examples=43793, total_duration=25619.509374, train/accuracy=0.989450, train/loss=0.035553, train/mean_average_precision=0.275527, validation/accuracy=0.986233, validation/loss=0.046522, validation/mean_average_precision=0.213181, validation/num_examples=43793
I0206 03:59:56.666097 139735693903616 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.06189940869808197, loss=0.034887902438640594
I0206 04:00:28.430632 139716992366336 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.07030485570430756, loss=0.031241051852703094
I0206 04:01:00.224852 139735693903616 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.14134301245212555, loss=0.036548223346471786
I0206 04:01:32.347366 139716992366336 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.06783450394868851, loss=0.030908599495887756
I0206 04:02:04.482506 139735693903616 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.07301896065473557, loss=0.029598305001854897
I0206 04:02:36.513757 139716992366336 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.11669783294200897, loss=0.03346315398812294
I0206 04:03:08.598486 139735693903616 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.1802709400653839, loss=0.03315049037337303
I0206 04:03:31.511222 139919816816448 spec.py:321] Evaluating on the training split.
I0206 04:05:08.859669 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 04:05:11.965837 139919816816448 spec.py:349] Evaluating on the test split.
I0206 04:05:15.027493 139919816816448 submission_runner.py:408] Time since start: 25963.27s, 	Step: 56673, 	{'train/accuracy': 0.9895038604736328, 'train/loss': 0.03523600473999977, 'train/mean_average_precision': 0.2716795602529939, 'validation/accuracy': 0.9862877130508423, 'validation/loss': 0.046465057879686356, 'validation/mean_average_precision': 0.21102340437640538, 'validation/num_examples': 43793, 'test/accuracy': 0.985371470451355, 'test/loss': 0.04939175024628639, 'test/mean_average_precision': 0.20396700578340565, 'test/num_examples': 43793, 'score': 18019.612231731415, 'total_duration': 25963.269825220108, 'accumulated_submission_time': 18019.612231731415, 'accumulated_eval_time': 7939.627541303635, 'accumulated_logging_time': 2.4657275676727295}
I0206 04:05:15.053046 139735302588160 logging_writer.py:48] [56673] accumulated_eval_time=7939.627541, accumulated_logging_time=2.465728, accumulated_submission_time=18019.612232, global_step=56673, preemption_count=0, score=18019.612232, test/accuracy=0.985371, test/loss=0.049392, test/mean_average_precision=0.203967, test/num_examples=43793, total_duration=25963.269825, train/accuracy=0.989504, train/loss=0.035236, train/mean_average_precision=0.271680, validation/accuracy=0.986288, validation/loss=0.046465, validation/mean_average_precision=0.211023, validation/num_examples=43793
I0206 04:05:24.005234 139759004710656 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.12136770784854889, loss=0.03266933932900429
I0206 04:05:55.225218 139735302588160 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.0846385508775711, loss=0.03957989811897278
I0206 04:06:27.096075 139759004710656 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.08139757812023163, loss=0.03556299954652786
I0206 04:06:59.252194 139735302588160 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.090399369597435, loss=0.03488430380821228
I0206 04:07:30.961358 139759004710656 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.09389765560626984, loss=0.034684550017118454
I0206 04:08:02.454087 139735302588160 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.12028515338897705, loss=0.03813864290714264
I0206 04:08:34.131098 139759004710656 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.10549202561378479, loss=0.037002984434366226
I0206 04:09:05.979341 139735302588160 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.09537220001220703, loss=0.03387757018208504
I0206 04:09:15.078892 139919816816448 spec.py:321] Evaluating on the training split.
I0206 04:10:50.804247 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 04:10:53.957162 139919816816448 spec.py:349] Evaluating on the test split.
I0206 04:10:57.019600 139919816816448 submission_runner.py:408] Time since start: 26305.26s, 	Step: 57429, 	{'train/accuracy': 0.9898420572280884, 'train/loss': 0.03476691246032715, 'train/mean_average_precision': 0.28116842626885963, 'validation/accuracy': 0.9861756563186646, 'validation/loss': 0.04659731686115265, 'validation/mean_average_precision': 0.21251331949642285, 'validation/num_examples': 43793, 'test/accuracy': 0.9852796196937561, 'test/loss': 0.049422599375247955, 'test/mean_average_precision': 0.2015535494588887, 'test/num_examples': 43793, 'score': 18259.605164289474, 'total_duration': 26305.261911392212, 'accumulated_submission_time': 18259.605164289474, 'accumulated_eval_time': 8041.5682673454285, 'accumulated_logging_time': 2.5035622119903564}
I0206 04:10:57.045518 139716992366336 logging_writer.py:48] [57429] accumulated_eval_time=8041.568267, accumulated_logging_time=2.503562, accumulated_submission_time=18259.605164, global_step=57429, preemption_count=0, score=18259.605164, test/accuracy=0.985280, test/loss=0.049423, test/mean_average_precision=0.201554, test/num_examples=43793, total_duration=26305.261911, train/accuracy=0.989842, train/loss=0.034767, train/mean_average_precision=0.281168, validation/accuracy=0.986176, validation/loss=0.046597, validation/mean_average_precision=0.212513, validation/num_examples=43793
I0206 04:11:19.866740 139739177916160 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.06678366661071777, loss=0.033528804779052734
I0206 04:11:51.540152 139716992366336 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.11836405843496323, loss=0.0415458045899868
I0206 04:12:23.017829 139739177916160 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.08160272240638733, loss=0.031739991158246994
I0206 04:12:54.469194 139716992366336 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.08225436508655548, loss=0.0359172485768795
I0206 04:13:25.895296 139739177916160 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.07603487372398376, loss=0.031565651297569275
I0206 04:13:57.506268 139716992366336 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.06316453963518143, loss=0.035390377044677734
I0206 04:14:28.863656 139739177916160 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.1429220587015152, loss=0.03475575894117355
I0206 04:14:34.766654 139716992366336 logging_writer.py:48] [58120] global_step=58120, preemption_count=0, score=18477.279756
I0206 04:14:34.819391 139919816816448 checkpoints.py:490] Saving checkpoint at step: 58120
I0206 04:14:34.958666 139919816816448 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_1/ogbg_jax/trial_4/checkpoint_58120
I0206 04:14:34.959629 139919816816448 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_1/ogbg_jax/trial_4/checkpoint_58120.
I0206 04:14:35.101767 139919816816448 submission_runner.py:583] Tuning trial 4/5
I0206 04:14:35.102009 139919816816448 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.0, label_smoothing=0.0, learning_rate=0.004958460849689891, one_minus_beta1=0.13625575743, beta2=0.6291854735396584, weight_decay=0.1147386261512052, warmup_factor=0.02)
I0206 04:14:35.106447 139919816816448 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.5323500037193298, 'train/loss': 0.7277898788452148, 'train/mean_average_precision': 0.022713738959025315, 'validation/accuracy': 0.5230699777603149, 'validation/loss': 0.7331878542900085, 'validation/mean_average_precision': 0.02551145948130271, 'validation/num_examples': 43793, 'test/accuracy': 0.521492063999176, 'test/loss': 0.7347381114959717, 'test/mean_average_precision': 0.026847203975459377, 'test/num_examples': 43793, 'score': 11.923594951629639, 'total_duration': 118.22934317588806, 'accumulated_submission_time': 11.923594951629639, 'accumulated_eval_time': 106.30569434165955, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (756, {'train/accuracy': 0.9868640899658203, 'train/loss': 0.05360216647386551, 'train/mean_average_precision': 0.041551055168208686, 'validation/accuracy': 0.9841504096984863, 'validation/loss': 0.0639973059296608, 'validation/mean_average_precision': 0.04087225139175344, 'validation/num_examples': 43793, 'test/accuracy': 0.9831761717796326, 'test/loss': 0.06718604266643524, 'test/mean_average_precision': 0.042759928672415015, 'test/num_examples': 43793, 'score': 251.88759779930115, 'total_duration': 467.25233030319214, 'accumulated_submission_time': 251.88759779930115, 'accumulated_eval_time': 215.32289910316467, 'accumulated_logging_time': 0.02102208137512207, 'global_step': 756, 'preemption_count': 0}), (1500, {'train/accuracy': 0.9869927763938904, 'train/loss': 0.05052226781845093, 'train/mean_average_precision': 0.06667004980286692, 'validation/accuracy': 0.9842925071716309, 'validation/loss': 0.06098408252000809, 'validation/mean_average_precision': 0.06739874671941877, 'validation/num_examples': 43793, 'test/accuracy': 0.98328697681427, 'test/loss': 0.06431528925895691, 'test/mean_average_precision': 0.06560076879112647, 'test/num_examples': 43793, 'score': 492.0900263786316, 'total_duration': 815.5264096260071, 'accumulated_submission_time': 492.0900263786316, 'accumulated_eval_time': 323.3419051170349, 'accumulated_logging_time': 0.050023555755615234, 'global_step': 1500, 'preemption_count': 0}), (2254, {'train/accuracy': 0.9870300889015198, 'train/loss': 0.04844581335783005, 'train/mean_average_precision': 0.08347689953276526, 'validation/accuracy': 0.9843335151672363, 'validation/loss': 0.058096155524253845, 'validation/mean_average_precision': 0.07764661735126936, 'validation/num_examples': 43793, 'test/accuracy': 0.9833593964576721, 'test/loss': 0.061391979455947876, 'test/mean_average_precision': 0.0773755137544642, 'test/num_examples': 43793, 'score': 732.36257147789, 'total_duration': 1163.9381144046783, 'accumulated_submission_time': 732.36257147789, 'accumulated_eval_time': 431.4344410896301, 'accumulated_logging_time': 0.07634162902832031, 'global_step': 2254, 'preemption_count': 0}), (3012, {'train/accuracy': 0.9873122572898865, 'train/loss': 0.046275608241558075, 'train/mean_average_precision': 0.10629623996906212, 'validation/accuracy': 0.9846282005310059, 'validation/loss': 0.056142181158065796, 'validation/mean_average_precision': 0.09584960419535594, 'validation/num_examples': 43793, 'test/accuracy': 0.9836963415145874, 'test/loss': 0.05957218259572983, 'test/mean_average_precision': 0.09474019072285637, 'test/num_examples': 43793, 'score': 972.623060464859, 'total_duration': 1505.4653851985931, 'accumulated_submission_time': 972.623060464859, 'accumulated_eval_time': 532.6530933380127, 'accumulated_logging_time': 0.10355210304260254, 'global_step': 3012, 'preemption_count': 0}), (3767, {'train/accuracy': 0.987439751625061, 'train/loss': 0.045254599303007126, 'train/mean_average_precision': 0.12752641015774666, 'validation/accuracy': 0.9846375584602356, 'validation/loss': 0.05470998212695122, 'validation/mean_average_precision': 0.1125843895014612, 'validation/num_examples': 43793, 'test/accuracy': 0.9836066365242004, 'test/loss': 0.05755944922566414, 'test/mean_average_precision': 0.11312817246859927, 'test/num_examples': 43793, 'score': 1212.7634572982788, 'total_duration': 1855.2609441280365, 'accumulated_submission_time': 1212.7634572982788, 'accumulated_eval_time': 642.2608568668365, 'accumulated_logging_time': 0.1302659511566162, 'global_step': 3767, 'preemption_count': 0}), (4524, {'train/accuracy': 0.9877470135688782, 'train/loss': 0.0434085913002491, 'train/mean_average_precision': 0.14673611209759083, 'validation/accuracy': 0.9848189949989319, 'validation/loss': 0.052754007279872894, 'validation/mean_average_precision': 0.12317377579610293, 'validation/num_examples': 43793, 'test/accuracy': 0.9839351773262024, 'test/loss': 0.05557785555720329, 'test/mean_average_precision': 0.12852256187186392, 'test/num_examples': 43793, 'score': 1452.7605681419373, 'total_duration': 2196.578159570694, 'accumulated_submission_time': 1452.7605681419373, 'accumulated_eval_time': 743.5346171855927, 'accumulated_logging_time': 0.1563253402709961, 'global_step': 4524, 'preemption_count': 0}), (5270, {'train/accuracy': 0.9880778789520264, 'train/loss': 0.04199328273534775, 'train/mean_average_precision': 0.16621979128001976, 'validation/accuracy': 0.9851506352424622, 'validation/loss': 0.05195803567767143, 'validation/mean_average_precision': 0.13905560615272636, 'validation/num_examples': 43793, 'test/accuracy': 0.9842110872268677, 'test/loss': 0.05524878203868866, 'test/mean_average_precision': 0.13999589254461053, 'test/num_examples': 43793, 'score': 1692.9004187583923, 'total_duration': 2541.892335653305, 'accumulated_submission_time': 1692.9004187583923, 'accumulated_eval_time': 848.6604132652283, 'accumulated_logging_time': 0.18234682083129883, 'global_step': 5270, 'preemption_count': 0}), (6029, {'train/accuracy': 0.9879913330078125, 'train/loss': 0.0421731173992157, 'train/mean_average_precision': 0.1533392112329769, 'validation/accuracy': 0.9850727319717407, 'validation/loss': 0.0521065816283226, 'validation/mean_average_precision': 0.1399027724744271, 'validation/num_examples': 43793, 'test/accuracy': 0.9840817451477051, 'test/loss': 0.055003903806209564, 'test/mean_average_precision': 0.13523668927573052, 'test/num_examples': 43793, 'score': 1932.8552432060242, 'total_duration': 2884.456712245941, 'accumulated_submission_time': 1932.8552432060242, 'accumulated_eval_time': 951.2205955982208, 'accumulated_logging_time': 0.2111051082611084, 'global_step': 6029, 'preemption_count': 0}), (6784, {'train/accuracy': 0.9880300164222717, 'train/loss': 0.04225614294409752, 'train/mean_average_precision': 0.16079885263891208, 'validation/accuracy': 0.9850122332572937, 'validation/loss': 0.05152441933751106, 'validation/mean_average_precision': 0.14707804685112305, 'validation/num_examples': 43793, 'test/accuracy': 0.9840371012687683, 'test/loss': 0.05428755655884743, 'test/mean_average_precision': 0.14234278442446044, 'test/num_examples': 43793, 'score': 2173.0677292346954, 'total_duration': 3228.763679265976, 'accumulated_submission_time': 2173.0677292346954, 'accumulated_eval_time': 1055.2664711475372, 'accumulated_logging_time': 0.23958039283752441, 'global_step': 6784, 'preemption_count': 0}), (7543, {'train/accuracy': 0.9881213903427124, 'train/loss': 0.04118350148200989, 'train/mean_average_precision': 0.16851221433698832, 'validation/accuracy': 0.9851640462875366, 'validation/loss': 0.05082281306385994, 'validation/mean_average_precision': 0.1580675882901569, 'validation/num_examples': 43793, 'test/accuracy': 0.984203040599823, 'test/loss': 0.05370954051613808, 'test/mean_average_precision': 0.15149005069847016, 'test/num_examples': 43793, 'score': 2413.096028804779, 'total_duration': 3574.0399181842804, 'accumulated_submission_time': 2413.096028804779, 'accumulated_eval_time': 1160.4659247398376, 'accumulated_logging_time': 0.26740050315856934, 'global_step': 7543, 'preemption_count': 0}), (8297, {'train/accuracy': 0.9882885813713074, 'train/loss': 0.040482938289642334, 'train/mean_average_precision': 0.18335730076382278, 'validation/accuracy': 0.9854526519775391, 'validation/loss': 0.04996418207883835, 'validation/mean_average_precision': 0.16653534896192793, 'validation/num_examples': 43793, 'test/accuracy': 0.9844747185707092, 'test/loss': 0.05280674248933792, 'test/mean_average_precision': 0.16006408266821517, 'test/num_examples': 43793, 'score': 2653.2899854183197, 'total_duration': 3918.1962225437164, 'accumulated_submission_time': 2653.2899854183197, 'accumulated_eval_time': 1264.378232717514, 'accumulated_logging_time': 0.2971460819244385, 'global_step': 8297, 'preemption_count': 0}), (9059, {'train/accuracy': 0.9884369373321533, 'train/loss': 0.040347833186388016, 'train/mean_average_precision': 0.18804448035695187, 'validation/accuracy': 0.9853227734565735, 'validation/loss': 0.04992473125457764, 'validation/mean_average_precision': 0.15702354491908133, 'validation/num_examples': 43793, 'test/accuracy': 0.9844157695770264, 'test/loss': 0.05274470895528793, 'test/mean_average_precision': 0.1588039709738697, 'test/num_examples': 43793, 'score': 2893.515405654907, 'total_duration': 4260.960469484329, 'accumulated_submission_time': 2893.515405654907, 'accumulated_eval_time': 1366.8684968948364, 'accumulated_logging_time': 0.3246288299560547, 'global_step': 9059, 'preemption_count': 0}), (9818, {'train/accuracy': 0.988334596157074, 'train/loss': 0.04056818038225174, 'train/mean_average_precision': 0.17742767170688034, 'validation/accuracy': 0.9853284358978271, 'validation/loss': 0.05031628534197807, 'validation/mean_average_precision': 0.1624657584398002, 'validation/num_examples': 43793, 'test/accuracy': 0.984444797039032, 'test/loss': 0.053260620683431625, 'test/mean_average_precision': 0.15851285429824236, 'test/num_examples': 43793, 'score': 3133.483088493347, 'total_duration': 4605.794593811035, 'accumulated_submission_time': 3133.483088493347, 'accumulated_eval_time': 1471.6854536533356, 'accumulated_logging_time': 0.35410261154174805, 'global_step': 9818, 'preemption_count': 0}), (10579, {'train/accuracy': 0.9884284138679504, 'train/loss': 0.03996816277503967, 'train/mean_average_precision': 0.187111713207441, 'validation/accuracy': 0.985492467880249, 'validation/loss': 0.049749307334423065, 'validation/mean_average_precision': 0.16473999486327945, 'validation/num_examples': 43793, 'test/accuracy': 0.9845644235610962, 'test/loss': 0.052593156695365906, 'test/mean_average_precision': 0.16523719161665631, 'test/num_examples': 43793, 'score': 3373.640467405319, 'total_duration': 4949.39643740654, 'accumulated_submission_time': 3373.640467405319, 'accumulated_eval_time': 1575.0813403129578, 'accumulated_logging_time': 0.38223719596862793, 'global_step': 10579, 'preemption_count': 0}), (11336, {'train/accuracy': 0.988300621509552, 'train/loss': 0.04015148803591728, 'train/mean_average_precision': 0.19675313600423047, 'validation/accuracy': 0.9854080080986023, 'validation/loss': 0.04994430020451546, 'validation/mean_average_precision': 0.17165726892233704, 'validation/num_examples': 43793, 'test/accuracy': 0.9844300746917725, 'test/loss': 0.0530269481241703, 'test/mean_average_precision': 0.1644090315504303, 'test/num_examples': 43793, 'score': 3613.6902854442596, 'total_duration': 5291.022964477539, 'accumulated_submission_time': 3613.6902854442596, 'accumulated_eval_time': 1676.585940361023, 'accumulated_logging_time': 0.4333629608154297, 'global_step': 11336, 'preemption_count': 0}), (12099, {'train/accuracy': 0.9884596467018127, 'train/loss': 0.03974561765789986, 'train/mean_average_precision': 0.19346122641698732, 'validation/accuracy': 0.9854969382286072, 'validation/loss': 0.04996122419834137, 'validation/mean_average_precision': 0.16202028624192666, 'validation/num_examples': 43793, 'test/accuracy': 0.9845282435417175, 'test/loss': 0.053220633417367935, 'test/mean_average_precision': 0.16265730998179825, 'test/num_examples': 43793, 'score': 3853.765954732895, 'total_duration': 5637.733014345169, 'accumulated_submission_time': 3853.765954732895, 'accumulated_eval_time': 1783.1696891784668, 'accumulated_logging_time': 0.4635937213897705, 'global_step': 12099, 'preemption_count': 0}), (12858, {'train/accuracy': 0.9882970452308655, 'train/loss': 0.04016558453440666, 'train/mean_average_precision': 0.20592343113745598, 'validation/accuracy': 0.9854335784912109, 'validation/loss': 0.05081567540764809, 'validation/mean_average_precision': 0.1689011888219761, 'validation/num_examples': 43793, 'test/accuracy': 0.9844393730163574, 'test/loss': 0.05402006208896637, 'test/mean_average_precision': 0.16800944106961194, 'test/num_examples': 43793, 'score': 4093.766298055649, 'total_duration': 5982.928592443466, 'accumulated_submission_time': 4093.766298055649, 'accumulated_eval_time': 1888.3162214756012, 'accumulated_logging_time': 0.492189884185791, 'global_step': 12858, 'preemption_count': 0}), (13619, {'train/accuracy': 0.9884986281394958, 'train/loss': 0.03974822536110878, 'train/mean_average_precision': 0.2073664034633914, 'validation/accuracy': 0.9854303598403931, 'validation/loss': 0.04989549517631531, 'validation/mean_average_precision': 0.16710760332893343, 'validation/num_examples': 43793, 'test/accuracy': 0.9844822883605957, 'test/loss': 0.052785005420446396, 'test/mean_average_precision': 0.1698031474244022, 'test/num_examples': 43793, 'score': 4334.008366107941, 'total_duration': 6331.93460059166, 'accumulated_submission_time': 4334.008366107941, 'accumulated_eval_time': 1997.0318686962128, 'accumulated_logging_time': 0.5204670429229736, 'global_step': 13619, 'preemption_count': 0}), (14376, {'train/accuracy': 0.9886337518692017, 'train/loss': 0.03938262537121773, 'train/mean_average_precision': 0.19114874640162213, 'validation/accuracy': 0.9855237007141113, 'validation/loss': 0.04945814982056618, 'validation/mean_average_precision': 0.16695185913129676, 'validation/num_examples': 43793, 'test/accuracy': 0.9846579432487488, 'test/loss': 0.05213755741715431, 'test/mean_average_precision': 0.1663883366384637, 'test/num_examples': 43793, 'score': 4574.039252996445, 'total_duration': 6674.336926460266, 'accumulated_submission_time': 4574.039252996445, 'accumulated_eval_time': 2099.3500323295593, 'accumulated_logging_time': 0.5519809722900391, 'global_step': 14376, 'preemption_count': 0}), (15132, {'train/accuracy': 0.9885455369949341, 'train/loss': 0.0397002212703228, 'train/mean_average_precision': 0.2028603164464443, 'validation/accuracy': 0.9855809211730957, 'validation/loss': 0.04987575486302376, 'validation/mean_average_precision': 0.16837117460650874, 'validation/num_examples': 43793, 'test/accuracy': 0.9846330881118774, 'test/loss': 0.05285630002617836, 'test/mean_average_precision': 0.170379749437865, 'test/num_examples': 43793, 'score': 4814.0303320884705, 'total_duration': 7020.977716207504, 'accumulated_submission_time': 4814.0303320884705, 'accumulated_eval_time': 2205.947719812393, 'accumulated_logging_time': 0.58294677734375, 'global_step': 15132, 'preemption_count': 0}), (15892, {'train/accuracy': 0.9885801672935486, 'train/loss': 0.039336010813713074, 'train/mean_average_precision': 0.19845201110629346, 'validation/accuracy': 0.9856215119361877, 'validation/loss': 0.04933474585413933, 'validation/mean_average_precision': 0.17170922124132204, 'validation/num_examples': 43793, 'test/accuracy': 0.9847198724746704, 'test/loss': 0.05216008797287941, 'test/mean_average_precision': 0.17798927606950105, 'test/num_examples': 43793, 'score': 5054.056452035904, 'total_duration': 7362.055555820465, 'accumulated_submission_time': 5054.056452035904, 'accumulated_eval_time': 2306.9480743408203, 'accumulated_logging_time': 0.6138248443603516, 'global_step': 15892, 'preemption_count': 0}), (16644, {'train/accuracy': 0.9884446263313293, 'train/loss': 0.03972674533724785, 'train/mean_average_precision': 0.19522588930474225, 'validation/accuracy': 0.9856138229370117, 'validation/loss': 0.04969978705048561, 'validation/mean_average_precision': 0.1718890388904479, 'validation/num_examples': 43793, 'test/accuracy': 0.9847078323364258, 'test/loss': 0.05255919694900513, 'test/mean_average_precision': 0.17442957654614527, 'test/num_examples': 43793, 'score': 5294.24645280838, 'total_duration': 7709.170320272446, 'accumulated_submission_time': 5294.24645280838, 'accumulated_eval_time': 2413.8230736255646, 'accumulated_logging_time': 0.6432387828826904, 'global_step': 16644, 'preemption_count': 0}), (17393, {'train/accuracy': 0.9886024594306946, 'train/loss': 0.03929731249809265, 'train/mean_average_precision': 0.21046139357105487, 'validation/accuracy': 0.9857274889945984, 'validation/loss': 0.04928113892674446, 'validation/mean_average_precision': 0.1825998260774477, 'validation/num_examples': 43793, 'test/accuracy': 0.9848057627677917, 'test/loss': 0.05209784954786301, 'test/mean_average_precision': 0.17891039444669973, 'test/num_examples': 43793, 'score': 5534.219587802887, 'total_duration': 8056.105494737625, 'accumulated_submission_time': 5534.219587802887, 'accumulated_eval_time': 2520.733085632324, 'accumulated_logging_time': 0.6736774444580078, 'global_step': 17393, 'preemption_count': 0}), (18143, {'train/accuracy': 0.9886455535888672, 'train/loss': 0.039150483906269073, 'train/mean_average_precision': 0.19768793801215342, 'validation/accuracy': 0.9855988025665283, 'validation/loss': 0.04940321296453476, 'validation/mean_average_precision': 0.17184504168876086, 'validation/num_examples': 43793, 'test/accuracy': 0.9847438931465149, 'test/loss': 0.0521351657807827, 'test/mean_average_precision': 0.17146011683788295, 'test/num_examples': 43793, 'score': 5774.297125339508, 'total_duration': 8403.12332034111, 'accumulated_submission_time': 5774.297125339508, 'accumulated_eval_time': 2627.6177830696106, 'accumulated_logging_time': 0.7077550888061523, 'global_step': 18143, 'preemption_count': 0}), (18896, {'train/accuracy': 0.988558292388916, 'train/loss': 0.03911030665040016, 'train/mean_average_precision': 0.2097071248810855, 'validation/accuracy': 0.9856860637664795, 'validation/loss': 0.04947445169091225, 'validation/mean_average_precision': 0.17108575622241304, 'validation/num_examples': 43793, 'test/accuracy': 0.9847944378852844, 'test/loss': 0.05227375775575638, 'test/mean_average_precision': 0.1771656161784761, 'test/num_examples': 43793, 'score': 6014.361703634262, 'total_duration': 8748.009192943573, 'accumulated_submission_time': 6014.361703634262, 'accumulated_eval_time': 2732.3874497413635, 'accumulated_logging_time': 0.7390317916870117, 'global_step': 18896, 'preemption_count': 0}), (19650, {'train/accuracy': 0.9885598421096802, 'train/loss': 0.0392596535384655, 'train/mean_average_precision': 0.203455128542813, 'validation/accuracy': 0.9856438636779785, 'validation/loss': 0.04947600141167641, 'validation/mean_average_precision': 0.16671810223241684, 'validation/num_examples': 43793, 'test/accuracy': 0.984729528427124, 'test/loss': 0.05226048454642296, 'test/mean_average_precision': 0.1714548320037481, 'test/num_examples': 43793, 'score': 6254.322240352631, 'total_duration': 9093.64295911789, 'accumulated_submission_time': 6254.322240352631, 'accumulated_eval_time': 2838.0074141025543, 'accumulated_logging_time': 0.7716796398162842, 'global_step': 19650, 'preemption_count': 0}), (20396, {'train/accuracy': 0.9887438416481018, 'train/loss': 0.03857656568288803, 'train/mean_average_precision': 0.212028509293876, 'validation/accuracy': 0.9856438636779785, 'validation/loss': 0.04899085685610771, 'validation/mean_average_precision': 0.1850536108788391, 'validation/num_examples': 43793, 'test/accuracy': 0.9847375750541687, 'test/loss': 0.05189521238207817, 'test/mean_average_precision': 0.17919727958794127, 'test/num_examples': 43793, 'score': 6494.273699045181, 'total_duration': 9438.594544649124, 'accumulated_submission_time': 6494.273699045181, 'accumulated_eval_time': 2942.955656528473, 'accumulated_logging_time': 0.8033504486083984, 'global_step': 20396, 'preemption_count': 0}), (21159, {'train/accuracy': 0.9888488054275513, 'train/loss': 0.03830825537443161, 'train/mean_average_precision': 0.2137575777576014, 'validation/accuracy': 0.9856317043304443, 'validation/loss': 0.04880497232079506, 'validation/mean_average_precision': 0.17733595286877527, 'validation/num_examples': 43793, 'test/accuracy': 0.9846756458282471, 'test/loss': 0.05158416926860809, 'test/mean_average_precision': 0.17797966573308246, 'test/num_examples': 43793, 'score': 6734.436192750931, 'total_duration': 9782.50632095337, 'accumulated_submission_time': 6734.436192750931, 'accumulated_eval_time': 3046.6530838012695, 'accumulated_logging_time': 0.834456205368042, 'global_step': 21159, 'preemption_count': 0}), (21912, {'train/accuracy': 0.9886640906333923, 'train/loss': 0.03873438388109207, 'train/mean_average_precision': 0.22411031784235116, 'validation/accuracy': 0.9857713580131531, 'validation/loss': 0.04842014238238335, 'validation/mean_average_precision': 0.18335871642119683, 'validation/num_examples': 43793, 'test/accuracy': 0.984855055809021, 'test/loss': 0.051150087267160416, 'test/mean_average_precision': 0.18211586940575714, 'test/num_examples': 43793, 'score': 6974.460539340973, 'total_duration': 10126.857418060303, 'accumulated_submission_time': 6974.460539340973, 'accumulated_eval_time': 3150.928318500519, 'accumulated_logging_time': 0.8655087947845459, 'global_step': 21912, 'preemption_count': 0}), (22666, {'train/accuracy': 0.9887723326683044, 'train/loss': 0.0386015810072422, 'train/mean_average_precision': 0.21176870812917725, 'validation/accuracy': 0.9856633543968201, 'validation/loss': 0.0487646721303463, 'validation/mean_average_precision': 0.17440925763893128, 'validation/num_examples': 43793, 'test/accuracy': 0.9847897887229919, 'test/loss': 0.05144037678837776, 'test/mean_average_precision': 0.17965303088513426, 'test/num_examples': 43793, 'score': 7214.716456651688, 'total_duration': 10468.675518989563, 'accumulated_submission_time': 7214.716456651688, 'accumulated_eval_time': 3252.439876317978, 'accumulated_logging_time': 0.89583420753479, 'global_step': 22666, 'preemption_count': 0}), (23426, {'train/accuracy': 0.9883869886398315, 'train/loss': 0.040114033967256546, 'train/mean_average_precision': 0.20837487977254418, 'validation/accuracy': 0.9855829477310181, 'validation/loss': 0.050475310534238815, 'validation/mean_average_precision': 0.18149632972056115, 'validation/num_examples': 43793, 'test/accuracy': 0.9846773147583008, 'test/loss': 0.053886838257312775, 'test/mean_average_precision': 0.18220864852989385, 'test/num_examples': 43793, 'score': 7454.776722192764, 'total_duration': 10813.931846141815, 'accumulated_submission_time': 7454.776722192764, 'accumulated_eval_time': 3357.5840377807617, 'accumulated_logging_time': 0.9260289669036865, 'global_step': 23426, 'preemption_count': 0}), (24182, {'train/accuracy': 0.9885193705558777, 'train/loss': 0.039419785141944885, 'train/mean_average_precision': 0.20867398454830233, 'validation/accuracy': 0.985595166683197, 'validation/loss': 0.04940466210246086, 'validation/mean_average_precision': 0.17790979633379897, 'validation/num_examples': 43793, 'test/accuracy': 0.9847131371498108, 'test/loss': 0.05240046605467796, 'test/mean_average_precision': 0.1733469909451849, 'test/num_examples': 43793, 'score': 7694.763477563858, 'total_duration': 11158.016758441925, 'accumulated_submission_time': 7694.763477563858, 'accumulated_eval_time': 3461.6296710968018, 'accumulated_logging_time': 0.9567892551422119, 'global_step': 24182, 'preemption_count': 0}), (24932, {'train/accuracy': 0.9886791110038757, 'train/loss': 0.039127543568611145, 'train/mean_average_precision': 0.20610971730959943, 'validation/accuracy': 0.9856958389282227, 'validation/loss': 0.04969356581568718, 'validation/mean_average_precision': 0.17424173414313215, 'validation/num_examples': 43793, 'test/accuracy': 0.9847312569618225, 'test/loss': 0.05299022048711777, 'test/mean_average_precision': 0.173159771894493, 'test/num_examples': 43793, 'score': 7935.051184654236, 'total_duration': 11503.084161281586, 'accumulated_submission_time': 7935.051184654236, 'accumulated_eval_time': 3566.3573133945465, 'accumulated_logging_time': 0.9878280162811279, 'global_step': 24932, 'preemption_count': 0}), (25691, {'train/accuracy': 0.988594651222229, 'train/loss': 0.03934819996356964, 'train/mean_average_precision': 0.20413834738695363, 'validation/accuracy': 0.9856219291687012, 'validation/loss': 0.04998236149549484, 'validation/mean_average_precision': 0.17955457727038346, 'validation/num_examples': 43793, 'test/accuracy': 0.9847291111946106, 'test/loss': 0.053024083375930786, 'test/mean_average_precision': 0.1790258355233416, 'test/num_examples': 43793, 'score': 8175.208832502365, 'total_duration': 11846.830078840256, 'accumulated_submission_time': 8175.208832502365, 'accumulated_eval_time': 3669.8920063972473, 'accumulated_logging_time': 1.0200772285461426, 'global_step': 25691, 'preemption_count': 0}), (26445, {'train/accuracy': 0.9887518882751465, 'train/loss': 0.03899611532688141, 'train/mean_average_precision': 0.21371595474408023, 'validation/accuracy': 0.9856349229812622, 'validation/loss': 0.04871748015284538, 'validation/mean_average_precision': 0.17790845851272608, 'validation/num_examples': 43793, 'test/accuracy': 0.9847640991210938, 'test/loss': 0.05142880603671074, 'test/mean_average_precision': 0.17431201489801942, 'test/num_examples': 43793, 'score': 8415.195302963257, 'total_duration': 12192.255960464478, 'accumulated_submission_time': 8415.195302963257, 'accumulated_eval_time': 3775.2796547412872, 'accumulated_logging_time': 1.051387071609497, 'global_step': 26445, 'preemption_count': 0}), (27195, {'train/accuracy': 0.9886566400527954, 'train/loss': 0.03898187354207039, 'train/mean_average_precision': 0.21840967196923997, 'validation/accuracy': 0.985464870929718, 'validation/loss': 0.04904794692993164, 'validation/mean_average_precision': 0.18207372190279822, 'validation/num_examples': 43793, 'test/accuracy': 0.9845766425132751, 'test/loss': 0.051882777363061905, 'test/mean_average_precision': 0.1705559020023737, 'test/num_examples': 43793, 'score': 8655.201495409012, 'total_duration': 12542.291038513184, 'accumulated_submission_time': 8655.201495409012, 'accumulated_eval_time': 3885.2557418346405, 'accumulated_logging_time': 1.0838682651519775, 'global_step': 27195, 'preemption_count': 0}), (27950, {'train/accuracy': 0.9887489676475525, 'train/loss': 0.038225021213293076, 'train/mean_average_precision': 0.22385914357152437, 'validation/accuracy': 0.9858261346817017, 'validation/loss': 0.04866309463977814, 'validation/mean_average_precision': 0.1828395677585086, 'validation/num_examples': 43793, 'test/accuracy': 0.9847881197929382, 'test/loss': 0.05182139575481415, 'test/mean_average_precision': 0.17715077544494223, 'test/num_examples': 43793, 'score': 8895.445118188858, 'total_duration': 12888.176303863525, 'accumulated_submission_time': 8895.445118188858, 'accumulated_eval_time': 3990.845261335373, 'accumulated_logging_time': 1.1152944564819336, 'global_step': 27950, 'preemption_count': 0}), (28714, {'train/accuracy': 0.9886225461959839, 'train/loss': 0.038634009659290314, 'train/mean_average_precision': 0.2167209401037269, 'validation/accuracy': 0.9856897592544556, 'validation/loss': 0.048975858837366104, 'validation/mean_average_precision': 0.17476428675753872, 'validation/num_examples': 43793, 'test/accuracy': 0.9847543835639954, 'test/loss': 0.05206332728266716, 'test/mean_average_precision': 0.17598293705416335, 'test/num_examples': 43793, 'score': 9135.70709347725, 'total_duration': 13232.066885709763, 'accumulated_submission_time': 9135.70709347725, 'accumulated_eval_time': 4094.420918226242, 'accumulated_logging_time': 1.1478347778320312, 'global_step': 28714, 'preemption_count': 0}), (29464, {'train/accuracy': 0.9890115261077881, 'train/loss': 0.03765709698200226, 'train/mean_average_precision': 0.22815512080052855, 'validation/accuracy': 0.9858322143554688, 'validation/loss': 0.04821404442191124, 'validation/mean_average_precision': 0.18028048993088003, 'validation/num_examples': 43793, 'test/accuracy': 0.9848394989967346, 'test/loss': 0.05123216658830643, 'test/mean_average_precision': 0.17881495194181926, 'test/num_examples': 43793, 'score': 9375.913627147675, 'total_duration': 13577.565557718277, 'accumulated_submission_time': 9375.913627147675, 'accumulated_eval_time': 4199.661685228348, 'accumulated_logging_time': 1.1791231632232666, 'global_step': 29464, 'preemption_count': 0}), (30214, {'train/accuracy': 0.9887194037437439, 'train/loss': 0.038636524230241776, 'train/mean_average_precision': 0.21387103798558202, 'validation/accuracy': 0.9857904314994812, 'validation/loss': 0.04857921600341797, 'validation/mean_average_precision': 0.18531219922152187, 'validation/num_examples': 43793, 'test/accuracy': 0.984809160232544, 'test/loss': 0.05179094895720482, 'test/mean_average_precision': 0.17941489697811877, 'test/num_examples': 43793, 'score': 9615.896932125092, 'total_duration': 13921.591437339783, 'accumulated_submission_time': 9615.896932125092, 'accumulated_eval_time': 4303.645714521408, 'accumulated_logging_time': 1.2145650386810303, 'global_step': 30214, 'preemption_count': 0}), (30969, {'train/accuracy': 0.988382875919342, 'train/loss': 0.03963419795036316, 'train/mean_average_precision': 0.21936030365713555, 'validation/accuracy': 0.9854400753974915, 'validation/loss': 0.050116200000047684, 'validation/mean_average_precision': 0.18122469766378826, 'validation/num_examples': 43793, 'test/accuracy': 0.9845911860466003, 'test/loss': 0.053077880293130875, 'test/mean_average_precision': 0.1786106783703574, 'test/num_examples': 43793, 'score': 9855.93932056427, 'total_duration': 14264.628145694733, 'accumulated_submission_time': 9855.93932056427, 'accumulated_eval_time': 4406.588081121445, 'accumulated_logging_time': 1.246260166168213, 'global_step': 30969, 'preemption_count': 0}), (31722, {'train/accuracy': 0.9886970520019531, 'train/loss': 0.03857322037220001, 'train/mean_average_precision': 0.21386019552262678, 'validation/accuracy': 0.9858525395393372, 'validation/loss': 0.048105161637067795, 'validation/mean_average_precision': 0.18594841492022302, 'validation/num_examples': 43793, 'test/accuracy': 0.984890878200531, 'test/loss': 0.05095868930220604, 'test/mean_average_precision': 0.18221947399078975, 'test/num_examples': 43793, 'score': 10096.182999134064, 'total_duration': 14613.498676538467, 'accumulated_submission_time': 10096.182999134064, 'accumulated_eval_time': 4515.162441253662, 'accumulated_logging_time': 1.278688669204712, 'global_step': 31722, 'preemption_count': 0}), (32467, {'train/accuracy': 0.9887663125991821, 'train/loss': 0.038279566913843155, 'train/mean_average_precision': 0.22456073744872496, 'validation/accuracy': 0.9856763482093811, 'validation/loss': 0.048430588096380234, 'validation/mean_average_precision': 0.18508520542216764, 'validation/num_examples': 43793, 'test/accuracy': 0.9847211241722107, 'test/loss': 0.05151313170790672, 'test/mean_average_precision': 0.17428486357819395, 'test/num_examples': 43793, 'score': 10336.253832101822, 'total_duration': 14956.602952480316, 'accumulated_submission_time': 10336.253832101822, 'accumulated_eval_time': 4618.139933824539, 'accumulated_logging_time': 1.3119385242462158, 'global_step': 32467, 'preemption_count': 0}), (33217, {'train/accuracy': 0.989039421081543, 'train/loss': 0.03772735968232155, 'train/mean_average_precision': 0.23207682323916878, 'validation/accuracy': 0.9859669804573059, 'validation/loss': 0.04795393720269203, 'validation/mean_average_precision': 0.19852442841848777, 'validation/num_examples': 43793, 'test/accuracy': 0.9849864840507507, 'test/loss': 0.05098487809300423, 'test/mean_average_precision': 0.18731190501152217, 'test/num_examples': 43793, 'score': 10576.26902961731, 'total_duration': 15300.3434009552, 'accumulated_submission_time': 10576.26902961731, 'accumulated_eval_time': 4721.810349941254, 'accumulated_logging_time': 1.3457691669464111, 'global_step': 33217, 'preemption_count': 0}), (33971, {'train/accuracy': 0.9889031052589417, 'train/loss': 0.03798950836062431, 'train/mean_average_precision': 0.22943758440364542, 'validation/accuracy': 0.9858683347702026, 'validation/loss': 0.048191137611866, 'validation/mean_average_precision': 0.1919149423802982, 'validation/num_examples': 43793, 'test/accuracy': 0.9849759340286255, 'test/loss': 0.05101734399795532, 'test/mean_average_precision': 0.18969810832651524, 'test/num_examples': 43793, 'score': 10816.320188999176, 'total_duration': 15644.702628612518, 'accumulated_submission_time': 10816.320188999176, 'accumulated_eval_time': 4826.063565731049, 'accumulated_logging_time': 1.380265235900879, 'global_step': 33971, 'preemption_count': 0}), (34728, {'train/accuracy': 0.9889699220657349, 'train/loss': 0.037672411650419235, 'train/mean_average_precision': 0.22945121345328973, 'validation/accuracy': 0.9858846068382263, 'validation/loss': 0.048056960105895996, 'validation/mean_average_precision': 0.18836389896567085, 'validation/num_examples': 43793, 'test/accuracy': 0.9849582314491272, 'test/loss': 0.05112144351005554, 'test/mean_average_precision': 0.1860170698764786, 'test/num_examples': 43793, 'score': 11056.37366938591, 'total_duration': 15991.229589700699, 'accumulated_submission_time': 11056.37366938591, 'accumulated_eval_time': 4932.482318401337, 'accumulated_logging_time': 1.4145681858062744, 'global_step': 34728, 'preemption_count': 0}), (35491, {'train/accuracy': 0.9889968633651733, 'train/loss': 0.037359390407800674, 'train/mean_average_precision': 0.24800388102685672, 'validation/accuracy': 0.9859061241149902, 'validation/loss': 0.04793180152773857, 'validation/mean_average_precision': 0.18544006624674064, 'validation/num_examples': 43793, 'test/accuracy': 0.9849721789360046, 'test/loss': 0.050931718200445175, 'test/mean_average_precision': 0.18398665496825234, 'test/num_examples': 43793, 'score': 11296.35082745552, 'total_duration': 16336.828307151794, 'accumulated_submission_time': 11296.35082745552, 'accumulated_eval_time': 5038.0507435798645, 'accumulated_logging_time': 1.4474399089813232, 'global_step': 35491, 'preemption_count': 0}), (36246, {'train/accuracy': 0.9889400601387024, 'train/loss': 0.03762971982359886, 'train/mean_average_precision': 0.22899459290143964, 'validation/accuracy': 0.9858976006507874, 'validation/loss': 0.04782705381512642, 'validation/mean_average_precision': 0.19022740009314312, 'validation/num_examples': 43793, 'test/accuracy': 0.9849587082862854, 'test/loss': 0.050787489861249924, 'test/mean_average_precision': 0.18202914172458476, 'test/num_examples': 43793, 'score': 11536.564465284348, 'total_duration': 16682.931549072266, 'accumulated_submission_time': 11536.564465284348, 'accumulated_eval_time': 5143.885582208633, 'accumulated_logging_time': 1.4814918041229248, 'global_step': 36246, 'preemption_count': 0}), (36989, {'train/accuracy': 0.9891141057014465, 'train/loss': 0.03726128116250038, 'train/mean_average_precision': 0.23637925039261792, 'validation/accuracy': 0.9858058094978333, 'validation/loss': 0.04803333431482315, 'validation/mean_average_precision': 0.185534120927033, 'validation/num_examples': 43793, 'test/accuracy': 0.9848929643630981, 'test/loss': 0.05071304738521576, 'test/mean_average_precision': 0.1814267991041178, 'test/num_examples': 43793, 'score': 11776.666022777557, 'total_duration': 17025.721967220306, 'accumulated_submission_time': 11776.666022777557, 'accumulated_eval_time': 5246.514811038971, 'accumulated_logging_time': 1.5184621810913086, 'global_step': 36989, 'preemption_count': 0}), (37742, {'train/accuracy': 0.9889169335365295, 'train/loss': 0.037845052778720856, 'train/mean_average_precision': 0.23672534421328206, 'validation/accuracy': 0.9857680797576904, 'validation/loss': 0.0481749102473259, 'validation/mean_average_precision': 0.18994630255589512, 'validation/num_examples': 43793, 'test/accuracy': 0.9848238825798035, 'test/loss': 0.05113251134753227, 'test/mean_average_precision': 0.1817323313886335, 'test/num_examples': 43793, 'score': 12016.776044130325, 'total_duration': 17369.446642637253, 'accumulated_submission_time': 12016.776044130325, 'accumulated_eval_time': 5350.075026988983, 'accumulated_logging_time': 1.5522980690002441, 'global_step': 37742, 'preemption_count': 0}), (38492, {'train/accuracy': 0.9891027808189392, 'train/loss': 0.03733918070793152, 'train/mean_average_precision': 0.23002880038922274, 'validation/accuracy': 0.9859219193458557, 'validation/loss': 0.04792803153395653, 'validation/mean_average_precision': 0.19331691561223313, 'validation/num_examples': 43793, 'test/accuracy': 0.9850079417228699, 'test/loss': 0.05091828107833862, 'test/mean_average_precision': 0.1852429104902491, 'test/num_examples': 43793, 'score': 12257.044536828995, 'total_duration': 17710.656693458557, 'accumulated_submission_time': 12257.044536828995, 'accumulated_eval_time': 5450.9605281353, 'accumulated_logging_time': 1.5866341590881348, 'global_step': 38492, 'preemption_count': 0}), (39249, {'train/accuracy': 0.9887359738349915, 'train/loss': 0.038305122405290604, 'train/mean_average_precision': 0.23918781732939232, 'validation/accuracy': 0.9858935475349426, 'validation/loss': 0.04855850338935852, 'validation/mean_average_precision': 0.1977071193646147, 'validation/num_examples': 43793, 'test/accuracy': 0.9849451780319214, 'test/loss': 0.05175068974494934, 'test/mean_average_precision': 0.18730468646185872, 'test/num_examples': 43793, 'score': 12497.11010313034, 'total_duration': 18051.52519583702, 'accumulated_submission_time': 12497.11010313034, 'accumulated_eval_time': 5551.708795070648, 'accumulated_logging_time': 1.6208436489105225, 'global_step': 39249, 'preemption_count': 0}), (40010, {'train/accuracy': 0.9889850616455078, 'train/loss': 0.0373271107673645, 'train/mean_average_precision': 0.2340222648566641, 'validation/accuracy': 0.9859641790390015, 'validation/loss': 0.047820981591939926, 'validation/mean_average_precision': 0.19187176273520837, 'validation/num_examples': 43793, 'test/accuracy': 0.985103964805603, 'test/loss': 0.05049894377589226, 'test/mean_average_precision': 0.19605617002786224, 'test/num_examples': 43793, 'score': 12737.06660580635, 'total_duration': 18392.110761642456, 'accumulated_submission_time': 12737.06660580635, 'accumulated_eval_time': 5652.283429861069, 'accumulated_logging_time': 1.6545679569244385, 'global_step': 40010, 'preemption_count': 0}), (40763, {'train/accuracy': 0.9889816045761108, 'train/loss': 0.037441350519657135, 'train/mean_average_precision': 0.229548285313721, 'validation/accuracy': 0.9859832525253296, 'validation/loss': 0.047773219645023346, 'validation/mean_average_precision': 0.19864646194954774, 'validation/num_examples': 43793, 'test/accuracy': 0.9850677847862244, 'test/loss': 0.05081846937537193, 'test/mean_average_precision': 0.19083416605251394, 'test/num_examples': 43793, 'score': 12977.28224658966, 'total_duration': 18735.97519636154, 'accumulated_submission_time': 12977.28224658966, 'accumulated_eval_time': 5755.875368833542, 'accumulated_logging_time': 1.6887316703796387, 'global_step': 40763, 'preemption_count': 0}), (41526, {'train/accuracy': 0.988987386226654, 'train/loss': 0.0376761332154274, 'train/mean_average_precision': 0.23214384180276643, 'validation/accuracy': 0.9857603907585144, 'validation/loss': 0.04862070456147194, 'validation/mean_average_precision': 0.18923447743174016, 'validation/num_examples': 43793, 'test/accuracy': 0.9847068190574646, 'test/loss': 0.05169043317437172, 'test/mean_average_precision': 0.18251327295114547, 'test/num_examples': 43793, 'score': 13217.424660682678, 'total_duration': 19081.433025598526, 'accumulated_submission_time': 13217.424660682678, 'accumulated_eval_time': 5861.133890390396, 'accumulated_logging_time': 1.7246840000152588, 'global_step': 41526, 'preemption_count': 0}), (42287, {'train/accuracy': 0.9890965819358826, 'train/loss': 0.03697286173701286, 'train/mean_average_precision': 0.24316292176873197, 'validation/accuracy': 0.9859690070152283, 'validation/loss': 0.04771135374903679, 'validation/mean_average_precision': 0.1947566100337512, 'validation/num_examples': 43793, 'test/accuracy': 0.9851654767990112, 'test/loss': 0.05071566626429558, 'test/mean_average_precision': 0.1907572109509011, 'test/num_examples': 43793, 'score': 13457.660992622375, 'total_duration': 19429.478434562683, 'accumulated_submission_time': 13457.660992622375, 'accumulated_eval_time': 5968.8874089717865, 'accumulated_logging_time': 1.7596709728240967, 'global_step': 42287, 'preemption_count': 0}), (43042, {'train/accuracy': 0.98912513256073, 'train/loss': 0.03699813038110733, 'train/mean_average_precision': 0.2451984672837403, 'validation/accuracy': 0.9858740568161011, 'validation/loss': 0.04763544350862503, 'validation/mean_average_precision': 0.19165971281291913, 'validation/num_examples': 43793, 'test/accuracy': 0.9849687814712524, 'test/loss': 0.050660889595746994, 'test/mean_average_precision': 0.1921175012712416, 'test/num_examples': 43793, 'score': 13697.648321390152, 'total_duration': 19772.52578687668, 'accumulated_submission_time': 13697.648321390152, 'accumulated_eval_time': 6071.891428232193, 'accumulated_logging_time': 1.7949151992797852, 'global_step': 43042, 'preemption_count': 0}), (43802, {'train/accuracy': 0.9891272783279419, 'train/loss': 0.03686603158712387, 'train/mean_average_precision': 0.24670075616815768, 'validation/accuracy': 0.9859893321990967, 'validation/loss': 0.04760757461190224, 'validation/mean_average_precision': 0.18880544708293598, 'validation/num_examples': 43793, 'test/accuracy': 0.9850454330444336, 'test/loss': 0.050481002777814865, 'test/mean_average_precision': 0.1850773565321149, 'test/num_examples': 43793, 'score': 13937.767841815948, 'total_duration': 20117.138426303864, 'accumulated_submission_time': 13937.767841815948, 'accumulated_eval_time': 6176.329439401627, 'accumulated_logging_time': 1.8292901515960693, 'global_step': 43802, 'preemption_count': 0}), (44565, {'train/accuracy': 0.9892011284828186, 'train/loss': 0.036679789423942566, 'train/mean_average_precision': 0.25198689902907984, 'validation/accuracy': 0.9859905242919922, 'validation/loss': 0.0475577712059021, 'validation/mean_average_precision': 0.19386990843271576, 'validation/num_examples': 43793, 'test/accuracy': 0.9850686192512512, 'test/loss': 0.050529588013887405, 'test/mean_average_precision': 0.19110150840640316, 'test/num_examples': 43793, 'score': 14177.993677854538, 'total_duration': 20458.01491880417, 'accumulated_submission_time': 14177.993677854538, 'accumulated_eval_time': 6276.924092531204, 'accumulated_logging_time': 1.8644392490386963, 'global_step': 44565, 'preemption_count': 0}), (45324, {'train/accuracy': 0.9893069863319397, 'train/loss': 0.03671472892165184, 'train/mean_average_precision': 0.2476210653071464, 'validation/accuracy': 0.9859580397605896, 'validation/loss': 0.04814805090427399, 'validation/mean_average_precision': 0.19630345933155982, 'validation/num_examples': 43793, 'test/accuracy': 0.9850033521652222, 'test/loss': 0.051478613168001175, 'test/mean_average_precision': 0.1892441479288406, 'test/num_examples': 43793, 'score': 14418.119437456131, 'total_duration': 20803.637764453888, 'accumulated_submission_time': 14418.119437456131, 'accumulated_eval_time': 6382.364597320557, 'accumulated_logging_time': 1.9001915454864502, 'global_step': 45324, 'preemption_count': 0}), (46075, {'train/accuracy': 0.989287257194519, 'train/loss': 0.03662101551890373, 'train/mean_average_precision': 0.25164082840433777, 'validation/accuracy': 0.9860518574714661, 'validation/loss': 0.04700767621397972, 'validation/mean_average_precision': 0.19854097081874142, 'validation/num_examples': 43793, 'test/accuracy': 0.9850778579711914, 'test/loss': 0.05002352595329285, 'test/mean_average_precision': 0.19016802194958837, 'test/num_examples': 43793, 'score': 14658.144562959671, 'total_duration': 21149.18072462082, 'accumulated_submission_time': 14658.144562959671, 'accumulated_eval_time': 6487.826305150986, 'accumulated_logging_time': 1.9359188079833984, 'global_step': 46075, 'preemption_count': 0}), (46830, {'train/accuracy': 0.9892902970314026, 'train/loss': 0.03647993132472038, 'train/mean_average_precision': 0.2555223045112521, 'validation/accuracy': 0.9860911965370178, 'validation/loss': 0.04714075103402138, 'validation/mean_average_precision': 0.20659838503066164, 'validation/num_examples': 43793, 'test/accuracy': 0.9850918054580688, 'test/loss': 0.05010078102350235, 'test/mean_average_precision': 0.20143208691363684, 'test/num_examples': 43793, 'score': 14898.28629231453, 'total_duration': 21493.333992242813, 'accumulated_submission_time': 14898.28629231453, 'accumulated_eval_time': 6591.78231549263, 'accumulated_logging_time': 1.970759391784668, 'global_step': 46830, 'preemption_count': 0}), (47588, {'train/accuracy': 0.9891781806945801, 'train/loss': 0.03695470094680786, 'train/mean_average_precision': 0.25523705425384435, 'validation/accuracy': 0.9860566854476929, 'validation/loss': 0.047206051647663116, 'validation/mean_average_precision': 0.20168868363425443, 'validation/num_examples': 43793, 'test/accuracy': 0.9851625561714172, 'test/loss': 0.050044842064380646, 'test/mean_average_precision': 0.1926117407578925, 'test/num_examples': 43793, 'score': 15138.5211622715, 'total_duration': 21844.761209726334, 'accumulated_submission_time': 15138.5211622715, 'accumulated_eval_time': 6702.919489622116, 'accumulated_logging_time': 2.0053157806396484, 'global_step': 47588, 'preemption_count': 0}), (48349, {'train/accuracy': 0.9893152117729187, 'train/loss': 0.03633428364992142, 'train/mean_average_precision': 0.24807640584328788, 'validation/accuracy': 0.9860628247261047, 'validation/loss': 0.047021202743053436, 'validation/mean_average_precision': 0.20590093571940493, 'validation/num_examples': 43793, 'test/accuracy': 0.9851372838020325, 'test/loss': 0.04998750612139702, 'test/mean_average_precision': 0.19512421181932446, 'test/num_examples': 43793, 'score': 15378.47325873375, 'total_duration': 22186.35283780098, 'accumulated_submission_time': 15378.47325873375, 'accumulated_eval_time': 6804.498358488083, 'accumulated_logging_time': 2.044494390487671, 'global_step': 48349, 'preemption_count': 0}), (49112, {'train/accuracy': 0.9893460273742676, 'train/loss': 0.03618969768285751, 'train/mean_average_precision': 0.24361136020841997, 'validation/accuracy': 0.9861687421798706, 'validation/loss': 0.046943336725234985, 'validation/mean_average_precision': 0.20543517990381355, 'validation/num_examples': 43793, 'test/accuracy': 0.985241711139679, 'test/loss': 0.04996630921959877, 'test/mean_average_precision': 0.19933563518916733, 'test/num_examples': 43793, 'score': 15618.42730998993, 'total_duration': 22530.29949116707, 'accumulated_submission_time': 15618.42730998993, 'accumulated_eval_time': 6908.433473587036, 'accumulated_logging_time': 2.0814638137817383, 'global_step': 49112, 'preemption_count': 0}), (49869, {'train/accuracy': 0.9894251823425293, 'train/loss': 0.036164749413728714, 'train/mean_average_precision': 0.2601694948034501, 'validation/accuracy': 0.9860376119613647, 'validation/loss': 0.04690441116690636, 'validation/mean_average_precision': 0.20304835872201743, 'validation/num_examples': 43793, 'test/accuracy': 0.9851536750793457, 'test/loss': 0.049883656203746796, 'test/mean_average_precision': 0.20062726799576788, 'test/num_examples': 43793, 'score': 15858.633188962936, 'total_duration': 22871.511667251587, 'accumulated_submission_time': 15858.633188962936, 'accumulated_eval_time': 7009.377001285553, 'accumulated_logging_time': 2.1225428581237793, 'global_step': 49869, 'preemption_count': 0}), (50631, {'train/accuracy': 0.9894543290138245, 'train/loss': 0.035730570554733276, 'train/mean_average_precision': 0.26220534981854027, 'validation/accuracy': 0.9860794544219971, 'validation/loss': 0.04686795175075531, 'validation/mean_average_precision': 0.20630899890789783, 'validation/num_examples': 43793, 'test/accuracy': 0.9852059483528137, 'test/loss': 0.04976944252848625, 'test/mean_average_precision': 0.1960011431400151, 'test/num_examples': 43793, 'score': 16098.7771692276, 'total_duration': 23215.639623880386, 'accumulated_submission_time': 16098.7771692276, 'accumulated_eval_time': 7113.305378198624, 'accumulated_logging_time': 2.1576476097106934, 'global_step': 50631, 'preemption_count': 0}), (51381, {'train/accuracy': 0.9894623160362244, 'train/loss': 0.03548566251993179, 'train/mean_average_precision': 0.26926252193776995, 'validation/accuracy': 0.986139714717865, 'validation/loss': 0.047138918191194534, 'validation/mean_average_precision': 0.20698086382406308, 'validation/num_examples': 43793, 'test/accuracy': 0.9851823449134827, 'test/loss': 0.050275906920433044, 'test/mean_average_precision': 0.20079182484563576, 'test/num_examples': 43793, 'score': 16338.885677576065, 'total_duration': 23559.649918794632, 'accumulated_submission_time': 16338.885677576065, 'accumulated_eval_time': 7217.148921728134, 'accumulated_logging_time': 2.195411443710327, 'global_step': 51381, 'preemption_count': 0}), (52137, {'train/accuracy': 0.9895649552345276, 'train/loss': 0.03519775718450546, 'train/mean_average_precision': 0.274967294293767, 'validation/accuracy': 0.9861720204353333, 'validation/loss': 0.04690484330058098, 'validation/mean_average_precision': 0.20737156710161236, 'validation/num_examples': 43793, 'test/accuracy': 0.9852589964866638, 'test/loss': 0.04989057034254074, 'test/mean_average_precision': 0.19962056512129037, 'test/num_examples': 43793, 'score': 16579.111583709717, 'total_duration': 23906.143963336945, 'accumulated_submission_time': 16579.111583709717, 'accumulated_eval_time': 7323.358339309692, 'accumulated_logging_time': 2.2338502407073975, 'global_step': 52137, 'preemption_count': 0}), (52887, {'train/accuracy': 0.9894105792045593, 'train/loss': 0.035687174648046494, 'train/mean_average_precision': 0.27322752372913345, 'validation/accuracy': 0.9862263798713684, 'validation/loss': 0.04678715765476227, 'validation/mean_average_precision': 0.2090118597592896, 'validation/num_examples': 43793, 'test/accuracy': 0.9852981567382812, 'test/loss': 0.04992193728685379, 'test/mean_average_precision': 0.19961591993997813, 'test/num_examples': 43793, 'score': 16819.154947042465, 'total_duration': 24251.066781044006, 'accumulated_submission_time': 16819.154947042465, 'accumulated_eval_time': 7428.175041437149, 'accumulated_logging_time': 2.2763447761535645, 'global_step': 52887, 'preemption_count': 0}), (53642, {'train/accuracy': 0.9896687865257263, 'train/loss': 0.035085875540971756, 'train/mean_average_precision': 0.27331444403648536, 'validation/accuracy': 0.9860932230949402, 'validation/loss': 0.046895693987607956, 'validation/mean_average_precision': 0.20682647016133246, 'validation/num_examples': 43793, 'test/accuracy': 0.9852105379104614, 'test/loss': 0.04975775256752968, 'test/mean_average_precision': 0.19736626017748696, 'test/num_examples': 43793, 'score': 17059.26643562317, 'total_duration': 24593.81644487381, 'accumulated_submission_time': 17059.26643562317, 'accumulated_eval_time': 7530.755472898483, 'accumulated_logging_time': 2.313117027282715, 'global_step': 53642, 'preemption_count': 0}), (54398, {'train/accuracy': 0.9894760251045227, 'train/loss': 0.03575289249420166, 'train/mean_average_precision': 0.26404784370415507, 'validation/accuracy': 0.9861346483230591, 'validation/loss': 0.04675270989537239, 'validation/mean_average_precision': 0.20830687535056122, 'validation/num_examples': 43793, 'test/accuracy': 0.9852269887924194, 'test/loss': 0.04970965534448624, 'test/mean_average_precision': 0.1957582689787017, 'test/num_examples': 43793, 'score': 17299.366802215576, 'total_duration': 24937.78490638733, 'accumulated_submission_time': 17299.366802215576, 'accumulated_eval_time': 7634.566126823425, 'accumulated_logging_time': 2.349884271621704, 'global_step': 54398, 'preemption_count': 0}), (55156, {'train/accuracy': 0.9895650148391724, 'train/loss': 0.035204917192459106, 'train/mean_average_precision': 0.2680131655883736, 'validation/accuracy': 0.986196756362915, 'validation/loss': 0.04677639529109001, 'validation/mean_average_precision': 0.2084649784585847, 'validation/num_examples': 43793, 'test/accuracy': 0.9852808713912964, 'test/loss': 0.049832772463560104, 'test/mean_average_precision': 0.202013044800903, 'test/num_examples': 43793, 'score': 17539.38094496727, 'total_duration': 25277.153856515884, 'accumulated_submission_time': 17539.38094496727, 'accumulated_eval_time': 7733.860890388489, 'accumulated_logging_time': 2.3890788555145264, 'global_step': 55156, 'preemption_count': 0}), (55922, {'train/accuracy': 0.9894495010375977, 'train/loss': 0.03555284067988396, 'train/mean_average_precision': 0.2755270016128907, 'validation/accuracy': 0.9862328767776489, 'validation/loss': 0.046522095799446106, 'validation/mean_average_precision': 0.21318092778342917, 'validation/num_examples': 43793, 'test/accuracy': 0.9852501749992371, 'test/loss': 0.04951382055878639, 'test/mean_average_precision': 0.20283296752641977, 'test/num_examples': 43793, 'score': 17779.42640852928, 'total_duration': 25619.509373903275, 'accumulated_submission_time': 17779.42640852928, 'accumulated_eval_time': 7836.111313343048, 'accumulated_logging_time': 2.427854061126709, 'global_step': 55922, 'preemption_count': 0}), (56673, {'train/accuracy': 0.9895038604736328, 'train/loss': 0.03523600473999977, 'train/mean_average_precision': 0.2716795602529939, 'validation/accuracy': 0.9862877130508423, 'validation/loss': 0.046465057879686356, 'validation/mean_average_precision': 0.21102340437640538, 'validation/num_examples': 43793, 'test/accuracy': 0.985371470451355, 'test/loss': 0.04939175024628639, 'test/mean_average_precision': 0.20396700578340565, 'test/num_examples': 43793, 'score': 18019.612231731415, 'total_duration': 25963.269825220108, 'accumulated_submission_time': 18019.612231731415, 'accumulated_eval_time': 7939.627541303635, 'accumulated_logging_time': 2.4657275676727295, 'global_step': 56673, 'preemption_count': 0}), (57429, {'train/accuracy': 0.9898420572280884, 'train/loss': 0.03476691246032715, 'train/mean_average_precision': 0.28116842626885963, 'validation/accuracy': 0.9861756563186646, 'validation/loss': 0.04659731686115265, 'validation/mean_average_precision': 0.21251331949642285, 'validation/num_examples': 43793, 'test/accuracy': 0.9852796196937561, 'test/loss': 0.049422599375247955, 'test/mean_average_precision': 0.2015535494588887, 'test/num_examples': 43793, 'score': 18259.605164289474, 'total_duration': 26305.261911392212, 'accumulated_submission_time': 18259.605164289474, 'accumulated_eval_time': 8041.5682673454285, 'accumulated_logging_time': 2.5035622119903564, 'global_step': 57429, 'preemption_count': 0})], 'global_step': 58120}
I0206 04:14:35.106643 139919816816448 submission_runner.py:586] Timing: 18477.279756069183
I0206 04:14:35.106708 139919816816448 submission_runner.py:588] Total number of evals: 77
I0206 04:14:35.106758 139919816816448 submission_runner.py:589] ====================
I0206 04:14:35.106814 139919816816448 submission_runner.py:542] Using RNG seed 356686224
I0206 04:14:35.170980 139919816816448 submission_runner.py:551] --- Tuning run 5/5 ---
I0206 04:14:35.171124 139919816816448 submission_runner.py:556] Creating tuning directory at /experiment_runs/prize_qualification/study_1/ogbg_jax/trial_5.
I0206 04:14:35.173657 139919816816448 logger_utils.py:92] Saving hparams to /experiment_runs/prize_qualification/study_1/ogbg_jax/trial_5/hparams.json.
I0206 04:14:35.303959 139919816816448 submission_runner.py:206] Initializing dataset.
I0206 04:14:35.389480 139919816816448 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0206 04:14:35.393718 139919816816448 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0206 04:14:35.524643 139919816816448 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0206 04:14:35.561845 139919816816448 submission_runner.py:213] Initializing model.
I0206 04:14:38.007633 139919816816448 submission_runner.py:255] Initializing optimizer.
I0206 04:14:38.600195 139919816816448 submission_runner.py:262] Initializing metrics bundle.
I0206 04:14:38.600405 139919816816448 submission_runner.py:280] Initializing checkpoint and logger.
I0206 04:14:38.601071 139919816816448 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification/study_1/ogbg_jax/trial_5 with prefix checkpoint_
I0206 04:14:38.601200 139919816816448 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification/study_1/ogbg_jax/trial_5/meta_data_0.json.
I0206 04:14:38.601411 139919816816448 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0206 04:14:38.601474 139919816816448 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0206 04:14:40.906266 139919816816448 logger_utils.py:220] Unable to record git information. Continuing without it.
I0206 04:14:43.201555 139919816816448 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification/study_1/ogbg_jax/trial_5/flags_0.json.
I0206 04:14:43.205736 139919816816448 submission_runner.py:314] Starting training loop.
I0206 04:14:56.309043 139679742572288 logging_writer.py:48] [0] global_step=0, grad_norm=1.851978063583374, loss=0.7274106740951538
I0206 04:14:56.320753 139919816816448 spec.py:321] Evaluating on the training split.
I0206 04:16:37.214632 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 04:16:40.630245 139919816816448 spec.py:349] Evaluating on the test split.
I0206 04:16:43.929684 139919816816448 submission_runner.py:408] Time since start: 120.72s, 	Step: 1, 	{'train/accuracy': 0.5324588418006897, 'train/loss': 0.7277070879936218, 'train/mean_average_precision': 0.02175867816848414, 'validation/accuracy': 0.5230699777603149, 'validation/loss': 0.7331878542900085, 'validation/mean_average_precision': 0.02548130539809287, 'validation/num_examples': 43793, 'test/accuracy': 0.5214918851852417, 'test/loss': 0.7347381114959717, 'test/mean_average_precision': 0.026820638123804655, 'test/num_examples': 43793, 'score': 13.114962816238403, 'total_duration': 120.72387218475342, 'accumulated_submission_time': 13.114962816238403, 'accumulated_eval_time': 107.60885667800903, 'accumulated_logging_time': 0}
I0206 04:16:43.940090 139697948911360 logging_writer.py:48] [1] accumulated_eval_time=107.608857, accumulated_logging_time=0, accumulated_submission_time=13.114963, global_step=1, preemption_count=0, score=13.114963, test/accuracy=0.521492, test/loss=0.734738, test/mean_average_precision=0.026821, test/num_examples=43793, total_duration=120.723872, train/accuracy=0.532459, train/loss=0.727707, train/mean_average_precision=0.021759, validation/accuracy=0.523070, validation/loss=0.733188, validation/mean_average_precision=0.025481, validation/num_examples=43793
I0206 04:17:16.149941 139735302588160 logging_writer.py:48] [100] global_step=100, grad_norm=0.2922704219818115, loss=0.2678224444389343
I0206 04:17:47.532150 139697948911360 logging_writer.py:48] [200] global_step=200, grad_norm=0.09320424497127533, loss=0.11452888697385788
I0206 04:18:19.111777 139735302588160 logging_writer.py:48] [300] global_step=300, grad_norm=0.03005778044462204, loss=0.06601028889417648
I0206 04:18:50.692846 139697948911360 logging_writer.py:48] [400] global_step=400, grad_norm=0.014678459614515305, loss=0.058897748589515686
I0206 04:19:22.412561 139735302588160 logging_writer.py:48] [500] global_step=500, grad_norm=0.015004844404757023, loss=0.05937953293323517
I0206 04:19:53.818531 139697948911360 logging_writer.py:48] [600] global_step=600, grad_norm=0.016165385022759438, loss=0.05194910615682602
I0206 04:20:25.445023 139735302588160 logging_writer.py:48] [700] global_step=700, grad_norm=0.0358772948384285, loss=0.05466710031032562
I0206 04:20:44.146258 139919816816448 spec.py:321] Evaluating on the training split.
I0206 04:22:23.862510 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 04:22:27.138366 139919816816448 spec.py:349] Evaluating on the test split.
I0206 04:22:30.258663 139919816816448 submission_runner.py:408] Time since start: 467.05s, 	Step: 760, 	{'train/accuracy': 0.9868366122245789, 'train/loss': 0.05256984755396843, 'train/mean_average_precision': 0.04591733329195911, 'validation/accuracy': 0.9841719269752502, 'validation/loss': 0.06288707256317139, 'validation/mean_average_precision': 0.045798354438788554, 'validation/num_examples': 43793, 'test/accuracy': 0.983195960521698, 'test/loss': 0.06619127094745636, 'test/mean_average_precision': 0.04798481508347616, 'test/num_examples': 43793, 'score': 253.0232331752777, 'total_duration': 467.05287051200867, 'accumulated_submission_time': 253.0232331752777, 'accumulated_eval_time': 213.72126507759094, 'accumulated_logging_time': 0.2872607707977295}
I0206 04:22:30.274312 139679751386880 logging_writer.py:48] [760] accumulated_eval_time=213.721265, accumulated_logging_time=0.287261, accumulated_submission_time=253.023233, global_step=760, preemption_count=0, score=253.023233, test/accuracy=0.983196, test/loss=0.066191, test/mean_average_precision=0.047985, test/num_examples=43793, total_duration=467.052871, train/accuracy=0.986837, train/loss=0.052570, train/mean_average_precision=0.045917, validation/accuracy=0.984172, validation/loss=0.062887, validation/mean_average_precision=0.045798, validation/num_examples=43793
I0206 04:22:43.404534 139679759779584 logging_writer.py:48] [800] global_step=800, grad_norm=0.014015678316354752, loss=0.05189470946788788
I0206 04:23:15.475090 139679751386880 logging_writer.py:48] [900] global_step=900, grad_norm=0.02126217447221279, loss=0.053366903215646744
I0206 04:23:47.732444 139679759779584 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.012379905208945274, loss=0.05076739937067032
I0206 04:24:20.460579 139679751386880 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.02632233500480652, loss=0.049490079283714294
I0206 04:24:53.104262 139679759779584 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.014613479375839233, loss=0.0451640747487545
I0206 04:25:25.672889 139679751386880 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.014748436398804188, loss=0.05169753357768059
I0206 04:25:57.276148 139679759779584 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.014796482399106026, loss=0.04463517665863037
I0206 04:26:29.061533 139679751386880 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.04232604801654816, loss=0.05538879334926605
I0206 04:26:30.337131 139919816816448 spec.py:321] Evaluating on the training split.
I0206 04:28:10.085647 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 04:28:13.146520 139919816816448 spec.py:349] Evaluating on the test split.
I0206 04:28:16.094658 139919816816448 submission_runner.py:408] Time since start: 812.89s, 	Step: 1505, 	{'train/accuracy': 0.986958384513855, 'train/loss': 0.04801762476563454, 'train/mean_average_precision': 0.09383095081732748, 'validation/accuracy': 0.9844183325767517, 'validation/loss': 0.05740370228886604, 'validation/mean_average_precision': 0.0944905234983526, 'validation/num_examples': 43793, 'test/accuracy': 0.9834386110305786, 'test/loss': 0.0608416348695755, 'test/mean_average_precision': 0.09337897309384578, 'test/num_examples': 43793, 'score': 493.0518238544464, 'total_duration': 812.8888652324677, 'accumulated_submission_time': 493.0518238544464, 'accumulated_eval_time': 319.4787435531616, 'accumulated_logging_time': 0.3140983581542969}
I0206 04:28:16.109828 139716992366336 logging_writer.py:48] [1505] accumulated_eval_time=319.478744, accumulated_logging_time=0.314098, accumulated_submission_time=493.051824, global_step=1505, preemption_count=0, score=493.051824, test/accuracy=0.983439, test/loss=0.060842, test/mean_average_precision=0.093379, test/num_examples=43793, total_duration=812.888865, train/accuracy=0.986958, train/loss=0.048018, train/mean_average_precision=0.093831, validation/accuracy=0.984418, validation/loss=0.057404, validation/mean_average_precision=0.094491, validation/num_examples=43793
I0206 04:28:46.612155 139735302588160 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.014501670375466347, loss=0.047545868903398514
I0206 04:29:18.283331 139716992366336 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.027748236432671547, loss=0.05374909192323685
I0206 04:29:49.782639 139735302588160 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.012324996292591095, loss=0.043660685420036316
I0206 04:30:21.864722 139716992366336 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.029833124950528145, loss=0.04537595063447952
I0206 04:30:53.238348 139735302588160 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.012626385316252708, loss=0.04573839530348778
I0206 04:31:25.657175 139716992366336 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.011907577514648438, loss=0.047471463680267334
I0206 04:31:57.831919 139735302588160 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.010303105227649212, loss=0.04867209866642952
I0206 04:32:16.241907 139919816816448 spec.py:321] Evaluating on the training split.
I0206 04:33:51.700835 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 04:33:54.734373 139919816816448 spec.py:349] Evaluating on the test split.
I0206 04:33:57.726631 139919816816448 submission_runner.py:408] Time since start: 1154.52s, 	Step: 2258, 	{'train/accuracy': 0.9876599907875061, 'train/loss': 0.04372995346784592, 'train/mean_average_precision': 0.13444942583743152, 'validation/accuracy': 0.9849497079849243, 'validation/loss': 0.05331363528966904, 'validation/mean_average_precision': 0.13050464298387648, 'validation/num_examples': 43793, 'test/accuracy': 0.9839305281639099, 'test/loss': 0.056407343596220016, 'test/mean_average_precision': 0.12865337006852393, 'test/num_examples': 43793, 'score': 733.1513078212738, 'total_duration': 1154.5208294391632, 'accumulated_submission_time': 733.1513078212738, 'accumulated_eval_time': 420.96342611312866, 'accumulated_logging_time': 0.3397960662841797}
I0206 04:33:57.742582 139678888466176 logging_writer.py:48] [2258] accumulated_eval_time=420.963426, accumulated_logging_time=0.339796, accumulated_submission_time=733.151308, global_step=2258, preemption_count=0, score=733.151308, test/accuracy=0.983931, test/loss=0.056407, test/mean_average_precision=0.128653, test/num_examples=43793, total_duration=1154.520829, train/accuracy=0.987660, train/loss=0.043730, train/mean_average_precision=0.134449, validation/accuracy=0.984950, validation/loss=0.053314, validation/mean_average_precision=0.130505, validation/num_examples=43793
I0206 04:34:11.705955 139697948911360 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.018464036285877228, loss=0.05040406808257103
I0206 04:34:43.832400 139678888466176 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.011053654365241528, loss=0.04214583709836006
I0206 04:35:15.951919 139697948911360 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.016733545809984207, loss=0.042813051491975784
I0206 04:35:47.893081 139678888466176 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.0131218321621418, loss=0.04064059630036354
I0206 04:36:19.827113 139697948911360 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.01466927770525217, loss=0.041561298072338104
I0206 04:36:51.437601 139678888466176 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.022569358348846436, loss=0.0440516397356987
I0206 04:37:23.498905 139697948911360 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.013064649887382984, loss=0.048115797340869904
I0206 04:37:54.766366 139678888466176 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.013924377039074898, loss=0.04428980126976967
I0206 04:37:57.988449 139919816816448 spec.py:321] Evaluating on the training split.
I0206 04:39:31.039699 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 04:39:34.088487 139919816816448 spec.py:349] Evaluating on the test split.
I0206 04:39:37.068361 139919816816448 submission_runner.py:408] Time since start: 1493.86s, 	Step: 3011, 	{'train/accuracy': 0.988203763961792, 'train/loss': 0.04144752025604248, 'train/mean_average_precision': 0.1611080446191192, 'validation/accuracy': 0.9852355122566223, 'validation/loss': 0.050933822989463806, 'validation/mean_average_precision': 0.15108787353642003, 'validation/num_examples': 43793, 'test/accuracy': 0.9842051863670349, 'test/loss': 0.054171618074178696, 'test/mean_average_precision': 0.14581284698418215, 'test/num_examples': 43793, 'score': 973.3656969070435, 'total_duration': 1493.8625662326813, 'accumulated_submission_time': 973.3656969070435, 'accumulated_eval_time': 520.0432939529419, 'accumulated_logging_time': 0.3667905330657959}
I0206 04:39:37.085050 139679759779584 logging_writer.py:48] [3011] accumulated_eval_time=520.043294, accumulated_logging_time=0.366791, accumulated_submission_time=973.365697, global_step=3011, preemption_count=0, score=973.365697, test/accuracy=0.984205, test/loss=0.054172, test/mean_average_precision=0.145813, test/num_examples=43793, total_duration=1493.862566, train/accuracy=0.988204, train/loss=0.041448, train/mean_average_precision=0.161108, validation/accuracy=0.985236, validation/loss=0.050934, validation/mean_average_precision=0.151088, validation/num_examples=43793
I0206 04:40:07.085442 139716992366336 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.010248909704387188, loss=0.040746014565229416
I0206 04:40:39.087028 139679759779584 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.012029950506985188, loss=0.042728815227746964
I0206 04:41:11.024560 139716992366336 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.012076139450073242, loss=0.04108477756381035
I0206 04:41:42.899219 139679759779584 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.011909909546375275, loss=0.03692546486854553
I0206 04:42:14.885363 139716992366336 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.01798209175467491, loss=0.04067615792155266
I0206 04:42:46.803637 139679759779584 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.013415703549981117, loss=0.039477113634347916
I0206 04:43:18.661293 139716992366336 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.011137501336634159, loss=0.0422307550907135
I0206 04:43:37.381360 139919816816448 spec.py:321] Evaluating on the training split.
I0206 04:45:20.122819 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 04:45:23.122174 139919816816448 spec.py:349] Evaluating on the test split.
I0206 04:45:26.143306 139919816816448 submission_runner.py:408] Time since start: 1842.94s, 	Step: 3759, 	{'train/accuracy': 0.9883896112442017, 'train/loss': 0.040495552122592926, 'train/mean_average_precision': 0.19664721625610154, 'validation/accuracy': 0.9854575395584106, 'validation/loss': 0.04961736127734184, 'validation/mean_average_precision': 0.17224777324275892, 'validation/num_examples': 43793, 'test/accuracy': 0.9845345616340637, 'test/loss': 0.052431534975767136, 'test/mean_average_precision': 0.1698998980243238, 'test/num_examples': 43793, 'score': 1213.6298730373383, 'total_duration': 1842.9375042915344, 'accumulated_submission_time': 1213.6298730373383, 'accumulated_eval_time': 628.8051879405975, 'accumulated_logging_time': 0.3951599597930908}
I0206 04:45:26.159954 139678888466176 logging_writer.py:48] [3759] accumulated_eval_time=628.805188, accumulated_logging_time=0.395160, accumulated_submission_time=1213.629873, global_step=3759, preemption_count=0, score=1213.629873, test/accuracy=0.984535, test/loss=0.052432, test/mean_average_precision=0.169900, test/num_examples=43793, total_duration=1842.937504, train/accuracy=0.988390, train/loss=0.040496, train/mean_average_precision=0.196647, validation/accuracy=0.985458, validation/loss=0.049617, validation/mean_average_precision=0.172248, validation/num_examples=43793
I0206 04:45:39.910132 139697948911360 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.010132716037333012, loss=0.03757944703102112
I0206 04:46:12.169198 139678888466176 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.014891327358782291, loss=0.03982603922486305
I0206 04:46:44.241391 139697948911360 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.013704951852560043, loss=0.04254142940044403
I0206 04:47:16.410223 139678888466176 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.01088038645684719, loss=0.039053015410900116
I0206 04:47:48.276656 139697948911360 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.010497884824872017, loss=0.036812923848629
I0206 04:48:19.648630 139678888466176 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.01524023525416851, loss=0.03922535106539726
I0206 04:48:51.170696 139697948911360 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.009764203801751137, loss=0.0379926823079586
I0206 04:49:22.757440 139678888466176 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.010904689319431782, loss=0.040161192417144775
I0206 04:49:26.216633 139919816816448 spec.py:321] Evaluating on the training split.
I0206 04:51:01.565004 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 04:51:04.917628 139919816816448 spec.py:349] Evaluating on the test split.
I0206 04:51:08.245152 139919816816448 submission_runner.py:408] Time since start: 2185.04s, 	Step: 4512, 	{'train/accuracy': 0.9884734749794006, 'train/loss': 0.039681486785411835, 'train/mean_average_precision': 0.21041611738254148, 'validation/accuracy': 0.9857286810874939, 'validation/loss': 0.048848893493413925, 'validation/mean_average_precision': 0.18536137968006355, 'validation/num_examples': 43793, 'test/accuracy': 0.984804093837738, 'test/loss': 0.05158129334449768, 'test/mean_average_precision': 0.1859877344371198, 'test/num_examples': 43793, 'score': 1453.6540446281433, 'total_duration': 2185.0393300056458, 'accumulated_submission_time': 1453.6540446281433, 'accumulated_eval_time': 730.8336308002472, 'accumulated_logging_time': 0.4234771728515625}
I0206 04:51:08.262741 139679759779584 logging_writer.py:48] [4512] accumulated_eval_time=730.833631, accumulated_logging_time=0.423477, accumulated_submission_time=1453.654045, global_step=4512, preemption_count=0, score=1453.654045, test/accuracy=0.984804, test/loss=0.051581, test/mean_average_precision=0.185988, test/num_examples=43793, total_duration=2185.039330, train/accuracy=0.988473, train/loss=0.039681, train/mean_average_precision=0.210416, validation/accuracy=0.985729, validation/loss=0.048849, validation/mean_average_precision=0.185361, validation/num_examples=43793
I0206 04:51:36.817603 139716992366336 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.01382372435182333, loss=0.04502948001027107
I0206 04:52:08.788264 139679759779584 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.0140288220718503, loss=0.035692423582077026
I0206 04:52:40.909191 139716992366336 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.013766846619546413, loss=0.039080746471881866
I0206 04:53:12.994837 139679759779584 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.012124530971050262, loss=0.037226736545562744
I0206 04:53:44.898201 139716992366336 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.012025441974401474, loss=0.03945259377360344
I0206 04:54:16.999037 139679759779584 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.010714772157371044, loss=0.035619303584098816
I0206 04:54:49.434308 139716992366336 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.011461193673312664, loss=0.04053455591201782
I0206 04:55:08.494688 139919816816448 spec.py:321] Evaluating on the training split.
I0206 04:56:47.845808 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 04:56:50.903506 139919816816448 spec.py:349] Evaluating on the test split.
I0206 04:56:53.920212 139919816816448 submission_runner.py:408] Time since start: 2530.71s, 	Step: 5260, 	{'train/accuracy': 0.9889073967933655, 'train/loss': 0.03776993602514267, 'train/mean_average_precision': 0.240450900537608, 'validation/accuracy': 0.9860051274299622, 'validation/loss': 0.0472903698682785, 'validation/mean_average_precision': 0.20116533712170387, 'validation/num_examples': 43793, 'test/accuracy': 0.9851145148277283, 'test/loss': 0.04979237541556358, 'test/mean_average_precision': 0.20280649626517522, 'test/num_examples': 43793, 'score': 1693.8498435020447, 'total_duration': 2530.7144179344177, 'accumulated_submission_time': 1693.8498435020447, 'accumulated_eval_time': 836.2591185569763, 'accumulated_logging_time': 0.4546363353729248}
I0206 04:56:53.937816 139678888466176 logging_writer.py:48] [5260] accumulated_eval_time=836.259119, accumulated_logging_time=0.454636, accumulated_submission_time=1693.849844, global_step=5260, preemption_count=0, score=1693.849844, test/accuracy=0.985115, test/loss=0.049792, test/mean_average_precision=0.202806, test/num_examples=43793, total_duration=2530.714418, train/accuracy=0.988907, train/loss=0.037770, train/mean_average_precision=0.240451, validation/accuracy=0.986005, validation/loss=0.047290, validation/mean_average_precision=0.201165, validation/num_examples=43793
I0206 04:57:07.116586 139735302588160 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.012019574642181396, loss=0.0366259403526783
I0206 04:57:38.551033 139678888466176 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.011617688462138176, loss=0.044297389686107635
I0206 04:58:10.547514 139735302588160 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.014676414430141449, loss=0.04112289845943451
I0206 04:58:42.423105 139678888466176 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.01652481034398079, loss=0.040318310260772705
I0206 04:59:14.167215 139735302588160 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.01202454511076212, loss=0.03215205669403076
I0206 04:59:45.543561 139678888466176 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.020529774948954582, loss=0.038261778652668
I0206 05:00:17.102882 139735302588160 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.01171557791531086, loss=0.03665035590529442
I0206 05:00:48.929506 139678888466176 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.010306346230208874, loss=0.0339791364967823
I0206 05:00:53.985994 139919816816448 spec.py:321] Evaluating on the training split.
I0206 05:02:32.949407 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 05:02:35.974829 139919816816448 spec.py:349] Evaluating on the test split.
I0206 05:02:38.929083 139919816816448 submission_runner.py:408] Time since start: 2875.72s, 	Step: 6017, 	{'train/accuracy': 0.9888168573379517, 'train/loss': 0.037816647440195084, 'train/mean_average_precision': 0.24340651811467862, 'validation/accuracy': 0.9859337210655212, 'validation/loss': 0.047266535460948944, 'validation/mean_average_precision': 0.20417978165163772, 'validation/num_examples': 43793, 'test/accuracy': 0.9850445985794067, 'test/loss': 0.04999808222055435, 'test/mean_average_precision': 0.20397912744073982, 'test/num_examples': 43793, 'score': 1933.8649094104767, 'total_duration': 2875.7232887744904, 'accumulated_submission_time': 1933.8649094104767, 'accumulated_eval_time': 941.2021589279175, 'accumulated_logging_time': 0.4842491149902344}
I0206 05:02:38.945717 139758996317952 logging_writer.py:48] [6017] accumulated_eval_time=941.202159, accumulated_logging_time=0.484249, accumulated_submission_time=1933.864909, global_step=6017, preemption_count=0, score=1933.864909, test/accuracy=0.985045, test/loss=0.049998, test/mean_average_precision=0.203979, test/num_examples=43793, total_duration=2875.723289, train/accuracy=0.988817, train/loss=0.037817, train/mean_average_precision=0.243407, validation/accuracy=0.985934, validation/loss=0.047267, validation/mean_average_precision=0.204180, validation/num_examples=43793
I0206 05:03:05.605783 139857137497856 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.019048016518354416, loss=0.033176012337207794
I0206 05:03:37.913475 139758996317952 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.012994558550417423, loss=0.03660070151090622
I0206 05:04:10.347271 139857137497856 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.011765210889279842, loss=0.03595146909356117
I0206 05:04:42.168866 139758996317952 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.013851393945515156, loss=0.03834383562207222
I0206 05:05:14.186264 139857137497856 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.016825499013066292, loss=0.03746875002980232
I0206 05:05:46.177722 139758996317952 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.012658670544624329, loss=0.03464991971850395
I0206 05:06:18.231189 139857137497856 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.012042669579386711, loss=0.03863608464598656
I0206 05:06:39.074797 139919816816448 spec.py:321] Evaluating on the training split.
I0206 05:08:16.699905 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 05:08:19.726904 139919816816448 spec.py:349] Evaluating on the test split.
I0206 05:08:22.706955 139919816816448 submission_runner.py:408] Time since start: 3219.50s, 	Step: 6766, 	{'train/accuracy': 0.9892390966415405, 'train/loss': 0.03620724380016327, 'train/mean_average_precision': 0.271036118425679, 'validation/accuracy': 0.9861992001533508, 'validation/loss': 0.046460848301649094, 'validation/mean_average_precision': 0.2200321466350437, 'validation/num_examples': 43793, 'test/accuracy': 0.9852948188781738, 'test/loss': 0.04915662109851837, 'test/mean_average_precision': 0.22953859957963182, 'test/num_examples': 43793, 'score': 2173.9619414806366, 'total_duration': 3219.5011546611786, 'accumulated_submission_time': 2173.9619414806366, 'accumulated_eval_time': 1044.8342657089233, 'accumulated_logging_time': 0.5117506980895996}
I0206 05:08:22.723908 139697948911360 logging_writer.py:48] [6766] accumulated_eval_time=1044.834266, accumulated_logging_time=0.511751, accumulated_submission_time=2173.961941, global_step=6766, preemption_count=0, score=2173.961941, test/accuracy=0.985295, test/loss=0.049157, test/mean_average_precision=0.229539, test/num_examples=43793, total_duration=3219.501155, train/accuracy=0.989239, train/loss=0.036207, train/mean_average_precision=0.271036, validation/accuracy=0.986199, validation/loss=0.046461, validation/mean_average_precision=0.220032, validation/num_examples=43793
I0206 05:08:34.092782 139716992366336 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.013629361987113953, loss=0.03545868769288063
I0206 05:09:05.984550 139697948911360 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.015531112439930439, loss=0.03661090135574341
I0206 05:09:37.979199 139716992366336 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.01520902942866087, loss=0.03556431457400322
I0206 05:10:09.724164 139697948911360 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.0125166280195117, loss=0.041724011301994324
I0206 05:10:41.684269 139716992366336 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.014380809850990772, loss=0.03472832590341568
I0206 05:11:13.658286 139697948911360 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.013800423592329025, loss=0.03533316031098366
I0206 05:11:45.644557 139716992366336 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.026956574991345406, loss=0.0393272265791893
I0206 05:12:17.998527 139697948911360 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.013858918100595474, loss=0.037502121180295944
I0206 05:12:22.761824 139919816816448 spec.py:321] Evaluating on the training split.
I0206 05:14:00.595378 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 05:14:03.862974 139919816816448 spec.py:349] Evaluating on the test split.
I0206 05:14:06.891040 139919816816448 submission_runner.py:408] Time since start: 3563.69s, 	Step: 7516, 	{'train/accuracy': 0.9895783066749573, 'train/loss': 0.034987322986125946, 'train/mean_average_precision': 0.2932618939418534, 'validation/accuracy': 0.9863753914833069, 'validation/loss': 0.04642264544963837, 'validation/mean_average_precision': 0.22873084220148676, 'validation/num_examples': 43793, 'test/accuracy': 0.9854813814163208, 'test/loss': 0.04922619089484215, 'test/mean_average_precision': 0.23487831542870644, 'test/num_examples': 43793, 'score': 2413.9666571617126, 'total_duration': 3563.6852464675903, 'accumulated_submission_time': 2413.9666571617126, 'accumulated_eval_time': 1148.9634318351746, 'accumulated_logging_time': 0.5409078598022461}
I0206 05:14:06.908336 139735302588160 logging_writer.py:48] [7516] accumulated_eval_time=1148.963432, accumulated_logging_time=0.540908, accumulated_submission_time=2413.966657, global_step=7516, preemption_count=0, score=2413.966657, test/accuracy=0.985481, test/loss=0.049226, test/mean_average_precision=0.234878, test/num_examples=43793, total_duration=3563.685246, train/accuracy=0.989578, train/loss=0.034987, train/mean_average_precision=0.293262, validation/accuracy=0.986375, validation/loss=0.046423, validation/mean_average_precision=0.228731, validation/num_examples=43793
I0206 05:14:34.186740 139758996317952 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.01567872054874897, loss=0.03788791224360466
I0206 05:15:06.309194 139735302588160 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.017944365739822388, loss=0.03443596512079239
I0206 05:15:38.200844 139758996317952 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.030195236206054688, loss=0.0366271436214447
I0206 05:16:10.425700 139735302588160 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.023300083354115486, loss=0.034793853759765625
I0206 05:16:42.258881 139758996317952 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.017952561378479004, loss=0.03852120414376259
I0206 05:17:13.972848 139735302588160 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.022967981174588203, loss=0.03642484173178673
I0206 05:17:46.160266 139758996317952 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.014958369545638561, loss=0.03235873207449913
I0206 05:18:06.928746 139919816816448 spec.py:321] Evaluating on the training split.
I0206 05:19:38.636845 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 05:19:41.786651 139919816816448 spec.py:349] Evaluating on the test split.
I0206 05:19:44.855680 139919816816448 submission_runner.py:408] Time since start: 3901.65s, 	Step: 8266, 	{'train/accuracy': 0.9897823333740234, 'train/loss': 0.03420835733413696, 'train/mean_average_precision': 0.31957378834514716, 'validation/accuracy': 0.9863818883895874, 'validation/loss': 0.04541264474391937, 'validation/mean_average_precision': 0.23256084924965317, 'validation/num_examples': 43793, 'test/accuracy': 0.9855201244354248, 'test/loss': 0.048025909811258316, 'test/mean_average_precision': 0.23596214485473122, 'test/num_examples': 43793, 'score': 2653.955867290497, 'total_duration': 3901.649888753891, 'accumulated_submission_time': 2653.955867290497, 'accumulated_eval_time': 1246.890321969986, 'accumulated_logging_time': 0.5691602230072021}
I0206 05:19:44.872777 139716992366336 logging_writer.py:48] [8266] accumulated_eval_time=1246.890322, accumulated_logging_time=0.569160, accumulated_submission_time=2653.955867, global_step=8266, preemption_count=0, score=2653.955867, test/accuracy=0.985520, test/loss=0.048026, test/mean_average_precision=0.235962, test/num_examples=43793, total_duration=3901.649889, train/accuracy=0.989782, train/loss=0.034208, train/mean_average_precision=0.319574, validation/accuracy=0.986382, validation/loss=0.045413, validation/mean_average_precision=0.232561, validation/num_examples=43793
I0206 05:19:56.188716 139857137497856 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.016832109540700912, loss=0.035192087292671204
I0206 05:20:28.445728 139716992366336 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.023183677345514297, loss=0.036687761545181274
I0206 05:21:00.789776 139857137497856 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.02164405584335327, loss=0.03296835720539093
I0206 05:21:33.190434 139716992366336 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.031275179237127304, loss=0.03538022190332413
I0206 05:22:04.846784 139857137497856 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.015572781674563885, loss=0.034110598266124725
I0206 05:22:36.575337 139716992366336 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.01645955815911293, loss=0.036953434348106384
I0206 05:23:08.796314 139857137497856 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.016988176852464676, loss=0.036755967885255814
I0206 05:23:40.502726 139716992366336 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.018180804327130318, loss=0.03916751220822334
I0206 05:23:44.971921 139919816816448 spec.py:321] Evaluating on the training split.
I0206 05:25:24.741462 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 05:25:28.050325 139919816816448 spec.py:349] Evaluating on the test split.
I0206 05:25:31.258338 139919816816448 submission_runner.py:408] Time since start: 4248.05s, 	Step: 9015, 	{'train/accuracy': 0.990003228187561, 'train/loss': 0.03346382826566696, 'train/mean_average_precision': 0.3346555605107079, 'validation/accuracy': 0.9865231513977051, 'validation/loss': 0.04527439922094345, 'validation/mean_average_precision': 0.2372158837428065, 'validation/num_examples': 43793, 'test/accuracy': 0.9856511354446411, 'test/loss': 0.04779340699315071, 'test/mean_average_precision': 0.24273888082615314, 'test/num_examples': 43793, 'score': 2894.023741006851, 'total_duration': 4248.052522659302, 'accumulated_submission_time': 2894.023741006851, 'accumulated_eval_time': 1353.1766684055328, 'accumulated_logging_time': 0.5972564220428467}
I0206 05:25:31.277290 139697948911360 logging_writer.py:48] [9015] accumulated_eval_time=1353.176668, accumulated_logging_time=0.597256, accumulated_submission_time=2894.023741, global_step=9015, preemption_count=0, score=2894.023741, test/accuracy=0.985651, test/loss=0.047793, test/mean_average_precision=0.242739, test/num_examples=43793, total_duration=4248.052523, train/accuracy=0.990003, train/loss=0.033464, train/mean_average_precision=0.334656, validation/accuracy=0.986523, validation/loss=0.045274, validation/mean_average_precision=0.237216, validation/num_examples=43793
I0206 05:25:59.099502 139758996317952 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.01843315176665783, loss=0.03613724932074547
I0206 05:26:31.518941 139697948911360 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.016138410195708275, loss=0.035228390246629715
I0206 05:27:03.874628 139758996317952 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.025739997625350952, loss=0.03636864945292473
I0206 05:27:35.969162 139697948911360 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.021786516532301903, loss=0.03563768416643143
I0206 05:28:08.042256 139758996317952 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.022256530821323395, loss=0.032713644206523895
I0206 05:28:39.649953 139697948911360 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.024506721645593643, loss=0.0289154052734375
I0206 05:29:11.911620 139758996317952 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.024950135499238968, loss=0.035868290811777115
I0206 05:29:31.482280 139919816816448 spec.py:321] Evaluating on the training split.
I0206 05:31:12.212653 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 05:31:15.532091 139919816816448 spec.py:349] Evaluating on the test split.
I0206 05:31:18.784346 139919816816448 submission_runner.py:408] Time since start: 4595.58s, 	Step: 9761, 	{'train/accuracy': 0.9901729226112366, 'train/loss': 0.03294522315263748, 'train/mean_average_precision': 0.3521926542739725, 'validation/accuracy': 0.9865406155586243, 'validation/loss': 0.044867224991321564, 'validation/mean_average_precision': 0.2515968842157506, 'validation/num_examples': 43793, 'test/accuracy': 0.9857004284858704, 'test/loss': 0.04741131514310837, 'test/mean_average_precision': 0.24142989780850443, 'test/num_examples': 43793, 'score': 3134.1962015628815, 'total_duration': 4595.578535079956, 'accumulated_submission_time': 3134.1962015628815, 'accumulated_eval_time': 1460.478672027588, 'accumulated_logging_time': 0.6274769306182861}
I0206 05:31:18.803554 139716992366336 logging_writer.py:48] [9761] accumulated_eval_time=1460.478672, accumulated_logging_time=0.627477, accumulated_submission_time=3134.196202, global_step=9761, preemption_count=0, score=3134.196202, test/accuracy=0.985700, test/loss=0.047411, test/mean_average_precision=0.241430, test/num_examples=43793, total_duration=4595.578535, train/accuracy=0.990173, train/loss=0.032945, train/mean_average_precision=0.352193, validation/accuracy=0.986541, validation/loss=0.044867, validation/mean_average_precision=0.251597, validation/num_examples=43793
I0206 05:31:31.694996 139857137497856 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.017162496224045753, loss=0.036801937967538834
I0206 05:32:04.194718 139716992366336 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.02706730365753174, loss=0.03945557400584221
I0206 05:32:36.375295 139857137497856 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.04072703793644905, loss=0.031824991106987
I0206 05:33:08.524402 139716992366336 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.02947072684764862, loss=0.03471670299768448
I0206 05:33:42.110536 139857137497856 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.0379033125936985, loss=0.03434843569993973
I0206 05:34:14.165116 139716992366336 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.02397829480469227, loss=0.037948135286569595
I0206 05:34:45.943941 139857137497856 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.03366643190383911, loss=0.03607752546668053
I0206 05:35:17.643139 139716992366336 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.021951381117105484, loss=0.03742346540093422
I0206 05:35:18.960904 139919816816448 spec.py:321] Evaluating on the training split.
I0206 05:36:59.463939 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 05:37:02.636026 139919816816448 spec.py:349] Evaluating on the test split.
I0206 05:37:05.680972 139919816816448 submission_runner.py:408] Time since start: 4942.48s, 	Step: 10505, 	{'train/accuracy': 0.9900105595588684, 'train/loss': 0.03330980986356735, 'train/mean_average_precision': 0.34147219152732555, 'validation/accuracy': 0.9866453409194946, 'validation/loss': 0.04445862025022507, 'validation/mean_average_precision': 0.2510299167733832, 'validation/num_examples': 43793, 'test/accuracy': 0.9858183264732361, 'test/loss': 0.0471859909594059, 'test/mean_average_precision': 0.2474191243238686, 'test/num_examples': 43793, 'score': 3374.321034193039, 'total_duration': 4942.475167989731, 'accumulated_submission_time': 3374.321034193039, 'accumulated_eval_time': 1567.1986813545227, 'accumulated_logging_time': 0.6583380699157715}
I0206 05:37:05.697968 139697948911360 logging_writer.py:48] [10505] accumulated_eval_time=1567.198681, accumulated_logging_time=0.658338, accumulated_submission_time=3374.321034, global_step=10505, preemption_count=0, score=3374.321034, test/accuracy=0.985818, test/loss=0.047186, test/mean_average_precision=0.247419, test/num_examples=43793, total_duration=4942.475168, train/accuracy=0.990011, train/loss=0.033310, train/mean_average_precision=0.341472, validation/accuracy=0.986645, validation/loss=0.044459, validation/mean_average_precision=0.251030, validation/num_examples=43793
I0206 05:37:36.217282 139758996317952 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.0201508030295372, loss=0.03238388150930405
I0206 05:38:08.298523 139697948911360 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.029678188264369965, loss=0.03810994699597359
I0206 05:38:40.021253 139758996317952 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.02134711481630802, loss=0.03173638880252838
I0206 05:39:12.209298 139697948911360 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.022324364632368088, loss=0.03533046320080757
I0206 05:39:43.805921 139758996317952 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.023543087765574455, loss=0.037410203367471695
I0206 05:40:15.971613 139697948911360 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.01961182989180088, loss=0.03398788720369339
I0206 05:40:47.634801 139758996317952 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.024565517902374268, loss=0.03460633382201195
I0206 05:41:05.800014 139919816816448 spec.py:321] Evaluating on the training split.
I0206 05:42:45.380130 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 05:42:48.778237 139919816816448 spec.py:349] Evaluating on the test split.
I0206 05:42:52.060861 139919816816448 submission_runner.py:408] Time since start: 5288.86s, 	Step: 11258, 	{'train/accuracy': 0.9902216196060181, 'train/loss': 0.032521139830350876, 'train/mean_average_precision': 0.3451225530368046, 'validation/accuracy': 0.986581563949585, 'validation/loss': 0.0445379801094532, 'validation/mean_average_precision': 0.25830052866323944, 'validation/num_examples': 43793, 'test/accuracy': 0.9857791662216187, 'test/loss': 0.04718675836920738, 'test/mean_average_precision': 0.2567930607884815, 'test/num_examples': 43793, 'score': 3614.390597343445, 'total_duration': 5288.855046987534, 'accumulated_submission_time': 3614.390597343445, 'accumulated_eval_time': 1673.4594593048096, 'accumulated_logging_time': 0.6867432594299316}
I0206 05:42:52.081527 139716992366336 logging_writer.py:48] [11258] accumulated_eval_time=1673.459459, accumulated_logging_time=0.686743, accumulated_submission_time=3614.390597, global_step=11258, preemption_count=0, score=3614.390597, test/accuracy=0.985779, test/loss=0.047187, test/mean_average_precision=0.256793, test/num_examples=43793, total_duration=5288.855047, train/accuracy=0.990222, train/loss=0.032521, train/mean_average_precision=0.345123, validation/accuracy=0.986582, validation/loss=0.044538, validation/mean_average_precision=0.258301, validation/num_examples=43793
I0206 05:43:06.115449 139735302588160 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.026919513940811157, loss=0.03347645699977875
I0206 05:43:38.057657 139716992366336 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.020846126601099968, loss=0.03079882264137268
I0206 05:44:10.248006 139735302588160 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.02191554754972458, loss=0.033669184893369675
I0206 05:44:42.246075 139716992366336 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.02595130354166031, loss=0.03407151252031326
I0206 05:45:14.077371 139735302588160 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.033356133848428726, loss=0.03434978425502777
I0206 05:45:46.204056 139716992366336 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.02377326600253582, loss=0.03128015249967575
I0206 05:46:18.482129 139735302588160 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.02352888137102127, loss=0.03557225689291954
I0206 05:46:50.230724 139716992366336 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.030521415174007416, loss=0.03505432605743408
I0206 05:46:52.169713 139919816816448 spec.py:321] Evaluating on the training split.
I0206 05:48:31.617370 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 05:48:34.640593 139919816816448 spec.py:349] Evaluating on the test split.
I0206 05:48:37.638721 139919816816448 submission_runner.py:408] Time since start: 5634.43s, 	Step: 12007, 	{'train/accuracy': 0.9903555512428284, 'train/loss': 0.032215844839811325, 'train/mean_average_precision': 0.35489287274553577, 'validation/accuracy': 0.986707866191864, 'validation/loss': 0.044431596994400024, 'validation/mean_average_precision': 0.2554358455220814, 'validation/num_examples': 43793, 'test/accuracy': 0.9858141541481018, 'test/loss': 0.04727277159690857, 'test/mean_average_precision': 0.2503482735494056, 'test/num_examples': 43793, 'score': 3854.4456446170807, 'total_duration': 5634.432926416397, 'accumulated_submission_time': 3854.4456446170807, 'accumulated_eval_time': 1778.9284162521362, 'accumulated_logging_time': 0.7189178466796875}
I0206 05:48:37.657122 139758996317952 logging_writer.py:48] [12007] accumulated_eval_time=1778.928416, accumulated_logging_time=0.718918, accumulated_submission_time=3854.445645, global_step=12007, preemption_count=0, score=3854.445645, test/accuracy=0.985814, test/loss=0.047273, test/mean_average_precision=0.250348, test/num_examples=43793, total_duration=5634.432926, train/accuracy=0.990356, train/loss=0.032216, train/mean_average_precision=0.354893, validation/accuracy=0.986708, validation/loss=0.044432, validation/mean_average_precision=0.255436, validation/num_examples=43793
I0206 05:49:07.099796 139857137497856 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.025720123201608658, loss=0.03583207726478577
I0206 05:49:38.265922 139758996317952 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.03361814096570015, loss=0.034126996994018555
I0206 05:50:09.750211 139857137497856 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.02312028594315052, loss=0.03056180663406849
I0206 05:50:40.980728 139758996317952 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.024091610684990883, loss=0.02887396700680256
I0206 05:51:12.606116 139857137497856 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.024807892739772797, loss=0.03198360279202461
I0206 05:51:44.080366 139758996317952 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.030487680807709694, loss=0.036163125187158585
I0206 05:52:15.974091 139857137497856 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.02930287830531597, loss=0.0326731838285923
I0206 05:52:37.644283 139919816816448 spec.py:321] Evaluating on the training split.
I0206 05:54:10.648107 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 05:54:13.663873 139919816816448 spec.py:349] Evaluating on the test split.
I0206 05:54:16.681346 139919816816448 submission_runner.py:408] Time since start: 5973.48s, 	Step: 12768, 	{'train/accuracy': 0.9902983903884888, 'train/loss': 0.03216234967112541, 'train/mean_average_precision': 0.3728056786919886, 'validation/accuracy': 0.9867314100265503, 'validation/loss': 0.044263798743486404, 'validation/mean_average_precision': 0.26053046081291353, 'validation/num_examples': 43793, 'test/accuracy': 0.985888659954071, 'test/loss': 0.04718273878097534, 'test/mean_average_precision': 0.25743486912238867, 'test/num_examples': 43793, 'score': 4094.4016149044037, 'total_duration': 5973.4755423069, 'accumulated_submission_time': 4094.4016149044037, 'accumulated_eval_time': 1877.9654395580292, 'accumulated_logging_time': 0.7478671073913574}
I0206 05:54:16.700182 139697948911360 logging_writer.py:48] [12768] accumulated_eval_time=1877.965440, accumulated_logging_time=0.747867, accumulated_submission_time=4094.401615, global_step=12768, preemption_count=0, score=4094.401615, test/accuracy=0.985889, test/loss=0.047183, test/mean_average_precision=0.257435, test/num_examples=43793, total_duration=5973.475542, train/accuracy=0.990298, train/loss=0.032162, train/mean_average_precision=0.372806, validation/accuracy=0.986731, validation/loss=0.044264, validation/mean_average_precision=0.260530, validation/num_examples=43793
I0206 05:54:27.313874 139735302588160 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.03599892929196358, loss=0.03502039611339569
I0206 05:54:58.554209 139697948911360 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.025444094091653824, loss=0.03098677098751068
I0206 05:55:30.221174 139735302588160 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.02828395925462246, loss=0.03191126510500908
I0206 05:56:01.889478 139697948911360 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.031481653451919556, loss=0.03697779029607773
I0206 05:56:33.639218 139735302588160 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.022647935897111893, loss=0.031708355993032455
I0206 05:57:05.300088 139697948911360 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.02999049611389637, loss=0.030247775837779045
I0206 05:57:37.209804 139735302588160 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.03973263502120972, loss=0.03853804990649223
I0206 05:58:08.877888 139697948911360 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.02561168745160103, loss=0.027078351005911827
I0206 05:58:16.812572 139919816816448 spec.py:321] Evaluating on the training split.
I0206 05:59:51.045069 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 05:59:54.133440 139919816816448 spec.py:349] Evaluating on the test split.
I0206 05:59:57.125645 139919816816448 submission_runner.py:408] Time since start: 6313.92s, 	Step: 13526, 	{'train/accuracy': 0.9905072450637817, 'train/loss': 0.031183667480945587, 'train/mean_average_precision': 0.3852428672374816, 'validation/accuracy': 0.986772358417511, 'validation/loss': 0.044338926672935486, 'validation/mean_average_precision': 0.2648099308155049, 'validation/num_examples': 43793, 'test/accuracy': 0.9858831763267517, 'test/loss': 0.04731597751379013, 'test/mean_average_precision': 0.2539916084768826, 'test/num_examples': 43793, 'score': 4334.481739997864, 'total_duration': 6313.919853925705, 'accumulated_submission_time': 4334.481739997864, 'accumulated_eval_time': 1978.2784700393677, 'accumulated_logging_time': 0.7787857055664062}
I0206 05:59:57.143555 139758996317952 logging_writer.py:48] [13526] accumulated_eval_time=1978.278470, accumulated_logging_time=0.778786, accumulated_submission_time=4334.481740, global_step=13526, preemption_count=0, score=4334.481740, test/accuracy=0.985883, test/loss=0.047316, test/mean_average_precision=0.253992, test/num_examples=43793, total_duration=6313.919854, train/accuracy=0.990507, train/loss=0.031184, train/mean_average_precision=0.385243, validation/accuracy=0.986772, validation/loss=0.044339, validation/mean_average_precision=0.264810, validation/num_examples=43793
I0206 06:00:21.326204 139857137497856 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.04065854102373123, loss=0.03232947364449501
I0206 06:00:52.976743 139758996317952 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.04026397317647934, loss=0.035553332418203354
I0206 06:01:24.688923 139857137497856 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.0343586690723896, loss=0.03554738312959671
I0206 06:01:56.648961 139758996317952 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.03545283153653145, loss=0.0329209566116333
I0206 06:02:28.569533 139857137497856 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.03553704917430878, loss=0.03261278569698334
I0206 06:03:00.178782 139758996317952 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.03229202330112457, loss=0.03433002158999443
I0206 06:03:31.871548 139857137497856 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.05067356303334236, loss=0.03180043399333954
I0206 06:03:57.398095 139919816816448 spec.py:321] Evaluating on the training split.
I0206 06:05:34.844668 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 06:05:38.331329 139919816816448 spec.py:349] Evaluating on the test split.
I0206 06:05:41.695748 139919816816448 submission_runner.py:408] Time since start: 6658.49s, 	Step: 14281, 	{'train/accuracy': 0.9905017018318176, 'train/loss': 0.031227940693497658, 'train/mean_average_precision': 0.3851849660303862, 'validation/accuracy': 0.9866887331008911, 'validation/loss': 0.0443001464009285, 'validation/mean_average_precision': 0.25957978612968485, 'validation/num_examples': 43793, 'test/accuracy': 0.9858419299125671, 'test/loss': 0.047129031270742416, 'test/mean_average_precision': 0.25754915633653663, 'test/num_examples': 43793, 'score': 4574.704688310623, 'total_duration': 6658.48993730545, 'accumulated_submission_time': 4574.704688310623, 'accumulated_eval_time': 2082.5760612487793, 'accumulated_logging_time': 0.807380199432373}
I0206 06:05:41.716289 139697948911360 logging_writer.py:48] [14281] accumulated_eval_time=2082.576061, accumulated_logging_time=0.807380, accumulated_submission_time=4574.704688, global_step=14281, preemption_count=0, score=4574.704688, test/accuracy=0.985842, test/loss=0.047129, test/mean_average_precision=0.257549, test/num_examples=43793, total_duration=6658.489937, train/accuracy=0.990502, train/loss=0.031228, train/mean_average_precision=0.385185, validation/accuracy=0.986689, validation/loss=0.044300, validation/mean_average_precision=0.259580, validation/num_examples=43793
I0206 06:05:48.821686 139716992366336 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.02963246963918209, loss=0.03223172947764397
I0206 06:06:21.971948 139697948911360 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.03598155081272125, loss=0.03436519578099251
I0206 06:06:54.344094 139716992366336 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.040094707161188126, loss=0.029248187318444252
I0206 06:07:26.361537 139697948911360 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.031481750309467316, loss=0.032387085258960724
I0206 06:07:57.923756 139716992366336 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.03750975802540779, loss=0.03094182163476944
I0206 06:08:29.763718 139697948911360 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.04176562279462814, loss=0.03366054221987724
I0206 06:09:00.940207 139716992366336 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.03192654997110367, loss=0.03666694462299347
I0206 06:09:32.620964 139697948911360 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.03461385518312454, loss=0.032347459346055984
I0206 06:09:41.976862 139919816816448 spec.py:321] Evaluating on the training split.
I0206 06:11:15.736874 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 06:11:18.802921 139919816816448 spec.py:349] Evaluating on the test split.
I0206 06:11:21.867625 139919816816448 submission_runner.py:408] Time since start: 6998.66s, 	Step: 15031, 	{'train/accuracy': 0.9906373023986816, 'train/loss': 0.03075084462761879, 'train/mean_average_precision': 0.4132004282045805, 'validation/accuracy': 0.9867650866508484, 'validation/loss': 0.04462047293782234, 'validation/mean_average_precision': 0.26286550689563964, 'validation/num_examples': 43793, 'test/accuracy': 0.9859139323234558, 'test/loss': 0.047488342970609665, 'test/mean_average_precision': 0.2540615756671235, 'test/num_examples': 43793, 'score': 4814.931003808975, 'total_duration': 6998.6618309021, 'accumulated_submission_time': 4814.931003808975, 'accumulated_eval_time': 2182.466774225235, 'accumulated_logging_time': 0.8405594825744629}
I0206 06:11:21.886577 139735302588160 logging_writer.py:48] [15031] accumulated_eval_time=2182.466774, accumulated_logging_time=0.840559, accumulated_submission_time=4814.931004, global_step=15031, preemption_count=0, score=4814.931004, test/accuracy=0.985914, test/loss=0.047488, test/mean_average_precision=0.254062, test/num_examples=43793, total_duration=6998.661831, train/accuracy=0.990637, train/loss=0.030751, train/mean_average_precision=0.413200, validation/accuracy=0.986765, validation/loss=0.044620, validation/mean_average_precision=0.262866, validation/num_examples=43793
I0206 06:11:43.902508 139857137497856 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.03362664580345154, loss=0.033648502081632614
I0206 06:12:15.218257 139735302588160 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.029966697096824646, loss=0.03233801946043968
I0206 06:12:46.163557 139857137497856 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.04172743111848831, loss=0.0343659371137619
I0206 06:13:17.601154 139735302588160 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.04325427860021591, loss=0.03535104915499687
I0206 06:13:48.871364 139857137497856 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.0335589237511158, loss=0.030989309772849083
I0206 06:14:20.351295 139735302588160 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.04537243768572807, loss=0.03649520128965378
I0206 06:14:52.098229 139857137497856 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.03849898278713226, loss=0.03208601102232933
I0206 06:15:21.895796 139919816816448 spec.py:321] Evaluating on the training split.
I0206 06:16:59.394044 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 06:17:02.489582 139919816816448 spec.py:349] Evaluating on the test split.
I0206 06:17:05.504095 139919816816448 submission_runner.py:408] Time since start: 7342.30s, 	Step: 15795, 	{'train/accuracy': 0.990784227848053, 'train/loss': 0.030104296281933784, 'train/mean_average_precision': 0.40460155244264295, 'validation/accuracy': 0.9867382645606995, 'validation/loss': 0.04417121782898903, 'validation/mean_average_precision': 0.2618256099036918, 'validation/num_examples': 43793, 'test/accuracy': 0.9858680367469788, 'test/loss': 0.04678452014923096, 'test/mean_average_precision': 0.2557980212280484, 'test/num_examples': 43793, 'score': 5054.909048080444, 'total_duration': 7342.298298835754, 'accumulated_submission_time': 5054.909048080444, 'accumulated_eval_time': 2286.0750284194946, 'accumulated_logging_time': 0.8707151412963867}
I0206 06:17:05.522104 139697948911360 logging_writer.py:48] [15795] accumulated_eval_time=2286.075028, accumulated_logging_time=0.870715, accumulated_submission_time=5054.909048, global_step=15795, preemption_count=0, score=5054.909048, test/accuracy=0.985868, test/loss=0.046785, test/mean_average_precision=0.255798, test/num_examples=43793, total_duration=7342.298299, train/accuracy=0.990784, train/loss=0.030104, train/mean_average_precision=0.404602, validation/accuracy=0.986738, validation/loss=0.044171, validation/mean_average_precision=0.261826, validation/num_examples=43793
I0206 06:17:07.419304 139758996317952 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.028416845947504044, loss=0.03022766299545765
I0206 06:17:39.280973 139697948911360 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.048907212913036346, loss=0.03300290182232857
I0206 06:18:11.245289 139758996317952 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.0366283617913723, loss=0.032546043395996094
I0206 06:18:42.528076 139697948911360 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.03446107730269432, loss=0.03197276219725609
I0206 06:19:13.960460 139758996317952 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.033100955188274384, loss=0.03136138617992401
I0206 06:19:45.047400 139697948911360 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.030747801065444946, loss=0.02694239467382431
I0206 06:20:16.366872 139758996317952 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.034841541200876236, loss=0.03191589191555977
I0206 06:20:47.745772 139697948911360 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.0314469039440155, loss=0.029205039143562317
I0206 06:21:05.536489 139919816816448 spec.py:321] Evaluating on the training split.
I0206 06:22:42.697928 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 06:22:46.119258 139919816816448 spec.py:349] Evaluating on the test split.
I0206 06:22:49.537577 139919816816448 submission_runner.py:408] Time since start: 7686.33s, 	Step: 16557, 	{'train/accuracy': 0.9911503791809082, 'train/loss': 0.029118113219738007, 'train/mean_average_precision': 0.44197708028239857, 'validation/accuracy': 0.986614465713501, 'validation/loss': 0.044292621314525604, 'validation/mean_average_precision': 0.26905837310345176, 'validation/num_examples': 43793, 'test/accuracy': 0.9858490824699402, 'test/loss': 0.04684639722108841, 'test/mean_average_precision': 0.26177670806063214, 'test/num_examples': 43793, 'score': 5294.891679048538, 'total_duration': 7686.331763267517, 'accumulated_submission_time': 5294.891679048538, 'accumulated_eval_time': 2390.0760481357574, 'accumulated_logging_time': 0.8997907638549805}
I0206 06:22:49.558244 139716992366336 logging_writer.py:48] [16557] accumulated_eval_time=2390.076048, accumulated_logging_time=0.899791, accumulated_submission_time=5294.891679, global_step=16557, preemption_count=0, score=5294.891679, test/accuracy=0.985849, test/loss=0.046846, test/mean_average_precision=0.261777, test/num_examples=43793, total_duration=7686.331763, train/accuracy=0.991150, train/loss=0.029118, train/mean_average_precision=0.441977, validation/accuracy=0.986614, validation/loss=0.044293, validation/mean_average_precision=0.269058, validation/num_examples=43793
I0206 06:23:03.964971 139857137497856 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.04893931746482849, loss=0.03284745663404465
I0206 06:23:36.587900 139716992366336 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.044274892657995224, loss=0.034475456923246384
I0206 06:24:09.136075 139857137497856 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.045081328600645065, loss=0.03401222079992294
I0206 06:24:41.033449 139716992366336 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.039463043212890625, loss=0.03389720246195793
I0206 06:25:12.839881 139857137497856 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.04069606587290764, loss=0.033507976680994034
I0206 06:25:44.591774 139716992366336 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.04026772081851959, loss=0.03254040703177452
I0206 06:26:16.608303 139857137497856 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.03375204652547836, loss=0.03370817005634308
I0206 06:26:48.399190 139716992366336 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.03849174082279205, loss=0.02884441614151001
I0206 06:26:49.666122 139919816816448 spec.py:321] Evaluating on the training split.
I0206 06:28:24.193637 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 06:28:27.285886 139919816816448 spec.py:349] Evaluating on the test split.
I0206 06:28:30.275271 139919816816448 submission_runner.py:408] Time since start: 8027.07s, 	Step: 17305, 	{'train/accuracy': 0.9909698367118835, 'train/loss': 0.029629463329911232, 'train/mean_average_precision': 0.425239852981252, 'validation/accuracy': 0.9866148829460144, 'validation/loss': 0.04416673630475998, 'validation/mean_average_precision': 0.2680382857825597, 'validation/num_examples': 43793, 'test/accuracy': 0.9858777523040771, 'test/loss': 0.04673495516180992, 'test/mean_average_precision': 0.2607631513044034, 'test/num_examples': 43793, 'score': 5534.9659996032715, 'total_duration': 8027.069473028183, 'accumulated_submission_time': 5534.9659996032715, 'accumulated_eval_time': 2490.6851439476013, 'accumulated_logging_time': 0.9323804378509521}
I0206 06:28:30.293811 139697948911360 logging_writer.py:48] [17305] accumulated_eval_time=2490.685144, accumulated_logging_time=0.932380, accumulated_submission_time=5534.966000, global_step=17305, preemption_count=0, score=5534.966000, test/accuracy=0.985878, test/loss=0.046735, test/mean_average_precision=0.260763, test/num_examples=43793, total_duration=8027.069473, train/accuracy=0.990970, train/loss=0.029629, train/mean_average_precision=0.425240, validation/accuracy=0.986615, validation/loss=0.044167, validation/mean_average_precision=0.268038, validation/num_examples=43793
I0206 06:29:00.778301 139735302588160 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.03896544128656387, loss=0.03425123915076256
I0206 06:29:32.259212 139697948911360 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.04088349640369415, loss=0.03551984578371048
I0206 06:30:03.747566 139735302588160 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.04284157603979111, loss=0.031568292528390884
I0206 06:30:35.401234 139697948911360 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.04004211351275444, loss=0.03262762725353241
I0206 06:31:06.702169 139735302588160 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.04195862263441086, loss=0.03481357917189598
I0206 06:31:38.311371 139697948911360 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.04279147461056709, loss=0.034303560853004456
I0206 06:32:09.782553 139735302588160 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.055153414607048035, loss=0.03371705114841461
I0206 06:32:30.370607 139919816816448 spec.py:321] Evaluating on the training split.
I0206 06:34:09.572531 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 06:34:12.598865 139919816816448 spec.py:349] Evaluating on the test split.
I0206 06:34:15.588328 139919816816448 submission_runner.py:408] Time since start: 8372.38s, 	Step: 18067, 	{'train/accuracy': 0.9908579587936401, 'train/loss': 0.03003951907157898, 'train/mean_average_precision': 0.41192477326662935, 'validation/accuracy': 0.9867743849754333, 'validation/loss': 0.04401408135890961, 'validation/mean_average_precision': 0.2692154002615283, 'validation/num_examples': 43793, 'test/accuracy': 0.9858953952789307, 'test/loss': 0.04672873765230179, 'test/mean_average_precision': 0.26471816355767297, 'test/num_examples': 43793, 'score': 5775.012037992477, 'total_duration': 8372.382522583008, 'accumulated_submission_time': 5775.012037992477, 'accumulated_eval_time': 2595.9028055667877, 'accumulated_logging_time': 0.9617249965667725}
I0206 06:34:15.607804 139716992366336 logging_writer.py:48] [18067] accumulated_eval_time=2595.902806, accumulated_logging_time=0.961725, accumulated_submission_time=5775.012038, global_step=18067, preemption_count=0, score=5775.012038, test/accuracy=0.985895, test/loss=0.046729, test/mean_average_precision=0.264718, test/num_examples=43793, total_duration=8372.382523, train/accuracy=0.990858, train/loss=0.030040, train/mean_average_precision=0.411925, validation/accuracy=0.986774, validation/loss=0.044014, validation/mean_average_precision=0.269215, validation/num_examples=43793
I0206 06:34:26.449704 139857137497856 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.05562759190797806, loss=0.03389362618327141
I0206 06:34:57.717728 139716992366336 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.04010496288537979, loss=0.03209440037608147
I0206 06:35:29.280853 139857137497856 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.07381665706634521, loss=0.03313199430704117
I0206 06:36:00.907295 139716992366336 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.0536283403635025, loss=0.032989103347063065
I0206 06:36:32.431827 139857137497856 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.037924837321043015, loss=0.03010588511824608
I0206 06:37:04.343082 139716992366336 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.043675173074007034, loss=0.03315736725926399
I0206 06:37:35.778011 139857137497856 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.04571906104683876, loss=0.029672356322407722
I0206 06:38:07.715533 139716992366336 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.0426601767539978, loss=0.03026062808930874
I0206 06:38:15.608513 139919816816448 spec.py:321] Evaluating on the training split.
I0206 06:39:49.245733 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 06:39:52.265125 139919816816448 spec.py:349] Evaluating on the test split.
I0206 06:39:55.232240 139919816816448 submission_runner.py:408] Time since start: 8712.03s, 	Step: 18826, 	{'train/accuracy': 0.9908071160316467, 'train/loss': 0.03033352829515934, 'train/mean_average_precision': 0.40827320062189715, 'validation/accuracy': 0.9867638349533081, 'validation/loss': 0.0440635085105896, 'validation/mean_average_precision': 0.26480815617896264, 'validation/num_examples': 43793, 'test/accuracy': 0.9859059453010559, 'test/loss': 0.04682458937168121, 'test/mean_average_precision': 0.261026378969448, 'test/num_examples': 43793, 'score': 6014.981848716736, 'total_duration': 8712.026437044144, 'accumulated_submission_time': 6014.981848716736, 'accumulated_eval_time': 2695.526474237442, 'accumulated_logging_time': 0.9920079708099365}
I0206 06:39:55.251689 139697948911360 logging_writer.py:48] [18826] accumulated_eval_time=2695.526474, accumulated_logging_time=0.992008, accumulated_submission_time=6014.981849, global_step=18826, preemption_count=0, score=6014.981849, test/accuracy=0.985906, test/loss=0.046825, test/mean_average_precision=0.261026, test/num_examples=43793, total_duration=8712.026437, train/accuracy=0.990807, train/loss=0.030334, train/mean_average_precision=0.408273, validation/accuracy=0.986764, validation/loss=0.044064, validation/mean_average_precision=0.264808, validation/num_examples=43793
I0206 06:40:18.797431 139758996317952 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.04456859454512596, loss=0.03204170614480972
I0206 06:40:50.284859 139697948911360 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.03962750732898712, loss=0.033038098365068436
I0206 06:41:21.841923 139758996317952 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.03986235335469246, loss=0.031023414805531502
I0206 06:41:53.298223 139697948911360 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.049964968115091324, loss=0.027797769755125046
I0206 06:42:24.920596 139758996317952 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.046686362475156784, loss=0.030623609200119972
I0206 06:42:56.553448 139697948911360 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.04236810654401779, loss=0.030664555728435516
I0206 06:43:28.074291 139758996317952 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.036865562200546265, loss=0.02783769741654396
I0206 06:43:55.445041 139919816816448 spec.py:321] Evaluating on the training split.
I0206 06:45:30.270601 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 06:45:33.449493 139919816816448 spec.py:349] Evaluating on the test split.
I0206 06:45:36.518954 139919816816448 submission_runner.py:408] Time since start: 9053.31s, 	Step: 19588, 	{'train/accuracy': 0.990985095500946, 'train/loss': 0.029373522847890854, 'train/mean_average_precision': 0.42571662301725, 'validation/accuracy': 0.986893355846405, 'validation/loss': 0.044145215302705765, 'validation/mean_average_precision': 0.27269999620141266, 'validation/num_examples': 43793, 'test/accuracy': 0.9860677123069763, 'test/loss': 0.04679865390062332, 'test/mean_average_precision': 0.269728862963543, 'test/num_examples': 43793, 'score': 6255.140200138092, 'total_duration': 9053.31315279007, 'accumulated_submission_time': 6255.140200138092, 'accumulated_eval_time': 2796.600333929062, 'accumulated_logging_time': 1.0257935523986816}
I0206 06:45:36.537656 139735302588160 logging_writer.py:48] [19588] accumulated_eval_time=2796.600334, accumulated_logging_time=1.025794, accumulated_submission_time=6255.140200, global_step=19588, preemption_count=0, score=6255.140200, test/accuracy=0.986068, test/loss=0.046799, test/mean_average_precision=0.269729, test/num_examples=43793, total_duration=9053.313153, train/accuracy=0.990985, train/loss=0.029374, train/mean_average_precision=0.425717, validation/accuracy=0.986893, validation/loss=0.044145, validation/mean_average_precision=0.272700, validation/num_examples=43793
I0206 06:45:40.673101 139857137497856 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.04822215810418129, loss=0.03410128876566887
I0206 06:46:12.251647 139735302588160 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.07591637969017029, loss=0.033323854207992554
I0206 06:46:43.852593 139857137497856 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.0571555532515049, loss=0.03565417230129242
I0206 06:47:15.254635 139735302588160 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.03781568631529808, loss=0.028447994962334633
I0206 06:47:46.827076 139857137497856 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.046002451330423355, loss=0.02990724705159664
I0206 06:48:18.572802 139735302588160 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.04702463746070862, loss=0.02914540097117424
I0206 06:48:50.712802 139857137497856 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.03754542022943497, loss=0.029546987265348434
I0206 06:49:23.272168 139735302588160 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.04123033210635185, loss=0.029799265787005424
I0206 06:49:36.660238 139919816816448 spec.py:321] Evaluating on the training split.
I0206 06:51:13.991800 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 06:51:17.398948 139919816816448 spec.py:349] Evaluating on the test split.
I0206 06:51:20.648741 139919816816448 submission_runner.py:408] Time since start: 9397.44s, 	Step: 20343, 	{'train/accuracy': 0.9908535480499268, 'train/loss': 0.029793960973620415, 'train/mean_average_precision': 0.42994988858239147, 'validation/accuracy': 0.9868730306625366, 'validation/loss': 0.04385649785399437, 'validation/mean_average_precision': 0.27432950631949, 'validation/num_examples': 43793, 'test/accuracy': 0.9860925674438477, 'test/loss': 0.046628862619400024, 'test/mean_average_precision': 0.2667238021460736, 'test/num_examples': 43793, 'score': 6495.229241847992, 'total_duration': 9397.442924976349, 'accumulated_submission_time': 6495.229241847992, 'accumulated_eval_time': 2900.5887792110443, 'accumulated_logging_time': 1.0550963878631592}
I0206 06:51:20.670637 139697948911360 logging_writer.py:48] [20343] accumulated_eval_time=2900.588779, accumulated_logging_time=1.055096, accumulated_submission_time=6495.229242, global_step=20343, preemption_count=0, score=6495.229242, test/accuracy=0.986093, test/loss=0.046629, test/mean_average_precision=0.266724, test/num_examples=43793, total_duration=9397.442925, train/accuracy=0.990854, train/loss=0.029794, train/mean_average_precision=0.429950, validation/accuracy=0.986873, validation/loss=0.043856, validation/mean_average_precision=0.274330, validation/num_examples=43793
I0206 06:51:39.410087 139758996317952 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.0392000675201416, loss=0.02996453456580639
I0206 06:52:11.081863 139697948911360 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.044581878930330276, loss=0.033079613000154495
I0206 06:52:42.305681 139758996317952 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.04957160726189613, loss=0.025950726121664047
I0206 06:53:14.127150 139697948911360 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.05458948388695717, loss=0.03244103863835335
I0206 06:53:46.106850 139758996317952 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.04960448294878006, loss=0.03180099278688431
I0206 06:54:18.549644 139697948911360 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.04011566564440727, loss=0.030120333656668663
I0206 06:54:50.485898 139758996317952 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.06750354915857315, loss=0.03242680802941322
I0206 06:55:20.970964 139919816816448 spec.py:321] Evaluating on the training split.
I0206 06:56:55.428935 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 06:57:00.954582 139919816816448 spec.py:349] Evaluating on the test split.
I0206 06:57:04.125585 139919816816448 submission_runner.py:408] Time since start: 9740.92s, 	Step: 21098, 	{'train/accuracy': 0.9910489320755005, 'train/loss': 0.02918689325451851, 'train/mean_average_precision': 0.4427940285589564, 'validation/accuracy': 0.9868564009666443, 'validation/loss': 0.04386270046234131, 'validation/mean_average_precision': 0.2687042846949644, 'validation/num_examples': 43793, 'test/accuracy': 0.9860213398933411, 'test/loss': 0.04656833037734032, 'test/mean_average_precision': 0.26099991978009857, 'test/num_examples': 43793, 'score': 6735.494882106781, 'total_duration': 9740.919793367386, 'accumulated_submission_time': 6735.494882106781, 'accumulated_eval_time': 3003.7433593273163, 'accumulated_logging_time': 1.089904546737671}
I0206 06:57:04.146231 139679759779584 logging_writer.py:48] [21098] accumulated_eval_time=3003.743359, accumulated_logging_time=1.089905, accumulated_submission_time=6735.494882, global_step=21098, preemption_count=0, score=6735.494882, test/accuracy=0.986021, test/loss=0.046568, test/mean_average_precision=0.261000, test/num_examples=43793, total_duration=9740.919793, train/accuracy=0.991049, train/loss=0.029187, train/mean_average_precision=0.442794, validation/accuracy=0.986856, validation/loss=0.043863, validation/mean_average_precision=0.268704, validation/num_examples=43793
I0206 06:57:05.145478 139735302588160 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.04416542500257492, loss=0.029104314744472504
I0206 06:57:36.773006 139679759779584 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.04848725348711014, loss=0.030657785013318062
I0206 06:58:08.158645 139735302588160 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.05299921706318855, loss=0.03333836421370506
I0206 06:58:39.955501 139679759779584 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.0461631640791893, loss=0.031263817101716995
I0206 06:59:12.079941 139735302588160 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.08054213970899582, loss=0.029409771785140038
I0206 06:59:43.821496 139679759779584 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.04623415321111679, loss=0.03383921831846237
I0206 07:00:15.334995 139735302588160 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.03896830976009369, loss=0.02979286015033722
I0206 07:00:46.896832 139679759779584 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.0542670339345932, loss=0.03113662451505661
I0206 07:01:04.362831 139919816816448 spec.py:321] Evaluating on the training split.
I0206 07:02:41.410362 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 07:02:44.545749 139919816816448 spec.py:349] Evaluating on the test split.
I0206 07:02:47.600090 139919816816448 submission_runner.py:408] Time since start: 10084.39s, 	Step: 21856, 	{'train/accuracy': 0.9909344911575317, 'train/loss': 0.029195064678788185, 'train/mean_average_precision': 0.43935341446327436, 'validation/accuracy': 0.9868978261947632, 'validation/loss': 0.04463415965437889, 'validation/mean_average_precision': 0.27351905659019016, 'validation/num_examples': 43793, 'test/accuracy': 0.9861317276954651, 'test/loss': 0.04742466285824776, 'test/mean_average_precision': 0.2671671664715521, 'test/num_examples': 43793, 'score': 6975.677932262421, 'total_duration': 10084.394299507141, 'accumulated_submission_time': 6975.677932262421, 'accumulated_eval_time': 3106.980573654175, 'accumulated_logging_time': 1.1237335205078125}
I0206 07:02:47.620304 139716992366336 logging_writer.py:48] [21856] accumulated_eval_time=3106.980574, accumulated_logging_time=1.123734, accumulated_submission_time=6975.677932, global_step=21856, preemption_count=0, score=6975.677932, test/accuracy=0.986132, test/loss=0.047425, test/mean_average_precision=0.267167, test/num_examples=43793, total_duration=10084.394300, train/accuracy=0.990934, train/loss=0.029195, train/mean_average_precision=0.439353, validation/accuracy=0.986898, validation/loss=0.044634, validation/mean_average_precision=0.273519, validation/num_examples=43793
I0206 07:03:02.191436 139758996317952 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.04068312793970108, loss=0.03146540746092796
I0206 07:03:33.887720 139716992366336 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.0401306189596653, loss=0.029553232714533806
I0206 07:04:05.847608 139758996317952 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.03852318972349167, loss=0.027025790885090828
I0206 07:04:37.132627 139716992366336 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.050040293484926224, loss=0.03292738273739815
I0206 07:05:08.701809 139758996317952 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.05181668698787689, loss=0.03275423124432564
I0206 07:05:40.881494 139716992366336 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.06277569383382797, loss=0.03204389289021492
I0206 07:06:12.721431 139758996317952 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.05023590475320816, loss=0.028876032680273056
I0206 07:06:44.284125 139716992366336 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.060930196195840836, loss=0.03434057533740997
I0206 07:06:47.696153 139919816816448 spec.py:321] Evaluating on the training split.
I0206 07:08:23.113329 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 07:08:26.123241 139919816816448 spec.py:349] Evaluating on the test split.
I0206 07:08:29.048852 139919816816448 submission_runner.py:408] Time since start: 10425.84s, 	Step: 22612, 	{'train/accuracy': 0.9911161065101624, 'train/loss': 0.028925830498337746, 'train/mean_average_precision': 0.4461390763695887, 'validation/accuracy': 0.9867297410964966, 'validation/loss': 0.04394465684890747, 'validation/mean_average_precision': 0.2784422440583035, 'validation/num_examples': 43793, 'test/accuracy': 0.9859792590141296, 'test/loss': 0.04665583744645119, 'test/mean_average_precision': 0.2688925711781614, 'test/num_examples': 43793, 'score': 7215.721256017685, 'total_duration': 10425.84306025505, 'accumulated_submission_time': 7215.721256017685, 'accumulated_eval_time': 3208.3332257270813, 'accumulated_logging_time': 1.1560900211334229}
I0206 07:08:29.068480 139679759779584 logging_writer.py:48] [22612] accumulated_eval_time=3208.333226, accumulated_logging_time=1.156090, accumulated_submission_time=7215.721256, global_step=22612, preemption_count=0, score=7215.721256, test/accuracy=0.985979, test/loss=0.046656, test/mean_average_precision=0.268893, test/num_examples=43793, total_duration=10425.843060, train/accuracy=0.991116, train/loss=0.028926, train/mean_average_precision=0.446139, validation/accuracy=0.986730, validation/loss=0.043945, validation/mean_average_precision=0.278442, validation/num_examples=43793
I0206 07:08:57.196911 139697948911360 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.04466124251484871, loss=0.030494974926114082
I0206 07:09:28.848344 139679759779584 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.05194902420043945, loss=0.03611275926232338
I0206 07:10:00.816782 139697948911360 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.058846957981586456, loss=0.03017519786953926
I0206 07:10:32.791645 139679759779584 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.0516044944524765, loss=0.03235457092523575
I0206 07:11:04.691311 139697948911360 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.05644002556800842, loss=0.03135417774319649
I0206 07:11:36.435262 139679759779584 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.04772460088133812, loss=0.028013605624437332
I0206 07:12:08.284468 139697948911360 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.048576053231954575, loss=0.030934548005461693
I0206 07:12:29.203827 139919816816448 spec.py:321] Evaluating on the training split.
I0206 07:14:05.055171 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 07:14:08.053832 139919816816448 spec.py:349] Evaluating on the test split.
I0206 07:14:11.028240 139919816816448 submission_runner.py:408] Time since start: 10767.82s, 	Step: 23366, 	{'train/accuracy': 0.9914578199386597, 'train/loss': 0.0278907660394907, 'train/mean_average_precision': 0.46277013743523543, 'validation/accuracy': 0.9868608713150024, 'validation/loss': 0.04406419023871422, 'validation/mean_average_precision': 0.27692518753141804, 'validation/num_examples': 43793, 'test/accuracy': 0.9861093759536743, 'test/loss': 0.046633679419755936, 'test/mean_average_precision': 0.2682321435300443, 'test/num_examples': 43793, 'score': 7455.825122117996, 'total_duration': 10767.822446346283, 'accumulated_submission_time': 7455.825122117996, 'accumulated_eval_time': 3310.157596349716, 'accumulated_logging_time': 1.1869611740112305}
I0206 07:14:11.048897 139735302588160 logging_writer.py:48] [23366] accumulated_eval_time=3310.157596, accumulated_logging_time=1.186961, accumulated_submission_time=7455.825122, global_step=23366, preemption_count=0, score=7455.825122, test/accuracy=0.986109, test/loss=0.046634, test/mean_average_precision=0.268232, test/num_examples=43793, total_duration=10767.822446, train/accuracy=0.991458, train/loss=0.027891, train/mean_average_precision=0.462770, validation/accuracy=0.986861, validation/loss=0.044064, validation/mean_average_precision=0.276925, validation/num_examples=43793
I0206 07:14:22.161696 139758996317952 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.04745345935225487, loss=0.03267644718289375
I0206 07:14:54.042433 139735302588160 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.04279686510562897, loss=0.029041094705462456
I0206 07:15:25.975558 139758996317952 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.04226664453744888, loss=0.02934691123664379
I0206 07:15:57.432368 139735302588160 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.04858021065592766, loss=0.032678939402103424
I0206 07:16:29.094284 139758996317952 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.0485684759914875, loss=0.02921442687511444
I0206 07:17:00.601593 139735302588160 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.07239871472120285, loss=0.031504422426223755
I0206 07:17:32.133318 139758996317952 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.04264006018638611, loss=0.03188418969511986
I0206 07:18:04.066344 139735302588160 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.05700558423995972, loss=0.02888300269842148
I0206 07:18:11.164934 139919816816448 spec.py:321] Evaluating on the training split.
I0206 07:19:43.990501 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 07:19:47.030788 139919816816448 spec.py:349] Evaluating on the test split.
I0206 07:19:50.022938 139919816816448 submission_runner.py:408] Time since start: 11106.82s, 	Step: 24123, 	{'train/accuracy': 0.9916008114814758, 'train/loss': 0.027223728597164154, 'train/mean_average_precision': 0.48737572996290834, 'validation/accuracy': 0.9869104027748108, 'validation/loss': 0.043956320732831955, 'validation/mean_average_precision': 0.2830847361580403, 'validation/num_examples': 43793, 'test/accuracy': 0.9861299991607666, 'test/loss': 0.0465262271463871, 'test/mean_average_precision': 0.27606768093356254, 'test/num_examples': 43793, 'score': 7695.909645080566, 'total_duration': 11106.817147493362, 'accumulated_submission_time': 7695.909645080566, 'accumulated_eval_time': 3409.015555858612, 'accumulated_logging_time': 1.2186834812164307}
I0206 07:19:50.042790 139679759779584 logging_writer.py:48] [24123] accumulated_eval_time=3409.015556, accumulated_logging_time=1.218683, accumulated_submission_time=7695.909645, global_step=24123, preemption_count=0, score=7695.909645, test/accuracy=0.986130, test/loss=0.046526, test/mean_average_precision=0.276068, test/num_examples=43793, total_duration=11106.817147, train/accuracy=0.991601, train/loss=0.027224, train/mean_average_precision=0.487376, validation/accuracy=0.986910, validation/loss=0.043956, validation/mean_average_precision=0.283085, validation/num_examples=43793
I0206 07:20:15.239898 139716992366336 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.05272388085722923, loss=0.03261934593319893
I0206 07:20:46.995206 139679759779584 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.03982166573405266, loss=0.030722854658961296
I0206 07:21:18.707801 139716992366336 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.05323093757033348, loss=0.02945503033697605
I0206 07:21:50.319065 139679759779584 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.05420367419719696, loss=0.031192006543278694
I0206 07:22:21.610836 139716992366336 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.05168593302369118, loss=0.028513018041849136
I0206 07:22:53.277087 139679759779584 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.04709096997976303, loss=0.03314535692334175
I0206 07:23:24.804162 139716992366336 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.04700974375009537, loss=0.027945205569267273
I0206 07:23:50.340588 139919816816448 spec.py:321] Evaluating on the training split.
I0206 07:25:26.976232 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 07:25:30.009465 139919816816448 spec.py:349] Evaluating on the test split.
I0206 07:25:32.983814 139919816816448 submission_runner.py:408] Time since start: 11449.78s, 	Step: 24882, 	{'train/accuracy': 0.9916938543319702, 'train/loss': 0.026998370885849, 'train/mean_average_precision': 0.4925730935016073, 'validation/accuracy': 0.9868324398994446, 'validation/loss': 0.04409567266702652, 'validation/mean_average_precision': 0.2810012658860785, 'validation/num_examples': 43793, 'test/accuracy': 0.9860736131668091, 'test/loss': 0.0468006432056427, 'test/mean_average_precision': 0.2785355949087478, 'test/num_examples': 43793, 'score': 7936.176169872284, 'total_duration': 11449.778022766113, 'accumulated_submission_time': 7936.176169872284, 'accumulated_eval_time': 3511.65873837471, 'accumulated_logging_time': 1.2492575645446777}
I0206 07:25:33.003569 139697948911360 logging_writer.py:48] [24882] accumulated_eval_time=3511.658738, accumulated_logging_time=1.249258, accumulated_submission_time=7936.176170, global_step=24882, preemption_count=0, score=7936.176170, test/accuracy=0.986074, test/loss=0.046801, test/mean_average_precision=0.278536, test/num_examples=43793, total_duration=11449.778023, train/accuracy=0.991694, train/loss=0.026998, train/mean_average_precision=0.492573, validation/accuracy=0.986832, validation/loss=0.044096, validation/mean_average_precision=0.281001, validation/num_examples=43793
I0206 07:25:39.091573 139735302588160 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.04352845624089241, loss=0.027438785880804062
I0206 07:26:10.960859 139697948911360 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.04613626375794411, loss=0.027483755722641945
I0206 07:26:42.463688 139735302588160 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.06074976548552513, loss=0.031938761472702026
I0206 07:27:13.786648 139697948911360 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.05057992413640022, loss=0.0297191571444273
I0206 07:27:44.939437 139735302588160 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.047199446707963943, loss=0.03206774219870567
I0206 07:28:16.312752 139697948911360 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.059515293687582016, loss=0.027026431635022163
I0206 07:28:47.525858 139735302588160 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.047459255903959274, loss=0.030939863994717598
I0206 07:29:19.101691 139697948911360 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.05300499498844147, loss=0.03157911077141762
I0206 07:29:33.072852 139919816816448 spec.py:321] Evaluating on the training split.
I0206 07:31:05.106699 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 07:31:08.124401 139919816816448 spec.py:349] Evaluating on the test split.
I0206 07:31:11.220690 139919816816448 submission_runner.py:408] Time since start: 11788.01s, 	Step: 25646, 	{'train/accuracy': 0.9915238618850708, 'train/loss': 0.02756897173821926, 'train/mean_average_precision': 0.4713439378127574, 'validation/accuracy': 0.9869733452796936, 'validation/loss': 0.043928198516368866, 'validation/mean_average_precision': 0.28169856780816016, 'validation/num_examples': 43793, 'test/accuracy': 0.986088752746582, 'test/loss': 0.04671822860836983, 'test/mean_average_precision': 0.27525278862473707, 'test/num_examples': 43793, 'score': 8176.215074539185, 'total_duration': 11788.014889717102, 'accumulated_submission_time': 8176.215074539185, 'accumulated_eval_time': 3609.8065259456635, 'accumulated_logging_time': 1.2795326709747314}
I0206 07:31:11.242446 139679759779584 logging_writer.py:48] [25646] accumulated_eval_time=3609.806526, accumulated_logging_time=1.279533, accumulated_submission_time=8176.215075, global_step=25646, preemption_count=0, score=8176.215075, test/accuracy=0.986089, test/loss=0.046718, test/mean_average_precision=0.275253, test/num_examples=43793, total_duration=11788.014890, train/accuracy=0.991524, train/loss=0.027569, train/mean_average_precision=0.471344, validation/accuracy=0.986973, validation/loss=0.043928, validation/mean_average_precision=0.281699, validation/num_examples=43793
I0206 07:31:28.564704 139716992366336 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.043928999453783035, loss=0.030510710552334785
I0206 07:32:00.052795 139679759779584 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.06486014276742935, loss=0.03118196688592434
I0206 07:32:31.408066 139716992366336 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.05647497624158859, loss=0.031540341675281525
I0206 07:33:02.535736 139679759779584 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.0437399297952652, loss=0.028705887496471405
I0206 07:33:35.945276 139716992366336 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.04840102419257164, loss=0.027227411046624184
I0206 07:34:07.446017 139679759779584 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.05072252079844475, loss=0.031299617141485214
I0206 07:34:38.955946 139716992366336 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.06082339212298393, loss=0.03183874860405922
I0206 07:35:10.308095 139679759779584 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.060147568583488464, loss=0.03015373833477497
I0206 07:35:11.281065 139919816816448 spec.py:321] Evaluating on the training split.
I0206 07:36:46.790607 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 07:36:49.825987 139919816816448 spec.py:349] Evaluating on the test split.
I0206 07:36:52.820980 139919816816448 submission_runner.py:408] Time since start: 12129.62s, 	Step: 26404, 	{'train/accuracy': 0.9913026094436646, 'train/loss': 0.028051326051354408, 'train/mean_average_precision': 0.45402465697035443, 'validation/accuracy': 0.9868937730789185, 'validation/loss': 0.04407404735684395, 'validation/mean_average_precision': 0.2784015307768114, 'validation/num_examples': 43793, 'test/accuracy': 0.9861670732498169, 'test/loss': 0.04672127217054367, 'test/mean_average_precision': 0.27219365552121805, 'test/num_examples': 43793, 'score': 8416.223066806793, 'total_duration': 12129.615074634552, 'accumulated_submission_time': 8416.223066806793, 'accumulated_eval_time': 3711.346279144287, 'accumulated_logging_time': 1.3122563362121582}
I0206 07:36:52.841273 139697948911360 logging_writer.py:48] [26404] accumulated_eval_time=3711.346279, accumulated_logging_time=1.312256, accumulated_submission_time=8416.223067, global_step=26404, preemption_count=0, score=8416.223067, test/accuracy=0.986167, test/loss=0.046721, test/mean_average_precision=0.272194, test/num_examples=43793, total_duration=12129.615075, train/accuracy=0.991303, train/loss=0.028051, train/mean_average_precision=0.454025, validation/accuracy=0.986894, validation/loss=0.044074, validation/mean_average_precision=0.278402, validation/num_examples=43793
I0206 07:37:23.376324 139735302588160 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.05243251472711563, loss=0.0293223038315773
I0206 07:37:54.966471 139697948911360 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.05155688524246216, loss=0.03072497993707657
I0206 07:38:26.633914 139735302588160 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.05050896108150482, loss=0.028987200930714607
I0206 07:38:57.934662 139697948911360 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.050560273230075836, loss=0.03240213170647621
I0206 07:39:29.488316 139735302588160 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.05419301241636276, loss=0.027117887511849403
I0206 07:40:00.894534 139697948911360 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.06433235853910446, loss=0.03357251361012459
I0206 07:40:32.359804 139735302588160 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.05506109446287155, loss=0.030312618240714073
I0206 07:40:53.036837 139919816816448 spec.py:321] Evaluating on the training split.
I0206 07:42:31.317965 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 07:42:34.382875 139919816816448 spec.py:349] Evaluating on the test split.
I0206 07:42:37.352130 139919816816448 submission_runner.py:408] Time since start: 12474.15s, 	Step: 27167, 	{'train/accuracy': 0.9913366436958313, 'train/loss': 0.027996491640806198, 'train/mean_average_precision': 0.45123302985127556, 'validation/accuracy': 0.9870614409446716, 'validation/loss': 0.04383734241127968, 'validation/mean_average_precision': 0.2824528369850656, 'validation/num_examples': 43793, 'test/accuracy': 0.9861894249916077, 'test/loss': 0.046678170561790466, 'test/mean_average_precision': 0.2716197370065153, 'test/num_examples': 43793, 'score': 8656.387609004974, 'total_duration': 12474.14634013176, 'accumulated_submission_time': 8656.387609004974, 'accumulated_eval_time': 3815.66153049469, 'accumulated_logging_time': 1.343522071838379}
I0206 07:42:37.371939 139679759779584 logging_writer.py:48] [27167] accumulated_eval_time=3815.661530, accumulated_logging_time=1.343522, accumulated_submission_time=8656.387609, global_step=27167, preemption_count=0, score=8656.387609, test/accuracy=0.986189, test/loss=0.046678, test/mean_average_precision=0.271620, test/num_examples=43793, total_duration=12474.146340, train/accuracy=0.991337, train/loss=0.027996, train/mean_average_precision=0.451233, validation/accuracy=0.987061, validation/loss=0.043837, validation/mean_average_precision=0.282453, validation/num_examples=43793
I0206 07:42:48.029729 139758996317952 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.04663638025522232, loss=0.028563261032104492
I0206 07:43:19.454352 139679759779584 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.05096088722348213, loss=0.03122733160853386
I0206 07:43:51.119345 139758996317952 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.05276881158351898, loss=0.03086088038980961
I0206 07:44:22.719378 139679759779584 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.05059767886996269, loss=0.028083063662052155
I0206 07:44:54.089754 139758996317952 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.08629754185676575, loss=0.029227308928966522
I0206 07:45:26.076152 139679759779584 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.06276894360780716, loss=0.031604789197444916
I0206 07:45:58.175769 139758996317952 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.04950116574764252, loss=0.03195924311876297
I0206 07:46:30.289465 139679759779584 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.04718680679798126, loss=0.026410264894366264
I0206 07:46:37.542796 139919816816448 spec.py:321] Evaluating on the training split.
I0206 07:48:10.869879 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 07:48:13.893165 139919816816448 spec.py:349] Evaluating on the test split.
I0206 07:48:16.914642 139919816816448 submission_runner.py:408] Time since start: 12813.71s, 	Step: 27924, 	{'train/accuracy': 0.9912831783294678, 'train/loss': 0.028219128027558327, 'train/mean_average_precision': 0.45971099259131304, 'validation/accuracy': 0.9870139360427856, 'validation/loss': 0.04407530277967453, 'validation/mean_average_precision': 0.2801922874370537, 'validation/num_examples': 43793, 'test/accuracy': 0.9862703084945679, 'test/loss': 0.04676474630832672, 'test/mean_average_precision': 0.27914873193636414, 'test/num_examples': 43793, 'score': 8896.527443885803, 'total_duration': 12813.708843708038, 'accumulated_submission_time': 8896.527443885803, 'accumulated_eval_time': 3915.0333251953125, 'accumulated_logging_time': 1.3740589618682861}
I0206 07:48:16.938614 139697948911360 logging_writer.py:48] [27924] accumulated_eval_time=3915.033325, accumulated_logging_time=1.374059, accumulated_submission_time=8896.527444, global_step=27924, preemption_count=0, score=8896.527444, test/accuracy=0.986270, test/loss=0.046765, test/mean_average_precision=0.279149, test/num_examples=43793, total_duration=12813.708844, train/accuracy=0.991283, train/loss=0.028219, train/mean_average_precision=0.459711, validation/accuracy=0.987014, validation/loss=0.044075, validation/mean_average_precision=0.280192, validation/num_examples=43793
I0206 07:48:41.456100 139716992366336 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.055111438035964966, loss=0.03071913681924343
I0206 07:49:13.196120 139697948911360 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.053146667778491974, loss=0.033369433134794235
I0206 07:49:44.932122 139716992366336 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.053362853825092316, loss=0.0287124402821064
I0206 07:50:16.805572 139697948911360 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.054239947348833084, loss=0.025143031030893326
I0206 07:50:49.047068 139716992366336 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.06799235194921494, loss=0.030344724655151367
I0206 07:51:20.840735 139697948911360 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.054877690970897675, loss=0.030465709045529366
I0206 07:51:52.851100 139716992366336 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.07965666055679321, loss=0.03548299893736839
I0206 07:52:16.978940 139919816816448 spec.py:321] Evaluating on the training split.
I0206 07:53:56.958321 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 07:54:00.023510 139919816816448 spec.py:349] Evaluating on the test split.
I0206 07:54:03.126772 139919816816448 submission_runner.py:408] Time since start: 13159.92s, 	Step: 28676, 	{'train/accuracy': 0.9914658069610596, 'train/loss': 0.027606068179011345, 'train/mean_average_precision': 0.48040133922354344, 'validation/accuracy': 0.9868357181549072, 'validation/loss': 0.04391742870211601, 'validation/mean_average_precision': 0.2829397828021821, 'validation/num_examples': 43793, 'test/accuracy': 0.9860512614250183, 'test/loss': 0.04667437821626663, 'test/mean_average_precision': 0.26969004395085033, 'test/num_examples': 43793, 'score': 9136.534759044647, 'total_duration': 13159.920971632004, 'accumulated_submission_time': 9136.534759044647, 'accumulated_eval_time': 4021.1811108589172, 'accumulated_logging_time': 1.4105889797210693}
I0206 07:54:03.147526 139735302588160 logging_writer.py:48] [28676] accumulated_eval_time=4021.181111, accumulated_logging_time=1.410589, accumulated_submission_time=9136.534759, global_step=28676, preemption_count=0, score=9136.534759, test/accuracy=0.986051, test/loss=0.046674, test/mean_average_precision=0.269690, test/num_examples=43793, total_duration=13159.920972, train/accuracy=0.991466, train/loss=0.027606, train/mean_average_precision=0.480401, validation/accuracy=0.986836, validation/loss=0.043917, validation/mean_average_precision=0.282940, validation/num_examples=43793
I0206 07:54:11.350714 139758996317952 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.049073126167058945, loss=0.028606412932276726
I0206 07:54:43.737000 139735302588160 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.04688074439764023, loss=0.027411920949816704
I0206 07:55:15.961093 139758996317952 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.054311566054821014, loss=0.03343290090560913
I0206 07:55:47.663228 139735302588160 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.05501007288694382, loss=0.03158235177397728
I0206 07:56:19.264509 139758996317952 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.05456161126494408, loss=0.029325654730200768
I0206 07:56:50.700615 139735302588160 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.053387027233839035, loss=0.030015576630830765
I0206 07:57:22.519102 139758996317952 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.056796666234731674, loss=0.03165876120328903
I0206 07:57:54.122798 139735302588160 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.06203794851899147, loss=0.03235280513763428
I0206 07:58:03.210825 139919816816448 spec.py:321] Evaluating on the training split.
I0206 07:59:39.600085 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 07:59:42.805886 139919816816448 spec.py:349] Evaluating on the test split.
I0206 07:59:45.952532 139919816816448 submission_runner.py:408] Time since start: 13502.75s, 	Step: 29429, 	{'train/accuracy': 0.9914921522140503, 'train/loss': 0.027481088414788246, 'train/mean_average_precision': 0.47864087462123883, 'validation/accuracy': 0.9869875311851501, 'validation/loss': 0.044133707880973816, 'validation/mean_average_precision': 0.280014814493845, 'validation/num_examples': 43793, 'test/accuracy': 0.9862012267112732, 'test/loss': 0.04700068011879921, 'test/mean_average_precision': 0.27495089302911, 'test/num_examples': 43793, 'score': 9376.566728830338, 'total_duration': 13502.746740341187, 'accumulated_submission_time': 9376.566728830338, 'accumulated_eval_time': 4123.922771692276, 'accumulated_logging_time': 1.4421751499176025}
I0206 07:59:45.974032 139679759779584 logging_writer.py:48] [29429] accumulated_eval_time=4123.922772, accumulated_logging_time=1.442175, accumulated_submission_time=9376.566729, global_step=29429, preemption_count=0, score=9376.566729, test/accuracy=0.986201, test/loss=0.047001, test/mean_average_precision=0.274951, test/num_examples=43793, total_duration=13502.746740, train/accuracy=0.991492, train/loss=0.027481, train/mean_average_precision=0.478641, validation/accuracy=0.986988, validation/loss=0.044134, validation/mean_average_precision=0.280015, validation/num_examples=43793
I0206 08:00:08.935722 139716992366336 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.06175398826599121, loss=0.0309114009141922
I0206 08:00:40.304461 139679759779584 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.05023575946688652, loss=0.030346618965268135
I0206 08:01:11.321003 139716992366336 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.047033440321683884, loss=0.028814392164349556
I0206 08:01:42.956890 139679759779584 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.05573246628046036, loss=0.03016682155430317
I0206 08:02:14.541502 139716992366336 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.06934130936861038, loss=0.029281239956617355
I0206 08:02:45.719476 139679759779584 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.07280761003494263, loss=0.033540837466716766
I0206 08:03:17.147099 139716992366336 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.06547518074512482, loss=0.029994245618581772
I0206 08:03:46.074011 139919816816448 spec.py:321] Evaluating on the training split.
I0206 08:05:20.861323 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 08:05:23.905956 139919816816448 spec.py:349] Evaluating on the test split.
I0206 08:05:26.941248 139919816816448 submission_runner.py:408] Time since start: 13843.74s, 	Step: 30193, 	{'train/accuracy': 0.9915912747383118, 'train/loss': 0.02712496928870678, 'train/mean_average_precision': 0.48205674043474084, 'validation/accuracy': 0.9869375824928284, 'validation/loss': 0.04378962889313698, 'validation/mean_average_precision': 0.2850369521914939, 'validation/num_examples': 43793, 'test/accuracy': 0.9862092137336731, 'test/loss': 0.046412695199251175, 'test/mean_average_precision': 0.2754781499849553, 'test/num_examples': 43793, 'score': 9616.63538479805, 'total_duration': 13843.735455036163, 'accumulated_submission_time': 9616.63538479805, 'accumulated_eval_time': 4224.7899651527405, 'accumulated_logging_time': 1.474400281906128}
I0206 08:05:26.962405 139735302588160 logging_writer.py:48] [30193] accumulated_eval_time=4224.789965, accumulated_logging_time=1.474400, accumulated_submission_time=9616.635385, global_step=30193, preemption_count=0, score=9616.635385, test/accuracy=0.986209, test/loss=0.046413, test/mean_average_precision=0.275478, test/num_examples=43793, total_duration=13843.735455, train/accuracy=0.991591, train/loss=0.027125, train/mean_average_precision=0.482057, validation/accuracy=0.986938, validation/loss=0.043790, validation/mean_average_precision=0.285037, validation/num_examples=43793
I0206 08:05:29.492248 139758996317952 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.09180724620819092, loss=0.026662899181246758
I0206 08:06:00.727210 139735302588160 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.06301158666610718, loss=0.03002876043319702
I0206 08:06:31.794963 139758996317952 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.07728087157011032, loss=0.028245581313967705
I0206 08:07:03.128153 139735302588160 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.06161956861615181, loss=0.026492565870285034
I0206 08:07:34.276508 139758996317952 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.06475014984607697, loss=0.03180009871721268
I0206 08:08:05.558796 139735302588160 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.04990129545331001, loss=0.026699401438236237
I0206 08:08:36.901275 139758996317952 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.05640161782503128, loss=0.03233683854341507
I0206 08:09:08.270389 139735302588160 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.062386125326156616, loss=0.028742020949721336
I0206 08:09:26.990520 139919816816448 spec.py:321] Evaluating on the training split.
I0206 08:11:04.619014 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 08:11:07.631350 139919816816448 spec.py:349] Evaluating on the test split.
I0206 08:11:10.664055 139919816816448 submission_runner.py:408] Time since start: 14187.46s, 	Step: 30960, 	{'train/accuracy': 0.9916066527366638, 'train/loss': 0.026982925832271576, 'train/mean_average_precision': 0.4897253311971881, 'validation/accuracy': 0.9870520830154419, 'validation/loss': 0.043927162885665894, 'validation/mean_average_precision': 0.28138059817879196, 'validation/num_examples': 43793, 'test/accuracy': 0.9862736463546753, 'test/loss': 0.0467136912047863, 'test/mean_average_precision': 0.27366750933361444, 'test/num_examples': 43793, 'score': 9856.632263422012, 'total_duration': 14187.458263158798, 'accumulated_submission_time': 9856.632263422012, 'accumulated_eval_time': 4328.463456869125, 'accumulated_logging_time': 1.5062589645385742}
I0206 08:11:10.686592 139679759779584 logging_writer.py:48] [30960] accumulated_eval_time=4328.463457, accumulated_logging_time=1.506259, accumulated_submission_time=9856.632263, global_step=30960, preemption_count=0, score=9856.632263, test/accuracy=0.986274, test/loss=0.046714, test/mean_average_precision=0.273668, test/num_examples=43793, total_duration=14187.458263, train/accuracy=0.991607, train/loss=0.026983, train/mean_average_precision=0.489725, validation/accuracy=0.987052, validation/loss=0.043927, validation/mean_average_precision=0.281381, validation/num_examples=43793
I0206 08:11:23.672056 139716992366336 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.06377534568309784, loss=0.029166612774133682
I0206 08:11:55.727022 139679759779584 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.07039521634578705, loss=0.03122817724943161
I0206 08:12:27.323288 139716992366336 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.06365691870450974, loss=0.02923753671348095
I0206 08:12:58.566029 139679759779584 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.05774828791618347, loss=0.02823391743004322
I0206 08:13:29.971946 139716992366336 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.066791832447052, loss=0.028153587132692337
I0206 08:14:01.052624 139679759779584 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.06027333810925484, loss=0.02679949440062046
I0206 08:14:32.354937 139716992366336 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.06492647528648376, loss=0.030060047283768654
I0206 08:15:03.533580 139679759779584 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.05613004416227341, loss=0.029774537310004234
I0206 08:15:10.664382 139919816816448 spec.py:321] Evaluating on the training split.
I0206 08:16:41.657530 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 08:16:44.780703 139919816816448 spec.py:349] Evaluating on the test split.
I0206 08:16:47.843594 139919816816448 submission_runner.py:408] Time since start: 14524.64s, 	Step: 31724, 	{'train/accuracy': 0.991977870464325, 'train/loss': 0.02589631825685501, 'train/mean_average_precision': 0.49881832888686783, 'validation/accuracy': 0.9869737029075623, 'validation/loss': 0.04373054951429367, 'validation/mean_average_precision': 0.287502596920722, 'validation/num_examples': 43793, 'test/accuracy': 0.9861574172973633, 'test/loss': 0.04638619348406792, 'test/mean_average_precision': 0.27596476724142216, 'test/num_examples': 43793, 'score': 10096.578315734863, 'total_duration': 14524.637803077698, 'accumulated_submission_time': 10096.578315734863, 'accumulated_eval_time': 4425.642622709274, 'accumulated_logging_time': 1.5399868488311768}
I0206 08:16:47.864992 139735302588160 logging_writer.py:48] [31724] accumulated_eval_time=4425.642623, accumulated_logging_time=1.539987, accumulated_submission_time=10096.578316, global_step=31724, preemption_count=0, score=10096.578316, test/accuracy=0.986157, test/loss=0.046386, test/mean_average_precision=0.275965, test/num_examples=43793, total_duration=14524.637803, train/accuracy=0.991978, train/loss=0.025896, train/mean_average_precision=0.498818, validation/accuracy=0.986974, validation/loss=0.043731, validation/mean_average_precision=0.287503, validation/num_examples=43793
I0206 08:17:12.563161 139758996317952 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.05441856384277344, loss=0.028243204578757286
I0206 08:17:43.701546 139735302588160 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.053867071866989136, loss=0.027491221204400063
I0206 08:18:15.311766 139758996317952 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.057209525257349014, loss=0.032556772232055664
I0206 08:18:46.724732 139735302588160 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.05379695072770119, loss=0.02608814649283886
I0206 08:19:18.147294 139758996317952 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.05232429876923561, loss=0.02951047196984291
I0206 08:19:49.471275 139735302588160 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.05536090210080147, loss=0.029350521042943
I0206 08:20:21.286363 139758996317952 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.061586908996105194, loss=0.03128524497151375
I0206 08:20:48.008881 139919816816448 spec.py:321] Evaluating on the training split.
I0206 08:22:24.662577 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 08:22:27.724412 139919816816448 spec.py:349] Evaluating on the test split.
I0206 08:22:30.777094 139919816816448 submission_runner.py:408] Time since start: 14867.57s, 	Step: 32485, 	{'train/accuracy': 0.9921050071716309, 'train/loss': 0.025456955656409264, 'train/mean_average_precision': 0.5302573368344676, 'validation/accuracy': 0.986983060836792, 'validation/loss': 0.044046539813280106, 'validation/mean_average_precision': 0.28281440925602086, 'validation/num_examples': 43793, 'test/accuracy': 0.9862816333770752, 'test/loss': 0.04657791927456856, 'test/mean_average_precision': 0.28005280151764833, 'test/num_examples': 43793, 'score': 10336.690904140472, 'total_duration': 14867.57130074501, 'accumulated_submission_time': 10336.690904140472, 'accumulated_eval_time': 4528.410791397095, 'accumulated_logging_time': 1.5721287727355957}
I0206 08:22:30.799273 139679759779584 logging_writer.py:48] [32485] accumulated_eval_time=4528.410791, accumulated_logging_time=1.572129, accumulated_submission_time=10336.690904, global_step=32485, preemption_count=0, score=10336.690904, test/accuracy=0.986282, test/loss=0.046578, test/mean_average_precision=0.280053, test/num_examples=43793, total_duration=14867.571301, train/accuracy=0.992105, train/loss=0.025457, train/mean_average_precision=0.530257, validation/accuracy=0.986983, validation/loss=0.044047, validation/mean_average_precision=0.282814, validation/num_examples=43793
I0206 08:22:35.794862 139697948911360 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.05209391936659813, loss=0.028012380003929138
I0206 08:23:07.411876 139679759779584 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.06718127429485321, loss=0.03169555217027664
I0206 08:23:39.041736 139697948911360 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.07007426768541336, loss=0.027250878512859344
I0206 08:24:10.223749 139679759779584 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.0598452165722847, loss=0.02914731577038765
I0206 08:24:41.714779 139697948911360 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.06043786182999611, loss=0.027927340939641
I0206 08:25:13.248984 139679759779584 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.05393596738576889, loss=0.02701432630419731
I0206 08:25:44.706327 139697948911360 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.05598168820142746, loss=0.030184784904122353
I0206 08:26:16.183137 139679759779584 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.06978069245815277, loss=0.02954498678445816
I0206 08:26:31.057517 139919816816448 spec.py:321] Evaluating on the training split.
I0206 08:28:02.780501 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 08:28:05.911253 139919816816448 spec.py:349] Evaluating on the test split.
I0206 08:28:08.998403 139919816816448 submission_runner.py:408] Time since start: 15205.79s, 	Step: 33248, 	{'train/accuracy': 0.9921290874481201, 'train/loss': 0.025368524715304375, 'train/mean_average_precision': 0.5184021366579336, 'validation/accuracy': 0.9869611263275146, 'validation/loss': 0.04421015456318855, 'validation/mean_average_precision': 0.2848071117613658, 'validation/num_examples': 43793, 'test/accuracy': 0.9862163662910461, 'test/loss': 0.04685094580054283, 'test/mean_average_precision': 0.2793535608483301, 'test/num_examples': 43793, 'score': 10576.917943954468, 'total_duration': 15205.792612075806, 'accumulated_submission_time': 10576.917943954468, 'accumulated_eval_time': 4626.351637125015, 'accumulated_logging_time': 1.6052203178405762}
I0206 08:28:09.026004 139735302588160 logging_writer.py:48] [33248] accumulated_eval_time=4626.351637, accumulated_logging_time=1.605220, accumulated_submission_time=10576.917944, global_step=33248, preemption_count=0, score=10576.917944, test/accuracy=0.986216, test/loss=0.046851, test/mean_average_precision=0.279354, test/num_examples=43793, total_duration=15205.792612, train/accuracy=0.992129, train/loss=0.025369, train/mean_average_precision=0.518402, validation/accuracy=0.986961, validation/loss=0.044210, validation/mean_average_precision=0.284807, validation/num_examples=43793
I0206 08:28:25.655237 139758996317952 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.05673370137810707, loss=0.028952067717909813
I0206 08:28:57.572419 139735302588160 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.0694572776556015, loss=0.030690059065818787
I0206 08:29:29.443291 139758996317952 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.05735797435045242, loss=0.027793021872639656
I0206 08:30:01.463634 139735302588160 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.0643840879201889, loss=0.026540691033005714
I0206 08:30:33.037646 139758996317952 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.05258532613515854, loss=0.025664344429969788
I0206 08:31:04.536850 139735302588160 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.06316132843494415, loss=0.026525648310780525
I0206 08:31:36.077026 139758996317952 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.06374388933181763, loss=0.030058613047003746
I0206 08:32:07.690323 139735302588160 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.0826110765337944, loss=0.030385956168174744
I0206 08:32:09.020505 139919816816448 spec.py:321] Evaluating on the training split.
I0206 08:33:45.190521 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 08:33:50.731864 139919816816448 spec.py:349] Evaluating on the test split.
I0206 08:33:53.787123 139919816816448 submission_runner.py:408] Time since start: 15550.58s, 	Step: 34005, 	{'train/accuracy': 0.9920830726623535, 'train/loss': 0.02560705691576004, 'train/mean_average_precision': 0.5206663689703317, 'validation/accuracy': 0.9869534373283386, 'validation/loss': 0.0438498817384243, 'validation/mean_average_precision': 0.2907716835350484, 'validation/num_examples': 43793, 'test/accuracy': 0.9860959053039551, 'test/loss': 0.046714600175619125, 'test/mean_average_precision': 0.2774428806202462, 'test/num_examples': 43793, 'score': 10816.88122177124, 'total_duration': 15550.581326246262, 'accumulated_submission_time': 10816.88122177124, 'accumulated_eval_time': 4731.118203163147, 'accumulated_logging_time': 1.6438887119293213}
I0206 08:33:53.811355 139679759779584 logging_writer.py:48] [34005] accumulated_eval_time=4731.118203, accumulated_logging_time=1.643889, accumulated_submission_time=10816.881222, global_step=34005, preemption_count=0, score=10816.881222, test/accuracy=0.986096, test/loss=0.046715, test/mean_average_precision=0.277443, test/num_examples=43793, total_duration=15550.581326, train/accuracy=0.992083, train/loss=0.025607, train/mean_average_precision=0.520666, validation/accuracy=0.986953, validation/loss=0.043850, validation/mean_average_precision=0.290772, validation/num_examples=43793
I0206 08:34:24.763022 139716992366336 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.05531296506524086, loss=0.02715735137462616
I0206 08:34:56.226639 139679759779584 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.057043157517910004, loss=0.028081486001610756
I0206 08:35:27.559169 139716992366336 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.058511726558208466, loss=0.025832731276750565
I0206 08:35:59.224192 139679759779584 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.06309719383716583, loss=0.02955203875899315
I0206 08:36:30.851159 139716992366336 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.0579192154109478, loss=0.02718975767493248
I0206 08:37:01.916142 139679759779584 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.04885970056056976, loss=0.025121696293354034
I0206 08:37:33.212815 139716992366336 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.058250490576028824, loss=0.02851160243153572
I0206 08:37:53.899416 139919816816448 spec.py:321] Evaluating on the training split.
I0206 08:39:26.764954 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 08:39:29.750230 139919816816448 spec.py:349] Evaluating on the test split.
I0206 08:39:32.716524 139919816816448 submission_runner.py:408] Time since start: 15889.51s, 	Step: 34766, 	{'train/accuracy': 0.9918881058692932, 'train/loss': 0.0260873194783926, 'train/mean_average_precision': 0.5010801180054504, 'validation/accuracy': 0.9869778156280518, 'validation/loss': 0.04406430944800377, 'validation/mean_average_precision': 0.2841008787928426, 'validation/num_examples': 43793, 'test/accuracy': 0.9861780405044556, 'test/loss': 0.04691501334309578, 'test/mean_average_precision': 0.27400146489981886, 'test/num_examples': 43793, 'score': 11056.937723875046, 'total_duration': 15889.510733604431, 'accumulated_submission_time': 11056.937723875046, 'accumulated_eval_time': 4829.9352684021, 'accumulated_logging_time': 1.679349660873413}
I0206 08:39:32.740163 139697948911360 logging_writer.py:48] [34766] accumulated_eval_time=4829.935268, accumulated_logging_time=1.679350, accumulated_submission_time=11056.937724, global_step=34766, preemption_count=0, score=11056.937724, test/accuracy=0.986178, test/loss=0.046915, test/mean_average_precision=0.274001, test/num_examples=43793, total_duration=15889.510734, train/accuracy=0.991888, train/loss=0.026087, train/mean_average_precision=0.501080, validation/accuracy=0.986978, validation/loss=0.044064, validation/mean_average_precision=0.284101, validation/num_examples=43793
I0206 08:39:43.886882 139758996317952 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.07325738668441772, loss=0.02980414591729641
I0206 08:40:15.539368 139697948911360 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.06665777415037155, loss=0.029921388253569603
I0206 08:40:47.155512 139758996317952 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.05488424748182297, loss=0.02891688607633114
I0206 08:41:18.510224 139697948911360 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.06322165578603745, loss=0.028361726552248
I0206 08:41:50.113648 139758996317952 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.06256945431232452, loss=0.03030180186033249
I0206 08:42:22.118679 139697948911360 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.06324073672294617, loss=0.028113218024373055
I0206 08:42:53.682386 139758996317952 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.07551968097686768, loss=0.03015037626028061
I0206 08:43:25.237698 139697948911360 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.0864013135433197, loss=0.03403307497501373
I0206 08:43:32.787462 139919816816448 spec.py:321] Evaluating on the training split.
I0206 08:45:05.773339 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 08:45:08.869464 139919816816448 spec.py:349] Evaluating on the test split.
I0206 08:45:11.874462 139919816816448 submission_runner.py:408] Time since start: 16228.67s, 	Step: 35525, 	{'train/accuracy': 0.9919905662536621, 'train/loss': 0.025885270908474922, 'train/mean_average_precision': 0.5093072745034118, 'validation/accuracy': 0.9869050979614258, 'validation/loss': 0.04390646144747734, 'validation/mean_average_precision': 0.28743019110891205, 'validation/num_examples': 43793, 'test/accuracy': 0.986100971698761, 'test/loss': 0.04679521173238754, 'test/mean_average_precision': 0.27350426060281957, 'test/num_examples': 43793, 'score': 11296.953873872757, 'total_duration': 16228.668662548065, 'accumulated_submission_time': 11296.953873872757, 'accumulated_eval_time': 4929.022217512131, 'accumulated_logging_time': 1.7137835025787354}
I0206 08:45:11.896575 139716992366336 logging_writer.py:48] [35525] accumulated_eval_time=4929.022218, accumulated_logging_time=1.713784, accumulated_submission_time=11296.953874, global_step=35525, preemption_count=0, score=11296.953874, test/accuracy=0.986101, test/loss=0.046795, test/mean_average_precision=0.273504, test/num_examples=43793, total_duration=16228.668663, train/accuracy=0.991991, train/loss=0.025885, train/mean_average_precision=0.509307, validation/accuracy=0.986905, validation/loss=0.043906, validation/mean_average_precision=0.287430, validation/num_examples=43793
I0206 08:45:36.465936 139735302588160 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.06195920705795288, loss=0.027035271748900414
I0206 08:46:07.759598 139716992366336 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.05876980349421501, loss=0.02793067879974842
I0206 08:46:38.987971 139735302588160 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.06265760958194733, loss=0.03046688251197338
I0206 08:47:10.361591 139716992366336 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.0831034705042839, loss=0.030223684385418892
I0206 08:47:41.732625 139735302588160 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.06291688233613968, loss=0.02585737220942974
I0206 08:48:13.631850 139716992366336 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.0720655545592308, loss=0.027961984276771545
I0206 08:48:45.241034 139735302588160 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.05461505800485611, loss=0.026954691857099533
I0206 08:49:12.108354 139919816816448 spec.py:321] Evaluating on the training split.
I0206 08:50:50.733730 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 08:50:54.174123 139919816816448 spec.py:349] Evaluating on the test split.
I0206 08:50:57.562018 139919816816448 submission_runner.py:408] Time since start: 16574.36s, 	Step: 36284, 	{'train/accuracy': 0.9918835759162903, 'train/loss': 0.026082046329975128, 'train/mean_average_precision': 0.50537218004944, 'validation/accuracy': 0.987015962600708, 'validation/loss': 0.044364724308252335, 'validation/mean_average_precision': 0.28679058414749675, 'validation/num_examples': 43793, 'test/accuracy': 0.9861708879470825, 'test/loss': 0.04732805863022804, 'test/mean_average_precision': 0.2739957375683177, 'test/num_examples': 43793, 'score': 11537.133439779282, 'total_duration': 16574.356207609177, 'accumulated_submission_time': 11537.133439779282, 'accumulated_eval_time': 5034.475823879242, 'accumulated_logging_time': 1.7479205131530762}
I0206 08:50:57.587079 139697948911360 logging_writer.py:48] [36284] accumulated_eval_time=5034.475824, accumulated_logging_time=1.747921, accumulated_submission_time=11537.133440, global_step=36284, preemption_count=0, score=11537.133440, test/accuracy=0.986171, test/loss=0.047328, test/mean_average_precision=0.273996, test/num_examples=43793, total_duration=16574.356208, train/accuracy=0.991884, train/loss=0.026082, train/mean_average_precision=0.505372, validation/accuracy=0.987016, validation/loss=0.044365, validation/mean_average_precision=0.286791, validation/num_examples=43793
I0206 08:51:03.113236 139758996317952 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.06773880869150162, loss=0.03186379000544548
I0206 08:51:35.442419 139697948911360 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.05768560245633125, loss=0.026185816153883934
I0206 08:52:07.587828 139758996317952 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.08394298702478409, loss=0.03119724430143833
I0206 08:52:38.978508 139697948911360 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.061719659715890884, loss=0.030720487236976624
I0206 08:53:10.618214 139758996317952 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.05754042789340019, loss=0.031320419162511826
I0206 08:53:42.238018 139697948911360 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.06837711483240128, loss=0.027638385072350502
I0206 08:54:13.690770 139758996317952 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.059135545045137405, loss=0.023525094613432884
I0206 08:54:45.026428 139697948911360 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.05686750262975693, loss=0.028061073273420334
I0206 08:54:57.586757 139919816816448 spec.py:321] Evaluating on the training split.
I0206 08:56:28.287991 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 08:56:31.374352 139919816816448 spec.py:349] Evaluating on the test split.
I0206 08:56:34.405174 139919816816448 submission_runner.py:408] Time since start: 16911.20s, 	Step: 37041, 	{'train/accuracy': 0.991921603679657, 'train/loss': 0.02600712701678276, 'train/mean_average_precision': 0.5088854546962834, 'validation/accuracy': 0.9869713187217712, 'validation/loss': 0.04405609890818596, 'validation/mean_average_precision': 0.28551041680893874, 'validation/num_examples': 43793, 'test/accuracy': 0.9860870838165283, 'test/loss': 0.046863798052072525, 'test/mean_average_precision': 0.27493427809221893, 'test/num_examples': 43793, 'score': 11777.099576950073, 'total_duration': 16911.1993830204, 'accumulated_submission_time': 11777.099576950073, 'accumulated_eval_time': 5131.294198036194, 'accumulated_logging_time': 1.7850227355957031}
I0206 08:56:34.427668 139679759779584 logging_writer.py:48] [37041] accumulated_eval_time=5131.294198, accumulated_logging_time=1.785023, accumulated_submission_time=11777.099577, global_step=37041, preemption_count=0, score=11777.099577, test/accuracy=0.986087, test/loss=0.046864, test/mean_average_precision=0.274934, test/num_examples=43793, total_duration=16911.199383, train/accuracy=0.991922, train/loss=0.026007, train/mean_average_precision=0.508885, validation/accuracy=0.986971, validation/loss=0.044056, validation/mean_average_precision=0.285510, validation/num_examples=43793
I0206 08:56:53.702057 139716992366336 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.0816250741481781, loss=0.03150434419512749
I0206 08:57:25.864863 139679759779584 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.06654474139213562, loss=0.030461296439170837
I0206 08:57:57.501312 139716992366336 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.06719215959310532, loss=0.027652669697999954
I0206 08:58:29.323038 139679759779584 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.06263457238674164, loss=0.02586684189736843
I0206 08:59:01.178531 139716992366336 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.076494000852108, loss=0.028427572920918465
I0206 08:59:33.017178 139679759779584 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.061461757868528366, loss=0.029430700466036797
I0206 09:00:05.242119 139716992366336 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.08360695838928223, loss=0.029044965282082558
I0206 09:00:34.494725 139919816816448 spec.py:321] Evaluating on the training split.
I0206 09:02:13.480990 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 09:02:16.521054 139919816816448 spec.py:349] Evaluating on the test split.
I0206 09:02:19.516355 139919816816448 submission_runner.py:408] Time since start: 17256.31s, 	Step: 37792, 	{'train/accuracy': 0.9920625686645508, 'train/loss': 0.02546514943242073, 'train/mean_average_precision': 0.5246759598596786, 'validation/accuracy': 0.9870808720588684, 'validation/loss': 0.04423892870545387, 'validation/mean_average_precision': 0.29014781513482246, 'validation/num_examples': 43793, 'test/accuracy': 0.9861211776733398, 'test/loss': 0.04714534804224968, 'test/mean_average_precision': 0.2790645129473144, 'test/num_examples': 43793, 'score': 12017.13541841507, 'total_duration': 17256.310554504395, 'accumulated_submission_time': 12017.13541841507, 'accumulated_eval_time': 5236.3157749176025, 'accumulated_logging_time': 1.8184189796447754}
I0206 09:02:19.539121 139735302588160 logging_writer.py:48] [37792] accumulated_eval_time=5236.315775, accumulated_logging_time=1.818419, accumulated_submission_time=12017.135418, global_step=37792, preemption_count=0, score=12017.135418, test/accuracy=0.986121, test/loss=0.047145, test/mean_average_precision=0.279065, test/num_examples=43793, total_duration=17256.310555, train/accuracy=0.992063, train/loss=0.025465, train/mean_average_precision=0.524676, validation/accuracy=0.987081, validation/loss=0.044239, validation/mean_average_precision=0.290148, validation/num_examples=43793
I0206 09:02:22.443766 139758996317952 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.06232941523194313, loss=0.024831166490912437
I0206 09:02:54.739336 139735302588160 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.06256645917892456, loss=0.028469325974583626
I0206 09:03:27.074665 139758996317952 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.07256108522415161, loss=0.030156325548887253
I0206 09:03:59.331896 139735302588160 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.07172251492738724, loss=0.027897868305444717
I0206 09:04:31.630570 139758996317952 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.07000978291034698, loss=0.030598649755120277
I0206 09:05:03.575103 139735302588160 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.06497228890657425, loss=0.026768602430820465
I0206 09:05:35.717658 139758996317952 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.057891737669706345, loss=0.030079813674092293
I0206 09:06:07.662398 139735302588160 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.08521105349063873, loss=0.026907315477728844
I0206 09:06:19.747511 139919816816448 spec.py:321] Evaluating on the training split.
I0206 09:07:56.306437 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 09:07:59.340519 139919816816448 spec.py:349] Evaluating on the test split.
I0206 09:08:02.456602 139919816816448 submission_runner.py:408] Time since start: 17599.25s, 	Step: 38539, 	{'train/accuracy': 0.9921326041221619, 'train/loss': 0.02521633170545101, 'train/mean_average_precision': 0.5271417505934661, 'validation/accuracy': 0.9870041608810425, 'validation/loss': 0.04437185823917389, 'validation/mean_average_precision': 0.284068813649276, 'validation/num_examples': 43793, 'test/accuracy': 0.9862340688705444, 'test/loss': 0.04699847847223282, 'test/mean_average_precision': 0.27985943950190395, 'test/num_examples': 43793, 'score': 12257.310943841934, 'total_duration': 17599.25080871582, 'accumulated_submission_time': 12257.310943841934, 'accumulated_eval_time': 5339.024819612503, 'accumulated_logging_time': 1.8535473346710205}
I0206 09:08:02.480024 139697948911360 logging_writer.py:48] [38539] accumulated_eval_time=5339.024820, accumulated_logging_time=1.853547, accumulated_submission_time=12257.310944, global_step=38539, preemption_count=0, score=12257.310944, test/accuracy=0.986234, test/loss=0.046998, test/mean_average_precision=0.279859, test/num_examples=43793, total_duration=17599.250809, train/accuracy=0.992133, train/loss=0.025216, train/mean_average_precision=0.527142, validation/accuracy=0.987004, validation/loss=0.044372, validation/mean_average_precision=0.284069, validation/num_examples=43793
I0206 09:08:22.315405 139716992366336 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.06544779986143112, loss=0.026376811787486076
I0206 09:08:54.034113 139697948911360 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.07040854543447495, loss=0.029468959197402
I0206 09:09:25.729802 139716992366336 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.0641411766409874, loss=0.027700377628207207
I0206 09:09:57.655679 139697948911360 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.06677009165287018, loss=0.025461765006184578
I0206 09:10:29.206701 139716992366336 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.07795745879411697, loss=0.030359594151377678
I0206 09:11:00.825539 139697948911360 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.09092811495065689, loss=0.028922438621520996
I0206 09:11:32.286418 139716992366336 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.06659246236085892, loss=0.028526199981570244
I0206 09:12:02.643690 139919816816448 spec.py:321] Evaluating on the training split.
I0206 09:13:36.500842 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 09:13:39.539936 139919816816448 spec.py:349] Evaluating on the test split.
I0206 09:13:42.561589 139919816816448 submission_runner.py:408] Time since start: 17939.36s, 	Step: 39297, 	{'train/accuracy': 0.9923343062400818, 'train/loss': 0.024549124762415886, 'train/mean_average_precision': 0.5338019086037505, 'validation/accuracy': 0.9870902299880981, 'validation/loss': 0.0443548746407032, 'validation/mean_average_precision': 0.28783983629516036, 'validation/num_examples': 43793, 'test/accuracy': 0.9863966703414917, 'test/loss': 0.047153059393167496, 'test/mean_average_precision': 0.2812594940719487, 'test/num_examples': 43793, 'score': 12497.441692590714, 'total_duration': 17939.355797052383, 'accumulated_submission_time': 12497.441692590714, 'accumulated_eval_time': 5438.942674875259, 'accumulated_logging_time': 1.8888821601867676}
I0206 09:13:42.584726 139679759779584 logging_writer.py:48] [39297] accumulated_eval_time=5438.942675, accumulated_logging_time=1.888882, accumulated_submission_time=12497.441693, global_step=39297, preemption_count=0, score=12497.441693, test/accuracy=0.986397, test/loss=0.047153, test/mean_average_precision=0.281259, test/num_examples=43793, total_duration=17939.355797, train/accuracy=0.992334, train/loss=0.024549, train/mean_average_precision=0.533802, validation/accuracy=0.987090, validation/loss=0.044355, validation/mean_average_precision=0.287840, validation/num_examples=43793
I0206 09:13:43.868008 139735302588160 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.059641722589731216, loss=0.027654627338051796
I0206 09:14:15.986461 139679759779584 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.07881193608045578, loss=0.030174855142831802
I0206 09:14:48.271097 139735302588160 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.10355286300182343, loss=0.028010761365294456
I0206 09:15:20.262167 139679759779584 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.06621094048023224, loss=0.02705293521285057
I0206 09:15:51.998954 139735302588160 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.07253140211105347, loss=0.02725186198949814
I0206 09:16:23.910035 139679759779584 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.07350004464387894, loss=0.028922712430357933
I0206 09:16:55.655763 139735302588160 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.06465893238782883, loss=0.026393001899123192
I0206 09:17:27.475055 139679759779584 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.06627330929040909, loss=0.025547780096530914
I0206 09:17:42.864820 139919816816448 spec.py:321] Evaluating on the training split.
I0206 09:19:23.786948 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 09:19:26.820910 139919816816448 spec.py:349] Evaluating on the test split.
I0206 09:19:29.820007 139919816816448 submission_runner.py:408] Time since start: 18286.61s, 	Step: 40049, 	{'train/accuracy': 0.9923198223114014, 'train/loss': 0.024319665506482124, 'train/mean_average_precision': 0.5546808189751687, 'validation/accuracy': 0.9871109127998352, 'validation/loss': 0.04435906186699867, 'validation/mean_average_precision': 0.2893713804221078, 'validation/num_examples': 43793, 'test/accuracy': 0.9862483739852905, 'test/loss': 0.047381769865751266, 'test/mean_average_precision': 0.27669284161279245, 'test/num_examples': 43793, 'score': 12737.689799547195, 'total_duration': 18286.614214897156, 'accumulated_submission_time': 12737.689799547195, 'accumulated_eval_time': 5545.8978316783905, 'accumulated_logging_time': 1.9227867126464844}
I0206 09:19:29.843172 139716992366336 logging_writer.py:48] [40049] accumulated_eval_time=5545.897832, accumulated_logging_time=1.922787, accumulated_submission_time=12737.689800, global_step=40049, preemption_count=0, score=12737.689800, test/accuracy=0.986248, test/loss=0.047382, test/mean_average_precision=0.276693, test/num_examples=43793, total_duration=18286.614215, train/accuracy=0.992320, train/loss=0.024320, train/mean_average_precision=0.554681, validation/accuracy=0.987111, validation/loss=0.044359, validation/mean_average_precision=0.289371, validation/num_examples=43793
I0206 09:19:46.473257 139758996317952 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.08370596915483475, loss=0.02849246747791767
I0206 09:20:18.978072 139716992366336 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.07273950427770615, loss=0.02648446522653103
I0206 09:20:51.007528 139758996317952 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.07649495452642441, loss=0.026024481281638145
I0206 09:21:23.035785 139716992366336 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.06322236359119415, loss=0.026904650032520294
I0206 09:21:55.094516 139758996317952 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.07589349895715714, loss=0.030391190201044083
I0206 09:22:26.614692 139716992366336 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.059944286942481995, loss=0.02356537990272045
I0206 09:22:58.368971 139758996317952 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.057689182460308075, loss=0.024486688897013664
I0206 09:23:29.801600 139716992366336 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.07111725211143494, loss=0.028085988014936447
I0206 09:23:30.128513 139919816816448 spec.py:321] Evaluating on the training split.
I0206 09:25:04.836686 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 09:25:07.844833 139919816816448 spec.py:349] Evaluating on the test split.
I0206 09:25:10.890073 139919816816448 submission_runner.py:408] Time since start: 18627.68s, 	Step: 40802, 	{'train/accuracy': 0.9927132725715637, 'train/loss': 0.02329072542488575, 'train/mean_average_precision': 0.5798397030318193, 'validation/accuracy': 0.9870597720146179, 'validation/loss': 0.04475073888897896, 'validation/mean_average_precision': 0.2860022023207019, 'validation/num_examples': 43793, 'test/accuracy': 0.9862837791442871, 'test/loss': 0.0476432628929615, 'test/mean_average_precision': 0.2762729613561593, 'test/num_examples': 43793, 'score': 12977.942219495773, 'total_duration': 18627.68426156044, 'accumulated_submission_time': 12977.942219495773, 'accumulated_eval_time': 5646.659322977066, 'accumulated_logging_time': 1.9570858478546143}
I0206 09:25:10.913084 139679759779584 logging_writer.py:48] [40802] accumulated_eval_time=5646.659323, accumulated_logging_time=1.957086, accumulated_submission_time=12977.942219, global_step=40802, preemption_count=0, score=12977.942219, test/accuracy=0.986284, test/loss=0.047643, test/mean_average_precision=0.276273, test/num_examples=43793, total_duration=18627.684262, train/accuracy=0.992713, train/loss=0.023291, train/mean_average_precision=0.579840, validation/accuracy=0.987060, validation/loss=0.044751, validation/mean_average_precision=0.286002, validation/num_examples=43793
I0206 09:25:42.062789 139697948911360 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.08653607964515686, loss=0.027990158647298813
I0206 09:26:13.640499 139679759779584 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.06956010311841965, loss=0.025054356083273888
I0206 09:26:45.007482 139697948911360 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.07643107324838638, loss=0.029647773131728172
I0206 09:27:16.528838 139679759779584 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.06048646196722984, loss=0.028078604489564896
I0206 09:27:47.821531 139697948911360 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.08888338506221771, loss=0.02709825709462166
I0206 09:28:19.228758 139679759779584 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.06633561104536057, loss=0.029338717460632324
I0206 09:28:50.887846 139697948911360 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.0627686083316803, loss=0.024279655888676643
I0206 09:29:10.980183 139919816816448 spec.py:321] Evaluating on the training split.
I0206 09:30:52.098744 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 09:30:58.626915 139919816816448 spec.py:349] Evaluating on the test split.
I0206 09:31:02.022798 139919816816448 submission_runner.py:408] Time since start: 18978.82s, 	Step: 41565, 	{'train/accuracy': 0.9927103519439697, 'train/loss': 0.02319791540503502, 'train/mean_average_precision': 0.5746574374680143, 'validation/accuracy': 0.9871835708618164, 'validation/loss': 0.04461555927991867, 'validation/mean_average_precision': 0.28433454948306003, 'validation/num_examples': 43793, 'test/accuracy': 0.9863145351409912, 'test/loss': 0.04759086295962334, 'test/mean_average_precision': 0.2851083116733161, 'test/num_examples': 43793, 'score': 13217.97829079628, 'total_duration': 18978.816974401474, 'accumulated_submission_time': 13217.97829079628, 'accumulated_eval_time': 5757.701861858368, 'accumulated_logging_time': 1.9907326698303223}
I0206 09:31:02.067164 139716992366336 logging_writer.py:48] [41565] accumulated_eval_time=5757.701862, accumulated_logging_time=1.990733, accumulated_submission_time=13217.978291, global_step=41565, preemption_count=0, score=13217.978291, test/accuracy=0.986315, test/loss=0.047591, test/mean_average_precision=0.285108, test/num_examples=43793, total_duration=18978.816974, train/accuracy=0.992710, train/loss=0.023198, train/mean_average_precision=0.574657, validation/accuracy=0.987184, validation/loss=0.044616, validation/mean_average_precision=0.284335, validation/num_examples=43793
I0206 09:31:13.547581 139735302588160 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.08252718299627304, loss=0.02694113738834858
I0206 09:31:45.315059 139716992366336 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.07005629688501358, loss=0.028393657878041267
I0206 09:32:17.180702 139735302588160 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.08671771734952927, loss=0.030325451865792274
I0206 09:32:49.044578 139716992366336 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.06695008277893066, loss=0.027689095586538315
I0206 09:33:20.790475 139735302588160 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.06825770437717438, loss=0.024686625227332115
I0206 09:33:52.523115 139716992366336 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.07077502459287643, loss=0.027267364785075188
I0206 09:34:24.339666 139735302588160 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.07354006916284561, loss=0.02489987015724182
I0206 09:34:55.784630 139716992366336 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.07989872246980667, loss=0.0264640673995018
I0206 09:35:02.180402 139919816816448 spec.py:321] Evaluating on the training split.
I0206 09:36:47.517097 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 09:36:50.948811 139919816816448 spec.py:349] Evaluating on the test split.
I0206 09:36:54.253445 139919816816448 submission_runner.py:408] Time since start: 19331.05s, 	Step: 42321, 	{'train/accuracy': 0.9925374984741211, 'train/loss': 0.023752626031637192, 'train/mean_average_precision': 0.5571466336294107, 'validation/accuracy': 0.9870622158050537, 'validation/loss': 0.04495025426149368, 'validation/mean_average_precision': 0.2945649533936957, 'validation/num_examples': 43793, 'test/accuracy': 0.9861207604408264, 'test/loss': 0.04833002761006355, 'test/mean_average_precision': 0.27362558635591244, 'test/num_examples': 43793, 'score': 13458.056258440018, 'total_duration': 19331.047621250153, 'accumulated_submission_time': 13458.056258440018, 'accumulated_eval_time': 5869.774827003479, 'accumulated_logging_time': 2.050032138824463}
I0206 09:36:54.283253 139679759779584 logging_writer.py:48] [42321] accumulated_eval_time=5869.774827, accumulated_logging_time=2.050032, accumulated_submission_time=13458.056258, global_step=42321, preemption_count=0, score=13458.056258, test/accuracy=0.986121, test/loss=0.048330, test/mean_average_precision=0.273626, test/num_examples=43793, total_duration=19331.047621, train/accuracy=0.992537, train/loss=0.023753, train/mean_average_precision=0.557147, validation/accuracy=0.987062, validation/loss=0.044950, validation/mean_average_precision=0.294565, validation/num_examples=43793
I0206 09:37:20.101682 139758996317952 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.0891641229391098, loss=0.02717757411301136
I0206 09:37:52.286600 139679759779584 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.07413914054632187, loss=0.024229519069194794
I0206 09:38:24.910902 139758996317952 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.06448188424110413, loss=0.026293164119124413
I0206 09:38:57.068508 139679759779584 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.06820166856050491, loss=0.029021019116044044
I0206 09:39:29.145256 139758996317952 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.0734001025557518, loss=0.02597503922879696
I0206 09:40:01.187273 139679759779584 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.0727166160941124, loss=0.027326535433530807
I0206 09:40:33.250710 139758996317952 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.06808146834373474, loss=0.026160694658756256
I0206 09:40:54.350399 139919816816448 spec.py:321] Evaluating on the training split.
I0206 09:42:29.072100 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 09:42:32.108192 139919816816448 spec.py:349] Evaluating on the test split.
I0206 09:42:35.141683 139919816816448 submission_runner.py:408] Time since start: 19671.94s, 	Step: 43067, 	{'train/accuracy': 0.9924070835113525, 'train/loss': 0.02418462187051773, 'train/mean_average_precision': 0.5549188476915506, 'validation/accuracy': 0.9869388341903687, 'validation/loss': 0.04476182162761688, 'validation/mean_average_precision': 0.291296804280545, 'validation/num_examples': 43793, 'test/accuracy': 0.9861182570457458, 'test/loss': 0.04772588238120079, 'test/mean_average_precision': 0.28009756637386835, 'test/num_examples': 43793, 'score': 13698.089906215668, 'total_duration': 19671.935883522034, 'accumulated_submission_time': 13698.089906215668, 'accumulated_eval_time': 5970.566066265106, 'accumulated_logging_time': 2.0920493602752686}
I0206 09:42:35.165325 139697948911360 logging_writer.py:48] [43067] accumulated_eval_time=5970.566066, accumulated_logging_time=2.092049, accumulated_submission_time=13698.089906, global_step=43067, preemption_count=0, score=13698.089906, test/accuracy=0.986118, test/loss=0.047726, test/mean_average_precision=0.280098, test/num_examples=43793, total_duration=19671.935884, train/accuracy=0.992407, train/loss=0.024185, train/mean_average_precision=0.554919, validation/accuracy=0.986939, validation/loss=0.044762, validation/mean_average_precision=0.291297, validation/num_examples=43793
I0206 09:42:45.888981 139716992366336 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.07319194823503494, loss=0.023968487977981567
I0206 09:43:17.732148 139697948911360 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.06874672323465347, loss=0.02531994879245758
I0206 09:43:49.413990 139716992366336 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.07940837740898132, loss=0.027706395834684372
I0206 09:44:21.341362 139697948911360 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.09794384986162186, loss=0.025914903730154037
I0206 09:44:52.946548 139716992366336 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.080992192029953, loss=0.02693614922463894
I0206 09:45:25.421459 139697948911360 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.06365984678268433, loss=0.022458715364336967
I0206 09:45:57.355965 139716992366336 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.0768028125166893, loss=0.02643696777522564
I0206 09:46:28.892425 139697948911360 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.08038449287414551, loss=0.028185421600937843
I0206 09:46:35.437385 139919816816448 spec.py:321] Evaluating on the training split.
I0206 09:48:11.953544 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 09:48:14.977008 139919816816448 spec.py:349] Evaluating on the test split.
I0206 09:48:17.987622 139919816816448 submission_runner.py:408] Time since start: 20014.78s, 	Step: 43822, 	{'train/accuracy': 0.9925096035003662, 'train/loss': 0.023991784080863, 'train/mean_average_precision': 0.5581068188704152, 'validation/accuracy': 0.9870642423629761, 'validation/loss': 0.044694602489471436, 'validation/mean_average_precision': 0.28834239686739227, 'validation/num_examples': 43793, 'test/accuracy': 0.9862075448036194, 'test/loss': 0.04762270301580429, 'test/mean_average_precision': 0.27817579339649895, 'test/num_examples': 43793, 'score': 13938.330757379532, 'total_duration': 20014.78182053566, 'accumulated_submission_time': 13938.330757379532, 'accumulated_eval_time': 6073.116245508194, 'accumulated_logging_time': 2.1265196800231934}
I0206 09:48:18.011431 139679759779584 logging_writer.py:48] [43822] accumulated_eval_time=6073.116246, accumulated_logging_time=2.126520, accumulated_submission_time=13938.330757, global_step=43822, preemption_count=0, score=13938.330757, test/accuracy=0.986208, test/loss=0.047623, test/mean_average_precision=0.278176, test/num_examples=43793, total_duration=20014.781821, train/accuracy=0.992510, train/loss=0.023992, train/mean_average_precision=0.558107, validation/accuracy=0.987064, validation/loss=0.044695, validation/mean_average_precision=0.288342, validation/num_examples=43793
I0206 09:48:43.178796 139735302588160 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.07236278057098389, loss=0.024324605241417885
I0206 09:49:15.279719 139679759779584 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.07696525007486343, loss=0.028375426307320595
I0206 09:49:47.236082 139735302588160 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.06747879832983017, loss=0.02589135803282261
I0206 09:50:19.573544 139679759779584 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.08164332062005997, loss=0.03071843832731247
I0206 09:50:51.224636 139735302588160 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.10512775927782059, loss=0.028828006237745285
I0206 09:51:23.229043 139679759779584 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.08425749093294144, loss=0.02595594897866249
I0206 09:51:55.208609 139735302588160 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.07281523942947388, loss=0.025019459426403046
I0206 09:52:18.166014 139919816816448 spec.py:321] Evaluating on the training split.
I0206 09:53:55.069410 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 09:53:58.176937 139919816816448 spec.py:349] Evaluating on the test split.
I0206 09:54:01.156983 139919816816448 submission_runner.py:408] Time since start: 20357.95s, 	Step: 44573, 	{'train/accuracy': 0.9925292134284973, 'train/loss': 0.023891253396868706, 'train/mean_average_precision': 0.5533522608731953, 'validation/accuracy': 0.9870536923408508, 'validation/loss': 0.0445699468255043, 'validation/mean_average_precision': 0.2946664919696532, 'validation/num_examples': 43793, 'test/accuracy': 0.9861910939216614, 'test/loss': 0.047599904239177704, 'test/mean_average_precision': 0.2807915067602074, 'test/num_examples': 43793, 'score': 14178.45221710205, 'total_duration': 20357.951191186905, 'accumulated_submission_time': 14178.45221710205, 'accumulated_eval_time': 6176.107170343399, 'accumulated_logging_time': 2.1627137660980225}
I0206 09:54:01.181030 139697948911360 logging_writer.py:48] [44573] accumulated_eval_time=6176.107170, accumulated_logging_time=2.162714, accumulated_submission_time=14178.452217, global_step=44573, preemption_count=0, score=14178.452217, test/accuracy=0.986191, test/loss=0.047600, test/mean_average_precision=0.280792, test/num_examples=43793, total_duration=20357.951191, train/accuracy=0.992529, train/loss=0.023891, train/mean_average_precision=0.553352, validation/accuracy=0.987054, validation/loss=0.044570, validation/mean_average_precision=0.294666, validation/num_examples=43793
I0206 09:54:10.194380 139758996317952 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.06872965395450592, loss=0.026126932352781296
I0206 09:54:41.726380 139697948911360 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.07570584118366241, loss=0.02389567345380783
I0206 09:55:13.473722 139758996317952 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.06927350163459778, loss=0.022533206269145012
I0206 09:55:45.427860 139697948911360 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.07900559157133102, loss=0.02670949697494507
I0206 09:56:17.748198 139758996317952 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.0909382551908493, loss=0.02659093216061592
I0206 09:56:49.468419 139697948911360 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.08472678810358047, loss=0.027125222608447075
I0206 09:57:21.069258 139758996317952 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.06579417735338211, loss=0.027465086430311203
I0206 09:57:52.660587 139697948911360 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.07220897078514099, loss=0.02399793267250061
I0206 09:58:01.414207 139919816816448 spec.py:321] Evaluating on the training split.
I0206 09:59:32.284700 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 09:59:35.377386 139919816816448 spec.py:349] Evaluating on the test split.
I0206 09:59:38.402139 139919816816448 submission_runner.py:408] Time since start: 20695.20s, 	Step: 45328, 	{'train/accuracy': 0.9924713969230652, 'train/loss': 0.02383287064731121, 'train/mean_average_precision': 0.5691807750617297, 'validation/accuracy': 0.9870431423187256, 'validation/loss': 0.044838715344667435, 'validation/mean_average_precision': 0.2838232387063066, 'validation/num_examples': 43793, 'test/accuracy': 0.9862349033355713, 'test/loss': 0.04757899418473244, 'test/mean_average_precision': 0.2797156681465258, 'test/num_examples': 43793, 'score': 14418.654555082321, 'total_duration': 20695.19634771347, 'accumulated_submission_time': 14418.654555082321, 'accumulated_eval_time': 6273.095078229904, 'accumulated_logging_time': 2.1975855827331543}
I0206 09:59:38.425843 139679759779584 logging_writer.py:48] [45328] accumulated_eval_time=6273.095078, accumulated_logging_time=2.197586, accumulated_submission_time=14418.654555, global_step=45328, preemption_count=0, score=14418.654555, test/accuracy=0.986235, test/loss=0.047579, test/mean_average_precision=0.279716, test/num_examples=43793, total_duration=20695.196348, train/accuracy=0.992471, train/loss=0.023833, train/mean_average_precision=0.569181, validation/accuracy=0.987043, validation/loss=0.044839, validation/mean_average_precision=0.283823, validation/num_examples=43793
I0206 10:00:01.173247 139735302588160 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.08167349547147751, loss=0.025229714810848236
I0206 10:00:33.106914 139679759779584 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.06978429853916168, loss=0.02563498541712761
I0206 10:01:04.444906 139735302588160 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.09300833940505981, loss=0.028550487011671066
I0206 10:01:35.908630 139679759779584 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.07416687905788422, loss=0.024003876373171806
I0206 10:02:07.010144 139735302588160 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.09177756309509277, loss=0.02926046960055828
I0206 10:02:38.450732 139679759779584 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.08417577296495438, loss=0.03041347675025463
I0206 10:03:09.468930 139735302588160 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.07487622648477554, loss=0.02634657360613346
I0206 10:03:38.417944 139919816816448 spec.py:321] Evaluating on the training split.
I0206 10:05:18.353049 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 10:05:21.479862 139919816816448 spec.py:349] Evaluating on the test split.
I0206 10:05:24.561506 139919816816448 submission_runner.py:408] Time since start: 21041.36s, 	Step: 46094, 	{'train/accuracy': 0.9926730990409851, 'train/loss': 0.0232164915651083, 'train/mean_average_precision': 0.5673038654986291, 'validation/accuracy': 0.9871312379837036, 'validation/loss': 0.044832371175289154, 'validation/mean_average_precision': 0.29277687646388556, 'validation/num_examples': 43793, 'test/accuracy': 0.9862656593322754, 'test/loss': 0.04793919250369072, 'test/mean_average_precision': 0.2842984568305961, 'test/num_examples': 43793, 'score': 14658.615586519241, 'total_duration': 21041.355713129044, 'accumulated_submission_time': 14658.615586519241, 'accumulated_eval_time': 6379.238595485687, 'accumulated_logging_time': 2.2321431636810303}
I0206 10:05:24.585625 139697948911360 logging_writer.py:48] [46094] accumulated_eval_time=6379.238595, accumulated_logging_time=2.232143, accumulated_submission_time=14658.615587, global_step=46094, preemption_count=0, score=14658.615587, test/accuracy=0.986266, test/loss=0.047939, test/mean_average_precision=0.284298, test/num_examples=43793, total_duration=21041.355713, train/accuracy=0.992673, train/loss=0.023216, train/mean_average_precision=0.567304, validation/accuracy=0.987131, validation/loss=0.044832, validation/mean_average_precision=0.292777, validation/num_examples=43793
I0206 10:05:26.816860 139716992366336 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.091232068836689, loss=0.024221738800406456
I0206 10:05:58.599824 139697948911360 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.0793057531118393, loss=0.025388631969690323
I0206 10:06:30.716994 139716992366336 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.08670111000537872, loss=0.02754424698650837
I0206 10:07:03.015307 139697948911360 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.08664011210203171, loss=0.02325751818716526
I0206 10:07:34.684214 139716992366336 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.08093655854463577, loss=0.02303009107708931
I0206 10:08:06.652998 139697948911360 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.09070646017789841, loss=0.02773159183561802
I0206 10:08:40.479469 139716992366336 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.06881768256425858, loss=0.026062751188874245
I0206 10:09:12.257736 139697948911360 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.11531351506710052, loss=0.02736484259366989
I0206 10:09:24.702849 139919816816448 spec.py:321] Evaluating on the training split.
I0206 10:11:00.069887 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 10:11:03.318207 139919816816448 spec.py:349] Evaluating on the test split.
I0206 10:11:06.382584 139919816816448 submission_runner.py:408] Time since start: 21383.18s, 	Step: 46841, 	{'train/accuracy': 0.9926999807357788, 'train/loss': 0.022925419732928276, 'train/mean_average_precision': 0.5850914346518163, 'validation/accuracy': 0.9871032238006592, 'validation/loss': 0.0455067902803421, 'validation/mean_average_precision': 0.2930319312146007, 'validation/num_examples': 43793, 'test/accuracy': 0.9862319827079773, 'test/loss': 0.04854682832956314, 'test/mean_average_precision': 0.2778092930415843, 'test/num_examples': 43793, 'score': 14898.701581716537, 'total_duration': 21383.176791906357, 'accumulated_submission_time': 14898.701581716537, 'accumulated_eval_time': 6480.91828584671, 'accumulated_logging_time': 2.2671091556549072}
I0206 10:11:06.407068 139679759779584 logging_writer.py:48] [46841] accumulated_eval_time=6480.918286, accumulated_logging_time=2.267109, accumulated_submission_time=14898.701582, global_step=46841, preemption_count=0, score=14898.701582, test/accuracy=0.986232, test/loss=0.048547, test/mean_average_precision=0.277809, test/num_examples=43793, total_duration=21383.176792, train/accuracy=0.992700, train/loss=0.022925, train/mean_average_precision=0.585091, validation/accuracy=0.987103, validation/loss=0.045507, validation/mean_average_precision=0.293032, validation/num_examples=43793
I0206 10:11:25.562835 139735302588160 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.07141381502151489, loss=0.02450421452522278
I0206 10:11:57.086451 139679759779584 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.07915052026510239, loss=0.025566738098859787
I0206 10:12:29.256571 139735302588160 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.07928897440433502, loss=0.027144527062773705
I0206 10:13:01.176746 139679759779584 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.0736183449625969, loss=0.02407895028591156
I0206 10:13:33.104248 139735302588160 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.08320058882236481, loss=0.026586735621094704
I0206 10:14:05.017906 139679759779584 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.09341572970151901, loss=0.026212027296423912
I0206 10:14:36.996718 139735302588160 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.09050558507442474, loss=0.02543989010155201
I0206 10:15:06.405709 139919816816448 spec.py:321] Evaluating on the training split.
I0206 10:16:42.748268 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 10:16:45.909975 139919816816448 spec.py:349] Evaluating on the test split.
I0206 10:16:48.993408 139919816816448 submission_runner.py:408] Time since start: 21725.79s, 	Step: 47593, 	{'train/accuracy': 0.992985725402832, 'train/loss': 0.022269995883107185, 'train/mean_average_precision': 0.5896478412213515, 'validation/accuracy': 0.9871113300323486, 'validation/loss': 0.04491627588868141, 'validation/mean_average_precision': 0.2926934946306269, 'validation/num_examples': 43793, 'test/accuracy': 0.9862273335456848, 'test/loss': 0.04789400100708008, 'test/mean_average_precision': 0.2869319142049717, 'test/num_examples': 43793, 'score': 15138.667674064636, 'total_duration': 21725.787615060806, 'accumulated_submission_time': 15138.667674064636, 'accumulated_eval_time': 6583.505940437317, 'accumulated_logging_time': 2.3025686740875244}
I0206 10:16:49.017564 139716992366336 logging_writer.py:48] [47593] accumulated_eval_time=6583.505940, accumulated_logging_time=2.302569, accumulated_submission_time=15138.667674, global_step=47593, preemption_count=0, score=15138.667674, test/accuracy=0.986227, test/loss=0.047894, test/mean_average_precision=0.286932, test/num_examples=43793, total_duration=21725.787615, train/accuracy=0.992986, train/loss=0.022270, train/mean_average_precision=0.589648, validation/accuracy=0.987111, validation/loss=0.044916, validation/mean_average_precision=0.292693, validation/num_examples=43793
I0206 10:16:51.595939 139758996317952 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.08460386097431183, loss=0.025183582678437233
I0206 10:17:23.349027 139716992366336 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.08929210901260376, loss=0.028993908315896988
I0206 10:17:55.088150 139758996317952 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.07661526650190353, loss=0.025663794949650764
I0206 10:18:26.753562 139716992366336 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.08369654417037964, loss=0.025621632114052773
I0206 10:18:58.355341 139758996317952 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.07975830882787704, loss=0.025389304384589195
I0206 10:19:29.895691 139716992366336 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.0649515688419342, loss=0.020963478833436966
I0206 10:20:01.731391 139758996317952 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.07394249737262726, loss=0.023127738386392593
I0206 10:20:33.333334 139716992366336 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.0684753879904747, loss=0.025238411501049995
I0206 10:20:49.135933 139919816816448 spec.py:321] Evaluating on the training split.
I0206 10:22:23.050102 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 10:22:26.132830 139919816816448 spec.py:349] Evaluating on the test split.
I0206 10:22:29.187176 139919816816448 submission_runner.py:408] Time since start: 22065.98s, 	Step: 48351, 	{'train/accuracy': 0.9932321310043335, 'train/loss': 0.021475225687026978, 'train/mean_average_precision': 0.6224910459932752, 'validation/accuracy': 0.9870350360870361, 'validation/loss': 0.04480258747935295, 'validation/mean_average_precision': 0.2923386664862276, 'validation/num_examples': 43793, 'test/accuracy': 0.986178457736969, 'test/loss': 0.04778258129954338, 'test/mean_average_precision': 0.28342984384527276, 'test/num_examples': 43793, 'score': 15378.75329375267, 'total_duration': 22065.98137497902, 'accumulated_submission_time': 15378.75329375267, 'accumulated_eval_time': 6683.557134151459, 'accumulated_logging_time': 2.3388681411743164}
I0206 10:22:29.228715 139679759779584 logging_writer.py:48] [48351] accumulated_eval_time=6683.557134, accumulated_logging_time=2.338868, accumulated_submission_time=15378.753294, global_step=48351, preemption_count=0, score=15378.753294, test/accuracy=0.986178, test/loss=0.047783, test/mean_average_precision=0.283430, test/num_examples=43793, total_duration=22065.981375, train/accuracy=0.993232, train/loss=0.021475, train/mean_average_precision=0.622491, validation/accuracy=0.987035, validation/loss=0.044803, validation/mean_average_precision=0.292339, validation/num_examples=43793
I0206 10:22:45.136364 139697948911360 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.08255898952484131, loss=0.029060328379273415
I0206 10:23:16.892856 139679759779584 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.0778476670384407, loss=0.023945247754454613
I0206 10:23:48.444682 139697948911360 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.08843362331390381, loss=0.024695131927728653
I0206 10:24:20.001317 139679759779584 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.101069375872612, loss=0.0265364870429039
I0206 10:24:51.753771 139697948911360 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.06921426951885223, loss=0.02344501204788685
I0206 10:25:23.586623 139679759779584 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.09040958434343338, loss=0.02292264997959137
I0206 10:25:55.110105 139697948911360 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.10174254328012466, loss=0.02793230302631855
I0206 10:26:26.845797 139679759779584 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.10775704681873322, loss=0.026568828150629997
I0206 10:26:29.400348 139919816816448 spec.py:321] Evaluating on the training split.
I0206 10:28:00.871847 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 10:28:04.021236 139919816816448 spec.py:349] Evaluating on the test split.
I0206 10:28:09.444982 139919816816448 submission_runner.py:408] Time since start: 22406.24s, 	Step: 49109, 	{'train/accuracy': 0.9933977723121643, 'train/loss': 0.021004248410463333, 'train/mean_average_precision': 0.6172502556198101, 'validation/accuracy': 0.9870760440826416, 'validation/loss': 0.04562529921531677, 'validation/mean_average_precision': 0.29262855930217957, 'validation/num_examples': 43793, 'test/accuracy': 0.9862424731254578, 'test/loss': 0.04866080731153488, 'test/mean_average_precision': 0.28572762484789316, 'test/num_examples': 43793, 'score': 15618.89215707779, 'total_duration': 22406.239191532135, 'accumulated_submission_time': 15618.89215707779, 'accumulated_eval_time': 6783.601722478867, 'accumulated_logging_time': 2.3929810523986816}
I0206 10:28:09.471036 139735302588160 logging_writer.py:48] [49109] accumulated_eval_time=6783.601722, accumulated_logging_time=2.392981, accumulated_submission_time=15618.892157, global_step=49109, preemption_count=0, score=15618.892157, test/accuracy=0.986242, test/loss=0.048661, test/mean_average_precision=0.285728, test/num_examples=43793, total_duration=22406.239192, train/accuracy=0.993398, train/loss=0.021004, train/mean_average_precision=0.617250, validation/accuracy=0.987076, validation/loss=0.045625, validation/mean_average_precision=0.292629, validation/num_examples=43793
I0206 10:28:38.574686 139758996317952 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.07931051403284073, loss=0.024223728105425835
I0206 10:29:10.486045 139735302588160 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.08958401530981064, loss=0.02470872364938259
I0206 10:29:42.127837 139758996317952 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.0847918763756752, loss=0.02534106932580471
I0206 10:30:13.860258 139735302588160 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.09640565514564514, loss=0.02349773421883583
I0206 10:30:45.668226 139758996317952 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.08865409344434738, loss=0.025854356586933136
I0206 10:31:17.281430 139735302588160 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.07091499865055084, loss=0.02280675433576107
I0206 10:31:49.578746 139758996317952 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.08699018508195877, loss=0.02525673247873783
I0206 10:32:09.725577 139919816816448 spec.py:321] Evaluating on the training split.
I0206 10:33:44.264548 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 10:33:47.277330 139919816816448 spec.py:349] Evaluating on the test split.
I0206 10:33:50.238146 139919816816448 submission_runner.py:408] Time since start: 22747.03s, 	Step: 49864, 	{'train/accuracy': 0.9931698441505432, 'train/loss': 0.02152334712445736, 'train/mean_average_precision': 0.6213504048826404, 'validation/accuracy': 0.9871621131896973, 'validation/loss': 0.045335717499256134, 'validation/mean_average_precision': 0.2962699552477201, 'validation/num_examples': 43793, 'test/accuracy': 0.9862942695617676, 'test/loss': 0.048360370099544525, 'test/mean_average_precision': 0.2828741073363153, 'test/num_examples': 43793, 'score': 15859.115253686905, 'total_duration': 22747.032354831696, 'accumulated_submission_time': 15859.115253686905, 'accumulated_eval_time': 6884.114263057709, 'accumulated_logging_time': 2.4300451278686523}
I0206 10:33:50.263342 139697948911360 logging_writer.py:48] [49864] accumulated_eval_time=6884.114263, accumulated_logging_time=2.430045, accumulated_submission_time=15859.115254, global_step=49864, preemption_count=0, score=15859.115254, test/accuracy=0.986294, test/loss=0.048360, test/mean_average_precision=0.282874, test/num_examples=43793, total_duration=22747.032355, train/accuracy=0.993170, train/loss=0.021523, train/mean_average_precision=0.621350, validation/accuracy=0.987162, validation/loss=0.045336, validation/mean_average_precision=0.296270, validation/num_examples=43793
I0206 10:34:01.865723 139716992366336 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.08195437490940094, loss=0.023640340194106102
I0206 10:34:33.832617 139697948911360 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.09395556896924973, loss=0.02300335094332695
I0206 10:35:05.564895 139716992366336 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.09883339703083038, loss=0.023475496098399162
I0206 10:35:37.097693 139697948911360 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.08622727543115616, loss=0.02679501473903656
I0206 10:36:09.010861 139716992366336 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.08578126132488251, loss=0.024055443704128265
I0206 10:36:40.792153 139697948911360 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.09129674732685089, loss=0.0248098261654377
I0206 10:37:12.721048 139716992366336 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.0872139185667038, loss=0.025873597711324692
I0206 10:37:44.054636 139697948911360 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.08550500869750977, loss=0.021300792694091797
I0206 10:37:50.370764 139919816816448 spec.py:321] Evaluating on the training split.
I0206 10:39:27.439326 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 10:39:30.552050 139919816816448 spec.py:349] Evaluating on the test split.
I0206 10:39:33.645603 139919816816448 submission_runner.py:408] Time since start: 23090.44s, 	Step: 50621, 	{'train/accuracy': 0.9931451082229614, 'train/loss': 0.021721987053751945, 'train/mean_average_precision': 0.5996855145951254, 'validation/accuracy': 0.9870354533195496, 'validation/loss': 0.04542071744799614, 'validation/mean_average_precision': 0.2876952709587337, 'validation/num_examples': 43793, 'test/accuracy': 0.9862441420555115, 'test/loss': 0.04850063845515251, 'test/mean_average_precision': 0.2809669364114627, 'test/num_examples': 43793, 'score': 16099.191632032394, 'total_duration': 23090.439814329147, 'accumulated_submission_time': 16099.191632032394, 'accumulated_eval_time': 6987.389058351517, 'accumulated_logging_time': 2.4660558700561523}
I0206 10:39:33.670916 139735302588160 logging_writer.py:48] [50621] accumulated_eval_time=6987.389058, accumulated_logging_time=2.466056, accumulated_submission_time=16099.191632, global_step=50621, preemption_count=0, score=16099.191632, test/accuracy=0.986244, test/loss=0.048501, test/mean_average_precision=0.280967, test/num_examples=43793, total_duration=23090.439814, train/accuracy=0.993145, train/loss=0.021722, train/mean_average_precision=0.599686, validation/accuracy=0.987035, validation/loss=0.045421, validation/mean_average_precision=0.287695, validation/num_examples=43793
I0206 10:39:59.422508 139758996317952 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.09064903110265732, loss=0.02485538087785244
I0206 10:40:31.099991 139735302588160 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.09463632851839066, loss=0.027086656540632248
I0206 10:41:02.433201 139758996317952 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.08290350437164307, loss=0.02346784807741642
I0206 10:41:34.238377 139735302588160 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.08246871829032898, loss=0.023780563846230507
I0206 10:42:05.958881 139758996317952 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.08745382726192474, loss=0.02530020847916603
I0206 10:42:37.410355 139735302588160 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.08141431212425232, loss=0.02330799028277397
I0206 10:43:08.813336 139758996317952 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.100755974650383, loss=0.027055665850639343
I0206 10:43:33.860874 139919816816448 spec.py:321] Evaluating on the training split.
I0206 10:45:14.014933 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 10:45:17.089427 139919816816448 spec.py:349] Evaluating on the test split.
I0206 10:45:20.058439 139919816816448 submission_runner.py:408] Time since start: 23436.85s, 	Step: 51379, 	{'train/accuracy': 0.9931361675262451, 'train/loss': 0.021795811131596565, 'train/mean_average_precision': 0.6021560760301627, 'validation/accuracy': 0.9870212078094482, 'validation/loss': 0.04552444443106651, 'validation/mean_average_precision': 0.29312871920614536, 'validation/num_examples': 43793, 'test/accuracy': 0.9861915111541748, 'test/loss': 0.048558417707681656, 'test/mean_average_precision': 0.27908030634769676, 'test/num_examples': 43793, 'score': 16339.3504114151, 'total_duration': 23436.852647542953, 'accumulated_submission_time': 16339.3504114151, 'accumulated_eval_time': 7093.5865795612335, 'accumulated_logging_time': 2.5020413398742676}
I0206 10:45:20.084689 139679759779584 logging_writer.py:48] [51379] accumulated_eval_time=7093.586580, accumulated_logging_time=2.502041, accumulated_submission_time=16339.350411, global_step=51379, preemption_count=0, score=16339.350411, test/accuracy=0.986192, test/loss=0.048558, test/mean_average_precision=0.279080, test/num_examples=43793, total_duration=23436.852648, train/accuracy=0.993136, train/loss=0.021796, train/mean_average_precision=0.602156, validation/accuracy=0.987021, validation/loss=0.045524, validation/mean_average_precision=0.293129, validation/num_examples=43793
I0206 10:45:27.194797 139697948911360 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.09646530449390411, loss=0.025651240721344948
I0206 10:45:58.830157 139679759779584 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.08637914806604385, loss=0.026250628754496574
I0206 10:46:30.772258 139697948911360 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.09284880012273788, loss=0.021891092881560326
I0206 10:47:02.521799 139679759779584 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.09115295112133026, loss=0.024277372285723686
I0206 10:47:34.007168 139697948911360 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.10868804901838303, loss=0.027768796309828758
I0206 10:48:05.655266 139679759779584 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.11665583401918411, loss=0.027571648359298706
I0206 10:48:36.980699 139697948911360 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.10489863157272339, loss=0.026764655485749245
I0206 10:49:08.513039 139679759779584 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.09013111889362335, loss=0.0271401759237051
I0206 10:49:20.207336 139919816816448 spec.py:321] Evaluating on the training split.
I0206 10:50:58.238937 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 10:51:01.338168 139919816816448 spec.py:349] Evaluating on the test split.
I0206 10:51:04.340322 139919816816448 submission_runner.py:408] Time since start: 23781.13s, 	Step: 52138, 	{'train/accuracy': 0.9931796193122864, 'train/loss': 0.02150198630988598, 'train/mean_average_precision': 0.6067070557150565, 'validation/accuracy': 0.9871442317962646, 'validation/loss': 0.04563544690608978, 'validation/mean_average_precision': 0.293471528182527, 'validation/num_examples': 43793, 'test/accuracy': 0.9862989187240601, 'test/loss': 0.04884331300854683, 'test/mean_average_precision': 0.2831817590489205, 'test/num_examples': 43793, 'score': 16579.44235610962, 'total_duration': 23781.13451218605, 'accumulated_submission_time': 16579.44235610962, 'accumulated_eval_time': 7197.719510793686, 'accumulated_logging_time': 2.5390548706054688}
I0206 10:51:04.365789 139716992366336 logging_writer.py:48] [52138] accumulated_eval_time=7197.719511, accumulated_logging_time=2.539055, accumulated_submission_time=16579.442356, global_step=52138, preemption_count=0, score=16579.442356, test/accuracy=0.986299, test/loss=0.048843, test/mean_average_precision=0.283182, test/num_examples=43793, total_duration=23781.134512, train/accuracy=0.993180, train/loss=0.021502, train/mean_average_precision=0.606707, validation/accuracy=0.987144, validation/loss=0.045635, validation/mean_average_precision=0.293472, validation/num_examples=43793
I0206 10:51:24.233407 139735302588160 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.07726331800222397, loss=0.022702747955918312
I0206 10:51:55.933849 139716992366336 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.08592936396598816, loss=0.021889783442020416
I0206 10:52:27.484551 139735302588160 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.08405906707048416, loss=0.023279495537281036
I0206 10:52:59.202877 139716992366336 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.09405531734228134, loss=0.023276368156075478
I0206 10:53:30.737194 139735302588160 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.10152245312929153, loss=0.027193421497941017
I0206 10:54:02.945939 139716992366336 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.10373026132583618, loss=0.025759577751159668
I0206 10:54:34.315249 139735302588160 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.09524182975292206, loss=0.023811733350157738
I0206 10:55:04.375581 139919816816448 spec.py:321] Evaluating on the training split.
I0206 10:56:39.422443 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 10:56:44.894511 139919816816448 spec.py:349] Evaluating on the test split.
I0206 10:56:47.883579 139919816816448 submission_runner.py:408] Time since start: 24124.68s, 	Step: 52896, 	{'train/accuracy': 0.9931758046150208, 'train/loss': 0.021432511508464813, 'train/mean_average_precision': 0.6115915335475446, 'validation/accuracy': 0.9871913194656372, 'validation/loss': 0.04552037641406059, 'validation/mean_average_precision': 0.3000649690933642, 'validation/num_examples': 43793, 'test/accuracy': 0.9863018989562988, 'test/loss': 0.048905059695243835, 'test/mean_average_precision': 0.2820189862990282, 'test/num_examples': 43793, 'score': 16819.4209959507, 'total_duration': 24124.677789211273, 'accumulated_submission_time': 16819.4209959507, 'accumulated_eval_time': 7301.227471590042, 'accumulated_logging_time': 2.5754523277282715}
I0206 10:56:47.910145 139679759779584 logging_writer.py:48] [52896] accumulated_eval_time=7301.227472, accumulated_logging_time=2.575452, accumulated_submission_time=16819.420996, global_step=52896, preemption_count=0, score=16819.420996, test/accuracy=0.986302, test/loss=0.048905, test/mean_average_precision=0.282019, test/num_examples=43793, total_duration=24124.677789, train/accuracy=0.993176, train/loss=0.021433, train/mean_average_precision=0.611592, validation/accuracy=0.987191, validation/loss=0.045520, validation/mean_average_precision=0.300065, validation/num_examples=43793
I0206 10:56:49.505480 139758996317952 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.08853965252637863, loss=0.024944940581917763
I0206 10:57:21.350539 139679759779584 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.09027538448572159, loss=0.022784925997257233
I0206 10:57:53.274732 139758996317952 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.09232980757951736, loss=0.021585622802376747
I0206 10:58:25.054654 139679759779584 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.09341096878051758, loss=0.024624474346637726
I0206 10:58:56.695685 139758996317952 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.11928395926952362, loss=0.02356182597577572
I0206 10:59:28.505819 139679759779584 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.09449079632759094, loss=0.01977924443781376
I0206 11:00:00.174750 139758996317952 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.09557702392339706, loss=0.026267090812325478
I0206 11:00:32.170590 139679759779584 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.09448089450597763, loss=0.024952327832579613
I0206 11:00:47.967632 139919816816448 spec.py:321] Evaluating on the training split.
I0206 11:02:21.679896 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 11:02:24.732376 139919816816448 spec.py:349] Evaluating on the test split.
I0206 11:02:27.831161 139919816816448 submission_runner.py:408] Time since start: 24464.63s, 	Step: 53651, 	{'train/accuracy': 0.9933634400367737, 'train/loss': 0.02081306278705597, 'train/mean_average_precision': 0.6317767925373252, 'validation/accuracy': 0.987166166305542, 'validation/loss': 0.045723769813776016, 'validation/mean_average_precision': 0.2988973768460285, 'validation/num_examples': 43793, 'test/accuracy': 0.9862803816795349, 'test/loss': 0.048975300043821335, 'test/mean_average_precision': 0.28024822331556093, 'test/num_examples': 43793, 'score': 17059.447257995605, 'total_duration': 24464.625368595123, 'accumulated_submission_time': 17059.447257995605, 'accumulated_eval_time': 7401.090955257416, 'accumulated_logging_time': 2.6128950119018555}
I0206 11:02:27.857487 139697948911360 logging_writer.py:48] [53651] accumulated_eval_time=7401.090955, accumulated_logging_time=2.612895, accumulated_submission_time=17059.447258, global_step=53651, preemption_count=0, score=17059.447258, test/accuracy=0.986280, test/loss=0.048975, test/mean_average_precision=0.280248, test/num_examples=43793, total_duration=24464.625369, train/accuracy=0.993363, train/loss=0.020813, train/mean_average_precision=0.631777, validation/accuracy=0.987166, validation/loss=0.045724, validation/mean_average_precision=0.298897, validation/num_examples=43793
I0206 11:02:43.952338 139716992366336 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.12395328283309937, loss=0.023063795641064644
I0206 11:03:15.833068 139697948911360 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.1068747416138649, loss=0.0251716710627079
I0206 11:03:47.533007 139716992366336 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.08569029718637466, loss=0.02276248298585415
I0206 11:04:19.532958 139697948911360 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.13391494750976562, loss=0.024559099227190018
I0206 11:04:51.291744 139716992366336 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.10159527510404587, loss=0.02282049134373665
I0206 11:05:22.790787 139697948911360 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.0937112495303154, loss=0.022980131208896637
I0206 11:05:54.682318 139716992366336 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.08929150551557541, loss=0.023369714617729187
I0206 11:06:27.242340 139697948911360 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.08972548693418503, loss=0.02344100922346115
I0206 11:06:27.924549 139919816816448 spec.py:321] Evaluating on the training split.
I0206 11:08:03.318221 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 11:08:06.405191 139919816816448 spec.py:349] Evaluating on the test split.
I0206 11:08:09.471388 139919816816448 submission_runner.py:408] Time since start: 24806.27s, 	Step: 54403, 	{'train/accuracy': 0.9933775663375854, 'train/loss': 0.020713327452540398, 'train/mean_average_precision': 0.6225736436664356, 'validation/accuracy': 0.987188458442688, 'validation/loss': 0.0462249219417572, 'validation/mean_average_precision': 0.29682682278138706, 'validation/num_examples': 43793, 'test/accuracy': 0.986240804195404, 'test/loss': 0.04948662593960762, 'test/mean_average_precision': 0.2819279003808288, 'test/num_examples': 43793, 'score': 17299.483036756516, 'total_duration': 24806.26559662819, 'accumulated_submission_time': 17299.483036756516, 'accumulated_eval_time': 7502.6377511024475, 'accumulated_logging_time': 2.65012526512146}
I0206 11:08:09.497249 139679759779584 logging_writer.py:48] [54403] accumulated_eval_time=7502.637751, accumulated_logging_time=2.650125, accumulated_submission_time=17299.483037, global_step=54403, preemption_count=0, score=17299.483037, test/accuracy=0.986241, test/loss=0.049487, test/mean_average_precision=0.281928, test/num_examples=43793, total_duration=24806.265597, train/accuracy=0.993378, train/loss=0.020713, train/mean_average_precision=0.622574, validation/accuracy=0.987188, validation/loss=0.046225, validation/mean_average_precision=0.296827, validation/num_examples=43793
I0206 11:08:41.338116 139758996317952 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.1058977022767067, loss=0.02352151647210121
I0206 11:09:13.177877 139679759779584 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.09742816537618637, loss=0.024493275210261345
I0206 11:09:44.817350 139758996317952 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.0938539206981659, loss=0.024857476353645325
I0206 11:10:16.562888 139679759779584 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.10142762959003448, loss=0.024285098537802696
I0206 11:10:48.118639 139758996317952 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.09571627527475357, loss=0.022053761407732964
I0206 11:11:19.965346 139679759779584 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.10791268944740295, loss=0.02285599149763584
I0206 11:11:51.787537 139758996317952 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.09626105427742004, loss=0.022911913692951202
I0206 11:12:09.508057 139919816816448 spec.py:321] Evaluating on the training split.
I0206 11:13:43.228956 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 11:13:46.394040 139919816816448 spec.py:349] Evaluating on the test split.
I0206 11:13:49.449427 139919816816448 submission_runner.py:408] Time since start: 25146.24s, 	Step: 55156, 	{'train/accuracy': 0.993545413017273, 'train/loss': 0.020183144137263298, 'train/mean_average_precision': 0.645784824725203, 'validation/accuracy': 0.9871426224708557, 'validation/loss': 0.04620135575532913, 'validation/mean_average_precision': 0.2940088533353942, 'validation/num_examples': 43793, 'test/accuracy': 0.9862689971923828, 'test/loss': 0.04963694140315056, 'test/mean_average_precision': 0.2751525052977334, 'test/num_examples': 43793, 'score': 17539.461524248123, 'total_duration': 25146.24363541603, 'accumulated_submission_time': 17539.461524248123, 'accumulated_eval_time': 7602.579082250595, 'accumulated_logging_time': 2.6879782676696777}
I0206 11:13:49.475033 139697948911360 logging_writer.py:48] [55156] accumulated_eval_time=7602.579082, accumulated_logging_time=2.687978, accumulated_submission_time=17539.461524, global_step=55156, preemption_count=0, score=17539.461524, test/accuracy=0.986269, test/loss=0.049637, test/mean_average_precision=0.275153, test/num_examples=43793, total_duration=25146.243635, train/accuracy=0.993545, train/loss=0.020183, train/mean_average_precision=0.645785, validation/accuracy=0.987143, validation/loss=0.046201, validation/mean_average_precision=0.294009, validation/num_examples=43793
I0206 11:14:03.883456 139716992366336 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.10437034070491791, loss=0.02421589381992817
I0206 11:14:35.167705 139697948911360 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.08331917971372604, loss=0.021405069157481194
I0206 11:15:06.608338 139716992366336 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.11721713095903397, loss=0.023161491379141808
I0206 11:15:37.795201 139697948911360 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.10260016471147537, loss=0.024146266281604767
I0206 11:16:09.330770 139716992366336 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.09237010776996613, loss=0.02105981856584549
I0206 11:16:40.759996 139697948911360 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.0975642278790474, loss=0.023709608241915703
I0206 11:17:11.909552 139716992366336 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.0867011770606041, loss=0.021600309759378433
I0206 11:17:43.225863 139697948911360 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.12431345134973526, loss=0.021741289645433426
I0206 11:17:49.456659 139919816816448 spec.py:321] Evaluating on the training split.
I0206 11:19:22.458821 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 11:19:25.457472 139919816816448 spec.py:349] Evaluating on the test split.
I0206 11:19:28.373720 139919816816448 submission_runner.py:408] Time since start: 25485.17s, 	Step: 55921, 	{'train/accuracy': 0.9938235282897949, 'train/loss': 0.019473331049084663, 'train/mean_average_precision': 0.6602483439614082, 'validation/accuracy': 0.9870220422744751, 'validation/loss': 0.04617263376712799, 'validation/mean_average_precision': 0.29621272627443535, 'validation/num_examples': 43793, 'test/accuracy': 0.9861751198768616, 'test/loss': 0.04953019693493843, 'test/mean_average_precision': 0.278072814459413, 'test/num_examples': 43793, 'score': 17779.411629915237, 'total_duration': 25485.167813539505, 'accumulated_submission_time': 17779.411629915237, 'accumulated_eval_time': 7701.495981454849, 'accumulated_logging_time': 2.7243547439575195}
I0206 11:19:28.399114 139679759779584 logging_writer.py:48] [55921] accumulated_eval_time=7701.495981, accumulated_logging_time=2.724355, accumulated_submission_time=17779.411630, global_step=55921, preemption_count=0, score=17779.411630, test/accuracy=0.986175, test/loss=0.049530, test/mean_average_precision=0.278073, test/num_examples=43793, total_duration=25485.167814, train/accuracy=0.993824, train/loss=0.019473, train/mean_average_precision=0.660248, validation/accuracy=0.987022, validation/loss=0.046173, validation/mean_average_precision=0.296213, validation/num_examples=43793
I0206 11:19:53.654116 139735302588160 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.10682768374681473, loss=0.023299681022763252
I0206 11:20:25.192302 139679759779584 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.09909317642450333, loss=0.020482754334807396
I0206 11:20:56.500691 139735302588160 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.09812185913324356, loss=0.02357988990843296
I0206 11:21:27.777439 139679759779584 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.10307395458221436, loss=0.022186702117323875
I0206 11:21:58.980063 139735302588160 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.09603632241487503, loss=0.018850641325116158
I0206 11:22:30.480007 139679759779584 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.08664656430482864, loss=0.021573401987552643
I0206 11:23:01.967378 139735302588160 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.1024029552936554, loss=0.021717717871069908
I0206 11:23:28.573754 139919816816448 spec.py:321] Evaluating on the training split.
I0206 11:25:08.338370 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 11:25:11.413128 139919816816448 spec.py:349] Evaluating on the test split.
I0206 11:25:14.358473 139919816816448 submission_runner.py:408] Time since start: 25831.15s, 	Step: 56685, 	{'train/accuracy': 0.9939702749252319, 'train/loss': 0.01885921321809292, 'train/mean_average_precision': 0.676547356716646, 'validation/accuracy': 0.9871426224708557, 'validation/loss': 0.0465945266187191, 'validation/mean_average_precision': 0.29267476355681427, 'validation/num_examples': 43793, 'test/accuracy': 0.9863424897193909, 'test/loss': 0.04987763613462448, 'test/mean_average_precision': 0.28062367859106613, 'test/num_examples': 43793, 'score': 18019.554752588272, 'total_duration': 25831.152674913406, 'accumulated_submission_time': 18019.554752588272, 'accumulated_eval_time': 7807.280650138855, 'accumulated_logging_time': 2.760650157928467}
I0206 11:25:14.384850 139697948911360 logging_writer.py:48] [56685] accumulated_eval_time=7807.280650, accumulated_logging_time=2.760650, accumulated_submission_time=18019.554753, global_step=56685, preemption_count=0, score=18019.554753, test/accuracy=0.986342, test/loss=0.049878, test/mean_average_precision=0.280624, test/num_examples=43793, total_duration=25831.152675, train/accuracy=0.993970, train/loss=0.018859, train/mean_average_precision=0.676547, validation/accuracy=0.987143, validation/loss=0.046595, validation/mean_average_precision=0.292675, validation/num_examples=43793
I0206 11:25:19.454983 139758996317952 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.08339808881282806, loss=0.02182287722826004
I0206 11:25:51.438071 139697948911360 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.12035506218671799, loss=0.025276601314544678
I0206 11:26:23.001958 139758996317952 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.11648711562156677, loss=0.022199681028723717
I0206 11:26:54.553692 139697948911360 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.10901129245758057, loss=0.02346273511648178
I0206 11:27:26.098581 139758996317952 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.10310050845146179, loss=0.02296927012503147
I0206 11:27:57.679654 139697948911360 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.12430807948112488, loss=0.024353839457035065
I0206 11:28:29.762911 139758996317952 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.11000422388315201, loss=0.023266030475497246
I0206 11:29:01.331508 139697948911360 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.11252246052026749, loss=0.02230066992342472
I0206 11:29:14.667886 139919816816448 spec.py:321] Evaluating on the training split.
I0206 11:30:50.542109 139919816816448 spec.py:333] Evaluating on the validation split.
I0206 11:30:53.676646 139919816816448 spec.py:349] Evaluating on the test split.
I0206 11:30:56.715562 139919816816448 submission_runner.py:408] Time since start: 26173.51s, 	Step: 57443, 	{'train/accuracy': 0.9942606687545776, 'train/loss': 0.01822679676115513, 'train/mean_average_precision': 0.6956112092775892, 'validation/accuracy': 0.9871117472648621, 'validation/loss': 0.04643256217241287, 'validation/mean_average_precision': 0.2988261302469323, 'validation/num_examples': 43793, 'test/accuracy': 0.9862766265869141, 'test/loss': 0.04957572743296623, 'test/mean_average_precision': 0.2852527176577683, 'test/num_examples': 43793, 'score': 18259.80503797531, 'total_duration': 26173.50976252556, 'accumulated_submission_time': 18259.80503797531, 'accumulated_eval_time': 7909.328285217285, 'accumulated_logging_time': 2.7992615699768066}
I0206 11:30:56.741622 139716992366336 logging_writer.py:48] [57443] accumulated_eval_time=7909.328285, accumulated_logging_time=2.799262, accumulated_submission_time=18259.805038, global_step=57443, preemption_count=0, score=18259.805038, test/accuracy=0.986277, test/loss=0.049576, test/mean_average_precision=0.285253, test/num_examples=43793, total_duration=26173.509763, train/accuracy=0.994261, train/loss=0.018227, train/mean_average_precision=0.695611, validation/accuracy=0.987112, validation/loss=0.046433, validation/mean_average_precision=0.298826, validation/num_examples=43793
I0206 11:31:15.081866 139735302588160 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.10732792317867279, loss=0.022071119397878647
I0206 11:31:46.392166 139716992366336 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.12207911908626556, loss=0.02558235265314579
I0206 11:32:17.598917 139735302588160 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.10698293894529343, loss=0.021058252081274986
I0206 11:32:48.996386 139716992366336 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.10955511778593063, loss=0.02333279326558113
I0206 11:33:20.142926 139735302588160 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.10741172730922699, loss=0.021012477576732635
I0206 11:33:52.144632 139716992366336 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.10010034590959549, loss=0.022266753017902374
I0206 11:34:23.701783 139735302588160 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.11555591225624084, loss=0.021856257691979408
I0206 11:34:34.249338 139716992366336 logging_writer.py:48] [58134] global_step=58134, preemption_count=0, score=18477.266256
I0206 11:34:34.306068 139919816816448 checkpoints.py:490] Saving checkpoint at step: 58134
I0206 11:34:34.440561 139919816816448 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification/study_1/ogbg_jax/trial_5/checkpoint_58134
I0206 11:34:34.441549 139919816816448 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification/study_1/ogbg_jax/trial_5/checkpoint_58134.
I0206 11:34:34.588071 139919816816448 submission_runner.py:583] Tuning trial 5/5
I0206 11:34:34.588301 139919816816448 submission_runner.py:584] Hyperparameters: Hyperparameters(dropout_rate=0.1, label_smoothing=0.0, learning_rate=0.0017486387539278373, one_minus_beta1=0.06733926164, beta2=0.9955159689799007, weight_decay=0.08121616522670176, warmup_factor=0.02)
I0206 11:34:34.593436 139919816816448 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.5324588418006897, 'train/loss': 0.7277070879936218, 'train/mean_average_precision': 0.02175867816848414, 'validation/accuracy': 0.5230699777603149, 'validation/loss': 0.7331878542900085, 'validation/mean_average_precision': 0.02548130539809287, 'validation/num_examples': 43793, 'test/accuracy': 0.5214918851852417, 'test/loss': 0.7347381114959717, 'test/mean_average_precision': 0.026820638123804655, 'test/num_examples': 43793, 'score': 13.114962816238403, 'total_duration': 120.72387218475342, 'accumulated_submission_time': 13.114962816238403, 'accumulated_eval_time': 107.60885667800903, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (760, {'train/accuracy': 0.9868366122245789, 'train/loss': 0.05256984755396843, 'train/mean_average_precision': 0.04591733329195911, 'validation/accuracy': 0.9841719269752502, 'validation/loss': 0.06288707256317139, 'validation/mean_average_precision': 0.045798354438788554, 'validation/num_examples': 43793, 'test/accuracy': 0.983195960521698, 'test/loss': 0.06619127094745636, 'test/mean_average_precision': 0.04798481508347616, 'test/num_examples': 43793, 'score': 253.0232331752777, 'total_duration': 467.05287051200867, 'accumulated_submission_time': 253.0232331752777, 'accumulated_eval_time': 213.72126507759094, 'accumulated_logging_time': 0.2872607707977295, 'global_step': 760, 'preemption_count': 0}), (1505, {'train/accuracy': 0.986958384513855, 'train/loss': 0.04801762476563454, 'train/mean_average_precision': 0.09383095081732748, 'validation/accuracy': 0.9844183325767517, 'validation/loss': 0.05740370228886604, 'validation/mean_average_precision': 0.0944905234983526, 'validation/num_examples': 43793, 'test/accuracy': 0.9834386110305786, 'test/loss': 0.0608416348695755, 'test/mean_average_precision': 0.09337897309384578, 'test/num_examples': 43793, 'score': 493.0518238544464, 'total_duration': 812.8888652324677, 'accumulated_submission_time': 493.0518238544464, 'accumulated_eval_time': 319.4787435531616, 'accumulated_logging_time': 0.3140983581542969, 'global_step': 1505, 'preemption_count': 0}), (2258, {'train/accuracy': 0.9876599907875061, 'train/loss': 0.04372995346784592, 'train/mean_average_precision': 0.13444942583743152, 'validation/accuracy': 0.9849497079849243, 'validation/loss': 0.05331363528966904, 'validation/mean_average_precision': 0.13050464298387648, 'validation/num_examples': 43793, 'test/accuracy': 0.9839305281639099, 'test/loss': 0.056407343596220016, 'test/mean_average_precision': 0.12865337006852393, 'test/num_examples': 43793, 'score': 733.1513078212738, 'total_duration': 1154.5208294391632, 'accumulated_submission_time': 733.1513078212738, 'accumulated_eval_time': 420.96342611312866, 'accumulated_logging_time': 0.3397960662841797, 'global_step': 2258, 'preemption_count': 0}), (3011, {'train/accuracy': 0.988203763961792, 'train/loss': 0.04144752025604248, 'train/mean_average_precision': 0.1611080446191192, 'validation/accuracy': 0.9852355122566223, 'validation/loss': 0.050933822989463806, 'validation/mean_average_precision': 0.15108787353642003, 'validation/num_examples': 43793, 'test/accuracy': 0.9842051863670349, 'test/loss': 0.054171618074178696, 'test/mean_average_precision': 0.14581284698418215, 'test/num_examples': 43793, 'score': 973.3656969070435, 'total_duration': 1493.8625662326813, 'accumulated_submission_time': 973.3656969070435, 'accumulated_eval_time': 520.0432939529419, 'accumulated_logging_time': 0.3667905330657959, 'global_step': 3011, 'preemption_count': 0}), (3759, {'train/accuracy': 0.9883896112442017, 'train/loss': 0.040495552122592926, 'train/mean_average_precision': 0.19664721625610154, 'validation/accuracy': 0.9854575395584106, 'validation/loss': 0.04961736127734184, 'validation/mean_average_precision': 0.17224777324275892, 'validation/num_examples': 43793, 'test/accuracy': 0.9845345616340637, 'test/loss': 0.052431534975767136, 'test/mean_average_precision': 0.1698998980243238, 'test/num_examples': 43793, 'score': 1213.6298730373383, 'total_duration': 1842.9375042915344, 'accumulated_submission_time': 1213.6298730373383, 'accumulated_eval_time': 628.8051879405975, 'accumulated_logging_time': 0.3951599597930908, 'global_step': 3759, 'preemption_count': 0}), (4512, {'train/accuracy': 0.9884734749794006, 'train/loss': 0.039681486785411835, 'train/mean_average_precision': 0.21041611738254148, 'validation/accuracy': 0.9857286810874939, 'validation/loss': 0.048848893493413925, 'validation/mean_average_precision': 0.18536137968006355, 'validation/num_examples': 43793, 'test/accuracy': 0.984804093837738, 'test/loss': 0.05158129334449768, 'test/mean_average_precision': 0.1859877344371198, 'test/num_examples': 43793, 'score': 1453.6540446281433, 'total_duration': 2185.0393300056458, 'accumulated_submission_time': 1453.6540446281433, 'accumulated_eval_time': 730.8336308002472, 'accumulated_logging_time': 0.4234771728515625, 'global_step': 4512, 'preemption_count': 0}), (5260, {'train/accuracy': 0.9889073967933655, 'train/loss': 0.03776993602514267, 'train/mean_average_precision': 0.240450900537608, 'validation/accuracy': 0.9860051274299622, 'validation/loss': 0.0472903698682785, 'validation/mean_average_precision': 0.20116533712170387, 'validation/num_examples': 43793, 'test/accuracy': 0.9851145148277283, 'test/loss': 0.04979237541556358, 'test/mean_average_precision': 0.20280649626517522, 'test/num_examples': 43793, 'score': 1693.8498435020447, 'total_duration': 2530.7144179344177, 'accumulated_submission_time': 1693.8498435020447, 'accumulated_eval_time': 836.2591185569763, 'accumulated_logging_time': 0.4546363353729248, 'global_step': 5260, 'preemption_count': 0}), (6017, {'train/accuracy': 0.9888168573379517, 'train/loss': 0.037816647440195084, 'train/mean_average_precision': 0.24340651811467862, 'validation/accuracy': 0.9859337210655212, 'validation/loss': 0.047266535460948944, 'validation/mean_average_precision': 0.20417978165163772, 'validation/num_examples': 43793, 'test/accuracy': 0.9850445985794067, 'test/loss': 0.04999808222055435, 'test/mean_average_precision': 0.20397912744073982, 'test/num_examples': 43793, 'score': 1933.8649094104767, 'total_duration': 2875.7232887744904, 'accumulated_submission_time': 1933.8649094104767, 'accumulated_eval_time': 941.2021589279175, 'accumulated_logging_time': 0.4842491149902344, 'global_step': 6017, 'preemption_count': 0}), (6766, {'train/accuracy': 0.9892390966415405, 'train/loss': 0.03620724380016327, 'train/mean_average_precision': 0.271036118425679, 'validation/accuracy': 0.9861992001533508, 'validation/loss': 0.046460848301649094, 'validation/mean_average_precision': 0.2200321466350437, 'validation/num_examples': 43793, 'test/accuracy': 0.9852948188781738, 'test/loss': 0.04915662109851837, 'test/mean_average_precision': 0.22953859957963182, 'test/num_examples': 43793, 'score': 2173.9619414806366, 'total_duration': 3219.5011546611786, 'accumulated_submission_time': 2173.9619414806366, 'accumulated_eval_time': 1044.8342657089233, 'accumulated_logging_time': 0.5117506980895996, 'global_step': 6766, 'preemption_count': 0}), (7516, {'train/accuracy': 0.9895783066749573, 'train/loss': 0.034987322986125946, 'train/mean_average_precision': 0.2932618939418534, 'validation/accuracy': 0.9863753914833069, 'validation/loss': 0.04642264544963837, 'validation/mean_average_precision': 0.22873084220148676, 'validation/num_examples': 43793, 'test/accuracy': 0.9854813814163208, 'test/loss': 0.04922619089484215, 'test/mean_average_precision': 0.23487831542870644, 'test/num_examples': 43793, 'score': 2413.9666571617126, 'total_duration': 3563.6852464675903, 'accumulated_submission_time': 2413.9666571617126, 'accumulated_eval_time': 1148.9634318351746, 'accumulated_logging_time': 0.5409078598022461, 'global_step': 7516, 'preemption_count': 0}), (8266, {'train/accuracy': 0.9897823333740234, 'train/loss': 0.03420835733413696, 'train/mean_average_precision': 0.31957378834514716, 'validation/accuracy': 0.9863818883895874, 'validation/loss': 0.04541264474391937, 'validation/mean_average_precision': 0.23256084924965317, 'validation/num_examples': 43793, 'test/accuracy': 0.9855201244354248, 'test/loss': 0.048025909811258316, 'test/mean_average_precision': 0.23596214485473122, 'test/num_examples': 43793, 'score': 2653.955867290497, 'total_duration': 3901.649888753891, 'accumulated_submission_time': 2653.955867290497, 'accumulated_eval_time': 1246.890321969986, 'accumulated_logging_time': 0.5691602230072021, 'global_step': 8266, 'preemption_count': 0}), (9015, {'train/accuracy': 0.990003228187561, 'train/loss': 0.03346382826566696, 'train/mean_average_precision': 0.3346555605107079, 'validation/accuracy': 0.9865231513977051, 'validation/loss': 0.04527439922094345, 'validation/mean_average_precision': 0.2372158837428065, 'validation/num_examples': 43793, 'test/accuracy': 0.9856511354446411, 'test/loss': 0.04779340699315071, 'test/mean_average_precision': 0.24273888082615314, 'test/num_examples': 43793, 'score': 2894.023741006851, 'total_duration': 4248.052522659302, 'accumulated_submission_time': 2894.023741006851, 'accumulated_eval_time': 1353.1766684055328, 'accumulated_logging_time': 0.5972564220428467, 'global_step': 9015, 'preemption_count': 0}), (9761, {'train/accuracy': 0.9901729226112366, 'train/loss': 0.03294522315263748, 'train/mean_average_precision': 0.3521926542739725, 'validation/accuracy': 0.9865406155586243, 'validation/loss': 0.044867224991321564, 'validation/mean_average_precision': 0.2515968842157506, 'validation/num_examples': 43793, 'test/accuracy': 0.9857004284858704, 'test/loss': 0.04741131514310837, 'test/mean_average_precision': 0.24142989780850443, 'test/num_examples': 43793, 'score': 3134.1962015628815, 'total_duration': 4595.578535079956, 'accumulated_submission_time': 3134.1962015628815, 'accumulated_eval_time': 1460.478672027588, 'accumulated_logging_time': 0.6274769306182861, 'global_step': 9761, 'preemption_count': 0}), (10505, {'train/accuracy': 0.9900105595588684, 'train/loss': 0.03330980986356735, 'train/mean_average_precision': 0.34147219152732555, 'validation/accuracy': 0.9866453409194946, 'validation/loss': 0.04445862025022507, 'validation/mean_average_precision': 0.2510299167733832, 'validation/num_examples': 43793, 'test/accuracy': 0.9858183264732361, 'test/loss': 0.0471859909594059, 'test/mean_average_precision': 0.2474191243238686, 'test/num_examples': 43793, 'score': 3374.321034193039, 'total_duration': 4942.475167989731, 'accumulated_submission_time': 3374.321034193039, 'accumulated_eval_time': 1567.1986813545227, 'accumulated_logging_time': 0.6583380699157715, 'global_step': 10505, 'preemption_count': 0}), (11258, {'train/accuracy': 0.9902216196060181, 'train/loss': 0.032521139830350876, 'train/mean_average_precision': 0.3451225530368046, 'validation/accuracy': 0.986581563949585, 'validation/loss': 0.0445379801094532, 'validation/mean_average_precision': 0.25830052866323944, 'validation/num_examples': 43793, 'test/accuracy': 0.9857791662216187, 'test/loss': 0.04718675836920738, 'test/mean_average_precision': 0.2567930607884815, 'test/num_examples': 43793, 'score': 3614.390597343445, 'total_duration': 5288.855046987534, 'accumulated_submission_time': 3614.390597343445, 'accumulated_eval_time': 1673.4594593048096, 'accumulated_logging_time': 0.6867432594299316, 'global_step': 11258, 'preemption_count': 0}), (12007, {'train/accuracy': 0.9903555512428284, 'train/loss': 0.032215844839811325, 'train/mean_average_precision': 0.35489287274553577, 'validation/accuracy': 0.986707866191864, 'validation/loss': 0.044431596994400024, 'validation/mean_average_precision': 0.2554358455220814, 'validation/num_examples': 43793, 'test/accuracy': 0.9858141541481018, 'test/loss': 0.04727277159690857, 'test/mean_average_precision': 0.2503482735494056, 'test/num_examples': 43793, 'score': 3854.4456446170807, 'total_duration': 5634.432926416397, 'accumulated_submission_time': 3854.4456446170807, 'accumulated_eval_time': 1778.9284162521362, 'accumulated_logging_time': 0.7189178466796875, 'global_step': 12007, 'preemption_count': 0}), (12768, {'train/accuracy': 0.9902983903884888, 'train/loss': 0.03216234967112541, 'train/mean_average_precision': 0.3728056786919886, 'validation/accuracy': 0.9867314100265503, 'validation/loss': 0.044263798743486404, 'validation/mean_average_precision': 0.26053046081291353, 'validation/num_examples': 43793, 'test/accuracy': 0.985888659954071, 'test/loss': 0.04718273878097534, 'test/mean_average_precision': 0.25743486912238867, 'test/num_examples': 43793, 'score': 4094.4016149044037, 'total_duration': 5973.4755423069, 'accumulated_submission_time': 4094.4016149044037, 'accumulated_eval_time': 1877.9654395580292, 'accumulated_logging_time': 0.7478671073913574, 'global_step': 12768, 'preemption_count': 0}), (13526, {'train/accuracy': 0.9905072450637817, 'train/loss': 0.031183667480945587, 'train/mean_average_precision': 0.3852428672374816, 'validation/accuracy': 0.986772358417511, 'validation/loss': 0.044338926672935486, 'validation/mean_average_precision': 0.2648099308155049, 'validation/num_examples': 43793, 'test/accuracy': 0.9858831763267517, 'test/loss': 0.04731597751379013, 'test/mean_average_precision': 0.2539916084768826, 'test/num_examples': 43793, 'score': 4334.481739997864, 'total_duration': 6313.919853925705, 'accumulated_submission_time': 4334.481739997864, 'accumulated_eval_time': 1978.2784700393677, 'accumulated_logging_time': 0.7787857055664062, 'global_step': 13526, 'preemption_count': 0}), (14281, {'train/accuracy': 0.9905017018318176, 'train/loss': 0.031227940693497658, 'train/mean_average_precision': 0.3851849660303862, 'validation/accuracy': 0.9866887331008911, 'validation/loss': 0.0443001464009285, 'validation/mean_average_precision': 0.25957978612968485, 'validation/num_examples': 43793, 'test/accuracy': 0.9858419299125671, 'test/loss': 0.047129031270742416, 'test/mean_average_precision': 0.25754915633653663, 'test/num_examples': 43793, 'score': 4574.704688310623, 'total_duration': 6658.48993730545, 'accumulated_submission_time': 4574.704688310623, 'accumulated_eval_time': 2082.5760612487793, 'accumulated_logging_time': 0.807380199432373, 'global_step': 14281, 'preemption_count': 0}), (15031, {'train/accuracy': 0.9906373023986816, 'train/loss': 0.03075084462761879, 'train/mean_average_precision': 0.4132004282045805, 'validation/accuracy': 0.9867650866508484, 'validation/loss': 0.04462047293782234, 'validation/mean_average_precision': 0.26286550689563964, 'validation/num_examples': 43793, 'test/accuracy': 0.9859139323234558, 'test/loss': 0.047488342970609665, 'test/mean_average_precision': 0.2540615756671235, 'test/num_examples': 43793, 'score': 4814.931003808975, 'total_duration': 6998.6618309021, 'accumulated_submission_time': 4814.931003808975, 'accumulated_eval_time': 2182.466774225235, 'accumulated_logging_time': 0.8405594825744629, 'global_step': 15031, 'preemption_count': 0}), (15795, {'train/accuracy': 0.990784227848053, 'train/loss': 0.030104296281933784, 'train/mean_average_precision': 0.40460155244264295, 'validation/accuracy': 0.9867382645606995, 'validation/loss': 0.04417121782898903, 'validation/mean_average_precision': 0.2618256099036918, 'validation/num_examples': 43793, 'test/accuracy': 0.9858680367469788, 'test/loss': 0.04678452014923096, 'test/mean_average_precision': 0.2557980212280484, 'test/num_examples': 43793, 'score': 5054.909048080444, 'total_duration': 7342.298298835754, 'accumulated_submission_time': 5054.909048080444, 'accumulated_eval_time': 2286.0750284194946, 'accumulated_logging_time': 0.8707151412963867, 'global_step': 15795, 'preemption_count': 0}), (16557, {'train/accuracy': 0.9911503791809082, 'train/loss': 0.029118113219738007, 'train/mean_average_precision': 0.44197708028239857, 'validation/accuracy': 0.986614465713501, 'validation/loss': 0.044292621314525604, 'validation/mean_average_precision': 0.26905837310345176, 'validation/num_examples': 43793, 'test/accuracy': 0.9858490824699402, 'test/loss': 0.04684639722108841, 'test/mean_average_precision': 0.26177670806063214, 'test/num_examples': 43793, 'score': 5294.891679048538, 'total_duration': 7686.331763267517, 'accumulated_submission_time': 5294.891679048538, 'accumulated_eval_time': 2390.0760481357574, 'accumulated_logging_time': 0.8997907638549805, 'global_step': 16557, 'preemption_count': 0}), (17305, {'train/accuracy': 0.9909698367118835, 'train/loss': 0.029629463329911232, 'train/mean_average_precision': 0.425239852981252, 'validation/accuracy': 0.9866148829460144, 'validation/loss': 0.04416673630475998, 'validation/mean_average_precision': 0.2680382857825597, 'validation/num_examples': 43793, 'test/accuracy': 0.9858777523040771, 'test/loss': 0.04673495516180992, 'test/mean_average_precision': 0.2607631513044034, 'test/num_examples': 43793, 'score': 5534.9659996032715, 'total_duration': 8027.069473028183, 'accumulated_submission_time': 5534.9659996032715, 'accumulated_eval_time': 2490.6851439476013, 'accumulated_logging_time': 0.9323804378509521, 'global_step': 17305, 'preemption_count': 0}), (18067, {'train/accuracy': 0.9908579587936401, 'train/loss': 0.03003951907157898, 'train/mean_average_precision': 0.41192477326662935, 'validation/accuracy': 0.9867743849754333, 'validation/loss': 0.04401408135890961, 'validation/mean_average_precision': 0.2692154002615283, 'validation/num_examples': 43793, 'test/accuracy': 0.9858953952789307, 'test/loss': 0.04672873765230179, 'test/mean_average_precision': 0.26471816355767297, 'test/num_examples': 43793, 'score': 5775.012037992477, 'total_duration': 8372.382522583008, 'accumulated_submission_time': 5775.012037992477, 'accumulated_eval_time': 2595.9028055667877, 'accumulated_logging_time': 0.9617249965667725, 'global_step': 18067, 'preemption_count': 0}), (18826, {'train/accuracy': 0.9908071160316467, 'train/loss': 0.03033352829515934, 'train/mean_average_precision': 0.40827320062189715, 'validation/accuracy': 0.9867638349533081, 'validation/loss': 0.0440635085105896, 'validation/mean_average_precision': 0.26480815617896264, 'validation/num_examples': 43793, 'test/accuracy': 0.9859059453010559, 'test/loss': 0.04682458937168121, 'test/mean_average_precision': 0.261026378969448, 'test/num_examples': 43793, 'score': 6014.981848716736, 'total_duration': 8712.026437044144, 'accumulated_submission_time': 6014.981848716736, 'accumulated_eval_time': 2695.526474237442, 'accumulated_logging_time': 0.9920079708099365, 'global_step': 18826, 'preemption_count': 0}), (19588, {'train/accuracy': 0.990985095500946, 'train/loss': 0.029373522847890854, 'train/mean_average_precision': 0.42571662301725, 'validation/accuracy': 0.986893355846405, 'validation/loss': 0.044145215302705765, 'validation/mean_average_precision': 0.27269999620141266, 'validation/num_examples': 43793, 'test/accuracy': 0.9860677123069763, 'test/loss': 0.04679865390062332, 'test/mean_average_precision': 0.269728862963543, 'test/num_examples': 43793, 'score': 6255.140200138092, 'total_duration': 9053.31315279007, 'accumulated_submission_time': 6255.140200138092, 'accumulated_eval_time': 2796.600333929062, 'accumulated_logging_time': 1.0257935523986816, 'global_step': 19588, 'preemption_count': 0}), (20343, {'train/accuracy': 0.9908535480499268, 'train/loss': 0.029793960973620415, 'train/mean_average_precision': 0.42994988858239147, 'validation/accuracy': 0.9868730306625366, 'validation/loss': 0.04385649785399437, 'validation/mean_average_precision': 0.27432950631949, 'validation/num_examples': 43793, 'test/accuracy': 0.9860925674438477, 'test/loss': 0.046628862619400024, 'test/mean_average_precision': 0.2667238021460736, 'test/num_examples': 43793, 'score': 6495.229241847992, 'total_duration': 9397.442924976349, 'accumulated_submission_time': 6495.229241847992, 'accumulated_eval_time': 2900.5887792110443, 'accumulated_logging_time': 1.0550963878631592, 'global_step': 20343, 'preemption_count': 0}), (21098, {'train/accuracy': 0.9910489320755005, 'train/loss': 0.02918689325451851, 'train/mean_average_precision': 0.4427940285589564, 'validation/accuracy': 0.9868564009666443, 'validation/loss': 0.04386270046234131, 'validation/mean_average_precision': 0.2687042846949644, 'validation/num_examples': 43793, 'test/accuracy': 0.9860213398933411, 'test/loss': 0.04656833037734032, 'test/mean_average_precision': 0.26099991978009857, 'test/num_examples': 43793, 'score': 6735.494882106781, 'total_duration': 9740.919793367386, 'accumulated_submission_time': 6735.494882106781, 'accumulated_eval_time': 3003.7433593273163, 'accumulated_logging_time': 1.089904546737671, 'global_step': 21098, 'preemption_count': 0}), (21856, {'train/accuracy': 0.9909344911575317, 'train/loss': 0.029195064678788185, 'train/mean_average_precision': 0.43935341446327436, 'validation/accuracy': 0.9868978261947632, 'validation/loss': 0.04463415965437889, 'validation/mean_average_precision': 0.27351905659019016, 'validation/num_examples': 43793, 'test/accuracy': 0.9861317276954651, 'test/loss': 0.04742466285824776, 'test/mean_average_precision': 0.2671671664715521, 'test/num_examples': 43793, 'score': 6975.677932262421, 'total_duration': 10084.394299507141, 'accumulated_submission_time': 6975.677932262421, 'accumulated_eval_time': 3106.980573654175, 'accumulated_logging_time': 1.1237335205078125, 'global_step': 21856, 'preemption_count': 0}), (22612, {'train/accuracy': 0.9911161065101624, 'train/loss': 0.028925830498337746, 'train/mean_average_precision': 0.4461390763695887, 'validation/accuracy': 0.9867297410964966, 'validation/loss': 0.04394465684890747, 'validation/mean_average_precision': 0.2784422440583035, 'validation/num_examples': 43793, 'test/accuracy': 0.9859792590141296, 'test/loss': 0.04665583744645119, 'test/mean_average_precision': 0.2688925711781614, 'test/num_examples': 43793, 'score': 7215.721256017685, 'total_duration': 10425.84306025505, 'accumulated_submission_time': 7215.721256017685, 'accumulated_eval_time': 3208.3332257270813, 'accumulated_logging_time': 1.1560900211334229, 'global_step': 22612, 'preemption_count': 0}), (23366, {'train/accuracy': 0.9914578199386597, 'train/loss': 0.0278907660394907, 'train/mean_average_precision': 0.46277013743523543, 'validation/accuracy': 0.9868608713150024, 'validation/loss': 0.04406419023871422, 'validation/mean_average_precision': 0.27692518753141804, 'validation/num_examples': 43793, 'test/accuracy': 0.9861093759536743, 'test/loss': 0.046633679419755936, 'test/mean_average_precision': 0.2682321435300443, 'test/num_examples': 43793, 'score': 7455.825122117996, 'total_duration': 10767.822446346283, 'accumulated_submission_time': 7455.825122117996, 'accumulated_eval_time': 3310.157596349716, 'accumulated_logging_time': 1.1869611740112305, 'global_step': 23366, 'preemption_count': 0}), (24123, {'train/accuracy': 0.9916008114814758, 'train/loss': 0.027223728597164154, 'train/mean_average_precision': 0.48737572996290834, 'validation/accuracy': 0.9869104027748108, 'validation/loss': 0.043956320732831955, 'validation/mean_average_precision': 0.2830847361580403, 'validation/num_examples': 43793, 'test/accuracy': 0.9861299991607666, 'test/loss': 0.0465262271463871, 'test/mean_average_precision': 0.27606768093356254, 'test/num_examples': 43793, 'score': 7695.909645080566, 'total_duration': 11106.817147493362, 'accumulated_submission_time': 7695.909645080566, 'accumulated_eval_time': 3409.015555858612, 'accumulated_logging_time': 1.2186834812164307, 'global_step': 24123, 'preemption_count': 0}), (24882, {'train/accuracy': 0.9916938543319702, 'train/loss': 0.026998370885849, 'train/mean_average_precision': 0.4925730935016073, 'validation/accuracy': 0.9868324398994446, 'validation/loss': 0.04409567266702652, 'validation/mean_average_precision': 0.2810012658860785, 'validation/num_examples': 43793, 'test/accuracy': 0.9860736131668091, 'test/loss': 0.0468006432056427, 'test/mean_average_precision': 0.2785355949087478, 'test/num_examples': 43793, 'score': 7936.176169872284, 'total_duration': 11449.778022766113, 'accumulated_submission_time': 7936.176169872284, 'accumulated_eval_time': 3511.65873837471, 'accumulated_logging_time': 1.2492575645446777, 'global_step': 24882, 'preemption_count': 0}), (25646, {'train/accuracy': 0.9915238618850708, 'train/loss': 0.02756897173821926, 'train/mean_average_precision': 0.4713439378127574, 'validation/accuracy': 0.9869733452796936, 'validation/loss': 0.043928198516368866, 'validation/mean_average_precision': 0.28169856780816016, 'validation/num_examples': 43793, 'test/accuracy': 0.986088752746582, 'test/loss': 0.04671822860836983, 'test/mean_average_precision': 0.27525278862473707, 'test/num_examples': 43793, 'score': 8176.215074539185, 'total_duration': 11788.014889717102, 'accumulated_submission_time': 8176.215074539185, 'accumulated_eval_time': 3609.8065259456635, 'accumulated_logging_time': 1.2795326709747314, 'global_step': 25646, 'preemption_count': 0}), (26404, {'train/accuracy': 0.9913026094436646, 'train/loss': 0.028051326051354408, 'train/mean_average_precision': 0.45402465697035443, 'validation/accuracy': 0.9868937730789185, 'validation/loss': 0.04407404735684395, 'validation/mean_average_precision': 0.2784015307768114, 'validation/num_examples': 43793, 'test/accuracy': 0.9861670732498169, 'test/loss': 0.04672127217054367, 'test/mean_average_precision': 0.27219365552121805, 'test/num_examples': 43793, 'score': 8416.223066806793, 'total_duration': 12129.615074634552, 'accumulated_submission_time': 8416.223066806793, 'accumulated_eval_time': 3711.346279144287, 'accumulated_logging_time': 1.3122563362121582, 'global_step': 26404, 'preemption_count': 0}), (27167, {'train/accuracy': 0.9913366436958313, 'train/loss': 0.027996491640806198, 'train/mean_average_precision': 0.45123302985127556, 'validation/accuracy': 0.9870614409446716, 'validation/loss': 0.04383734241127968, 'validation/mean_average_precision': 0.2824528369850656, 'validation/num_examples': 43793, 'test/accuracy': 0.9861894249916077, 'test/loss': 0.046678170561790466, 'test/mean_average_precision': 0.2716197370065153, 'test/num_examples': 43793, 'score': 8656.387609004974, 'total_duration': 12474.14634013176, 'accumulated_submission_time': 8656.387609004974, 'accumulated_eval_time': 3815.66153049469, 'accumulated_logging_time': 1.343522071838379, 'global_step': 27167, 'preemption_count': 0}), (27924, {'train/accuracy': 0.9912831783294678, 'train/loss': 0.028219128027558327, 'train/mean_average_precision': 0.45971099259131304, 'validation/accuracy': 0.9870139360427856, 'validation/loss': 0.04407530277967453, 'validation/mean_average_precision': 0.2801922874370537, 'validation/num_examples': 43793, 'test/accuracy': 0.9862703084945679, 'test/loss': 0.04676474630832672, 'test/mean_average_precision': 0.27914873193636414, 'test/num_examples': 43793, 'score': 8896.527443885803, 'total_duration': 12813.708843708038, 'accumulated_submission_time': 8896.527443885803, 'accumulated_eval_time': 3915.0333251953125, 'accumulated_logging_time': 1.3740589618682861, 'global_step': 27924, 'preemption_count': 0}), (28676, {'train/accuracy': 0.9914658069610596, 'train/loss': 0.027606068179011345, 'train/mean_average_precision': 0.48040133922354344, 'validation/accuracy': 0.9868357181549072, 'validation/loss': 0.04391742870211601, 'validation/mean_average_precision': 0.2829397828021821, 'validation/num_examples': 43793, 'test/accuracy': 0.9860512614250183, 'test/loss': 0.04667437821626663, 'test/mean_average_precision': 0.26969004395085033, 'test/num_examples': 43793, 'score': 9136.534759044647, 'total_duration': 13159.920971632004, 'accumulated_submission_time': 9136.534759044647, 'accumulated_eval_time': 4021.1811108589172, 'accumulated_logging_time': 1.4105889797210693, 'global_step': 28676, 'preemption_count': 0}), (29429, {'train/accuracy': 0.9914921522140503, 'train/loss': 0.027481088414788246, 'train/mean_average_precision': 0.47864087462123883, 'validation/accuracy': 0.9869875311851501, 'validation/loss': 0.044133707880973816, 'validation/mean_average_precision': 0.280014814493845, 'validation/num_examples': 43793, 'test/accuracy': 0.9862012267112732, 'test/loss': 0.04700068011879921, 'test/mean_average_precision': 0.27495089302911, 'test/num_examples': 43793, 'score': 9376.566728830338, 'total_duration': 13502.746740341187, 'accumulated_submission_time': 9376.566728830338, 'accumulated_eval_time': 4123.922771692276, 'accumulated_logging_time': 1.4421751499176025, 'global_step': 29429, 'preemption_count': 0}), (30193, {'train/accuracy': 0.9915912747383118, 'train/loss': 0.02712496928870678, 'train/mean_average_precision': 0.48205674043474084, 'validation/accuracy': 0.9869375824928284, 'validation/loss': 0.04378962889313698, 'validation/mean_average_precision': 0.2850369521914939, 'validation/num_examples': 43793, 'test/accuracy': 0.9862092137336731, 'test/loss': 0.046412695199251175, 'test/mean_average_precision': 0.2754781499849553, 'test/num_examples': 43793, 'score': 9616.63538479805, 'total_duration': 13843.735455036163, 'accumulated_submission_time': 9616.63538479805, 'accumulated_eval_time': 4224.7899651527405, 'accumulated_logging_time': 1.474400281906128, 'global_step': 30193, 'preemption_count': 0}), (30960, {'train/accuracy': 0.9916066527366638, 'train/loss': 0.026982925832271576, 'train/mean_average_precision': 0.4897253311971881, 'validation/accuracy': 0.9870520830154419, 'validation/loss': 0.043927162885665894, 'validation/mean_average_precision': 0.28138059817879196, 'validation/num_examples': 43793, 'test/accuracy': 0.9862736463546753, 'test/loss': 0.0467136912047863, 'test/mean_average_precision': 0.27366750933361444, 'test/num_examples': 43793, 'score': 9856.632263422012, 'total_duration': 14187.458263158798, 'accumulated_submission_time': 9856.632263422012, 'accumulated_eval_time': 4328.463456869125, 'accumulated_logging_time': 1.5062589645385742, 'global_step': 30960, 'preemption_count': 0}), (31724, {'train/accuracy': 0.991977870464325, 'train/loss': 0.02589631825685501, 'train/mean_average_precision': 0.49881832888686783, 'validation/accuracy': 0.9869737029075623, 'validation/loss': 0.04373054951429367, 'validation/mean_average_precision': 0.287502596920722, 'validation/num_examples': 43793, 'test/accuracy': 0.9861574172973633, 'test/loss': 0.04638619348406792, 'test/mean_average_precision': 0.27596476724142216, 'test/num_examples': 43793, 'score': 10096.578315734863, 'total_duration': 14524.637803077698, 'accumulated_submission_time': 10096.578315734863, 'accumulated_eval_time': 4425.642622709274, 'accumulated_logging_time': 1.5399868488311768, 'global_step': 31724, 'preemption_count': 0}), (32485, {'train/accuracy': 0.9921050071716309, 'train/loss': 0.025456955656409264, 'train/mean_average_precision': 0.5302573368344676, 'validation/accuracy': 0.986983060836792, 'validation/loss': 0.044046539813280106, 'validation/mean_average_precision': 0.28281440925602086, 'validation/num_examples': 43793, 'test/accuracy': 0.9862816333770752, 'test/loss': 0.04657791927456856, 'test/mean_average_precision': 0.28005280151764833, 'test/num_examples': 43793, 'score': 10336.690904140472, 'total_duration': 14867.57130074501, 'accumulated_submission_time': 10336.690904140472, 'accumulated_eval_time': 4528.410791397095, 'accumulated_logging_time': 1.5721287727355957, 'global_step': 32485, 'preemption_count': 0}), (33248, {'train/accuracy': 0.9921290874481201, 'train/loss': 0.025368524715304375, 'train/mean_average_precision': 0.5184021366579336, 'validation/accuracy': 0.9869611263275146, 'validation/loss': 0.04421015456318855, 'validation/mean_average_precision': 0.2848071117613658, 'validation/num_examples': 43793, 'test/accuracy': 0.9862163662910461, 'test/loss': 0.04685094580054283, 'test/mean_average_precision': 0.2793535608483301, 'test/num_examples': 43793, 'score': 10576.917943954468, 'total_duration': 15205.792612075806, 'accumulated_submission_time': 10576.917943954468, 'accumulated_eval_time': 4626.351637125015, 'accumulated_logging_time': 1.6052203178405762, 'global_step': 33248, 'preemption_count': 0}), (34005, {'train/accuracy': 0.9920830726623535, 'train/loss': 0.02560705691576004, 'train/mean_average_precision': 0.5206663689703317, 'validation/accuracy': 0.9869534373283386, 'validation/loss': 0.0438498817384243, 'validation/mean_average_precision': 0.2907716835350484, 'validation/num_examples': 43793, 'test/accuracy': 0.9860959053039551, 'test/loss': 0.046714600175619125, 'test/mean_average_precision': 0.2774428806202462, 'test/num_examples': 43793, 'score': 10816.88122177124, 'total_duration': 15550.581326246262, 'accumulated_submission_time': 10816.88122177124, 'accumulated_eval_time': 4731.118203163147, 'accumulated_logging_time': 1.6438887119293213, 'global_step': 34005, 'preemption_count': 0}), (34766, {'train/accuracy': 0.9918881058692932, 'train/loss': 0.0260873194783926, 'train/mean_average_precision': 0.5010801180054504, 'validation/accuracy': 0.9869778156280518, 'validation/loss': 0.04406430944800377, 'validation/mean_average_precision': 0.2841008787928426, 'validation/num_examples': 43793, 'test/accuracy': 0.9861780405044556, 'test/loss': 0.04691501334309578, 'test/mean_average_precision': 0.27400146489981886, 'test/num_examples': 43793, 'score': 11056.937723875046, 'total_duration': 15889.510733604431, 'accumulated_submission_time': 11056.937723875046, 'accumulated_eval_time': 4829.9352684021, 'accumulated_logging_time': 1.679349660873413, 'global_step': 34766, 'preemption_count': 0}), (35525, {'train/accuracy': 0.9919905662536621, 'train/loss': 0.025885270908474922, 'train/mean_average_precision': 0.5093072745034118, 'validation/accuracy': 0.9869050979614258, 'validation/loss': 0.04390646144747734, 'validation/mean_average_precision': 0.28743019110891205, 'validation/num_examples': 43793, 'test/accuracy': 0.986100971698761, 'test/loss': 0.04679521173238754, 'test/mean_average_precision': 0.27350426060281957, 'test/num_examples': 43793, 'score': 11296.953873872757, 'total_duration': 16228.668662548065, 'accumulated_submission_time': 11296.953873872757, 'accumulated_eval_time': 4929.022217512131, 'accumulated_logging_time': 1.7137835025787354, 'global_step': 35525, 'preemption_count': 0}), (36284, {'train/accuracy': 0.9918835759162903, 'train/loss': 0.026082046329975128, 'train/mean_average_precision': 0.50537218004944, 'validation/accuracy': 0.987015962600708, 'validation/loss': 0.044364724308252335, 'validation/mean_average_precision': 0.28679058414749675, 'validation/num_examples': 43793, 'test/accuracy': 0.9861708879470825, 'test/loss': 0.04732805863022804, 'test/mean_average_precision': 0.2739957375683177, 'test/num_examples': 43793, 'score': 11537.133439779282, 'total_duration': 16574.356207609177, 'accumulated_submission_time': 11537.133439779282, 'accumulated_eval_time': 5034.475823879242, 'accumulated_logging_time': 1.7479205131530762, 'global_step': 36284, 'preemption_count': 0}), (37041, {'train/accuracy': 0.991921603679657, 'train/loss': 0.02600712701678276, 'train/mean_average_precision': 0.5088854546962834, 'validation/accuracy': 0.9869713187217712, 'validation/loss': 0.04405609890818596, 'validation/mean_average_precision': 0.28551041680893874, 'validation/num_examples': 43793, 'test/accuracy': 0.9860870838165283, 'test/loss': 0.046863798052072525, 'test/mean_average_precision': 0.27493427809221893, 'test/num_examples': 43793, 'score': 11777.099576950073, 'total_duration': 16911.1993830204, 'accumulated_submission_time': 11777.099576950073, 'accumulated_eval_time': 5131.294198036194, 'accumulated_logging_time': 1.7850227355957031, 'global_step': 37041, 'preemption_count': 0}), (37792, {'train/accuracy': 0.9920625686645508, 'train/loss': 0.02546514943242073, 'train/mean_average_precision': 0.5246759598596786, 'validation/accuracy': 0.9870808720588684, 'validation/loss': 0.04423892870545387, 'validation/mean_average_precision': 0.29014781513482246, 'validation/num_examples': 43793, 'test/accuracy': 0.9861211776733398, 'test/loss': 0.04714534804224968, 'test/mean_average_precision': 0.2790645129473144, 'test/num_examples': 43793, 'score': 12017.13541841507, 'total_duration': 17256.310554504395, 'accumulated_submission_time': 12017.13541841507, 'accumulated_eval_time': 5236.3157749176025, 'accumulated_logging_time': 1.8184189796447754, 'global_step': 37792, 'preemption_count': 0}), (38539, {'train/accuracy': 0.9921326041221619, 'train/loss': 0.02521633170545101, 'train/mean_average_precision': 0.5271417505934661, 'validation/accuracy': 0.9870041608810425, 'validation/loss': 0.04437185823917389, 'validation/mean_average_precision': 0.284068813649276, 'validation/num_examples': 43793, 'test/accuracy': 0.9862340688705444, 'test/loss': 0.04699847847223282, 'test/mean_average_precision': 0.27985943950190395, 'test/num_examples': 43793, 'score': 12257.310943841934, 'total_duration': 17599.25080871582, 'accumulated_submission_time': 12257.310943841934, 'accumulated_eval_time': 5339.024819612503, 'accumulated_logging_time': 1.8535473346710205, 'global_step': 38539, 'preemption_count': 0}), (39297, {'train/accuracy': 0.9923343062400818, 'train/loss': 0.024549124762415886, 'train/mean_average_precision': 0.5338019086037505, 'validation/accuracy': 0.9870902299880981, 'validation/loss': 0.0443548746407032, 'validation/mean_average_precision': 0.28783983629516036, 'validation/num_examples': 43793, 'test/accuracy': 0.9863966703414917, 'test/loss': 0.047153059393167496, 'test/mean_average_precision': 0.2812594940719487, 'test/num_examples': 43793, 'score': 12497.441692590714, 'total_duration': 17939.355797052383, 'accumulated_submission_time': 12497.441692590714, 'accumulated_eval_time': 5438.942674875259, 'accumulated_logging_time': 1.8888821601867676, 'global_step': 39297, 'preemption_count': 0}), (40049, {'train/accuracy': 0.9923198223114014, 'train/loss': 0.024319665506482124, 'train/mean_average_precision': 0.5546808189751687, 'validation/accuracy': 0.9871109127998352, 'validation/loss': 0.04435906186699867, 'validation/mean_average_precision': 0.2893713804221078, 'validation/num_examples': 43793, 'test/accuracy': 0.9862483739852905, 'test/loss': 0.047381769865751266, 'test/mean_average_precision': 0.27669284161279245, 'test/num_examples': 43793, 'score': 12737.689799547195, 'total_duration': 18286.614214897156, 'accumulated_submission_time': 12737.689799547195, 'accumulated_eval_time': 5545.8978316783905, 'accumulated_logging_time': 1.9227867126464844, 'global_step': 40049, 'preemption_count': 0}), (40802, {'train/accuracy': 0.9927132725715637, 'train/loss': 0.02329072542488575, 'train/mean_average_precision': 0.5798397030318193, 'validation/accuracy': 0.9870597720146179, 'validation/loss': 0.04475073888897896, 'validation/mean_average_precision': 0.2860022023207019, 'validation/num_examples': 43793, 'test/accuracy': 0.9862837791442871, 'test/loss': 0.0476432628929615, 'test/mean_average_precision': 0.2762729613561593, 'test/num_examples': 43793, 'score': 12977.942219495773, 'total_duration': 18627.68426156044, 'accumulated_submission_time': 12977.942219495773, 'accumulated_eval_time': 5646.659322977066, 'accumulated_logging_time': 1.9570858478546143, 'global_step': 40802, 'preemption_count': 0}), (41565, {'train/accuracy': 0.9927103519439697, 'train/loss': 0.02319791540503502, 'train/mean_average_precision': 0.5746574374680143, 'validation/accuracy': 0.9871835708618164, 'validation/loss': 0.04461555927991867, 'validation/mean_average_precision': 0.28433454948306003, 'validation/num_examples': 43793, 'test/accuracy': 0.9863145351409912, 'test/loss': 0.04759086295962334, 'test/mean_average_precision': 0.2851083116733161, 'test/num_examples': 43793, 'score': 13217.97829079628, 'total_duration': 18978.816974401474, 'accumulated_submission_time': 13217.97829079628, 'accumulated_eval_time': 5757.701861858368, 'accumulated_logging_time': 1.9907326698303223, 'global_step': 41565, 'preemption_count': 0}), (42321, {'train/accuracy': 0.9925374984741211, 'train/loss': 0.023752626031637192, 'train/mean_average_precision': 0.5571466336294107, 'validation/accuracy': 0.9870622158050537, 'validation/loss': 0.04495025426149368, 'validation/mean_average_precision': 0.2945649533936957, 'validation/num_examples': 43793, 'test/accuracy': 0.9861207604408264, 'test/loss': 0.04833002761006355, 'test/mean_average_precision': 0.27362558635591244, 'test/num_examples': 43793, 'score': 13458.056258440018, 'total_duration': 19331.047621250153, 'accumulated_submission_time': 13458.056258440018, 'accumulated_eval_time': 5869.774827003479, 'accumulated_logging_time': 2.050032138824463, 'global_step': 42321, 'preemption_count': 0}), (43067, {'train/accuracy': 0.9924070835113525, 'train/loss': 0.02418462187051773, 'train/mean_average_precision': 0.5549188476915506, 'validation/accuracy': 0.9869388341903687, 'validation/loss': 0.04476182162761688, 'validation/mean_average_precision': 0.291296804280545, 'validation/num_examples': 43793, 'test/accuracy': 0.9861182570457458, 'test/loss': 0.04772588238120079, 'test/mean_average_precision': 0.28009756637386835, 'test/num_examples': 43793, 'score': 13698.089906215668, 'total_duration': 19671.935883522034, 'accumulated_submission_time': 13698.089906215668, 'accumulated_eval_time': 5970.566066265106, 'accumulated_logging_time': 2.0920493602752686, 'global_step': 43067, 'preemption_count': 0}), (43822, {'train/accuracy': 0.9925096035003662, 'train/loss': 0.023991784080863, 'train/mean_average_precision': 0.5581068188704152, 'validation/accuracy': 0.9870642423629761, 'validation/loss': 0.044694602489471436, 'validation/mean_average_precision': 0.28834239686739227, 'validation/num_examples': 43793, 'test/accuracy': 0.9862075448036194, 'test/loss': 0.04762270301580429, 'test/mean_average_precision': 0.27817579339649895, 'test/num_examples': 43793, 'score': 13938.330757379532, 'total_duration': 20014.78182053566, 'accumulated_submission_time': 13938.330757379532, 'accumulated_eval_time': 6073.116245508194, 'accumulated_logging_time': 2.1265196800231934, 'global_step': 43822, 'preemption_count': 0}), (44573, {'train/accuracy': 0.9925292134284973, 'train/loss': 0.023891253396868706, 'train/mean_average_precision': 0.5533522608731953, 'validation/accuracy': 0.9870536923408508, 'validation/loss': 0.0445699468255043, 'validation/mean_average_precision': 0.2946664919696532, 'validation/num_examples': 43793, 'test/accuracy': 0.9861910939216614, 'test/loss': 0.047599904239177704, 'test/mean_average_precision': 0.2807915067602074, 'test/num_examples': 43793, 'score': 14178.45221710205, 'total_duration': 20357.951191186905, 'accumulated_submission_time': 14178.45221710205, 'accumulated_eval_time': 6176.107170343399, 'accumulated_logging_time': 2.1627137660980225, 'global_step': 44573, 'preemption_count': 0}), (45328, {'train/accuracy': 0.9924713969230652, 'train/loss': 0.02383287064731121, 'train/mean_average_precision': 0.5691807750617297, 'validation/accuracy': 0.9870431423187256, 'validation/loss': 0.044838715344667435, 'validation/mean_average_precision': 0.2838232387063066, 'validation/num_examples': 43793, 'test/accuracy': 0.9862349033355713, 'test/loss': 0.04757899418473244, 'test/mean_average_precision': 0.2797156681465258, 'test/num_examples': 43793, 'score': 14418.654555082321, 'total_duration': 20695.19634771347, 'accumulated_submission_time': 14418.654555082321, 'accumulated_eval_time': 6273.095078229904, 'accumulated_logging_time': 2.1975855827331543, 'global_step': 45328, 'preemption_count': 0}), (46094, {'train/accuracy': 0.9926730990409851, 'train/loss': 0.0232164915651083, 'train/mean_average_precision': 0.5673038654986291, 'validation/accuracy': 0.9871312379837036, 'validation/loss': 0.044832371175289154, 'validation/mean_average_precision': 0.29277687646388556, 'validation/num_examples': 43793, 'test/accuracy': 0.9862656593322754, 'test/loss': 0.04793919250369072, 'test/mean_average_precision': 0.2842984568305961, 'test/num_examples': 43793, 'score': 14658.615586519241, 'total_duration': 21041.355713129044, 'accumulated_submission_time': 14658.615586519241, 'accumulated_eval_time': 6379.238595485687, 'accumulated_logging_time': 2.2321431636810303, 'global_step': 46094, 'preemption_count': 0}), (46841, {'train/accuracy': 0.9926999807357788, 'train/loss': 0.022925419732928276, 'train/mean_average_precision': 0.5850914346518163, 'validation/accuracy': 0.9871032238006592, 'validation/loss': 0.0455067902803421, 'validation/mean_average_precision': 0.2930319312146007, 'validation/num_examples': 43793, 'test/accuracy': 0.9862319827079773, 'test/loss': 0.04854682832956314, 'test/mean_average_precision': 0.2778092930415843, 'test/num_examples': 43793, 'score': 14898.701581716537, 'total_duration': 21383.176791906357, 'accumulated_submission_time': 14898.701581716537, 'accumulated_eval_time': 6480.91828584671, 'accumulated_logging_time': 2.2671091556549072, 'global_step': 46841, 'preemption_count': 0}), (47593, {'train/accuracy': 0.992985725402832, 'train/loss': 0.022269995883107185, 'train/mean_average_precision': 0.5896478412213515, 'validation/accuracy': 0.9871113300323486, 'validation/loss': 0.04491627588868141, 'validation/mean_average_precision': 0.2926934946306269, 'validation/num_examples': 43793, 'test/accuracy': 0.9862273335456848, 'test/loss': 0.04789400100708008, 'test/mean_average_precision': 0.2869319142049717, 'test/num_examples': 43793, 'score': 15138.667674064636, 'total_duration': 21725.787615060806, 'accumulated_submission_time': 15138.667674064636, 'accumulated_eval_time': 6583.505940437317, 'accumulated_logging_time': 2.3025686740875244, 'global_step': 47593, 'preemption_count': 0}), (48351, {'train/accuracy': 0.9932321310043335, 'train/loss': 0.021475225687026978, 'train/mean_average_precision': 0.6224910459932752, 'validation/accuracy': 0.9870350360870361, 'validation/loss': 0.04480258747935295, 'validation/mean_average_precision': 0.2923386664862276, 'validation/num_examples': 43793, 'test/accuracy': 0.986178457736969, 'test/loss': 0.04778258129954338, 'test/mean_average_precision': 0.28342984384527276, 'test/num_examples': 43793, 'score': 15378.75329375267, 'total_duration': 22065.98137497902, 'accumulated_submission_time': 15378.75329375267, 'accumulated_eval_time': 6683.557134151459, 'accumulated_logging_time': 2.3388681411743164, 'global_step': 48351, 'preemption_count': 0}), (49109, {'train/accuracy': 0.9933977723121643, 'train/loss': 0.021004248410463333, 'train/mean_average_precision': 0.6172502556198101, 'validation/accuracy': 0.9870760440826416, 'validation/loss': 0.04562529921531677, 'validation/mean_average_precision': 0.29262855930217957, 'validation/num_examples': 43793, 'test/accuracy': 0.9862424731254578, 'test/loss': 0.04866080731153488, 'test/mean_average_precision': 0.28572762484789316, 'test/num_examples': 43793, 'score': 15618.89215707779, 'total_duration': 22406.239191532135, 'accumulated_submission_time': 15618.89215707779, 'accumulated_eval_time': 6783.601722478867, 'accumulated_logging_time': 2.3929810523986816, 'global_step': 49109, 'preemption_count': 0}), (49864, {'train/accuracy': 0.9931698441505432, 'train/loss': 0.02152334712445736, 'train/mean_average_precision': 0.6213504048826404, 'validation/accuracy': 0.9871621131896973, 'validation/loss': 0.045335717499256134, 'validation/mean_average_precision': 0.2962699552477201, 'validation/num_examples': 43793, 'test/accuracy': 0.9862942695617676, 'test/loss': 0.048360370099544525, 'test/mean_average_precision': 0.2828741073363153, 'test/num_examples': 43793, 'score': 15859.115253686905, 'total_duration': 22747.032354831696, 'accumulated_submission_time': 15859.115253686905, 'accumulated_eval_time': 6884.114263057709, 'accumulated_logging_time': 2.4300451278686523, 'global_step': 49864, 'preemption_count': 0}), (50621, {'train/accuracy': 0.9931451082229614, 'train/loss': 0.021721987053751945, 'train/mean_average_precision': 0.5996855145951254, 'validation/accuracy': 0.9870354533195496, 'validation/loss': 0.04542071744799614, 'validation/mean_average_precision': 0.2876952709587337, 'validation/num_examples': 43793, 'test/accuracy': 0.9862441420555115, 'test/loss': 0.04850063845515251, 'test/mean_average_precision': 0.2809669364114627, 'test/num_examples': 43793, 'score': 16099.191632032394, 'total_duration': 23090.439814329147, 'accumulated_submission_time': 16099.191632032394, 'accumulated_eval_time': 6987.389058351517, 'accumulated_logging_time': 2.4660558700561523, 'global_step': 50621, 'preemption_count': 0}), (51379, {'train/accuracy': 0.9931361675262451, 'train/loss': 0.021795811131596565, 'train/mean_average_precision': 0.6021560760301627, 'validation/accuracy': 0.9870212078094482, 'validation/loss': 0.04552444443106651, 'validation/mean_average_precision': 0.29312871920614536, 'validation/num_examples': 43793, 'test/accuracy': 0.9861915111541748, 'test/loss': 0.048558417707681656, 'test/mean_average_precision': 0.27908030634769676, 'test/num_examples': 43793, 'score': 16339.3504114151, 'total_duration': 23436.852647542953, 'accumulated_submission_time': 16339.3504114151, 'accumulated_eval_time': 7093.5865795612335, 'accumulated_logging_time': 2.5020413398742676, 'global_step': 51379, 'preemption_count': 0}), (52138, {'train/accuracy': 0.9931796193122864, 'train/loss': 0.02150198630988598, 'train/mean_average_precision': 0.6067070557150565, 'validation/accuracy': 0.9871442317962646, 'validation/loss': 0.04563544690608978, 'validation/mean_average_precision': 0.293471528182527, 'validation/num_examples': 43793, 'test/accuracy': 0.9862989187240601, 'test/loss': 0.04884331300854683, 'test/mean_average_precision': 0.2831817590489205, 'test/num_examples': 43793, 'score': 16579.44235610962, 'total_duration': 23781.13451218605, 'accumulated_submission_time': 16579.44235610962, 'accumulated_eval_time': 7197.719510793686, 'accumulated_logging_time': 2.5390548706054688, 'global_step': 52138, 'preemption_count': 0}), (52896, {'train/accuracy': 0.9931758046150208, 'train/loss': 0.021432511508464813, 'train/mean_average_precision': 0.6115915335475446, 'validation/accuracy': 0.9871913194656372, 'validation/loss': 0.04552037641406059, 'validation/mean_average_precision': 0.3000649690933642, 'validation/num_examples': 43793, 'test/accuracy': 0.9863018989562988, 'test/loss': 0.048905059695243835, 'test/mean_average_precision': 0.2820189862990282, 'test/num_examples': 43793, 'score': 16819.4209959507, 'total_duration': 24124.677789211273, 'accumulated_submission_time': 16819.4209959507, 'accumulated_eval_time': 7301.227471590042, 'accumulated_logging_time': 2.5754523277282715, 'global_step': 52896, 'preemption_count': 0}), (53651, {'train/accuracy': 0.9933634400367737, 'train/loss': 0.02081306278705597, 'train/mean_average_precision': 0.6317767925373252, 'validation/accuracy': 0.987166166305542, 'validation/loss': 0.045723769813776016, 'validation/mean_average_precision': 0.2988973768460285, 'validation/num_examples': 43793, 'test/accuracy': 0.9862803816795349, 'test/loss': 0.048975300043821335, 'test/mean_average_precision': 0.28024822331556093, 'test/num_examples': 43793, 'score': 17059.447257995605, 'total_duration': 24464.625368595123, 'accumulated_submission_time': 17059.447257995605, 'accumulated_eval_time': 7401.090955257416, 'accumulated_logging_time': 2.6128950119018555, 'global_step': 53651, 'preemption_count': 0}), (54403, {'train/accuracy': 0.9933775663375854, 'train/loss': 0.020713327452540398, 'train/mean_average_precision': 0.6225736436664356, 'validation/accuracy': 0.987188458442688, 'validation/loss': 0.0462249219417572, 'validation/mean_average_precision': 0.29682682278138706, 'validation/num_examples': 43793, 'test/accuracy': 0.986240804195404, 'test/loss': 0.04948662593960762, 'test/mean_average_precision': 0.2819279003808288, 'test/num_examples': 43793, 'score': 17299.483036756516, 'total_duration': 24806.26559662819, 'accumulated_submission_time': 17299.483036756516, 'accumulated_eval_time': 7502.6377511024475, 'accumulated_logging_time': 2.65012526512146, 'global_step': 54403, 'preemption_count': 0}), (55156, {'train/accuracy': 0.993545413017273, 'train/loss': 0.020183144137263298, 'train/mean_average_precision': 0.645784824725203, 'validation/accuracy': 0.9871426224708557, 'validation/loss': 0.04620135575532913, 'validation/mean_average_precision': 0.2940088533353942, 'validation/num_examples': 43793, 'test/accuracy': 0.9862689971923828, 'test/loss': 0.04963694140315056, 'test/mean_average_precision': 0.2751525052977334, 'test/num_examples': 43793, 'score': 17539.461524248123, 'total_duration': 25146.24363541603, 'accumulated_submission_time': 17539.461524248123, 'accumulated_eval_time': 7602.579082250595, 'accumulated_logging_time': 2.6879782676696777, 'global_step': 55156, 'preemption_count': 0}), (55921, {'train/accuracy': 0.9938235282897949, 'train/loss': 0.019473331049084663, 'train/mean_average_precision': 0.6602483439614082, 'validation/accuracy': 0.9870220422744751, 'validation/loss': 0.04617263376712799, 'validation/mean_average_precision': 0.29621272627443535, 'validation/num_examples': 43793, 'test/accuracy': 0.9861751198768616, 'test/loss': 0.04953019693493843, 'test/mean_average_precision': 0.278072814459413, 'test/num_examples': 43793, 'score': 17779.411629915237, 'total_duration': 25485.167813539505, 'accumulated_submission_time': 17779.411629915237, 'accumulated_eval_time': 7701.495981454849, 'accumulated_logging_time': 2.7243547439575195, 'global_step': 55921, 'preemption_count': 0}), (56685, {'train/accuracy': 0.9939702749252319, 'train/loss': 0.01885921321809292, 'train/mean_average_precision': 0.676547356716646, 'validation/accuracy': 0.9871426224708557, 'validation/loss': 0.0465945266187191, 'validation/mean_average_precision': 0.29267476355681427, 'validation/num_examples': 43793, 'test/accuracy': 0.9863424897193909, 'test/loss': 0.04987763613462448, 'test/mean_average_precision': 0.28062367859106613, 'test/num_examples': 43793, 'score': 18019.554752588272, 'total_duration': 25831.152674913406, 'accumulated_submission_time': 18019.554752588272, 'accumulated_eval_time': 7807.280650138855, 'accumulated_logging_time': 2.760650157928467, 'global_step': 56685, 'preemption_count': 0}), (57443, {'train/accuracy': 0.9942606687545776, 'train/loss': 0.01822679676115513, 'train/mean_average_precision': 0.6956112092775892, 'validation/accuracy': 0.9871117472648621, 'validation/loss': 0.04643256217241287, 'validation/mean_average_precision': 0.2988261302469323, 'validation/num_examples': 43793, 'test/accuracy': 0.9862766265869141, 'test/loss': 0.04957572743296623, 'test/mean_average_precision': 0.2852527176577683, 'test/num_examples': 43793, 'score': 18259.80503797531, 'total_duration': 26173.50976252556, 'accumulated_submission_time': 18259.80503797531, 'accumulated_eval_time': 7909.328285217285, 'accumulated_logging_time': 2.7992615699768066, 'global_step': 57443, 'preemption_count': 0})], 'global_step': 58134}
I0206 11:34:34.593660 139919816816448 submission_runner.py:586] Timing: 18477.266256332397
I0206 11:34:34.593717 139919816816448 submission_runner.py:588] Total number of evals: 77
I0206 11:34:34.593759 139919816816448 submission_runner.py:589] ====================
I0206 11:34:34.596407 139919816816448 submission_runner.py:673] Final ogbg score: 18477.167249679565
